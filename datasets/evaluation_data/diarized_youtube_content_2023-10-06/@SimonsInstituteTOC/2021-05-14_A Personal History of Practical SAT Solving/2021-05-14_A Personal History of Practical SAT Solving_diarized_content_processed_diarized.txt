00:00:01.160 - 00:00:47.304, Speaker A: A minute earlier than usual, but the second derivative on arrival rate seems to have calmed down. So this is our final meeting of the 50 years of satisfiability workshops. It's almost the last meeting for the SAP program, but we do have one further meeting today at 11:00, in two and a half hours from now, I should say, which for a town hall wrap up meeting. And Antonita sent some announcement out to the program participants and encourage you to come. If you didn't get her message and want to come, just let me know, we can drop you the link. I think the link for the meeting, though, is not the same as the seminar link. So you should go to the link that's in Antonina's email message for that.
00:00:47.304 - 00:01:52.550, Speaker A: So I encourage everyone to come to the town hall wrap up meeting after this, and then otherwise, as I said, this is the final of the workshops in the 50 years of satisfiability sequence. And one of the themes both of the workshop, but of the whole SAP program, is the interplay between theory and practice. And so we actually are ending with two talks, one by a person who I would say is maybe more on the practical side, Armin, and the one who's more on the theory side. But nonetheless, I mean, both people have contributed to both sides, right? Both the theory and the practice. So Armin has done a lot of practical sat solving, but also contributed a lot to the theory of sat solving, especially as it pertains to the practical aspects. And he's speaking today on a personal history of practical sat solving topic I'm really looking forward to. And he's asked me, and I think he'll say it again himself, to encourage interaction and questions.
00:01:52.550 - 00:02:04.634, Speaker A: So for this, please post to the chat window or just break in and speak up and we'll be happy to take interruptions and otherwise. Armin, please go ahead. Thank you.
00:02:05.054 - 00:02:45.244, Speaker B: Okay, thanks, Sam, for the introduction. And also thank you for the invitation. When I said yes, when Sam invited me, I knew that this would be a really, really, really busy time. And then when I had to come up with a topic for this talk, like two weeks ago, I thought like, okay, I almost have no time, but there's this thing I always wanted to explain a little bit more. And that's actually this slide. So this slide is usually the slide I end with most of my invited auditorial talks on SAT. So if you have seen my two other talks in this semester.
00:02:45.244 - 00:03:29.664, Speaker B: So like one was 4 hours in the bootcamp, and then there was another one which was 2 hours, then it ended with this slide, actually with a previous version of that one. And now I have like slightly less than 1 hour. And what happened in these other talks is usually that I cannot really go into this history. And it looks to me that why not trying to, like, go through some of these, hopefully very interactively with you would be a best fit for this workshop. So here's my personal satsovg history. And of course, I apologize if I miss anything else. It should say maybe personal practical side solving history, of course.
00:03:29.664 - 00:04:22.956, Speaker B: And if I miss some name, please, I apologize anyhow. So please interrupt me anytime. So Sam will monitor the chat and of course, like for the panelists, I'd be happy to see you too, but I'm also happy to just answer your questions through Sam. So we come back to this slide at the very end again of this talk. So the talk doesn't have that many slides and should have then also plenty of time to go through some of these points. But why do I actually do this? And he's like four or five slides I did also for the bootcamp introduction talk. Well, this is a slide I did for my dean once, but he wanted to know what I'm doing, and I just collected the slides, the medals we received for our sad silvers until 2015.
00:04:22.956 - 00:05:10.384, Speaker B: And I took this picture, and maybe more interesting and more recent, of course, here is this plot, which was done together with marine. So we took the sad competition 2020 benchmarks, and then they're running it on our cluster for 5000 seconds. So 1 hour, 23 minutes. And what you see down here, here is here this time. So 5000 seconds here, the time limit. And on the y axis, you see the number of solved instances, if you paid attention to sart solving history in particular on the practical side, so you will see that this is not a cactus plot. It's kind of a cactus plot where x and y axis are flipped.
00:05:10.384 - 00:05:40.084, Speaker B: And there are some who prefer that I started to become one of those. So higher is better here. So you solve more benchmarks, obviously. And what you see here, this is this legend here on the right. And you see also, like here, this blue dot here is this oldest sovereign here. This was the oldest Soviet which won a competition in this century. So we have had two competitions before, in 1992 and one in Beijing.
00:05:40.084 - 00:06:15.374, Speaker B: And these are like those competitions which happened in the last 20 years. And like my solver hill, Kiss art here, solved a huge number of benchmarks, compared at least the previous year's winners. So last year there was a version of the year before. So in 2019, there was a version of Maple, not by the original authors. Actually, this is already the second generation of authors which modified maple here. And, yeah, every year, like last year, there was a maple version. So that was a big improvement in 2016.
00:06:15.374 - 00:07:00.426, Speaker B: And this one, you see this jump here. In 2014, my solver won, and also in 2013, in other words, then there were two glucose solvers and so on. So you see how here on the right, the solvers, the winner of this particular competition. A map to the number of solved instances here on the left. And I would claim this shows a big improvement in sad solving. If you have attended also the talk from Holger last year, we're struggling a little bit in how to assess sort of the improvement of SAP. But I think this plot is convincing, and in particular for one reason.
00:07:00.426 - 00:07:41.094, Speaker B: In 2016, the SAP community in Bordeaux, where this solver won, and there were lots of solvers very close to this one. We kind of were a little bit frustrated because we don't understand, like, how to rank solvers anymore. But I think, like, this method here shows some improvement. I should also say immediately that there was last year, actually to solve was right behind this winner here, which was my solver, Kisak, and. But they're not here on this plot. So there was just a big jump last year in terms of sort of robustness and solving capability and. Yeah, so this triggered some tweets, and I explained this already in the bootcamp.
00:07:41.094 - 00:07:43.010, Speaker B: Yes, yes.
00:07:43.042 - 00:07:52.272, Speaker A: So on your previous slide, actually, it's two questions. First, you mentioned there were two other solvers that had a similar improvement. Was it due to the same techniques or different.
00:07:52.328 - 00:08:26.964, Speaker B: Yes, same technique. So it's like, interesting. So they both put local search in it, which actually, I proposed in 2019 already. And they also use this variant of target phases, both of them. So this is crypto minisat and the version of Maple, like this maple here, but with CCA in our edit, this came in second. And so all three, also catego, by the way, now have local search solver in it to sort of fix phases. And they all have some variant of these target phases, which I described with Matthias last year's post talk.
00:08:26.964 - 00:08:29.964, Speaker B: We can go into that if you want, but.
00:08:30.784 - 00:08:41.744, Speaker A: Well, that would be interesting. But let me ask my other question is, what if you ran the same types of tests, say, on one of the early SAT programs, say, went back to, say, 2005, and ran all these software on?
00:08:41.784 - 00:08:47.008, Speaker B: This is. Of course. Of course I will do this. This is one of the parts I show you six more of these, actually.
00:08:47.096 - 00:08:47.624, Speaker A: Oh, very good.
00:08:47.664 - 00:09:09.164, Speaker B: Okay, so you see lots of them, actually. So we come to that. And so in some sense, I'm showing like cdfs, which like this community distribution functions, which are reactions to like what people ask me after I'm showing this the first time. So that's why I kind of anticipated this question. All right, there's more questions. Or should I.
00:09:10.864 - 00:09:20.652, Speaker A: Daniel posted a link in the chat window to one of your papers. Armin Bureau flurry pods 20 talk.
00:09:20.828 - 00:09:44.712, Speaker B: Yes, exactly. Now again, I have this window which I can't get away. I should have not clicked on this. So I move this up just a second. It doesn't go away. Maybe last time, what did I do? So I clicked on participant, then the window shows up and then close. Okay, great.
00:09:44.712 - 00:10:18.152, Speaker B: Working again, sorry about that. Okay, now there was also some of you might have seen this. So we posted this, or I posted this with the help of marine on Twitter. Actually got quite some reactions to that, which was surprising. And people were astonished that you can solve this many variables. So here's like a distribution function. Actually the, the largest instance in the sub competition has 700 million variables in the planning track.
00:10:18.152 - 00:10:46.968, Speaker B: Not here, this is the main track. So this is quite substantial. And also like on the right, you see here, there's this issue by somebody using CBMC to verify deep neural networks. And inside of this verification, they're using bullector, one of our SMT solvers. And in that there's lingiling by default, like an old solver of mine. And this has a variable limit of two to the power 27, what you see here. And then of course the scene crashes.
00:10:46.968 - 00:11:15.488, Speaker B: And it's really amazing, like how far they got. Actually, they would probably need like a half a terabyte machine to actually run this. But it turns out that actually people are producing larger and larger benchmarks. And it's really no joke what I'm doing here. So Andrew Joan is using bolekte in the company in the UK. And he already also figured out that, yeah, maybe sometimes you really want to have like int max variables. So this is 2 billion variables.
00:11:15.488 - 00:11:44.056, Speaker B: And now my actually are working with this large number of variables. So this was direction then of Ina and. Yeah, so you see here the numbers, maybe this is interesting. So lingering has two to the power seven. Catechel has really int Max keysat has currently two to the power 28, which is like 256 million variables. And I'm pretty sure if I continue hacking sat solvers for a while, I will need to move to 64 bit variables. So more than 2 billion variables.
00:11:44.056 - 00:12:00.524, Speaker B: I think this is really amazing. Then, of course, we did the handbook. You must have seen this a couple of times. So I'm just showing this to sort of like, yeah, I should show this. So this is the old handbook. We had this from 2009, and we had these chapters. And.
00:12:00.524 - 00:12:27.726, Speaker B: And then there's a picture with Donald Knuth. Donald Knuth also got into SAaT, and he actually visited us here in Linz, gave a nice talk, and. And played the Bruckner origin. And then he wrote this 300 page subsection of the volume four. And it's also only about saTs. And. Yeah, here's the beginnings.
00:12:27.726 - 00:12:59.888, Speaker B: I'm sharing this, too, that there's really, like, 300 pages. Of course, half of them are exercises, so you should know it's not like 150 pages of real material, but the exercises are really tough. So some of our papers just turn into, like, a two line exercise. Okay, then this year we're going to have the second edition. So this is, of course, like, should be mentioned when we talk about the history of saTs. So these are seven new chapters, and you can read them. So I'm not going through that now again.
00:12:59.888 - 00:13:34.632, Speaker B: And they were also mentioned here in several talks in this special semester already. So this is how it looks. Actually, I haven't seen the physical version yet, so people keep asking. But I got an invoice yesterday, so it seems to be shipped. And now I come to one variant of what Sam asked. So this is another plot. So it's very similar to the other one, but it has 21, I think, or like roughly 20 sartors.
00:13:34.632 - 00:14:09.424, Speaker B: The other one had just 13. So it, for instance, has here, it's the same data as before. It has the grasp from 1997. So with the help of Shoao, I was able to get this working, because it's not trivial to port this old C core to newer machines. The same is true to Sharad and Moskiewicz. Yeah, actually, this was a little bit difficult because they had various versions of this, Jeff and the hard disks, and some of them were compilable, some not. So I have here one version which I think is consistent.
00:14:09.424 - 00:14:40.040, Speaker B: And you see it fails here somehow. Actually, my first solver, Leemat, I should tell you this history about that one. So I read this paper by the friends from Princeton, by chaff, and I could not believe it. So I don't know how often this happened to you in your career. So I would say at least, at most, half a dozen times. And this was one case I read this paper, and I couldn't believe what they were, what they're claiming, okay. And my reaction was, I had to implement this, what they're saying.
00:14:40.040 - 00:15:13.324, Speaker B: And of course, originally I failed, so I was never able to get their speed up. But then at one point, when the source code became available, it actually was Linthao's version, not this version. So Lintao Chang with a z. This is the z chef. So this is sometimes called Mchef, or the original chef was called just Chef by Moskovic. So this was the sort of the source code of the original paper, which actually kind of was never really published, was only available very briefly on the Internet. So the one everybody was doing afterwards was the version of Lintau.
00:15:13.324 - 00:15:55.376, Speaker B: And only after, like, Linta's version came available, I was able to fix some issues actually related to unique implication points, which is actually an interesting current topic. Yeah, I added some more. We had a big step forward here in 2005 with this. This is kind of the preprocessor I did with Niklas am, together with Minisat, or it was not minisat. It was the predecessor of minisat. And minisat was also here in this competition, but was worse than the version with the satellite. Then the next minisat here, they incorporated this preprocessing insight.
00:15:55.376 - 00:16:30.024, Speaker B: So this is why people improved. Then there's cryptominisat, like precocatis of one of micellar, which won a competition. Then there's kind of a period where glucose was leading. Then I came up with this lingering, which had lots of this in processing. So this was kind of the start of the in processing phase. Actually, all the solvers above here use in processing, not just pre processing, but in processing you can think of. So pre processing started here in 2005, and then since 2013, all the winners have in processing in it.
00:16:30.024 - 00:17:19.194, Speaker B: And then claims that sort of we came here also to kind of from here, right to a kind of point where there was progress, but slow progress. And then last year, we saw another big jump. So maybe one thing you can really see here, these jumps come in maybe five year distance, and we have seen them every five years. And furthermore, there are often combinations of techniques, which is also, of course, difficult or not nice from a scientific perspective. When I explained today, the big jump here was due to two techniques. The same was true here for this jump from here to here because it was the minisat implementation. But also it has lots of things which I could explain, plus the pre processing.
00:17:19.194 - 00:17:41.694, Speaker B: Okay. And, yeah, one thing I also want to point out in the SAT competition had only, like in the very beginning, closed source solver. So this Birkin here was never available and there's a siege set solver for which we don't have source code and there's some intel solvers too. Okay.
00:17:42.634 - 00:17:55.810, Speaker A: Armin, could I ask since no one else is asking a question, I'll jump in again, but I hope someone else will take up the slack for me. I just heard on Monday a rumor that minisat was named minisat because of clause. Clause minimization.
00:17:56.002 - 00:18:08.242, Speaker B: I always thought that could well be, but I don't know because I have papers with both authors and I don't know. So I should ask Niklas. The Niklas. Niklas was the one who invented this minimization.
00:18:08.338 - 00:18:17.938, Speaker C: Actually, no, mini set is minimalistic set. It's not. Yeah. So in 2003 that was the first version of mini set it. Minimalistic set.
00:18:18.106 - 00:18:19.018, Speaker A: Ah, okay.
00:18:19.066 - 00:18:19.362, Speaker C: Yeah.
00:18:19.418 - 00:18:19.850, Speaker B: Okay, good.
00:18:19.882 - 00:18:22.706, Speaker A: That's what I always thought. But I heard this other rumor going around.
00:18:22.770 - 00:19:04.262, Speaker B: No, it's also because it's on Niklas Servantsen invented this minimization and it was later. Right. This was parallel with our satellite work here, which is with the Niklas A, not with Nicholas servants. Okay, so time wise, that's probably right. All right. All time winners on the sad competition 2011 benchmarks. There was kind of a claim by some people, I will not repeat the names in the post workshop two years ago that they were not happy with how we do the ranking and nothing changed since.
00:19:04.262 - 00:19:54.054, Speaker B: 2011 was one of the things put forward. And before I explain that note. So these are the same solvers, but on the 2011 benchmarks, these here are way harder benchmarks. So these are 400 benchmarks and we're like solving 250 while, while, sorry, while these guys here, they are from 292, right? So except for now maybe 40, 50, we solve all of them now. So this is a quite different benchmark set. So in a certain sense I would maybe say yeah, because these benchmarks were picked in a certain way. But I guess some of the people who helped to pick them are in the audience.
00:19:54.054 - 00:20:17.354, Speaker B: So maybe you should really sort of cut off here because here maybe this is a different type of benchmarks. Anyhow, I wanted to show that we have exactly the same trend except that the things are more compact. But that's probably just due to a smaller number of benchmarks like 292 versus 400.
00:20:17.814 - 00:20:33.994, Speaker C: Yeah. Amin, may I comment on this? I think that there is also a big difference is that some of the solvers have seen those benchmarks. So all the solvers and so they have been train tuned and which is not the case for the 2020.
00:20:34.574 - 00:21:13.104, Speaker B: So that I should point this out that of course, like, all these solvers here, like, since 2013, are heavily tuned on these guys, right? So they have seen them and try to improve them. So they've been running on the cluster for a long time. But here, these benchmarks are brand new. And I think, like, there were like 100 use old ones, but 300 new ones or something. But I'm happy to be corrected here. And they are considered way harder than in 2011.
00:21:14.564 - 00:21:23.144, Speaker C: And I guess the variety of benchmarks is also much more important currently than ten years ago.
00:21:23.724 - 00:21:42.514, Speaker B: Yes, exactly. The organizers do the following, so you have to bring your own benchmarks. That means you have to always submit some interesting benchmarks. And then they pick from each source at most 20. And they don't even always pick 20, so that. And this is not the case here for. For 2011.
00:21:42.514 - 00:22:13.982, Speaker B: Okay, then, Danielle, because you're here. So you tweeted it two weeks ago, and. Well, because I was busy, I wrote, oh, I cannot do this. What you write here at the bottom. So he was announcing this talk here, Daniel, and he said, like, I remember the solvers, leemat Komsa, nanosat, Picosat and more. And I've been, like, submitting to all these competitions. And then Daniel asked he would want to see a cactus blot.
00:22:13.982 - 00:22:41.120, Speaker B: And then I answered, which is not on the slide. Yeah. Well, I will show you a CDF, because we kind of disagree whether CDF or cactus blot is better. But then, actually, Mathias was so nice. Matthias Fleury at postdoc in my group was so nice to run some of these solvers, actually, most of them. And I just finished one more run an hour ago. So I'll show you this on the next slide.
00:22:41.120 - 00:23:23.514, Speaker B: And here's one more thing I wanted to show you. So, in the middle, there was another tweet where somebody was working on this Conway life, and they tried to find sort of like some sort of stable or sort of producing or things which kind of walk forever, um, state machines, in a sense. And Adam Gaucher has been doing this with the SATs over. So he puts a thank you to me because he was only able to do that with. With Kisat in catechol. Okay, so. So this is the plot which Matthias helped me to produce.
00:23:23.514 - 00:24:28.700, Speaker B: These are almost all solvers of mine. There's like, just a second, it says my Internet is unstable, but you can hear me, right? So I have a couple of solvers I lost, it seems. So I found them in my cv, but I cannot find the source code anymore. And most of them, I mean, like, I claim, actually all of them are really written from scratch. So I do this thing that you kind of throw things away and then start from scratch. Of course the algorithm are kept and then refined and then you kind of concentrate on the sort of minimalistic part you need to achieve a certain effect. And you saw already Leemat here, which one track in the competition in 2002, but it's like a little bit strange because they had at the end a tie and then they used three benchmarks to break the tie.
00:24:28.700 - 00:24:49.636, Speaker B: And my solver was solving one more benchmark than the competitor. So I was winning kind of by one instance. Yeah. And then there's lots of solvers like this. Such solver I did just for this seminar, for this bootcamp. So this is the latest, this 4.17 is the latest public version.
00:24:49.636 - 00:25:32.484, Speaker B: In the meantime, I played with adding preprocessing to it, but only variable elimination. And when I be able to release it, there will be something more. And you see from here to here also a small jump. And of course, the reason why such really leads all the other ones here is because it kind of incorporates all the lessons learned from keysight and cadicle in terms of what you need to get like a fast CDCL loop. And it was a highly configural documented, so I urge you to look at it so you can also watch the other two talks if you want to learn more about this sober. I expected a question here, but nobody asked the question.
00:25:33.104 - 00:25:41.280, Speaker C: Yeah, so, I mean, so what was the big difference between nanosat and picosat? Because you see, there is a huge difference.
00:25:41.432 - 00:25:54.384, Speaker B: Oh, this is this Rsat thing. And also reducing was not good in nanosat. So these two silvers, nanosat and Komsat did not reduce clauses at all.
00:25:54.764 - 00:25:55.404, Speaker C: Yeah.
00:25:55.524 - 00:26:23.554, Speaker B: So. And which is very funny. So if you would compare. So you will, you will see that grasp has similar performance as nanosat. Sorry, Compsat. And then nanosat actually also had inferior scheme into reducing and also had another problem. So you only used one watch for learn clauses, which is of course not something you want to, want to do.
00:26:23.554 - 00:26:44.648, Speaker B: And then Picassat was a big step forward and yes, then cliniling was like such a solver, which was meant for sort of a summer school, actually tutorial and. Yeah, and so on. So I could explain more about the single ones if you want, but I have some more stuff.
00:26:44.816 - 00:27:03.604, Speaker C: Yeah, and just one more thing. So which solvers are running local search among those ones in parallel to include local search in addition to CDCL. So key set.
00:27:07.344 - 00:27:08.964, Speaker A: I think we may have lost.
00:27:09.604 - 00:27:11.668, Speaker C: No, no. Amin is here.
00:27:11.796 - 00:27:13.044, Speaker A: Oh, here is back.
00:27:13.084 - 00:27:14.264, Speaker C: Okay, good. Yeah.
00:27:15.564 - 00:27:29.476, Speaker B: Armin, you're muted. Yeah, I was kicked out. It said before my Internet connection is strange, so I'm not sure. Anyhow, so we were talking about local search, right?
00:27:29.620 - 00:27:37.544, Speaker C: Yes. So I was just asking how many of your solvers on this plot are using local search?
00:27:38.904 - 00:28:18.392, Speaker B: This was exactly the point, I want to say here. So there's this jalsat, which is a local surf, and I put this on purpose here, and it solves more benchmarks than Limaat. And if you go back here to this plot. Oh, here, I don't have it. But you see that Limat was actually here, way above the source, which were, like, from the. From last century. So it's easy to claim that, at least in 2020, and the same is true, but to a less extent here in 2009, that these pure local self, so we can only solve satisfiable instances, is actually doing.
00:28:18.392 - 00:28:34.804, Speaker B: Not bad. Right. And that's true to some of the part of the history I have on my slide. But, yeah. So these two are only doing local search. So satch does not have local search in it yet, so only these two guys. And, of course, this is a pure local search solver.
00:28:35.494 - 00:28:53.358, Speaker A: So, Armin. Armin, you mentioned a couple of concepts that I'm not sure what you meant, and maybe others don't. You mentioned the earlier solvers didn't do very good. Clause reduction. And then you've mentioned local search several times. I was maybe just say a few sentences on what those two concepts are.
00:28:53.486 - 00:29:23.922, Speaker B: Yes. So clause reduction means that, like, so in compsat, I just learned clauses, and I never. I always kept them forever. And I really thought, like, this is good. But then there was a competition where, like, oms had lost traumatically against other solvers. And the main reason was because the other solvers eagerly were throwing away clauses. And so I learned.
00:29:23.922 - 00:29:39.510, Speaker B: So throwing away learn clauses which are kind of not used. This is the. It's called reduce in the solvers. And, yeah, you could think of it as a cache. Right. So it's like the learn clauses are a cache of the search, and you cannot. You don't want to keep.
00:29:39.510 - 00:29:53.694, Speaker B: Even though you would have space, you don't want to keep these clauses because they slow down the propagation, does the whole sovereign. So you just throw them away. Yes. And then the local search, it's really. Yes.
00:29:53.814 - 00:29:57.574, Speaker A: The clause reduction is the same as clause deletion, in other words.
00:29:59.274 - 00:30:29.484, Speaker B: So reducing the clause, the learned clause database was kind of the, I think, original idea why people started to use reduce as a term for that. It's a short word like restart, reduce. We have one which is called refacing. So that's why I like this. So it's a work which describes this part of reducing non cost database. And then local search really is a classical local search algorithm. And yeah, this is what I expected.
00:30:29.484 - 00:30:58.284, Speaker B: So we're running out of time. So I would have a way of explaining it on the blackboard because this is one of my favorite things which happened in the last ten years. But maybe we'll do this only if we come back to this. I would want to explain pubsat, which is a very cool idea and very easy to explain. And there was not much talk about practical local search in this whole seminar. So that's why I thought maybe that will be a way to, to sneak this in. But, and Trialsat is a version of this popsat idea.
00:30:58.284 - 00:31:01.984, Speaker B: Okay, so it's a walksat variant.
00:31:02.144 - 00:31:13.056, Speaker A: There's a question in the chat window, if you don't mind. Massimilario asks, are any of the results are obtained with something like XOR or cardinality detection question?
00:31:13.160 - 00:32:05.984, Speaker B: Yes, Lingerling has this in it, and that's one of the reason why, like for instance, if you also saw the talk by Holger and you paid attention to his way of measuring a contribution, that was lingering, actually highly ranked. And the reason for that is because lingeling does pre processing with exel clauses and cardinality constraints. And if you have sort of instances like pigeonhole or things like that, which can just be done by extracting for instance, cardinality constraint and then doing some kind of elimination, or the same with satin formulas, and you run lingering on it, then it will just instantly solve these instances. And that's of course gives you a big contribution, but from a real practical perspective, it's kind of useless. So none of my new solvers is implemented, but if I would implement it, I could solve like more instances.
00:32:09.004 - 00:32:09.864, Speaker A: Okay.
00:32:11.564 - 00:32:34.344, Speaker B: All right. Not much more. So I have the same for 2009. So we discussed this and I have less solvers, so we ran out of time in producing this. I probably want to skip there, unless somebody wants to ask something here. And then I have two more slides and then we could go back to the timeline plot. So here's like things we could also discuss and.
00:32:34.344 - 00:33:23.988, Speaker B: But I again think we should focus on the timeline and come back to this if, if you're interested. Okay, so this is the timeline I usually show at the end of invited talk. And now if I count right, I have eight minutes to explain these things. All right, so here we have DP and DPl. Actually, this should be put probably a little bit earlier because, yeah, I saw a technical report where this research was described in from 58, but the journal article appeared in 1960 and 1961, I think. And, yeah, then you would ask, what's the next thing here? Well, this is where I was born. And then we have this starting point for this 50 years, name of the seminar.
00:33:23.988 - 00:33:51.856, Speaker B: Right. Like 71 was the proof from Stephen Cook and pre completeness of satisfaction. And this red line here, this is like where we are now. And you see already sort of like one milestone here. Well, we should have the handbook of the second edition of the handbook in our hand this year. The first one appeared in 2009 here. And in the middle there was Knuth's book on Sat.
00:33:51.856 - 00:34:25.396, Speaker B: So this is kind of the, I would say for me at least personally, the biggest milestone. Yeah. Then let's look here at the first part here. So here I put satin encoding. This was of course used in the proof complexity community, not really in the practical Satsovic community. But when we did this bound model checking at CMU 98, we worked on this. Actually, people worked on bdbs and didn't know how to actually get the formula into CNF.
00:34:25.396 - 00:34:57.008, Speaker B: Right. And kind of from a practical side, it was extremely important to know about this fact. And we just learned this sort of like through the theory community. And yeah, I remember talking to some colleagues at CMU who said it's impossible to do sad solving to really industrial benchmarks because you cannot get a short formula to describe something. But it's not true with the satin approach. All right, then. Yes.
00:34:57.008 - 00:35:11.284, Speaker B: Wasn't the original citing encoding already in around where the asterisk is by citing? Yes, I think this is the wrong date. You're right. So I think they're like, there are citations here. Right. Also in Russians.
00:35:12.984 - 00:35:14.624, Speaker A: Yes. 1968.
00:35:14.784 - 00:35:28.234, Speaker B: Yes. Also like close where I was born. Okay, good. I need to move this. Thanks. But anyhow, I wanted to point this out. This was really like early, and this was extremely important for adopting here.
00:35:28.234 - 00:36:03.324, Speaker B: When we wrote this BMC paper for DAG, this was the main point explaining the Tzetin transformation. Right. And that led these people really apply Saat in the practical sense. Yeah. Then there was a, like in AI, there was a big sort of concentration on sats and in the early nineties, and it started already like ten years earlier maybe. And this, the focus back then was this local search I was talking about. But people also started to look at, look ahead, but like, not that much.
00:36:03.324 - 00:36:56.660, Speaker B: And one important part there was, they applied it to Sat. They applied sat for planning, but I would say still sort of more in an academic way. So that the difference to our BMC application work here is, of course, this has had immediate consequences for applying these things in practice. Then there was a big. So they used this GSAT method, another method which appeared later, which is called Walksat, also based on local search, is better and improved these local search firmatically. And then, of course, you all know that Shaw Mar Kisila, Mkarem Sakala came up with this CDCL, which where in my view, the main part is this learning and the implication graph. And in learning, in a sense, exactly what we discussed before, keeping those clauses.
00:36:56.660 - 00:37:24.238, Speaker B: So, in this CP community, they also had this idea of no good learning. They had back jumping. They don't describe how they compute these no goods. But Shaw and Karen were explicit on using this implication graph, and then they also introduced the first UIP clause. But you need this reduction. We talked before, actually, in order to really make this work. And that's kind of the big game changer here.
00:37:24.238 - 00:38:17.182, Speaker B: Now, our DMC work led to some more people looking in at the SAT. So, like Sharad Malik and his group in Princeton actually used this to get a project on trying to implement SARS over into, into hardware. And out of this project came this sort of the Jaff. And this results on vsits, which was another big breakthrough inside. And then I should put here a minisat, which I simply forgot, it seems. So the two minisat authors here are responsible for. It should be here for really putting lots of new insights into how to implement these, these ideas fast, and in particular, how to do visits fast and other things in the Sarzo.
00:38:17.182 - 00:39:12.998, Speaker B: And actually around the same time, which is also quite interesting from historical perspective, the satisfiability model view theory came around, and it really needs this idea of CDCL to really make use of this sort of communicating theory solvers. And that's, for many people, one of the big improvements. What's also funny, there was a big distinction between SAT and SMT, so people were careful to say this uses a use of SMT solver. Now people use more and more, actually call it even SMT solver, Sat solvers, which is also a pretty intriguing question. All right, so here is like bounded by yeb elimination, which is the first really very powerful proof preprocessor, and still today probably is the most important one. But in essence, it's just a variant of this old DP, but using in a slightly different way. Right.
00:39:12.998 - 00:39:54.144, Speaker B: Really using it as, as a pre processing and bounding it. Then two more things I wanted to explain. So there's this propsat, which is a very cool local search variant of walksat. Very easy to explain. If I would have more time, I would have explained it, but I think we're running out of time. Then face saving was also a big idea. And this is just to which value should you assign a variable? And this gives really impressive practical improvements without really being really complicated from a sort of implementation perspective, it's like two lines or three lines of code.
00:39:54.144 - 00:40:29.370, Speaker B: And yeah, here is LBD, or glucose level, which was invented by Gilles and Otmar. And this gave a new view on how to, how useful are learned clauses. Once again, this CDCL here, compared to the previous CP work, kept the learn clauses. Now keeping all the learn clauses. You cannot because you will just run out of steam in propagation. This was my compsat. Now you need good ways to remove clauses.
00:40:29.370 - 00:41:23.120, Speaker B: And this was kind of the big sort of improvement here was done by my two french colleagues in 2009. Yeah. Then I want to maybe highlight the, in processing I mentioned already, like in, since 2013, all the windows of the sub competition use in processing, which means you, you really interleave CDCL search and pre processing. But I want to point out here this part which, which I think I'm sort of like as a SAT person, I'm really proud of. So this is avatar is a technique the vampire theorem prover, which is kind of the world leader in first order theorem proving uses to implement first order resolution. They actually drive the whole resolution process by assets over these days, and they even query sat servos. It's actually very similar to how sad servers are used in SMT.
00:41:23.120 - 00:42:16.884, Speaker B: So you can really claim that first order theorem proving. The best solvers there, in particular in practical application, use sat as their core. And it goes further, because if you think about like, what do you do if you verify, let's say, an operating system kernel? Well, you will probably do this in Isabel or in some other like higher order logic framework. And the biggest improvement probably in the last ten years in this domain was to come to use the use of these hammers. And hammers are sort of like techniques to take this first order problem and translate it, abstract it to a, say, first order or SAT or SMT problem, and then use a sats over. In any case, even if they would use the first order theorem prover, they would use a Sat solver to solve sort of higher order logic problems. And then from that you would get either proof or if you try to find the counter example.
00:42:16.884 - 00:42:49.084, Speaker B: Yeah. And then this brings me and I'm running out of time, 1 minute over time. But we still have like 15 minutes for discussion. So there's a couple of things which, which are green actually today I removed this green thing here for arithmetic solvers because we do have a couple of papers. We also contributed to it on really using algebraic reasoning. That's what I mean here. But also maybe pseudoboolean reasoning could be put there to improve the effectiveness of Sard solvers.
00:42:49.084 - 00:43:27.454, Speaker B: So what's missing here, that's why I was reluctant to really put this here is really doing it on the CNF level that nobody has done yet. So the only way how we can do it is like this is what lingering does like extracting this algebraic information and then running an arithmetic solver on it. Also encrypted minis that they're like attempts in this direction. Yeah. Here are things which I think still need some work. So the parallel part is, is not solved yet. So doing sat on parallel machines from on all levels is unclear.
00:43:27.454 - 00:44:10.080, Speaker B: Maybe we know what to do if we run a portfolio. So I mentioned this here. So maybe up to eight cores, 16 cores or something but having like 96 cores or I don't know, thousands of cores on your graphic cards or running the software in the cloud. This is really unclear. We know how to do it. For hard combinatorical problems this is this cubing conquer somehow still partially manual this work the splitting part but using SaaS overs in a massively parallel way just for any problem that's unclear for QBF made big improvements. We have also new chapter but we are still lacking a sort of killer app.
00:44:10.080 - 00:44:55.258, Speaker B: So sats had killer apps. I claim the first one was our DMC but there are other apps now of course lots of applications of Satnow for QVF. You don't have that I claim and I would be really happy to prove it wrong here sat and SMT everywhere. So this is slowly actually happening. So if you're like do submit jobs to the Amazon cloud they would schedule these with CVC four and would solve some constraints which in turn you would use that technology. So maybe these sats overs are slowly getting anywhere. So I would say we're looking forward to more work on this.
00:44:55.258 - 00:45:18.764, Speaker B: And yeah I still think the history of SATso this improvement is Sat revolution which I put in the abstract is actually continuing and I don't see any slowdown. We're making progress every year both on theory and on the practical side. Okay, thanks so that concludes the things I wanted to say.
00:45:21.104 - 00:45:34.888, Speaker A: Thank you very much, that was a very nice talk. I'll clap on behalf of everybody, and we have definitely some time for questions, so I hope everyone will jump in with questions and so forth. Let's see.
00:45:34.936 - 00:45:49.534, Speaker C: Yeah. May I ask. So, I mean, so we're in, we are back in 2002, and you have a chance to implement one thing from 2021 in limat. What would it be?
00:45:53.114 - 00:46:23.146, Speaker B: That's interesting. Well, the face saving for sure. I'm not sure, like, face saving only works together with restart, so I'm not sure, actually, no, I think, like, the way how cost reduction done is extremely clever. So maybe I should mention this. So this is restarting and the cost reduction. So these are things like, which partially were solved here. Now I think I know how to do that.
00:46:23.146 - 00:46:55.510, Speaker B: So of course, like, as I said, it was partially solved around the time. So with this LBD, and also here with face saving, but it took still ten years to really find a way how to do it precisely. And of course, like, maybe somebody comes up with an even better way, but the current way. How cost reduction. You can read this up in my, in the source code mentioned of such. This is what I would say is the biggest improvement. I'm also not sure I forgot to mention Chan sec O's work.
00:46:55.510 - 00:47:47.366, Speaker B: I should really put this here. This happened in 2013, I think, like, switching between fast and slow restarts, and that's also very important. Yeah, I don't know. Very, very hard to say. I would say it's. The reduction is doing. Reduction is probably the most important, because if you can sort of remove, like you see, like the LVD allowed us to remove, instead of keeping the sort of like a fraction of the learned clauses, a fixed fraction, because you had a geometric increase of the reduced schedule with this LBD, we were able to move to an arithmetic one, which essence means you can, after, like 10,000 conflicts, you only keep a square root of 10,000 clauses without losing efficiency.
00:47:47.366 - 00:47:52.434, Speaker B: And this really improves your solvers kind of quadratically, if you think about it. Right.
00:47:55.134 - 00:47:55.874, Speaker C: Thanks.
00:47:56.444 - 00:48:11.104, Speaker A: There's a question in the chat window from mate Souz. There seems to be a lack of understanding how different techniques interact. Why do you think this is the case? Too difficult, too obvious, too many techniques?
00:48:11.844 - 00:48:51.134, Speaker B: No, it's. So this is a very good question. And Martin knows this, of course, but he doesn't do it this way. So this is why I throw away the sad silvers, like, once in a while, because I think the only way to figure that out is you only implement some of the stuff you did before and then figure out what's really sort of the smallest combination of things. So this just happened actually to Matthias and me. So we presented this target facing, which is one of the parts of one of the reasons for this improvement last year and this target phase. We thought we also need to randomly flip the signs and also to, to flip it somewhere.
00:48:51.134 - 00:49:13.674, Speaker B: But this is really not useful. This is just not necessary. And we only came to this conclusion after implementing such and in such, it didn't make a difference and then reported it back to Catego and Kisab. So it's a very painful way of figuring out what are the things which are kind of dependent and which need to be kept and which not.
00:49:16.834 - 00:49:30.814, Speaker A: See another question of the chat window. Valentine Meyer Eichberger asks, quote, QBF lacking killer application. Quote, isn't the application side waiting for the solvers to be ready slash mature to solve more than trivial formulas?
00:49:31.714 - 00:50:14.310, Speaker B: Well, the issue is, well, first of all, there's like a format problem. So QVF wasn't always considered as an existence of, of Satan. So it worked on CNF, but that's not good for various reasons. So it's just because kind of in QVF solutions and counterexamples are kind of should be in the same format, right. But if you move from a CNF to its negation, you get like, not a CNF anymore. And this is one, one way, one thing, I think, where the sort of the community made a mistake on kind of just adopting CNF. So now there are solvers and technologies which work also on CNF.
00:50:14.310 - 00:51:06.314, Speaker B: But my feeling is you would also need some sort of applications where you start off with a circuit or with the formula and then use the full power of QBF? That's my belief. And we haven't seen examples like that much. I have some old, like more than ten years old paper where, for instance, in the context of model checking, where we actually, in 1998, we first tried to use QVF solvers for model checking, then failed, and then did this BMC, right. And there we only have negative results. And so maybe we need, just need a different sort of application where it's obvious to use QVF and it's the right formalism. That's my. I think the solvers are already, but not like the problems don't fit the input format.
00:51:08.334 - 00:51:23.710, Speaker A: Interesting. Okay, cool. Neal asks in the chat window, what about non CDCL based solvers? And also, can you comment on non CNF representations such as PB or XOR. Pseudoboolean or XOR.
00:51:23.782 - 00:51:57.506, Speaker B: Right. So first one would be, yeah, we do of course, this look ahead thing. And yeah, of course, I've been playing with this for many times. And this look ahead currently is most important for this hard combinatorical problems. And there we will see more sort of mixed things then. But if Kuldeep actually meant a different sort of solving technology, like PB solving and ILP solving, for instance. Yes, I totally agree.
00:51:57.506 - 00:52:28.234, Speaker B: There should be sort of more interaction. And we saw this actually in this last half year, there is interaction between the groups and. Yeah, but maybe you see like with these PV solvers and also with the arithmetic solvers, we always struggle to get. Get it back into the. From the CNF into the arithmetic problem. And that might actually be sort of a similar problem as in QBF. Maybe this is not the right way to present the problems.
00:52:28.234 - 00:53:05.164, Speaker B: And of course we need proofs for this. This is something I didn't discuss much. This is a big thing here on my slide here. I completely forgot to mention this. Like the last five years, I would say before the competition last year, the biggest thing, I think, in SAPs practical sat that we have these proofs, like both for mathematical problems, but also sort of in practice. And that's a big thing. And now when we go to this arithmetic solvers or PD solvers, what do we do with the proofs? It's unclear and we heard some talks here, but like, I would say, we don't know exactly where we're going.
00:53:07.424 - 00:53:08.324, Speaker A: Okay.
00:53:09.024 - 00:53:10.856, Speaker D: So, Armin, can I ask you a question?
00:53:11.000 - 00:53:11.764, Speaker B: Yep.
00:53:12.864 - 00:53:59.144, Speaker D: So you mentioned that minisat played a very important role for SAT solver development. And now, I mean, looking at this slide, you have this plethora of different techniques, like face saving and switching phases and reduction techniques and LBD and whatnot. And it's a bit intimidating. So what, what do you think is a way forward? Are we waiting for like a. A new, simple, clean, unifying mini sat, like solver or should we, I don't know, embrace all these techniques and just put some machine learning on top to really learn how to harness it without really understanding what's going on? Or like, what would, if you were like, advice some young people going into the field, like, where do you think we should go?
00:53:59.924 - 00:54:23.588, Speaker B: I mean, there's this. There are easy things. So this. But, but you see, like this phase saving is also something which is easy. And I for sure tried this since also Niklas tried it, but it only started to work if you combine it with this rapid restart. It's completely strange in a certain sense, and.
00:54:23.756 - 00:54:24.060, Speaker C: Right.
00:54:24.092 - 00:55:05.336, Speaker B: So the argument is if you do these rapid restarts, then the face saving helps you to avoid, like, redundant work because you will go into exactly the same search corner again, and you want to have this fast restarts and the face saving to auto find short proofs for unsatisfiable form. And that's actually the interesting part. And I'm not sure whether they did this actively. I don't think so, actually. So they didn't really search for this. Like, how can I get short, unsafe proofs? But this is the effect. And yes, in the paper by Laurent and Gilles, the LBD paper, it's also very similar.
00:55:05.336 - 00:55:38.284, Speaker B: So they try to find a metric like what could allow you to predict whether the clauses are useful. And this is at the very heart of the CDCL. So this is not like some strange technique. No, this is very hard, as I try to explain, because if you do CDCL and you learn, you also have to forget these clauses. And the best way we have now to do that, which gives you kind of, as I said, like a reduction, a square root reduction in terms of memory. That's this LBD and something else, like, which I don't want to go into now, but this is.
00:55:39.064 - 00:56:01.704, Speaker D: Yes, yeah, I agree. But just to sort of play the devil's advocate, as it were, you know, if you look at. I understand that these chensei o settings are the best ones with these tiers and stuff, but it's not like, you know, a clean Minnesota like impression. I mean, it seems like it's very hacky and mysterious, and this is how you should do it, and it just works.
00:56:02.284 - 00:56:15.332, Speaker B: Oh, I wouldn't say mysterious. I wouldn't say it's mysterious. So this is like. This is exactly the part, what I wanted to say. So some of these came about and also this transigo thing by. By seeing some effect empirically. Right.
00:56:15.332 - 00:56:36.554, Speaker B: But now, if, for instance, you would go into my latest solver, it's precisely explained, and I claim it's also not sort of, like, convoluted or something. It's completely clear what you should do. But the way. How it sort of appeared, this technique is different. I completely agree. But this is probably sort of the sort of, like, you do empirical science to figure out, like, what could help. Right.
00:56:39.214 - 00:56:40.622, Speaker D: And what's the way forward?
00:56:40.798 - 00:57:43.264, Speaker B: Like some, you know, I mean, that's one issue. Of course. The issue is with all these practical words. Right? So people have this term like rocket science. Right? So. So you have all this technology which build on top of each other and of course like now finding something new but it's very difficult so I claim this target phases is something new right and also the way to put this local search in it but of course to get to this point you require maybe like 20 years like in my case or more clearly not just like the five years of PhD student tests to get to this point so unless you have of course somebody who guides you into this direction right yeah I mean there's also one, one related to point to this is of course the way how we actually review papers and in this empirical domain right so in order to get some of these sort of improvements in one has to be patient but one has to encourage people to go towards doing strange empirical measurements right.
00:57:46.944 - 00:57:50.784, Speaker A: Okay any other short question maybe.
00:57:50.824 - 00:57:51.404, Speaker B: Or.
00:57:55.224 - 00:58:04.944, Speaker A: I think we're probably done thank you again for the talk this was very interesting so thank you thank you so we're going to take a.
