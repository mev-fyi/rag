00:00:00.720 - 00:00:24.194, Speaker A: We will get started on our final session of speakers. We're very lucky to have speaking first, Tao Fan visiting us from Monash University in Melbourne. I won't do any more introduction, I'll just leave it to you to get started. And thank you very much.
00:00:24.654 - 00:00:50.034, Speaker B: Thank you. Hello. Thank you everyone. My name's Tao than. I'm a research fellow at Monash University in Melbourne, Australia. I'm part of the Australian Research Council's center of Excellence for automated decision making in society, and I'm also part of the emerging Technologies Research lab. The paper I'm presenting today is based on work I've published in collaboration with my colleague Scott Walk, who's at the University of Warwick.
00:00:50.034 - 00:02:04.144, Speaker B: And together we've been thinking through how race and racism manifests in algorithmic culture. Now, there's certainly no shortage of examples of things like racial vilification, racial discrimination, other racist actions that take place on platforms today. And most of this is done in very explicit ways, in ways that we are able to see through the use of, say, directed racial slurs, the reward and circulation of inflammatory racist content, or the routine delivery of demeaning results for racialized search terms like black girls or asian girls. These racist acts are rightly understood as the online continuation of a pre existing state of affairs, the symptom of an overall racist society. The problem of bias in algorithms is often characterized in this way as a problem of history repeating itself, the flaws of the past being encoded within infrastructure, which then traps the future inside of a recursive loop, in Wendy Chun's words, a model of prediction that closes the world it pretends to open. So, for example, a principle like garbage in, garbage out captures this really well. Any model that's been trained on a flawed or dirty data set will reproduce the problems inherent within that data set.
00:02:04.144 - 00:03:01.234, Speaker B: In this case, it's racism in, racism out. You put racist data into a model, and it'll reproduce and perpetuate that same racism. But the provocation I'd like to make today is that while platforms are undeniably reproducing racism, they're also doing something which I think is much more subtle and arguably much more radical. They're fundamentally transforming the category of race itself, and they're doing it in ways that has significant consequences for how we traditionally think, identify, and confront racism, racialization. So let me use an example to illustrate. Between 2016 and 2020, Facebook very controversially allowed advertisers to target users in the US using three broad categories of what they called ethnic affinity categories. These were african american, us, hispanic and asian american, and they were controversial for two main reasons.
00:03:01.234 - 00:04:15.834, Speaker B: The first was that advertisers could use these categories to not only target, but to exclude racial groups from receiving their advertising messages. So, for example, advertisers could purchase ads targeted at Facebook users who are house hunting, and then exclude anyone from an african american, us, hispanic or asian american affinity from receiving those ads. What, as many commentators pointed out, is effectively a violation of the US federal anti discrimination laws. The second has to do with the way people were being categorized, so Facebook does not explicitly ask users to identify themselves according to race. Instead, these categories are algorithmically determined using behavioral data and indirect proxy indicators like language use, interests and ip address. Now there's a lot of attention being a lot of attention analysis being placed on that first issue of the ways in which algorithmic techniques are being used to continue the work of racism, in this example, through racial segregation and systematic housing discrimination. It's an exemplary case study for what Safiya Noble calls technological redlining, the use of algorithmically driven software to reinforce oppressive social relationships.
00:04:15.834 - 00:05:37.794, Speaker B: But what receives less attention, though, is that second issue, which is about how platforms, and indeed algorithmic culture more broadly, is producing new forms of racialization, new ways, new modes of categorizing and recognizing people as raced, and as a result, transforming what it means to be raced today. So my talk is roughly divided into two parts. In part one, I'll be going into a bit more detail about what's new in this particular moment, focusing specifically on the post visual, or what Anna Munster and Adrienne McKenzie have called the invisual aspects of algorithmic culture. And then in part two, I'll be unpacking the specific challenge this presents for scholars who are invested in studying race and its effects in contemporary culture. So, over two decades ago, the critical race and cultural studies scholar Paul Gilroy made the contentious proposition that the time of race may be coming to a close. Developments in molecular biology and digital imaging techniques like mris and PET scans, had allowed the human body to be visualized at new, previously imperceptible scales. By rendering the body intelligible in new ways, Gilroy argued, these technoscientific processes challenge the modern representational economy critical to the reproduction of race.
00:05:37.794 - 00:07:04.444, Speaker B: If race operates, operates by differentiating and classifying bodies based on how they look, then changes in the apparatuses that mediate how bodies can be looked at ought to allow us to reimagine race, or indeed potentially do away with it altogether. And while there are a lot of shortcomings with this argument. What we find compelling is the insight it offers about the close link between racialization and mediation. For Gilroy, race has to be understood as an effect of the techniques we use to represent it. If racial difference is no longer perceptible at, say, the genetic or the molecular scale, should it matter as a mode of organizing bodies? In this way, Yillro's post race polemic is attached to a post visual discourse. Like many critical race scholars, Gilroy framed his argument in relation to a conceptualization of race that is predicated on an epistemology of visibility, and this particular way of approaching race has become extremely dominant within race and critical algorithm studies, or sometimes called race critical code studies. Scholars like Ruha Benjamin, Simone Brown, Timnek Jabu, join Buolamwini, for instance, have given us really vital and incisive critiques based on how new technologies perceive or don't perceive racialized traits like blackness.
00:07:04.444 - 00:08:34.854, Speaker B: For Benjamin and Brown in particular, whose work explicitly builds on the approach of scholars like Fanon, visualization is a central aspect for processes like digital epidermalization, that is, explicit acts of looking and interpolation. So how then do we account of the example I introduced earlier on Facebook of modes of racialization that explicitly cannot be seen either because its constitution occurs beyond human scrutiny at scales imperceptible to the human, or because it's deliberately obscured and opaque because they operate, for example, through, say, proprietary processes. Take another example. In the 2013 Snowden leaks, it was revealed that the US National Security Agency used a procedural algorithmic tool to determine whether an individual could be classified as foreign for the purposes of state surveillance. Under us law, us citizens are afforded privacy protections under their constitution's Fourth Amendment. To bypass this law and to justify their expansive collected or surveillance program, the US government created a method of determining citizenship by measuring foreignness as a likelihood percentage. Media scholar John Cheney Lippold puts it this way, if an individual's foreignness is found to be at or above 51% confidence, then that individual legally becomes a foreigner and thus loses the right to privacy.
00:08:34.854 - 00:09:54.444, Speaker B: The data points used to determine foreignness are explicitly non visual, yet operate in ways that reproduce and reconstruct ethnic, racial, and cultural ideas of national identity. It's this post visual logic, we argue, that disrupts how race is being conceptualized and acted upon today. The challenge that we have now, then, is to try and find a conceptual, language and methodological approach that can be used to think, identify and confront racialization in this post visual regime. So this is easier said than done precisely because the novel techniques of racialization associated with this regime are particularly good at obfuscating themselves and the way that they produce racialized difference. Typically, studies of media and digital culture have conceptualized race as a visual process and studied racialization using visual methods. Dominant discussions of race and algorithms have primarily focused on visual applications, such as facial recognition or the disproportionate impacts of systems on communities that are racialized visually. Again, the work of scholars like Simone Brown or Sophia Noble, who focused on the effects of of the technologization of anti blackness on black subjects and black communities.
00:09:54.444 - 00:10:58.624, Speaker B: In much of this work, the effect of algorithmic systems on race has been primarily read through the principle of garbage in, garbage out. That is, scholars tend to identify racist outputs produced by algorithmic systems as products of racist inputs. These inputs are typically understood in a few key ways, in the form of data that is biased because it inadequately represents racialized communities or because it reflects historical injustice to prejudice. And what we want to stress is that the concepts of race entered into such systems are not necessarily the same ones that come out. The model of garbage in garbage out misses what happens in the middle when data is subject to algorithmic techniques of sorting, classifying, and categorizing. What the existing discussion of race and digital technology has overlooked, we think, is how these algorithmic techniques transform the inputs they work on. A category like ethnic affinities is generated using inductive and correlative machine learning processes.
00:10:58.624 - 00:12:10.654, Speaker B: There is not necessarily a direct causal relation between the data that the system has on you and the inferences it makes about you. These inferences might emerge from correlations between behavioral data points held on millions of users or through proxies that act as stand ins, etcetera. Several removes for explicitly racial categories. Moreover, these categories are striking because they reorder the relationship between racialized categorization and visibility. As Louise Amore notes, critical accounts of the rise of algorithms have placed great emphasis on the power of algorithms to visualize, to reprogram vision, or indeed, to see, which is not otherwise available to human regimes of visuality. Yet algorithms cause great anxiety because they operate on a plane in excess of human visibility and at scales that are inscrutable to the human. What we're referring to here as a post visual regime now encapsulates large scale data collection and processing, which occurs at scales and through processes that are not only not visible to us beyond our perception, but which also produce effects in registers that are also not at all seeable.
00:12:10.654 - 00:13:36.056, Speaker B: Adam Munster and Adrian McKenzie refer to these forms of platform seeing as in visual perception, arguing that despite visual techniques and practices continuing to proliferate from data visualization through to lidar technologies for capturing non optical images, the visual itself as a paradigm for how to see and observe, is being evacuated, and that space is now occupied by a different kind of perception for them. This new kind of perception is tied to ensembles of software, hardware, and platforms that understand images in non representational terms. Similar we're arguing that large scale data processing generates novel racial formations, recapitulating them as what we call data formations. Here, racialized bodies are figured not as single, coherent subjects but as shifting clusters of data, or, as Lee put it on Wednesday, as fragmentary, correlational, probabilistic. These data formations conceive us in new ways, in ways that we ourselves are unable to see. New media scholar Bernard Reiner has recently described these kinds of computational systems as engines of order, that is, systems that use classificatory techniques to make order out of a surface of data. But engines of order are also engines of difference.
00:13:36.056 - 00:14:47.014, Speaker B: In his words, they not only imply forms of standardization, homogenization, and commensuration, but also produce their own vectors of differentiation. The differences they propagate take real people as their objects, but do so in ways that are extremely difficult to reverse engineer and consequently, to resist. Because these techniques are inductive, the categories they produce depend on the data they're trained on. In the absence of a category explicitly labeled race, these technologies produce racializations indirectly inferring them from correlations between other categories or through the use of proxies. With enough data, the invisible correlates of race, like postcode, language group degrees of contact with institutions like the criminal justice system, render race actionable once again. And these techniques change race's epistemological basis, establishing a novel, non visual ground for a novel racializing logic. We want to identify a break, a disruption instituted by technology that employs these inductive logics.
00:14:47.014 - 00:15:45.034, Speaker B: Some forms of data processing are racist because they incorporate data that bears the trace of a racist society. Garbage in, garbage out. But large scale, inductive data processing also institutes another, more pernicious kind of racism. This racism emerges in and through correlative models and is endogenous to those models behind the ideological smokescreen that such technologies are post racial. The power to classify, to differentiate, and, crucially, to determine context produces a wholly novel and new capacity to racialize. The infrastructure of racialization instituted by such technologies might ultimately reproduce existing forms of racialized inequality, but their capacity to produce race by other means turns racial categories into something else entirely emergent epiphenomena of large scale automated data processing. In short, racial formations as data formations.
00:15:45.034 - 00:16:57.044, Speaker B: I'm just concluding now, so I'm just going to conclude with a few speculative reflections on the methodological problem posed by algorithmic racialization. This post visual regime I've begun to sketch raises methodological and political challenges for scholars of race and technology. While it's important to note that algorithmic systems can recapitulate or perpetuate already existing regimes of inequality like racism, our argument is that they carry out a more fundamental operation that they've begun to transform the category of race itself, correlated, inferred, and acted on via proxies. Racial formations as data formations not only transforms the category of race, they also locate the sites of racialization elsewhere, behind places that can they can typically be identified and contested out of sight and out of easy reach. To study the novel racial formations that are produced by such systems, we need to go beyond races encoding into data. We also need to go beyond how such modes of racialization are felt. Most critically, we also need to go beyond critical accounts of mediated racialization that operate in visual registers.
00:16:57.044 - 00:17:54.544, Speaker B: We've started to do this work in analyzing examples like Facebook's ethnic affinities, and all this is these are particularly significant points for scholars invested in questions of race and culture, because it confronts us with some important questions. How can we advocate against a process that operates beyond our perception? How can we keep up with the dynamic pace of classification classifications that are being assessed and reassessed with every new piece of behavioral data? And how do these issues make redundant the traditional tools of resistance? Things like advocating for inclusion or representation, make no sense within a post visual regime. How can we resist a category we don't even know we've been placed within? And how do we form communities of solidarity under those conditions? Thank you very much.
00:18:00.764 - 00:18:04.844, Speaker A: Thanks, Tao. And now we have Fabian offutt.
