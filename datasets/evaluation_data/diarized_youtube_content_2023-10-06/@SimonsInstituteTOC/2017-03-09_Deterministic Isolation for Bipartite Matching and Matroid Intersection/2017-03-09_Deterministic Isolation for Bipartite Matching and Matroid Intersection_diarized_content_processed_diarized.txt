00:00:00.200 - 00:00:03.754, Speaker A: The next talk is on de randomizing the isolation lemma by Rohit Guttra.
00:00:05.134 - 00:00:37.290, Speaker B: I would like to thank the organizers for inviting me for giving this talk. So this talk is on de randomizing the famous isolation lemma of Mulmalia Vazirani and Vazirani. And we will give a geometric approach towards de randomization of isolation lemma. And we will show for two settings. This approach works. So this is based on two joints work with Stephen Fenner and Thomas Tiro. So, let us start with the isolation lemma.
00:00:37.290 - 00:01:28.936, Speaker B: So, for any weight function defined on a set, let us say e, e is a set, and we have an integer weight function. So, weight of any subset of e is just taken to be the sum of the elements in that set, some of the weights of the element in that set. And now what the isolation lemma says is that suppose you are given any family of subsets from this set e. And now you assign weights randomly. So each element is given a weight randomly from, say this range from one to twice the size of the set. Then with a probability at least half, there will be a unique minimum weight set in your family b. And I mean, this is a bit surprising when you see it for the first time.
00:01:28.936 - 00:02:06.344, Speaker B: I mean, you might expect that the different, the number of sets in all weight classes will be uniformly distributed. But this says that when you look at the minimum weight or the maximum weight with high probability, there will be only a unique set. And another important thing is that it works for any family. So here there is no assumption on this family. So it's an arbitrary family of subset. And so this lemma was given in the context of perfect matching. So they, using this lemma, they gave a randomized parallel algorithm for the perfect matching problem.
00:02:06.344 - 00:03:02.774, Speaker B: And later it was also applied to give a randomized parallel algorithm for the linear metroid intersection problem. And then, since there are many other applications in complexity theory, so you can solve polynomial, I mean, you can give a randomized, randomized polynomial time test for the polynomial identity testing problem, which Michael introduced. And so, this reduction from so valiant Vazirani gave a reduction from sat to unambiguous satisfying. It is a randomized reduction. And so, mulmule gave an alternate proof using the same isolation lemma. So they reduce from clique to unique clique. So I mean, this reduction shows that even if you assume that your np hard problem has at most one satisfying assignment, even then it's hard to solve.
00:03:02.774 - 00:03:58.300, Speaker B: And Reinart and Allender showed that this class nl non deterministic lock space and class ul unambiguous non deterministic lock space. They are sort of equal with polynomial advice. And there is another problem called disjoint paths. So given a graph, you are given two sources and two sinks, s one and s two and t one, t two, and you want to find out whether there is a path from s one to t one and s two to t two, which are disjoint. So this problem has a randomized polynomial time algorithm. Again, it uses the isolation lemma. So in all these settings, the only randomized thing used is coming up with such a weight assignment, which implies a unique minimum weight set in appropriate families.
00:03:58.300 - 00:04:33.154, Speaker B: So the family of sets here are all different corresponding to the setting. So if you can de randomize this lemma, meaning if you can construct such a weight assignment deterministically, then here you will get deterministic results. So for example, you will get perfect matching and linear metroid intersection in NC, and you will solve polynomial nd testing if you de randomize it for appropriate family of sets. And you will get a deterministic reduction here and you will show NL equal to ul, and you will solve this disjoint paths problem in deterministic polynomial time.
00:04:33.494 - 00:04:36.406, Speaker C: What do you precisely mean by de randomizing this dilemma?
00:04:36.550 - 00:05:31.804, Speaker B: Yeah, so, I mean, I will go to it in the next slide. Yeah, so roughly, it means you want to construct such a weight assignment deterministically. But yeah, there are many conditions on it. So first of all, I mean, so here we see the weights were polynomially bounded in the random weight assignment. So we want that property that when we construct this weight assignment deterministically, the weight should be polynomially bounded. And I mean, now first thing you can see is you can't do it for all families of subsets, because if you give me any weight assignment with polynomially bounded weights, there will be two sets which have equal weights because there are exponential many sets. So you can give this as your family, and then the weight function will not isolate in this family.
00:05:31.804 - 00:06:19.826, Speaker B: So what we want is, I mean, you can relax this, you can say that you are allowed to give a polynomial large list of weight assignments, not just one weight assignment. And now you want the guarantee that for any family at least one of the weight assignments should be isolating. But even this is impossible. You can't do it for all families because, I mean, you can show using the arguments involving polynomial identity testing that this is impossible. I don't know of any direct counting argument. So what you can hope to do is you can hope to do it for families which have succinct representation. So what is succinct representation? I mean, it can be, for example, it can be set of perfect matchings of a given graph.
00:06:19.826 - 00:07:06.300, Speaker B: So here the graph is the succinct representation and perfect matchings are not given explicitly, but you are given the graph. And here you want to design weight assignment deterministically, a weight assignment which will imply unique minimum weight perfect matching in the graph. A more general setting you can consider is a set of the set of strings accepted by a circuit. So here, every string, you can view it as a set. So this, the strings accepted by circuit, you can see it as a family offsets. And you can talk about isolating element in this family. So I mean, again, using the isolation lemma, you know that a random weight assignment works.
00:07:06.300 - 00:07:39.894, Speaker B: So you can use it to show that there exists a list of weight assignments such that for any succinct, any family which succint representation, one of the weight assignment will work. I mean, it's the standard argument. And the goal here is to construct such, such weight assignments explicitly. So this just shows existence. Yeah. So let's look at some examples where this deterministic isolation is known. So first example is sparse families.
00:07:39.894 - 00:08:06.718, Speaker B: If your family has only polynomial many sets, it's very easy to isolate. And yeah, there is. If this sparse family is given to you explicitly, then it's really trivial. Like you can just take one set, you can assign zero weight to all its elements. You can ensure all other sets get non zero weight. So that's really simple. But you can do it even if you are, if this sparse family is not given to you.
00:08:06.718 - 00:08:50.393, Speaker B: So even in a black box sense, you can come up with a list of weight assignments such that for any sparse family, one of these weight assignments will work. So without, this can be done without knowing the given family. Now another example is spanning trees in a graph. So you just, for spanning trees, you just give different weights to all the edges. It is easy to show that with such a weight assignment, the minimum weight spanning tree is unique in the graph. In general, it is true for any metroid maximum independent sets. So if you give different weights, then there is a unique minimum weight maximum independent set.
00:08:50.393 - 00:09:33.952, Speaker B: Then there was a lot of work on perfect matchings for special graphs because the effort was to de randomize it for the perfect matching family for general graphs. And I mean, there are many graphs classes like bipartite planar graphs and strongly chordal graphs. So on. Another example is this, st paths in a graph, and here we do not know a polynomial polynomially bounded weights. We know quasi polynomial bounded weights. So we know an isolating weight assignment with quasi polynomially bounded weights. So I mean, it is easy to see actually.
00:09:33.952 - 00:10:19.366, Speaker B: So for, just for simplicity, I will consider layered graph. So let us say we have some layered graph from s to t. The edges are going from s to t. What you can do is you split the graph. I mean, consider the middle layer and suppose recursively assume that all paths from s to this node there is a unique minimum weight path from here to here, same from here to here, from here to here, same from here to here, here to t and so on. So what you can see now that from s to t there are exactly n at max n minus. So you can give a new weight assignment which distinguishes among these n minus.
00:10:19.366 - 00:11:10.782, Speaker B: And you need to do this in log and rounds because this is a divide and conquer approach. So that, that will give you quasi polynomial bounded. Another point I want to repeat is, again, I mean, if you could just find an St path, you could assign zero weights here and weight one to all other edges. So that is a trivial way to isolate. But we are sort of assuming that we, I mean, we don't have such, so much power. So in fact, in the context of St paths, we want to construct an isolating weight assignment in log space because that is, this question is related to the NL equal to nl versus ul question. So in general you can think of, I mean, I would put the question in this way that in some sense you have to design your isolating weight assignment in a black box way.
00:11:10.782 - 00:11:14.874, Speaker B: You can't just go and find out a set in your family.
00:11:18.014 - 00:11:18.366, Speaker A: Yeah.
00:11:18.390 - 00:11:53.108, Speaker B: So I mean this strings accepted by read one's formula or read one's branching programs. This follows from this st paths. And then recently we have this two results on perfect matchings in bipartite graphs and common independent sets of two metroids. And here we can give isolating weight assignments with quasi polynomial bonded weights. And this is the approach I am going to present. So the approach also works for this minimum vertex. Minimum vertex covers in a bipartite graph.
00:11:53.108 - 00:11:57.944, Speaker B: I mean, this problem is related to perfect matchings in a bipartite. So this is not very important.
00:11:58.844 - 00:12:00.824, Speaker C: How about things like DNF's?
00:12:03.884 - 00:12:12.184, Speaker B: DNF's, I don't think anything is known because it's, yeah, I'll come to, in the end, I'll come to this.
00:12:15.294 - 00:12:15.606, Speaker A: Yeah.
00:12:15.630 - 00:13:01.910, Speaker B: So now I will start with the approach. What is the approach? So the approach, you will consider this polytope for the given family. So for any given set, for any given subset of this set e, you consider its characteristic zero one vector. So one if the element is in the set zero otherwise. And now for your family family b, you consider this polytope pb, which is nothing but the convex hull of all these characteristic vectors of your sets in your family. So this is a zero one polytope sitting in the r to the e, and its corners are exactly coming from your sets in the family, because these are, it's a zero one polytope. All these sets will correspond to corners.
00:13:01.910 - 00:13:33.274, Speaker B: They are not interior points. So these corners are exactly coming from your sets in your family. And now what we do is we the weight assignment on the elements of your set e. We view it as function over this polytope. So what I mean is you, for any point in r to the e, you consider this function w dot x. So view w as a vector vector in r to the e. And consider this dot product w dot x, and define this as the weight of this point x.
00:13:33.274 - 00:14:27.574, Speaker B: So what is the point here is. So this is easy to observe that for any set s, this dot product of w with x s is nothing but the weight of the set. Because x s is a zero one vector. When you take the dot product, it is just sum of the sum of the weights of the element in the set. So easy observation is that this w weight assignment is isolating for your family b if and only if w dot x has a unique minima over the polytope and w dot x is this linear function, which will be important. So now our goal is we look at this polytope pb, which sits in r to the e, and we want to design a w such that w dot x has a unique minima over the polytope. Again, we want small weights, polynomially bounded weights.
00:14:27.574 - 00:15:14.954, Speaker B: So the strategy is that instead of doing it in one go, we will do it this in many rounds. So first observe that because w dot x is a linear function for any such w, the points minimizing this w dot x in the polytope will form a face of the polytope. So for example here it might be this phase or just an edge or a vertex. So our vertex is also a face. A vertex is a zero dimensional phase. So what we will do is we will start with some weight assignment, and that will give us some phase. Let's say this is the minimizing phase.
00:15:14.954 - 00:16:27.410, Speaker B: Then the strategies that modify your weight assignment slightly, if you modify it too much, then maybe you will get a completely different minimizing phase. So what we do is we slightly modify it such that now the new minimizing phase is a surface of your, so maybe this edge, it's a surface of your current phase and so on. So in each round you try to decrease the dimension of your minimizing phase and keep decreasing the, yeah, so you stop when you reach the zero dimensional phase, which is a vertex. So let me formalize this when I say I will slightly modify this weight function. What I mean is, we can formalize this. So suppose you have w is a weight assignment and w prime is another weight assignment, such that w prime is bounded by this capital number n. So basically what we do is this, w is our current weight assignment and w one is our new weight assignment, which is obtained by adding w prime with w.
00:16:27.410 - 00:17:03.416, Speaker B: But w is put on a higher scale. So w gets a, the old weight function gets a higher preference. So the point is that, I mean, you can also view it like this, that your weight function is basically w plus w prime. So this thing gets a low priority. So basically your new minimizing phase will be a subset of your old minimizing phase, because this gives some minimizing points and this further distinguishes among them. That's the only thing it does. Yeah.
00:17:03.416 - 00:17:45.634, Speaker B: So note that this weights will grow as exponentially in number of rounds because of this multiplication by this scaling. So what we want ideally is constantly many rounds. But what we can achieve is this approach will give a log n rounds. So that will make it quasi polynomially bounded bits. So, yeah, so let me, the important thing is how to, in each step, how to reduce the phase dimension significantly. So we would like something like constant fraction reduction in the dimension of the phase. So let's say, so we have some weight function w zero, and let's say f zero is the minimizing face.
00:17:45.634 - 00:18:24.494, Speaker B: Consider any vector parallel to the face. So by vector parallel to the face, I mean, formally it means that v is sum alpha times v one minus v two, where v one and v two both are points in the face. So something like this. So if this is the face, you take two points, you consider this vector. So I call this vector parallel to the face. Now, an easy observation is yes. For example, if you take two vertices of the face, this vector is parallel to the face.
00:18:24.494 - 00:19:12.984, Speaker B: So clearly this w zero dot v is zero, because what we know is w zero dot v one and w zero dot v two, because w zero is the f zero is the minimizing phase for w zero. These two things are equal and w zero dot v one minus v two is zero. So that is written there, w zero dot v zero. So now what we are going to do is we will change our weight function such that the new weight function w one will ensure that its dot product with v is non zero. So that is how we will change our weight function. So this will imply that now this chosen vector v is not parallel to your new minimizing phase. So say new minimizing phase is f one.
00:19:12.984 - 00:20:15.006, Speaker B: So this also tells you that f one is a strict subset of f zero, because there is a vector which is not parallel to f one, but it is, it was parallel to f zero. Now again, the point is how to guarantee a significant decrease in the dimension. So what we are going to do, instead of just choosing one vector, we will choose many such vectors parallel to the face and will ensure that for all such vectors, this dot product is non zero. So there are standard tricks to do this for polynomial many vectors. If I give you polynomially many vectors with which are polynomially bounded coefficients, then you can design a weight assignment such that for all vi's w dot vi is non zero. So there is a standard trick for this. You take exponentially high weights which will clearly, which can clearly achieve non zero dot product, and then you, but we want small small weights.
00:20:15.006 - 00:20:48.294, Speaker B: So what we do is we go modulo small numbers. So if you try many numbers, what you can show is for one of the j's w mod j will have this property that w dot v I is non zero for each I. And the important thing is here is the numbers. The j you need to try is bounded by mk log t. So basically, if you have polynomially many vectors, you will get polynomially bounded weights. So we can handle polynomial many vectors. That's the summary.
00:20:48.294 - 00:21:13.634, Speaker B: Also important point is that this construction is black box. So we do not need to know these vectors, we just need to know the bound on the coefficients, the bound on their number, and this scheme will work. So this will also be important that this such weight assignment can be constructed in black box. Now let us say we have some phase, let's say the set of integral vectors parallel to this phase.
00:21:17.694 - 00:21:30.810, Speaker D: If we instead wanted to look at arbitrary families instead of represented families, the reason this wouldn't work is because there could be too many facets and we'd have to, this method would blow up the weight by having to account for all of it.
00:21:30.922 - 00:22:06.334, Speaker B: Yeah. So in the end I will come to a sufficient condition and that, I mean that will clearly will not be true for all polynomials. Yeah. So let's say lf is the set of integral vectors parallel to the phase. Now what we do is in the first step, design a weight function w zero, which gives nonzero dot product to all, all vectors which are bounded lengths, let us say bounded by two all small vectors. Now once you do this, you can do this because there are only polynomial vectors. So you can do this.
00:22:06.334 - 00:22:38.150, Speaker B: Now let's say the f one is the face, minimizing this function w zero. So clearly f one. Now this, in this set l f one, there are no length two vectors because we have ensured non zero dot product with length or for all length two vectors. These length two vectors cannot be parallel to l f one. So I mean integral vectors, integral length two vectors cannot be parallel to the face f one. So in some sense, we have removed length two vectors from our face. So we keep doing this.
00:22:38.150 - 00:23:17.986, Speaker B: In the next round, we handle length four vectors, so we again, they are polynomially bounded, so we can ensure non zero dot product. So in the new phase f two, now there are no length four vectors. So we keep doing this. Each round double the length of the vectors you handle. And in ith round you, there are, you can assume there are no lengths two to the I vectors, and I want non zero dot product for length up two, two to the I plus one. Yeah. Okay, so what is the, I mean, this, this number can blow up how many vectors of length two to the I one.
00:23:17.986 - 00:23:42.540, Speaker B: There is no guarantee. What we want is it should be polynomially bounded. Then this approach will work. So, and we hope that this will be polymerized bounded for some nice polytopes, assuming that the minimum length vectors are two to the I. Yeah. So I will come to precise condition what we need. So if this, suppose this thing remains polynomially bounded the number of such vectors, then there is no problem.
00:23:42.540 - 00:24:23.708, Speaker B: And in log n rounds we will reach to a phase where there are no length m vectors, and then the face should be a corner, because we are dealing with zero one polyto. If there are two corners in your current phase, then you can see this vector has length bounded by m, where m is the dimension and it is parallel to the face. So when we say there are no length m vectors, it means your face must be a corner. So that gives you a unique minimum. That gives you a unique minimum. So now I just come to this sufficient condition, which is discounting this vectors. So basically you take face f and you consider these vectors parallel to your face.
00:24:23.708 - 00:24:56.300, Speaker B: So formally you can say that if your face ax equal to b are the all the equalities satisfied by your face. Then you consider all these integral vectors which satisfy a x zero. And what we want in this set lf, is this, that suppose lambda one l f is the length of the shortest vector. In this set, what we want is the number of vectors of length up to two. Lambda one f is polynomially bounded, so lambda one f is the shortest. So we count from shortest to twice the shortest. We want this to be polynomially bounded.
00:24:56.300 - 00:25:41.668, Speaker B: And if the, if we want this for all phases, so this, if this holds for all phases of your polytope, then this achieves isolation in this poly two. Now I want to show that this condition holds for the perfect matching polytope. So for bipartite graphs, not for general graphs, it only works for bipartite graph. So for bipartite graphs, the perfect matching polytope is given by these two set of constraints. The first is just non negativity, x is greater than zero. Second constraint is saying that for all edges incident on a vertex, the sum should be one. So I mean, you can see that for every perfect matching you take exactly one edge incident on a vertex.
00:25:41.668 - 00:26:12.984, Speaker B: So this is just saying that that around every vertex the sum should be one, exactly one. So it turns out that these two constraints define the polytope of perfect matchings. Now what is the phase? So the only inequalities are this non negativity constraints. So when you look at a face, it's just for you choose some edges, some subset of edges. Put x equal to zero. That's how a face can look like. And what is this? Lf the integral vectors parallel to your face, they are nothing.
00:26:12.984 - 00:26:53.034, Speaker B: But for this set s, they will satisfy x equal to zero. And for each vertex you need this sum around the vertex should be zero, because we shift the face to origin and then look at the integral vector. Now what are these vectors? So what are the vectors which satisfy these two constraints? So you can forget about this x equal to zero. This means just you delete these edges. Now look at the rest of the edges. The rest of the edges, you need this condition that on each vertex the sum should be zero. So consider any cycle in your graph.
00:26:53.034 - 00:27:41.358, Speaker B: Now consider this vector plus one minus one plus one minus one. So such a vector has the property that sum around every vertex is zero. The sum around this vertex is zero, sum around this vertex is zero. And so this kind of vector will satisfy these two constraints. In fact, you can show that all integral vectors satisfying these two constraints are actually integral combination of such cycles. So basically, the question when we are in a phase, basically we are in a subgraph, and when we say our shortest vector is lambda one, length, shortest vector length is lambda one, it means the shortest cycle in your graph has lengths some bound, let's say lambda one. And now you want to bound the number of cycles up to length twice that length.
00:27:41.358 - 00:28:20.324, Speaker B: So the, this lemma shows exactly that, that for any graph, if you assume there are no cycles of length up to r, then the number of cycles up to length two r is polynomially bounded. And that's the only thing we needed in the, because the cycles are exactly the integral vectors parallel to the phase. Yeah. So this again has a simple proof, and then I'll be done. So how do you show this bound on cycles? So you assume there are no cycles of length r. Consider any cycle of length two r. So let us say this is a cycle of length two r, divide into four equal parts.
00:28:20.324 - 00:28:46.924, Speaker B: And let us say these nodes are u one u two u three u four. Now with this cycle c, we associate this tuple, u one u two u three u four. Now, the claim is if you take another cycle c prime, this tuple will be different. Suppose this is true. This claim is true. Now, the number of tuples can be at most n to the four. So that will give a bound on the number of cycles.
00:28:46.924 - 00:29:16.398, Speaker B: So why there should be a different tuple. So assume there is, you get same tuple for both the cycles. So the cycle c primes maybe passes like this and goes like this and this and so on. Now, because this was divided into equal parts, the all parts were less than r by two. All parts were less than r by two reals. But now here you see, there are two parts, both of length less than r by two. This will give you a cycle of length r, which is a contradiction because we assume there are no cycles of length r.
00:29:16.398 - 00:30:02.442, Speaker B: So this gives a bound on the number of cycles up to length two r, assuming there are no cycles of length. So this proves the, this sufficient condition that if lambda one be the length of the shortest vector, then we want to bond number of lengths, vectors, length to lambda one. Yeah. So another example where this works is this metroid intersection problem. So now I don't have time too much to go into the details, but what it turns out is its polytope is well known by, it is given by edmunds. You look at the polytopes, you analyze how its face can look like it. Turns out its face will have exactly these sort of constraints, which comes from the bipartite graph.
00:30:02.442 - 00:30:40.686, Speaker B: So actually you will get a bipartite graph and the integral solutions parallel to the face will exactly come from cycles of some bipartite graph. You can build a bipartite graph and the cycles of that graph will give you the vectors. And again, we have already shown the bound on number of short cycles. So that solves this linear metroid intersection problem. Yeah. Yeah. So the question open question is for what other polytopes we can show this kind of bound assume, I mean, the number of short vectors, parallel short integral vectors.
00:30:40.686 - 00:31:29.004, Speaker B: Can we show it's polynomially bounded? Yeah. So one open, one question is matching in general graphs for which we don't know. Yeah, and other question was this in the beginning there was this reduction from sad to unambiguous sad. So there we need to de randomize where we need to isolate among the satisfying assignments of a given formula. So this approach cannot work in such a setting because we do not know a polytope for the satisfying assignments of a formula because, and there is no hope to know that unless NP equal to co NP, you cannot have a nice description of this polytope. So this approach certainly will not work in that setting. That is, it.
00:31:34.784 - 00:31:37.404, Speaker A: Lets take a quick question as the next speaker sets up.
00:31:38.344 - 00:31:48.004, Speaker C: Is there an example of a family known where approximate counting counting is easy, but deterministic isolation is known to be hard?
00:31:48.304 - 00:31:51.204, Speaker B: Known to be hard, I mean, not lower bound.
00:31:52.344 - 00:31:59.248, Speaker C: Plausibly, is the following statement plausible that whenever you have approximate counting, you have good isolation?
00:31:59.416 - 00:32:13.276, Speaker B: So there is perfect matching in planar graphs. So there you can count exactly perfect matchings in planar graphs. But isolation is not known till now. But I mean, I'm not saying it can't be known, I was just checking.
00:32:13.300 - 00:32:17.104, Speaker C: If my statement was obviously stupid or something immediately wrong with it.
00:32:17.524 - 00:32:31.684, Speaker A: Let's take other questions offline. The next talk is on prgs for small space using free analysis by Thomas Stinky.
