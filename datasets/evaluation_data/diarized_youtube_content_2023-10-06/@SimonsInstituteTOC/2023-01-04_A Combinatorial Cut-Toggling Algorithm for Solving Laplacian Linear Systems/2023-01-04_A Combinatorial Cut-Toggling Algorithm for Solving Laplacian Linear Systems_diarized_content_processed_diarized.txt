00:00:00.800 - 00:01:06.434, Speaker A: Hello, my name is Billy Jin. I'm a PhD student at Cornell University, and I'm happy to present my joint work with Monica Hensinger, Richard Pang and my advisor David Williamson. And it's about a combinatorial cut toggling algorithm for solving the plot linear systems. The problem we're interested in solving is the given a weighted graph g and a vector of real numbers on the vertices of the graph, goal is to solve the equation lx equals b, where l is a laplacian matrix of the gene, and it's defined as follows. So, the Laplacian matrix is the sum of these rank one matrices, where there's a rank one matrix for every edge. Each matrix is weighted by the width of the edge, and the rank one matrix itself is of this form, where it has a one in the II entry, a one in the JJ entry, and minus ones in the ij and j ith entries. So, for example, for this triangle graph here, where the blue numbers indicates the weight on the edges, here is the corresponding Laplacian matrix.
00:01:06.434 - 00:02:18.742, Speaker A: Why do we care about solving laplacian linear systems? Well, it turns out that it's used as an algorithmic primitive in many applications, from max flow algorithms to analysis of social networks and beyond. Many people have studied solving lx equals b in the past. For example, Spielman and Tang in 2004 show how to solve these systems in o tilde m time, which is much faster than solving a typical linear system. And the asymptotically fastest algorithm runs in o of n times square root of log n time up to poly log factors. And the paper that's most related to our work is one by Kellner, Orchia, Sidfer and drew from 2013, who give a simple combinatorial algorithm for solving laplacian linear systems. And our paper is motivated by their algorithm. So we'll begin by describing their algorithm, and then we'll describe our algorithm before we move on, just a small note, so that all of these algorithms, and also our algorithm, are not going to solve these systems exactly.
00:02:18.742 - 00:03:00.156, Speaker A: Rather, given a value of epsilon, the algorithms will find an approximate solution x that satisfy this inequality, where x star is the optimal solution to the linear system. But for clarity, we'll omit the running time dependence on epsilon. In this talk. It'll turn out that the dependence will be a multiplicative factor o of log one over epsilon. All right, so, um, let's look at the algorithm of Kellner et al, which we'll call the coase algorithm from now on. So, consider this primal dual pair of convex optimization problems. Let's look at the dual first.
00:03:00.156 - 00:03:40.894, Speaker A: The dual is an unconstrained optimization problem. Um, and it turns out that the optimal solution to the dual is exactly the same as solving lx equals b. This can be seen by taking the gradient of the objective and setting it equal to zero. Now, the dual of this problem is this primal problem here, where the variables are f. The constraints of this problem enforce that f is a valid b flow of the graph. So what does that mean? Well, a is the adjacency matrix of the graph g, and so af b means that the flow flowing out of every vertex of the graph is equal to the value of b at that vertex. That's what a b flow is called.
00:03:40.894 - 00:04:42.774, Speaker A: And the objective of the primal is to minimize the l two, normally ff weighted by these resistances, r I j, where rij is simply the reciprocal of the weight on edge ij. And from now on, we'll be working with the resistance of the graph instead of the weights, just to make the notation more simple. And what the coase algorithm does is it finds an electrical flow f. And it turns out that once you have an electrical flow f, you can easily convert it to a solution of lx equals b via convex programming duality. So in particular, here's how the duality works. So if you write out the kkt optimality conditions for this primal dual pair of optimization problems, they're as follows. So f is going to be optimal for the primal and x optimal for the dual if and only if the following two conditions hold.
00:04:42.774 - 00:05:39.284, Speaker A: First, we need that f is a feasible b flow, and second, to the complementary slackness condition, we need that for every edge ij, the value of f on the edge is equal to xi minus xj divided by the resistance on edge ij for every edge in the graph. That's also known as Ohm's law. The cos algorithm is going to use another law, which is called Kirchhoff's potential law, which is equivalent to Ohm's law. So KPL is going to say that for all directed cycles, c, if you sum over all the edges in the cycle of the resistance, times the flow that should be equal to zero. So here's how the Cos algorithm works. It's going to begin by computing a low stretch spanning tree t. The definition of a low stretch spanning tree is not going to be important for really understanding the algorithm.
00:05:39.284 - 00:06:44.204, Speaker A: So we're going to admit it in this talk, but it's some kind of special spanning tree in the graph. And it's going to initialize a flow to be the unique b flow supported on t. Since t is expanding tree, there's going to be unique b flow that's supported on it, and it's going to iteratively update the flow iteration by iteration, and it's going to show that this is going to converge to the optimal electrical flow. In particular, here's how each iteration works. You're going to sample an off tree edge of the graph according to some probability distribution, and it's going to check if KPL is satisfied on the fundamental cycle closed by this off tree edge. And if it's not satisfied, the algorithm is going to push a constant amount of flow across the cycle, where that constant is exactly what's needed to make KPL be satisfied on that cycle. The overall running time of this algorithm is as follows.
00:06:44.204 - 00:07:36.614, Speaker A: Um, computing t is going to take otot m time, and the number of iterations of this algorithm is going to be also o tilde n. This turns out to be equal to the stretch of this low stretch mining tree. And furthermore, each iteration which involves pushing flow along a fundamental cycle, can be implemented in o of log n time per iteration using a tree separator data structure or link cut trees. And so the overall running time of the coase algorithm is nearly linear in the number of edges in the graph. So here's a more detailed example of how the algorithm works. Suppose we're working with this graph here, where the blue edges are the spanning tree and the black numbers indicate the values of balance. For simplicity, let's assume that the resistances are one.
00:07:36.614 - 00:08:26.384, Speaker A: At the beginning, we initialize f to be the b flow supported on the blue tree edges. At iteration, we sample a fundamental cycle in the tree. So for example, let's say we sample this cycle here. Note that KPL is not satisfied in this cycle because the sum of the resistances times the flow is equal to two and not zero. And so the algorithm is going to subtract two thirds from each edge on this cycle because that's the number required, so that the resulting flow satisfies kPl on this cycle. In the next iteration, the algorithm samples another fundamental cycle. KPL is not satisfied because the sum is equal to minus five.
00:08:26.384 - 00:09:25.786, Speaker A: The algorithm pushes the constant amount of flow along the cycle to satisfy KPl. Note that the flow values are pushed are plus five thirds, plus five thirds and minus five thirds here, because this edge here is directed in the opposite direction compared to the other two edges. Pushing flow along this cycle means that this edge will have an opposite sign from the other two, and the resulting flow will satisfy KPL. And the coase algorithm just keeps doing this. And you can show that this will make the resulting flow converge to the electrical flow in a nearly linear number of iterations. Now, co sets us what we call a cycle toggling algorithm because iteratively updates along cycles. Now in network flow theory, another typical algorithm approach is called cut toggling.
00:09:25.786 - 00:10:45.268, Speaker A: And what this involves is that you're going to maintain potentials x on the vertices of the graph, and in each iteration the algorithms will sample some cut s and update the potential on vertex on all the vertices on one side of the cut by adding a constant to them. And often s is taken to be a fundamental cut of some spanning tree. So traditionally, for example, for min cost flow, there's many different cycle toggling algorithms, and dual to those are corresponding cut toggling algorithms. Now, dual cost is a cycle toggling algorithm, but there has no been, there's no dual cut toggling algorithm for solving the plateau linear systems known in literature. And that motivates us to ask the following research question, which is is there a cut toggling algorithm for solving laplacian linear systems, and if so, how efficiently can it be implemented? So our main contribution is that yes, there is a dual cut toggling algorithm to the cycle toggling cos algorithm in terms of how many iterations it takes. We show that it can run in still a nearly linear number of iterations. Just like Cos.
00:10:45.268 - 00:11:53.374, Speaker A: However, each iteration has to take o of n time. Moreover, it's unlikely that each iteration can be implemented in faster than o of n time via reduction to online matrix vector multiplication, which is an algorithmic problem that's been conjectured to be hard. Therefore, straightforward implementation of dual coasts, um, will require ult of mn time. However, we showed that we can circumvent this online matrix vector multiplication lower bound, because that lower bound assumes that we implement each iteration one at a time. Um, however, in this problem we can actually generate all the required updates in advance, and so we can show that if we batch those updates and process many of them at the same time, we can improve, get an improved o tilde of mt 1.5 running time. And with some more ideas like sparsification and recursion, we can get it down to o tilde of m to the one plus epsilon time for any epsilon bigger than zero.
00:11:53.374 - 00:13:07.724, Speaker A: But for the remainder of this talk, I'll mostly focus on describing the dual coast algorithm and maybe sketching how the batching works all right. So recall the KKT conditions for electrical flow. And so what the coase algorithm did was it always maintained a feasible b flow and it gradually toggled along fundamental cycles to gradually satisfy the second condition, ohm's law, or KPL. On the other hand, what dual cos will do is it's going to maintain potentials x, and it's always going to, it's going to toggle along fundamental cuts so that the flow obtained by Ohm's law gradually becomes feasible. In more detail, here's how dual cos works. So it's going to begin by still computing a low strep spanning tree, and it's going to initialize the initial potentials to be zero, and the corresponding potential defined flow is also zero. Now, in each iteration, it's going to sample a fundamental cut in the graph.
00:13:07.724 - 00:14:03.126, Speaker A: And what it's going to do, it's going to add a constant to the potential of every vertex in that cut, where that constant is exactly the amount that needs to be added, so that the resulting flow defined by the potentials satisfies flow conservation out of that cut. And so the running time of this algorithm is going to be as follows. Computing the low set spanning tree t takes ot n time, just like before. The number of iterations is also ultimate of m time, which is the stretch of t just as before. However, each iteration is going to take over n time because it involves adding a constant to the potential of every vertex on one side of a fundamental cut. And that could take o of n time because it could be o of n vertices in that cut. Here's an example of how it works.
00:14:03.126 - 00:14:41.438, Speaker A: So let's look at the same EXAMPLE from beFore, and recall the resistance are equal to one. InitiallY, the potentials are initialized to be zero. Now in each iteration we're going to sample a cut fundamental cut in the graph. So let's say we sample this one here. The amount of flow that's supposedly flowing out of this cut is two, but currently zero flow is flowing out. Remember that the flow value of an edge is given by the potential difference divided by the resistance. SINCe resistances are one, the flow is just difference between the potential and two endpoints.
00:14:41.438 - 00:15:25.974, Speaker A: So all the flows are zero. Right? Now the algorithm is going to add a constant to the potential of every vertex on one side of the cut, so that the resulting potential induced flow has a flow value of two coming out of the cut. So the right value to add in this case is two thirds, because that's going to make each of the edges coming out of the cut have a flow value of two thirds. And so the net flow value will be two. Okay. And the next iteration the algorithm samples some other fundamental cut. Let's say this one here, there's currently a flow value of two thirds flowing out of this cut, but they're supposed to be minus one flow flowing out of this cut.
00:15:25.974 - 00:16:28.954, Speaker A: And so the algorithm is going to subtract five nine from the potential of this vertex here so that the resulting flow out of this cut is minus one. And the algorithm just repeats this process. And we can show that also in if you do this for a nearly linear number of iterations, then the resulting potentials are going to converge to the solution of Lx equals b. Now just to give you an idea of how this can be sped up using batching. So the naive runtime of this algorithm is ot mn time because there's o tilde m iterations and each iteration takes o of n time. But note that um, the fundamental cuts that we need to be update, they can actually be sampled in advance. Because the probability distribution that they're sampled from is fixed at the very beginning, it doesn't change over time.
00:16:28.954 - 00:17:15.494, Speaker A: And so we can generate all of these updates in advance. And what we can do is we can split them up into batches. So let's call the batch this batch size equal to l. We can process l updates simultaneously, like at the same time. And what that allows us to do is if we only focus on l updates, we can contract all of the tree edges that are not involved in those l updates. And so in essence we're working with a contracted tree of size l plus one in each batch. That makes updates faster because instead of adding, having to add a potential to o of n nodes, we only have to add the potential to o of l nodes because our tree only has size l plus one now.
00:17:15.494 - 00:18:25.984, Speaker A: But there's also a downside because after we finish processing the batch, we have to uncontract the tree and propagate those changes back to the full tree. And um, we can show that um, the best way to trade off these two is by choosing the batch size to be square root of n. And that results in the overall run time of otos of m to the 1.5. Um, not in this talk are the following. So we didn't talk about the reduction to the online matrix spectrum application which says that, you know, assuming this, this, this hardness conjecture, if you implement the iterations of the algorithm one by one, you can't implement it to be faster than o of n time. And this is surprising because in the coase algorithm, you could implement each iteration of log n time even if you just do them one by one. So this shows that like in a dual code setting where you do cut toggling instead, these ideas, like batching and more complicated ideas are necessary to improve the runtime.
00:18:25.984 - 00:20:03.324, Speaker A: And I also didn't go into too many details about the batching and also the more complicated algorithm using sparsification recursion, which allows you to get the algorithm runtime down to otol to the m plus m to the power of one plus little o of one. Um, so to summarize, we're motivated by the following question, which is there is a cycle toggling algorithm known for lx equals b, the CoS algorithm, but no cut toggling counterpart. But for network flows, um, there's usually kind of a dual cut toggling algorithm to any cycle toggling algorithm. So our research question was, is there a cut togging algorithm for solving laplacian linear systems, and if so, how fast can it run? And what we show is that there is a cut togging algorithm that's dual to the CoS algorithm, and a naive implementation takes ultra m iterations and o n time per iteration. However, unlike the COS algorithm, where each iteration can be implemented in logarithmic time, we show that assuming this online matrix vector multiplication conjecture, you can't implement each iteration of our dual coast algorithm faster than o of n time if you have to run them one after the other. However, by exploiting the structure of the updates, we show that faster runtimes of m to the 1.5 and m to the one plus little o of one time are possible using batching and specification recursion.
