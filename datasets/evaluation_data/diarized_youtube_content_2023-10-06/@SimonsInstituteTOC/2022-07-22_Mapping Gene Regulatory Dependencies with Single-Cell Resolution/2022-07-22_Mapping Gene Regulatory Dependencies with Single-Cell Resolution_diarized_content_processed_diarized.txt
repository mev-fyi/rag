00:00:05.400 - 00:00:06.474, Speaker A: Like a charm before.
00:00:12.494 - 00:00:14.434, Speaker B: Now it works great.
00:00:17.534 - 00:00:52.252, Speaker A: Yeah. Thank you for, for having me here. I thought I'll present some combination of some data we generated and also the analytical questions. So I hope I, I have something interesting for the breadth of the audience in different areas before I get into the actual topic, which is actually quite related to what Davido was presenting. I want to also just sort of motivate a few sort of tools we're using as a backbone that I'll need to start out with. So we have a very strong interest in, obviously, leveraging different technologies that drive novel methodologies. I'm not going to talk about spatial assets today, although that would be also very interesting interest for us.
00:00:52.252 - 00:01:44.002, Speaker A: I want to talk a little bit about using multi omic data also in the context of spatial and temporal covariate, like here as an example, triple omic sequencing, where we azure multiple omics layers from the same cells. And the reason is that there's sort of two elements that we took away from this, that also in the remainder of my presentation be very useful. The first is it's been turned out very important and also sort of complicated and something I think we sometimes also underestimate to solve the engineering to align different matrices of rna and attack profiles from later on a target of hundreds of samples and putting these together. And we've been engaging in these activities and also thinking about the software infrastructure. And I think it would be also, it's quite a useful exercise to think about how to do this as a community. There's maybe this consortium sewers or for the python users. You might want to take a look at that, that we set with others to make these things interoperable.
00:01:44.002 - 00:02:49.168, Speaker A: The other side I want to quickly mention, because I will use this quite extensively, is factorizations of data. I'm not going to say much, I think it has been mentioned in several talks, but at several occasions I will also use now tools where we co factorize using group fact analysis, multi omics modalities that we have observed in parallel here, for example, using the example that we have methylation transcriptome accessibility from the same cells, the core. What I want to care about for this talk is really this latent representation that's shared, that we map these cells in a latent space, factors that we're going to use quite a bit in this. And I'm happy to also discuss offline there's, I think, quite exciting questions over how we incorporate spatial and temporal covariance, these types of approaches. But that's not for this talk. What I really want to focus on is a sort of technology that came our way a few years ago that really enabled scaling single cell sequencing to generating data from hundreds of people and what we can do with that. And that actually sparked a lot of interest because it's actually why we got engaged in the field, because it allows us addressing questions in human genetics.
00:02:49.168 - 00:03:26.274, Speaker A: And I have also a few slides of background why this question is irrelevant and then what approaches we've been taking to tackle them. The real motivator is maybe represented by this slide. This is a picture from the GVAS catalog. These different stretches here, human chromosomes, every dot here is a disease or phenotypic trait. We've been mapped to heritable variants in our genome. I guess most of you familiar with this type of representation. So we essentially have mappings between variants and disease traits of all sorts that can be as benign as earbuds type or eye color or disease risk for something you really don't want to have in advanced age.
00:03:26.274 - 00:04:18.802, Speaker A: And what we now really want to do is figure out what are the molecular intermediates that sit causally, ideally between variant and phenotype, and mediate or mitigate that signal, and then also confer targets for intervention and ultimately develop treatments. And I use this picture to represent this problem that I hope will help to get us here on the same page. So on the right hand side here, variant to phenotype, that's actually what genome assession studies do. For example, course like UK biobank. I'm not going to really talk about this here, but the key is we need hundreds of thousands of samples, and these associations are plus minus persistent, robust. There's a bit of g by e, but it's a regression problem we can better understand and linking variants to phenotypes. What I want to focus on is the other side of this diagram, which is linking these variants to molecular readouts like expression changes of genes.
00:04:18.802 - 00:04:54.366, Speaker A: And that has a positive and a negative aspect to it. The positive is we can do this in much smaller cohorts, so hundreds of people is often enough. And the reason is that these molecular effects on expenditures and gene expression are much stronger now you have a lot more power to detect those changes. So that's the good news. The bad news is that this association is incredibly in robust in the sense of it really depends on which cell type cell states are measuring. You need to find the right cell type, the right time development. To look at this, I'm having here this diagram from the lifetime white paper, which I quite like.
00:04:54.366 - 00:05:25.202, Speaker A: It shows that we really want to look at using single cell assays, wherein live trajectory cells mitigate and propagate. And you need to, at the right time, basically assess this association. And that is quite, quite a challenge in figuring out how you go from variant to expression and how to look at that. So this is where single cell assays come in. I'm talking just like a few more slides about data before I can come to the model. So we combined ips derived models, where we take IPS cells differentiate to different lineages. I'm talking about neurons in this case here.
00:05:25.202 - 00:06:11.774, Speaker A: And on the other hand, single cell sequencing, that really allows you to capture this complexity of cell types as they emerge and that we then can understand how do these variations in our genome then link to expression changes, and then we can also go back all the way to disease variance at the end. So a few years ago, we set this up with Florian Merkle and Daniel Gaffney, the sanger in Cambridge. And the approach we took, we took 215 people. So this is the diversity of individuals from which we had IPS alliance. So it's not a super large end, but substantial. And we created pools of human IPS cell lines. And I mentioned this pooling here will become later on, and I hope I can close the loop in time.
00:06:11.774 - 00:06:41.320, Speaker A: And basically, what this experiment conducts are just ten pools of 20 lines each. So this whole dataset are ten different stations. That's exciting because you obviously can do this very efficiently. We differentiate these pools towards this neuronal fate. Details don't really matter, but we collect cells at day 1130 and 52 and then collect the quite substantial datasets of a million cells or something like that, we can look at. So, one note about pooling. I think many of you here in the single cell field be using this in different contexts.
00:06:41.320 - 00:07:13.512, Speaker A: In a way, the pooling is, in our case, straightforwardly resolved because these lines are genetically diverse. So you can assay variants in your single cell unseq data. These data are extremely sparse, but given that any one of a pair of lines is roughly 10,000 variants segregating between, there's more than enough information. You find these 1020 variants that allow you to say, oh yeah, this cell comes from this guy. This cell comes from this guy. So you can resolve that. And there's also work from Jimmy Yi and others pioneering pooling methods in this context.
00:07:13.512 - 00:07:43.854, Speaker A: I also want to mention that this is great on the one hand, because you get more throughput and lower batch effects, but there's a downside, which you create very uneven representations of samples, you don't get the same numbers of cells from each individual. That's something to deal with in the analysis downstream. This is the data we're getting. We're starting with these blue guys here. These are progenitor cells. Then here there are neurons appearing, these green populations we care about in the end, and they also actually mature. But I want to emphasize that of subtle changes, of how cells change across differentiation.
00:07:43.854 - 00:08:16.004, Speaker A: And that's something I won't come back to later. What is actually a neuron and what are these cell types? And is it sensible to cluster these cells discretely? But for this purpose doing this, there's many questions we could offer. This. I just want to mention two major insights that we took away from this. The first is, that's exactly. It's quite important for downstream questions, is actually not all these lines make neurons in the same way. There are some lines that basically barely make any 20% efficiency in a way, while others make them at high proportions.
00:08:16.004 - 00:08:52.976, Speaker A: But that's not for this talk. What I really want to focus on, but there is a connection, is that we can use this data and then map genetic effects. Here there's green populations, for example, here, these are dopamine neuroneurons, and these guys here are serotogenic neurons. And what I'm showing here is just a stereotypical example. How we map these qtls is expression level of a particular gene. And now what we essentially do is in this population here, pseudobulc, or aggregate across all cells from one particular individual. So every dot here in this, in this cartoon is a person, is a line and asking whether those individuals carry a particular allele change in expression.
00:08:52.976 - 00:09:24.512, Speaker A: And here you can see presence of an association or absence of a cessation. In a different case, there's one detail that I want to mention that is quite important. That's exactly caused by this unequal sampling. So what we are doing, I come to that later on a little bit more detail, is actually fitting random effect models where we really account explicitly for the fact that, one, we have multiple repeat lines from the same person. That's the one relatedness you need to account for. The second is that you have a lot less information about these guys than these guys. So really, your noise properties are extremely skewed.
00:09:24.512 - 00:09:57.836, Speaker A: You need to really think about what properties you actually measure and what you don't measure. And there's a price to pay from pooling many lines. I want to just show you one example here. For those who care about the downstream biochem implication, we can actually find a number of interesting co localization event. Here is one hit for schizophrenia. So these are variants risk for schizophrenic outcomes later in life. And the same pool of variants that are here downstream of this gene, SFX and five are also associated with expression changes.
00:09:57.836 - 00:10:20.426, Speaker A: With statistical evidence for collocalization. In our data in this particular condition, for example, they don't show up in GTEx. So you can really drill in really this very fine defined cell subtitles. Understand what is going on. So far so good. So what's the statistical question? Well, Ana and Tobias in the lab, they thought about this problem. And what I talked about essentially is solving lots of regression problems.
00:10:20.426 - 00:10:47.156, Speaker A: We want to fit a cessation between variance and expression. And non zero slope is all we care about in molecular genetics. And we can do this across cell types. For example, cell type a, cell type b. And there is a slope that changes. And that might hint towards an interaction effect that here cell type modulates the way genetics controls gene expression. Now the challenge is this requires a definition of cell type.
00:10:47.156 - 00:11:30.378, Speaker A: And this is, at least in our data, not so well defined. And what we really would like to get at is actually a regression problem where every cell has its slope. We would like to assess genetic effects in each single cell because then we don't have defined cell types. I could ask, how does variance in the german genome affect gene expression in cell one to n? The problem with this model is it's not well defined because it has as many parameters as data points. The trick that Ana and Tobias came up with is to say, well, let's capture the similarity between these different cells on the transcriptome level. Using a similarity metric, we fit a covariance based model. I'll show you later how that works.
00:11:30.378 - 00:11:52.502, Speaker A: It basically captures how similar they are. That actually includes a special case of discrete stuff. So discrete would be two blocks. These guys are all group a. These guys are all group b. So you can capture the discrete nature, but you can obviously encode any other arbitrary specializations as well. And this leads us to fit a very simple, embarrassingly simple linear model where we now take a vector, which is the single cell expression value.
00:11:52.502 - 00:12:30.414, Speaker A: So this is the expression level of a particular gene across all cells. There's an additive effect that basically says these are these x vectors, the genotypes of the individuals where these cells come from. And that's basically just an additive effect. That's regardless of cell type, cell state. And there's an additional component where we now have one effect size as a dot wise product for every single cell. And as mentioned before, this is not identifiable because you have as many parameters in this model as you can easily see as you have data points. Now the way to make it interpretable or identifiable is to put a multivariate normal prior on this gamma matrix that exactly comes from this covariance I mentioned before.
00:12:30.414 - 00:12:59.790, Speaker A: And it essentially ties together the effective degrees of freedom. If you have a block structure, you would have only two possible values, but you can encode arbitrary assumptions about sharing or similarity between effect sizes across cells. And there is the baby estimate. The sigma is actually we use a matrix equation with single cell uniseq. So we fit one of these sparse factors models and use a couple of factors. That's important because it creates a low rank covariance. If this guy is low rank, the problem becomes tractable even for tens of thousands of cells.
00:12:59.790 - 00:13:40.570, Speaker A: Otherwise you're faced with inverting a very, very big matrix, which is in practice not possible. We also control for repeat structure, we include this relatedness component. There's another random effect matrix that accounts for the fact that multiple cells observed from the same person. That's also an important factor because otherwise you have double counting on this space and we can do efficient inference by exploiting the low rank structure of this. I'm not going to go into detail, but it's basically low rank covariance matrix that you can invert efficiently and deal with that. This is just to show that this works compared to other methods. So what we're showing here is basically our approach in blue and what we're looking at is powerful.
00:13:40.570 - 00:14:38.760, Speaker A: Recovering these interactions, or interactions is really this component, non zero interaction effects between genetics and cell state. And what I'm showing you here is on the one hand, you know, the variance explains more variants, you have more power. That's easy. But what's very interesting is basically how many environmental factors or what these individual ranks that make up a covariance matrix really explain g by c in the simulation and the comparison partner is just fitting a simple linear interaction model very test one factor at a time as a conventional interaction test, which obviously really suffers from a multiple testing problem. On the one hand that you test more and more environments and you really see that you gain power, particularly for complex interactions, when you have many, many environments that drive that signal. Does that also matter in practice? Here's one example. I'm focusing now just on these neurons, these dopamine neurons, just the subpopulation here that we lumped previously all together, although they were originating from different days of differentiation with and without treatment.
00:14:38.760 - 00:15:21.996, Speaker A: So it's clearly some substructure going on here, biologically speaking. And the first thing we asked is basically, does this continuous modeling help? And what I'm comparing here is basically just using these low rank composition of cell states for finding interactions. The resolution is a little bit poor, but this is the x axis versus discretizing data. So the first is I just use these three conditions, and then we just use clustering, nine clusters, 18 clusters and so forth. So basically just discretizing your data in different groups and doing a pairwise standard test. What's I think quite nicely to see is you really get more power by using this continuous modeling, actually quite a bit. And if you discretize, even if you overclock the data heavily, you only get a fraction of this discovery.
00:15:21.996 - 00:15:51.510, Speaker A: So the same results we see in simulations also hold on real data. And the reason really being that even if you discretize, you don't capture really the sharing or similarity between these different clusters. You assume that they're all independent, which obviously doesn't really hold. There's shared reflects across those. I just also want to show you a little bit about how this works in practice. These are four eQtl genes. And what I'm showing here in color is not the expression level of these genes, but the estimated effect sizes.
00:15:51.510 - 00:16:28.258, Speaker A: So this model estimates in every single cell a genetic effect size. And again, that's basically done by sharing pool information across all other cells. So there's strong assumptions inherent. That's important to realize. But essentially we can really see that, you know, there are some effects that look at particular subpopulations of cells in other EQ tails of other sub populations of cells, although these are all the same cell type, old neurons going forward. And actually if you cluster genes by their effect size profiles, we discover a number of regular pair patterns. For example, here is a number of regulatory variants that are only active in this top population, others only this tiny population at the bottom, and so forth.
00:16:28.258 - 00:16:58.224, Speaker A: I think there's really strong evidence that you need to look at the right cellular subset and cell type to look as regulatory variants. That is definitely not just one effect that you can lump together. I want to close this part by just making one tentative. And this is early days observation that is also matters for disease variants. For example, look at collocalization with sleeplessness, insomnia. This is another gbas hit. You can really see that you only actually find overlaps with eqtail associations.
00:16:58.224 - 00:17:32.510, Speaker A: If you look at this subpopulation, it actually comes out from, from our model based approach. So this also actually really matters for anatomy and disease variants in going forward. Okay, so let me sum up this part. I think I tried to convince you of two pieces. The one is that population scale single cell sequencing is a useful tool and we can, particularly using pooling, really scale this to really look at hundreds of people. And I think there's much more in this space. I've also shown you one idea how we can really use the full data set to model effects at single cell type.
00:17:32.510 - 00:18:19.960, Speaker A: We have to be sharing the actually interesting questions also about double dipping in this. I'm happy to discuss that offline or in questions, but it really allows you to share effect sizes in statistical evidence across cells and you can mitigate the definition of cell types. I think it's quite interesting because if you think about cell types being defined by function, you could even argue that you really want to cluster cells based on these genetic effect size profiles to define what cell types are, rather than on the distance on some UMap or tSne plot. I think that's an interesting direction to go into. I would like to take a few more minutes to just tell you what we're doing now. And actually this is early days for us and I'm very excited to hear your thoughts. What we are now interested in is I talked about variant to gene and this is something we can do using natural genetic variation that we explore.
00:18:19.960 - 00:18:50.516, Speaker A: These are typically small effects and what's on next question is how we can get the downstream players in these networks. How do we get the effects downstream pathways and so forth. And you know, in other words, trans effects of these variants. And you know, mapping trans effects is notorically difficult using population data. So what we are now doing is essentially combining these ideas of pooling. This is joint work with Leo, part the sanger of pooled ips cell lines with CRISPR perturbation. So we're now looking at 26 donors, that's roughly what we have right now, 34 lines.
00:18:50.516 - 00:19:36.544, Speaker A: So these are multiple lines from the same person. And we then combine these different lines with 7000 plus targeted CRISPR eye perturbations that all have either moderate or strong effect on phenotypes. So they have phenotypic effects and then we sequence those at different days of after perturbation stimulus, defending the effect sizes. But what's really important is in the end what we get is a matrix where every row is a cell. And for each of these cells we know now which donor these transcriptome profiles come from. But we also know the presence or absence of a perturbation state. So you can really now start tingling together donor of origin effects and variation that's present in the population with obviously the effect of this CRISPR eye perturbations.
00:19:36.544 - 00:20:12.024, Speaker A: And there's a number of interesting modeling questions that one can tackle on. Think about what do you get from these designs? And I just want to show you two examples of. One is, because it's genome scale, we can actually look at how similar are two targets. If you knock out two genes, we can ask, what's the sharing between target effect profiles? Because we have thousands of those. And what we're seeing at the moment is that many of these actually similarity between two phenotypic effects of two knockouts is driven by protein complexes. There's very, very strong structure about co complex membership. Those genes which behave similar if you knock them out, they share co complex memberships, which is, I think, something that's partially known.
00:20:12.024 - 00:21:09.946, Speaker A: But we cannot really look at this in IPS cells at scale. And there's also lots of interesting gaps where there's no known complex membership that we can look into. What I'm more excited about is the fact that we can use these multiple donors. So if you look at the umap of this data, the core structure is actually line, so there's strong donor effects. Remember, although these are pooled and these line effects are actually stronger than the individual perturbations, these CRISPR perturbations. This leads us to thinking about reactivating old ideas we had quite a few years ago, actually Vidi following up and David's talk to think about whether we can exploit these multiple donor components to fit graphical models using, building on independent causal mechanism principles. The core idea being that the causal graph should be invariant across these different people, and whether we can use that because we basically have in a very controlled condition now data for many, many, many individuals.
00:21:09.946 - 00:22:03.446, Speaker A: And we can look at whether we can look at this invariance and something that kai in the lab has been following up. And it's actually something we, quite a few years ago discovered in other contexts in yeast, that basically invariance to different contexts, stimulus and other elements, really helps for causation. Way before this was statistically established. This is, I think, a very useful principle to think about when we type in modeling this data, these invariances, and the moment we're doing this is by fitting causal graphs that explain variation, individual nodes, which are genes by parents. We have a regularizer that basically accounts for the graph structure, which we relax using l one constraint to optimize over possible Dax. And then we can obviously also include intervention effects because if you intervene at a particular variable, you break the causal independence assumptions. There's interesting results to that.
00:22:03.446 - 00:22:34.056, Speaker A: We get graphs out of that. What we're moment doing is we're taking these ICM inferences and taking next experiments to validate whether these things are real, which I can't show results for. But I think it's an interesting approach to think about how we can use these invariances for causal discovery. And maybe that's a contribution for the next coffee break. And with this, I want to end here. I think I acknowledge people along the way, but obviously really I want to thank the whole lab collaborators. I think I acknowledged Dan Gaffney and Flora Merkel for the IPS work and Leopold parks for this perturbation work.
00:22:34.056 - 00:22:36.994, Speaker A: At the end with this, I thank you and take questions.
00:22:41.734 - 00:23:13.814, Speaker B: Thank you very much, Oliver. So are there any questions? Fascinating, Oliver. So coming back to this uneven representation in the pools, do you worry about donors having very different number of cells and if so, what are some of the practical things you are doing to adjust for that?
00:23:13.894 - 00:23:34.644, Speaker A: Yeah, I mean, your question is well posed. I think pools are a really fascinating tool, but they have issues. So this is one pool that we followed over time. Every line here is a donor in the pool. And what you can see, we don't even start balanced. That's something to be aware of. We just pool for maximizing throughput.
00:23:34.644 - 00:24:12.632, Speaker A: But you can see that here in this case, two lines outgrow all the others. And that's something that we see routinely. We are just completely accepting it. What's something that was very interesting for us to observe? I mentioned that some lines more or less efficiently create these neurons at the end, the relative cell type proportions, they don't care about pooling. If we plot at the end stage, day 52, what fraction of the cells from each donor are neuronal? That doesn't really matter whether you differentiate them independently or in a pool. And that's very important for our purposes for looking at cell intrinsic properties. And many of the properties, gene regulation are very cell intrinsic.
00:24:12.632 - 00:24:49.908, Speaker A: And apparently in this case, even the sort of differences in outcomes pooling is finding and quite robustly observed. But I would agree in other cases it might make a difference. What we do in practice in terms of dealing with this number is really account and propagate errors. So whatever you do, you basically have a huge additional error sauce that's simply driven by cell count and it depends on what model you do. If you have a count based model, you really deal with this normally. I think there's actually a beautiful example where count based models make a big difference because you get so much differential sort of signal to noise ratio in different observations. You can also use other simple approximations.
00:24:49.908 - 00:25:11.884, Speaker A: It turns out if you just use a multivariate normal and have a diagonal error covariance matrix and scale them with one over square root of n, the numbers of cells, you get almost the same result, which I think is sort of, you know, similar to this sort of lima boom type ideas that you can, you know, work in an approximate gaussian space if you account for different residual variances. But that's how you have to do that and, and that's the price to pay. But you know, you get increased from.
00:25:15.824 - 00:25:36.974, Speaker B: Really interesting during the break. Okay, just join. Yeah, yeah, really interesting talk. So sorry if I missed the beginning. Are you looking only at annotated kind of common variants? And if you are, do you have any estimate of, you know, the effect of the rare variants on unexplained?
00:25:37.594 - 00:26:10.068, Speaker A: Yeah, it's a very good question. So at the moment, you know, this is a, it's a matter of sample sizes. In current spaces we're looking at mainly common variants. I think there's two interesting dimensions to this question. The one is leveraging information we have from other large bulk cohorts over to these more bespoke assays. If you want to go to tiny single cell subset, that's great, but you don't want to go in blindly. So I think reweighting, independent hypothesis reweighting or other methods would be very useful.
00:26:10.068 - 00:26:30.754, Speaker A: Leveraging the bulk information we have and not treating variants the same. That's one approach. The other approach obviously is to aggregate across various effects for looking at burdens. But I think for this we need larger sample sizes. So we have now efforts in the way to do 5000 people single cell and seek and uk biobank. And that's the scale I think we need to really look at rare variants in any shape or form. Yeah.
00:26:35.494 - 00:26:56.794, Speaker B: Oliver. Thanks everyone for the question. Oh, you had a question or a reminder the picture, that's what I was. So we have a half an hour break. We resume here at 03:00 but please, before you grab coffee, go to the front of the Simons Institute outside and we'll be taking a group photo. And we're back here at 03:00 for the rest of the talks. Thanks everyone.
