00:00:00.280 - 00:00:47.654, Speaker A: Okay, so this is the crash course on sampling. And so there are some additional references. So, first of all, you find the slides of this talk on my homepage, which is here. And you also find some lecture notes of a course on MCMC methods that we gave last year on my homepage. And this has much more material than I can talk about now. And another nice reference are these lecture notes by Montenegro and cataly on mathematical aspects of mixing times of Markov chains that you also find in the Internet. And then there is some complementary reference.
00:00:47.654 - 00:01:36.394, Speaker A: So this is not really on the material I'll be talking about here, but complementary. So this is about Markov chains with a finite state space and their mixing times. There's a very nice book by Levine, Paris and Wilma. Okay, so now let's start. So, here we are looking at probability measures on RD. So, we assume that we are given a probability measure mu on RD, and we assume it has a density with respect to the Lebec measure. And we write the density in the form one over a normalization constant times e to the minus u of x.
00:01:36.394 - 00:02:20.184, Speaker A: This density is also denoted by mu of x. Now, to make the connection to the optimization course yesterday. So, in optimization, we had this function f appearing, and this function f is now replaced by this function u here. So, yesterday we wanted to minimize such a function. And now, instead of wanting to minimize, we want to sample from a probability measure with density proportional to e to the minus u. So our goal is to generate approximate samples from you. We can also make another connection to optimization.
00:02:20.184 - 00:03:15.430, Speaker A: So suppose instead of considering this measure here, we consider measure with density e to the minus beta u of x times a normalizing constant. Then if we let beta go to infinity, then this measure concentrates on the global minima of u, or near the global minima of u. So that means if we could simulate u beta for every beta, and then let beta go to infinity, then we could also find a global minimum. Okay, so, now to the sampling problem. There are many approaches to sampling, and let me first briefly describe some simple classical approaches. So, first of all, there are direct methods. You can somehow directly, sometimes directly create numerically samples.
00:03:15.430 - 00:04:21.524, Speaker A: So, for example, if you have a probability measure on r1, then you can just create a uniform random number, and then you apply the generalized inverse of the distribution function to that uniform random number. And that gives you a sample from your measure mu. So, if you have access to this generalized inverse of the distribution function, then this gives you an algorithm to generate samples from you. But this is limited to the one dimensional case, something more general is acceptance rejection. So in acceptance rejection, we have another probability measure, nu, and we assume that we are already able to create samples from nu. So nu is called the proposal distributions. Because we propose samples from nu, then we assume that the measure we are interested in has a density with respect to nu, a density proportional to rho of x.
00:04:21.524 - 00:05:19.964, Speaker A: And we assume that this function rho is upper bounded by a constant c. Then we can create samples from mu in the following way. We first create a sample from mu. Then we create independently a uniform random number, which we call u. And then if u is smaller than rho of x over c, then we accept the sample from nu as a sample from mu, and otherwise we create a new sample from mu, again, draw a new uniform random number, and again, if this holds, we accept, and otherwise we start again and so on. So this will terminate in finite time, and it will create us an exact sample from you. What's the disadvantage? Well, the disadvantage is often the running time.
00:05:19.964 - 00:06:33.154, Speaker A: So what can we say about the running time? Sorry for that, have to go up again. So what can we say about the running time of this algorithm? Well, since, sorry. Okay, since we are creating here iid samples, the number of steps we need until we get acceptance is geometrically distributed with parameter p. And here p is one over c times the integral of rho with respect to the measure mu. That means the average running time, that's the expectation of t. And this is one over p, and one over p is this constant c divided by this integral. Now, the point is that c is often very large.
00:06:33.154 - 00:07:31.074, Speaker A: So, for example, if you're in high dimensions, if you have a product model, then c will grow exponentially in the dimension. So that means your running time typically will grow exponentially in the dimension, at least in many situations. So it will often be too large. So, although in special situations, this is useful, and it's definitely useful as a building block, in other algorithms, usually on its own, this will not be sufficient. Okay, so because of that, we have to give up something. And now we are looking no longer at creating exact samples, but we are only asking, can we create approximate samples? And the most widely used method for creating approximate samples is Markov chain Monte Carlo. And that's what I will talk about today.
00:07:31.074 - 00:08:48.934, Speaker A: The idea in Markov chain Monte Carlo is that you create a Markov chain which has mu as its invariant measure, and then you simulate this Markov chain for a long time, and then you hope that it has nice ergodic properties, and that after a long time, it has mixed well, and so its law is approximately equal to the equilibrium distribution. And then you take a sample from the Markov chain after a long time, and this gives you an approximate sample from your measure mean. Okay? So to make it more precise, in order to define this Markov chain, we need a transition kernel. And the transition kernel is a probability kernel on RD. So what's the probability kernel? This is just a measurable function in the first variable and the probability measure in the second variable. Okay, and then we assume that our measure mu is an invariant measure for this probability kernel. What does that mean? Well, that means this equation here.
00:08:48.934 - 00:09:22.204, Speaker A: So mu of b is equal to mu PI of b. What's mu PI of b? This is just the integral over RD with respect to the measure mu of PI XB. So this is the law if you start with mu after the next step of the Markov chain. And so this identity says that if you start with mu, then in the next step you also have law mu. So that means mu is preserved. That means invariant. Okay, so that's what you usually want.
00:09:22.204 - 00:09:57.388, Speaker A: You want the Markov chain, which has mu, is its invariant measure. Sometimes you even make something less restrictive. You allow that mu is only approximately equal to mu PI. So that would be some relaxation here. But most of the time, we will assume that the two are equal. Okay, so then you want to create a Markov chain with this transition kernel PI. So you start with some initial distribution that you can choose.
00:09:57.388 - 00:10:41.354, Speaker A: So the initial distribution will be a probability measure on RD again. And this you can choose. If the Markov chain has very good mixing properties, then you can basically choose what you like, because the Markov chain will forget its initial law rapidly. But if the mixing properties are maybe not so good or you don't know, then maybe you have to think about also to make a good choice for the initial distribution. Okay, so then you create a Markov chain. That means a stochastic process such that x zero is distributed according to nu. And then the transition mechanism from one step to the next is given by this transition kernel PI.
00:10:41.354 - 00:11:32.604, Speaker A: Okay, if you do that, then the law at time t of your Markov chain is denoted by new t. So that's just the probability that xt is in b. So the distribution after t steps of your Markov chain, this is just given by the product nu PI to the t. So you just apply the transition kernel t times to the measure nu. Okay, so that's your law at time t. And now the idea, or the hope, is that if t is sufficiently large, then under certain assumptions, then mu t will be close to mu in some sense. And that means a sample from mu t, which is given by x t will be an approximate sample from mu.
00:11:32.604 - 00:12:39.722, Speaker A: Of course, there are now a lot of question marks that have to be made precise. So, for example, what does sufficiently large mean? What does this approximately equal here means? So here we need some distance between probability measures or something like that. And then can we prove this implication? And in particular, under which assumptions does it hold? So, there are a lot of questions that need to be answered. Indeed, they boil down to the following key questions. So the main question is you want to get upper and lower bounds for w nu t mu, where w is either some distance function on probability measures or what is called divergence. Divergence is some kind of non symmetric distance function, distance function. This can, for example, be total variation distance or some Wasserstein distance.
00:12:39.722 - 00:13:43.834, Speaker A: So you want to bound these distances to the target and divergence, this can, for example, be the chi square divergence or the Quilbeck leplar divergence, which is the same as the relative entropy. Okay, so you want to get upper and lower bounds for these four different types of Markov chains. And you can also formulate it in a different way in terms of mixing and relaxation times. So what is a mixing time? Well, if you're given a distance w, then you can define a mixing time according to this distance. So you give a perimeter epsilon. So that's given, that's a positive number. And then you define the mixing time with perimeter epsilon as the minimum of all times, such that the distance of mu t to mu is smaller than epsilon times the distance of the initial or to mu.
00:13:43.834 - 00:14:37.250, Speaker A: So that means that's the first time where for all initial loss, your distance has decayed by a factor epsilon. Okay, so this you can define for general distances. Now, for total variation distance, you usually use a slightly different definition, and that's because total variation distances are always bounded by one. So therefore, you drop this one here. So you define the total variation mixing time is the minimum of all t, such that the total variation distance from nu t to mu is smaller than epsilon for any initial law nu. Okay, so now if you do that, then it turns out that this total variation mixing time will usually be infinite on unbounded state spaces. So that's not a good definition.
00:14:37.250 - 00:15:29.504, Speaker A: So you have to modify it. And what you do is you restrict to compact sets. So you choose some set k, and then you define the total variation mixing time on the set kick to be the first time such that the total variation distance after t is smaller than epsilon, but now only for all probability measures that live on the set k. So you only allow initial loss in this compact set k, and in that way you get something finite. Okay, so that's another way to measure convergence to equilibrium. And there are corresponding time times related to divergencies, and these are called relaxation times. Sorry for that.
00:15:29.504 - 00:16:46.374, Speaker A: Okay, then there's a third question that's of interest, but that I do not have the time to discuss today. And that's bounds for ergodic averages, because why do you want to grade your samples? Well, there are different purposes, but one purpose is often to estimate integrals numerically. And how can you estimate integrals so integral of a function f with respect to the probability measure mu? How can you estimate that numerically? Well, you can use the ergodic theorem and approximate it by ergodic averages. So you take such an average from time b to b plus t minus one, where b is called the burn in. So that's a given number of steps that you ignore, and then you average over the next t steps. And then the ergodic theorem tells you that under certain conditions that should be close to integral fd. And then you want to quantify the probability that the distance is greater than epsilon, say.
00:16:46.374 - 00:17:25.796, Speaker A: Okay, so that's another problem which is closely related to the other two problems, but it's not the same. Okay, so which mathematical approaches are there to get bounds on mixing times and things like that? Well, basically there are two families of approaches. One is more probabilistic, the other more analytic. First family are coupling methods. So they use couplings. And I will explain this afterwards in more detail. But there are different types of couplings.
00:17:25.796 - 00:18:02.898, Speaker A: So for example, you can just look at couplings of the transition kernels of your Markov chain, for example, and this will be closely related to contractions in Wasserstein distances. Or you can look at couplings of Markov processes. So here you do not just couple just a kernel, but the whole process. And this also gives you bounds, for example, through the coupling lemma. This is just an overview. We'll come to that. Or you can look at special couplings for diffusion processes.
00:18:02.898 - 00:18:44.584, Speaker A: So, processes in continuous time. And there's also something which is not really only coupling, which is called Harris theorem or weak Harris theorem. So that combines some coupling ideas, some minorization or Wasserstein contraction with elapunov function. So these are more probabilistic approaches. And then there are more analytic approaches based on functional inequalities and the first approach is conductance. We'll come to that in a moment. Here you do not really see the functional inequality, but it's actually equivalent to one.
00:18:44.584 - 00:20:04.414, Speaker A: And others are spectral gaps, which are linked to Poincare inequalities, logarithmic subolov inequalities, and then more recent approaches for non reversible Markov processes based on hypercoercivity, and so on. So, that's just a short overview, and some of these things I will come to now. Okay, so how do we get a Markov chain with the right invariant measure? Well, the most well known procedure is the metropolis algorithm, or more generally, metropolis Hastings. Okay, so here what we are doing is we first have another probability kernel, which is not the transition kernel of our Markov chain, but which is used to create proposals. So we have another probability kernel, PxDy, which can be rather arbitrary, but we assume that it has a density with respect to the back measure, and we assume that the density is strictly positive. Now, the idea is to modify these transitions here so that you get the right invariant measure. And this modification is again done by an acceptance rejection procedure.
00:20:04.414 - 00:20:56.678, Speaker A: So, you now construct a markov chain, and its transition step is given by the following algorithm. So, if you add x, you move to x prime in the following way. So, you first create a sample from your proposal kernel, and again, you sample a uniform random number independently, and now you accept your move with a certain probability. Namely, you accept it if your uniform random number is smaller than this quotient here. Okay, so what's this quotient here? Well, the numerator. So this is the flow from y to x, and the denominator is the probability flow from x to y. So you look at the quotient of the two.
00:20:56.678 - 00:21:40.444, Speaker A: If the two are equal, then you have a detailed balance condition, and that implies invariance. So, if numerator and denominator are already equal, then you have invariants. But if they are not equal, then you look at this quotient, and then you only accept with a probability which is determined by this quotient. So if you accept, then you move to y, and otherwise you just stay where you are at position x. Okay, so in that way, you define a Markov chain. And what's the transition kernel? Well, you have this, sorry for that. Didn't happen before to me.
00:21:40.444 - 00:22:35.894, Speaker A: I don't know why this is happening. Okay, so this defines the Markov chain. And what's the transition kernel of the Markov chain? So you have the proposal kernel p, and then it gets multiplied by the acceptance probability axy. And then you have another probability r of x that you reject, and in that case, you stay at x. Okay, so what's the acceptance probability? Well, the acceptance probability is essentially this number here. But this number can be greater than one. So if it's greater than one, then you always accept.
00:22:35.894 - 00:23:14.794, Speaker A: So therefore the acceptance probability is the minimum of this ratio and one. Okay, and then the rejection probability, that's just the remaining probability for not accepting integrated overall proposed moves. Okay, so now it's easy to check that your measure mu is really an invariant measure for PI. So that's a little exercise. And actually you can show a bit more. You can show the detailed balance condition. You can show that the flow from x to y is the same as the flow from y to x.
00:23:14.794 - 00:23:55.854, Speaker A: If you don't like this notation here with the measures, just write it with density. So mu of x PI xy is the same as mu of Y PI yx. So you have the detailed balance condition, and this condition is actually stronger than the invariance. Actually, if you integrate this condition here over x over the whole state space, then you get the invariance. Then you get that nu PI is equal to mu. So we have a better condition than invariance. We have detailed balance, which corresponds to reversibility of the Markov chain with invariant, with initial law of mu.
00:23:55.854 - 00:24:41.140, Speaker A: Okay, so now here you can choose your proposal kernel. And so therefore, there are many variants of metropolis Hastings algorithm. And the real question is, how do you choose your proposal kernel in a good way? So here are some simple choices. So the easiest choice, maybe, is to choose your proposal distribution independently of where you currently are, so, independently of x. So just choose it always equal to mu. That's called the independent sampler, and that gives you a method which is quite similar to acceptance rejection. It's not the same, but it has similar properties.
00:24:41.140 - 00:25:35.474, Speaker A: You can study it precisely, and you see that it basically works well in the same situations where acceptance rejection works well. Okay, so this doesn't help us a lot, but there are situations where you can use it, or you can use it as an ingredient in more complicated things. Okay, a second simple possibility is the random walk metropolis algorithm. So here, your proposal kernel is the transition kernel of a random walk. So that means you propose to make a transition of a random walk. So that means if you're at x, you jump to a point which is distributed around x. And for example, you assume that it's normally distributed around x with a given variance h times the identity matrix.
00:25:35.474 - 00:26:21.322, Speaker A: And here h is a fixed constant. So that's something like a step size for your proposal. Okay? So in that case, you can check that the proposal density is symmetric. And so therefore, your acceptance probability simplifies. So the expression for the acceptance probability is now just e to the minus, the positive part of u of y minus u of x. Okay? And your proposal in that case is just if you add x, then you move to x plus square root h z, where z is a standard normal random variable. Okay, I'll come back to that in a moment.
00:26:21.322 - 00:26:55.654, Speaker A: But before that, let me mention something we come back to later. So you can do a somehow better adjusted proposal by taking into account the gradient of your function u. And so that gives the metropolis adjusted Langema algorithm. So here you make the following proposal. So you first move like in gradient descent. So you move from x to x minus h times nabla u of x. And then you add a random perturbation.
00:26:55.654 - 00:27:27.200, Speaker A: So you add square root two h times a standard normal random variable. We will see later why this is reasonable. So that's some stochastic perturbation of gradient descent that you use as your proposal. And then you put your metropolis hasting step. So that would be another possibility. Okay, so now let's go back to random walk metropolis. And, yeah, I will now for several algorithms.
00:27:27.200 - 00:28:24.504, Speaker A: I will just look at them in the case of a standard normal target distribution. Of course, nobody is interested in sampling from a standard normal target distribution, but it's nevertheless a good method to see which problems could occur with the algorithm. Yeah, so let's look at random walk metropolis, where your target distribution is just a standard normal distribution. So that means the function u of x is just quadratic. So this is your proposal. And now we set the acceptance probability is this one here. The acceptance probability is this one here.
00:28:24.504 - 00:29:11.688, Speaker A: And so we have to look at u of y minus u of x to study the acceptance probability. Okay, so what is u of y minus u of x in that case? So y is x plus square root h times a standard normal. So you just see this gives square root h x times z. And then you get plus h half times the euclidean norm of z squared. And the euclidean norm of z squared, that's the sum of the zi squared. But now the z I squared, these are iid random variables. And then you recall, for sums of iid random variables, you have a law of large numbers and a central limit theory.
00:29:11.688 - 00:29:52.994, Speaker A: So that means you can study this in high dimensions. And to do that, we just subtract the expectation value so that's this one here. And then we add it again here. And now, you see, this quantity is now a sum of standard normal random variables. And so it's of stochastic order square root d by the central limit zero. Okay? So that means this term here is roughly of order h times square root d, whereas this term is of order h times d. And then you see that this last term is dominant if Hd is large.
00:29:52.994 - 00:30:37.708, Speaker A: So that means if Hd is large, then this quantity will typically be large. But if this quantity is large, then you see your acceptance probability is very small. Okay, so from this, you see if HD is large, your acceptance probability will be typically very small. And you can make that precise. So, you have this bound here on the acceptance probability. So that means the probability that the acceptance probability is greater than this small number is very small. So this is some fixed positive constant, okay? So that means most of the time you will not accept.
00:30:37.708 - 00:31:21.502, Speaker A: It will be very unlikely that you accept your proposed move in this case, if HD is large. And that means it will not mix well if HD is large. Okay, so this gives me the opportunity to introduce a quantity by which we can link this to the mixing time. And that's the conductance. Okay, so what is the conductance? The conductance gives us both lower and upper bounds for the mixing time, but it's often used to get lower bounds. So, to introduce the conductance, we first define the equilibrium flow from a set a to the set b. That's this Qab here.
00:31:21.502 - 00:32:13.594, Speaker A: So that's just the integral over a with respect to the measure mu of PI x b. So, this is the probability that before your move. So, if you start with the measure mu, then it's the probability that before the move, you're in the set a, and after the move, you're in the set b. So that's the equilibrium flow from a to b. And then you somehow have a problem with mixing if you have a set, or if you have two disjoint sets such that the equilibrium flow between these sets is small. But these sets have both large mass, and that's measured by the conductance. So the conductance is just the quotient of the flow from a to a complement divided by the minimum of the masses of a and a complement.
00:32:13.594 - 00:32:59.174, Speaker A: So that's the conductance of the set a. So, for example, if you look at this probability measure on r one, then you see there's a bottleneck here. So, you see, if you have a local algorithm, it might have problems moving through this bottleneck. And that can be quantified by the conductance. Namely, you can now look at this set a here, and then your set a complement will be this one here, and the boundary will be here. And in that case, if you have some local algorithm, then the flow from a to a complement will be small, but the mass of both sets will be large. So the conductance will be small.
00:32:59.174 - 00:33:45.004, Speaker A: So the conductance gives you a way to see bottlenecks. And now you have the following theorem, which is also not so difficult to prove. So, if the mass of a is smaller than one half, say, then the total variation mixing time of the set a is bounded from below by one over four times the conductance of a. So this one quarter here, this is quite arbitrary. You could replace that by another epsilon, and then you would just get another constant here. So that's not important. What is important is that the inverse of the conductance gives you a lower bound for the mixing time.
00:33:45.004 - 00:34:49.014, Speaker A: And so the nice thing about the theorem is that this is valid completely, generally. So you don't need any assumption on your Markov chain for this. Okay, so for example, for your random walk metropolis algorithm, you see now that the mixing time of n, if you start in any ball around the origin, is greater than one over eight e to the h e over four. And now you immediately see that if HD is large, your mixing time will be very, very large. So you immediately see that you have to choose your step size of order d inverse. At least here you see a problem of the Randen walk metropolis algorithm. Even if you could achieve this upper bound here, which is not clear, but even if you could do, you still would have to choose h of order d inverse.
00:34:49.014 - 00:35:22.064, Speaker A: Okay, so the conductance also provides upper bounds on mixing times. Well, not quite. It actually provides upper bounds on l two relaxation times. And these can be used to get bounds on mixing times. But you have to assume a warm start. We'll come back to that later. So you cannot start from an arbitrary initial value.
00:35:22.064 - 00:35:57.004, Speaker A: Okay, but there's also a converse to that result. Okay. Because of these problems of random walk metropolis, it seems reasonable to look for improved proposals. And so the idea is similar to what we saw yesterday. Now, the next step would be to go to a first order method. So that means take into account the gradient of u. And that's what we want to do now.
00:35:57.004 - 00:36:52.436, Speaker A: But in order to do that, I like first to move into continuous time, because it helps us clarify the concepts. So similar also to what we saw in optimization, we will first look at continuous time things and then afterwards, we will look at how to discretize them and how to get algorithms in discrete time. Okay, any, are there any questions up to here? So, now we would start with the next part. There's something in the chat. I'm not able to really follow the chat. Let's see. Okay.
00:36:52.436 - 00:37:25.454, Speaker A: No, but that's not a question. Okay. Okay. So if you have questions, just interrupt, just ask. Now we move to continuous time. And so we come to what is called over damped longeworn dynamics, which is a continuous time Markov process with the right invariant measure. So we still have our invariant, we still have our measure mu with density proportional to e to the minus u of x.
00:37:25.454 - 00:38:01.734, Speaker A: But now we want to find a continuous time Markov process, which has this measure as invariant measure. And this you can do by looking at a stochastic differential equation. So what you look at is the following. So you look at an equation d x t is equal to b x t dt. So without this noise term, this is just like an ode. This is just the ode generated by the vector field b. And then you add this noise term, which is square root two times the increment of a brownian motion.
00:38:01.734 - 00:38:52.774, Speaker A: So that's a quite simple stochastic differential equation. And if you choose b in the right way, then you get mu of is your invariant measure. So how do you have to choose b? Well, you can choose b, for example, exactly equal to minus the gradient of, uh, and then we are again back to what we saw yesterday. So then here we would have just gradient descent, just gradient flow, and then here we have a stochastic perturbation of the gradient flow. So that's somehow the easiest choice that you can do. But there are also other choices. So you can actually add here another vector field ETA, with certain properties, and you will still have the right invariant measure.
00:38:52.774 - 00:39:41.402, Speaker A: And this is what the following theorem says. Well, first of all, it says that under certain conditions, you have a solution of this STE, and it defines the Markov process. So you need some condition on the drift, this one here. So this just guarantees that your solution does not explode. So if you assume this condition, then for every initial law, so for every initial law new, you have a unique, non explosive strong solution of this stochastic differential equation. And you can also show that this solution defines a strong Markov process. And so this now has a transition function.
00:39:41.402 - 00:40:09.974, Speaker A: The transition function now depends on t. We are in continuous time. And I now use the notation ptf of x. So that's just the integral with respect to your transition kernel at time t of the function f of y, and ptf of x is just the expectation of f. If you start at position x. So that's your transition function. And this now forms a semigroup.
00:40:09.974 - 00:41:15.384, Speaker A: So pspt is equal to pt plus squared. And then you can also identify the generator of this, and at least on nice functions, so on compactly supported smooth functions, the generator lf is given by laplace f plus b times nabla f, this b times nabla f, that would be the generator of the ode. And this Laplacian here, that's the generator of the brownian motion part. And now you have both together here. Okay? And this limit you can take in this case, for example, in the supremum. Okay, so you have a solution, and then you ask, when is mu an invariant measure for this? And you find that this is the case if and only if the divergence of e to the minus u times beta vanishes. This is somehow some divergence of beta with respect to the measure mu.
00:41:15.384 - 00:42:08.564, Speaker A: So if this divergence of beta with respect to the measure mu vanishes, then the ste with this drift here will have the right invariant measure. Okay? On the other hand, you may ask, when do we have detailed balance, which is a stronger condition? And turns out this is only the case if your drift is exactly minus the gradient of u. So this is only the case if beta vanishes. Okay, so how do you prove this? Yeah, maybe just briefly. So the generator you identify by stochastic analysis using Ito's formula. Now to check for the invariants. That's maybe interesting for us since we need the right invariant measure.
00:42:08.564 - 00:43:00.496, Speaker A: So you want to check if mu pt is equal to mu for all t. Now you integrate a function f with respect to these two measures. Then you see by Fubini's theory that this is equivalent to saying that the integral of Ptf Dmu is the same as the integral of f d mu for all measurable functions f. Now you can see that if this holds, then by differentiating in t, you get that the integral of Lfd mu vanishes where l is the generator. The generator is somehow like the derivative of Ptf at time zero. So from this you can conclude that integral Lfd vanishes. But you can also go back.
00:43:00.496 - 00:43:37.084, Speaker A: Going back is not so trivial. You need some analysis for this. You have to show that these functions that you have, these test functions form a core for your operator. But with the appropriate analysis in this case, you can also go back. You see that invariance is equivalent to this condition. And now if you write down your generator and integrate by parts, then you see that this is equivalent to this divergence condition. So you see invariance holds exactly in this case.
00:43:37.084 - 00:44:49.864, Speaker A: And well, you can also check that if b is just minus the gradient of u, so if the beta vanishes, then by integration by parts, you see that this integral LfGD mutation is symmetric in FnG. So it just distilled form integral nabla f nabla gdMu. So it's symmetric. And this symmetry implies that also your transition function is a symmetric linear operator. And that turns out to be equivalent to the detailed balance condition. Okay, so you see that you have detailed balance if and only if b is a negative gradient of u, but you have invariance also for other drifts. Okay? So now that you have that, you ask, what can we say about convergence to equilibrium for this Markov process in continuous time? Okay? And in order to do that, there are these different approaches that I mentioned before, and we start with the coupling approach.
00:44:49.864 - 00:45:40.432, Speaker A: So for the coupling approach, you need a distance on probability measures. And the simplest distance, simplest appropriate distance to use here is an LP Waserstein distance. So what is the LP wasserstein distance between mu and mu? Well, that's just, you do the following. You sample x from Nu. So you take two random variables, x and y, which are defined on the same probability space, and such that x has law nu and y has law mu. Two such random variables are called a coupling, or more precisely, a realization of a coupling of the measures mu and mu. Okay? And now you just look at the Lp distance of these two random variables.
00:45:40.432 - 00:46:19.684, Speaker A: So you just look at the Lp distance of x and y. Now, of course, this lp distance depends on the coupling that you choose, and it can be very large. But now you look for the optimal coupling. So that means you take the infimum of this lp distance over all couplings of mu and mu. And that defines the lp Wasserstein distance between mu and mutual, which is a very natural distance on probability measures. Okay? So that's one possibility. And you see here you apply some convex function to the difference x minus y in this case.
00:46:19.684 - 00:47:29.774, Speaker A: So there's some convexity here because of this, you can also look at a counterpart where instead of something convex here, you put something concave that gives these distances wf. So here, instead of applying a convex function, you apply a concave function f to x minus y, and then you do the same. So of course, not an arbitrary concave function, but one that maps zero infinity to zero infinity, that maps zero to zero, which is non decreasing and convex. So I have mind functions that look like this. So usually they will be continuous, but they might have a jump at zero and they are convex and non decreasing. So you take such a function, apply it to x minus y, and then you take the same, and that defines also a Wasserstein distance, actually an L, one Wasserstein distance, but with respect to a different metric, which is defined by this concave function f. I'm a bit confused.
00:47:29.774 - 00:47:49.794, Speaker A: You're using convex and concave. Can you tell us what is convex? What is concave? Sorry. This is of course a concave function. Yeah. So here we have a convex function. Here we have a concave function. Thanks.
00:47:49.794 - 00:49:00.340, Speaker A: Okay, so we look at these two distances. So why is the second distance interesting? Well, first of all, one can often prove bounds in these distances where one cannot prove bounds in the lp Wasserstein distances. And secondly, this distance also includes the total variation distance. For example, so if you take f of t, just the indicator function that t is positive, then, and then you look at this wf, then you have here the infimum of the probability that x is not equal to y over all couplings x and y. And that's exactly the coupling characterization of the total variation distance between mu and mu. So the total variation distance is somehow an extreme case of these wf distances. Okay, so now we have distances.
00:49:00.340 - 00:50:15.432, Speaker A: And so these distances turn out to be useful in connection with couplings. So why is that? This is because suppose you have a coupling x t yt of your Markov of two Markov processes, the Markov process, which starts at some initial or new and has the transition function pt and the Markov process with the same transition function, but starting at initial norm u, what is the coupling of these two Markov processes? So this is just a Markov process on the product space such that the first component is a Markov process with initial or new and transition function pt. And the second component is such a Markov process. Okay, so suppose you have such a coupling. Well then xt has lower new pt and yt has lower new pt. And so therefore, by the coupling definition of the Wasserstein distance, you can bound the LP Wasserstein distance between Nupt and nupt by the lp distance between xt and yt. Yeah.
00:50:15.432 - 00:51:18.154, Speaker A: And so now in particular, if mu is your invariant measure, mu is your invariant measure, then this is just equal to mu. And then you have here bound on the LP Wasserstein distance between your law at time t and your target distribution mu. So you can use couplings to get bounds on these Waserstein distances, and exactly the same for these distances. Wf. Okay, so now how do I get couplings of two diffusion processes? Well, so what you should have is you should have two Markov processes defined on the same probability space, which satisfy the same SDE, but start at different initial laws. Okay, and which choice do I have in that case? Well, there's only one choice I have. I can choose these brownian motions here in particular.
00:51:18.154 - 00:51:50.462, Speaker A: I can choose how the two are related to each other. So they might be the same, they might be independent, and there are other possibilities. That's the only choice I have. And each choice for these brownian notions defines a coupling of these processes. Okay, so now how can I connect these two brownian motions? Well, the easiest way is just to choose them equal. So you choose B. Twiddle equal to b.
00:51:50.462 - 00:52:23.464, Speaker A: And that's called a synchronous coupling, and that's the most widely used coupling. Okay, let's do that. Let's consider this coupling. So, we just have two solutions of our process with different initial laws, but they satisfy the same SDE with the same brownian motion. Well, then we can just take difference of the two. So we can just look at x, t minus yt, and then the noise cancels because they have the same brownian motion. And you just get an ode for the difference.
00:52:23.464 - 00:53:30.624, Speaker A: So the time derivative of x minus y is just b of x minus b of y, same type of thing as we saw yesterday. And then you know well how to study this equation. Well, you know that if we now have this condition here, which I call ak, if x minus y times b of x minus b of y is bounded from above by minus k times x minus y squared, which in the case where b is the negative gradient of u is just k, strong convexity. So if we have this condition for some positive constant k, then we get exponential decay even point wise, since it's an ode. And as a consequence of that, you immediately get that. Also, all these lp Wasserstein distances decay exponentially for every p. Okay, so the proof is easy.
00:53:30.624 - 00:54:14.460, Speaker A: So, the first part, I think everybody here knows how to do that. So I leave it to you. And so, for the second part, you just have to note that this distance here is defined as an infimum over all couplings of mu and mu. So we want to bound this side here by an infimum over all couplings. So that means we take an arbitrary coupling of mu and mu. So take an arbitrary coupling x zero y zero. And then you evolve your Markov process, your coupled Markov process, and then the process at time t x t yt has marginal loss mu pt and mu pt.
00:54:14.460 - 00:54:51.516, Speaker A: So that means this defines a coupling of nuptial and mu pt. And that means you can bound the lp Wasserstein distance of nuptial by the LP norm of xt minus yt. And then you just apply this bound, you just insert this bound here, and then you get your exponential decay. And now you just minimize overall couplings of mu and mu. And then on the right hand side you get this Wasselstein distance. Okay, so that's very easy. And there are nice features, so it's easy.
00:54:51.516 - 00:55:28.418, Speaker A: Moreover, the result is somehow sharp. So if this supremum here is greater or equal to zero, then you can also show that you do not have strict Wasserstein contractivity for any p between one and infinity. So in that sense the result is sharp. What's also nice, the dimension does not enter at all, so it's completely dimension free. And what's nice is you have a contraction in some distance. And if you have that, it's quite robust. So that's very nice.
00:55:28.418 - 00:55:58.884, Speaker A: But of course there are also disadvantages. First, disadvantage, it only applies in the strongly context case. That's because it's sharp. Secondly, the noise has been kicked out. So when we look at the coupling difference, then the noise disappears. So it's not used at all in our argument. But now people would say that the noise can be beneficial for the mixing properties.
00:55:58.884 - 00:56:31.744, Speaker A: So somehow you should make use of the noise. And here you just throw it away. Okay, so that's a disadvantage. And then another disadvantage is that you cannot get total variation bounds with this method. This is because the two copies just come closer and closer together exponentially. But they never reach the same point if you do synchronous coupling, so they never meet each other. And because of that, it doesn't give you total variation bounds.
00:56:31.744 - 00:57:16.180, Speaker A: Okay, so you see, it's nice, but it has some disadvantage. And so therefore you might ask if we can do something else. And yeah, maybe I just introduce what you can do before the break. And then we go into the break. So there's another coupling which has been introduced by Lindwald and Rogers, and that's called reflection coupling. And in that case you do not use the same noise for both copies, but you do the following. So suppose your first copy is at position xt and your second copy is here at position yt, so at these two positions.
00:57:16.180 - 00:58:03.298, Speaker A: But now you have a drift, which brings them apart. So you no longer have strong convexity, so they move apart by the deterministic dynamics. Can you nevertheless have some good mixing properties? And of course you can, but you have to use the noise. Okay, and how can you do that? What you do is you define the noise increments by reflection. So you put a hyperplane between the two current positions, xt and yt. That's this hyperplane here. And then if you have here this noise increment DBT, this is of course only formally now, because this is an infinitesimal noise increment.
00:58:03.298 - 00:58:53.600, Speaker A: But formally, if you have this infinitesimal noise increment DBT here, then you reflect it on that hyperplane and take the reflected increment for the other process. So why do you want to do that? Well, later you want to look at, you get included in your equations the difference of these two increments. Now, if the difference shows in the direction connecting the two, then that might help you. Then sometimes you will move closer together, and sometimes you will move further apart. So on average you will not gain. But sometimes you will come closer together, and that might help you. On the other hand, moving in the orthogonal directions will not help you.
00:58:53.600 - 00:59:30.824, Speaker A: So if this moves somehow in the orthogonal direction and this moves differently in the orthogonal direction, this will only increase your distance. So that will never help you. So therefore, in the orthogonal directions, you don't want noise, but in the direction connecting the two, you want noise. And this you can do. You can define your second brownian motion by this equation here, which is just translating this picture into mass. So this is an e two differential equation. So your increment of b, twiddle is your increment of b, and then you reflect it at the hyperplane.
00:59:30.824 - 01:00:15.704, Speaker A: And this is just the reflection at the hyperplane. So et is the unit vector connecting the two, and you do that reflection as long as they are not at the same position. Otherwise you choose the same noise. That's called reflection coupling, and you can show that this really defines, again, brownian motion B. Twiddle. And so you can really define this coupling process and that will give you some improved conditions. Okay, so then I would stop here for the first part and then, yeah, maybe you have questions and otherwise be back in after the break.
01:00:15.704 - 01:01:00.744, Speaker A: Questions for this method. How do you choose the hyperplane? Is it in the middle between the two points or is it arbitrary? Hyperplane. The audio is not so good. I understood something with hyperplane and. Sorry. Yeah, I was wondering, how do you choose a hyperplane? The hyperplane is just the hyperplane in the middle between xt and yt. So all points that have the same distance of xt and yt, time t plus one, the hyperplane change.
01:01:00.744 - 01:01:35.720, Speaker A: Sorry? The hyperplane changes with t. Was that your question? Yes, it changes with t. That's very important. So it's randomly moving. Questions, actually, yeah, go ahead. So, as you mentioned, this coupling, I mean, maybe we'll get back to this, but this coupling allows sometimes them to get closer and sometimes it makes them go away. Is there a way to find a coupling that always gets them closer? No.
01:01:35.720 - 01:02:08.544, Speaker A: You somehow do not have a free lunch. But nevertheless, we will see that this coupling is already quite good. All right. Somehow, if you change this brownian motion, you only change the Martingale part in your e two equation. And for the Martingale part, of course, if you take average, you do not gain. All right, no more questions. Join me in thanking Andreas again.
01:02:08.544 - 01:02:19.834, Speaker A: We have a break. I guess we start again at eleven, is that right? Yeah. All right.
