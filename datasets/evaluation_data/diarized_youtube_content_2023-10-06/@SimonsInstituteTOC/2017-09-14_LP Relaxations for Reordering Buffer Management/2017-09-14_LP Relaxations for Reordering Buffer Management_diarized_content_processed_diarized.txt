00:00:00.200 - 00:00:40.904, Speaker A: Next up will be Yuval Rabani on reordering buffers. Thank you. Okay, so this is the reordering buffer management problem. We have an input stream of balls of various colors, and we have a buffer that has some fixed capacity k. And in this example, k equals three. The balls, as they arrive, enter the buffer. And then of course, at some point, the buffer becomes full.
00:00:40.904 - 00:01:19.096, Speaker A: And in order to be able to process the next input item, you have to remove one of the balls that are currently in the buffer. And the order by which they enter the buffer does not matter. So you can choose any one of these three balls to remove. Now, for instance, the green ball. And then a new input item enters the buffer. And now you have to remove some other ball. And you keep doing this until the entire input sequence is exhausted.
00:01:19.096 - 00:01:56.694, Speaker A: And then you just have to clear the buffer. So the final steps are just clearing the buffer of all the remaining balls. So this is how you produce the output. So the output is some permutation of the input, but it's not an arbitrary permutation. It's a permutation that can be realized by a buffer of size k. Now, so that's the input and the output. What is the objective? The objective is to minimize the context switching cost of the output sequence.
00:01:56.694 - 00:02:44.264, Speaker A: And in this specific case that we will talk about, for most of the talk, the context switching cost is simply one whenever we change colors. So this particular solution costs five. Because there are five color blocks in the output, this isn't the optimal solution. The optimal solution in this case is four. And our mistake was actually the removing the green ball as our first move. If we would have delayed that, we would have had only four color batches. So here's a list of excuses why to study this problem.
00:02:44.264 - 00:03:45.310, Speaker A: They're not that interesting for us right now. For me, perhaps the last one is the most interesting. So I think this problem reveals some fascinating facets of online computing, and therefore it's an appealing model to study. And of course, you can produce slides with colorful and joyful photos. Okay, so in the next two slides, I'm going to summarize everything that's known about this problem right now. Hopefully first of all, in the offline setting, so we know the entire input sequence in advance, and we want to compute an optimal scale scheduling of removing stuff from the buffer. The problem is known to be np hard.
00:03:45.310 - 00:04:57.290, Speaker A: There are at least two proofs of that, and at least one of them is also correct, most likely. And there is a constant factor approximation, the initial constant by Vigdoll Gable and myself was, I think, something a little bit less than 135, and that was improved to so about half of that, 60 something. So that's the best constant that's currently known in the online setting. So there are no hardness results. I mean, it's not known to be APX hard or anything like that in the online setting. In fact, things are much better due to a lot of work by many people, including Anna, who is right here with us. So there is a deterministic upper bound of square root of log k competitive ratio, and that's nearly matched by a deterministic lower bound of square root of log k over log k.
00:04:57.290 - 00:05:25.844, Speaker A: So there's a small gap there. In the randomized case, the bounds are asymptotically tight. So other than the leading constant, we know everything. There is an order log log k randomized algorithm, and that's matched by previously known omega log log k lower bound on the competitive ratio. I'm sorry, k what k is. K is the size of the buffer. K is the size of the buffer.
00:05:25.844 - 00:06:04.974, Speaker A: Now, we talked about this very simple uniform context switching cost, where changing to a different color just costs one whenever it's done. In the output, there are more sophisticated context switching cost models. So perhaps the next simplest one is the case of non uniform costs. In non uniform costs, each color has a different cost. So if I switch to green, I'll pay something. If I switch to blue, I'll pay something else. And there, the picture is somewhat murkier.
00:06:04.974 - 00:06:45.886, Speaker A: So, almost all the results include a term that depends on the maximum ratio of the costs. And of course, the maximum ratio of the cost could be arbitrarily large. In this model. It need not give us anything that depends only on k. So here's what's known. There is an approximation algorithm, triple log of this gamma times k. There is a square root of log gamma k upper bound on the deterministic competitive ratio.
00:06:45.886 - 00:07:11.960, Speaker A: The only bound that does not depend on this gamma is this bound order log k over log log k. So that's the only thing that we know that doesn't depend on the weights themselves. And there's also a log log gamma k squared randomized algorithm that's known. Yeah.
00:07:11.992 - 00:07:17.304, Speaker B: Excuse me, another question in the star metric. So if you are on color green and you go to red.
00:07:17.424 - 00:07:17.728, Speaker A: Yeah.
00:07:17.776 - 00:07:19.144, Speaker B: Do you weigh the green?
00:07:19.184 - 00:07:51.784, Speaker A: Again, it doesn't really matter. Right. It's just a factor of two. So you could. So the star metric is not exactly identical to the non uniform costs model, because in the non uniform cost model, you're only paying the weight of the color you're switching to. But you can define a star where you know each of the colors has half the weight of their actual weight, and then you're paying for moving into the color and for moving out of the color. Each one, you pay half.
00:07:51.784 - 00:08:50.394, Speaker A: So that's the same up to an additive term. Now, there are more general cost functions, so you can induce a metric on the colors and then even less is known. I mean, there's been a lot of work on this, or some work on this and some progress. But you see the, I won't go over the results, I'm afraid I might not have enough time. The one thing to know, one important and very simple fact to know, is that if you do not change the input sequence at all, so you just output the input sequence, you don't actually need the buffer for that, right? You need a buffer of size one. You're not even using the buffer. Except for one position, the input sequence is no worse than two.
00:08:50.394 - 00:09:47.390, Speaker A: K minus one times the optimal solution. So there is a bound here on how bad you can do, and it's a bound in terms of k alone, whereas all of these results do not depend just on kick. We don't know if there's something better than linear in k, which depends only on k. Then there are other models. There's the block devices model, which is a slight variation of this. In fact, this result of adamace ketal, one of the ingredients of what I'll show you here, borrows heavily from that paper. It's a variation where when you remove a color, you only remove the stuff that's currently in the buffer and you can't append to it new items of the same color that are arriving while you're removing it.
00:09:47.390 - 00:10:13.374, Speaker A: So that's the difference. It makes a rather significant difference in how you need to solve the problem. But the work on that actually is useful also for the reordering buffer problem. C. C is the set of colors. Sorry, I didn't mention that. C is just a set of colors.
00:10:13.374 - 00:10:40.764, Speaker A: So it's k. No, it's not k. K is the size of the buffer. Okay, so I want to talk about linear programming relaxations, because we're supposed to be all about continuous optimization here. So here is a linear programming relaxation for the uniform reordering buffer problem. And if it's non uniform, you just have to plug in the weights here. So it just changes the objective function.
00:10:40.764 - 00:11:46.764, Speaker A: So what does the Lp represent? X, I, j in the integer version of this is an indicator variable that says that item I is removed at output slot j. So the output sequence, the input sequence starts at slot one and goes all the way to slot n, and the output sequence starts at slot k plus one. Because the first k steps are just the initial k items entering the buffer, and it ends at slot k plus n x I. J is the indicator, or in the Lp solution, it's the fraction of item I that is removed at at output slot j. And then we have two obvious sets of constraints. So every item has to be removed at some point. So this is the first set of constraints, and the second set of constraints is that at every time step we cannot remove more than one item.
00:11:46.764 - 00:12:49.044, Speaker A: So these are two obvious constraints. This is somewhat less obvious set of constraints. This set of constraints is necessary in order to be able to express the objective function in terms of these xijs and intuitively what the set of constraints does. We will soon get rid of it and have a different representation. But what this set of constraints says is that if you remove an item I at step j, where the next item of the set of the same color is already available, then you're also going to remove that next item. So this constraint doesn't allow you to cut sequences of the same color if they can be continued. And of course, an optimal solution would have that property.
00:12:49.044 - 00:13:14.272, Speaker A: There's no reason to change the context if you can continue with the same color. So this is a pictorial view of how the fractional solution looks like, or at least one way that you can express it. You can think of it as, I'm.
00:13:14.288 - 00:13:17.614, Speaker B: Sorry, was it of you said the objective was.
00:13:20.154 - 00:14:12.010, Speaker A: Yeah, but. Yes, but it will become even more obvious now. So that's why I'm not going into it. So the way to think about the fractional solution is as follows. Think of here's the timeline, and our schedule starts at time k plus one and ends at time k plus n. So these are, I mean, it looks continuous, but what I mean is a discrete timeline, and we have these, what we call batches. So a batch ij or a blue batch ij is just a sequence I of consecutive blue items, consecutive in the input sequence that are removed, starting from slot jack.
00:14:12.010 - 00:14:52.044, Speaker A: So j indicates the initial time slot where these are removed. And the LP just computes a fractional packing of these batches. So each batch here has a height. I mean, the total height is one, because in every time unit, one item is removed and each batch has a height corresponding to its weight in the packing. An integer solution is just a packing of integral batches instead of fractional batches. So this is how an integer solution would look like. And this leads us to the following equivalent formulation.
00:14:52.044 - 00:15:46.144, Speaker A: So here, instead of the xijs, we will have x capital ij variables, and x capital ij simply indicates that the weight at which the batch ij is packed in the packing. So now the objective function is clear, right, because we're just summing the total weight of all the batches that we're scheduling. And the constraints are also obvious. Every item has to be removed at least once. So the sum of weights of all the batches that contain a certain item has to be at least one. And in every time slot, if I look at all the batches that use that time slot, their total weight has to be at most one. And of course, because of the indexing, both of these in any feasible solution will be equations and not inequalities.
00:15:46.144 - 00:16:18.244, Speaker A: Yes. What is like a batch? A capital I? What is really capital? It's a sequence of consecutive items of the same color that are removed starting in slot j. So I indicates the sequence and j indicates the starting output slot for. So there's an I for every subsequence of items of the sequence. Yeah, but there's only a polynomial number of them. Right, because they're consecutive, so they have a starting point and an end point, and that determines the whole sequence. It's not a subset.
00:16:18.244 - 00:16:43.854, Speaker A: Does I range over? Yeah, I range is over starting and finish positions of the items in the batch. So this is the dual program. The dual program is also pretty simple. We have variables, y I for all the input positions and variables, zj for all the output positions. And I'm missing something.
00:16:43.974 - 00:16:52.230, Speaker B: There might be balls that are not consecutive of the same column, that are not consecutive in the input, that you might still be able to put consecutive in the output.
00:16:52.422 - 00:17:31.872, Speaker A: Yes, by consecutive, I mean, when you look at this particular color, there's no missing ball in that range. That's what I meant by consecutive. Not that they come one after the other. There could be other balls of other colors in the middle. Yeah, exactly. Yes. So the constraints here that if I look at any batch, I sum the y is over all the items there, and I sum the zone j's over all the positions where these items are removed by this particular batch.
00:17:31.872 - 00:17:58.384, Speaker A: This has to be bounded by one. Okay, so let's do some things with this LP. I mean, we have an LP, so it could be good, it could be bad. Here is one very simple thing we can do with it. Let's consider the following. So this is now the offline world. For now, let's consider the following rounding procedure.
00:17:58.384 - 00:18:44.204, Speaker A: So, I'm doing the rounding by going over the time index. So I'm looking at the first time index where nothing is being removed, and I have to fill that up. So I'll look at my buffer. If there exists in the buffer an item that has LP weight at most one, two. So that means that the LP removed this item with weight at least one, two, I'm going to evict this entire color from the buffer. And of course, everything that can be appended to that while we're removing it. And otherwise we just keep accumulating items in the buffer.
00:18:44.204 - 00:19:53.328, Speaker A: So now item is an interval or it's one item? No, it's one item, but the item determines an entire sequence of items that will be removed. If there is one specific item that has weight less than a half, I will remove the entire color. Okay, now, of course, the cost of my solution increases by a factor of at most. I mean, this requires some proof, but you can show that. And on the other hand, what's the size of the buffer that I need to store all the items, because sometimes I have no item that has weight less than a half in the LP. So the point is that at every point in time, every item in the buff in my buffer has weight at least half in the LP. And since the LP constrains indirectly the capacity of the buffer to be k, I can accommodate all these items with a buffer of size two k.
00:19:53.328 - 00:19:53.848, Speaker A: Yes.
00:19:53.936 - 00:19:57.696, Speaker B: Can you show again the LP? How does it constrain it to be at most k?
00:19:57.760 - 00:20:54.394, Speaker A: Ah, I knew you would ask that. It constrains it indirectly by setting the frame first time slot where an item is removed to be k plus one. That's where the buffer capacity is determined, because I have to remove all of these items, and I have exactly n positions to remove them. So I must remove one item in every position, and that maintains the capacity of the buffer. So I can accommodate all of this with a buffer of size two. So, in fact, if I really want to solve the k buffer, I need to solve the LP for a buffer of size k over two and then do this rounding. Now, what do I pay for that? Well, if I'm solving the LP for a smaller buffer, I might get a cost which is much higher than the optimal cost for a buffer of size k.
00:20:54.394 - 00:21:44.364, Speaker A: But there is a result of Englert and Westermann that shows that if it's not for a factor of two, it's for a factor of four. It shows that the optimal solution for a buffer of size k is its most order of log k times. The optimal solution for a buffer that is four times as large. So that's also true for a buffer that's at least, that's twice as large, right? That's a smaller buffer. So I'm not, and this log k is tight for, for the factor four. We don't really know the truth for factor two. So in fact, this thing gives you log k approximation algorithm and also a log k integrality gap for the LP.
00:21:44.364 - 00:22:16.684, Speaker A: Now there are much better results. The integrality gap. And there's a constant approximation algorithm and integrality gap. We don't know that for non uniform costs, but for uniform costs, that's known. I don't think I'll have time to show that, but let's see. So let's talk a little bit about online algorithms. So here's a very simple online algorithm.
00:22:16.684 - 00:22:56.622, Speaker A: I hope I'll be able to show a little bit more than this. So think of it as follows. Every item has a penalty counter. And I look at all the items in my buffer. So the items just accumulate these penalties until they are evicted from the buffer. So I take all the items in my buffer and I raise their penalties uniformly and continuously. Of course, you need to discretize that in order to get an actual algorithm.
00:22:56.622 - 00:23:53.358, Speaker A: Now, as soon as I have a color in my buffer that has total penalty equal to one, I evict that color from the buffer. So that includes all the items that accumulated this penalty of one and possibly additional items that will enter the buffer as I'm removing those items. So this very simple algorithm can be shown to be log k over log log k, competitive even for non uniform cost. So for non uniform costs, the penalty has to reach the cost of the color, not one. And the proof, in fact uses the LP. It uses the LP. Of course, the algorithm doesn't use the LP, but the proof uses the LP by constructing a dual solution that bounds the cost of the algorithm.
00:23:53.358 - 00:24:27.808, Speaker A: And the dual solution is very simple. The proof that this is a feasible solution is the part that's non trivial in this result. So here is the dual solution. Zj. The Zj's, remember these corresponding to output slots, they just grow monotonically. And Zj indicates the total penalty accumulated per item up to slot j. So this is sort of a global counter of this continuous process that we're doing.
00:24:27.808 - 00:25:22.668, Speaker A: And y I is set when I is removed. It's set in a way that y I minus z of the slot where I is removed is equal to the total penalty that I accumulated divided by the approximation factor. And this is a feasible. So the hard part here is to prove that this is a feasible solution. Once you have this as a feasible solution, you know that then you get the approximation guarantee. Another thing that you can show, and this is somewhat important, is the following thing. This algorithm, again by dual fitting, is not if all the color blocks are large, the algorithm performs very well.
00:25:22.668 - 00:26:35.784, Speaker A: So if all the removed color blocks are large, if they all have size, at least s, the algorithm will be order log k over s competitive. And this might not necessarily be tight for all s. But s is a parameter of the output, right? S is a parameter of the output. So if you happen to be lucky, and this is what happened, then you know that your algorithm performed well. Okay, how much time do I have? Okay, let me do this very quickly so I won't go over any of these details. So, ever since Nev and Cephe did a lot of work on online algorithms, the obvious tool to use is the primal dual schema. And in fact, Anna and her co authors did just that for the block devices problem.
00:26:35.784 - 00:27:19.926, Speaker A: And here we're borrowing rather heavily from that. So I just wanted to point out there are two major problems applying the primal dual schema to this formulation. The first problem is that the primal has both covering and packing constraints. So normally the primal dual schema applies when you have a covering problem or when you have a pure packing problem. And here they're mixed. And what happens when they're mixed is that the dual has negative. And that's an annoying thing for the primal dual scammer, because it causes problems.
00:27:19.926 - 00:28:14.454, Speaker A: So it turns out that you can do things. The other problem is that when you raise a primal variable, it doesn't determine a decision just for the current time slot, but it determines decisions for future time slots because you're scheduling a batch, you're not scheduling an individual item. And that also causes various problems. I don't have time to go over this, but let me just explain one little thing here. So the solution to all of our problems is the following. So first of all, we're going to raise some of the Yi and some of the ZJ's variables simultaneously. So we're raising both the y is and the Zj's.
00:28:14.454 - 00:28:52.354, Speaker A: We're raising all the yis that are still not fully scheduled. So that includes all the ones that are in the buffer and all the future ones, and we're raising all the ZJ's starting from the current position. Now the dual increase rate is going to be the number of items that are still present in the buffer minus the capacity of the buffer. And this is a problem because the number of items in the buffer could be exactly the capacity of the buffer. Right. So that could be zero. And that's bad because we're not gaining anything in the duo.
00:28:52.354 - 00:29:43.986, Speaker A: The solution to this is a resource augmentation algorithm, sorry, is a resource augmentation argument where you actually compete against a dual, that is, using a slightly smaller buffer. So you're using a buffer of size k minus k over log k instead of k. And this gives you some slack here. And this slack is good enough if the batches that you're removing are small are at most k over log k. Okay, so we have an algorithm that's good if all the batches are big. If all the batches are at least k over log a, we get log k. If all the batches are below k over log k, we this argument leads also to a log log k.
00:29:43.986 - 00:30:31.824, Speaker A: So we need to combine these two things. And that's actually the biggest difficulty here, how to combine these two arguments into one. And I won't explain that. Just what is the essence of the problem here? When you start accumulating a color in the buffer, you have no idea if it's going to be big or small, when you will remove it. So you need some mechanism that will decide if it's going to be big or small. And of course, you can't just guess, because then your guess might be wrong and the whole algorithm won't work. So there is some complicated mechanism that needs to be used, freezing some of the items and waiting to see if they accumulate and then releasing them.
00:30:31.824 - 00:31:25.384, Speaker A: This is a big mess. So I won't go into this. Just to conclude, here are some open problems. The uniform case is almost completely resolved, except for this deterministic gap, small gap that seems pretty hard to close. But one remaining question is the approximability of the problem. So, can we find an approximation algorithm that has a small constant, not something like 60 something, and a ptas is not excluded by anything that we know. So this problem might as well have a ptas then in the non uniform cost case.
00:31:25.384 - 00:32:24.184, Speaker A: My belief is that these should be the same bounds as in the uniform case. But right now this seems pretty hard. The problem there is that all the strong arguments charge certain colors on other colors, and if they don't have the same weights, this charging is a difficult thing. It involves packing you know, charging many low weight colors on one big weight color, and that's hard to do. So that's the difficulty there. And then, in general metrics, of course, any little o of k guarantee is interesting, both online and offline. So we don't know any approximation algorithm better than the online algorithms for general metrics, any lp relaxation for the problem.
00:32:24.184 - 00:32:48.180, Speaker A: It seems like a challenging thing to come up with, or SDP relaxation or whatever interesting tool can be developed. And there are no approximation algorithms that are not derived from the online algorithms. So that's it. One quick question.
00:32:48.332 - 00:33:04.164, Speaker B: So you had this one point that you didn't talk about. Limited extra memory. Wouldn't contradict anything we know so far. To say that a constant factor, extra buffers and a constant performance competitor.
00:33:05.464 - 00:33:09.964, Speaker A: Yes, there is the slower bound. Right. I actually mentioned it.
00:33:13.704 - 00:33:18.204, Speaker B: Actually, you can achieve. So it's not clear that by doing the concept.
00:33:21.084 - 00:33:35.904, Speaker A: Smaller buffer is k over four, you have to pay a logarithmic factor for that. So there are sequences where the smaller buffer will cost log k times the bigger buffer.
00:33:36.644 - 00:33:46.126, Speaker B: I'm saying I want to measure performance as a constant factor times the optimal, but not the true optimal, the constant factor. For a constant factor, smaller buffer.
00:33:46.220 - 00:34:15.954, Speaker A: Oh, no, nothing. In fact, there might even be such results. I didn't mention the results that, that actually compare buffers of different sizes. Yeah, so there's nothing that would. In fact, that's how one of the earlier results works. It gives a constant factor competitive ratio against a buffer that is one quarter the buffer that you want. Okay, I didn't understand the question.
00:34:15.954 - 00:34:37.094, Speaker A: Final speaker of the day will be EOm Huzet Simons.
