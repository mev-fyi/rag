00:00:00.800 - 00:00:41.786, Speaker A: Okay, thank you. So I think I can use the opportunity of being almost the last speaker to say that I, and I'm sure many of us are enjoying this workshop really, really a lot. So thank you organizers. It's great to meet everybody together and hear about all that progress. So I will be so in the spirit, as I said yesterday, in the spirit of the conjectures need to keep flowing. That's the spirit of this talk. So it's about the work that I have done with Florent and Mark and two members of the japanese school that use replicas and cavity methods and provide cool conjectures to be proven.
00:00:41.786 - 00:01:22.672, Speaker A: Ayoshiuki Kabashima and his postdoc Ayaka Sakata. So about the work that we did a couple years back. And at first we somehow motivated it one way. But in this talk, I will show you somehow since we were thinking about a couple of problems actually found maybe like a more interesting way to motivate that problem. So that's how I start. Then I will talk about the problem, how we solved it, and some results. So what is feature learning? So this is just like one slide that one can see today in number of blogs and Facebook and all over the place.
00:01:22.672 - 00:02:03.242, Speaker A: Even if you don't follow what's happening in machine learning, you can't escape it. So what does it mean, feature learning? So I presented some data. In this case, this would be some database of faces. And then what machine learning tries to do is it tries to find the hierarchy of features. So it first realizes that it's useful to think of the faces as linear combination of some kind of edges or wavelengths like in this picture. And then in the second level, if you make linear combinations of those, you get little like eye or nose, etcetera, feature like that. And then you get some proto faces that then serve to do some classification, face recognition.
00:02:03.242 - 00:02:40.182, Speaker A: So this is like a very hand wavy thing. Now how is this done mathematically? So the prototypical model, how is this done mathematically would be this multilayer feed forward perceptron. So that goes back to works more than 50 years ago. So what are the things here? So your data would be this matrix y. So you have p samples of, so you have p images of, each of them has n pixels. So that's what you may think about if you want. And then for each of the images, you have some labels.
00:02:40.182 - 00:03:16.844, Speaker A: You say it's person a or person b, or it's a woman or it's a man. Say that you want to recognize women from men in the figures. So what you do, you present your pictures, the bits, the variables, to the first layer of the network. And then these arrows mirrored means that you multiply them by some matrix f one, which is representing the first layer of the features. So this, so these arrows that are coming, for instance, to this point, that would be one of these like wavelets, one of these like edges, little somewhere. This would be another one. This would be another one.
00:03:16.844 - 00:03:56.236, Speaker A: And then this matrix f two corresponds to the second level of the feature. So this would be, say, like I, and this would be the nose. And then this third layer of the neural network would correspond to. Is this a man or is it a woman, very roughly. And what people got very good at in the past, say, ten years, is that we have really good algorithms to actually given the data and given the labels. So that's the things that, you know, we learn the hierarchy of the features, f one, f two, f three, et cetera, such that you know the corresponding function. So, so we do y times f one.
00:03:56.236 - 00:04:36.048, Speaker A: And then there is some activation function, like what this variable here is, is some function, usually some sigmoid or some like, other function. So this would be these functions, g one, two, three, etcetera. And so that's what people are very good at. Now, this is about supervised learning when you have some labels. So now, in many situations, you either don't have many labeled data or the labels are bad, or like you say, you know, the ultimate goal of machine learning is to do unsupervised learning. So no labels, just data, and find the features just from the data. So there is a little twist on this multi layer perceptron, which is called autoencoder.
00:04:36.048 - 00:05:04.660, Speaker A: And again, it's a set of ideas that goes long time back. So it's very related, except you don't have the labels. You still have your data, the same thing. So the beginning is the same. You try to decompose your data into features, but then you don't know, you know what you, you don't have any labels. So what you do, you somehow copy, you make a mirror image. Like these functions here would be a little different, but otherwise it's basically a mirror image of your network.
00:05:04.660 - 00:05:35.158, Speaker A: And at the end you also put the data. So what you're trying to do, you are trying to learn features and somehow compress your image in such a way that if you somehow do something related on the other side, that you reconstruct your image back. So you can also think about this as a kind of a compression. Right? The middle layer here, that would be like a compressed version of your features, of your data. It's able, just by using the features, to reconstruct your data back. So that's the auto encoder. So again, we learn.
00:05:35.158 - 00:06:03.242, Speaker A: So here I put it in a way that the f one is exactly the same as the f one here. So there's the tight ways this can be relaxed, and you can do also different. Okay, there are many, many variants of this auto encode. So, fine, that's still interesting. So that's interesting. So we would like to analyze something like that. But when we were thinking about it, we somehow saw, you know, we ended up realizing that basically having like the copy here and copy here is okay, that's what it's done.
00:06:03.242 - 00:06:54.774, Speaker A: Because for this architecture, we have the algorithms. But if we, like, want to think about it, theoretically, maybe we can just forget about the right hand side and just think about the left hand side and just think about the right hand side. And, okay, maybe it has some other name in literature, but we are thinking along these lines with Ricardo and Florent and Mark and, and Callo, and we call this for ourselves, the invertron. So what is this? So this is again the data. And you try to decompose the data into features and some matrix f, which is like the compressed version of your data, such that this is equal to the data. And now we would like to get some algorithms and understand when is this possible, how little this layer can be, et cetera. But in order to build a theory for something like that.
00:06:54.774 - 00:06:56.074, Speaker A: Yes, a question.
00:06:56.494 - 00:07:05.598, Speaker B: So you're saying you want a generative model with a small number of input variables, which produces a distribution of outputs, which looks like your data set.
00:07:05.726 - 00:07:44.794, Speaker A: Yeah, which is exactly my data set. It's as close to my data set as possible. So how to think about a theory? So, ideally, we would like the data to be some real data, like these faces, database of images, or something like that. That. So with the kind of techniques that we like have, and we were hearing about in this talk, I mean, if we want that, then we cannot do much. I mean, okay, what do I do? I mean, I can maybe do some algorithms, but if I want some theory, I cannot do much. So what is some case where I could do something? So say that y the data were random IId.
00:07:44.794 - 00:08:28.298, Speaker A: So some matrix with random IID elements. So, okay, that's a good case for like, our kind of methodology, like the replica method, cavity method, and indeed for yet simpler version of this, this case. If I knew these matrices f one and f two, then this is basically one. This is basically perceptron. And there has been a long line of work in the eighties and nineties with replica and cavity methods solving, saying things about neural networks, random data, but random data do not have features. So if you want to learn something about feature learning, and we use random data, then, okay, we are somehow doomed. What can be done if we don't have.
00:08:28.298 - 00:09:30.432, Speaker A: So this, we are not happy either. So what about creating data, but by planting in some features? So in this talk, we heard a lot about, like, inference problems, planted problems. So what, what about doing that? What about like, starting with some x star, f one and f two star, that creates the data, and then we forget the star matrices and we want to reconstruct them back. So that would be the planted invertron. The data are still random, and now elements of this would be iig. So we are still in the framework where these kind of methods can be applied, but we are also in a framework where we plant it in random features, but at least we can meaningfully ask the questions, can we find the features back? And we at least know what the features are? So that's somehow the strategy. Okay, and now I'm getting to the form of this problem that I will actually talk about in this talk.
00:09:30.432 - 00:10:13.724, Speaker A: And that's the absolutely simplest, non trivial case where I only have one layer here. So I was, the previous two slides ago, I had two layers, so here I really have only one layer. So what am I doing? These are the data, and I'm trying to find f and x such that the product of Fnx and the activation function on it. This can be some, you know, noisy, some arbitrary element wise function reproduces well the data. So, and set like that. This is actually a, you know, problem that is known out of neural network setting. That's sometimes, is often it's called dictionary learning, often it's called sparse coding.
00:10:13.724 - 00:10:53.736, Speaker A: And setting up this problem and thinking of some algorithms to solve it goes back to works again 20 years ago, or matrix factorization. I like to call it matrix factorization because that's what it really is. Okay, so now, okay, matrix factorization. So that was the motivation. So if you, like, think, okay, this is irrelevant, or I didn't understand, okay, start, let's start again, because the talk would have started here. Okay, the previous part was just like auxiliary. So what am I? So, okay, I motivated this problem as a smallest, nonchival piece of feature learning, but you can forget about that.
00:10:53.736 - 00:11:48.266, Speaker A: And just. So what we are doing here is that we want to represent p samples of n dimensional data that we know by some features, f, that we don't know, and some weights, x that we don't know through some activation function, f, which, you know, in the examples I will be showing about, this will just be additive gaussian noise to the product of f times x. But you can think of sigmoid, you can think of some, like, whatever function that is, like, not completely crazy. And so the way to motivate, you know, maybe, like, the way I like to explain was dictional learning is like, think of. Think of that you are. That you have some sequences of sound. So how should you represent sequences of sound? Okay, so in acoustics, people worked for 20 years and, you know, noticed among others and worked hard that, like representing it.
00:11:48.266 - 00:12:35.024, Speaker A: In Fourier transform, if you just Fourier transform sound, you get some representation that is sparse, right? And Fourier sound appears like just few peaks. And this is very useful because this is, like, very compressed and sparse, and you can do many things on it. Okay? In images, in image processing, people did the same thing and worked out wavelets. Right? Now, if I give you, like, a generic data set, what is the representation in which this data set can be written? As sparse or as much like much lower dimension? We don't know. And that's what this problem is about, right? So, ideally, without knowing anything, if y was sound, this dictionary should learn that good f is a Fourier transform and x is sparse. So that's somehow the idea. But here we are, you're not thinking about some real data.
00:12:35.024 - 00:13:05.496, Speaker A: We will be working in this planted case where everything is just like random bits. And this problem is related to two talks that we had earlier this week, the one by Florin Ksekela, who talked about lowering matrix factorization. So this would be also matrix factorization. But the r that I will be treating here is. Will be big. Whereas in his talk, it was constant. And it's also related to the talk by David Steuer, who talked about tensor factorizations.
00:13:05.496 - 00:13:42.244, Speaker A: So if I had had X here and x transpose, and if I added a third one here, that would be exactly the case about which David works. So this is just the matrix version of that. Okay? So diction learning is a problem that, since 20 years, is studied. So here I just, like, copy pasted the corresponding references from our article. I'm not saying this is exhaustive. There are really many works. So both on the algorithmic size, there are really many algorithms that somehow do reasonably well and with which you can, in many applications, achieve something really interesting.
00:13:42.244 - 00:14:18.864, Speaker A: The thing is, that they somehow all work. The number of samples that all these algorithms need is very, very big. So this is common to the neural networks, neural networks today. To learn something, they need many, many, many of these labeled samples. So one of the first questions we asked, like, what's somehow the minimal number of samples you need such that the information is still there and such that it is algorithmically tractable. So that's definitely like a, theoretically, maybe the first question that one asks. And then there is also theory where people study the sample complexity, etcetera.
00:14:18.864 - 00:15:02.684, Speaker A: So this would be more like classical statistical papers, many of them, they would assume some incoherence of rows of the atoms in the dictionary and sparsity of x, and they would be proving things under quite generic conditions. But they mostly work when the sparsity of s is very big. Like there are only few elements in the x that are non zero. So maybe for number of applications, when the sparsity is not so small, maybe the relevant regime, so not much is. So all that is known for that regime is not very good. Like you need n log n times, some big constant of samples. Like the bounds that are existing are not very good, and something like the Mmses in some biases settings are not normal.
00:15:02.684 - 00:15:56.740, Speaker A: So these are the things that we can do. So once again, the setting, we have this matrix factorization, and the teacher creates this x, f star and generates the data. And then the student knows the data, knows the distributions px and pf, but does note the x star and f star and tries to reconstruct it. And. All right, so how to solve this? So if you have a student that followed, well, some, like, biasian statistics undergraduate lecture, then the student will know that the optimal way of solving this is to build the posterior for this problem. So, posterior. So what is the probability of x and f given y? Well, you need to put your prior on f in, put your prior on x here, and put the likelihood of generating your data if f and x was what you assume it was.
00:15:56.740 - 00:16:48.834, Speaker A: Okay, and the optimal way then to estimate x and f that will be in the mean square error closest to the true one, is to take marginal probabilities of this posterior and take the means of these marginal posteriors. That's the bias optimal estimate. This is like page 30 of some bias adherence book. So we know what to do. In principle, what's the best way? Well, of course, the trouble is that this is not tractable. So let's see what can be done if we need to stick to a tractable algorithm, but also what is the performance of this bias optimal thing if we actually could do it? So, these are the two basic questions that we have here. So, to compute the MMSE, which would be the mean square error of the bias optimal estimator, and then the MSE, which is the best algorithmically achievable one.
00:16:48.834 - 00:17:47.410, Speaker A: Okay? So it cannot be done in general, but in this setting, where we generated the data with some, like planted random fnx, we can solve this problem with replica. We can compute this performance, both the best bias optimal performance and the best algorithmic performance with the, via the replica method. So how is that working? Well, that's basically computing the z of this, the normalization of this posterior in some limits. So the limit is kind of. So that's somehow the interesting thing here, because it also distinguishes from the existing works, which may be worked with different limits. So the limit we are working. So, the dimensions of these matrices, which are like the f is p times n, and the x is p times r times n, and the f is p times r, they are all of the same order, they all go to infinity.
00:17:47.410 - 00:18:29.024, Speaker A: But if I divide one by another, it's a constant. Okay? So I have these two constants, the alpha and p, and then the elements of these matrices. So we choose it in a way that y is order, that elements of y and x are of order one. But since I multiply all the n terms of x with all the n terms of f, then f needs to scale differently. And that's how we choose it. And we have one more condition is for the, for this, for the computations to go through, we need the mean of the f to be zero. Okay, so that's, so that's somehow a restriction on whether the p of f needs to be.
00:18:29.024 - 00:19:17.664, Speaker A: So all I'm saying will be working only if the distribution of f has zero mean. And then how does the replica method work? So that's the usual thing. I mean, you know, find it fraction of people that can do that casual, that calculation casually sitting on the room, or as a co author of this paper. So it's a standard thing, although it's some work. So what we do is that, okay, we want to average, we want to get a typical value of this normalization, because then from this, by derivating it with respect to various parameters, we can compute like things like the margins and the mean square error and things like that. So we know that this little identity is true. And we also noticed that if n was integer, then computing this average is not that hard.
00:19:17.664 - 00:19:56.872, Speaker A: And then we pretend that the same expression holds also when n is not integer and send n to zero and get some expression. So after we do that, we end up with a form that looks like this, like the, what we want will be integral over three parameters. So these are like real valued numbers between zero and one of some exponential where there is n squared. So something very big times some function. So we'll see, this integral will be dominated by the place where this function is maximum. Okay, and so now how this function looks like. So that's the expression for this function in this problem.
00:19:56.872 - 00:20:33.678, Speaker A: Okay, so, okay, maybe it's not so easy to parse, but if I, you know, it's actually not. So it's, it's kind of simple because the first line of this expression depends only on this output probability. It's some integral of some stuff where the parameters appear. The other variables that you see there, that's basically only the integration variables. There is this alpha and p, which were the ratios, the dimensions of our problem. Yeah. The second line is only related to the distribution of the, the dictionary f, and the third line is only related to the distribution of these weights.
00:20:33.678 - 00:21:17.438, Speaker A: So somehow the three, the three things that determine our problem, they are all decoupled here. And moreover, when you think like one more minute about it, you have all this horrible integral here. It's actually not so horrible. That's like if you had a scalar denoising problem and you wanted to compute the mutual information of a scalar denoising problem. So that's something like easy to do, not high dimension, just color. That's the mutual information that you would get with somewhere where the parameters like m hat would somehow be related to the noise in that denoising problem. So the form this equation takes is not like it's quite natural.
00:21:17.438 - 00:22:02.098, Speaker A: I mean, once you somehow get familiar to it. And, okay, so I said that we need to find maximum of that, and then the global maximum of that is then directly related to the best error. This bias optimal procedure would do an estimation of the, of the matrix of the f and x. All right, and so then, okay, it's the global maximum. So then what I would like to write is the stationary equation, stationary conditions of, for that energy. And, okay, that's another, like correspondingly, you know, not easy to parse set of equations. But again, I mean, these are set of three equations that are somehow decoupled.
00:22:02.098 - 00:22:42.104, Speaker A: And none of this, none of it here depends on the distribution of the x and f and the p out. So this is very generic. And the only thing that actually depends on what precisely was your problem are these particular functions here. So, all you need is basically these integrals of gaussians times your priors, times your output. Somehow the function that determines the channel output. So, okay, so this is like, at the end, it's quite an elegant set of equations that, like, solve all these problems at once, like, for all priors, for all outputs. Like, we can tell what this problem, what's happening in this talk.
00:22:42.104 - 00:23:08.810, Speaker A: Okay, so that was the replica solution. Now, if you, when we ask about algorithms. So this is, you know, this is a problem where somehow every variable talks to every variable. So, Andrea taught us on Monday that in cases like that, the good thing to do is this approximate message passing. So that's what we will do. So that's kind of. So let's wrap, let's take the problem that we had, but now we represent it as a factor graph.
00:23:08.810 - 00:23:41.654, Speaker A: We really take this posterior distribution and write the corresponding factor graph. So this, like it picture that one draws when one explains what's a neural network. It's not the factor graph. The factor graph is a little different. The Y's are not variables that are the checks, and the f and x are both variables. And these connections are like, where, you know, where in the posterior, these variables appear together with the checks, and then the priors are represented here. Okay, so that's a factor graphic.
00:23:41.654 - 00:24:19.684, Speaker A: And then, as Guillerm said yesterday, this is the point where I can switch off the brain, and I have a factor graph. I have a posterior belief propagation. It's a complicated factor graph, so it's complicated belief propagation, but straightforward. Except that this one here, I'm making integrals over all the n variables. So this would take time exponentially in n. So, writing belief propagation here, sure, I can do it, but it's not so useful. But at the same time, I realize that by assumptions of the belief propagation, the incoming messages are independent, and there are many.
00:24:19.684 - 00:24:48.584, Speaker A: And so it smells like central limit theorem. Okay, if I have sums of, or here, it's products. But if I take logarithm, that it's sums of many independent terms, so I will be able to treat only its variant, only to keep only the variances and the means. And that's exactly what happens. And if we do that, that's what leads to this approximate message passing. So, this is something that we heard a bit already in the previous talks. Yes.
00:24:48.584 - 00:25:18.188, Speaker A: Well, because they all come from Y through y, right? I mean, the, I should have posterior here. I mean, all, all the constraints I have are related to some y. And this y is taking some x and is taking some f's. But that's it. I mean, it's not like the Y's are the interactions between f and x. Y is no given y.
00:25:18.316 - 00:25:23.324, Speaker C: In that photograph, I can infer f without knowing anything about x.
00:25:23.364 - 00:25:30.896, Speaker A: But y is not the variable. Y is a constraint. It's what you meant, it's what I measure, but it's a constraint.
00:25:31.040 - 00:25:32.884, Speaker C: The constraint involves. Okay.
00:25:35.584 - 00:27:03.212, Speaker A: Okay, so where are we here? So, from physics point of view, this is very closely related to the Thales Anderse Palmar equations that we are very familiar with in the spin glass literature. And for problems that are formally more related to the prism problem, like the codivision multiplexes problem, or the perceptron, Yoshiuki Kabashima and collaborators basically wrote these equations like twelve years ago. There are several papers in which they didn't quite get them right, but in one of them, they got them right. But, okay, this paper was kind of forgotten, or not much attention was put to it. So, when really a lot of people started to work on this approximate message passing, that was after the work of, of Andrea and Maliki and Donoho, when they generalized this approach to non biasian cases, to continuous variables, and also had a beautiful line of works where they basically set the firm, rigorous foundations to this equation, to this algorithm and its state evolution, and what it all means. So, after this set of works, many people, including us, started to work on approximate message passes. But note that in the present problem, this approximate message passing is the same spirit of an algorithm, but it doesn't quite fit in the setting for which things are already proven, like the state evolution, etcetera.
00:27:03.212 - 00:27:48.280, Speaker A: So it's an approximate message passing, but not much is known about this one rigorously, and we somehow concentrate on the phase diagrams and the theory. But there is this beautiful work by Phil Schnitter, Parker and sever, where they concentrate on the algorithm and have some demonstrations on applications, some like video processing and quite some. So if you're interested in that, that's the paper to go to. See, this is how the algorithm looks like. So that's another before last slide with like horrible set of equations, difficult to parse, but it's somehow the same. You know, it's very similar to this simpler form that Andrea was presenting, which was just one equation. One has the estimators, this x head and f head.
00:27:48.280 - 00:28:41.358, Speaker A: That's of course the estimators of the elements of the matrices that we are looking for. And they are given by some functions of some variances, which basically, I don't know, I wrote them down explicitly, but in the large n limit they converge to some constant. So usually if I wanted to simplify, this would just be some like, I wouldn't write them and there would just be some t dependence of these f functions. And everything is somehow copied twice because I have the estimators for the x and the estimators for the f. So in a sense you can only look at half of these equations. So what is left there? The really important one would be an expression like that, this t, which is like the mean that is fed to this function that then estimates the x. And it has the part that is like the x here.
00:28:41.358 - 00:29:11.514, Speaker A: Then this gets divided by something. And then there are two terms that are the onsegger terms. Some of the structure of these equations is very close to the structure of the equations for compressed sensing, say, for instance. But there are, you know, I have terms where I multiply the two estimators against each other. So this is somehow what makes the difference. It's not like multiplied by a matrix, that I know it's one multiplied by the other. And that's what somehow makes it not straightforward to apply.
00:29:11.514 - 00:29:57.800, Speaker A: Some of the method has been done about it. What's nice about it, it's like some matrix multiplication, so you can code it easily. And everything that depends on the specificities of the problems are these functions that you put apart in your code that depend in the explicit way of your two priors and of the output distribution. So everything is modular and you can solve many problems at once just by writing these. So then we write the state evolution for these equations. So that's just to see what happens to the, when you run these equations. How does the error evolve, for instance? So for that we derive this, we define these order parameters and track their evolution.
00:29:57.800 - 00:30:50.856, Speaker A: So that's again some computation, and you end up with another horrible set of equation. But if you paid attention, that's just a repetition of the slide like six slides ago, that was the result of the replica method. So I don't need to explain them, just say the bottom line, right? The state evolution of amp always gives the exact same expression as the replica method. Like these two seems quite different, but they are interlinked. And since the eighties, like by the work of Mazar Parisi, we know that these are the, you know, they lead to the same predictions. So we have the, you know, they lead to the same function, which if we look at its global minimum, that's the optimal performance. And if you look at its local max, maximum, sorry, I said minimum, global maximum and local maximum with the worst error.
00:30:50.856 - 00:31:26.608, Speaker A: That's the performance of the, of the ImP algorithm. Okay, so if this function has a unique optimum maximum, then imp is information theoretically optimal. Well, if it doesn't, then it is suboptimal. So now, okay, I can show you some figures of what happens in some specific cases. So now I specify, first time, you know, I specify what these, like distributions are. So I am factorizing a matrix into a product of two matrices plus gaussian noise. That means that this p out is just this Gaussian.
00:31:26.608 - 00:32:00.944, Speaker A: And I'm looking at this f, just random I ied Gaussian, and this x, I'm looking sparse. So I'm trying to find a dictionary such that the data can be explained with this dictionary as something sparse. And the sparsity of this x is the rho. So rho is the number of non zeros. So the smaller the row, the somehow easier it should, you know, the more information there should be in this y about what these matrices are. And so this is just. Okay, in that case, these expresses I was writing simplify something that only has like one integral.
00:32:00.944 - 00:32:31.614, Speaker A: So numerically put it in MATlaB or Mathematica, whatever it's not, it's quite easy to plot this function and look at what the maxima are. And this is, for instance, the optimal error that you obtained. So what are the axes here? So this axis, that's the error. So, blue, it's like very good performance. Red is like basically doing as bad as randomly guessing from the prior. Then here is the noise. So of course, the bigger the noise, the worse it should be.
00:32:31.614 - 00:33:16.934, Speaker A: And this axis, finally, that's how many samples of the data I presented divided by some of these reference dimensions. So the more data I presented, the easier it should be to recover the dictionary. And the slopes are in the direction that we somehow expect. And we see that we have some nice phase transitions here, there are some very sharp jumps. So we are always happy when we see something like that, because we can study it. So here is the same picture, but looking from above and comparing to the mean square arrow that the algorithm gives you. So you see there is a region where it's exactly the same, but then there is a region here where it's not the same, or the algorithm is doing bad.
00:33:16.934 - 00:34:29.954, Speaker A: But if I had infinite computational power, I could do very well. So now let's just think of the case without noise. Where would this phase transition be? So at least I can have some very elementary lower bound of where the phase transition can be, and that's if I have no noise and the variables of x and f are continuous, then basically what needs to be true so that I can recover f and x from y is that at least the number of elements in y should be bigger than the number of elements I try to recover, right? Because if it's not, then like clearly I will have some degeneracy and I will not be able to recover it. So that's what gives me this simple hour bound. So in the parameters I introduce, alpha needs to be bigger, p needs to be bigger than alpha divided by alpha minus rho. And if I plot the position of this phase transition, I obtain that the information theoretic transition. So when I don't pay attention to the computation complexity, is exactly at that point.
00:34:29.954 - 00:35:30.284, Speaker A: So at least that's the prediction of the replica method. So this is saying that whenever, in the noiseless case, whenever I have more measurements than number of unknowns, then information, theoretically the dictionary is discoverable. So that's quite, very simple result, but it's very far away from the results. Admittedly, these existing results, they use much weaker conditions on what the dictionary could be. Here we really had it IID random, but you know, showing something like that, that for the dictionary IID random, this is actually the information theoretics transition. That's like one open question that maybe should not be so hard. And then so how does the algorithmic transition compare? No, it's not this, it's some constant times, this a quite small constant when we are at small density.
00:35:30.284 - 00:36:24.124, Speaker A: So it's worse, but not so much. And again, the performance of the other. So there is this like hard phase that we heard about this week, about several times. That's interesting from the theoretical point of view. But somehow these lines gives us the benchmark of how, at least in this like simple random case, how the algorithms should be able to do well, right? And in some other problems, like your first answer about the stochastic block model, we have like the algorithmic transition, and we have spectral algorithms, SDP algorithms, beauty propagation, Monte Carlo, that all get very close to it, edit or very close to it. This matrix factorization problem, we are not at all yet there. All the existing algorithms, they are somewhere far, far above this line, but we should try to get to it.
00:36:24.124 - 00:36:32.070, Speaker A: That should be opening work in that direction. Which algorithms in this random setting would get to this line?
00:36:32.152 - 00:36:42.130, Speaker C: That's the question, yes, in that hard phase. So the problem remains hard, no matter what the sample complexity is. So I give you infinite amount of data. The problem.
00:36:42.162 - 00:37:13.318, Speaker A: Yeah, that's a good remark. So I should have said that. So that's because even if you knew the diction, if you knew the dictionary, then the problem we are talking about is linear estimation. Right? If I have y is f times x, and I give you the dictionary, you have sparse linear estimation, x is. And so you can understand this denominator here. The alpha needs to be bigger than rho, that's like the number of measurements. Even if you knew the dictionary, the number of measurements needs to be bigger than the sparsity, because otherwise you cannot do anything.
00:37:13.318 - 00:38:05.322, Speaker A: And there is another asymptote that even if you knew the dictionary algorithmically, you cannot quite get to this alpha equal row. Even if you knew the dictionary, that would be like if you were doing the l one minimization, that would be the Donoho thunder line, and in this case is the biasian setting. So that would be the line that comes out. This rho SCS Cs stands for compressed sensing. This is the smallest density for which you could recover the signal even if you knew the dictionary. So the asymptote of this curve is given by that. You know, it's like no matter how sure, like if you had infinitely many samples, you would maybe, you know, the best you could do, you could discover exactly the dictionary.
00:38:05.322 - 00:38:23.354, Speaker A: But then even if you had the dictionary, you wouldn't be able to say anything about the signal. And so since you are not able to say anything about the signal, you actually wouldn't be also able to discover the dictionary. The MSC will be very big on both of them, but that's where these asymptotes comes from.
00:38:23.854 - 00:38:25.554, Speaker C: Depends on the algorithm.
00:38:26.494 - 00:39:03.916, Speaker A: So the blue curve. So this blue curve is for the amp. So now the conjecture is, you know, nothing should do better than this. Now, if you ask me, does the present algorithm reach it? So not quite. There are some like convergence problems. So this, if I take the implementation, the best one that there is out there so far by Schnitter and Parker and sever, you know, there is a bunch of implementation tricks. They slow it down and do some adaptive dumping and quite a number of tricks to make it converge on the practical problems.
00:39:03.916 - 00:40:00.534, Speaker A: But if you run it on these random instances, it's not quite reaching this line. So even for the amp, there is still some work to do to actually algorithmically match it. And then there is the question, what about all the other algorithms? So, yeah, none of it is like even close. So the question is how close can we get, or is it even a meaningful line here? What's happening? So that's the conclusion. So, I presented you this problem, which can be thought as the simple model for feature learning, if you want. And as such, why to study like a randomly generated data, if all the data that are in the world are certainly not random? Well, the idea is the same, like if with the stochastic block model, right? It's also a random model, and nothing looks like generated from the stochastic block model. But at least we can do some theory and compare the existing algorithmic performance to what should be possible on that model.
00:40:00.534 - 00:40:49.018, Speaker A: And that gives us some understanding. So that's somehow the spirit of this work. And so what we derived where the, you know, the formula for was the best, best error independently of algorithmic complexity, and was the conjecture for the best achievable error with algorithms. And the message here is that in this particular problem, the state of our algorithms are not close to it yet, so there is definitely space for improvement. And if you want to read about it more, this is our paper, and the algorithmic application paper is this one. And the final slide is the to do list. So what are the things that somehow seems within reach? And probably, if you like, sit here again in, say, 510 years, then a lot of it will be done.
00:40:49.018 - 00:41:41.686, Speaker A: So, on the math side, proving that the state evolution for this high rank matrix factorization is actually correct, that's an interesting open problem. Proving that the detectability lower bound is tight in the noiseless blended factorization might also be achievable. We are some second moment method, we haven't done it, but it's the type of problem that should be doable. Hopefully. Then this question of what are the algorithms that provably or empirically both are interesting work down to the phase transitions that we computed. So that's both for math and computer science, then usually the belief propagation is the first candidate that has like good empirical performance. In this case, it's not quite the case.
00:41:41.686 - 00:42:22.378, Speaker A: So what's really happening when we try to implement the Imp in this case? What's the problem here? We're still not quite sure. The next question would be, you know, we could, like, solve. So far I was, we were assuming that we know what were the distributions PX and Pf. So what if we don't know that? Or we want to find some ground states, etcetera. So, you know, doing the replica symmetry breaking for this, for these problems, or generalize to tensors, to more layers, or to priors that are non separable that are not like this IID priors. Okay. These are all directions that I think are interesting to look at.
00:42:22.378 - 00:42:23.454, Speaker A: That's all.
00:42:30.314 - 00:42:48.730, Speaker C: Yeah. In the limit is all the difficulty of the problem coming from what you mentioned, even if I gave you the dictionary, the other problem is ours. Is there some additional difficulty from the other side also, like from not knowing or is asymptotically the problem just.
00:42:48.922 - 00:43:13.504, Speaker A: Yeah, no, it's both. It's both because, I mean, I just explained that asymptote, but if you knew the dictionary, you couldn't pass it anyway. But if you look how, you know, above, if I go back to this figure, like above the blue line, if you look, what's the MSE that the algorithm is getting? It's, it's bad on both. So it's, yeah, it's both that you like.
00:43:15.804 - 00:43:18.624, Speaker C: But where will the blue line be if I do the picture?
00:43:19.244 - 00:43:49.264, Speaker A: That would just, that would be this black line. That would be this black line. So with, no, you would not need any. If you knew the dictionary, you know the samples, you have them to be able to learn the dictionary. If you know the dictionary, then basically the problem becomes independent sample by sample, right? You have no, the posterior is just the product over the different samples. So just for one sample, you could learn it up to here.
00:43:53.884 - 00:43:54.804, Speaker C: Other questions?
00:43:54.884 - 00:44:23.674, Speaker B: Yeah, I'm curious about the state of actual benchmark data sets in this area. So, for instance, if it seems like I would have to show you an awful lot of audio files before, in terms of sparsity, it was worth describing it in terms of fourier functions or wavelets, as opposed to, to saying, oh, your basis is one for each audio file. So, I mean, and similarly with images.
00:44:24.814 - 00:45:05.246, Speaker A: So will people do? Will people do, I don't know, so much for sound, but for images, will people do, like, first of all, you know, these algorithms, like, you at least need to multiply the matrices, though they are n squared and they cannot be better than n squared. So it's, it's quite slow to start with. So people take like little patches. Usually they would take like eight times eight patches from the images and that would be the samples. So you cannot even with the current stuff, you cannot even like really do learning on the full images. Usually they do learning on the patches and. Yeah, but even in the, like, very, in the Ozhausen field work in 97, I mean, that's what they did.
00:45:05.246 - 00:45:44.814, Speaker A: And they, and they learn a dictionary with their KSVD type of algorithm, which looks like when you look at it, those are the wavelengths. So that's somehow. Okay, the high level of why their paper was the punchline of their paper, that they could somehow learn something that looks like the wavelengths from small patches. But, yes, you need to present a lot, a lot of them. So. But I think it's time to ask, you know, what's the smallest number of patches that you need to learn something and that somehow where we try to go.
00:45:47.194 - 00:45:54.274, Speaker C: Another question. Okay. All right, so we'll see you at 1145 for the grand finale.
