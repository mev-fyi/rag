00:00:00.120 - 00:00:54.968, Speaker A: Basically, there's two points I'd like to make today. So, first, to suggest some people have critiqued the human in the loop as simply being a way to scapegoat human operators, the failure of systems which, beyond their capacity to control. I think these critiques simultaneously enshrine an overly conservative or restrictive conception of the function of the human in the loop in a way which aligns the human in the loop's capacity for creative interpretation and intervention in the use of even complex technologies in the workplace or within automated systems. So I argue. I argue nothing because my slides aren't working. That would be the problem, because it's the same as the first slide. Great.
00:00:54.968 - 00:01:24.958, Speaker A: Good. All right. Thank you. Okay, so I argue the inadequacy of the human in the loop, what I will describe as its anachronistic nature, is also the condition of its necessity and its possibility. Specifically argue that the human loop has not become an acronym, but it is born as an anachronism. It's always too late, always too much after the fact. But the fact of it being after the fact can compels it always to reinterpret its situation and to produce its situation anew.
00:01:24.958 - 00:02:03.434, Speaker A: And this is a kind of interpretive creativity which I think can't be automated. And which, as such, remains the necessity of the human in the loop. Second point I'd like to make is to sketch out a genealogy of the human in the loop, the human factor, the human operator, beyond the better known anglo american tradition. And namely, within the french, the francophone tradition of work studies. And I'd like to point to particularly two factors which have been influential in the french work science tradition. One is the insistence upon understanding the milieu of the workplace and its specificity. And within a broader social conception of struggle and antagonism.
00:02:03.434 - 00:03:07.886, Speaker A: An antagonism which expresses itself in complex ways, but which is important to bear in mind because, as especially Veronica de Kaiser argues, is only ultimately able to be taken care of by workers themselves. The second key contribution is the emphasis in french work studies on what borrowing from the language of linguistics is termed catachresis. Catachresis, in the linguistic tradition, refers to the use of an incorrect word to describe something you lack the right term for. So in French, you call a potato or pomme de terre, like an apple from the earth, even though obviously a potato is not an apple from the earth. Right. So in french work studies, though, catachresis is used cataclysmically. To describe at one level the wrong use of tools in the work environment, when a given environment is found to lack the right tools for the job, but also, more suggestively, the creative capacity of error, the capacity to reinterpret the meaning of work itself and of one's role beyond, or at any rate differently to the ostensibly proper definition of that role.
00:03:07.886 - 00:04:24.594, Speaker A: And this kind of error I also don't think can be automated, and so remains the ironically proper, improper role of the human in the loop. Now, the figure of the human in the loop has been proposed as a safeguard against the dangers of AI and automation, but its inadequacy has been much commented upon, including by some of my learned friends in this seminar room. One of the most forceful, thoughtful, and influential critiques, though, has been that of Madeleine Elish and her argument regarding the human in the loop as a moral crumple zone. So Ehlers traces genealogy of the human in the loop and human factors theory, arguing the human operator is often borne the brunt of blame in a way which distracts from our understanding the broader sociotechnical context of accidents and other harms. And Alice uses the term moral crumple zone to describe the way the human loop becomes, quote, more than just the articulation of a scapegoat. The term is meant to call attention to the ways in which automated and autonomous systems deflect responsibility in unique and structural ways, protecting the integrity of the technological system at the expense of the nearest human operator. And this is a thoughtful and welcome critique, and one which alish supports through a careful genealogical account of the concept and particularly through an analysis of the three Mile island accident.
00:04:24.594 - 00:06:08.372, Speaker A: The 1979 Three Mile island accident marked an important inflection point in the development and regulation of automated work systems. Investigations into the partial meltdown of reactor number two at the three Mile island nuclear power plant emphasize the significance of control room operator errors in the unfolding of the accident. The failure in the control room to respond adequately to cascading alarms and ambiguous indicator lights was taken by many as indicating a need to, quote, eliminate human operators from such systems as a way of eliminating human errors and enhancing system safety, as one investigator put it, EH argues that the way the accident was covered in the news media and in official federal investigations placed the blame for the accident unfairly on the human operator in a way which distracted from the responsibility of designers and policymakers producing inherently dangerous socio technical systems like nuclear power plants, and that this is colored the way that the human in the loop is framed. Even today, she writes, cultural perceptions of fault in automated and robotic systems may permeate formal frameworks of accountability through both media accounts and official accident reports. These perceptions influence the outcome of legal interpretations of new technology. Now my concern, though, is that in articulating the human in the loop as a moral crumple zone, as that which crumples, we reified too much the assumptions of normative military cybernetic models of the human operator or human in the loop. Eh's concern is with the cultural legacy, particularly of three Mile island, and certainly the official government accounts thereof, having contributed to this and the associated news coverage.
00:06:08.372 - 00:08:01.716, Speaker A: But I would argue that the most influential and perhaps enduring american cultural account of Three Mile island was not the telling of the event in and of itself, but rather the hugely successful creative nonfiction of Thomas Wolfe's book the right Stuff, which came out several months after Three Mile island and became an enormous bestseller, selling several million copies and eventually being made into the eponymous film in 1983. Now, the right stuff isn't about Three Mile island, but it's essentially a history of american human factors, which is the interpretive frame that was used to define and understand three Mile Island. Woolf's book is about how the daredevil recklessness of american navy test pilot culture came to define the definition of what made for the right stuff in the human operator, understood through the formation of that most american of 20th century heroes, the astronauts who embraced the dangerous challenge of the Gemini, mercury and Apollo space programs. The curious thing about this history, though, is that Wolf's thesis is essentially that the right stuff, that which defines the normative function of the human operator, is exactly the effacement of human factors altogether. Wolf saw the key quality of test pilots and astronauts as being essentially a finely calibrated death drive, and he positions his analysis explicitly against the effete european anti war tradition of remark and his new world acolytes like Ford, Maddox, Ford Woolf seeks to redress this anti war tendency, and for him, the right stuff became the story of why men were willing, willing, delighted to take on such odds in this, an era literary people had long since characterized as the age of the antihero. The spirit of the test pilot and the astronaut program is incarnated in the spirit of the eagerness to shut up and die like an aviator. And this is what the right stuff or courage amounts to for Woolf.
00:08:01.716 - 00:10:17.474, Speaker A: Now, Woolf's literary historiography is, of course exceptionally one sided, just as influential as remark, in fact surely more influential for its political impact upon german conservatism was the work of Ernst Junger and his theorization of total mobilization, whose great, whose tragic celebration of the chthonic elemental experience of war defined the battlefield as a privileged site for the experience of the uselessness of modernity, and Junger's work, I think, provides a prefiguration of Woolf's own much more boy zone dollar store Kerouac account. Junger's conservative understanding of the significance of martial experience had an immense influence on Heidegger and in turn on Friedrich Kittler. And Kittler's martial determinism has in turn had an immense impact on the way much materialist media history is conducted, including, I think, the work of Peter Galison's ontology of the enemy, who famously argued that to understand the specific cultural meaning of cybernetic devices is necessarily to track them back to the wartime vision of the pilot as server mechanism. Now, we do certainly see the term human in the loop first emerge in the early years of NASA's space program, and it's obviously a borrowing from the military cybernetic concept of the man in the middle of the cybernetic modeling of the human operator as a gunner or pilot as an element within a military server mechanism. The problem, though, I think, in such cultural imaginaries of the cybernetic human operator and its legacies, is that there's a tight coupling between the progressive critique of the human in the loop as both not culpable as well as the conservative militarist critique of the human in the loop as not capable. In both accounts, there's a kind of helplessness the human operator faces when confronted with the immensity of modern, complex sociotechnical systems, and this immensity is irreconcilable with the scale of human phenomenological capacity, and so must be sublimated either in death or within the experience of the kind of blameless culpability that Ehlers describes. What's remarkable, though, is to consider how the human in the loop is born as an anachronism, as military researcher and NASA engineer Joseph Saunders put it, as the weakest link in the weapon system.
00:10:17.474 - 00:11:11.714, Speaker A: What is an anachronism? For Georges D. Huberman, he draws attention to the creativity of anachronism. That which exists out of time, demands an explanation. Why is it still here? What is it doing here? What is it which has survived of the human? Didi Uberman invokes the freudian idea of afterwardsness, the compulsion to reinterpret the after the factedness of traumatic experience in particular, but also of experience in general. The point here is to insist upon the normative function of reinterpretation as fundamental to the production of meaning. And to argue that when this capacity is blocked or suppressed, it is not merely critical accounts which become impossible, but strictly speaking, any account at all. Human in the loop is an anachronism, but it's one which demands an explanation, as well as being the one who must account for this explanation.
00:11:11.714 - 00:12:20.968, Speaker A: Now, I'm running out of time. But I think it's possible to trace the genealogy of the human in the loop in a way which points to its agentive capacity, its capacity for responsibility, in a socially situated and interventionist and creative way, and not merely in terms of culpability or as that which crumples. Particularly, I think there's a tradition in francophone work studies which takes up and transforms sort of american human factors research by emphasizing the workplace is a site of social antagonism, but also this idea of catacresis. Catacresis, in Yves Clowes definition, is what happens when you lack the vocabulary to define certain ideas. So a catechistic situation is one where a tool serves a usage beyond its own. When a machine works beyond the limits of its normal function, and it's an index of the fact that users, in spite of everything, contribute to the conception of the usage of the technical art effect. In this activity rights, there is as well, an activity of reconception.
00:12:20.968 - 00:14:32.100, Speaker A: So catachresis describes not simply adaptability and not an understanding of error as the magnitude of difference between a prediction and the true value of an observation, but a kind of creative and necessary error, not simply or solely deviation from a real use, but a deviation from the norm which creates new norms and new possibilities for interpretation and reinterpretation or reconception. De Kaiser concluding now, Veronique de Kaiser argued that it is precisely the human operator for whom the question of risk and responsibility is most fraught, who has the most at stake, namely, their lives or their mental or physical health, who must be allowed the interpretive capacity to intervene in defining their own role within the workplace, and that the evaluation of the workplace environment is a process which is, quote, continuous and collective and conducted by workers, one in which the scientific criteria is replaced or doubled by a collective verification. So the implications of these arguments, I think, are that when we're defining the role of the human in the loop, we must understand this role in terms of maximizing what de Kaiser, drawing from Simondon, refers to as the zone of autonomy, the capacity and the freedom to reinterpret and redescribe the meaning of industrial or automated systems. The human in the loop must have the freedom to reinterpret. And this capacity, I think, is as much the outcome of, like, workplace protections, worker organization, as of any sort of particular protocol for making the correct interpretation. So the point here is sort of to point to an alternate conceptualization of the human in the loop in which the question of culpability cedes ground to the capacity for capability, but also for reinterpretation, intervention and workplace solidarity, and that these should be sort of considered as key to the definition of the role of the human in the loop today as well. I think so, yeah.
00:14:32.100 - 00:14:33.304, Speaker A: There we go. That's it.
00:14:43.404 - 00:14:50.644, Speaker B: Yes. We have about 1520 minutes for q and a, so let's welcome the two.
00:14:50.684 - 00:14:51.544, Speaker A: Speakers.
00:14:53.604 - 00:14:56.264, Speaker B: And we will collect some questions.
00:15:15.744 - 00:16:26.642, Speaker C: Yeah, thank you for both of those excellent papers. I suppose this is a kind of response to the request for options, and then maybe it feeds into a question for Chris. Yeah, a lot to think through. So, I mean, when I was preparing my talk earlier, I kind of wanted to give this example, but took it out for reasons of time. But an interesting example from stuff that I research where the human has kind of got a particular function, but also serves as sort of, as a risk factor, is in the way that the European Court of Human Rights has parse the issue of what's called bulk interception of communications for intelligence purposes, for crime fighting purposes. So basically, in the aftermath of the Snowden revelations, it transpires that there's this huge apparatus for real time interception and analysis of data, and a lot of the way in which that data is parsed is done automatically. But what the difference makes a difference, as it were, to the intelligence agents, takes the form of a string of selector terms that have to be input into the interface, and then they sort of try to find things that are related to that.
00:16:26.642 - 00:17:11.772, Speaker C: And European Court of Human Rights has really wrestled with what this means in terms of mass, you know, surveillance and political power and privacy and all the rest of it, where to find a sort of balance between that. But it's really, I think, kind of slightly weird. Just in one of the most recent decision on it, the big decisions that have been made in Big Brother watch in the grand chamber, the european court found that the british oversight system, which is being kind of very heavily specified and over determined, now has this one little lacuna, which is the point at which an intelligence operator puts in the terms for searching system. Those selections have got to be themselves justified by reference to the values of proportionality. That's a kind of assessment that I.
00:17:11.788 - 00:17:12.344, Speaker A: Think.
00:17:14.084 - 00:17:59.294, Speaker C: It'S pretty much automatic already because the terms are already given. The way these things are already audited is that they're done on a kind of, they're audited by machine learning processes. They're looking for, like, is there originality in each entry or are they copying and pasting? So it's already a kind of a machine kind of process. And so that's an example where the human is the kind of risk factor, but also performs a certain legal normative sort of role. And then I guess that for Chris, I mean, I guess the question, your presentation is brilliant, but is this really work already rooted in the kind of concern for workers, which puts it antagonistically in relation to capital, but maybe doesn't make the same sense if you think about it, in relation to, say, like, how the government aims to control and survey population.
00:18:06.274 - 00:18:07.774, Speaker B: A lot to digest.
00:18:08.914 - 00:18:11.654, Speaker D: That's a great example, though. I appreciate it.
00:18:18.594 - 00:18:21.858, Speaker A: I don't know would be my response.
00:18:22.026 - 00:18:22.530, Speaker B: I don't know.
00:18:22.562 - 00:18:44.914, Speaker A: I mean, it depends. I mean, I'm not like, I'm not a law scholar, you know, and I'm not an expert in european governance mechanisms. Sorry. Maybe. Sorry. Yeah, I don't know. Maybe we can talk about it more later.
00:18:57.214 - 00:19:33.644, Speaker E: Thanks. So I wanted to respond to Chris's talk. Sorry, Ben, if you could sort of, just so I can, as one of the folks who have been critical of the human in the loop paradigm. So just want to draw out some of more of your thinking on this, and then perhaps we should also have a longer conversation later. But I guess two points. I'd love to hear your thoughts. I mean, one, it sort of sounded like at times you were positioning the critiques of the human in the loop as calling for just saying, get rid of the human in the loop and let the system go on.
00:19:33.644 - 00:20:36.450, Speaker E: Otherwise, without that human, which I would say is not how most of those critiques would go. So I'm sort of, how do you see what the critiques are calling for as an alternative? And sort of what would you call for as the alternative? Sort of given those limitations of human in the loop that goes beyond just saying get rid of the human overseer because they fall short. But then the other piece to maybe you sort of have this binary between culpability and capability and sort of shifting between them. And I would say potentially that's not a binary. My particular stance on it would be that actually, the problem is both that we do have the sort of l ish style culpability issues, but we also have capability issues that actually, the types of capabilities you're calling for, even if we do design more in that direction than for pure scapegoating are actually very hard to achieve in practice. So sort of just curious how you think about those dimensions of the human in the loop problem.
00:20:36.562 - 00:20:57.988, Speaker A: Yes. No, I should. I mean, you know, I was trying. My point is certainly not to consider culpability and capabilities as binaries or as a dialectic. You know, the point is they are very close, you know, and they have to be taken up as they're, you know. Yes, part of the same problem. I completely agree.
00:20:57.988 - 00:21:41.286, Speaker A: You know, this is sort of part of the thing, you know, the kind of. I guess that I'm also not. I mean, I think Elish's work is brilliant, you know, and I think that account is probably the best critique I've seen of it. You know, I mean, there's very good critiques of it. The point is, you know, to kind of like coming back to this question of contingency, which seems to be kind of keep returning to, you know, today, you know, is the kind of like, openness of that contingency. You know, it's not. The question is like, if you sort of consider politics as, like, the kind of like, question of the distribution of the sensible.
00:21:41.286 - 00:22:36.994, Speaker A: Right. You know, it's the question of intention, you know, what is it that you kind of take as the thing you emphasize, you know? You know, within a legal framework, it makes sense to kind of focus on problems of culpability, you know, because it's a question of responsibility within a particular kind of particular sort of legal epistemology, you know? But I guess my point is, the point of contingency is there's an absence or a lack at the kind of. Within the definition of any kind of epistemological system. And there's a kind of openness there. And if it's not the kind of human operator which is able to take that sense of culpability, then who would I can think of? I mean, it would have been better if I could have. There's examples I could draw upon. Here.
00:22:36.994 - 00:23:20.644, Speaker A: In Australia, for example, we had the robodebt scandal, I don't know if you know about this, where there was a sort of an appalling automated recalculation of social benefits, and it was disastrous. It wasn't a particularly sophisticated model. It's estimated that several thousand people died from suicide as a result of the stress. So in terms of an industrial accident, it would be one of Australia's largest industrial accidents ever. Right. The point is here it's not. You have to consider that it's not simply the kind of unforeseen outcome of a kind of badly designed system.
00:23:20.644 - 00:23:45.526, Speaker A: That operation is situated within a particular force of a political contingency. And there's sort of stories coming out. There's hopefully going to be a proper royal commission into robodeirt. But the story is already coming. Everybody in their department knew what it was doing. It was quite plainly obvious, too, the humans in the loop. But they couldn't, what I'm calling reinterpretation in a broad sense.
00:23:45.526 - 00:24:11.740, Speaker A: They didn't have the freedom to describe it as a bad system and to, you know, to be whistleblowers for fear of losing their jobs, essentially. Right. Or for facing legal punishments. Right. Now, the point, I'm not arguing that they are morally culpable in the same way that, like, the conservative politicians who, or the designers are. But my point is, they were really the only ones. Well, them and the actual people who.
00:24:11.740 - 00:24:20.424, Speaker A: There was obviously a social, solid movement that organized against it in the end. Right, sorry. No, no, no, sorry.
00:24:21.124 - 00:24:34.124, Speaker B: Several folks who raised hands. I think we have like five or six questions. So I'll just collect all the questions all at once after we finish your answer. And then we'll. Then they turn back to nobody.
00:24:34.204 - 00:25:12.236, Speaker A: No, that's right. But so this is the thing. It's not a particular. I recognize that it's very, you know, in terms of defining even the legal frameworks is very complicated in the actual, whatever, quotidian, day to day operation of these things. But this is the problem. Contingency is dangerous, you know, and you kind of have to. I think the human in the loop is the figure who can, like, encounter that danger, not in the kind of death desiring way that yunga celebrates, but in a positive.
00:25:12.236 - 00:25:23.464, Speaker A: So I think there's a possibility there for solidarity, social organization and so forth. You know, I think there has to be. That's the argument.
00:25:25.684 - 00:25:35.964, Speaker F: I mean, my question was sort of broadly for Chris as well, but I feel like there's probably people who have questions for JD, which maybe we need to hear about as well. So I'm happy to.
00:25:43.464 - 00:25:44.016, Speaker A: Hi.
00:25:44.120 - 00:25:48.736, Speaker G: Yeah, this question was for JD, but it plays on this like this discussion.
00:25:48.760 - 00:25:50.680, Speaker A: Of culpability and capability.
00:25:50.832 - 00:26:14.460, Speaker G: Since you mentioned capabilities, these llMs, I was just thinking, like, when we think about the relationship between how scale might result in new forms of culpability as well, you mentioned the let 1000 flowers bloom approach. I would just be curious to hear more about your thoughts on that emerging, the possibility of that expansion of access.
00:26:14.572 - 00:26:16.156, Speaker A: To these sorts of systems, because off.
00:26:16.180 - 00:26:40.682, Speaker G: The top of my head, in my networks with, like, my AI friends and whatnot, there was a lot of solidarity and just sort of meme formation around, like, playing with Dolly, two outputs and having it do some really offensive shit. That's funny and interesting, but problematic for all sorts of reasons. But there was a lot of energy on Twitter for my Twitter for two weeks was just taken up with all.
00:26:40.698 - 00:26:42.242, Speaker A: These meetings from that system.
00:26:42.298 - 00:26:43.882, Speaker G: So I would just be curious to.
00:26:43.898 - 00:26:45.054, Speaker A: Hear your thoughts on it.
00:26:54.244 - 00:28:28.154, Speaker H: Yeah, this starts out as a question for JD, and then I think it also has a human in the loop implication, and I'm much struck by your concept of, let me see, labor of love. And I have my favorite example of if I knew that the birthday card I received from my spouse was generated with a lot of help from LLM, despite the fact that it's probably better formulated and it's going to come on the right day and all of those things, I'm not going to see this as a labor of life. And it kind of gets to this thing of human in the loop, which is in that instance, we really. The human in that loop is absolutely essential. So it kind of begs, and I don't think I've ever had this thought before with human in the loop and man in the middle, whatever it is, which is any large, any model, any machine learning model, and it has come up in our cluster discussions, involve humans because, you know, the training data is based on human practice, human speech, human. So, like, machine learning, people are just. We've got it, you know, lots of humans in this loop.
00:28:30.094 - 00:28:30.674, Speaker B: Yeah.
00:28:30.774 - 00:28:34.610, Speaker A: So, anyway, yeah, thanks.
00:28:34.642 - 00:29:13.162, Speaker I: I had a quick question, kind of for both, but appreciate it's not answerable in the time allowed, but still maybe something to think about. So, time, I like the temporal element, and I think it's really important. We haven't talked about it very much at all, but, of course, french philosopher Bernard Stiegler defines the production of historical time through technology and technicity. That's how we get it. And so what occurred to me here was that there's sort of really interesting going on with temporality. First of all, JD, do you know Grammarly? Do you know what Grammarly is? So is that what you're talking about? Because I had a visceral reaction to Grammarly as a trained linguist. I said, well, this is kind of a disaster.
00:29:13.162 - 00:29:30.082, Speaker I: We're going to freeze language production in this loop of recommendations for how to finish my sentence. And this is going to ossify or it's going to do something to a language process that I personally hate and hate anyone who uses it as well. No, no, but I mean, I'm serious.
00:29:30.138 - 00:29:30.306, Speaker A: Right.
00:29:30.330 - 00:30:06.552, Speaker I: So do you agree that there's a kind of this risk of ossifying or freezing cultural life through these. Through these products? And I mean, I guess more for Chris as well. I mean, so my critique of humanity in that paper, blah, blah, blah, it would be something like, it's precisely through this frozen anachronism of what this frozen narrative or imaginary about what is happening in decision making that this anachronism arises. I agree with your analysis. I think it's important as a diagnostic of that sort of arrangement, of the necessity and structure of that thing. But I just. It's another version of this political problem.
00:30:06.552 - 00:30:11.080, Speaker I: But I guess I come at it from quite a different angle. So I think maybe we could talk about that another time.
00:30:11.192 - 00:30:11.752, Speaker A: Great.
00:30:11.888 - 00:30:15.664, Speaker D: I think your first questionable. Is answer. First question is answerable in the time we have remaining.
00:30:15.704 - 00:30:21.244, Speaker A: And that is, yes, I think Jake should get his.
00:30:27.244 - 00:31:38.346, Speaker F: I mean. So, Chris, you know that I have this idea that, for instance, article 20 two's prohibition on a fully automated decision or constraint on a fully automated decision. What's often taken to be sort of a paradigmatic human in the loop legal provision, I've argued, has its roots in european industrial democracy, has its roots in workers movements. To have a say with respect to the operation of the factory system. And I was very interested to hear the reframing of that in the context. In a different work context where the human is the, like, the operator capable of. I'm just trying to, like, resolve for myself the subject position of the human in these two different accounts.
00:31:38.346 - 00:31:43.650, Speaker F: And whether they are maybe the same or whether they're like. I don't know. You read that paper of mine.
00:31:43.682 - 00:31:46.248, Speaker A: Yeah, that's great. That's.
00:31:46.376 - 00:31:47.204, Speaker F: That's just.
00:31:48.064 - 00:32:26.502, Speaker A: I mean, the problem that. I mean, we've talked about this before, right? Is the question too. It's not like I'm pointing here, you know, just pointing the project of genealogy, you know, is like exploding, you know, the subject of history. Right. You know, so it's just pointing to saying there's one way american, anglo, and even anglo american accounts of the imam are very complex. You know, I'm wildly over, you know, making things too simple. But so just pointing to this kind of different conception and different tradition.
00:32:26.502 - 00:32:54.060, Speaker A: The point is, of course, not that you can just simply take like, what the French were doing in the seventies and say, like, this is the normative model, you know, because exactly as we've talked about this before. The works work Council tradition in Europe is itself historically contingent. Right. It's come out of a particular kind of historical juncture. There's, you know, and we've in. It's. Which is not operative in America, not operative in Australia.
00:32:54.060 - 00:33:54.366, Speaker A: There's a similar. There's an interesting kind of like, example in the nineties that we've also talked about of kind of early forms of automation of warehouse systems, which led to like this kind of proto Amazon thing that led to an enormous industrial unrest and very interesting legislation which was brought in kind of model, maybe not modeled, but something like the european tradition, demanding worker contribution to the definition of KPI's and so forth. But it collapsed because, like, the political structure is not there for it. Right. So, so I don't know. I mean, I don't think these are mutually incompatible. But the point is I'm kind of like challenging, say like Galison, for example, he's arguing any conception of cybernetics, he talks in french theory as well, has to be traced back to this like, very particular kind of military cybernetic model.
00:33:54.366 - 00:34:11.404, Speaker A: And I said, you know, in my opinion, this is, you know, it's like, this is like the normative kind of methodological claim is that, no, the human in the loop is itself exploded. And there's multiple traditions that we can draw upon. So it's certainly not. I'm not trying to. Yeah.
00:34:11.524 - 00:35:01.758, Speaker B: Anyway, you get the answer. In the interest of time, we'll hand off to JD and answer, I think, three questions for you. But I would also just add one last question to both of you on the theme of AI and humanity. And we're very much interested as a group, we're interested in how AI, how humanities are represented in AI, captured in AI. And I think to JD, I'm very curious what you think LLM, whether LLM brings anything categorically different in terms of how we think about humanity. And for Chris, I'm curious how AI, whether you believe AI would add anything categorically different for your work. So maybe we don't have any time for all of this, but I would love to just throw out there if you have anything to share.
00:35:01.846 - 00:35:02.118, Speaker A: Sure.
00:35:02.166 - 00:35:26.368, Speaker D: I mean, I think so. The question sort of, of culpability and responsibility, I think perhaps those mean slightly different things has come up a bunch of times. And you actually, Chris, you asked the question of like, well, if not the human, the loop, then who is responsible? And I think, you know, I haven't really heard that much talk about the system designers except in the anecdote that you gave that I wasn't familiar with about the. What was it?
00:35:26.376 - 00:35:26.928, Speaker A: What did you call it?
00:35:26.936 - 00:35:30.728, Speaker D: The robot. Robot death. Was it in Australia that the change.
00:35:30.776 - 00:35:32.484, Speaker A: Oh, robodess. Yes, robo death.
00:35:33.544 - 00:35:34.296, Speaker I: What was it?
00:35:34.360 - 00:35:37.344, Speaker D: Maybe a very different kind of chat. What was the actual name?
00:35:37.384 - 00:35:54.974, Speaker A: I'm sorry? Well, I can't. The actual name was much less. If they'd called it robo debt from the beginning, people might have been more suspicious. Automated debt recovery algorithm, I think. Something like that. Yeah. Yeah.
00:35:55.094 - 00:36:07.086, Speaker D: I think that, you know, maybe we should be asking some questions of the system designers if there's not a human in the loop, rather than, you know, treating the human in the loop as sort of the end of the chain of humans involved in that.
00:36:07.190 - 00:36:08.634, Speaker A: Oh, certainly. No, no.
00:36:10.094 - 00:36:29.794, Speaker F: In the settlement decision, yeah. There was an explicit judicial decision that none of the designers had a culpable mental state. None of them adequately. That this was incorrect and going to cause harm in a way that would.
00:36:29.834 - 00:36:31.254, Speaker G: Generate illegal liabilities.
00:36:34.074 - 00:37:09.820, Speaker A: Of policy. Quite right. No, this is the thing. It's hard in 15 minutes. But I have to fundamentally reiterate that I'm not critiquing the idea that you should blame the human operator and ignore the designer and ignore policy. The question is the kind of openness of what that means by capability and culpability. And in terms of political agency, the question, I mean, in terms of what AI it does so much.
00:37:09.820 - 00:38:00.124, Speaker A: Obviously, these systems are so different compared to the kind of industrial systems at Three Mile Island. I think an interesting one, which I don't know enough about to really comment upon in any kind of intelligent way. But the functions of sort of back propagation and forward propagation in machine learning, that's sort of interesting to me as a way of maybe trying to automate this afterwards ness, back and forthness. But I think there's still a. There's a different kind of way of error involved here, again, in terms of magnitude, as opposed to kind of the way humans are wrong about things, which I also think has a lot to do with love, or at least with desire, or the lack of desire. You know, it's different, you know, fair enough.
00:38:00.164 - 00:38:51.730, Speaker D: Yeah. The one thing maybe I'll add in the remaining 15 seconds here is there are a number of questions. I think, Tom, you asked a question about and sort of dolly, and like, where does that sit into these kinds of questions? And I think, I'm sorry, I can't read your name. Karl was asking a little bit about. And Talon, you were also about like, you know, who does it actually is there, is the human that is involved in that process have the same level of impact or effort of ownership or ownership over that part of the process? Like, is it really that person's work if in fact, it was in part generated by some other system and their role was more curatorial than. Than generative? I mean, I think these are all great questions. And Chen, you were asking also about whether or not large language models sort of bring humanity into the AI world.
00:38:51.730 - 00:39:37.736, Speaker D: And I don't know that there are answers to these questions, because I think in large part, it depends on how you define these things. And in many cases, we define AI and humanity in weird, exclusive ways. And it's not at all clear to me that they are exclusive. Large language models do incorporate the writings of billions, if not hundreds of millions, certainly of people, if not billions of things that they've said online and that are now sort of accumulated in some relatively unknown way into a system that reproduces things that kind of look like what those people said? And so it's not clear that you can actually draw such an obvious bright line between that system and the people whose data went into it.
00:39:37.920 - 00:40:25.824, Speaker B: I'm not gonna let you go that easily with those three questions about, I think, the sort of unintended consequences of LLM. So we mentioned, like, offensive meme generation, sort of automating personal writing, and then there's a Grammarly question. Do you want to comment on that? And I think I also want to point out, I think there's a very interesting dynamic here where Chris was critiquing the afterwards ness of the worker, as well as I. Unfortunately, you're the worker in this case. You are trying to predict some of the second order effects before these llms are already ubiquitous. So I wonder how. I'm sure there are a lot of the part of the question cannot be answered because we don't quite know.
00:40:25.824 - 00:40:30.260, Speaker B: But, like, do you want to comment on the part? Like, which part might be answerable?
00:40:30.312 - 00:40:43.516, Speaker D: Well, yeah, it's a good question. I mean, so I think, Connor, to your question about Grammarly, you know, there's this, there's this. I think there's an interesting question about the use of those kinds of systems and what they really mean.
00:40:43.540 - 00:40:43.676, Speaker A: Right?
00:40:43.700 - 00:41:16.440, Speaker D: Like, you could have made the same argument about spell checkers. Like, you know, are people who use spell checkers, like, are they really. You don't think so? I mean, so. Well, maybe we can chat about that distinction later. I think there is an open question about whether or not the, like, maybe this is what you were asking, like, are the things that are produced with the help of Grammarly then being fed back into the same training models that are then being produced sort of as though they were correct, but in fact they're just sort of echoes of previous corrections of the existing text. Yeah, it's a different loop. Totally right.
00:41:16.440 - 00:41:53.114, Speaker D: It's a loop that's now intermediated behind some other system that thinks this is correct rather than some actual human judgment. Yeah, I think it's a really interesting question. I don't think I have an answer to it yet. Sorry. I think there are explicit attempts to address those kinds of questions. I forget if it was GPTV or some other language model that explicitly said, our training data ends here. Please include this token in your website if you are using text that was generated by GPT-3 so that we know not to include it in future training sets.
00:41:53.114 - 00:42:16.786, Speaker D: So I think this concern exists among the folks who are working in this domain. But I, you know, I suspect. I mean, even if they do, you know, I suspect that most don't. Right. Even if some do, just because it is, you know, to the extent that the LLM is even doing the entirety of the generation, if there's like a human curator and choosing different words, et cetera, like, where do you draw the line of whether it was generated or not?
00:42:16.970 - 00:42:17.666, Speaker A: Yeah.
00:42:17.810 - 00:42:28.684, Speaker B: Great ending on this note. Thank you, everyone. We have a short coffee break, I think, and we'll come back and 25 minutes for the next session.
00:42:29.144 - 00:42:29.744, Speaker A: 15.
00:42:29.864 - 00:42:52.334, Speaker B: Oh, 15. Okay. The boss has spoken and there is some refreshing outside.
