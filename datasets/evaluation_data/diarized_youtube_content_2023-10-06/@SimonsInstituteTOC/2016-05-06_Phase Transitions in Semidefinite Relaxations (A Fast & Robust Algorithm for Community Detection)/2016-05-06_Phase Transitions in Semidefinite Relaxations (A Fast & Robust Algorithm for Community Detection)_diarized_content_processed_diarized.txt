00:00:01.240 - 00:00:50.494, Speaker A: Thank you. And thanks to the organizer for the invitation. This very nice workshop and very nice place. So today I would like to tell you about a very recent work we have done with Adele Giovannard and Andrea Montenari, which appear essentially in these two prep prints. And now it will appear in pneus and another conference proceeding. So what is the, essentially the outline of the talk? I would like to start from a problem, a problem which has been already introduced by many of the speakers, which is essentially a community detection problem. I would like to convince you that spectral method may have some problems, and maybe optimization method based on another different kind of relaxation are better in terms of robustness, and at the same time are almost optimal.
00:00:50.494 - 00:01:54.138, Speaker A: Then I will show you the performance of this SDP based algorithm just to convince you that it works. It works very well. And then finally, in the second half of the talk, I would like to convince you that we can even compute the algorithm threshold for this algorithm, and so it's robust and quasi optimal in some sparse, difficult cases. Okay, so I don't have to convince you that the community detection problem is interesting because half of this audience is studying it. So I'm, I will skip this, but let me just focus on two features that, in my opinion, are essential. So, we want a community detection algorithm to be optimal in the sense that when you run it on a random ensemble, it detects the hidden community as soon as it can, but at the same time you want to be robust. So if you take a random graph and you perturb it, you want the algorithm to return an answer which is close to the one in the purely random case.
00:01:54.138 - 00:02:30.624, Speaker A: Okay? Otherwise it would be difficult to use it in real unreal data. Okay, so as a benchmark for the random, purely random case, I will consider the stochastic block model, which has been already defined by many of the speakers. Chris. Yesterday and also today. Mark. And so this is just to show you my notation. We consider end nodes and cube groups of equal sides, and then you put edges with two different probabilities, depending whether the two nodes belongs to the same group or different groups.
00:02:30.624 - 00:03:32.698, Speaker A: And I will stick mostly to the assortative model, where the inner connectivity is larger. Okay, so essentially this is the problem, the problem I'm focusing on to convince you, that is interesting to study. So you take an adjacency matrix and you fill it, the red region, denser than the orange part, but then you remove the colors, you remove the ordering, and you ask someone to recover from this unordered and unlabeled ads and c matrix, the original matrix with the communities. Okay, so this is our problem. And in my notation I call x not the true partition. So the one you built into the, into the graph and x hat is the estimated partition written by different statistical estimator. And we measure the quality of the inference via this scalar product between the original and the infer partition.
00:03:32.698 - 00:04:50.616, Speaker A: Okay, so this essentially is the fraction of nodes correctly identified by the, by the inference algorithm. Okay, so what are the relevant parameters? I will mainly concentrate on the q equal to equal size stochastic log model, for which we have very nice result on the bias optimal inference. The two relevant parameters in these problems are the mean degree and the signal to noise ratio. The mean degree essentially is telling you how sparse is the problem. So if d is very large, the problem is dense and is somehow easier than the problem in the various parts regime. And the signal to noise ratio is such that you can prove that in the bias optimal framework there is a threshold phenomenon at lambda c equals one, such that for lambda smaller than one is impossible, informatically, theoretically impossible, to detect any hidden partition, while for lambda larger than lambda one you can. And actually it has been proved that an algorithm based on belief propagation can return an overlap larger than zero.
00:04:50.616 - 00:05:56.426, Speaker A: Okay, so all this has been obtained by very ingenious spectral method that Mark today told us about. But the question is essentially what happens if you don't know exactly the generating model. So all this can be done if you know the generative model underlying the graph that you are working on. But if you have no information about the generative model and you only know very few things, for example that the model is assortative. So nodes in the same cluster are more connected than nodes between clusters, and for example that you have two equal size groups. Just to make computation easier. The best thing you can do is to maximize the likelihood, which actually amounts to maximize this quadratic form, subject to the constraints that the sum of the of the variables is equal to zero in order to satisfy the two equal size group.
00:05:56.426 - 00:06:43.584, Speaker A: The problem is that this problem is np hard. This is essentially the mean cut problem discussed by Dembo yesterday. And so you have to relax this problem to make it tractable, otherwise you are not able to solve it. So let me rewrite the problem in a lagrange of formulation. So you want to maximize this function, fixing these constraints to introduce a Lagrange parameter. And as long as the Lagrange parameter is large enough, the maximization of this Lagrangian happen at the point where the sum of the x is zero. In particular, if you take this Lagrangian parameter, which is equal to the mean degree of the graph, actually you can define this centered matrix, which is nothing but the matrix minus the average value, such that the mean value of the matrix is zero.
00:06:43.584 - 00:07:23.872, Speaker A: And then you can maximize this quadratic form without the constraint. Okay, so this what we would like to do. Still, this is np hard. It's just a reformulation of the problem of the previous slide, and so we have to start with some relaxation. Otherwise the problem is unsolvable for very large n. So what spectral relaxation does, essentially it tries to maximize this function instead of maximizing on the set of all these discrete points, maximize over the entire hypercube, which is a convex space. So you can easily maximize this function.
00:07:23.872 - 00:08:34.704, Speaker A: And then you compute, this amounts to compute the eigenvector, the principal eigenvector of the centered matrix. And once you compute this principal eigenvector, you project back in one of the vertices of the hypercube, which is one of the set of possible solutions by just taking the sign. All this is fine as long as all the components of the principal eigenvector are similar. But if the principal eigenvector starts to have components very different, this projection is unlikely to give you the right result. And indeed, this is actually what happened. If you look, for example, you do PCA on a sparse matrix, as long as the degree is very large, you see that the component of the principal eigenvector are very much correlated to the partition. The blue and the red are two different partitions, while if the degree is very small, sorry, if the degree is very small, then you have localization, and the component of the principal eigenvector are more or less useless for understanding the underlying partition.
00:08:34.704 - 00:09:26.326, Speaker A: So the reason why this happens is well known. If you can write the centered matrix as a rank one matrix bringing the signal plus a random matrix, and if you study the spectrum of these random matrix, you realize that, well, for large d you have this bigger semicircle law, but for small degrees you have a tail with localized eigenvector. And this is the where the problem arises. So PCA fails. For this reason people know this and so invented new spectral method. But all the methods based on the adjacency metrics, they are suboptimal. And in particular you can prove that many of these have a threshold which is strictly larger than one.
00:09:26.326 - 00:10:36.104, Speaker A: Until very recently. All these authors propose a new spectral method, which is much better in the sense that, well, I don't want to spend too much time on this because we already heard the talk by mark on this. But essentially these new matrix avoids some kind of localization, essentially because you do non backtracking paths, so you are not going forward and backward on the highly connected nodes. And thanks to these, you can prove that these two algorithms for numbers and the betation, which is equivalent and much, much easier because these matrix is symmetric, so all the spectrum is real, they can achieve optimality. Okay, so this is very nice. Unfortunately, if you try to apply this spectral method, which are optimal for random graphs on non random or quasi random graphs, you realize that there are some problems. In particular, let me define this quasi random graph in the following way.
00:10:36.104 - 00:11:43.420, Speaker A: You generate a graph according to the stochastic block model. So purely random graphs. Then you choose a small, very small subset of size alpha of nodes, and you build a clique around each of these small fraction of nodes. And you expect, since the number of edges that you are adding is a fraction alpha times d, you expect that if alpha is much smaller than one over d, the result of the inference method should not change, it should not be, let me say, should not modify sensibly by the addition of this tiny fraction of edges. Unfortunately, this is not what happened. And suppose that I'm looking here to the inference provided by the beta essential, just because it's much simpler, is symmetric matrix, where you have to compute the second eigenvalue and the second eigenvector. The second eigenvector is this one, and as you can see in the case with no clicks added, is very much correlated with the partition.
00:11:43.420 - 00:12:19.384, Speaker A: So the eigenvector is very much correlated with the partition. Look, we are on a very large graph very close to the threshold. So 1.1, the threshold is at one. Indeed the overlap is more so 39% of the nodes are correctly identified. The problem is that now if I add a very tiny fraction of cliques, so in this case alpha is ten to the minus three, so is much smaller than one over d, which is one three. What happened to this eigenvector is that it gets localized.
00:12:19.384 - 00:12:57.654, Speaker A: And so essentially the overlap with the original eigen partition is practically zero. Ok, so this is just the second eigenvector. So I ask myself, maybe the other eigenvectors still bring some information. Unfortunately, if you look at the entire spectrum, these are the eigenvectors eigenvalues. What happened is that all this negative tail is due to this newly added noise on top of the random graph. And so even if you look at all these 30 eigenvectors. None is bringing information.
00:12:57.654 - 00:13:12.274, Speaker A: You have to move into the bulk. Okay. But then you have to start digging into the bulk in order to find ion eigenvector, which is bringing information is not very convenient. Okay, so this is the problem of the spectral relaxation. So let's.
00:13:12.614 - 00:13:31.906, Speaker B: Yeah, so we thought about this a little bit with the non backtracking paper, and, I mean, it's a little bit of a philosophical issue. Right. So these cliques are very small, but they're also incredibly dense compared to the rest of the graph. So, you know, at some level, the fact that a community function.
00:13:31.930 - 00:13:34.534, Speaker A: Yeah. Consider that the clique TBA is four nodes.
00:13:35.194 - 00:13:36.842, Speaker B: All right, well, that is really very small.
00:13:36.938 - 00:13:54.014, Speaker A: So I won't say that it's so dense. These are four nodes forming a tetrahedron. So I essentially added less than 100 tetrahedrons in a graph of ten to the five nodes.
00:13:54.514 - 00:14:04.164, Speaker B: Okay. I guess I'm just trying to say that cliques are. They are communities that would jump out at you.
00:14:04.584 - 00:14:17.644, Speaker A: Yes. To reconstruct groups of 40. Essentially, these 30 eigenvectors are each one localized on each of the cliques that you put.
00:14:20.104 - 00:14:23.404, Speaker C: This is exactly the case that was looked at by Marcus large.
00:14:24.424 - 00:14:27.496, Speaker A: Yeah. But he modified the algorithm because he.
00:14:27.520 - 00:14:28.408, Speaker C: Know what is the.
00:14:28.496 - 00:15:08.752, Speaker A: He knows what is the perturbation. Yeah. You should modify the generative model in order to take into account that. And then if a new noise arrives, you have to modify it again, and you can do everything, but you have to make a much more general answers. So, let's see what we can do without to rely on the generating model. So we try to do a different kind of relaxation, an SDP relaxation, same indefinite programming. So again, we start that we want to maximize this quantity over this discrete set, which is equivalent to maximize the trace of a centered time x, subject to the fact that x is positively defined.
00:15:08.752 - 00:16:15.990, Speaker A: So all the eigenvalues are non negative. These are an n times m matrix on the diagonal xii is equal to one, and x is of rank one, because essentially is the outer product of x of small x by itself. Okay, so this formulation is equivalent to the original formulation, and now SDP relaxes the rank. So instead of looking for the matrix x of rank one, maximizing this function, this linear function, because now is a linear function in the matrix x, we relax the rank, and we maximize this trace of a times x over the convex space of all over the cone of all the positive semi definite matrices. Okay? So these are a convex optimization problem. And once we find the maximizer, which now is a matrix of rank m where m can be between one and n. We have to project it back on a rank one matrix.
00:16:15.990 - 00:17:14.023, Speaker A: And I will explain you in the next slide how we do it. So why do we expect this to be better than the spectral relaxation? Essentially, because among the matrices that we are studying, we are sampling. In order to maximize this linear function, we are forcing all the diagonals to be one. What does it mean? Suppose that this matrix now is representing the outer product of the true hidden, well, the inferred partition with itself. The fact that along the matrix is one, this doesn't allow the vector small x to localize, because if it gets localized on the diagonal, you will have heterogeneities, you will have the diagonal element, which are very much different. So you are in some sense forcing the system to produce a solution which is homogeneous over the graph. Okay? So you avoid the localization.
00:17:14.023 - 00:18:20.024, Speaker A: Now, how do we do this in practice? So in practice, we have to look for the maximum of a linear function of a huge space of all the matrices, okay? And we want to do it in efficient way. So, first of all, you realize that the rank m positive semi definite matrices can be written as correlation matrices between variables x having n components. Okay? This immediately gives you a matrix which has one along the diagonals is a positive semi definite. And so you just have to work in this reduced phase of matrices where m now is a parameter. You can set it very large, very small, and I will show you that you can play with that, okay? And then, so you try to maximize this quantity, which is nothing but this color product between vectors of m components subject to this constraint. And we do it in the simplest possible way, as physicists will do it. So zero temporal dynamics.
00:18:20.024 - 00:19:23.284, Speaker A: So at each step we take one variable and we align to the local field. In this way, we are doing steps that at any steps we improve, we increase this quantity. Okay, finally, once you find the maximizer, which is now is a vector of n m component variables, you compute the empirical covariance matrix, you compute the principal eigenvector of this matrix. The idea is that in the solution, most of these vectors. So suppose that you have, let me say, a cluster which is very much connected in this part, and these are the two communities. The idea is that once you find the optimizer, most probably most of these m component spins are roughly aligned in this part and roughly aligned in the other part. Some spin in between is not aligned.
00:19:23.284 - 00:20:27.764, Speaker A: Okay? So once you see this blue configuration, you want to understand what is the principal direction along which you want to project the eigenvector. So you compute the empirical covariance matrix that gives you a direction, which is this one, and then you project and you put all these to minus one and all these two plus one. Okay, so this is the idea. And if you want to play with the code, you can find here a publicly available code, which I hope is clear enough to be used. Okay, so obviously this algorithm depends on the parameter m. So we are working with spin with m components, and in principle, we obtain the SDP convex objective function only when m is equal to n, while if we stick m equal to one, we have the original maximum likelihood problem with discrete variables. Okay, so you see that the same function changes drastically when you change this parameter m.
00:20:27.764 - 00:21:08.298, Speaker A: For m equal one, you have a very rough objective function, which leads you to an NP hard problem. For m n, you have a convex objective function. But actually you don't need to go to m n, because already for m larger than square root of n, you can prove that there are no local maxima. In particular, recently, Andrea also showed that for any m value, all the local minima are close enough to the global minima by a factor one over square root of m. So the idea is that even for m small, this algorithm may work very well. Okay, because the function you want to optimize is no longer so rough. So maybe you can optimize it easily.
00:21:08.298 - 00:21:48.500, Speaker A: And still the algorithm is much faster because it's linear in M, the running times is very mild in the size of the system. So let's see how it works. Now, I'm taking a stochastic block model, various parts very close to the theoretical threshold and with no clicks at all. And the red line is the expected best overlap that you can achieve via bias, optimal because it has been computed via the beta. Okay, so this is the best you can do in this problem. And you can see that if the.
00:21:48.532 - 00:21:51.764, Speaker C: Best you can do should be obtained with BP.
00:21:51.884 - 00:22:31.738, Speaker A: Okay, I'm lazy. I run bet, because according to your paper, it's very close to the optima. You are right. So maybe the optima is slightly above exactly where we go with this data. Indeed. Okay, so this is the result of a typical spectral method, the most economic, to use the one that I use. Now, if M is equal one or two, you see that during the time of the algorithm, you are not able to achieve the optimum.
00:22:31.738 - 00:22:57.992, Speaker A: You get stuck in some local maxima. But if you increase m, the convergence to the global maximum is faster and faster, but you don't need to go to M very large, like here, this is 64 even for m equals six, you approach very closely the global maximum. So you see that this objective function already for rather small values of m is very smooth.
00:22:58.168 - 00:23:06.084, Speaker D: Just out of curiosity, if you do m equal one but not zero temperature. If you do simulate it and unique, where would you be like on this figure.
00:23:08.864 - 00:23:32.830, Speaker A: At this value of lambda? I don't know, because we are quite close to the critical point. I never run similar meaning on this problem, certainly better than this, because this geother denier. So this is the worst you can do. By version you will grow a little bit, but exactly where you go. I haven't run it, so I cannot answer.
00:23:33.022 - 00:23:36.374, Speaker C: So here is time normalized by the number of vertices or not?
00:23:36.454 - 00:23:45.394, Speaker A: Yes, so essentially here I updated 1000 times each vertex, but I will show you also actual times. And it's very fast.
00:23:46.574 - 00:23:53.594, Speaker D: Do you have a sense of the asymptotic number of dimensions that would be sufficient to get close to the optimum? Like should m scale like?
00:23:56.794 - 00:25:05.364, Speaker A: Well, here you see that for this size, square root of n is already 200, but we have a very good result even for m equal eight. So it's clearly not scaling with n. What I can show you is that if you run this with different size of the system, essentially the values of m that you need don't scale with nice, so they are really finite, or maybe scale like log n, but it's really enough to have very small values of m to the algorithm. Work perfectly well. And the good news is that it's very robust. So now we add ten to the minus two fraction of clique in the way I explain you. And while, sorry, standard spectral metal are not able to, to detect the more hidden partition, essentially you see that the two curves obtained for m equals 16 and 16 and 64 essentially don't depend on the value of alpha, so it's practically insensitive to the newly added noise.
00:25:05.364 - 00:25:47.804, Speaker A: A monomer is extremely fast. If you run it on this laptop for a size ten to the five, you need a few seconds. So you can solve a convex optimization problem with a million of variables in few seconds on a laptop. This is why we don't compute any gradient, we don't compute any session, we just do single variable update. And these are extremely fast. Okay, now let's go to the second part of the talk. I would like to convince you this SDP based algorithm is fast, robust, and now I want to convince you that it's quasi optimal.
00:25:47.804 - 00:25:58.224, Speaker A: So if we run it on a random ensemble of problems where we know where is the optimality? I would like to convince you that we are very close to the optimality.
00:25:59.644 - 00:26:15.216, Speaker C: How is destiny using the same that versioning and collaborating. So they also prove some, some robustness, right? You can prove in the paper which paper is.
00:26:15.400 - 00:27:35.194, Speaker A: But, okay, some robustness is kind of obvious in value. It's obvious, right, that if you change epsilon fraction of the edges, value changes at multiple fraction. Yeah, it's an optimization problem. So if you change, but then the value of the objective function, if you change the objective function by epsilon, I cannot change more than epsilon. So what is the main thing that we like to prove is that it exists a threshold for this algorithm, for SDP, algorithm between a phase where essentially you cannot detect anything, to a phase where you can detect something. And actually, in our paper, we proved this first in a simpler synchronization problem, where the matrix y that you observe is dense and is made by summing a rank one matrix with a random matrix, which is the noise, and lambda represents the signal to noise ratio, depending whether x are real or complex. According the w matrix is filled with gaussian or complex gaussian numbers.
00:27:35.194 - 00:28:39.432, Speaker A: And from the observation y, you would like to infer the signal x. And you can do it using different statistical estimators. So you can do bias optimal by marginalizing over the posterior, you can use maximum likelihood by computing the arcmax of this function, this quadratic function. Or you can do SDP, which is what I just told you. So find the matrix x such that it maximize the linear function trace xy in the cone of semi definitely positive semi definite matrices. The nice thing is that, okay, and you can convert this again as a problem of maximization of a new function where the variables x now lives in a m dimensional space. The good thing is that all these can be plugged in a single unified framework in statistical mechanics.
00:28:39.432 - 00:29:53.244, Speaker A: Okay, so essentially we study just one statistical mechanics measure, and from this, taking different limits, we can recover all different statistical estimator. Okay? So the goal now is to understand what happened to this measure, and from this we can infer the outcome of different statistical inferences. Okay? And without surprise, we get the Shelton Kilpatrick model that is always under the gun. So in particular for is invariable. When you study this function here for real variables is equivalent to study this Hamiltonian, which is the Schernhpatrick, where the couplings have a non zero mean proportional to the signal to noise ratio, and a variance which is one over n. Okay? So the idea is that here the signal is the magnetization. So if the, if the model, if the measure has a non zero magnetization, then it means that by that measure you can identify the hidden partition.
00:29:53.244 - 00:30:33.722, Speaker A: For example, in the temperature lambda phase diagram, you have a phase, which is the paramagnet where you cannot identify anything. Indeed, it happens for small lambda, you have a phase ferromagnet where the magnetization non zero. And so you can identify the inner partition. You have a phase, the spin glass that you want to avoid because it's mp hard. Okay, now the bias line passes here. Actually, the full phase diagram should consider the breaking of the replica symmetry and what we know from the solution of the Sheridan kipatic model. So we don't have to solve.
00:30:33.722 - 00:31:23.628, Speaker A: Again, a model is that we have a phase transition in lambda lambda critical equal to one. But while in the bias case, it happened between two replicasymmetric phases, so we can sample easily the measure at zero temporal, which corresponds to the maximum likelihood estimator. The phase transition happened between two phases, which are replicasymmetry breaking. So it's unlikely that we are able to sample correctly the measure in a polynomial time. Okay. And so this makes the maximum likelihood estimator np hard. Okay, so we solved this model in the dense case, in the dense case, by these answers, essentially.
00:31:23.628 - 00:32:08.064, Speaker A: So the main answer, it works like this. It says each variable locally fills a local magnetic field, xi, that changes from side to side. So xi is a random variable, is a gaussian random variable, and moreover, he has the correlation. The local correlation are controlled by a matrix ci, because now locally you have a m component variable. So the local correlation are controlled by an m times m matrix. And what you find, essentially, is that in the thermodynamic limit, this quantity does not fluctuate. So it can be described by a unique matrix c.
00:32:08.064 - 00:32:53.524, Speaker A: And this quantity is a gaussian vector that is fully described by the mean vector mu and by the covariance matrix q. And so you finally write self consistency equation for these three objects, a vector and two matrices, and you solve these different values of lambda and m, and you find the solution. And then the solution looks like this. I'm plotting here the mean square error. So the expected value of the. Well, the distance is minimized over the symmetries, but is the distance between the infer partition and the original partition. And you see that for lambda smaller than one, you cannot do better than a random guess.
00:32:53.524 - 00:33:35.244, Speaker A: And then at lambda equal one, you have a phase transition and the mean square error decreases. And the blue line is what we get by SDP, which is quite close to the bias optimal, which is the red dashed line, and definitely much better than PCA, which is the blue dashed line. Because at large lambda values, PCA is not exploiting the fact that the solution must be on the integers. So it even looks, it finds a solution of the vertices. Okay. And the points are numerical simulation that you see that already. For not so large sizes are very close to the dynamic limit.
00:33:35.244 - 00:33:37.380, Speaker A: So it works quite well.
00:33:37.532 - 00:33:45.084, Speaker B: So we're a step or two away from the block model here. Should I think of this as these analytic calculations are in the dense limit of.
00:33:45.124 - 00:34:14.980, Speaker A: Yes, dense limit. I will show you the stochastic block model in a, in a minute. Okay, so now stochastic block model, okay, this is the answer to your question. We did two things. First, we run the SDP based algorithm. So we take the algorithm, we run it for a very large value of m. So like if we are really doing SDP on the cone of positive semi definite matrices, if M is large enough, there is no dependence on mix.
00:34:14.980 - 00:35:03.657, Speaker A: But since this is quite heavy to do, because we know that the algorithm is very fast with very small m. But if you take m 100, well, it takes some time. Then we also propose an approximate ansatz, which is exact in the large d limit and still provides a quite good answer also for finite d. So I will show you the result of these two. So the result for the stochastic block model with d equal five and this lambda, then signal to noise ratio. So the idea is that up to a certain critical value that we have to compute, the overlap is zero, then becomes different from zero. And the points are what we get running the algorithm, the actual algorithm, and the black line is the approximate analytical solution.
00:35:03.657 - 00:36:04.710, Speaker A: So I would say that is approximate very well, at least here then close to criticality. We have to be careful because now let's try to compute better the critical point. How do we do? Well, essentially we run the algorithm, the actual algorithm, for different sizes, for many, many samples, because as you know, who did simulation of the disordered system, there are huge sample to sample fluctuations. If you want to achieve a very high precision, you have to run a huge number of samples. And this is a quantity called Bender cumulant that must cross at the critical point. So you see they are indeed crossing. And if you zoom and you run even better simulation with larger sizes, you see that the crossing of the actual behavior of the algorithm is very close to the number we get from the approximate analytical solution that I, I will describe you in the next slide.
00:36:04.710 - 00:36:30.038, Speaker A: And so these our best estimates for the algorithmic threshold for SDP run on stochastic block model with mean degree five, which doesn't mean that he's able to find the hidden partition, as long as lambda is larger than this value, which is less than 2% worse than the optimal one, which is one.
00:36:30.206 - 00:36:33.030, Speaker C: How sensitive is this to d? If I take d to b one.
00:36:33.062 - 00:37:04.232, Speaker A: I will show you the plot. I already have the answer to your question. I hope so. Now they approximate analytical solution. If you want to solve this problem in principle, you want to solve the, let me say the Bp equation for this problem, at least at the replicasymmetric level. The problem is that we are working with the variables living on a m dimensional sphere. Okay, so you would like to parameterize with few numbers, as in a good mean field approximation, the measure of a sphere.
00:37:04.232 - 00:38:14.288, Speaker A: This is not easy. Okay, and so I'm writing here the most general answers we use that already contains quite a good number of parameters. And the idea is the following, since we are interested in the situation where the Om symmetry gets broken along one direction, because if, if we identify the signal, the symmetry gets broken immediately from the beginning, we take the variables x and we divide into two sectors, one variable s along the direction where the symmetry will get broken, and an orthogonal vector in m one minus one dimensional space, which looks like the random part, the random orthogonal part. And then on s we use, well, always the exponential. We use a quadratic function, and on the orthogonal part we use the same answers we use in the dense case. So essentially the measure depends only on the scalar product with a random vector. And then we try to close, and the random vector is a gaussian with this distribution.
00:38:14.288 - 00:38:58.904, Speaker A: And then we try to close on the number c, h and r, which are our three parameter, three parameter per side. Okay, this answer should be correcting the two limit in the two extrema limit, high degree and degree close to the, to the percolation threshold. Then you take this answer, you plug into the cavity method, and you derive self consistency equation. You solve by coordination dynamics, and you compute the a mean overlap with the true partition function. And what you get is something like this. So it's zero up to here and different from zero. So it works quite well, but even better.
00:38:58.904 - 00:39:42.054, Speaker A: Now, since I would like to understand that when this becomes different from zero, I can do some kind of linear approximation, because here all these parameters are small, some of these parameters are small. And so if you do linear approximation in h, which is the quantity which is small, you can convince yourself that the r, which is the term of the quadratic term, is zero. And so you end up with simpler equations, which are linear. And so you can try to understand when this linear equation becomes unstable, which corresponds to the critical point. And doing this well, this is the solution. I will show you just this. So this, the rate actually is this quantity here.
00:39:42.054 - 00:40:35.868, Speaker A: So it's telling you the rate of growing of the linear part. So as long as it is zero, the symmetric fixed point where the OM symmetry has not been broken is stable. Beyond this point, the OM symmetry gets spontaneous broken. So this is the critical point for the appearance of a nonzero magnetization in the direction of the hidden partition. And so these are analytical approximate estimate of the critical point. You can compare the red points, which is what we compute running the actual algorithm with a black curve, which is our estimates through the analytical approximately solution. And there too are quite close, considering that for large d, they must coincide.
00:40:35.868 - 00:41:25.194, Speaker A: So here everything is going fine, and here they are quite close. And most importantly, with respect to the optimal threshold, which is one, we are always less than 2% off. So this algorithm for the stochastic block model can recognize the hidden partition in the worst case, when lambda is larger than this curve, which is not more than 2% towards the optimal value. Okay, I think that it's time to finish, and so let me just recap the take home messages. So, these are especially for physicists. So I'm thinking what I would like to know two years ago, before starting working on this. So, SDP relaxation are extremely effective.
00:41:25.194 - 00:42:07.714, Speaker A: I didn't know this before as a physicist, and I think that many physicists are not aware of this. They are robust, quasi optimal, and they may also outperform spectral relaxation. In many cases, even better than SDP are SDP inspired algorithms. So if instead of using m equal to infinity, use small m, this is even better. And obviously, doing the theory for that maybe is harder, but for practical application is extremely powerful. Finally, another take home message for physicists. We have to study these models with m component variables, because we know much, much less than the easing models.
00:42:07.714 - 00:42:20.334, Speaker A: But they have a different physics, and most probably they will provide us with better algorithms. So, okay, it's much harder, but it's worth doing it. And I thank you for your attention.
00:42:25.674 - 00:42:26.694, Speaker C: Questions.
00:42:32.954 - 00:42:34.094, Speaker A: Already, many.
00:42:36.514 - 00:42:53.154, Speaker C: Algorithms with good theoretical guarantees for solving these kind of relaxations that work with the cots and I mentioned are the vectors and where the evolution is. I guess I'm not sure how it's related to your evolution.
00:42:54.614 - 00:43:00.958, Speaker A: I did explain, well, how do we evolve the. So how do we maximize the objective function?
00:43:01.126 - 00:43:08.594, Speaker C: It's a pretty simple evolution, basically continuous time random blocks on the graph. Did you.
00:43:08.934 - 00:44:11.310, Speaker A: No, I don't know if this work, but what we did is very simple updating rule that is very effective because you have to maximize this function in n variables with m component each. So it's not an easy object. The problem is that if you start computing a gradient, it takes quite a lot of time even computing the gradient, even in the spread. So actually what we do is we take, we do the following. At each time you can compute this variable which is interacting with other variables, and each variable has its own spin in an m dimensional. And so all these variables are producing a field on this variable here. And at each time, regardless of the value that this variable has, at this moment, we compute the local field, which is maybe here is something like this.
00:44:11.310 - 00:45:00.584, Speaker A: I don't know if there is a majority pointing up, and we align the variable which is of unitary norm with the field. This is actually very fast. It's like saying we do a coordinate wise ascent, but with the advantage that on each coordinate we already know where is the maximum. So on each step we maximize in one coordinate. When you compute a gradient, then you have to do a step, you don't know where is the maximum, you should compute the Hessian, but computation is impossible because it takes too much time. So if you don't want to computation, then if you use the gradient, you have to do a step, but adaptive step, you don't know where is the maximum. Here you are doing something much simpler.
00:45:00.584 - 00:45:41.502, Speaker A: So you take one coordinate, you maximize, one other coordinate, you maximize. And in steps of this, which is our, let me say one Monte Carlo step is n update of this type, you do better than going along the gradient and it takes more or less the same time because computing one gradient is like doing n of these updates. And so this makes the algorithm extremely fast. I think it's different from what you were saying, but with respect to we program different way of maximizing the function, this one looks really the fastest.
00:45:41.558 - 00:45:45.366, Speaker C: So it's the key feature that you normalize. So once you know the direction, you.
00:45:45.390 - 00:46:09.312, Speaker A: Go in a big step, even if the direction is smaller, we align the variable, the variables are all normal one because otherwise the diagonal of the matrix is not one. So we take the variables which are normal one and we align to the local field. And this is the best you can do. So in this way you are maximizing the objective function coordinate wise, it's block coordinate ascent. I don't know where the block comes from.
00:46:09.408 - 00:46:09.656, Speaker C: Block?
00:46:09.680 - 00:46:19.524, Speaker A: You update m variable? Ah, for that. No, for me it's one variable. It's an m component variable. Now I understand where the blocks come from.
00:46:21.004 - 00:46:25.644, Speaker B: Any other questions? All right, let's thank Federico again.
