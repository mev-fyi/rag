00:00:00.080 - 00:00:23.434, Speaker A: You folks are still going to be showing up, but I don't want to wait too long. So let's get started with the afternoon talks. And we have Kelsey Allen. Thank you, and thanks, everyone, for making it back from lunch. I know it's a little quick. Hopefully, this talk will be intriguing enough that you will feel the rush back was worth it. I'll be talking about physical problem solving in minds and machines.
00:00:23.434 - 00:01:39.756, Speaker A: And in particular, I find physical problem solving so exciting because it's something that we see not just in humans, but actually across the animal kingdom, from octopuses that can use coconuts as shields to protect themselves, to crows that can solve really impressive physical reasoning problems with fluids. But humans, of course, we go even further. Right across our evolutionary history, we've made all kinds of different incredible tools, such as things like stone knives, to hammers by creating new objects we didn't have access to before that allow us to do new kinds of things that we couldn't do before. Across many, many generations, we've refined these kinds of tools to do really completely amazing things. Like, this is an example of an artificial lung that was created by an expert biomedical engineer that's then being implanted in a patient by these expert tool users that are surgeons, using other kinds of tools in order to put this artificial lung into a person to allow them to breathe. But even if many of our tools have been created and passed down through generations, the sort of spark of innovation to repurpose an object as a tool exists in all of us. So this is one of my favorite videos of this very clever young boy who's using the shovel for an unusual, but very important purpose of getting that tasty, out of reach Easter egg.
00:01:39.756 - 00:02:20.264, Speaker A: And he tries both ends of the shovel in order to do so. Right. He's actually making no progress towards his goal, but he is extremely persistent, trying a variety of different hand angles. Now, unfortunately, he is scooped, in this case, by a child with an unfair height advantage. But I'm sure that his cleverness will come back in the future to help him out. So this kind of behavior is very intuitive to us, but it's actually pretty difficult to replicate in machines. And if you look at sort of the broad literature on this, we can sort of classify it into two approaches of learning based approaches that are common in machine learning and deep reinforcement learning, and then structured based approaches that are common in robotics.
00:02:20.264 - 00:03:06.970, Speaker A: Now, these pure learning approaches take, you know, 11,000 years of experience, sometimes in order to come up with policies that can sometimes do reasonable things, but also, even when they're trained on the Internet, as is this case on the bottom here. This is a model called palm e. In this case, all it needs to do is get that red cylinder over to the coffee cup, and it actually goes away from the goal, right. It eventually does kind of get the cylinder into the right area, but it's certainly not doing so efficiently, despite literally consuming the Internet's worth amount of data. So, in order to get more efficient approaches, robotics often bakes in more structure into their systems. And they can do pretty amazing things, like these examples of really effective kinds of tool use. But they fail when the structural.
00:03:06.970 - 00:04:10.264, Speaker A: When the data from the real world violates the structural assumptions of the creators. So, for example, making bad estimates of different material properties in this Boston dynamics case, or assuming very unrealistic things about paper in this top example. And for those kinds of problems, these massive learned models are much more effective. However, animals and humans can strike a balance between these two extremes, right? We can learn behaviors we didn't know how to do before, but it doesn't take us hundreds of thousands of years of experience to do so. Sometimes it only takes one or two failures to get the right idea. So, my research asked this question of how is it that humans, and to a more limited extent, animals, strike this balance between flexibility or adaptivity and efficiency? And can we use these insights to build better machines? So, my research program spans both minds and machines to get at this question. And I look at how to incorporate structures that we know are important from cognitive science with learning in order to achieve this balance.
00:04:10.264 - 00:05:21.070, Speaker A: So, for example, we know that the world is composed of objects, and that these objects exist in relation to one another in the world, which can support us to do things like making analogies. Similarly, physics constrains the way these objects move through space and time, and it also guides both children's and adults inferences about many everyday phenomena. So, to structure the talk, I'm going to first talk a little bit about the experiments that I've done in humans, incorporating both structure and learning in these kinds of ways, and then talk about how that's motivated my work in machine learning for the latter half of the talk. And to start out, what I want to do is just show you this picture of this unusual object. Now, probably most of you in this room can figure out that that's a fork, but you also probably immediately understand that you could not use this fork as usual, right? If you tried to do so, that chain would immediately mean that it would wiggle around in a way that wasn't useful. And how did you know that? Right. You probably introspected and thought about something like what would happen if I picked this fork up in a particular way, in essence, running a sort of noisy simulation about what would happen in that case.
00:05:21.070 - 00:06:01.318, Speaker A: And based on that simulation, you might be able to figure out an alternative way of using that fork, for example, grabbing it just above these tines here as opposed to here. Right. So it allows you to also make sort of rapid updates to the kinds of plans you're considering. So in my PhD, we developed a game to try to capture these same kinds of reasoning processes with more general sorts of tools. This is called the virtual tools game, and it's always a game in which you can select one of three objects on the right hand side here, and just click anywhere in the scene to place it. And as soon as you do, physics is turned on and you get to see the result of your action. The goal is always to just get a red object into some green area.
00:06:01.318 - 00:07:09.502, Speaker A: But the key is that you can only take one action in order to succeed. And if that action is unsuccessful, you have to reset the environment and try again, as opposed to being able to use multiple actions here. So this is an example of a particular participant playing this catapulting level, where they eventually figure out this catapulting strategy in order to get the ball into the goal. So in order to study human behavior in this game, we made a bunch of different levels that cover a variety of different kinds of physical concepts from catapulting. Like I just showed you how to move something without hitting it directly to other levels, like ones that require support where this table is going to fall down unless you put a table leg beneath it, to yet others like hinges, where in this case you need to remove this container lid in order for the ball to roll into the golden. And we wanted to build this game to have a very simple system to study human behavior in that nevertheless has some real challenges. For how to actually solve these, it requires intervention, not on a static system, but on a dynamical system where many things could be moving even before you interact with the world.
00:07:09.502 - 00:07:59.960, Speaker A: There are also many possible actions that you could take. You have to choose one of three tools and then a continuous position in space, which means that there's many possible actions, but there's still a very small solution space. And it requires making predictions over many time steps if you want to be efficient in the number of actions that you actually try through both these complex shapes as well as many different contacts. So to understand the cognitive ingredients of this kind of behavior that we want to see in humans. We built this computational model to try to capture this kind of behavior called the sample simulate update framework. And it's a fairly simple model, but it has a few key components. The first is this sampling mechanism, which, in order to cut down on the total solution space that the model has to consider, we imagine that people are actually thinking about their actions relationally.
00:07:59.960 - 00:08:52.923, Speaker A: That is, they're choosing an object in the scene to try to interact with, and then choosing a way to try to interact with this. And we operationalize this as an object oriented prior shown here in orange. We then imagine that after people are sampling something from that relational prior, they're simulating it with a noisy physical simulator that, for example, has some uncertainty about the way that contacts are going to be resolved. And here, these trajectories are just showing two possible simulations of what the model thinks could happen if that block is dropped into the scene. Based on the outcomes of those simulations, you're just going to update your distribution of which actions are likely to be successful with the simple policy gradient mechanism. And based on whether or not these simulations are successful, you're going to choose to actually try that action in the real world. And if it fails, observe that outcome and again, use that information to update your distribution.
00:08:52.923 - 00:10:05.618, Speaker A: Yes. So, sampling is just essentially picking a dynamic object in the scene, which are shown as any of these objects that have non black color, and then choosing an offset in y uniformly across the entire space, and a gaussian around the bounding box of that object in X. And basically what that means is essentially like, you will interact with that, you will cause that object to move if you drop it. So the point of this model is that now we have a model that's going to be able to play the game, and we're interested in whether or not this is going to capture human behavior. So, at a very coarse level, what we first find is that the model does do a reasonably good job of capturing human behavior across these 20 levels in the game, where here on the y axis, we have human accuracy, which is the total proportion of level of the total proportion of people that solved that given level. So, for example, for table b right here, we had only just over a quarter of participants solve it. But the model similarly struggles with that.
00:10:05.618 - 00:11:05.998, Speaker A: And what we have is a very nice correlation, where what we're finding here is that the number, the the model is doing a reasonably good job of predicting which levels are actually hard for people, both by accuracy and the number of actual attempts that people took to solve it. It also, though, does a pretty nice job of predicting not just these very coarse measures of difficulty, but at a more fine grained level, predicting the actual actions that people are likely to take. So, visualized here in the colored points, each point is an individual participant, with the color being the identity of the tool they chose and the position being where they place that tool, with the top row being the first placements and the bottom row being the last placements. And the model is shown as these sort of blobby background colors. And we were excited to see some similarities in what the model and humans are doing. Like, for example, in this catapulting level, both people and the model are trying two kinds of things. Initially, either trying to hit the ball directly towards the goal, or trying to catapult it towards the goal with one of the three available tools.
00:11:05.998 - 00:11:14.714, Speaker A: But by the last action, all of the model runs, and all of the people have converged on the successful action here, which is catapulting the ball into the goal.
00:11:19.974 - 00:11:23.854, Speaker B: The model is, after you run the RL and all that, the model is.
00:11:23.894 - 00:11:25.234, Speaker A: Basically doing the RL.
00:11:25.564 - 00:11:34.108, Speaker B: I see. So, can you compare the sample complexity of how many trials the humans take versus the.
00:11:34.276 - 00:12:01.664, Speaker A: Yeah, exactly. So that's what's shown here on the. On the left here. So the model attempts here, each of these dots is the number of attempts. Yes, it is, and it's exactly because I'll show you in a minute with the ablations that we have. But it's exactly this relational prior that I mentioned that's cutting down the solution space as well as the mental simulation that's actually making that efficient. It's a very good point.
00:12:01.664 - 00:12:02.040, Speaker A: Yeah.
00:12:02.112 - 00:12:02.704, Speaker B: Kelsey?
00:12:02.784 - 00:12:05.856, Speaker A: Yeah. The subjects, I imagine, will solve a.
00:12:05.880 - 00:12:07.624, Speaker B: Number of these puzzles, one after another.
00:12:07.704 - 00:12:08.104, Speaker A: Yes.
00:12:08.184 - 00:12:10.444, Speaker B: Let me carry some learning from one to the other.
00:12:10.744 - 00:12:13.192, Speaker A: Is the model quite clean each time.
00:12:13.248 - 00:12:16.896, Speaker B: Or is it also learning ensemble?
00:12:17.000 - 00:13:05.364, Speaker A: So, this is super fascinating. People actually don't show learning across the levels of this game, and we think that that's because the actual levels here are fairly unrelated to one another. And what we think people are doing is having to solve what they could be learning across levels. Here is some better estimates of physics, but what we think they're already coming to this with, because we believe in their sort of general, intuitive physics is good enough intuitive physics that they can solve, that they can use that effectively for these levels, and there's not an extra amount of learning to be done. We have some separate work where we have, for example, people play related kinds of levels that all involve containers and things like this. And then you do see learning, and we have a separate model of that that I won't talk about today. Yeah, I think if you told people they had only one shot to do it, that you'd see different.
00:13:05.364 - 00:13:44.102, Speaker A: So I will show you some tantalizing evidence of that. Not exactly that, but hold that question for just three or four slides. Okay. What I wanted to point out just one more thing on this slide, quickly, is some interesting failure modes of this model relative to humans in particular, in this falling a example. So in this particular level, it's very easy to solve because this container is going to start falling immediately. Just place any tool underneath the container, and then when this container falls, it will become destabilized and the ball will roll out. And the model finds that solution really, really quickly because it occupies actually a surprising amount of the space.
00:13:44.102 - 00:14:36.974, Speaker A: But people are really biased to instead try to flip this container over from above. And it takes them much longer to find this destabilization from below, which suggests that people are sort of using more interesting contextual priors than what the model has accounted for. And just to cut off any questions about this, you can see it's not just a general below prior. If you just look to this one on the left, right, because here people were very happy to place something below an object. So it's really something that's more conceptual than just spatial. So we can show that these sort of components of this relational policy prior, the sampling, the simulation and updating mechanism are necessary by comparing different ablations of this model to our human data. And we also just at this time, took our favorite deep reinforcement learning model and tried to have it learn from a lot of randomly generated templates, some sort of generally useful policy.
00:14:36.974 - 00:15:52.504, Speaker A: But if we compare each of these models to our human data in terms of the accuracy on the different levels, we find that the correlation is much worse than this full model. So, for example, without the prior, in general, the model solves far fewer of the different levels than it does when we include that. So now I just want to very briefly talk about this excellent question that was raised on what if you only give people a limited number of attempts to actually solve something? Now, we didn't do exactly that, but instead what we looked at was a population of individuals who we hypothesized might approach physical problem solving differently because they have different embodiments. So in this project, we studied people, both children and adults, who were born with only one or even zero limbs on the same virtual physical problem solving task. But the hypothesis that now, in this virtual world, we've done various controls. The people with one hand and with two hands have the same capacities for action. They're both equally good at cursor control, but it's possible that when they've grown up in an environment where action is more costly, maybe they will treat differently how much to think about a problem and how much to actually act.
00:15:52.504 - 00:16:54.322, Speaker A: And that's exactly what we find. So we found that despite the fact that individuals born with limb differences are just as good at solving the puzzles, overall, the dynamics of how they arrive at solutions differs. And in particular, if you look at the number of attempts to the solution, which I'm showing you here on the left, people born without limb differences are happier to take more attempts. People that have only one arm take significantly fewer actual attempts in the game to succeed, but they spend significantly more time thinking between each attempt, such that the total time to the solution is actually equal between these two groups, but the dynamics of how they approach that differ. And so our takeaway here was that people with limb differences learn to rely more on actually thinking and simulation for physical planning, and that this may be learned at an early age based on their real world experiences. So that concludes the sort of cognitive part of the talk. And there are sort of two big takeaways from this that I want you to have in mind.
00:16:54.322 - 00:17:46.683, Speaker A: One is that mental simulation is crucial. Even if noisy, it has big impacts on the amount of time that it takes you to actually solve these kinds of problems. But mental simulation on its own is not enough for action. We required also these sort of relational action priors and good ways of using those simulators in order to be successful at planning. So, in the next part of the talk, I'm going to talk to you about some work that I've done with my collaborators at DeepMind to basically investigate these questions of how could we then learn mental simulation based on data? And that's something that we sort of saw earlier as well. And then how do we actually use that to do different kinds of tasks that involve action? So, we'll first start with this mental simulation component. Yeah, it's just a simple, like, mixture of Gaussians prior.
00:17:46.683 - 00:18:42.304, Speaker A: But the the thing I actually want you to take away is that it's a relational one. So choosing to interact with an object, as opposed to just acting in continuous space, that's the thing to keep in mind. So, first, I want to give some motivation for how we're thinking about learning these different mental simulation models. And in particular, we're taking inspiration, as I think a lot of us do, from infant learning, and in particular, as basically as young as we can measure. It seems that infants have notions of objects, but also notions of contact. But the specific sort of detailed understanding of physics, such as, for example, the fact that this object will not be stably supported by this one because of its shape here, doesn't seem to emerge until later. So there are some things that seem almost innate, but it's not all of physics, right? Some amount of learning is still required.
00:18:42.304 - 00:19:44.304, Speaker A: And so, taking inspiration from that, a lot of my work on learning mental simulation models uses a class of models called graph neural networks. And graph neural networks are very nice because they basically bake in principles of objects and relations between objects, but then try to learn the rest of physics from data. And they include, by representing physics in this way, they have a lot of nice inductive biases, such as spatial equivariance, assuming that you have pairwise interactions, assuming that interactions are local rather than global. And this can lead to some very nice properties that I will show you in just a little bit. So these graphs, network simulators, seem to work really, really well for a variety of problems. For example, there's some very nice examples of cloth and rigid interaction coming from my collaborator, Toby Faff. And they assume that basically the way you represent objects is as these connected meshes, these triangulated meshes.
00:19:44.304 - 00:20:25.324, Speaker A: And the way you represent interactions between objects is that you connect the nodes of those meshes. Here. You're seeing that as all of these different red lines with the ball being the yellow nodes here and the cloth being the blue ones here. Now, that seems like a reasonably nice way to represent objects and their interactions, but it can actually have unintended effects that are not so physical when you actually try to train these on data. For example, here, if you stretch your cloth significantly, then these triangles become very, very large and no longer connected between these different nodes, with the ball coming through here. Like, imagine this is a gigantic triangle here. And so you can have these sort of unexpected penetrations.
00:20:25.324 - 00:21:17.284, Speaker A: On the other hand, if you look at what people have done in graphics and in engineering, they've built these really amazing analytic simulators that instead assume that contacts are happening between the surfaces of objects, between the actual triangles themselves, instead of the nodes that make up those triangles. And you get extremely nice predictions here. There's never any penetration. They're very realistic. But if you try to actually fit these to real world data, you sometimes struggle to do so. So even for this extremely simple case where here you're just tossing a gray cube onto the ground, which is shown as the gray re rendered thing. Here, if you try to fit all of the parameters of the simulator to that exact cube, you're still not doing a particularly good job of predicting how it's rotating through time, just due to the challenges of representing these contacts.
00:21:17.284 - 00:22:35.204, Speaker A: So in this project, we're trying to take some of the insights from these simulators, these analytic simulators in graphics and engineering, but combine them with our knowledge in graphnets to get more realistic and learnable kinds of mental simulation models. I just want to illustrate that a little bit more concretely with a very simple example of just a yellow polyhedra here interacting with this blue cube. Now, if we just represented these objects as triangulated meshes and represented their interactions as connections between these vertices, then what we're going to do is connect everything within this red circle. And in order to represent an interaction, if this red circle is this size, then for something like a cube, you're not actually going to make any connections between this yellow polyhedra and this blue cube, meaning that you won't be able to resolve how these two things should interact over time. You could fix this by basically considering a much larger radius by which to connect these two objects. But if you do so, you're going to end, you're going to add a huge amount more edges, which is going to end up being really expensive. You could also do something like treat these things as particle based models, but now you have really massive graphs that have lots and lots of nodes and lots of edges, and it's computationally infeasible.
00:22:35.204 - 00:23:37.344, Speaker A: And so instead, in this project, again, we took inspiration from graphics and we make these connections based on the actual triangulated faces themselves rather than the vertices of these objects. And we call this fignet. I'm not going to go into the details of how we set that up just for interest of time, but I wanted to give you a sense of just how well this model can work. So this is just a simple scene of tossing a bunch of objects of different shapes together onto a table. And the learned model is essentially indistinguishable from ground truth here. If we look at that again, I don't know how many people would guess left versus right as ground truth, but we're doing a really excellent job of capturing the dynamics here. If we look quantitatively at the comparison of using this phase based model compared to these alternatives, we see massive differences also in the predicted translation and rotation error by using the sort of, um, these biases from analytic models.
00:23:37.344 - 00:25:01.614, Speaker A: What's also cool is that you can compare these to analytic models, right? And so we did this for this example of the cube that I mentioned before, and we again see that the graphnet, the fignet rollouts here are doing a really nice prediction of the ground truth. But what's even more exciting is that, again, if you just compare it to analytic simulators where we've tried to fit their parameters to the real data, you're getting a much better, much better rotation and positional error than even the best models with the best tuning that you can do with as few as just 32 trajectories. So this goes back to some conversations we've had about how small should n be when we just bake in relational inductive biases. Here we can make n actually reasonably small and still do a really nice job of learning. You can tell a similar kind of story for these pushing dynamics, where here you have a robot pushing one of eleven different shapes along surfaces at different speeds. One more example there, and we can again train our graph neural network models on those trajectories and see their error in both position and rotation compared to our various analytic models. And here these are split over eleven objects, so you only have about ten trajectories per object, and still you're already doing significantly better than the best analytic models that we have available.
00:25:01.614 - 00:26:11.754, Speaker A: So, to summarize just this mini section, uh, it seems like we can get more accurate predictions with these graphnet simulators by baking in these relations and notions of objects than other, uh, learning simulators. And we can also get more accurate predictions than analytic simulators for these real world scenes, suggesting the importance of learning here. Now, this has an obvious limitation. Um, if we wanted to actually apply it as a theory for how children might learn mental simulation models, because we assumed that we had access to all of the 3d shapes and positions of all of these objects at every point in time, which is of course, not realistic. That's not the data that children actually get. So in this next project, I'm going to talk a little bit about how we might actually get towards learning, not from three d states, but instead from sensors. The first thing we might want to do is just say, well, could we convert all of the scenes that we would want into the same kind of representation that we expected we had for the graph neural network? And that was a 3d scene with all the 3d shapes and 3d positions of all the objects.
00:26:11.754 - 00:27:00.624, Speaker A: However, the first struggle comes from just trying to apply object segmentation to real world kinds of scenes. So on the right, this is the segment anything model applied to the scene on the left. And it makes a lot of mistakes, right? For example, in this stack of four fruits here, it's connecting these top two fruits and the bottom two fruits, each as a single object. But even if we had those objects segmented, we don't have correspondences between the frames. Like which object in the first frame corresponds to which object in the second frame. We wouldn't have that for either the objects or the particles. So we instead built a model that doesn't assume access to any kind of correspondences through time that is still going to be able to learn from sensor data.
00:27:00.624 - 00:27:44.150, Speaker A: This model we call visual particle dynamics. And basically the main way to think about it is that it's a way of representing 3d latent space, but which is learned by predicting future images. The big caveat is that what this model requires is access to multiview rgb depths, video input, which is not always available. But the plus side is that you then get this nice 3d representation that you can then do all kinds of different things with that I'm going to talk about next. So if you train this model, you get some very nice behavior. For example, predicting deformable object interactions. This model here was just trained on two deformable objects interacting.
00:27:44.150 - 00:28:42.528, Speaker A: And so the fact that it can then capture two objects deforming as they interact is not surprising, but instead should be reassuring. But because it has this 3d latent structure, we can also do various kinds of editing and generalization. So here on the left, we're generalizing to any novel view because we have the 3d structure, we can just point the camera at a different location and get a new image, despite the fact that this was only trained with three views in order to learn the dynamics. And you can also do things like just add a lot more objects to the scene again, and it does a very reasonable job of generalization. You can also very directly edit the cloud, which we saw earlier in Dan's talk. For the counterfactual world modeling here, you can just do things like delete all of the particles corresponding to the cylinder. And the ball now falls without deforming, which is quite nice.
00:28:42.528 - 00:29:35.734, Speaker A: Or you can, for example, delete the floor. And now these two objects fall together without either one of them deforming at all. And of course, these are really far out of distribution because there's never a training example that doesn't have the floor in it, for example. So to summarize this section, we can learn these sort of 3d editable dynamic simulation models from RGBD sensors, and it didn't require these correspondences across time or object masks or any other kind of privileged information. So I now want to move on and talk for this last section about how to actually use these models for action. And I want to focus on this point, that mental simulation alone is not sufficient. And we're going to switch gears a little bit and talk not about tool use, but now about tool creation.
00:29:35.734 - 00:30:50.546, Speaker A: And tool creation is particularly inspiring to me because it's something that we see a lot in humans that is not as widely seen in the animal kingdom. And it strikes this really massive range of different things that we do with it. So, for example, it's very flexible. We can make a new tool as a table out of just a collection of books, or we can play different kinds of flexible physical reasoning games like happy glasses, where you might have to just draw a tool here in order to reroute fluid into a cup, but then it can also occupy massive scales like irrigation, where we have to transform entire landscapes in order to reroute water. And it can also require extremely high precision, like building an airplane, that the particular shape of these wings is going to matter for how much lift you get. So in this project, what we wanted to understand was whether or not we could create machine learning models that could actually design or innovate new objects. So in the past, people have looked at these design problems with these surrogate reward models, where what they're trying to do is train a model that takes in design parameters, for example, the heat map here of a landscape, and just predicts the reward associated with that.
00:30:50.546 - 00:32:17.554, Speaker A: That then gives you a surrogate model, a surrogate reward model that you could try to use for optimization. But that has some pretty obvious immediate challenges, right? Like for example, if you wanted to change the task where you have the same design input, but now you want to try to reroute the water into three pools, you would just have to train a completely new surrogate reward function, right? There's no way of reusing the one that you used before, so you don't see any generalization across tasks. And as I'm going to show you in a little bit, this also suffers with generalization outside of the training designs. We instead, for this project, took inspiration from how we think people actually design new tools, which we don't think they do by just blindly mapping from designs to rewards, but instead try to use an understanding of physics in order to optimize a shape. So what we did was we pre trained, again these same graphnet simulators on just next step physical prediction with data pretty different from the design tasks. So the graphnet models for the dynamics are trained in just these scenes where you have four random line segments and you just toss a block of water particles into them, and you're trying to predict the next step. But our design tasks, which require optimizing the shape of the black objects to get the blue particles into the yellow region, are then going to require you to design things that have curves, as well as up to 36 different line segments, so significantly more complicated than the data distribution that we use to train the dynamics model.
00:32:17.554 - 00:33:21.860, Speaker A: You can then use the dynamics model for design a test time by simply having a design function that takes in design parameters phi, like the height of all the points in this landscape, as well as the initial conditions alpha, which here are the pipe on the wall, to give you some initial scene. Then run your graphnet simulator forwards in time to get a final state, and then compute the reward on that final state using some parameters, theta, r. Here you can then optimize in the regular way you would optimize by starting with some random design and then refining it based on that reward function until you get something that produces exactly what you want. So this kind of alternative model based approach to design is sort of complementary with approaches in graphics and engineering. The classic ways that you might try to do this previously are to use gradient based optimization methods with analytic models. What this requires is that you have an analytic model that has gradients baked into it, and that can be very expensive to do. Right.
00:33:21.860 - 00:34:14.150, Speaker A: There's actually whole teams at Google that are trying to make differentiable physics models such that you can then use gradient based optimization methods to do very high dimensional design spaces. If you don't want to go through all the trouble of making those differentiable models, you can just use any kind of general learned dynamics function here and use a sampling based optimization procedure. But these sampling based models fail when you have very high dimensional design spaces. And so the approach that we took of taking the gradients through the differentiable graph network simulators allows us to get the best of these both worlds. We can use it for any dynamics that we could train a graphnet simulator to emulate. And it allows us to do high dimensional design by allowing us to still use gradient based optimization methods. If we look at how these perform on our different design tasks.
00:34:14.150 - 00:35:25.714, Speaker A: This is the optimization procedure unfolding for this 2d fluid optimization task, where you need to optimize the shape of the black lines to get the blue fluid into the yellow regions. And compared to the sampling based optimization procedure, it really excels most in places where you have to make a lot of decisions simultaneously about how to move the blue fluid into the yellow region, which you can also see in just the total reward achieved across a variety of randomly sampled tasks within these templates. What's perhaps really surprising about this is so we optimize with respect to the learned model, but then evaluate it in the real dynamics. But you could also directly use the true simulator with just a sampling based optimization proceed uh approach. But that actually still underperforms how well you can do with gradient based optimization under the learned graphnet simulator, suggesting that these gradients really matter for being able to find a solution at all, uh, even if the model is not perfectly accurate. We then wanted to really test the scalability of this approach. So in this particular problem, you now have 625 design dimensions to optimize at the same same time.
00:35:25.714 - 00:36:13.954, Speaker A: And now only the gradient based optimization procedure through the graphnet can handle this at all. If you try to use a sampling based approach here, it's completely unable to solve the task. You can also get really nice speedups relative to analytic simulators. In this particular case, for airflow optimization, there's a lot of very highly engineered solutions, like this defoam differentiable simulator here. And our model, which is just a graph network simulator trained to predict fluid mechanics here, is able to find the same solution that the foam is, but in a fraction of the time. So it's able to solve this problem about 16 to 48 times faster than you could get with the analytic differentiable solver. I'm going to skip this now.
00:36:13.954 - 00:37:23.346, Speaker A: So, in conclusion, the gradients through these graph network simulators can support general purpose design over very complex physical domains. This task agnostic training does a very good job, and it can outperform different surrogate models, which I don't have too much time to talk about. And it can match the accuracy of these specialized solvers for airfoil optimization in less time. So, in summary, what I talked to you about today is that human physical problem solving, or tool use, requires mental simulation. Coupled with these relational policies and people born with both one and two hands, these mental simulation models can be learned as graph neural networks. And they demonstrate these surprising generalization capabilities, which supports even things like being able to innovate new tools by taking advantage of the graphnet gradients in order to do things like airfoil optimization, as well as creating landscapes. And I think this is a really interesting area to be working on at the moment, because there's all kinds of crazy things happening in the world of generative video modeling.
00:37:23.346 - 00:38:16.124, Speaker A: For example, like, Shiri already showed a really nice example from sora of a failure. But this is my particular favorite, where these archaeologists have made the most important discovery ever of completely new physics, which, you know, being able to create a new chair and all of that. And these models do look pixel perfect. If you look at any local region, they look visually amazing, but they're not physics perfect at all. And I think for cognition, it's the physics perfect that actually matters. And so I really want to stress, how do we get to things that actually maybe don't look as pretty, but are useful because they predict correct physics that could support us to do things like make telescopes for this first time or eventually invent an artificial lung. So that's it for me, and I'd like to thank all my amazing collaborators at MIT and DeepMind, and happy to take any questions.
00:38:21.784 - 00:39:11.472, Speaker B: So, I like to talk very much, but I was trying to connect it to our sort of practical experience in robotics, where we in microfiber, trying to use simulation and RL and simulation to train policies and so on. So, going back to your first example, which was very clever, but maybe too clever in the following sense, that a lot is built into that prior. And, I mean, therein lies the rub, right? That human ingenuity as opposed to machine ingenuity. Maybe I'm being unfair, but that's at least how I read it. It's like classic AI. It's human ingenuity rather than machine ingenuity. My other remark was that there are these two practical issues.
00:39:11.472 - 00:39:24.878, Speaker B: I think, when we try to do this for real robotics, one is the dimensionality of the action space. So in your task, it's relatively low dimensional.
00:39:25.016 - 00:39:26.934, Speaker A: Yeah, in the first tasks, yeah.
00:39:27.274 - 00:39:43.138, Speaker B: So two, three dimensional. Whereas when we deal with a robot jointed system or a multi fingered hand, we are going to, you know, 2030 dimensions. And since the complexity grows exponentially, that's one thing which leads us to trouble.
00:39:43.266 - 00:39:43.954, Speaker A: Yeah.
00:39:44.114 - 00:40:08.382, Speaker B: The second remark is that we. There is the centerial gap, which has to do with all the various parameters of physics, which in your setting were relatively few. But when you try to have a robot walk in sand or whatever, we just land up with a lot of parameters of the real world that are not exactly the same as in simulation.
00:40:08.558 - 00:40:09.198, Speaker A: Yes.
00:40:09.326 - 00:40:14.174, Speaker B: I wonder if you have anything to that, or.
00:40:14.254 - 00:41:02.124, Speaker A: Yeah, I definitely have some reactions to that one is. So I wanted to emphasize that for a lot of. So for I'll go first to the high dimensional robot space one, and then second, talk about the real, the sim to real problem. So exactly, this project was trying to get at how to solve the high dimensional action problem. So, like, this particular task is a 625 dimensional problem because you essentially have to choose the height of each point in this landscape. And the point was that if you use gradient based optimization methods, then now you can back propagate through your dynamics model and optimize all of those dimensions at once. So for something like the 30 dimensional.
00:41:02.284 - 00:41:14.886, Speaker B: Robot arm, I mean, you're correct, obviously, because differentiable is better than without difference, you don't have a differentiable thing. And that's what we do most of the time. We don't have access to differentiable simulation.
00:41:14.990 - 00:42:15.148, Speaker A: But the point is that you can get access to differentiable simulation by training a graphnet on your dynamics. So the point here is that any neural network that you would train on your dynamics function is, in theory, differentiable if you want to back propagate gradients through it. The question is whether or not those gradients are useful, which sometimes they are not, and whether or not those gradients lead you to designs that are actually good under the true dynamics. Or if they hallucin, they take you to areas where you don't have good predictive power, and so you hallucinate bad designs. So what we found was that, I didn't talk about this, but, for example, if you use these surrogate models and you also do gradient based optimization through them, they do lead you into these weird hallucinations that basically are like, here's a great design, I promise. And they're actually very, very bad designs when evaluated against a true dynamics model. What's nice about these graphnet simulators is because of the relational inductive bias present inside them.
00:42:15.148 - 00:43:21.264, Speaker A: They don't seem, when you take gradients through them and optimize it to get into regions that are unpredictable. And so, the surprising thing about this paper is that you can train a graphnet model on a general dynamics function, and the gradients through it are both useful and lead you to actually good designs instead of leading you to hallucinations that don't work. So, it'd be interesting to try that, for example, for the robot joint control case, because in theory, we could apply exactly the same procedure there and see what would happen. Does that make sense? Sure. The second question you had was about Sim two real, right? So, for this paper that I talked about, this face interaction graphnet paper, the reason I was so excited about this was because all of this is trained directly on real data. So it's not trained in simulation at all, in fact. And here, these curves, the number of training trajectories are real data samples without any pre training at all.
00:43:21.384 - 00:43:24.324, Speaker B: Yeah, but this is, again, a lower dimensional problem.
00:43:24.744 - 00:43:25.524, Speaker A: Yes.
00:43:26.074 - 00:43:28.934, Speaker B: So it becomes a lower dimensional problem.
00:43:29.994 - 00:44:21.798, Speaker A: Yes. Yes. That is definitely a fair complaint. And one thing that, like, I didn't show here, but we have some evidence of, and I'd be happy to talk about it later, is that you can take these models trained in simulation, like for example, trained on these 3d mesh interactions in simulation, and then you take meshes extracted from a nerf, from a real 3d scene, and run the model on those. And it still produces reasonable rollouts, actually, somewhat shockingly, without any fine tuning at all. So I think that this could be scalable to real environments, but it's a matter of, can you get the data into the right format, which is like this 3d representation, or can you get these rgb depth sensors? Those are the two kinds of ways we're trying to go at it. But I think these are both very important points that are gonna be interesting going forward.
00:44:21.798 - 00:44:25.634, Speaker A: Thank you. Let's do one more question. And Kim, you can get that up.
00:44:28.854 - 00:44:29.886, Speaker C: Yeah, so, interesting work.
00:44:29.910 - 00:44:30.390, Speaker B: So I have a question.
00:44:30.422 - 00:45:01.292, Speaker C: So you refer to these as incommunic, as a follow up, as actions. So, one thing that I'm interested in is using, looking at coupled optimization, where you're optimizing over the design of a system, in this case physical design, but also the control policy or the strategy that controls its motion. So here you might have a physical system that has 10,000 different. There's 10,000 different on top of that to optimize over. Is that something that you've looked at?
00:45:01.428 - 00:45:18.908, Speaker A: So, it's something that we are definitely excited about. We haven't looked at it too much, although there's some focus ground. Have one reason why we were so excited that the gradients go graphics familiars aren't garbage, is that one could imagine actually just also propagating gradients into the control policy.
00:45:18.996 - 00:45:28.332, Speaker C: We've tried some of that. We found issues with these simulators. So getting back to the simdereal gap, the gap is quite huge. And then we end up learning designs that are really not physically realizable or.
00:45:28.348 - 00:45:30.812, Speaker A: That don't generalize for what kinds of.
00:45:30.948 - 00:45:48.444, Speaker C: We've done this for leg and locomotion, soft robots, other things where you're designing sensor arrays and then the computational piece is inference. So, yeah, I have a thousand sensors. I figure out where to place them, and then I'm taking readings from these sensors, and I'm trying to infer a location of an agent or whatever.
00:45:49.064 - 00:45:50.928, Speaker A: And you tried graphne specifically?
00:45:51.016 - 00:45:53.976, Speaker C: We've tried. This is a while ago. We tried these.
00:45:54.080 - 00:46:13.264, Speaker A: I should be really curious to try it again because there's a bunch of tricks in graphne simulation learning that if you don't do them correctly, you basically don't get multiple generalization performance. So it would be interesting to actually talk more and hear about those problems and try it. Yeah. Okay, let's think. Chelsea.
