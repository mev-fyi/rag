00:00:01.560 - 00:00:53.998, Speaker A: Hi, I'm Thomas, and I'm going to be telling you about algorithms with more granular differential privacy guarantees. I'll start by giving some background and motivation, and the high level question is, how do we interpret large privacy parameters? And that's going to lead us to defining per attribute privacy, or partial differential privacy. And then I'm going to tell you about some algorithms that provide per attribute privacy guarantees. I'm going to look at the tasks of kway marginals, histograms, and learning hub spaces, and then we'll wrap up. Now, I hope many of you are familiar with differential privacy, but I'll give a brief introduction. The setting is as follows. We've collected the private data of several people.
00:00:53.998 - 00:01:47.968, Speaker A: We've stuck that through some processing algorithm, and that's produced statistical information which we're going to release to the whole world. Now, the concern is that that statistical information that we made publicly available is going to leak some sensitive details about the people whose data we've collected. And we want to prevent that. And the way we formalize preventing that is differential privacy. So the way we do that is we consider the real world that I just described to you and an ideal world where your data has been completely removed from the input. Now, the ideal and real worlds are going to give you different distributions and outputs, and the guarantee of differential privacy is simply that these output distributions are similar. So any event e, the probability that occurs in the real world is the most e to the epsilon times the probability it occurs on the ideal world plus delta.
00:01:47.968 - 00:02:29.452, Speaker A: Now, I'm going to ignore delta, because that should be very small. But the intuitive interpretation of this is that if there's any bad outcome that you're concerned about, the probability that this occurs in the real world is the most e to the epsilon times the probability occurs in the ideal world. Ideally, of course, the ideal world has a very low probability of that bad outcome occurring. Okay, so how does this interpretation work? Now, if epsilon is 0.1, then that means the probability of a bad event increases by at most a 1.1 factor. Epsilon equals one is a 2.7
00:02:29.452 - 00:03:10.624, Speaker A: factor. At most, epsilon equals three, as at most a 20 fold increase in the probability of a bad outcome. But if we set epsilon larger, things rapidly start to fall apart. So epsilon equals ten means we're looking at a 20,000 fold increase in the probability of that outcome. And once you reach epsilon equals 20, the probability of a bad outcome could increase by a factor of a billion. Now, this is, I want to emphasize that this is not a vacuous guarantee, but it's definitely getting quite hard to interpret this. But despite this, a minority of real world applications of different privacy do consider large values of epsilon greater than ten.
00:03:10.624 - 00:04:19.904, Speaker A: So there's a disconnect here. We're using large values of epsilon, but we don't have a good way to interpret it because we're facing this exponential cliff. Now, there is an informal intuition that I think a lot of people have, which basically says that if your algorithm is, is nice, if the data is natural, and if the adversary or attacker that you're worried about is realistic, then the real privacy guarantee is bent. Instead of this exponential cliff, it's going to be a much more gradual failure as epsilon becomes large. The high level question, of course, is, can we formalize the situation? Now, I think it's important to be able to formalize this and give explicit and robust guarantees because it should be falsifiable. We don't want to be in a situation where we just have a hand wavy intuition that says large epsilons are always good. We should know what the criteria are for when it's okay and when it definitely is not okay.
00:04:19.904 - 00:05:24.864, Speaker A: So what we look at in this work is, what are nice algorithms? Can we formalize this? Can we construct some? So, our contribution specifically is that we're going to design some, some algorithms that satisfy a more granular DP guarantee that we present as a way of formalizing an algorithm being nice. Okay, so what do these nice DP algorithms look like? I think the easiest way to address this question is to flip it around and ask, what do worst case difference private algorithms look like? And the canonical example is randomized response on your most sensitive attribute. So just imagine your most sensitive binary attribute. It could be whether you have some disease or whether you're pregnant, whether you've committed some kind of crime. And now imagine an algorithm that takes that attribute and correctly outputs it. Whoops. With probability one over one plus e to the minus epsilon.
00:05:24.864 - 00:06:03.134, Speaker A: And with probability one over one plus e to the plus epsilon, these two add up to one. It will flip the bit and output the reversed value. This satisfies epsilon differential privacy. But if epsilon becomes large, so epsilon equals ten would mean that you're revealing the sensitive bit with 99.995% accuracy. So, obviously, this is not a good algorithm. And intuitively, what's unnatural about it is that it is spending such a huge amount of privacy budget epsilon equals ten on a single binary attribute.
00:06:03.134 - 00:06:59.002, Speaker A: And this is not nice. And we want to formalize, we want to formulate something that rules this out. That's what leads us to per attribute differential privacy. So we have various generalizations of this definition, but let's just stick with per attribute differential privacy. So an algorithm is going to satisfy epsilon naught per attribute DP if for all pairs of inputs x and x prime that differ only on a single attribute of a single person. The output distributions are similar in the usual sense, that the probability of any event can increase by most either the epsilon or alternative effect. Now, how this differs from the standard definition of differential privacy is that the standard definition allows me to change a single person's data arbitrarily.
00:06:59.002 - 00:07:57.444, Speaker A: Now, we're saying you can only make a small change to a person's data. Now, this intuitively corresponds to making an assumption about the adversary. We're basically saying the adversary is only interested in a few attributes. So a single attribute or a function of a couple of attributes. So this protects against the class of attacks, but it doesn't protect against everything. So for example, it doesn't protect against membership difference, it doesn't protect whether or not you're in the data data set, because that's a function of all of your attributes. That being said, if you have epsilon zero per attribute Dp and you have d attributes, then you have epsilon DP under the standard definition, where epsilon is equal to epsilon naught times D, so you still get a standard Dp guarantee as a corollary of per attribute DP.
00:07:57.444 - 00:09:11.074, Speaker A: Now, the claim I want to make is that if I tell you my algorithm satisfies per attribute DP with epsilon node equals one and there are ten attributes, this is more informative than just telling you that we have a ten DP guarantee. And hopefully there are scenarios where you might be willing to accept this kind of algorithm when you are not comfortable with just a tandy guarantee. So this is giving you more information, and in particular is trying to encode something about how nice the algorithm is. Okay, so now is a good time to situate this work relative to what's been done before. So, in terms of definitions, there are plenty of papers that have given definitions which are more or less equivalent to our definition of per attribute DP, aka partial DP. One thing that we did that is new is that we looked at a concentrated DP variant, partial DP, which I think is important because it gives you better composition. But what I want to emphasize is our contributions on the algorithmic side.
00:09:11.074 - 00:10:09.352, Speaker A: So if you look at the prior work, pretty much all of it looks at some variant of the Laplace mechanism where you just change the sensitivity to match your definition, or rather you change your definition to match your sensitivity. There's also some work that looks at the gaussian mechanism, but overall the algorithmic toolkit in the prior work is quite limited. So that's what we're trying to address in this work, and we're going to present a couple of different algorithms. So just as a startup, we're going to look at the projection mechanism, which gives you average error guarantees for families of query queries. We're also going to look at a variant of the multiplicative weights exponential mechanism, which gives you something more like a max error guarantee over the queries. We're also going to look at histograms, aka heavy hitters and robust half space learning. And the overall message that I want to emphasize about our algorithmic results is that we show a separation.
00:10:09.352 - 00:10:52.214, Speaker A: So we show that there are settings where it's possible to have a small per attribute guarantee, but the per person standard epsilon is going to be large. So this is a setting where having this additional information, a per attribute guarantee, could be useful. Okay, let's start talking about our algorithms. Let's talk about kwa margins. Um, so here our data is a vector of d bits for each person. And let's start with one way marginal. So one way marginals are just the mean of all of the attributes.
00:10:52.214 - 00:11:33.384, Speaker A: Now understand a DP, this is a very easy task, very standard task. We're going to add Laplace noise to each attribute mean. And the scale of Laplace noise needs to scale with the number of attributes by composition under per attribute DP, the scale of them. We also can add Laplace, but the scale doesn't grow with the number of attributes. Now this is a trivial algorithm. We're just exploiting the fact that we're doing per attribute DP and computing per attribute statistics. So where things get a bit more interesting is if we look at kway conjunctions.
00:11:33.384 - 00:12:17.566, Speaker A: So kway conjunction, basically I'm going to pick all sets of size k subsets of attributes, take conjunctions and output the mean of those conjunctions. So now I have a much larger set of queries, and each query can depend on a number of attributes. There's no clear way I can separate them out into per attribute sets of query queries. So a standard algorithm for this is the projection mechanism. It's actually very simple. You add gaussian noise to each of the queries, and then you do a projection step. So you take the noisy answers and you find you reconstruct some data set y.
00:12:17.566 - 00:12:55.372, Speaker A: That is as close, whose answers are as close to those noisy answers as possible, and those projections have greatly decreases the error. And we can analyze this under per attribute DP. Now if you compare the standard DP guarantee here, if you look, we're looking at concentrated DP, but I want to ignore that detail. If you look at the average error over the query, so the number of queries is m. Averaging over all the queries, there's a vector of query answers. The error grows as the minimum of two terms. So the first term is just the guarantee I would get from noise addition.
00:12:55.372 - 00:13:39.336, Speaker A: This grows with the number of queries m. So this is not very good if I have a large family of queries. But the second one, the second term is what I get from the projection mechanism. And here we see that the arrow grows as the square root of the dimension, square root of dimension of epsilon times n under per attributes under per attribute privacy, I can add less noise and then the guarantee the error grows not with d but with k. So instead of the total dimension, I only need to look at the dimension of the queries. So there's a separation here. However, this is just for average error.
00:13:39.336 - 00:14:28.114, Speaker A: I have to average over all the queries. So it could be that 99% of my queries have great error, but why percent have terrible error. So that's why we're interested in also getting guarantees for maximum error. Okay, so let's keep looking at KrA marginals, koa conjunctions, and look at the maximum error. So a standard algorithm for this is the multiplicative weights exponential mechanism, which is based on the earlier private multiplicative weights. The algorithm is simply described as polos. So we maintain a distribution over the data domain and the goal is to make that distribution match the private data set in the sense that it gives answers that are close to the true answers.
00:14:28.114 - 00:15:16.574, Speaker A: What we do is we use the exponential mechanism to select one query, such as there's a big difference between the answer on that distribution we're maintaining and on the private data set. Once we've identified that query, we're going to reweight the distribution using the multiplicative weights update rule. So the distributions answer matches the true answer. And then we're going to repeat. And the nice guarantee of multiplicative weights is that this is going to rapidly converge. And the way we modify this to give per attribute guarantees is that rather than picking one query at a time, we're going to pick multiple queries in parallel. And here is the algorithm in all its gory detail.
00:15:16.574 - 00:16:06.904, Speaker A: I don't want to go through it. One thing to note is that there's a parameter L, which is the number of queries we sample in parallel. If L equals one, we get exactly the standard algorithm, and we're going to set Al to be something like d over k to get a better guarantee. So the, the standard guarantee for multiplicative weights is that the maximum error over all the queries is this expression. If we look at the per attribute privacy guarantee, we basically get to substitute the d with a k. So again, we're going to replace the total dimension with just the dimension of the queries. And that's a big win, because we're often interested in queries with dimension like two or three.
00:16:06.904 - 00:16:43.664, Speaker A: But this is the log d term that appears there. There's an important caveat on this result. So we don't actually get to bound the maximum error, we get to bound the worst case error averaged over a set of our attributed joint queries. So it's an open problem to close that gap. Okay, great. So I've told you about the projection mechanism multiplicative weights, which are algorithms for answering families of queries such as koe marginals. Let's look at histograms.
00:16:43.664 - 00:17:18.164, Speaker A: So I'll give you some background on histograms. So the data is just a set of points in some domain x. We're going to take x to be zero, one to the d. So d attributes, and we're interested in the frequency of a given element y. So the frequency of y is the number of times y occurs in x one three, x ten. Now the histogram is the output. The histogram problem is to output f, sub y, for every y up to some additive error.
00:17:18.164 - 00:18:22.244, Speaker A: The heavy hitters is a closely related problem, basically an equivalent problem where I ask you to output a list of all y's that occur with a certain frequency. These problems are basically equivalent, except that the heavy hitters is asking for a succinct representation. Now this problem is extremely well studied in differential privacy. Most of the time people consider variants of the problem. We have communication or computation constraints, which we're not looking at. But the canonical algorithm, if you're not worried about communication or computation, is just to add Laplace or gaussian noise to each of the frequency counts independently. Now the scale of the noise is one of the epsilon, and it's important to know that this doesn't depend on the demand size is because adding or removing one person only changes the frequency of one element.
00:18:22.244 - 00:18:54.624, Speaker A: Now the maximum error is then just obtained by a union bound. So there's two to the d elements. We take a union bound over two to the D laplace noise samples. We get error d of epsilon. And there's an additional trick we can throw into this. So we can truncate the noise. So if we have delta greater than zero, approximate differential privacy, we can truncate the noise at log one of a delta over epsilon, and then we can get an error bound that's independent of the dimension d.
00:18:54.624 - 00:19:53.758, Speaker A: Now, I've just given you an algorithm which is very good in a sense. People understand that histograms are easy under standard differential privacy, which raises the question, why would you want to consider per attribute differential privacy for an easy problem? Histograms. And the reason I think this is interesting is that the histogram algorithm I just described is, in a sense, a worst case algorithm. It's precisely the kind of algorithm that we don't, we want to rule out with partial DP because it's not nice. And to illustrate why that is the case, let's suppose I'm the adversary, and I'm trying to figure out whether or not your data is included. So, this is the ambition reference question. Now, assuming the histogram is sparse, so there's no collisions, this is just the question of asking, is f sub y one or f sub y zero, where y is your data? Similarly, I might want to know what your secret bit is.
00:19:53.758 - 00:20:45.500, Speaker A: I know that your data is y naught or y one. I'm trying to figure out which. This just works out to. Figuring out is the pair fy zero, fy one 10, or zero one. And the problem is, if the privacy parameter epsilon is greater than one, then the scale of the noise that I'm adding, that the algorithm is adding, is less than one, which means I should be able to distinguish between these. So we actually get this exponential cliff, where as epsilon gets larger than three or ten or whatever, the probability that I can figure out your secret information is very high. And to make matters worse, if I apply this trick of truncating the noise, then this roughly corresponds to saying, when I release your data in the clear with probability delta.
00:20:45.500 - 00:21:38.474, Speaker A: So, histograms are an easy problem, but they're also a worst case problem. Okay, so let me describe our algorithm for heavy hitters, which avoids this worst case, this worst case behavior, by looking at per attribute privacy. So, the key idea is to take the attributes, split them, and then recursively compute heavy hitters on the subsets. So, if you look at the diagram I have here for a table, each row is one person's data. I'm going to split it up based on the columns. I'm going to compute the, recursively compute the heavy headers on the left and the heavy hitters on the right. And now, once I have those heavy hitters, I can concatenate those heavy hitters from the left and the right.
00:21:38.474 - 00:22:25.564, Speaker A: And this is giving me a short list of all the heavy hitters at the top. So I only need to look at the frequency f, sub y for y. That's on my shortlist. So, when we're doing the analysis of this algorithm, this means we only need to do the union bound over the shortlists, rather than over the exponential set of all possible strings. Then there was the privacy analysis. Each attribute is only looked at on the path from the root to the, from the root to the leaf. So we can do a rough privacy analysis, so we can split the privacy budget, epsilon naught to epsilon naught over log d for each label.
00:22:25.564 - 00:23:08.878, Speaker A: If you look at the total size of the short list over the entire tree, this is roughly polynomial in d. So I'll take over union bound over polynomial in d. Laplace noise draws, or each is scale log d over epsilon naught. And that gives me a maximum error of log squared d over epsilon. It turns out we can actually shave that one of those logs by doing some clever accounting, which I won't go into. So we get max error, log d of epsilon. So the separation here is that, understand a dp, the max error is d over epsilon.
00:23:08.878 - 00:23:45.024, Speaker A: This is for pure Dp, so delta and under per attribute Dp, it's log d of epsilon node. So there's an exponential separation here for pure Dp. Okay, so I've told you about three algorithms. Unfortunately, I don't have time to tell you about robust Haas based learning. You'll have to take a look at the paper. So, let's give a, give a summary and wrap up. So, again, the motivation for this work is finding ways to interpret large epsilon by providing some additional information, by breaking it up into a more granular grantee.
00:23:45.024 - 00:24:24.214, Speaker A: And we're trying to capture this intuition that nice algorithms spread their privacy budget of the attributes that are spin up all on a single attribute. This corresponds to making some assumptions about the adversaries. Specifically, they're only interested in one or two attributes. Obviously, there's limitations here. It doesn't protect against membership inference attacks, because that's a function of all your attributes. And in general, the interpretation of this kind of guarantee is context dependent. If you don't want to do context dependent things, you're stuck with standard differential privacy.
00:24:24.214 - 00:25:11.544, Speaker A: And our main technical contribution is that we provide several algorithms that give per attribute differential privacy guarantees for kway marginals, heavy hitters, half spaces. In all these cases, we show there's a separation between the standard per person epsilon and the per attribute epsilon. And I think our work demonstrates that there is something algorithmically interesting that can be done in the per attribute DP world. So lastly, let's talk about some further work. Obviously, the high level question of interpreting privacy guarantees is still open. A couple of thoughts on this. I think whatever justification we come up for tolerating larger epsilons than we can interpret normally should be falsifiable.
00:25:11.544 - 00:25:59.324, Speaker A: It shouldn't just apply everywhere, there should be specific conditions under which that justification works, and we should not be aiming to replace differential privacy with a new definition. It should just be a way of supplementing it with providing additional information. Obviously other direction for further work designing and analyzing more algorithms. I think we only scratched the surface in this work. There's also natural extensions like having different epsilons for different attributes. Obviously not all attributes are equally sensitive, and I think the great question would be to find applications where this kind of formalism actually is providing useful additional information. Thank you very much for listening.
