00:00:02.760 - 00:00:05.438, Speaker A: So in this second part, I'm going.
00:00:05.446 - 00:00:46.374, Speaker B: To be focusing on econometrics in auctions. In particular, this was that branch of the figure where we had continuous actions. But now this table, we will impose more structure that we're in an auction. So I'm going to be focusing primarily in identification estimation in first price auctions, and then I'll briefly talk about other works that go beyond the first price auction setting. So, first price auctions are important. For instance, many of the procurement auctions are first price auctions. And so that's examples where these things have been applied in empirical papers.
00:00:49.274 - 00:00:50.014, Speaker A: So.
00:00:53.374 - 00:01:44.838, Speaker B: What is the goal? We have deployed a sealed bid first price option where people submit a bid, the highest bidder wins and they pay their bid for the item. We'll assume that bidders values for the item are drawn from some distribution f. That's common. And what we observe is the submitted bids for the auction. And we have multiple samples of different options that happened and the bit vectors that were submitted. And we're going to assume that the bits that people submit come from a symmetric based NASA equilibrium. Then the identification question is, if we observe the bid distribution of that symmetric Bayes NASA equilibrium, let's call that g.
00:01:44.838 - 00:02:55.854, Speaker B: So this is the distribution of bits under symmetric Bayes Nas equilibrium, can we reverse engineer and calculate f, the distribution of values? So let's first try to argue that, yes, you can do it. You can reverse engineer, and there's a one to one mapping between f and g. And if I give you g, you can tell me what f is. And that will also give us an approach for estimation. If we have samples from the distribution and don't know exactly the whole distribution. So the symmetric equilibrium, you can write the following equation. If a player imagines that he is some other value z and submits based on the equilibrium strategy of that other value z, so submits a bit of.
00:02:55.894 - 00:02:56.474, Speaker A: S.
00:02:59.454 - 00:03:49.644, Speaker B: Then whenever they win, they're going to pay s of z. Their value is v, so this is their utility whenever they win. And their probability of winning is the probability that all other players have a type that's smaller than z, because it's a monotone symmetric equilibrium. If they have a type smaller than z, they will be bidding also lower than z. And so I'm going to be winning. Now, if this s is an equilibrium, it must be that when I'm maximizing this quantity over here, the value that I'm going to be getting for z is my value v, right? So I'm going to be behaving as if I am my value, when I'm maximizing my utility.
00:03:55.424 - 00:03:58.844, Speaker A: This is your maximizable types that you could be pretending to be.
00:04:10.334 - 00:04:17.954, Speaker C: So, these are the first order conditions in the case that z equals v, that it is our request.
00:04:19.374 - 00:04:36.082, Speaker A: So let's write now the first order condition of this objective and evaluate it at the optimum, which is v. So we take the derivative of this. This is going to be what we get out. We evaluate it at v. It has.
00:04:36.098 - 00:04:44.534, Speaker B: To be equal to zero. And then you can solve for v.
00:04:45.594 - 00:04:50.890, Speaker A: And you get that v is, you can write v as a function of.
00:04:50.922 - 00:04:53.018, Speaker B: The equilibrium bid, the derivative of the.
00:04:53.026 - 00:04:56.802, Speaker A: Equilibrium bead and the distribution of player.
00:04:56.858 - 00:04:58.602, Speaker B: Values, and also the density of the.
00:04:58.618 - 00:05:16.690, Speaker A: Player of that distribution. Now, you can also do a change of variables, say b, which is my bit, is my, is s of v. Sorry, I'm sorry.
00:05:16.842 - 00:05:20.050, Speaker B: Yes, human condition, symmetric equilibrium.
00:05:20.082 - 00:05:31.164, Speaker A: Yes. The distribution of bits, let's write down the density.
00:05:33.104 - 00:06:54.734, Speaker B: Is the probability that a random draw from that s of v, where v is drawn from f is smaller than b. So it's the probability that some random draw v from f is smaller than the inverse of b. So the density, the cdf of the distribution of bits, you can write it as a function of the cdf of the distribution of values and the equilibrium, and the inverse of the equilibrium strategy. Similarly, the density of the distribution of beads. You can take the derivative of this equation here and you can write it as a function of the density of the distribution of values and some property. And the equilibrium strategy. Now, if you go up to this equation over here and make the change of variables, we're going to write v be the inverse of some bit b and s b.
00:06:54.734 - 00:06:56.946, Speaker B: And then the nice thing here is.
00:06:56.970 - 00:06:57.534, Speaker A: That.
00:07:02.594 - 00:08:04.870, Speaker B: S prime divided by f is going to be equal to the density of the distribution of bits. And in the numerator, you're going to have the CDF of the distribution of bits. You have this sort of nice cancellation due to this formula. So what does this say? It says that my value, the hidden value, which is the inverse of the equilibrium strategy for the bid that I observed, can be written in a unique manner as a function of the bid and the observed distributions. So these are the distributions that someone gives me. These are distributions of bids. And now, given those distribution of bids, if you give me a bit of a player, I can give you what your, what their value was, right? That's what this says.
00:08:04.870 - 00:08:56.844, Speaker B: It says that if you give me what, what bit you submitted, and you also gave me what is the equilibrium CDF and PDF. I can then reverse engineer and tell you what that hidden value is. V if that symmetric one at an equilibrium assumption was satisfied. Okay, so because you can do that, then you can essentially estimate the CDF and the PDF of the distribution of values, because for every bit that you observe in the sample, you can reverse engineer what their value was if someone gave you the equilibrium CDF and PDF. And so now you essentially have, if in the population limit, you essentially have samples from the, from the, from values. And then it's a matter of saying I give you samples from values. You can find what the CDF is and the PDF is.
00:08:56.844 - 00:09:11.124, Speaker B: You can also write it down formally based on these mappings. But that's the idea. So that means that the distribution of values is point identified. If someone gives you access to the distribution of bids.
00:09:23.564 - 00:09:25.184, Speaker D: Are you going to discuss, right.
00:09:26.244 - 00:09:27.424, Speaker A: Briefly, yeah.
00:09:29.764 - 00:09:30.504, Speaker E: So.
00:09:35.964 - 00:09:38.304, Speaker A: Then this gives also rise to.
00:09:38.644 - 00:10:15.464, Speaker B: A reasonable estimation strategy. If given that you told me that if I give you the CDF and the PDF of the distribution of bids, then I can roughly get approximate samples of the values of players. What I can do to estimate, I can estimate the empirical CDF. Given empirical distribution of bids. I can estimate an empirical PDF using some standard kernel based estimators.
00:10:20.954 - 00:10:22.922, Speaker A: So this basically sort of counts in.
00:10:22.938 - 00:10:38.894, Speaker B: Some, you know, like stylistically just counts, you know, splits the space into bins and counts how many things are in beams. But it does it in a smooth manner. That's a kernel density estimator. And so this will be consistent at some rates. And then I can.
00:10:42.174 - 00:10:47.474, Speaker C: Out of curiosity, does it matter the kernel what it would be?
00:10:48.814 - 00:10:56.286, Speaker A: So as long as it satisfies some derivative conditions, it achieves the optimal non parametric rates for estimating a PDF.
00:10:56.350 - 00:11:00.114, Speaker C: But yeah, is something that typically people are using.
00:11:02.094 - 00:11:09.994, Speaker A: Yes. You know, then you have also, you need to tune like the bandwidth and things like that. There are auto tuning procedures. This is more of a more standard.
00:11:10.034 - 00:11:13.114, Speaker C: Like which kernel are you using typically?
00:11:13.274 - 00:11:17.174, Speaker A: Oh, you know, people are using, I don't know. We call this a finetnik of kernel.
00:11:18.114 - 00:11:19.614, Speaker B: Different types of kernel.
00:11:20.874 - 00:11:32.498, Speaker A: Even if you just use like a standard like plotting package out there like in Python or are you like, it will have some kernel density estimator in the backend with some optimized bandwidth to give you a nice density function.
00:11:32.546 - 00:11:42.154, Speaker C: So just to be sure, instead of doing a histogram and finding the portion bins, you're trying to find something will interpolate also.
00:11:42.234 - 00:12:00.414, Speaker A: Yeah. You're trying to find a smooth g function, right? Yeah. You're not going to do a step function. So if I understand correctly, s was not known, right, in the s was not you. Did you end up learning s as well in the previous setup?
00:12:02.234 - 00:12:02.974, Speaker B: No.
00:12:05.114 - 00:12:09.126, Speaker A: So you were not told what the equilibrium is, right?
00:12:09.150 - 00:12:10.806, Speaker D: You just observed the distribution.
00:12:10.910 - 00:12:22.702, Speaker A: Yeah, but it's, yeah, exactly. I was only told, yeah, I only observed the distribution. So as one is from here, I.
00:12:22.718 - 00:12:33.450, Speaker C: Think the question would be as equation. It is, you feel that for a different bids, you learn the inverse of the strategy. So you, you can, you could like.
00:12:33.482 - 00:13:06.454, Speaker A: You know, you do get this. So you could try to invert it through. But yes, like, uh. Yeah, yeah, yes. So, yes, so, yes, I mean, you don't use it, you don't argue about like estimation rates for, for, for this per se. But you're saying how accurate is the value that I got from that version? And especially when applying it into the second stage where I calculate the CDF. But yes, this also give you this.
00:13:06.454 - 00:13:10.114, Speaker A: You can empirically estimate this, you can break the estimate that gives you the inverse of that.
00:13:10.454 - 00:13:11.354, Speaker B: And so.
00:13:17.134 - 00:13:29.010, Speaker A: Well, it has to be that the assumption is that it is a symmetric and monotone equilibrium for all of this calculation to be correct. Yeah.
00:13:29.042 - 00:14:00.334, Speaker B: So then given these estimates, capital g hat and small g hat, I can do a plugin, plug them into that formula and get an approximate estimate of the value of a player, which is going to be approximate, because this is not really the correct distribution of bids at equilibrium. It's only an empirical analog of them. And so for every sample of bids in my data, I'm going to get a sample from the values.
00:14:03.954 - 00:14:04.362, Speaker A: And then.
00:14:04.378 - 00:14:26.654, Speaker B: I can define similarly empirical analogs of the CDF of the values by looking at how many values fall below every bit. And also do like a kernel based estimator of the density. So that's the approach proposed in this paper. GPV paper is heavily cited.
00:14:37.514 - 00:14:42.226, Speaker A: It has to be like strictly more than equilibrium. Otherwise even the first formula said not.
00:14:42.250 - 00:14:45.262, Speaker C: Being below makes me wonder that if.
00:14:45.278 - 00:15:01.034, Speaker E: You go with a kernel approach to estimate g, you may not end up like, if you have to estimate the g, this is a theorem method. You might end up having a distribution.
00:15:04.734 - 00:15:06.718, Speaker A: I don't think I ever break down everything.
00:15:06.806 - 00:15:07.718, Speaker E: Or are you going to?
00:15:07.806 - 00:15:23.976, Speaker A: I never did. The self consistency. Are you saying the self consistency after you estimate this, you can then compose it with s and see if you get back g. Yes. You never go back and do that. You just do in one part. You never say, okay, now I have a distribution of values.
00:15:23.976 - 00:15:30.364, Speaker A: I also have an approximate estimate of my s. Let me compose them and see if I get GMAT.
00:15:30.944 - 00:15:36.280, Speaker E: I think that the whole formula that you came up with is based on this monotonicity of s function.
00:15:36.352 - 00:15:37.004, Speaker A: Yes.
00:15:37.944 - 00:15:56.364, Speaker E: But then if you end up with estimating g distribution g, and then that does not satisfy monotone has already like some similar condition. So the essence may not be invertible. And I'm just wondering if that's okay with the whole digital result as well as it's.
00:15:58.584 - 00:15:59.324, Speaker A: Not.
00:16:05.424 - 00:16:15.004, Speaker E: For Myerson. Like, then I use like data on like using kernel method to estimate distribution of the beats and then having like a weird shape.
00:16:15.084 - 00:16:31.180, Speaker A: Let me take this offline, because maybe that if you do the calculations here, you might always be getting monitored. But like, let me take it offline. I'm not sure that it will be a problem if you take the derivative what you end up with. And it might be some benign assumption.
00:16:31.212 - 00:16:33.304, Speaker B: On the, on the estimates.
00:16:53.044 - 00:16:58.276, Speaker A: Yeah. Anyway, so maybe by taking the derivatives of this with respect to b, you'll.
00:16:58.300 - 00:17:11.574, Speaker B: See what, what you need from the estimate to be monotone. But I mean, the G's are gonna cancel out. I don't know. Yeah, it's like not too. Yeah. To do online.
00:17:22.234 - 00:17:24.254, Speaker D: Value capital g will appear.
00:17:27.634 - 00:18:00.520, Speaker A: Yeah, I guess it would be fun if you get like a normal mapping beads map. Yeah. Lower beams to higher values, how many counter. And then there's also the question about dates you get. And the paper provides some number of traits, conditions. So if the density of the values has are uniformly bounded continuous derivatives, then.
00:18:00.552 - 00:18:46.904, Speaker B: You can get rates for the density of the distribution of bids that scale slightly worse than the rates that you would get if you were actually observing samples from the values directly. Instead of getting n to the minus r over two r plus one, which is what you would get for density estimation. Nonparametric density estimation in 1D, you get like a slightly worse exponent in the rate. So. And that's because to estimate the density of f, we need the, that depends on the derivative of the estimate of the, of the density of g. And this extra derivative that you're taking is what leads to the worst rates.
00:18:50.244 - 00:18:53.130, Speaker A: I believe it's the sub norm of.
00:18:53.162 - 00:18:59.654, Speaker D: The cumulative of the PDF. The PDF, so point. It's not like a.
00:19:05.834 - 00:19:08.774, Speaker C: So this is not total variation distance.
00:19:09.474 - 00:19:22.574, Speaker A: That's why I think you also have the login in the denominators. You just the worst case difference of the PDF over all the, over the domain. Typically there should be some zero one domain.
00:19:23.754 - 00:19:49.544, Speaker D: Typically for kernel methods, even the constant depends on the actual distribution. So it's like the rates that we get depending on the object. It's not like it's more of a. It's not like the computer science approach, which is like some absolute constant that does not depend on the instance. It's more like, yeah, the order depends on the distribution. You're trying to estimate correctly. This is what happens with kernels.
00:19:50.844 - 00:19:51.624, Speaker A: Yeah.
00:19:55.964 - 00:19:59.612, Speaker D: I mean, unless you know a lot about the derivatives of it, like.
00:19:59.748 - 00:20:26.892, Speaker A: You'Ll get some bounds on those derivatives. Yeah, we assume some further bound on those. You could have a worst case over at last. Okay, I'm gonna. This bills have been work if only the winning bit is observed. But I'm gonna skip that and get to the next, to get to the next section. So, another notable literature is going beyond.
00:20:26.948 - 00:20:42.144, Speaker B: The independent private value that I mentioned. So there's this seminal work of Athen Hale that looks at affiliated private values, but for second price and ascending options, where the winning price and the bidder is observed.
00:20:44.124 - 00:20:46.722, Speaker A: But only asks about the.
00:20:46.738 - 00:20:56.494, Speaker B: Identification question and not the estimation question. So just tells you when is the distribution of values point identified? If I give you the distribution of bids.
00:21:01.554 - 00:21:06.894, Speaker D: I mean, because like even the IPP setting for asymmetric distributions.
00:21:10.874 - 00:21:19.546, Speaker A: No, that is more work than there is an estimation. I think a subset of the authors of GBP have looked at some asymmetric.
00:21:19.610 - 00:21:21.334, Speaker B: But again.
00:21:24.154 - 00:21:28.334, Speaker D: Okay, so your comment here is, even for symmetric, there's only identification.
00:21:29.074 - 00:21:47.442, Speaker A: For current, for common value, private value, there's mostly identification in this paper and estimation strategies. Yeah. Further, because there will be some recent.
00:21:47.498 - 00:21:48.694, Speaker B: More recent papers.
00:21:54.674 - 00:21:58.914, Speaker A: Another paper looks at what happens if I give you, see.
00:21:58.994 - 00:22:36.880, Speaker B: If I look at ascending options, but bids are observed only in increments, which is typical in ascending options. So the price goes up by some minimum increment of say $5 or whatever. And so then you don't really observe, you don't have a continuous third space. Even if my optimal bid is 5.5, I only need to bid either five or ten. And that leads to problems where because of that discontinuity of the bid space, you get partial identification of the private values. You only can only infer a set of cDFs where your true cDF will lie in.
00:22:36.880 - 00:22:38.944, Speaker B: So you don't get point identification.
00:22:39.444 - 00:22:41.036, Speaker A: If you want like a more complete.
00:22:41.140 - 00:22:49.836, Speaker B: Treatment of structure of the current state of structural estimation in auctions, there's this book by Parson Hong that goes over all of these different streams.
00:22:50.020 - 00:22:51.316, Speaker A: What's the IFC?
00:22:51.460 - 00:23:00.304, Speaker B: Independent private values. So some main takeaways from that literature is that clause form solutions of.
00:23:04.544 - 00:23:05.160, Speaker A: Equilibrium.
00:23:05.192 - 00:23:42.606, Speaker B: Bit functions in auctions allows for non parametric identification of the unobserved value distribution. So you can non parametrically, as opposed to the types of estimation strategies we had in the previous part where we made some parametric assumptions on the private socks. Here you can non parametrically identify the private value distribution and there's this sort of easy two stage estimation strategies where you estimate observed bid distributions and then plug them into sort of like an analog of the best response equation and get back the distribution of values.
00:23:42.750 - 00:23:51.954, Speaker D: Do we ever really expect options or keyword options.
00:24:01.334 - 00:24:02.150, Speaker A: Fully independent?
00:24:02.182 - 00:24:02.566, Speaker D: Maybe not.
00:24:02.590 - 00:24:12.382, Speaker A: But maybe if many of the, you could have like observed characteristics of the.
00:24:12.398 - 00:24:15.542, Speaker B: Object and then some things are known and they all shifted, and then the.
00:24:15.558 - 00:24:25.554, Speaker A: Private structure to those values are independent. So you would have common sifters that are known, maybe even to the auctioneer of the quality of the object, and then the other socks are independent.
00:24:25.934 - 00:24:27.942, Speaker B: Like my cost of production if I'm.
00:24:27.958 - 00:24:33.304, Speaker A: In a procurement option, could just be independent for different firms.
00:24:41.404 - 00:25:03.004, Speaker D: Typically used as a model of manufacturing is for non durable goods goods. It's not an oil field. But like, you know, I want to go to the movies, so I'm going to go to the movies and that's over. And tomorrow I'm going to go again. If there's resale value of my ticket to the movie theater, it's a different story.
00:25:06.224 - 00:25:35.546, Speaker F: I guess, you know, one way of thinking about this is if I observe the valuation of my arrival, is that going to tell something about how much I should value, for instance, in advertising options? The idea is that if I acquire a pair of eyeballs is, you know, how much I value those eyeballs is going to depend on how much my diagonal values, those eyeballs. I think, you know, typically the answer is no, right? Unless my rival knows about how much this guy is going to spend on my album.
00:25:35.690 - 00:25:36.138, Speaker A: Right?
00:25:36.226 - 00:25:40.018, Speaker F: So it's not completely, I think, sense.
00:25:40.106 - 00:25:40.498, Speaker A: Yeah.
00:25:40.586 - 00:26:01.214, Speaker D: So for this, like, yes, sponsored search and stuff is a good example of non durable. I see I get the eyeballs and I get some value out of it. But like for example, being on a noisy, if I know that you, you know, you have, you know, you have a good signal that informs my. And the point is they're not symmetrical.
00:26:03.394 - 00:26:39.128, Speaker A: Like in these results, like the one we saw was symmetric. Yeah, I think there are also papers, but. Yeah. And then the rate for estimated density of the body distribution are typically quite slow. So that's the other drawback of these, of these approaches. But look at the uniform convergence rate of density. So in the remainder, I'll go into.
00:26:39.216 - 00:28:19.444, Speaker B: Some, into new paper, into two papers from the computer science literature that tried to tackle some of the drawbacks of the literature so far. The first one is trying to address the issue of these slow rates and say if we really care about understanding, optimizing over a space of mechanisms with respect to revenue, we don't really need to estimate those value distributions and their cdfs and their PDF's. At these slow rates. We can get root end rates for estimating the revenue of any counterfactual auction in that class, and so we can optimize at faster rates. And the second one was a paper we did with Eva and Dennis, where we're trying to relax the equilibrium assumption and see what we can say when you do value inference in online adoptions, subject to the assumption that players use a no regret learning algorithm, connecting it to the topics that were discussed so far in the tutorials. So the first paper aimed to identify a class of auctions such that by observing bits from the equilibrium of one auction, you can infer the equilibrium revenue of any other option in the class in a fast sort of like the fast estimation rate, and such that the class of auctions that we consider contains auctions with high revenue, always for any distribution of values compared to the optimal auction that we could have run even outside of the class.
00:28:22.724 - 00:28:24.404, Speaker A: So the class of auctions that were.
00:28:24.444 - 00:29:54.774, Speaker B: Analyzed were the ones called rank based options. So these are position auctions, where every position has a weight in a decreasing decreasing weight, and bidders are allocated to positions based on their relative rank of their bid. So the kth highest bidder gets allocation xk and pays xk times bk. And the feasibility constraint is that because the amount of allocation I assign to the top tau bidders should not be more than the allocation that's there in the top tao slots. So this is the class of auctions. And it is known that for regular distributions, the best sat rank based auction is a two approximation to the optimal auction. So it achieves at least half of the revenue of the optimal auction for the given distribution of values.
00:29:55.994 - 00:30:09.290, Speaker C: Just to be sure, the best rank is with the meaning is a specific auction model, or with the meaning that if we get the. What does it mean, the best rank?
00:30:09.362 - 00:30:12.012, Speaker A: Well, if I, if I can choose the w's.
00:30:12.178 - 00:30:13.520, Speaker C: Oh, that means.
00:30:13.592 - 00:30:25.016, Speaker A: Okay, so if I can, if I can.
00:30:25.040 - 00:30:33.124, Speaker B: Sorry, if I am constrained to have this visibility, if I can choose. Yeah, okay.
00:30:36.024 - 00:30:42.424, Speaker C: The optimal w's in the auction. That is the meaning of the best rank based auction.
00:30:42.844 - 00:31:07.122, Speaker A: No, I think it's can you explain the ocean? Yes. So I have this slot wnwn wn with these weights. Okay. And bidder submits, I can, what do the weight capture? The weight capture, say the first slot has a probability of click of 0.9.
00:31:07.268 - 00:31:09.954, Speaker B: The second slot has a probability of click of 0.8.
00:31:12.534 - 00:31:25.794, Speaker A: It is arriving. I submit bits and I, in some cases the auction does not look at the actual value of those bits, just looks at the rank. So the top bidder.
00:31:27.854 - 00:31:29.354, Speaker F: This is a paper click.
00:31:30.494 - 00:31:30.870, Speaker A: Yeah.
00:31:30.902 - 00:31:31.566, Speaker D: I mean you.
00:31:31.670 - 00:31:48.374, Speaker A: Yes. Yeah. You should think if you want a story. Yes. But like, it's just a class of options. Yeah. So the top bidder is going to be allocated to these bids based on some.
00:31:51.394 - 00:32:21.334, Speaker B: Is going to be randomly allocated on those slots with some distribution. The second highest bidder is going to be allocated to the slots with some other distribution and so on and so forth. Overall, they're going to get some allocation from these assignments and that allocation is going to be Xk, which is sort of like the total allocation that they get from each of those randomly assigned slots. And Xk is a vector.
00:32:21.454 - 00:32:22.982, Speaker A: No, no, xk is sort of like.
00:32:22.998 - 00:32:28.586, Speaker B: The sum of those allocation probabilities. So you can think of the, this.
00:32:28.610 - 00:32:33.914, Speaker A: Is the sort of like the mechanics of the option, but you can also think of them equivalently as each bidder.
00:32:33.954 - 00:32:46.290, Speaker B: Gets in some total allocation xk and paying first price. So Xk is sort of my total allocations, I'm BK. But because this xk come from randomly.
00:32:46.322 - 00:33:55.244, Speaker A: Assigning bidders to these slots, those allocations have to satisfy these sort of like prefix constraints that I cannot allocate to the top tao bidders more than what is there in the top Tao slots. This is maybe the most confusing statement. Maybe this is the class of options. You just need to look at these last three bullets. But this is sort of like how you implement it. And I think the optimality is not what I said before, it's sort of like a model options. With such visibility constraints, what is the, what optimal revenue could you have achieved? Okay.
00:33:55.244 - 00:34:06.484, Speaker A: And the constraints here is that, you know, like you only look at the round of the bids and you're also allowed to, you're charging first price.
00:34:10.184 - 00:34:24.506, Speaker C: Just to be sure, the allocations are a continuous number between zero and 10 and wi. So that means that it's dividable the item that I am. Okay.
00:34:24.610 - 00:34:28.654, Speaker A: And you can think of it as mechanistically as your probability.
00:34:29.754 - 00:34:37.974, Speaker D: So what is strange about the constraint? It's on the runs. So I here represents the height.
00:34:38.594 - 00:34:39.414, Speaker A: Yeah.
00:34:45.454 - 00:35:15.374, Speaker D: Your typical feasibility constraints on the, what's the overall allocation across all regions? Like why, if I look at the bottom three bullets there, it's hard. I mean the bottom, I guess the first price option because it uses ranks, right?
00:35:29.954 - 00:36:02.114, Speaker E: Or XL, this probability, like this bigger composition. So now the question is that they will give you those probabilities. Can you come up with consistent permutation for those probabilities? So this feasibility constraint ensures that for those probabilities that exist, like ranking and some probabilities.
00:36:13.094 - 00:36:22.502, Speaker A: I can choose any way to assign those slots to bidders. And this is just what comes out of, and the constraint is that these are the assignments, the feasibility constraints, like.
00:36:22.598 - 00:36:25.006, Speaker B: Just an assignment and the space of.
00:36:25.190 - 00:36:42.658, Speaker A: Allocations that come out. How to satisfy this? All I'm doing is ranking people based on their. So every rank based option can be.
00:36:42.666 - 00:36:51.774, Speaker B: Viewed as a new position option with a different set of weights that satisfy these prefix feasibility constraints.
00:36:57.574 - 00:37:05.830, Speaker A: And so every, yeah, like you can.
00:37:05.862 - 00:38:00.314, Speaker B: View every such rank based option as a new position auction, where these are the weights of the position auction, and I'm assigning people to positions and they get w 1 bar or w two bar based on the position that they're allocated to. So there's no randomization. And so the auctioneer's optimization over such modifications to the setting and sat position auctions is also equivalent to running a mixture over k unit auctions, where the probability of running the k unit auction is the difference between this wk bar and wk plus 1 bar. So you can show, after all these manipulations, that all of these rank based auctions that we talked in the previous slide are equivalent to choosing a mixture over k unit auctions.
00:38:01.454 - 00:38:02.314, Speaker A: Pk.
00:38:02.694 - 00:38:58.816, Speaker B: And so I run the kth unit auction with some probability pk. And these pks are things that I can optimize over, subject to the constraints that are coming from the original feasibility constraints. So optimizing over rank based auctions is equivalent to optimizing over the space of randomizations over k unit auctions, where by k unit auction I mean I have k equally valued units, each has an allocation of one, and if you get one of those units, you get an allocation of one. If you don't, you get an allocation of zero, and you pay your bid if you are allocated a unit. So this is the single k unit auction. And so then the question is to calculate the revenue of any rank based auction. It's equivalent to calculating the revenue of any such k unit auction for the distribution of values of the player in the setting.
00:38:58.816 - 00:39:41.404, Speaker B: So we decompose the revenue of any rank based auction into mixtures over revenues of rank based of k unit auctions. And so it suffices to calculate the expected revenue rk of each of those k unit options. So the main question that was asked in that paper is that if I give you one rank based auction and I give you bid from the equilibrium of that rank based auction, can you calculate the revenue of any other such k unit auction, which would then allow you to calculate the counterfactual revenue of any other auction in the class?
00:39:53.784 - 00:39:55.024, Speaker D: It's a first price option.
00:39:55.064 - 00:39:55.264, Speaker A: Right?
00:39:55.304 - 00:39:58.764, Speaker D: So you're not using the truth in all here in the statement.
00:40:03.664 - 00:40:04.564, Speaker A: In what.
00:40:05.264 - 00:40:17.688, Speaker D: I don't mean before. So the announcer, like when you say distribution over unit options, you mean I have a set of allocation rules and price rules.
00:40:17.776 - 00:40:18.208, Speaker A: Yeah.
00:40:18.296 - 00:40:23.444, Speaker D: And I put a distribution over those, I announce a distribution, then people have to bid.
00:40:24.424 - 00:40:24.808, Speaker A: Right.
00:40:24.856 - 00:40:28.248, Speaker D: So how do they bid when facing distribution?
00:40:28.336 - 00:41:45.826, Speaker A: Yes, and we're going to assume that, but the point is to get rid of the slowdown. A quarter, sorry, the slow rates that we were getting for the PDF for this quantity over here, which was RK. So if we don't care about getting the PDF of the values in this model of equilibrium, and we just care about estimating rk, which is the revenue we would have gotten for that whatever distribution of values, if we were to run a k unit option, then we can optimize over that space of options. So the approach that was used is again trying to write the best response conditions, but now does a transformation of variables. And instead of talking about values, we're going to be talking about quantiles of values. So instead of talking about the actual absolute value of a player, we're going to be talking about its quantile, which is equivalent to the rank of a player. If I were to order sort of like the percentile of a player, I mean the top ten percentile, the top 15 percentile, and so on, so forth.
00:41:45.826 - 00:43:07.428, Speaker A: And the reason why this is useful is also because the first, there's like a one to one mapping, and also because the auction only uses ranks and the equilibrium is symmetric. This is only really only what matters, what percentile of the distribution you are. So you can write a similar formula that you don't want to deviate to any other b, and you can arrive after first order conditions, a similar inversion equation equation as the one that we arrived in the analyzing the GPB paper, where the value of the player is their bit at equilibrium. And here the derivative of the bit is roughly connected to the density and the CDF. Now the nice thing is that this one is over here, which is the allocation probability as a function of your quantile, and the delivery of the allocation probability as a function of your quantile.
00:43:07.596 - 00:43:09.476, Speaker B: Are known from the rules of the.
00:43:09.500 - 00:43:49.984, Speaker A: Auction, because the auction only uses rungs and the equilibrium is monotone. These are sort of like quantities at the auctioneer that we know just from the rules of the auction. What is the your probability of allocation if you're in the top ten percentile, which will also mean that you're in the top ten percentile of the bid distribution. And that is just the rules of the auction. So this we don't need to estimate, we can just plug in. And the only thing that remains to estimate from the observed data are BNB prime and the which is sort of like the v of Q. You can think of it as the inverse of the CDF of the distribution of beads.
00:43:49.984 - 00:44:35.824, Speaker A: And b prime is the derivative of that inverse, which will contain both cdfs and densities. So B prime is really the problematic part, because it contains the density of the observed distribution of bits. So if we want to get rid of the slow rates, in some sense, we need to get rid of that derivative, which will be estimable only at slower rates. You can see also these mappings here. BFQ is really the inverse of the CBF. B prime is one of the density. And so this is sort of like an equivalent formulation of the GPV.
00:44:35.824 - 00:44:54.654, Speaker A: So the main message of the paper is that the quantity are kicked depends only on b of q and not on B prime of Q. And this will lead to fast rates. And the reason that you can do that is because the revenue is really.
00:44:54.694 - 00:44:57.194, Speaker B: An expectation over quantiles.
00:45:01.734 - 00:45:17.354, Speaker A: Of quantities that contain the estimated value that we recover. So even though for recovering the actual value of a player, you crucially depend on v prime, when you calculate the expected revenue in another option.
00:45:19.974 - 00:45:20.470, Speaker B: Even though.
00:45:20.502 - 00:46:18.020, Speaker A: V of q enters there, then you covered v, it only enters into an expectation, and that expectation will smoothen out the objective. And by doing some integration by part, you can get rid of the derivative in that quantity. And so it's this smoothening that comes out of the fact that we're looking at an expected revenue quantity and not recovering the actual value of a particular player that gives you the smooth. And this just does this during integration by parts from the ledge. With this, you can get one of the written rates for that revenue arcade. Since BFQ is essentially the same as estimating a CDF, the CDF of the distribution of bits and inverted, of course you need to for the inversion to note mess things up. You need some assumptions about the probability of allocation of your mechanism.
00:46:18.020 - 00:46:42.614, Speaker A: And the first paper required that the mechanism have a strictly monogamous allocation, but that was dropped later on using other tricks in subsequent papers. So that's the main idea, because many times we might care about quantities that are sort of like expected values of hard to estimate quantities, we can get rid of the slower rates and get faster rates.
00:46:44.874 - 00:46:52.746, Speaker C: Out of curiosity, game theoretical change to.
00:46:52.850 - 00:47:00.194, Speaker A: Avoid strict positiveness of the mathematical statistics.
00:47:00.814 - 00:47:06.594, Speaker C: Just because it seems interesting how you make it happen, that it is always.
00:47:15.214 - 00:48:33.454, Speaker A: The main message of the pivot. By isolating the mechanism designed to run based options, we can achieve constant approximation to the optimal revenue, even if we could run any other option, and estimation rates of revenue of each of these options in the class approximates so and so. This allows if you are building a system that optimizes how many options, you can do it with fewer samples. You can do a b testing between options with and succeed with fewer samples. So in the last part, I'm going to be briefly also connecting to the previous tutorials and talking about what if we drop the equilibrium assumption and assume that players play according to some learning algorithm? Can we still do some form of econometrics and reverse engineer the private values? So in this paper we analyze repeated strategic interactions like in an adduction setting, where for a finite time horizon, players are learning over time. We're not going to be looking at this dynamic mark of perfect limit that I mentioned the previous session, but we're going to be looking at no regret learning behavior. So we're going to be assuming that the actions of the players that we observed satisfy the no regress property.
00:48:33.454 - 00:49:16.234, Speaker A: And by now I guess we've seen what no regret means in many tutorials. I'm going to be going fast. So, given that this is the only thing that we assume about the observed data, can we reverse engineer and learn the values of the players? And basically the approach is that these sets of inequalities are going to give you a partial identification of the parameters of the underlying parameters, primarily because you also have this wiggle room Epsilon here, which you don't know exactly what it is, so you don't know exactly what the level of regret of players have.
00:49:16.394 - 00:49:20.106, Speaker B: And so the non regret assumption is.
00:49:20.130 - 00:49:21.386, Speaker A: Basically going to give you a set.
00:49:21.410 - 00:49:32.454, Speaker B: Of inequalities that your true parameter needs to satisfy. And then there might be multiple values theta that satisfy these slack best response constraints.
00:49:36.004 - 00:49:41.828, Speaker F: So just a very fine question about the setting. Is this a repeated interaction with the same set of players?
00:49:41.956 - 00:49:46.812, Speaker A: Yes. Yes. So assuming like a sponsors instruction will be on that same keyboard all the time.
00:49:46.868 - 00:49:52.024, Speaker F: So you learn and you're learning simultaneously on both strategies and values?
00:49:52.404 - 00:50:00.630, Speaker A: Yes, I can assume like that there's a private value that you know that you, at least the advertiser knows the private value, but you don't really need.
00:50:00.772 - 00:50:02.106, Speaker B: For an order grid algorithm to work.
00:50:02.130 - 00:50:10.774, Speaker A: You don't really need to know the value. You just need to get like banded feedback every day to respond. And then you can use bandwidth learning algorithm.
00:50:12.194 - 00:50:14.314, Speaker C: The value stays the same?
00:50:14.474 - 00:50:21.682, Speaker A: Yes, the value stays the same. That is maybe the most restrictive part.
00:50:21.698 - 00:50:29.460, Speaker C: Of this with the meaning that I want to buy something today, if I don't have it today, maybe it's more important for meet tomorrow.
00:50:29.572 - 00:50:43.384, Speaker A: No, this is more like, yeah, like the value for a click for an advertiser remains the same for say period of a week that we can analyze which is the value of someone visiting an average users visiting the webpage and buying something.
00:50:44.804 - 00:50:52.892, Speaker F: So the assumption is that rivals are playing equilibrium conditional on their value or that rivals are playing whatever rivals are playing anything.
00:50:52.948 - 00:51:59.714, Speaker A: As long as I am satisfied with no regular conditions, I'm going to infer your private value. So for different values of Epsilon, this constraints are going to be giving us different sets of parameters that are rationalizable. And then you can basically what? These constraints are going to be giving us a set that is the identified set. And then from samples you can try to estimate that identified set. Well, we are looking at particularly auction settings and say sponsor search auction settings where players have some value per click and the utilities have this quasi linear form where this is your value per click, this is your allocation probability, and this is your payment. Then these identified sets are also going to be taking some nicer forms because you have this, you're trying to identify the rationalizable set for this value bi and you have this quasi linear particular utility. So back then we analyzed nine frequent bid changing advertiser.
00:51:59.714 - 00:53:11.226, Speaker A: Each advertiser has bids on many keywords and studied the data for a period of a week. These were the types of data that we were looking at and we were getting sort of rationalizable sets which for the particular type of utility you can solve is going to be this convex sets. And the question is, okay, if you want to predict the value for a player, what do you do? What is the level of regret that is reasonable? And what point prediction could you use to say perform a prediction in the future. Back then, we proposed that one uses the lowest value of regret. Subsequent work of Noti and Nissan proposed a quantile regret solution to this, to this problem, where they chose some distribution over points in that lower end of the curve. Yeah, some evidence that this could be a reasonable solution concept. We were observing that the regret over time was dropping from one week to the next for the recovered value that we observe.
00:53:11.226 - 00:54:06.090, Speaker A: And if you're sort of saying, okay, so is this a reasonable assumption to people learn based on these no regret rules over time? It's hard to do that just by doing the empirical strategy of identifying recovering the private values. One thing that you could try to do is not only recover private values, but also try to predict future bids from past behavior based on such structural assumptions that people are playing based on a no regret rules. And so in a more recent work, what we did was to simultaneously try to infer both the update rule that they were using in this no regret framework, and also the private parameters in their utility. And particularly, we looked at one of the most widely used, sort of like online learning algorithms. So, online grading descent and its generalizations of the regularized leaders that many people.
00:54:06.282 - 00:54:08.774, Speaker B: Talked about in the previous tutorials.
00:54:10.114 - 00:54:41.360, Speaker A: It's going to be noted that alternative regulators were continuous and concave, which, because of the randomness of the auction, many times they tend to satisfy these properties, at least for a large region of the bids. So this gives you some formulas of how the bids should be looking between two periods of a player. And then using the data, you can infer the parameters of those update rules, which is sort of like the step size and the private value, and use those parameters to extrapolate in the future and predict these.
00:54:41.552 - 00:54:43.844, Speaker D: What is this thing about random step size?
00:54:45.424 - 00:55:04.822, Speaker A: I think one way to view, like fitting a model that fits this data is to say that ETA is, you know, what is the interpretation of the error. If you run this regression, you can view it as some sort of like random step size, or you can also view it as the value changing over time.
00:55:04.878 - 00:55:07.758, Speaker D: This is noisy.
00:55:07.886 - 00:55:17.314, Speaker A: Yeah, this is like, if you're trying to fit the data to this, you won't fit you perfectly, right. So when you run the regression, it's going to be an error. How would that error be coming from?
00:55:17.774 - 00:55:21.878, Speaker B: So that's, that's what we did.
00:55:21.926 - 00:56:13.764, Speaker A: We tried to see how well it predicts. The key place to look at this one over here, where the most important thing is, how does it predict when the test distribution is different from the training distribution, which is the more important part, whereas I was saying these structural methods could be more relevant when you're trying to predict out of sample. So here we try to say let's look at the two periods of two days where like two periods of during the day where the distributions of bits are very different. So day and night were very different. So let's train models on the single, solely on the day data and predict the bids of players on the night data. And over here we were observing that.
00:56:14.944 - 00:56:17.080, Speaker B: For instance, in the series prediction, where.
00:56:17.112 - 00:56:52.344, Speaker A: You try to predict the whole series of bits of the player in the next few days, this is the best you could do with machine learning time series approaches, in particular this profit algorithm for Facebook. And you see that OGD based methods perform even better than the some state of the art time series prediction methods from machine learning. So even though LGD is a very simple parametric model, it was performing better than these more complicated fancy machine learning approaches when there was a covalent shift, and when there wasn't, it was comparable.
00:56:54.644 - 00:56:58.508, Speaker D: Let me so by serious prediction, you need several steps in the future.
00:56:58.596 - 00:57:47.216, Speaker A: Yeah, and step ahead is I see your last bit, I predict your next period. So let me conclude what I, what I think are some potential points of interaction between the TCS community and the structural econometric theory literature, as things that have already been done so far are in terms of objectives like growth and revenue, and combined with approximation bounds. So these are some papers that already been there. So instead of looking at recovering distributions of values, you can just look directly at whatever you're trying to maximize. The other thing is computational complexity. Proposed econometric methods many of the methods that I mentioned, especially in the dynamic game setting, were very inefficient when you try to implement it, many of them.
00:57:47.320 - 00:57:49.864, Speaker B: Use some grid search methods, whether you.
00:57:49.944 - 00:57:58.204, Speaker A: Just search over a grid of parameters in a d dimensional space. So finding more computational efficient alternative estimation procedures.
00:58:01.944 - 00:58:03.004, Speaker E: Many of the.
00:58:06.944 - 00:59:09.816, Speaker A: Econometric methods are not applied to game structures that we have studied exhaustively in the algorithm game tier community, such as routing games or simple auctions. And maybe we can look at different classes of games that we have studied from econometric perspective, especially when game models have a combinatorial flavor like combinatorial options. Econometric theory in auctions for combinatorial options is a pretty open area, and then we can look at computational learning theory techniques. And online learning theory techniques apply to econometrics like recovering distributions with more benign distribution distances and with better finite sample errors, and more generally, looking more at finite sample properties of our estimators, which more what we try to do, maybe in the cold computational learning theory community, but maybe more stylized settings than the.
00:59:09.840 - 00:59:13.244, Speaker B: Ones that have been analyzed in econometric theory papers.
00:59:15.104 - 00:59:16.244, Speaker A: Thank you very much.
00:59:22.944 - 00:59:28.088, Speaker D: We went a bit over time, but let's take questions that you may have.
00:59:28.216 - 00:59:28.924, Speaker A: Yeah.
00:59:31.864 - 00:59:54.280, Speaker F: So I was thinking about conceptually, the last exercise, one way of executing it could have been thinking about the Bayesian updating us learning. So think about agents repeatedly interrupting and doing bayesian updating based on the observed beats. So you're doing something else, right, so you're, and so what's the right way.
00:59:54.312 - 00:59:55.554, Speaker A: Of thinking about these?
00:59:55.664 - 01:00:07.234, Speaker F: You know, one possibility could be I'm thinking about a market in which these learning algorithms are deployed instead of, say, patient.
01:00:09.774 - 01:00:36.416, Speaker A: Yeah, that's because most of these automated building tools are doing some form of online learning algorithms, especially because in these markets, the mechanism is very opaque and the market, the feedback is very opaque. Isn't that I really know all of the, like all of the mechanics of the mechanism, or how am I located here? How am I located there? Many times the feedback is in the form of, you got this many clicks and you have to pay this much. So it looks more like I submit a bid, I get some utility feedback.
01:00:36.560 - 01:00:39.256, Speaker B: And this looks more like an online.
01:00:39.320 - 01:01:06.328, Speaker A: Learning setting where I submit bids and I get back from this black box what utility I derived. And that's exactly where, you know, these are algorithms that were designed for. And so, yeah, it might be hard to form beliefs about exactly what's going on in the bid matching algorithm in the auction, or in the keyword matching and all of these. Or like I was much here, I was informed beliefs about all of these.
01:01:06.416 - 01:01:07.968, Speaker B: So that I can basically update, given.
01:01:08.016 - 01:01:13.920, Speaker A: The price I was charged and the number of clicks, I go, yeah, so.
01:01:13.952 - 01:01:24.294, Speaker F: One thing that you could do with this exercise, you could sort of tap if you have a good feed, that would be sort of evident that indeed no regret learning.
01:01:24.454 - 01:02:07.536, Speaker A: Yeah, that's what we tried to do with a bit prediction. That was one place where you could have, like you have, you're trying to predict, are you predicting better? Fit is always a bit hard to like because you have this partially identified structure. And so you could fit like, it's okay. What fit means when you're in this partially identified setting. The other question is, you know, like, what level of regret are you saying is a good fit or not? Because you can also choose like what level of regret? And that's why we were showing you, like, there were people in most players, you could rationalize the bids with a small level of regret. That was a meaning of fit. And in most players we would see that this smaller, smallest level of regret would drop between two weeks.
01:02:07.536 - 01:02:12.764, Speaker A: So somehow that would be another instance of that their regret is going down.
01:02:14.174 - 01:02:16.234, Speaker B: Those were the figures that I.
01:02:19.574 - 01:02:38.394, Speaker F: Promise. So one natural question would be what you know, if I know that my rival is using particular learning paradigm, can I try to exploit it in an adversarial way, like trying to teach him something that would eventually.
01:02:39.004 - 01:02:49.204, Speaker A: But if you go, I think if you go back to actual equilibria in some sense, like here in the marketplace once, right.
01:02:49.284 - 01:02:58.704, Speaker F: If you iterate infinitely often, then you go back thinking about adversarial attacks in this framework, assuming that, you know, one leader is learning.
01:03:00.484 - 01:03:13.994, Speaker C: May be disconnected with that. I was saying, if you know that everyone else is playing FTRL, but you will not play, is there something extremely better that we can play?
01:03:15.134 - 01:03:48.876, Speaker A: These are good question. I mean, one flavor of such results is like Matt has some results. When people play like that, the auctioneer can manipulate and get a lot of revenue. That would be one style of results. If the auctioneer knows that they're playing exactly these things, I can push them in the right direction and extract maybe all welfare of everything. But here, I guess I'm not saying that people know that the other player is playing a particular type of algorithm. It's like playing any algorithm that could have smaller grid, which could be anything.
01:03:48.876 - 01:04:00.940, Speaker A: So then it's unclear what attack I can do, if that's my knowledge set. Maybe if I know exactly like in the other final setup where I was saying the other player is using gradient descent online game, maybe I can do an attack.
01:04:01.012 - 01:04:02.244, Speaker B: But if all I know is that.
01:04:02.284 - 01:04:05.304, Speaker A: They'Re using some no regret, it's not clear what the attack would be.
01:04:09.044 - 01:04:11.404, Speaker D: All right, let's have it out.
