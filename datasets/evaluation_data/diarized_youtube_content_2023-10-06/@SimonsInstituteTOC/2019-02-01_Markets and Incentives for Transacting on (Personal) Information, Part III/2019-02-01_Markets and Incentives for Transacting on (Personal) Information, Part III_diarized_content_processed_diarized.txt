00:00:00.480 - 00:00:03.214, Speaker A: And we'll turn it over to Katrina.
00:00:03.334 - 00:00:03.678, Speaker B: All right?
00:00:03.726 - 00:00:04.646, Speaker C: Yeah. Thank you, guys.
00:00:04.750 - 00:01:12.764, Speaker B: So this morning we did our whirlwind tour through various works in information and economics. So the plan for this hour is we'll do a little bit of thinking about what makes privacy an interesting thing to buy and sell and provide and quantify its value. Um, then we'll do a different whirlwind tour, but in this case, thinking about work that's been done mostly in the differential privacy community on various economic ideas and the relationship to differential privacy. How do you quantify privacy loss? What's the role of privacy in the context of mechanism, design issues like this? Then I'll do a little bit of the philosophical, less technical effects on society, where probably Helen should be doing that part. And then I have a few more vignettes from economics I want to share towards the end. That's the plan. We have an hour go.
00:01:12.764 - 00:02:18.204, Speaker B: So the first question is, how is data different from sneakers? Economists are really good at thinking about how we should buy, sell price things, what's special about data? And then we'll go on to think about sort of other ways that we can sort of put data in this sort of traditional economic context. So what do I mean when I say good? A good is something that's useful, that you might provide or buy or sell. It can be physical, it could be service. And so let's think about data versus sneakers. So, in no particular order, if you got data, you can make more copies of it, basically for free. If you have sneakers, you still have to pay for the additional work to make more sneakers. Uh, if you have data and you sell it to somebody, it's really, really hard to control its spread thereafter.
00:02:18.204 - 00:02:58.360, Speaker B: With sneakers, it's a little more clear what might happen. If you have data, it can sometimes be very difficult to find the right buyer for that data or to figure out what version of the data they might want. Whereas generally we have pretty effective markets for selling traditional goods. Obviously there are market inefficiencies and issues there, but I think we understand the market space much less for data degradation. With use, this is a tricky one. Sneakers. When you use them, they get older and dirtier, and at some point they become less useful.
00:02:58.360 - 00:03:43.564, Speaker B: Data, when you use it, it gets older and dirtier and becomes less useful, but in a different way. Um, so as you use data, you in some sense exhaust its ability to support meaningful statistical conclusions. Um, and there's been nice work in this space on understanding this. I'll talk a little bit more about this uh, later. Um, but it's an important factor to consider when you are thinking about pricing it or offering access to it. Um, but it's a different kind of degradation. Um, the value of holding data, at least in some contexts, depends strongly on who else has access to the same or similar data.
00:03:43.564 - 00:04:45.124, Speaker B: Um, so if I have data that lets me figure out the right way to price for a market segment, or data that lets me figure out exactly, you know, what's the product I should be selling that's really, really valuable if my competitors don't have access to it, and maybe much less. So if my competitors do sneakers less, the issue, um, the, what do I mean by the curve of marginal value? So I, it's natural to think about if you had a certain number of sneakers, how much would it be worth to you to have another pair of sneakers? Um, and that's what we mean by marginal value, um, with many, but not all goods, we see generally decreasing marginal value. Um, so if you have one or two sneakers, maybe it's good to have a few more. Yeah, I know. Okay, fine. I know. The pears.
00:04:45.124 - 00:04:46.484, Speaker B: I know.
00:04:47.784 - 00:04:48.232, Speaker C: Yeah.
00:04:48.288 - 00:05:42.376, Speaker B: Okay, so, so forget the complementarities for the moment. I know, I just, I don't know why I like sling sneakers so much, but I do. But generally there's a point where you've got enough sneakers, like you're never going to wear, say, pairs of sneakers and you're not going to wear more of them in your lifetime. And so the marginal value kind of goes down, whereas with data, it's a little less clear what's the shape of the curve. So, for a particular thing you might want to do with data, you know, I want to use data in order to distinguish images of cats from images of dogs. Maybe there is a curve that you can draw that goes something like, you know, a few pictures of cats and dogs. Not so useful.
00:05:42.376 - 00:06:21.564, Speaker B: More more more useful. And then boom, like my algorithm actually basically starts working and then more, less useful. I'm not so confident in that curve that I just drew, and it may have some other bumps and weird things going on, but that was for one application. So even suppose I got that curve right for cats and dogs. Now suppose you want to distinguish bicycles from porcupines or do other things with data. There's going to be another curve and it's going to be somehow layered on top of this one in some interesting way. So I think the shape of the curve is not so well understood for data, and I think that's a really interesting and important question.
00:06:21.564 - 00:06:23.524, Speaker B: Um.
00:06:26.024 - 00:06:26.764, Speaker C: The.
00:06:28.464 - 00:07:41.704, Speaker B: One of the trickiest things about transacting on data is it's much more complicated to ask people, how much do I need to pay you for this piece of data about yourself than it is to ask you, how much do I need to pay you for this pair of sneakers? And the reason is that the value of the data, or the harm you might incur in giving it out to somebody may actually depend on the content of the data. So to use the example of being tcs positive, maybe I'm perfectly happy to tell people that piece of information for very little compensation if I'm TCS negative. But if I'm TCS positive, I'm worried that I'm going to suffer harms as a result of people finding this out. And so I want to be compensated more for it. But then sort of traditional mechanisms for discovering how much I should be compensated are harder to deal with because I'm not going to tell you. Then you'll find out that I'm TCS positive just from knowing how much it's worth. So this is something we'll get into as we continue on.
00:07:41.704 - 00:08:39.924, Speaker B: And then one of the other things is that it's pretty clear, for the most part, what people are going to do when they buy a pair of sneakers. Like they're going to wear them or maybe keep them as a collector's item. If people are going to buy data, it's much, much less clear what they're going to be able to do with it, what its future value is going to be, how it's going to be repurposed. And generally, when you're selling something, that price is supposed to reflect all of the future value and uses of that thing that you're selling. But if you can't even comprehend how it's going to be used in the future, it's trickier. Okay, so this is probably missing lots of other interesting ways that data and sneakers are different. But I think this is already basically a list of open questions, um, because for the most part, we don't fully understand how to deal with these differences.
00:08:39.924 - 00:09:05.998, Speaker B: Um, and certainly as you start to take combinations of these interesting aspects of data as a good, um, we become less and less confident in our ability, uh, to do something useful there. Um, so I think you should take this as an invitation to think about sort of these interesting aspects of markets for data. One of the. Sorry.
00:09:06.086 - 00:09:06.754, Speaker C: Yes.
00:09:07.094 - 00:09:21.934, Speaker D: So, I mean, I've seen comparisons like this before where you compare sort of data to a physical good. Have people tried to compare data to other things that are somewhat similar to, like, software.
00:09:22.084 - 00:09:24.106, Speaker C: Yeah, yeah, yeah, yeah.
00:09:24.130 - 00:09:35.906, Speaker B: So, yeah, so there's been some work on digital goods. So some of these issues are better studied, like the ability to make free copies in the digital goods space. Yeah. Issues of resale, protections against resale, stuff like that.
00:09:35.970 - 00:09:36.210, Speaker C: Yeah.
00:09:36.242 - 00:09:47.432, Speaker B: There's a whole literature there. I should have pointed to it. But as you start to mix those concerns with others that are more specific to the privacy considerations, it gets more murky.
00:09:47.618 - 00:09:48.100, Speaker C: Yeah.
00:09:48.172 - 00:10:55.348, Speaker B: Thanks. Other questions here. Okay, so one of the tricky things about talking about pricing privacy or selling data is it's not even immediately clear. What is the thing that you're selling? Are you selling a single use of something? Are you selling indefinite use of something? Resale of it? And then what's the something? You selling a single record or aggregate statistic, a synthetic data set? Are you selling some sort of service that you run on sensitive data? Are you selling the ability to serve information to a targeted audience? There are many different things that one could sell if one had access to sensitive data, and it's not at all clear which are the right ones to be selling. And then once you've picked some of them to be selling, how they should be priced, because of all those considerations about how data's an interesting and difficult thing to sell, and then how the prices should interact with each other.
00:10:55.436 - 00:10:56.064, Speaker C: Yeah.
00:10:57.684 - 00:11:00.424, Speaker E: And now, all of a sudden, we talk about privacy.
00:11:04.584 - 00:11:05.032, Speaker C: Yes.
00:11:05.088 - 00:11:07.164, Speaker B: So, yes.
00:11:08.424 - 00:11:14.160, Speaker E: Are you saying, what is the good of data?
00:11:14.352 - 00:11:31.300, Speaker B: Sorry, what is the good meaning? What is the good in the sense of what is the thing that you're selling? As in the same sense that sneakers are a good that one might sell. So I meant, what is the commodity for sale? Sorry, I couldn't hear what you said in the back. Oh, okay.
00:11:31.332 - 00:11:32.184, Speaker C: Yes, yes.
00:11:32.524 - 00:11:48.548, Speaker B: What is the thing that is for sale? And so I'm saying, regardless of whether you put the word data or privacy here, there's a sort of tricky question of. So start just with data as the first word on the side.
00:11:48.676 - 00:11:49.044, Speaker C: Yeah.
00:11:49.084 - 00:11:54.404, Speaker B: So. And then I ended this transition to privacy. And you want to slow that transition down, which is great.
00:11:54.524 - 00:11:55.184, Speaker C: Yeah.
00:11:55.864 - 00:12:08.284, Speaker E: We've gone through the differences between standard commodities like synthetic data as a commodity, and now we're doing a third parallel, which is privacy as a commodity.
00:12:08.584 - 00:12:52.610, Speaker B: So I thought of this more as. So you can do this slide, this question of what's for sale for data. And I think this slide only gets more sort of complicated and interesting to think about when it's sensitive data or a privacy related service. But maybe I should have separated that into two steps, which is even when we're just talking about data and we haven't started to worry about privacy, there are many functions of data that one could sell access to. And then when that data is personal sensitive and we're thinking about the ability to perhaps introduce some protections as part of the thing that we're selling, then it gets even more complicated. So perhaps that should have been two steps.
00:12:52.722 - 00:12:53.874, Speaker C: Yeah. Thank you.
00:12:53.994 - 00:12:57.306, Speaker F: And you could have other stuff like statistical validity.
00:12:57.410 - 00:13:01.534, Speaker B: Oh, yeah, yeah, yeah. This goes on and on and on, but, yeah.
00:13:02.314 - 00:13:07.218, Speaker C: Oh, yeah, yeah.
00:13:07.306 - 00:13:54.074, Speaker B: That's a great point. Yeah. So you could think about how do you price that as a good. And there's been some work there. And for every comment, I realize that there's a whole literature that we could have spent a week on, but oh, well, next time. So one thing that's, I think, interesting about pricing data is what one might hope that the price would be doing. So if I'm going to pay somebody for access to their data, that payment is potentially doing the work of sort of multiple things at once.
00:13:54.074 - 00:15:14.814, Speaker B: This is kind of interesting. So maybe part of what it's doing is it's compensating them, that person, for the value of that data. Meaning I'm going to use that data to go and market something or make some money down the line, and the value that that data contributed should perhaps be compensated. But perhaps I'm also, when I'm paying somebody for their data, compensating them for some loss of privacy or other rights, and perhaps I'm also ideally giving them some insurance against future harms that might result from the use of the data and maybe other things as well. And so that's a complicated set of things to price and sort of put together all at once. I mean, people do price insurance, people do price things that are very hard to price, like the value of a life, people do price future economic utility of a good. But it's an interesting mix of things to be pricing together.
00:15:14.814 - 00:17:17.054, Speaker B: And so this next slide is in some ways a little bit of a joke, but I mean, sort of the economics 101 way of thinking about sort of the interaction about prices and supply and demand, you quickly see that it's not quite enough of a toolkit, and obviously there's much more out there. But the space that we're dealing with is so complicated that what happens to demand as supply changes in the privacy world? What do these questions even mean? So this is here not as a lesson, but more as a joke that just to point out that there are a lot of things we don't understand with the traditional tools here. So now that I convinced you that there is a lot of work to be done, I do want to talk a little bit about the work that has been done. And so the question is, you know, how do individuals value privacy? Or what is the value to individuals of privacy, of data and various questions sort of in this space and one sort of economist's hat take on this is that, well, if you want to know how or whether people value privacy, we could take a revealed preferences approach. So what does that mean? So revealed preferences means if I want to know what you think about something or how you care about something, I should look at your behavior and use the behavior to judge what your true preferences were. And it's pretty clear that if you take this to an extreme, revealed preferences suggest that, that people don't really care that much about privacy. It's not really worth anything.
00:17:17.054 - 00:18:07.654, Speaker B: There's this nice work of art where the artist made these beautiful cookies. These are actual real cookies that you can eat with interesting and hip sounding flavors. And she handed them out to people in exchange for their photos and addresses and driver's license numbers and phone numbers and mother's maiden name, Social Security number, you name it. She took people's phone, fingerprints, and people were delighted to hand this stuff over in exchange for a cookie. So revealed preferences approach says, yeah, preference is basically worth a cookie. Um, yes, obviously there are problematic things about that interpretation, but it's worth pausing for a minute to think about what's problematic with the revealed preferences approach. You know, why is this not convincing evidence that privacy is basically not of value?
00:18:07.954 - 00:18:14.614, Speaker D: I mean, it's unclear whether people have all the information they needed to make the right.
00:18:15.674 - 00:18:17.778, Speaker B: Sure. Yeah. For example.
00:18:17.826 - 00:18:18.010, Speaker C: Yes.
00:18:18.042 - 00:18:33.274, Speaker B: I mean, so there are many, many reasons, but I think it's worth, we don't need to enumerate them here, but it's worth stopping to think about why do we think it's still worth studying privacy in the, you know, in the president's, in the presence of this evidence?
00:18:33.694 - 00:18:34.474, Speaker C: Yes.
00:18:34.774 - 00:19:04.462, Speaker D: To answer your second question, there was, and this is not a scientific study. Well, it was a news article based on scientific study, I think. But there's a New York Times article recently which is a stated preference study in which they asked a bunch of people, you know, how comfortable are they with giving up their data and return for targeted ads and stuff like that? And the answer was they were not comfortable. So there's a big mismatch between people's revealed and stated preferences, which is why it might be so interesting to study this.
00:19:04.518 - 00:19:04.750, Speaker C: Yeah.
00:19:04.782 - 00:20:35.478, Speaker B: So one of many reasons why it might still be interesting to study this. And so there is a lot of experimental, sort of data based evidence about how people value privacy. I actually really don't have a comprehensive view of this literature, but there are many examples, and I can point you tomorrow. And people do go around running these experiments, trying to figure out in this particular context, exactly how many pennies will people pay to hide their browser history? I think this particular thing is, people were going to download an app, and there were versions of the app that had various types of privacy violations that you had to live with. How much are you willing to pay for the, the more privacy fancy ones? So people do these experiments, lots of them, and they do surveys and they ask about attitudes. And this is something that we're going to learn much, much more about at the workshop that's upcoming this month. And I'm not going to give you in any sense a comprehensive view here, but I'd just like to say that this is something that's obviously potentially a really, really useful input to our work as a community, and it's also something that's obviously really, really difficult to do well and to do in a way where there are conclusions that you can draw that generalize beyond the specific setting that was studied.
00:20:35.478 - 00:21:51.774, Speaker B: And so I don't want to spend a lot of time here, but I think there's, there's obviously a role for doing experimental work in this space and for doing survey work in this space. Um, and I think it's also obviously very, very challenging. Okay, so except for maybe a few minutes towards the end of this section, I'm actually going to focus now not on data driven or experimental evidence, but I'm going to talk now more about sort of theory and models, but back to the differential privacy world. And in this context, there's another survey that I want to point you to. This is a survey that appeared in Sigicom exchanges by Aaron Roth and Malesh Pai on privacy and mechanism design, which at the time did a pretty good job of covering all of the work sort of in the space of differential privacy and a little bit outside of differential privacy, but not much plus economic considerations. So one of the first things they do in this paper is they throw up a definition of differential privacy that doesn't look anything like the definition we saw earlier. Well, okay, looks a little bit like the definition we saw earlier this week, but it takes a little bit of parsing to see why it's equivalent.
00:21:51.774 - 00:22:05.684, Speaker B: So the definition that they show us is that a mechanism is differentially private. And for all pairs of neighboring type vectors, and for all utility functions.
00:22:07.544 - 00:22:07.928, Speaker E: The.
00:22:07.976 - 00:22:45.724, Speaker B: Expected utility that you get under the first type vector world and the expected utility you get in the second type vector world is very close. And so this is sort of a utility theoretic interpretation of the differential privacy definition. It says that one of the ways that you can think about differential privacy is as guaranteeing that if the mechanisms that are going to be run on you are differentially private, then your utility is not going to be terribly degraded. That's a nice interpretation to have.
00:22:45.884 - 00:22:47.864, Speaker F: What do you mean by type vector?
00:22:50.564 - 00:23:12.364, Speaker B: This is now our databases, but this is just sort of putting a database into the economic terminology. So in database land, we think about each person as having information. That's an entry in the database. When you speak economics language, you think about everybody as having a type. It's just through information. It's just the thing that defines who they are in this context.
00:23:13.064 - 00:23:13.936, Speaker C: Yeah.
00:23:14.120 - 00:24:01.294, Speaker B: So the next sort of technical observation about differential privacy has to do with incentives. So for this, you need to have encountered the concept of dominant strategy truthfulness and economics. What is it? So here's a formal definition. We'll look at that in a second. But what does it mean? So, dominant strategy truthfulness of a mechanism or a process that you might participate in means that, well, except for some small. So we're going to talk about approximate dominant strategy truthfulness, except for some small epsilon. Basically, the best thing for you to do in the face of this mechanism is to reveal your true data, no matter what everybody else is doing, no matter how crazy they are.
00:24:01.294 - 00:25:05.992, Speaker B: Your model of what, you know, what they think about privacy dominant strategy truthfulness says the best thing for you to do is to just blindly participate with your true information. So this is a concept for mechanism design that is really, really powerful in the mechanism design world. Because if you try to design mechanisms that are not dominant strategy truthful, it's much, much, much harder to think about how people are going to behave in response to them. And so it's much, much, much harder to reason about sort of the impact and outcomes of these mechanisms. And so it's sort of by far the dominant paradigm in mechanism design to focus on dominant strategy truthfulness. So here we have this sort of epsilon relaxed version of this notion. And the really striking proposition, and this comes from a paper of Kunal Talwar and Frank McSherry from 2007, is that if a mechanism is epsilon differentially private, then it also is approximate dominant strategy, truthfully.
00:25:05.992 - 00:25:28.104, Speaker B: Okay, so what this says is that, in a very, very formal sense, if a mechanism is differentially private, then you are basically best off by handing it your true data. And this is a pretty remarkable implication. Yeah.
00:25:28.264 - 00:25:32.244, Speaker G: Like basically isn't the same thing as is.
00:25:32.364 - 00:25:33.684, Speaker B: You are on the next slide.
00:25:33.764 - 00:25:34.676, Speaker C: Yes, yes, yes.
00:25:34.740 - 00:25:47.260, Speaker B: So let's get into that approximate and that concern in just a second, but let's appreciate the glory of it for a second before we get upset about the approximate aspect of it.
00:25:47.292 - 00:25:47.476, Speaker C: Yeah.
00:25:47.500 - 00:26:08.376, Speaker B: So there's something really quite striking about this. It says that the interpretation of differential privacy is if you face a differentially private mechanism, you might as well participate with your true data, because there's really not any benefit in lying.
00:26:08.560 - 00:26:26.620, Speaker G: Yeah, I think a bigger limitation than, I don't mind the approximate at all. But in this model, utility doesn't depend on the amount of information that other people learn about it. So it doesn't admit the possibility that maybe at some point enough information leaks out that has a real consequence. You lose your job.
00:26:26.732 - 00:26:27.196, Speaker C: No, no.
00:26:27.260 - 00:26:28.836, Speaker B: So that's actually incorporated here.
00:26:28.900 - 00:26:33.844, Speaker G: It only depends on the outcome of the mechanism. Meaning like an allocative distribution of things.
00:26:33.884 - 00:27:13.648, Speaker B: No, no. All future consequences of the mechanism. That's a really important point, actually, to draw out. Actually, it incorporates all of those future consequences. And so what this is actually formally saying is that any future world that you might potentially experience, you know, the pathway might go down if this mechanism is run and then you're later denied insurance versus the path where this mechanism is run. And, you know, wonderful things happen to you, all of those possible future worlds, the probability that you end up in some bad subset of those worlds is almost exactly the same, regardless of whether or not you provide your true data to the mechanism. And that's a really important point.
00:27:13.648 - 00:27:43.374, Speaker B: All this post process processings, and we didn't get to emphasize this because we didn't really have a serious differential privacy tutorial, tutorial at the beginning of this week, is all of these possible future worlds really basically are unaffected in their probability by your participation. And so that's really important. It's not just about the outcome of this particular mechanism. It's about everything to come in the future. Yeah. So that's something really striking and not obvious here.
00:27:44.074 - 00:27:51.682, Speaker F: And actually, one can actually use the proposition as a way to oppose approximately dominant surgery truthfulness.
00:27:51.778 - 00:27:52.410, Speaker C: Yes.
00:27:52.562 - 00:27:54.010, Speaker B: And that is on the next slide.
00:27:54.042 - 00:27:57.842, Speaker F: The paper is as if the paper is mocking this concept.
00:27:57.898 - 00:27:58.202, Speaker C: Yes.
00:27:58.258 - 00:28:33.414, Speaker B: And that's the next slide. Okay, so the good news is a couple of things. Wow. We have this fantastic new tool for mechanism design, and it has really cool properties that we appreciate. Um, so differential privacy has these composition guarantees that we've only talked about briefly here. Um, and actually, this is something that's special. In the world of mechanism design, the standard strategy proof or truthful mechanisms don't necessarily compose nicely, so you can do something that's truthful and something else that's truthful, and then the whole process together might not actually be truthful.
00:28:33.414 - 00:29:13.490, Speaker B: Another really cool thing is that we're doing mechanism design without money. So differential privacy is getting people to hand over their data without actually needing to compensate anybody. And mechanism design without money is considered to be an interesting desideratum. Now the bad news. The approximate truthfulness, and sort of, you know, this is a mockery. Approximate truthfulness is not truthfulness. There is some slack in the sense that there is potentially some small benefit that you could get by lying about your data or about withholding your information from the mechanism.
00:29:13.602 - 00:29:14.250, Speaker C: Yes.
00:29:14.402 - 00:30:19.114, Speaker B: And so approximate truthfulness is probably only convincing to you as a solution concept. If you think that people have sort of a default tendency to participate and be truthful, or if you think that there's sort of a cost to lying, effort to lying, then you can start to justify approximate truthfulness to yourself. As a concept, this 1 may be even more disturbing. Well, so I told you that telling the truth and providing your data is an approximately dominant strategy. What I swept under the rug is any report to the mechanism is also an approximately dominant strategy, because differential privacy has this sort of paradox of what it's telling people is your data really matters, but your data doesn't really matter. And in particular, no matter what you hand over to the mechanism, about the same things are going to happen. And so that says, ok, telling you the truth, approximately dominant strategy, making up whatever you want, also approximately dominant strategy.
00:30:19.114 - 00:31:03.286, Speaker B: And so, as Kobi says, this is a bit of a mockery in some sense. This says it's okay, so why would people participate? Why would they give us anything useful? But this is what we have. And so differential privacy, sort of with these ideas in mind, has been used as a tool for mechanism design. So the original paper that sort of highlighted this truthfulness property, approximate truthfulness property of differential privacy. It was the Micksharitawar paper, zero seven. And then they applied this as a tool in the context of digital goods auctions getting some new results in that space. There's been some interesting work using differential privacy for mechanism design and equilibrium selection mechanisms.
00:31:03.286 - 00:31:56.510, Speaker B: In this context, an interesting sort of alternative notion of privacy, known as joint differential privacy, has come out where the basic idea is. In many contexts, it makes sense, not that the entire output of a mechanism would be made public, but that each person would receive the part of the output that's relevant to them. So you find out what path you're supposed to drive from home to work. You don't find out all the paths that everybody's supposed to drive, because if I found out your path, I'd know where your home is and where your work is. That doesn't make a lot of sense. So you can sort of generalize this idea of sort of separating the outputs and generalize the notion of differential privacy there. But one of the sort of more striking things that's happened in this space, this is paper of Kobe and Smordinski and Tenenholz, where they use differential privacy as a tool to get not just approximately truthful mechanisms, but exactly truthful mechanisms.
00:31:56.510 - 00:33:15.324, Speaker B: And the cool thing that happens there is, they say, basically, most of the time, you can go around running the differential private mechanism, as long as some of the time you can incorporate a threat that prevents people from taking advantage of that approximate truthfulness. And so there's a clever design there and sort of a slight twist on the usual setting in order to achieve this. Okay, so, stepping aside for a little bit from differential privacy as a tool for mechanism design, there's also been a literature specifically tailored to this question of eliciting private data. And so this line of work was started by a paper of Goshen Roth in 2011, where they show how to use standard mechanism design tools to run truthful auctions and get accurate statistics in settings where individuals don't have privacy concerns about their costs for their data. So now we're returning again to that concern, that the amount that you might need to pay me for my information might be correlated with the content of that information. So again, if I'm TCS plus, maybe you have to pay me $1,000. If I'm TCS negative, maybe you only have to pay me $10 for the data.
00:33:15.324 - 00:34:21.426, Speaker B: And so when you set that concern aside, you can basically deploy standard mechanism design tools, which is a nice observation. And then subsequent work has observed that actually, there are some impossibility results that emerge for designing mechanisms with appealing properties when the costs themselves are really sensitive information. And there have been various responses in the literature to these negative results. So I'll just give sort of a brief tour of some of the ideas there. So, one approach is sort of to inject additional assumptions about sort of the structure and the information that's available. Another approach that's been taken in the literature is to add sort of a different tool to the toolkit of the surveyor who's trying to collect the sensitive data, and to allow the surveyor the following ability. I can stand on a street court and say, $5 for your data, $5 for your data, $5 for your data.
00:34:21.426 - 00:35:07.574, Speaker B: And you can't avoid having heard the offer. If you can't avoid having heard the offer, I gotcha in the following sense, because if you take the offer, great. If you don't take the offer, I learned something about you, and I can take advantage of that. Okay, so that's another way, sort of around this impossibility result. Um, you can also introduce additional assumptions and that are actually fairly benign in order to get some positive statements here. Um, and there are sort of other ways of injecting other forms of information, um, where. Where you can potentially get some more positive statements.
00:35:08.704 - 00:35:09.484, Speaker C: Yes.
00:35:10.384 - 00:35:25.104, Speaker H: In all these models and papers, where that's played through, in particular with the example that you just gave. I mean, that is, those are stylized examples without any context.
00:35:25.184 - 00:35:25.608, Speaker C: Yes.
00:35:25.696 - 00:35:41.514, Speaker H: So in the context of contextual integrity, it's not clear if anyone would have evaluated, and have you seen any of these economists sort of trying to make that context dependent?
00:35:42.174 - 00:36:52.364, Speaker B: So this literature is not really explicitly incorporating context. And let me just say what I mean by context when I say it, in case I'm not using the language in the same way as everyone else. So this literature is not generally thinking about the type of the data. It's not generally thinking about the relationship between the person who's providing the data and the surveyor is collecting it. That all has been abstracted from the model in all of these. But I think you're getting to sort of the next slide in some sense, which is what can we offer to sort of explicitly provide some bounds or models about how people's harms from privacy change as you change the epsilon in your differentially private mechanism? And is there anything that's universal about that, despite the context, where maybe there's context, we'll do rescaling of the harms. But is there some general trend that you can state that as the epsilon changes, the harm changes, things like that? Yeah, but I think there's plenty of work to be done in alternate models, models in this space.
00:36:52.364 - 00:37:48.864, Speaker B: So in the context of sort of, how should harm be measured as you change the epsilon or the epsilon and the delta of your differential privacy guarantee there's been some work here. So you see, in the Goshen Roth paper, sort of an initial proposal of some linear function of epsilon. In this paper of Colby's, they point out that maybe you should think of that linear function of epsilon is more of an upper bound, um, because privacy guarantees are very, very worst case case. Case guarantees in the context of differential privacy. So, worst case over the content of the. Of the database, and worst case over your data, and worst case over the outcomes that you're concerned about, gives some upper bound on harms in some sense. And so whatever that epsilon is that's giving that upper bound, it shouldn't necessarily be that your harms are tied directly to that epsilon, but maybe that epsilon can be used to upper bound.
00:37:48.864 - 00:38:59.834, Speaker B: And so, and sort of relatedly, if you take this epsilon too, too seriously, we can show that you get to very problematic predictions as a result. So, really, at the best, some function of epsilon might be an upper bound for harmony. And there have been more sophisticated proposals in this space, other functions of epsilon that you might consider for particular reasons. But I think there's still plenty that we don't understand about how you take a guarantee of differential privacy and interpret the value or the harm that an individual experiences as a result of their data passing through this mechanism slightly differently. Different vein of work, but I think also really important one to point out, there's this paper of Lili, McLeod, and Susyu, which has a model for how you might go about pricing private data. In fact, maybe this belongs on a later slide. One of the interesting things that comes out of this paper is sort of the consideration of arbitrage opportunities.
00:38:59.834 - 00:40:37.586, Speaker B: So if you want to price access to many, many different data resources or computations, one of the things you might have to think about is whether or not this computation of interest could also have been accomplished by doing this sequence of these three other computations. And ought we to have priced things so that people don't need to think really hard about complex ways to decompose the computations of interest in order to strategically pay less in order to find out what they wanted to find? So there's also been some work in this line, um, but thinking about sort of the impacts of privacy concerns and things you might want to do with data. Um, so in a paper with Rachel Cummings and strata, we look at this question of what if you want to take people's sensitive data and input it into a computation? Maybe you want to do a linear regression on data that you've elicited from people what can go wrong? And how can we sort of redesign existing algorithms? And you end up with some interesting problems, because these private estimators that we previously had for linear regression introduce biases that make this difficult. Rachel also has some interesting work on introducing privacy guarantees into prediction markets, which relates to Adam's point before that. It's really interesting to think about how we can aggregate the information that a group of people hold. And that's basically the purpose of a prediction market. And what they show is kind of a nice dichotomy, where if you're doing a one shot prediction market, so there's a single time where you're going to gather everybody's information.
00:40:37.586 - 00:41:54.844, Speaker B: In this particular way, you can introduce privacy. But if you want to do this in a dynamic way where there's a sort of continual process that is gathering the information, then introducing privacy becomes really problematic, because there's this tension between hiding what you believe and taking advantage of what you believe in order to move the market. And this is not the only paper of this slaver, by any means, but I wanted to leave this slide basically blank because I think there's so much left open in this space, is that there's been relatively, very little work done on actually properly the problem of the design of a market for data, sort of in the context of formal guarantees of privacy. And I think there's just so much to do in this space. Returning to the theme of how hard it is to get empirical evidence and design experiments to learn about people's privacy preferences, I wanted to highlight two important lessons that I've taken from economists who run experiments, and they are these. The first is, don't lie to your subjects. So this is an absolute, you know, hard rule in economics.
00:41:54.844 - 00:42:59.050, Speaker B: It is not an absolute hard rule in other experimental sciences, but I think it's a really important one. Why do economists insist that we never lie to our subjects in the design of an experiment? Experiment because if you might lie to your subjects and they know you might lie to them, how are we supposed to interpret their behavior? They're having to make guesses about what's really going to happen, because maybe we lied about it. We can't look inside their heads to guess what they think we're going to be doing, and then they do something. There's absolutely no way to make sense of it. And this is really, really important. And economists insist on this so much that they won't use a subject pool where people have lied to people in that pool in the past because they want the subjects to believe the experiment and to actually do what they would do sort of in real life, in the context of the experiment. That's one really important lesson, I think, to take from sort of the economic perspective.
00:42:59.050 - 00:43:37.174, Speaker B: Another is whenever you can prefer a design that involves incentivized choice. So what do I mean by this? It's one thing to go around and ask people how much do you value privacy? And it's completely a different thing to see how they behave when they have to make the choice. And it has real meaningful economics consequence. And economists always prefer to see how people behave than to see how people say they would behave. And this is, I think, another interesting thing to take from this literature.
00:43:37.794 - 00:44:16.570, Speaker E: This actually gets back to what you know at the beginning when you're talking about the so called privacy paradox. I have great skepticism with the conclusion you just drew and also with the conclusions that economists are willing to build based on experimental settings such as this one. So I really want to lose five pounds, but here they serve such great snacks, so I go out and I.
00:44:16.602 - 00:44:17.866, Speaker B: Eat all this stuff.
00:44:18.010 - 00:44:26.054, Speaker E: So is it true that I actually want to learn, you know, lose the five pounds? Because here I am eating all this pie.
00:44:26.914 - 00:44:30.754, Speaker B: Great. Yeah, I think, number one.
00:44:30.794 - 00:44:51.806, Speaker E: Number two, I flew united. I fly united all the time. And this is where I think Frank's question is really critical. And you could frame it as a, you know, lack of contextual information. So based on my behavior, I love United Airlines. But what you're not, you know what?
00:44:51.910 - 00:45:23.304, Speaker B: You threw things out of the model. I think what you're saying is that if you throw things out of the model, you can confuse yourself to no end. And I think that that's the point to take. It's not that incentivized choice is a bad idea. It's that misinterpreting observed incentivized choice is a bad idea. And it's easy to do that because it's so hard to get everything into the model. And so when I see that people make bad privacy choices in the real world, the right interpretation is not, people don't care about privacy.
00:45:23.304 - 00:46:19.488, Speaker B: The right interpretation is, wow, there's a lot of complicated stuff going on here, and what are we missing from the model? Did they not have the right information? Did they not understand the choices? Did they not understand the implications? Have they been conditioned by society to make these choices despite the fact that they're not in their best interest? Fill in 100 other reasons for why they might be behaving this way? And people have worked on that, but I don't think it's to say that you shouldn't observe incentivize choice when you can. It's to say that even incentivized choice does not answer the whole question if you're missing pieces of what went into the decision making process. And I think what you're saying is also that there are important things to be gained from surveys that you maybe can't access by way of incentivized choice, or we don't know how to.
00:46:19.576 - 00:46:30.264, Speaker E: Well, I mean, the way some of us would argue is that there's these confounding variables. I know Fraka had her hand up and maybe.
00:46:30.424 - 00:46:31.284, Speaker H: Go ahead.
00:46:31.584 - 00:46:31.992, Speaker C: Yeah.
00:46:32.048 - 00:46:44.720, Speaker E: So if you're performing an experiment where you're failing to integrate key variables into it, I think that's bad science. I don't think that that's like, I'm not learning anything.
00:46:44.792 - 00:47:08.784, Speaker B: Sure, sure. Yeah, absolutely. And that's one of the things that's very hard in this space is because there are so many confounds and it is such a complex model that I think most of the experiments give interesting vignettes from which one cannot actually scientifically conclude anything of much use. Yeah, so has.
00:47:10.804 - 00:47:33.084, Speaker H: I mean, there are many different people who operate on that, but within economics, it would be, I guess, the behavioral economists. Have there been studies where instead of trying to sort of rationalize, analyze this out and have prediction on rationale choice, some operated with heuristics that guide these behaviors.
00:47:33.584 - 00:48:23.264, Speaker B: You mean to incorporate sort of heuristic decision making into the model and look at the predictions from that? I know some things that are vaguely related. I don't know of anything off the top of my head that quite fits that model, but that may be that I just don't know the right literature there. We're very sort of low on time. There are a few more things that I wanted to say. So I'll just mention briefly, despite the fact that it seems so difficult to do something useful in a lab setting, Rachel and I, along with a couple of econ co authors, are planning on running an experiment starting next week, actually in a lab. And I just want to say what I think of as the key idea behind this experiment is the following. If you want to do an experiment about people's privacy related behaviors, you have a few choices.
00:48:23.264 - 00:49:21.484, Speaker B: One is you give them some pretend data and you say, that's your sensitive data. Okay, go make some choices. That doesn't make a lot of sense. Why are people going to behave properly about pretend data? Another possibility is you actually threaten them with exposing real private data, and that doesn't pass IRB or shouldn't. So what we've tried to do is come up with another choice, which is that we've created sensitive data in the lab context by having people play a game where they essentially have a choice between doing something that's economically good for them or good for society. And when they pick the thing that's good for them, it's a little bit embarrassing. And the hope is that we'll be able to use the sensitive data that we create in a lab context in order to ask and answer some very, very limited questions about privacy sensitive behaviors.
00:49:21.484 - 00:49:31.304, Speaker B: I don't have time to get into the details, but come talk to us and, you know, preferably before we start running it and give us your thoughts. So I'm going to skip ahead.
00:49:33.624 - 00:49:33.912, Speaker C: And.
00:49:33.928 - 00:50:36.682, Speaker B: Talk about a couple other things. All right, so, but if you want to think about whether or not, you know, the work you're doing defining new mechanisms to preserve privacy or new privacy notions or whatever it is, is helping society and, you know, improving welfare, one of the things you need to think about is what are the effects on society? It turns out that's a pretty complicated thing to think about. So we saw from Franke's talk that, you know, we shouldn't take lightly the idea that when we introduce a new mechanism and it requires an extra thousand samples, we shouldn't take lightly the effect on society. More samples can be expensive, time consuming. But the good news is it's relatively easy to estimate this kind of cost and this kind of impact on society. Maybe a harder thing to estimate is when you introduce new privacy rules or technologies or hurdles, that makes things harder for a lot of people. They have to implement new technologies or hire experts, and maybe those experts don't exist or they're expensive.
00:50:36.682 - 00:52:05.914, Speaker B: It takes time and money, and there's economic evidence that lots of money is wasted in some sense on getting privacy right, because maybe they're not getting it right. That's why I would say wasted. And there's a concern that all of this time and money and effort that goes into sort of following the rules on privacy may actually favor the big players in the sense that they're the ones who have the big legal departments to figure out exactly what they need to do to follow the regulations, and they're the ones who have the big budgets to get the tech in place and hire the consultants or whatever it is, and the small companies don't have that. And so that's a concern. There's also a slide that I am not qualified to discuss. But at some point you have to think about what are the benefits to society, broadly, legally, morally, philosophically, of providing privacy protections? If you wanted to try to put on an economist's hat in this context, you might think about trying to survey people to elicit the importance of living in a privacy just world, and what would the effects be on people's happiness or other desirable outcomes? But this seems like a really hard thing to quantify, and in some sense, I kind of hope that privacy wins even before you include this in the equation and that this is just a bonus. But it's a question.
00:52:05.994 - 00:52:06.654, Speaker C: Yeah.
00:52:08.634 - 00:52:27.942, Speaker F: I'm not sure if it's related to. Here's maybe a flip side of it or something. What would be like to live in a privacy devoid world where my information is exposed? I have no privacy at all. Everyone can read my diary in my gmail, and everyone can do it.
00:52:27.958 - 00:52:31.114, Speaker B: You think, I'm not. I'm not reading it.
00:52:33.894 - 00:52:39.834, Speaker F: Has there any meaning words? So, like, why do people need privacy in that sense?
00:52:39.954 - 00:52:40.654, Speaker C: Yes.
00:52:41.114 - 00:52:43.554, Speaker B: By the way, you should read.
00:52:43.594 - 00:52:50.934, Speaker E: It's an old book. It's by David Brin. It's called the Transparent Society. He makes the case.
00:52:51.354 - 00:53:01.242, Speaker B: Yeah, there's definitely work in this area. Helen's the person to talk to in the room about this. Yeah, but it seems hard to quantify. But it has to enter the equation somewhere.
00:53:01.298 - 00:53:04.846, Speaker A: Yes, on the alarmist side, but 1984 is also.
00:53:04.910 - 00:54:05.880, Speaker B: Yeah, yeah, there's a relevant literature there. Good. There's also this sort of impact that privacy protections prevent harms, and these are harms that didn't previously appear in the equation, but maybe they belong there somewhere. So, you know, databases that don't exist can't be hacked. Trying to protect yourself as an individual from all of the things that might happen to you from various future privacy considerations is costly and has various types of overhead, and people are actually paying real money in today's world for insurance against privacy harms. That's another cost that you can put into the equation. And then of course, there's sort of the positive side of the equation to think about, which is, you know, maybe as you introduce privacy technologies, more data will be captured, better data will be captured, broader access of various segments of society to data might occur.
00:54:05.880 - 00:54:53.218, Speaker B: Depends on the technologies, depends on the model. But it's important to think about what are the impacts going to be as you introduce privacy regulations or technologies. Does this increase competition? Does it decrease competition? Does it increase interoperability across platforms? Are these things that are going to contribute to economic growth. And this sort of repeated concern about sort of complexity and interactions shows up in a number of places in the literature that belongs in the equation. And then I promised I was going to come to this. So here it is. There's also this idea that privacy has this sort of side effect of inducing stability or robustness of computations, at least differential privacy and related notions.
00:54:53.218 - 00:55:33.134, Speaker B: And this has benefits for statistical validity. And so in some sense, the societal benefits of more privacy prevalent technologies may be that we get to reuse our data more, we get to share our data more, we get better scientific conclusions. We waste fewer resources as a society by doing things as a result of false scientific conclusions. We get better government statistics, we allocate our resources better. So getting better statistics could have potentially huge economic impact. And then there's also sort of the meta of potentially increasing trust in science and in government statistics and other statistical resources. And I think this is a really interesting question.
00:55:33.134 - 00:57:09.724, Speaker B: Could we ever argue that the cost of additional samples is just completely offset by the and benefits of statistical validity? Can we say that actually it was just worth it just on that level, because I'm low on time? I'm going to skip another example that has Rachel's really fun slide and say there are a few other lessons from economics that I wanted to point to. One is that there are a lot of counterintuitive effects that are relevant to us that have been observed in various places in the literature. So one of them is that when people perceive that they have greater control of their information, they may actually take greater risks that leave them more vulnerable. And so we have to be careful when we design systems that will be used by real humans. The way you present information to people has substantial effects on behavior, even if the system is behaving the same way. And this is in the context of data information, there's all sorts of behavioral biases that various people have mentioned at various points. So to pull on one that a student of mine raised, raised at some point, there's this question of when you introduce privacy technologies and you make people aware, and there's legislation and all of this, you make people think a lot more about privacy.
00:57:09.724 - 00:57:24.156, Speaker B: Is that a good thing? Does society actually suffer a harm from having to think more about privacy and worry more about privacy and be more aware about privacy? Is there some sense in which ignorance is bliss?
00:57:24.220 - 00:57:24.824, Speaker C: Here?
00:57:26.364 - 00:58:07.746, Speaker B: There's this really interesting daycares paper by Gneissi and Rusticini, which showed that there were daycares in Haifa that were having difficulty with parents showing up late to pick up their kids. They had a policy, you have to show up by 06:00 p.m. Whatever it is. Parents were showing up late, and so they introduced a fine, you know, a dollar per minute. And all of a sudden, people started showing up even later. So the point is, when you attach a price to something, it can somehow change the nature of the thing entirely. And I think that's relevant to things like privacy and data.
00:58:07.746 - 00:58:58.522, Speaker B: And we have to think very careful about what we're doing and how people will respond when we attach a price to it. There's also just another theme to point to, this theme of adverse selection in compensated surveys. So a lot of people who gather data, and Fraco probably can say much more about this than I can worry about paying people for their data, because you get a completely different population of people responding, and they sometimes just make stuff up. And this is something that we're going to have to deal with if we're getting into a world where we're compensating people for the use of their data. I don't know why, but there's this one quote in all of the papers that I read when preparing this talk that somehow just disturbed me the most of all. I mean, there's so many things to be disturbed by, but I don't know why. Evite.com
00:58:58.522 - 00:59:26.274, Speaker B: may sell lists of consumers attending a party in a given location. And somehow I just thought, this is the world that we live in. You can't go to your kid's friend's birthday party without this being sold to somebody. What is this world? But anyway, with that, I'll leave it and take questions. Thanks, guys. So they exhausted you?
00:59:26.314 - 00:59:51.998, Speaker A: Yes, yes. I guess a question that, I mean, came up, incidentally, a few times. But is it realistic to quantify, like, public good benefits to, like, in the context of, like, government statistical agencies? Right. They're the, you know, the census itself has made various attempts to make a case. People have a census.
00:59:52.166 - 01:00:02.026, Speaker B: I think we should try. I think it's a very difficult thing to do, but, yeah, I think there should be concerted effort in that direction. I don't have. Sorry, you were going to say something?
01:00:02.090 - 01:00:09.834, Speaker H: Yeah. I mean, that is the messaging, right? I mean, they are. The campaigns that run prior to the census are all about the public good.
01:00:09.874 - 01:00:15.954, Speaker A: No, no, I understand that. I meant more like formally, rigorously capture trade offs between.
01:00:16.114 - 01:00:17.970, Speaker H: Oh, trade offs between the two. Okay.
01:00:18.082 - 01:00:18.370, Speaker C: Yeah.
01:00:18.402 - 01:00:46.164, Speaker A: I mean, you know, like, for example, you know, John Abbott and they have this paper where they are trying to quantify that. But as far as I understood, it boils down to saying, well, however you want to quantify that, you should optimize for the trade off between these two, which is an important point, but it leaves this gaping problem of estimating the benefit.
01:00:46.244 - 01:00:46.980, Speaker C: Yeah.
01:00:47.172 - 01:00:51.320, Speaker B: I don't have good answers, but I think it's obviously a very important question.
01:00:51.472 - 01:00:52.204, Speaker C: Yeah.
01:00:55.224 - 01:01:04.896, Speaker D: So it seemed to me, like most of your talk, you focused on sort of classical economics with rational actors and so on, and then that last slide.
01:01:05.000 - 01:01:07.344, Speaker E: Was the new behavioral side.
01:01:07.384 - 01:01:08.336, Speaker C: Yeah, yeah.
01:01:08.520 - 01:01:10.764, Speaker B: And then it all goes to hell. Yeah, exactly.
01:01:12.024 - 01:01:12.440, Speaker C: Yeah.
01:01:12.472 - 01:01:43.930, Speaker B: So basically that's because there's so, so much we don't understand that's already sort of in the pre behavioral, pre psychological world. But, yeah, absolutely. That sort of modern slice of academic thinking, of course, belongs here, and it's incredibly relevant. I think, what we're talking about, privacy considerations. So it's not that I neglected it because I think it's not important. Yeah, absolutely. It belongs.
01:01:44.042 - 01:01:53.442, Speaker D: Especially when you were talking earlier about, you know, losing your job and so on and so forth. I mean, even if you say that it's approximately the same, I mean, there's loss avoidance and stuff.
01:01:53.498 - 01:01:54.538, Speaker B: Yes, absolutely.
01:01:54.586 - 01:01:54.834, Speaker C: Yeah.
01:01:54.874 - 01:01:59.842, Speaker B: And those were on the slide at some point and they just had to drop off. But, yeah, I'm completely with you.
01:01:59.978 - 01:02:00.694, Speaker C: Yeah.
01:02:06.614 - 01:02:26.234, Speaker F: And data versus snickering. You mentioned that yesterday. Data versus labor, because labor is also one of these amorphic things that relates to all these different things that are all different kinds of goods classified under labor. How do you price it and how do you assess it and how you sell it and what are the repercussions of your labor.
01:02:27.134 - 01:02:27.494, Speaker C: Yeah.
01:02:27.534 - 01:02:52.754, Speaker B: So there's, of course, incredibly rich literature and understanding when it comes to labor and markets for labor. And there are a couple of papers that think about the implications of that work for data and potential tools that we can use from sort of the labor perspective. But I think there's plenty more to be done, and I don't think I can give you pointers.
01:02:52.794 - 01:02:52.986, Speaker C: Yeah.
01:02:53.010 - 01:03:14.504, Speaker B: But I don't think that we should restrict ourselves to that interpretation. I think we should view it as a tool, toolkit. I think the data is not just labor, if it is labor at all, but it's very, very useful to learn from everything we understand about labor.
01:03:18.684 - 01:04:08.020, Speaker H: The last piece that you mentioned on the, reminding me on the same stuff that I've been pointing out with the work from Eleanor Singh, sort of the kind over emphasizing privacy or putting this in place in places where it's not necessarily well understood that it would have that value. Right. It does remind me of that discussion that we have around census. Right. I mean, if they are putting differential privacy in place for the ten question census, the decennial census, where it's information that is technically out there about you or pretty easily out there about you, would you think that this could have the backlash effect? Because it doesn't seem like that sensitive and yet there is like this big.
01:04:08.052 - 01:04:53.884, Speaker B: Machinery going on potentially. I hadn't thought about it, particularly in that context, but I do think that we have to be very careful about introducing heavy technologies for a number of reasons. I think there are a number of sorts of resistance that you get to technologies that are difficult or foreign or filled in various words that are sort of uncomfortable. I haven't thought about sort of the salience issue, particularly in the context of census, and I haven't heard it. I haven't really heard people talking about the salience issue. So if you know the literatures where people are talking about them in the context of privacy, I'd love to see that.
