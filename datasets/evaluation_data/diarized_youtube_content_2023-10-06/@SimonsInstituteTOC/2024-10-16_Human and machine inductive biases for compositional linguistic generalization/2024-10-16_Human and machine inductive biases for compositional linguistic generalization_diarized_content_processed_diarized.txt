00:00:04.000 - 00:00:04.392, Speaker A: Hi everyone.
00:00:04.392 - 00:00:45.273, Speaker B: So now let's welcome Najun Kim Najun. She's currently an assistant professor in the linguistic and also computer science department in Boston University. Her research lies broadly in the intersection between linguistics, LP and also cognitive science. So she has many interesting work in understanding the meaning and generalization of how both human and machine learner can learn. And she will talk to us about how they handle the novel input and also implicit meanings. So her research has been recognized by many best paper awards. And today she'll talk to us more about the recent results on compositional linguistic generalization.
00:00:45.273 - 00:00:46.805, Speaker B: Please welcome. Naju.
00:00:49.425 - 00:01:10.627, Speaker A: Hi. Oh, is it working? Yeah. Thanks for the introduction and thanks for having me. I don't often talk in like this type of audience, so I'm very excited to like hear your thoughts. Maybe like you have thoughts from like a different angle that we didn't think of. So I will, yeah, just feel free to interrupt and like ask questions or like, you know, raise points. Okay, cool.
00:01:10.627 - 00:01:56.271, Speaker A: So I want to start off by explaining what I mean by compositional linguistic generalization. So I'm using this expression to mean producing and comprehending novel linguistic expressions that haven't been encountered before by a speaker. So hence that's generalization. And I would say like, it is widely accepted, at least in the cognitive and linguistic communities that this generalization is achieved by composing constituents or smaller parts in the language that speakers already know. And you know. Like this is kind of obvious if you think of like the things that you produce and like understand in language and day to day life. Like, it's not necessarily because like you have heard of some sentence before that you are able to like produce it and like understand a lot of the things that we say.
00:01:56.271 - 00:02:37.095, Speaker A: Like the sentences that I'm saying now probably hasn't really been uttered before or like you haven't heard of them before. So I'll walk you through an example. So say you as a person who has knowledge of English, know the sentences that I'm going to show you and their meanings. So you know the meaning of the sentence Alice kicked the Mad Hatter. So I'm going to use this shorthand logical form or like meaning representation notation to represent the sentence meanings here. So the first argument of the predicate kick is the agent of the action and the second one is what we call the theme or like the person affected by the action. So kick AMH here is shorthand for Alice is the agent of the kicking and the Mad Hatter is on the receiving end of the kicking.
00:02:37.095 - 00:03:19.387, Speaker A: So you might like have a Picture like this to represent the meaning. Yes. And then you also know meaning of some other sentences. I'm going to tell you that like Alice kicked the Cheshire cat, maybe corresponds to something like this in a kick acc and could be visually depicted like this. And now I'm also going to tell you that the meaning of the sentence the Mad Hatter kicked the Moobler is this particular meaning representation and this is the visualization for it. So, you know, even though I didn't actually tell you, okay, this weird, slimy thing here is the Moobler. You probably have figured it out by just reading the sentence that Moobler is the thing that is at the receiving end of the action.
00:03:19.387 - 00:04:12.405, Speaker A: So it's like thing being kicked, not the kicker. So here comes the exciting part. What do you think is the meaning of the sentence that you've presumably never have heard of before attending this talk? Although, like, unless you've attended my previous talks before, what does it mean the Mubler kicked Alice? You would probably think that Mubler is the kicker and Alice is the person being kicked, right? So this is a meaningful that you can construct by composing the meanings of parts that you already know. I don't think you can see the highlights, but you know the kicking, the predicate kick and the agent and the theme in the right place. So if you agreed with me that like, that should be the meaning of this new sentence, you've made a compositional generalization. But if you think about it, this isn't the only generalization that is compatible with what is known in the known set of things. Here.
00:04:12.405 - 00:05:33.483, Speaker A: For example, one could make a generalization that any sentence that contains the word mubler has exactly the same meaning. So the corresponding meaning to that generalization would be, you know, like, you had this here, so it would be this meaning. And you could also make a generalization that because you haven't really seen mubler in the subject position of the sentence, you will just take the meaning of it to be the most kind of probable kicker in the set of known things. In which case the meaning would be something like kick am Alice smoobler, because Alice is the most frequent kicker that you have observed in your set of observations or known things. So if you know the these alternative generalizations, if you don't have any other considerations and the only consideration that you have is consistency with what is known, none of these generalizations, like even the red ones, like, contradict the known set of form, mean and correspondence. But we as speakers of English can probably all agree that the green compositional one holds a special status, like it at least seems more correct than the others, like, and by at an intuition level. So, and then like, you know, I think we have such a strong prior towards the first meaning that the other possible red options seem even like ridiculously implausible.
00:05:33.483 - 00:06:10.455, Speaker A: You probably wouldn't even have floated this possibility as a potential interpretation of the sentence. I mean, you can disagree with me, but that's the intention I have. So I said that these are bizarre for humans. This is not necessarily true to neural networks. So when seeing sentences that are expressing infrequent agent theme combinations, the weird, like, you know, the weird slimy things kicking Alice instead of being kicked by Alice, they often generalize to like observed things that are frequent, like Alice kicking stuff instead. So I generated, I tried to generate images of like weird slimy things kicking Alice. It was really, really hard.
00:06:10.455 - 00:07:05.887, Speaker A: And I wasn't like trying to teach the T2I model like what Moogler is. But like, you know, the point still holds that like, I used certain sort of like agent theme combination, but it was just trying like very hard to get them to generate these things because I guess, like, in your data, like these things are more frequent than the other way around. So this kind of generalization is actually not something that's so bizarre for neural networks to be making, as opposed to us as humans. So this problem of generalization to unseen examples and kind of our strong preferences about what's the right generalization is connected to our inductive bias or learning bias. So it determines how a learner generalizes to unseen inputs. So in the previous example we saw, we were asking what the meaning of the sentence that you haven't seen before, like the Mubler kicked Alice is. And I said, all of these are consistent with what is known.
00:07:05.887 - 00:08:24.091, Speaker A: And to actually make a choice between these different available things, the learners must have some sort of a predisposition or a bias to prefer one over others. So by examining the learner's actual generalizations when faced with this ambiguity, we can get a sense of what the inductive bias of the learners are. If we test both human and machine, they might converge. Sometimes they might diverge on others. In constructing an evaluation for this compositional linguistic generalization, we use this poverty of the stimulus experimental paradigm in human subject studies, which has been proposed by Wilson in 2006, that lets us tease apart the learner's inductive bias. Based on this experimental paradigm shown here, where multiple generalization targets are given and are plausible, and they are plausible given a known set of things or like their training data, if it is a model. So if we as humans actually agreed that, you know, this thing is the right meaning for this particular novel example, where does this inductive bias actually come from for humans? So I guess as linguists, the linguistic answer would be that there is an underlying linguistic system that gets you to the right meaning.
00:08:24.091 - 00:09:37.631, Speaker A: So, you know, for sentences that could be analyzed as like transitive constructions with like noun phrase, let's say like noun phrase, subjects and objects, like sentences like Agnetha loves Bjorn, you can posit a structure like this that guides your composition when you're trying to understand a sentence like that. So positing something like this, you know, the important consequence is that the meaning of a new complex expression like Bjorn labs Agnitha can be predicted. It falls out of the compositional system. And even without having to observe this correspondence directly. So this kind of system is what many of like, at least some group of linguists believe that lets us interpret fairly complicated novel expressions like the woman which introduced the niece of Hearst to the rock star, which she liked. And then you have some structure behind it that guides the compositional process that lets you put the meanings of the parts together, and that's how you could generalize. And how do neural networks fit into this picture? So it has traditionally been considered a challenge both empirically and conceptually for neural networks in the connectionist literature.
00:09:37.631 - 00:11:01.545, Speaker A: If you're familiar, some of you might be familiar with so called systematicity debate involving lengthy arguments between Jerry Fodor and other people in the past. Although I won't go into the details about the history of the debate or like the terminology like systematicity, compositionality too much. But in the current context, there has been a recent resurgence and interest in like, compositional generalization because neural networks started being much better than they were before. And when I say recent, this actually refers to late 2010s, 2018, 2019, which I guess is kind of ancient, like it feels ancient from the current ML AI landscape. So in this context, we developed a series of benchmark called Cogs and slog back in 2020 and like more recently, like late 2023, where we adopted the task of meaning representation assignment to surface forms or semantic parsing, if you like the NLP lingo, which is the task of assigning a logical form to a sentence. So I'll talk about why there's two benchmarks a little later. But just as an illustration, the model tested will get a surface form of a complex expression as an input, like A cat danced and is required to output a corresponding logical form or a meaning representation that captures some generalization about the meaning of the input.
00:11:01.545 - 00:12:20.377, Speaker A: So it's a surface form to meaning representation, mapping task or semantic parsing. So the benchmarks cover a wide range of linguistic generalization phenomena. And you can look at the papers if you want. But what I want to point out today, and what I want you to take away today, importantly, is that these generalizations can be grouped into two types, lexical and structural. So what do I mean by lexical generalization? So if we see things like the wug saw the dog, and I tell you that it's this meaning I'm seeing the dog, and then, you know, what do you think is the meaning of the sentence that you probably didn't hear before, like the dog saw the wug, probably that, you know, most of you will think that this is the meaning of the novel sentence and presumably the individual parts and the linguistic structure allowed us to compositionally generalize to a convergent complex meaning here. And the particular example falls into the case of what we called lexical generalization, because this problem has the form where the target has the same underlying structure as some of the examples that are already known to the speaker. So in particular, you know, the dog saw the wug, which is the generalization target has, is you can posit like some the same underlying structure with an example that you already know.
00:12:20.377 - 00:13:17.805, Speaker A: So here the assumption is actually that you never see the wug as a grammatical object during training or exposure, although I can show you only one example of known things because of the lack of space. So you can imagine this is like, oh, you have a bunch of known things that the model gets trained on, and at least one of them has exactly the same structure as the generalization target. And you also see all of the individual lexical items in your training data here, as a first approximation, we consider human generalization patterns to be the gold target. So some of the gold generalizations actually have been attested by yourself, hopefully by convergent interpretations in the wug scenario and the Mubler scenario. These are all lexical generalizations. Hopefully you agree with me. But just in case, if you want empirical evidence, this generalization has also been observed in children as young as 20 months old in the developmental linguistic literature.
00:13:17.805 - 00:14:39.857, Speaker A: On the other hand, structural generalization refers to generalization scenarios where the target has different underlying structure to any of the complex expression that the learner has encountered so far. So if you look at this particular example, generalizing from the cat saw the red dog to the red cat saw the dog has different underlying structure posited by linguistic theory or whatever framework that you want to use. You can see that again, the individual components have all been observed. It's just how they got put together has different underlying structures. And another example of structural generalization would be generalization to deeper recursive structures. So given that you know how to map expressions like ava saw a ball in a bowl on the table, can you interpret or like assign a meaning representation to 1 degree deeper recursive prepositional phrase structures? Um, so that is also a case of structural generalization because the training and generalization structures differ, although they have all observed, like underlying constituents. So again, maybe here we want to say that human generalization is the gold target, but here is where the story gets a little bit complicated.
00:14:39.857 - 00:15:44.845, Speaker A: So I think people generally find the two kinds of structural generalization that I talked about before, modification in different positions and generalization of additional degrees of embedding quite intuitive. I don't think anybody discernibly was concerned that I was positing this as a generalization task. But it is actually difficult to find empirical evidence for specific structural generalizations. We're positing here because it's actually much harder to control for structural exposure in humans, as you can imagine, than lexical exposure. So you can come up with new words easily like mubler or wug or whatever to use experimentally, but it's harder to come up with general structures or have some confidence that, oh, my subject has never seen this particular structure before coming into my lab. So that's why it's hard. And this leads to lexical generalization being much more amenable to lab studies and hence kind of a wealth of supporting evidence for those.
00:15:44.845 - 00:16:26.235, Speaker A: But I'd also want to mention that probably because the target generalizations are so intuitive for adults. So I just used one example to illustrate. So a lot of the generalization studies target children, so that's, I guess, less obvious than an adults. We agree that this does happen. So one example that I did find was McCoy et al. 2021, where humans generalized to depth three center embedded structures in an artificial language when exposed to embeddings of up to depth 2. So these are structures that look like this and only expose being exposed to structures up to depth two.
00:16:26.235 - 00:17:15.365, Speaker A: This is depth two. Like, can they generalize to like depth three center embedded structures in that, like, they think that it is an acceptable structure in this language. But the caveat here is that this experiment was only on like, the acceptability of the surface forms and not about like meaning that I was talking about today. So then that was about like kind of embeddings. And what about modifier generalization of this kind? So I think modifier generalization is still intuitive in its general form. We can't like expect to have observed like you know, red modifying like a noun in like every possible syntactic position that is listed, assuming that language allows for unlimited embedded structures. So like you can't possibly observe them all before thinking that okay, like red is licensed here and then like elsewhere.
00:17:15.365 - 00:18:46.515, Speaker A: But we actually don't know human generalization patterns under very limited exposure conditions that we were assuming in the benchmark, which is only ever seeing modification in a single syntactic position. So like if you only ever see red as modifying a grammatical object, do you actually generalize as a human to elsewhere or do you withhold? Thinking about this generalization problem to design benchmarks actually made us realize that we actually don't know the human patterns. We took this as an opportunity to understand human generalization better by specifically asking do humans generalize modifiers only observed as modifying the object during training to elsewhere? So we tested this with artificial language learning with human subjects where they are faced with sort of like similar training generalization gap in terms of modification. So this like cracking of an object we used as the semantics for modification and gave them some modified structures during training and saw whether they were willing to generalize this to elsewhere. Here there were a bunch of like experimental manipulations like you know, making the artificial language be substantially from like different from English when which was the participants native language. And also like using semantics that can't be expressed as modification on the noun phrase in English to make sure that there is no kind of transfer available. So we did a bunch of those and saw whether humans generalize.
00:18:46.515 - 00:19:24.095, Speaker A: Yes. So we only tested like, we only recruited participants. We controlled the native language of the participants. But that's an interesting question because we said, oh, we use like semantics that can't be expressed with the structure in English, but like in other languages, like Mandarin or Korean. Like you could also have like such modification structures express this kind of semantics that we used. So that's a really good question. And I think like there would be probably transfer effects because we also had other experiments where we made the modification align with like English.
00:19:24.095 - 00:19:36.935, Speaker A: And then the generalization pattern was a little bit like quantitatively different, but not in the, not in a different direction. So I could see like, you know, your native language affecting generalization here. Yes.
00:19:37.015 - 00:19:39.231, Speaker C: How much variance is There between human subjects.
00:19:39.303 - 00:19:43.313, Speaker A: Yes, that's the next slide. Yes, I will talk about it. Yes. Does you have a question?
00:19:43.489 - 00:19:44.845, Speaker C: Artificial language.
00:19:46.665 - 00:20:13.473, Speaker A: We create. We started from kind of first principles and we came up with words and then we came up with the structures that we include. So this language is really small. This isn't rich and diverse, real natural languages. So it only has expressions for verbs with two arguments, agent and theme, and verbs with one argument. So it's very impoverished. But like we just sat down and thought about it like based on our desiderata for the experiment.
00:20:13.473 - 00:20:43.823, Speaker A: Yes. Yeah. So during the training phase, you show them a scene like so this is actually a video. So like you see like the circle hitting the triangle and like it cracks. And like you see this description and say like, hey, this is like the description for the scene. And then ask them to type it out until they get it right. And then after sufficient iterations, we show them a new scene.
00:20:43.823 - 00:21:14.269, Speaker A: And this is kind of our target when if they do generalize, this will be what they will produce. But they don't see this during generalization. It's like free form typing task. Yes, yes. So I will talk about the inter human variation. So the results show that the human subjects show a significant tendency to generalize to novel structures in a way that I described before, even though only ever seeing this modifier in one syntactic position. So that's like vast majority produce novel structures.
00:21:14.269 - 00:22:06.257, Speaker A: There were like a couple people who did only that did show this like withholding generalization pattern and like some creative ways of generalizing. But I would say like the majority had a bias towards like, like generalizing, not withholding. But this also points to the fact that, okay, like there is no like one human generalization. Maybe like there are kind of like distributions of generalizations that humans are making. But we took this for now as some degree of empirical verification for ourselves for the purported target generalizations that we included in the benchmark of modifier generalization in very, very limited exposure settings. So you know, we found our empirical support to some degree that we were looking for. And now we can call this particular generalization as human like.
00:22:06.257 - 00:22:07.685, Speaker A: But. Yes.
00:22:08.145 - 00:22:13.025, Speaker C: Why isn't it so natural to like take subjects and test them on a.
00:22:13.065 - 00:22:41.745, Speaker A: Natural language that they have not seen before? Sorry, so like are you an artificial language? Like take English speakers and then test them on Mandarin thing? Yeah, that's also possible. Like we just happen to have this approach. But then like, I guess like you also need like people who speak the language to design the experiments correctly. And I Think, like, you know, you could design the experiments this way. But I think, like, that doesn't have a benefit over, like, artificial languages that I can imagine.
00:22:41.825 - 00:22:44.025, Speaker C: Like, it's a bigger corpus, I guess.
00:22:44.105 - 00:23:08.101, Speaker A: Yeah, it's a bigger corpus, I guess. Like, you could use more naturalistic examples. But I think, like, the vocab, like, yeah, there's. There's a bunch of issues there, but I think like, one big issue that we see is like, you know, we can, like, we can survey the participants and say, like, oh, like, you have. You must not speak this language. But then, you know, there is, like, less guarantee of them, like, never being exposed to that language. Yes, sure.
00:23:08.101 - 00:23:25.667, Speaker A: But then, like, you also need domain experts. We didn't have, so. But yeah, that's. That's an interesting thought that I hadn't thought about before. But yeah, I guess, like, you could design a study that is like, based on kind of a. Kind of a, like, low probability of exposure. Natural language.
00:23:25.667 - 00:24:31.065, Speaker A: Yes. Yeah. So, you know, like, we can maybe label this particular generalization test in our benchmark as human like. But I want to digress here a little bit and maybe pose a somewhat controversial sounding, but not really, I think, question of, like, does it matter? Like, does human generalization, like, does how humans generalize, like, matter in developing these benchmarks? I think this is where interestingly, exactly the goals of cognitive modeling and AI modeling diverge. For example, we know from natural language studies that five plus tail recursive structures or three plus center embedded structures are very, very infrequent and naturally occurring data like corpora and humans often have trouble comprehending these structures. And you can easily see that, okay, if this extended up to like 5 plus or like 6 degrees of embedding, you might need to think about it before comprehending it. Although there is a meaning out there that corresponds to that structure, you could imagine some processing difficulties.
00:24:31.065 - 00:25:18.987, Speaker A: I say that there should be a cognitive model of human language processing if we want to understand the human linguistic competence and performance that is able to account for this difficulty. If our goal is to model humans. But I want to think, I want to throw out a question which is that. But should AI models be subject to the same limitation? Do we want them to fail or give up on these difficult cases, I think the answer is no. But I mean, you could have various other views. So I have this, like, silly little Venn diagram here showing that there are, like, things that are human like, which is important for human science. There are things that are useful, which is important for building useful things, like AI Models, and there is an intersection between them, but they're not completely overlapping.
00:25:18.987 - 00:26:27.781, Speaker A: So benchmarks for AI, I think should reflect destarada for an idealized model to some degree. And then to some degree, human level performance here is like, almost irrelevant as long as we can agree on what the ideal target should be. So that's also not like a trivial thing to define, but that's that. This kind of reflects my thoughts on proposing a benchmark for AI models. So rules of thumb for if you have, like, things that seem hard for humans, we would think about, like, do we want our models to succeed on similar examples or fail? Can we agree on what the ideal behavior should be? Although it is difficult for humans, and if the answers are yes to both of these things, I think superhuman targets that fit these conditions are arguably more useful than human level targets, because at least I want the models to do things that I can't, not like things that I can't. So, you know, that's just some thought. But this kind of line of thinking led us to kind of a shift in the design principle of the two benchmarks that I talked about, which is based on frequency gap instead of human likeness.
00:26:27.781 - 00:27:57.915, Speaker A: So in our initial 2020 EM NLP paper, we tried hard to find examples of human likeness compositional generalization with some idealization like, okay, like, you know, we know what the recursive structures should mean, so like, we should include them. And this one had both lexical and structural generalizations. But in our more recent benchmark called Slog, again EM NLP 2023, we kind of shifted our design principle to include what we thought was useful, which was defined as generalizing from parts of frequent structures to infrequent ones, because, you know, that's a problem that the models will face in like, naturally occurring distributions of language data. And we focused on the structural cases because, as I'll be talking about later, structural generalization seems to pose kind of a unique challenge to the current generation of models. So we constructed a trained generalization set gap such that, you know, the structures included in the generalization set are less frequent by some corpus analysis. And you know, such that, you know, you have to have the parts and the training data so that you have the parts to work with, like in a compositional, in a compositional process. So based on these design principles, this also meant that many of the training test gap we included in our newer benchmark are also untested with respect to human generalization.
00:27:57.915 - 00:28:32.047, Speaker A: So things included things like more frequently modified positions. So we find that in the Naturally occurring human language production data. Direct objects of a sentence are more likely to be modified than subjects or indirect objects. I crossed the subject out because that was already in the original COGS data set. But indirect objects are modified less commonly. You could also have things like WH questions of simple transitives to WH questions with long movement. You probably like if you don't know linguistics, you probably don't know what I'm talking about.
00:28:32.047 - 00:29:28.695, Speaker A: So these are examples like, you know, in the training set you have questions like what did the cat find? And complex sentences with embedded structures. Like, you know, Emma said that this sentence and you want to generalize based on like, you know, knowing the meaning of the question of like simple cases and complex declarative sentences. Can you form questions like about like the complex structures that you saw as a declarative sentence. So here I want to assert that untested doesn't mean infeasible or implausible for humans. So I think these are just like not known. So I think this as well as serving as some sort of a benchmark for AI systems, I think there are still interesting opportunities for human studies in the language sciences zone to understand humans better. Okay, but you know, like, I talked about humans a lot enough about humans.
00:29:28.695 - 00:30:26.281, Speaker A: What about neural networks? So given all of this, how do like contemporary neural networks generalize? So ever since we released the first evaluation suite in 2020, a lot of different papers used our evaluation to test generalization. And I tried to find some high level takeaway from this pile of literature that were built up over the past four years. So I did some meta analysis. So the first finding from that is that lexical generalization isn't actually all that hard for contemporary variants of neural networks. I think I mentioned the T2i failure, but I could imagine there being some complex things about mapping that to the image space and generating stuff. You know, this, these are, you know, when I say lexical generalization is not too hard, I'm talking about like either sequence to sequence or autoregressive language models. And you know, this graph.
00:30:26.281 - 00:30:55.683, Speaker A: What I'm trying to show you here is that a lot of the approaches do achieve good lexical generalization. So this red line here is the percent of lexical generalization in our original benchmark. Unfortunately, a lot of the papers don't report the structural lexical division that we thought was like really important. But just kind of inferring from this, I think a lot of the models kind of like suspiciously hover around this red line. Yes.
00:30:55.819 - 00:31:01.355, Speaker C: What are the rules of the game are they pre trained models that are then fine tuned on data set?
00:31:01.475 - 00:31:27.035, Speaker A: It's mixed. So some of these use pre trained models. Some of these are just vanilla networks of some architectural modification. So I have some opinions about what is fair game. But like people just used this, this evaluation for whatever. So like the literature review I did like includes a mix of like pre trained versus not pre trained models as a base. But like you usually fine tune it on our training set and then evaluate.
00:31:27.035 - 00:31:31.263, Speaker A: Yes. What are these models like for the.
00:31:31.279 - 00:31:34.103, Speaker B: One that do achieve good performance?
00:31:34.279 - 00:32:54.935, Speaker A: Yeah, so I'll talk about it since so, so you know, like I think like this quantitatively suggests that a lot of the like, regardless of like what the approach is like, it's pretty not very hard to achieve full ish lexical generalization and they're kind of maxing out on that and this I think like I'm not going to go into the individual approaches but I think like one notable case is this short s et al. 2021 paper where they showed that just finding the right parameters, like doing very hardcore hyperparameter tuning on a vanilla sequence to sequence Network, maybe the 2017 original Transformer model can actually achieve quite high performance on lexical generalization, suggesting that overall lexical generalization might be achievable without some specialized solutions with contemporary flavors of neural networks. Their main failure mode to briefly discuss is they ignore word order. So they think the meaning of the wug saw the cat, which is the target is what they have seen the cat saw the wug. They ignore word order. I think this reflects my struggle or failure to get the T2I model to generate things that they haven't really seen in the dataset. That's one failure mode.
00:32:54.935 - 00:33:20.037, Speaker A: A different failure mode is that they ignore a combination of unobserved syntactic position in the word. So if you've never seen the wug in the grammatical subject position, they kind of tend to replace it with things that they've frequently seen. So they think the meaning of the wug saw the cat is the meaning equivalent to the dog saw the cat. That's their kind of generalization in a failure mode. Yes.
00:33:20.141 - 00:33:25.029, Speaker C: How much difference is there in these failure modes or something in things like Transformers?
00:33:25.157 - 00:34:03.535, Speaker A: Yeah, there is kind of a kind of an in depth analysis in the original paper. But I think like there was some interesting difference in terms of whether they can form like valid logical representations structurally. Like RNNs tend to make less mistakes about like structurally valid meaning representations. I thought that was kind of interesting. That's one thing I could think of. There are like kind of minor things that are like hard to characterize concretely, but they seem to be quite different sometimes. But like this kind of failure mode is observed pretty frequently in both models.
00:34:03.535 - 00:35:10.259, Speaker A: So to kind of talk about structural generalization, I think like my meta analysis indicates that structural generalization is quote unquote hard. So why do I say that only a handful of solutions achieve non trivial accuracy? If you remember the previous blue bars, the density of the bars that are high are much less dense. It's not shown on the graph. But not so amazing performance on structural generalization also seems to hold up for like, you know, the newer generation of like big ish models. The llama model only gets 40% on the new benchmark when you fine tune the model on it. But what's more noticeable is that the non trivial performance on structural generalization is only achieved by a purchase that incorporate explicit signals about the target linguistic structures. So you know, they, they, so some models explicitly incorporate kind of a symbolic parser in their loop and then they also have like algorithms that kind of decompose the structures that are targeted.
00:35:10.259 - 00:36:10.705, Speaker A: I'll show you an example later on. Or they kind of do some like grammar induction and do data augmentation based on the inferred grammar. So these all involve all the, all the solutions that work currently kind of involve like either latently inferring the structure or like explicitly kind of baking in the known things about target structures in our generalization set. So I coded them in terms of whether like some sort of structural supervision or like some sort of like structural prior from the researchers were incorporated into the architecture design or not. That's like green versus red. And you can see a pretty stark difference here that like you know, like if you don't have those like the models almost completely fail and the main failure modes, you know, the models tend to think that the meaning of the dog on the table saw the cat. Where you don't have observed, you haven't really observed like modification on the grammatical subject.
00:36:10.705 - 00:37:00.611, Speaker A: Like you kind of fall back to known structures. And there was also kind of an interesting bias that we characterized by observing a lot of outputs in like kind of sequence to sequence neural networks and like pre trained autoregressive ones. So there was a bias towards like short distance dependencies to the degree that like they are truly doing novel things that were not in the training data. But that's totally implausible. So, so let's think of the sentence like Emma gave a Cat that slept a fish. So you can decompose the meaning of it. So if you think about what this sentence means, it means something like, you know, the cat is the sleeper and then you know, the like giver of the fish is Emma and the thing being given is the fish and the cat is the receiver.
00:37:00.611 - 00:38:14.835, Speaker A: Right? So that's the meaning of this particular sentence. But if you look at like model interpretation, and often we find these in things like T5 fine tuned on our dataset, llama fine tuned on our dataset, and like kind of a vanilla transformer trained from scratch on our dataset, what we often saw is that, you know, there's this really weird thing where, you know, there is Emma the giver and the thing given is the cat and there is no receiver. And then you create this like transitive meaning of sleep where, okay, like the sleeper, like agent of sleep. So it's like, you know, basically corresponding to a cat slept a fish, which doesn't really mean anything in English. But you know, like, maybe you can think of it as like the cat ordering the fish to sleep. So I say that that's a short distance dependency because then you have like, you know, Emma gave a cat and then like cat slept a fish, like this local dependency. Whereas if you want to have the original meaning, there is like a long distance dependency between like gave and the thing being given intervened by this whole thing that has a verb intervening with it.
00:38:14.835 - 00:39:00.929, Speaker A: So I think this reflects, we can characterize it as like a bias towards short distance dependencies. And the model really wants to do that. And this kind of failure in structural generalization seems to persist in relatively newer work that doesn't explicitly incorporate linguistic priors. So without these kind of signals, neural models seem to almost categorically fail, like kind of in line with the meta analysis that I did. So there was this like recent Nature paper by Lake and Baroni in 2023 proposing kind of a meta learning approach for compositionality. And they also note that meta learning for compositionality, their approach fails to handle novel and more complex sentence structures. The structural generalization in COGS with error rates at 100%.
00:39:00.929 - 00:39:43.997, Speaker A: So there is categorical failure if you don't have kind of linguistic priors. And a similar categorical failure is noted in a more recent 2024 paper by Zhang et al. As well. So similar statement saying we find that our approach SQ transformer doesn't achieve better performance than the vanilla transformer on cogs test examples with deeper recursion depth or a novel combination of modified phrases in Grammatical worlds, which is the modifier generalization. And these are both cases that fall under structural generalization. So the words in square brackets are main. And I'll show you a solution that has been proposed that works.
00:39:43.997 - 00:40:13.281, Speaker A: So those ones have explicit linguistic guidance. So Drostav et al. Interesting people proposed parsing with prompting. So their prompting pipeline is pretty intense. You have like iterative subclass decomposition, you do phrase identification, iterative prepositional phrase decomposition, noun phrase annotation, verb phrase normalization. So each pipeline step is an LLM call with designed prompts and examples to solve our problem. So it works really well.
00:40:13.281 - 00:41:07.813, Speaker A: You get almost 100% but the pipeline involves pretty strong linguistic design. So you know this is a functional solution and the model successfully generalizes. But I think it is because you have this linguistic prior built into the approach. So I think the conclusion here is that no evidence of human like in brackets structural generalization in neural networks without externally supplementing structural knowledge. Okay, so I think I'll end with open questions. So in light of this takeaway, I think the first kind of observation I want to make is the lack of a generalizable scalable solution to compositional generalization, including the structural kind. And many solutions to the generalization problem I discussed today, they're not non existent, they do work.
00:41:07.813 - 00:41:52.515, Speaker A: But it involved providing structural signals through maybe architecture change, data augmentation, linguistically informed prompting or task decomposition, or even like program induction, grammar induction and so forth. But all of these kind of involved heavy design. But I think they tend to be like kind of phenomenon, like things that we precisely included in the data set as a challenge. And also like language tailored. Like you know, it is unclear whether, you know, some languages don't even have prepositional phrases, they have post positional phrases. So if you have prepositional phrases in your linguistic priors that will, it could generalize, but maybe it's a question. So they're language tailored, they're task specific and or hard to scale.
00:41:52.515 - 00:42:49.005, Speaker A: So I think we are still in the lookout for more generalizable and scalable solution. And I think if such a solution is found, that would be really exciting. And I think it could also inspire a lot of new ideas even in terms of human sciences as well as practical applications. But I said practical applications, but when does this kind of a solution matter? So I think our results showed that models are actually pretty good at interpolating within observed structures. So surely like one could say that, okay, then an easy way out is like developing rather than like developing a solution at a fundamental modeling level. Like we could just do like data augmentation or kind of like trust ourselves with the power of data scaling at least for high resource languages saying like okay, like we will cover the long tail by expanding our data, like let's make everything in domain or something. So I think there is future work to be done here.
00:42:49.005 - 00:43:47.203, Speaker A: Kind of want to have a little bit more concrete sense on what is the data scaling and the modeling solution. Trade off in whatever measure that makes sense. And also doing a learning trajectory analysis for pre trained models, maybe with application applications to lower resource scenarios in mind. So I think like compositional linguistic generalization will be really helpful in practice. I think like kind of conceptually I think it will help the most with lower resource languages if we believe that compositionality is a principle that applies to all of the human language. So you know, I think having these scenarios in mind or like even better, like demonstrations of improvements in low resource scenarios would be really cool. And the second issue that I want to mention is like this like with the model of like, with the issue of like LLMs and like huge pre training, huge pre training data and then kind of the opacity that surrounds it.
00:43:47.203 - 00:44:29.661, Speaker A: So you know this paradigm that I keep talking about about poverty of the stimulus, this really really really requires withholding critical examples from the pre training data or like training data. So like you know this generalization makes sense as a generalization task if you don't see certain examples during your training. Right. So if the pre training data is unknown, the intended training test gap can't be guaranteed. Like it's just like kind of mysterious. But also you could also imagine that like oh, it's like really unreasonable to expect the model to have never seen subject modification like concurrent LLMs. So I think that is an issue about exactly.
00:44:29.661 - 00:45:18.001, Speaker A: Because the interpretation of success is unclear. So if they fail, they fail. But if they succeed, is it because they have seen the target structures in the pre training data or is it because they are compositionally generalizing? I think that really makes things murky and it also like empirically leads to overestimation of generalization capacity. And our work shows that. So we try to take the best reported result on like our COGS generalization set and try to quantify the extent of like kind of the effects of seeing certain lexical items that we were meaning to hold out from the training set. So you know, we used real words in as like the held out lexical item. Like things like wog or like these are not real words.
00:45:18.001 - 00:46:18.105, Speaker A: But in our original Benchmark we used words like hedgehog as like never appearing as a part of a grammatical subject. So but then like you know, in large pre training corporate that is just not going to be true. So what we did was like we re generated our dataset with fake words with varying various approaches to generating the novel lexical items. So like random character sequences or kind of like consonant vowel alternations so they are kind of like pronounceable and have kind of more valid transition probabilities between characters and stuff like that. But the kind of, the takeaway here is that all of these approaches when you evaluated on like kind of things that we think help in maintaining the training test gap that we proposed. The models do like the best model with the best reported performance do perform a lot less well. So 14 to 19 percentage point overestimation in the best case.
00:46:18.105 - 00:47:02.145, Speaker A: So I think our tests themselves are challenged by the paradigm shift to models with opaque pre training data. In this paper we propose some remedies regarding lexical generalization, but there really isn't a good way out of structural generalization because controlling for structural exposure is hard. That is the 2.5 evaluation paradigm. I think there is also a critical issue that the semantic parsing setup, like translating from surface form to logical form requires fine tuning. Almost like, you know, always requires fine tuning, which is either unavailable if you have like an API based model that don't allow you to fine tune models. You can't do this.
00:47:02.145 - 00:47:59.783, Speaker A: And it's slightly out of paradigm for the current generation of models. A lot of the benchmarks are taking this like few shot framework and the main issues is that the task requires grasp of the syntax of the meta, like what I call the meta language, which is like the syntax of the logical form itself. So you can't, it's hard to learn from like you know, few shot. I guess like you could have like a lot of few shot examples these days with like long context models but you know, like you can't really teach the model the syntax of the logical form with like small number of n shots. So we're thinking maybe like question answering or like some other downstream task that requires the correct comprehension of target structures rather than mapping to a meaning representation might be a better test for the current general generation of models. And this kind of an attempt exists. But I think there's a lot to work out still in terms of removing obvious heuristics.
00:47:59.783 - 00:48:50.085, Speaker A: I think there's just some things are cleaner if you do the meaning representation mapping than creating an evaluation based on downstream task. But I think if this reformulation gets done well, it could also help establish a better connection between our compositional generalization tests and downstream useful task performance. Finally, there is an open question three which is independently of AI. What we found is that human structural generalization is a little bit underexplored. There's a lot of methodological blockers, this opacity and training data and hard to control for structural exposures. I presented it as a problem for LLMs, but it's also a problem for humans. We don't know what structures people have observed or children have observed before coming into the lab.
00:48:50.085 - 00:49:44.457, Speaker A: It's also hard to design experiments that have this clean structural training test gap that may be also an opportunity for methodological breakthroughs. Okay, to summarize, contemporary neural networks seem capable of matching human generalization patterns for lexical generalization that fail at structural generalization without being explicitly supplemented with linguistic structural information or priors. And I don't think there is a general scalable solution that doesn't require hand designing of some of the components of the model. Exciting opportunities here. Well, at least I'm excited about it. And I want to note that evaluation is increasingly becoming harder in our framework with current models due to the opacity of pre training data and the requirement to maintain this training test gap. And.
00:49:44.457 - 00:50:09.095, Speaker A: But I think like overall thinking about these problems also provides good opportunities for understanding human generalization better. At least I took that as an opportunity. Yeah. So I think a lot of exciting future branching directions and thank you. And here are my collaborators. And a cat that I left behind in Boston who jumped into the carrier when I was trying to bring him to a Sutter. Smart guy.
00:50:09.095 - 00:50:13.995, Speaker A: Yeah. Okay. I'll be happy to. Do I have time? Yes. Yeah.
00:50:19.695 - 00:50:21.675, Speaker B: I think we have time for a few questions.
00:50:22.015 - 00:50:22.795, Speaker A: Yes.
00:50:24.055 - 00:50:49.433, Speaker D: So when you. During the entire talk you haven't mentioned stuff like common sense and how that could help disambiguate like linguistic structures. Like maybe there are two parts of the trees. But then I wouldn't. Without common sense, I probably would make. Would see like a. A cat slip a fish a grammatical sentence, but with a common sense at home.
00:50:49.433 - 00:50:52.153, Speaker D: Like that just doesn't make sense to me.
00:50:52.329 - 00:50:59.329, Speaker A: Sorry. Like, so what wouldn't make sense to you? A cat. I gave a cat that slept a fish.
00:50:59.497 - 00:51:01.377, Speaker D: That just doesn't make sense to me.
00:51:01.441 - 00:51:06.285, Speaker A: Oh, really? Like, okay, maybe I don't know. Like I don't know if people agree.
00:51:06.625 - 00:51:08.725, Speaker D: I guess it's just the common sense.
00:51:09.305 - 00:52:05.479, Speaker A: Yeah, yeah, yeah. But I take your question. Yes, I think Like I understand your question, so I think like there are two parts of it. So I think like there is kind of the, the linguistic structure building system that I wanted to kind of like emphasize in our work. And there is also the real world inference that really connects to what the lexical semantics of the individual words mean and how plausible they fit together and things like that. So I think we wanted to separate that a little bit here to tease apart the structure building compositional machinery versus like the commonsensical stuff. So I think like it is totally fair to think about like, okay, like what is like what is the generalization scenario where common sense is really recruited to tease things apart? I think that would also like kind of require this kind of like thinking about like what we need in a benchmark.
00:52:05.479 - 00:52:43.105, Speaker A: Again about like, okay, like what do humans do? And then like what do we want the models to do? And kind of build those scenarios up to make it into a benchmark. So we didn't do that. Like we didn't really do that. And that's why I didn't talk about it because like we kind of ignored it and like we treated it as like symbolic manipulation. Although in our like synthetic like training test example generation, we try to at least like take into account some kind of restrictions, semantic restrictions about like what verbs go with other things. So like we did have some of those considerations so even the pre trained models wouldn't be so thrown off. Although like, you know, that's kind of crude.
00:52:43.105 - 00:53:07.445, Speaker A: So like there are some corner cases where this doesn't seem to hold up. So there are some considerations but like to really kind of build a benchmark about like how does human common sense like aid compositional generalization to novel structures? I think like you need to kind of start from ground up and like think about what the problem actually is and build a benchmark for that. But I think it's an interesting question.
00:53:09.205 - 00:53:22.565, Speaker C: Yes, I'm curious for these cases where humans did well, like the language with the geometric shapes kind of crashing. If you wanted to test rather than fine tuning how much in context learning works for these types of things, how large would the context be to be?
00:53:22.605 - 00:54:31.585, Speaker A: Like, yeah. So I think like if we wanted to keep the logical form mapping paradigm, I think you would need a lot of illustrative examples because you just don't know the syntax of the meaning representation. But if it is just like, oh, like I want the model to generalize to understanding of this particular complex expression, then like, you know, maybe you could construct if it's like a qa about that particular sentence then like maybe you don't need that many fuchsia examples because like the model probably knows how to do question answering and like knows what question answering is from its training. But I think like there you have to like there I think like you need knowledge about like what went into the pre training data still because like that defines what counts as generalization. So I think like depending on how you construct the generalization task, the need for like few shots like the N and the end shot would differ. But I think given this like pre training testing paradigm, you need like a deeper understanding of like what went into the pre training in the first place.
00:54:32.125 - 00:54:38.045, Speaker C: But is it not like, I mean, okay, somehow it still feels like apples to apples comparison in the sense that you don't know what went into a human.
00:54:38.085 - 00:54:38.213, Speaker A: Right.
00:54:38.229 - 00:54:49.941, Speaker C: You don't know the human priors that much as well. And neither somehow it feels to me that if you give a human N examples or something examples to a pre trained model.
00:54:50.013 - 00:55:21.091, Speaker A: Yeah, that's fair. So I think one direction that I am actually kind of going into is using artificial language learning for models. So like this, if you create an artificial language you probably have guarantee that the model hasn't been exposed to this language. So I think like for that like you like that's kind of a nice compromise and then maybe you can give as many examples as the human subjects saw in the human experiment. And that's fair. Yeah, I think so. I think so.
00:55:21.091 - 00:56:00.645, Speaker A: But I think like you know, I think the generalization test or like a benchmark shouldn't be tailored to tokenization problems. I think it should be kind of designed with general applicability in mind. I think depending on the tokenization there could be generalization behavior. That's interesting. I think the blue bars actually showed that because we use different words that's probably tokenized in different ways to represent a word that's restricted context. So I, so I think there would be a quantitative effect and that would be interesting. But I think the benchmark itself should just make a decision like not consider.
00:56:01.585 - 00:56:18.379, Speaker B: For people who don't know much about linguistics, we sometimes use simple synthetic tasks that we think are proxies all languages and then we say we can learn about how the model performs. All languages use these proxy. What do you think is the major missing components in these synthetic texts?
00:56:18.387 - 00:57:04.465, Speaker A: That's not reflective. So I think it also depends on the claim or the goal of the benchmark. So I think even if it's not based on linguistic insights, you can still have linguistic tasks defined that are argued to be useful. So I think if you can well argue for the utility of your data set and propose it as an AI benchmark, I think that is fine. But I think you just need. But then maybe it's harder to use it for human likeness claims saying hey, my model achieved this in my data set, so it is human. Like I think that claim might be a little bit far fetched if there isn't grounding in the human language literature.
00:57:04.465 - 00:57:05.585, Speaker A: Yeah.
00:57:09.015 - 00:57:10.155, Speaker B: One last question.
00:57:10.895 - 00:57:11.687, Speaker A: Yes.
00:57:11.871 - 00:57:13.735, Speaker B: I think a lot of power of.
00:57:13.775 - 00:57:16.295, Speaker A: Generalization, the cool things that happen these.
00:57:16.335 - 00:57:19.495, Speaker B: Days are coming from massive pre training.
00:57:19.655 - 00:57:23.039, Speaker A: And then somehow generalization ability emerges from it.
00:57:23.167 - 00:57:36.805, Speaker B: But in this setting you take that and then you fine tune on your specific data set. So I'm wondering whether, do you think it's possible that some generalization ability gets destroyed during the spine tuning process?
00:57:39.105 - 00:58:18.605, Speaker A: Yeah, yeah, that's I think like that's fair. So I think like the need for fine tuning like so I think like, like I said like for to correctly use this data so you kind of need fine tuning. Right. So I think like we want to also kind of move beyond that paradigm a little bit. And I agree that like there are certain things that might be more interesting in terms of findings. If you could just directly take the models and like test generalization in a, in kind of a like you know, trustworthy way and also tests for like exciting generalizations. But you know like then if it's generalization with respect to the pre training data, you need an understanding of it.
00:58:18.605 - 00:59:10.223, Speaker A: But I guess like you, you could also define like harder tasks that like you can likely think that like you haven't the model hasn't seen during pre training. So like if there are like exciting capacities that can be tested and could be attributed to compositional generalization that is already hard for like pre trained models like in a setting that doesn't require like fine tuning. I think that would be ideal. But like I don't have that benchmark yet. And then I think like people, it would be really nice people if people could work on it. I think like one application though is like and if does better compositionally generalizing models like measured by this benchmark mean faster training on lower resource languages and things like that. I think that hasn't really been explored to my knowledge.
00:59:10.223 - 00:59:24.125, Speaker A: So I think that would be a real exciting capacity that could be demonstrated. Faster acquisition of lower resource or new languages. I don't know if anyone's working in that direction but that would be exciting for me.
