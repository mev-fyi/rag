00:00:01.840 - 00:00:33.170, Speaker A: So this is joint work with Felipe Kukr, who I'm going to tell you about. So my first transparency is just motivation. Actually, there's no need for motivation in this workshop. It's dedicated to studying how to solve polynomial equations. This, of course, a very fundamental problem, and we know it's an NPR problem. If you look at it over the field with two elements, it's NP complete. It's also NP complete if you look at it over the complex numbers in the blue shop smell model.
00:00:33.170 - 00:01:12.806, Speaker A: There has been a lot of work in symbolic computation. It has been studied in detail, not only practically, but also in terms of complexity. But the problem is, with all these methods, the running time somehow is exponential. So if you have like D is an upper bound on degree of the polynomials, and n is the number of variables, you always have this d to the n in the upper bound. So if you, for instance, if you have a system of quadratic equations, it will be something like two to the n. Okay. In all these symbolic methods.
00:01:12.806 - 00:01:41.166, Speaker A: But the question is whether one can do better. Nowadays, there's this field of numerical algebraic geometry that is growing. We have heard several talks about that in this workshop. And these methods can go much further. They can solve much bigger problems, they perform better in practice. And the goal of my talk is, I try to understand from the complexity point of view why this is the case. So I want to, that's the goal.
00:01:41.166 - 00:02:21.016, Speaker A: And somehow hoping to, in the end, it will require a lot of research. But at the moment, there is a huge gap between what we can see in practice and what we can actually prove. And we try to narrow this gap. So it's always good to have some light tower or leading problem. And there was a problem that was posed by Steve Smale some time ago. And it reads as follows. We have given is a system of complex polynomial equations, n polynomials in n unknowns, and we want to solve it.
00:02:21.016 - 00:02:47.762, Speaker A: We want to compute the solution. How difficult is that? And he asked, can it be, can such a solution be found approximately on the average in polynomial time by uniform algorithm? So these expressions, these words in blue at the moment, are not precisely defined. I will going to define them later on. But this is the problem. Yeah, we ask for polynomial time. We don't want to go away from these two to the n. Okay.
00:02:47.762 - 00:03:12.974, Speaker A: And this problem has its origin in a series of papers written by Mike Schub and Steve Smale at the beginning of the nineties. They all have in the title complexity of Bezou theorem. And then it goes 12345. And then it stopped. And. Okay. And they partly, in that paper, actually in Bezou five, this problem was partly solved, but not completely.
00:03:12.974 - 00:03:44.366, Speaker A: So, okay. And then later on, Veltran and Pardo took up this story again, and they had a very nice idea that I'm going to explain. And they came up with an answer, with a positive answer to this problem by Steve Smale. Positive. When we allow randomized algorithms. Okay. And, okay, let me briefly explain the work that I've done with Felipe.
00:03:44.366 - 00:04:30.960, Speaker A: What are our contributions? So, in a sense, there are different results, and we enlarged kind of the toolbox. So, one result was that we found a deterministic algorithm for smale seventeen's problem, and it's almost polynomial time. So if n is the input size, and the polynomial is given in dense representation, capital n is the input size, meaning that number of coefficients. So it's not bit size, then the expected running time. Well, okay, expected, the algorithm is deterministic, but the average over the input space in a certain mean. I will explain you exactly what it means. Then the average running time will be n to the o log, log n.
00:04:30.960 - 00:04:54.484, Speaker A: So it's almost polynomial. And actually, what we prove a little bit more, if you restrict ourselves to systems of bounded degree, think of a system of quadratic polynomials, then it's actually, the expected running time is polynomial. So in this case, it will be n squared. Okay, so this was one result, the other result. Okay.
00:04:56.784 - 00:05:05.884, Speaker B: So d is. So you have d prefixed. Right. Then it's polynomial.
00:05:06.044 - 00:05:07.788, Speaker A: There will be more details. But you see.
00:05:07.836 - 00:05:08.644, Speaker B: Okay, I'll wait.
00:05:08.684 - 00:05:46.950, Speaker A: There will be more details. There will be, this is just overview. There will be more details, there will be more precise statements. So if you'll be patient, this is just the overview. So let me briefly talk about this idea of smooth analysis. So we want to analyze algorithms, but what traditionally has been done in computer science is the worst case, worst case and sometimes average case. Yeah, especially when you have algorithms like the simplex algorithm, where we know it works well in practice.
00:05:46.950 - 00:06:20.284, Speaker A: But there are bad inputs, they are very rare and hard to construct, but still they exist. So you want to have a theoretical justification why the algorithm is good. So what people did, they did an average analysis. But the problem is the average analysis is not so convincing because the input is never follows these distributions. So there was this great idea by Den Spilman and Shan Wang Teng more than ten years ago. It smoothed analysis. They said, pictorially, if you want, it's very easy to explain.
00:06:20.284 - 00:07:08.692, Speaker A: So this is your, you have your input here, and then you assume that it has some noise. Okay, maybe, maybe you model this by a gaussian, that this has a mean f bar and some variance sigma squared. Okay, so you have, and then what you do, if you have a, let's say t is the running time of the algorithm that you are studying. So this is a function of your input f. And then what you do, you take the expectation over this noise. So this is a local expectation. And then you want to bound this and you want to bound it over all centers, f bar, you don't know, you don't care where this is.
00:07:08.692 - 00:07:44.240, Speaker A: You want to bound this by a small quantity. And if you can do that, then you're happy. Of course, this quantity will involve this variance sigma. And if sigma is very small, then this will grow. Okay, so this is the idea of smooth analysis and what we managed to do, we managed to perform a smooth analysis of this algorithm, this randomized algorithm that was proposed by Beltran Pardo. So we get smooth expected running time. So this is the overview, but I write so, okay, this of course was a bit vague.
00:07:44.240 - 00:08:26.904, Speaker A: Now, in order to state this precisely, I have to introduce notation. So this is an important slide and I will come back to, it contains a lot of the definitions. So let's go through it. So I fix some degree vector d d one up to dn. And this will define my input space, denoted this calligraphic d hd. So what are the elements of Hd? These are n tuples of polynomials of homogeneous polynomials. F is f one up to f, and f I is a homogeneous polynomial of degree di in n plus one variables.
00:08:26.904 - 00:09:02.024, Speaker A: Okay, so the input size is just the dimension, the complex dimension of this vector space. What is the output space? It's complex projective space. So I would say, well, in the end we will want to analyze an algorithm. So we want to make use of the geometry of the problem. So everybody in algebraic geometry agrees that complex projective space is the nicest space you can think of. That's why we look at projective situation of a fine situation. So everything is homogenized.
00:09:02.024 - 00:09:23.932, Speaker A: The output space is complex projective space. So we look for a zero in this space of the given f. Okay, then we need to measure distances. So we have a metric on the complex projective space, which is. Think of it in terms of an angle. It comes from the so called fubini study metric. That's not so important.
00:09:23.932 - 00:10:05.022, Speaker A: Think of projective space as something like a sphere and we are measuring angular distance. I denote it by D. Also, I have to measure distances between inputs because I want to talk about condition. I want to say what happens if I have an input f and I perturb it a little bit. Now, it's important to choose the right distance measure. So what I do, and, well, what I do, what Schubert's male have done all the time, they choose an inner product on the input space HD, that is invariant under the action of the unitary group. So we have a symmetry here.
00:10:05.022 - 00:10:45.898, Speaker A: We can have an orthogonal transformation on the input space in the natural way, of course, this induces orthogonal unitary. I wanted to say, of course, this defines a transformation, an operation of the unitary group on this space. There is not much choice here. So we choose one of these hermitian inner products, let's call it the while inner product. Once I have this, I can talk about the norm of an input and about angles. Okay, about angles. So I have this notion of angular distance.
00:10:45.898 - 00:11:03.402, Speaker A: Okay, good. And then last notion on this slide. This is what we call the solution variety. So it's an incidence variety. It consists of all pairs f and zeta. F is an input, it's an element of HD. Zeta is a zero.
00:11:03.402 - 00:11:38.006, Speaker A: Okay, so this is a subset of this algebraic variety. One can prove it's a manifold. So I think it's important here to understand that. Okay, it's a design question. What should the input space be? Now, if you see, if I have this f here and I multiply f by hundred, what will change? Nothing. I mean, the zeros will be the same. So one could say maybe the input space should rather be the projected space that belongs to HD.
00:11:38.006 - 00:12:04.624, Speaker A: Okay, I could do that. I could also scale this f one up to fn individually. This is another possibility that don't do it here. But for some reason and for our techniques, it will be very important that you also have this notion of norm, even though it doesn't have a meaning for the problem. It doesn't have a meaning for the problem directly, but it will be very important in the analysis.
00:12:05.204 - 00:12:12.284, Speaker B: Okay, so why is this solution variety smooth? It's a fiber bottle in this direction.
00:12:12.624 - 00:12:38.384, Speaker A: Thank you, Frank. I couldn't have answered it. Thank you. Okay, condition number. It's a vector bound. It depends on the vector bound, on the fiber bound, actually, this is important.
00:12:38.724 - 00:13:02.536, Speaker B: Let's say we just have one quadratic equation, right? And then we have this hypersurface in Pn. When the quadratic equation drops rank, the quadrant becomes singular, the projection onto Pn becomes singular. But you're saying the fiber over. No, you're looking at, it's an instance variety. Instance varieties are usually smooth fiber bundle. So I see, this is desimulated.
00:13:02.560 - 00:13:08.240, Speaker A: It's an incident. Sorry. For each second coordinate, it's a linear space. Non zero linear space.
00:13:08.312 - 00:13:10.800, Speaker B: Oh, sure. Yeah. Okay, I got it.
00:13:10.832 - 00:13:11.456, Speaker A: Okay.
00:13:11.600 - 00:13:13.976, Speaker B: It's a great trick. Everybody should know that.
00:13:14.080 - 00:13:36.748, Speaker A: Yeah, yeah. Okay. I agree. It's a great trick. Okay, so what is condition number? So now, very simple. Our input is f, and we have a perturbation of f. Maybe the data is inaccurate and I don't know what.
00:13:36.748 - 00:13:55.614, Speaker A: Then it will have some impact. The solution zeta will change somehow. And we have to quantify this. So this can be quantified by the condition number. So what happens here? So you see here, I wrote it here just as a definition. But let me explain a little bit. That's very important.
00:13:55.614 - 00:14:33.632, Speaker A: So what is this condition? Now, this is just a formula, actually. Okay. Maybe I should explain it. So I just, okay. I want to convince you that what we do here is very, very natural, is very, very natural. So this f theta is in the solution variety. And then I project it down to f.
00:14:33.632 - 00:15:00.100, Speaker A: Okay, now let's call it PI one. So what we do, we take the derivative. So we have a tangent space here. This goes to the tension space here, which I can identify with this. Okay, this is the derivative. I don't write the argument. Okay.
00:15:00.100 - 00:15:32.164, Speaker A: If the zeta happens to be a simple zero of f, then this is a subjective map. Actually, it's an isomorphism. It's an isoform isomorphism. So I can locally invert it. So this will be nice. And so I can locally invert this map. So then I get this map g.
00:15:32.164 - 00:16:06.634, Speaker A: I call the solution map. So this map just takes the input f and gives me zero, zeta. You see, if you have one f in general, you will have many, many zeros. I mean, by Bezou theorem, you have bezou number, which is the product of the degrees, many zeros. So you pick one of those, but locally you have this move dependence. Okay, so how do we define the condition number? So we write. So we have this gap.
00:16:06.634 - 00:16:37.356, Speaker A: This is g. What? We take the derivative of G. Okay, so this grows from the tangent space of the input space to the. Okay, it would go to the tangent space of the projective space. It's a linear map. It's a linear map. And then, so, okay, now on these vector spaces here we have, we have norms because here on this hd, we have chosen the Hermitian in the product.
00:16:37.356 - 00:16:49.756, Speaker A: So we have a norm here. On this space, we also have a norm. So we can talk about operator norm of this derivative. Can you switch pens to one that.
00:16:49.940 - 00:16:52.772, Speaker B: Throw that one away and the one just to the right of this handle.
00:16:52.948 - 00:17:25.550, Speaker A: I'm sorry about. We take the operator norm of this linear map, and this is, this is the condition, this is the condition number, essentially. Okay, here one has to, in order to make, turn it into scale invariant, we put this norm of f here. But that's what it is. So you see, what I want to stress here, that this is like a very general framework we are working in, and I explain this framework to you in the setting of polynomial system solving. But it's very, very general. Okay.
00:17:25.550 - 00:17:58.464, Speaker A: Mike Schube will this afternoon talk about the problem. To compute an eigenvalue eigenvector, the same methodology applies. Okay. Okay, maybe what does, right, so this is the condition number. So if you work this out, this actually, it's an exercise in calculus. So if you know calculus, you can do this exercise, and then you will get essentially this formula. So what you get here, the derivative of f.
00:17:58.464 - 00:18:18.888, Speaker A: Think of f as a map from zn plus one to zn. You take the derivative. This is a matrix. It's an n times n plus one matrix. Then here I decorate it and multiply it with this diagonal matrix. This is just for technical reasons, actually, you don't get this when you do this calculation. It's not important for the purpose of the survey talk.
00:18:18.888 - 00:18:52.372, Speaker A: Essentially, m is this matrix, the derivative of f at zeta. You take the Moore Penrose inverse, the norm. Then you multiply with this. And why do I have to do this? I want to have a notion that is invariant under scaling. If I multiply f by hundred, the condition number shouldn't change. So what happens if I multiply f by hundred? This matrix is multiplied by 100. Because I have the inverse here, it multiplies by one over 900.
00:18:52.372 - 00:19:23.082, Speaker A: But because I have this, it compensates. So it's a scale invariant. Excuse me, but this works well. Even if DF is a singular matrix, all of the theory, only I'm, only I'm focusing on simple zeros. If there are multiple zeros, I can't say anything, but I will do this. Local averaging, I don't see the singular solution. So, okay, think of the, everything is smooth.
00:19:23.082 - 00:19:55.330, Speaker A: So, okay, so to summarize, this condition number is well defined, actually, on the projected space of hd times pn. Okay, I have to go on. So then what we do here, I have to explain the algorithm. We do a Newton iteration, as everybody does. So everybody knows what is Newton iteration here. The only twist to this problem is that we have this projective space. So you have there are some choices to do, and I don't want to go into the detail.
00:19:55.330 - 00:20:44.126, Speaker A: Believe me, it's quite straightforward to come up with a Newton operator n sub f, with a rational map from pn to pn that I can iterate. And if I have a starting point x zero, then I do this iteration and hopefully it will converge quickly to a zero. Okay. And this hopefully has been turned into a theorem by Steve Smale, sometimes called this the gamma theory. Gregorio has talked about it in one of our seminars and even proved it from the f and k. So what is this theorem saying? Well, we all know that if the starting point of the Newton iteration is sufficiently close to a simple zero, then we have quadratic convergence. That's what we learn in our classes in numerical analysis.
00:20:44.126 - 00:21:34.874, Speaker A: But what does it mean to be sufficiently close? And here is a quantitative statement saying that if your zeta is the zero, x zero is the approximation. If this distance projective space is less than some constants, forget about the d, essentially less than one over the condition of the pair f theta. And then if you start your Newton iteration at x zero, you have immediate convergence with graduate speed. Okay, big amount theorem. So, okay, what else do we need? Ah, okay, what we say here, we say x zero is approximate zero of f, if this inequality is satisfied. So this maybe explains one of the notions that appeared in the statement of smale 17. So this has a precise meaning.
00:21:34.874 - 00:22:08.394, Speaker A: So now then, another idea that we are all familiar with, I mean, homotopic continuation. Charles Wampler explained in detail what this is. So I think I can be quick here. It's a very beautiful idea. So you have to choose a start system. You choose your favorite system consisting of the g and with the zero zeta. So it's an element of the solution variety.
00:22:08.394 - 00:22:48.304, Speaker A: It should have nice condition, of course. And then what you do, somebody gives you the input f, and then you connect f and g by a smooth curve. For simplicity, let's assume it's just a straight line segment, is the simplest one can think of. So I connect those by this line segment and right. And then let's assume that this line along this line segment here, I denote this by these polynomial sessions by q index t. Let's assume that none of this qt has a multiple zero. Then I can continue the zero theta from g to f.
00:22:48.304 - 00:23:19.214, Speaker A: So under this assumption is none of the qt has a multiple zero. There exists a unique lifting of this map. T goes to qt to a solution path in the solution variety. This is actually a well known topological argument. Maybe I can make a drawing here that I can reuse later on. You think I can remove the pizza? Information?
00:23:20.034 - 00:23:20.930, Speaker B: Everybody will be there.
00:23:20.962 - 00:23:49.404, Speaker A: Just remove a beer information. If you take notes of the important things. So what I have here. So zero. So my polynomial system, g, is a vector in HD. So this, this board is h, sub d. And.
00:23:49.404 - 00:24:24.704, Speaker A: Okay, here I have f. Okay, and then what did I say? I connect those by a straight line segment, and then I just go from here to here with constant speeds. Let's say this is q, sub t. This is one minus t, g plus tf. Okay. Now, somehow, with this picture, I do emphasize the euclidean nature of this picture. But on the other hand, I said everything happens in projective space.
00:24:24.704 - 00:24:49.718, Speaker A: Projective space is hard to draw. So I draw a sphere, or if you want, this is a circle. So actually, I can project everything here on the sphere. So the length of the vectors actually don't matter. The only thing that matters is where you are on the sphere. So it's a thing about angles. I go from here to here.
00:24:49.718 - 00:25:17.564, Speaker A: But I will explain later why this is important. Okay. Okay. So then the question is, what is the step size? I mean, if you go from here to here, I mean, you will subdivide. You will have steps of different sizes. And then one can figure out the following algorithm. You have a step size here.
00:25:17.564 - 00:25:34.804, Speaker A: Assume you are at Qi. You want to go to Qi one. And here, what do I suggest? The step size should be proportional to one over the squared of the condition number at the approximation. You are currently. It's a suggestion. Yeah. And I can do it this way.
00:25:34.804 - 00:26:06.304, Speaker A: But just keep in mind that this distance here, this is an angular distance. This defines an algorithm. I'm a bit dizzy. So this was lunch. I had too much lunch. So if I am okay, I have this approximation, and what do I do? I replace Qi by Qi plus one. Okay, I move on a little bit.
00:26:06.304 - 00:26:45.086, Speaker A: And what is the idea? The idea is I should. Okay, I want to have that ci is an approximate zero to qi in this strict sense that I defined before in the. In the sense of the gamma theorem. Now, what I do is, if I go from Qi to Qi plus one, I want to make sure that ci is still an approximate zero to qi plus one. So I know that, and I want to make this formula. I need to keep track of what happens with the condition if I go from here to here. So.
00:26:45.086 - 00:27:22.948, Speaker A: But this can be settled by some lipshit's property. Okay, so if I figure out, if I do it this way, then what I do in order to get the Zi plus one, I apply one Newton iteration here. But with regard to the qi plus one, this is what I wrote here. So this is the definition of this adaptive linear homotopy. Adaptive linear hypothy Alh. Okay, so this is the algorithm. If you want, you can implement it.
00:27:22.948 - 00:27:47.648, Speaker A: And it's, the question is, oh, my time is running. The question is, what is the complexity? What is the complexity of this procedure? Okay, let's denote it this way. K like complexity depends on this triple. So G zeta is the input pair. F is, now is the starting system. F is the input pair. This is the number of Newton continuation steps.
00:27:47.648 - 00:28:18.368, Speaker A: Okay. Okay. Then basically there was an analysis of this procedure. Implicitly was already in the Bezou series. But Mike Shub wrote an important paper, Bezou six, where he essentially proved this. He bounded the number of steps of this homotopy method by this integral. It's the integral of the squared condition number along the curve in HD.
00:28:18.368 - 00:28:52.974, Speaker A: So in this situation, it would be along this line segment, but it's a very general result. So it's a curve integral. So if you want to understand the complexity, we have to deal with this integral. So, okay, what are several issues? Okay, it's a long story and it's hard to explain. We tell you in an hour. So what was Gamma dot in the last one? Gamma is the. Okay, it's the continuation.
00:28:52.974 - 00:29:05.932, Speaker A: Yes, it's the solution path, but thanks. It's the solution path. Solution path. Oh, and it's an error. There's an error. Should be. Sorry.
00:29:05.932 - 00:29:17.904, Speaker A: It should be qt dot. I am sorry. Thank you. This is an error. Okay, thank you. Okay, how to choose. This looks.
00:29:17.904 - 00:29:58.704, Speaker A: No, everything is defined except for the question how to choose the start system. And actually, this is the most difficult problem. I can prove that almost all elements in the solution variety are good in the sense that the condition is bounded by a polynomial in n. But we don't know how to efficiently construct such a starch system. It's this problem that we encountered here during this problem is the problem to find hay in haystack. So take any one at random, it will be good. Nobody knows how to construct it.
00:29:58.704 - 00:30:58.144, Speaker A: So, okay, but what do we do if we have this? Hey, in haystack situation, we try randomization. Okay, so, okay, so let me, what does randomization mean? So what we could do, we could say, okay, let's choose g uniformly at random, where, well, I said the norm of g doesn't play any role. So let's, for instance, choose g in the sphere belonging to HD uniformly at random. Alternatively, we may say, okay, let's think of gaussian distribution. Let's choose g according to the standard gaussian distribution. So, using this density, okay, so if we have a g that is chosen this way, and we normalize it, what we get is, of course, we get the uniform distribution on the sphere. It has no implication on this algorithm.
00:30:58.144 - 00:31:32.856, Speaker A: Right? So let me define a distribution of a solution variety in two steps. Just, it's a mind. It's a thought experiment. It's not something that you actually do. First, you choose your g here from the standard gaussian distribution, and then, almost surely, it will have bzool number many zeros. You choose one of them uniformly at random. Okay? This way, you get a pair g, zeta, and I call it the standard distribution.
00:31:32.856 - 00:32:20.374, Speaker A: Okay? Of course, you can't do this efficiently this way, because we are talking about efficiently solving systems of poly equations. But you can turn it around. This was a very nice observation by Carlos Peltran, Luis Miguel pardo. And they realized one can efficiently sampling the solution variety by kind of turning this around. And if we have this, then, okay, we find this using randomization and use this pair then, as to start the homotopy. So, we get Las Vegas algorithm Lv. So, on input f, we draw the start system at random, and then we run the adaptive linear homotopy.
00:32:20.374 - 00:32:54.590, Speaker A: This is an algorithm. A randomized algorithm. Yes. Choose one of the zero second, what's the mean? Choose? I mean, it's for you in general. Okay, this is a way of defining a probability distribution. I say choose, but in print, I define a probability distribution. Okay? I mean, if I have chosen g, standard gaussian, then I know almost surely, I have bzu number, many zeros.
00:32:54.590 - 00:33:06.844, Speaker A: They exist. Yes. Then pick one of them at random, uniformly at random. I think of it. This defines a probability distribution. I don't actually do it. It's just to define it.
00:33:06.844 - 00:33:53.704, Speaker A: Good. So I have an algorithm uses randomization now, okay, it's a bit tricky. So because it uses randomization, I I want to talk about expected running time. So what I have here, I have this function k, f, g, zeta, and I take the expectation over g and zeta. Then what I get is a function which only depends on f, k f, the complexity of the input f. And then again, what Beltrano pardo proved. Okay, now let's take the average of kf over all f.
00:33:53.704 - 00:34:15.896, Speaker A: So this is another. Okay, this is maybe very confusing. Like, the g zeta is like random coins, something like it helps you in the algorithm. And this is taking an average over the input space. Okay, they proved this beautiful result. The expectation of kf is bounded by this very small quantity, capital d. I probably forgot.
00:34:15.896 - 00:34:36.832, Speaker A: What is capital d? It's the maximum of the degree of the di. I think I forgot to write it. I'm sorry. No, you have to distinguish this d, which is the maximum on this calligraphic d, which is the bezel number. It's not the same. Okay, so this is a small d. It's just.
00:34:36.832 - 00:34:58.604, Speaker A: Okay, if quadratics, quadratic symbol is just two to the three half times number of coefficients times number of variables. So it's a very small bound. Okay. It's very polynomial, much, much less. That's the amazing thing. That's very amazing.
00:34:58.644 - 00:35:01.620, Speaker B: But I guess that's because you're just finding one. You think it's because you only have.
00:35:01.652 - 00:35:10.224, Speaker A: 10, of course, if you want to find all. But if you try to find just 10 using rapid basis, you know.
00:35:11.124 - 00:35:13.132, Speaker B: Okay, that's why we use numerics.
00:35:13.308 - 00:35:20.044, Speaker A: That's right. That's right. And. Okay. Okay. Okay. Now what is this moved exactly.
00:35:20.044 - 00:35:53.932, Speaker A: At least I wanted to state the theorem correctly. So, again, back to the smooth analysis. At the beginning of my talk, I just had this drawing, but now it's more formal. So what do I do? I fix an f bar in HT, and I have a positive number sigma, which is like the standard deviation. Then I look at the isotropic Gaussian on the input space HD with mean f bar and variance sigma squared. And the density is this. This expression just defines it.
00:35:53.932 - 00:36:05.554, Speaker A: And then we have this usual notation for denoting this. Okay. And then we have. There is a technical issue. Okay, don't worry. For. Okay.
00:36:05.554 - 00:36:38.094, Speaker A: For the purpose of the analysis, we do truncate discussion. So, what do we do? Okay, so this, the expectation of the norm of f is roughly square root of two n. So, believe me, it's. So we truncate there. I mean, actually, what you. I don't want to write the density, but you can also truncate that ten times. This, it doesn't really matter.
00:36:38.094 - 00:37:13.628, Speaker A: So we don't want to have this distribution, to have a tail, so we just cut it off. But because it goes down exponentially, it doesn't make a big difference, but it simplifies the analysis. So we truncate it. But that's a technical issue. So I denote this by this index, t here. Then this is the result with Felipe, the smooth analysis of these Las Vegas algorithms is this. So you see, now we take the local average here over the function kf, you can take the supremum over all f bar.
00:37:13.628 - 00:37:51.600, Speaker A: So actually I have to put this condition here that the norm of f bar is one. Otherwise this is not a good model here. So otherwise I should scale the variance with sigma. That's the supremum over this. Local expectation is bounded by this. You see, it's like in the Beltrampardo d to the three half capital n small n divided by sigma. Or is it Sigma square sigma? Okay, so, so that's okay, I think so.
00:37:51.600 - 00:38:13.696, Speaker A: I think. Now you see. Okay, you can think of yourself what the meaning of this is. I mean, it's, it's, I think it's a much better result than just having average. You take any system, you perturb it a little bit, then you run the algorithm, then you solve something. Of course, you don't solve the original system. You solve a pert system, but you find a solution very quickly.
00:38:13.696 - 00:38:58.820, Speaker A: May I ask a question? Yes, sure. So I guess it is isotropic gaussian. That means you are perturbating in every direction, right? Right. But you know, for, depending on your fixed part, your direction, closeness to the discrepant variety could change. One direction could be worse than the other. Is there a way to perturb this a little bit and make it nonisotropic depending on your fish part? Yeah, it has to do, I mean, this question is related to the following question. When you, I mean, you want to do the homotopy, and when you do the homotopy, you want to avoid the discriminant variety.
00:38:58.820 - 00:39:24.664, Speaker A: So, I mean, you cannot come up, you can come up with all kinds of clever ways of avoiding the discriminant variety, but I don't see a simple way that I could analyze. You can come up with all kinds of ideas. If you want to do an analysis like we did, you have to come up with something very clean. Otherwise it's impossible to analyze. Yeah, of course. Yeah, yeah. But you're right.
00:39:24.664 - 00:39:42.436, Speaker A: You're right. So some steps are more dangerous than some directions are more dangerous than others. Okay. Okay. And then let's go quickly. So what did we do? This near solution to Sumer seventeen's problem. I think somehow this is probably the cleanest result that we have.
00:39:42.436 - 00:40:20.024, Speaker A: But as a byproduct of that smooth analysis was that we could find this deterministic algorithm for smale seventeen's problem. And what did we do so, well, what do we choose as a start system? We just choose this system. This is the system here, where the roots are these tuples of roots of unity. And it was also in the talk by Charles Wandler. It seems that people have used that a long time. We just use that. And the problem with this system is that the condition is quite big.
00:40:20.024 - 00:40:38.464, Speaker A: It's something like n to the d. D is the maximum degree. You see, if d is two, it's okay. But if the degree is big, this is not a nice start system. It has a large complexity. So that's okay. So what we did, okay, we said the following.
00:40:38.464 - 00:41:08.476, Speaker A: Let's assume the maximal degree is less than n. Then we run our adaptive linear homotopy. And if the d is greater than n, so what we do, we switch the world and we do symbolic methods, essentially. So use methods from computer algebra. So there was an analysis by Jim Reneker, which was. Well, he used, it was numerical, not just symbolic. You can just use that.
00:41:08.476 - 00:41:56.480, Speaker A: This takes roughly capital d to the n steps. And if you do the, you know, if you do your numerics, then you see that you get this bound here. So, okay, I can be more precise. For instance, if the maximum degree is less than or equal n to the one minus epsilon for some fixed epsilon, then this n to the d, the n to the d is the bad thing here, because that's the bound for the condition of the initial system. This n to the d is polynomial bounded in capital n. This is just some dealing with the numbers. And similarly, if the maximal degree is greater than n to the one plus epsilon, you can bound now not n to the d, but d to the n as a polynomial n.
00:41:56.480 - 00:42:27.900, Speaker A: So this doesn't have a deep meaning. It's just how it is the bad case, if capital d equals n, then we just get this somehow. This is not the right algorithm. You see, this homotopy is nice if the degrees are small, but if they are bigger, one should have something else. So it's not the final answer, but it works. One can prove the theorem. Okay, so how is my time? 15 minutes on the proof.
00:42:27.900 - 00:43:31.884, Speaker A: Okay, I still have decided what to explain. Okay, what is the proof? Okay, I want to show you. I want to show you actually how this works. But, okay, for, to keep it simple, let's assume we are in the average situation. So actually, I show you how one can prove the Bertrand pardo result. This one with the idea of Gaussians, there is a main auxiliary result, and it's the following. Okay, so we have some q in h sub t, and it has almost surely it will have bzu number many zeros for each zero zeta, we have this pair q zeta, and it has a condition.
00:43:31.884 - 00:44:10.716, Speaker A: Okay? And then I take, I have the vector of all these conditions, and I take the l two norm. So this is, I take, I have here the sum of the squares of these conditions, divide by the Bezou number, and take the square root. This I call the mean square condition number. Okay, the mean square condition number. The main auxiliary result is like the machine in all these sinks. In these proofs is the following result. So what do we have here? The square of the mean square condition number divided by the square norm.
00:44:10.716 - 00:44:56.974, Speaker A: This quantity we analyze, we do a smooth analysis of this quantity here. You see, we have a local expectation over q with two any q bar, sigma squared supremum over all q bar of norm one. We bound this by a constant n times n over sigma squared. Okay? So this is the, okay, I have only 50 minutes left. So I wanted to show you two things. I wanted to show you how to come up with the average analysis using this main auxiliary result. It was one thing and the other thing, I wanted to give you an idea how to prove this.
00:44:56.974 - 00:46:01.554, Speaker A: So probably I just show you the second thing and leave out the first. I cannot. Probably you are all tired like me, but let's make it short. I tell you some of the ideas. What is behind this? Okay? The disadvantage is that you won't see why it is so nice to use Gaussians. But, okay, now can we switch off this middle thing, not too right on the wall? So it's about the proof of this main auxiliary result. So let's recall the solution variety.
00:46:01.554 - 00:46:50.014, Speaker A: So it's this incidence variety consisting of these pairs. Q Zeta Q is in my space, h sub d. Okay? Now if. Right, okay, for the, for the purpose of the talk, actually doesn't matter at all. The solution variety, actually one should take here the sphere, but it doesn't really matter. So the idea is somehow to reduce to the case which is linear. So there is a special case.
00:46:50.014 - 00:47:27.420, Speaker A: There's a special case where all the degrees are linear. All these degrees are one. Okay, this is the case we want to solve a system of linear equations that we believe we all understand. So what is in this special situation? What do we have? An input consists of n linear forms. Okay. A linear form is a row of a matrix, okay? So I can encode it as an n by n plus one matrix. I know that you are all tired.
00:47:27.420 - 00:48:18.544, Speaker A: I can see that, okay, the calligraphic m is my abbreviation of the space of n times n plus one matrices. Now, the solution writing this special case consists of the pairs m zeta, okay? In calligraphic m times c, okay, times en such that m theta equals zero. Good. And now I look at the following map. As Frank noticed, these are vector bundles, okay? These are both vector bundles. And now I write a bundle map from v to w. Then I denote psi.
00:48:18.544 - 00:48:58.190, Speaker A: What do I do? I map q zeta to m zeta. You see, the base point is the same, so it's a map of vector bundles. Okay? And now let me define this m. Probably you have forgotten, but this is this quantity that arises when I define the condition number. The important thing is this. I take q, okay, think of this. Q.
00:48:58.190 - 00:49:56.270, Speaker A: What is q? Is q one up to qn? And I can think of it as a polynomial map from cn plus one to cn. Okay? So I can take the derivative, I can take the derivative of this thing at zeta. And this is a matrix in the space calligraphic m. And then I decorate it, I have to post multiply it with this diagonal matrix, which is not so important for us at the moment. For the proof, very important. But for the global rough understanding, not so important. Right? And then by definition, by definition, I know this condition of the pair q zeta divided by the norm of q is exactly the operator norm of this pseudo inverse of m.
00:49:56.270 - 00:51:05.704, Speaker A: So that was my definition of the condition. Okay, if you remember in the slide, it was f instead of q, but it's the same. The condition of q zeta is the norm of q times this norm of this matrix. Okay, so now, okay, so what do we want to understand in order to prove this main technical result? So what do we have? We have this squared means this square of the mean square condition divided by the squared norm. And we need to bound the expectation of this over this non centered Gaussian. Now, maybe erase, erase this. I think.
00:51:05.704 - 00:51:43.330, Speaker A: I won't use it. Let me put the diagram here. So, if you recall, I defined a distribution on the solution variety. I said choose g uniform G Boscauchen, and then one of the roots uniformly at Raymond. Now replace this. Okay, I can do the same here if I choose q. Now, it's not a standard Gaussian.
00:51:43.330 - 00:52:10.824, Speaker A: It's just this non centered Gaussian. And after choosing it, I choose one of the zeros uniformly at random. I can do the same. And this way, I define some probability distribution on this solution variety. Okay, it can be quite complicated because it comes from something non centered. Okay, so then I have this bundle map. Psi goes from v to W.
00:52:10.824 - 00:53:14.294, Speaker A: Okay, now, this probability distribution defines a push forward distribution here. So I get here some probability distribution. Just push forward. Okay? Now the first observation is that this, this expectation I want to estimate is the same as the expectation of mu q zeta squared divided by norm of q squared with respect to distribution on v that I defined before. So this, because what is this? What is this mu two squared? It's just the average over all the, all the squares of the conditions. So if you think about it for a moment, if you are not too tired, then you realize it's exactly this. Okay, so I have this expectation on v now.
00:53:14.294 - 00:54:07.612, Speaker A: But you see, this is the square of the operating norm of the pseudo inverse. This only depends, okay, on the element here, okay, only on this guy. So this is the same, just right here, this expression. So what I have to analyze is the expectation of this squares operator norm with respect to the push forward distribution on w. I just write w, but I mean the push forward distribution. Okay, good. Right.
00:54:07.612 - 00:54:55.520, Speaker A: And so next, now the elements of w have two components, m and zeta. So I think it's important here to think probabilistically. After that, one has to write down all the formulas. But what I can do is I can, okay, I can choose, okay, right? If I have this map here, goes here. So this distribution on w defines here another distribution, okay, if I would start here with the Gaussian, the standard Gaussian, then I would get here the uniform distribution. But in this situation, it's something more complicated. It probably concentrates somewhere, some distribution.
00:54:55.520 - 00:56:02.864, Speaker A: Okay? So what I can say is just again, as a mind experiment, I choose zeta according to distribution, and then I choose m in the fiber of this map. So this map has a fiber w zeta. The fiber is a vector space consisting of all those matrices m such that m zeta equals zero. So what I want to do, I want to write this as an iterated expectation, expectation of expectation. So here the expectation is over zeta in pn, and then the expectation is over m in the fiber of this thing. Okay, there are some technicalities that one has to, okay, if you want to justify this, I'm sure you have seen this. Usually when you apply this, then you are in the situation of a product of two spaces.
00:56:02.864 - 00:56:42.526, Speaker A: And then if you want to justify this, theoretically, the use for beanie, the problem is here that you don't have a product, but it's got this bundle. So then in this. So there are some deformations taking place here that you can take into account by using the co area formula. So the co area formula is one of the main tools. This is like it's like the transformation formula of calculus, when you don't have isomorphisms but things that have kernels. It's also generalized for beni. So this you need, if you want to make this formal.
00:56:42.526 - 00:57:52.244, Speaker A: But the idea is just this, okay, and then, right, we have to, okay, then one has to prove some lemma, because what I want to do now, I want to bound this guy. So I fix the zeta and I want to bound this, okay? And the goal is I want to bound it. I want to bound this expectation by some constant. This constant does not depend on zeta times n over sigma squared. If I can do that, okay, then it, it doesn't matter that I don't understand too well, what kind is distribution of pn? I can just bound this by the same constant times n sigma squared. Okay, that's the idea. So in order to prove, I mean, this red thing here, if I want to prove that, I need to understand what is the distribution on this fiber, maybe let me write this, and then I stop.
00:57:52.244 - 00:58:53.768, Speaker A: Induced distribution on the fiber has the density. Okay, so I write it, and then I explain it as a density of this form. And this row here, this is a non centered Gaussian on w zeta. W zeta is a vector space. So this Gaussian is somehow induced from this q bar here. I started with a non centered Gaussian which has a center q bar, and this somehow defines this. But I don't have time to explain it.
00:58:53.768 - 00:59:16.350, Speaker A: But it's a Gaussian. It's a friendly distribution, okay, something we like. And then it turns out that it gets multiplied with the square squared determinant of this. Oh, I shouldn't have written m. Probably, I should write m. Sorry, I should write, mm. I should write it this way.
00:59:16.350 - 00:59:59.302, Speaker A: I think because it's not a square matrix, I should write it this way. And this comes from the, this co area formal, some normal jacobian thing. So, okay, so this is, one has to prove. And then once we are here, then if you look at this, then proving this inequality, which I have in this red circle, boils down to some smooth analysis of a matrix condition number, if you want. So we are in a situation where we just have matrices. So that's this, of course, one has to prove as well. This is the idea of proving this main tactical lemma on them over time.
00:59:59.302 - 01:00:01.254, Speaker A: And thank you for your attention.
