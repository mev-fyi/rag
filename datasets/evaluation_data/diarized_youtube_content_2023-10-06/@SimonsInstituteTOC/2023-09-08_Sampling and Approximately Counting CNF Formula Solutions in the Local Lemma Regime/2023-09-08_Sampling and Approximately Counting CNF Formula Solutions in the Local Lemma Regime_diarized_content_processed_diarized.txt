00:00:26.954 - 00:00:33.562, Speaker A: Okay, next we have counting CNF solutions in the local lemma regime.
00:00:33.698 - 00:00:48.682, Speaker B: Yeah. Okay. Yeah, thank you. Okay. Thank you. I'm weiming. Today I will talk about something about approximate counting or sampling thing of solutions.
00:00:48.682 - 00:01:17.104, Speaker B: So first I introduce myself. I get my PhD degree in engineering. Then I spent two years in Edinburgh. Now I am a postdoc in the Simons Institute. So after this program, I will move to the Zurich as a postdoc. Okay, so my research, interesting, is based on basically the sampling problem. So the input is a description of a high dimensional joint distribution and all we don't want to draw a random sample from the given distribution.
00:01:17.104 - 00:02:09.004, Speaker B: So basically I studied the global distribution defined by local interactions. What I mean is that suppose we have a set of variables. Each of them takes a value from a finite domain, and each variable have a function phi v, which basically the local preference from taking values. Then we add some local interactions, which is a hyper edge. In this graph, every local interaction is described by function fe, which just maps the local assignment to a weight. And for any, all the local constraints together defines a global distribution, which is called a Gibbs distribution, such that for any global assignment, the probability of a sigma in the Gibbs distribution is a product of all we functions. So this model is widely used in many research areas, including physics, tcs, and machine learning.
00:02:09.004 - 00:02:59.924, Speaker B: Another problem that is closely related to sampling is approximate counting. So given a Gibbs distribution, approximate counting problem asks the algorithm to estimate the normalizing factor of the Gibbs distribution, which is often called the partition function. In physics. The definition is basically we enumerate all possible configurations and sum all the way together. For many natural Gibbs distributions, sampling and approximate counting are equivalent to each other among some self reducible reductions. Okay, in this talk I just focus on one example that is a CNF formula. So given a Cn's formula, we assume that every clause contains exactly k boolean variables and the variable degrees at most d, which means one variable appears at most d different clauses.
00:02:59.924 - 00:03:44.294, Speaker B: A special case of th formula is a monotone thing f. In this case, every Boolean variable appears positively in every clause. As we can see, this monotone thing has a trivial solution such that we assign every variable the value true. Then this thing f is satisfied, and this monotone thing f is also equivalent to the hypergraph independent set. So the main question is that under which condition between the parameter d and k we can sample a th solution uniformly random, or approximately count the total number of solutions. We must assume some condition between d and k, because for general tHNF formula, if we don't have such a condition. Even the decision problem is np complete.
00:03:44.294 - 00:04:30.994, Speaker B: But for monotone CNF, although the decision problem is trivial. But if you want to sampling or counting, you can think we still need a condition between d and k. Okay, so to make this question reasonable, we first need to guarantee the solution exists. So here is a femur sufficient condition, that is the low vast local lemma condition, which says if k is roughly greater than log d, then the CNF solution must exist. In this case, the sampling problem is well defined and the femos most than TarDis algorithm can find a solution under the same condition efficiently. But for the sampling and approximate counting, the current best result for general single formula is k greater than five log d. For monotone, we can push the threshold from five to two.
00:04:30.994 - 00:05:15.086, Speaker B: And such threshold is tight because we can show that, because some work can show that if k is smaller than threshold, even for monotone thing f, sampling and approximate counting is intractable. So as we can see, the solution exists if k greater than log d and we can find a solution under the same condition. But the sampling and accounting problem requires k to be at least two log d. This gives us strong separation between finding a solution and sampling a solution. Okay, so I basically tell what is our technique. So there is a famous technique of a sampling, which is the Markov chain Monte Carlo. The most classical one is global dynamics.
00:05:15.086 - 00:05:37.096, Speaker B: So this algorithm starts by an arbitrary set solution. In every step we pick a variable uniformly random. The update is value conditioning on the current value of other variables. So in this case, suppose we pick x five. The x five can both take value true or false. So we update the x five to true or false uniformly random. Suppose next time we pick x two.
00:05:37.096 - 00:06:10.750, Speaker B: In this case, to satisfy the first clause, x two must take the value false. So in this case, we keep x two unchanged. As we can see abstractly, it's just a random walk over the solution space. Every time we change the value of one variable, we move from solution to another solution. And we hope that after simulating this microchip for polynomial number of steps, we can convert to the uniform distribution over all solutions. This actually captured by the connectivity of the solution space. If the space is well connected, we can do it.
00:06:10.750 - 00:07:07.226, Speaker B: Otherwise we need exponential time to convert it to the uniform distribution. But the worst case is that the solution space is disconnected, which means the microchain cannot convert to the uniform distribution. So here is our in our setting, if the monotone, if the signal for the monotone, this result shows that if k greater than two log d, this is a tight condition. Then we are in this good case the microchip mixing analog n step, but for general thing a formula in a local lemma regime, where in the worst case the solution space is disconnected. The natural question is how to sample from a a general thing of solution if the solution space is disconnected. So here is our result. In this result we show that if k is roughly greater than 20 log d, then we can do sampling and counting efficiently, and this work further improved analysis and pushes the threshold to 5.7.
00:07:07.226 - 00:07:42.678, Speaker B: But now the state of the art result is five log d, but they use a different sampling algorithm. So what is our technique? Our technique is a projection. So the original solution space is disconnected. We project it into a projected space and run microchip on this space. This picture illustrates some principle of our algorithm. So these three red objects are disconnected in a three dimensional world, but after the projection their shadows becomes connected. So what is our algorithm? Do we first construct a good subset of variable sets.
00:07:42.678 - 00:08:37.250, Speaker B: Then we use the microchain to sample from the marginal distribution of the subset, and after we get a configuration on this subset s, then we try to sample from the conditional distribution. So to make this framework actually gives us an efficient algorithm, we actually prove the following three things. The first is that there is an algorithm can efficiently construct such a good subset s. This step is actually inspired by previous work. Next, we need to show the Markov chain the rapid mixing on this marginal distribution, and we can show that if you find a proper subset, we can use a coupling argument to prove the Markov chain rapid mixing and finally the how to implement the whole algorithm. This includes in two steps how to simulate this microchain and how to sample from the conditional distribution. If we directly sample from mu, simulate the microchain trivial, but now we sample from the marginal distribution.
00:08:37.250 - 00:09:27.500, Speaker B: So if only condition the assignment of a partial set of variables, you want to compute the marginal distribution of another variable is a hard problem. But thanks to the local lemma condition, we can show that both steps can be implemented efficiently using a rejection sampling. Okay, by the standard reduction. If we have a sampling algorithm, then we can have a randomized algorithm to approximate count the total number of Singapore solutions. The next natural question is to ask can we do approximate counting deterministically? So here is a famous reduction by Jerome Bunyan, Valley Ranney. They say that if you want to solve approximate counting, you only need to approximate the marginal distribution of one vertex. So let's just consider monotone thingf for a simple example.
00:09:27.500 - 00:10:26.092, Speaker B: Okay, in this monotone thing f we don't have any connectivity issue. We can just simulate the most classical Markov chain and it mixed in analog and steps. And by slight modification we can turn this Markov chain into an algorithm that only use om login random bits to draw a uniform random solution of this monotone thing formula. Basically you can imagine that you input n log n independent random bits to this algorithm and the algorithm output n dimensional boolean random vector which is a uniform solution of this thing as formula. But now, okay, to do a randomized approximate counting we just need to simulate this microchip many times. Then take average we can estimate the marginal distribution, but this only gives us a randomized algorithm. How to do it deterministically imagine that we have such an algorithm that only use all login random bits.
00:10:26.092 - 00:10:51.678, Speaker B: Then we can sample from the marginal distribution on one vertex. Then we can do brute force randomization. This is because this always use log and random bits. We can enumerate all possible choice of such random bits and deterministically compute the output distribution of the algorithm. Then we can do deterministically approximate counting. But the non trivial step is this step. This basically asks us a direct sum question.
00:10:51.678 - 00:11:40.868, Speaker B: So we sample a joint distribution of with okay all with an output m random bits using n log n random output n dimensional random vector, use om log and random bits. The question is, if we just want a marginal distribution or one vertex, can we only use all log n random bits? This statement may not true in general, but it will show that for such a monotone thing, we have a technique to get this algorithm. So this give us a deterministic approximate counting algorithm. Finally, I talk about two open problems. The first open problem is for general thickness formula. Currently the hardest result is a two login, but the best algorithmic result is a five login. We don't know what happened in the middle, but for monotonous enough we have closed.
00:11:40.868 - 00:12:13.220, Speaker B: We have a tight condition. People in my community often believe that the algorithmic result is not tight. We hope to have a better algorithm to achieve these thresholds. Now for monotone thing as even if we understand the communication, sorry. Even if we understand the computational complexity, we can feel the after the fine grained algorithms or complexity for approximate counting. Let's just take a monotone thing as an example. Now suppose we want to approximately count the number of monotone thing of solutions.
00:12:13.220 - 00:13:10.062, Speaker B: Suppose both parameter kd and epsilon. Epsilon is error, bond are constant, and we assume we are in the good regime. In this good regime, then the current best randomized algorithm runs in time o two n squared and the current best deterministic algorithm runs into n two sum function depend on dnk, which means it only works for constant degree instance. And such a running time not only appears for the monotone CNF, but for many natural graphical models such as sampling uniform graph coloring or sample graph independent set. So the question is, okay, even if we are in the good regime, we can solve this problem. Can we get a better algorithm for approximate counting? Or someone can show if there is this barrier so we cannot break this n cubed, sorry, n squared threshold. That's all.
00:13:10.062 - 00:13:10.914, Speaker B: Thank you.
00:13:16.564 - 00:13:18.264, Speaker A: Thank you. Any questions?
00:13:25.444 - 00:13:45.064, Speaker B: The quick question is that, is there a non separation between deterministic algorithm and randomized algorithm in this area? Actually we don't know. But if you assume some, such as estimated the volume of convex birdie, you assume the algorithm can only do something, then there is a separation, but in general we don't know. Thank you.
00:13:47.484 - 00:13:57.984, Speaker C: I was wondering if you can do approximate counting if you can somehow get hold of a approximate CNF or approximate Gibbs distribution that is close to the original one, but somehow tractable or.
00:13:58.284 - 00:14:00.020, Speaker B: Sorry, can I repeat your question?
00:14:00.092 - 00:14:12.064, Speaker C: So if you can do, in the regime where you can do approximate counting, can you also somehow approximate the CNF or the distribution as a whole? Get, get some object that is close?
00:14:12.444 - 00:14:17.980, Speaker B: Yeah, if I can. Sorry, you mean if you mean if I can do approximate counting, can I do approximately, can I do sampling or.
00:14:18.012 - 00:14:31.492, Speaker C: Can I do not sampling, but maybe get another form of the distribution where it is almost everywhere the same, or it's only a few different models or. Yeah, can I approximate the original object?
00:14:31.628 - 00:14:42.604, Speaker B: Yeah, yeah, I understand your question. So your question is if I can do approximate counting, can I transfer this distribution to another one which may have some good properties? I don't know. Okay. Yeah.
00:14:45.024 - 00:14:51.368, Speaker A: So I had a question. How much of this is tied to CNF? Can you get similar results for say, two coloring hypergraphs?
00:14:51.536 - 00:14:59.408, Speaker B: Yes, we can extend it to the general instance that satisfy a stronger lowest local lemma condition. I see, such as a hypergraph curve.
00:14:59.456 - 00:15:05.408, Speaker A: Okay, any other questions? Thank you very much.
00:15:05.496 - 00:15:05.784, Speaker B: Thank you.
