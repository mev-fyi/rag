00:00:00.160 - 00:00:28.142, Speaker A: Positive semi definiteness and eigenvalue approximation. This joint work with Diana Nidale and my postdoc William Swartworth. So, let me start by defining the problem that we're going to study. So we're given a symmetric real valued matrix a. Let's say it's d by d. So it has d eigenvalues. And the question we're going to ask is how many linear measurements? I'll say what those could mean in a moment of a.
00:00:28.142 - 00:01:09.494, Speaker A: Do we need to distinguish between the following two cases? One is a is positive semidefinite. Okay, so all the non, all the eigenvalues are non negative. And two, a has a sufficiently large negative eigenvalue. Those are the two cases you want to distinguish. Yeah. So PSD here is symmetric and all non negative eigenvalues. Um, what are some example situations where this problem might arise? So a could be the hessian of a loss function f of a neural network, uh, evaluated at some, you know, with some current assignment of weights, let's say x zero.
00:01:09.494 - 00:01:57.336, Speaker A: And in this case, this, uh, matrix a is not given explicitly. It's impractical or infeasible to write down the hessian of a large neural network. Um, and this is going to motivate us looking at this matrix a via these linear measurements. Um, and we maybe want to distinguish between the cases that we're minimizing a loss function f. Uh, so is it minimized at the current set of weights? In that case, the, uh, the Hessian would be PsD or, uh, is, is it not, is there a steep descent direction at the current assignment of weights? So it has a large negative eigenvalue. Um, and uh, in this particular application, certain linear measurements are very natural. Um, so one kind of linear measurement is a matrix vector product.
00:01:57.336 - 00:02:47.880, Speaker A: Given a matrix a that we don't have explicitly, we can still query it through vectors v of our choice. So we get to see a times v. Um, and in this particular example, it is possible to get hessian vector products using automatic differentiation and other techniques. Another kind of linear measurement, uh, are, um, is a bilinear form query. I'll call the, call it a vector matrix vector query where you choose vectors u and v on the left and the right and you observe u transpose av. This is also well motivated for this particular neural network application where this corresponds to uh, a single second derivative, uh, directional derivative calculation, and it can be approximated with a small number of function queries. So this is the setting.
00:02:47.880 - 00:03:36.194, Speaker A: These are examples that we're looking at. Also, linear measurements come up all the time in streaming and communication settings. As you know, they're useful because linear sketches are easy to update and they're mergeable and so on. Okay, so let me formally define the epsilon LP tester problem that we'll look at. So given this matrix a, implicitly, if a is PSD, we'd like to declare that it's PSD with some constant probability. And if a is epsilon, far from PSD, what that's going to mean that its minimum eigenvalue is less than minus epsilon times some matrix storm of a. We have this gap depending on the norm, so that it's scale invariant.
00:03:36.194 - 00:04:21.774, Speaker A: Okay? And this matrix norm, I've written a sub p. This is the shaddon p norm. It's just the p norm of the eigenvalues of your matrix. Okay? So are you PSD or are you sufficiently far from it in shaddon p norm? And when p is two, this is the Frobenius norm. So we ask ourselves, how many queries does a tester need in these various models? Matrix vector queries, where we just query with adaptively or non adaptively with vectors v vector, matrix vector queries. I said we're going to focus on these two models, though. You could also look at general linear measurements where you just take the dot product of your matrix entry wise with some other matrix m.
00:04:21.774 - 00:04:58.184, Speaker A: As I said, we're going to consider both adaptive and non adaptive testers. Um, adaptive testers are motivated from, uh, you know, various, uh, optimization procedures that are often performing things like a power method or Krylov subspace iterations. Nonadaptive testers are motivated from streaming. Um, and we aim to give tight query complexity bounce. Okay. Um, so let's first look at matrix vector queries and let's look at p equals one. Okay.
00:04:58.184 - 00:05:40.138, Speaker A: Um, and without loss of generality, we can assume that the one norm, it's just the sum of the absolute values of eigenvalues of your matrix is one. Okay? The whole problem is scale and vary. Okay? So how can we tell if a is PSD or it's, it's, it has an eigenvalue that's less than minus epsilon. And here's what we're going to do. We're going to use what's called the Krylov iteration to approximate the top eigenvalue not of the original matrix a, but of an appropriately scaled and shifted matrix. Okay? So let me say what happens here. Here's your matrix a in pictures.
00:05:40.138 - 00:06:15.910, Speaker A: Okay, so this is the uh, the spectrum, um, here's zero. And a might have a lot of positive eigenvalues. In fact, it might be PSD or there might be this red negative eigenvalue. Okay? And we want to know if this exists or not. So how can we figure out if it exists? So we can look at minus a. That'll flip the spectrum around zero. So now it looks like this, we can then add a multiple of the identity.
00:06:15.910 - 00:06:56.294, Speaker A: So I'm going to add lambda times the identity. That'll shift all the eigenvalues. And so now this new matrix has all positive eigenvalues. If I choose lambda appropriately, I'm going to choose lambda to be larger than the largest eigenvalue of a. The largest eigenvalue of a is is at most one, because the, the norm of a is is one. Now, I have this new matrix, and my goal is to estimate the top eigenvalue of this matrix. So I want to estimate this top eigenvalue up to additive epsilon error here.
00:06:56.294 - 00:07:43.484, Speaker A: That will tell me if this red dot existed or not. Now, as we said, lambda could be as large as one. So, um, what that means is that we actually have to estimate the top eigenvalue up to a multiplicative one plus or minus epsilon factor. Okay? In, in, you know, in the worst case, and a good way to do this is with the Chebyshev trick. Okay? So you can estimate the top eigenvalue of a matrix with one over square root epsilon matrix vector queries. Um, so let me just review that. So assume that you have a pst matrix m.
00:07:43.484 - 00:08:31.654, Speaker A: Then the idea is to choose a random vector v, maybe a gaussian vector. Then just iteratively compute v m times v, m squared times v, and so on m to the k plus one times v. This is an adaptive algorithm. And once you have these matrix vector products, now you know how this, you can combine these matrix vector products to learn the any polynomial of the eigenvalues. So I take any polynomial of this matrix m. That just means I apply the polynomial on the eigenvalues as long as the polynomial has degree at most k. Okay? So I learn p, sub k of m times v.
00:08:31.654 - 00:09:07.342, Speaker A: It's quite an appropriate combination of these. And the nice fact here is that there exists a polynomial p of degree k where k is square root lambda one times one over root epsilon times log one over delta, such at this polynomial. Well, it's going to evaluate to one on the top eigenvalue. Um, and it's going to evaluate to something really small in absolute value. So evaluate to a mouse delta. If I move a little bit away from the top eigenvalue. Okay.
00:09:07.342 - 00:09:52.254, Speaker A: If I move epsilon away. Um, and so this polynomial, uh, is great. You can think of it as just maintaining, retaining the top eigenvalue and zeroing out all the other eigenvalues as a logarithmic dependence on this delta. Um, and, uh, so if I look at this polynomial, then, uh, applying that to the matrix m and multiplying by v, this is going to align very well with the top eigenvalue of m. Okay. Because this p of m, as we just said, is kind of like just the outer product of the top eigenvect, eigenvector with itself. Um, and I take the dot product with, with v, and I get a vector which really looks like the top eigenvector.
00:09:52.254 - 00:10:40.428, Speaker A: Um, and so this Krylov space witnesses the largest eigenvalue to, um, to, to m, to within epsilon additive error, which is what we wanted. And so we can just compute these products and then maximize the quadratic form to compute the top eigenvector in the Krylov space, which we know. So we don't need to know this polynomial to figure out a good bound on the top eigenvalue. Okay. All we need to know is that this polynomial exists because we can just optimize, choose the best vector in the Krylov space, the one that maximizes the dot product with M. And we already have all the Krylov. We have the Krylov space.
00:10:40.428 - 00:11:04.024, Speaker A: We have all the matrix vector products we need. Okay. So this is very simple algorithms, and it gives one over root epsilon matrix vector products. Are there any questions about this? Okay. Do you think this is best possible? No. No. Okay.
00:11:04.024 - 00:11:29.164, Speaker A: I'm hiding log factors. Yeah. Yeah. Okay. Why do you say no? Okay. Okay. What do you think we might be doing suboptimally? What was one of sort of the bottlenecks in this algorithm? Yeah.
00:11:29.164 - 00:11:34.204, Speaker A: Eigenvalue close to one is where you have this worst case guarantee.
00:11:34.244 - 00:11:35.588, Speaker B: But that feels like a trade off.
00:11:35.596 - 00:12:43.834, Speaker A: The instructor, some of your eigenvalues has to be one. Yeah, that's exactly right. So, you know, we had a shift a lot, potentially a shift by, you know, one times the identity. So is there some way we could avoid shifting so much? And so the idea is, well, what if we could somehow first deflate that is, zero out the largest few eigenvalues of this matrix, and then perform this process? So then we would have to shift less, because, you know, if these, uh, eigenvalues go away repeating the same procedure where we negate and then shift, we see that we shift by less here. Okay, so maybe we can try to balance the number of matrix vector products we need to deflate together with this, uh, you know, this algorithm that we just presented. So the idea is let's deflate the top t eigenvalues where t is going to be chosen to be something like one over epsilon to the one third. Um, and I'll say how to do this in a moment.
00:12:43.834 - 00:13:42.394, Speaker A: Now, we know that the sum of absolute values of eigenvalues is one. And so the new maximum eigenvalue, after zeroing out the top t has to be at most one over t, which for the setting is epsilon to the one third. And now if we go through the same algorithm we just did on this deflated matrix a with the top t eigenvalues removed, then we don't have to shift by as much. We only have to shift, shift by epsilon to the one third instead of potentially one. And so now if you look at it from a relative error standpoint, we only need to estimate the largest eigenvalue up to one plus or minus epsilon to the two thirds. Multiplicative error. And now, using the Chebyshev trick on this matrix, we only need the square root of this, which is one over epsilon to the one third queries.
00:13:42.394 - 00:14:29.168, Speaker A: Okay. Um, now, I didn't say how to do the deflation yet. Uh, deflation can be carried out explicitly, uh, via Cameron and Chris's, uh, algorithm analysis of, um, subspace iteration. Um, but in fact, Krylov just does this directly for, for us. We don't need to do this explicitly. Um, so we can consider a polynomial, which instead of being degree one over epsilon to the one third that was this q of x. In the earlier algorithm description, there's this polynomial that, um, you know, if you unshift and go back to the original matrix, it's very large value at the negative eigenvalue and roughly zero at all the positive eigenvalues.
00:14:29.168 - 00:15:03.956, Speaker A: So that's that polynomial q. And what we can do is we can force it to be zero at the top t eigenvalues. So that's how we do the deflation. We just multiply by x minus lambda one, x minus lambda two, and so on. And so we choose the number of values that we set to zero to be the same as this original degree. So the overall degree just changes by a constant factor. We also normalize by a scalar here so that p of the negative eigenvalues still remains one.
00:15:03.956 - 00:15:42.676, Speaker A: And you can show that p evaluated at the smaller eigenvalues, at the positive eigenvalues is still very small. Okay, so you have a low degree polynomial in the Krylov space without having to explicitly do the deflation. Yeah. You need estimates of lambda one and lambda t to do this, right? No. So you don't actually need to know this polynomial. You just need to know that it exists, because in the end, what you're doing is you're going to build up this crylove space by doing these adaptive matrix vector products. You have this parallel space, and now, you know, there's some combination of the matrix vector products that you have that is, uh, you know, that realizes the polynomial applied to the matrix.
00:15:42.676 - 00:16:27.294, Speaker A: And if you were able to choose that, that'd be great. It would give you, you know, a vector that's very aligned with the negative eigenvector. Um, you don't know what that polynomial is, but you have all these matrix vector products, so you can just choose the best one in the space that you have. So it's a nice, in the sense that you just need to know that this exists. Any other questions? Yeah, I guess. Can you also, alternately. Oh, can you also alternatively think of like, a polynomial that kind of zeros out the interval between one and like something close to epsilon in the real line? So what we, I mean, you wouldn't be able to like, zero out the interval, right.
00:16:27.294 - 00:16:53.242, Speaker A: But you could make it very small. I mean, we don't really, we don't need to do anything beyond these topics. Yeah, yeah. Are there extreme results known about, like, the optimality, the degree of the polynomials? I don't know. In terms of like, constant factors and logs, I don't know exactly what the lower bounds are. Maybe someone else knows. Anyone.
00:16:53.242 - 00:17:18.654, Speaker A: Oh, yeah, I think it's optimal up to high order terms. Yeah. Any other questions? Okay. Um, yeah, so this is what the polynomial looks like, right. It zeros these out, uh, and it's, you know, very small, uh, on the positive eigenvalues, and it's very large at this negative eigenvalue. It exists. That's all we need to know.
00:17:18.654 - 00:17:48.804, Speaker A: Um, and now we can just minimize the quadratic form of a over the Krylov space, and we'll know that we'll get a negative value, and that's all we need as a proof that this matrix is not PSD. Okay. Yeah. So that's it. It gives one of reps on to the one third matrix vector products. It's kind of a weird bound. You can generalize it to arbitrary, uh, p norms.
00:17:48.804 - 00:18:17.848, Speaker A: You get something uh, like epsilon to the minus p over two, p plus one. This, this, uh, bound goes to, it gets worse as p increases. It goes to, uh, one over root epsilon for large p. And essentially what that's saying is the deflation is not as effective. Right. If p is infinity, then, you know, deflating the top few eigenvalues, well, you could still have another one of the same value. Um, so this bound, it doesn't look very nice, but in fact it's optimal.
00:18:17.848 - 00:18:45.434, Speaker A: So using a lower bound technique of bravery et al. We can show that these bounds are tight in this model. I'm not going to get too much into it, but they have a nice result that says if you have a Wishart matrix. So Wishart has the form gg transpose, where g is gaussian. It's a d by d matrix. And you want to estimate its smallest eigenvalue up to a constant factor. Then you need d, uh, matrix, vector queries.
00:18:45.434 - 00:19:20.520, Speaker A: Okay. So the smallest eigenvalue is not very concentrated, unlike the top eigenvalue. And if you want to estimate it, well, then you kind of need to read the whole matrix and you can embed this problem into our problem by appropriate setting of parameters. Yeah, infinity for p equals infinity. Um, arbitrary large p. Yeah. So for non constant p, I don't know what the dependence, if there's a dependence here on p or not, I'd have to check.
00:19:20.520 - 00:19:28.808, Speaker A: Yeah, but I mean, it gets arbitrarily close for constant p. Yeah, thanks. Yeah, just a small comment.
00:19:28.896 - 00:19:32.184, Speaker B: I think in the peak, in the peak was run setting, which is what you demonstrated.
00:19:32.224 - 00:19:32.804, Speaker A: Yes.
00:19:33.214 - 00:19:50.246, Speaker B: There's a very similar argument in the result of speedman move, which basically says in the graph setting, if you have such a sum on eigenvalues, conjugate gradient basically gets the one like at least theoretically gets a cube root kappa converges, which is the same proof.
00:19:50.430 - 00:19:53.886, Speaker A: Oh, okay. Does it generalize to, I mean, the.
00:19:53.910 - 00:19:55.414, Speaker B: Construction of the polynomial is the same.
00:19:55.454 - 00:19:57.326, Speaker A: But you have exactly zero at the.
00:19:57.350 - 00:20:02.362, Speaker B: Top roots and the lower one. And then, now if they're smaller by some non biased.
00:20:02.518 - 00:20:20.962, Speaker A: Okay, yeah, we should talk more about that afterwards. Yeah. Okay. So that's one example of techniques here. Let me look now at bilinear sketches. So, vector, matrix, vector queries. Let's again look at p equals one.
00:20:20.962 - 00:21:19.784, Speaker A: We want to test if we're PSD, we're far from this, a natural way of sketching is to compute ga G transpose, where g is a gaussian, uh, matrix, um, with hopefully a small number of rows. And, uh, so the number of vector, matrix, vector products here is, uh, one over epsilon squared. If I have one over epsilon, small dimension for g. And, uh, if, if a is PSD, then gad transpose is also PSD. And if a is far from PSD, then actually using analysis of Andoni and hui, if it's epsilon far, you can show that it won't be PSD. So you can just check if Gad transpose is PSD or not. This has a nice property of being a one sided tester, meaning that when the input is PSD, it always returns PSD.
00:21:19.784 - 00:22:27.872, Speaker A: And this is also optimal. I won't say why. Um, okay, so that's for p equals one, but what about p equals two? And interestingly, there's a, there's a very different behavior here. So there's a strong lower bound here that no one sided polynomial and one over epsilon l, two tester exist for vector matrix vector, uh, queries. Um, and, uh, in fact, you can show that you need, uh, epsilon to the minus two times d vector matrix vector queries. Okay, so it suddenly becomes polynomial in d once you look at norms p larger than one. Um, and, uh, I'm not going to say what the lower bound is, but it's, uh, since it's a streaming crowd here, I want to say that it's related to frequency estimation arguments in data streams, which say that if you, it's well studied, that you have a vector and you want to produce a sketch of it to approximate frequencies values in that vector.
00:22:27.872 - 00:22:57.914, Speaker A: And if you're required to always produce overestimates of these frequencies, then the problem actually becomes much harder. So if you're familiar with things like count sketch, these give you two sided estimates. Sometimes they're less than the actual value, sometimes they're larger. And that's actually essential. If you require one sided, then the space complexity grows a lot. And actually, that phenomenon is related to what's happening here. Okay, but that's just for one sided testers.
00:22:57.914 - 00:23:48.614, Speaker A: Maybe we can allow error, two sided error. So let's try to diagnose the problem. What's happening here? Well, what is the natural algorithm? We can take the algorithm of Andoni and Hui and, you know, it's computing gag transpose and look at its eigenvalues. It seems like a natural thing to do. The problem is that the eigenvalues of GAD transpose are going to concentrate around the trace of the matrix a, which can be very large. It can be large as, as large as square root d times the frabinius norm. So, you know, unlike the simple tester for p equals one, where you just check is there a negative eigenvalue in gag transpose, you're unlikely to observe any negative eigenvalues.
00:23:48.614 - 00:24:22.464, Speaker A: In the case where you're epsilon far in the two norm. What's going to happen is you're going to have a bunch of eigenvalues that concentrate around the trace of a. And then maybe if you had a negative eigenvalue in the original matrix, well, maybe it'll be a little bit less than the trace of a, but it's, you know, it's still going to be very positive. Okay, so the key idea that we use is, well, maybe we can look at the positions of the eigenvalues relative to the trace of a.
00:24:26.364 - 00:24:27.224, Speaker B: Okay?
00:24:28.164 - 00:25:31.668, Speaker A: And we're going to reject, if, you know, we're going to say gag, we're going to say a is not PSD if gag transpose has a suspiciously small looking eigenvalue. Um, okay, so formally, what does this mean? Uh, let me just say roughly how the analysis goes. So what we can show is that if the original matrix a has a negative eigenvalue smaller than minus epsilon, then with good probability, the minimum eigenvalue of gag transpose is a little bit away from the trace of a. Okay, so it's the trace of a minus at least epsilon times k, where k is the number of rows in your gaussian matrix. So it's a little bit negative, it's a little bit far away. Let me not say much about the proof, but the idea is just to choose the vector closest in the image of giving to the negative eigenvector. And you can show you gain this little bit of amount.
00:25:31.668 - 00:26:20.214, Speaker A: Okay, I'm not going to say more. On the other hand, what you can show is that if a is PSD, then actually all the eigenvalues of gag transpose are within additive root k from the trace. Okay? And this uses a net argument. So we, we, um, in fact, we only need the lower bound here because we're going to try to distinguish, we're going to look at the minimum eigenvalue of the sketch as our distinguisher. We only need the lower bound. And so the idea is to look@a.net of all vectors v in this k dimensional space and use a Hanson Wright inequality to it's concentrated and, you know, with the appropriate inequality, you can lower bound how much you deviate from the expectation.
00:26:20.214 - 00:27:01.554, Speaker A: Um, and, and that's it. So the idea is, if you're epsilon far from PSD, you can show the minimum eigenvalue is a little bit far away from the trace of a. On the other hand, if your PSD, the minimum eigenvalue is somewhat close to the trace of a, you then choose your number k of measurements to separate these two cases. And if you just solve, you see that you need your gaussian matrix to have one over epsilon squared, a small dimension. Um, and, uh, yeah, so the idea is just compute the minimum eigenvalue of the sketch in parallel. You need to estimate the trace of a. That can be done easily with known algorithms.
00:27:01.554 - 00:27:48.974, Speaker A: And then you just check at this minimum eigenvalue how close it is to the trace, to your estimate of the trace. Okay. Um, and that's one over epsilon to the fourth vector matrix vector queries, um, because it's ga g transpose, and that actually turns out to be optimal as well. Any questions about this, this algorithm? Yeah, yeah. So when you talk about concentration around the trace, is, is there assumption here that dimensionality is just much larger than k? And that's why this is the case, because if they're proportional, then it wouldn't concentrate. Right? Yeah, I'm thinking of k as, you know, being something that's, you know, much smaller than the matrix dimension you're trying to sketch and reduce it. Yeah.
00:27:48.974 - 00:28:30.664, Speaker A: Other questions. Okay, don't have much time left, so let me just, you know, just give you a flavor of other techniques in this area. Very quickly. We saw that for l one testing, there's this bilinear sketch, gad transpose, that has one over epsilon squared vector matrix vector measurements. That was Andoni Hui's algorithm. You could ask yourself, can you improve this with adaptivity? I said it was tight if it's non adapted. And in fact, you can get one over epsilon measurements with adaptivity.
00:28:30.664 - 00:29:17.994, Speaker A: And to me, it's kind of surprising because I haven't seen algorithms of the form of vector matrix, vector queries that exploit, uh, adaptivity. Usually it's like, you know, these Krylov methods, you know, matrix times vector matrix times vector and so on. But, um, so how, how do we do this? What's the rough idea? The idea is, well, we're trying to minimize this quadratic form, x transpose ax, right? We're trying to see if this value ever gets negative. That means it's not PSD. And we're going to run a kind of cheap, uh, gradient descent here which, which uses very few vector matrix, vector queries. And here's the idea. We're going to initialize a vector to be gaussian and we're just going to do the following iteration.
00:29:17.994 - 00:30:08.690, Speaker A: The new iterate at time k plus one is the old iterate minus an unbiased estimate of the gradient. A nice thing about this unbiased estimate is there's only one vector matrix vector query here. So you choose a Gaussian at the case step, and you look at gaussian transpose times matrix times your previous iterate. That's your single vector matrix vector query, and you multiply that by the Gaussian you chose. Okay, so there's only one query to a here. It's an unbiased estimate of the gradient, because if you look at GG transpose, the expectation is the identity. And, uh, what you can show is this algorithm, if you run it for one over epsilon iterations with high probability, you will actually choose an iterate.
00:30:08.690 - 00:31:04.614, Speaker A: Uh, find an iterate which witnesses, which shows the quadratic form is actually, uh, negative. Um, so, uh, that's one way of using adaptivity here. Um, any questions about this? Okay, and finally, let me just say one more thing, is that this study into testing of PST matrices led to new algorithms for spectrum estimation. So, approximating all the eigenvalues of a matrix. So here's the problem statement. Assume you have a symmetric d by d matrix a, not necessarily PSD, your goal is to construct a sketch of a to recover the eigenvalues. Um, and what we want to do is we want to estimate every original eigenvalue of a up to additive epsilon times the frabinius norm of a.
00:31:04.614 - 00:31:40.368, Speaker A: So this is going to be good for eigenvalues that are large. For very small eigenvalues, the estimates are not going to be interesting, will just be zero. But that is for good reason. Okay. You can't actually estimate very small eigenvalues well with small sketching dimension. So the question is how small can a sketch be? Previous work that I mentioned shows you can do this with one over epsilon to the 6th vector matrix vector queries. It's this ga g transpose sketch with one over epsilon cubed small dimension on both sides.
00:31:40.368 - 00:32:17.734, Speaker A: But it has some drawbacks in the sense that the sketch can't actually recover the signs of the eigenvalues. It loses that information inherently. Um, and, uh, so what we, we show is that by taking this approach, uh, for, for testing. But our analysis of gad transpose, well, what we can do is, um, subtract off the trace. That's the sort of, uh, you know, most concise description. Um, so we look at gad transpose the same sketch. Um, and uh, the expected value, uh, of its trace is the original trace.
00:32:17.734 - 00:32:48.464, Speaker A: But now it only has k dimensions. So the eigenvalues average at one over k times the trace of a, instead of one over d times the trace of a. And we correct for this by subtracting off one over k times the trace of a times the identity. So we have our sketch and we subtract off a multiple of the identity to try to correct for this. Um, and so this is the whole algorithm. And I'll just end with this. Uh, it's a very simple algorithm for estimating eigenvalues.
00:32:48.464 - 00:33:28.514, Speaker A: Um, choose a gaussian matrix g with roughly one over epsilon squared rows. Compute the sketch gag, transpose really small one over epsilon squared by one over epsilon squared, and just output the eigenvalues of this sketch minus, uh, this multiple of the identity trace of a over k. Compute those eigenvalues, you're only going to get k of them and you want to estimate the eigenvalues for the original matrix. For all the remaining eigenvalues, just estimate them with zero. This algorithm works. It's simple, it's easy to implement, and it gives optimal sketching dimension. And let me just conclude then.
00:33:28.514 - 00:34:03.962, Speaker A: So we gave type bounds for PSD testing and spectrum estimation under natural measurement models. There are lots of nice questions here. Our proof for spectrum estimation requires the sketching matrices to be gaussian. It uses gaussianity in several places, uses rotational invariance to imply independence of certain quantities. We don't know if you can use sparse sketching matrices. That would speed it up. We don't know if adaptivity helps for spectrum estimation, um, in terms of vector, matrix vector queries.
00:34:03.962 - 00:34:25.914, Speaker A: So we got a type one over epsilon to the fourth bound. Maybe you can do better with adaptivity. And there are also other models for PSD testing. Instead of just having one large negative eigenvalue, maybe you have a lot of negative eigenvalue mass, but no particularly large one. What can you do in that setting? So that's all. Thank you. Running a bit over time.
00:34:25.914 - 00:34:28.314, Speaker A: So maybe one question before our next speaker.
