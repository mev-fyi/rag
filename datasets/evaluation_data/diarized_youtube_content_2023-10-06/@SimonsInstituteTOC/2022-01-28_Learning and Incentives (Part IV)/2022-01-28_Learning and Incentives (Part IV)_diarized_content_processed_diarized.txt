00:00:00.240 - 00:00:09.830, Speaker A: So this is the last session of the day, and right after, we're going to also have the town hall right here. So stay after Nika's talk to also discuss about reading groups and other events.
00:00:09.862 - 00:00:11.886, Speaker B: I can go until 425?
00:00:11.950 - 00:00:12.310, Speaker A: Yes.
00:00:12.382 - 00:00:39.920, Speaker C: Okay. Hopefully we won't go over. Okay, so what we just discussed was that there's a type of an incompatibility between two different notions of regret, the external and the Stackelberg, because they're optimizing for a different thing. So I just want to sort of wrap up that by saying that what that's really telling you is that it's important you know which world you're in, because you actually cannot just commit to one of these algorithms.
00:00:40.112 - 00:00:40.600, Speaker B: Sorry.
00:00:40.672 - 00:01:35.384, Speaker C: If you are coming to one of these algorithms, you need to know the world you are implementing these algorithms. Is the world where it's strategically reacting to you, or is it a world that's not, because if you mismatch the world with an algorithm, you're going to end up in the wrong, wrong way. So somewhat there's no best of both worlds in the worst case. Okay. And another thing I wanted to say is that it does help, and I talked about this a couple of times, but I think it's nice to write it down, that I could write Stuckerberg regret as just external regret for this other function, this f of p of t. It's just that this f of p of t itself is not a nice function, but it's something I could. So I could go between these two notions by just changing the actual function.
00:01:35.384 - 00:01:48.344, Speaker C: Nevertheless, it's nice to think about this, because we can now talk about the challenges that we face, and in general, we need to face in terms of these functions, rather than thinking about games and strategies and all of that.
00:01:53.314 - 00:02:33.840, Speaker B: So stack over, regress with respect to the function class f, defined this way, is equivalent to external regress for u. Sorry. External regret, defined for ft, is equal to stack over defined for u. Okay. For the u box. Um, so there are two challenges. The first of them is that this st itself is, um, not, doesn't have structures that we typically identify as nice structures, like, it doesn't have complexity concavity necessarily, but it does happen to be structured nevertheless, like piece by structure.
00:02:33.840 - 00:03:09.796, Speaker B: So if you do forget about all of this new, and you want to think about f, nevertheless, you need to remember that you want to use structures that are non typical outside of game theory, perhaps. And the other is that the partial information we thought about talked about membership oracles or these best response oracles and those are type of partial information. What I mean is that if at one round I know the value of my FTPT, I don't necessarily know what that function ft is on everything else.
00:03:09.950 - 00:03:11.152, Speaker C: I cannot do that kind of a.
00:03:11.168 - 00:03:25.864, Speaker B: Computation which regrets that we were talking about yesterday, you could have done, because yesterday you would see the instance, and then you could have known how good any of your other predictors would have been. Right now, you cannot know how good.
00:03:25.984 - 00:03:28.084, Speaker C: Other options would have been.
00:03:28.784 - 00:04:15.424, Speaker B: So, this is typically referred to as bandit, except that here we have more than bandit information. It's not that we just observe what the value, the utility we observe, but that we see the action. And this best response is a little bit more information than bad. So what we need to do is to now think about, how do I design these exploration algorithms that use the best response and help with the structure of this optimization together? Okay, good. I thought for a second I was hallucinating.
00:04:19.684 - 00:04:22.864, Speaker A: So what do you get here beyond Bandit?
00:04:24.044 - 00:05:24.244, Speaker B: So, Bandit's assumption is that you only see the value of the thing you play. Now, this best response gives you a lot more information, and in particular, think about binary search a little bit. If it's half in your space, in some sense, it's actually telling you if this would have been higher or lower. So it does have a lot more power for the specific type of structure that we're actually working with. There are, if you're interested in this, I did not put this paper up, but it's a paper of mine in DC 2015 that actually talks about the exploration basis for these algorithms. And that's something that you might want to look at. It specifically talks about how to use best response to form these things that are called very centric bases, low variance sort of exploration basis.
00:05:24.244 - 00:05:35.054, Speaker B: Okay, so the last part of this section before we talk about collaboration is the question that many of you asked, which was.
00:05:36.794 - 00:06:53.174, Speaker C: You hinted at asking, which was, what is exactly happening in terms of commitment? Who is committing to what, especially I'm talking about in a repeated way, what does it even mean to add every time, commit to something new? And sort of what we were thinking about right then was that, well, the reason was, okay, to think about an immediate, instantaneous best response was that you were dealing with a myopic follower who is just trying to do the best possible. That's exactly why you would best respond. But now we're going to see whether or not this makes sense, first of all, and what happens if we are and learning situations where people actually do know that the learner is learning something from their observations. So those are non myopic examples. The ride sharing example is one. I know that immediately that our, at least I hope that by not participating or changing the supply and demand, I'm actually impacting the statistical aspect of the algorithm and getting a different type of solution, perhaps immediately after. So those are situations that we have a non myopic agent.
00:06:53.174 - 00:07:19.824, Speaker C: And when you think about that in some sense, I started today's talk by telling you how it's great to commit. And honestly, like, yeah, okay, I wasn't giving you a life lesson, but it is great to comment, you know, perhaps in life and now, and now.
00:07:22.604 - 00:07:22.940, Speaker B: But.
00:07:22.972 - 00:08:32.550, Speaker C: Then I gave you algorithms that specifically did not, I gave you algorithms that they explored. They tried a bunch of different things and they were exactly the opposite of commitment. So it's really no surprise that sort of commitment and online learning specifically, but learning in general, they go against each other. One of them is asking for stability while the other one is asking for exploration. And this is quite problematic because we started thinking about commitment because we thought it's doing better than not committing. And we want to be able to recover that, both in terms of making sure that our settings make sense, but also making sure that we are getting algorithms that we claim, like in terms of their utility, is great about actually meaningful in terms of Sackburg equilibrium. So non myopic agents, these are agents who are optimizing over both past and future, and sort of, you can think about them as participating in an infinitely repeated game.
00:08:32.550 - 00:09:18.994, Speaker C: And these are the ones who are problem for us, why they're a problem. They're problematic because of this cycle that goes between the leader and follower. Let's say that the leader is trying to actually learn, okay? So it's to learn, it's not committing. And now the follower, who knows that the learner is learning, can sort of be a little bit more patient or essentially send out some garbage signals in the hope that, you know what, I'm infinitely playing this game. It's okay. If I lose for the next ten rounds, from round ten to infinity, I'm going to gain. And if the algorithm, the leader knows that this is happening, well, the algorithm says, okay, I'll ignore the first eleven rounds and then I stop paying attention.
00:09:18.994 - 00:10:00.992, Speaker C: So this type of essentially the free channel for taking actions that have no visible impact on one's utility, long term utility, can allow for something that's called a cheap talk. This is like essentially two agents just talking to each other. Neither of them mean what they're saying. And neither of them are taking actions that actually change their long term utility. And this inhibits learning because it can just, it doesn't matter what the real life is until infinity. You can talk about an imaginary life. So that's the problem.
00:10:00.992 - 00:10:55.644, Speaker C: And to talk about this problem, I'm going to actually talk about a very recent and ongoing work of mine. I was trying not to include ongoing works because the preprint for this is not available. But it's coming out in like ten days, hopefully. So if you want the pre print, let me know. But otherwise I'm just going to give you a little bit about overview of it, because I think it's a really interesting line of work to be thinking about. Okay, so the typical thing when we talk about infinitely repeated games, part of it to get around this type of infinite cycle, it's pretty standard to talk about two settings. One is that one of these agents is getting discounted utility, so that they cannot like any time that they're sending some garbage signal, it's going to be loss of opportunity for them.
00:10:55.644 - 00:12:03.874, Speaker C: The other one is something called the large market assumption, which says that actually it's not that the same agent keeps reappearing, and I'm dealing with the same agent who is infinitely patient, but that I'm seeing new people. Most of the time it's just a person comes and sticks around for like ten days and then they move on. You can make that assumption for either of the players, but we are just going to make it for the follower. This is a very standard assumption, reinforcement learning, in many other settings as well. And it allows us to actually think about long term utility and now talk about optimization of that long term utility as something that's not necessarily infinite, but something that you could actually approximately optimize. Now with that we can think about followers that don't necessarily best respond instantaneously at least, but they have these long term strategies that account for both past and present. And they're choosing these strategies using policies in a way that they optimize a long term utility and expectation.
00:12:03.874 - 00:12:39.674, Speaker C: What that looks like is some discounting factor gamma to the table. Let's ignore that for now and the sum of their utilities. Of course, the expectation is calculated based on what they think the algorithm is doing and what they expect the algorithm will do in the future rounds, given what their own policy is. It accounts for the progression of the algorithm as a response to what they have done in the past and are going to do in the present, and are going to do in the future.
00:12:40.494 - 00:12:41.550, Speaker B: So this is what it means.
00:12:41.582 - 00:12:43.574, Speaker C: It's a little bit complicated, but luckily.
00:12:43.614 - 00:12:46.394, Speaker B: We never actually have to exactly compute this.
00:12:46.894 - 00:13:55.164, Speaker C: It just says that the agents are long term optimizers. And the point of having this discount factor is that if the discount is large enough, at some point you'd rather make instantaneously semi optimal actions rather than hoping until the infinity time to get some additional utility from having misled the learner. Okay, now what is commitment here? Commitment, there's two notional commitments. There's of course, the Stackelberg type. You are talking about that parrot. Your strategy is committed in a verifiable way, but actually the algorithm itself is now a medium of commitment in the sense that before this optimization starts, the algorithm has been committed to. And now what you want to talk about is that what are some principled approaches to designing this algorithm? So that even when the algorithm is responded to in such and such way, we are going to end up with a good Stackelberg regret does the other side.
00:13:55.164 - 00:14:08.484, Speaker C: Yeah, so that's exactly what it means, the conditioning on the algorithm. It means that they're responding to the algorithm, but it's a policy. Sorry.
00:14:11.944 - 00:14:30.496, Speaker B: Yes, it means that they're going, that's the algorithm that's implemented like they know. And these are usually long term. If somebody deploys an algorithm in real life, there's actually some. Okay, so as a principle, there are many ways to actually design such and such algorithms, but I want to give.
00:14:30.520 - 00:15:03.032, Speaker C: You some few principal ways of thinking about it. Okay, so I am going to use the fact that an agent is discounted by saying that even without doing anything additional, the cheap talk is not so cheap anymore. Because discounted utility means loss of opportunity. Because if you didn't best respond instantaneously, you're going to be losing some utility right now. And it's not clear if you're going to gain too much of that. There's a question.
00:15:03.168 - 00:15:05.124, Speaker A: Yeah, just about the commitment.
00:15:13.524 - 00:15:39.304, Speaker B: So your algorithm shouldn't be deterministic. Your algorithm should be deterministic assumptions that you have. And then this is contractor learning. For example, reserve prices in auction. So I'm just wondering, how does this thing relate to the kind of the work set?
00:15:39.684 - 00:15:41.092, Speaker C: I will mention them.
00:15:41.188 - 00:16:39.994, Speaker B: It's a difference. The way I think about this is that you're actually giving you, that's why I was saying principled approach. We're going to give you a reduction that works for reserve prices, it's going to work for Sackelberg, it's going to work for any principal agent scenario, and we're going to just say that if you are going to use this reduction, so it's kind of a general framework, what is it you want to reduce to? That's kind of the question we're going to ask. So what is rather than actually designing these algorithms, these algorithms have been designed, and I'll give you some citations, these algorithms have been designed in the past, but sort of, if I didn't want to keep designing them, I just wanted to sort of have a framework to reduce to them. What would that framework look like? Okay, so as I said, even the existence of discounted utility is not letting.
00:16:40.034 - 00:17:26.944, Speaker C: The making the follower be a little bit more cautious about when it's not best responding because it's losing utility in those rounds. We can actually control the rate of this. We can make this loss bigger or smaller, but control by controlling the flow of the information between the learner or the leader and the follower. So what I would think about is that algorithm that I was saying, if you want to design a framework, you can think about, you have an algorithm on this side and you're not going to change the algorithm much. What you're going to do is actually change the information set of the algorithm, or how that algorithm is going to be used. And this flow of information is something that you could turn up or turn down. And then you can talk about how you would do this.
00:17:26.944 - 00:18:27.712, Speaker C: The idea is that by putting this wall, you're encouraging approximate best responding, because you're making the cost of this lost opportunity high. But you also cannot make this too strong because you still need to make sure that there are algorithms on this side that can learn despite this wall that you've put up. So what kind of walls you're going to put up? A natural type of wall that you can think about, perhaps the most natural is the one that delays information. It's the one that works very nicely, as nicely as possible with these discounted utilities. And the idea is that for a large enough delay, the total expected gain in the future is going to be small. And that's why you're going to get something that looks like almost best response. Now, what does it mean? It doesn't mean that from the leader's perspective it looks like a best response.
00:18:27.712 - 00:19:28.904, Speaker C: What it means is that from the followers perspective, it looked like a best result. So the follower didn't lose that much by not having played the best response. Of course this is going to have a different type of impact on the leader's utility, but it usually comes with guarantee of some sort of closeness between, oh, I'm noticing a mistake between u one of ptqt and u one of pt br of pt. Sorry, I made it blue, which usually means leader in my slides, but I didn't change two to one. So those are leader utilities. And this would have been the utility if we really saw the best response. Okay, now in finite games, example I gave you before, this is going to look very structured.
00:19:28.904 - 00:20:04.980, Speaker C: In fact, in any finite game, I'm giving you an example of security here. What that says is that the utility of the leader is going to be accurate far from off the bound, far enough from the boundary, and near the boundary. There is no guarantee for it. It could be arbitrarily bad estimation of it. You can control how close by changing how good this wall is. That's one example. There are other examples where this closeness is a different notion of closeness.
00:20:04.980 - 00:20:05.624, Speaker C: Yes.
00:20:07.764 - 00:20:15.464, Speaker A: How the delay. But these steps helps us guarantee.
00:20:16.244 - 00:20:23.446, Speaker B: Okay, so let's look at this one, because this one is the very first one. So what the delay is saying.
00:20:23.590 - 00:20:46.734, Speaker C: So what is it? I want to see, I want to see the best response. This is the best thing that this minute the follower could have done if it's not doing that. This is a follower who is trying to gain its max utility over infinitely many times steps, because it's discounted utility that is a finite amount, and.
00:20:48.514 - 00:20:48.874, Speaker B: By.
00:20:48.914 - 00:21:39.794, Speaker C: Making it less and less likely that his immediate information is going to have any impact in the near future. Not far future, just near future. I'm saying that he's not going to gain anything right now until the next 15 time steps. If he's going to see any gains, it's going to be after 15 time steps. And the gain after 15 time steps is discounted so much that he's not going to actually see much gain. This is kind of like saying that in the ride sharing I want to be able to not waste my time more than half an hour sitting around because it's better for me to drive around right now rather than sitting around for half an hour. So if I delay the information that's coming from the riders and drivers, I don't update my model immediately, but I update them later.
00:21:39.794 - 00:21:48.194, Speaker C: I am discouraging them from trying to manipulate this so quickly. That's the high level idea.
00:21:59.774 - 00:22:04.634, Speaker B: So one way to think about is if it was completely discounted, it would have become a single step.
00:22:05.134 - 00:22:07.334, Speaker C: This wall is saying that I can.
00:22:07.454 - 00:22:28.330, Speaker B: Go between, between myopic to fully non myopic, essentially by changing how fast the information comes in. Because if I delayed infinitely non then I have barred my algorithm from seeing any information. But it's all very incentive compatible. There is no reason for that.
00:22:28.362 - 00:22:30.094, Speaker A: Do they know exactly to do this?
00:22:31.194 - 00:22:35.186, Speaker B: Can you make it a random thing? You can make it a random thing. It's not a problem.
00:22:35.330 - 00:22:37.186, Speaker C: They know the algorithm, and delay is.
00:22:37.210 - 00:22:42.414, Speaker B: A feature of this algorithm. So either they need to know it or they need to know what the distribution is coming from.
00:22:55.204 - 00:23:02.184, Speaker A: So they don't actually get the signal on the spot.
00:23:03.684 - 00:23:14.202, Speaker B: So it's slightly different. There are algorithms that asynchronous, there's this concept of delay. More generally, when you take an action, at some point in the future, you get an observation, but you don't know.
00:23:14.218 - 00:23:15.674, Speaker C: If that observation was for this time.
00:23:15.714 - 00:24:00.150, Speaker B: Step or next time step or whatever. This is easier than that, because my wall can tell you what time the observation coming. So it actually gives you more, much more information and freedom. And because of that, because you can design the wall, you can get better rates than those algorithms that have to work for adversarial delays. All right, so anyway, once you get this type of approximate best response, then that approximate best response is going to have some impact on you. And for this example, the approximate best response is only wrong, essentially far from the boundary, because if you're close to the boundary, those are the areas that the follower would have not cared to go which way to the other. But then you're far from it.
00:24:00.150 - 00:24:39.758, Speaker B: Now he doesn't care. So what do we want from the algorithms behind this wall now? Let's write a list of them. The first thing is that, well, I want to be able to actually, this algorithm should optimize from banded observations. So even if it was fully myopic, it should be able to optimize from these banded observations that we talked about in the previous section. But it also has to be robust to some misspecification of this function. In particular, the example I gave you was the one that said the dysfunction was off only in a very small, structured areas near the boundaries.
00:24:39.926 - 00:24:41.614, Speaker C: But there are also some other examples.
00:24:41.654 - 00:25:43.334, Speaker B: Like in strategic classification, it happens to be, point wise, close everywhere, but not exactly the same. There are different notions of being robust in an adversarial way, and they usually get put here. And the last thing I want is that the algorithm needs to be able to handle delays somehow. And the first two are actually things that we understand pretty well for myopic agents anyway, I have to be able to solve problem number one. So that's somewhat the bare minimum I needed to do, and I'm done number two is also something that is pretty well studied in the sense that the algorithm I gave you, for example, the Lisa Ford paper, comes already with these optimizations that are robust to membership query being sort of wrong near the boundaries and for point wise closeness. Again, this is something that either exists already or it's not very hard to design. Kind of like designing algorithms that are robust adversaries is a whole field that's pretty important and is studied a lot.
00:25:43.334 - 00:26:22.488, Speaker B: Now, how do you design algorithms to handle delays? That's part of the question. And in fact, I'm going to say that while I like to think about the wall as a delay, it's actually, it's easier to think about it in a completely different framework that's much more studied than this. And this is a framework of what's called batch bandits. So one way to think about batch bandits is that you have an algorithm that has to submit queries in batches. So even though I have these timelines, so that all of the queries in this line are submitted together, and I see the answer to all of them.
00:26:22.616 - 00:26:25.204, Speaker C: Only after my batch ends.
00:26:25.584 - 00:27:08.984, Speaker B: Same, I submit all the green queries and only see the answers after. So this is a type of batch bandit. I don't get any feedback in the middle of my batches. The reason I like to think about this is that this is more common in optimization, and it's a field that people have thought about. So there are algorithms, we understand their rates better now. Delays are actually equivalent to batches in the sense that you could think about. If you have an algorithm that could deal with batch sizes, you can actually go around and get one that can deal with delays and vice versa, with only a constant, actually a factor of two gaps between the requests they're going to achieve.
00:27:09.844 - 00:27:12.156, Speaker C: Now, this really just allows us, this.
00:27:12.180 - 00:27:37.598, Speaker B: Kind of characterization allows us to use both literature. There are algorithms that are much easier to handle and deliver than algorithms that have been studied in the batch version for a long time. But this is a nice way of thinking about, again, in a principle way, how do we go about designing these algorithms? So really, if I go back to this algorithm desert, what I'm saying is that I'm looking for robust batch banded algorithms.
00:27:37.686 - 00:27:39.190, Speaker C: If I have a robust batch banded.
00:27:39.222 - 00:28:12.486, Speaker B: Algorithm, everything I did would plug into them and has this kind of a reduction that makes any robust batch random algorithm affected in presence of non biotic agents. So that's kind of the high level ideas. I don't want to go into this a lot, but kind of in we've shown that like in three most common applications, this is very easy to put together because robust patch banded out either were known to exist or we can actually introduce them.
00:28:12.630 - 00:29:27.438, Speaker C: So for multi arm bandits, we introduce some nice ones specifically, and that works really nicely for any discretized set. So like for auctions, for example, for demand learning, and a bunch of other problems altogether fall in that really nicely and they get really nice sort of additive delay bounds, rather than getting multiple delay bounds. As you know, in terms of the LP, I already told you that the algorithm I gave you actually comes with some nice guarantees. A bunch of other algorithms can also be changed or already come with guarantees, like banded convex optimization comes with that kind of guarantee as well. So this is just saying that if you want to think about sort of the principles of how to use data and how not to use data, thinking about this flow of information and how you restrict it allows us to actually reduce the problem to an adversarial robustness type of question in a field that's been studied for quite some time. So that's an advantage of being able to really actually forget about these myopic, non myopicness and actually appeal to a different field to solve these problems. There's a lot more on this.
00:29:27.438 - 00:30:12.602, Speaker C: So some of, I put some of these here, especially for auctions. There are different ways to design the wall. Another, which has been definitely used for incentive compatibility, is privacy as a tool. And another paper that makes a really nice case about kind of thinking about algorithm as a medium of commitment is this one that kind of talks about reactiveness, like in some sense, it's also about how fast you update your algorithm, which you could have thought about it in the delay or batch version as well. So that's it for the non myopic part. If there are questions, we can take a minute to ask. Questions.
00:30:12.602 - 00:30:13.214, Speaker C: Yeah.
00:30:21.474 - 00:30:22.614, Speaker A: My alert.
00:30:28.594 - 00:31:28.264, Speaker B: That's the class equivalent. And by putting the wall, essentially we're saying that you're not relying on your algorithm to be careful and making sure that whatever your algorithm, I'm stopping you from being irresponsible. Other questions, can you comment a bit on the selection of cheap talks, like in game theory? Yeah. Okay, so, okay, so let me first, there's a lot to be said and there's a lot that we don't know. So first of all, the way that we're getting out of cheap talk is of course by making things possibly right, and that's a typical way to get out of cheap, doc. The other thing is that the way I talked about this coming to an algorithm is slightly different than the game theoretic notions of like subgame, like perfect equilibrium. In the sense that you're committing to an algorithm, it's not clear if you're not going to actually, you're going to actually take actions of a rational algorithm.
00:31:28.264 - 00:32:28.144, Speaker B: So there's definitely a difference between them. And the other difference, I think it's easier to tell you the differences, is that for several folk theorem style arguments, the amount of discounting needed is not the technicality. It's like it could be arbitrarily possible, but to actually get reverse bounds, you cannot allow this to be originally close to one. So there is a limit. I mean, we do understand lower bounds relatively well, and certain applicant options, we understand well, and options are very simple options, we understand them well. And that tells us kind of a rate in terms of how much cost you have to induce on people before you can get these long term, you're essentially, you have to go around the podium, otherwise everything would have been an equilibrium. So both the definition and everything is trying to get around that.
00:32:28.144 - 00:32:33.956, Speaker B: Okay, so in the next half an.
00:32:33.980 - 00:32:43.184, Speaker C: Hour, I'm going to talk probably faster and I'm going to talk about collaborative interactions.
00:32:43.724 - 00:32:44.504, Speaker B: Yes.
00:32:45.804 - 00:32:48.704, Speaker A: So how narrow are these results?
00:32:56.784 - 00:33:18.524, Speaker B: So these are definitely about two players. We haven't even thought about more than two, because we have to sort of. This is, as I said, it's like a stack over game on top of a stack of our game, and it's not exactly a sub game, perfect equilibrium. So I'm not even sure how I would define it for more than two. What was the second question?
00:33:19.074 - 00:33:20.174, Speaker A: Games with.
00:33:24.514 - 00:33:26.694, Speaker B: We're already dealing with bandits.
00:33:27.074 - 00:33:30.334, Speaker A: I meant when you don't know the status.
00:33:31.354 - 00:33:38.186, Speaker B: Yeah, so what I meant is that we are already doing with dealing with bandits and per time allowing this ft to be a different function.
00:33:38.330 - 00:33:40.058, Speaker C: So I imagine as long as we.
00:33:40.066 - 00:34:04.734, Speaker B: Can find a batch on the back, it's quite general in that sense, at least. Okay, so for collaboration, this is. I would argue that this is the newer part of not just this tutorial, but also the efforts of the community. So I'm going to try to get.
00:34:04.774 - 00:35:13.136, Speaker C: Across some very interesting questions, but also questions that aren't as well understood. Hoping that we can spend some of the semester actually talking about some of these questions, because I'd love to chat. So to do that, at first I'm going to actually talk about models for collaborative learning. What I mean, when I talk about collaboration and machine learning, then I'll talk a little bit more about aspects about how do we think about a single agent versus sort of social benefits, and then I'll talk about more about rationality and equilibrium. So to start taking a closer look at what collaboration is, collaboration and cooperation in computing in general is like, it's very general term and it's used in a lot of different domains. For example, a lot of times we think about collaboration as like a multi agent interaction, coordination of sort of how we take some decisions to act. But there is also this view that, um, that collaboration is not just about the decision that we're taking, but also about sort of, sorry, what's happening? Okay.
00:35:13.136 - 00:36:09.984, Speaker C: Also about the informations we are gathering. So it's not just about, hey, how do I like, split the utility of actions that we are taking, but actually how do we coordinate to increase our information set in ways that are, and then make decisions based on them? That's the view that I want us to have going into this talk. The reason for this is that there's more and more. Of course we are having more data, but also we are having more stakeholders in the sense that the data is spread across a lot of different sources. And these different sources are not necessarily data centers. They're actually people like agencies that hold the data. So they might have completely individualized and heterogeneous learning objectives, but they're also going to potentially have other objectives that it's not about learning, but more about the use of their data.
00:36:09.984 - 00:36:22.224, Speaker C: So this is why more and more data sharing is becoming important. There are whole fields in computer, in machine learning called like the fancier keyboard. Sorry, what's happening?
00:36:25.884 - 00:36:26.744, Speaker B: Okay.
00:36:28.724 - 00:37:15.128, Speaker C: It seems like my slideshow wants to just continue by itself. Give me 1 second. I think I have a had recorded this in the past. Okay. And data sharing, what is trying to do is it's trying to enable large number of learning agents to collaboratively accomplish their learning goals and from collectively fewer resources. And these resources are typically computation and data. And we're going to focus a lot more on data and it's starting to be used across networks of devices and hospitals.
00:37:15.128 - 00:38:22.654, Speaker C: And it's been, and also behind some major scientific breakthroughs, actually this ability, not to the learning, but the ability to share data in ways that benefits both the individuals and the society. So the whole idea is that if you're trying to create this large scale impact, we also need large scale participation. And large scale participation is going to mean that we are going to have to recruit agents and also retain them, which means that the incentives needs to be much more aligned for people to want to participate and participate fully in our protocols. So part of this is making sure that we are meeting individual learning guarantees. What does this mean? Well, let me give you an example from federated learning algorithms, because these are algorithms that were designed to work well, on average over data centers. That's usually the data sources were believed to be data centers. Of course, they've changed now a little bit, but still, the basic idea about them is that they're good for learning across these data centers.
00:38:22.654 - 00:38:59.304, Speaker C: And that means that it's actually pretty good. When these data sources are quite homogenous, it's the same data. It just was split sort of randomly across different sources. But when it comes to either human or organizational data, we don't have homogeneous data anymore. We have very heterogeneous data. And for heterogeneous data, the average becomes meaningless, because you might have average of 5% on average, but that just means that for 10% of people, you have an average of like 50%, you have an accuracy of 50%. So we don't want to have these, on average performance guarantees.
00:38:59.304 - 00:39:59.960, Speaker C: But we also know that on average, performance guarantees are coming with a lot less effort. So how can we sort of think about trying to get per agent guarantees, but still control the amount of efforts? So if you actually look at some standard runs of standard algorithms, what we see is that, let's say that this is 70%. This is when the average is 70%. Then to hit an average 70%, every agent has to use an amount of data set and amount of iterations that's relatively small. But if I want to continue the same algorithm without designing a new algorithm until each one of them needs a minimum accuracy of 70%, now every single one of them actually has to do more work. It's not that those other people stopped earlier, but that everybody actually has to be part of the system for much longer. This is not something that makes sense for a lot of agents.
00:39:59.960 - 00:40:26.862, Speaker C: I have to pay both for computation and data. So what we really want here is that can we ensure that every learning agent has high accuracy, but still make sure that it's from reasonably small amounts of data individually from them? So let's give a slightly more theoretically grounded model for this. This is from a paper we had in North's four years ago. Yeah.
00:40:26.958 - 00:40:30.274, Speaker A: Just to be sure. Mathematically, that means that we would like.
00:40:30.574 - 00:40:33.034, Speaker C: A mechanism that has better variance.
00:40:35.934 - 00:40:41.438, Speaker B: I want to tell you mathematically what it means. You could define it in terms of variance, but I'm actually going to talk about high probability.
00:40:41.566 - 00:40:42.994, Speaker A: Oh, yeah, yes.
00:40:45.054 - 00:41:19.050, Speaker B: Okay, so what is it that, mathematically, we want? We said that assume that there are k populations. These are, you know, distributions. And what you want is that you want to learn a function that's good. And goodness is defined by the worst error. So the error of the worst on the worst population or the worst person or the worst organization, and say that you want to have this to be more less than one. And then the question is how much data suffices for every learner to achieve this high accuracy through collaboration? It's not surprising that collaboration must need.
00:41:19.122 - 00:41:58.584, Speaker C: Interaction, because that's how we define collaboration. But it's good to think about what collaboration, what interaction means, because the other algorithms I gave you, they were also interacting. They were all sort of collectively trying to run some optimization on collective data. Okay, so the trouble with the earlier algorithms, the standard earlier algorithms, are lack of interactions. And really, the idea here is that the number of samples that a person were learner provides their learning rates, their update frequency. All of these are decided non interactively. They're decided at the beginning of time and never updated.
00:41:58.584 - 00:42:46.014, Speaker C: Never updated based on the performance of other agents, might be updated on their own personal performance, but not on a societal performance. And you can actually show that if you don't have these things being interactively updated, then you're going to be stuck with a type of sample complexity that is essentially as bad as if nobody collaborated at the first place to start it. What this is saying is that the sample complexity of collaboratively solving k problems is k times the sample complexity of solving one problem. So you could have just as well told them, each of you just go solve your problem. So this is without an interaction. It's saying that collaborative learning is almost as ineffective as not collaborating at all.
00:42:47.744 - 00:42:52.216, Speaker A: So this is from which objective function? The one on your previous slide?
00:42:52.320 - 00:42:54.368, Speaker B: Yes, the max error.
00:42:54.456 - 00:42:58.920, Speaker A: Why does it have a solution? Actually, that's. How do I know there is a function?
00:42:59.112 - 00:43:25.110, Speaker B: Good, because I'm making a simplifying assumption, which is not necessary, but it just changes how we are defining things. So I'm assuming that that's zero. I'm assuming that max I in k min H error of dih is zero.
00:43:25.302 - 00:43:27.078, Speaker C: Now, I could have defined this as.
00:43:27.126 - 00:43:48.042, Speaker B: Opt and asked for Opt plus Epsilon, but I'm just assuming that this is zero. And one place that this could have been zero is when you have a target classifier, h star, that every distribution is labeled according to the same classifier. What this means is that there is.
00:43:48.058 - 00:43:50.794, Speaker C: A ground truth, like there's the data.
00:43:50.834 - 00:44:03.036, Speaker B: Is not that noisy or something. There's a ground truth, but we care about different parts of this ground truth. That's an example. But almost everything I say you could do for the other one. Yeah.
00:44:03.130 - 00:44:11.644, Speaker A: A question maybe from your previous slides, from what's your definition of heterogeneity for the results you presented?
00:44:11.984 - 00:44:35.084, Speaker B: I mean that b one to decays are not the same, but with the exception of what I told classes as well, which was for this purpose, that there is an H star that would have labeled everything. So here, what I claim is that if you allow for interactivity, which really.
00:44:35.124 - 00:45:00.184, Speaker C: Just means that adjusting the amount of sample contribution and learning, then you can improve this k to log of k. Okay, now, why is this the case? I'm going to make a case for this without proving it, but by talking about min max theorem. But I see some puzzled faces already. Are there questions about this?
00:45:09.484 - 00:45:39.822, Speaker B: This is an offline learning. And one of the things I want you to take away from this is that online algorithms can be nice medium for collaboration, but the problem itself is defined in an alternative, and interaction can be enabled through all my algorithms. That's part of what I want you to pick. Okay, so why should you believe this? Well, you could really think about this.
00:45:39.838 - 00:46:17.134, Speaker C: As a min max optimization. In fact, what I wrote there is essentially saying that the min max value is zero, and asking you to sort of approximate. Give me a solution that's approximately that. So if I look at this, I can really think about this. Now, recall we talked about two players solving a min max game yesterday. I can think about having two players, player one and two. And I know that if one of them plays best response on the other plays in the record algorithm, on average, what's going to happen is that the average solution is going to be approximate min max equilibrium and approximate maximum equilibrium.
00:46:17.134 - 00:46:43.094, Speaker C: So that's exactly what I'm going to do. This is that I'm going to solve this with a nor regret algorithm. And one way to think about this is that the rules are a little bit different than yesterday. Whoever, the person who's playing no regret, and the person who's playing best response is swaps. But doesn't really matter here. Player one is best responding. What it means is that player one is the minimizer, and he's essentially running an erm.
00:46:43.094 - 00:47:31.774, Speaker C: This is just an algorithm that finds the best hypothesis on some sample. And what it's doing is that it's not taking the samples according to uniform distribution, or any one person's distribution. It's taking it according to some distribution that's defined by pL2. Now what is that distribution defined by pL2? That's the job of pL2. PL2 is a no regret learning agent who's maintaining a distribution over the key learning agents themselves. And you can think about these as like different weights, Alpha K at time t. And these alphas are supposed to be a proxy of how well or poorly I have done so far in terms of satisfying a good, like coming up with good classifiers for those agents so far.
00:47:31.774 - 00:48:38.274, Speaker C: Now what happens? Forget about this epsilon prime in the corner. But what I'm saying is that, well, because I have no regret algorithm from the perspective of the second player, sum of all these errors that I have accumulated over the distributions that pL2 defined is pretty close to the sum of the errors for the worst agent. That's what it means to be no regret with respect to agents. You're competing with the worst agents, in hindsight, in some sense. Now, if my, erm, algorithm at time pt takes enough samples so that this error is guaranteed to be small, then what I'm really saying is that, hey, actually I am competing with this best worst case solution. So this is not actually giving you an algorithm to actually get a good sample complexity that I told you that log of k blow up. You need to look at this a little bit more closely.
00:48:38.274 - 00:49:06.526, Speaker C: You need to actually design specific types of algorithms for them. But nevertheless, this shows you the main ideas. And now, where did the log of k come from? I have k agents, it's like I had k columns and I'm doing best response over k options. So we talked about little sum dimension. Little sum dimension of something with k is never larger than log of k. That tree, how to. The depth of the tree can never be larger than the number of things that are available under it.
00:49:06.526 - 00:49:50.394, Speaker C: So if I have k things, I cannot have more than log k. And that's why even simple online algorithms, it doesn't have to be the standard optimal algorithm. Even simpler algorithms can get you that log of k coming up. So that's the high level idea. And what I want to really remember from this is that I'm not saying that this is the right model or this is the right algorithm, but rather I think that we should think about online learning as a medium for actually designing protocols that are efficient. We don't have a ton of time. So let me talk a little bit more about another problem related, which is, I think, really interesting.
00:49:50.394 - 00:50:33.080, Speaker C: Going beyond accuracy guarantees, agents also have costs for collecting information. This could be the cost of curating the data set could be some other costs, like privacy costs. And we don't want to have these protocols that are asking for unreasonable amounts of data. And people have reasons not to want to give a lot of data. An example of this is that if you look at physical testing of, for example, water pipes for lead, this is an expensive process and it sort of ruins your backyard. You have to actually dig a hole. It's not something you want to do, but if you believe that whatever the result is, is going to be highly correlated with your neighbor, you really wish that they would take the sample.
00:50:33.080 - 00:51:06.274, Speaker C: So there's a little bit of, you know, these types of incentives of how you, how much you believe you have to actually participate versus how much you benefit from it. Those are questions that we need to understand. We need to achieve these desirable per agent type guarantees between accuracy and sample. And this tells us that we need to develop a theory for multi agent sample complexity. And I'm not going to tell you that we already have one. I'm just asking you to participate in creating it. But let me give you some definitions that I think are interesting.
00:51:06.274 - 00:52:23.304, Speaker C: So part of this is what is a reasonable share of data and things that are unreasonable is, for example, if I have k agents and I ask them, ask I to give me more data, that is necessary strictly for I by himself to learn, that's just completely unreasonable. They shouldn't have to pay more in collaboration than they would individually. Another thing is that if part of I's contribution in data was used exclusively to increase the accuracy for other agents, but it didn't actually have significant increase in their own accuracy of their own task. And we formalized this in a recent paper last year by two notions, individual rationality, which is actually how it's defined in economics in general. That's capturing the first type of reasonable or unreasonable contribution. Essentially, every agent has an accuracy constraint that has to be met. For example, if I think that each agent is providing MI samples, then I want that their accuracy constraint is met with this.
00:52:23.304 - 00:52:43.354, Speaker C: But if they were to provide any more like this should be less than what they would have needed to solve this problem by themselves, and meet their accuracy constraints by themselves. So if you want like 95% accuracy, how much data you would have needed by yourself, now I'm asking for less than that.
00:52:45.614 - 00:52:57.222, Speaker B: Why would I want to do something good for you? Who is I? So you're saying that maybe for you to get accuracy, what's my incentive to.
00:52:57.238 - 00:52:58.062, Speaker A: Give it to you?
00:52:58.198 - 00:53:28.596, Speaker B: Perhaps giving you data is going to make me less competitive. So actually. So if you're looking at this, there's no reason to have competition right now. There are versions of this competition, but at least the incentives are not misaligned. So you're assuming that their incentives are aligned? I'm assuming that they're not misaligned. It's slightly different in the sense that more data from other sources will help me strictly. More data from others will never hurt me.
00:53:28.596 - 00:53:29.904, Speaker B: Yes, but what about.
00:53:31.084 - 00:53:35.216, Speaker A: What about hurting if the person is giving it to data?
00:53:35.400 - 00:54:06.338, Speaker B: Exactly, so that's exactly what we're trying to capture. So what you are saying that I'm willing to give it to you in a minimal fashion, so whatever is going to make you accurate, but no more. But maybe don't even make you accurate? No, no, no. Whatever is making me accurate is that both of these are me, these are the same agents. I'm putting a strict upper bound per agent, and then I will give you something that's more constrained. But the strict upper bound says that.
00:54:06.466 - 00:54:08.506, Speaker C: I imagine a world where nobody was.
00:54:08.530 - 00:54:14.794, Speaker B: Helping me, how much data my task would have required. Now, in any world that I collaborate.
00:54:14.874 - 00:54:16.934, Speaker C: I should not give any more than that.
00:54:18.594 - 00:54:21.954, Speaker B: It's very minimal. Nothing fancy. Yes.
00:54:22.074 - 00:54:37.764, Speaker A: So this grant coalition wants to collaborate completely and not have a subset that is collaborating and benefiting.
00:54:39.144 - 00:54:53.524, Speaker B: So right now, I'm not actually defining a protocol, I'm defining a collection. And I'm saying a collection has these properties. Then I'll talk about what am I optimizing, given these constraints?
00:54:54.654 - 00:55:00.114, Speaker A: But don't you mean that a sub collection is not better than the old?
00:55:00.694 - 00:55:18.142, Speaker B: No. So in fact. So right now, this is just about individual rationality. It's not about how you split, you know, it's not cooperative game theory. In fact, I'm going to give you only non cooperative game theory type solutions. Let me give you one other example, because I know that we are running.
00:55:18.158 - 00:55:20.742, Speaker C: Out of time and I want there are to be enough things here so.
00:55:20.758 - 00:56:10.204, Speaker B: That we can continue the conversation after. So the other is something I'm going to call state of equilibrium. It's analogous to Nash equilibrium. What this means is that this amount of data is stable at stable equilibrium. If for every agent, that agent could not have unilaterally decreased their sample size or made it smaller. What that means is that if I am in this collaboration and I'm taking a thousand samples, if I were going to only contribute 900, I'm primarily hurting myself. Okay, so that's a more restrictive type of sort of allocation.
00:56:10.204 - 00:56:38.974, Speaker B: Okay, so then why do we care about these, of course, there is a notion of welfare for the agents. We started talking about what is reasonable and what is unreasonable, and they capture this. But it's also about the stability and usability of the system, because if the system is not individually rational, people are not going to sign up for it. If it's not stable, people are going to keep trying to game it by giving it different amounts of data, and then you don't know what's going to happen. In fact, if you take current algorithms.
00:56:39.844 - 00:56:41.700, Speaker C: You actually see that these algorithms are.
00:56:41.772 - 00:57:00.548, Speaker B: Pretty bad for, in terms of stability, at least what they're doing is that for more than 50% of the agents, the agents would have individually thought about decreasing their sample size by like 20 folds, which is pretty problematic. So now that we've defined these, the.
00:57:00.596 - 00:57:31.074, Speaker C: Questions you want to ask are, do these notions actually exist? Individual rationality always exists because you can just pretend that nobody's collaborating. But doesn't equilibrium exist? Unfortunately not in some corner cases, but they do generally exist under certain assumptions. The idea is that when they don't exist, it's because somebody else was better at doing your job using their own data set and much, much, much better than you are for doing your own job.
00:57:31.814 - 00:57:32.670, Speaker B: What do I mean?
00:57:32.742 - 00:58:11.914, Speaker C: Imagine this is not going to happen in real life. But just think about such a situation that I have three agents and each of them has a data set. And sort of the labels of the first person's data set is encoding the function that would have worked perfectly for the next agent. So a single sample from the first person's data set would have solved the situation completely for the next and so on and so forth. But for themselves, they need to see at least one point to be able to control their own gain. So what happens is that the first person is going to sample. That means that the second person is not going to need to sample.
00:58:11.914 - 00:59:07.420, Speaker C: The third person has to sample, which now means that this person's problem would have been solved. So this person now doesn't want to sample. So there's this natural cyclic behavior because it's actually very much closer to, like we are asking for something that's much closer to pure strategy, equilibrium, and we don't have continuous space and we don't have continuous options actions. And essentially the lack of continuous space and action is the only thing that's going to go wrong. So I'm not going to tell you when they do exist, but think about continuity now are equilibria efficient when they do exist. This is similar to types of questions that Eva was asking the other day, like, price of anarchy, but now this is about price of stability, of if I'm forcing myself to be in a. In an equilibrium, how much more samples do I have to take? Of course, sometimes the equilibria don't exist, but even in most realistic settings, this is not very good.
00:59:07.420 - 00:59:49.486, Speaker C: Like, one way to think about this is that even if I had continuity and many other aspects, you can imagine a scenario where I have one person whose data set is intersecting everybody else's. What this means is that both for rationality, actually, and for equilibria, in the optimal setting, if I don't care about incentives, the central person should do no work, because their problem is strictly, like, solved. But then everybody else's problem is solved. But at equilibrium, they should. Sorry, the other way. At equilibrium, they should do no work, because their problem is solved by everybody else's problem. But for the optimal solution, they have to do all of the work.
00:59:49.486 - 01:00:44.974, Speaker C: There's a trade off here. The person whose problem is subsumed by other people's problem happens to be also the person who is most efficient at solving other people's problem if they just overworked a little bit more on their own problem setting. And this trade off can show that you actually do have a pretty large gap between these solution concepts. There is a square root of k, at least that we know of. We don't quite understand these equilibria well at all, actually. But I think that one interesting thing is that it also very much depends on the shape of the error function. Of course, if this error function looked something that was linear, then we find a very interesting structural property, which is that free writing this thing that some people just do not contribute at all happens, and it has to happen as part of an equilibrium.
01:00:44.974 - 01:01:34.584, Speaker C: But it actually doesn't mess up anybody else's game, in the sense that if I think about those people who are not collaborating and I forget about them, everybody else is pretending to collaborate in an optimal way with the participating agents. So it's kind of nice to notice that in many of these situations, free riding effect is more kind of a local effect than a global effect. Okay. Um, so what I really want here is that I think we need a new theory, mathematical foundation for designing these learning algorithms that act globally, but have to consider pair agents objectives and incentives, and we don't have one. So it would be nice if the semester responds sometime on them. Okay, I'm out of time. So this is what we talked about today and yesterday.
01:01:34.584 - 01:02:09.512, Speaker C: Um, and sort of these were some of the questions we spent a lot of time on. Question one, actually, yesterday. These are all about learnability and question one and two. And today question two, we also talked about the principles of how to use data and how not to use it. And of course we saw things about collaboration. And one thing is that as we saw, like, no regret on online learning, actually played a role in everything. Sometimes as an optimization and achieving solution concepts, but other times also as some kind of a medium for collaboration, which I find quite interesting.
01:02:09.512 - 01:02:22.840, Speaker C: With that, let me not take more of your time, and thank you for listening for 4 hours. Do you want me to take questions or to.
01:02:22.992 - 01:02:24.816, Speaker A: Yeah. Thank you so much, Nica, for this.
01:02:24.920 - 01:02:25.472, Speaker C: Sure.
01:02:25.608 - 01:02:28.604, Speaker A: Incredible tutorial. Yeah. If someone has a great.
01:02:34.204 - 01:02:38.944, Speaker B: Is this issue going to be resolved if you allow for monetary transactions?
01:02:41.004 - 01:02:41.340, Speaker C: Yeah.
01:02:41.372 - 01:03:21.084, Speaker B: The question is that if you allow for monetary transactions, what happens? You can always allow for monetary transactions and go to a different solution concept. Okay. It doesn't mean that you're going to end up with high accuracy necessarily. Let me just say that I specifically pick non cooperative, non cooperative type of solution concept. There are solution concepts that are more about cooperative gains and I'm happy to chat about them. I think they are inherently very different from, like, you have to have a different perspective on what this data sharing is trying to achieve. Thank you.
01:03:21.084 - 01:03:23.444, Speaker B: Okay. Thank you.
01:03:26.414 - 01:03:39.774, Speaker A: So, yes, we have about an hour in this room to discuss with the events of the semester. Primarily, it's time for all of you to speak up and express your interest.
