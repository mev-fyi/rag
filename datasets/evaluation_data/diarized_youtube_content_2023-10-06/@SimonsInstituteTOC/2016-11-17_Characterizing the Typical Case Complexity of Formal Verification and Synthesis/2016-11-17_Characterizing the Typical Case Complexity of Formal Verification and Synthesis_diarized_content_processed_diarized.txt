00:00:18.400 - 00:00:43.290, Speaker A: Our next speaker is Sanjeev Seshya, who's going to talk about verification. Sanjeev. Right. Thank you for inviting me. So, I work in an area called formal verification, more generally, formal methods, and I'll tell you what that's about in a minute. But I think at a very high level, this area. One of the things about this area is that the problems are hard, in the computational sense, to start with.
00:00:43.290 - 00:01:28.184, Speaker A: So we start with NP and we go higher. And so that's why. One of the reasons why it's actually a good place to look for structure. That might explain why, in spite of problems being very high complexity, or even undecidable in practice, we're able to solve these problems in an industrial context. Okay, so the other thing I'll say is that I'm going to talk about three explorations spread over ten years, approximately. And there's very little really understood about the structure of problems in my field. And so, I think it's perfect for this workshop because there's a lot of industrial successes, but we don't understand why we're having these successes from a theoretical sense.
00:01:28.184 - 00:02:03.144, Speaker A: Okay, so what is formal verification? Well, basically, the essence is it's about computational proof. Computational proving. Right. So, algorithmic ways of proving or disproving that a program p satisfies a property phi. Okay, I'm going to use the term program, but, you know, you can think of it as a circuit or a model of a system that has both discrete and continuous components in it, whatever. Okay. These kinds of things that verification is applied to can be very diverse, but let's think of them as programs that have to satisfy properties in the abstract.
00:02:03.144 - 00:02:45.810, Speaker A: Okay? So, I want to mention two areas where formal verification has found industrial success. The first is in the design of digital circuits. So, microprocessors, ASICs, things like that. And, in fact, one of the places where formal verification is routinely used, I mean, really, every digital circuit goes through this process, is equivalence checking. So the equivalence checking problem is that you have two implementations of what should be the same boolean function. Um, um. So there's two combinational circuits, and you want to check that they're functionally equivalent, that they're the same boolean function.
00:02:45.810 - 00:03:22.802, Speaker A: Okay. And one place this happens is in the compiler tool chain for circuits, where you start with a high level programming language. Um, um. And then you turn it into gates, and you want to make sure that these two implementations are the same. Um, and so, um, it's easy to see that the underlying computational problem is satisfying, right. But in spite of that, you know, equivalence checking since the mid 1990s has been part of the tool flow, and there are commercial tools sold, and it runs in practice on these industrial designs. And so, I mean, there's obviously the question, you know, is sat easy for these instances.
00:03:22.802 - 00:03:53.442, Speaker A: Okay. So that's one thing we'll talk about today. The second area where it's increasingly being used is in the design of embedded software. So embedded software is the software that runs in everyday objects. So avionics has been a big customer of formal verification from the beginning because of the obvious safety critical nature. But increasingly, automobiles are another area where there's increasing need for verification. And people are looking into tools, and with IoT spreading, this is going everywhere.
00:03:53.442 - 00:04:19.122, Speaker A: Now, what are some of the key questions here? So you have programs and you want to verify safety properties of programs that they, they don't make out of bounds, accesses to memories that will then cause the program to crash and lead to some bad failure, things like that. Okay. What are the underlying computational problems here? It's very diverse. It's not as clean as that. But one of the things that has to be done is something called invariant inference. I'll explain that. But also theorem proving.
00:04:19.122 - 00:04:56.682, Speaker A: And the class of theorem proving that has become very popular in industrial practice is called satisfiability. Modular theories. Just think of this as sat with richer types. So don't just have boolean variables, but you have integer variables, you have variables that take array types, trees, more rich data structures. All right, so I'll talk a little bit about this as well today. So at a high level, how does formal verification work when you have your program P and you have your property phi, and the first step really is to generate a formal model. Yeah, question.
00:04:56.682 - 00:05:23.910, Speaker A: Yeah, just on the previous slide. Yep. Is. So the use of verification in embedded system, is that as successful as what is happening? It's growing. That's why it's not routine, but it's, I mean, it's the, this is, this is where all the demand is from industry these days. I'm just asking about. Not about the growth, but the success of it.
00:05:23.910 - 00:06:00.600, Speaker A: Yes, there are success stories and I can mention. But I mean, the reason I have the Airbus here is because the Airbus uses formal verification. And in fact, we talk a little bit about how that relates to the typical case structure. Okay, so Toyota is another big. I mean, I can mention various industrial studies if you want. All right, so the way this works is you take the program and the property, you generate an internal formal model, which typically has to have hints to the prover so that it can actually make progress. And then at some point, you throw it over to a solver.
00:06:00.600 - 00:06:19.180, Speaker A: Could be a sat solver or an SMT solver, a model checker or something. And the solver churns away and may need more hints. And this process iterates until you get an answer, hopefully. So, let's look at a toy example and see how this may work. So, here's a little program. Um, and so there's two variables, x and y. Uh, x is assigned the constant c.
00:06:19.180 - 00:06:49.574, Speaker A: Y is assigned zero. Then you have a while loop. So while x is greater than or equal to zero, uh, you decrement x. And then there's some non deterministic function that returns non deterministic, returns zero and one. Imagine this is something that we are abstracting away. We don't want to model the details, but if that returns true, then you, you increment y, and then you go back. Okay, so, eyeballing this, we can see that if x starts at c and it gets decremented by one in each iteration, then after c, one iterations, it should quit, and y gets incremented each time.
00:06:49.574 - 00:07:12.832, Speaker A: So it starts at zero. It should end at at most c plus one. Okay, so we expect that assertion to be true. Now, how would a verifier deal with it? Well, this is a program that has a loop. So it turns out what you need is something called a loop invariant. The loop invariant essentially is a property that holds at this point in the program. And if you go once around the loop, the property is still true.
00:07:12.832 - 00:08:05.878, Speaker A: So that's what the invariant means. So, I won't write down the exact invariant for this program, but the idea is there is some invariant l, and that allows you to not have to reason about the number of iterations of the loop explicitly. Instead, what you do is you break the proof up into three steps. So the first step is the initial condition, proof by induction. Basically, the initial condition says that if x is initially c and y is zero, and when this condition is true, then the loop invariant holds. And then the second one says that if the loop condition is true and the loop invariant holds initially, then after you go once around the loop, which is updated to x prime, y is updated y prime, the loop invariant holds on the primed variables. Okay? And then the last part says, once the loop exits, the invariant is still true and the assertion must hold.
00:08:05.878 - 00:08:26.926, Speaker A: Okay, so. So you break this up into basically answering the conjunction of three logical queries. You want each of these to be logically valid. Right. So it's true for all values of x and y. And this particular formula is over. Let's think of these as integer variables.
00:08:26.926 - 00:08:47.992, Speaker A: So, so this is, this would be a theorem proving or SMT solving. Question over integer linear arithmetic. Um, right, so that's the, that's the idea. So you call the theorem prover and you ask it, is it valid? Okay, so how was l updated? I don't understand. Um, l is not updated. You. So the idea is that let's assume magically somebody gives you l.
00:08:47.992 - 00:09:00.660, Speaker A: So that's going to be a formula in terms of x and y. And so you just have, you just plug in that I'll talk about later. This is also an important problem, like how do you come up with L? Right. L is a formula. L is a formula. That's right.
00:09:00.732 - 00:09:02.196, Speaker B: And what are the variables in the formula?
00:09:02.260 - 00:09:16.420, Speaker A: The program variables. Yeah. And you're ignoring termination. Well, I mean, the idea is that the loop in variant. That's right. So you can basically saying if it terminates, then the property holds. Right.
00:09:16.420 - 00:09:56.778, Speaker A: Of course you could try to prove termination, but I'm going to ignore that for now. What is ite if then else? Ok, so, all right, so again, this is the picture I showed you earlier. And so, one of the key steps in this model generation is the synthesis of the loop invariant L. That's actually a key computational problem that the verification community spends a lot of time on. The second part is, remember, it was reasoning over integer variables. So you needed really not a SAt solver, but something higher level SMT solver. But the way SMT solvers work is by reduction to Sat.
00:09:56.778 - 00:10:44.962, Speaker A: This is true of all SMD solvers, all the theorem provers today, internally, they have a Sat solver, and what they're doing is they're taking the original problem and encoding it to an equally satisfiable SAT problem. Okay? And so, basically, if you want to be able to do SMT solving, you have to be able to do Sat solving effectively. All right, so those are the three topics I'll talk about today. Okay? I'll talk about structure in SAT, structure in SMT and trying to understand why invariant synthesis is effective. And we'll see how far I'm able to go. Hopefully, I'll be able to cover all of these in a reasonable amount of detail. So let's start with SAt.
00:10:44.962 - 00:11:25.758, Speaker A: Right? So the question is, why do SAt solvers work well in practice? We talked about this yesterday a little bit at the discussion, and of course, the SAT community has been thinking about this for a while. And the hypothesis is that, especially with these industrial sat instances, there must be some property, right? That makes them easy. If we can capture that property, then we can try to prove something about it. And one popular proposal that was mentioned yesterday is this idea of community structure. So let me tell you what that is. Okay? So the idea of community structure actually comes from graphs, studies of graphs by not just computer scientists, but mathematicians and theoretical physicists. And the idea is that if you have.
00:11:25.758 - 00:12:53.974, Speaker A: If you have a graph and if you can partition it into components where the components themselves internally, are well connected, but the components between them are only sparsely connected, then the idea is that you can think of this graph as being partitioned into a bunch of communities. Internally. They're well connected, but they're not that greatly connected to each other. Now, one way of measuring community structure is using a metric called modularity, which I will define in a few slides. It's also called the q value. There are many metrics, and in fact, one of the things that we studied over the last year or so was trying to understand what would happen if we looked at a variety of metrics that measure community structure for SAT problems. And the reason community structure is so interesting in the SAT context is there's actually considerable experimental evidence that when a certain graph that you can build from the SAT problem, called the variable incidence graph, when that has good community structure, so it's got these components that are well connected, but only sparsely connected to each other, then the underlying SAT problems tend to be easy to solve by the leading approach to SAT, which is called conflict driven clause learning, or CDCL.
00:12:53.974 - 00:13:27.986, Speaker A: Okay. For purposes of this talk, it's not so important how CDCL works. I can tell you more about that after the talk. But all of these papers have given lots and lots of data from the publicly available industrial benchmarks showing that community structure seems to be a good explanation for why CDCL does well. So, in particular, here's one piece of experimental data from this paper that was at Ichkai last year. Um, now, q is this measure of modularity. So a high value of q.
00:13:27.986 - 00:14:02.454, Speaker A: So q value is between zero and one. A high value of q, close to one means that the structure is highly modular. Right? So very well connected components with very sparse connections between them. A Q value that's low is one where you don't really have this. Okay, so if you imagine things like random sat tend to be more like this. Now, they looked at two Sat solvers, glucose, which was one of the champion sat solvers of the CDCL. Type at the time, and march, which is a different sat solver that does well on random sat instances.
00:14:02.454 - 00:14:27.794, Speaker A: And what they found here is that. So they proposed a certain model we'll study soon. But what they have seen is that when the instances are highly modular, this solver, the look ahead solver, you know, it doesn't do well. So it times out at 10,000 seconds on many instances, whereas this, the CDCL solver, basically finishes in all of them within about, you know, a few thousand seconds. Question. Yeah.
00:14:28.254 - 00:14:30.566, Speaker B: Is the clause learning important in this result?
00:14:30.630 - 00:14:38.350, Speaker A: The correlation or clause learning is an important distinction between these two. Yeah, but no, what I mean is.
00:14:38.502 - 00:14:44.152, Speaker B: This kind of performance. Do other solvers that don't use clause learning also have that?
00:14:44.278 - 00:15:05.564, Speaker A: That's a good question, because I don't. The comparison was only between these two solvers. So it's. I think this is empirically based on other papers. This is also true. That is, high modularity is good for a number of CDCL solvers. I don't know if that holds for whether it's bad for the other non CDCL solvers.
00:15:05.564 - 00:15:23.710, Speaker A: That I don't know. All right. Yeah. And so, and you see, low modularity here is bad for the CDCL solver because it seems to be doing really bad, whereas the other solver does fine. Okay. Okay. So that's where we were last year.
00:15:23.710 - 00:16:30.146, Speaker A: And so, over the last year, two of my students, an undergrad, and one of my graduate students, and I started to try to understand, make an attempt to understand if we can prove or disprove something about this. That is to say, can we say something, in a theoretical sense about CDCL performance and community structure? And so the first thing we did is we generalized this metric, called modularity, to a broader class of metrics that we call polynomial clique metrics. I'll discuss that. And basically showed that if you look at. If you try to define modular instances according to any of these metrics, then the class of modular SAT instances according to this metrics, the SAT problem remains np hard for them. So, basically, it's not really explaining everything. And secondly, the folks who presented the data that I showed in the previous slide, they actually proposed a random modular SAT model similar to the random KSAT model.
00:16:30.146 - 00:17:14.024, Speaker A: They proposed a random modular KSAT model, and that was, they generated instances from that model, and that was what the previous slide's data was from. So, you know, there was a question about whether you can actually prove that CDCL will work efficiently on that class of instances. And we showed a hardness, a negative result, basically saying that with high probability, the instance is going to be hard for CDCL in the sense that this is for unsat instances. So if you look at unsat instance from this model, then CDCL is going to need an exponential time to determine that it's unsatisfiable.
00:17:14.364 - 00:17:25.367, Speaker B: So typically, modularity tends to want like a graph that breaks up into two or three communities or like a small number. What if you had many, many small communities? Does that change these results?
00:17:25.455 - 00:17:54.446, Speaker A: Yeah, so it's a good question. So, I mean, here you can see that the conditional nature of our negative result is if you have a small number of communities, then the result applies. And it's an open question, although we have some experimental evidence suggesting that this result may extend even for larger communities. But it's still an open theoretical question. All right. Okay. So at a high level, the point, the thing that we've become reasonably convinced about is that the definitions of community structure, as the SATs community is using it today are not enough.
00:17:54.446 - 00:18:13.302, Speaker A: You need to refine them. Okay, so, um, so let me go through a few definitions. So the graph abstraction of the Sat problem is something called the variable incidence graph. Basically, every node corresponds to a variable. And if two variables appear in the same clause, you add an edge between them. It's an undirected graph. Okay.
00:18:13.302 - 00:19:01.804, Speaker A: Um, and so, uh, we're going to look at metrics on these, on these sorts of graphs. Um, and so in general, a graph metric is something that is a function that maps a graph to a number in zero one. Um, and so this modularity, uh, that has been defined in the community is always, is a metric like this. Uh, we're going to denote, uh, by sat m, comma, epsilon, the class of sat formulas, um, that have high modularity according to this metric m. So basically, m on the variable incidence graph of the formula from this class, phi is greater than equal to one minus epsilon for any specified parameter epsilon. Okay? So you fix an epsilon, you get a class of formulas. Okay? And so modularity is this q metric that I'll define soon.
00:19:01.804 - 00:19:48.618, Speaker A: So when I say sat q epsilon, it's the class of formulas with high modularity as defined by the parameter epsilon. Okay, so what is the q metric? So consider the variable incidence graph of some formula phi, and then consider a partition of the vertices. Okay? So delta is a partition that breaks g phi into n components c one through cn. So then the q value is defined as follows, right? So it's the max over all partitions delta of this particular term. So let's see what that term is. So if you look at this first term, it's basically. So it's defined for weighted graphs, but you can assume that the weights are one for the variable incidence graph.
00:19:48.618 - 00:20:29.086, Speaker A: And so this is basically the sum of overall edges within a component c divided by the total number of edges. And so basically this term, the red term, is the fraction of edges that lie within the communities. And the second term is essentially, it's based on the degree of vertices. And if you work it out, it turns out to be the expected fraction of edges that lie within the communities. If you maintain the same degree distribution, but you randomly distribute the edges. Okay. All right, so it was previously observed.
00:20:29.086 - 00:21:03.974, Speaker A: I mean, this was not proved, but it was observed that any CNF formula can be polynomially transformed into an equally satisfiable formula with arbitrarily high modularity. And actually it's a very simple construction. Okay, but I'll show you this in this next slide. But the idea is that, you know, if you can of course do this, then you can embed any sat problem into a high modularity instance. And therefore the set of all high modularity instances according to q is np hard. Okay? And the idea is as follows. You take arbitrary Sat problem, you compute its variable incidence graph, let's say it's that.
00:21:03.974 - 00:21:41.870, Speaker A: And then you just replicate it. Okay, so basically if your original formula was phi, then you make little c copies of phi, where internally you rename the variables. So each of these copies, the nodes correspond to different variables, but basically they're all symmetric. And you can see that this new formula, the conjunction of phi one through phi c, is equally satisfiable with the original one. So it's satisfiable if and only if the original one is satisfiable. Um, and, um, uh, but, but you can also see that this is excellent from a modularity perspective, right? Very well connected internally. Not connected at all.
00:21:41.870 - 00:22:12.770, Speaker A: Right? And so, um, so basically, if you work out the details, and I won't go through it this in very detail, but the idea is that for this new formula, phi prime, you can work out that the modularity is one over one minus one, one minus one over the number of communities. And if you let the number of communities increase with the number of variables, then you can make it arbitrary close to one. So that's about modularity. And so the question that was much harder.
00:22:12.802 - 00:22:40.396, Speaker B: By doing that, you replicated, I mean, you increased the size. The way I was looking at it is that your community structure on your original stat problem is essentially giving you almost decomposability. So your. At least intuitively, you are solving the SAT separately in the communities with sort of some weak interaction, and you're exploiting that weak interaction to it, but this seems like you're replicating the original.
00:22:40.420 - 00:22:51.224, Speaker A: That's right. So I'm not. This is not a solution strategy. Right. It's just basically, this is showing that modularity is a bad explanation. Right. Because highly modular instances can be hard.
00:22:51.764 - 00:22:54.188, Speaker B: I think the point is polynomial versus exponential.
00:22:54.236 - 00:22:56.932, Speaker A: Right. And that, too. Right. You want to make polynomial in many copies of this.
00:22:56.988 - 00:22:59.180, Speaker B: There's only many copies that won't change.
00:22:59.292 - 00:23:25.332, Speaker A: That's right. Yeah. Yeah. That's an important point. All right, so, um, so now we can generalize this to actually a broader class of metrics. And the idea is that, you know, modularity. So the question is, you know, maybe the q function was some weird thing, right? And if you defined a better metric, then it that also captured modularity in the intuitive sense, then maybe that would be the right explanation.
00:23:25.332 - 00:25:12.312, Speaker A: And so we thought about that and said, okay, what graphs have ideal community structure, right? And can we define a class of metrics that gives a high value to these graphs and low values to graphs that don't have this structure? And if you think about what graphs have ideally structured, one extreme is to basically look at cliques and basic collections of cliques, unconnected collections of cliques. So if I look at just a single clique, then if that itself is the entire variable incidence graph, of course, this has poor community structure because it's globally well connected. But if I take a clique and I make many, many copies of that clique, probably not many copies of that clique, and don't connect them up, then this has, ideally, community structure. So we defined a family of metrics that we call polynomial clique metrics, and we don't have to read through the details, but the idea is that a polynomial creek metric is one that gives large values for any epsilon. It gives you a modularity value bigger than one minus epsilon for a polynomial connection of creeks. And essentially, the previous transformation that I showed from any arbitrary sat problem to. To a problem that is a conjunction that shows that modularity is a PCM, but it can also be slightly tweaked to show that any polynomial clique metric, even for any other polynomial click metric, you can take an arbitrary sat instance and embed it into an equally satisfiable instance that is highly modular according to that polynomial click metric question.
00:25:12.448 - 00:25:25.488, Speaker B: Generally, doesn't it suggest that graph structure can't help you because I can take a hard instance and then put it in a quadratically large graph, where the quadratically large graph has lots of structure, but it's avoiding the core hardness that comes from this tiny portion.
00:25:25.576 - 00:26:01.038, Speaker A: Yeah. So I think in general, at least, the variable incidence graph by itself is not enough. And one important thing it ignores is polarity, which I think is pretty important for Satan. But by itself also, I don't think it's enough. There's a few other things that you need, and one thing that we feel is important is the number of communities. But in fact, I don't really know have the answer to your question. I mean, maybe Kevin and Holger people have studied lots of sad instances, convey in and say what else might be necessary.
00:26:01.038 - 00:27:10.494, Speaker A: If you take the size log in, you'd be happy. Boom, boom, boom. Solve, mom. Okay, so that's one result, that is, for any PCM, if you compute this class, sat m epsilon, that's going to be np hard. But then we wanted to dive in a bit deeper into just the random SAT model that was proposed last year, right, for random modular Sat, and try to analyze if, that is, if you can prove something about that. And so let me tell you a little bit about that particular result. So these conflict driven clause learning Sat solvers, CDCl sat solvers, all use one proof rule in coming up with a proof of unsatisfiability, which is resolution, right? So the resolution proof rule essentially is if you have two clauses which have the same variable but in opposite polarities, so b here and not b here, then you can derive from these two a new clause that drops b and b and not b and.
00:27:10.494 - 00:28:35.334, Speaker A: But keeps all the other difference, right? And then the proof of unsatisfiability from a CDCl solver is essentially a sequence of resolution rules, applications of resolution rules, that results in the empty clause being derived. That is, you have b and not b at some point, and you derive the empty clause, okay? And because all CDCl solvers, at the core, even they're proving unsatisfiability, they are essentially, they're doing resolution. The length of the shortest resolution proof is going to be a lower bound for the runtime on unsat instances. Right? Um, and so if we can show that, um, for, uh, some collection of CNF formulas from some family of CNF formulas, if, um, and especially if they come from some random model, if we can show with high probability the resolution proofs are going to be exponential sized, then we know that CDCL is going to take exponential time. Okay, so, um, so, such as analysis was already done for the standard random sat model, uh, in the nineties. So you can think of the set of random k stat formulas is being represented by this. So it's FknM, n is the number of variables, m is the number of clauses, and k is the number of literals in each clause.
00:28:35.334 - 00:29:15.964, Speaker A: And let me skip that. And so the result in particular is by beam and petasi from 96, which, where they show that if you look at this class of formulas. Okay. Um, then, uh, it requires, uh, exponentially long, uh, resolution repetitions, okay? With high probability defined over the distribution of formulas. Okay. Um, and so the, we, basically, what we did is, uh, we took the model proposed last year, which I'll explain in a, in a slide, um, and essentially proved the same theorem for it. We took beam and Pitasi's proof technique and just adapted it for the new model.
00:29:15.964 - 00:30:08.410, Speaker A: Okay, and so what is this new model? It's called the community attachment model and it's the one from which I showed the data earlier. And the way you construct formulas is, follows. So you have two more parameters. C for the number of communities and p for probability that a clause is inside a community or across communities. And the way it works is you fix a set of variables and then you fix a number of communities and you make them even. Okay? And then you're generating clauses, right? And so for m iterations with probability, you toss a coin and if it comes up head, then you add a clause within a community, okay? With k literals. If it comes up tails, then you add a clause that bridges communities, that has one literal from each community.
00:30:08.410 - 00:30:51.010, Speaker A: And so these clauses are called community clauses. These clauses are called bridge clauses. And so your overall sat problem ends up being a combination of community clauses and bridge clauses. Okay, so I'll just skip through this in the interest of time. And so basically our theorem is that if you look at the following families, so the number of clauses is of the order of number of variables, the number of communities is, is small in the sense it's about n raised to 0.1. Then that random modular family will, requires exponentially long resolution repetitions with high probability. And this holds for all p.
00:30:51.010 - 00:30:56.534, Speaker A: So p is this parameter which decides whether something is to be added inside a community or across communities.
00:30:59.094 - 00:31:08.394, Speaker B: Sorry. So those are not standard models for communities. I mean, this is not quite preferential attachment. It's not quite preferential attachment. Right.
00:31:10.054 - 00:31:11.634, Speaker A: What is preferential attachment?
00:31:12.254 - 00:31:18.874, Speaker B: Preferential attachment models for communities are more like, new note, comes in and then.
00:31:19.694 - 00:31:22.246, Speaker A: Oh, I see. Okay. Okay. I see what you mean.
00:31:22.350 - 00:31:22.790, Speaker B: That's not.
00:31:22.822 - 00:31:38.476, Speaker A: This is probably not. Yeah. Yeah. So, I mean, again, the term community attachment model is something that the authors of the paper, the HKAI paper last year used. So I'm not. It may have been that this, that phrase is used in a different context by. By others.
00:31:38.476 - 00:31:58.314, Speaker A: And so anyway, so, I mean, all I'm saying is a community of tag. It is a community. That's right. Indeed. Yeah. Okay. But I think for the context of this work, it's the model which empirically was shown to have good, positive correlation with the performance of CDCL sat.
00:31:58.314 - 00:32:40.180, Speaker A: And what this is essentially showing is that when the number of communities is small relative to the number of variables. Right. Then it doesn't really explain things. I see that I'm going short on time, so I'll just wrap up this. Okay, so I think, I want to talk about this slide. So there's a number of questions that I think this initial work has thrown up. Like, we just basically did this investigation over the last year, and I think we have more questions than answers here.
00:32:40.180 - 00:33:22.576, Speaker A: So the first is, of course, you know, can we define a better measure? The second is actually trying to improve our result. So our result only holds for the number of communities that's, you know, o of n raised to 0.1. And in particular, we don't do anything interesting with this parameter p. And so we're interested in seeing if our result can be improved by a result that actually uses some properties of the parameter p. And of course, in particular, what we're interested in doing is trying to extend the result where the number of communities grows bigger than 0.1. N raised to 0.1. Okay.
00:33:22.576 - 00:34:11.624, Speaker A: And in particular, a related question is, you know, can we do some experimentation to find out what happens as I grow the number of communities? Okay. Relative to the number of variables. And so, so let me actually show some very preliminary data on that. But I have to admit, I don't really do a lot of these large scale sat benchmarking experiments. There are others in the room who have more experience with that. And so this is very, very primary. So the idea is, what we did is we, on the x axis, we varied community size, and we did this for fixing the relation of number of communities to number of variables based on this parameter alpha.
00:34:11.624 - 00:34:42.002, Speaker A: So the number of communities is n raised to alpha, where n is number of variables, and we varied alpha from 0.1 to 0.4. So remember that point one is basically threshold at which our negative result holds. So we're trying to see what happens as you go higher. And the running time of a CDCl sat solver minisat is on the y axis in log scale. And so what we are seeing here is that for all of these values of alpha, the runtime grows linearly, exponentially with the community strives. Okay.
00:34:42.002 - 00:35:34.456, Speaker A: Um, and so there is still exponential growth in the runtime for even as you increase alpha. But what you can see is actually that all of these are bunched up pretty closely, which means that for higher values of alpha, in some sense you have to go further out. You have to look at larger instances to see the real blow up in runtime. And furthermore, it seems to be that, um, the size of communities seems to be a stronger determinant of the runtime than sort of the overall size of the instance. Right. And so, um, maybe that's a pointer to how, to, how to refine the model, because the model, as it is doesn't really take the size of the community into account, uh, in trying to predict the runtime. Okay, so, um, I'm a little bit over 1050.
00:35:34.456 - 00:36:27.360, Speaker A: How much time do I have? 1111. Okay, great. So I have a few more things to talk about, and I'll have to make a choice here, but let me give you a one slide overview of this, and then I'll talk more about the invariant synthesis, which I think has a different link to learning theory. And I want to explore that link along with this audience. So, again, so now let's switch from Sat to SMT, because for a lot of problems, you start not necessarily with a Sat problem, but you start with a problem that involves these richer types. And in particular, whoops. One of the very useful theories, logical theories in program verification, is, is linear integer arithmetic.
00:36:27.360 - 00:37:14.344, Speaker A: Right. Um, so, uh, it's basically, uh, arbitrary Boolean combinations of affine constraints over integer variables. Okay, so this is, I mean, it's basically like disjunctions of integer linear programs, right? And just looking at feasibility questions. Uh, but the, the reason, um, this is usually integer programming solvers don't really do well on these, on these sorts of formulas that come from verification. And the reason is that they have very rich Boolean structure, which sat based techniques do much better on. Okay? Now, so this problem is NP complete. And one of the techniques that has been found quite effective is this idea called small domain encoding.
00:37:14.344 - 00:38:04.610, Speaker A: Now, initially, it may seem a little counterintuitive to many of you, but, but actually, trust me, this works really well. So the way this works is you have a formula over n variables. They're all integer variables, so each xi lies in z. What you do is you find a set of values, si, which is a subset of the domain, and it is usually a small finite subset, much, much smaller than the size of the domain. And then what you do is you restrict your search for, for satisfying solutions to just the sis. Okay? Um, and one way to do that is you encode each of the integer variables as a finite precision integer that takes values only in the set. And then you translate the overall formula into a Sat problem, and you give it off to a sat solver.
00:38:04.610 - 00:39:09.434, Speaker A: Okay? Um, and so, and, and there are variants. I mean, this is one encoding called small domain encodings. There are other encodings that SMT solvers use, and they all translate to sat, exploiting these sorts of things. All right? So now, clearly, the size of this domain is important, because in general, if your domain size is really large, even though the number of bits in your Sat problem is logarithmic in the domain size, still, this can be really, you have to be careful to make sure that you find these are really small domains. Okay? And so in general, um, you know, a lot of these bounds come from, uh, originally came from the proofs that, um, integer linear programming is in NP, right? So you have a small satisf, a polynomial bounded, satisfying solution in the number of bits. And so this is one such bound that, that, uh, from a paper by Christos, um, and you can see that if you take log of this, where m is the number of, uh, inequalities, n is the number of variables, b, max is the largest constant term, and a, max is the largest coefficient. Right? If you take log of this, it's a polynomial.
00:39:09.434 - 00:40:13.794, Speaker A: But as, um, you know, as a number, it's pretty large because you can think of m as in the thousand. So you have thousand raised to 1000. Right? So the size of the domain here is not something that you would really ordinarily want to search on. All right? So, um, so the reason I'm talking about it here in the presentation is that, um, really what we've done in the verification community is we've studied the structure of linear constraints that come from program verification and hardware verification, and it turns out to be not arbitrary linear constraints. Okay? Um, so one special case that's been well studied is if you, what if you had just equalities? And you may say, well, that seems really, uh, like a really dumb, stupid class, but it shows up amazingly, uh, often. I mean, most of the inequalities, most of these, these, uh, linear constraints are equalities because in a program, for instance, every assignment in the program becomes an equality, and many of them are just relating, there's copying things around, okay? And it turns out for that, the size of the domain is just number of variables, okay? Yeah. Do you just mean equality between two variables or do you mean equality between two variables? Yeah.
00:40:13.794 - 00:41:12.914, Speaker A: Or in general two terms, I mean, they can be one side can be like an array axis or things like that, but. Okay, so I'm, again, in the interest of time, I'm going to skip over some of this and I'll show a summary table, but I think this is perhaps the most interesting slide. So we did a pretty exhaustive study, this is of course, almost ten years old now, of what kinds of linear constraints show up in verification. And we found two characteristics. One is that they are mostly difference bound constraints. So they are constraints of the form xi minus xj greater than equal to a constant, okay? And even those that are not difference constraints are very sparse. So, you know, if I think of a one, x one plus a two, x two to a and x ten, most of the AI's are zeros, right? And those that are not zero tend to be small.
00:41:12.914 - 00:41:51.476, Speaker A: Okay? And so basically you can exploit that. So this is, you know, you introduce new parameters for the number of non difference constraints and to measure sparsity, like how many variables are there per constraint. And this bound that I showed you earlier simplifies very nicely. So the exponent goes from a linear function of the number of constraints to simply the number of non difference constraints. And then in the Bayes again, m gets replaced by the number of variables, the maximum number of variables in any constraint that has non zero coefficients.
00:41:51.580 - 00:41:53.220, Speaker B: So you observe this in practice.
00:41:53.292 - 00:42:46.914, Speaker A: We observe this in practice, and then we introduced these two new parameters and proved a new bound, okay? That is, if you want to bound the size of the set of solutions, such that if there's no solution in this set, then there's no solution at all. If you want to find that bound, um, it simplifies as follows. All right? So, um, so, in fact, I mean, for, and we have a range of different things. So, equalities, difference bound constraints, and there's something called octagons that I'll talk about next, which is also very similar. And then full integer linear character. And so, um, and this is one of the reasons why, uh, you know, solving integer linear arithmetic in the SMT community is, is now it's a solved problem, routinely done very big formulas. And really, what's happening is they're on solving arbitrary linear constraints.
00:42:46.914 - 00:43:41.574, Speaker A: Okay, so I'll finish up with one last thing, which is very new, but it has some links to machine learning theory and actually has this genesis of this work is in a conversation AvrAHAm and I had when I was at CMU 15 years ago. So this is about learning program invariance. So, if you look at the previous example, the problem was to find a loop invariant. And it turns out this is the loop invariant that works here, x plus y less than or equal to c. So you can see it's true initially, and every time you go down over the loop, x gets decremented and y maybe gets incremented. Right? Now, this constraint is what is called an octagon constraint. So it's an octagon constraint is a generalization of difference bound constraints where the coefficients can be plus or minus one.
00:43:41.574 - 00:44:19.774, Speaker A: So there are 45 degree hyperplanes that mention two variables. And why is octagon constraint interesting? Well, in the work that I mentioned previously done for Airbus, so there's a program verification tool called astray that's been developed in France. They did an extensive evaluation and found that in order to infer invariants, octagons, the class of linear inequalities called octagons, was the perfect balance in terms of automatically inferring these and then being able to prove it on millions of lines of code.
00:44:22.084 - 00:44:23.932, Speaker B: Or is it a property that's learned?
00:44:24.028 - 00:44:57.700, Speaker A: It has to be generated, is inferred. So it has to be inferred from the. Yeah, so how it is generated varies depending on the technique. But one of the big ways that recently people have been doing it is learning invariance from test data. And so really, the way it works is you give a bunch of test cases and you find what are some likely invariants just by learning relationships that are consistent with your test data. And then you invoke a theorem prover to try to prove that indeed it's a valid loop invariant. And if it succeeds, you're done.
00:44:57.700 - 00:46:05.954, Speaker A: You have a loop invariant, but if not, you get a counterexample, which is, again, a test case that tells you why this is not an invariant. And you add that back in your set and you iterate this. And so this, actually, this process looks very much like query based learning, because you can think of the invariant synthesis technique as a learner and the solver as the teacher or the oracle that answers correctness queries and gives counterexamples if it's not correct. And so the obvious question is, how many iterations do you need? Because potentially every call to your theorem prover is a little expensive. And so you want to do this quickly, you want to converge quickly. And so it turns out this is related to this idea of teaching dimension from, from learning theory, which is essentially, if there's a teacher who knows a concept and they want to convey the concept to students by just using labeled examples, then what's the minimum number of examples? They need to convey any concept from a concept class, and it's best understood looking at an example. So this is an example from the Goldman Kern's paper that I cited earlier.
00:46:05.954 - 00:47:10.144, Speaker A: If the teacher has to teach a two dimensional box that's lying on a grid, then, but they can only give examples labeled examples on the grid, then this is an optimal teaching set. So you say that the diagonally opposite corners are positive examples, and then their outside nearest neighbors are negative examples. And is this in two dimensions? And in d dimensions, the construction extends. So you have the diagonally opposite corners, and then you have the d neighbors on each side. So it's 2d plus two. All right, now what's interesting about this is that hyper boxes correspond to a very simple class of inequalities, which are just bounds. So if I have program variables that just lie between upper and lower bounds, then the conjunction of such inequalities is going to be a hyperbox, right? And these are, these appear in things like array bounds checks, and so simple program invariants, but they're not enough in general.
00:47:10.144 - 00:48:18.274, Speaker A: And so that's where we started. And so one of the things is that it's easy to see that the teaching dimension is a lower bound on the number of interactions you have between the learner and the teacher. And so what we looked at was, what is the teaching dimension for octagon constraints? Because it's, you know, in program verification, octagon constraints seems to be a class that's found to be quite useful. And so, you know, I'll skip over this, but basically the idea is here you can visualize in two d and say, well, you know, how many points do I need in the same way as a rectangle so that I can I define this octagon exactly, but I can't move it in or out. And so it turns out that you can do this with four positive examples and eight negative examples. And the idea is that the positive examples are vertices that cover all the faces covering set. And then the negative examples are points that essentially disallow you from moving these faces anymore outward.
00:48:18.274 - 00:48:57.722, Speaker A: Um, and it generalizes nicely so, we've been able to prove that for d dimensions, the teaching dimension of octagons is 2D plus 2d squared. Okay? And so, now, thinking back in the. In the context of program invariant synthesis, what this is telling us is that, uh, so D is the number of variables in your invariant. And, um, in program, uh, verification, what happens is that invariants tend to be over programs that. Variables that are in scope at that point in the program. And so, even though your program may have, you know, thousands of variables, usually, if it's well written, the number of variables in scope at any given point of time is much smaller. It's on the order of ten.
00:48:57.722 - 00:49:19.484, Speaker A: Right. And that's what d is. And so. And, you know, empirically, people have found that octagon constraints are easy to infer with just a few test cases. And so this seems to be a sort of a theoretical explanation of why that might be happening, because, really, the teaching dimension of octagons is actually. Is polynomial. Right.
00:49:19.484 - 00:49:39.504, Speaker A: Okay. So, again, I mean, this is very, very preliminary, so I think there's a lot more to be done. And this is, teaching dimension is only a lower bound. So we still want ways to understand an upper bound, techniques that are used for invariant inference. All right, so that's the summary. So, I talked about three things. Sat solving is really the.
00:49:39.504 - 00:50:21.174, Speaker A: The core computational problem in verification. And although community structure is a promising start, we need more SMt solving. I think it's really written on ways of exploiting special structure of constraints that arise in verification. And then invariant synthesis, I think, is, there's a growing sub community that work on invariant synthesis who are interested in using techniques from learning. And I think what's really interesting is to understand why these techniques are doing well by leveraging ideas from learning theory. So, with that, I'll stop here. Thank you.
00:50:21.174 - 00:50:36.404, Speaker A: So, there's a break starting 15 minutes ago, going until 1120. So maybe we just have questions over the. Over the coffee. How about, let's do that. Let's thank Sanjeev again.
