00:00:38.754 - 00:00:39.974, Speaker A: David, you ready to start?
00:00:40.314 - 00:00:41.694, Speaker B: Yeah, absolutely.
00:00:46.274 - 00:01:05.544, Speaker C: All right. Hello, everyone. Thanks for making it here on a nice, crisp Berkeley morning. It's my pleasure to introduce David Gamarnik, who until very recently was my only office mate, and now I have an empty office with just me. And he's going to tell us about a curious case of a symmetric binary perceptron. So take it away, David.
00:01:05.924 - 00:01:33.230, Speaker B: Thank you, Ashwin. And, yeah, good to see everybody. Well, good to sort of see you. I missed already great interactions at Simons. I had to be back for teaching, and I only wish I could spend the whole semester there, as many of you do. So enjoy it while you can. It's a pleasure to share with you some recent work.
00:01:33.230 - 00:02:00.838, Speaker B: It's actually work in progress, and I'll explain what part of it is still in progress as we progress, no pun intended. And it's called the curious case of the symmetric binary perceptron model. I will explain what's curious about that. This is a joint work with Aaron Kiseldak, my student at MIT. He should be somewhere in the audience, so. Okay, good. Right.
00:02:00.838 - 00:02:29.802, Speaker B: So, Erin. Okay, so if there's any questions from the audience, Aaron will be happy to answer them. It's also Changji Su, who is a postdoc at Harvard, and Will Perkins from the University of Illinois in Chicago. Most of you probably know will very well. Okay, my battery is almost dead, so. Just a second. Okay.
00:02:29.802 - 00:03:07.294, Speaker B: Just like yesterday's Lenka talk, I want to start by acknowledging. By acknowledging Giorgio Parisi receiving a Nobel prize. That's an incredible. For us. As needless to say, Georgia is an incredible contributor to the field. To our field. So it's not just physics, but his name has been mentioned many, many times in this and previous workshops because of his fantastic contribution to the structural and algorithmic ideas in the random structures and optimization.
00:03:07.294 - 00:03:48.836, Speaker B: And this work is actually not an exception for that. And so I thought, as my own tribute to Georgia, I'll do my own rendition of his picture. So I have to say that my handwriting skills is not that good, but my drawing skills are even worse. So you. Please. Maybe I'll give him a little bow tie. I think it's appropriate.
00:03:48.836 - 00:04:10.414, Speaker B: So. All right, so great. Great news. Okay, now, slightly and more serious note, there's a word, Perceptron, in the title. In the title. Just a brief history of that. Arguably, Perceptron is the marks the birth of the machine learning field.
00:04:10.414 - 00:05:03.746, Speaker B: It was introduced by Frank Rosenblatt from the University of Cornell in 1958. And it was supposed to. It was implemented on this little machine in IBM, a little machine, you can imagine, it's several rooms large, and it was supposed to do the recognition, face recognition, or I think maybe some kind of an image recognition, I don't remember. And it was arguably the birth that marked the birth of the machine learning field. This was so amazing that back then the New York Times wrote an article about that. And I quote here, New York Times reported that the Perceptron will be the embryo of an electronic computer that the Navy, of all things, expects to be able to walk, talk, see, write, reproduce itself and conscious of its existence. And this is 1958.
00:05:03.746 - 00:05:41.490, Speaker B: So as you can see, the doomsday scenarios of machines overtaking humans is not something new. It goes back ages. Anyways, a little bit of history on that and back to our business. Don't worry, this is not going to be an applied stock. We're going to just talk theory, right? A little, just a little digression. Okay, so let me introduce the model, the binary perceptron model. So its binary is in reference to the solution space being a binary cube.
00:05:41.490 - 00:06:34.800, Speaker B: So we're going to fix a binary cube of n dimensions. Okay? Then we fix a matrix, which is m by n, a matrix. We also fix a subset of a set of real values, r. And our goal is to find sigma in the binary cube such that a. So when we hit this matrix with the vector sigma, it lies in the set c to the power n. So in other words, for every row, when you hit it with the vector sigma, it belongs to the set circle. And the two common examples that have been considered in the literature and the ones we will consider today are the following special cases.
00:06:34.800 - 00:07:38.888, Speaker B: This case, one, and this is going to be the main focus of today, is when c is just a symmetric interval around zero. And we will refer to it as a symmetric binary perceptron model. But I will also mention another case, case two, when c is just a set of non negative reals, which is referred to asymmetric binary perceptron model. In this case, you need to find the vector sigma such that a of sigma in l infinity, norm is in the interval from kappa minus kappa to kappa. And I will also use a normalization square root n normalization in anticipation of the scaling that we will be interested in. And that's a natural scaling to do. We will see why.
00:07:38.888 - 00:08:28.432, Speaker B: All right, so just algorithmically, you have a matrix, you have a set. You want to find a binary vector such that when you hit the matrix with this vector, it lies in some subspace of euclidean space. And that's the problem. Now, naturally we will focus on the case of the random instances. And in random instances, we are interested in the cases where entries of the matrix are either normal, standard normal, or they are rademache. Okay, plus minus one, with probability one half. So, random variables.
00:08:28.432 - 00:09:40.288, Speaker B: I will freely go between the two cases, because for most of the party wouldn't matter. The details of the distribution for many of the things I'm about to say wouldn't don't matter. In some cases they do. It's not surprising that many of the things are exhibit universality property, where the details of the distributions are irrelevant, as long as the first two moments match, the first moment here being zero, of course. So if you have this model in mind, then the kappa being constant, kappa being constant, makes sense, because if you take matrix a one over square root n times sigma, then each row, then you get the vector, each entry of this vector is going to be normal if a is standard normal. So you want to make sure that all of such standard normals are in the interval between minus kappa and kappa. And of course that automatically wouldn't be the case because the extreme of the Gaussian is like square root, log n.
00:09:40.288 - 00:10:46.654, Speaker B: So it's not by no means obvious that such a thing is possible. Okay, so that's the, this is the model. And the questions we wanted, we want to ask are fairly standard questions that we ask in this conference. So the first questions of existence, and let me introduce the notation let sigma of a and the underlying set where we want the vector to be is. I'm not going to make it part of notation. This is all sigma in the binary cube, such that a sigma is normalized by square root n is in the set, whatever set we care about. Okay, so it's a set of satisfying vectors, satisfying solutions, satisfying solutions.
00:10:46.654 - 00:11:31.772, Speaker B: So the questions of existence is that is a empty or not? And that's the first question. And the second question is algorithms. If we somehow know that such a set is non empty, can we actually find such a vector? Because this solution space here is binary, it's not obvious a priori that such a thing can be done, as opposed to the classical Rosenblatt case, where if there is a linear separator, you can actually find it by many ways. All right, so this is the model. It obviously goes without saying. If you have any questions about the model, go ahead. I hope the model is clear.
00:11:31.772 - 00:12:45.844, Speaker B: But any questions on the model before I turn over and share with you what we know about this model, mostly I will talk about the symmetric case. So let me actually start now with the. Give you some background. And for the symmetric. So, in this, this means that we want to all entries of the vector to be in the interval minus kappa, two Kappa. So first, so we're going to consider, for both symmetric and asymmetric case, we're going to consider the proportional regime, and that means that m, the number of rows, is going to be some alpha times n, which is the number of columns, and alpha is going to be fixed. And so what do we know about that? The fairly straightforward argument shows that this solution is empty with high probability, if alpha is bigger than alpha critical, and alpha critical is actually log two divided by absolute value of log kappa.
00:12:45.844 - 00:13:10.320, Speaker B: Okay. In particular, it scales like order one over log. So what is this coming from? It comes from a very straightforward analysis. You altogether, you have two to the n solutions in the binary space, and you look at the probability. Oh, no, sorry, I made a mistake here. I made a mistake. It's a bit premature.
00:13:10.320 - 00:13:21.406, Speaker B: The denominator here is the probability that absolute value of the Gaussian is at most kappa. Okay, so I jumped too fast.
00:13:21.550 - 00:13:24.914, Speaker A: So, David, we need an extra log, I guess.
00:13:25.974 - 00:13:26.862, Speaker B: I'm sorry.
00:13:26.998 - 00:13:29.254, Speaker A: We need an extra log in the denominator.
00:13:29.374 - 00:13:30.158, Speaker B: Thank you.
00:13:30.286 - 00:13:30.994, Speaker A: Yeah.
00:13:32.094 - 00:13:47.510, Speaker B: Thank you. Yes. So this comes from. Absolutely. So this comes from. From the fact that, okay, each row, upon hitting with the binary vector becomes a Gaussian. This GAussian needs to be in the interval.
00:13:47.510 - 00:14:25.404, Speaker B: So you have this condition, and all of this, this needs to happen for the alpha times n different rows. On the other hand, you have two to the n different solutions. So you want to, at the very least, make sure that this goes to infinity as n goes to infinity. And if an alpha is critical, is reversed engineered from that. So if alpha is bigger than one, the expected number of solution goes to zero. So that's kind of obvious. But what is not obvious is actually this alpha critical is, Alpha critical is tight.
00:14:25.404 - 00:15:31.144, Speaker B: And this is a fairly recent result by Xu, Chang Jieksu and Perkins, or Perkins and Xu. I'm sorry if I change the order inadvertently and simultaneously. Abe Lee and Sly, also in about same year, they have shown that, in fact, Alpha critical is tight. And what it means is that the probability that the solution space is non empty actually goes to one as n goes to infinity when alpha is less than alpha critical. Now, that sort of comes from the second moment analysis, but not quite the second moment analysis gives you that this probability is bounded away from zero. But if you actually want to argue the tightness of that, you have to do more work. And this is what has been done.
00:15:31.144 - 00:16:10.884, Speaker B: But something else really interesting, I think, emerged in these two pieces of work. And that has to do with the clustering picture, the familiar clustering picture that we keep discussing in context of various constraint satisfaction problems. And we can think of this also maybe as a constraint satisfaction problem. So what they have discovered is the following. So horizontal line will correspond to Alpha. You have alpha critical here. Actually, let me move it further.
00:16:10.884 - 00:17:26.254, Speaker B: Okay, so you have alpha critical here and here. That corresponds to the Unsat regime. What they have discovered, perhaps not surprisingly, that before the alpha critical, there is the solution space is split into clusters, not the entire solution space, most of the solution space. And I will explain, I'll explain. Okay, so what I have discovered is that somewhat more formally, you can find a subset sigma hat over sigma, which is almost the entire space. So the ratio of sigma hat over sigma is at least one minus exponentially small in the number of columns, such that in this subset, all solutions are in fact singletons. So that's why I draw them as little dots as opposed to circles, singletons separated by extensive differences of order n.
00:17:26.254 - 00:18:29.290, Speaker B: Okay, now that is in reference to, that is in reference to this big subset of the solution space. It admittedly almost the entire solution space, but not everything, which means that potentially, if you, if you include everything else that has been omitted in the solution space, then this is no longer true. And you can have some little like tunnels here connecting various singletons, but modular, this exception, this model exhibits an extreme form of clustering, where each cluster actually consists of a single dot. Alternatively, you can say that the number of frozen, the percentage of frozen variables is 100. This is in references of so called frozen variables in clusters. Maybe not, not something familiar to all of you, but, but if you've seen this stuff before, you know what I mean. Now here's the most, this is perhaps not surprising.
00:18:29.290 - 00:19:02.224, Speaker B: The most surprising thing about this is that that actually happens above zero. So this picture I've described holds for this picture I have described holds for every positive alpha. And you can contrast it with the picture that you see in the random caset model. And I want to do this. I want to do this comparison, and I will explain why. I think it's an interesting comparison. Okay.
00:19:02.224 - 00:20:16.252, Speaker B: But before I do that, you might ask, and we'll come back to that, whether there exists a different phase where this clustering does occur, that should be at a higher threshold value where this clustering occurs. And without exception is that the case or not, I'm asking that because that is the case for the random caset model. So first, any, any questions about what I drew here? So this picture is an implication of these two pieces of work by. Okay, now let me remind you what we know for a related similar problem of the random case set. Random case problem. Okay, so again, we have a critical alpha. I'm not going to define what random case it is, so I'm gonna, I'm kind of hoping that many of you have seen this, this before.
00:20:16.252 - 00:21:12.732, Speaker B: Again, there is a phase which is unset, and there is, but now the onset of clustering. So call it cluster, unlike in the perceptron model, happens at a positive density. So you have, which means that below that, the solution space is mostly connected. Maybe there's some exceptions here and there, but it's mostly connected, whereas here the solution space is most of it disconnected, disconnected by order and distances. So you have clusters. The clusters are no longer Singleton, so it's not 100% of variables are frozen. Most of them are frozen.
00:21:12.732 - 00:22:24.164, Speaker B: And that, again, applies to the majority of variables and majority of solution space, which means that some tunnels between the clusters might occur, but in fact, provably, there is another face here, another face. And actually, let me call this type of face as a weak clustering. And let me call this strong clustering. So strong is in reference to no exceptions. So in this phase, what do we have? In this case? We have this, clusters of solutions, but there is no tunnels between them. Probably all 100% of the solution space is clustered. What's the relevance of this? And why am I spending so much time on that? The relevance of this is the potential algorithmic implication of all of this for the case of the random k set model.
00:22:24.164 - 00:23:23.960, Speaker B: Random K set model, there is a fairly strong evidence that the algorithmic hardness occurs actually here at the onset of the weak clustering. The most recent confirmation of that is in the work of Guy Bresler and Bryce Huang of this year, who showed that pretty tightly at this threshold, many of the algorithms that we know, like low degree polynomial type algorithms, fail. Okay. And that's not very surprising. This line has been conjectured to be the onset of the algorithmic harm hardness a while ago already. So that's not a surprise. Okay, so presumably weak clustering property is the onset of hardness.
00:23:23.960 - 00:24:20.546, Speaker B: If that's the case, if we go back to the perceptron model, maybe the problem is hard for every positive density alpha. And that's the curious part of this talk. The answer is no. There exists positive alpha, for which the problem is solvable. So let me, let me share with you what we know about. Okay, so the answer is no. How do we know that? First, a while ago in the heuristic study, the empirical study by Braunstein and Zekina, and this is all six they did experiments.
00:24:20.546 - 00:25:48.034, Speaker B: And these experiments revealed that it seems, at least experimentally, that the perceptron model is solvable at some small but positive density, even though we know now that it exhibits a clustering. Okay, but then later this was confirmed, and the paper that the most relevant for us today that I will spend more time on is by Bansal and Spencer. Spencer appeared in 20, posted a little early and the Banzallen page Spencer work shows that indeed there exists a positive alpha for which the symmetric. So this is in reference to the symmetric model. So we have kappa here that for each, the critical kappa has to be at least as large as something. And you can show that this something, as size of the interval goes to zero, is actually quadratic in kappa squared. Quadratic in kappa.
00:25:48.034 - 00:26:52.744, Speaker B: Okay, so it's a positive result. They construct an algorithm, very clever one, that shows that indeed symmetric perceptron model is solvable at some positive density. The critical value here, therefore, for the symmetric perceptron model is at least order kappa squared. So that at the very least, I guess, suggests that showing this picture that we have here as an evidence of the algorithmic hardness, in other words, the onset of weak clustering as evidence of algorithmic hardness, that flies in the face of this picture where we have the weak clustering occurring at every positive density. Yet there are algorithms to solve this. So let me pause here before I sort of recollect my thought. Let me pause here and ask if you have any questions.
00:26:52.744 - 00:28:14.678, Speaker B: Okay, so, all right, this is not an exhaustive list of the positive results on the symmetric perceptron model. In fact, there is an earlier work by Loewett and Mecca from 15. They also construct an algorithm that solves the symmetric binary perceptron model. But their analysis seems to be applying only in the case where the number of rows is bigger than number of columns, and that cannot. Therefore, from that it's hard to figure out what's the implication when kappa goes to zero? Because if kappa goes to zero, that the only critical alpha goes to zero as well. That means that you have to work with way fewer rows than columns. And that kind of makes sense if you want all your answers to be in a tiny little intervals.
00:28:14.678 - 00:29:01.616, Speaker B: You cannot afford to have too many rows in your matrix. So let's summarize. So what do we have? First, we have a critical alpha. So this is, again, in symmetric binary perceptron model, we have a critical alpha c, which looks like this. It's log two divided by the log of the probability that standard gaussian is at most kappa. So we're going to call it alpha c of kappa. As kappa goes to zero, this becomes approximately log two divided by the log kappa absolute value.
00:29:01.616 - 00:29:56.574, Speaker B: So let's keep this scaling in place. Okay? So this is as far as the existential result is concerned. On the other hand, as I said by Bansal and Spencer, the implication is that when, as long as alpha is less than or equal to something of order kappa squared, the solution is non empty with high probability. So there is still a big gap between kappa squared and log two and something which is here, which is order one over log kappa absolute value. So the positive results exist up to here. Existentially, we have it up to here. So we have a gap.
00:29:56.574 - 00:30:59.894, Speaker B: So, clustering aside, we have a gap. And can we explain this gap some way? And that's what I want to report. We're going to take, surprise, surprise. We're going to take the approach of the overlap gap property, which I will explain shortly. As a side remark, there is a reading group now, ongoing assignments by Bryce and Aaron on this. So if you wanted to know more about this, I would refer you to this reading group. So we want to take the overlap gap property or another, or put it more generally, the different landscape approach to try to understand the true algorithmic hardness of this problem.
00:30:59.894 - 00:31:25.094, Speaker B: So here's what we know. Let me state it in a theorem. This is going to be likely the only theorem that I'm going to formally state. It's going to be long. I bear with me, please. And. But then I'll state the theorem formally.
00:31:25.094 - 00:32:15.618, Speaker B: I hope it's correctly stated. Aaron will correct me if something is not accurate, and then I'll give the intuition. So here we take the kind of full sort of power of the OGP approach. The theorem is stating that a model exhibits the so called symmetric ensemble multi OGP. Probably very little of this makes sense at this stage, but I'll explain. Okay, so recall, we're dealing with the symmetric model. So we have parameter Kappa.
00:32:15.618 - 00:33:07.694, Speaker B: So the claim is that for every small enough kappa, there exists a critical number, let me call it known creatively as alpha o gp of kappa. It's going to be certainly less than critical value for the existence of the solution. I'll say much more about that. How much below it is such that the following is true. However, if it takes alpha slightly larger than the critical value. Ah, I apologize. I have to, I have to do the digression into.
00:33:07.694 - 00:34:16.914, Speaker B: Okay, sorry, sorry, I have to, I have to first introduce the ensembler version of the model. So I gonna, I guess I'll come back to the theorem and introduce the ensemble model. I will do it only for the case of the standard normals. So every entry of the matrix is standard normal. So here's what we're going to do. Let's create another matrix hat which is independent from the original matrix a, and let's create an interpolated family of models a of t, which is defined as follows. It's cosine of t times a plus sine of t times a hat.
00:34:16.914 - 00:35:08.830, Speaker B: Okay, so in other words, we have a one matrix, we have another matrix, this is a, this is a hat. And now we create this whole family of matrices a of t in between. And one thing to observe is that entries of each of such matrix are against standard normal because the variances match. Okay, so that's what we have. So we have this interpolated family of models, and the theorem is in reference to this interpolated family. What's the significance? And so on. That's to be discussed, but let me now state the theorem.
00:35:08.830 - 00:37:00.594, Speaker B: So for every alpha larger than this claimed value, there exists a large enough constant, integer constant m such that I'm reaching the limit of the number of quantifiers that I've ever had to use before. And more quantifiers are coming. There exists a number m such that for every sequence of numbers starting from zero up to PI over two, y PI over two, because as t is equal to zero in this interpolated family, we have the first matrix, and when t is equal to PI over two, we have the new freshly independent matrix. So this interval goes as t spends from zero to PI over two. Okay. All right. So if you fix any time instances in this interval, so there exists an m such that I made a mistake for every sequence of such numbers and for every sequence of solutions, sigma one in instance associated with time one, sigma two in instance, satisfying assignment with respect to the instance at time two, and so on, sigma m associated with the satisfying assignment at time tm that exists.
00:37:00.594 - 00:38:19.574, Speaker B: Ok. And. All right, something else is missing here that exists m and two numbers nu one mu two less than one, such that anytime you have this. It is also the case that there exists two numbers k n l between one nm, such that the inner product of sigma k and sigma l, normalized by n, belongs to either the interval from zero to mu one or interval from new. To write it cleanly, mu two to one. Okay, now, if you were able to parse what I have here, upon seeing something like this for the first time, I would be hugely impressed, because this is probably nearly impossible to say what this even says, let alone understand its significance or implication. So now let me try to.
00:38:19.574 - 00:38:20.414, Speaker B: Yes.
00:38:20.574 - 00:38:22.558, Speaker A: Where's the probabilistic quantifier?
00:38:22.606 - 00:38:32.514, Speaker B: I mean, where do I. Oh, good, good. I miss that there exists, there exists so that that existence is with high probability.
00:38:33.414 - 00:38:46.760, Speaker A: But do I pick the keys before picking the endpoints? I think the high probability, high probability.
00:38:46.912 - 00:39:06.800, Speaker B: Is with respect to the randomness of the a and independent copy of a. Once you fix a and fix a copy of a, the interpolated family is deterministic, is conditionally deterministic, and that's in reference to this random. That's the only randomness, but they happen again.
00:39:06.872 - 00:39:14.134, Speaker A: So this prohibited v one, v two is chosen before choosing.
00:39:15.594 - 00:39:35.770, Speaker B: Ah, okay, good. Thanks for asking. I think it nazi right if I'm not, if I'm here. Okay, so the, all of these parameters are deterministic. New one is this. All of these parameters, alpha, alpha, OGP, new one and new two and m. These are all mixed.
00:39:35.770 - 00:39:47.194, Speaker B: Nu one, nu two are all deterministic. They're fixed up before deterministic, meaning only kappa dependent, but nothing else dependent.
00:39:53.214 - 00:39:56.474, Speaker A: K and l, are they also deterministic or k and l?
00:39:58.934 - 00:40:25.044, Speaker B: No, they're not. They're not. Okay, now, they might depend on how you actually drawn such solutions. Sigma. So you fix any collection of times. Let me come back to that. Maybe when I try to give some intuition, if that's okay, and maybe that will clarify also what is random and non random here.
00:40:25.044 - 00:41:30.584, Speaker B: Okay, so what is, what is happening here? Firstly, as a warm up, as a warm up, let's consider the case where m is equal to two and we just pick two identical times t zero is equal to t one. Did I start with t zero or t one? Doesn't probably matter. T one. Okay, so let's suppose we just look at the special case when two times are equal to zero. In other words, we're just dealing with a single instance case. So the implication of that, that is very simple, as long as you take two satisfying solutions. In the symmetric perceptron model, it should be the case that they're in a product normalized by nice is either in zero from nu one or nu, two to one, in other words, exhibits again, and geometrically, that's very simple.
00:41:30.584 - 00:42:12.060, Speaker B: That basically says that. Okay, so pairwise distances. For every two solutions, pairwise distances have to be either small or large. So pairwise distances, small quote unquote or large, quote unquote. Okay, so that was easy, but I realize it doesn't quite answer the question that Nettie has raised. So let me now come back to the theorem, and now in a pictorial way. So here's what we do, sort of.
00:42:12.060 - 00:42:51.900, Speaker B: Here's the sort of dynamically, sort of what happens. We draw a copy of a, that's a random copy. We draw another copy of the same randomness, a hat, which is independent. We construct a sequence, continuous sequence of models, so parameterized by time, continuous time, going from zero to PI over two. So each of these corresponds to a of t. Fine. Okay, now, right.
00:42:51.900 - 00:43:35.124, Speaker B: So then we fix, fix any numbers, t one and so on, tm. So in other words, we fix collection of instances here, maybe this one, this one, and this one. Any m instances in this continuous family, we pick. For each instance, we pick any solution. So, sigma one is in here, and so on. Sigma M is in satisfying solution. With respect to the m three instance.
00:43:35.124 - 00:44:07.088, Speaker B: Those could be picked arbitrarily as long as they are in a satisfying, in a satisfying set. So those are picked. So we picked a vector here associated with this instance, the vector associated with this instance, the vector associated with the instance. So we have the whole bunch of vectors in the binary binary cube, one vector sigma one, sigma two, sigma three, and so on. Sigma m. We look at the pairwise distances, all m. Choose two of them.
00:44:07.088 - 00:44:28.284, Speaker B: And all we're saying is that there should be two instances, just two of them, for which the distance or the overlap is not in the intermediate interval from mu one and u two. Nancy, is this good enough? I just.
00:44:28.324 - 00:44:29.228, Speaker A: Can you mind this?
00:44:29.356 - 00:44:29.636, Speaker B: Sorry.
00:44:29.660 - 00:44:39.984, Speaker A: At least me is mu one, mu two. The period is dependent. M. Or is it? Can you show the theorem again, please? Just to all the quantifiers. You scroll back to all the quantifiers.
00:44:41.724 - 00:44:54.880, Speaker B: Yes. Right. Yeah. Exists m and u one and u two. So both m and u one and u two depend on kappa. Okay. All right.
00:44:54.880 - 00:45:41.706, Speaker B: So I know I'm running out of time, but let me tell you the punchline of all of this. Well, there's actually two punchlines. One is not very surprising to many people in the audience. One is perhaps surprising. So, first is that why would we care about structure like that? Because using the structure, we can rule out classes of algorithms, and that holds here as well. So the presence of this multi symmetric multi ensemble OGP rules out stable algorithms. Now, one, there's two algorithms.
00:45:41.706 - 00:46:16.654, Speaker B: One algorithm I didn't mention earlier is it's for the, admittedly for the asymmetric model. Kim and Rusch, we can prove that this algorithm is stable, although it applies to asymmetric model, not symmetric model. So its implications not entirely clear. Bansal and spencer algorithm appears to be symmetric. This is the work in progress, but we believe we have an idea to show that in fact it's symmetric. But the implication is there. So once, but the multi presence of the structure can be used to rule out algorithms.
00:46:16.654 - 00:47:36.718, Speaker B: This is not very surprising. Perhaps the novel part and interesting part is that if you work out the critical kappa, sorry. So here we have critical if you work out for which Kappa for each kappa, for small enough kappa, where is the onset, the overlap gap property that turns out to be kappa two minus little o one. And if you contrast it with this Banzall and Spencer algorithm, that works up to Kappa squared. So in this landscape, in this one dimensional parameter, remember the critical is something of the order, one of the log kappa, the algorithm exists. So the algorithm exists by Banzal and Spencer when kappa is, when alpha is of the order kappa squared. But pretty much immediately after that, we have the OGP regime, indicating that likely the problem is hard.
00:47:36.718 - 00:48:28.862, Speaker B: So the OGP picture here provides a fairly tight, fairly tight characterization tied with respect to the known algorithmic threshold. Okay, I think I'm over my time, so perhaps I should stop here and take questions. Symmetric multi ensemble OGP. So this characterizes the difference between weak clustering and the strong crust costume. Good, good. So neither. So let me actually, you remind.
00:48:28.862 - 00:49:22.644, Speaker B: Thank you. I think it was Niklos. Thank you for reminding me that. So the multi OGP picture by itself is not, this is not, I don't know how to describe it in terms of the clustering property of a kind. However, if you look at the two OGP, so in the case where at least two pairwise solutions are in a product is disconnected, the onset of the two OGP implies, implies strong implies strong clustering. And what it means is that in this context, remember I posted this as a question mark, there is no question mark. So we know there exists a strong clustering property.
00:49:22.644 - 00:50:25.146, Speaker B: Clustering property for the symmetric binary perceptron model, it does happen, but it happens not at the algorithmic threshold. It happens much at a larger value, at least for computation at the values larger than Kappa squared. We don't know exactly how it scales with Kappa, but it doesn't get all the way to the algorithmic threshold. And the algorithm threshold is somewhere here, which is again, it's hard to connect it with the clustering picture per se. Thanks. So the two OGP and this MOGP would be different thresholds? Oh yes, yeah, that's the right. Sorry for not clarifying that the reason we needed to go, and this is historically, let me actually mention several pieces of work I meant to mention in reference to the multi OGP picture.
00:50:25.146 - 00:51:25.952, Speaker B: One is by Alex Wang in the context of independent set. I mentioned already Bresler and Huang in context of the caset. And there is a more recent work which I guess will be posted soon by Bryce and Mark Selk in the context of PsPin. In all of these papers, the multi OGP structure is introduced and proven in order to match the algorithmic threshold. And multi OGP does occur at the threshold lower than the two OGP. Great, thank you, great talk. Can you give some intuition for what Benzahl Spencer is doing and like maybe.
00:51:26.008 - 00:51:27.924, Speaker C: How it's navigating the.
00:51:29.864 - 00:52:13.270, Speaker B: Good, right. So not going to have time to go into details of the algorithm, but let me describe it in high level terms. But you also remind me something else I wanted to mention. It is noteworthy that both Kim and Roche algorithms that I barely mentioned here, but also bansal and Spencer algorithms, they are not instantiations of the low degree polynomials. Otherwise our work will be easier because we already know how to rule out low degree polynomials. They are not. So we have to conduct an independent stability analysis of that.
00:52:13.270 - 00:53:18.834, Speaker B: So Banzallen Spencer algorithm works like this. So you have a matrix, this is matrix a. So you have columns, you essentially set up values. So for each associate with each column, you need to set up the value of the sigma, which should be plus minus one. Essentially, you do it in a dynamic way from the left to the right, where you sequentially set each of the values of the sigma based on the values of what you've seen up to that point. So it's effectively one sweep online algorithm and this value sigma is obtained. Well, there's some potential function analysis, but basically what you do is that you look at the values corresponding to the sums across previous t minus one columns, and then you set your value plus minus one, depending based on this sign of the sum of those values.
00:53:18.834 - 00:53:40.734, Speaker B: Not a very description of the algorithm, I realize, but this is basically what's going on. It's a one sweep algorithm setting the values of sigma one by one, depending what you've seen before. Thanks.
00:53:44.474 - 00:53:49.334, Speaker C: All right, for further questions, you can get in touch with David later on, but let's thank him again.
00:53:50.364 - 00:53:51.304, Speaker B: Thank you.
00:53:54.284 - 00:53:57.404, Speaker C: Be back at 15.
