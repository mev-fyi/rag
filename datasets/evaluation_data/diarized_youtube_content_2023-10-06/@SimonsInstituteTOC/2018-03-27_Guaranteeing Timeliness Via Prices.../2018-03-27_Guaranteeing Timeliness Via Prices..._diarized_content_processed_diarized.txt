00:00:00.160 - 00:00:33.934, Speaker A: PR Kumar, guaranteeing time limitations. Okay, thank you for coming. So I'm going to talk about the work done by Rahul Singh. And he's somewhere in this Bay area, I don't exactly know where right now. So the. So about a year and a half ago, the FCC released about 10.85 spectrum in the millimeter wave band.
00:00:33.934 - 00:01:06.648, Speaker A: 3.85 GHz, licensed 7 GHz, unlicensed. This is not megahertz, this is Gigahertz. And actually, the fact that the FCC released unlicensed, the way I interpret it is that it's a very liberal move by the FCC. You can think of it this way. The spectrum has traditionally been allocated by command and control. But in the case of Wi Fi, what happened is they were talking about the 2.4
00:01:06.648 - 00:01:33.854, Speaker A: GHz band, which is the same frequency as the microwave oven. So that's called a junk band. So FCC said, guys, do what you want. And that's where the revolution happened. So in fact, that 2.4 GHz band is more utilized than anything else. So I like to think of it as FCC learning that you can create a lot of innovation and economic activity through innovative users in an unlicensed manner.
00:01:33.854 - 00:02:20.394, Speaker A: And in fact, they're proposing to release another 18 GHz later, though I don't know what will happen in the ajit pai era. Okay, so it could be that we may be coming to some kind of an information age where there's a lot of information flowing around us, but there are challenges. This is in the millimeter wave, and these transmissions are directional. So they're not omnidirectional. They're kind of focused. There is absorption. And the question is, the particular question I'm going to look at is how can we support low latency applications in this technology? And you can think of application, any interactive application, maybe augmented reality or whatever.
00:02:20.394 - 00:02:56.934, Speaker A: Okay, so the problem I'm going to consider is multi hop networks. So we have flows. So this is a green flow from one origin to destination. There's a blue flow, multipath routing is possible, and there's red flow and so on. And we'll suppose there are capital f flows and each flow has a hard end to end deadline. So the little f flow has an end to end deadline, tau f. What I mean by a hard end to end deadline is that I will only give credit for packets which are received within that deadline.
00:02:56.934 - 00:03:34.174, Speaker A: So I'm looking at the rate of delivery of packets which meet the deadline. And so that I'm going to denote by RF. And this RF is like throughput, but it is throughput, meeting hard deadlines. So I call it the timely throughput. And what about the nodes themselves? So we suppose that every node has a average power constraint. This is traditional in information theory. So on an average, a node can transmit at one watt, so occasionally two watts, occasionally half watt, but the average is one watt.
00:03:34.174 - 00:04:09.302, Speaker A: Now, I'm going to logicalize this average power constraint into a concurrency constraint on the number of packets. So just suppose that each packet transmission takes one watt. So I will suppose that the average number of packets that Nodai can have concurrently under transmission is CI. This is just for convenience. This is the long term average. And packet, these links. Now, this is not omni directional transmission, so there is no collisions as such, but links are unreliable.
00:04:09.302 - 00:04:51.916, Speaker A: So if node I sends a packet to node j, that may or may not get there, and p I j is the probability of a successful transmission. And as I said, there is no interference. Now, we want to schedule this network optimally. And there are two challenges. The first challenge is a challenge of information, and the second challenge is one of computation. So the information challenge is how to instantaneously obtain all the information needed to schedule the network optimally. And the second challenge is, even if you could get that information, what is the complexity of determining the optimal scheduling policy? So, let me illustrate what the information challenge is.
00:04:51.916 - 00:05:25.338, Speaker A: Well, let us suppose that you have a node here, and let us suppose that it has this dilemma. It either needs to transmit a packet, a blue packet, or a green packet. And let us suppose that if it transmits the blue packet, unfortunately there is downstream congestion. So this packet is going to get stuck in the downstream traffic and probably won't make it to its destination on time. So you don't get the credit for receiving the packet. On the other hand, the green packet, there is no congestion, so it's better to transmit it. So this shows that knowing downstream congestion is important in scheduling.
00:05:25.338 - 00:06:05.084, Speaker A: By a similar token, knowing upstream congestion is also important because maybe this particular packet is already pretty delayed. There is another packet coming, let us say. So I should wait for that and not use my power on this packet, which is already delayed. So the point is that in order to schedule a network optimally, you have to know the complete state of the network and. But how? But you need this instantaneous knowledge of network state. But the whole point of this communication is to schedule the network so that packets go from place a to place b with low latencies. So we have a chicken and egg problem.
00:06:05.084 - 00:06:26.952, Speaker A: And in fact, this is one of the fundamental reasons why optimal scheduling of distributed systems is difficult. Okay. All right, so that's the information problem. Then there's a computation problem. Let us say that this information could be given to you right away. Okay. The question is, how do you compute the optimum policy? So, and that the problem is that the network state is pretty huge.
00:06:26.952 - 00:06:57.074, Speaker A: If there are v vertices or nodes, if the maximum delay is delta, if there are capital f flows, then the state space, the network state space is huge. There are f flows. Each flow could be, could have up to delta packets. So there are up to f delta packets in the network. And then each packet can belong to one of flow flows and have a certain time to live, et cetera. So you do the calculation and it's exponential. And now dynamic programming with this exponential state space is intractable.
00:06:57.074 - 00:07:43.034, Speaker A: You can reduce dynamic programming to linear programming by what are called state action probabilities. So you simultaneously look at the probability of being in a certain state and taking a certain action. And with those as decision variables, you can formulate dynamic programming as a linear programming problem. Unfortunately, that's a huge number. So even if you had the information optimally, scheduling this is difficult. So question how do we address this? And what I'm going to give you is a fully decentralized and tractable and optimal solution. Okay? So let rf, as I indicated earlier, denote the throughput of packets of flow f that meet this hard end to end deadline constraint tau f.
00:07:43.034 - 00:08:10.244, Speaker A: And that's the timely throughput. Now what I'm going to do is we have to look at the feasible region right of delivery rate vectors. I'm going to examine one point on the Pareto frontier. So weight flow f's throughput by alpha f. And look at the problem of maximizing this weighted timely throughput. So there are capital f flows. R f is the throughput of flow f, and this is the weight.
00:08:10.244 - 00:08:50.374, Speaker A: And the problem I'm going to examine is how to schedule the network for maximizing this weighted timely throughput. And this slide is the heart of the whole idea. So we're going to look at this as a Markov decision process with constraint. So we know that Markov for Markov decision problems, there is an optimal in the class of stationary randomized strategies. These are mappings from states to probability distributions and actions. Okay? And this is what we want to maximize. I'm writing it as a long term average for each flow f.
00:08:50.374 - 00:09:30.884, Speaker A: When you deliver a packet, you get alpha f dollars. And this is the event that the packet is delivered, that the packet is delivered at time t within its deadline. So this is the reward that you collect and there is a power constraint or a concurrency constraint. So this is the average number of packets under transmission at node I is bounded by Ci. So this is a constrained Markov decision process. Now we're going to augment the constraint through a Lagrange multiplier. So we have a Lagrange multiplier for each power constraint, we adjoin that and we get this lagrangian.
00:09:30.884 - 00:10:29.478, Speaker A: Now what we are going to do is we are going to rearrange this lagrangian. The way it is written is the reward accrued at each time instant, depending on whether the packet was delivered or power was consumed averaged over time. But we'll reorganize this through via packets. And it turns out that it can be completely decoupled on a packet by packet basis. So the way I look at it is I look for each flow, I look at the flows, the packets of that flow that were released before a certain time and average over time. Now how does a packet experience a reward or cost? If a packet reaches its destination on time that is within its end to end deadline, then it collects alpha f dollars. On the other hand, every time it requests transmission by node I, it pays lambda I dollars for the cost of energy, right? So this is the reward or cost by the packet, and you can average over the packets to get the same reward.
00:10:29.478 - 00:11:02.610, Speaker A: But now this problem completely decouples and we have a decoupled packet level decision making. So what is that? So this is what a packet is confronted with. It gets a reward if it reaches a destination on time, but has to pay money a toll every time it gets transmitted. So the problem looks as follows. So let us say there is a packet and this packet can completely forget about the complete network state. It doesn't have to worry about the network state. It says to itself, I am in state I and I have tau days to live.
00:11:02.610 - 00:11:45.550, Speaker A: So this is the state of the packet where it is and how long before its deadline expires. And it is saying if I request to be transmitted over this link to node j, then there's a probability pij that I will make it. And the question is, should I pay the toll lambda that this node is asking? And if it reaches the destination d in time, then it collects alpha of dollars. So this is what a packet needs to optimize for itself. So it's a packet level decision making problem, completely oblivious to a network state. And that's a very simple dynamic programming problem. First of all, the packet state is very small it is not exponentially large, it is just the number of nodes and the maximum deadline.
00:11:45.550 - 00:12:16.792, Speaker A: And this is dynamic programming. If it reaches its destination with time to spare, time to live is non negative. Then it collects alpha of dollars. Whereas if it is in no die with tau days to live, it can either not get transmitted or get transmitted. If it doesn't get transmitted, then it stays at the same node one less day to live. If it does get transmitted, it pays lambda dollars with probability pij. It will go to node j with one less day to live, or if it doesn't, it will stay where it is.
00:12:16.792 - 00:13:14.576, Speaker A: So it's a simple dynamic programming problem that could be solved offline and stored and easy to solve because of the very small state space. Now there are some issues, we'll get to that. Now, so this optimal solution completely decouples. So each packet makes its own decision to be transmitted or not, depending only on its own state, and it does not depend on the state of other nodes, doesn't depend on the state of other flows or even of other packets within its flow. Now technicalities now let us suppose lambda star is a price vector, and this is a optimal randomized policy for that price vector. And supposing at every node, either the power constraint is satisfied with equality or the cost of power is zero. That means either the constraint is tight or it is loose, in which case price is zero.
00:13:14.576 - 00:13:37.234, Speaker A: Then complementary slackness is satisfied and this is optimal. So technically there is no problem. There is a linear problem. But there is one small issue. There is a need for randomization. Whenever you have a Markov decision problem with average cost constraint, there may be a need to randomize just to use up all the resource completely. So the easy way to.
00:13:37.234 - 00:14:12.084, Speaker A: So supposing this constraint is satisfied with equality. And just consider the following example. Supposing you transmit all the packets at a node, you use ten watts, and supposing you do not transmit any packets at the node, it is zero watts. And supposing both these decisions are optimal for the given price. But supposing your power constraint is eight watts, then you need to randomize and transmit 80% of the packets to use of the resource. So there is a randomization, and there is this additional decision on which flow you should randomize, and so on. So there are these things which couple the flows.
00:14:12.084 - 00:14:54.352, Speaker A: So we do need to determine this optimal randomization probabilities also. Now what about prices? Well, one way to, one way to think about prices is a straightforward way. If the price of energy is too low, then too many packets will ask to be transmitted and so the average power constraint will be exceeded. On the other hand, if the price is set too high, then too few packets will have to be transmitted and the power won't be used up. And that's not good either. So one possible solution is just adjustment. So you look at the empirical running power, and if you're using too little power, jack it up and so on, ok? And you can think of this as a subgradient algorithm for the dual, ok? But this is not the way we're going to do it.
00:14:54.352 - 00:15:28.744, Speaker A: This is just to give you the intuition. So I'll give you one package tractable packet that solves all these issues. We're not going to go via prices. We'll directly solve the primal. So how can we exploit all the structure to determine the complete solution? Now the point to note is that we do not need to decide how, we only need to decide how to optimally schedule each packet. We do not need to decide how to schedule the entire network. So it's only packet level decision making.
00:15:28.744 - 00:16:22.298, Speaker A: Now, there are this many types of packets, there are capital f flows. Each packet can be in one of these states and have a certain time to live. So this is the number of packet types, okay? And for each packet type we need to make a decision which link to send it onto or to which node and at what power level. And so the number of state action pairs for a packet level decision making is small, okay? And so we can write down a linear program with these many variables, and that will be highly tractable and it will give the optimal solution. So here's how it looks. So we combine all these flows now into one big pot, and here's a linear program. So this is the reward function that you want to maximize, and these are the state action probabilities.
00:16:22.298 - 00:17:04.754, Speaker A: So for flow f, which is a node I and which has s days to live, if you, and if you transmit it to the final destination, then it will make, it will arrive there with this probability, I am sorry, it will get transmitted with this probability, and then it will get arrived with this probability. And then you collect alpha f dollars. And af is, by the way, the arrival rate of packets. So this is the accrued reward. This is the power constraint. At each node you can look at the average power, and these are the balance equations, that is the mass conservation of mass packets in equals packet out. And this is just the fact that these are all probabilities.
00:17:04.754 - 00:17:35.386, Speaker A: So this is a very tractable linear program, very low complexity, just this many variables. And this many constraints. And in addition, it is huge reduction from the exponential complexity. And in addition, it is linear program, which is very easy to solve. So that is a complete solution, both informationally and computationally. Now, I've solved here the average cost problem. That is average power constraint at each node.
00:17:35.386 - 00:18:08.568, Speaker A: Now what? Supposing you have peak power constraints. So that means that this node, I cannot ever transmit at more than ten watts. So it can never transmit more than, say, ten packets. It's not average concurrency, it's maximum. In that case, you can convert this into a kind of a banded problem through Whittle's relaxation. So suppose nodai can transmit only CI packets concurrently. Then Whittle has shown that when you have a multi arm banded problem, we know that if there is just one arm, there is this beautiful gittens index.
00:18:08.568 - 00:18:50.364, Speaker A: But if there are multiple arms, there is no index. But if you relax the number of arms that you can pull at any given time to an average constraint on the number of arms you can pull, then under some conditions, it is indexable. So that's what we do. We simply truncate, we relax this peak to an average, we get an optimal policy, and then we truncate that optimal policy so that it does no more than CI pulls of the arm. And this is exactly similar to truncation of vittles policy. And that is asymptotically optimal. As the network is scaled up everything in proportion, so it is quantifiably near optimal.
00:18:50.364 - 00:19:40.730, Speaker A: So we can say something about how to do that problem also. And let me just make a few comments about how this compares with what has been done. So, there have been many important developments in scheduling networks of the past 25 years. And one important, very important development is max weight and back pressure policies. This goes back to Leandro Stasiulus and Tony Ephraimides, and then later on by a whole bunch of other people. And now these back pressure policies are based on a Lagrangian decomposition of the fluid model. And the fluid model is appropriate for studying throughput.
00:19:40.730 - 00:20:09.082, Speaker A: In fact, there are things, theorems which say that the fluid model is stable, then it's throughput stable, and so on. So the fluid model is appropriate for studying throughput, but. And it has been very successful at designing throughput optimal policies. But delay depends on stochastic variations. It needs a stochastic model. You need to consider second moments and things like that. And this is like the difference between the law of large numbers and the central limit theorem.
00:20:09.082 - 00:21:04.106, Speaker A: Okay, so what we are doing here is not a relaxation of the fluid model, but a relaxation of the complete Markov decision process that captures all the stochasticity in the system. And these resulting Lagrange multipliers are very different. So for the fluid model, for back pressure, the Lagrange multipliers are the differences in Q lengths, the back pressure. But in our case, the Lagrange multiplier is the price of energy. So quite, quite different. Now, this theory can be used to explicitly solve examples, just to give you an illustration, supposing you have, so you can put in textbooks, so supposing you have a red flow this way, a blue flow this way, and these are the reliabilities of the links, and these are the weightings, I should have called them alpha. Then you can calculate the optimum price is just hand calculation, and you can determine the optimal solution.
00:21:04.106 - 00:21:47.414, Speaker A: That is, at node one, you should transmit with probability 0.5 if the time deadline is three, else drop at flow for flow one, flow flow two, at node three you should transmit with this probability, and so on. So explicit optimal solution, you can also do numerical comparisons, which I mean it is optimal, so it does better. So I have addressed here the problem without contention. We can say some things for contention, not as complete and as thorough as this, but there are some things we can say, and there are lots of other issues that you can examine, including things like coding and so on and so forth.
00:21:57.054 - 00:22:08.126, Speaker B: One thing is instead of fixing the time bounds, you use the same thing to try and find the optimal delay policy by operating over what you define the timeline.
00:22:08.270 - 00:22:10.158, Speaker A: You mean by optimal? Optimal for what?
00:22:10.286 - 00:22:19.274, Speaker B: Now I want to actually look at the delay rather than looking at timely through 40. Could I iterate over what is the timeliness bound that I have for each bucket?
00:22:24.874 - 00:22:30.254, Speaker A: What do you want to maximize delay within which packets? Minimize mean delay.
00:22:31.194 - 00:22:32.094, Speaker B: Minimize.
00:22:34.474 - 00:22:36.254, Speaker A: Minimize mean delay.
00:22:41.194 - 00:22:47.994, Speaker B: Keep adaptively shrinking with threshold from which flow. Where do you think that would break down?
00:22:48.814 - 00:23:37.116, Speaker A: It may not be optimal, see, it may not be optimal to regulate all packets with the same delay in order to reduce mean. You may, in that case, you may want to take advantage of network conditions or and so on. Yeah, so that one here, if your transmission property is highly reliable, close to one, is there some simplification as to what the arch is or not? Then it becomes like a max flow min cut thing. Because if I give you the concurrency constraint, that is like a cut capacity, and so it may reduce to a deterministic problem. Max flow. Yeah. You think it's close to one anyway? Yeah.
00:23:37.116 - 00:24:05.614, Speaker A: So just think of it as one. It is like fluid model with the fluid pipes bounds. And so then it is just a deterministic linear problem, let us say. Yeah. Can you mix delay sensitive flows and non delay sensitive flows? Yes. Because the thing is that I can have different deadlines for different flows in this formulation. So you can just think of that as infinity.
00:24:05.614 - 00:24:15.514, Speaker A: So that is not a problem. Yes. Yes. So you can have, yeah. You can have some flows which have no delays. Some have delays, different delays, and still optimal. Yes.
00:24:15.514 - 00:24:40.064, Speaker A: Didn't the DP scale with the maximum delay? In principle it would, yes, in principle it would. But I suspect that you can get pretty close to near optimal just because after some number of Bernoulli trials, it is not worth it. Yeah, should be.
00:24:44.244 - 00:24:44.636, Speaker B: Okay.
00:24:44.660 - 00:24:45.324, Speaker A: Thank you. Thank you.
