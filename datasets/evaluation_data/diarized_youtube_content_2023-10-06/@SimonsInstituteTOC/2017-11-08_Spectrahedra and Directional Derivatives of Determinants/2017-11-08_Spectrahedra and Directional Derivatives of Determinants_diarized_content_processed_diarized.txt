00:00:06.320 - 00:01:00.152, Speaker A: Okay, so now we switch gears a bit from stable polynomials to hyperbolic polynomials, and our speaker will be James Saunderson from Monash University. Okay, thanks very much, and thank you very much for the organizers for inviting me to give this talk. So, yep, today my talk is called spectrahedra and directional derivatives of determinants. All right, so what's the narrow minded view of what this talk is about? Is that I'm going to be looking at a very particular multivariate polynomial. This is the thing you get by differentiating determinants in the identity direction, some kind of polynomial of degree n minus one in the symmetric matrix. And we're going to see that associated with this is a very nice convex cone. And the main question going to answer in this talk is whether or not we can express this convex cone as the feasible region of a semi definite program.
00:01:00.152 - 00:01:36.216, Speaker A: Okay. As a spectra here. That sounds like a very kind of narrow and precise question. Um, so I'm going to spend much of the talk in kind of explaining where this question comes from and connecting it to much broader questions. Uh, and these broader questions are about the relationship between certain or conjectured relationships between families of multivariate polynomials called hyperbolic polynomials, very closely related to stable polynomials. And these are multivariate polynomials with certain real rootedness type conditions. Associated with these are nice geometric, in fact, convex cones, called hyperbolicity cones.
00:01:36.216 - 00:02:16.794, Speaker A: And there's close relationships and conjectured questions about how hypervelicity cones relate to the feasible regions of semi definite programming. So there's going to be this background, and we'll see that somehow this is a question on the boundary of what we know about these relationships. So, to get started, let me tell you what hyperbolic polynomials are. I mean, Mohit almost very briefly said this, let's go a bit more slowly. So, a hyperbolic polynomial is a polynomial that's homogeneous of some degree d in n variables. And so it's a polynomial together with a direction. And we say that p is hyperbolic with respect to e.
00:02:16.794 - 00:02:47.884, Speaker A: If p does not vanish when I plug in e. So number one and number two. And this is kind of maybe the more exciting part of it is that if I look at my polynomial, I start at some point x in rn, and I look along the direction e. So this is a univariate polynomial, okay, along the direction e. That univariate polynomial has all real roots. Okay? So the number of real roots counted with multiplicity equals the degree of this polynomial, p. So, just to give you some concrete example and a non example, just to clarify things.
00:02:47.884 - 00:03:11.114, Speaker A: So imagine I take this polynomial in three variables, and I consider my direction to be up this direction. Up. If I start anywhere in this picture, maybe here, and I move down in the picture, I'll intersect this hypersurface where the polynomial vanishes exactly twice. Okay. It's a degree two polynomial. So things are good. This is a hyperbolic polynomial with respect to this direction.
00:03:11.114 - 00:03:39.094, Speaker A: A non example is if I take this quartic polynomial. So you should think of this as, like, the l four norm ball in the cone over it. And if I start somewhere in this picture and I go in this direction, I mean, any direction but in this particular direction, say I'll meet the hypersurface, this real hypersurface, twice. This is a degree four polynomial. There's two complex roots hiding somewhere. Okay, yeah. It might be hyperbolic with respect to a vector on the boundary of this code.
00:03:39.094 - 00:04:00.770, Speaker A: No. So, because I have this condition, right, exactly where, I mean, this is the surface where this is the point where it vanishes. So, okay, so the answer is no. And this is actually seems like a technical condition, but actually, you need this condition. This is important. So there's no direction. This thing is hyperbolic.
00:04:00.770 - 00:04:38.802, Speaker A: That's the question claim yet. All right, so here, there's some examples. Um, and, okay, what's. Why are these things interesting from the point of view of convex optimization? That's the route by which I come to these things, is that, um, we can define a very natural convex cone associated with these polynomials. And that is to take all the points in my space rn for which all the roots of this univariate polynomial, think of this like the characteristic polynomial of a symmetric matrix. And it's an example we'll see next are non negative. Okay, so somehow, these are somehow like the eigenvalues of this point x.
00:04:38.802 - 00:05:28.154, Speaker A: And I ask for the set of things which all these eigenvalues are non negative. Here's a very nice and quite an old result now that an amazing result is that, in fact, this real rootedness properties and so on on p. This hyperbolicity condition means that this cone is, in fact, convex. Beautiful convex cone coming from multivariate polynomials. And again, if I take this example from the previous slide, we can see that the hypervelicity cone will be, depending on which direction we choose, up or down will be one of these nice quadratic ice cream cones, also called Lorentz cone or second order cone. So that's one example. What's perhaps the key or the most somehow one of the most concrete and nicest, most interesting examples.
00:05:28.154 - 00:06:17.088, Speaker A: And when I take a polynomial p, that has what we'll call a definite determinant or representation, okay? So we say that p is. So here I have a collection of d by d symmetric matrices, a one up to a n. I write this linear pencil, this linear combination of them in variables x, I take the determinant, so this gives me a degree, d polynomial and n variables. And I also ask that there's a point e, okay, for which this matrix inside is strictly positive definite. So that's where the definite indefinite determinant or representation comes from. So if I had this sort of polynomial, okay, this thing inside is a symmetric matrix. So if I look along this direction, I mean, I'm always going to get real eigenvalues, okay? Which is exactly my hyperbolicity condition.
00:06:17.088 - 00:06:55.924, Speaker A: It's exactly having real roots, okay? And the corresponding hyperbolicity code in this case is exactly what we call a spectrahedron. So it's the intersection of the cone of d by d symmetric matrices with some, in this case linear subspace. So everything's homogeneous. I'm only working with cones here. These are exactly sort of the feasible regions of semi definite programs. So that's sort of a more general example. And if you specialize in various ways, you'll see that you can certainly get any polyhedral cone out of this by taking the product of the linear forms that define facet inequalities.
00:06:55.924 - 00:07:28.824, Speaker A: This will give me polyhedral cones. And again, I need my e to be a point in the interior of the cone. This will do the job. And of course I get the PSD cone if I don't choose any interesting subspace here. All right, so these are my key examples. And so what I'm going to be interested in, as I said more broadly, is how do these hyperbolicity cones relate to the feasible regions of semi definite programs? How do they relate to spectrohedral? We can see that any spectra, hedron is a hypervelocity cone. That's what we saw in the previous slide.
00:07:28.824 - 00:08:43.756, Speaker A: And we're also interested in understanding, do we get essentially more things by considering these hyperbolicity cones, or can we really redescribe any of these hyper velicity cones somehow in terms of the feasible regions of semi definitely programs? This is kind of one of the big questions in this area, and we'll push on this question a bit more in this talk. Why are these hyperbolicity cones interesting in the context of optimization? Well, it's this result of Guler that says that, okay, I mean, not only is this a nice convex cone, okay? But it has kind of some good computational properties, so that if I take the negative log of this polynomial, I get a self concordant barrier function, and the barrier parameter is the degree of the polynomial for this code. Okay? So this opens up the possibility of using interior point methods, um, to solve conic optimization problems over these codes. So, from the point of view of convex optimization, this is kind of this. This is the point at which convex optimizers, I think, started to think about and get interested in, um, hyperbolic polynomials and hyperbolicity codes. And we call these things hyperbolic programs. So, of course, these contain some of our favorite conic optimization problems, linear programs, second order cone program.
00:08:43.756 - 00:09:57.290, Speaker A: So that's where I have my cone is a product of these Lorentz cones, these ice cream cones, and semi definite programming as special cases. And somehow, at the level of optimization, a question we're interested in is, at least at some level, is hyperbolic programming more general than semi definite programming? Can I do more with hyperbolic programming than I can with semi definite programming? And there's also quantitative questions. I mean, you might have a hyperbolic program that has a nice, concise description, but if you want to describe it using semi definite programming, assuming you can, you really blow up the sides of your description. We sort of expect this to happen in some level. This is a question. But so far, the only examples I've shown you are ones where we already know, baked into their DNA, that we can only see hyperbolic polynomials and hyperbolicity cones that can be described as spectrahedra as the feasible regions of semi definite programs. So what I want to show you next is a construction that, again, you know, came up briefly in market's talk, that gives you new hyperbolic polynomials and new hypervelicity cones from old ones in a way that we don't yet know how to kind of mimic in terms of semi definite programming descriptions.
00:09:57.290 - 00:10:49.182, Speaker A: All right, so this is this idea of taking derivatives. So if I take a polynomial p that's hyperbolic with respect to some direction, I can take the directional derivative in that direction. So that gives me a new multivariate polynomial of one of degree, one smaller, and it turns out this remains hyperbolic with respect to this direction. Why is this true? Well, it's sort of true from this picture. So if I think about p along this direction as being this blue polynomial, okay, if I take the directional derivative, what happens along this direction is I really just get the the derivative of this univariate polynomial, and we know that, okay, it should have n minus one roots where the one before had n roots. And these roots all interlace essentially by Rolle's theorem or something we learned in first year calculus. So this is really the proof of why this works.
00:10:49.182 - 00:11:24.056, Speaker A: Okay. And this is a construction, I don't know when this was first realized, but it's certainly in papers from the seventies about hyperbolic polynomials in the context of PDE's that identified this construction as something that, that works. But really in the context of optimization, the person who really kind of realized this seemed like a really interesting construction in the context of optimization is Jim renegade. Sometimes we call these renegade derivatives because of these connections. So that's on the polynomial side. What happens on the geometric side? On the geometric side. So this is a picture of, inside here, the spectrahedron.
00:11:24.056 - 00:11:49.904, Speaker A: I think this has, this is a size eight spectrahedron. This is degree eight curve. Okay. In the plane and what I'm plotting here are the next curve is the first derivative relaxation. This is what I do when I take the directional derivative, and then I look at the hypervelocity cone, and I can, of course, keep doing this. So imagine I do this a few more times, and what you see is that these hypervelocity cones, they get bigger. This is why we like to use this word relaxation sometimes to talk about these things.
00:11:49.904 - 00:12:26.608, Speaker A: So when I take the derivative, the hypervelocity cone gets bigger. Interestingly, yeah. Unrelieved. What are the black points? Yeah, the black points I'm not going to say anything about, because I reused this picture from somewhere else where they had meaning. Okay, so I can tell you later, but actually this black point is interesting. So one other comment I want to make is that these derivative relaxations preserve low dimensional faces, okay? So if you take, if you have some low dimensional face of your, things like this thing, think about the cone over this object, this kind of a one dimensional extreme ray. And if you take this first derivative of relaxation, it remains an extreme ray.
00:12:26.608 - 00:12:52.244, Speaker A: Okay, so there's a whole lot of interesting stuff that happens along the line because of this. And if you want to learn about that, I suggest you read Jim's paper. Hyperbolic programs and their derivative relaxations. There's lots of interesting information about what goes on. Okay, so let's try to use this to get some nice new examples of hyperbolic polynomials. I mean, they're not new. People have known this for a while, but new to us in this talk.
00:12:52.244 - 00:13:45.424, Speaker A: So, for example, if I take the product of all the variables. So I've got n variables, take the product I'll call it en, that's the elementary symmetric polynomial of degree n in n variables, and I differentiate in the all one's direction. What I end up getting is the elementary symmetric polynomial in n variables of degree n minus one. And if I keep iterating this construction, I'll get all the elementary symmetric polynomials. So these are all hyperbolic polynomials, hyperbolic with respect to the or ones direction. They have interesting hypervelocity cones that are all somehow relaxations of the authent that are really nice and symmetric and have interesting facial structure. That's if I differentiate in the same direction each time I get these nice symmetric objects by differentiating in a whole bunch of different directions, I essentially get something related to the permanent you can get quite complicated objects by doing this differentiation.
00:13:45.424 - 00:14:31.906, Speaker A: If I instead start with the determinant. So I think about the determinant perhaps as the product of the eigenvalues, and I do this same operation. Okay? If I take the directional derivative in the identity direction, I get a degree n minus one polynomial in the entries of the matrix, and that turns out to be the sum of the n minus one by n minus one principal minus. And I can also think of this, it turns out as being exactly the elementary symmetric polynomial of degree n minus one in the eigenvalues. So we can do all this kind of thing at the level of eigenvalues of symmetric matrices as well. And again, if I do this lots of times, I'll get somehow that the elementary symmetric polynomial in the eigenvalues. Elementary symmetric polynomials of any degree in the eigenvalues of a symmetric matrix.
00:14:31.906 - 00:15:33.646, Speaker A: These are hyperbolic polynomials. And again, their hyperbolicity cones are interesting relaxations of the PSD cone. All right, so now we've seen that there are interesting hyperbolic polynomials that are not obviously not obviously coming from polynomials that have this definite determinant or description. Or if you like, there are hypervolicity cones that are not obviously spectrahedra, not obviously intersections of the PSD cone of the linear side. Now let me present to you the big question in this area about how do hypervelicity cones relate to spectrophe? There's this conjecture from Peter Lacks. This is also, again, goes back quite a long way to the PDE literature. The conjecture of lacks was that every hyperbolic polynomial in three variables has a definite determinant of representation.
00:15:33.646 - 00:16:13.316, Speaker A: Okay? So I can write it as a determinant of a linear matrix expression in three variables. And that was a conjecture. And it's now a theorem so, theorem of essentially of helt and Winnikov that this is true. So if you take any hyperbolic polynomial in three variables, you can find symmetric matrices a, b, c, so that that polynomial is determinant of ax plus vy plus cz. And that's a definite description. So you can find a point so that this is positive definite, so that part of the conjecture is true. And then, okay, people have been trying to come up with a decent generalization of this conjecture in higher dimensions.
00:16:13.316 - 00:16:45.796, Speaker A: So you could try to ask, does every hyperbolic polynomial have a definite determinantal representation? The answer to this question is no. Right? So there's various reasons why you can't have this strong algebraic statement, but you can make a weaker statement. And I'll show you the difference in a second. And you can still ask this question. Is every hyperbolicity cone a spectra here subtly different question? I'll try to point out the subtlety in a second. And this is, I would say, a wide open question. Not much evidence either way.
00:16:45.796 - 00:17:26.832, Speaker A: I'll show you all the positive evidence in a second. So that's kind of the geometric version of this question. What's the algebraic version? The algebraic version of this conjecture says if p is hyperbolic with respect to some direction, then I can find some other polynomial q. So that if I multiply q times p, that has a nice determinantal description. But multiplying q times p might mess up the hyperbolicity cone. I mean, it turns out if you multiply hyperbolic polynomials, you intersect the hypervelicity cones. So you also need to have this other condition, that the hypervelicity cone of q strictly just contains the hypervelicity cone of p.
00:17:26.832 - 00:18:21.160, Speaker A: In this case, when I multiply these two things, they won't change the geometric object, they won't change the hypervelocity cone, but they will change the algebraic object, and they may give us a chance of constructing this definite determinant. So, one of the subtleties here is that you can have the same geometric object with different algebraic descriptions. That's one of the things that makes this stuff rich. And just to kind of have one of the subtleties here highlighted is that asking for my polynomial to have a definite determinant of representation is strictly stronger than asking for my hyperbolicity cone to be spectra he draw by this multiplier business. And then there's this other business I haven't talked about much that will come up in a second that's strictly stronger than asking that my hypervelocity cone is a projection of a spectrohedron, whatever that is. That will come up just a tiny bit. So this generalized lux conjecture is like, again, wide open.
00:18:21.160 - 00:18:38.886, Speaker A: And it's kind of. If you just give me a hyperbolic. A polynomial. You claim to be a hyperbolic polynomial. It's pretty hard to imagine like, how one would concretely come up with an explicit spectrahedral description. So what I want to do is pose a simpler question. It's still, I'd say, wide open.
00:18:38.886 - 00:19:10.422, Speaker A: Okay, which is, you know, one of the most interesting constructions we have of new hyperbolic polynomials from old ones is this derivative construction. So what happens if I give you a hyperbolic polynomial. And I tell you that it's a spectrohedron. That the hypervolicity cone is a spectrahedron. So I give you some explicit spectrahedral description of the hypervelocity cone. If the lass conjecture were true, it would be true that then if I looked at some directional derivative. I would again be able to find a spectrahedral description of this thing.
00:19:10.422 - 00:19:51.556, Speaker A: So at least I start with concrete data. Can I find a concrete spectrohedral description of this derivative relaxation? If I could do that, then I could find spectrohedral descriptions of lots of the things we know and love. So I don't know how to do this, but I pose this as an intermediate problem that we should think about. But what this talk, the last part of this talk is going to be about. Is an easier version of this problem that we now know is true. Let me take an easier version of this problem as to say, suppose I start not with a hyperbolic polynomial, for which the cone is a spectrohedron, but with this stronger description. That is a polynomial that has a definite determinant of representations.
00:19:51.556 - 00:20:13.146, Speaker A: That's a strictly stronger statement. So if I start with one of those, what we're going to show is that. What we've shown is that if you then look at the hypervelicity cone of the derivative that is a spectrohedron. So that's the kind of main result. Yes. Can you do it twice? No. And so we won't be able to do it twice.
00:20:13.146 - 00:20:39.494, Speaker A: Because the output is not a definite determinant or representation. Remember, that's like. Come how an important point. Do you have an example where p is determinant by de of p is not determinant? I don't have a proof of that, but I expect the example I'll show you. I think we could show that that would be the case. I'm not 100% sure, but I expect that's the case. A lot of elementary symmetric polynomials are not determined.
00:20:39.494 - 00:21:19.712, Speaker A: Okay, there you go. So, all right, let me tell you what we know, and that will maybe answer the question from CYnthia's observation. So this is pretty much everything that's known about the likes conjecture, all the positive evidence for the Lys conjecture. Okay, so Roman Sarniel showed, and it's kind of implicit in earlier work, but showed that the elementary, the cone associated with the elementary symmetric polynomial of degree n minus one. So the first derivative of the orthod, if you like, has an explicit spectrohedral representation of size n minus one. This was generalized by Peter Brandon to show that if you take any of these cones associated with elementary symmetric polynomials, they have spectra hydraulic descriptions. They have pretty big size.
00:21:19.712 - 00:22:00.364, Speaker A: I mean, they have size n to the k minus one. And much more recent or more recently, Amini has showed that if you look at certain multivariate matching polynomials, you look at their hyperbolicity cones. So these are hyperbolic polynomials, but these are all spectrohedra. This construction sort of contains these cones as special cases. So there's been quite a lot of development recently in that direction. And then Mario Kumar has also shown that the specialized varmus polynomials has something to do with the vamos metroid. It also has a spectrahedral hypervelocity.
00:22:00.364 - 00:22:23.534, Speaker A: These are sort of the things we know, essentially. And then just. Yeah, yeah. So n here, n to the k is number of variables, or degree, I guess n is number of variables. Degree is k. But when you say it has size n to the k, then that that's the size of the matrices, or if you. In the spectrohedral representation, or the degree of the determinant, in the determinant or thing.
00:22:23.534 - 00:23:07.996, Speaker A: So you have to multiply as a very high degree to get this, if you like, to get this representation. So just a quick word on this is that if we allow ourselves projected spectrahedral descriptions, which turns out to be a bigger class. So it's not just spectra, Hydra, but I'm allowed to apply linear maps to them. Um, a few other things are known. Okay, so this cone was understood earlier in that context. In some previous work with Pablo, we showed that all these, um, hyperbolicity cones associated with elementary symmetric polynomials in the eigenvalues all have polynomial size descriptions when you allow projections. Okay? And then there's also results that any smooth hypervelocity cone has such a projected spectra hydraulic description.
00:23:07.996 - 00:24:09.954, Speaker A: So a bit more is known here, but you have more to work with here. All right, so let me just, in the last of the talk, give you a bit of a flavor of what these representations look like. I'm not really going to prove anything. What I'm going to do is first you show you what Staniel's description looks like, okay? And then use that to hint at how ours looks like for the matrix code. So if I wanted to get a determinantal description of the elementary symmetric polynomial of degree n minus one, one way to do it is to take the n cycle and put weights on the edges corresponding to my variables x and compute the spanning tree polynomial. The spanning tree polynomial that is going to be the sum of all the products of edges in spanning trees is going to give you exactly the sum of all the n minus one subsets. This is not how it's written in Raman's paper, but this is one nice way to see this for a talk with a picture.
00:24:09.954 - 00:25:02.286, Speaker A: So in this case, if I took the spanning tree polynomial of this graph, I'd get the elementary symmetric polynomial of degree three in four variables. And this idea generalizes. And we sort of know that if you take the reduced, because the Laplacian has a zero eigenvalue, edge weighted Laplacian of a graph, and I take the determinant of this thing, this gives me this spanning tree polynomial. So this gives me a kind of a nice determinant or description for the elementary symmetric polynomial of degree n minus one. So what does the spectrohedral description look like? Well, it looks like, I mean, this is just looks like the set of x such that this Laplacian, or reduced Laplacian is PSD. And the thing I want to point out about that is what this looks like is, I could write this as v transpose diag xv, where the columns of v v is n minus one by n, or n by n minus one. Sorry.
00:25:02.286 - 00:25:53.914, Speaker A: And the columns of v are span the orthogonal complement of the cycle space, which is the orthogonal complement of the all ones vector. So that's the bit that is going to come up. So what's the main result? The main result is, is, um, to get a spectrahedral description of this elementary symmetric polynomial in the n in degree n minus one in the eigenvalues. Okay, so this is the first derivative of the PSD cone, if you like, and that's enough to get this other slightly more general looking statement that if you take anything with a definite determinant of representation, you take a directional derivative that that will be a spectrum here. So this is the essential case. And what you do, you take a basis for the set of symmetric matrices with trace zero. So think of that as the analog of vectors that are orthogonal to all one's.
00:25:53.914 - 00:26:29.498, Speaker A: And you write out some matrix that's linear in x corresponding to these things and ask for that to be PSD. This is some kind of natural analog of san l representation. And it turns out that this works. So let's see, I've got maybe a minute to show you a little bit about why this works, to give you kind of a flavor of why it looks reasonable. And then we'll wrap up. So in the paper we have two proofs of this thing. One is what I call a geometric proof, where I don't explicitly write out an algebraic identity, but I give some separate argument.
00:26:29.498 - 00:27:08.764, Speaker A: And what's nice about this is that you don't need to be able to know what the multiplier is. And the algebraic argument, this geometric argument, the multiplier sort of, it just comes out of it for free. So this is what's sort of nice about this. So I'll just explain this one. I'll skip the algebraic argument. So this is another way to write this representation of Samuel that we saw a couple of slides ago, is to say that it's a set of x in rn such that this quadratic form in Y is non negative for all y in the subspace one, orthogonal to all ones. That's just a different way of writing that thing, just rewriting the PSD constraint as a bunch of non negativity constraints.
00:27:08.764 - 00:28:02.332, Speaker A: And what's, you know, if you had to guess an analog of this, okay, if I had to guess an analog of this, which I guess I did, what would I choose? Well, I would say, let me consider the set of symmetric matrices such that the trace such that this quadratic form in matrices y is non negative for all y orthogonal to the identity. This seems like a reasonable guess if you're going to guess an analog. Okay, it turns out this is correct. The main thing you need to do, because of all the symmetry, the main thing you need to check is that this works for diagonal matrices x, and that for diagonal matrices x, it gives you back this. So anything you need to check, and to do this you need to use, you know, one nice thing you need to use is the fact that the diagonal of a symmetric matrix is majorized by its eigenvalue. This is somehow required in here to do this argument. So let me skip the algebraic version.
00:28:02.332 - 00:28:37.928, Speaker A: But just to say that if you, if you now take this description and write determinant of this PSD constraint, you would get somehow what pops out for you is some multiplier. Okay, that's nice. And huge algebraic identity. Okay, so just a couple of questions before I finish. So, of course, what we've shown is that the hyperbolicity cones corresponding to the degree n minus one elementary symmetric polynomial is a spectrahedron. We don't know how to iterate the argument, but it's a fair question. Okay, these things, perhaps these are all spectra, hydra for all degrees.
00:28:37.928 - 00:29:06.100, Speaker A: The smallest example we don't know is the situation of five by five matrices and the elementary symmetric polynomial in the eigenvalues of degree three. This is the smallest example. So maybe it's not a spectrohedron. The lax conjecture is false. I don't know. One other nice question I want to highlight that I like, mostly because of the name, is this question. So this should also be like, if the lax conjecture were true, this would be true, this question be.
00:29:06.100 - 00:29:42.704, Speaker A: Yes. So suppose I have a permutation invariant spectrahedron, okay? Like the hypervelocity cone corresponding to the elementary symmetric polynomial of degree k. That's a spectrohedron. I could ask about the spectral set corresponding to this, the set of symmetric matrices whose eigenvalues are in there. This is a nice convex set. It's known to be a hypervelicity cone. Is it a spectrohedron? Can I have an explicit way to take a spectrohedral description of some vector symmetric set and build a sort of matrix symmetric set out of it? So I don't know how to do this, but I think it's a nice question.
00:29:42.704 - 00:30:27.788, Speaker A: All I know at the moment is the title of the paper. Okay. I can know anything else. Okay, so if somebody else writes it, that would be great, too. So just to finish, what have we seen? The motivating question, somehow the big level about this stuff is, one question is, what's the relationship between this hyperbolic programming and semi definite programming? In particular, are hyperbolicity cones feasible regions of semi definite programs? Okay, the main result in this talk was that, okay, this new explicit family of hyperbolicity cones that come from differentiating determinantal representations where we can construct explicit spectrahedral descriptions. Okay, so a tiny bit more positive evidence for the lexiconjecture. Okay.
00:30:27.788 - 00:31:02.086, Speaker A: And there's a preprint if you'd like to read more. Thanks very much. So we have time for a couple of questions. Yeah, just do we believe that maybe the lax conjecture or the lax conjecture with spectrahedra is possibly false or projected spectra? Hebrew is not. I mean, I think all. But anything could be true. I mean, that's but what I would say is maybe that there's another conjecture in this sort of convex algebraic geometry.
00:31:02.086 - 00:31:17.322, Speaker A: Visibility was open for a while, which was that any convex semi algebraic set. The conjecture was that that was a projected. There was projected spectra. Hydra. So true in two dimensions. True for smooth things. Okay, this is false.
00:31:17.322 - 00:31:34.736, Speaker A: Klaus Scheiderer showed this was false late last year. And so with this in mind, it's, I think, completely unclear what the situation is. I would not be surprised if it's all false. That would not surprise me. And we're not that imaginative about dreaming of examples. Yeah. Morally.
00:31:34.736 - 00:31:56.140, Speaker A: Related to this. Has anyone tried to show that the corner non negative polynomials is the hyperpodernist? I guess it's. Yeah, I think it's not. I mean, one thing you have to be to be a hyperbolicity code is you need to be facially exposed, for example. So you need to be basic semi algebraic. It's got a lot of extra properties that are shared by spectra. Hydra.
00:31:56.140 - 00:32:14.964, Speaker A: And this is why this conjecture makes sense. Because we don't know any. Of course we don't know any property. Every property we can dream up that we know about hypervelocity cones is true for spectrohedra. Any other questions? Okay, let's think.
