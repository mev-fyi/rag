00:00:00.920 - 00:01:45.972, Speaker A: Okay, so I'm going to be talking about joint work with Nike sun and my student Yu Meng Zhang. So apologies to those of you who've already heard them give better versions of this talk, but okay, so for this audience, I don't need to tell you what a random constraint satisfaction problem is, you know, a bunch of variables and some constraints, like random colorings of random graphs or random kSAT, and we'll choose the constraints randomly. Okay, and a major theme for this workshop, and also the program as a whole has been connections to between these sorts of problems and theoretical physics, where they're studied. As you could think of, many of the same problems as dilute mean field spin glass models. And today I'll be talking about various, or trying to make parts of this theory rigorous maths. So again, I'm sure almost all of you know the random KSAt problem, but let me just say it so that we're notationally on the same page. So we have n variables, true or false, and constraints will be an or of k variables or their negations, and we'll choose them uniformly at random.
00:01:45.972 - 00:02:51.160, Speaker A: So a three sat formula would look something like this, with four clauses, and for each clause we just choose three of the variables or the negations uniformly at random. And of course they get harder and harder to solve, and there are fewer and fewer solutions the more constraints you have. So you measure the difficulty of it by the clause density, the ratio of constraints to variables. And throughout the talk I'll call this alpha. So the number of constraints divided by the number of variables. Okay? And we now know sort of more and more about KSAT, including for large values of k, we know what the satisfiability threshold is. It can also be good to think of these models in terms of a graphical representation as a spin system on a random network.
00:02:51.160 - 00:04:23.354, Speaker A: So if we have, say, a forsat formula, we could encode it as a bipartite factor graph where the variables are on one side of the graph and the clauses are on the other, and we just add an edge between the variable and clause. If that variable appears in the clause, we might also label the edges or color them according to whether or not they're negated. Okay, and because we're choosing the clauses at random, we get a random graph. And like I get, well, many models of random graphs, it's locally tree like. So you pick a, you pick a vertex at random, look at the local neighborhood, you won't see any short cycles, you'll see, you'll see a tree. So what do we want to say about random constraint satisfaction problems? Well, the, like a basic initial question you can ask is for which values of alpha are the satisfying assignments? How many constraints can you run on them? And for this, for this question, we've, you know, there's been sort of substantial progress. And in at least a number of example problems, we know what the answer is, including various versions of not a legal sat or random KSAT, large independent sets and so on.
00:04:23.354 - 00:05:58.914, Speaker A: But we could ask further set, knowing the answer to the first question, we could ask further questions. When you have solutions, how many are there? What do the solutions look like? If you pick a random solution to one of these random constraint satisfaction problems, what can you say about various local statistics of it? Or the overlap between two randomly chosen ones? Or algorithmically, can you find them? So, for today's talk, I want to focus on this question of how many solutions there are, and in particular what we might call the free energy, the limit of log of the number of solutions divided by n, assuming that, I mean, first of all, we to show that such a limit exists and then calculate what it is. So that will be the aim, and I'm not going to do it for random k Sat, we'll look at the not a liquor Sat model. So this is like KSAT, except you form a formula in the same way, except you ask that not just that x is a solution to it, but the negation of x is also a solution. So. Or another way of saying that is, if we look at any clause, at least one of them has to evaluate to true, and at least one of them has to evaluate to false. Hence the not all equal name.
00:05:58.914 - 00:06:58.870, Speaker A: Okay? And I'm also going to ask that it's deregular in the sense that every variable appears in exactly d clauses. Okay, so why study regular noddle equals that? This doesn't appear at the top of anyone's list of most interesting random constraint satisfaction problems. It's, you know, something like random colorings of random graphs or random k sat, you know, more interesting. Well, first of all, we expect it to have the same sort of set of phase transitions that these other models that we might care more about have. And the same kind of general universality picture should describe this model in the same way. And secondly, it has some sort of extra bonus nice features to it that make our life easier. So first of all, it's binary.
00:06:58.870 - 00:07:53.648, Speaker A: So unlike random colorings, each variable can only take hash, has only two choices. It's also symmetric. So if you look at any variable, it's equally likely to be true and false. Just, I mean, that's trivially true, because if x is a solution, then the negation of x is also a solution. And thirdly, and most importantly, it's what I might call locally homogeneous. So, if we look at the graph that encodes the problem and pick a random vertex, the local neighborhood of it looks like a periodic tree where the even rows are degree d and the odd rows are the ones corresponding to the variables are degree d. The ones corresponding to the clauses are degree k.
00:07:53.648 - 00:08:54.784, Speaker A: So in particular, every local neighborhood looks the same. And this is a really helpful feature in the analysis of these models. And this is, for instance, in understanding, say, the satisfiability threshold for the KSAT problem. The lack of this local homogeneity is by far the biggest challenge in that, or at least the, the part that added the most number of pages. So it has these. So these properties are going to make the problem easier to study. So, any questions about the problem so far? Okay, so what's the first thing you might want to do in terms of counting how many solutions there are, which, and I'll call z, the number of solutions, the partition function.
00:08:54.784 - 00:09:33.112, Speaker A: Well, the first thing you might try is just to work out the expected value. And this is a really easy calculation. This is something I'd like to think my undergraduate probability students could do, but that's a bit optimistic. So every solution is equally likely. And also the clauses all act independently. So it's just this formula, which we could write as an exponential. It's e to the n to some exponent, which is a linear function in alpha.
00:09:33.112 - 00:10:38.754, Speaker A: So we could just calculate what's the largest value of alpha, for which this is non negative. And so it's about two to the k minus one times log two. So this certainly at least gives us an upper bound on the satisfiability threshold. And the question is, is this the right formula for the free energy? Okay, so first moments are good for giving upper bounds, second moments are good for giving lower bounds. So we could check when is the second moment the same order as the first moment squared? And because of various nice properties of the model, this works for a wide range of alpha up to. And you can work out how far it works up to, up to this value, which is almost the same as alpha one, except it's about one half less. So there's only a.
00:10:38.754 - 00:11:46.044, Speaker A: So there's a small gap between when the first moment and the second moment work, but there's still a gap. And this gap is a hallmark of many of these problems in random constraint satisfaction problems, there are ways you can sort of improve this threshold a bit, and quadrigon Zed Verdova did that. But there's still sort of fundamentally a gap you'll get for reasons which I'll describe on the next few slides. So the picture that happens is, well, if this, if the blue line is just one over n times log of the expected value of z, we'll get a linear function. Alpha one is its root. And this you might call the replica symmetric solution. So we know it's the right thing below alpha two by the second moment method.
00:11:46.044 - 00:13:07.848, Speaker A: What it turns out is that it's correct. Even beyond that, up to a further threshold, the condensation threshold, which I'll describe in more detail in a few slides. Time. But after that, the free energy one over n times log of z is strictly less than this, than what the expected value is predicting up to a threshold alpha sat, which I should have said was calculated earlier in paper with DNS. So the expected value doesn't give the right estimate in this range between the condensation threshold and the satisfiability threshold. Okay, so an initial, so let me give first an initial sketch of why, why we should expect this to fail, or the expected value to give the wrong picture. If I take a random solution and look at a typical vertex, there's some chance that it's what I might call free.
00:13:07.848 - 00:14:06.034, Speaker A: So if I switch it from true to false, it will still satisfy the equation. So it's not in any, any clauses where it's forced to take its value. And you can show that for a typical solution, we should expect a constant frac, like a small but constant fraction of the variables to be free. And we could take a subset of those so that they're all at least distance. Yes, so that none of them share a clause, so they're all also independent of that. So if we have epsilon times n such variables, then there must be two to the n times epsilon solutions nearby, where we, by just switching the values of these three variables. So it says if you have one solution, you should have at least two to the n epsilon solutions.
00:14:06.034 - 00:15:14.084, Speaker A: So, first of all, that's saying that the value of one on n log z has to be at least epsilon log two. So there's kind of a smallest value this curve could take, which is strictly bigger than zero. Okay, so that immediately says that this line can't get all the way to alpha one, but we'll give a more precise description of what's going on based on the theory coming from physics. But just to state the main result, the free energy is, first of all, it's well defined. And the value for the formula in this, there's an explicit formula, which I'll show later on, um, in this range. And it's the one that's predicted by the one step replica symmetry breaking. Okay.
00:15:14.084 - 00:16:26.534, Speaker A: And it actually, the, it doesn't say this in the result, but also the result says that below the condensation threshold, it's, you know, this, the expected value is the right prediction. That's the main result. So let me. So, you know, we're saying that the prediction from physics is correct. So let me describe, at least from a mathematician's point of view, how you come to that prediction. Okay, so this, you know, these sort of heuristics that are used, I guess, go back to earlier work on spin glasses and the Sharington Kirkpatrick spin glass models. But even in the eighties, there was work by Mizard and Parisi on sort of sparse constraint satisfaction problems and applying ideas from spin glasses to those.
00:16:26.534 - 00:17:23.824, Speaker A: But maybe, but more recently, there's sort of a much more complete picture, which you might call the one step replica symmetry breaking phase diagram given by these authors, who I think they're all here for this workshop, which is, I think, well done to the organizers. Yeah. So. And unfortunately, you have me to describe it rather than them, but yeah. So probably many of you have seen this picture at least 20 times in talks. I know, but. So I'll describe the aspects of it I want to focus on today.
00:17:23.824 - 00:18:36.424, Speaker A: So the idea is, if we take multiple solutions to these, we can think of two solutions to a random constraint satisfaction problem as being adjacent, if they differ in one bit or a small number of bits. And then that gives us a sort of geometry of what the spacer solutions look like. And we can say, do they come in connected clusters, or are they sort of isolated from each other? And as we go from left to right, we're thinking of increasing alpha, the constraint density. So initially, the space starts off as well connected. After some point, there's a threshold where the solutions break up into exponentially many clusters, each with only an exponentially small fraction of the solutions. This is a long way from the satisfiability threshold in most of these models. Much closer to the satisfiability threshold is this condensation threshold.
00:18:36.424 - 00:20:07.104, Speaker A: I mentioned a few just now. After this, the solutions are still split into exponentially many clusters. But the difference is, apart from the fact that the free energy is now small, has a phase transition after this point, if you look at the geometry of the solutions, another thing is that rather than if, most of the solutions should be on just the largest handful of clusters, so you expect something like the thousand biggest clusters to have 99% of the solutions or something like that. Okay. And the final threshold is the satisfiability one. So we'll be interested in this regime, the condensed regime between the condensation threshold and the satisfiability one. Okay, so how do you say things about this? I'm going to do some sort of initially heuristic calculations that require a couple of leaps of faith in performing it.
00:20:07.104 - 00:21:03.224, Speaker A: So the idea is to break up the expected value of the number of solutions by summing it up over clusters and collecting the clusters according to how big they are. The contribution of clusters of slides e to the n times s, we expect them to be exponentially large. So clusters of about this size, and we'll say that the expected number will be e to the n times sigma of s. And, you know, at the moment, I'm just assuming the existence of such a function. And this is sometimes called the complexity function. Okay, so if we had such a formula, what cluster size would make the biggest contribution? Well, it would be the one that maximizes s plus sigma of s. Okay.
00:21:03.224 - 00:21:51.344, Speaker A: Or alternatively, if we just drew sigma of s, we expect it to be concave. That would be the point at which the derivative is, is minus one. Okay. And this is the picture as it should look. In the clustered regime, as we increase alpha, this curve should get smaller and smaller. And at the condensation transition, this should be the point at which the second or the last root of sigma has derivative minus one. And beyond that, in the condensation regime, this tangent point should be for a value that has a strictly negative value of sigma.
00:21:51.344 - 00:22:22.674, Speaker A: We keep going, the satisfiability threshold will be, well, what does the satisfiability threshold mean? It's the last time you have any solutions. So the last time at which sigma is non negative for any value of s. So the point at which it's just the maximum value of sigma is zero. And then in the unsatisfied regime, sigma is just always negative. So you're implicitly assuming that the expected.
00:22:22.754 - 00:22:25.442, Speaker B: Number of clusters reflects the true number of clusters.
00:22:25.578 - 00:23:00.884, Speaker A: Yes. Yeah. Without. Yeah. And I'm not justifying that at this point. So in principle, this could just be, this only gives you an upper bound unless you assume that something like that. Okay, so what happens in the condensation regime? Well, okay, we said that this, the biggest contribution here comes from the value of s such that the derivative is minus one.
00:23:00.884 - 00:23:44.738, Speaker A: Okay, but if we look at the value of s here, sigma of s is negative. That says the expected number of clusters of that size is exponentially small. So you don't expect to see any of them. Or in other words, the expected value is dominated by an event that has exponentially small probability. Okay, so it follows that the expected value can't be the right, can't give you the right value for the typical value of z. Okay, so what does. Well, we should take the biggest size of clusters that do exist.
00:23:44.738 - 00:25:03.674, Speaker A: So the last time in which sigma is, is non negative. And this is the prediction that the free energy is given by the largest value of s such that sigma is non negative. Okay, so this would be great if we knew what sigma was, then we did. We just have to. Okay, another thing I want to mention is we talked about what's predicted for the geometry of the space of solutions in this condensed regime, that most of the solutions are given in just a small number of clusters, a constant number of clusters. So why should you expect that? Well, we're saying that it's clusters of this size that dominate the partition function, and that's ones with sigma of s equals zero. So the expected number of clusters that we will see of that size should be of a constant order.
00:25:03.674 - 00:26:23.514, Speaker A: And hence you just see most of the contribution to the mass of the distribution would just number of solutions. I mean, there are more refined predictions that if you look at the largest collection of clusters, you should see a Poisson Dirichlet distribution amongst their sizes. So there are, there are also some challenges that come in working in this condensed regime, in that some quantities are sort of not concentrated, and it comes from the fact that you just have most of the mass dominated by a constant number of clusters. So if you looked at z over divided by its median, say this wouldn't converge to a constant. It would have some non trivial distribution, which I guess should be something you could work out from the Poisson Dirichlet distribution. If you look at the overlap between. So you pick two random solutions.
00:26:25.454 - 00:26:25.766, Speaker B: This.
00:26:25.790 - 00:27:35.344, Speaker A: Will be concentrated on two points. And so, like you pick two solutions, you look at the hamming distance between them or the inner product. Well, there's two things that can happen. One is you get two solutions in the same cluster, or you get two solutions in a different cluster, and so you'll get two different hamming distances depending on which one happens. So it will be concentrated on two points, but the probabilities of those will depend on the actual random problem that you've chosen and depend on essentially the proportions in the biggest few clusters, which will tell you what's the chance of hitting the same cluster twice as simple, essentially. So some things aren't very well concentrated in this regime. But luckily enough, enough things are that we can still do this.
00:27:35.344 - 00:28:35.604, Speaker A: Okay, so the next thing we have to do is try and get a hand on sigma. Okay, so, so the first thing is that we should work with counting clusters rather than counting solutions. And so, what do I mean by saying one step replica symmetry breaking? Well, there's various ways in which you could think about that, but one that we find useful is to say that when you project to looking at clusters rather than the. So solutions come in with some structure. They come in clusters, but when you look at the clusters themselves, there's no further structure. You don't get clusters of clusters or complicated things like that. Okay.
00:28:35.604 - 00:29:40.144, Speaker A: And counting the expected number of clusters has been great for, for working out what the satisfiability thresholds are for different models. But this isn't enough for our purpose, because we need to look at essentially not just how many clusters we have, but how many clusters we have of a particular size. So the properties of the clusters, particularly their size, also matters. So what we. So how you could think about this is if we looked at the expected number of solutions, this sort of roughly corresponded to working out e to the n times s plus sigma of s, or working out this tangent point with derivative minus one. For working out the satisfiability threshold, this was just counting clusters unweighted. So e to the n times zero times s plus sigma of s, or looking at the maximum of this curve.
00:29:40.144 - 00:30:54.324, Speaker A: But actually, we want this point here, which will have some other derivative between zero and minus one. And so if you choose lambda to be, or if the derivative is minus lambda, then this calculating the contribution from this point is like work out e to the n times lambda s plus sigma of s. So if we weight clusters by their size to the power of lambda, we'll be able to understand this. So z lambda means summing over clusters of the number of solutions in the cluster to the power of lambda. And why would you do this? Well, one thing is this is just, if we work out the free energy of this z lambda, you just get back the legendre transform of sigma of s. And because it's concave, we could then recover sigma of s. So if we could calculate this quantity, then we'd be in good shape.
00:30:54.324 - 00:32:18.382, Speaker A: Okay, so this one doesn't necessarily look like the easiest thing to calculate, because you first say, okay, I've taken some counting problem. Now I group the solutions up into clusters, then I raise them to some non integer power and add it all up, which might not immediately sound like a promising thing to do. But it turns out with this, the way solutions are structured, it's something that you can carry out. So I promised an explicit formula, so I'm not really going to say what it means. And the first set of equations are essentially saying you solve some fixed point equation, and the way you can think of this is saying, suppose you have a problem with n variables, and then you want to add another variable to it. Well, the distribution around this new variable should look pretty much the same as all the other ones. Um, and so, so there must be some sort of consistency relationship between, um, things.
00:32:18.382 - 00:33:29.894, Speaker A: If adding in, going from n variables to n plus one variables, doesn't change things much, so you get a fixed point equation, which is these ones, and then as you, as you add an extra variable, you can say, well, how much did this partition function, z lambda change when we added in a new variable? And you can, if you make some assumptions, you get sort of a calculation that said it should, by plugging this in here, and this is also the formula for z lambda, or sigma of lambda. Nike is going to tell you more, going to say more about how you calculate things. I hope I'm not promising too much. Anyway. So anyway, we get an explicit formula. That's all I really wanted to say about that. And in the time remaining, let me say a bit about the proof.
00:33:29.894 - 00:34:38.302, Speaker A: The upper bound is kind of standard. It follows from using the interpolation method, which was something developed for the spin glasses by Huerra and Tonanelli. It was adapted to the sparse setting by Franz Leone and Panchenko Telegrand. We're using sort of the regular version of it that was, I guess, done in Bayati, Germanic and Tatali. Basically what it boils down to is you get some functional that you can plug some distribution into, and it will give you an upper bound on the free energy. And if you know the right distribution to plug into it, and the heuristics coming from physics tell you what the right distribution should be, then you get the right upper bound. So the main work is to come up with a matching lower bound to this.
00:34:38.302 - 00:35:55.304, Speaker A: So this is 5% of the paper, and doing the lower bound is the remaining. Okay, so how do we, how do we do this? So essentially what we boiled down to is we needed to calculate the expected value of z lambda, this partition function where we weighted clusters by the, the number of solutions to the power of lambda. So I've been pretty vague so far about what a cluster of solutions actually means. So we could think of the following procedure to start off with a solution and describe what its cluster looks like. So we'll add in a new spin called free. So plus and minus will still stand in for true and false, and we'll call a variable free if the variable isn't forced to be either true or false by any of the clauses that it's in. So if it.
00:35:55.304 - 00:37:09.242, Speaker A: So if you had a so being, I mean, ignoring the negations that we get, this variable would be, I could say, would be forced to be a plus because I can't change it from plus to minus without violating this clause. Okay, so we take the variables that aren't forced and call those free variables. Now, if we decided that this one was free, then this clause would no longer be forcing this one. And so we just iterate this procedure until it stops. So every variable will either be true, false or free. And this gives us a new spin system with the following rules. So first of all, no clusters are violated or no clauses are violated.
00:37:09.242 - 00:37:56.014, Speaker A: Sorry. But also free variables can't be forced by any clauses. So you can't have something where it's false and then all negative, because then it would have to be true free. Sorry, you can't have free. You're not allowed to have something that looks like something that looks like this because this one could only be, could only be a minus. And. Okay, so free variables can't be forced in any clause, and true and false variables have to be forced by at least one clause.
00:37:56.014 - 00:38:58.316, Speaker A: Okay, so this is a new spin system. Why is this more complicated spin system better than the original one that we just had? The reason is it's rigid or locked in the sense that you can't make small perturbations to it. If you just. So if you looked at this on, say, an infinite tree, if you have one solution, you can't make a finite number of changes and get a new solution. The only way you can make solutions is that if you only way you can perturb it is if you perturb it on at least a cycle. And because the graphs we get are locally tree like, that essentially says that there are no small perturbations. So if you have one solution, it will be sort of isolated from other solutions.
00:38:58.316 - 00:39:57.444, Speaker A: And this is exactly what you want for the second moment. Method to work so that you do if you have one. If you have one cluster, you don't have any nearby. Okay? And so this, this does give the right notion of what a cluster should be in these models. Okay, now, the free variables, you can't set, you can't set them all independently of each other. For instance, if we had a sort of piece of configuration that looked like this, these two variables, the left one and the middle one, three variables, we couldn't say, set both of these to true, because then in this clause we'd have, it would all be true, and we couldn't set the middle and right one both to be false. So there are some dependencies between the three variables.
00:39:57.444 - 00:41:04.290, Speaker A: What we need to do is group together the free variables that have dependencies between them. And if you do that, we'll get a sort of, the three variables will be a subgraph, which will be a forest of small, constant sized trees. They'll look like very subcritical branching processes, actually, most of them will just be singletons. There'll be a much smaller number of pairs and so on. Okay, so we have, so the picture is we have our graph and then just occasional, and then some free variables, some of them which come in groups, most of them which are just isolated. And then, okay, and this represents our cluster. So how many solutions are there in the cluster? Well, it's the number of ways of assigning these free variables.
00:41:04.290 - 00:42:19.694, Speaker A: So the isolated ones can just be true or false independently of each other. So we get two for each of these. But these connected ones, there are some dependencies. So there might be, you'd have fewer than four solutions for this pair, fewer than eight for this pair. But there's some number of solutions for each group and the total number of solutions we can get by just multiplying the number in each of the groups. So in order to make this into a spin system where we can calculate things, we need a sort of richer out, a richer Alphabet, where rather than just declaring that a variable is free, the variable also needs to know what the topology of the tree of free variables it's in and also where it sits in that, that group. Okay? And so that gives you a spin system with a really large Alphabet.
00:42:19.694 - 00:43:28.446, Speaker A: Now, why would we want to make our life this complicated? So the idea is we want to add in weights to the spin system so that we can count the number of solutions, so that when we multiply together the weights, we'll get the number of clusters in this number of solutions in this cluster. And the way we can do that is by thinking about the belief propagation algorithm. Okay, belief propagation, if you remember. So this is an algorithm which people use on networks to find the marginal probabilities on vertices and work out the partition function. It's not guaranteed to work on graphs. There can be multiple fixed points and so on, but it is guaranteed to work on trees. And so you can use the belief propagation algorithm, work out a set of weights which you multiply together to give the number of solutions on a tree.
00:43:28.446 - 00:44:50.554, Speaker A: So we just calculate what the corresponding weights are for each of the variables in these trees of free variables and take those to be the weights. And then if we multiply them together, we get the size of the cluster. And so z lambda, we get by just summing over these cluster configurations of w to the power of lambda, which is just like multiplying these weights to the power of altogether to the power of lambda. So it gives a spin system with a set of weights that we can try and analyze. Okay? And now we just need to work out the expected value of it. For those of you who've done these calculations of trying to work out the expected value of a spin system on a random graph, essentially it boils down to trying to work out what's the best choice of the empirical distribution. So how many plus and minus and free variables you have, and slightly more information, like on each edge, what fraction do you have? A plus on one side and a free on the other.
00:44:50.554 - 00:46:08.534, Speaker A: If you know all of that, then you just get a formula which has a sort of multinomial term and product of weights, and you just get something that's exponential n times some function of the empirical distribution. And so you just need to maximize this empirical distribution, which is then boils down to a huge multidimensional, non convex optimization to do okay, but you can do that by finding fixed points of belief propagation equations. I mean, this is, so there's a, you know, there's lots of references to this. One place where it's done in lots of generality is a dembo montanarian sun. The trouble is, when you do this, you need to find fixed points that are of belief propagations, which are fixed points of essentially bidirectional pairs of messages. They're basically, okay, so this is for the experts. But if you.
00:46:08.534 - 00:47:26.154, Speaker A: Belief propagation gives you a set of messages that run sort of on every directed edge in the graph. So this will be variables to clauses and clauses to variables, and then so on each half edge, you'd have messages going in both directions, and then the BP equations would be finding fixed points of distributions over those pairs of messages. And that's a really, that's something that we've found to be really hard to solve these equations. So our main, well, okay. One of the main technical parts of our work was essentially finding a way to make it a belief propagation equation over a unidirectional set of messages. And, well, maybe I think I'm out of time, so I won't. We take the graph, we re sample it, and sort of magically it says that the fixed points have to solve some simpler set of equations.
00:47:26.154 - 00:48:12.824, Speaker A: Okay, so, you know, there's lots more questions to try and answer. What can you. Other models, you know, this was the sort of low on the list of most interesting random constraint satisfaction problems. Can we do it for sort of ones higher up on the list, like kSAT colorings of random graphs? We also, some of the geometric properties of clusters, like the sizes of the largest clusters, follow a Poisson Dirichlet distribution. Let me finish. Thanks.
00:48:18.364 - 00:48:19.704, Speaker B: All right, questions.
00:48:23.104 - 00:48:45.474, Speaker C: On the last point, you comment, the fact that if you write BP, you need set consistency equation for page or message. But actually, when you are on a tree, you expect that the two messages are uncorrelated because they bring information from very different parts of the system. So is your construction at the end equivalent to assuming that the two messages are independent?
00:48:45.544 - 00:48:48.554, Speaker A: Yeah, yeah. Yes.
00:48:48.854 - 00:48:57.474, Speaker C: Is that what it fails when you are on a loopy graph? On a loopy graph, the two messages pass on the same link are actually correlated.
00:48:58.814 - 00:50:19.518, Speaker A: Yeah, no, that's exactly what it boils down to. And the one whole slide formula that I had earlier was just what you get from the one directional message, where you make that assumption if you. Yeah, so, yeah, another, I guess another way of thinking about it is you have messages coming from below the tree, and this should tell you what all the messages are in the graph, except that you also need a message coming from the root. One of the things we do is modify the spin system a little bit so that these trees of free variables are forced to be of a bounded size. And that's something we can do because we just want a lower bound. And in doing that, it sort of forces the effect of, you know, how much this message from the top can propagate down so that it's only a constant amount. And that will imply that this sort of.
00:50:19.518 - 00:50:31.442, Speaker A: Yeah, the fact that the message from the top is sort of uncorrelated from the. Yeah.
00:50:31.458 - 00:50:35.530, Speaker B: Let me ask you, these free variables form a four, as we say.
00:50:35.602 - 00:50:36.066, Speaker A: Yeah.
00:50:36.170 - 00:50:41.294, Speaker B: Is that something that is only true for high values of k? Or, in principle, should it be true for low as well?
00:50:47.874 - 00:51:25.574, Speaker A: Sorry, I should have said all of this is assuming high values of k. Yeah. In this, I should probably defer that to the physics friends, but I think maybe, like, close to the. Like, close enough to the satisfiability threshold below condensation. Is it all. I mean, it does if k is big. If otherwise, no, probably not.
00:51:27.354 - 00:51:30.554, Speaker B: All right, thank you. More questions? No.
