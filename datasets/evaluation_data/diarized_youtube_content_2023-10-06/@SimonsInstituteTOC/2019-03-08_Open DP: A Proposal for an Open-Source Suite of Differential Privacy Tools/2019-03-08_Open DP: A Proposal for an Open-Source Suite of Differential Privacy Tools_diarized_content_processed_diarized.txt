00:00:00.240 - 00:00:45.034, Speaker A: And we have two presentations, as Marcus suggested were hencey, and Salil Vadam is going to be our next speaker. Celan has been doing a lot of work on privacy and a lot of work on bringing professional privacy to use, although I should say that this is the opinions of mine, but also the Census Bureau should have this opinion. And he will talk about give a page for community of effort on differential policy. And this is going to be a presentation and then a discussion.
00:00:45.814 - 00:01:03.556, Speaker B: Thanks, Kobe. Yeah, thanks, Kobe and all the organizers for this fantastic week. I really enjoyed it. Wish I could spend more of the semester here with you all. Yeah. So this is gonna be a little unusual compared to, I guess, other talks this week. I'm not gonna be talking about results or work that has been done.
00:01:03.556 - 00:02:13.394, Speaker B: It is a forward looking talk about something that should be done in the future and should be done collectively by us as a community. And so I wanna start a conversation around that and try to kind of refine this idea and see if we can figure out a path forward as a group. So this is inspired by work we've been doing in the privacy tools project and some at Harvard, with Georgetown, BU, and all our collaborators. And so a lot of people involved with the privacy tools project have influenced what I'm going to talk about. And in particular also our efforts in developing Psy that James Honaker talked about on the first day kind of lead to some of the thinking here. All right, so what am I do I want to pitch? I think the time is ripe for us to really have a community effort as a differential privacy community to build a. So the highlighted words trustworthy and open source suite of differential privacy tools that can be easily adopted by custodians of sensitive data.
00:02:13.394 - 00:03:25.272, Speaker B: And in this, I want to focus on making data available for research purposes, whatever one means by research. Why so, for the world, I think there is a real need and demand that we're seeing for such tools to be available. And I think there's been a lot of progress in that direction, but we're not quite there in satisfying the needs. And then from our perspective, I think the real incentive is to magnify the impact of the work that we all are doing. So it's sort of try to think about scope. I have a few use cases in mind, and one we should feel free to kind of talk about what other use cases make sense? So the one that we've been thinking about in the Privacy tools project most is tools for data repositories. So James talked about how SCI was going to integrate with dataverse.
00:03:25.272 - 00:04:44.574, Speaker B: But there are many other data repositories where human subjects researchers, which human subjects researchers use to share their research data with other researchers for secondary analysis and verifying their results. Currently, most of these repositories are not really equipped to take on and make available access to sensitive data. Second, we've heard a lot about government agencies trying to make data available to the public. We've in this week talked mostly about official statistics and statistical agencies. But as has come up in the conversations, there are many government agencies that don't have the statistical expertise of the statistical agencies that are also struggling to kind of respond to open data mandates. I mean, I mentioned earlier, police departments and city governments and so on, wanting to make data available and struggling with how to do so in a way that's protective of privacy. And then a final one that has also been part of the motivation some of you may have heard of, probably most people haven't, this new effort to make data from companies available for research.
00:04:44.574 - 00:06:48.004, Speaker B: So there is this social science. One effort, which is their first project, is trying to make Facebook data available for researchers to study election manipulation. And while there's a lot of thought has gone into the infrastructure to how such a thing might be designed in a way that's repeatable for many companies and many different kinds of studies, and they're thinking about what are the, how are researchers selected and having that done in a way that's independent, has sufficient independence from the company, and how are the set of research questions, the type of data that you're going to make available, how all that's decided, but something that's still kind of missing for kind of these efforts to happen in a kind of repeatable and scalable way is again the tools for privacy protection of sensitive data when you make it available to the researchers. So all of these use cases, I think they are all compatible with focusing, for starters, in such a suite of tools on the centralized model of, of differential privacy. And in part, I mean, of course we're seeing many of the applications of differential privacy are not only the centralized model, but also the local model, finding quite a bit of use. But it seems like at least my thinking is that that would be, one should think about doing, having a similar sort of effort in this direction for local DP, but that would be sufficiently different and not share as much infrastructure with building a suite for the centralized model and then of course the intermediate multi party models that might be used for hospital sharing data and so on. And that too would raise a whole different set of challenges.
00:06:48.004 - 00:08:25.754, Speaker B: All right, so why are we not there yet? So from kind of as I see it, but when people should feel free to push back if this is not really an accurate view of things. So we've seen this week in particular, I think, in the talks on Monday, the huge amount of thought and engineering that's going into building really good DP systems, for example, the 2020 decennial census. But these, of course, many of those ideas will be generalizable, but they're really being built with this particular use case instead of releases in mind. And also, maybe the same is true of if one thinks of the local model, Google and Apple and other companies using the local model are building systems really for collecting data of a particular type and wanting to do particular kinds of analyses on it. So we want things that are more generalizable that, again, an organization that doesn't have the expertise and resources that the Census Bureau or these big companies have to be able to make use of differential privacy. We have lots of wonderful academic research projects that are producing software and tools, I think, that are kind of leading to a lot of the same goals. But most of these are standalone projects that don't integrate with each other.
00:08:25.754 - 00:10:24.362, Speaker B: And so we're not currently in a state where all of the developments that are happening across the community can be kind of easily combined with each other and taken advantage of. We don't currently have a way that we are collectively vetting the software that one group is producing is not being kind of vetted by the rest of the community. And because they're academic research projects, often the goal is to demonstrate that something is possible, be able to write some scholarly papers on that and not necessarily to bring it to the point where it's easily adopted by someone who wants to make use of it. And then there are some closed commercial products that are appearing, and those may be good for particular clients of, of those companies, but they don't have the benefits of having open tools that have the kind of trust that one can have when they're produced by, invented by an entire community. Okay, so what are some of the things that we should be, that I think we should be shooting for? So, open source, but that's not just, you know, having a repository where the, where the, where the code is available, but really thinking about how do we create a community that is committed to contributing to this single resource. And we need to think about what are the right processes for there to be contributions and for them to be vetted appropriately. And then also, what are the incentives? How do we give credit, make sure that there's a way that, again, most of us are academics and people are primary objective or certainly important consideration is our career considerations.
00:10:24.362 - 00:11:40.654, Speaker B: What's the right way that we can give credit to people in the community for those contributions? All right, vetting I mentioned a few times. So there needs to be, we need to be thinking about in terms of processes, code that is security critical or privacy critical. How is that going to be evaluated? One thing that comes to mind also in the way that things may be deployed is that probably will be the case that one cannot move the sensitive data around due to legal constraints, for example. And so probably there's going to be need for the actual code to be shipped to the data, rather than the other way around. Tools need to be scalable and extensible, I think is really important. There's a very rapidly moving field where our understanding of what's possible and the algorithmic developments are progressing at a fast pace. And so how do we architect such a thing from the start, to be able to, to grow with the science in advance with the science, and not have to be thrown away five years from now.
00:11:40.654 - 00:13:34.274, Speaker B: So some of the components, obviously a key component will be some library of vetted, differentially private algorithms, vetted or formally verified, as the case may be. But there needs to be some process for verification. One has to think about what are the languages that are going to be supported for that? Will it be multiple? There needs to be thinking of how does one make, what are the interfaces to this and architecture around this set of algorithms. One can imagine that the thing is designed in a way that supports multiple different interfaces, some aimed at less sophisticated users and some aimed at data scientists who will, will write code. What are the kinds of data that's going to the formats of data we heard in some bullet I added after Michael's talk this morning, supporting simple flat tables only, or do we want to support multirelational databases and so on? And then there's a lot of stuff that I, I'm probably not the right person to talk about that I'll just skip over, but are probably important. And I imagine that we want to have this set up in a way that people can use the components in different subsets according to their needs and sophistication. So already just a library of vetted, differentially private algorithms could have a lot of use, meaning for more sophisticated organizations that want to build their own DP systems but don't want to start from scratch.
00:13:34.274 - 00:15:47.422, Speaker B: But then one can imagine really also the system as a whole, being something that's easily deployed by an organization that has no differential privacy expertise at all, where it's really a service that will manage budgets and generate differentially private code to run on the remote data sets, allows for both releases as well as interactive queries and so on. Different extremes of this, when you think about what kind of team there needs to be, it needs to be obviously some sort of leadership for this project of people who are kind of trying to maintain a consistent direction for its growth, which hopefully be something that could, a team that evolves over time, but then maybe some way that people in the community could be. So if we have this human vetting of code, you need a team of people that are going to be vetting contributions, maybe doing optimization on, or taking things from the literature and adding it to the library. So should there be a way in which people can come and spend a portion of their researchers can come and DP researchers can come and spend a portion of their time contributing to this effort? These are a bunch of roles that come to mind. Again, I'm not the right, having never really been part of an effort like this, we should probably learn from other major open source projects what kind of organizational structure is effective. And another question, and I realized that was missing in the initial list that we had for this sort of thing, is what should be the right kind of domain, experts representation from the eventual user communities of these things in the team that's developing this system. All right, so if this is a plausible idea, what are some next steps? One, we held some kind of workshop that's around kind of really fleshing out how this should be structured.
00:15:47.422 - 00:16:49.694, Speaker B: And really, I think learning from other major open source projects, raising funds, assembling a team, and then thinking about, we've heard how, I think John made the comment during Michael's talk. Talk. Having some real use cases is really important, even if the goal, I think, is to build a generalizable system and having those as sort of providing also a deadline that by one year out, we want to have something that's not just open, but actually is ready to be used for certain particular use cases. So now that was said, I wouldn't talk for long. So a bunch of questions for discussion. I really want to open up. I mean, one basic one for this room is like, what would it take for something like this to engage you all in terms of how it's structured or what the incentives are.
00:16:49.694 - 00:18:48.102, Speaker B: Another is this, we should be realistic, is do we believe we're ready for something like this? In the sense that if we're building kind of, again, a generalizable suite of tools, can they be actually useful enough without the kind of extensive engineering that has gone into the big deployments of differential privacy so far? Those set of use cases that I. Those are kind of, I guess, prototypes of use cases, you know, government data repositories and companies making data available for researchers. Is that the right set of things? Are there other ones that we should be talking about or drop some of those because they will involve some complications that are too difficult? Can we architecture thing that we'll really allow it to evolve with the development of the field? And I think I raised a lot of these questions before, and then there's this question about the type of vetting of the software. So it was nice to see in the talks this week that the formal verification of DP has advanced a lot and becoming a lot, I think, easier to use. And more general in terms of the kinds of things that can be formally verified, is that at a stage where it's ready to say that that's the way that code is going to be added, or if we have to go with manual vetting of verification of differential privacy, is that going to be sufficiently scalable? So let me just open it up and. Thoughts, reactions? That's pretty much all I wanted to say, Chris.
00:18:48.278 - 00:18:57.314, Speaker A: I think, you know, getting engaged simply knowing that something is going to be used and some form of.
00:19:00.494 - 00:19:01.150, Speaker C: Capturing.
00:19:01.222 - 00:19:35.474, Speaker A: So people using things from this repository, if they say, oh, we're actually using this for the following in a way that you could, that it was recorded in this site and someone could point to it and say something I wrote has been used, I think that would be a fairly significant incentive. But it means that people who have to use things out of the repository have to be willing to say they're using it, which I think is probably the hard part of that.
00:19:37.294 - 00:19:43.918, Speaker B: Yeah, maybe if this is set up in a way that it can somehow by default track that sort of thing, but, you know, possibly giving people more.
00:19:43.926 - 00:19:54.486, Speaker A: Of an agreement saying, look, you agree if you're using something, there's a repository to let people say you're using it. I mean that's a, that's very different.
00:19:54.550 - 00:19:55.474, Speaker C: From what people.
00:19:57.414 - 00:19:58.904, Speaker B: Are used to. Yeah.
00:20:01.324 - 00:20:44.286, Speaker D: So one sort of natural tension that might arise is I think we now have a market for DP engineers. I mean, there are companies who want people trained in DP and they would love for people to basically work at this organization that you're setting up here for six months, a year, or two and then steal them away. So that's great on the one hand, but I have a feeling that they're going to be companies trying to steal all of your employees constantly. So the question is, can you take advantage of knowing that there's that demand and maybe structure things, for example? So this is a place where a company can send a top engineer for a six month sabbatical, they pay them and you give them the DP training and benefit from them.
00:20:44.430 - 00:20:50.590, Speaker B: It's a great idea. Oh, yeah, yeah, yeah, yeah, yeah, exactly.
00:20:50.662 - 00:20:51.486, Speaker D: So take advantage of.
00:20:51.510 - 00:20:52.574, Speaker B: Yeah, that's a really.
00:20:52.694 - 00:20:57.478, Speaker D: And not have them steal the people you're paying to train, but rather have them pay to provide.
00:20:57.566 - 00:21:06.994, Speaker B: So let me just put it, let's use we instead of you in the john.
00:21:07.534 - 00:22:38.324, Speaker A: So without trying to make this sufficiently too broad or too broad, but sufficiently focused, there's also efforts like this going on in the reproducible science domain. Now, these are very similar structure, not quite the same problem, but they come together in many of the social science disciplines, with discipline journals being unwilling to entertain submissions that can't be verified in some reasonable way. So combining efforts to, to do privacy enhanced data analysis with efforts to document reproducible code streams lets you partner with more agencies like ICPSR is a good example. You've already done one with dataverse parts. The reproducible science parts share a lot of the infrastructure that you talked about here, that offering up differentially private analysis routines or formerly private analysis routines, and possibly a policy where general referees are allowed this budget for examining the data that they're. That opens up a lot of possibilities that could bring in economics, demography, political science as partners in this effort, too.
00:22:39.344 - 00:22:58.604, Speaker B: Great idea. If we think about which of those we should be trying to at least be compatible with, if not share infrastructure with, with an effort like this. All right, so I try to thank Rachel, then Thomas and Deirdre, then Adam.
00:23:01.144 - 00:23:14.164, Speaker D: So I know that you have been working towards these goals for a while through the privacy tools project at M. Harvard. Can you talk about how much progress you have made so far versus how much of this is left to be done?
00:23:16.024 - 00:24:14.274, Speaker B: Yeah, so we, James talked about kind of where we are on Monday, and we really have kept our while. A lot of this stuff about eventually having an open source library were things that we always talked about as the long term goal. Our main focus has just been trying to make progress towards integration with dataverse, and there's a lot here that very small suite of differentially private algorithms. Most of the effort has been on what is the wrapping, the interface and the backend that's going to be needed for it to be able to integrate with, with dataverse. But we've been doing it on a shoestring in terms of the number of people. So it's certainly a lot of thinking in this direction and a small amount of progress. But I think here this is something of a different magnitude.
00:24:14.274 - 00:24:22.874, Speaker B: I mean, I hope the work we've been doing will be a good, will help in this sort of effort.
00:24:24.294 - 00:24:52.204, Speaker A: So there are people publishing papers on differential privacy around the world. They have code, but it's all standalone code. So I think we need to think about how to incentivize those people to integrate their code into the system. And one suggestion that I have is that there should be some benchmarks in the system so that then one of the benefits that they get is they integrate their code, they can run it on the benchmarks, they can compare to what everyone else has done, and that will help them publish their paper rather than having a standalone system as long.
00:24:55.444 - 00:24:59.264, Speaker B: Great, Deirdre.
00:25:00.204 - 00:26:10.960, Speaker D: So I'm not sure whether or not I feel like there's some danger in suggesting that there's a bunch of tools and you can take them and go use them, given having spent the week here too, the complexity and the level of detail with which you have to understand your data, what sort questions you want to ask, all these things. And I've been concerned about some of the kind of, you know, the machine learning as a tool. Oh, yeah, just come and use our algorithms and you'll get answers. And if you put this out there as a tool, I feel like you might actually encourage people to do things where they're like, oh, we're doing differential privacy, we're doing a great job, and they're actually doing a really crappy implementation. And so I feel like the, you know, maybe it's differential privacy as there's a suite of tools, but it's a service that there are. I think you would want the expertise, or at the very least, you would want a lot of documentation about what are the questions you need to answer. It's like the intake form, because I think the truth of the matter is that a lot of people don't understand.
00:26:10.960 - 00:26:26.208, Speaker D: And the way the folks who've been here for the past week talking about their struggles with doing implementations, people don't even know the right questions to ask. And so I would be very worried about making this, like, off the shelf. Come grab some tools.
00:26:26.256 - 00:26:41.892, Speaker B: Yeah, yeah, that's a really good point in terms of how this is made easy to use correctly and to make sure that people don't have disasters with it of different from the choice of Epsilon.
00:26:41.948 - 00:26:53.504, Speaker D: And Cynthia and I have been doing. What are the other key questions that one would need to understand so that we could understand anything about your implementation or you could understand anything about your implementation.
00:26:54.444 - 00:27:02.984, Speaker B: Great. Thank you. So let me just try to. Adam, Marco, Frakka, Ashwin. We'll go from a circle like that.
00:27:06.104 - 00:28:00.968, Speaker C: It probably makes sense to identify a smallish number of open source projects that have been successful, regardless of domain, and see what makes them tick and what has worked and not work. Who the contributors are. I don't think actually there are that many open source, big open source projects that are where most of the contributors are, like research oriented academics. Okay, insert your favorite joke about radware and whatever. So we could think about what the model is. And one specific example I think that's important are the various crypto libraries. And this sort of comes both to Deirdre's point and Katrina's point about model, about the model, the economic model, which is that, first of all, with crypto libraries, it's not expected that you can use these as like a total novice.
00:28:00.968 - 00:28:43.784, Speaker C: I mean, they're relatively simple to use, but, you know, they do come with various warnings that the form handled with care. And then, if I understand correctly, who's actually writing that software, and I might be wrong, a lot of the contributions come from actual companies and stuff that are using the software, and they contribute code to the project because they see sort of a real benefit in having a common code base that they can point to, that they're using the thing that everybody else uses. Like there's a small number of places where there's a real benefit to that. Privacy, security.
00:28:44.444 - 00:28:45.292, Speaker A: Yeah, that's a great point.
00:28:45.308 - 00:28:48.544, Speaker B: It fits really well with the model that Katrina was suggesting, too.
00:28:48.964 - 00:29:05.904, Speaker C: Yeah, I mean, these are all important questions. And certainly crypto is harder in the sense that picking security parameters in crypto is a much simpler, as thorny as it is, is a much simpler problem than picking parameters for privacy.
00:29:08.964 - 00:29:09.324, Speaker B: Yeah.
00:29:09.364 - 00:30:06.080, Speaker E: So maybe this is not well aligned with what Adam said. And I point to the more verification manual versus automated verification. But one what I think here we have a good occasion in the sense that the community is starting somehow something new. So integrating some verification would be good from the start. And I think we don't have to commit to either manual or automated. We can have a combination where when there is the manual vetting, we have some support for the people that do the manual vetting. So something of this kind, I think it would be a good help for them and a good occasion for the community to have something that is a bit more reliable than just few people checking.
00:30:06.202 - 00:30:06.884, Speaker A: Right, right.
00:30:06.924 - 00:30:09.984, Speaker B: Think of it like almost like a proof assistant. Yeah.
00:30:12.724 - 00:31:18.688, Speaker F: Yeah. Two follow up comments to what John said and what I think the replication community idea is great from the perspective of the IAB, the institute, we often have this problem that papers published on the administrative data, we can never, I mean, to certain journals, you can't send them, or you have trouble convincing the editor that there is a procedure with which you can replicate in house, but because it takes so long to get the approval and replication itself wouldn't be allowed in many cases because it doesn't add anything new. So there, it really would be helpful. And I wonder if those tools even have the chance to be then very simple, because the models are already known, the variables are already known, and you only need enough precision to get to the right conclusion. You don't even need to necessarily get the right, the same coefficients. I think having something simple there that would help. This is out of the large admin data set that you were working with.
00:31:18.688 - 00:32:00.834, Speaker F: A way to create a subsample or to basically dp the sample that you work with would be a huge service to all of those using these kind of data. And then I follow up to what Deirdre said. The American association for Public Opinion Research, they have this transparency initiative, and there's sort of a seal for data collection that follows a set of steps that if done, I mean, at least you know what's going on there, and all of these decisions point, and that might be a model to think through. What would one need to know so that they are not saying, oh, I employed the psy DP algorithm and that's it. But that you really have sort of a framework. So that could be a good template to look at.
00:32:01.134 - 00:32:26.370, Speaker A: Thank you, Ashwin. Thank you. Things that I wanted to say. So I agree with Deidre, I agree with Adam. I wanted to sort of point out one key difference between crypto. So there's a big, there's a lot of commonality between crypto and privacy. And so I think sort of trying to take some of the lessons learned from crypto here would be very, very useful.
00:32:26.370 - 00:33:21.474, Speaker A: But there is a big difference between the two communities, too. You would like to go, you would like to start off with use cases where we know we might be successful, which are cases where you have lots of cases, but lots of data means you must have a system that can be architected to handle that. Lots of data, which you actually mentioned. So definitely this effort has to engage people who know how to handle archaeological data. I think that is very important. People building things like Spark and who will have experience building, handling, doing data analytics is very, very important. And the second point I want to raise is, I think to Deirdre's point, thinking about how we can teach people to do analysis under privacy constraints is an important thing.
00:33:21.474 - 00:33:38.324, Speaker A: And one thing you want to think about in this effort is really what kind of education we can bring in. How do we not only just releasing code, but also education. Right. So how can we educate people to think about data analysis under these?
00:33:41.264 - 00:33:53.376, Speaker B: Yeah. So not just documentation for the data custodians, but for the analysts who are going to be using these systems as well. All right, so I think I saw Ohlone and Jordan.
00:33:53.520 - 00:34:06.634, Speaker G: The comments about replication made me think about there's discussion in the machine learning world about over dependence on.
00:34:08.934 - 00:34:09.782, Speaker B: Benchmark data.
00:34:09.838 - 00:34:39.543, Speaker G: Sets and how the only thing that matters is you get your extra 0.1% in accuracy. Right. And people in that world recognize that that's a problem. So maybe just throwing out an idea, like what if we got a company, somebody like Google, to produce a new benchmark data set for machine learning models, some huge voice data set or image recognition data set.
00:34:42.443 - 00:34:44.063, Speaker D: They thought it was a good idea.
00:34:46.483 - 00:34:47.235, Speaker B: They heard you.
00:34:47.259 - 00:34:47.863, Speaker A: Yes.
00:34:48.963 - 00:35:22.042, Speaker G: Try to tackle a few problems at once. Have this behind, like use this as a holdout behind differential privacy, which gets at some of this over dependence on these benchmarks. If it's the new big data set that everybody will be testing against, and ML researchers will maybe be motivated to use it, their reproducibility concerns might come along with statistical validity connection.
00:35:22.138 - 00:35:32.026, Speaker B: So you have in mind the connection between differential privacy and false discovery when you talk about the holdout behind.
00:35:32.210 - 00:35:46.074, Speaker G: Exactly. And having buy in from some people in the machine learning community to use this to solve their problem, too, which is this overfitting and over reliance on benchmarks.
00:35:46.494 - 00:36:22.064, Speaker B: Yeah, certainly. I've heard in conversations from, for example, the social scientists that one of the appeals of such a thing could be this. Like, if you can tell people that they also won't make false discoveries if they use this, it will help. A question I have for those who have been thinking about the false discovery problems more is whether that theory is at the level of maturity where it's ready for to, you know, give people quantitative information about false discovery or not. I just haven't been tracking it closely enough. But it's certainly something that I think that holds appeal for potential users.
00:36:23.364 - 00:36:25.236, Speaker A: Yeah, I just wanted to follow up.
00:36:25.260 - 00:36:46.694, Speaker C: On what Ashwin said in terms of education. I think, I feel that now there's a stage now for DP where it's moving to the next level in some sense that it's. I mean, I can see that the term popping up everywhere. And I had even a venture capital firm asking me whether it's worth to.
00:36:46.734 - 00:36:48.958, Speaker A: Invest in the DP market now.
00:36:49.126 - 00:37:31.030, Speaker C: I mean, it's really becoming a general topic, but I feel that a lot of these people, they really don't understand what it's all about. And they still feel, it's like, you know, this is just the, with the panacea, the solution for everything, and not even understanding that you have still this trade off between utility and privacy, they just think you do that and then everything is protected and we are safe. And so I think if this should be applied on a broader level and it should be like a general tool, I think there is a lot of teaching still required in general, not even.
00:37:31.102 - 00:37:32.158, Speaker A: Only to the users.
00:37:32.206 - 00:37:41.154, Speaker B: But calibrating expectations is really important. We're not the next blockchain.
00:37:44.214 - 00:37:44.774, Speaker G: At one.
00:37:44.814 - 00:37:58.394, Speaker C: Point, I feel, because I see that. So, for example, in our AI strategy in Germany, our government all talks about the country privacy, but they don't really have an idea. And so I think that's an important aspect.
00:37:58.514 - 00:38:04.042, Speaker B: Thanks, Cynthia. And or so, just following up a.
00:38:04.058 - 00:38:06.094, Speaker D: Little bit on what Ohlone said.
00:38:08.314 - 00:38:08.674, Speaker A: I'll.
00:38:08.714 - 00:38:31.674, Speaker D: Add a friendly amendment of maybe not trying for the industrial data set, but the kinds of data sets for benchmarks that would come from census or the federal statistical agencies to prioritize that record in that direction. I had something else to go away.
00:38:37.334 - 00:39:52.482, Speaker A: To answer your very first bulletin, I think everyone here is very, very interested in the discussion, and they hear lots of really, really interesting ideas, including the service datasets and the need for speed. But I actually feel that we are not the right set of people to be asking about this. With no disrespect to Katrina's economy expertise, she doesn't really know how to build such a. It feels like a non profit company, basically, or a nonprofit service that maybe needed a governmental support issues and release of data on a government level so that it can reach every city in every small region, not for profit business owners or makers or I think to get these kind of things started. What do you need? We can come up with a bunch of ideas, but they're probably going to look really, really good on research papers. That's not necessarily true. I mean, the systems community has been building test beds, has been building things like plant lab, and then somebody has to start this effort, and then it gets taken over by a larger organization.
00:39:52.482 - 00:40:10.154, Speaker A: Right. I mean, the fact that you know this and I don't already points out to how illiquid at least I am, how to consume, construct such a not for profit business. And again, I'm pointing myself, but I don't think that I am alone with the lack of knowledge about.
00:40:11.694 - 00:41:02.794, Speaker C: Yeah, I mean, I think your point is aligned with what Ashwin said earlier, that, like, you have to make sure you have the right expertise in such a group. And it's not clear that we're representative of all the sets of expertise that are needed. But there are a number of other successful open source projects that at some point, some of the really big ones, I guess, did transition to like, sort of being their own little, like nonprofits. But I don't know, you know, I could name. There are a few that come to mind, but I don't know if, I don't know enough about them to know if they're really good examples. You know, r the language is an example, as horrible a programming language as it is. And other openssL and things like that are very successful, widely used.
00:41:02.794 - 00:41:08.434, Speaker C: I'm sure there are lots of failed examples, too.
00:41:09.854 - 00:41:11.434, Speaker B: All right, so any last.
00:41:13.294 - 00:41:13.710, Speaker A: Thoughts?
00:41:13.742 - 00:41:14.354, Speaker B: Yes.
00:41:14.694 - 00:41:48.504, Speaker A: So there are a lot more software developers in the world than Percy experts. So a little bit, to Karina's point, maybe this is an opportunity as well as a risk. And I think if you can get regular software developers and companies involved, it's a big education opportunity as well. So that now with that needs to come the education, but it's a way to get the world engaged in this, to bring real use cases. I think there's a lot to be said for getting the non academic community involved, and then you can get a lot of the heavy lifting, perhaps done by people outside the economic community.
00:41:53.404 - 00:42:17.524, Speaker B: Yeah, that's a really great idea. And I think also resonates with, I mean, Aura's comment, which certainly hits home for me. Oh, the idea was to really engage software developers, use this education in industry as kind of along the lines of what Katrina was suggesting. Use an opportunity to educate them, and then get a lot of the heavy lifting and code development to be done by professional developers in industry.
00:42:18.024 - 00:42:36.032, Speaker D: Can you formulate this as your, or our response to Zuckerberg, say, fund this? It just came out saying privacy is everywhere. Now let's see that you're serious. Can we talk about vetting? I think it's.
00:42:36.088 - 00:42:36.304, Speaker A: Sure.
00:42:36.344 - 00:42:39.124, Speaker B: I think we are. Are we out of time? Okay.
00:42:39.674 - 00:43:03.934, Speaker D: Okay. So I also, previous times when we've talked about this sort of thing, when even you have written about it, we've had this question of who maintain who, who are the custodians and how does it get determined, what gets included and excluded and so on. I don't have an idea for this. What do people think? What are the suggestions from the room?
00:43:06.134 - 00:43:41.474, Speaker G: So I think one interesting example in terms of, like, turning like, cutting edge science into working code are like linear algebra libraries. So if you look at the blas and Lapak like, behind these implementations, which have, like, lots of subtleties similar to the ones that you find in DP, in terms of, like, changing pivoting strategy will change your stability a lot. So on top of this code, there's documentation which has, like, math paper form, but with more detailed proofs than what you usually find in papers.
00:43:41.854 - 00:43:48.834, Speaker D: Who is going to take the responsibility of reading that and vetting that, even if they came with proof?
00:43:50.334 - 00:43:51.278, Speaker G: The community.
00:43:51.446 - 00:43:55.994, Speaker D: Well, I think we need to be way more specific and precise and have a plan.
00:43:56.294 - 00:44:03.514, Speaker G: Yeah, no, I'm just pointing out this as a standard that might be useful to look at and then maybe a doctor or.
00:44:03.674 - 00:44:16.922, Speaker A: Can you refine what you mean by the vetting? What vetting of what, precisely? Because there's a lot of things to be vetted. What concerns you the most? What concerns me the most concerns you the most in the vetting process is.
00:44:16.938 - 00:44:23.150, Speaker D: That the vetting is a huge effort and responsibility. And specifically, it's a responsibility overdate.
00:44:23.262 - 00:44:26.790, Speaker A: No, but vetting the users, vetting the code. Vetting the code.
00:44:26.862 - 00:44:27.598, Speaker C: Vetting the code.
00:44:27.646 - 00:44:28.414, Speaker A: Vetting the code.
00:44:28.494 - 00:44:29.154, Speaker B: Yeah.
00:44:29.814 - 00:44:51.042, Speaker C: One thing that helps a little bit is there aren't that many different differentially private algorithms that really get used heavily. And that is there are many different differentially private algorithms, but they rely on a relatively small number of trusted components. There's, you know, resolution with a few different distributions. There's the exponential mechanism.
00:44:51.218 - 00:45:03.258, Speaker B: This came out very clearly in Michael's talk this morning, and that you have this small number of components that you carefully verify and then some way of also verifying that they're used in a plan correctly.
00:45:03.346 - 00:45:18.848, Speaker C: One simple thing one could do, assuming funding were available, is to have a bug bounty program where if people who find mistakes get money, and not just money, academic credit, you know, they get to write papers if it's a really.
00:45:18.896 - 00:45:24.128, Speaker D: Interesting mistake, but somebody has to check their claims that they found to buy, well, money.
00:45:24.216 - 00:45:41.262, Speaker C: Yeah. Again, you know, that requires funding. It requires, as you say, people who are kind of. And as Salil said, you know, people who are dedicated to the project, at least on some time scale, long enough to maintain. Yeah.
00:45:41.278 - 00:46:25.734, Speaker A: So, I wanted to echo what Adam said. I think 90% of algorithms fit into a very narrow band of techniques, and maybe we could automate that checking and just restrict ourselves to dealing with the remaining 10% of more sophisticated techniques. The other thing I wanted to suggest was that although we can't automatically verify differential privacy, most bugs are extremely obvious. And maybe we could come up with a set of sanity checks where you could actually detect bugs if they were just, you know, someone added 1% of the noise that they were supposed to. Maybe we could actually take that like we have checking that you used the right version of the exit CCS desk paper and Daniel software.
00:46:28.274 - 00:47:06.894, Speaker B: So I think we're out of time. But as I said, this is a conversation. To be continued. I didn't hear anyone saying we shouldn't do this, which is. But what I would like also, I'm sure you'll all have more thoughts, do share them. And in particular, and to Orr's point, this thing cannot be all just relying on a complexity theorist who hasn't written code in 20 plus years. So people who are really interested in being engaged, not just once this thing is out there, but from the start in structuring it and helping, getting off the ground.
00:47:06.894 - 00:47:08.558, Speaker B: I really would love to hear from you.
00:47:08.686 - 00:47:09.814, Speaker A: Okay, let's. Thanks, Aline.
