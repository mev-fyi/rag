00:00:02.600 - 00:00:42.024, Speaker A: So today I will be talking about learning or complete latent variable models through a tensor power method. So this is based on joint work with Anima Anand Kumar and Majid Jenzamin, both from Uclan, and they are both here. So sorry for the long and complicated title. I guess I will start by explaining the first part of the title. So what do I mean by learning or complete latent variable model? So first, just a very brief recap. There are many different kinds of learning problems. The most popular one might be the supervised learning problem.
00:00:42.024 - 00:01:25.964, Speaker A: In this problem we are given data and labels. For example, we are given emails and we want to tell whether they are spam or not. The underlying assumption or belief is that there should be some simple classifier in a certain class, and this classifier is able to give correct labels to at least most of the data. And the goal of learning is just to learn this classifier. But in this talk, the learning we are going to focus on are mostly unsupervised learning. So in the unsupervised learning model, we are given just data. There are no labels.
00:01:25.964 - 00:02:10.080, Speaker A: The underlying assumption or belief we have is that these data should be generated by some probabilistic model. This model may have some unknown parameters. So the goal of learning is just after observing all these data, we want to learn all the model parameters. So of course these two kinds of learnings are also closely related. And you have heard from Sanjeev's talk this morning. In this talk I will focus on the unsupervised learning. So many times we call a model a latent variable model.
00:02:10.080 - 00:03:22.454, Speaker A: And what that means is in the model we have two kinds of variables. We have first latent variables, which anything latent will be the this white circle will have the color white, and we will also have the observed variables and they will always be represented by blue things. So of course, as the name suggests, we don't get to see the latent variables, we only observe variables. So the model will specify how the data is generated. And usually for latent variable models this comes from two steps. So in the first step, the model specify some simple distribution that we can sample the latent variable from this simple distribution. And then after we sample this latent variable condition on the value of the latent variable, the model also specifies what is the conditional distribution of the observed variables conditioned on the latent variable.
00:03:22.454 - 00:04:27.304, Speaker A: So it's very possible that maybe both the distribution on latent variables and the conditional distribution, both of these are very simple distributions. But because we are just seeing the observed variables it's possible the marginal distribution is actually much more complicated. And that is why we need to introduce these latent variables. And as in many other unsupervised learning tasks, the goal is to given many samples of the observed variables, we want to learn the model parameters. So, parameters both determine what is the distribution for the latent variables and also what is the conditional distribution, uh, for the observed variables. Uh, in many cases, we might also want to learn the latent versus the value of the latent variable after we learn the model. But that's, uh, called the inference problem, and, uh, we are not going to talk about that in this talk.
00:04:27.304 - 00:05:26.244, Speaker A: Uh, so let's look at some examples for latent variable models. So, the first example will be a mixture of Gaussians. So for mixture of Gaussians, we assume the data actually come from the mixture of k gaussians. And for simplicity, we will, in this talk, we will assume they are all spherical Gaussians, and we know the covariance of these Gaussians. So the data will be just points from these Gaussians. In this case, the model parameter will be the means and the weight of the Gaussians. By weight of a Gaussian, I mean, what is the fraction of samples that come from this particular Gaussian? And we can view this as a latent variable model, because we can think of latent variable as number from one to k.
00:05:26.244 - 00:06:13.934, Speaker A: And this number tell us which gaussian are we going to sample from. So, for each sample, we first choose the latent variable one to k according to the weights. Then we know which Gaussian we want to sample from, and after that we can sample observed variable condition on that Gaussian. So, as you see, the distribution for latent variable is just multinomial distribution. It's very simple. The conditional distribution is also just a gaussian, which is also very simple. But the marginal distribution on the observed variables are mixture of Gaussians, which is much more complicated than either one of them.
00:06:13.934 - 00:06:58.474, Speaker A: Well, so now the problem is, given samples of these data points, we want to learn the means and the width of the Gaussians. So this is what I mean by learning a latent variable model. And as you may recall in the title, I also have the word or complete. Overcomplete might mean different things for different models, but for this particular mixture of Gaussians model. By overcomplete, I just mean the number of Gaussians should be larger than the number of dimensions they are in. For example, in this case, we have three Gaussians in a two dimensional space. So this is one of our complete settings.
00:06:58.474 - 00:07:49.664, Speaker A: Okay, so, uh, another, uh, so, mixture of Gaussians problem. Uh, actually, we know how to, uh, so, empirically, people use, uh, expectation maximization algorithms to learn mixed Ralph Gaussians. But in general, we have no guarantee for whether EM algorithms converge to the two parameters. We actually know how to learn these Gaussians in many cases, but they are usually using some complicated algorithms. Actually, many of those algorithms are developed by people in the audience. So another interesting model is called the multibiew model. In this model, it's a bit more abstract, depending on.
00:07:49.664 - 00:08:39.404, Speaker A: So there are k classes of objects, and we will have three independent views of these objects. For example, you can think of these three views as video, audio and text. That's describing the same object. And the key assumption is these three views should really be independent conditioned on this latent variable. So, in this model, the latent variables are still a number from one to k that specifies what is the class of the object. Then, after we have this latent variable, we just choose an object and we have the three independent views of that. So, observed variables are these maybe video, audio and text.
00:08:39.404 - 00:08:48.731, Speaker A: So, um, so this model is not only interesting by itself, it's like you.
00:08:48.747 - 00:08:51.987, Speaker B: Got three samples of the same Gaussian. Is that like what?
00:08:52.075 - 00:09:09.637, Speaker A: Uh, right. Uh, you, you can think of it as, uh. Um, I have a hidden variable, and then these are three independent Gaussians, um, where they can be any other distributions. They don't have to be gaussian.
00:09:09.755 - 00:09:18.258, Speaker B: In the previous example, it would be that you get three independent Gaussian from the same Gaussian. So you get one choice between one and k. Then you get three samples of it instead of only one sample.
00:09:18.306 - 00:10:00.464, Speaker A: Yes, that's one of the cases, yes. Yeah. So this problem is not just interesting by itself. It's actually also this structure that given a latent variable condition, on that we have at least three independent wheels. This kind of structure actually happens a lot in latent variable models. In fact, many other models, including hidden Markov models or some community models, they can all be reduced to this multi view model. And what are the model parameters in this multi view model? Well, the model parameters will be the conditional expectations.
00:10:00.464 - 00:10:56.950, Speaker A: So let's assume all these fields are vectors in r to the d. Then, um, conditions on the hidden variable having value I, there will be an expected vector for this first view, and that vector will say it is equal to a. And we collect all these d dimensional vectors, a one, a two to a k as a matrix. So this a will be a d by k matrix, and its columns will be the conditional expectations of the first view. And similarly, we will have the conditional expectations of the second view and third view. So these are the model parameters, and also the mixing weights are also part of the model parameters. And again, we also have our complete multiview model.
00:10:56.950 - 00:11:52.144, Speaker A: And by that we mean the number of classes. This number kick should be larger than the number of dimensions, uh, d here. Well, in general, they can have different dimensions, but in this talk, we'll focus on the simpler case where they have the same dimension d. So, uh, in this work, we show that, uh, we can actually learn these kind of models, actually some, including some other models, even when they are over complete. So for a multi wheel or mixture of Gaussians model, if the component means are chosen at random, mixing weights are close to uniform. Then we show there's a simple polynomial time algorithm that learns all the component means with accuracy, k over d, when number of components is constant, factor larger than number of dimensions. So let me parse this more slowly.
00:11:52.144 - 00:12:46.888, Speaker A: So, our algorithm works for over complete models because we allow the number of components to be a constant factor larger than the number of dimension d. It can be any constant, but the running time will depend on this constant c. Also, a drawback of our algorithm is we do not really learn the true component means. Instead what we output. So these component means are actually random unit vectors. So what we output will be a vector that has l two distance within square root k over d to the true component means. And finally, we need to, we say here we assume the component means are random vectors, but that's actually just for simplicity.
00:12:46.888 - 00:13:35.836, Speaker A: We have a set of deterministic conditions. All those conditions are satisfied with high probability for random vectors. And here I just state random for simplicity. So, another part of our results says if we have a model when the variance is small, so this is easier to think about in the mixer of Gaussians case. So, if the variance of each sample, if the variance of these Gaussians is bounded by some constant, then we have a polynomial time algorithm that achieves the same accuracy. But in this case, we can handle much more over complete models. We don't need a number of component to be constant, only constant factor larger.
00:13:35.836 - 00:14:30.168, Speaker A: We can actually handle as long as k is much smaller than d two z 1.5. And notice that even in this case, the Gaussians are not really well separated. The distance between zero means is a constant, but their variance is also a constant. So they are separated but they are not as well separated as some of the previous algorithms require. And finally, we prove very tight sample complexity bonds for these algorithms. So we show, in order to get accuracy epsilon here we would need to use roughly order k over epsilon square samples up to poly log factors when the variance is bounded by a constant. And this is tied up to poly log factors.
00:14:30.168 - 00:15:38.164, Speaker A: Because let's do think about the mixture of Gaussians case, even if I tell you, for all the samples which Gaussian it came from. So in that case, how do you estimate the component means? Well, you will just take the average of all the samples that come from the same component, and even that procedure will have sample complexity, order k over epsilon squared. So in that sense, our algorithm has almost tight sample complexity. So how can we hope to prove these results? Well, as you can see, the title is learning these latent variable models through tensor power method. So I've shown you what are these latent variable models, and tensor is some high dimensional array. So how do we relate these models to tensors? Well, this is actually a very classical method called the method of moments. So basically this tensor will be some third order momentum for these latent variable models.
00:15:38.164 - 00:16:56.504, Speaker A: And what do I mean by a third order moment? Well, let's focus on this multi view model and start from the much simpler first moment. So the first moment of this model is just actually the expectations. So what is the expected vector of the first wheel, what is the expected second view, and what is the expected third view? And of course, we can estimate those expectations using the data. So this mu vector is something that we can observe, but it's also related to the model parameters that we don't know. For example, the expectation of the first wheel, based on our model assumptions, it's going to be equal to this matrix a times the weight vector w. Because remember, the I'th column of a is the conditional expectation of a when the hidden variable has value I, and this w I will be the probability that this hidden variable has value I. So when you multiply a and w, you should really get the expectation of the first view.
00:16:56.504 - 00:17:48.094, Speaker A: So similarly, we can define second order moment. So these will be covariance matrices. We can compute, say, the expectation of the first rule times the second wheel transposed, so that will be a matrix. And again, we can estimate this matrix from the samples, and it is related to all these model parameters in this way. Just believe me, this is the right formula. And similarly, we can do the same thing for b and c or a and c. We don't do it for a, because the coherence matrix for a is not really determined by the model assumptions.
00:17:48.094 - 00:19:13.484, Speaker A: And we also need the third order moment, which now is not a vector or a matrix. It has to be represented by a d by d by d tensor. And the I one I two I three component of this tensor will be the expectation of the I one's entry in the first wheel times I two's entry in the second wheel times I three's entry in the third wheel. And if you are familiar with tensor product, then this tensor is really the expectation of the tensor product of a, b and c of the three views. From now on, for simplicity, I will just drop the weights and assume all the weights are one. Uh, in that case, uh, this, uh, third order moment, the tensor will really be equal to the sum from I from one to k AI tensor bi tensor ci, because these AI bi cis are conditional expectations for these three views. Um, so then, uh, so method of moments is actually very classical tool in statistics.
00:19:13.484 - 00:20:11.834, Speaker A: It's introduced by Pearson in 1894. So it's really old. So generally there are two steps when we want to apply method of moments. So the first step, given these samples of these data, we will estimate these lower order moments. And then given these moments, we would like to find a set of model parameters that's consistent with observed moments, because all these moments are closely related to the model parameter. In fact, in our case, it will be polynomials in the model parameters. So hopefully, if we could solve that system of polynomial equations, it will tell us what is the set of model parameters that are consistent with the moments.
00:20:11.834 - 00:21:15.594, Speaker A: But before applying this method of moments, there are things that we need to worry about. So the first thing we need to worry about is an identifiability problem. So given all these moments, these expectations, is there a unique set of model parameters that generates a model that agrees with all these moments. If the solution to that problem is not even unique, then we cannot hope to learn the model parameters using just these moments. In fact, that's the reason why we need to use this tensor, because we can show if you just have the first order, the means and the second order covariances, it is not good enough to, uh, learn all the parameters. There can be many different models that generates the same first and second order moment. But the identifiability problem, uh, the solution will be unique once we have the tensor.
00:21:15.594 - 00:22:10.294, Speaker A: Uh, and then, uh, suppose the solution is unique. So next question we want to ask is algorithmic. So how do we find the model parameters if the solution is unique. This could be really hard in general, because in general it's trying to solve some system of polynomial equations. And of course that is hard. But the hope here is these moments will have some very nice structures and we can use that to design efficient algorithms. And finally, if we have an algorithm, a problem that's very important in practice is what is the sample complexity of this algorithm? Because in practice we can estimate all these moments, but we don't have infinitely many samples.
00:22:10.294 - 00:23:35.704, Speaker A: So the moments we get are only going to be accurate up to some epsilon arrow. So how many samples do we need until these moments are accurate enough for the algorithm to recover a reasonable set of parameters? So that is also something that's really important when we want to apply method of moments. So for now, let's first focus on the first two parts, identifiability and algorithm. Only towards the end, I will talk briefly about sample complexity. So in fact, for this particular mixer, for this particular multi viewer model, this identifiability and algorithm problem is quite well studied. Basically, if we have the third order tensor, uh, in order to get the model parameters, we would want to write this tensor as the sum of k rank one components. So this inner product, tensor product of AI, Bi and Ci, will be a rank one tensor, and this decomposition will be trying to decompose a tensor as some of rank one tensors.
00:23:35.704 - 00:24:49.124, Speaker A: This is usually called the CP decomposition in the related literatures, and the smallest case that you can do this decomposition is usually one definition of tensor rank. So people have been studying the CP decomposition since 1970s, and there are many results known. So for example, for identifiability, when does this tensor have a unique low rank decomposition? Well, this solution is unique whenever two k is smaller than three d minus two. When these components are in general positions, I will not define that. But so even when this tensor is over complete, when number of components k is larger than the dimension d, in many cases this tensor decomposition is still unique. And that's a famous result by Kruskal. We can get much stronger results for random components when these components AI vi cis are really random unit vectors.
00:24:49.124 - 00:25:43.034, Speaker A: This tensor decomposition is really unique when k is much smaller than d to z 1.5. So in general, we can even hope to solve this problem in the more over complete setting. But unfortunately, finding this best rank k decomposition is np hard in general, even for third order tensors. And in fact so is many other problems related to tensors. This is a result by Heller and limited in the algorithmic side for a long time. We only know how to solve this problem in the under complete case here. By under complete I mean the number of component k should be at most number of dimension d for all these components.
00:25:43.034 - 00:26:42.078, Speaker A: So there are many algorithms for that. For example, you can do simultaneous diagonalization, which appeared even early seventies. But in practice, people often use an algorithm called tensor power method, which is an analog of matrix power method that's used to find top singular vectors of matrices. So what does tensor power method do? It actually is an algorithm that's designed to find tensor decomposition in the orthogonal case. David, what does the CP stand for? Oh yes, so CP is an abbreviation. That's for two other abbreviations. So c stands for this candy comp, and p is some parafac, and both these, I don't know, what are those things? Yes.
00:26:42.078 - 00:27:22.382, Speaker A: So I think those two names come from these two papers independently, and they are around the same time. So that's why it's called CP decompositions. Yeah, so tensor power method, as I said, is analog to matrix power method, and it works in the orthogonal case. By orthogonal, I mean these components, AI, bi, ci, are orthogonal, to be more precise. I don't mean that AI should be orthogonal to bi. I mean AI's should be also known to ajs, bi should be also known to bjs. But there's no constraint on AI, bi and cis.
00:27:22.558 - 00:27:24.594, Speaker B: So you're always in the low rank situation.
00:27:25.414 - 00:28:15.284, Speaker A: Yes. So because they are in all orthogonal and we cannot have more than d orthogonal vectors, this has to be in the under complete or low rank case. So this algorithm, how does it work? Well, it first gets vectors. What, you can randomly pick vectors a hat and b hat, and then you do this tensor power update, which I'm not going to define what this means, but think of it as I have a vector. If I have a matrix, I can left multiply this vector by a matrix and that will give me a new vector. A similar thing is true for tensors, except because tensor is a d by d by d object. I need to multiply by two vectors to get a new vector.
00:28:15.284 - 00:29:00.504, Speaker A: And this is something we can compute, but it's actually also closely related to this tensor decomposition that we don't really know. So this is really equal to that, and let's come to that later. So now we compute this vector C hat. Once we get c hat, we will fix c hat and b hat, and try to update this vector a hat. So we will compute a hat as the tensor applied to the vector b hat and c hat. And similarly we will then fix a hat and c hat, and compute a new vector, b hat. So we do all these three steps iteratively until they converge.
00:29:00.504 - 00:29:36.416, Speaker A: So it's not hard to see that if a hat, b hat, c hat are equal to some of the components AI, bi and ci, that is actually a stable point of this iteration, because look at this formula, the AI's are orthogonal. So essentially, if a hat is equal to some AI, then the sum really has only one non zero term. And that nonzero term will give me ci. So AI, bi, Ci should really just be, be a stable point of this kind of updates fixed point, right?
00:29:36.480 - 00:29:40.244, Speaker B: Not stable fixed points. You're not saying that if I start nearby.
00:29:41.464 - 00:30:07.924, Speaker A: Yeah. So what I explained only proves that they are fixed points. They are in fact stable points. And we can show that. We can in fact show that these are the only stable points. There are other fixed points, but these are really the only stable points. And we can show if you start randomly, it usually converges in log D steps if you have the exact answer.
00:30:07.924 - 00:30:56.044, Speaker A: And that's known quite early. So in the work with Anema and Daniel, Sham and Matus, we show that this kind of algorithm works even under noise. So even if we are not given the exact answer, we get something that's close. This algorithm can still work. But all these analysis are really in the orthogonal case. And in general, for under complete case, we can do some linear transformation to reduce them to the orthogonal case. But in the over complete case, this is not really possible, because as you pointed out, in d dimensions, we cannot have more than d orthogonal vectors.
00:30:56.044 - 00:31:43.954, Speaker A: Then what we know about overcomplete tensor decompositions? Well, only recently we started to know algorithms that can proof the decompose over complete tensors. For example, last year there are a couple of results that show how to decompose, say, a general tensor of rank order d squared. So when k is order d squared using fifth order tensor. So Moses Scherekar is going to talk partly about this result in his Thursday talk. So if you are interested, go to that. A similar result is by santos and others. So many of the authors are in the audience.
00:31:43.954 - 00:32:49.358, Speaker A: So also if we have the exact tensor, actually, it's known that we can decompose a tensor in general position of rank order d squared using just fourth order tensor. So we don't really need to go to the fifth order, but both. So all these algorithms say. So these algorithms have sample complexity bounds. This algorithm we don't know. But all these algorithms require, although they require polynomially many samples, it is a large polynomial depending on the data. And also, sometimes we don't really want to use fourth and fifth order tensor, not just because using higher order tensor you would require more computation, but naively, if you have the multi view model, if you only have three wheels, then we can only construct a third order tensor.
00:32:49.358 - 00:33:27.312, Speaker A: In order to get a fifth order tensor, we really need to have five independent views, and that is sometimes hard to get. Yes, renormalize every step. Yes, you need to renormalize after all steps. Sorry, I just omitted that for simplicity. Yeah. So what happens in practice? Well, in practice, people rarely use these algorithms. People often use alternating leak squares, which I will explain later, or some variant of power method.
00:33:27.312 - 00:35:03.343, Speaker A: And they don't run these algorithms on higher order tensor, because running them on third order tensor is already quite expensive. So we only have proof that says power method works for under complete case, but actually it works reasonably well in practice, even when we have over complete tensors. Well, alternating these squares even work better in that case. And also empirically, we observe that these algorithms do not need too many samples, they do not need some huge polynomial depending on the dimension or the number of components. So as you see, there is still a large gap between what's known in theory and what we observe in practice, and that is the main motivation of our work. So what is our results when we restate them in the tensor decomposition language? Well, in tensor decomposition language, our results show that if there is a rank k tensor that looks like this, because this tensor is what we observe if we compute the expectation of the tensor product of the three wheels. So if these components AI bi cis are random unit vectors, and when the number of components is just a constant times larger than the dimension d, then there's a polynomial timing algorithm that finds all these components up to accuracy order square root k over d.
00:35:03.343 - 00:36:30.224, Speaker A: And in addition, if we don't just have this tensor instead of this tensor, we also have very good initialization vectors, in the sense that we have vectors that are constant close to AI, bi and ci, we can refine these approximate versions of a, b and c to accuracy order square root k over d. And this works for the more overcomplete setting. And finally, for the sample complexity bond. It will eventually follow from the fact that using order k over epsilon squared samples, we can estimate this particular tensor t with accuracy epsilon in tensor spectral norm, which I will define later. Is there some point at which if you have a, you can refine your estimate to be arbitrarily accurate? Yeah, so that's a very nice question we now know the answer should be yes. But since some upcoming work that I'm not able to talk about today, the difficulty is that if the vectors are random, then they're not quite orthogonal. And so the power method naively gives you.
00:36:30.224 - 00:37:16.582, Speaker A: So, yes. So all these statements are done using power method, but if you continue to use power method, there's no way that you can go beyond this accuracy. So what we actually need to do is after we get these k over d close vectors, we need to use some other algorithm in order for them to converge to the true components. But I'm not going to talk about that. So I'm a little confused. How does the power method find array k is bigger than d, right? So you just try to find all of them. So basically, let's look at this case.
00:37:16.582 - 00:38:07.164, Speaker A: Suppose we have these very nice initialization vectors. Then given these initialization vectors, we can try to run the power method and we can show it will converge to these vectors to accuracy. Root k over d. If we have k groups of such initialization vectors, we can find all these k components up to the same accuracy. Yeah, so as I said, in practice, people often use alternating least code instead of power method. So, alternating these squares is this algorithm that given the tensor t, it first randomly guess two parts of the components a and b. So here a and b are d by k matrices.
00:38:07.164 - 00:38:56.206, Speaker A: And then it tries to minimize over d by k matrices c such that this tensor t minus the, this rank k tensor, the frobbing its norm is minimized. Notice that what's inside is a linear function on this matrix ci. And what we are optimizing is a floppiness norm squared. So this is really a least squares problem. So we can find these optimal ci's efficiently. And then after we find the c I's, we again fix c and b and optimize our a. Then we fix a and c and optimize our b, and we alternate between all these steps.
00:38:56.206 - 00:40:05.524, Speaker A: That's why it is called alternating these squares. So this procedure, the problem is trying to optimize this highly non convex problem. If you look at this optimizing over a, b and c, then this is really highly non convex problem. Even if we just have a and b is already non convex. And essentially we don't know any convergence results unless these AI, bi and cis are like inverse polynomially close to the true components, and in fact much closer than what we guarantee the square root k already. So, uh, and uh, in fact, if we just assume our nice initialization vectors that are column wise, only, say a constant point one close to the true component. Then when you are solving this, uh, least squares problem for the matrix c, well, the usual way to do that, you need to do a matrix inversion that intrigues under just this assumption may not even have an inverse.
00:40:05.524 - 00:40:58.578, Speaker A: It might not be invertible. So even if it's invertible, it can be, it can have very bad condition number. And in that case, the algorithm is very unstable. So sometimes, in order to overcome these problems, people instead use alternating rank one update. So here the idea is I will not try to find the rank k decomposition in one shot. Instead, I will try to find a rank one approximation to the tensor, and hopefully this rank one approximation will be close to one of the components. So basically, given this tensor t, we get initially a hat and b hat, and we minimize the difference between t and this rank one component.
00:40:58.578 - 00:41:46.034, Speaker A: So in a sense, this is trying to find the best rank one approximation of the tensor. But of course, if we optimize or all a, b and c, it's still not convex. And then, similar to all the previous algorithms, this algorithm then alternates between finding a hat and b hat and c hat. The algorithm converges. And in terms of this alternating rank one update is really equivalent to a power method. The C hat that optimize this function is really, well, it's not necessarily equal. It's proportional to the tensor applied to a hat and b hat.
00:41:46.034 - 00:42:18.274, Speaker A: And we had the same expression as we had before. So that's why I say people often also use power method in practice. So, although this function is still not convex, at least we don't need to solve the u conditioned least squares problem. So it's often more stable than the alternating these squares. And we can hope to analyze this updating rule.
00:42:18.894 - 00:42:22.374, Speaker B: So when I'm saying this is equivalent, this under the orthogonality assumption.
00:42:22.414 - 00:42:44.524, Speaker A: No, this is under no assumption as long as your tensor. Well, it's really under no assumption. So optimal C is really equal to the tensor applied to this because, well, it takes some linear algebra to see that you can expand this.
00:42:45.624 - 00:42:50.164, Speaker C: It's the best rank one update. That's what power method is.
00:42:52.064 - 00:43:22.028, Speaker A: Yeah, it's like if I already have this direction, a hat and b hat, what is the most correlated direction to the tensor? And that is actually given by this tensor. No, we don't need. So, so this fact does not require orthogonality. So the C hat is proportional to t applied to this. It's just. Oh yes. So that's what we are trying to show.
00:43:22.028 - 00:44:20.614, Speaker A: So, before we show that for this algorithm to converge to the optimal solution, you need orthogonality. But now we are going to say, even though these are not orthogonal, it still kind of works in the sense that it converges to something that's very close to one of the components. If they are nearly random, is it easier to see that one of the vectors, one of the decomposition terms, is actually the best rank one approximation? Right. So that actually takes some proof, especially when k is very large, when k is like d to the 1.5. And yes, and the reason is really this t of a hat, b hat, c hat is some objective function, and that is maximized when you are close to one of the components. So that's something that you need to show.
00:44:21.714 - 00:44:34.104, Speaker C: I mean, essentially you need conditions like these random vectors. Spectral norm of the tensor is controlled, the spectral norm of the matrices are controlled. All these conditions lead to the.
00:44:34.844 - 00:45:47.226, Speaker A: Yeah. So how can we hope to analyze this power method iteration in the non orthogonal case? Well, the intuition is, hopefully, when these vectors, a hat and b hat, are close to the one of the components AI and bi, then this tensor applied to a hat and b hat, hopefully it is equal to where it's proportional to ci plus noise, and then we will show the noise is small. And why do we believe that's true? Well, we can write this tensor applied to a hat and b hat as the sum of two terms. So the first term only has to do with the, this particular component I. And as we assumed, these inner products are like larger than 0.9. So this is really a large constant times a vector ci, and then in the remaining terms we have sum of, or all the other components, something like that. So this will be the noise terminal.
00:45:47.226 - 00:46:34.934, Speaker A: And we can hope the noise term is small, because remember, these aj bjcjs are random vectors. So for a vector, a hat is really hard for it to be correlated. To have large inner product with all of these random vectors, that's not possible. And also for b, it's again very hard for it to have, uh, large inner product with all of them. So these, uh, coefficients should be somewhat small. And also these CJ's are random vectors, so hopefully they will not align with each other and there might be some cancellations going on. So that's intuitively why we believe this noise, noise term should be small.
00:46:34.934 - 00:47:25.648, Speaker A: And in order to prove that, we need to do some work. And in the end, we are going to be able to prove this key lemma that says if my initial vectors, a hat and b hat are delta close to AI, and bi, and this delta is small enough, then the C hat found by the algorithm. Well, basically t hat applied to a hat and b hat. And after normalization, it is less than two delta squared plus order k over d to the 1.5 times delta times order root k over d close to ci. And this is very good, because delta is small. So two delta squared is much smaller than delta over two.
00:47:25.648 - 00:48:47.734, Speaker A: Say this guy, by our assumption, is also very small. So this is also contracting. That's just this term that does not become smaller as we take more iterations. So basically, this lemma implies as long as delta is much larger than square root k over d, so the ci we find will be much closer to, sorry, the C hat we find will be much closer to CI than either of a hat and b hat. And when we iterate over this kind of update, we can hope the algorithm to converge in a log number of iterations. And when it converges, all these factors, a hat, b hat and c hat, will be order square root k over d close to this component AI, bi and ci. So now the only problem we need to solve is how do we really get these very nice initialization vectors? How do we get a hat and b hat that's constant closed to these components AI and bi? Well, there are many cases for this.
00:48:47.734 - 00:49:36.374, Speaker A: So in the low noise case, recall that this is the case when, say, for a mixture of Gaussians, the variance of Gaussian is a small constant. If it's a small constant, then we can just use the samples themselves. And these samples themselves are very close to the mean of the samples. If this variance is slightly larger, we can try to do average over a constant number of samples. So that's also in polynomial time. So in the low noise cases, really simple, we just use the samples. So there's actually an intermediate case, which we call the semi supervised setting.
00:49:36.374 - 00:50:22.544, Speaker A: In this setting, we are actually given the latent variables for some of the samples. Yeah. So in that case, for example, for mixture of gaussian, we know some samples come from the same distribution, sorry, come from the same gaussian. Then we can just take the sample mean of those samples and hopefully that will be close enough to the component min. And finally, what do we do for the unsupervised case? In this case, we can hope to do a single value decomposition over a random slice of this tensor. So this tensor applied to a random vector u. So this actually gives us a matrix.
00:50:22.544 - 00:51:35.504, Speaker A: So, I don't have time to explain why this works, but for intuition is if we pick a random vector u with some inverse polynomial probability. It will have very large inner product with one of these components c I's, and have smaller inner products with all the other c I s, which means this matrix we get will really have a very large singular vector that's close to the sea ice using the perturbation bonds on matrix singular values. And we can use that bond to get a good initialization pair a that's close to AI and bi. So finally, in the rest five minutes, I'll talk about, briefly about the. Oh, sorry. Oh, okay, sorry. So for the sample complexity, we can prove that with n samples, the estimated tensor is root k over n close.
00:51:35.504 - 00:53:10.624, Speaker A: And because the final accuracy is proportional to the spectral norm, we need k over epsilon squared samples. Tensor spectral norm is defined to be the tensor applied as a maximum over the unit vectors uv and w tensor applied to these unit vectors, similar to matrix spectral norm, but unlike that, it's np hard to compute, and it's not easy to prove tight sample complexity bonds for tensors, because the usual way of doing that is we can think of a d by d by d tensor as a d by d squared matrix, and maybe we can bond the matrix spectral norm of that matrix, and that's always larger than the tensor spectral norm. But this approach cannot work in here, because the matrix spectral norm is in this case is actually indeed larger than the tensor spectral norm. And another idea is you to use epsilon nets, but that also does not work as there are too many vectors. So in our work we use some more refined epsilon net arguments. We distinguish between uncorrelated vectors and correlated vectors, and then using this tradeoff, we will be able to get a tight sample complexity bonds. So in conclusion, we show that there's a tensor decomposition algorithm that works on over complete third order tensor has applications in learning latent variable models and have tight sample complexity bonds.
00:53:10.624 - 00:53:37.824, Speaker A: There are many open problems. So first we want to remove the arrow root k over d, which we now know how to do, but it's in an upcoming paper. So also we want to do a tensor decomposition. For this, k to the k is equal to d to the 1.5 without using good initial vectors. So this is somewhere when sum of squares could actually be useful. And we are thinking about that.
00:53:37.824 - 00:53:49.344, Speaker A: And finally, what can we say about tensors that do not have random components, but have components in general position? Thanks, and sorry for going over time.
