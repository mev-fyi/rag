00:00:00.200 - 00:00:12.686, Speaker A: So we're happy to have Nika continue her tutorial from yesterday. This is going to be going into journal some games, and journal statistics interactions and collaborative interactions. Thanks, Nika.
00:00:12.790 - 00:01:15.530, Speaker B: Okay, thank you for being here. Can I just ask, is there anybody in the room today who didn't manage to catch any part of the previous day's tutorial that I gave? It's okay if you can confess. Okay, that's good. Tonight, I just wanted to see how much of an overview I have to give. So, since everybody was here, we can actually dive right in. But let me just set the stage anyway to sort of remember what we were talking about, because yesterday, we dedicated most of, almost all of the time, actually, to thinking about interactions with an adversary. And that meant that we talked about online learning, min max optimization, kind of connections between them, and a little bit about what happens, like, what is so brittle about adversarial interactions and how we can sort of think it more about more socially realistic ways of thinking about adversaries.
00:01:15.530 - 00:02:23.510, Speaker B: And then we saw how classical things you've learned from theory of machine learning still stays very relevant today to the needs of beyond worst case analysis and learning to understanding games. And at the end, I also talked a little bit about computational aspects, but sort of in the interest of time, I didn't go into the details of it. So what I tried to do today is to actually have the remainder of tutorial be essentially independent of that computational aspect. So, just to be fair to you, because I didn't go through it. So don't worry, we are not going to go over it again, but you're also not going to really need to talk about computational aspects of the first part as much anymore. So what we are going to do today is think much more about general sum, as Desalu said, rather than thinking about sort of adversarial interactions, thinking about more general incentives and strategic structure. And then we're going to see something very specific, which is more about collaborative interactions.
00:02:23.510 - 00:02:51.554, Speaker B: And then I'll highlight, once we get to the end, as to some of the connections that might have not been as obvious, I'll just repeat them so that we kind of can bring them back in our minds. Okay. If you have any questions at any point, feel free to stop me. I really enjoyed that. Yesterday. We were having a conversation, essentially. That was very enjoyable for me, but I think it also helped a lot of other people in the room actually follow along better.
00:02:51.554 - 00:04:06.950, Speaker B: Cool. So, let's start kind of in terms of the structure of this section of the tutorial, which is going to be essentially this full hour and maybe a little bit of the next hour we're going to talk about general sum games, and then we're going to talk about some of the solution concepts in these general sum games, how you'd compute them, how you will learn about them, and also thinking about non myopic behavior, which is something we haven't yet talked about at all, but we'll get there. Okay, so let's start with general sum games. And if you recall from yesterday, we talked about zero sum games, and we thought that zero sum games were sort of, the point of them were to capture our interactions with an adversary. So whenever we were gaining, they were losing and vice versa. General sum games, relax that assumption that my gain is your loss or your gain is my loss, by saying that there are these more general aspects of how your utilities translate to mine. And you could think about them as more general strategic manipulations that are not necessarily malicious, but they're just type of interactions that somebody is just trying to optimize stuff in their own life and it still affects us.
00:04:06.950 - 00:05:29.758, Speaker B: So examples of this we talked about a very little bit at the very beginning of yesterday's session was in ride sharing apps where drivers or even riders try to manipulate whether or not they're available for a ride, whether or not they're going to accept a ride at this moment, in the hope that immediately, soon after, like in the next 15 minutes, in the next half an hour, the next day, they're going to get better rates. Another example we very briefly again talked about that's good to bring back in your mind is admissions, hiring, lending, a lot of other search type problems where we're essentially looking at some available representation of a person, their cv, their, any other information we have about them, and we're making decisions based on that. But the person has ability to change the CV, either changing the representation of it, or even maybe getting some qualification, doing some certificates, certificate courses. And again, they're not being malicious, they're not trying to mess up my learning algorithm. All they're trying to do is to actually get better deal out of the learning algorithm. And that's why this is more generally strategic rather than zero sum. Okay, so we're going to look at the same setup.
00:05:29.758 - 00:06:29.576, Speaker B: The only difference is now that we are not making any assumption about zero sum, which means that we're going to be talking about general utilities u one and u two. As before, if I use the capital of u one and u two, it just means that I'm accounting for expectations. And because we don't have the zero sum assumption, we also don't have the min max theorem. The min max theorem, if you remember, one way to say it was that it doesn't matter if you go first or last, it's all the same, and you're going to lose this very nice property. And what that means is that now it actually does matter who's going to go first. And an example of this is, just remember that we're again going to work with these mixed strategies where each player is going to pick a strategy that is a distribution over actions. And let's look at such an a matrix.
00:06:29.576 - 00:07:40.424, Speaker B: It's no longer zero sum. So if you were to be player one, and I was interested in a Nash equilibria of this game, what now a Nash equilibria would look like would say that, well, Nash equilibrium means that neither of the players can unilaterally improve their payoff by just changing their own strategy. And the interesting thing here is that this is actually quite simple to compute for this game, because player one has a dominant strategy. What that means is that for every cell, if no matter what pL2 has played, player one would rather play up, because it's getting more utility by playing up, regardless of what pL2 has done. So what this is saying is that, well, player one, in any Nash equilibria should be playing up. Now, what does the second pL2 second the player number two would do? Well, player number two knows that player number one has committed to this, essentially, because in any Nash equilibria, you have to be here. So player number two says that, okay, but I'm going to gain more utility by actually going left.
00:07:40.424 - 00:08:27.174, Speaker B: So this ul is going to be the only, the Nash equilibria, which happens to also be pure in this game. Note that in this setting, player one is getting plus one utility. So Nash equilibria, you can think about it as some kind of a simultaneous play, right? We both go with p and q, and this is the outcome we are going to get. Now, let's consider a setting where player one could actually verifiably commit to their strategy. What that looks like is that they actually pick a strategy and they somehow either shout it out, there's somebody who's holding them responsible after they have shouted out their strategy. There's no way that they're going to change it. Maybe there's a contract, there's no way to back out.
00:08:27.174 - 00:09:11.314, Speaker B: So now, if player one could commit in a verifiable way, now, what can happen is that let's say player one decided for whatever reason, to commit to playing down. What did pL2 do? PL2 is going to say, well, player one has already committed to being here. What I am going to do is to see what I'm going to gain and I'm going to. I should have changed it slightly so that it would have been a little bit higher. So I'm going to actually play right, because let's assume this was one plus a little bit. So I'm going to play right. And what happens here is that this is the outcome I'm going to see.
00:09:11.314 - 00:10:05.124, Speaker B: And player one is actually getting more utility. So player one benefited from the ability to commit. This is what it means to go first and commit in a verifiable way. In fact, player one could have committed to a slightly different strategy, a random mixed strategy, by saying that I'm going to go up almost half the time and I'm going to go down the other half the time, and this is still good enough for the pL2 to be encouraged to take column number r. And what happens is that player number one is going to be able to get 2.5 utility, which is the best player number one can do if it were to commit to a mixed strategy. So what is this saying? This saying? This is saying that there is a different solution concept, which is not Nash, it's not min max, it's not Maxmind.
00:10:05.124 - 00:10:58.254, Speaker B: That is what they should be paying attention to. And this is called the Stackelberg solution concept. What optimal solution, and sometimes I also refer to it as Stackelberg equilibrium, is that player one is going to take the position of a leader and pL2 is going to take the position of a follower. Dealer and followers are different. The leader has to commit to a mixed strategy P, and then the follower will best respond to this strategy that was announced. What is this best response? It's just the best action it's going to take given that P has been committed to now, it's a Sackleberg optimal solution. If the leader commits to the best mixed strategy, accounting for the fact that the follower is going to respond.
00:10:58.254 - 00:11:50.712, Speaker B: This is going to be shown this way. Oh, this should be p. So this is called a mixed Zackelberg equilibrium. If I don't call it mixed, just assume it's mixed by default. So what we showed in the previous slide is that there are some gains that Stackelberg equilibria for the first player has better payoff than any Nash equilibrium. In fact, what we can show is something very general. And this is also due to Bon, Stengel and Zemer from previous slide, which is that in any general sum game, the Stackelberg equilibrium's payoff to the first player is, at least weakly, better than any Nash equilibrium.
00:11:50.888 - 00:11:51.728, Speaker C: That's a question.
00:11:51.816 - 00:12:06.208, Speaker D: Yeah. Actually, the way that people usually imagine it is that, yeah, maybe there's a contract, or the environment itself is allowing for this.
00:12:06.296 - 00:12:09.088, Speaker C: So somebody's enforcing this, or the environment.
00:12:09.176 - 00:12:11.960, Speaker D: By itself is enforcing. Yeah, either a third party or just.
00:12:11.992 - 00:12:20.476, Speaker B: The channel that's there is enforcing it. So, let's say that what I do is that I actually play my chips and then I come back and then.
00:12:20.500 - 00:12:21.784, Speaker D: It gets realized.
00:12:27.724 - 00:12:37.028, Speaker C: And you're even thinking about a cryptographic way to enforce it, that this idea that they actually have to follow up is not so clear.
00:12:37.196 - 00:12:45.704, Speaker D: Yeah. Yeah. So I can give you two examples.
00:12:45.744 - 00:12:48.200, Speaker B: Of this where this is being used in practice.
00:12:48.392 - 00:12:49.960, Speaker D: I have slides about this, but maybe.
00:12:49.992 - 00:12:51.944, Speaker B: It'S also a good place to say this.
00:12:52.104 - 00:12:55.688, Speaker D: So, one place that this is used.
00:12:55.736 - 00:13:00.880, Speaker B: Especially in the mixed Stackelberg concept, is.
00:13:00.912 - 00:13:14.804, Speaker D: For physical security, not for digital security. And the idea is that for physical security, sort of your commitment is something like, almost like this petroling schedule that you have.
00:13:15.144 - 00:13:39.644, Speaker B: And then criminals or whatever, people who are trying to abuse and get around the security features that you have installed are the ones who sort of know this commitment because they've seen it, and then they're trying to get around it. There's another example, which is for a slightly different notion of Sackleberg equilibrium. I'll give that in a minute. Yeah.
00:13:45.264 - 00:13:55.804, Speaker A: I don't want to say that it is easier, but now it's like singularization. I have to define what the answer is.
00:13:56.264 - 00:14:40.232, Speaker D: So, let's see if that's a fair characterization or not. You're right that I can think about this function as just a function of p if I knew what the best response was. But to be fair, even in the min max, you were doing the same thing. This is a generalization of min max. So, in fact, if this was a zero sum game, the Stackelberg equilibrium payoff is the same as max, min min max and Nash equilibrium payoff. So this is not actually getting around the fact that it's not making the problem just a simple maximization of a concave function. You're right that it's a structured stuff, and we're going to see a little bit more about that structure.
00:14:40.328 - 00:14:51.720, Speaker A: So, can I ask something? So, I believe that a. Please correct me if I'm wrong. If I compute a nash that is a solution to the stack of, it's.
00:14:51.752 - 00:15:01.574, Speaker D: Not the optimal solution, sure, but it is. Yeah. So why is it a solution? So Nash requires stability on both sides, right?
00:15:01.614 - 00:15:07.078, Speaker B: So Nash requires that the player, the follower best response, and the leader also.
00:15:07.126 - 00:15:19.558, Speaker D: Best responds to follower cycle advantage, is that it's actually relaxing. The fact that the leader has to be best responsible. That's why exactly this result holds, because.
00:15:19.606 - 00:15:21.854, Speaker B: It'S a relaxation of notches.
00:15:22.584 - 00:16:27.826, Speaker D: And you're going to see that kind of like I think you were asking about computation, we're going to actually see this. The fact that now we are not really looking for a fixed point anymore is going to help us in terms of computation, but it's still a function of, it's still not a concave function that's easy to just maximize. Okay, so this is just, this is about mixed strategy Stackerbird optimal stacker equilibrium. Now, there's also a notion of pure strategy solution concepts for Stackerberg, and it's exactly the same as before. The only thing here is that now the leader has to commit to a pure strategy, and it has to commit to the best pure strategy knowing that it your followers want to best respond. So enforcing these actions are easier because there is nothing non deterministic is happening in the environment. And one thing about this is that because this is about pureness.
00:16:27.826 - 00:16:50.418, Speaker D: Now, it's unfair to compare a pure Stackelberg optimal solution to a mixed match equilibrium. In fact, if I were to commit it to a, compared to pure Nash, it would have been fine. But pure Nash is not guaranteed to exist, so you will lose the nice characterization of this. But let me say that in most.
00:16:50.466 - 00:16:52.826, Speaker B: Applications, when we actually talk about pure.
00:16:52.850 - 00:17:08.952, Speaker D: Stackelberg equilibrium, our environment is really, really structured in the sense that, in fact, you have sort of like convexity and concavity in the right ways, almost in.
00:17:08.968 - 00:18:09.374, Speaker B: An artificially right way, just so that you don't lose too much by talking about pure stack over this is not going to be the focus of my talk. I'll give some examples about it, but essentially I'm going to treat it the same as I would a mixed strategy equilibrium, because whenever we work with it, it happens to have a lot of nice structures that we want anyway. Okay, so kind of like what you would think about such a result about Stackelberg equilibrium. What is telling you is that commitment is actually good for you in the sense that if you can go first and commit, it's much better to check to take that option, rather than rely on something that's like a general Nash equilibrium. In terms of applications, I'm going to give you two applications. So this is the first one I was talking about the notion of physical security. The idea is that in security games, sophisticated attackers target the weakest point of a system.
00:18:09.374 - 00:18:58.244, Speaker B: And so what you want to do is that you're going to protect targets so that the targets that are higher value to you are not being attacked. Of course, what value you assign to a target is different than a value that an adversary might assign. And that's why this is not necessarily a zero sankey here. The leader or the defender has a set of resources, xs, and it can choose how to assign resources to different targets. And usually you actually want this to be random, because you want it to be unpredictable a little bit. So you will decide that such and such police force is going to be actually looking at such and such street or such and such neighborhood with some random schedule of events. The attacker here is strategy.
00:18:58.244 - 00:19:42.430, Speaker B: Set is a set of targets, and you want a new. Two are essentially utilities that depend only on whether the resources that you had in X were assigned to protecting the target. Y that was attacked. If it wasn't protected and it was an attack, you're not going to get any utility from it. This is an example of security games. It's actually something that, the specific model of security games that I mentioned to you has been used a lot in practice for stopping, actually poaching activities, for checking tickets on trains and buses in the LA area that you don't actually have to car. Then you sit in the train and you have your ticket.
00:19:42.430 - 00:20:11.324, Speaker B: Somebody has to come and check it. That's been used by coast Guard, by a lot of different security agencies. The other example is the example of strategic classification. Strategic classification. Here the decisions are. This is like thinking about, again, admissions, lending, hiring. The idea is that the decisions are based on some observable attributes, and people can actually change those attributes to some degree.
00:20:11.324 - 00:21:00.240, Speaker B: The learner here is the leader who's trying to come up with a classifier such that when a distribution of followers respond to, it is still a good classifier. Now, what does responding look like? Each follower has some initial representation, and they may have to pay a cost to change this a little bit. Maybe they're just changing their presentation of their cv. Maybe they're taking some certificate courses. And then what you want, and you two are capturing is either the accuracy of this hypothesis on the new representation versus the old representation. Maybe it's how much you encourage people to actually take certificate courses that are good for them rather than just manipulating their cv's. These are settings that we typically think about pure strategy.
00:21:00.240 - 00:21:28.676, Speaker B: But as I said, even the pure strategy setting is highly structured in these settings. In particular here, pure strategy itself is like a parameterized classifier. And if you are working with parameterized linear classifiers, you're already putting a lot of structure on your domain. So these are two examples of application domains. And next I'm going to talk about how you would now compute these equilibriums because there were already some questions about.
00:21:28.700 - 00:21:29.264, Speaker C: That.
00:21:44.044 - 00:21:47.624, Speaker A: Manipulated so much that there was inefficiency.
00:21:48.504 - 00:22:05.520, Speaker D: Yeah, so there are a lot of stories. There was a, there was an insurance company that was tracking I think maybe a Fitbit or like the number of steps that people were taking to set insurance rates. So then there were all these like.
00:22:05.552 - 00:22:09.184, Speaker B: Hamster wheels for your Fitbit that were.
00:22:09.264 - 00:22:12.444, Speaker D: Sort of counting it up and it was a very low cost manipulation.
00:22:12.784 - 00:22:14.524, Speaker B: That's one example of this.
00:22:14.904 - 00:22:18.002, Speaker D: But these have been used in a.
00:22:18.018 - 00:22:24.494, Speaker B: Lot of different places. There are some entertaining stories about them. So that's an example of this, but.
00:22:24.834 - 00:22:36.810, Speaker D: Maybe a more light hearted story of that. But there are definitely darker ones as well. I don't know if they were used by this.
00:22:36.922 - 00:22:40.666, Speaker B: I think they definitely changed their approaches.
00:22:40.770 - 00:22:42.218, Speaker D: I don't know if they actually use.
00:22:42.266 - 00:22:44.014, Speaker B: This type of robustness yet.
00:22:47.284 - 00:23:15.446, Speaker D: Yeah, I have to make it think better or not. People have actually used this after they have figured out that they messed something up. I don't have a good example to give you right now, but this has been used a lot to actually identify situations where people are doing these types of manipulations or where these types of manipulations are in particular bad for some equity or fairness aspects as well.
00:23:15.590 - 00:23:16.670, Speaker B: Those are definitely.
00:23:16.822 - 00:23:23.838, Speaker D: I would say there are still more scientific research. Rather than saying that some company changed their approaches using specifically strategic classification, at.
00:23:23.846 - 00:23:25.222, Speaker B: Least I don't know of it, but.
00:23:25.238 - 00:23:27.194, Speaker D: They'Ve definitely been used to analyze them.
00:23:28.134 - 00:23:41.682, Speaker A: So just one thing about the, how sequential you're doing this, this is still like at the same time you go something knowing that I went first.
00:23:41.818 - 00:23:42.722, Speaker D: No, no.
00:23:42.778 - 00:23:46.614, Speaker A: So I have to go first and shout it and then.
00:23:46.954 - 00:24:06.494, Speaker D: Yeah, or the history is enforcing it. I'll talk a little bit about this. But you have. So the best response is coming after your strategy is announced, right. Once you can, right now, think about that. This is being played only once. We're gonna come back and change that a little bit.
00:24:06.494 - 00:24:08.278, Speaker D: But then we talk about Sackler.
00:24:08.366 - 00:24:13.674, Speaker B: It's just a two round game. It's not an intimately many rounds of game.
00:24:14.854 - 00:24:21.022, Speaker D: And the idea is that if you know everything about u one and u two, you can just do this computation and this is telling you what's the.
00:24:21.038 - 00:24:22.726, Speaker B: Best policy to commit to.
00:24:22.830 - 00:24:34.554, Speaker D: And then you should never touch that policy. You just commit to it and that's it, you're done. That's the best you could do. But then if you didn't know, then there are questions about, oh, what am I going to do?
00:24:35.014 - 00:24:38.646, Speaker B: It seems like I need to maybe tune my strategy and is that going.
00:24:38.670 - 00:24:47.398, Speaker D: To mess up with all of this incentive structure? We'll talk about that sort of at the end of the section. Okay, other questions.
00:24:47.486 - 00:25:10.246, Speaker A: Yeah, just to be sure. The way that I see it is the following. I aim to commit to publicly to a strategy. So I'm thinking first, given that, what is the best response of the other player when I commit? And I will choose the best one that will give me the best for me.
00:25:10.350 - 00:25:22.394, Speaker D: Yeah, yeah, exactly. You sort of pre compute in your head this best response function. So then you sort of choose the best action for yourself accounting for the best response function.
00:25:23.454 - 00:25:23.870, Speaker C: Okay.
00:25:23.902 - 00:25:32.658, Speaker D: So I needed to change this to a presenter so that I can. Okay, so next we're going to talk.
00:25:32.706 - 00:26:00.894, Speaker B: About how it would compute. Stackelberg equilibrium. And an important aspect of this is to actually look at that utility function that if you wanted to just parameterize it just by your actions accounting for best response. What does this look like? Well, we need to compute this, right. This is the function that I only care about essentially. If I knew this best response, like the best response is computed this. But that's, I'm going to plug it in and I'm going to look at that u one.
00:26:00.894 - 00:26:45.436, Speaker B: So you could think about it this way. But let me tell you that in most cases this is not an advantage. This is not actually a simplification. Because if you look at this u one of p as a function of just p, you're going to end up with non convex, non concave, non lipshit functions that are so messy that you didn't actually get anything. There are very rare situations and I would say that partially like artificially curated sometimes to make them be almost lipscious and almost convex or concave. And there are some examples of this. If you want to look at where in strategic classification, what kind of classifiers in real life actually have those properties.
00:26:45.436 - 00:27:18.370, Speaker B: This paper has a really nice list of them. Generally these functions are not very well behaved. But what you can see is that even if they might not be well behaved over their full domain, they still appear to have piecewise nice properties and we can see them. Okay, so let's take a look at this again. So I want to compute a mixed tackleberg optimal solution. And I'm in any finite game. So this is exactly what I need to compute.
00:27:18.370 - 00:28:01.046, Speaker B: And there is this very general approach that says that actually this is like computing multiple linear programs. This is called the multiple LP approach. And the idea is that maybe this is complex if I just look at this function. But this best response function is going to take only y different values. Those are the columns in my game, actions of the adversary or actions of the follower. So let's fix each action and let's just look at my mixed strategies that would have been consistent with that best response. This set of, this is the set of strategies that are consistent with the best response.
00:28:01.046 - 00:28:58.304, Speaker B: Now what do I see? That set happens to be a polytope. It's a polytope because it's defined by these half spaces separating. When my strategy, sorry, the follower strategy, y is better than any other y prime. And remember that these capital letters are always linear functions. They're just telling you expectation of something. So you have two linear functions, you're thresholding them and you're taking an intersection of them, which means that if I just look at what kind of strategies would lead to a follower taking a specific type of action, then I am looking at the polytopes. Once I compute the polytopes, then all I need to do is to find the best solution in every polytope and take the best out of all of them.
00:28:58.304 - 00:29:53.432, Speaker B: It happens to be that not only these polytopes are linear, but the best thing that I'm trying to optimize, this u one of PMY that is also a linear function. Again, it's a capital letter meaning expectation, so that I can optimize easily with a linear program. So what this is saying is that you just run a linear program per column of the game and then take the best one, and this is going to be your optimal cycle berg equilibrium. How much time does it take? Well, it's going to be polynomial in the size of the game because I have to run an LP for each one, and the LP is in the dimension of my row space. Any questions about this can be a.
00:29:53.448 - 00:29:55.304, Speaker A: Big strategy, like maybe two.
00:30:00.124 - 00:30:05.548, Speaker D: Okay, so that's a good question. So I simplify this by talking about.
00:30:05.596 - 00:30:09.824, Speaker B: The player number two only playing pure strategies. And in general.
00:30:11.764 - 00:30:51.876, Speaker D: It'S okay to make this assumption because you assume that players will always can for any non deterministic strategy, there's a deterministic strategy that's getting just as much, so there's no reason to look beyond. But if they. So because of that, it doesn't actually affect it. The only thing that it affects is that it's actually changing the meaning of Stackerbird equilibrium. And typically the opt, like people like you might have a lot of Stackelburg, you might have the best response is a set, not necessarily a single. Exactly. It's like it could be a set of columns rather than a single column.
00:30:51.876 - 00:31:07.864, Speaker D: And you usually think about tie breaking in the worst or in the best case for the meter. And once you make whatever assumption you're making, you can adjust all of this. Let me tell you that it doesn't really matter. Essentially what's happening is that.
00:31:09.564 - 00:31:10.076, Speaker B: You have.
00:31:10.100 - 00:31:19.574, Speaker D: These different regions, like p one, p two, p three. What happens is that, let's say that this is something that looks like this.
00:31:19.614 - 00:31:25.142, Speaker B: Strategy that would have been optimal for one tie breaking. As long as I move it a.
00:31:25.158 - 00:31:49.074, Speaker D: Little bit in, it's approximately optimal and it's out of the tie usually. So it's essentially, you could really ignore these ties for now, in fact, for the duration of this tutorial. But they're interesting and you should take a look at it, especially when you want to think more about robust algorithm. So they come up again. Other questions.
00:31:53.974 - 00:32:09.154, Speaker A: Maybe I'm wrong, but I thought that this polygon might have exponentially point corners so that the linear program is not.
00:32:12.194 - 00:32:27.414, Speaker D: Well, you can talk about the separation oracle because it's a polytope with y number of hypothesis half spaces. Then you don't have to think about the corners, you can just think about the separation.
00:32:30.954 - 00:32:32.214, Speaker B: Other questions.
00:32:35.454 - 00:32:44.914, Speaker A: One question I would like to understand. Do we have right now assumption about the u one if it is concave or not?
00:32:47.934 - 00:32:56.334, Speaker D: Okay, remember, whenever we have capital letters here, they mean expectation. So we actually have linearity for that.
00:32:56.374 - 00:32:57.674, Speaker A: Okay. Okay. Okay.
00:32:58.174 - 00:33:05.666, Speaker D: Yeah. So that's the advantage of talking about unique strategies and finite games.
00:33:05.810 - 00:33:07.642, Speaker B: I don't need to make any additional assumption.
00:33:07.698 - 00:33:11.894, Speaker D: Finite games by itself is strong enough assumption to do.
00:33:13.994 - 00:33:14.854, Speaker C: Okay.
00:33:16.034 - 00:33:49.694, Speaker D: So this is also a minute to take a pause and notice this, that this is much easier than that simple relaxation on one side. The fact that I don't have to find a fixed point is making this essentially polynomial in the size of the game. Whereas we know in general, in general, some natural equilibrium is hard to compute. So not only this gives you a better utility for the leader, but also it actually allows the leader to compute it more effectively.
00:33:54.094 - 00:34:25.350, Speaker B: What is an example of this? Multiple LP approach. I want us to maybe understand a little bit better. I'm going to give an example from the security games, just so that you also maybe gain some appreciation of what security games and practice somewhat look like. So this is part of green security games. These are security games that are used to stop poaching or to limit it. And you can think about it, that there are two types of potential targets you're trying to to protect. You have elephants and gorillas.
00:34:25.350 - 00:34:59.404, Speaker B: They're both close to extinction, so you want to protect them. And you have maybe sort of one resource that could be sent to one or the other. Now, these two targets are slightly different in terms of how you value them and how a follower values them. Elephants have ivory tusks, so they're very expensive. Attackers would love to get their hands on them. On the other hand, gorillas are closer to extinction, so you really don't want them to be attacked. So this is why it's not necessarily a general sangay.
00:34:59.404 - 00:35:32.714, Speaker B: You also have two resources. One, it's a little bit of a oversimplification. You can decide which of these two areas you're going to send your resource. It turns out that what you will do in an optimal cycle burglarium is to use almost one third on the gorillas, almost two thirds on the elephants. You do care about gorillas more, and the attackers care about them less. So you can get away with actually a little bit less protection than half and half. And that's why one three is optimal.
00:35:32.714 - 00:36:05.262, Speaker B: Now, what does it look like in terms of the optimization? This is the face of the optimization. I should have drawn it on a line. But two, decimplix is a little bit easier to see. So any point here is a probability of how much you're protecting elephants and how much you're protecting the gorillas. And it's divided into two parts. How does these, the two parts are defined? Is that constraints on the followers utilities? These are the lines. This line says that I prefer to attack the elephants over attacking gorillas.
00:36:05.262 - 00:36:39.334, Speaker B: And they are written by the difference and by the comparison of the expected utility of the second player. Now, once you are within a region, you're looking at the objective, which is the leader's objective. Again, it's a linear function, it's the expectation. And now it's ignoring player number two. It's just player number one. So that's what linear program would look like. We take each of these regions, those are your pfizer, and for each of them, it will optimize this linear function.
00:36:39.334 - 00:36:41.534, Speaker B: Any questions about this?
00:36:48.574 - 00:36:50.394, Speaker A: How do you decide who goes first?
00:36:51.254 - 00:37:02.566, Speaker D: Good. In all of these stack over games, the idea is that the person can set a policy is the person who's going first. So that's why protection policies, admission policies.
00:37:02.710 - 00:37:05.006, Speaker B: Those are usually long term and they're.
00:37:05.030 - 00:37:07.430, Speaker D: Sort of verifiably somebody sticking to them.
00:37:07.462 - 00:37:08.980, Speaker B: Because they have put a lot of.
00:37:09.012 - 00:37:19.784, Speaker D: Money or resources into making them happen. That's why there is a difference in power. So usually these rules are not inversible.
00:37:20.204 - 00:37:22.252, Speaker B: Unless you actually don't have a hierarchy.
00:37:22.268 - 00:37:24.384, Speaker D: Of power, which then messes things up.
00:37:24.764 - 00:37:29.772, Speaker B: So. Good point. I'll talk a little bit towards the end, sort of algorithmic perspective on who.
00:37:29.788 - 00:37:40.656, Speaker D: Is committing and who's not committing is itself an interesting way of dealing with. Okay, so here really one thing we don't want to forget is that we.
00:37:40.680 - 00:37:41.984, Speaker B: Want to thinking, we want to think.
00:37:42.024 - 00:37:53.244, Speaker D: About these general sum games and especially a Stackelberg solution and commitment in them, because they're both easier and better. So whenever we can commit, we should really use that.
00:37:54.464 - 00:38:58.820, Speaker B: Okay, we talked about computing these tackleberg equilibria, but the computation needed us to know the game matrix. The question is, how much information do we actually need to know? And in realistic situations, do we have that information or not? Okay, so in the realistic situations, what is it typically that we do know, and what is it that we don't know? Well, for this specific scenario of general sum games, and let's think about security, I may know how much me as the leader, values the different targets. I might know how close gorillas are to extinction. I might know how close elephants are to extinction. That's the my half of the matrix that you want. The thing that's harder to know is what is you two. Because even in the security games, even though I even know about the ivory task being valuable, it also really depends on what resources are available to the adversary and to the attackers, and many other things like where are they sleeping at night, what are, how close they are to any of these targets.
00:38:58.820 - 00:39:57.180, Speaker B: So that is u two, and I do not know u two. Sometimes you can do market research for certain things to figure this out, but usually it's quite sensitive. But the thing that we are hopefully able to observe, at least that's a simplifying assumption, is that even if you don't know u two, you can always observe what people have done. You can see what they've done in the past, what targets they've attacked, what targets they're going to attack right now. So that's the actual action of the follower. Now what you're going to really be thinking about here is if I'm trying to think about learning Fakkerberg optimal strategy, what I'm trying to do is to compute this function without actually knowing the py. Because u one, I know, I just don't know the optimization region because the optimization region was the thing that really required me knowing what u two is.
00:39:57.180 - 00:40:35.724, Speaker B: That's the polytopic. So how can I do this? How can I optimize a linear function over an unknown polytope? That's the gist of the problem here really. If you could do it, then you could do the multiple lp approach that I was recommending and put it all together. Any questions about this? So I wrote this in this very specific form of polytopes. There would have been many different ways of solving this because the answer essentially going to come out of this representation.
00:40:35.884 - 00:40:37.544, Speaker A: So can you do.
00:40:40.244 - 00:41:12.476, Speaker D: Can I do ellipsoid? So ellipsoid technically needs a separation oracle. So that's a good question. So if I give you a separate, if I give you an oracle, which is just about Br, can you get separation oracle? That's certainly something you could do. In fact, there's more generally this thing is called a membership query. Let's say that for every, I have a polytope, I don't know a separation.
00:41:12.540 - 00:41:15.052, Speaker B: I cannot for any point, I cannot separate it.
00:41:15.228 - 00:41:16.860, Speaker D: But if you put something in it.
00:41:16.892 - 00:41:21.584, Speaker B: I can tell you whether it's in or out. If you put something out, I will tell you that it's out.
00:41:22.084 - 00:41:27.644, Speaker D: So there are definitely ways to reconstruct separation from membership, but you can even.
00:41:27.684 - 00:41:31.304, Speaker B: Do it more directly here. I'm actually going to cite.
00:41:36.184 - 00:41:39.004, Speaker D: I'm going to actually give you an example of.
00:41:40.064 - 00:42:33.138, Speaker B: A very nice work that actually covers all of the questions that you have. How can I go from one form of oracle to another? How can I optimize with a lower form of oracle? I'll do that in the next slide. But before I do that, I just wanted to tell you what this looks like because I somewhat jumped a little bit to membership queries. This BR, how am I going to use it? Essentially I'm starting with not knowing anything, and BR is going to tell me that this is going to be in the elephant region and at least this is going to tell me a little bit about the area. And what I have to do is to decide how I'm going to like search the rest of this. You could think that it's almost like a binary search, but on a larger dimensional space that's a little bit more complicated than that. In fact, you want one algorithm that can use this is like simulated annealing.
00:42:33.138 - 00:43:10.424, Speaker B: There are a bunch of other algorithms. It's independent, and we're not going to talk about it in the tutorial, but you can read about these. The optimization from membership oracles with membership queries is well studied. Really interesting branch of optimization we've known even before Kalayama Zero five. We have known algorithms for this problem, but the bound I gave you is from the 2018 paper. So we could do these. We could do this quite effectively, actually.
00:43:10.424 - 00:43:48.314, Speaker B: Now that we know how to solve an LP, we can put it back in the wrapper that I was suggesting to you and put it in the linear program formulation and just solve these problems. That's going to give you, for any finite game, an approach for learning the Stackelberg equilibrium. Now, the number of queries is going to depend polynomially on the number of the rows and number of the columns. There are other ways to do this. For certain games, you can do much better. I'm not going to go into the details of them. For security games in particular, you can do much better.
00:43:48.314 - 00:43:58.090, Speaker B: For certain other games. You don't need to depend on the size of the game, but you can depend on certain structures in the game. There are also other approaches that can be used here.
00:43:58.122 - 00:43:59.466, Speaker D: Yeah, I always have to think of.
00:43:59.490 - 00:44:01.374, Speaker C: How this stuff gets implemented.
00:44:12.754 - 00:44:42.874, Speaker D: Okay, it's a good question, because it's an oversimplification in its simplest form. So what I'm technically suggesting to you is to sacrifice the five borlas that you have left, because, what am I saying? I'm saying you're going to start knowing nothing. You're going to start knowing nothing. Knowing nothing. And then you're going to actually add random to some point here. This is the point.
00:44:43.234 - 00:45:32.614, Speaker B: And then you might see that actually this point is quite suboptimal, and maybe you even know and suspect that it's quite suboptimal. And then you see an elephant getting attacked, and then the next time you're like, okay, I'm going to move this, and the next time your gorilla is getting attacked. And technically, if you do this before you can figure out a good strategy, your whole population of elephants and gorillas are dead. So this is not the way you should put this in practice. Nevertheless, this is actually used to inform practical decisions. It's not directly. The multiple LP approach I gave you is more of a scientific exercise, but these learnings, slightly safer ways of learning, is what you would do, especially in low value, when to check tickets.
00:45:34.234 - 00:45:52.938, Speaker C: If I think about this, all these patients, and I think of this as data rather than attack or not, but maybe more about the value of data. And depending on that, would they take an action then you could have some kind of maybe multiparty computation, or you.
00:45:52.946 - 00:45:53.654, Speaker A: Could have.
00:45:56.154 - 00:45:59.546, Speaker C: Seen it, but without, you know what I'm saying? So there might be ways to.
00:45:59.570 - 00:46:01.176, Speaker D: I'm not sure if I caught that thing.
00:46:01.280 - 00:46:07.284, Speaker C: So you want to know what would have happened if you would have done certain actions.
00:46:26.884 - 00:46:49.388, Speaker D: So I'm thinking that. So, but who are the parties? Are you asking the adversary to do some computational for you?
00:46:49.476 - 00:46:51.116, Speaker B: Is that how I should think about that?
00:46:51.220 - 00:46:57.794, Speaker C: No, I keep away from two parties that everybody. And even though it's.
00:47:02.014 - 00:47:03.034, Speaker A: Accessible.
00:47:12.854 - 00:47:13.630, Speaker B: I'm not.
00:47:13.662 - 00:47:16.274, Speaker D: Sure if I understand the role of crypto here.
00:47:17.454 - 00:47:19.126, Speaker C: I don't understand what you get, because.
00:47:19.150 - 00:47:22.502, Speaker D: You'Re saying you're going to make an activity, you're not going to make an.
00:47:22.518 - 00:47:31.078, Speaker C: Action, because then the monkeys or whatever they are, the gorillas, are going to get attacked. I understand. So you have to think of other ways to realize it.
00:47:31.166 - 00:47:32.502, Speaker D: I see that nonetheless.
00:47:32.558 - 00:47:34.502, Speaker C: Indeed, people have ways to realize it.
00:47:34.518 - 00:47:48.884, Speaker D: And then they get things. So the way I would fix this, I still have to think. I'm still not sure how I would think about role of crypto, because this is another way to think about this, is that this is about lack of information. Right.
00:47:48.924 - 00:47:50.904, Speaker C: So what I'm saying is, if.
00:47:53.084 - 00:47:53.436, Speaker D: I'm.
00:47:53.460 - 00:48:01.060, Speaker C: Saying that instead of doing it, instead of taking actions and then seeing responses, you could sort of do a simulation of what would. If you would have done all these.
00:48:01.092 - 00:48:02.636, Speaker A: Actions, what the simulation would be.
00:48:02.700 - 00:48:04.092, Speaker D: That's definitely what you're trying, what the.
00:48:04.108 - 00:48:06.356, Speaker C: Response would have been without actually doing the action.
00:48:06.460 - 00:48:19.980, Speaker D: That's definitely true. So this is definitely an exercise. So this, like, at a high level, falls under you are you've taken less than, if you haven't taken any actions, then you are starting from no information set, and you don't have much, but.
00:48:20.012 - 00:48:21.996, Speaker B: You'Ll have some observations, usually, and that.
00:48:22.020 - 00:48:31.532, Speaker D: Observation has brought it down to some set of uncertainty regions, and then you're trying to understand those uncertainty regions a little bit better. And yes, you could sort of hallucinate.
00:48:31.588 - 00:48:33.900, Speaker B: A utility for the adversary and then.
00:48:33.932 - 00:48:35.504, Speaker D: Best respond to that utility.
00:48:36.084 - 00:48:37.596, Speaker B: I'm going to talk a little bit.
00:48:37.620 - 00:48:41.020, Speaker D: About online decisions which actually do something like that.
00:48:41.172 - 00:48:42.144, Speaker B: They're not.
00:48:42.604 - 00:48:45.464, Speaker D: We have to make these kind of fast decisions.
00:49:05.454 - 00:49:16.004, Speaker C: But maybe it's much more without losing, you know, without check, without taking action. But they do release some information, which is what the Oracle query would have been.
00:49:16.424 - 00:49:17.280, Speaker A: What would they ask?
00:49:17.352 - 00:49:21.168, Speaker C: Oracle would have been had. You see what I'm saying?
00:49:21.296 - 00:49:30.364, Speaker D: Yeah, I mean, let me put what you said in a different way. I could have designed a game and I could have released it and asked a bunch of.
00:49:32.864 - 00:49:34.360, Speaker B: I'm going to stereotype teenagers.
00:49:34.392 - 00:49:37.888, Speaker D: Sorry. I'm going to ask a bunch of teenagers to play this game for me.
00:49:37.936 - 00:49:45.376, Speaker B: And then I'm going to look at the simulation that is produced by a bunch of people playing some kind of strategic games, especially if it was a pretty complex game.
00:49:45.560 - 00:49:52.712, Speaker D: As an estimation of what my best response is. That's true. I can always have these other sorts of.
00:49:52.848 - 00:49:56.284, Speaker C: You don't know the inputs that you're really going to have in the game.
00:49:57.584 - 00:50:18.756, Speaker D: I can create the inputs. That's okay. Actually, what I don't know is whether or not people are actually like, what is the utility of the people or how rational people are? So I can do these experiments to get an estimation of the stress response, to have a warm start from our algorithm and then come up with a very small set of these uncertainty sets and then try this.
00:50:18.940 - 00:50:24.264, Speaker C: In order to do crypto, I need to know what the response would have been given exactly.
00:50:24.724 - 00:50:39.986, Speaker D: I need to see this best response or an estimate of it. Now, I can come up with different tricks to getting that estimate, but at the end of the day, it's about information that I need to see from whatever it means. I saw a bunch of hands.
00:50:40.010 - 00:50:40.842, Speaker B: I don't know the order.
00:50:40.938 - 00:50:46.218, Speaker D: You all do want to go next? Who goes first? Comment. Go ahead.
00:50:46.306 - 00:51:08.674, Speaker A: I also want to inject something not crypto, but analysis. So you're talking about a situation where actions have some costs as you're going along, as you're trying to learn, but actually to do so, why not add that into the model?
00:51:09.134 - 00:51:10.446, Speaker D: Good, I will.
00:51:10.590 - 00:51:11.486, Speaker A: You will?
00:51:11.670 - 00:51:26.790, Speaker D: I will talk about it in a little bit. It's not competitive analysis, it's regret. But you can actually nicely go between regret. Well, you can go between regret and competitive analysis. Nine minutes. How about. Are there questions?
00:51:26.942 - 00:51:28.582, Speaker A: I wanted just to answer that, so.
00:51:28.598 - 00:51:32.074, Speaker D: Maybe we can take that offline. Do you have a question?
00:51:32.374 - 00:51:36.286, Speaker A: Yeah, just so we're assuming here that u two is fixed, right?
00:51:36.470 - 00:51:44.754, Speaker D: Good. I'm going to relax that, too. Is that the question? So you guys are all ahead. Let's go back to where we were.
00:51:46.134 - 00:52:14.318, Speaker B: Ah, it's just the next slide. Okay. So. Okay, so I had made a simplifying assumption in some sense. It's not necessarily simplifying but I assume that there is one type of follower, or even if I don't know, it is somewhat fixed. And I had assumed that I'm spending all of this time coming up with a solution and then implementing that solution at the end. But you could have thought about this problem as one that is an online way of decision making.
00:52:14.318 - 00:53:22.514, Speaker B: You have some system, you're making some decision, and then you're going to tweak these decisions. And you can even think about having potentially different types of followers being presented to you that are they coming from some distribution or they're coming adversarially from sunset. So in a repeated game formulation, I could actually write this as something that we now call Stackelberg regret. And Stackelberg regret is very similar to the other notion of regret we talked about, in the sense that I am going to measure the leader's utility and average it out. But the thing I'm competing against is going to be the Stackelberg optimal strategy. Why is this a Stackelberg optimal strategy? First, note that this t I put here is exactly for what was the suggestion, which was, if I'm dealing with a different follower, what would that happen if I fix this follower? This is the definition of a Stackelberg optimal strategy. Now, if I don't fix it, it's essentially the Stackerberg optimal strategy for the sequence that I have observed.
00:53:22.514 - 00:54:34.220, Speaker B: So this is what we call a Stackelberg regret. And if you have an algorithm that has low Stackelberg regret, what it now means is that maybe you were never close related to what you have to this strategy itself. But the utilities that you gathered around when at every time the follower somewhat myopically responded to you, best responded to you would have not been very far from the Stackelberg equilibrium. That's how you would interpret this StackelbErg regret. And the idea is that actually the type of offline algorithms, I told you, the search algorithms, that they spend a lot of time with, membership query or whatnot, and then they learn something. They could all be turned into online algorithms that spend that time in one way or another in an online fashion, and either towards the end they are comfortable enough to commit to something, or they continue sort of exploring and never necessarily committing. So there are these algorithms that are designed for Stackelberg regret.
00:54:34.220 - 00:54:41.744, Speaker B: Many of them are actually inspired by the type of algorithm I already gave you, which spend some time searching and then commits.
00:54:43.324 - 00:54:46.224, Speaker A: So is p star adapt?
00:54:49.164 - 00:54:51.292, Speaker D: It's decided in hindsight.
00:54:51.348 - 00:54:53.020, Speaker B: So I stopped the sequence, I look.
00:54:53.052 - 00:55:45.348, Speaker D: Back and I define it okay, so now we have two notions of regret that we've been talking about, the Stackelberg, which I just introduced, and the regret we talked about yesterday. I'm going to refer to that external regret. That's another way of just another terminology for the same idea, and they're slightly different from each other. So if I were to think about external regret, external regret, forget about Sackberg for a second. What it was saying was that you look back in hindsight, you see the observations, the actions that were taken, and you try to optimize for those actions. That is this type of maximization, in the sense that the observations were not at all a response to the optimal thing I'm doing. It's not about a policy.
00:55:45.348 - 00:56:54.334, Speaker D: It's the actual observations that my algorithm, for whatever reason the adversary had produced for it. Whereas Stackelberg regrets, the benchmark, has in it this type of the best responsibility. So they're different from each other, and they're different from each other in the sense that in the worst case, they're incompatible. What does that mean? It means that they're really optimizing for two different notions. If I get a stack of Stackelberg algorithm and no Stackerberger algorithm, it's going to have regret, external regret on some sequences, and vice versa. If I get an algorithm that's guaranteed to be no external regret, it's going to have stack of regret in some sequences. Why is this the case? It's essentially at the heart of it, partly is that same difference I was telling you about, Minmax, Max Min, Sakoburg, Nash, all of that.
00:56:54.334 - 00:58:04.058, Speaker D: In some way you are looking at utility in best hindsight on historical observations versus utility on best responses. In fact, the advantage of Stackelberg optimal strategy was, as Costas asked, was that the leader was not best responded, that relaxation was the thing that was allowing you to do better than other solution concepts. And the external regret is essentially telling you that you should be long term, this respondent. Otherwise you will have some effect. And that's why, in some sense, Stackelberg's solution must leave on the table, at least as it appears for the historical observations, something so that it appears not stable, it's not historically, it's not taking the full benefit of historical, full benefit of utility on historical observations also. On the other hand, external regret is not at all accounting for what this hypothetical is. What would have happened if I did this or did that.
00:58:04.058 - 00:58:26.834, Speaker D: It's just talking about the history rather than the type of history that would have been produced as part of your policy and that's why this work. Okay. I'm gonna start from here. And I know that break is coming soon, but I already see some questions, so how about we pause now, I'll take some questions, and then we'll continue in 15 minutes. Does that sound good.
00:58:34.174 - 00:58:34.742, Speaker C: For the.
00:58:34.798 - 00:58:43.322, Speaker D: In terms of the external regret? I can give an understanding of that. We don't have a characterization, but one class is zero sum games.
00:58:43.458 - 00:58:45.814, Speaker B: But I'm cheating because that's just what you say.
00:58:46.474 - 00:58:52.694, Speaker D: I'm not even sure if convex concave has that property. I don't know.
00:58:57.914 - 00:59:04.960, Speaker A: Phrase must appear, should appear necessary.
00:59:05.112 - 00:59:10.208, Speaker D: It's not in every game, but in some game it must. That must happen in some game.
00:59:10.376 - 00:59:22.044, Speaker A: And just to follow up question, do we have any information that also for any convex printing?
00:59:23.304 - 00:59:24.624, Speaker B: No, I don't know.
00:59:24.664 - 00:59:27.044, Speaker D: So, like, I haven't thought about this question before.
00:59:27.584 - 00:59:28.736, Speaker B: If you ask me to give a.
00:59:28.760 - 00:59:30.560, Speaker D: Class, I'll cheat by Sanjay. Or sometimes.
00:59:30.592 - 00:59:33.414, Speaker B: But more reasonable answer, I don't have right now.
00:59:34.914 - 00:59:36.094, Speaker D: Other questions.
00:59:38.074 - 00:59:42.734, Speaker A: Could you say something about the no Spikelberg regression algorithms, what they do?
00:59:44.554 - 00:59:51.334, Speaker D: Yes. Am I going to talk about them? Not really. Okay, so.
00:59:53.154 - 00:59:53.826, Speaker B: Good.
00:59:53.970 - 01:00:37.676, Speaker D: So let's go back to the example I was giving you with. You spend some time doing the setup. Best response. Now, one example of this is you could hallucinate the type of adversaries, or you might not even commit to a certain type of adversary, but, like, just kind of try to cover your basis for a bunch of different types of adversaries. So, the fact that you were never again deterministically committing to any one type of query, you have a distribution over these queries that you're asking. That's something that looks like an online no stack of birth type question. Another way to think about this is that many of these algorithms look at this as a function.
01:00:37.780 - 01:01:17.080, Speaker B: They think about this function as just some function of p. It's just that function is not very nicely structured. And then what they have to do is to design ways to get information about this function while sort of covering their basis in terms of all possible ways that this function could have been written. I'll talk very briefly when we come back about bandits. I'm not going to go into the details of them, but that part, hopefully, is going to answer your question a little bit more. But because I didn't cover the computational aspects yesterday, I don't want to give actual algorithms right now. One more question.
01:01:17.202 - 01:01:17.904, Speaker A: Yeah.
01:01:22.564 - 01:01:58.024, Speaker D: For finite games, or you can get the same thing I was telling you. As long as you limit the possible size of. So you start with different possible followers. Right. As long as you make that small, that set, you can get the same regret, but usually logarithmic in that size or something that has some additional annoying dependencies on X and Y's. Again, it's a. Yeah, it's actually.
01:01:58.144 - 01:01:58.784, Speaker B: It's very.
01:01:58.864 - 01:02:11.082, Speaker D: It's not that hard. If I just limit the types of games to say that it's one of these, k. I just don't know which one I'm going to get. Like poly in k and then root.
01:02:11.138 - 01:02:14.890, Speaker B: T and everything else. But to do better than poly k.
01:02:14.922 - 01:02:17.894, Speaker D: You need to use structures of the game better.
01:02:20.714 - 01:02:24.094, Speaker A: So let's take the rest of line and we'll have a 30 minutes break.
01:02:29.554 - 01:02:30.490, Speaker B: Oh, yes, sorry.
01:02:30.522 - 01:02:30.898, Speaker D: Yes.
01:02:30.986 - 01:02:31.338, Speaker A: Sorry.
01:02:31.386 - 01:02:32.098, Speaker D: You're right.
01:02:32.226 - 01:02:35.574, Speaker A: Totally right. Yes. So this time it's going to be a 15 minutes break.
01:02:36.184 - 01:02:36.976, Speaker D: Yes.
01:02:37.160 - 01:02:46.164, Speaker A: Because we're going to have the town hall at 415. At five, we need to exit the room. So as long as we have some time for the town hall, we're going to have the 15 minutes. Breakfast time.
01:02:47.064 - 01:02:49.564, Speaker D: Okay, I'll see you guys at 315.
01:02:55.344 - 01:02:58.360, Speaker B: I finished on time. You guys ask questions, we can start.
01:02:58.392 - 01:03:00.044, Speaker A: The town hall a few minutes later.
01:03:00.584 - 01:03:02.392, Speaker B: Yeah, we can do 320 if you want.
01:03:02.408 - 01:03:06.334, Speaker A: Yeah, 320, yeah, let's do like that. Yeah, let's take a 20 minutes break.
