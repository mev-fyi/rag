00:00:01.040 - 00:00:49.400, Speaker A: Last talk to tell us about explicit decoding of univariate multiplicity and cold degrees holes. Good to be in person and giving the talk here. Thanks for the invite. I'll be speaking on some recent work with Rohan Goyal from, who's an undergrad at the Chennai Mathematical Institute and my colleagues and student at TIFR, Manal Kumar and Ashutosh Shankar. So the starting point for this work comes from the seminal results of list decoding up to capacity. The explicit list decoding up to capacity results first of Guruswami and Rudra in the context of FRS codes, and subsequently of Koparti and Gurswami and Wang for univariate multiplicity codes. So let's just quickly recall what these results show.
00:00:49.400 - 00:01:26.680, Speaker A: They show that for every constant rate and epsilon, there exists explicit families of frs or univariate multiplicity codes that are efficiently less decodable all the way up to the maximum number of possible errors. That is basically one minus r minus epsilon fraction of errors. And these are beautiful algorithms which do this. So our focus is going to be how efficient can we make this efficient algorithms out here? Can we make them as. Can we make them nearly linear? Can we make them linear time algorithms? That's good. Okay. And some sense the answer.
00:01:26.680 - 00:02:01.400, Speaker A: Our main result is that, yes, we can get nearly linear time versions of these algorithms. But actually, as stated here, it's unclear whether one can even expect nearly linear time, because the original algorithms of Puruswamy, Rudra Kopati and Gurswami Wang only promised polynomial size lists. They did list decoding. And I'll state all of these problems formally, but I just want to state the result first and then we'll go into more. They only promise polynomial size lists. Now the list size is super linear. There's no chance of having nearly linear time algorithm.
00:02:01.400 - 00:03:27.530, Speaker A: So one could ask, is it, is this question, does it even makes sense? But thankfully, about work showed that these algorithms can actually shown that they are not just efficiently list decodable, but they actually have constant list sizes. This was shown a beautiful work by Swastik, Noga, Shivangi and Mary six years ago. And so this sort of, at least now you can ask, there are constant size lists. So we can ask the question, do they exist nearly linear time? And our main result is basically we can further modify these algorithms such that now we can actually get with that run nearly linear time. In particular, they run in time order n poly log n. It does make sense, except that we don't know what the list size we have right now. We only know upper bounds on list size.
00:03:27.530 - 00:04:19.344, Speaker A: So I could say it's linear, linear in these suggested upper bounds, even at the Johnson radius, for example, we don't know for these codes, what's the exact look size. Yeah. So actually, so this sort of answers affirmatively. Yeah, so it's a randomized algorithm, but the randomness component is going to come from the KRSW result. Our, our algorithm is going to be fully deterministic. We are going to, then we're going to give a deterministic implementation of the first part, which will then leave some, which will output a fine subspace, and then from the affine subspace to get to the actual list is going to be randomized, is a randomized pruning algorithm of KRSW. We're just going to use that.
00:04:19.344 - 00:04:55.796, Speaker A: So if you can determine as that our algorithm also gets determined, this was actually posed as an open question. And Swastik's beautiful survey on multiplicity codes. Actually it's an excellent survey. Ranal and I have sort of, the last four papers of ours are multiplicity codes have been just basically answering one open question after there are several more out in that list. It's a beautiful survey. I would recommend all of you to look at it and. Yeah, okay, so that's result.
00:04:55.796 - 00:06:00.644, Speaker A: I said it's going to be a, in fact, when fastic wrote the survey, he, because KRSW was not yet there, he actually asked this question for up to just the Johnson radius, not even up to capacity. It was not known even up to Johnson radius, if you could get nearly linear time algorithms. So we'll answer both the questions up to Johnson radius. We'll get as well all the way up to capacity algorithms. So that's going to be, so what, I'll spend the next ten minutes or so just state what these codes are. Yes, we did them in the bootcamp, but I'll sort of formally state what these codes are, put them in the historical context, and then we'll in the remaining sort of last 20 minutes of the talk or so, we'll go into what are the proof techniques? How do we manage to get such nearly linear time algorithms? Any questions on the statement of. So let's just put some quick next starting point of all these polynomial are what our favorite family of codes, Reed Solomon codes.
00:06:00.644 - 00:07:26.052, Speaker A: We're going to be working with some finite field, some degree D parameter, some degree parameter, and s, which is some subset, some set of evaluation points. And the Reed Solomon code word is basically, you're given some polynomial f of degree utmost d, and this is encoded as a set of evaluation points. You basically are going to give it as p alpha one, p alpha two alpha M going to do it. It will be convenient for me. This is a notation with sort of a to view this as p of x mod x minus alpha. When alpha is the same, it'll be easier for me to define this, because when I go to the multiplicity and frs restrictions, this definition will be useful. And it's clear that the rate of these codes is d plus one over n.
00:07:26.052 - 00:08:18.796, Speaker A: The distance follows from the degree mantra is one minus d over n. So these are actually MDS codes. And we have a rich history of decoding algorithms all the way up to the unique decoding radius, and then from in the sixties and seventies and then in the nineties up to the Johnson radius for these algorithms. So let's. And all these, this history, we first had efficient algorithms that went all the way up to the, the unique decoding radius. Then we actually had nearly linear time implementations of these, the original algorithms at Peterson Burleigh camp Maciever polynomial time algorithms that did unique decoding up to half the distance. And then there were implementations of particular the Burleigh camp Macie algorithm, which ran a nearly linear time, which gave n poly log at time.
00:08:18.796 - 00:09:06.088, Speaker A: So that was in the context of unique decoding. So you have this zero to one, that's the distance of the code. So that's the unique decoding radius. Up to that point, we had, there were fast implementations of Berlik amp Macie algorithm, which basically gave. So in the unique decoding world, it gave you fast algorithms. Then following work of Madhu and Venkat, we knew how to decode this all the way up to the Johnson radius, which is one minus square root, one minus delta. And we had polynomial.
00:09:06.088 - 00:10:15.398, Speaker A: They gave you, they gave us polynomial time algorithms to list decode all the way up to that point. So Sudan Swami Sudhar gave it all the way up to the Johnson radius. And then this G's is from 98. And after four years later, Aleknowicz gave an implementation of the Guruswami Sudhana algorithm, which ran in nearly linear time. So just like we had a fast implementation of Burnick MSc, Aleknow, which had to do some extra work in order to get. It was a nearly linear time implementation one. And what we wanted to know if we can actually go beyond what happens beyond the Johnson radius.
00:10:15.398 - 00:10:50.596, Speaker A: Yeah, G's has a super linear list size, so you have to be careful about this. So just below the GSS, so the list size by the just the combinatorial bound for Johnson radius is anything one minus epsilon. From the Johnson radius the list size is constant. At the Johnson radius the list size is possibly polynomial. So elect now which says anything below the Johnson radius he'll get nearly linear time. He doesn't answer actually. So at the Johnson radius I don't know if you get nearly linear time and I don't even know.
00:10:50.596 - 00:11:48.014, Speaker A: I really, I would like to say his algorithm runs in linear in the list size, but I don't know whether it could be that at the Johnson radius two the list size is actually constant. It just our current bounds give you some linear in N. So alec know which result in some sense. Guru Swami Sudan result goes all the way up to the Johnson bond. Aleknow it says for any one minus epsilon of J Delta it will give you nearly linear time algorithms. And there we know that the list sizes are constant and we will be working in that regime everywhere there will be an epsilon will be off by epsilon from the final point, and hence, hence we know that the list sizes are constant from previous results and hence we can expect any linear time algorithms. So there was this quest for asking for explicit codes, possibly accompanied with efficient list decoding algorithm, that were less decodable beyond the Johnson radius.
00:11:48.014 - 00:12:49.316, Speaker A: And it was in this context guruswami and Rudra actually rediscovered folded Reed Solomon codes, which were introduced by Krachowski in 2002. They rediscovered it in the context of following works of parvadi context of list decoding, and showed that actually they were list decodable all the way up to capacity. And then a few years later, Swastik and Venkatan and Carol showed that actually some other codes, namely the univariate multiplicity codes, also have these properties. So everything I say today will actually be to be true both for FRS and univariate multiplicity slightly partial to multiplicity. So I will focus my attention fully on multiplicity codes for the rest of this money. But everything I say holds good even for FRS codes. So what are multiplicity codes? So let's change this one.
00:12:49.316 - 00:13:48.504, Speaker A: So multiplicity codes are sort of, it's just the fact that it's just going to be the multiplicity, because at every point, in addition to giving the evaluation, I'm also going to give a derivatives up to a particular order. So I'm going to work with univariate multiplicity codes. Other boards on white boards, you will know why they are covered. One extra parameter, which is the multiplicity parameter thing, is going to be similar. And what we will give is now p of x mod x minus alpha to the power s. That's what the Alphabet is going. So it's, the encoding is going to come from.
00:13:48.504 - 00:14:32.040, Speaker A: In the Reed Solomon world, this was living in f to the n. In the multiplicity world, this will live in fs to the n. And by the way, this definition works over all fields. When I talk about derivatives, you have to be a little careful about what derivatives. Derivatives make sense only when the characteristic is large enough. And currently all the efficient list decodable results are only known when the characteristic is large enough. So we will assume that the characteristic in fact is large enough.
00:14:32.040 - 00:15:10.156, Speaker A: So this is the same as this quantity is equivalent to more or less giving p p at alpha, the first derivative at alpha, the second derivative at alpha, and all the way up to the s minus one th derivative. For small characteristic, you have to work with hasid derivatives. But I'll completely hide that under the rug. Let's just assume it's regular derivatives for now. Anyhow, these results work only for large characteristics. So we can, without loss of generality, assume that we actually are given at each point the regular notion of derivatives. No.
00:15:10.156 - 00:15:55.742, Speaker A: So the definition needs a large with respect to s. But the proof actually requires characteristic to be larger than the degree parameter. We don't know. Yeah, so the definition, at least away the definition will possibly that require s actually in the, if you go with the has, this definition doesn't require any restriction on the characteristic. But the current proofs of list decoding up to capacity, we have a requirement, all proofs, all earlier proofs, including our proof that they work only when the characteristic is zero or larger than the degree of the underlying polynomial, the rate and everything just changes by an extra factor s. These are also MDS codes. And want to ask if these are list decodable up to.
00:15:55.742 - 00:16:36.934, Speaker A: Firstly, you can ask up to the Johnson radius, a slight modification of the guru for me soothing algorithm will show you that they're actually list decodable efficiently all the way up to Johnson Radius. Now you can ask are these, and they are in fact efficiently list decodable all the way up to capacity. And what we have in result will actually be these two statements. These are the exact statements. What we'll show is we'll show both list decoding up to capacity. Clearly linear time algorithms, as well as efficient list decoding algorithms up to the Johnson radius. Yeah, so this is just a restatement of this.
00:16:36.934 - 00:17:25.233, Speaker A: Basically, for every epsilon, we'll show that for a large enough multiplicity parameter, and for degree d and all fields of characteristic larger than d, you do have that the corresponding multiplicity code is less decodable from all the way up to capacity in time, which is nearly linear in n. So it's n and a whole bunch of polylog factors. Alphabet size goes up. The Alphabet size is f to the s, so think of s as a constant. Now, for all of these results, s is a constant, so it moves from f to log f to s log f, the Alphabet size. So it depends on epsilon. The constant is dependent on epsilon.
00:17:25.233 - 00:17:59.204, Speaker A: So you tell me how close we want to capacity, and I'll tell you how large multiplicity parameter you want. For all multiplicity parameters larger than that, this is less decodable up to those many fractions. And what's the dependence? What is so, yes, naught is one over epsilon Square. Yeah. The final Alphabet is like n to the one over epsilon square. The final Alphabet will be d to the d is like n. D is like n.
00:17:59.204 - 00:18:23.600, Speaker A: Yeah, yeah. And you get a similar result for up to the Johnson radius and. But here we don't have. This works for. The only key difference between these two results is this works for all multiplicity parameters, not just from a large enough point, and it works for all fields of all characteristics. There's no characteristic. It works just like it is an extension of the reads of.
00:18:23.600 - 00:18:57.476, Speaker A: In some sense, this is a strict generalized Alec Novich proved this result for Reed Solomon quotes. It was an fast implementation of guruswami, Sudan, and we get a fast implementation for every possible univariate code we get. That's the questions. Is the characteristic better than FRS codes? Depending like you don't need, you also need, again, at least the, in Frs school. No, f, by the way, I have stated it for multiple sequels. So FRS codes, FRS codes. One thing, there's no Johnson radius.
00:18:57.476 - 00:19:31.428, Speaker A: There's absolutely no characteristic for this one. It's not a dependence on the characteristic doesn't play a role there. They're the order of this FRS codes. You evaluate it at alpha, gamma, alpha, gamma, square, alpha. Where gamma is an element of sufficiently high order, the order has to be larger than d. That will play the role of characteristic. It can work with low characteristic fields, but you need a, you need an element with large multiplicative order, the Frs, which is there.
00:19:31.428 - 00:20:04.888, Speaker A: So in some sense, frs work in our list, decodable in every field. As long as you want to lay your hands on this gamma. These are these we currently don't know for every field. We only know it for large characteristics. In that sense, FRs is more general than multiplicity. What time did I start have another. Huh? Ten minutes more? 15 minutes more.
00:20:04.888 - 00:20:15.564, Speaker A: Huh? Am I, am I bad at math? Oh, sorry. We started at. No, it started at 915. It's not nine, so good. Yeah. Okay. Right.
00:20:15.564 - 00:20:59.134, Speaker A: Okay. So what I will do right now, what I'll do is we'll sort of give a sketch of the proof of this statement, how to over doing it, and the way we go about it, we actually look at the proof that I've been for multiplicity codes. There are two proofs of this. One due to swastik, another one due to Venkat and Carol. Venkat and Carol's approach is in fact further inspired by a linear algebraic approach suggested by Saleel in his pseudo randomness manuscript. They use that to get a very nice linear algebraic framework to give both, to solve both frs and univariate multiplicity. What we will do is we will actually recall this linear algebraic framework of guruswomi and Wang.
00:20:59.134 - 00:21:30.754, Speaker A: Look at the steps broadly. It will follow the same steps, and there will be some steps which use solve huge linear systems. But because these are, we are working with polynomials, they'll be solving linear systems and hence run in cubic or quadratic time. But because these are polynomials, we'll show that the underlying linear systems actually have very nice structure, and hence you can apply fft like techniques to get nearly linear time algorithms. Is that also what. Yeah, this is actually. So there.
00:21:30.754 - 00:22:20.628, Speaker A: So I'll tell you. So this is Alek Novich's approach also. So I'll let no shall be come to it. So let's look at, so let's look at the sort of the linear algebraic framework of boroswami and Wang. So what was the, what's the question over here? So you're given some received word that is basically you're given n points, alpha one, alpha two, alpha n, and the purported derivatives, list of derivatives at these endpoints. And you want to find all polynomials f such that, that have good agreement on these endpoints, that the number of agreements is large enough. I want for the rest of the proof, I'm not going to go in quantitative parameters, I'm just going to give a high level what's going on.
00:22:20.628 - 00:23:14.154, Speaker A: And since Sudan's algorithm for list decoding, broadly, all algorithms for list decoding are followed two main steps. The first step is sort of what's called the interpolation step. You get an algebraic explanation of the entire data, and then once you have got this algebraic explanation, you sort of extract the polynomials from this, how you actually implement those changes from algorithm to algorithm. But this is the broad step of all of them have this. All algebraic list decoding algorithms have the same structure. There's a step one, which is this algebraic explanation, or the interpolation step, and step two, which is extracting the polynomials from this algebraic explanation. So let's state what is this in the context of univariate multiplicity codes as done by Guruswami and Wang.
00:23:14.154 - 00:23:37.514, Speaker A: The first step is basically, you want to find some explanation in this way. The explanation will be a polynomial of the type. It will be an m plus two variate polynomial. M is another parameter we are going to introduce into the system. Now, m is not so s was the multiplicity parameter. M is typically, think of it as smaller than s. It's going to be one over epsilon.
00:23:37.514 - 00:24:14.012, Speaker A: S will be something like one over epsilon. Square m will be something like one over epsilon. We're going to come up with an algebraic explanation of the following type. It looks it's an arbitrary polynomial in x, but linear in the y variables. It's going to be of the form ax plus summation I equals zero to 10 to m y I b I x. Going to look at this expression such that every f that is has higher agreement that every f in the list will satisfy the following differential equation. There's a bunch of interpolation conditions.
00:24:14.012 - 00:24:49.614, Speaker A: Interpolation conditions are tailor made such that this is a consequence of it. I haven't written down the interpolation. They write a bunch of interpolation conditions so that now you take this polynomial q and plug into it f, the first derivative of f all the way up to the nth derivative of f. These are for all formal polynomial formal polynomials. When you plug them as formal polynomials, this formal polynomial actually vanishes. So in some sense, the interpolation conditions are tailor made such that we'll see how this happens. But the interpolation conditions are basically a bunch of linear conditions.
00:24:49.614 - 00:25:16.396, Speaker A: To ensure that this happens for every f in the list. We come to that a little later. But these will be just, if you actually stare at them strong enough, you can see that this will be just a bunch of linear equations. So it's solving. This will be actually the standard way. How it's done is you actually solve a linear system, and that typically incurs cubic time or so. The next step is you.
00:25:16.396 - 00:25:38.792, Speaker A: Now you're given q. You know, q contains in its belly all the f in the list. You have to extract these f. So you want to now solve the differential equation that is, given q. You want to find all f such that q of f is zero. So you're given this differential equation and you want to solve it. If you stare at this closely also, you'll realize this is also a linear system and you can solve this.
00:25:38.792 - 00:25:55.844, Speaker A: And that's basically the entire algorithm of Gourus swami and Wang. They will solve this. There's a linear system, a cubic time algorithm. This is another linear system. Solve it. So you, at this point you get a basis. You can notice that the f's that are solutions to this will actually form an affine subspace.
00:25:55.844 - 00:26:29.386, Speaker A: The f's that solved satisfy this, form an affine. It's not hard to see this. They form an affine subspace. So the Gouraswamy Wang approach will spit out a basis for this affine subspace. And it says all the close enough code words are in that affine subspace. And that was the end of the Guruswang Wang algorithm. The subsequent observation by Copperti and all shows that you can, if it's contained in such a low dimensional affine subspace, you can actually prune it and get show that the list size is even smaller.
00:26:29.386 - 00:27:20.904, Speaker A: So at this point they had shown that the affine subspace, they shown affine subspecial of dimension m. So Murusomy Wang approach said that the list size is potentially, possibly q power m. And what these guys, what KRSW further defines it, then they show that they an affine subspace of. If you have an affine subspace which is just contained of code words which have considerably far from each other, any intersection of it with a ball can't contain more than one over epsilon to the power m code words. That's the statement which they prove. This actually a subsequent improve this version of it is a subsequent improvement due to Tamu. But once this you can, and they all showed that you not only hold this holds combinatorially, there's an algorithmic randomized procedure which given the affine subspace in the, in its basis, can actually extractly output this.
00:27:20.904 - 00:27:55.624, Speaker A: So that's the sort of broad outline for the existing algorithms for this decoding of univariate multiplicity codes. And what we want to know is we want to make each of these steps run a nearly linear time. Thankfully, this step is already nearly once given the affine basis for the affine subspace, it's already linear. So step three is not going to be a concern. Step one and step two are going to be our concerns. We want to make both these steps run a nearly linear time. What is this one over epsilon to the n? That's not a constant m.
00:27:55.624 - 00:28:24.168, Speaker A: It's a constant. M is one over m is itself one over epsilon. Okay, m. Yeah, yeah, sorry. This is m. Okay, so what we want to do is actually take these two steps, stare at them a bit closely, find out if actually these steps can be run in nearly linear time. Questions.
00:28:24.168 - 00:28:57.074, Speaker A: So this, so far I've done basically a recall what Mary and I did in the boot camp. M here is s in the previous board, right? What are same. No, no, no. So m is more like one over epsilon. S is more like one over epsilon Square. So M is another parameter that we will choose in the algorithmic, which will be, which will be large compared to epsilon, but small compared to s isn't the lower one. Then m to the power of m is a constant.
00:28:57.074 - 00:29:15.016, Speaker A: M is a con. Okay, so how is it better than m? No, no, this is. Sorry. This is dimension m. Therefore, the list size is q to the m and q is n. So at this point of time, the list size is n to the power one over epsilon. So here it becomes the list size is one over epsilon to the power one over epsilon.
00:29:15.016 - 00:29:51.706, Speaker A: So goes down, the list size goes from polynomial dependence on n to a constant on. So let's, so what we'll do is we'll, our broad outline will be the same thing. We'll have the interpolation step, we'll father differential equation. By the way, even Aleknow, which, going to Yerit's question, Araknovich had the same sort of issues over there. There was an interpolation step of, it was a different one. It was there, it was only bivariate. It was not an m plus two variate polynomial, the bivariate algebraic explanation.
00:29:51.706 - 00:30:14.964, Speaker A: And then you had to factorize and that. So the by way. So the recipe we will follow for this will be inspired by alaknowch. But even for people who are familiar with it, will give us slightly more straightforward proof of this step. This one. Alaknowicz noticed that we had to do factor in Aleknowicz's cases. We had to factorize.
00:30:14.964 - 00:30:33.564, Speaker A: Particularly, you can ask whether we can do nearly linear time factoring. We still don't have an answer to this. The best factoring algorithms of bivariate polynomials run n to the 1.5 time, not nearly linear. Notice we don't have to factorize. We want to only find solutions of the form y minus f. These are special kinds of factors.
00:30:33.564 - 00:30:58.634, Speaker A: And Roth and Rookenstein had shown that actually you can with this. So alaknowitz just uses an off the shelf product. Okay, this step, this is his main step. And this uses the shelf product. And that's how Alaknow gets the fast implementation of guru Swami Sudan here. We do not have fast solutions for. We have to also go into this step and solve this in nearly linear time.
00:30:58.634 - 00:32:12.768, Speaker A: Let us get to, what is the step? What is the first step? So, let us do each step one at a time. So, let us take the first step. I want to find now a equation q that captures the entire list. I want to find all, I want to find a polynomial q of not too large a degree, so that every f in the list satisfies this differential equation. How do we go about doing this? So, let's recall, let's get what exactly happens here. So, we want such a polynomial identity to be satisfied by when you plug in f. How does one ensure this? We ensure that at a.
00:32:12.768 - 00:33:27.804, Speaker A: So basically, this, to satisfy this, we say that for every point for each. So this is step one details. So, for each alpha n s, we want to ensure that some, we want to put some conditions on this one, so that every f in the list satisfies this equation. So, the way it is done is, you ensure that alpha I is a root of some polynomial, large multiplicity. In fact, the polynomial which I want is exactly of q of f with large multiplicity. That is the conditions you are going to with, in fact, multiplicity. The exact multiplicity condition is s minus.
00:33:27.804 - 00:34:00.064, Speaker A: So we're going to ensure that. We're going to write a bunch of linear equations that are going to do this. Note, a root is just stating one particular linear equation is zero, stating that it's with multiplicity, s minus m is going to state s minus m linear equation. We want each of these. We want s minus m linear equations to be zero. Notice that's s minus m linear equations. Just for one point alpha I, and there are n such points.
00:34:00.064 - 00:34:56.510, Speaker A: So in some sense, what, so this thing has got sort of broadly inspired by Archangel, which what we are trying to find is, for a particular alpha, let l of alpha, let l of alpha I, let l of I be the set of polynomials that satisfy just for this particular, this one. What we want to find is, want to find a q in the intersection of all the allies, because we want to find it for each of the allies. Therefore, if it has large enough agreement, then there is. Then you can say that this qf vanishes with the has a very high order zero, and therefore it has to be identically zero. You want to ensure this. So, electronic with electro contribution was writing it this way meant that you had to do some operation here, and you have to take an intersection of these objects. The Aleknowitz calls these lattices.
00:34:56.510 - 00:35:29.522, Speaker A: These are actually fx modules. These are, if q is a polynomial of this type, multiplying it by a polynomial by in just x will still retain it in the solution. So these are fx modules. What he wants to do, what we want to do is actually find an intersection of these modules. So working with an intersection module is this, the question is, is there an alternate description of this intersection module? And that's precisely what we'll show. We'll show that there is in fact a very nice alternate description. So let's look at this lattice instead.
00:35:29.522 - 00:36:46.654, Speaker A: L is defined to be h of x product x minus alpha I to the power s minus m I going from one to n plus summation. I have to define what this polynomial AI is. AI is some polynomial AI of x, where a I satisfies that the 8th derivative of a at the point alpha j is actually beta j I plus something to do with this one. Notice that this is some, basically, I'm sitting AI is a bunch of polymers that is derivative. Have this. So hermite interpolation will give you, you can lay your hands on the AI very fast. It's just interpolation.
00:36:46.654 - 00:37:19.194, Speaker A: If it was just evaluation, this would be Lagrange interpolation, which we know how to do fast. This is with derivatives, you do hermite interpolation and you can lay your hands on AI. So we get this. One thing I want to claim is any such polynomial, any. So notice the polynomials are of this type, of these polynomials here. I want to first claim that any such polynomial actually has the property that if alpha is a point of agreement, then actually it has multiplicity s minus m. The thing is set up such that for this to happen.
00:37:19.194 - 00:38:08.390, Speaker A: Now I want to show that this I can actually find a shortest non zero vector polynomial in this Fx module. So now I define the module that I care for, and I want to find a shortest a small vector in this. How do we go about doing this? One way of doing this is then this is there are two ways to go about doing this. One is to show that this lattice is exactly this intersection lattice. And then use the guruswami Wang analysis, which uses dimension counting to show that there exists a short vector in the lattice, and then run some shortest vector problem in the lattice, which run extremely fast. So what's one way of doing it? We actually do a straightforward approach. We'll actually show that this lattice will just stare at it, right.
00:38:08.390 - 00:38:50.648, Speaker A: I'll say in the next few minutes we'll stare at this long enough to show that this actually has a shot. We'll use Minkowski's theorem to show that it has a short vector and hence the shortest vector. When you run an SVP on these sort of FX modules, it will output a polynomial of not too large degree, which will be a candidate polynomial. We'll use for this. No, no, this is, I don't use, the fact that the characteristic is large. The characteristic is large is going to come in the solving the differential equation. Yeah, this will work for everything.
00:38:50.648 - 00:39:11.396, Speaker A: There's nothing about the field that I use here. Let's, I mean, you don't need to show there's always a solution, you just need to show that the desired f pops up as a solution. No, no. It's easy to show. No, no. The fact that all the f's are. No, no.
00:39:11.396 - 00:39:43.136, Speaker A: What? Yeah, that is clear from the way I defined. No, no. But I don't know why does there exist a q of low degree? Because f is, huh? No, no. This queue contains all the f's. So how do I know when I put all the f's together, I will get an object of low degree. There was this dimension counting argument that went on from. So then this one, you have to show that the number of constraints is less than the number of variables to show that there's a non zero solution.
00:39:43.136 - 00:40:26.844, Speaker A: You didn't show that any solution of this also. That's why step two, like we have that this is, this is done to ensure that any one, no, no, this, so we will get that any alpha a will be a root of this. But to get to be equality, we need that the degree of this polynomial is low enough so that when I put plug it in so many times, so I need the degree of q to have an upper bound on it. Otherwise I cannot guarantee this. I need the degree of q, so I don't. So this showing Q will satisfy that for every point of every input point you do have it. So it says root, but it doesn't tell you that as a functional equation, this is zero.
00:40:26.844 - 00:41:03.044, Speaker A: If q was had large degree, this will not be zero. So for that you need to show that there is an existence of a q with low degree. So for this, what we'll for ballistic ordinance algorithm, he takes all the endpoints and then he takes the variables and he just substant by the correct solution. This is exactly the same thing. This is exactly the same thing, except down in the Guru Swami Wang approach, not in the wise, are degree one, not high degree. This is exactly the same. Yes.
00:41:03.044 - 00:41:35.110, Speaker A: Now it is the dimension counting. The dimension counting, yeah. To do the dimension counting, you will have to show there are two. Right. Now I know two proofs of showing it. You have to show that this lattice and this lat intersection lattice are the same. This intersection lattice, Guru Swami Wang's analysis shows by dimension counting it has a low degree thing and hence we have it.
00:41:35.110 - 00:42:12.374, Speaker A: But the only way I know how to show these two are equal is using, come up with a basis for the two lattices using grobner basis and show that actually you can do it. So I'll give a more straightforward approach for this. So, notice that I'm going to write every polynomial here. We can write it as a column vector. This is the coefficient of, this is the Y freet, the y free part. This is the coefficient of Y naught y one and so on. So it's an m plus two dimension column dimensional column in which each entry is a polynomial in degree x.
00:42:12.374 - 00:42:45.642, Speaker A: And this f x has a, this representation says there's a basis, basically there's a basis of x minus alpha, s minus m and y minus a. This is a basis for this module. If you look at it right, if I write the, these guys first. So this, this is what I'm writing is the degree of the polynomials in, of the basis in the x. I had to actually write the basis. What I written is the degree of the poly. This is the first basis.
00:42:45.642 - 00:43:11.744, Speaker A: This is the basis, y zero minus a zero. This is y zero, my a one. And I am just writing the degree of the polynomial in x in the basis. What we can show is that the degree of the determinant of this is just going to be one plus one plus one. It's going to be one into one into one. So it's going to be s minus m. Minkowski's theorem will tell you there has to be a shortest vector of size.
00:43:11.744 - 00:43:46.224, Speaker A: So there exists the shortest vector of degree less than s minus m by the dimension of the lattice, which is m plus two. And that's sufficient for us. So we'll get such a thing and. Sorry, sorry, I made up, I messed up something over here. There is an n. This is s minus m into n. That is s minus m to the n.
00:43:46.224 - 00:44:03.836, Speaker A: Yeah. So this, and these are the exact same calculation which happen with the dimension counting. So you can see how this, notice this is a constant size. The polynomials here have large degree. They have degree up to n. But this is a constant dimensional lattice. It's an m plus two by m plus two lattice.
00:44:03.836 - 00:44:39.854, Speaker A: So you can run shortest. So elect now, which is observation is the fact that you can run shortest vectors on this lattice, find the exact shortest vector first in time, which is nearly linear in the maximum degree and polynomial in the dimension. So it's nearly linear in n, which is the degree of the matrix polynomials here. And the dimension of the matrix is m. So it's some poly dependence on m, which is fine, we can afford poly dependence on M. And that's basically the end of step one. So step one is going to do this, it's going to solve this lattice problem and be done with it.
00:44:39.854 - 00:45:51.134, Speaker A: And this is completely inspired by Alek Novich. Questions? Take another five minutes and wrap up step two. Yeah, step two is solving the differential equation I want to solve. I have given q of the form q of x equals a of x plus. Let us take a very simple case, bx y zero plus cx y one. Let's just take m to be one. And I want to find all f's of low degree, such that ax plus bx into f x plus cx into the first derivative of f is identically zero.
00:45:51.134 - 00:46:32.440, Speaker A: This is what we want to solve. And I want to solve this in nearly linear time. Nearly linear time in the degree of f, the number of variables is constant, but the degree of f is possibly all the way d. And I want to solve it in nearly linear time with this. Let's ask first the simpler this one. How do I solve when, say, when I don't even have y one, something of the form ax plus f of x bx is identically zero. How do we solve such an equation? You divide and hope.
00:46:32.440 - 00:47:13.916, Speaker A: So one way is f of x is minus ax by bx, and the hope is you can do this division fastly and chinese remainder theorem and all can be done fast so we can do it. What I want to say is I don't want to do division because that's not the thing. I can extend to the higher thing. So let's actually go into open this up and see if I can do, if I can do it. Let's see how to solve this efficiently. So let's for the first thing, notice that b zero. You can assume that the constant term in b x doesn't divide both ax and bx, because then you can take vertex.
00:47:13.916 - 00:47:52.474, Speaker A: Then furthermore you can assume that x doesn't divide bx, because if it divides this and does not divide this, there is no solution. So b zero is not zero. So once you have such a functional equation, you have an infinite set of equations going mod each x bar k. So I could write this. I could write, I could write each of these equations I have. For every choice of k, I have such an equation. And then notice when I put k equals one, I will just get a zero plus fz.
00:47:52.474 - 00:48:10.894, Speaker A: Let us assume f is of the form summation f I x bar I. I will get f zero, b zero. So you can extract f zero from this. You write the next equation. You will, it will be linear, and it will, you basically will get an upper triangular system. And you can solve this. That's what happens.
00:48:10.894 - 00:48:32.132, Speaker A: A linear system. Yeah, that's a solution for this. But that's not a fast solution. It goes time run. You have to solve. What you can further show is you can actually move from mod x bar k to mod x bar two k in one step. That is, if I have a solution up to mod x bar k, then I can find the solution mod x bar two k.
00:48:32.132 - 00:48:59.184, Speaker A: And this is using ideas like FFT going into the picture. And what we will do is we'll do, we'll show that this exact same thing will happen out for the differential equation. No, no, no. So FFT is just suggestion for the divide and conquer approach. There's nothing more similar to FFT than. Yeah, yeah. So we'll basically do that.
00:48:59.184 - 00:49:19.766, Speaker A: Don't have time in it. Time to explain more thing. But there's some more. You have to be a little careful, because in the case, in the case of this solution, there was a unique solution. Therefore, it's easy to do it. In our case, there's no unique solution to do this. So if you have a divide and conquer approach, if it doesn't, say each time the solution possibly multiplies by two, this will just blow up.
00:49:19.766 - 00:49:44.284, Speaker A: You can't afford to do it. So what we will do is we'll actually not work with this differential equation. We'll show that if this differential equation has a solution, a related differential equation actually has a unique solution. Actually, it took us a while to realize this. And then we'll do the divide and conquer approach on the related differential equation. So that will solve it. And then from that, we'll get back to the original.
00:49:44.284 - 00:50:11.630, Speaker A: Basically, here we use the observation that the solution to this is an m dimensional space. We know this. Therefore, for each basis, once you fix a basis element, you know, let me stop at this. I can answer this one. We'll basically show a related differential equation has a unique solution and then we exactly do this doubling trick from we'll get the solution mod x bar k. From that we'll go to mod x bar two k and so on. And that's basically the whole idea.
00:50:11.630 - 00:50:49.202, Speaker A: So both these will be run on nearly linear time and we have to do this both in. We can do the same thing for FRS also and get nearly linear time implementations for all the list decoding algorithms that you're familiar with. So the unique governess is by fixing the initial conditions. In fixing the initial conditions we have to get a basis. So we'll go for each of the choice 100100, we're going to get a basis. So the initial condition, if you recall pursuant this one, if you set the first few coefficients, after that it is unique. So we'll, for every possible one of them we are going to get a basis for it.
00:50:49.202 - 00:51:32.784, Speaker A: So we'll set, we'll take a basis for this for x, if the initial conditions are just x bar I, for I being from zero to m, we'll ask what's the extension? There'll be only a unique extension. Find that now we have a basis for the solution space, then run the pruning algorithm over it. But then this won't be near linear. Why? Because you are trying all initial conditions. No, the initial conditions are only for a basis for x bar I, I, going from one to m, I'm going to just get a. So I will find all f's of the form x bar I plus some f x power m. These they will be every solution will once I fix that, the first part is only this, it will be unique.
00:51:32.784 - 00:51:52.514, Speaker A: So I'll find this out. And then if that was fast past time, let's talk with this. Given the time, maybe we can take questions offline. But let's take a 20 minutes break and be back. It.
