00:00:01.200 - 00:00:05.278, Speaker A: By Emir Demirovich on unification of self.
00:00:05.326 - 00:00:20.714, Speaker B: Constraint programming and pseudo Boolean solving. Hello, everybody. Can you hear me? Yes, it works. Okay. Okay, great. So, my name is Amir Demirovich. I'm an assistant professor at Tu Delft in the Netherlands.
00:00:20.714 - 00:01:12.204, Speaker B: And I'm a person that's very excited about algorithm development, solver development, and essentially anything that can help us solve very interesting problems. So, that's the perspective I'm coming from here today. And in this talk, what I want to do, maybe a bit different from some other talks, is that I want to talk about my experience from the last 510 years, how I've seen the field develop and how I'm felt during this time, and then try to extrapolate this to see how we should go further. But this talk is also partially based on some projects that I recently started. So I think the things I'll be proposing are actionable, but some of the things will be based on my opinion, so you might be disagreeing with it. And in case you have something to say, I'm very happy if you interrupt me, and then we have a discussion on it there. Okay.
00:01:12.204 - 00:01:46.104, Speaker B: There are three main points I want to make in this talk. One is that I believe we need to embrace a unified view of Maxat, CP, and PB. I would argue that these paradigms are not very conceptually different. In fact, I would say they're all very conceptually the same. And I think if we view them through a unified view, we can get lots of algorithmic benefits, but also some clarity. And this doesn't mean abandoning our low level view. We can still use all the views we want, use all the information we have.
00:01:46.104 - 00:02:27.794, Speaker B: Second point is, which I labeled here the slogan, you are what you benchmark. It's more of a realization that our benchmarks are very important for the advancement of our field. And in some sense, it's really the becomes the core identity of our communities. So, since the techniques are not conceptually that different, we can interchange them between different paradigms. It seems like our fields are mainly. Our identity of a paradigm is actually just based around our benchmark. And then lastly, I would also like to argue that going forward, asking for scientific novelty might be the wrong question.
00:02:27.794 - 00:03:13.576, Speaker B: Instead, I would argue that we need to focus more on scientific impact, and we will see how these things differ. And then for all of these three points, I'll give you some evidence and some opinions, and then we can talk about that. All right? Okay. So most of the time, I'll spend on talking about the first part, and then we'll see how the second and third just come out naturally from the first point. So, just out of curiosity, has anybody been here? Sat 2013, this was a conference in Helsinki, a great deal there. And does anybody remember the talk of Professor Peter Stuckey? So, Peter Stuckey is a pioneer in constraint programming. He's done a lot of work there.
00:03:13.576 - 00:03:31.160, Speaker B: Was it there are no CNF problems. That's right. Thanks. The talk was titled there are no CNF problems. Professor Peter Stuckey was invited to Sat. He went to sat, where CNF is the creed. And then he has a tight talk saying there are no CNF problems.
00:03:31.160 - 00:03:44.024, Speaker B: Bringing that into question. And how do you think that was received in the community? Well, let's say that Professor Peterstecki was never invited to the CET conference. Again.
00:03:54.084 - 00:03:59.384, Speaker C: Better Sat coding CNF problems than Christian.
00:04:04.264 - 00:04:11.204, Speaker B: And here, of course, I'm a bit joking here. So Peter Spike wasn't invited to other side events, but it's true, he was never invited to.
00:04:12.024 - 00:04:17.624, Speaker D: I don't think you do repeats of invited speakers. I mean, to be fair, I think the talk was well received, right?
00:04:17.704 - 00:04:18.324, Speaker B: Yeah.
00:04:24.024 - 00:04:45.564, Speaker C: Also, one thing he also. The reason his, his SaT coding was better was because he used clauses that have blocked variables which would, which in processing would have deleted. It turns out that they helped to propagate better.
00:04:48.104 - 00:05:24.076, Speaker B: So there are no CF. It seems like Peter Psyche is well versed in creating CNF problems, encoding difference. So we had this talk here, and my talk has a similar flavor, but I'm coming from a different direction. So I'm not going to say there are no CNF problems. Instead, I'm going to do a different view and saying that we need to embrace more of a unified view. And this is essentially saying that the concepts are not, there's not conceptual difference between these paradigms. And to see that, we'll just have a look at how the field developed, let's say last period, and then we'll make some conclusions based on that here.
00:05:24.076 - 00:05:57.130, Speaker B: We all know about Koft analysis, a very core concept in set solving. 2016. We had this grass paper on clause learning. Everybody knows that. Very, very impactful work. And then, motivated by the success of clause learning, CP, people were saying, well, okay, we need that too. So then there was this idea of clause learning also in CP around for some time, but only around, let's say, 2007 or 2009, the professor, Peter Stackhimenko and Kodisch, they came up with something called lazy clause generation, which we heard about before from Kieran.
00:05:57.130 - 00:06:28.274, Speaker B: And this, essentially we're doing CP and then we're plugging in the SAT confident analysis. It's really taking the SAT analysis, plugging it in there. Then, of course, we've seen PB solving. We see also confidence analysis being very important. The conflict analysis transfers across all these three fields. Okay, let's look at how we do optimization in these paradigms. The most natural way of doing optimization is querying some oracle iteratively for a better solution.
00:06:28.274 - 00:07:26.532, Speaker B: A very natural way, very straightforward way of doing optimization. This what we call linear search, and comes out with several other names as well. This is a very classic way how to optimize things in CP, nobody really ever questions this kind of approach, but in Macsat, this approach is rather unpopular, I would say, until 2017. And this is also interesting from my experience, I was working on some timetabling problems around 2000 320 1420, 1314, and I would talk to different maxat practitioners, and also submit to the conference and get some reviews and so forth. Everybody's really surprised that the best solver for my application was sat four j, and it was performing miles better than any other solver at that time. Everybody was a bit doubtful of these results. Like, have you seen the recent competition? Have you tried this core guided solver? That core guided solver, which is a different technique, but somehow suffrage is running really well.
00:07:26.532 - 00:08:05.270, Speaker B: And it turned out that it was because it was using this linear search. So even though it was somehow unpopular, the community, it was still an approach that could be used. And coincidentally, in that time, we were also looking at how to improve CP solving, linear search, and we came up with some ideas to simulate local search. And this performed really well in CP. And then we translated this to maxed, and it performed really well in macset as well for incomplete solving. Here we see another example of linear search being translated across two different paradigms, another technique, and this goes on and on. We have Max sat community, we have this core guided search algorithms.
00:08:05.270 - 00:08:42.610, Speaker B: So I think this is something the next community is very proud of. There are lots of different algorithms, they're very interesting, and they worked really well on the given benchmark. And then, motivated by that success, we took this idea, we incorporated it in CP, we incorporated it in suitable Boolean solve, then we looked at some combinations of linear search and core guided search, core boosting, and again, worked really well in Maxat. It worked in CP, worked also in PB. And then lastly, maybe the last part I'm going to talk about is this logic based vendor decomposition. So we've seen Hooker himself present at this workshop. He was the inventor of this technique.
00:08:42.610 - 00:09:25.892, Speaker B: In the two thousands. And the idea here that you somehow decompose your problem in two parts, and you have some feedback loop between these two parts. And the point here is you have to do this manually. But maybe at this point, unsurprisingly, this type of algorithm can also be applied to Maxset, and it's now a very popular technique under the name implicit hitting sets. And we've seen it also being automated in CP, and then perhaps makes a lot of sense that we also see it perform really well in Sudabolean. Solve another example. So, to summarize, maybe this short historical progress we have on the x axis, we have the paradigms, and the y axis, we have the different techniques.
00:09:25.892 - 00:10:17.154, Speaker B: And now we look at where have we applied which technique, and we see that all the boxes are ticked, so we exhausted all these combinations. And how is it possible that we have these different paradigms, different communities, conferences and so forth, have these different techniques, but they all somehow seem to be applicable to each other? My conclusion was that these three paradigms are not fundamentally different, really. They are conceptually the same. The algorithms operate in very similar ways. Every time we transfer one to think to another, we had to do some work, some something. But overall, the spirit of the method is really the same. That motivates me to think about, okay, if it's really like that, then, well, we know that we have different views of the same problem.
00:10:17.154 - 00:10:52.684, Speaker B: We know a CP offers, let's say, a high level view of the problem. Sat and PB offer low level views. So maybe when we solve problems, we're interested in solving problems, we should take into account all this information when we do it. And this could help us find this structure, a structure somehow, some elusive term. We don't, can't define what the structure means, but somehow, in this workshop, we keep echoing how the structure seems to be very important. So you've seen different talks. Like, we have this slim or Lns approach to Maxat, let's find a good structure so we can find better solutions.
00:10:52.684 - 00:11:18.302, Speaker B: Adam Sakala was saying, maybe we should look at the structure, try to analyze what's going on. Maybe CNF is not ideal, maybe some other view. And we also had yesterday a talk on bdds. Essentially, it's about recovering structure. I think Dina's talk was quite interesting with this regard, because there we're also looking at the core structure and trying to conclude something. Somehow, the structure is very important. And we also have it in algorithms, too.
00:11:18.302 - 00:11:56.048, Speaker B: We have different algorithms, like detecting cardinality constraints from a CNF. We have bound variable addition, we have abstract cores and max set. And all of this revolves around the concept that there's some hidden structure in the CNF that we have to recover. And if we do that, we're going to get some really, really nice results. Okay, but the nice thing here is that if this problem, like Karen said, if it comes from somebody's head, there is a high level view of this problem. And this is given to us on a platter. So we know these definitions, we have it, and then we can also decompose this into our CNF view.
00:11:56.048 - 00:12:49.452, Speaker B: That's fine, but we can just keep both of these views and use that high level structure. If we know that the problem has a lot of cardinality constraints, we can take that as a starting point, and then we can still use our structure recognition algorithms, because even if we have the high level view, we still might be missing some parts here. Let's try to take both views and exploit everything we have when we're solving problems. And my point here is that also, it's not only important for, okay, let's exploit the structure, but I think it's also important for a way on how we describe our algorithms. I think if we take a high level of view, it may be easier to describe what we're doing with different people and just give us a different perspective. So it doesn't necessarily remove the low level perspective, it just gives us a different view of the same thing. To illustrate that, I'm going to look at core guided search approach.
00:12:49.452 - 00:13:42.424, Speaker B: So let's look at a maxed problem and solve this core guided search, but with a CP view. I'm not going to solve the CP techniques, but just I'm going to look at it from a CP perspective. This is going to be a quick example, something, some details be omitted, but you'll get the overall spirit. But here we have a max set problem, and as we've seen so far, you typically have unit subclauses in max set, which nicely corresponds to the linear objective function. So here we have x one, x two, x three, x four, or all binary variable. So what the idea of core guided search is, does there exist an optimistic solution? Is there some solution where all our objective variables are set to their lower bounds? The answer is no. We get a core, which is a constant clause telling us some combination of these initial optimistic assumptions do not work.
00:13:42.424 - 00:14:14.744, Speaker B: So then we have to take this into account. And what core guided search does. It reformulates the instance to explicitly take into account this new piece of information that we have and how we do that, we create an integer variable. It's the sum of the literals in our core. Since it came from a core, we know the lower bound is one. And then we use this integer to reformulate our original instance. So we no longer have an objective function, x one, x two, x three, that's encoded in our integer variable y.
00:14:14.744 - 00:14:52.730, Speaker B: And then we again have an optimization problem. And then we ask the same question, does there exist an optimistic solution with respect to this new problem? And this new problem is, it's the same as the original problem. It just explicitly has this new core information encoded in it. And I would argue that this is a very nice way of describing core guided search. We could also describe it using clauses, but I think it will take me a bit more time to explain that view. This is also another reason why this high level view is very useful. And if you're really a maxed out person, or like a core guided search person, you might say, okay, it's not exactly the same in practice.
00:14:52.730 - 00:15:25.534, Speaker B: Here we have an inequality sign. It's not an equality. And the way we encode this here, well, there are different ways to do that. And then depending on the way we do it, we're going to refer to different maxat algorithms. Still, this doesn't include all the techniques, but it gives us a very nice view of how things are done. And this is a very similar way in how we actually do core guided search in CP. The main point is that we have this high level view that we can use to really see what's going on and maybe even exchange information with each other.
00:15:25.534 - 00:16:07.754, Speaker B: And this following one is also very interesting. So, since we have these different views, but since they're conceptually the same, sometimes we can test some folklore that's going around in the community. So we know that, let's say you have a new problem, you have a new specific problem you want to solve, and then you come to a set solving person and you say, look, I have this new problem. Can I give you a variable selection heuristic? This will surely improve the performance. The set solving person will probably tell you, no, no, we don't touch that. We have very good heuristics in SAT, we don't change them for specific applications. However, in CP, it's really the opposite.
00:16:07.754 - 00:17:05.384, Speaker B: So if we have a new problem and somebody says, do you have heuristic? Yes, give us the heuristic, give us this domain specific knowledge that we can use, and both of these experiences are valid. So we've seen many times where this where this is really like that. But if the, if these paradigms are not that conceptually different, how come these experiences are still there? So who is right here in the end and why? I think we get lots of these kinds of questions if we view this through this common lens. Overall with this first part I want to argue here is that we have this high level view of the problem. Let's embrace it, let's celebrate this, and let's exploit this to get better algorithms and to also maybe explain our techniques on a higher level and get the message across faster. And here, the lucky, the nice thing here is that it's not about techniques subsuming each other or paradigms subsuming each other. It's really about exploiting all the information we have to solve our problem.
00:17:05.384 - 00:17:53.724, Speaker B: But then the question here is that, okay, if these, if I am right, that really the conceptually they are not different, they're the same, then why do we have this divides in our community? We have a set people, we have CP people, maybe also PB people. Why do we have this divide if the techniques can be interchanged? That's not the defining factor anymore. And as I hinted in the beginning, it's really about the benchmarks. The benchmarks, they shape the identity of our community. So when we say we're a sub solving person, well, we're actually a person that solves that benchmarks that appear in the SAP community. I think this is a very important realization. That's why I say you are what you benchmark.
00:17:53.724 - 00:18:42.372, Speaker B: So if we want to find some portal between the communities, maybe it's not the language that communities are using, maybe it's really about the benchmarks. However, despite this importance of these benchmarks, and despite them having such, I would say maybe even implicit but very impactful role, typically we don't really support benchmark developers. And this is here where I want to tell you my personal story about my experience with these kinds of issues. And I think these issues can be quite serious. So back when I was starting my PhD, I was a very enthusiastic person. I started, I was very happy to work on something called high school timetabling. Yeah, enthusiasm goes down over time, doesn't it? And you'll see one of the reasons, too.
00:18:42.372 - 00:19:19.868, Speaker B: So I was super interested in high school timetabling. I thought, I genuinely thought this was the most important problem in the world. There's so many high schools in the world, and everybody needs to solve this problem. And I had personal connection to the problem too, because I was studying computer science and music and I had. And every semester there were lots of clashes in between these subjects. And then I had to disentangle this somehow, and I got to know the timetablers, and I could really see that the job they had to do was, was really, really tedious, and nobody was ever happy with whatever they did. So I was a computer science student, so I thought, well, great, computers are good at computing.
00:19:19.868 - 00:19:37.908, Speaker B: This looks like a computing problem. Let's try to do that. Let's use computers to solve high school timetable. So I was very enthusiastic and I was working on this. And coincidentally, there was a competition in high school time. Till in 2011, people came up with a very general time tailing formulation. 16 constraints.
00:19:37.908 - 00:20:06.038, Speaker B: Every constraint has some various parameters. There were benchmarks for all across the globe, South Africa, Australia, Brazil, really, you name it. And surprisingly, every country has a completely different set of requirements. So I tried out different techniques, different things, and eventually I somehow got to Maxat and I tried Maxat, and I was getting really good results. So it was really beating the competition winner. Oh, great. What a success story.
00:20:06.038 - 00:20:53.204, Speaker B: Such a complex problem is being solved by Maxat. Fantastic. Surely the SAT community would be very interested in this kind of achievement. How do you think that was received? It was really a disaster. So I submitted this paper several times, and overall, the general sentiment that you get from the reviews, they're good reviews, high quality, they're good comments, but the general sentiment is, this is not interesting. There's limited novelty here, and we have some other problems that generally have this timetabling keyword in the title, so we don't need more. And, you know, as a enthusiastic PhD student starting very vulnerable period, you're looking for some recognition, and then you just kind of get cut down like that.
00:20:53.204 - 00:21:10.184, Speaker B: That's a very disappointing thing. But I submitted these benchmarks to the maxed evaluation, and they've been there for over five years. Great. They've been useful. These. Take these. Looking at these benchmarks, I was got inspired and some of the things I talked about before, I got inspiration looking at those benchmarks.
00:21:10.184 - 00:21:41.284, Speaker B: So I think those benchmarks are very useful. And it was really useful to success story, but it somehow wasn't appreciated, even though the benchmarks are really helping us develop these technologies, techniques. And why was it like that? It's because of this idea of scientific novelty. So what is scientific novelty? So that was a novel, solving such a great problem. And I also had another experience. And don't worry, it won't be all about my experiences. I have some other stories, too, I collected.
00:21:41.284 - 00:21:57.464, Speaker B: So another one is that I had this idea of looking at the solution guided search. It was kind of floating around the community. I packed it up nicely. We tried it in CPO. Let's try it in Maxat. Wow, amazing results in for the income feed track. Surely the community will be interested in these results.
00:21:57.464 - 00:22:23.032, Speaker B: I submitted that and it's again the same story. There's limited novelty and the meta review. The meta reviewer even cited a paper, which is true, it uses a very similar technique, but in different contexts. The context core guided search, where that technique made very little difference, but in my case, it made a lot of progress. But then the paper was rejected. And that makes sense. It's really not novel in that sense.
00:22:23.032 - 00:23:02.514, Speaker B: But fast forward now, five years, and essentially all the solvers in incomplete track are using either solution guided search or some variation of that technique. We had some lots of success with it. Alex Nadel took it, gave his spin on the idea, and now other solvers are now kind of combining different solvers. And so we could say that in the end, it was rather useful for the view. And last cough meeting, Kuldy Mill was complaining that his paper got rejected from Cav because it was only engineering work. Right? Nothing, nothing new there, just engineering rejected. And he submitted it to some other conference, and it got the nomination for best paper.
00:23:02.514 - 00:23:38.482, Speaker B: And if those stories we're not convincing, maybe the following one will be quite interesting. I think everybody knows about the mini SAT paper. Last year, in the last set conference, the SAT community decided to give an award to the miniset paper as the 20 years influential award. Something like that. It's the most cited SAT paper theory and admin Bile was giving a talk instead of the authors. And he recalled the story that when mini set paper was being published, what was the reviewer attitude towards the paper? I reject. It's not novel.
00:23:38.482 - 00:24:09.094, Speaker B: We don't read this, it's not new. So here you look at these stories and you see how also the techniques that we developed, like how we developed things, like in Maxat, CP and PB, you could also argue, like whether those ideas were novel or not. You could argue. And here, in a sense, yeah, you could also argue whether these things are novel. So I think it's debatable. So there's views. What is novelty? We can talk about that, but I think what we can agree on is that, okay, we maybe don't know what a novel is, but we can agree that these things were very impactful.
00:24:09.094 - 00:24:52.924, Speaker B: Definitely there's an impact that helps the community develop. So for that reason, I think that we shouldn't be asking necessarily for scientific novelty if novelty is necessary for progress. But maybe the main thing we should be looking for is how to make this scientific impact. And with that, I'm coming into my talk. So these will be the three points that I had. So one is let's embrace this unifying view. Satsi, PPB, kind of we're all part this really the same story, same techniques, everything the same in that sense, on a high level, of course we can use the high level view to get some structural insights, to use algorithmic benefits and also communicate maybe easier to each other.
00:24:52.924 - 00:25:29.992, Speaker B: You are what you benchmark. Benchmarks are very important if you want to merge the communities. Maybe we need to find ways how can we merge the benchmarks in a way that's very accessible to everybody. And lastly, let's, you know, scientific is great, but maybe scientific impact is our actually main goal. Thank you. And it'd be great if you have some questions and comments. There's lots of other paradigms that are not included here.
00:25:29.992 - 00:25:56.894, Speaker B: Yeah, I forgot to mention. So here I'm just focusing on satcppb. But if you're working in SMT or ASP or some other, you might see that. Yeah, there is similar things going on also in terms of whether they're conceptually different and in terms of benchmarking and all these other things, I agree with that. Nobody has any comments to this. I cannot believe that.
00:26:00.154 - 00:26:20.784, Speaker C: I'm going to say something similar. If you put mip up there, I think you would find conceptually different. I mean, some communities use different analogies when they talk to each other, they tell different jokes to each other. I mean, I think there is a difference.
00:26:21.164 - 00:26:31.904, Speaker B: Yeah, so this I agree with. So ILP, I think there's something, I think it's further away, let's say a little bit. But in terms of like maxed CP and PB, I think that they're much closer than.
00:26:33.564 - 00:26:39.884, Speaker E: How do you propose we judge impact before we've had ten years to see what people do with something.
00:26:40.664 - 00:27:07.444, Speaker B: Yeah, so this is. So the question is like how do we judge impact? So on one hand we had this scientific idea, now I'm saying let's not do that ill defined thing. Maybe impact is better defined. I think it's a bit more difficult to really just define what it is. But some things we can understand, like giving a really good success story, very good benchmark, for example, set. Maybe that's something we know it'll be useful because good benchmarks are really what we need. That's maybe an example of, let's say something that's impactful.
00:27:07.444 - 00:27:31.184, Speaker B: But you could discuss whether it's like novel, if you have these engineering improvements, why they're useful, because they might be impactful. And impact can also be from the theory side too. It's like impacts our understanding and so forth. So I think it's not necessarily something also that we can well define, but I think it's a bit easier to discuss maybe the impact of certain work is rather than its novelty.
00:27:33.064 - 00:27:53.524, Speaker E: I mean, I say this because for about ten years in the UK, we had to justify scientific impact on any grant application and it ended up being such a waste of time that the idea was finally abolished a couple of years back. Because I mean, there is no way of telling what's going to be impactful until.
00:27:54.224 - 00:28:01.194, Speaker B: Yeah, but for example, in these examples here, you see these things and. And what do you think? Are they novel? I don't know, but are they impactful?
00:28:02.334 - 00:28:03.862, Speaker E: Would you have said that at the time?
00:28:03.958 - 00:28:14.478, Speaker B: I would have said that at the time. Okay. But again, yeah, I agree. It's a, it's a collective thing, we have to decide them somehow. Jacob had something over there.
00:28:14.566 - 00:28:56.302, Speaker D: Yeah, so I agree with. So I am really missing Mip among the, you know, Sat, CP and PB. I think for me, like one of the really things I get away from a week like this is through. I believe these are different communities, they have different ideas. But what I think happened when they talk to each other and what we're maybe missing out on because I mean, SAT, CP, PB, SMT, people do talk to each other. I think we're not talking so much to the or community. I'm trying to understand why I think we're missing out on something there because I think all your examples are like, these were actually ideas that did arise in different communities that are conceptually different.
00:28:56.302 - 00:29:25.672, Speaker D: I think these communities do have conceptually different use of these problems. But when communication happens, like, you can steal the best ideas from your neighbor communities and implement them in your setting with some tweaks, and it works for you as well. So it's like. I would say that it's an argument in favor of communication and diversity are not all the same. They should not be. But there's a huge benefit from communication and understanding.
00:29:25.848 - 00:29:57.558, Speaker B: Yeah, I definitely agree there's benefit from communication and we do talk to each other. That's how I came up with those ideas, for example, for importing from one paradigm to another. But still, when we do certain analysis, like we look at the CNF level. For example, can we extract some analysis on the CNF level for some real world problems? Like we've seen Nina's thought, for example, there was these cores and if you look at the name of the benchmark it says at most 24 out of 55. But we don't see that in the benchmark. We just see the CNF and we don't have, let's say too much. Like we don't have any benchmarks where you have both views, for example.
00:29:57.558 - 00:30:02.874, Speaker B: It's usually one or the other. I think it's something that we could look towards.
00:30:03.974 - 00:30:53.864, Speaker C: Yeah. I'm certainly not against diversity, but I think some community are based on paradigms and some communities are based on goals based on like to take health or something. So we want to use any trick that anybody knows in order to improve people's health. But I find that uh, you know, physicists think different from mathematicians. I mean algebraists think different from geometry. I don't want to emphasize the differences but, but there are, but I don't want to say that there are such differences. I.
00:30:53.864 - 00:31:55.924, Speaker C: Difference. But, but, but I don't think uh, we're going to get uh, uh, many people who, who have their, their feet in, you know, in all camps. So, so I, you know, I, I'm glad to be at a university where, you know, where I've got, I've got physicists to call on and biologists to call on and musicians to call on and whatever, like, and historians, et cetera. But I also want to have students who have certain talents that I can develop their talents because I don't think it's my function to teach them to change their own style of thinking.
00:31:58.734 - 00:32:12.714, Speaker B: Okay, that's a good point. But here, the nice thing with, I think with this unified view, we don't have to give up on our way of thinking. If we like to think on CNF, let's do that. But let's also keep in mind this other high level. So let's just try to do everything.
00:32:15.014 - 00:32:17.022, Speaker A: Can I have a comments on the first point?
00:32:17.118 - 00:32:18.046, Speaker B: Oh, excellent. Yeah.
00:32:18.070 - 00:32:48.788, Speaker A: So I found that it's not that easy to synchronize information between two constraints, uh, to kind of constrain. For example, you have a high level constraint and you low level constraint, it's easier to infer. Like if you have uh, infer some assignment is easier to synchronize information from low level to high level. But, but if you learn a clause, if you learn some constraint from the, in the high level constraint, it's very hard. It's time consuming. If you pass this information to low level, you have to break this high level constraint into low level. And also if you learn a constraint from low level constraint.
00:32:48.788 - 00:33:01.984, Speaker A: And it's also like a time consumed if you compose multiple learn cloud, learn learn constraint. Like to pass it in a high level. So this kind of communication is very time consuming in practice.
00:33:03.604 - 00:33:04.604, Speaker B: Okay, that's a good point.
