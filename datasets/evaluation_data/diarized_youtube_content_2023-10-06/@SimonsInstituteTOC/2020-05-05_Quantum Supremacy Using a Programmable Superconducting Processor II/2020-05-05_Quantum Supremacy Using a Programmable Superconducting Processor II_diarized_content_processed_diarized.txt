00:00:00.320 - 00:00:20.526, Speaker A: At Google. Great. Yeah. So I just want to take a moment to thank the conference organizers for inviting me and also for throwing this together digitally. It's really great that this can still happen, and honestly, I think it's been going super well. Maybe I just jinxed myself, but we'll see. I suppose so, yeah.
00:00:20.526 - 00:01:01.914, Speaker A: I'm Julian Kelly. I'm one of the hardware team leads at Google AI Quantum, and I'm going to be talking about a recent result, quantum supremacy, using a programmable superconducting processor. So I want to talk about this experiment really framed as a system performance benchmark. And then I'm going to talk about some of the things we did to actually get the experiment to work. So I'll talk about the hardware, the sycamore processor in the stack. Then I'm going to talk in depth a little bit about software calibration and how we got that to work. So we'd like to show this plot that demonstrates our Google hardware development path, and we'd like to understand the state of the hardware, both by looking at the number of qubits, but also the limiting error rate of the devices.
00:01:01.914 - 00:01:57.914, Speaker A: And what you can see here is that you have an insufficient number of qubits, or if you have a lot of qubits, but their error rates are high, essentially everything that you can do is classically simulatable. This is really interesting point right here, where if you have sufficient qubits with sufficiently low error rates, you can demonstrate quantum supremacy. What's important about this is that this is really just a milestone on our development trajectory. We're planning to investigate NISC and near term applications, but eventually build towards a useful, error corrected quantum computer. So in 2012, John Preskell put forth his notion of quantum supremacy, when we will be able to perform tasks with controlled quantum systems by going beyond what can be achieved with ordinary digital computers. And as someone who works on a hardware, this is my favorite quote in that paper where he asks, is controlling large scale quantum systems merely really, really hard, or is it ridiculously hard? And after this, we get to conclude that it is merely really, really hard. So here's the article.
00:01:57.914 - 00:02:34.988, Speaker A: We have a whole bunch of authors on here, and this is a bit more fun way of looking at them. Here's a picture of our team. So what is the quantum sequencing experiment? And again, I want to pose this really as a benchmark of understanding the hardware. So when we do benchmarking, we want to be able to measure component error rates that are predictive of algorithm performance. So, for example, if we measure the fidelity of single cubic gaze, two cubic gaze measurements, it sure would be great if we could then compose an algorithm that looks something like this and say that the fidelity of the algorithm is the product of the gaze and the product of the fidelity. This isn't something that you take for granted. Experimental quantum computing is hard.
00:02:34.988 - 00:03:06.312, Speaker A: It's really, really hard. And in fact, we just heard an hour talk about how you can't just assume that this is the case. This is something that's a little bit tricky and subtle. And so, in order to get this to work, you need good hardware, and you really need a sophisticated understanding of how your hardware actually performs. So we want to put this to the test in the quantum supremacy experiment, where basically benchmarks algorithm performance, and we're comparing it to this model from the component error rates and seeing if we can get these to match up and see if we can really understand the system. So, this is the processor that we're doing it on. It's a sycamore device.
00:03:06.312 - 00:03:39.830, Speaker A: It has 54 qubits and 88 couplers. The actual device that we used had one broken qubit, but, you know, that's experimental physics for me. So, here are the results of benchmarking a sycamore device. And in here, we're looking at an integrated histogram, or ECDF, and I'm plotting the poly error rates and also the measurement errors. And here is these distributions summarized in a table. We're going to look at the single qubit error rates, the two qubit error rates, the two qubit error rates per cycle. This is kind of, kind of similar to what Joel is talking about when we do our benchmarking.
00:03:39.830 - 00:04:08.684, Speaker A: We compose a cycle of a two qubit gate and two single qubit gates. And we also look at the readout error. And importantly, what we're really interested in is the simultaneous operation. So, here, the single qubit gate errors in the poly representation are 0.16%. When 53 qubits are running at the same time, two qubit gate errors are 0.62, and the readout error is around 3.8%. We also have the isolated plotted here, but honestly, this is a bit more of a, like, curiosity in some sense, because this is what's predictive for running algorithms.
00:04:08.684 - 00:04:39.398, Speaker A: So what we're claiming is that we'd like to be able to predict these large algorithm performance based on distributions. So this is what it looks like mapped across a physical device. So each one of these x's is a qubit, and the color corresponds to a single qubit error rate. And these boxes in between represent the error rate of the two qubit gates. So the circuit that we're actually going to run is this cross entry benchmarking circuit. And the idea is that we're going to sample the output of a pseudo random one circuit. And some of the features that you notice of this is that it has a very dense structure, so it's highly parallel.
00:04:39.398 - 00:05:18.482, Speaker A: So, for example, there's many single qubit gates happening at the same time, and then there's also many two qubit gates happening at the same time. And we're going to go to a high circuit depth. And then basically at the end of it, we're going to assign some algorithm fidelity using a classical code simulation. This is for the circuits that we've chosen to be easy and verifiable to, to understand our system. So this is what the component model looks like. So again, we're going to say that the fidelity should basically be the product of the gates and the readout and taken from this map, depending on exactly which qubits you're going to use in your circuit. And we're going to fix the depth at 14 cycles of these single and two qubit gates, and then we're going to scale the number of qubits.
00:05:18.482 - 00:05:58.864, Speaker A: And what we see is that there's a model prediction that looks like this. It basically looks like an exponential decay that you'd expect from multiplying fatalities. And it's pretty uniform because the devices perform pretty consistently. So when we actually go compared to the experiment, we compare the model to the full circuit, and we also have these simplified circuits that Sergio talked a little bit about yesterday. And essentially what we see is that the full model matches the simplified prediction, matches a prediction from the component error rates. And this is working over 16 decades of Hilbert space, which is pretty normal. So what's important about this is this is saying that we understand our system and that there's no new kind of crazy features of quantum mechanics creeping in these higher dimension over spaces.
00:05:58.864 - 00:06:39.484, Speaker A: So why did this actually work experimentally? As we heard before, Crosstalk is a really important consideration. We've done a lot of engineering to make the crosstalk low on the device and use some mitigation strategies. The calibrations that we do are representative of the algorithm that we want to perform, which is important. And also these random circuits depolarize systematic errors, which is another thing that Joel touched upon before. They don't accumulate in an uncontrolled way. And it's based off these confidence in these error models that we can then make these predictions into the unsimulatable quantum supremacy space and say we believe that we got 0.2% fidelity and that to a depth of say 20 cycles is 53 qubits.
00:06:39.484 - 00:07:17.724, Speaker A: Okay, with that being said, I want to go in and talk a bit about the actual hardware and how we got these results. So this is the state of the art of our devices in 2015. Here's an example of a nine qubit linear chain. I'm particularly fond of this device because I did my PhD thesis on it. And one of the things that you note, if you look at this and you're thinking about scaling it up, it seems to be a bit tricky. You see that the control wiring takes up like half of the chip. If you want to go to two dimensions, where do you put the qubits? How do you get to 50? So one of the important technologies we introduced in the last number of years are these scalable designs using flip chip.
00:07:17.724 - 00:07:50.124, Speaker A: The basic idea is instead of having one chip, we partition this across two different chips. So we take the qubits, we put them on a qubit chip. That gives us room to make a two dimensional array. And then we have a control chip that brings in control wirings. We then electrically and mechanically stitch these chips together using indie and bond slippers. Here's an image of what one of these prototype devices like from a few years ago here. There's just six chips on this qubit device here, and you can see every one of these little dots is some indian bump ones.
00:07:50.124 - 00:08:26.644, Speaker A: Then we have a control chip here that has wiring coming in to control the devices. We then stack them together and we get this assembly that looks something like this with these two chips electrically connected. So we take this assembly, we put it into a our package where we have a circuit board where we make electrical connections to the chip. This goes inside some electromagnetic and infrared shielding. We then bring in an additional about 100 or more coax connectors to make connections to control the qubits. This package then goes into the bottom of the dilution refrigerator at around ten Millikelvin. And then we have lots of wiring going up to room temperature.
00:08:26.644 - 00:08:53.551, Speaker A: So at room temperature we have custom scalable electronics that we used to generate our control signals. Here's an example of one of our boards. We give it the name Heron board. It's got eight analog outputs, and these can be used to generate zero to 400 MHz arbitrary waveforms. And then we also have the option of up converting them to 6 drive the qubits directly. This is just one card. So we can also fill out, say, a crate of these, something like this.
00:08:53.551 - 00:09:34.114, Speaker A: And then we actually use about four of these crates to power a full synoor device. And here's all the coax going into to manipulate the processor. So the very first large 2d chip that we made was a bristlecone device, and this had 72 qubits. It looks something like this. And the qubit geometry, each one of these x's represents the qubit, where they come together represents a coupling between the qubits, where you can get entanglement or interactions. So one of the most fundamental things to understand when you're evaluating a quantum computing architecture is to understand how the coupling between the qubits is. So in this particular device, we use a fixed capacitance and always on coupling.
00:09:34.114 - 00:10:17.092, Speaker A: And here's an example from an older device where you see that we get that by basically putting the qubits close to each other in the absolute capacitance. This is a great, it's a simple, proven technology. A number of years ago, we were able to get 99.5% cz. And the idea is, it works by, the capacitance is always on, but we can modulate the coupling by using tunable qubits and changing the detuning between them to turn it on and off. So one of the system considerations that comes out of this is that what this means is that when you're doing interactions on two qubits, their neighbors, because they also have a fixed coupling, you have to perform echo sequences on them to cancel any straight coupling interactions. And this leads to imperfect parallelism of the gates you can employ.
00:10:17.092 - 00:11:02.566, Speaker A: So if you have an array of qubits like this, where we keep, it's a dot, we're doing, say, CZ gates between them. These qubits in the gray are actually forced to idle with echo operations, and they can't be doing something more useful. So one of the advances that we introduced in sycamore was this tunable coupling architecture. And what we found is that being able to turn the qubit interactions on and off are really helpful for making the system overall look better. And we've done this in a scalable 2d architecture, where we essentially introduced an extra tunable coupler device to mediate the interactions for the two keys. So what does that actually look like in experimental data? So here's my plot of that. We're going to take these two qubits, we'll just call them QA and QB, and we're going to put them at the same frequency, like they're exactly on resonance with each other.
00:11:02.566 - 00:11:44.856, Speaker A: And then we're going to sweep the coupler bias, which modulates the coupling between the qubits. What happens is that if we excite one and there's some coupling, we'll see this excitation will swap back and forth between the qubits and the, as long as they're on resonance with each other. And then if we change the coupler value over here, we can turn on a much stronger coupling. That takes make the excitation swap back and forth pretty quickly. What's very important, it's right in between, we actually have a zero crossing and the coupling, and we can essentially put the qubits perfectly in resonance with each other and have the excitation stay in one qubit and not move over to the other. One thing I want to point out is that this is experimental data. Actually, someone from our team put this on a slide, and I didn't really believe it.
00:11:44.856 - 00:12:21.870, Speaker A: I made them label it. Let me tell you that when I joined the field a number of years ago, superconducting qubits weren't making data look anything like this. So, it's fun to see these types of things come out of our more recent devices. Of course, this is the sycamore processor that we develop that uses this technology. So, with everything, you really need to evaluate the trade offs, thinking about making changes to the hardware, and I want to talk about the pros and cons of that in the future. Tunable coupler. So, as I touched on before, this coupling offer moves straight coupling, and you get kind of natively a two x increase in potential parallelism from gates by doing this.
00:12:21.870 - 00:13:09.604, Speaker A: So for example, on frizzle cone, before, we could have some maximum gate density looks like this. But because we can interactions off, we don't have to do these echo operations. We can actually have every single gate participating in its own two qubit gate at the same time. Additionally, the coupler makes it easier to access a wider gate set. So we can make CZ gates, swap gates, I swap gates, square root of iswap gates, and in fact, we can make everything in between. So, if we look at this 2d phase space of I swap kind of gates and conditional phase gates, we have a recent publication where you can actually access all of these with high fidelity and these sort of eyeswap light gates with a little bit of conditional phase is what we're using in the experiment. So, of course, with everything, there's some cons about introducing something like this, when we add another element, we're opening ourselves up to added decoherence and additional noise.
00:13:09.604 - 00:13:46.746, Speaker A: It costs an additional two control lines per qubit, which is really nothing to scoff at. That's actually expensive. And something not to be overlooked is that this increases the calibration complexity for our devices. So you can only measure the coupler indirectly, but you really need to have good control over it in order to implement high fidelity operations. So this brings me to our quantum hardware principles, how we think about this, the system design that we're doing. So we recognize that system performance is really what's key for algorithms. And that means we care about things like simultaneous performance, parallelism, the accessible gate set.
00:13:46.746 - 00:14:17.688, Speaker A: And we recognize that things like coherence times and isolated gate fidelity are only part of the story. And again, this also goes to the point that Joel was making in the previous talk. So our strategy is to make highly configurable processors with tunability. Strategically introduced in bristlecone, that meant tunable qubits. Tunable qubits are something that we've been working with for a very long time in our team, basically the whole history of it. And then in Sycamore, we're also introducing tunable couplers, which is also something we've been looking at for a long time, but now they're sort of deployed at scale. What this leaves us with is a very complex system.
00:14:17.688 - 00:15:02.162, Speaker A: We have a control challenge, and what we're going to do is we're really just going to embrace this control challenge, both with software and also actually artificial intelligence. So now I'm going to talk a bit about how we actually calibrate our processors and get them to work. So the process of calibration is learning to execute quantum logic. And really, that's just taking a physical device, like a physics experiment, and getting it to a point where you can execute single, two qubit gaze measurements, kind of whatever you want, you can program it. Some of the challenges of this is that qubits cannot be controlled identically. So some of that can come from, for example, the inhomogeneity in qubit parameters, remanufacturer qubits, are not identical, but really, this is actually a general property of quantum systems. They're analog.
00:15:02.162 - 00:15:49.680, Speaker A: Any inhomogeneity in your control system also means that you have to grapple with this. Additionally, these optimized parameters can drift in time. We learn these control parameters by carefully sequencing, bootstrapping experiments. So, for example, if you go into the lab, you may do some spectroscopy, you have to do that before you say, do a robbie floppy experiment. And there's a whole bunch of things you have to do in between before you do your, say, two cubic gate calibration, what we find is that we're calibrating something like ten of the two parameters per qubit to get up to the performance levels that we're happy with. So what we use for this is this idea called optimus, which is essentially an artificial intelligence for calibrating quantum processors. And the way to understand this is that these calibration experiments, they have a common structure.
00:15:49.680 - 00:16:25.144, Speaker A: So every experiment that you want to perform, there's some prerequisite experiments that you have to do first. This is sort of this bootstrapping thing I talked about where you want to do spectroscopy before you do raw flopping. And then each one of these experiments has some waveform sequence to learn to control parameters. So for raw b flopping, you might do some drive on the xy drive, and then read out the state of the qubit. And then you can, for example, fix the amplitude and sweep the duration. This gives you some data where you see the population of the qubit oscillating between zero and one. And then we can use an analysis function that looks at this data and says, oh yes, to get a pipulse, we're going to use a duration of, say, ten nanoseconds.
00:16:25.144 - 00:17:08.354, Speaker A: Then what we do is we basically take that parameter and we update a database that we have of our control parameters and say this qubit needs a pipe illustration of ten nanoseconds. So all this represents one calibration. And we basically say this is now going to be a node in a graph, and we're going to connect these nodes by directed edges. So these directed edges capture this bootstrapping idea where we need to perform these yellow calibrations before we perform this blue calibration, which needs to happen before we perform this green calibration. And essentially what we can do now is we can reduce this calibration problem into a graph traversal algorithm we can apply. So this is what a calibration graph looks like for Sycamore, actually, just for two qubits. And you see that we calibrate all kinds of things.
00:17:08.354 - 00:17:43.378, Speaker A: So we start over here, we do some really fundamental device parameters, we explore first. We then move on to calibrating things like single qubit gates readout. We go back and forth, look at some more device parameters, and end up fine tuning readout and fine tuning our two cubic eight calibrations over here. Every one of these nodes represents an experiment and the analysis associated with it. And we have to get through thousands of these calibrations in order to get a second word processor working with high fidelity. So I'm going to step through some of the calibrations that we do, mostly because it's kind of fun to look at. So here's an example of the really kind of rudimentary hello qubit.
00:17:43.378 - 00:18:14.424, Speaker A: Raw b oscillations we do early on in the graph. And you see that there's some oscillation here. We pick off this point to say this is a coarse high amplitude, and this is plotted for all three qubits used in our processor. And this is our favorite broken qubit that's missing right here. Here's an example of what calibrating readout looks like. We call these readout clouds, where you compare single shots of. We compare the zero to one state, then we can draw this discrimination line that tells us when we get a shot, do we label it in zero? Do we label it in one? And then this is what it looks like across the device.
00:18:14.424 - 00:18:42.352, Speaker A: And then this is how we calibrate our two qubit. I swap like interactions where essentially it's just a raw b flopping between two qubits where we pass the excitation of one to the other. We just identify this maximum. And you can do that for all the 86 couplers that we use. Nics pretty much are plotted between the qubits. So now it's a particularly sophisticated calibration that we do that has to do with the two qubit gates. So I'm just going to walk you through this a little bit.
00:18:42.352 - 00:19:20.624, Speaker A: So right here, this dashed green line is the two qubit XCB cycle fidelity, where the qubits are operating in isolation. And then if we then go say, instead of just operating two qubits, we're trying to operate the entire device, we then get this blue curve. What we see is we've gotten like a pretty significant degradation performance. Again, this is an integrated histogram, and we're looking at the polyurethane x axis. We get a kind of disappointing error per cycle of about 1%. And it turns out that the solution to this is essentially just calibrate. Instead of calibrating isolation and operating in parallel, calibrate in parallel and operate in parallel.
00:19:20.624 - 00:19:54.938, Speaker A: And what the difference actually is, is that we have this model right, right here where we see this one. One of these parameters is significantly different. If we're looking at the difference between the isolated versus parallel calibration this one really sticks out. What we see is we get something like a five degree control angle shift. And what is happening here is that essentially, the difference between isolated and parallel gates is that we're having these single qubit Z frame updates that sandwiches two qubit gate. And this is actually pretty simple to just compensate for in software. And that's.
00:19:54.938 - 00:20:40.188, Speaker A: That's effectively what we're doing in this parallel calibration. What I'll point out is that this is only about, say, a median five degree phase shift, which is great, because if you imagine either just operating two qubits at a time versus operating, say, 22 qubit gates at the same time, we're only incurring, like, a five degree control angle penalty, which is great. And it makes it a lot easier to actually handle this type of correction software. What we actually get for doing this is we improve the error per two gigabyte of about 0.2%, which really doesn't sound like a huge amount. But if you look at how that affects the final experimental fidelity, you look at the ratio of fidelity that we would have gotten erased to the whole 432 cubic gates that we actually execute in the algorithm. And that's like a factor of over two, 2.4
00:20:40.188 - 00:21:14.976, Speaker A: x. And Sergey actually mentioned yesterday that not having a factor like this would significantly degrade the types of claims that we. Okay, so the purpose of Optimus is allows us to distill experimental knowledge just into code that can be deployed. We use this for calibration, but we also use it for drift mitigation. This is an enabler of the performance that we needed for quantum supremacy. And we use this in all of our Google processors for both hardware and also for algorithm development. So I want to talk about just another interesting and tricky calibration problem, which is the frequency placement problem and how we solve that.
00:21:14.976 - 00:21:58.928, Speaker A: So, as I mentioned before, we use tunable qubits, and qubits are great, and they're also tricky because they're highly configurable. And what that means is that we can actually choose for each qubit what frequency we want to operate it for single qubit gates, what frequency we want to operate for two qubit gates, what frequency want to operate for. Readout. So if you imagine we have this sycamore processor, and we take these two qubits, we can choose what the space between the zero and the one state is for single qubit gates for each of them. Then we can also choose where we want them to be interacting with each other. We have something like 100 meaningful frequency choices per qubit. And that means our solution space is actually quite large, something like 100 to the forensic number of qubits, because we have four frequencies to choose per.
00:21:58.928 - 00:22:45.194, Speaker A: And that gives you something like ten to the 392 for sycamore, which is a little bit ridiculous, ridiculously large, especially, I think, at the context that we're set about exploring Hilbert spaces of ten. And so, so there's many experimental constraints to consider here when we're picking these frequencies. So we care about the coherence of the qubits, the straight coupling between them, pulse distortion, and all of these considerations are highly frequency dependent. So what we've done to actually deal with this is to develop a custom snake optimization algorithm, snake optimization algorithm that allows us to maximize assistant performance. And we've got a white paper. We're going to talk about this in a little more detail in the preparation. So that all is nice, but let's try and visualize what this actually means.
00:22:45.194 - 00:23:25.542, Speaker A: What are the inputs and the outputs of this? So, if we take the ideal case where again, this is a representation of the processor, each one of these pixels represents the qubit frequency. If we don't have any constraints, the problem is degenerate, and all the qubits are the same frequency, we're just going to make it this gray color. Now, if we consider, for example, that there is dephasing noise or flux noise on the qubits, then what we would like to be able to do is push the qubits to their flux insensitive points. So these are tunable transmons. There's a particular frequency that we can push them to that makes them insensitive to that. And what you see is that there's inhomogeneity in the qubits. Like there's the processor from fabrication, where the parameters are a little bit different.
00:23:25.542 - 00:24:02.748, Speaker A: This flux is sensitive frequency, frequency, but otherwise the qubits are all pretty similar. Then we can apply, for example, a constraint of. Now we're going to also worry about pulse distortions. And what that does is that it tries to minimize the difference in frequency between neighbors so that they don't have to move as far to get to an entangling point. And the effect of that is this pulls this entire region of the device closer to this sort of homogeneity. If we then introduce nearest neighbor parasitic coupling, that penalizes qubits being perfectly on resonance with each other, and we see this sort of checkerboard pattern emerge, which is really cool, because that matches your intuition. And if we also include next near saber parasitic coupling.
00:24:02.748 - 00:24:29.534, Speaker A: Then we actually get an even more splitting of this checkerboard pattern. And then lastly we can introduce things like worrying about two level state coherence defects or stray microwave modes in our processor. We get some configuration that comes out of the optimizer that's like this. So this is great. You know, it's a nice story, there's pretty pictures, but how does this actually perform? Is it actually useful? Is it all talk? So the way that we're going to test it.
00:24:38.194 - 00:24:59.794, Speaker B: I think we've lost Julian or we've lost his screen share. Anyway, uh, he's also. Mic is also muted. Hello, Julian, are you out there?
00:25:00.454 - 00:25:02.954, Speaker A: All right, is that. Can you guys hear me?
00:25:03.294 - 00:25:06.734, Speaker B: Aha. We got you back. Okay, great.
00:25:06.854 - 00:25:08.294, Speaker A: I'm gonna share my screen again.
00:25:08.414 - 00:25:09.230, Speaker B: Share your screen again?
00:25:09.262 - 00:25:15.974, Speaker C: Yeah, maybe time to ask some of the, some of the questions that are in the. The questions.
00:25:17.154 - 00:25:55.634, Speaker A: Yeah, so I'm actually about to get through in about two or three minutes and I'll should be at the end. And it's amusing that I definitely cursed myself at the beginning of this talk. So, you know, what are you going to do? So I just want to wrap up by talking about how we actually benchmark this to see how it's working. So we're going to use cross entry benchmarking. XCB is a two qubit gate and two single qubit gates for each pair of qubits. What we're going to do is we're going to optimize the frequencies with our custom optimizer on the device. We're then going to calibrate all the gates and then we're going to benchmark them and see how it performs.
00:25:55.634 - 00:26:47.044, Speaker A: So as we're working towards prepping for this quantum supremacy experiment and getting the sycamore processor online, we had a very talented expert human working on solving this frequency optimization problem. And this is the distribution that they were able to get by working on this carefully over a matter of weeks with their intuition and experimental feedback. And we deployed our first model and we saw that it was a bit disappointing. We were getting a cycle error above 1% and there's a significant gap between the model and the human. But at that point with this framework, we're able to go back, fine tune our models, understand how to actually weight the errors and the cost functions from each other, and deploy a new model. And this model actually worked better than what the human was able to do or what they had been working with. And I want to point out that this is sort of like weeks of human time versus, say, seconds of optimizer time.
00:26:47.044 - 00:27:24.884, Speaker A: And in fact, we can then continue to do this and continue to improve the performance and make the device work even better. And this has been an important enabler of getting the performance needed for demonstrating quantum supremacy. So, in conclusion, we recently completed this quantum supremacy experiment. It really should be seen, I think. Well, I'm biased because I do hardware as a hardware system performance benchmark, and we've been able to show low simultaneous error rates across all the qubits and operations. We've been able to run a 53 qubit algorithm where the performance was predicted by these component error rates. And we're really viewing this as a milestone towards a useful computer.
00:27:24.884 - 00:27:37.624, Speaker A: We talked about a number of hardware advances of the full stack featuring the sycamore processor, and we've talked about how we operate this complex device using custom AI and software, both Optimus and snake. So thank you.
00:27:39.684 - 00:28:03.704, Speaker D: Thank you very much. So we have time for questions. Maybe I can start with some of the questions that have been posed. And sure. One of the question is, does the NISQ exploration mean a detour from straight path towards QEC, or are the incremental steps pretty much aligned?
00:28:05.004 - 00:28:30.484, Speaker A: Yeah, so they're pretty well aligned, I would say, in that, like what you need in order to do error corrections, you need large arrays of qubits with high fidelity operations that you can perform reliably, needs to stay calibrated. All these types of things are pretty similar. A lot of the fundamental technology has a good overlap, and that's why we chose this path, because we want to be able to explore the near term and also be building towards the long term.
00:28:32.784 - 00:28:51.444, Speaker B: I'd like to ask one quick question. Among the many, many things that you had to deal with, you threw in the fact that there are still tlss. That's sort of an old subject in superconducting qubits. So how often do you encounter relevant TL's among these 54 qubits?
00:28:52.464 - 00:29:03.174, Speaker A: Yeah, so that's a great question. And that was actually a lot of the inspiration for this frequency optimization problem. Like, we know that all of our, like every qubit, they have TL's defects. They can couple to, and they're all at.
00:29:06.914 - 00:29:08.774, Speaker B: Yes, hello, Julian.
00:29:12.274 - 00:29:13.734, Speaker A: Quantum error correction.
00:29:14.194 - 00:29:15.494, Speaker B: Yeah, I think so.
00:29:21.034 - 00:29:22.214, Speaker A: I think I'm back.
00:29:22.634 - 00:29:23.882, Speaker B: Julian, you're back.
00:29:24.018 - 00:29:24.930, Speaker D: Okay, great, thank you.
00:29:24.962 - 00:30:06.664, Speaker A: Yeah, my chrome session keeps crashing for some reason. So, yeah, the tlss were really some of the inspiration for this frequency optimization, formalizing the problem, because every pivot has different tlss, and there are different frequencies, and they can also move. And so, basically, the purpose is that we can identify them, know where they are, and then we can pick frequencies to avoid them. And that actually works pretty well. I would say we see a little bit of a tail in the distribution that's related to TL's, can move, like, before and after calibration. But from the performance that we're getting that you can see that we're doing a pretty good job avoiding them. They're not necessarily catastrophic.
00:30:08.404 - 00:30:19.668, Speaker C: Can I jump on this, on this question to ask if that's specifically the problem with the 54th qubit, which is broken. Was that the problem, the presence of a TNS or what is broken?
00:30:19.716 - 00:30:28.746, Speaker A: No, it just, you know, it just. There's a broken line or something, and you go cool down a fridge, and 53 qubits are working, and one of them doesn't want to talk to you, and that's life.
00:30:28.810 - 00:30:29.534, Speaker C: I see.
00:30:29.914 - 00:30:32.306, Speaker A: Yeah. Okay.
00:30:32.330 - 00:30:44.094, Speaker D: So maybe a last quick question. So, Andrew Yang Wo asks, are you able to perform intermediate measurements and adaptive. Adaptive gates based on classical feedback with your processor?
00:30:46.594 - 00:30:54.762, Speaker A: You mean, is there feedback on the processor right now? So one can do. Sorry, what was that?
00:30:54.938 - 00:31:01.974, Speaker D: If you can perform intermediate measurements and then adaptive gates, depending on the outcome of the measurement.
00:31:02.314 - 00:31:14.094, Speaker A: Yeah. So that's. That's a technology that's been demonstrated a lot in the field of superconducting cabinets. We're not demonstrating that here, but there's really nothing that stops us from doing that in our technology.
00:31:14.394 - 00:31:15.418, Speaker D: Okay, very good.
00:31:15.466 - 00:31:15.698, Speaker A: Great.
00:31:15.746 - 00:31:17.694, Speaker D: Well, thank you very much.
00:31:18.954 - 00:31:19.754, Speaker A: Great. Thank you.
