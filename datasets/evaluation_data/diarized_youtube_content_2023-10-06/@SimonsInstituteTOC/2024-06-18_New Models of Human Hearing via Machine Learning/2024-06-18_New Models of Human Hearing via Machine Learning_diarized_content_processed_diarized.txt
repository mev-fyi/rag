00:00:00.080 - 00:00:24.394, Speaker A: So we'll start with Josh the third. All right, thanks. So I'm going to tell you a little bit about how we study how people hear. So I'm talking to you now, and there's a Sound wave that traveling through the air, and it reaches your ear, and it causes your eardrum to wiggle back and forth. Right. And just from the pattern of wiggles, you're able to understand what I'm saying. Right.
00:00:24.394 - 00:01:00.114, Speaker A: So the sense of hearing is pretty amazing and pretty powerful. It's also quite fragile. So this is kind of a depressing picture if you're hitting middle age like I am, because this is a picture that shows the onset of hearing loss with age. So this shows the average audiogram in different age groups. So the audiogram is the thing that they measure when you go to the doctor and they play little beeps, and then you press the button when you can hear them. So it's measuring your detection thresholds as a function of frequency. So the x axis here is frequency, and then the y axis here is the detection threshold.
00:01:00.114 - 00:01:16.634, Speaker A: And this is expressed relative to somebody that is considered to have normal hearing. So if you're zero, you're normal. And then the different lines correspond to different age groups. So you can see the people in their twenties and thirties. Life is pretty good. Then. I remember that.
00:01:16.634 - 00:01:45.954, Speaker A: And then after that, the thresholds go up, particularly at high frequencies. You can see for somebody who is in their sixties, they're at these high frequencies, four and 8 khz. There's about 30 decibels of threshold elevation. On average, 30 decibels. That's about the difference between talking quietly and shouting. It's very significant in real world terms. This makes it difficult for people to communicate, causes social isolation.
00:01:45.954 - 00:02:12.146, Speaker A: It's a massive public health problem. If you live long enough, you will almost surely start to go deaf. It's just one of those things that happens. It really dwarfs anything else that happens in any other sensory system. There's lots of vision problems, but they're mostly optical, and you fix them with glasses. These are problems with signal processing that changes in the ear if you are suffering from hearing loss. And I've had several conversations with workshop participants along these lines.
00:02:12.146 - 00:02:37.914, Speaker A: I encourage you to get hearing aids. They will probably help, but they don't work nearly as well as one would like. So you go into a restaurant, still pretty hard to hear. And we think the challenge of actually engineering good solutions to this huge public health problem is really limited by our understanding of how we hear. So I run a research group at MIT. Called the lab for computational audition. We work at the intersection of psychology and neuroscience and engineering.
00:02:37.914 - 00:03:18.578, Speaker A: And really, our main long term goal is to build good predictive models of human hearing. What I mean by that is that we would like our science to culminate in a computer program that takes sounds as inputs and that can make all the kinds of judgments that a person could make about those sounds, and that we think that if we're successful in that goal, it will enable us to do lots of interesting things, scientific and applied. And in particular, we think it will transform our ability to make people hear better. So I'll tell you a little bit about how this is going. So, from where I sit, the peripheral auditory systems, all the stuff in this picture is pretty well characterized. So sound enters your outer ear, travels down the ear canal, your eardrum vibrates back and forth. Those vibrations are transmitted via these three small bones.
00:03:18.578 - 00:04:01.334, Speaker A: These are the smallest bones in your body, and they enter the cochlea. This thing looks like a snail. So that's the organ that does the transduction in hearing, that turns the mechanical vibrations of sound into electrical signals that are carried to your brain via the auditory nerve. So people have been working on this for many, many decades. It's been a very successful research program, and it is culminated in what are now very standard models of this stage of the system. So there's a whole bunch of different variants of this that many people in the field work with. The details of this don't really matter, but we really understand this stage of the system in the sense that this is a computer program that can take sound as inputs, and that generates very accurate and well validated predictions for the responses in your auditory nerve.
00:04:01.334 - 00:04:46.558, Speaker A: All right. We often will depict the output of models like this with pictures, images where, conventionally, time will be on the x axis, and the cochlear channel, plotted in terms of frequency, will be on the y axis, and then the gray level here will be of the firing rate in a particular auditory nerve fiber at a particular moment in time. So it looks a little bit like a spectrogram. All right, so, most of the time in our group, we're thinking about what happens downstream. Okay, so the question is, how should we go about building models of the central auditory system? So those models of the peripheral auditory system, those were kind of constructed using the methods of kind of normal science, right? So people did experiments. They thought about what the experiments meant. They wrote something down.
00:04:46.558 - 00:05:18.878, Speaker A: It turned that into a model. The model made predictions. They compared that to data that motivated new experiments. They refined the model and so on and so forth, this virtuous cycle of experiments and theory and modeling. And for a variety of reasons, I think that that research program is unlikely to be successful in terms of getting us the models that we want of the central auditory system. Okay? And so I've been very interested in the fact that there are some other things that we could potentially leverage. One is the fact that the sounds that we evolve and develop to hear are created in the world, and the world is a lawful place.
00:05:18.878 - 00:06:14.736, Speaker A: And the consequence of that is that the sounds that we have to hear are this tiny, tiny subset of the space of all possible sounds. So that's got to shape the system in some very significant way. And we also know something about what we do with our auditory system. Right? We use our auditory system to perform behaviors. So the hypothesis that we'll consider is that the auditory system is pretty well optimized for the kinds of things that we have to do on a daily basis via some combination of evolution and development, such that it might be very highly constrained by the demands of having to represent and do things with real world sounds and to perform tasks and maybe also the nature of the cochlear input. This, of course, raises the possibility that you could attempt to build models of the auditory system just by applying machine learning to these same kinds of problems that we think are really important for human audition. And so, obviously, this is now very approachable because neural nets work really well in lots of things.
00:06:14.736 - 00:07:02.748, Speaker A: And so the approach that we typically take is to hardwire a model of the cochlea to be faithful to what we know from biology, on the grounds that we kind of understand that stage of the system pretty well. So we can just kind of put that in there, and then we optimize everything else with neural networks. So the general approach that we've taken is based on the logic that the task and the training data are really critical constraints. We work hard to try to make the task as ecologically valid as possible. We often use simulators to get enough labeled data, and we iterate on these tasks. And we started out with tasks that were just okay, and they've kind of gotten better and better. Um, and it turns out that this general approach works really well, um, in the sense that the models that you get from this are able to predict human behavior way better than anything that we had before across many, many domains.
00:07:02.748 - 00:07:38.494, Speaker A: Okay. Um, and so, um, I'll tell you a little bit about how that has gone, and then, um, tell you about how we hope to use these kinds of models to hopefully help people. Okay. Okay. Um, so one of the first problems that we, we worked on in this domain with speech recognition, and partially this was because this was a domain where, when we started on this, there was already a lot of labeled data that we could take advantage of. So we generated these big training data sets by taking excerpts of speech, superimposing it on sources of background noise. In this particular example, these are environmental soundtracks from YouTube.
00:07:38.494 - 00:08:09.252, Speaker A: So this is like a two second excerpt of speech. The task is to say, what was the word that happened at the midpoint? So it sounds like this gross domestic product grew one. The answer would be domestic, and it's on top of, like, some train station noise. Okay. So you can generate millions of clips like this and train a system up to actually solve this, this recognition task. So the methods that we use to build these models are now, like, really pretty old school. We train them in very standard ways.
00:08:09.252 - 00:08:37.340, Speaker A: There's usually some optimization of the model. Architectural hyper parameters. These are typically, like, pretty small scale convolutional neural networks. Those actually turn out to work really well on audio problems. Convolution, actually in frequency, turns out to be a pretty useful constraint. We have so far mostly been working in domains where the sounds are pretty short, so the perceptual effects are kind of local in time. So we're talking about a couple seconds of audio, and we think of these as, like, models of the auditory moment, if you will.
00:08:37.340 - 00:09:23.392, Speaker A: All right. We're neglecting things like the directionality of time effects of memory that are going to be very important in other domains, and we're starting to make advances in those directions. But there's been a lot of things to do in this space. There's sort of a long line of fantastic students that have kind of built up this modeling enterprise, including some of the folks listed here. All right, so just to give you a flavor of the sort of finding that emerges from this kind of research program, what I'm going to show you here is a comparison of one of these models and human listeners for the task of recognizing speech in noise. And this is across a very large set of natural noise sources that are not in the training data. So you train the model up on this huge set of speech, huge set of noises.
00:09:23.392 - 00:10:02.374, Speaker A: There's a whole bunch of environmental noises, things like the sound of a bathroom sink or applause or an airplane cabin. And we superimpose speech on that, and then you can measure how well a human can recognize speech in each of those types of noise. This was work by Mark Sadler, who's a recent graduate from our lab. And so this is a graph that is plotting human speech recognition on the x axis and the model speech recognition on the y axis. Each dot is a type of noise. And so you can see that there's, like, this type of noise that's really pretty easy for a human, and then this type of noise, which is, like, pretty hard, right? 30% to 90%. So same SnR, but there's, like, a huge range of performance.
00:10:02.374 - 00:10:39.730, Speaker A: And the point is that the model pretty much captures almost all of the explainable variants in the human judgment, which is a lot. The thing to emphasize here, and this will be true for everything that I show you, is that the model is not being fit here to match human results. You optimize it for the problem, and then you evaluate it, and this emerges. The way that we typically think about this is that this is a task for which humans are relatively well optimized. And we've now optimized this machine system for the same kind of task. And they're sort of approaching the limits of about as well as you can do on this. On this problem.
00:10:39.730 - 00:10:50.894, Speaker A: You may notice that a lot of these points are a bit above the diagonal. And we think that's mostly because the model is a little bit better at saying which word is in the middle of the clip. It gets pretty good at that. Humans don't have that much practice at that.
00:10:52.154 - 00:10:55.274, Speaker B: Each dot is a different type of.
00:10:55.314 - 00:11:00.554, Speaker A: Noise, and then you average over only many speech utterances. Yeah.
00:11:00.714 - 00:11:09.574, Speaker B: And so then you change the noise, and you have another dot and so on. And you have bigger bars for humans because you have fewer humans than bonds of your.
00:11:13.754 - 00:11:56.116, Speaker A: So there's not. We now have, like, lots of results that are of this flavor, but this is just an example of the kind of thing that sort of falls out of this sort of thing. Another kind of important setting in which you understand speech is what's often referred to as the cocktail party problem. So this is a setting where there are multiple voices. There may be one that you really want to understand, and it's thought to be dependent on attention to the target. So the idea is that you have some internal representation of the target voice, like, you know what it sounds like, maybe you know where the person is, and that that somehow helps you pick out the thing that you're trying to listen to. Okay, so this is work by Ian Griffith, who's a grad student in the lab, and Preston Hess, who's a undergrad.
00:11:56.116 - 00:12:28.614, Speaker A: And this is a version of the task okay, so here you're going to be presented with a cue. So this will be an excerpt of one person talking, and then you're going to hear a mixture. And that will be a combination of sound signals, one of which is uttered by talker, one saying something different, and the other is something uttered by talker two. Okay, so a mixture of voices. And then your task is to say what the cued talker said. Okay, so let's practice dark suit and greasy wash water. Cyclical programs.
00:12:28.614 - 00:12:50.532, Speaker A: Here's programs. Right. So it's easy. So you use the fact that you know what the target voice sounds like to do the task. Okay. Yeah. It's easier to actually follow what the cued voice is saying.
00:12:50.532 - 00:12:52.664, Speaker A: Is that actually a thing?
00:12:53.564 - 00:12:54.092, Speaker B: What do you mean?
00:12:54.108 - 00:12:55.144, Speaker A: It's easier to.
00:12:55.684 - 00:12:56.164, Speaker B: Right.
00:12:56.244 - 00:13:37.420, Speaker A: And then you play the mixture. Is it easier for the test of me to listen to the true voice in the mix? As opposed to what? Like, in other words, you're saying, imagine another task where you actually have to report the other talker. Yes, that would probably be harder. I've never measured it. Yeah, I think it'll be confusing to people, so it might be a little complicated. Establish why it would be harder. Okay, so the idea is that, like, there's some sense in which you're paying attention and there's some selection going on that's based on your memory of the properties of the target voice.
00:13:37.420 - 00:14:38.264, Speaker A: This is something people do all the time. All right, so how do we build a system that can do this? Well, you start with the same basic backbone, right. So there's some feedforward neural network takes the sound as input, and it's supposed to produce the word label, but in part inspired by actually neuroscience observations about attention, we just add in these multiplicative gains. Okay, so the idea is that these are going to be scalars that get multiplied by the activations in this neural network, and the scalars will be functions of the queue. So they're going to come from these little sigmoid functions, and the input to the sigmoid functions will be activations that are derived from the queue. So this looks like a really complicated picture that is super simple. The notion here is that if the queue, the thing you're supposed to be paying attention to, has got high activations for particular features, well, then you want to pass those through the system.
00:14:38.264 - 00:15:05.530, Speaker A: If it has low activations, then you want to attenuate those. That's what these sigmoidal functions do. But the whole thing is optimized at the same time. The auditory system is figuring out what features to extract. It's figuring out what the parameters of these sigmoids should be in order to apply the gains. It can decide to apply the gains at some stages and not others. So you could set theta one up here so that it passes everything.
00:15:05.530 - 00:15:52.692, Speaker A: Right. So the system just sort of has to figure out a solution to the, to the problem. I mean, it's, no, it's not really exactly the same as that, but it's kind of, there's sort of, sort of similar in spirit. Right. So all that you do is you just figure out the average features, like in every channel, right? So the average activations, those go into these functions, and then those get multiplied by the activations in this, the representation of the mixture. So it's sort of like, I don't know, we thought it was the simplest thing that one could do in this setting. Makes sense.
00:15:52.692 - 00:16:05.156, Speaker A: Okay, so the whole thing. And the whole thing just gets optimized. Okay. Okay. Um. So we know a lot about how humans solve this problem. So here's just a few graphs that illustrate, like, the human phenotype for the cocktail party problem.
00:16:05.156 - 00:16:26.700, Speaker A: So this is a graph that shows how well you can perform this task as a function of the signal to noise ratio between a target talker and a distractor talker. So over here the target is louder. Over here, the distractor is louder. And there's two lines. One is when the distractor is the same sex as the target talker. So two men or two women, that's the red. The other's when they're different sexes.
00:16:26.700 - 00:16:49.656, Speaker A: Right? So it's easier when the, when there's a sex difference. Right. So that's just how it is. The other thing that's kind of interesting is that sometimes people make selection errors. Okay, so instead of reporting the word that the target talker said, they mistakenly report the word that the distractor talker said. All right? So they've grabbed the wrong voice. And so that's what's plotted here.
00:16:49.656 - 00:17:45.340, Speaker A: So this is the proportion of what's called confusions, again, as a function of signal to noise ratio. And so you make more of those when the SNR is lower and when the toggle sucks is the same. And then there's lots of other stuff that you can measure and that has an effect on performance, and that's just shown here. Okay, so what we did is we optimized the model for this problem, and then we just run it in these experiments. Okay. And what you can hopefully see just by comparing the two columns is that the model reproduces human patterns of performance on this task to, like, a pretty good extent, right? So it does well when humans do well, does poorly when they do poorly, also makes kind of confusions to about the same extent and some of the same conditions. And so we've got sort of a pretty good model of how humans solve the cocktail party just for the benefit of those of us who don't know.
00:17:45.372 - 00:18:11.464, Speaker B: Much about your field. I understand that the theme you're cutting through is if you train the system in conditions that are ecologically similar to humans, then it will behave the same way as humans somehow. And second question is, from the point of view of engineering and technology, are we seeing state of the art results on cocktail party? Or if you pursue different techniques, you could get even better?
00:18:13.724 - 00:18:45.872, Speaker A: I don't know that anybody has tried to build a system to do exactly this. Part of it is like, the problems that we're solving here are sort of human centric problems. I mean, the way that this would typically be solved in an engineering context is probably using source separation, which is like a very different problem where you're trying to actually estimate the waveforms of the source signals. That's not what people do. So you'd have to have, I mean, you could take an ASR system and probably try to get it to report a qtalker. Probably somebody has tried to do this. I'm not really.
00:18:45.872 - 00:19:05.244, Speaker A: It's not a very common thing for people to try to do, though. Right. So normally you would like, you'd apply a source separation system and then run ASR on the, on the separated signals, you know, and again, in narrow domain source separation, that works pretty well. So our purposes are kind of different. Thank you.
00:19:05.364 - 00:19:51.204, Speaker C: So my question is about the ecological validity of this problem in the sense that, I mean, a lot of research in computer vision in the last several years has been. Andrew Zaseman's work is one of those. Christian Grauman, Andrew Owens and so on have got the issue that when you consider people are, when you have visual data as well, then the problem is easier. And the question is, okay, I put it in an extreme and unfair way, which is my claim is, okay. All the audition people have gone off on this artificial problem, whereas the real problem is not an audio problem, but an audio visual problem. And we vision people have a lot to offer you to help you solve that problem.
00:19:51.784 - 00:20:12.454, Speaker A: Yeah, I think the audio visual version of this problem is interesting. I mean, I was talking with Andrew. I mean, we've been attempting to make some inroads there. And that. I mean, that's definitely part of it, right? I mean, you definitely look at lips to some extent, but you. I mean, you can definitely. You can definitely do this, right? So there'll be some contribution from vision, but it's some.
00:20:14.194 - 00:20:31.892, Speaker C: There's a lot of. I mean, like, there's the mecca effect and so forth. Right. So my. My question is, in the setting of the cocktail party problem, is there a lot, a lot of sort of perceptual studies which sort of establish the relative role of vision versus audition?
00:20:31.948 - 00:20:49.542, Speaker A: I mean, yeah, there's definitely some. I mean, in the conditions where it's hard. Where it's hard, like low snr, it'll help you some. I mean, with people with hearing impairment, they rely on it a lot. So we know something about that and it's definitely going to be part of the story. It just. This is sort of.
00:20:49.542 - 00:21:40.012, Speaker A: This seems like a good place to start, you know? Okay, so just really quickly here, because there's been a lot of talk about interpretability, and, you know, what the benefit is of building these models that you don't understand. This is just one attempt to actually understand what was happening in this model in a certain sense. So, because the. I mean, the model can choose to apply attention anywhere it wants. And this is an analysis of where attentional selection is actually happening. So what's being plotted here? This is a correlation between the representation of the mixture and the representation of the target under conditions of attentional selection, acting on the mixture. So you get cued to the target, you have the representation of the mixture, and this is the correlation between those two things in different stages of the model.
00:21:40.012 - 00:22:13.618, Speaker A: This is the teal here is the exact same thing, but between the mixture and the distractor. Okay. And so the prediction here is that if attention is really operating on the mixture to select out the target, then the correlation between the representation of the mixture and the representation of the target should be higher than the same correlation applied to the distractor. Okay? And that's what happens. But you can see it really only happens in kind of the later stages of this model. So early in the model, there's really no evidence here that attentional selection is actually happening. So I don't know how many people ever remember the debates about early versus late selection.
00:22:13.618 - 00:22:31.214, Speaker A: It's like a huge thing in psychology for a long time. This is late selection. So the model's doing late selections. That's one solution to the problem, at any rate. So it gives you a hypothesis, I guess, about how to expect this might happen. In the brain. So another problem we've been working on a lot is sound localization.
00:22:31.214 - 00:22:56.286, Speaker A: So we've talked a little bit about this earlier in the week. The classical story is that there are these three main types of cues. Two of them have to do with the fact that you have two ears. There's also these spectral cues that you get from the wrinkles in your ears that filter sound differently depending on where it's coming from. But real world environments, they have noise, they have reflections. Reflections are a real pain if you're trying to localize sounds because they come from the wrong direction. So it's been a hard problem.
00:22:56.286 - 00:23:55.036, Speaker A: We've never had models that can actually localize sound, so we've built some of them. This is work that was led by Andrew Fransel, who's a grad student with us, and he trained this model in a virtual environment. So using a simulator to generate binaural audio that would enter the ears of a person, like, at this particular location in this room, there was a sound source here that they were trying to localize and some noise sources. Okay? So you can build up a training dataset with, like, lots of simulated rooms, lots of sound sources, lots of locations. You know the labels because you, you specified them, right? And you see, so there's a binaural audio signal that Pat gets passed through two models of the cochlea, and then a neural network has to estimate the location. And this turns out to work really well. So not only, like, the thing localizes sounds in kind of realistic environments, you can put microphones in a mannequin ear and show that mannequin, a mannequin head and show that the thing can localize sounds pretty well, but it also reproduces, like, this pretty detailed phenotype of human spatial hearing.
00:23:55.036 - 00:24:23.124, Speaker A: So there's a huge body of experiments. People have been studying sound localization for many, many decades, right? So lots and lots and lots of experiments. This is a bunch of graphs. You don't need to know the details. But the point is that when you run the model on the same set of experiments, you kind of reproduce all of the kind of key features of human spatial hearing. So one of the things that this opened up is the ability to kind of look at whether we could build models of spatial attention. So one of the things that people do when they're listening in this kind of cocktail party setting is use space.
00:24:23.124 - 00:24:59.582, Speaker A: So imagine same task as before, but now the target and the distractor can be at different locations in the world. So again, binaural audio, you're not telling the model anything explicitly about space, but it learns to solve the problem. It's the exact same framework, just with stereo audio. And what's pretty cool is that the model shows these pretty clear signs of learning to use spatial attention. So this is a matrix here that plots proportion. Correct. On this cocktail party problem task as a function of the distractor azimuth and the target azimuth.
00:24:59.582 - 00:25:25.394, Speaker A: Okay, so this is like the angular position of space. So the diagonal here, this is when the target and the distractor have the same location. And so it's lighter, which means it's doing worse. Okay. As you've moved to separate locations, performance improves. You may wonder, well, why is the diagonal wider out here than in the middle? And that's because your spatial acuity is best at the midline. Okay, so, like, the focus of attention is actually narrower here than out here.
00:25:25.394 - 00:25:57.146, Speaker A: Is that physics or psychopath physics at the derivative of the cues? Yeah. Yeah. Okay, so we've got models of spatial tension. All right, so the long term goal of all of this is to use these things to help people hear better. Right? So, yeah, question in this experiment. In that experiment. So the experiment has two sources of.
00:25:57.146 - 00:26:22.796, Speaker A: This experiment has two sources of sound and two ears. What happens if you have more than two sources of sound? Well, this is a case. This is a tricky experiment where you actually have symmetric distractors. So the targets in the middle, and you have distractors here and here. And if you move the distractors away from the middle, that also ends up helping. I don't know. There's, like, you can do many variants of this.
00:26:22.796 - 00:26:49.294, Speaker A: We have an explore. This is one that's kind of an important thing in the literature. Okay, so, like, the big take home message from all this, I've just given you a few examples where there's lots and lots and lots of examples of this. Like, you optimize these systems on, like, what we think are pretty reasonable problems, and you end up sort of seeing human like behavior fall out of them. Right. That's, like, way, way beyond anything that we could do. Pre machine learning.
00:26:49.294 - 00:27:16.554, Speaker A: Okay, so, as I said at the start, you're all going to lose your hearing. Some of you are already losing your hearing. There's, like, three main things that we know about that can kind of go wrong in the ear. There's, like, two types of hair cells you can lose. We also think you can lose nerve fibers. Okay? But we don't actually understand very well how those changes relate to behavior. And one of the reasons is that if you're somebody who has poor hearing, we can't look in your ear.
00:27:16.554 - 00:27:33.434, Speaker A: The ear is buried behind this very thick bone. And so, really, the only chance that we have to look at your ears is after you die. If you do die, it would be nice if you donate your cochleas to science. We'll slice them up and we'll look at them. Okay. But until you die, we won't know what's going on in your ears. Okay.
00:27:33.434 - 00:28:24.634, Speaker A: But we do have a pretty good understanding of how those changes to the ear turn into changes in auditory nerve firing. So one of the things we can do is ask, well, if we simulate different types of impairments, what kind of behavior does that produce? So we'd like to potentially be able to do three things. One is to predict the behavioral consequences of different types of hearing impairment. The other would be to put an audio transform in front of this model. And because this is a model, we can actually, in principle, optimize the audio transform to best mitigate some type of damage that is in your cochlea. And the third thing, potentially, is to develop diagnostic tests for particular types of impairment. So we can't look in your ear, but with a good model, we might be able to figure out what kinds of experiments would tell us what's happening in your ear.
00:28:24.634 - 00:29:06.320, Speaker A: So that's what we'd like to do to give you a flavor of how this might go. I'm going to tell you about one specific effort that we've been making in this domain, which is a lot of fun, which has to do with cochlear implants. Cochlear implants are a situation where hearing is achieved via electrical stimulation. They're the primary treatment for deafness. If your hearing is so poor that you're considered to be deaf, you would often get a cochlear implant. Now, typically, deafness is caused by dysfunction in these little cells in your cochlear, called hair cells, but very often the auditory nerve. So the pipe that kind of carries information from the ear to the brain, that's largely intact.
00:29:06.320 - 00:29:31.424, Speaker A: The idea behind the cochlear implant is that you can stick a wire into the cochlea, and these contacts here can stimulate the nerve fibers that would normally be carrying information about sound to your brain. Okay. And that would. That, and your brain would then, in theory, interpret that as sound. Okay. So these are now in fairly widespread use. There's quite a lot of them that have been implanted worldwide.
00:29:31.424 - 00:30:00.268, Speaker A: And the idea is to replicate the patterns of nerve firing that would normally be driven by sound, but to do it electrically. Right. So that's the goal. Okay. The problem is that for various kinds of technical implementation reasons, it's not possible to actually do that. Okay. Now, on the one hand, this is an unbelievable success story for biomedical engineering in the sense that you can take somebody who's deaf, and in the best cases, you can enable them to have a conversation on the phone.
00:30:00.268 - 00:30:22.772, Speaker A: Right. Which is amazing. Okay. On the other hand, these devices are not close to restoring normal hearing. So here's just a graph to drive home that point. This is a plot of word recognition as a function of signal to noise ratio. The black line is what a normal hearing person would do, and the red line is the average of a whole bunch of users of cochlear implants.
00:30:22.772 - 00:30:41.672, Speaker A: So there's a huge gap here. On the other hand, if they didn't have this, they'd be down here. So that's great, but we would like to close that gap. Yeah. Is there, like, a critical age by which if you get a cochlear implant, then at least for, like, younger is better? Yeah. Younger is better. Yeah.
00:30:41.672 - 00:30:44.840, Speaker A: The best outcomes are. And we'll come back to this in just a second.
00:30:44.992 - 00:30:52.944, Speaker B: Sorry, Josh. So the best results here are 60%, where the actors see when the sound is clean. How can you have a conversation on the floor if you're.
00:30:52.984 - 00:31:03.496, Speaker A: This is averaged across many, many users. So the best people would probably be, like, up here. Right. The spread, the variability in the outcomes is huge. And you'll see some examples of this. Right. And that's.
00:31:03.496 - 00:31:22.350, Speaker A: It's not really well understood why that is. We'd like to be able to maybe help with that. That. Okay. So we can build a model of how the cochlear implant stimulates the nerve. So there's all these stages of signal processing that are in the implant device itself. So, again, sound is the input here.
00:31:22.350 - 00:32:15.774, Speaker A: You pass this through a filter bank, you do a bunch of stuff, and then you get these electrical signals that get supplied to electrodes. I mean, there's been a fair bit of science that helps us understand how these electrical signals at the electrodes turn into patterns of firing in the auditory nerve. This is a little model stage that takes sound and turns this into the nerve patterns that should be produced by a cochlear implant. It's probably not completely accurate, but again, there's a lot of science that went into this, and we think it's pretty good. This is work that was led by Anisha Banerjee, who's a PhD student in the lab. This is showing you the patterns in the auditory nerve in response to a speech signal with a cochlear implant, and then with a normal ear, the same speech signal. So the point is that this is supposed to approximate this.
00:32:15.774 - 00:32:51.138, Speaker A: If you look closely and squint your eyes, you can tell that this is supposed to be the same signal, but it's also obvious that they look really different. The statistics of this are very different than this. And so you might imagine that a system that's built to decode this is going to have a hard time with that. This is with our model, so it's our best attempt to approximate what is happening in the nerve. Okay. And this has been validated to some extent with animals. So with non human animals, you can actually make measurements from the nerve, but it's very invasive.
00:32:51.138 - 00:33:25.484, Speaker A: Right. They can't do it? No, no, no. These are both models, but this one is, I would say, very well validated. And this is, like, moderately well validated would be my characterization. So it's our best attempt at reproducing what's likely to happen. Okay, so what is it that limits the outcomes? So, the current thinking is that there are at least three potential factors. Like, one is that the stimulation strategies that are in this thing that are turning sound into these electrical signals, that those are suboptimal.
00:33:25.484 - 00:34:02.714, Speaker A: And that's almost surely true to some extent. Okay. Because the simulation strategies essentially were the result of human thinking, and we're not perfect, and you can probably do better. The second thing that is likely limiting outcomes is neural degeneration. So it's often the case, if someone is deaf, that it's not the entire auditory nerve that's there. There's probably also loss of neurons centrally, and that probably has some role. And then a third possibility is that the information that you're getting from the auditory nerve is not being decoded optimally by your brain because your brain is not used to getting that type of information.
00:34:05.174 - 00:34:13.886, Speaker B: It's a small subset, some set of what the real nerve signal is, isn't it? You have 16 electrodes inside, or maybe more.
00:34:14.030 - 00:34:22.144, Speaker A: Yeah. And that's. I would fold this into this. Right. And some of that may be inevitable. Right. So the fact that it's suboptimal doesn't necessarily mean that you could do better.
00:34:23.604 - 00:34:27.908, Speaker B: But maybe simulate what's the best you can do with k electrodes or something.
00:34:28.036 - 00:34:56.214, Speaker A: Yeah, well, this is what we're trying. Okay? So what we'd like to be able to do is to predict what somebody with a cochlear implant ought to be able to hear. And so one way to do that is to take the models that I was just showing you and then just swap in the nerve patterns that would come from a cochlear implant. So imagine that you're deafened this afternoon. We implant you with a cochlear implant tonight, and we turn it on. Okay. Can you hear anything? Okay.
00:34:56.214 - 00:35:30.624, Speaker A: All right, so what happens? So this is what happens for a model that gets input from the normal ear. And if you just swap in the input from the cochlear implant, there's basically no generalization. So you're basically at f four. So the decoder that's been optimized from the normal ear, it just doesn't generalize. And this was a set of three people that we tested on exactly the same task. Okay, so one important clue this relates to several of the questions comes from the fact that when you give somebody one of these implants, they improve over time. So this is a really cool graph where every line is a person.
00:35:30.624 - 00:36:01.444, Speaker A: And this plots their word recognition scores on the y axis as a function of the number of months post activation they've lived. And all of these lines go up, a lot of them by a lot. So people improve a lot. And the time scale over which they're improving is pretty long. It's on the order of months. So the brain is changing. So something about the brain is changing that is enabling people to better decode information from.
00:36:02.944 - 00:36:09.652, Speaker C: Presumably, this might change depending on their age. Could be maybe younger people learn faster, whatever.
00:36:09.708 - 00:36:10.388, Speaker A: I don't know.
00:36:10.516 - 00:36:11.092, Speaker C: Better?
00:36:11.228 - 00:36:31.024, Speaker A: Yeah. I mean, there's confounds in that comparison. I mean, so what is known, as I said, the younger you get these things, the better. And the longer that you've had it, the better. What are the colors? Oh, they just grouped them arbitrarily into different asymptotic performance levels. Yeah, for our purposes, that's not correct. Okay.
00:36:31.024 - 00:37:12.290, Speaker A: All right, how do we model the plasticity that's happening in the brain in this case? Well, the only thing that we know how to do is to optimize stuff so we can at least ask, well, what happens if we reoptimize our neural network for the cochlear implant input? You might think that's going to tell you the best case scenario. That's the best you ought to be able to do modular. The architectural constraints of the neural network. Okay. And so when you do that, you get the green curve. All right? So if you can fully reoptimize, like, your central decoder for this cochlear implant input, you actually do pretty well. Not quite as well as normal hearing, you know, for some of the reasons that Shimon was alluding to.
00:37:12.290 - 00:37:32.314, Speaker A: Right. But really pretty well and much better than the cochlear implant users. And I should also. The other thing I should say, people that are kind of close to this red curve, those are, like, all star performers, right? There's a lot of people that are kind of down here. Right. Okay. All right, so you do better.
00:37:32.314 - 00:38:29.204, Speaker A: So this suggests that potentially we could get better outcomes if the rest of the brain could, like, optimally adapt itself to this new kind of input. Okay. But you might imagine, well, maybe the issue here is that you don't have plasticity throughout your entire system. You know, we often think there's more plasticity centrally in the cortex than maybe in the midbrain. Okay, so what happens if we just reoptimize some of the later stages of the model? And if you do that, you kind of get into the ballpark of what you see in human users. So, of course, this isn't proof that this is really what's underlying what we see in humans, but it's certainly suggestive that being able to re optimize your auditory system for these sort of for the altered input that you're getting in this setting could potentially have a lot to do with this. So the other factor that I mentioned that people have speculated might have a big impact on this is neural degeneration.
00:38:29.204 - 00:39:01.944, Speaker A: So we can simulate that in the models as well by just getting rid of a whole bunch of the auditory nerve fibers. And what's really interesting is, when you do this. So this is getting rid of half of the nerve fibers. So the dashed curves here are with half as much bandwidth in the input, and that has some effect, but it's actually really pretty modest. Right. And 50% would be kind of on par with what you would typically observe. Again, when somebody with a cochlear implant dies, you can look at what's left of the nerve, and 50% would be sort of a pretty common number.
00:39:01.944 - 00:39:44.374, Speaker A: So all this kind of suggests that correctly modeling plasticity, both the locus of the plasticity and maybe often also the nature of it, we're really modeling plasticity as optimization. It's not really clear that that is the right way to think about it, that that will be pretty important to predict detailed outcomes. So I think the long term prospects of this kind of thing are pretty exciting. Right. So one thing that we can do, which I sort of showed you a little bit is identifying best case scenarios for particular types of treatments. There's also, I think, the prospects of being able to determine the factors that limit outcomes. So what we did in all the stuff I showed you is we just had this one very standard type of stimulation.
00:39:44.374 - 00:40:49.006, Speaker A: But you can also imagine searching across different simulation strategies or directly optimizing stimulation strategies and seeing what is actually best for allowing people to sort of perform tasks. Okay, so I told you a little bit about our work on building new models of human hearing via deep learning applied to audio tasks. So we now see what I think are lots of pretty compelling matches to human behavior with real world sounds and tasks that replicate lots of classical psychophysical results. We can use these models also to get insights into the origins of behavioral traits. So, a lot of the behavioral traits that we see in humans, we think are primarily a consequence of natural task demands, in the sense that optimizing for the task is sufficient to really give you the behavioral trait. I didn't talk to you a whole lot about this, but we also often use these models to ask why questions. So what is it about, like, the natural environment that actually causes something that you see in perception? And so, especially when you train these models in simulators, you can actually manipulate the world.
00:40:49.006 - 00:41:29.104, Speaker A: So, for instance, you can train an auditory system in a world that doesn't have reflections, and that turns out to change very particular aspects of behavior. That's one of the ways in which you can get some of what we might consider understanding. So there's very exciting extensions, I think, to hearing impairment and cochlear implants. One really interesting open question is, like, what's the role of neuroscience constraints? Right? So, right now, all of the neuroscience here is in that peripheral front end. So there was a lot of neuroscience that went into that. Everything else is machine learning, and what we know about ecological acoustics. Seems plausible that neuroscience is going to have a role to play, but I think we don't yet really know what that is.
00:41:29.104 - 00:42:05.324, Speaker A: But in particular, the nature of plasticity, I think, is going to be kind of a key component of that. So I will just, again, acknowledge all these great people who did some of this work. Dan walked in late, but he was part of some of the very early work that we did on this. Alex Kell, Ian Griffith, Andrew Franco, Mark Sadler, and Anisha Banerjee did a lot of the heavy lifting for what I told you about. Happy to take questions. Number of nerve fibers.
00:42:07.744 - 00:42:08.744, Speaker B: Level of quality.
00:42:08.784 - 00:42:35.580, Speaker A: Of addressing individual fibers, how perspective things are. You get a sense of what specifically? Okay, so there's nerve fibers. Right. And then there's electrodes. Okay, so. Right, so you've got, you know, 30,000, 40,000 nerve fibers here. Right.
00:42:35.580 - 00:42:58.620, Speaker A: And typical implants might have 20 electrodes. 16. You can't really have more than 16 activated at once, otherwise you get too much interference. And. Yeah, so the central limitation on that is the fact that, so you can see you're sticking this wire, essentially, into one of the, like the cochlea is like a couple tubes separated by a membrane. So you're sticking the wire into one of the tubes. Okay.
00:42:58.620 - 00:43:11.760, Speaker A: And so the electrodes here, they're not right next to the nerve fiber. Right. There's some distance away. And so you get this sort of spatial spread of the activation. And that's kind of like an intrinsic limit for now. Okay. And so the.
00:43:11.760 - 00:43:54.514, Speaker A: The spatial stimulation is not that precise. There's also limits on the temporal stimulation, again, due to the nature of the way that electrical stimulation interacts with nerve fibers. And a lot of the. So all of the big manufacturers now do something a little bit fancier than what I. Than the algorithm that we used, and that they'll often select the electrodes that are most informative in some way at a given moment in time. Some of them will also sort of try to combine the electrodes to sort of focus the current a little bit better spatially. So there's a few things like that that we'll come up with, but that's pretty much the state of it.
00:43:54.514 - 00:44:31.284, Speaker A: Follow up question. I was curious if any able hearing people have had cochlear implants implanted in order to try to get a sense of what it sounds like. No, and the reason is that when you stick the electrode and you typically destroy any residual hearing that you would have. So, yeah, they would never put this in somebody unless there was pretty severe hearing loss. And they've gotten better over the years at retaining the hearing kind of here at the. At the apex. But it's.
00:44:31.284 - 00:44:36.584, Speaker A: Yeah, you don't normally get one of these things unless your hearing is pretty bad. So, yeah, it'd be nice to know what they sound like. But.
00:44:38.404 - 00:45:07.684, Speaker B: Two questions on possible improvements at the lower end and higher. And the lower end. When you place an electrode, I guess you don't know exactly where it ends up within the cochlea. So do you also, can you optimize the kind of signal that the electronic sends into to each electrode? So that's something inside. And then there's biology. It's not directly to this is there any way of changing plasticity or encouraging plasticity in certain brain areas?
00:45:08.624 - 00:45:27.080, Speaker A: Yeah. So with regard to the. Wait, what was the first question? Oh, yeah, yeah, yeah, yeah, yeah. So we. We actually think that we should be able to come up with some diagnostic tests that will help with that. So right now what happens is the surgeon basically just sticks this thing in as far as he can. That's pretty much what happens.
00:45:27.080 - 00:45:43.512, Speaker A: And then at some point, there's too much resistance, and then they stop. Right. And so the electrodes just kind of end up where they are. There's default settings for. The thing that I didn't say. Okay. Is that there is a frequency to place mapping along the cochlea, which you know about.
00:45:43.512 - 00:46:08.434, Speaker A: Right. So different frequencies are transduced in different places. And so one of the kind of basic ideas of the implant is that the different electrodes will essentially get the output of different bandpass filters that are tuned to different frequencies. So it's an attempt to leverage that frequency mapping that's on the cochlea. And so there's a default mapping that usually these things come with, which is based on where they expect, on average, the electrodes are going to land and they often just stick with that.
00:46:08.734 - 00:46:11.454, Speaker B: Can you change the frequency of same type?
00:46:11.494 - 00:46:26.772, Speaker A: Yeah, no, you can definitely do that. It's like, most clinicians won't. Don't mess around with that. Yeah. So we've done some experiments to try to look at, like, how sensitively things depend on whether there's mismatch, and it matters a bit. And so we think we could probably come up with a way to help people set that. So the second question.
00:46:26.772 - 00:46:51.704, Speaker A: What was the second question? Oh, yeah, yeah, yeah. So, yeah. Like, look, I think this is going to be super important, and I think there will be. There will be things that you can take that will reopen, like, critical periods in, like, different parts of your brain. And that's going to be a huge part of therapeutics. That's just my guess, because it's clearly, like, one of the limiting factors here is my sense. Yeah.
00:46:51.704 - 00:46:59.524, Speaker A: Right now, we don't have anything that is very. Just very fine grained. Some people think Prozac actually does that.
00:47:00.344 - 00:47:01.392, Speaker C: Psychedelics.
00:47:01.528 - 00:47:57.118, Speaker A: Yeah. But, yeah, that's, I think, an interesting thing. For the future, let's do one more question, and if you can get set up while we do that, is there similar effort or progress on just the other way of improving hearing with the conventional hearing aid without appropriate implant modeling, you know, hear loss, whatever, degradation for that? Yeah, I mean, we're working on that as well. That's not quite as far along. I mean we made a conscious decision to start with the cochlear impeller just because the effects there are huge and so it just sort of seemed like it would be easier to tell if we were on the right track. But I think like all these issues like map exactly onto things that happen with hearing aids, it's just like not quite as pronounced. So yeah, so I think all of the same approach will likely be fruitful.
00:47:57.246 - 00:47:58.074, Speaker B: We'll see.
00:48:10.424 - 00:48:38.614, Speaker A: Let me know when you're ready. So one of the themes for today is audition. So we're going to continue with that theme in case you didn't notice Josh's talk, but Pixar, this one makes a lot of ideas together.
00:48:42.794 - 00:48:43.754, Speaker B: Give me 1 second.
00:48:43.834 - 00:48:58.794, Speaker A: Okay, yeah, all set. Okay, great. So next we have Dave. Oh, okay, we'll give one more second. Hold on.
