00:00:01.240 - 00:00:49.994, Speaker A: Hi, everyone. Thank you for your interest in our work. I am Sachin Karod from University of Victoria, and this is joint work with Paul Beam at University of Washington. This talk is about lifting theorems, which are a powerful tool, and theoretical cs, which has led to breakthrough results in many adjacent areas like game theory, quantum computing, linear programming, etcetera. Today's talk in today's talk, we're going to talk about the pursuit of quantitatively improving these lifting theorems so that we can get further breakthroughs in these areas. Lifting theorems, as the name suggests, lift results from a weak model of computation to a strong model of computation. So here is a weak model of computation that's used in standard lifting theorems.
00:00:49.994 - 00:01:48.504, Speaker A: It's the model of decision trees. Decision trees are a simple model, but which are powerful enough to capture many revolutionary algorithms in areas like learning algorithms or quantum algorithms. So they are simple but powerful. And in this model, you are allowed to, you don't have the access, you don't have an unhindered access to the input, but you can only access the inputs bit by bit by querying them. So here is a decision tree computing the standard or function which is one, or function of two bits which is one if one of the two bits is a one, and it's zero if both of them are zero. So the decision tree queries variables and then compute sensor once it knows. And a lifting theorem translates lower bounds about a function being computed in a weak model like decision tree into a powerful model of computation.
00:01:48.504 - 00:02:37.148, Speaker A: Usually this is a powerful model of computation is the standard two party communication complexity model that you may be familiar with. So in this setting, there are two players, Alice and Bob each have their own individual inputs, x and y. Alice cannot see what Bob has. Alice doesn't know why, so Bob, and similarly Bob doesn't know x, and they want to compute some function, pre agreed function of their inputs, f of x, y. And the goal is to communicate as few bits as possible between them to compute the value of x, y. And another strong model of computation that one might consider are decision tree with more complex queries. So here you can consider.
00:02:37.148 - 00:03:20.314, Speaker A: Here we have the example of parity decision trees. So in parity decision trees, instead of having queries to individual bits, you can have queries to a bunch of inputs, the queries which ask for the parity of a subset of input bits. So for example, here, the parity decision tree at the root query is the parity of z one, z five and z eight. So the answer you get just reveals the parity. It doesn't tell you what z one, c five, or z eight is, but it tells you the parity. So this is a more powerful model, because trying to figure out the parity in the decision tree model would require you to query both. Like to query all z one, c five, and z eight.
00:03:20.314 - 00:03:53.440, Speaker A: To talk more about lifting theorems, I'd like to introduce an important operation of boolean functions known as block composition. It is a simple operation. It involves two boolean functions, let's say f, which is on n bits, and g, which is on n bits. The composition, the block composition of f and g, denoted by f circle g is a boolean function which takes nm bits of input and produces one bit of output. The precise definition of this lock on position. We'll defer it to the next slide. But its connection to lifting theorems is as follows.
00:03:53.440 - 00:04:47.714, Speaker A: So, lifting theorems, as we said, translate lower bonds from a weak model of computation to a strong model of computation. So what it does is that it takes lower bonds about a specific function f in the weak model of computation, let's say decision tree model, and then it would translate it into lower bounds about the block compose function f composed g in the strong model of computation. Let's say communication complexity. And the power of lifting theorems is that like once you prove a lifting theorem, it's usually for a specific, well crafted g, but it works for any function f. So once you have a lifting theorem for a specific g, it works with any function f. So you have lower bonds for f, it automatically translates lower bonds for f, compose g in the strong model. And this function g, which is carefully chosen, is known as the gadget.
00:04:47.714 - 00:05:37.570, Speaker A: Okay, so let's talk about block composition in a more detailed manner. So here I'd like to talk about block composition in the setting of communication complexity. So in the setting of communication complexity, the f you have is you want to translate the lower bounds on a standard boolean function f in the decision model to lower bounds about a two party function f, compose g, but in the two parts. So f, let's say, is on n bits, and it takes inputs z one to zn. Then we need, we take g to be like a two party function. So there's a natural partition of g's inputs into Alice's parts and Bob's part. So here, if you look at the first copy of g, it takes x one.
00:05:37.570 - 00:06:35.754, Speaker A: So x one is Alice's input and takes y one, which is Bob's input. So g is a community. It's a function in the communication setting f is a function in the standard like decision tree set in so and how do you define f compose c? So you take the ith input of f, let's say zi, and define it to be the output of g applied on xi and y I. Okay, so this is the natural definition that I was talking about in the earlier slide. And this is a very natural definition for many reasons. One of the reasons is that like many interesting functions that you want to understand in the setting of communication complexity can be written as a block composition. For example, one of the more prominent functions which are used in many applications of communication complexity is the disjointers function, or its complement, the set intersection function.
00:06:35.754 - 00:07:22.534, Speaker A: Here, Alice and Bob are given characteristic vectors of two sets, and they want to know if their sets have an intersection. This can be naturally written as the block composition of the standard or function on n bits with the and function on two bits in the following sense. So Alice has inputs x one to xn, and Bob has inputs y one to yn, and xi is one. If Alice's set has this element I and y is one if Bob's in set has the element I, so their sets intersect. If there's some index I and n where xi and y I are both one. So this and so if they do intersect, one of these ands would turn on. And when the output of an and is one, the output of the or naturally becomes one.
00:07:22.534 - 00:07:31.632, Speaker A: So this, this naturally computes the set intersection function. So just recall, many interesting functions can be written as natural block compositions.
00:07:31.768 - 00:08:26.738, Speaker B: So, to summarize, lifting theorems are theorems which work for a specific gadget g, and for all functions f, which essentially say that the communication complexity of the block compose function f compose c is approximately equal to the decision complexity of f. And if we are considering the lifting from query complexity to two party communication complexity. And the holy grail here is to get gadgets which are as small as possible. Ideally, we'd like to get gadgets which are just constant size. And lifting theorems have been instrumental in solving long standing open problems in many areas like combinatorial optimization, communication complexity, proof complexity, circuit complexity, game theory, etcetera. So in this slide, I want to talk about frontier selflifting theorems. So the main overhead in lifting theorems is the gadget size.
00:08:26.738 - 00:09:13.058, Speaker B: So recall that lifting theorems transfer lower bonds for a function f on n bits to a function f compose c on nm bits. So ideally, we want to keep the m of the gadget as small as possible. And the holy grail in this direction is to get m constant, but even getting m to be substantially smaller than what is known would result and breakthrough results in circuit complexity, proof complexity, etcetera. And these are on problems which have been open for a very long time, and they're really hard problems. So here's a quick overview of our results. So our first result shows that currently known techniques cannot reduce gadget size to even to constant. For general lifting theorems, we actually proved something stronger.
00:09:13.058 - 00:10:18.254, Speaker B: We show that even going beyond below log n is not possible with current techniques for general lifting terms. Nonetheless, we show that if you want to prove lifting terms which lift decision tree complexity to somewhat stronger models, maybe something not something as strong as two party communication complexity, but stronger model like parity decision trees, you can get a lifting theorem with the constant size gadget. We prove a lifting theorem with consciousness gadgets from decision tree simple decision tree complexity to parity decision tree complexity, we show that decision tree depth decision tree depth can be lifted to parity decision tree depth, and similarly decision tree size can be lifted to parity decision tree size. And this result, this lifting theorem has interesting consequences. Improved complexity. Using this result, we actually show that tree resolution proof size can be lifted to tree raspberry proof size. We'll talk in detail about what these proof systems are in the later slides.
00:10:18.254 - 00:11:17.334, Speaker B: So what do we mean by current techniques? So we use this to refer to simulation based approaches for proving lifting terms. So what are simulation based approaches to proving lifting theorems? So, usually in a simulation based approach, a query to communication lifting theorem is proved in two steps. The first step is the easy direction, and the second step is the harder direction, which is known as the simulation theorem. So in the first step, it shows that the communication complexity of the lifted function f compose c is upper bounded by the decision tree complexity of f times the communication complexity of g. This essentially says that if you have a efficient decision tree computing f, you can turn that into an efficient communication protocol. Computing f compose g the idea is fairly straightforward. So Alice and Bob want to simulate the decision tree computing f to come up with a communication protocol computing f composer.
00:11:17.334 - 00:11:48.446, Speaker B: So here, let's say this is the decision tree computing f. It queries the variable c one. But Alice and bob has no clue about the value of c one. So they need to know the query answer to z one to proceed along this decision tree. But they know that z one is the result of applying the two party function g on x one and y one. X one is Alice's first input, and y one is Bob's first input. So they can compute the value of g of x one, y one, using the best protocol for communication protocol for g using just cc of g bits.
00:11:48.446 - 00:12:25.612, Speaker B: So by just exchanging cc of g bits, they can figure out the value of c bit. And based on the value of z one, let's say it's zero, they continue the simulation and go down to the next node in the decision tree. And then at c two, they know they don't know what is the value of c two, but they know that z two is g applied to x two, y two. So they similarly learn the protocol for g again, so on and so forth, until they come to an answer. And the cost of doing the simulation is just the decision tree complexity of f, which is the number of queries you make on the path, times the communication complexity. So this is a, this is the easy direction. So in the hard direction, what you want to say is in the reverse direction.
00:12:25.612 - 00:13:18.090, Speaker B: Suppose you have an efficient protocol communication protocol solving f, compose g. Then you want to use that to come up with a decision tree compute computing f, whose query complexity is related to the communication complexity of f composite g divided by c c of g. So this is usually done by designing a meta algorithm recipe, which takes an arbitrary protocol that solves f composure, which is somewhat efficient, and uses that to come up with a decision tree. Computing f so the simulation, the high level idea behind the simulation is as follows. So we start with a protocol PI solving f composite given on the left, and we use that to come up with a design, a decision tree for f on the right. So we start at the root of the protocol. Initially, at the root of the protocol, their players haven't spoken anything, so they know they don't know anything about any of their inputs.
00:13:18.090 - 00:13:59.394, Speaker B: But after communicating for a few rounds, Alice and Bob might learn a good amount of information about some of their inputs. Let's say in this point we are in the simulation, and in the simulation we're going to trace a path from root to one of the leaves. And let's say we are at a node where Alice and Bob has learned quite a lot about x three and y three. But it's not all the information, but they've learned just enough information. At that point in the simulation theorem, we would query the variable c three. So before they learn enough information on x three and y three to decide what is the value of c three, we would just query z three. At this important threshold.
00:13:59.394 - 00:14:39.464, Speaker B: And based on the query answer that we get, we'll continue the simulation. And until and unless some other variable set of variables become revealed in terms of amount of information Alice and Bob has learned, at that point we query that variable. We continue the simulation until we get to a leaf node of the protocol. And at the leaf node of protocol, the players know the value of f compose g, and looking at that value, we can come, we can decide the value of f for the decision tree. So this is the high level idea behind simulation. To carry out such a simulation, we need to maintain a set of invariants. So at any point.
00:14:39.464 - 00:15:23.082, Speaker B: And so what are we doing in the simulation? We are trying to trace a path from root to one of the leaves so as to compute the value of f, consistent with the query answers that we have gotten so far. Right. So at any point in the simulation, we maintain a set of inputs of Alice and Bob, denoted let here by the set curly x and set curly y. And we want to maintain some invariants for these set of inputs. So the first is a very natural invariant. We call it the fixed coordinate invariant, which is that like so far in the simulation, we have queried a bunch of input variables, let's say z one, c two and c five. So we want to make sure that the set of inputs that we are currently keeping track are consistent with the query answer that we have gotten so far.
00:15:23.082 - 00:16:31.300, Speaker B: So here we denote I to denote the already queried coordinates. And we want to ensure this consistency constraint that if you look at the set of inputs that we are maintaining, and you look at the gadget output on those coordinates I which have been queried, the value of the gadget is consistent with the query answer. The other property, known as the dispersive property, is the more important one. And this ensures that, like for the remaining coordinates which are unquerried at this point, the players do not know anything on whether zi is one or zero. So for the remaining coordinates, we have to maintain all possible future query answers. So we want to say that this set of inputs x and y are so rich so that if you look at the gadget output and the remaining unquerried coordinates, they give you all possible zero one strings of that length. And to maintain this invariance, especially to maintain the dispersive property, we need to do some bookkeeping.
00:16:31.300 - 00:17:38.144, Speaker B: So we, as we go along in the simulation, we'll add new coordinates to this queried coordinates and do some cleaning up to make sure that everything is nice and tidy. Just a quick note here that if you're approving lifting theorems for randomized in the setting of randomized algorithms, you need a stronger property known as the extractive property instead of the dispersive property. But here we are focusing on deterministic lifting theorems. So let me tell you a bit about what these nice meshes that we were talking about in the last slide which ensure that the gadget has the dispersive property. So historically, the first nice measures that was used to ensure dispersal property for the index gadget was the average degree. It's used in the original paper of Ras and McKenzie, who proved a lifting theorem and as a consequence got separation of the monitor and say hierarchy. But it was already known that like this approach based on average degree, can only give you linear size index gadgets.
00:17:38.144 - 00:18:45.334, Speaker B: The other two measures which are usually used in combination are the information theoretic measures of mean entropy and deficiency. So the gadget, the dispersive property is also a function of the gadget, and not just the nice properties, nice structures, nice measures. So we introduce the two gadgets which are very commonly used in lifting theorems. So one is the index gadget and the other is inner product gadget. Index gadget so, gadget, where Alice gets an pointer which is a value in one to m, and Bob gets a memory bank of m locations, which we think of as a binary string in zero. Enter the m and the output of the index gadget at x comma y is just the value y has located has at location x. So whatever Alice is pointing to in Bob's input is the value of the output of the index gadget, and the communication complexity of the index gadget is log m.
00:18:45.334 - 00:19:51.518, Speaker B: Similarly, another very popular gadget is the inappropriate gadget. So Alice and Bob get two b bit strings, x and y, and the output of the gadget is just simply the inner product of x and y over f two. And the communication complexity of the gadget is exactly equals to its input length. Another interesting thing about index gadget is that it's a universal gadget. So if you think of y as a truth table of a login bits function, then you can encode any function as a specific index of x and y, where y is said to be the truth table of that function. So let's revisit nice measures with respect to index and inner product gadgets. We already mentioned that average degree cannot give you lifting theorems for index with less than linear size gadgets, but for min, entropy and deficiency, the current best gadget sizes are as follows.
00:19:51.518 - 00:20:35.354, Speaker B: So for index, we can already get m to be something close to linear, something like n login. So this was in the paper of Lmmbz in ITCs last year. And for inner product it's an older result from CKLM from CCC 2019. And for India product it's around theta of login. And for index it's almost linear. And even reducing the gadget size for index from almost linear to poly logarithmic would lead to breakthrough results. And these are this would be breakthrough results in circuit complexity and proof complexity for really big open problems.
00:20:43.414 - 00:22:01.464, Speaker A: Okay, so towards this goal of reducing the gadget size and how the current techniques fare like lowered mechanics, Merc Betty sang in their paper in ITCs 22 made the following conjunction about the dispersal property of index. They conjectured that there exists for sufficiently large m for every x and y, which is deficiency very small at most some delta. They they conjectured that this index is a good dispersion in the sense that it index on such inputs x and y would contain a subcube of co dimension c delta. So there's some set of coordinates I, which is at most c delta in size, which if you exclude them, the index produces every possible binary string. So this is what it means. Say that index contains a kobe and this is the dispersive property of the index ketchup. So our first result proves that this dispersal conjecture for index m is false as long as m is less than log n minus omega one.
00:22:01.464 - 00:23:13.258, Speaker A: And it also fails for inappropriate gadget for the same kind of range. So as long as you have an inappropriate gadget of size p where b is less than log n minus omega, and this dispersive property is no longer true. So this shows that the lifting theorems for ineptredded gadgets using current techniques is tights in terms of the gadget size parameter. So because we already know lifting theorems were in a product with the theta of login size gadgets, and our results prove that current techniques cannot improve it beyond that. Asymptotically beyond that. Okay, we also going beyond, we also prove a deterministic listing theorem for the index gadget with constant size, despite proving that the current techniques cannot improve it even to sub logarithmic. But we proved we achieved this by proving lifting theorems for a weaker class of protocols known as semi structured protocols.
00:23:13.258 - 00:23:30.574, Speaker A: So what is semi structured protocols? These are protocols where Bob can only communicate parities of his input. Okay, so given a boolean function f. So using this lifting theorem, we prove the following result.
00:23:31.034 - 00:23:32.826, Speaker B: Given a boolean function f on n.
00:23:32.850 - 00:24:54.632, Speaker A: Bit, we come up with a composed function f dash by composing f with an index gadget on constant size and prove that the decision tree complexity parity decision tree complexity of f is tight in terms of the decision tree complexity of f. And this decision tree complexity can be either the decision tree depth, which is dtf, denoted by dtff, or it could be decision tree size, which is denoted by size independent work by Chatto Bandai Mande Sanyal Sharif also proves the same results on parity decision trees, and you can listen to the next talk to know more about their results. Okay, so using this lifting theorem, we get the following consequences and proof complexity. So we consider a class of proof systems known as res parity proofs. They're also known as rest length proofs. It was introduced by its Keanu and Sokolov in 2014, and they proved lower bonds in 2020. So in an respiratory proof, each line in the proof is a affine subspace of this phase f two to the n.
00:24:54.632 - 00:25:11.634, Speaker A: And the inference rule just says that if you have a and b, you can derive c if C is in the it's a subset of a union b. So if a and B are affine subspaces, and c is another affine subspace, and the goal is to derive f two to them.
00:25:13.214 - 00:25:30.424, Speaker B: Using our lifting theorem for semi structured protocols, we actually show that tree rest size for a formula phi can be lifted to tree rest parity size for a formula phi dash, where phi dash is obtained by composing this unsatisfiable KCNA formula phi with an index gadget of constant size.
00:25:34.324 - 00:26:56.192, Speaker A: I'd like to conclude the talk with a few future directions. So our results essentially show that nice distributions and our min entropy rate simple information, just like min entropy rate, is not the way forward if you want to improve gadget size using the existing simulation framework. So in the counter example that we construct, it has this very peculiar structure that a few blocks of bobs inputs have only zeros or only ones. So one way to avoid such conflict, maybe pathological counterexamples, would be to consider inputs where every block of Bob has an equal number of zeros and ones. And once you have such a set of input means you restrict once you maintain such a set of inputs in the assimilation theorem, our counter example no longer works, and we don't know how to, like disprove the conjecture with such set of inputs. The other avenue is that okay, so currently there's this gap between the known limitations of this is Goldilocks zone, where you might be able to prove interesting results. But there are no known limitations for current techniques, which is the range of m between n, which is almost linear to n log n.
00:26:56.192 - 00:27:51.730, Speaker A: So can you prove a lifting theorem with m theta? And that's an interesting problem. The other thing is that a lot of these improvements and recent improvements in index gadget size came from connection to sunflowers. So can we like utilize this connection further to say maybe something about impossibility of improving the gadget size or improving it further? And in this paper we proved tree constant lifting theorem with constant size gadgets for tree like protocols. But there's this other notion of tag like protocols where it has more applications and proof complexity. And maybe you can prove tag like lifting for restricted protocols. That's another interesting venue. The other option to consider is that here we considered semi structured protocols where it was limited in the sense that Bob could only send parities.
00:27:51.730 - 00:28:29.282, Speaker A: So instead of parities you could consider other functions which are limited, like majority or other threshold functions. And we do not know anything in this regime. And this might have like considering an appropriate restricted function family like thresholds to functions or majorities. Yeah, you'd be able to get interesting results, improve complexity. Okay, that's the end of the talk. Thank you everyone, and if you have any questions or comments, please feel free to email either of us. It's escodat Ca at PS Washington, ed.
00:28:29.282 - 00:28:29.794, Speaker A: Thank you.
