00:00:00.160 - 00:00:53.054, Speaker A: Borne Eldant from Microsoft Research. And we talk about the two sided estimate for the gaussian noise stability deficit. All right, thank you for the invitation and for waking up this early to hear the talk. So the first couple of minutes are going to be about explaining all the words in the title. I guess those of you who attended Joe's talk yesterday should more or less already know what it means. So I'm going to go over it rather quickly. All right, so let's try to understand what is gaussian noise? So, given two standard gaussian random vectors, we say that they have correlation Rho between them.
00:00:53.054 - 00:01:53.014, Speaker A: Rho can in general be between minus one and one. But here we'll only consider positive correlation. So this happens if, equivalently, either each coordinate of x and y, so corresponding coordinates have a jointly bivariate gaussian distribution with variance one in correlation Rho. Or equivalently, you can think about it like this. X is root rho times some standard gaussian Z one plus root one over rho times another standard gaussian, which we'll think about as noise. And y is the same, only the copy of the noise will take just an independent copy. So you can think about both of them as if Rho is close to one, then they are more or less the same thing up to sum independent noise.
00:01:53.014 - 00:03:30.634, Speaker A: So we'll think about Y as a noised version of X. All right, what is noise stability? So, given a set in rn, the noise stability of a. So we'll denote it like that. This stability sub rho of a will just be the probability that for such x and Y, both x and Y are in a. Maybe it would make a little bit more sense to divide this thing by the product of probabilities that x in a and Y is in a. In some sense, the stability just measures. Given that a gaussian vector is in a, what's the probability that a noised version of this vector will still be inside a? So we should think about sets which look like this, like a cloud of many little sets, is very unstable, right? Because if I know that X is in a, it says practically nothing about Y, and we'll soon see which sets we should expect to be more stable or most stable.
00:03:30.634 - 00:05:45.634, Speaker A: And a theorem of Borel, which Joe already mentioned from, I think, 85, 86, states the following hyperplanes are basically the most noise stable sets. In other words, if h is a hyperplane having the same gaussian measure as that of a, then the noise stability of h must be at least as big as the noise stability of a half space. Sorry, did I also say hyperplane or I just. Well, this is also true for half spaces, but it's not a very clever result. All right, so the best real life example I could come up with for the application of this is the following. Say you have a large number of strings, street cats, and you're collecting them according to some properties which they have, for example, their weight, their height, how hard they meow, and all sorts of properties which have a continuous distribution. Let's assume that they have a gaussian distribution.
00:05:45.634 - 00:06:45.824, Speaker A: And if a cat is another cat sibling or ancestor, then we assume that each one of these genes have some correlation between them. Otherwise, they are all just independent. And I'm collecting them according to some criteria which I determine. And what I want to do is to set the criteria such that the expected number of cats I collect is 100, and I want to maximize the expected number of pairs of cats which are related by ancestry or by being siblings. What I'm going to do, in order to do that is just get some hyperplane of genes. So if all the genes are independent, then I'll just take one property, the one with the biggest correlation, and just collect all the capsules. I don't know, color is blacker than something.
00:06:45.824 - 00:07:16.664, Speaker A: Okay, should mention that no animals were injured in the making of this example. But you can also take linear combinations of genes. Yeah. So if they are all independent, then assuming the correlations are not exactly the same, I'll take the gene with the highest correlation. If they are dependent, like. Okay, basically, weight and height are usually dependent. And usually, I'll take the maximizer, will be some linear combination.
00:07:16.664 - 00:07:22.624, Speaker A: Okay, this example is not so important. Maybe we can move on.
00:07:23.564 - 00:07:28.988, Speaker B: Okay, what here meant is that you take a threshold function, like a linear threshold function.
00:07:29.036 - 00:08:16.846, Speaker A: It will always be a linear threshold. Exactly. For some linear combination of genes of properties. Right? Okay, so, yeah, so in general, rho is some matrix, but, okay, if we and the Gaussians are not independent, but we can normalize it such that rho is a diagonal matrix. I mean, we can. Right, and this is why I take the highest value. But let, in this, in this talk, we're assuming that rho is just a number, so it's just a scalar matrix.
00:08:16.846 - 00:09:30.356, Speaker A: Okay, from now on, it just, it didn't make sense for the example to say that all the rows are the same. But let's say that from now on, the cats are such that origins of the same correlation. All right, so one could hope to find, okay, maybe before I move on. I want to comment that this theorem also implies the so called isoperometric inequality for Gaussians. It follows from the fact that if we take the limit rho going to one, so the noise is very, very small, and I have the set a, which looks like this, then the probability that x is in a and y is in the complement of a, well, it'll be very small when rho is close to one. But we can consider the derivative with respect to rho. There is actually no derivative.
00:09:30.356 - 00:11:56.814, Speaker A: But if we normalize by the square root, it turns out that this looks more or less like the root of one minus rho times the surface area of a. And when I say surface area, I don't want to do the accurate definition, but it's the gaussian surface area. So I integrate the gaussian density over the surface area measure of the boundary of a. So when rho is close to one, it's pretty easy to imagine, or to at least agree more or less, that, well, the probability that I'll have one point here and one point outside of a is proportional to the gaussian surface area, right? So, if I take the limit at one, I recover a result, initially proven again by Borrell and independently by Sudokovn Sierratson, stating that hyperplanes minimize the surface area. Now, it seems pretty natural to ask if there is some kind of stability result for this gaussian noise. Stability. Now, since we're already using the word stability, let's ask if there is some robustness for this inequality, meaning that, okay, we know that hyperplanes maximize that, but can we say something about a set that almost maximizes this? So, can we say that given that the deficit between these two things is rather small? Small? Maybe a looks more or less like a hyperplane in some sense.
00:11:56.814 - 00:12:55.894, Speaker A: Half space. Yeah, so when I make a mistake once, I tend to make it again and again. So maybe when I say hyperplane from now on, you understand this is half space. All right, so maybe. Well, this implies that. I mean, I haven't really formulated what this is, but a reasonable formulation would imply that, well, inequality can happen here only if those sets are half spaces. And moreover, if there's almost an equality, then in some sense, we should expect the set to be almost a half space.
00:12:55.894 - 00:14:25.364, Speaker A: So, this was initially verified by Elhanan Mossel and Joe Niemann. I will soon state their exact result. But first of all, let's think what natural metrics one could use in order to compare a set to the closest half space to it. So I guess one natural metric we could consider is just the total variation distance. Let's define for set a delta of a to be the minimum over all possible half spaces h prime, such that the gaussian measures of h prime and a are the same of the gaussian probability of the symmetric difference between a and h prime. So for set I look at. So if this is the set a, then I kind of try to find the half space which minimizes the measure of these things.
00:14:25.364 - 00:16:12.444, Speaker A: Then I look at the gaussian measure and the theorem of Elhanan and Joe from 2012 says the following up to some constant that depends on the measure of a and on rho, we know that this delta, taken to the power four, is smaller or equal than the deficit. And here, and always in this talk, I'm assuming that h is such that the gaussian measure of h is equal to the gaussian measure of h. This is almost accurate. There is some plus epsilon here. But okay, I don't want to exactly, this is almost the exact formulation. And, well, yes, actually a functional version is equivalent. You can somehow go one dimension up, just add another dimension, and consider the set as a level set of a function, and get more or less the same.
00:16:12.444 - 00:17:53.374, Speaker A: Now let's go back to Borrell's inequality. So, since there are many experts here for applications of this, I won't state any applications. I'll just briefly say that this was initially used in the paper by Mosser, O'Donnell and Oliskiewicz in order to prove the discrete analog called majority is the stablest. It also has applications for finding max cut algorithms and many other applications. This theorem actually gives robustness a robust version for all such applications. So one example of a consequence of this theorem is a robustness for the majority stable, meaning that in the discrete version, under the assumption of low influences, a subset of the cube that has no stability close to the one of majority actually is actually close to the majority set or function. And I just want.
00:17:53.374 - 00:19:13.280, Speaker A: Okay, I just want to mention that another corollary of this, under some additional assumptions, is a robust version of the gaussian isoperometric inequality. Taking the limit rho goes to one. And this could be done only if this constant has a nice behavior with dependence on rho. So this is based on an idea of Michel Ledoux. One can apply this thing to get a corresponding robust version for the gaussian isoprometric inequality. Namely, if you have, say, whose surface area is close to the one of the corresponding half space, then the set will be close to a half space now, in the same, I think it was written in the paper. So they conjectured that this constant four can be replaced by the constant two.
00:19:13.280 - 00:20:12.144, Speaker A: We'll address this conjecture later. But before we do this, I want to try to convince you that maybe this metric is not the best metric to measure the distance between a second and the corresponding half space. And the reason is the following. So let's consider some half space h, and let's think about two possible perturbations of this half space. The first one is, I take a little strip from here, whose gaussian measure is epsilon. And I just move it over here. So I delete this, and I put a strip here of measure epsilon.
00:20:12.144 - 00:21:03.926, Speaker A: So this is the first perturbation, and the second one is the following. I take the same strip, this one, I also delete it, but instead of putting it here, I put it somewhere farther away. So, say at constant distance. So let's call the, let's call h minus the strip h one. This will be s one. And now we take again h one, but we add instead of s one. We add s two, which is a strip of the same gaussian measure, but far away.
00:21:03.926 - 00:21:20.634, Speaker A: Let's say at distance one, instead of here, it would be distance of order, epsilon. So this is s two, and this is the second perturbation. And.
00:21:25.874 - 00:21:28.814, Speaker C: At some point, it will be another space itself.
00:21:30.114 - 00:21:32.506, Speaker A: Excuse me, at some point, if you.
00:21:32.530 - 00:21:34.754, Speaker C: Go too far away, it will be another space.
00:21:34.914 - 00:21:36.426, Speaker A: Well, almost.
00:21:36.570 - 00:21:41.254, Speaker C: But exactly, at some point, you cannot go further.
00:21:43.354 - 00:22:06.574, Speaker A: Yeah, I fixed the measure to be epsilon, so. Yeah, yeah. Okay. I don't see why it would be, because even the entire half half space will have measure less than epsilon. It means what you add will become a half space. Okay. Right.
00:22:06.574 - 00:22:50.434, Speaker A: Then you cannot move the. But everything altogether will not be half this way. Now, what I claim is the following. Conditioning on the fact that x fell in s one. The probability that y fell in h one is almost the same as the probability that y fell in h one. Conditioning on x falling in the original strip, which I removed. Let's call it s zero.
00:22:50.434 - 00:24:37.594, Speaker A: What do I mean, almost, almost up to something which is of order, epsilon. Right. Because I only moved x by epsilon. On the other hand, conditioning on x following in s two, the probability of y falling in h one is, well, it's some constant, strictly smaller than one times the probability that y falls in h one condition and gone, x falling in s zero. So this is what I'm saying. If I compare these three things, then I claim that this is equal to this up to something of order epsilon. And on the other hand, if I compare it to this, and this is actually much smaller, so it's smaller equal to this sum, even if it's times some constant one plus c c strictly bigger than zero and independent of epsilon.
00:24:37.594 - 00:25:43.664, Speaker A: And this means that the stability of the first set. Well, what is it? So, it's the probability that both x and y are in h one, plus twice the probability that x is an s two. Oh, sorry. This is one twice the probability that x is in s one and y is here, plus something of the order epsilon square, which is the probability that both fall in s one. So I claim that this is something like the stability of the original thing, minus something of the order epsilon squared. Just because I have this epsilon of, this is the probability that one of them will even fall in either s one or s two. And if they fall in s one, I only get Luz and epsilon.
00:25:43.664 - 00:26:56.082, Speaker A: But the probability. But the stability of two, well, this is the stability of h times something of the order, at least epsilon. Now, both of these sets have the same total variation distance from h. So I can't hope, I mean, there is definitely something I'm not capturing when I'm considering only the total variation distance. So, 1 may hope for a metric that gives us some more information. And the theorem I want to talk about is the following. Oh, if I define the following matrix.
00:26:56.082 - 00:28:05.264, Speaker A: So, instead of delta of a, let's consider the metric epsilon of a, defined by, we take the gaussian centroid of h. So h is just, again, any half space whose gaussian measure is the same as that of a. And I look how far it is from the origin. And I subtract the norm of the gaussian centroid of a. If I want those really to be centroids, I have to divide by the volume. But it doesn't matter so much here. I claim that this metric does capture the difference between these two things, right, because it measures how much mass I moved and also how far I moved it from where it originally was.
00:28:05.264 - 00:28:58.704, Speaker A: This will be order epsilon Square here. And this will be order epsilon. If I look at. Yeah, this is already called epsilon, but I guess it's okay. And the theorem that we get is the following. So if I consider the same deficit, it can be bounded actually from both below and above by essentially almost the same quantity. So I have here a constant, depending on the measure of a and rho times epsilon of a.
00:28:58.704 - 00:29:34.434, Speaker A: And here the same thing with a different constant. Unfortunately, we don't get exactly this. We get this with a logarithmic correction. So, on this side, I also have. So this is times the logarithm of epsilon of a minus one absolute value.
00:29:39.254 - 00:29:42.874, Speaker C: What is the requirement on h? He has the same measure as a.
00:29:43.614 - 00:29:55.734, Speaker A: Well, yes, yes, yes. That's the only. That's the only. Right. But. But if I turn h around, the distance, just the module o of the centroid doesn't change. Right.
00:29:55.734 - 00:29:59.318, Speaker A: I mean, I look at the half space and I turn it.
00:29:59.406 - 00:30:01.674, Speaker C: So the mean is like that. You change it.
00:30:03.214 - 00:30:30.264, Speaker A: Right. But I take the norm of the mean. Okay. It's definitely natural. If I have a set a to look at the h whose norm is in the same direction as that of a. So, this will be the centroid of h, and this will be the centroid of a. It's not hard to see that the centroid of h is always farther away from the origin, which is, say, here.
00:30:30.264 - 00:30:39.464, Speaker A: All right, and then. And then this distance just measures how much mass and how far away to push it.
00:30:46.994 - 00:30:52.170, Speaker C: But you take the restriction to hd.
00:30:52.242 - 00:31:25.160, Speaker A: Yeah, well, yes, it is related, but not exactly. That's actually a very good question, but can I touch it? I actually wanted to touch it a little bit later. So, I think the metric that you now mentioned is, in some sense, even better. Okay. But it's kind of harder to approach, proof wise. Okay. Okay.
00:31:25.160 - 00:32:03.650, Speaker A: So, it took me a bit longer than I expected to do. So that was the introductory part. And what I'll do from now on is give some ideas of the proof, mainly focusing on trying to convince you that this quantity is very natural, not only because it captures more, but also because it appears in a very natural way relating to gaussian noise stability. Yes. Oh, right. Yeah. Thank you.
00:32:03.650 - 00:33:01.454, Speaker A: Thank you. So, one. So, two corollarys of this. So, the first corollary is that the conjecture, that this conjecture is verified up to a logarithmic term. So, actually, delta square of a is always smaller or equal to epsilon of a up to some constant, I don't know, ten. So, if I just plug this in, this verifies this conjecture, but basically, this inequality is an equality in this case. This is some of the worst case.
00:33:01.454 - 00:34:07.434, Speaker A: And the second corollary is. Well, again, using Michel Do's methods and taking rho to one, the dependence on rho in this constant is good enough to get a robust version of gaussian ISO perimetry. And I don't want to state the exact theorem, but basically, you can imagine what kind of. I don't remember if I mentioned this. This is not the first robust version. It improves the, a certain result of the result of Elhanan and Joe. And there was actually another result by guys whose names I don't remember, which give another, in some senses weaker and other senses stronger.
00:34:07.434 - 00:35:05.306, Speaker A: Isoperometric result. All right, so let's try to see some ideas from the proof. So, first of all, if I want to look at the stability of a set a, this is equal. Let's go back to the definition. It's the probability that x and y are in a. And if you remember, I could define x and y as square root of rho times some gaussian. Z one plus root one minus rho times an independent gaussian.
00:35:05.306 - 00:36:07.154, Speaker A: So this was x and y was the first term was the same, and the second was just an independent copy. So z one, z two and z three are just independent standard Gaussians. And definitely what I can do is the following. I can take expectation over z one and condition on z one inside this probability. This obviously doesn't change anything. Now, once I conditioned on z one, note that these two things become independent, right? So now that they are independent, what I can do is just erase one of them and put square here. Right.
00:36:07.154 - 00:37:50.874, Speaker A: Now, what we do now is we take, let wt be a standard brownian motion. It's clear that, well, w at time rho has the distribution of square root of rho times z one, right? And the joint distribution of w time rho and w time one is the same joint distribution as square root row of z one and square root rho z one plus root one minus rho z two. Right? Because w one minus w rho is an independent gaussian. So this will actually be root one minus rho z two. Which means that I can go back here and say that the noise stability of a is just the probability that w one is in a conditioned on w rho square expectation. What I did is replace this with w one. And, well, this would be w rho.
00:37:50.874 - 00:38:59.658, Speaker A: And therefore, it seems kind of natural to define the dub martingale empty to be the probability that the brownian motion at time one is in a conditioned on the filtration at time t. So if you're uncomfortable with the filtration, we condition on w at time t here. And what we get is that s rho of a. The quantity we're interested in is just the expectation of this martingale taken a time rho square. Right? How many of you feel a little bit uncomfortable with Ito's formula? No. No. How many really have no idea what.
00:38:59.658 - 00:39:23.102, Speaker A: Okay, a little or more. I mean, a little or more. Yes. Yeah, that's kind of plenty of people. Okay. All right, so really briefly, so this thing is a martingale. Obviously, it's a dube martingale.
00:39:23.102 - 00:40:34.504, Speaker A: Any martingale. I can write as follows. So I can write Mt to be the integral between zero and t of some process sigma taken at some s, saying how fast empty is varying times the increment of some brownian motion. And if you're uncomfortable with this thing, just imagine it as plus minus one times the root of ds. Just fix ds to be very small and take independent Bernoulli increments. And, well, this is a very, very, maybe the most basic application of Ito's formula. Just says to us another way to write this is that the increment of empty in a short time interval is just some sigma times the increment over brownian motion.
00:40:34.504 - 00:41:56.204, Speaker A: This can also be imagined as a Gaussian of variance ds. And if we try to think about what m at time t plus dt is equal to m squared times t plus dt, this will be empty plus dmt square, which is empty square plus two. I just opened the brackets mt dmt. So this thing is just a martingale thing, plus now dmt times dmt, unlike in usual differentiation, since I took root of ds here, it'll just give me dt, sigma t dt. Sorry. Right. And therefore, the expectation of m at time rho is just the expectation of the integral between zero and rho of sigma t squared dt.
00:41:56.204 - 00:43:27.534, Speaker A: When I take expectation, this term just vanishes and I am left with this thing. So all we care about is seeing how fast this martingale varies, right? And in order to do this, square. Sorry. Yeah, I'll get exactly to the point where this center of mass appears. So in order to do this, I want to look at Mt and try to differentiate it in a smarter way. So, first of all, let's note that the distribution of w one conditioned on what happened until time t, is just a normal vector centered at wt, whose standard deviation is one root, one minus t. Which encourages me to define a density ft of x to be just the gaussian density centered here with this standard deviation.
00:43:27.534 - 00:44:37.184, Speaker A: So some normalizing constant times e to the minus x minus w t squared over two, one minus t. So this is just a family of densities beginning from the standard Gaussian. Its center moves according to a brownian motion and its variance shrinks. So just imagine something moving and the time one, it becomes a delta measure somewhere. By definition, we have that mt is just the integral over a of ftx dx. And in order to calculate how mt varies, we want to differentiate this thing, and we'll actually differentiate inside the integral. So I want to calculate dft of x.
00:44:37.184 - 00:46:37.784, Speaker A: So, this is easily done by Ito's formula. And I think what I'll do is I'll just skip the calculation and tell you what the result is. So, it turns out that what we get here is this formula. Just half a minute to give some intuition about what happens here. So we see that in a little time increment, in order to move from ft to ft plus dt, what we do is we take ft and we add to it again ft, but multiply it by some linear function, right? This is just a linear function of x whose gradient is distributed according to some small normal variable. So basically, the process ft is just the product of many, many, many linear functions with infinitesimal gradient gradients. And, well, this kind of does make sense, because if we consider the one dimensional case, suppose that the first linear function was one plus some epsilon times x, and the second linear function was one minus some epsilon times x.
00:46:37.784 - 00:47:47.064, Speaker A: So we have many one plus and one minuses, which kind of cancel out. But each such cancellation gives us one minus epsilon square x squared. So a quadratic thing. And if we take, if we multiply many, many things like that, if we raise that to a high power, what we get is something like e to the minus x squared, right? So many, many linear functions kind of get a gaussian density, and, well, they won't all cancel each other out. The remaining terms will. Will give us something like e to the x, right, or some exponential thing, which will just move the center of this gaussian. So this may be a weird way to look at this simple process, but in this context, at least, it seems like a rather natural way, because I have five minutes left, right? Okay, great.
00:47:47.064 - 00:49:11.468, Speaker A: And now we get to the main point here. So, when I take something and multiply it by a linear function, and I want to see how it varies, how this integral varies. I only care where the centroid is located, right? The integral of something multiplied by a linear function. If I put everything in the centroid, everything will be the same. So if I want to measure how fast this thing varies, all I care is how far the centroid of a with respect to this measure is from the centroid of this measure, which is at wt. Now, if a was a half space in the beginning, then at any point in time, well, a will definitely still be a half space. Also, if we kind of normalize back so that this thing become a, becomes a gaussian measure, we can always consider some change of variables, which turns this back into a gaussian, a standard gaussian measure.
00:49:11.468 - 00:50:33.522, Speaker A: Sorry. So a will just move, but remain a half space if it started from a half space. And the point is that if I compare any set which is not a half space with the corresponding half space which has the same measure, the centroid of the half space will always be more far away. So I'll just finish with this picture at least. It's easy to agree that the time zero if a looked like this, and they consider the corresponding half space whose center of mass is in the same direction. Well, so if this is a and this is h, the centroid of h will be more far away from the origin than the centroid of a, because it's always worthwhile to take the mass located here and move it in this direction. So at least for small values of t, it's kind of clear that if I want to maximize the scale of this thing, then I want to maximize the norm of this vector, and then I want to take the set to be a half space.
00:50:33.522 - 00:52:03.944, Speaker A: And in the minus half minute I have left, all I'll say is that there is some coupling argument that kind of lets me take this principle and carry it to larger times. And. Well, for the robustness, you need a little like some more ideas. But the point is that if a wasn't close to a half space in the first place, all you have to do is show that it remains far from a half space for some significant amount of time. And then the process corresponding to a is kind of lagged behind a corresponding process for the half space. And I guess I'll finish here. Yeah, so the depend, I didn't really try to optimize this, but the dependence I get is something like the size of a times the size of, well, one, minus the size of a.
00:52:03.944 - 00:52:34.524, Speaker A: And this taken to some power, I don't know, 1010 or something. Well, ten at one side and minus ten at the other. Let's say I believe that, well, up to this constant, this is the correct dependence. But I didn't really, I mean, I don't really see an application to trying to optimize is constant.
00:52:36.944 - 00:52:43.624, Speaker B: In particular, you are improving the majority stable. So your proof is good for proving the majority.
00:52:43.704 - 00:53:19.284, Speaker A: Well, there's. To prove the majority is stablest. First of all, you need much less. So Joe showed yesterday probably a much simpler proof. But both what I did here and what Joe did yesterday lack this component of trying to understand what the low influences will give you if you just use this idea, or basically, if you just use the gaussian stability. Oh, yeah. Well, yes.
00:53:19.284 - 00:53:20.980, Speaker A: So this is exactly.
00:53:21.172 - 00:53:38.088, Speaker B: My question is, so is your proof. So if we. Of course, it gives much. I mean, we are using. It's like shooting over shooting. But still, if you are considering your proof idea, is it like a very different proof idea from the previous proof ideas?
00:53:38.216 - 00:53:50.444, Speaker A: Yeah, it's a majority. Yeah, it's a new proof. I mean, it seems like. I mean, yeah, it seems like a completely new type of proof.
00:53:56.404 - 00:53:57.500, Speaker B: How general is this?
00:53:57.532 - 00:54:05.304, Speaker A: If, let's say, instead of it here, you could use it. I mean, the argument here, you can take any martingale. Right.
00:54:05.724 - 00:54:08.012, Speaker B: You have the quadratic variation and you.
00:54:08.028 - 00:54:49.154, Speaker A: Have the same exact argument. Well, that's true, but somehow for this, specifically for the Gaussians, you get a process of measures which you can actually differentiate and. Right, so the ETo differential of this thing turns out to be a very simple thing. Well, you could conceive. So. Yeah, you could. Okay, so maybe I'll just say that there are versions of this equation where you could do so.
00:54:49.154 - 00:55:28.854, Speaker A: This appears in papers related to things, proving things in different contexts, concentration of measure and convex bodies, etcetera. You could start from this equation where your initial density was just any given density, and this will give you some family of measures which. Which you can also differentiate. But I don't see how to. Yeah, I'm not sure what kind of results you are hoping to do with this. I just want to know how general this was. But I think you made that.
00:55:28.854 - 00:55:29.794, Speaker A: Okay.
00:55:30.974 - 00:55:41.090, Speaker C: Even more general than linear space. Of course, you can take the mean, but here, for instance, will be the geodesic if it was in a setting.
00:55:41.202 - 00:55:47.042, Speaker A: Yes. Right. So you have to know what a linear function is.
00:55:47.098 - 00:56:03.854, Speaker C: But here you will have the geodesic going from the w two to x, and you have to take the initial vector. We are getting away from the gaussian aspect and already to the matoria.
00:56:04.694 - 00:56:18.334, Speaker A: Yes, yes, that's. Yeah, yeah, I agree. Okay, thanks. Next talk is at 10:20.
