00:00:00.600 - 00:00:51.558, Speaker A: In both information science and also affiliated with computer science. Today I want to talk to you about AI and humanity on the ground, a bit of a context. I am a designer. I design human AI in action. What does that mean? I don't invent AI algorithms, but I take the latest algorithmic advances from AI research and see what kind of thing we can build. And I also don't develop ethics or social science series, but I take research from those communities and see what are the potential harms that we need to consider when we design these applications, and we put these applications in real world context and see what additional lessons we can learn and can feed back to those communities. So today I want to give you.
00:00:51.558 - 00:01:59.020, Speaker A: I want to spend some time on one specific case study, and from there I want to talk about some lessons we learned in the process with regard to AI and humanity. This is from quite a few years back. A group of biomedical engineering researchers came to talk to us, and they want to build an AI that helps with one of their most difficult clinical decisions, that is, whether and when to implant an end stage heart failure. Patients with an artificial heart. Artificial heart is a mechanical pump that you put into the patient's heart and pumps the blood for them. When people have really severe heart diseases, there is a tube coming out of the patient's body, linking to an external battery, and the patient will need to carry that battery and take care of the wound for the remainder of their lives. As you can imagine, there's huge medical risks associated with this open heart surgery and recovery, and also it involves a huge lifestyle change.
00:01:59.020 - 00:03:05.264, Speaker A: You'd better remember to charge that battery. As in many other end of life decisions, it is the doctor's job to decide to identify patients who are most likely to benefit from this treatment, people who are sick enough to even make sense to do such an invasive, aggressive treatment on the patient. But also the patient should not be too sick, otherwise they cannot survive the open heart surgery. There is this very tricky task of identifying this right window of sickness and right time to implant. This is a very difficult decision. The idea is that machine learning can help if we have about a decade of data about the outcome of previous implant patients. The idea is that if we can learn what kind of features, what kind of factors that can predict better treatment outcome, then we can systematically identify people who are most likely to benefit from, from the treatment.
00:03:05.264 - 00:03:45.754, Speaker A: Here's a screenshot from such a system from 2008. And I have to say, systems today look roughly the same. You put in a list of patient variables in electronic medical records. And then you click enter and you see predictions of the likely outcome of this surgery. Now, over the years, we have. Research has repeatedly showed that doctors don't use this kind of system. Among the many buttons and many functions in electronic medical records, this is the least used one, not an easy achievement to reach.
00:03:45.754 - 00:04:32.924, Speaker A: And a common complaint is that their experiences are better than AI. And I think they have a really good .1 of the things when we work with those doctors. Doctors very often ask is, are these outcomes, are these previous outcomes? Based on my best effort, you know, I perform less well if I had a fight with my wife last night. And this kind of nuances might be very strange for us, for people who build AI systems, but for doctors, they really know it. They know all the limitations of the data set. On top of that, they know there are limitations of the AI, so they really resisted this kind of systems.
00:04:32.924 - 00:05:44.464, Speaker A: So our goal is just trying to find that kind of middle ground, like, how can we make these systems useful, knowing that there have been problems among the doctors, like human decisions, but also to understand how these. How they can complement each other. So we went to three hospitals across the US, and we spent about 14 full days in their advanced heart failure programs, observing how these clinicians take care of patients and make decisions. I think, well, we found many barriers that would prevent this AI from being useful. But one of the key takeaway for us was there is just really a lot of human messiness in that decision. I'll give you one example. So, one of the clinical guidelines is that the patient should be given more consideration if they have a spouse, if they have someone who can take care of them in this recovery process of the open heart surgery.
00:05:44.464 - 00:06:32.676, Speaker A: So the doctors would be struggling with. For example, I know this patient is about to have a divorce, but on paper, this person is still married, so actually meets the standard to be eligible for this treatment. Do I tell it? Do I tell them or not? These are the very human decisions that the doctors try to make. And the way they make this decision is not that. Let me see whether this fits the guideline or not, or how ethical it is, manipulate a little bit about that person's marital relationship. Instead, they're thinking, how can I save this patient? This is a last stop treatment. If I say no, this patient go to palliative care.
00:06:32.676 - 00:07:05.286, Speaker A: So for them, the decision is not really to consider eligibility. They're considering. Do I have a better alternative? If no, maybe I should just try it. Regardless how high the risk this treatment is in dealing with all these very messy conditions. What we found is doctors almost never make decisions in front of a computer. They always make decisions, especially the difficult ones, through conversations. I put this image because I cannot take any photo in my healthcare empirical work.
00:07:05.286 - 00:07:35.482, Speaker A: But this is really what the doctors meeting reminded me of. There are four chandeliers over your head. There is no big projectors like this, because in our environment, discussions are centered around computers. But for the doctors, the decisions are all centered around human beings. And the human beings are. The clinicians in this case are very working in a very hierarchical organization. People who are attending physicians, people who have the power to sign off clinical decisions set on one end of the table.
00:07:35.482 - 00:08:47.154, Speaker A: And then you sit according to ranks. So people who, the social workers, the nurse practitioners, the interns, who actually take care of the patient in and out almost 24 hours a day. Those people sit in the very far end of the table, and they don't get much to say in the meeting, even though they really have very felt understanding of what this patient condition has been. So with all these insights in mind, we started to consider a different kind of AI. So the idea is that instead of thinking about our AI as something that can inform the decision of implant or no implant itself, we want to think about AI as something that provides, like, one component of that decision. So here is the model we end up retraining. Instead of predicting whether you should or should not implant here, or survival, the longevity post surgery, we predict the likelihood of every adversarial event.
00:08:47.154 - 00:09:22.694, Speaker A: What does the chance of renal failure look like? Because that has all kinds of implications for not only the implant eligibility, but also the quality of life of that patient. We embedded this machine learning prediction. It's in a corner here into this meeting. Slide. The idea is that on this slide, every component represents one faces of that patient's condition. Here's some medical history. Here's that person's financial evaluation.
00:09:22.694 - 00:10:02.544, Speaker A: Here's that person's psychosocial evaluation. Maybe there is a clinical note about that person's marital status as well. The idea is that AI is really not trying to be at the front and center. Instead, it's just one another piece of information that you should consider in the clinical care process. One doctor puts it very elegantly, AI would just be a blood test or an x ray in the future. No doctor makes a decision solely based on blood test or solely based on an x ray result. It's always a big picture.
00:10:02.544 - 00:11:00.002, Speaker A: But just like x ray and just like blood test, AI is a really important component in this. We tested a system across three hospitals in the US, a different three hospitals, and we really found really encouraging results. Doctors really embrace these predictions and they actually use the AI. Spend a lot of time deliberating what AI means. How much can you trust a prediction versus a fact, where, I mean, everything in clinical decision making is about clinical trials and causal effects, like how do we deal with this prediction? So that elaboration was really helpful, and also it provides a lot of vigilance for the people who sit at the far end of that long table. They can say, hey, doctor, I don't agree with you. Here is an AI that also says, well, they don't need to say, I don't agree with it.
00:11:00.002 - 00:12:01.874, Speaker A: They can say, hey, doctor, why do you think the AI, you know, says this? And so it triggers a social conversation and in that sense, it's really, it really provided more perspectives in that discussion. And we have evidence that this kind of method and this kind of design, this idea that AI being one faces of clinical decision, really generalizes to all kinds of complex clinical decisions that involve multiple organs or usually involves multiple groups of clinicians. Here is just another quick case study where this is a collaboration with Google Brain and Google Health, where we applied the exact same method to cancer diagnosis on pathology slides. By the way, this is how clinical best practice looks like. This is what the doctors are making your cancer diagnosis based on. Really beautiful art. We found that doctors make these decisions really based on gut reaction.
00:12:01.874 - 00:13:01.674, Speaker A: They can diagnose, they can see cancer on that slide, which is only twice as large as our actual cells, and they can see cancer from there. So that really instinctive image recognition skill, when complemented by AI, we can improve the cancer diagnosis accuracy by about 30%. That's a really encouraging result. I also want to mention other scholars work that over time had discovered similar patterns in other critical domains, like civil litigation and artists working with language models and so on and so forth. What does this all mean? I want to share three ideas I think are really mainly for provocations. I'm curious what you guys all think. Point number one.
00:13:01.674 - 00:14:01.404, Speaker A: I think when we discuss the imperfections of AI, we should always talk about them side by side with human limitations, just like human beings. Data and the AI built upon it always captures humanities or always capture human needs in a quite partial and quite reductive way. But that's the same as how doctors make decisions. They cannot possibly think about every possible facet of the situation. In order to make decisions, you have to be reductive in some way. I think it's relatively fine in a lot of cases it's fine for AI to be partial and reductive, but the key is that we recognize these limitations. We recognize how these AI's are partial and are reductive and deliberate these limitations alongside with the human perspectives.
00:14:01.404 - 00:15:11.946, Speaker A: As long as there is this deliberation, most often we can get good results, as shown in all the case studies I mentioned earlier. But then here comes a new question. I think the real challenge of making AI, like accounting for AI unintended consequences, is really people's limited attention. I think it is not a coincidence that some of the most damaging AI outcomes come from seemingly trivial things like social media, reading social media feeds, YouTube videos where you see cute cat videos, rather than medicine law, these seemingly really remarkable places. That is because doctors and lawyers and so forth spend a lot of time deliberating. They recognize the AI's limitations, and when they are open enough to use it, they know how to put it in perspective with their personal understandings. But in a lot of other cases, people don't have the attention, people don't deliberate, or people cannot even imagine how the AI can be wrong.
00:15:11.946 - 00:16:10.034, Speaker A: And I think those are the places that are really dangerous. One last point. I think we're living in a society where deliberating on each and everyone, each and every AI, is becoming increasingly impossible. Our group currently does a lot of work in text generation and writing among everyday people, not artists. So people who don't have the time to deliberate on what does each AI suggestion mean for people who work on platform tags where like think about Google Search, which has more than 1000 AI systems running in the background, you cannot deliberate on each limitation of each AI. And that makes it really hard for us to think about human AI complementarity. Think about how you can make this process deliberative and thoughtful, and how can we prevent biases in these contexts are the real challenge for our next step.
00:16:10.034 - 00:16:11.014, Speaker A: Thank you.
