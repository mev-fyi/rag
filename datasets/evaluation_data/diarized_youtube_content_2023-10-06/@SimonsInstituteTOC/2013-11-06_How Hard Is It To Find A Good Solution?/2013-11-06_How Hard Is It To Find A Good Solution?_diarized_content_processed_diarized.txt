00:00:05.320 - 00:00:51.402, Speaker A: Well, first I want to say that the Simons Institute is a really nice place, and thank you very much for the invitation to be here and also to the challenge of giving this lecture. Of course, I will try to say something in this lecture, but I'll try not to say too much to the colleagues in theoretical computer science that so nicely showed up. But I try to tell things to people who are not in the field. So I really want to create a feeling for what kind of things we're thinking about. Tell you some results and also some open problems, actually. And you'll see that some of the open problems are so easy, so you can go home and think about them directly. And, well, the hope is that you'll understand some of it.
00:00:51.402 - 00:01:30.744, Speaker A: So if I start losing you and speaking too fast, please raise your hand and say that. Ask questions. I think that always adds to the talk. So what's efficient computation? Well, there's sort of a theory side and a practice side to it. And on the practice side, you in fact come into it every now and then. So, for instance, if you are in a big city which has good public transportation, the question how to get from point a to point b might be difficult. You expect your search engine to return it within a second, and that's sort of doable, as you can experience every now and then.
00:01:30.744 - 00:02:13.500, Speaker A: Sometimes you don't get as good answers as you hope, but that's a different point. The other sort of end of the scale is whatever you can do in real human time. And something that's been a little bit on the topic recently say that, you know, can anybody, or say the NSA break the AE's encryption with reasonable amounts of computation and say that's like 1000 PCs in a year's, and this we don't know the answer to? I would guess no, but nobody knows for sure. Or maybe the NSA knows for sure. But the Simons Institute is an institute for the theory of computations. I'll do theory for essentially the rest of the talk. Every now and then I'll say something in practice.
00:02:13.500 - 00:03:11.624, Speaker A: And what's efficiency in theory? Well, we had a problem. We have a problem of a certain size which is usually parameterized by number n, and we sort of ask how many elementary computations do you need to perform something? So let's do something very simple. You know, let's multiply two numbers that you should think of really big, you know, thousand digits or a million digits or something like that. But I made it slide, so I made for five digits. So the question is, suppose we all that's one of what we thought was mathematics once upon a time, that, you know, I get you to multiply large numbers, and suppose you multiply two five digit numbers. We set it up like this, or at least did in Sweden when I was in whatever is middle school or none, and then you add it up. And then, if you think about it for a while, this sort of says that the number of multiplications and additions of one digit numbers are about four n squared.
00:03:11.624 - 00:04:08.594, Speaker A: It would be like 100 elementor operations doing this. Five by five. You need to multiply each digit in one by a digit in the other and so forth. So the questions we love to study. Now, is this the best possible, or is there a more efficient way to do this? And the first thing I want to say is, this is obviously the best possible for multiplication, okay? Because you have n digits in the first number, you have n digits in the second number. You need to multiply each digit in this one by the other, right? How many feel that this was sort of a proof? How many people felt that this was sort of a fishy argument, that you sort of wasn't really couldn't be formalized? Okay, so that was a theoretician. All the theoreticians know that this was completely bogus, right? But some of it were a little bit caught up.
00:04:08.594 - 00:04:49.888, Speaker A: No, nobody dared raise their hands on the first one. So this is completely wrong. It sort of assumes that we're going to do multiplication the standard old way. And sort of the name of the game is that we don't want to do things the way we've always done, and we want to do them in a more efficient way. So how do you do multiplication? Well, for those who have a little bit of background, I mean, multiplication is really convolution, if you think about it, if you know what convolution is, and if you don't, you can start listening at the next slide. And then you really think of your numbers as vectors of digits. You do some fast Fourier transform.
00:04:49.888 - 00:05:25.942, Speaker A: You multiply the transforms, and then you do an inverse FFT, and then you clear up the result. And, in fact, if you do this, you have to choose certain things. Where are you going to do the fast Fourier transform? How are you going to do the. The multiplications of the point? By multiplications and a few details. And as late as 2007, this algorithm was actually improved to get a better running time. And it's essentially n log n. It's a little small factor which grows to infinity, but it's essentially n log n operations.
00:05:25.942 - 00:06:08.554, Speaker A: And this was an improvement. There was an improvement of a previous algorithm that was to champion for 35 years by Schoenhagen Strauss. So what I want to point out is that some so easy operations, multiplication of large numbers, we don't know how to do it efficiently. We keep finding better ways to do it, and we don't know that it's hard at all. We're nowhere close in proving that n log n is the best possible. It's quite possible that you can do it in ten operations, where you have n digits in each of the numbers and operations. I mean, you know, multiplications of one digit numbers.
00:06:08.554 - 00:06:52.982, Speaker A: So this is the first open problem that you can go home and think about whether this is possible or not. I would suspect it's not, but who knows? So this is sort of saying, you know, n squared is not so good. N log n is better, and this is with real utopia. But for the most of the talk, I really want to do theory. Theory. And that's sort of saying that any function of operation that grows a little bit slowly within, with the running time, with the input size, like a polynomial, is good. And even a polynomial like this, which almost doesn't appear in practice, will be considered good.
00:06:52.982 - 00:07:40.494, Speaker A: But anything that grows faster, in particular exponentials, is bad. That means that the running time will increase so quickly with the input size that, in fact, we cannot do it. And here the green curve is two to the n, and the blue curve is x cubed. It's just saying that what really matters is that, you know, how does running time increase as the input size goes up? So let's take the favorite of all problems, which is the traveling salesman problem, and you have a salesman that. Here's Sweden, by the way, Norway, Finland, Denmark. And the question is, he has 100 cities in Sweden. He wants to visit them as quickly as possible and travel as little as possible.
00:07:40.494 - 00:08:16.804, Speaker A: This sort of map, even if you don't see the cities, gives you a lot of intuition. But once you're in a computer, you should really think that you have to give your distances, and you're supposed to compute the best way to do it. And if you're. I just gave the five biggest cities, and then it's sort of clear how to find the best ordering. You just try all possible orderings. That's a very quick way to do it. And it turns out that you should go from Stockholm to Uppsala, to Lulu, to Gothenburg to Jotte Boran, and to Malmo.
00:08:16.804 - 00:09:15.450, Speaker A: So let's return to our, the situation where we had 100 cities and then it's sort of clear, how should we do the, we can do the similar things here. We can assume we start in Stockholm, which you always do in Sweden, and then you have 99 possibilities to the first city to go to, and then 98 for the second, 97 for the next, and then sort of, if you multiply all these numbers, that's the number of possibilities you get. Okay. And this seems like it would be a pretty big number, but you don't realize exactly how big a number it is until you write it out. And this is sort of the number of possibilities we would have to try. And this number for those who not experience is of course 99 factorial with an exclamation point. And this is something you could try in a contract with a non mathematician.
00:09:15.450 - 00:09:54.504, Speaker A: If you strike an exclamation point after the sum and they somehow, and they tell you they're going to pay the actual sum, they actually owe you a fair amount of money. The question is if that would stand up in court or not. But that's. Well, so let's think about it for a while. Computers are in fact pretty fast, and the standard computer on your desktop makes a billion operations a second. That's a pretty big number. And if you look at TSP, that in fact tells you that even if you're a very bad programmer, you can do 14 2nd cities in a second.
00:09:54.504 - 00:11:23.624, Speaker A: But in fact, if you want to do 100 cities, as you might have noticed on the previous page, this would be large number of seconds. And now when you're thinking about billions and lifetimes and so forth, do you expect to live in for a billion seconds? How many people expect to live for a billion seconds? How many people don't expect to live for a billion seconds? Okay? So hopefully most of you are wrong because a billion seconds is 30 years. Okay? So that's a way it sort of tells you a little bit about the numbers. So I hope to have about a billion more to go, but that's, that's the way life is. Okay, so the fact that computers are fast, I think, sort of brings this quote to mind that I wanted to say, which is attributed to Knuth, which is one of the old men of the field, says that you shouldn't optimize too early because, you know, computers are pretty fast and if you, it's more important to get the program correctly and then it probably will run fast on an actual computer. But during my lifetime, I sort of felt the other way around. Maybe complete absence of optimization is even worse because sometimes I can't understand why I have to wait 5 seconds while the computer doing something trivial in that period of time, it has done 5 billion operations.
00:11:23.624 - 00:11:41.164, Speaker A: That's a lot of operations. So that's new quote. Okay, but that's not where we were going really. I want to return to the traveling salesman problem. And in fact salt by held corp. I believe it is right.
00:11:41.864 - 00:11:43.056, Speaker B: Also Belmont.
00:11:43.200 - 00:12:05.662, Speaker A: Also Belmont. Yes. Prove that. In fact you can do it in a little bit better than m factorial in time, essentially two to the n. And in fact that sort of says that 25 cities is doable in a second, but still that 100 cities is still not doable in a lifetime. And that sort of, this happened in the. Must have been in the seventies or sixties.
00:12:05.662 - 00:12:43.904, Speaker A: Seventies, sixties, sixties. And we haven't really improved this in the worst case, running time. And the question that sort of steps out, maybe this is best possible, maybe nature is this bad, maybe we need these many operations. And the embarrassing thing is that I said before that we don't know how to do multiplication, that that requires much more time than reading the input. But in fact the same is true for the traveling salesman problem. We don't know that. In fact you have to read, I mean, note here that the input is n squared, because we have n squared distances here to read.
00:12:43.904 - 00:13:33.774, Speaker A: So you don't know how to do more than read the actual input that you have to do that actually compute the best tour. So this is where we are at, and our hope to go anywhere in this world was sort of done by forming the NP complete problems. And I won't tell you exactly why they're called NP complete and so forth. All I want to say is that this is a family of a very large number of computational problems. None of them is known to be solvable in polynomial time. And they have the property that if one of them is solvable in polynomial time, so are all of them. And this question, whether none of them or all of them are solvable in polynomial time, is the famous question whether MP is equal to P.
00:13:33.774 - 00:14:12.566, Speaker A: And that's, well, it's really the main open question on computer science. And when I started in the eighties, we thought we would soon solve this, but this hasn't really moved in the 30 years since I've been in the field. And there are lots of problems that are NP complete. I wrote a couple of one. I will return to this one later. But the main thing to note here is that the traveling salesman is one of these problems that's NP completed. So, as I said, we haven't been able to prove that MP is not equal to P.
00:14:12.566 - 00:14:58.234, Speaker A: But I would like to state that every reasonable researcher in this area believes that they're not equal. And this, by mathematics, is, in fact, a tautology, because anybody who believes that they're equal is not reasonable. So that's where this quote is done from. No, but joking apart, it seems like most people would come to the conclusion that MP is probably not equal to p. And, in fact, it seems like an extremely difficult problem. And the problem is that it's very hard to prove that something requires, you know, large computational resources. So, really, the way things have been going is that we simply assume that this is the truth.
00:14:58.234 - 00:15:34.394, Speaker A: We can't prove this as a mathematical theorem as we would have wanted to prove. But instead, we'll say that this is a law of nature and sort of take a physicist's point of view, and then we try to derive consequences of this law of nature. So, I'm not getting any objections here, which sort of makes me worried. But anybody can object and ask questions. Yes? Yeah. So, that's a quantum compute. I mean, the next semester at Simons Institute will be devoted partly to quantum computing.
00:15:34.394 - 00:15:59.246, Speaker A: Right? So I would be extreme. You know, it could be that quantum computer can solve NP complete problem. It doesn't seem so. People have been thinking about these problems for ten years. But let me take my next slide. Suppose we actually, in some word, that p MP actually sort of in a really equal speed. By this, I mean that, you know, we have really fast algorithms for all NP complete problems.
00:15:59.246 - 00:16:39.186, Speaker A: That's a little bit what you're saying. It's a logically conceivable world, and I think it's a good world in the sense that all planning problems are easy. It's a good or bad world in the sense that to prove a theorem is as easy to verify the proof, which means we don't need as many mathematicians, which mean that we could either think of it as a good or a bad thing for the world. A third thing that I think is that we have no security or privacy on the Internet, which is probably a bad thing for the world. But that also might be a matter of taste. But, yeah, it could be. But I would be extremely surprised if we're in this world.
00:16:39.186 - 00:17:32.154, Speaker A: I don't know how to answer it, but that's good. So now I'm sort of starting to close up on where we're going. We view the world that NP is not equal to P. That's our law of nature. Terminal salesman problem is np complete, and hence we do not think that there will be an algorithm that always solves it correctly and always runs fast, but we want to solve the problem. And then there are sort of two options to go. We write an algorithm and we have to sacrifice something, either that it sometimes is very slow or sometimes that it returns a not so good solution.
00:17:32.154 - 00:18:21.154, Speaker A: And both these are variable approaches. But I'm mostly interested in the second one. We want an algorithm that always runs fast and always returns a reasonably good solution. So let's see, the first algorithm that possibly comes to mind is the way many of us lead our lives and said, you know, we do one thing at a time and we do the one that's cheapest for the moment. We visit a few cities, and we always go to the closest city that we haven't visited, and then we sort of see what result we get. And if you look at this, for instance, this is very efficient. It's very easy to calculate what to do next.
00:18:21.154 - 00:19:12.854, Speaker A: There are some instances where it does work pretty well, but there are some instances where it works less well. And now we're sort of interested to say, how well does it work? We need to really say how efficient it is to calculate. But that's sort of an easy point. And the question is, how good the solution does it require return? And the dominating thing that to evaluate this is sort of to compute the ratio between the solution that we find and the ratio of the optimal solution and then see what guarantees we can give on this number. So then we essentially want to be really happy. We want two statements. The first is a proof that says that we will always be pretty close to the best value.
00:19:12.854 - 00:20:00.060, Speaker A: And what happens to be the case for the nearest neighbor. The greed heuristic is that you're at most an absolute constant times log of the number of cities times optimal. You never do something that's worse than this. And the second thing you want is sort of an example saying that, well, this is the best mathematical theorem around. There are, in fact, some instances, there are some sets of distances where this factor of log n shows up. And maybe you get a different constant here, because we can't always prove the optimal results. And the jargon way to say this is that the greedy heuristic is a theta log n.
00:20:00.060 - 00:20:54.784, Speaker A: This sort of means that this is both an upper and lower bound approximation algorithm for TSP. And the first thing you want to ask is, this is very bad, since when the number of cities grows, the approximation gets worse and worse. And the first question you ask is there an algorithm that gives you a ratio that's independent on the number of cities you're visiting. And this again was sort of an extremely natural question that was around Christophides already in 1976, said, well, let's see what kind of efficient algorithms we have. He says, well, finding a mean cost spanning tree is easy. Finding a mean cost matching is easy, and then, in fact, that's all we're going to do. So if this sketches a little bit too fast, let me do it in pictures.
00:20:54.784 - 00:21:34.514, Speaker A: This is a graph. So what's a graph? Well, these are sort of cities, and these edges are sort of rows between the cities. They should really have distances, but I didn't put the distances there not to clutter the picture. So the first thing is to find a spanning tree. So a spanning tree is just a set of roads that in fact connects the city somehow. It's not a tour, but it sort of connects them a little bit. And it's very efficient to find the cheapest spanning tree, the cheapest set, to connect them so that you can go between all the cities.
00:21:34.514 - 00:22:13.734, Speaker A: This sort of has a big problem that you need to go back and forth between these two things. So what you do is you take the guys of degrees one, and in particular the odd ones, and you add a matching to these. So you need to make it a little bit more connected. So you take the guys of odd degree and add a matching of these, and the union of the. So the cost of spanning. So this cost of the spanning tree is never more than the cost of the tour. The cost of the matching turns out to be at most half the cost of the tour.
00:22:13.734 - 00:22:59.528, Speaker A: And then to make it a real tour, you just need to make shortcuts that this here really looks like a tour. And the problem is you're visiting this too many times. So the second time you want to visit, you just go directly to where you're going, and that sort of creates a nice tour. So the second homework problem I want to give to you guys is that, you know, we are having a TSP, and I want you a polynomial time algorithm that finds a tour that's always a little bit better than 1.5, say 1.49. Okay? This is an easy homework problem and we have been working on it for 37 years. Okay? So when I first read, when I was in grad school, I got this 1.5
00:22:59.528 - 00:23:28.464, Speaker A: algorithm and said, well, we should soon be able to improve this. But nobody has been able to improve it. And people have been working more and more aggressively on it. It's really an easy to state problem that we don't know the answer. And when you go home and work on it, I just want to say that you want something that beats a factor 1.5 on every input. You know, getting on something on some nice inputs, that's sort of not too hard.
00:23:28.464 - 00:23:58.724, Speaker A: You also want to make sure that, you know, the distances are completely arbitrary. You shouldn't. For instance, if you work with, you know, flying distance, the distance between points in the plane, this is in fact easy to get. Well, not easy, but it's possible to get a factor better than 1.5. And it's been known for 15 years now. We really want the factor 1.49. And the only thing you can use about your distance function is that it's based the triangle inequality.
00:23:58.724 - 00:24:35.124, Speaker A: And this is an extremely intuitive inequality. It says that going from a to c, it's not worse to go directly than to go via a city b. That's all you may use. Is the problem clear to you? You can deliver it next, Simon's lecture. And then you can give it to me, and then I can be a co author. Okay, but where we're going, you can't solve this homeward problem. It's maybe 1.5
00:24:35.124 - 00:25:13.486, Speaker A: is the best possible constant. And we note here that if it's going to be the best possible constant, then in fact, we have to assume that MP is not equal to P. Because if MP is equal to P, then in fact we can solve it optimally. But this is a law of nature. So I'll state this every now and then to remind you that we have this law of nature and. But it turns out is that, well, we can't do, but we can do something. And it turns out that we can prove that you cannot get very close to optimal.
00:25:13.486 - 00:25:52.234, Speaker A: At least you have to be off by almost a percent unless NP is equal to P. This constant has been going up slowly and slowly, and this is a result by Karpinski, Lampis and I forget who messes at this point, but from this year. So this is a slow improvement. But there's a substantial gap for the moment. That's all I wanted to say about traveling salesman problem. I'll return to it in the end. Anybody wants to ask something about TSP? Yes.
00:25:57.694 - 00:26:01.270, Speaker C: And then also, what about the problems that are even worse than NP?
00:26:01.382 - 00:26:16.314, Speaker A: Yes. Yeah, no, I'm making a huge oversimplification. There are lots of complexity classes, right? Some between P and NP. And above NP, there's always a tremendous number of complexity classes. So, yes, there are lots of classes.
00:26:16.414 - 00:26:23.214, Speaker C: So is NP the interesting place to study? Or is there something in between that?
00:26:24.194 - 00:26:44.292, Speaker A: No. Well, this is, of course, a matter of personal taste. Right. Which problems are interesting, but I think NP is really the basic. I mean, it captures all these planning problems and optimization problems that show up in practice. So it's my favorite class and some of their more difficult problems. But we can't.
00:26:44.292 - 00:26:58.784, Speaker A: We can't. We have to get to extremely, extremely difficult problems before we can actually prove that they're difficult. There's a complexity class, p space, which is above, which is bigger than NP. And also those we can't prove is hard. But if you get to. Yes.
00:26:59.124 - 00:27:02.344, Speaker B: What about between p and Np? Like, n to the fifth.
00:27:06.084 - 00:27:09.104, Speaker A: Precisely. Time. N to the fifth, for example.
00:27:09.144 - 00:27:10.040, Speaker B: I mean, that's absurd.
00:27:10.072 - 00:27:12.724, Speaker A: But. No, it's not absurd. It's a good time.
00:27:13.624 - 00:27:17.512, Speaker B: Well, the reason I ask is because I do more practical stuff.
00:27:17.568 - 00:27:18.000, Speaker A: Yes.
00:27:18.112 - 00:27:21.928, Speaker B: N to the 100 is no better than two to the n practice.
00:27:22.056 - 00:27:34.940, Speaker A: Yes, I have to agree with this. No, I completely agree. We just wait for Simons to donate money to the practical, prove that you.
00:27:34.972 - 00:27:38.836, Speaker B: Can'T do factoring better than the end of the hundred. That doesn't break secure.
00:27:38.900 - 00:27:50.836, Speaker A: No, of course. Yeah. No. So, being more serious, I mean, I love this question about multiplication. Right? So we can't even prove that multiplication can't be done in linear time. And proving that anything. Yeah.
00:27:50.836 - 00:28:09.640, Speaker A: Distinguishing n to the fifth, you know, that would be great. I mean, proving that. So. Yeah, but we can't answer even these very coarse questions. Right. And once we've answered those, I would be extremely happy to see a proof that MP is not, you know, not contained in n squared time.
00:28:09.792 - 00:28:23.192, Speaker B: So it doesn't raise a question about the nature of the reduction. So I guess the question is how, sort of, how much of the efficiency do these reductions? Is there a classification of better reduction versus worse, for example?
00:28:23.248 - 00:29:05.504, Speaker A: Yes, there is, but, yeah, so this, what Mike is referring to is that we have all these, you know, np complete problems that are similar completely. The question is, well, there's a big difference between two to the n and two to the square root n, and this has been studied. And that would be, you know, I'm giving you an extremely simple black and white picture, which should be refined in many ways. I agree. Yes, you took the memor, but I'm happy for questions. Yes, I'm sorry I stopped this. So now I want to go to another favorite people question for complexity theorists, and that's formulas over boolean variables.
00:29:05.504 - 00:29:39.688, Speaker A: And boolean variables I'll denote by x with a little index down, which is called X sub I. And this is Sort of, this is, and the borrower is the negation that Sort of SAys you should treat it the other way. Turn true into false and false into true. And this little thing is a logical and, and this thing is a logical or. So, just to get this thing straight, we have two drivers. Alice is driving left, going west, sorry, on the left hand side going west. That's what x one says.
00:29:39.688 - 00:30:17.966, Speaker A: So if x one is true, this is true, and if x one is false, she's driving on the right hand side. Well, I'm cheating you slightly there, but this sort of means that she's on the left hand side. And the complement would be she's on the right hand side. And the same is for Bob and he's going the other direction. And the fact they're not colliding is Sort of saying that they're both on the left or they're both on the right. And both on the left says that X one and X two is true. And that they're both on the right means that it's both true that x one is false and X two is false.
00:30:17.966 - 00:30:58.744, Speaker A: And that sort of says that there's no collision. Anybody who's not a computer scientist that's happy with this, that's unfortunate. If you're unhappy with this and want to understand it, please ask a question. This is sort of a Boolean variable that says something is true or false. And this says that X one and X two is true, which says that Alice drives on the left going east, Bob drives on the left going, going west, and Bob is going east. So this says that they're both driving on the left, they're in the UK and they're not colliding. Right here it says that they're both driving on the right, they're in the US and they're not colliding.
00:30:58.744 - 00:31:52.424, Speaker A: So this is that they're both in the UK. And then you can sort of play around with these logical formulas and you can sort of turns out that this is logically expressing exactly the same thing. As you negate one of the variables, you move the words around and you put an and between them and you have the and of two little ors of size two. And now you immediately can go to bigger formulas and say, you know, I now have five different variables. I have the ands of lots of little small things, which is each the or of three variables. And the question is, can you satisfy all these constraints at the same time? What do you think, Mike. I mean, you should first encourage questions, and then you're vicious to the people that ask questions.
00:31:52.424 - 00:32:19.792, Speaker A: That's the way to proceed in class. Yeah, yeah, that's the point. So you stay at these things for a while and then turn up. Yeah. In fact, you can satisfy all the constraints. Here's the five variables, and you can check that this out actually works out. Okay, so, now we're interested in these kind of formulas, and this is called the SAT problem.
00:32:19.792 - 00:33:08.284, Speaker A: Just to do some notation, the formula just had this on something called conjunctive normal form, and it has a parameter k, which says that the number of guys in each ore. So, this is a three CNF formula that I had on the previous slide. And the satisfiability of formulas on a three, c and f formula is called three SAP. And we're interested in satisfying such formulas. And this was essentially the first problem proved to be NP complete. It's computationally difficult, given a formula like this, to try to see if there's a satisfying assignment. And also here, I want to relax it and say, let's instead try to satisfy as many constraints as possible.
00:33:08.284 - 00:33:59.548, Speaker A: So the game is now that I give you a formula like this, which you should think of as much larger, but pretty large. And for some reason, you might assume that there's some assignment that satisfy all these constraints, and you want to find some assignment that satisfies as many constraints as possible. Possible. Okay. And whenever you're going to see how well an algorithm works, here, we have a very nice algorithm, which is really thinking as little as possible. So don't even look at the formula. It's a big, complicated, ugly thing, right? Not too many of you probably read this formula, and you just set each variable randomly to true or false with equal probability.
00:33:59.548 - 00:34:37.037, Speaker A: You just flip a random coin and say, well, x one should be true, x two should be false, and so forth. And this has a pretty good property. Well, look at this constraint. It says that x one should be true, or x two should be true, or x three should be true. So this, you have three shots at making this true and sort of with probability seven h, you'll in fact make that constraint true, just flipping things randomly. And this gives, in fact, an approximation algorithm with the approximation ratio of seven eight. And the funny thing is that this is the best you can do.
00:34:37.037 - 00:35:25.724, Speaker A: This completely stupid algorithm is the best approximation algorithm you can get for this problem, which is called Max three sat. So, if I give you any number that's greater than seven eight, which is 0.875, like 0.876 unless NP is equal to p, you can't find such an assignment efficiently. Okay, so it really says that three SAT, which we felt original, was a bit complicated, is extremely complicated. We can't do anything that's a little bit efficient here. I happen to have a student that does a living in solving three set formulas, but that's what we're relating more to, that instances in the real world are small and computers are fast, but in theory, you can't do it.
00:35:25.724 - 00:36:06.054, Speaker A: So let's go for something simpler where we actually can do something. And suppose we still have boolean variables. And I just asked that the only type of constraint you can have, that some variable is not equal to some other variable. That's the only type of constraints I will allow. And let's at the same time introduce a different problem, which I call Max cut. And it's sort of, given that I give you a graph here we blow nodes and connections between the nodes. And the question is, divide this into two pieces so that whenever two things are connected, we want them to have different colors.
00:36:06.054 - 00:36:58.366, Speaker A: It's called Maxcut because it's cutting the graph into two pieces to cut all the edges. And here I made sort of a way to do this. I colored some fraction of them red in a smart way to try to cut as many edges as possible. So let's see if I can get what's, if I had this problem and I had this problem on the previous slide. Do you see any connection between them for a non theoretical computer scientist here? Does any theoretical computer scientist see a connection between the two problems? No, it's the same problem. Right. An edge is just a thing.
00:36:58.366 - 00:37:26.810, Speaker A: That thing should be different, and they're the different. Blue is sort of corresponding to true, and red is false. And it's really the same problem. Sure. I give you boolean variables, and I should ask them not to be equal. One should be true, the other one should be false.
00:37:26.982 - 00:37:29.914, Speaker B: So you want a solution that satisfies all the constraints.
00:37:29.994 - 00:37:45.874, Speaker A: Yeah, as many as you can. If you can't get all, you should get. Yeah, but Max cut. Yeah. Optimization problem and decision problem. But it's really. Right.
00:37:45.874 - 00:38:43.448, Speaker A: You want the color, red is false, blue is true. You connect two things. Okay, so this is easy, because if you start saying that this should be blue, that in fact, all its neighbors should be red. So if you want to satisfy, if you want to cut all the edges, it's in fact extremely simple to cut all the edges. But it gets very tricky if you just want to cut 99% of the edges. And the question can we find such a cut? Can we find something that's almost perfect so that almost all the edges have endpoints of different color? And what I wanted to describe next is an efficient algorithm for doing this, that in fact, this problem is so simple that we can do it. It took us 20 years to come up with the right idea, and this was done by Germans and Williamson.
00:38:43.448 - 00:40:03.970, Speaker A: And the hope is to convince you of this right idea, that in fact, if you think, if you change true and false for plus minus one and e is the set of edges, which is just a set of constraints, we're trying to optimize this mathematical sum. So German Williamson's great idea was that, well, we should make this simpler. We should take this product and introduce a new variable instead, which makes this a lot easier to maximize, since we don't have any constraints. But the key constraint here is to prove that to want the y variables to form a positive semifinal matrix with ones on the diagonal, which is sort of not such a, this gets a little bit technical, but I'm just trying to give you a flavor of these things. So what's the positive semi definite matrix? Well, it's a matrix which has all eigen, if it's symmetric, which it's assumed to be that it's all positive eigenvalues, that in fact, as viewed as a quadratic form, is strictly positive, or it's non negative. And in fact, this can be factorized as a matrix, the transpose of a matrix on the original matrix. And this is in fact true.
00:40:03.970 - 00:41:08.334, Speaker A: The original solution is just a special case when x is the y is the product of a vector and its transpose. And it turns out that this relaxation you can in fact solve efficiently, and in fact, you can optimize any linear function of the coefficients in the matrix, giving any set of linear constraints. And the intuitive reason this is true is that the set of positive semi definite matrices is convex. And you're optimizing a linear function over a convex set, and it has no local optima. So some kind of steepest descent should do it if you studied steepest descent. But let's ignore this sketch if you're not familiar with these things. So, backing up, I mean, the alternative way to propose this is that you're trying to maximize something like this when the x's are plus minus ones, and instead you say you want to optimize this, where this is an inner product between two vectors of unit length.
00:41:08.334 - 00:42:21.304, Speaker A: So we're relaxing the optimization problems from the real line to n dimensional real space. And this enable us to be able to solve the problem. So the vector problem, in fact is easy to solve, and we get a higher objective value. And the key problem is now, how do we take a solution to the vector problem and make it a boolean solution? And what Thielmanson Williamson realized apart from we should do it like this, is that an extremely simple to round the vector solution to a plus minus one solution. And that's just to pick a random vector and let xi be the sign of the inner product. And the reason this works is that if any two vectors that give a good contribution to the relaxed problem, it's extremely likely that the sign of the two vectors are different. I try to indicate this by the vector.
00:42:21.304 - 00:43:25.032, Speaker A: The two vectors, and sort of the bad directions where these signs are different, are sort of this narrow cone here. And a random vector over here, for instance, will give a good Boolean solution that satisfies the corresponding constraint. And trying to put this intuition in mathematical terms, you just make a variable for the angle between the vectors. The contribution to the objective function is one minus cosine of this vector over two. The probability that the two things are cut is the vector over PI. And the minimal quotients of this gives the approximation algorithm for the gamma Som Williamson algorithm. So, since I don't do as much algorithms as hardness result, I'm really saying, you know, well, we have this great algorithm that was proposed in the mid nineties, is this best possible? And I tried to prove that it's not so far, and I got lower bound that you can't at least do 16 over 17.
00:43:25.032 - 00:44:16.904, Speaker A: That's. And then this has sort of been standing still for an extremely long time. But then, in 2004, these two set of authors studied something called the unique games conjecture, and proved that in fact, if that conjecture is true, then the grammars on William Constantin is optimal. And this is sort of at least a 50 50 proposition at this stage of the game. Well, m here is El Chanon Mosel, who introduced me, so I should be a little bit nice, at least to mention him by name. So, the thing I want to tell you about is the unique games conjecture is one of the few things that's sort of the third homework problem. And this is that I give you a log integer m.
00:44:16.904 - 00:45:00.908, Speaker A: And I say that now we should have variables that take values in zero to m minus one. And I give you very simple linear equations modem. So this sort of says that when you take x one and add eleven, you should get x three more than. If this is greater than m, you subtract M. So this is linear equations more than with two variables in each equation, which is a problem denoted by this. And the question now is that, suppose I promise you that you can not solve all of these, then it's sort of easy, but you can satisfy 99% of these equations. Or I'm giving you an extremely difficult instance of this, where you can only satisfied 1%.
00:45:00.908 - 00:45:53.894, Speaker A: Can you tell which one it is? And that's essentially the unique games conjecture. Okay, so, I give you an extremely simple set of equations for them, and sort of 99 is really any number smaller than 100, and this is greater than zero. And this is a computational problem that's really been with us for the last ten years, and it's more and more open as time goes by. Various algorithms have been proposed. None works for all instances. On the other hand, whenever people come up with an ensemble of instances, there's always an algorithm that seems to solve them. So it's sort of a war between these two fronts, and we need some new idea to be able to solve the unique games instances.
00:45:53.894 - 00:46:52.484, Speaker A: So, let me, as a final point, return to the traveling salesman problem. You know, as I told you before, you can approximate it between 1.5. You can't do something 1.01, essentially. And you question, are we happy? You know, is this really what we want to achieve? And some people said, well, yeah, it's within a constant, and who cares about this constant? And I sort of the other way around, say, you know, this is an exciting constant of nature, and we have to find out what it is. But even if you're not so much into detail, let's look at a closely related problem and say that we have the traveling salesman problem, where it might be more expensive to go from a to b than b to a. You know, you might be up in the hills, for instance, in Berkeley Hills, and it's much easier to go to work than to go home.
00:46:52.484 - 00:48:02.544, Speaker A: And then it turns out that the approximation algorithm across the fields doesn't work. And the state of the art there is that the best algorithm is almost like, greedy for the standard. One is of PI, essentially factor log n. The hardness results gets a little bit better, but not much. And the question which of these two is the correct answer? And my view is, well, historically, we've always been better at finding algorithms and analyzing them and improving hardness. But on the other hand, here, the situation is extremely picky, in the sense that we do have an algorithm proposed by Helden Karp and it's been around now for 50 years, and we don't know if it's this bad or it might give the answer within a factor of two. So usually the hard question is to try to find a good algorithm, but here the question is, you know, we have the algorithm, we just don't know how good it is.
00:48:02.544 - 00:48:55.502, Speaker A: We know it's fast, but we don't know the quality of the result. So what have I been trying to do, this lecture? Well, I've been trying to tell you about approximation problems, and for some of these problems we know the answer. For Lexmax three sat, the rock bot amalgam that gave seven eight is the best one, and we can prove that this is the case. I gave you some other problems like asymmetric TSP, standard TSP and Maxcut, and in fact we don't really understand what's going on there. If we assume this strange probability unique games conjecture, then Maxcut is resolved. But these two other, we still don't get any better results for them. So we can go home and work on all three of these and.
00:48:55.502 - 00:49:21.694, Speaker A: Well, that's what I wanted to say. Any more questions or comments? Yeah, well now I won't lecture anymore, so I can't, I promise not to ask any embarrassing counter questions. Yes, thank you.
00:49:23.014 - 00:49:33.074, Speaker C: If there's an unsolved problem, really difficult problem, how do I start about thinking whether I should find that two x plus three x and 1.5 x?
00:49:35.854 - 00:49:54.914, Speaker A: Well, if you're looking at a fairly natural problem, there's many people that have these basic problems that are sort of easy to state. Probably somebody already looked at the problem. Yeah, I mean you should. That's, well, yeah, you should maybe that's. Yeah, Google the problem. That's.
00:49:56.854 - 00:49:57.622, Speaker C: Okay.
00:49:57.758 - 00:50:21.474, Speaker A: So, for instance, I had to Google to find the best constant for the approximately of the TSP. It keeps improving all the time. So Google is actually a good, and Google, I'm proud to say, sort of came out a little bit of theory of computation. If we're sort of stretching things a little bit, not too much, but a little bit. Okay, thank you.
