00:00:00.400 - 00:00:06.102, Speaker A: All right, so next up we have Jean Bernard Lasser, who will tell us about prony lt, or super resolution for sparse interpolation.
00:00:06.238 - 00:00:48.394, Speaker B: Thank you very much. I want to thank the organizer for this invitation. And the talk will be about sparse interpolation and a relationship with super resolution and the proni method. And there will be also link with what Monique has presented before. So this joint work with Cedric Jot, which is now a postdoc at UC Berkeley, and Bernard Mourinh from in Rhea. And it's based on this archive paper here. The message of the talk is that if you study some questions, for some question that you study about polynomials, they could be interpreted as assigned measures on appropriate geometrical objects.
00:00:48.394 - 00:01:55.962, Speaker B: And for example, it's known that if you take the homogeneous, the convex cone of homogeneous polynomial of degree 2d, then the dual, which is understood as a measure linear functionals, is also the space of polynomial, which are the 2d power of linear forms. That's an example where you can interpret a polynomial as a measure. And in fact, for example, this has been used in recent work on recovery of a tensor. For example, there was nice paper by tangential and potassium stirrer. So for example, if you would like to recover the decomposition of a three tensor, or in this case, if it is symmetric, you can also do the same thing for homogeneous polynomial cubic, then you can interpret so this is your tensor a. So you can interpret so you want to find this decomposition. So a is given and you want to find this decomposition, right? And you can interpret this as an atomic measure, okay, on the unit sphere.
00:01:55.962 - 00:02:53.764, Speaker B: And the idea is each coefficient AI DJK should be viewed as a moment of a measure on the sphere. So the idea is you would like to recover from the knowledge of the moment, you would like to recover a measure on the sphere. And that's typically what super resolution is doing. Theory of super resolution by Candice. And then there is a nice theory, because depending on whether or not your atoms are sufficiently separated geometrically, then you can recover exactly your tensor or your measure by solving an SDP, for example, in the univariate case. So those papers are exactly in this period. And Monict can be viewed also as exactly this approach, where for your tensor or CPSD rank, you want to retrieve a measure on an abstract sphere from knowledge of some moments.
00:02:53.764 - 00:03:39.244, Speaker B: And to do that, you minimize the total variation norm of your unknown measure. So that's one way to see some polynomials related to real measures, a coefficient of these tensors, the coefficient should be viewed as moment of some measure. This talk is different. But again, we will use this manipulation of a polynomial as a measure. So here is for passing depression. So what is it? So you have a black box, this black box is a non polynomial, okay? You know it is a polynomial, but you don't know the coefficient of this polynomial. And the only thing you can do is just do evaluations, okay? And the problem is that how could you retrieve, especially if you know that p is sparse, has very few coefficients, how could you retrieve pull from knowledge only of a few evaluations?
00:03:39.364 - 00:03:40.184, Speaker C: Okay.
00:03:41.284 - 00:04:04.088, Speaker B: And in this case, you have a lot of freedom, because you can choose the points where you evaluate your polynomial. So you put an entry zk and then the output is the evaluation of p at zk. And from this knowledge you would like to retrieve p. And the message of the talk is that in this case, p can be considered the sign Borrell measure atomic on the n torus. We will see how it works.
00:04:04.176 - 00:04:04.760, Speaker C: Okay?
00:04:04.872 - 00:04:54.774, Speaker B: And again, the game would be to retrieve your measure from knowledge or some moments, we will see that these are moments of this measure. So if p is sparse polynomial, then the idea is that you need few evaluation to recover your black box polynomial, p unknown. So what is, let's say there's a well known method for doing this, which is called prony method, which is very old method, and it was usually designed for recovering an atomic measure. So suppose you have a function h of x, which is given by this. So you have some finitely many exponentials with weight wi. So the unknown are wi and fi, and you want to retrieve the wi and fi given some evaluation h of x at some point.
00:04:54.894 - 00:04:55.714, Speaker C: Okay.
00:04:56.874 - 00:05:07.614, Speaker B: And the idea of poly is very simple. You generate first polynomials. You put the e of Fi as roots of some univariate polynomial to define this polynomial.
00:05:08.274 - 00:05:09.134, Speaker C: Okay.
00:05:09.594 - 00:05:50.354, Speaker B: This polynomial has some unknown coefficient qj, because you don't know, of course, those guys. And the thing is that this polynomial, the vector of coefficient of this polynomial, satisfy a linear relation where the blue data are coming from the evaluation of h of x. So you evaluate h of x at some points to give you some anchor matrix. And the h of q satisfies this linear system. H of q equals this. So you evaluate your h at some points, you solve for this linear system that give you q, and then from the q, you then recover the wi. So this is a purely algebraic method to recover a univariate polynomial.
00:05:50.354 - 00:06:18.620, Speaker B: Also for this is done usually for a measure, but you can do it for sparse interpolation also and permits to recover the unknown degree r and polynomial from two r evaluations. Well, I passed this, but the way to do it, this was for measure, but you can do it for polynomial interpolation, for the problem of interpolation. So your polynomial p, now is unknown with coefficient PI.
00:06:18.692 - 00:06:19.424, Speaker C: Okay?
00:06:20.644 - 00:07:26.874, Speaker B: And then again the same thing. You fix x zero. So you look at this polynomial, now you fix x zero, you look at this polynomial with a non coefficient, and it will satisfy again a linear system of equations where the blue are coming for just some evaluations. So, evaluate at some points, you use this matrix h and this vector sigma, and then you solve for this in our system, it will give you the queue, and from the queue, then you retrieve also the, sorry, you retrieve the coefficient like this. And there is an extension to the multivariate case, which is done by many people, but in particular by and why people don't use this for, I mean, it's not a systematic method which is used in this case, because the claim is that this method is not very robust to noise. If your data, if your evaluations are noisy, then you cannot recover your polynomial very nicely. You will have some propagation of error, and proni is not considered to be a stable method.
00:07:26.874 - 00:08:13.442, Speaker B: So what would be another method to recover your polynomial would be what I call here as naive compress sensing. So your polynomial is given like this. So you don't know the coefficients, you assume you have a bound on the degree, and the naive Lp would be the following. So you evaluate your polynomial at some point arbitrary, they give you bi, that you know this bi, and then you say, okay, I look for this vector of coefficients of my polynomial. So this is evaluation at the point zi. Okay, the q are the unknown, and then you minimize the l one norm. So it would be a very naive way to do it, okay? Because it's known that if you minimize the l one norm, you tend to recover a sparse solution to your, in our equations.
00:08:13.442 - 00:09:15.636, Speaker B: Okay, and why is it naive? Because there is a well known theory of compressed sensing to recover an optimal solution from this lp. But to guarantee that you recover the solution that you are looking for, you need some assumption on the a to have a guarantee that you recover your q from this linear system of equation when you minimize the l one norm, and this is called the restricted isometric property. So the idea is very simple. So your q is a very large size vector, and by minimizing the l one norm, which is sparsity inducing, you aim at finding your solution of your problem will be sparse. Here. If you solve this Lp, you will have a sparse polynomial, but there is no guarantee that you recover your polynomial. And this l one norm is just a subsidiary for this pseudonym of counting the number of distinct elements.
00:09:15.636 - 00:09:55.954, Speaker B: Okay. And the compression interior said that you would recover exactly your polynomial. If your matrix a at this particular isometric property, it's a sufficient condition, it's not necessary, but to be sure that you recover your polynomial. If a satisfies this property, indeed, you recover your polynomial. But for sparse polynomial interpolation, it does not satisfy in our setting. That's why I call it naively, because in this case, the matrix a. For if you try to recover your polynomial by solving this, your matrix a does not satisfy the restricted isometric property.
00:09:55.954 - 00:09:59.914, Speaker B: So you can still try. It works sometimes, but you have no guarantee.
00:10:00.734 - 00:10:01.594, Speaker C: Okay.
00:10:04.094 - 00:10:12.638, Speaker B: So what is super resolution? That's another method to do it. So consider for them enterers in the complex Cn.
00:10:12.766 - 00:10:13.606, Speaker C: Okay.
00:10:13.790 - 00:10:39.886, Speaker B: And suppose you are given a measure. So this is the basic idea behind super resolution. Suppose you have an atomic measure, so you have atoms decay on the torus and weights not necessarily positive. It could be a sign measure. So you have a measure unknown. You know, it is atomic and usually sparse with a few atoms, and you know only some moments of this measure. So, for example, you know the m alpha.
00:10:40.030 - 00:10:40.750, Speaker C: Okay.
00:10:40.902 - 00:10:46.694, Speaker B: And the goal again is to recover your measurement from knowledge of a few m alpha.
00:10:47.314 - 00:10:48.174, Speaker C: Okay?
00:10:49.514 - 00:11:54.774, Speaker B: And the idea, can you recover from a few scalar m alpha, can you recover the theta k and the g k? Super resolution theory asserts that indeed, mu star is a unique optimal solution of this infinite dimensional Lp. So you try now, you optimize an Lp over a set of measures on the torus. Your constraints say that you look for a measure mu that satisfy the moment constraints, okay? And you minimize the total variations. Total variation should be seen as the l one norm in the case of a vector, okay. And super resolution theory says the following. If you have enough measurements, moments here, and if the points, the atoms of your measure are sufficiently separated on the torus, then you can. First of all, this infinity dimensional Lp has a unique solution, which is exactly the measure you are looking for.
00:11:54.774 - 00:12:22.594, Speaker B: And in the univariate case, you can recover your measure exactly by solving a single SDP. And this is a famous paper by Candice and Fernandez Grande, okay? And there was some extension to the multivariate case. So what I claim is that this tensor recovery problem we have seen at the beginning, remember? Oh, sorry, I cannot retrieve.
00:12:26.844 - 00:12:29.824, Speaker A: I'll give this to the french speaking, the french speaking people.
00:12:32.404 - 00:12:33.344, Speaker C: Thank you.
00:12:34.404 - 00:13:13.288, Speaker B: How did you come back? Could you back for the slides? Yeah, okay, thank you very much. Remember, our tensor recovery program at the beginning was exactly super resolution. You are looking for a measure mu, not on the torus, but on the sphere. In this case, given some coefficient of your tensor, but the coefficient of your tensor should be viewed as moments of this measure that you look for. So, given the moments of your measure on the sphere, you try to recover your tensor and you minimize the error norm.
00:13:13.456 - 00:13:14.284, Speaker C: Okay?
00:13:14.744 - 00:14:00.864, Speaker B: And so there was this idea behind super resolution. Okay? So we have seen proni methods, the naive compressed sensing lp approach to recover your sparse polynomial, and the super resolution. There are three methods. Now we go back to the sparse interpolation, because we have seen this for just recovering a measure. And the central idea is very simple here. Just one slide is to interpret a polynomial p as a sign measure. How you do that? So you fix a point z zero on the n torus, fix z zero once forward and let p be your unknown polynomial.
00:14:00.864 - 00:14:57.754, Speaker B: And then suppose you evaluate p at the power beta of zero. What is it? So it's exactly equal to sigma of p alpha z zero beta to the power alpha, right, by definition of p. But you can, the idea that you can interplay, you can put the alpha instead of the beta and vice versa. So it's equal to p alpha, zero alpha to the beta. But then you can see, if you see there's a dirac at the point z zero alpha. This is just evaluation of the moment beta of this dirac, okay? So when you evaluate, in other words, when you evaluate the polynomial p at the power beta of this point z zero, then you are just evaluating the moment beta of this atomic measure, okay? And this atomic measure is what the coefficient is. The weight is just the coefficient alpha of your polynomial.
00:14:57.754 - 00:15:13.744, Speaker B: And the atom, okay, is just the Dirac at the point z zero to the power alpha, okay? Okay, the message is very simple. Evaluating p at zero beta is just computing the beta moment of this measure.
00:15:14.484 - 00:15:15.344, Speaker C: Okay?
00:15:15.964 - 00:16:34.194, Speaker B: So we're back to super resolution, because given an arbitrary fix zero on the torus, you fix this zero. Then every polynomial p can be viewed as a sign borrell measure on the torus, okay, with finite support, which are just the atoms zero to the power alpha, or the coefficient are different from zero and weights p alpha. And the evaluation of p at the point z zero beta is just a moment z of this measure. Hence, if you want to recover a sparse perm p from few evaluation pxk, this is exactly equivalent to recovering a measure mu with sparse support on the torus from few moments, okay? So you have your moment beta, which is the evaluation at the power beta of z zero, and you want to retrieve your measure on the n torus from the knowledge of these moments. But this is exactly what super resolution is doing, right? So now you just import the tools from super resolution to say that under some condition, there is a nice way to recover your unknown polynomial. It will be the unique solution of a certain problem. So your sparse polynomial would be the unique solution of this infinite dimensional LP on the space of measures.
00:16:34.194 - 00:17:00.814, Speaker B: So you look for a measure mu on the torus, you have some evaluations with view as moments of this measure, and you minimize the total variation. So, and the message is that if the atoms are sufficiently separated on the torus and you have enough measurements, I mean enough evaluation, then you, your polynomial is exactly the unique solution of this problem.
00:17:01.714 - 00:17:02.574, Speaker C: Okay.
00:17:04.154 - 00:17:46.038, Speaker B: For example, if you will be a polynomial in one variable, you can recover your polynomial in just solving a single SDP. So how do you call, how do you solve this infinite dimension on LP? So you have a measure mu, which is a sign measure. So you see, okay, I consider this measure to be the difference of difference of two positive measurements, five plus and five minus. So you have the moment condition here, and the total variation is just the mass of the two measure of five plus and five minus when they are de joints. And you solve this SDP by using the moment approach, right. Then you solve this SDP relaxation by considering a finite. You truncate your.
00:17:46.038 - 00:18:02.944, Speaker B: So you have the fixed moment up to order d, but you go to higher moment, which are unknown. And then this gives you a toplitz matrix, a moment, the moment matrix of this measure, unknown measure phi plus and phi minus are just two matrix, which are supposed to be positive, semi definite. And you solve this SDP.
00:18:04.604 - 00:18:05.504, Speaker C: Okay.
00:18:06.884 - 00:19:04.544, Speaker B: And if dimension range one and two, you are guaranteed to find the exact solution, infinitely many steps, and in general only symptomatically. And in fact, in practice, convergence is finite. An extraction procedure can be done. Okay, now there is an additional thing, which is relatively interesting in the fact that for the interpolation problem, it's a bit different from super resolution in the sense that it's even more. It's a special case of super resolution in the size that you have the choice of, you have the choice of your initial point, z zero on the torus. And for example, you can choose this point, you take n large enough, and if you take n large enough, you assume that your atoms are just on the grid, on your torus, on a fixed grid. In this case, when you do that if with n sufficiently large, you know that your atoms will be on this grid.
00:19:04.544 - 00:20:31.134, Speaker B: And this is a particular case of super resolution, which is called discrete super resolution, okay? And then there is a way of, in this case, you have an exact analog of this super resolution problem as an LP with total variation norm, which is the compressencing approach. So in this case, if you fix the zero, like I said, then your super resolution problem becomes an LP, okay? I mean, you can solve it as an LP with minimizing the l one norm. And since it is completely equivalent to the super resolution, you have the guarantee by Candace theorem that your polymer p will be the unique solution. The polynomial p that you look for is a unique solution of this Lp, even if the matrix a does not satisfy the restricted isometric property in this case. So if you have enough measurements, okay, this LP, which is not a naive one, because you don't do the evaluation arbitrarily, you do it on the torus by doing this, this is exactly equivalent to super resolution. And the, in this case, there is a unique solution to your problem, and this is also the unique solution of this LP. Okay, so this provides a nice example where you have an exact recovery, which is possible by solving an LP, even if the matrix a does not satisfy the restricted isometric property.
00:20:31.134 - 00:21:30.418, Speaker B: Just an example to show you how it works. First of all, in the unified case, to start with. So suppose you have a polynomial, for example, of one variable of degree 80 with three atoms, 80, 75 and 20 with only three monomials. So you fix, for example, there are two upper either you fix this zero to be this one, and like I said in the beginning, and you can solve also an LP. You want to find a measure on the torus with three atoms out of potentially 101, for example. And in fact, you will recover your result with four evaluations only and with only two pitch matrices of size four by four. So, for example, on the torus, your three atoms are, the blue are the positive one, you have two positive weights and one negative weight.
00:21:30.418 - 00:21:34.162, Speaker B: So the blue are the positive weights and the red are the negative one.
00:21:34.338 - 00:21:35.174, Speaker C: Okay?
00:21:35.994 - 00:22:11.086, Speaker B: And then this is your atomic measure on the torus, on the dual, which is, I didn't talk about the dual, but the dual is just looking for a polynomial which is between minus one and one. So it is bounded between minus one and one. And you will touch one at the blue points and minus one at the red point. This is a dual of the ACP. Okay, this is when I fix zero like this. But you can also fix zero to be irrational on the torus. And then this case, it's much more interesting in the sense, because, for example, with this choice, the points are much more separated.
00:22:11.086 - 00:23:52.148, Speaker B: And numerically it's even easier because then you have, the support is more separated, right? And then, for example, we have tried that on several examples of, for example, here we have three variables, ten variables, okay? And we have compared the three methods, either with a rigorous LP, okay, and super resolution. So you see, we know that we find the exact solution in this case, and this is a number of measurements for which you recover your exact, the number of evaluation needed to recover your exact polynomial by the LP, or super resolution, it's very comparable, or super resolution. This is with exact, no noise in the data, okay? And if you have, if you corrupt now your data by noise, with some noise, even relatively, with high value, for example, here, the noise was uniformly distributed in this interval. So it's quite a large error in your evaluation. And then you, so you find, you solve your rigorous LP or your super resolution, or Proni, you get something and you look at the error you commit, you have with the original polynomial that you had in your black box, and you see that indeed, proni is not that sensitive to error, okay? And it's quite comparable to super resolution. And so, for example, in the paper by Candice, it is claimed that algebraic methods like Proni are not robust to noise. And that's what we didn't find for those examples.
00:23:52.148 - 00:23:54.264, Speaker B: This was, Proni was quite robust.
00:23:57.804 - 00:24:04.344, Speaker D: The problem with Proni is that you have to know the degree. You have to know the, I mean, I don't know exactly.
00:24:05.124 - 00:24:32.468, Speaker B: Not necessarily, because you make your matrix in a sort of ankle matrix that grows, or you can have a bound on the degree also, if you want, with super resolution, you don't need a bound. The SDP, the super resolution version, you don't need to know the degree for the LP, you need to know the degree or bound on the degree. And for prony, I think you can propagate your ankle matrix. You don't know the size in advance, and you look just for a kernel of some ankle matrix.
00:24:32.596 - 00:24:35.452, Speaker D: Yes, except for that when you add noise, it's hard to detect.
00:24:35.628 - 00:24:36.380, Speaker B: That's right.
00:24:36.492 - 00:24:38.100, Speaker D: That's the reason why it's less robust.
00:24:38.172 - 00:25:05.544, Speaker B: What we observe is that, in fact, with the noise, because we have some cholesky decomposition to do at some moment. And what we observe in our experiment is that probably what people do is they do a very small threshold on the eigenvalues to declare the eigenvalue is zero. But if you let this threshold to be quite large, we find almost exactly the same robustness as the super resolution, which is a bit surprising.
00:25:06.484 - 00:25:25.844, Speaker D: In fact, at super resolution, you're also going to need at some point to be able to determine a rank, because you're going to optimize some matrices and then those you have to extract, you have to basically do a prony method on the output. Exactly. So in a way, the super, super resolution kind of makes it more regular.
00:25:29.344 - 00:26:05.786, Speaker B: Yeah. So in fact, well, I almost finished my talk, and so the, in fact, you can see super resolution at the end. What is super resolution is doing? You solve an SDP and when some rank condition is satisfied, you stop, but you still do not have your polynomial. You still have to extract your polynomial from your solution of your SDP. And the last step, this step of extraction is exactly a prony method, actually. So you have your moment matrix of the two measure, five plus and five minus, and from this you extract your unknown polynomial. But this extraction procedure is exactly a prony method, actually.
00:26:05.786 - 00:26:19.260, Speaker B: So you could see super resolution as a sort of a filter. You first do an optimization step and then you do a sort of pronimator to extract, whereas prony method is just doing extraction directly without doing any optimization.
00:26:19.412 - 00:26:21.748, Speaker D: You can also use the dual solution and just doctor.
00:26:21.756 - 00:27:14.964, Speaker B: Yeah, but then they see solving roots of polynomials. Yeah, in one variable now, but. So extraction of atom is exactly a prony method, actually. And you could see that the optimization step in your semi definite programming problem is acting as a filter to, to attenuate the effect of noise, actually. And erigorose LP compressed sensing behaves quite well, even with noise, it was quite satisfactory. But the setup to set up your LP is much more time consuming. So this talk was about to say that for some problems concerning polynomials, if you have some special glasses, you can see that as a measure.
00:27:14.964 - 00:27:57.888, Speaker B: And you have an inverse problem of recovering a measure from some data. For example, in the tensor problem is that your data are the coefficient of your tensor, and those coefficients are seen as moments of some measure on the sphere. And from the knowledge of this coefficient, you want to recover your measure on the sphere, your atoms on the sphere, and that gives you the decomposition for sparse interpolation. It's basically almost the same story. Now, your coefficient are not known, your polynomial is unknown. Okay, but you want to recover it. You say that it is a sign measure on the torus, and you have access to some evaluation and the evaluations.
00:27:57.888 - 00:28:29.324, Speaker B: So you evaluate your polynomial at some points on the torus that give you some data. And this data has exactly the moments of an unknown measure on the torus. And again, the game is given some moments on the torus or some measure on the torus, given some moments of a measure on the torus. Sorry. You want to recover the measure from knowledge of this moment, and then you minimize, in both cases, you minimize the total variation of your measure, either on the sphere for the tensor or on the torus for this sparse interpolation. That was the message of the at the top.
00:28:38.424 - 00:28:39.164, Speaker A: Question.
00:28:44.424 - 00:28:54.524, Speaker B: So in the case of the rigorous LP, you need to set up the LP with the dimension which is n. Yeah. You have a bound, you need a bound on the degree of your polynomial.
00:28:56.024 - 00:28:56.432, Speaker C: Yes.
00:28:56.488 - 00:29:00.724, Speaker B: That's why SDP doesn't need that. That's the difference between the two.
00:29:03.824 - 00:29:08.528, Speaker A: When you're doing sparse, do you also know the sparsity or do you just know the sparsity in advance?
00:29:08.576 - 00:29:21.634, Speaker B: No, no, no. Not at all. No. The fact, the sparsity appears in the fact that you will need few evaluation to recover it. If your polynomial will not be sparse, you will need a lot of measurements, moments or evaluations to recover it.
00:29:21.714 - 00:29:23.094, Speaker A: Do you know some bounds?
00:29:23.394 - 00:29:34.954, Speaker B: Well, for a sparse interpolation, you know that you have exactly, if you know a bound on degree, you know a bound on the evaluations, a number of evaluations. So that would be enormous.
00:29:35.034 - 00:29:42.026, Speaker A: Yeah. But, so then in practice, how do you discover you still do this sort of eigenvalue?
00:29:42.210 - 00:29:52.904, Speaker B: Well, you do this SSDP. The problem is that if you, if you don't know exactly, if you don't have information on your sparsity, you end up with something at the end.
00:29:52.984 - 00:29:53.776, Speaker C: Okay.
00:29:53.960 - 00:30:13.104, Speaker B: But the guarantee, the theory of super resolution guarantees you that it is your real object that you are looking for, provided you have some information that you don't have. So you have something and you would like to, in fact, in practice, you would have to check if it is a good one. But that's the only way to do, if you don't want to do all the evaluations.
