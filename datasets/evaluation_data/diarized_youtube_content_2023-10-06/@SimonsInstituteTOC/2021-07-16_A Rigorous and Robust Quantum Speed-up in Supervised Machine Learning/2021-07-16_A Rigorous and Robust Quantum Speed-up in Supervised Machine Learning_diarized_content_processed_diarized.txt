00:00:02.000 - 00:00:49.934, Speaker A: Okay, so, is the live stream going smoothly? Okay. Hi, everyone. It's my great pleasure to present my recent work on a rigorous and robust quantum speedup in supervised machine learning. This is joint work with my collaborators at IBM, Srinivasa, and Kristen. Here's the archive and journal reference. Okay, so, one of the most significant recent advances in the field of quantum computing is the exciting developments in NISQ experiments. For example, until today, we have seen a total of four quantum supremacy experiments.
00:00:49.934 - 00:01:55.504, Speaker A: And following the demonstration of quantum supremacy, the natural thing to do next is proof of principle demonstration of quantum algorithms that can be implemented on current devices and which we hope could potentially achieve a quantum advantage in the future. So this is indeed what many experimental groups have been doing. So in this talk, we will focus on one particular quantum machine learning algorithm called quantum kernel methods, which is implemented in this experiment and many others. And we will show theoretical evidence of quantum advantage using quantum kernel methods. So let's start with the high level overview of quantum machine learning algorithms here. By quantum machine learning, I mean that we want to use quantum computers to speed up classical machine learning problems. And there are many other schemes to think about quantum machine learning, such as in robots talk, where the data could be generated quantumly.
00:01:55.504 - 00:03:07.064, Speaker A: So, the first class of quantum machine learning algorithm is what we call QRAM based algorithms. Here, the basic model is that we use amplitude encoding, where n dimensional vector, which is the classical data, is assumed to be encoded using only log n qubits, and therefore these algorithms can achieve a polylon running time, which is very fast. On the other hand, there are also several limitations with QrAM based algorithms. Here, we can understand the model as following. So we have some classical input data and we store them in QrAM, and then use the quantum machine learning algorithm to access data from the QRAM and generate some classical outputs. However, we know that QRAM is very hard to implement in practice. And furthermore, the second limitation is that the exponential speedup of these algorithms disappears when we think about the algorithm as an end to end procedure.
00:03:07.064 - 00:04:15.574, Speaker A: In particular, if we think about the algorithm not as the blue box but as the red box, then the exponential speedup disappears because it is unknown if we can efficiently load data into q ram. In other words, the first step becomes the bottleneck of the entire procedure. So Aronson has a nice viewpoint article on this issue. The third limitation is the existence of de quantization arguments developed by even Tang. The argument is that in order to have a fair comparison between classical and quantum machine learning algorithms, since we are allowing quantum machine learning algorithms to use QRAM. We should also allow classical algorithms to use QRAM as well, which basically means that we are giving classical algorithms additional sampling access to data. And in this case, it was shown that for some problems, classical machine learning algorithms can also achieve a polylog and running time which de quantizes the exponential quantum speedup.
00:04:15.574 - 00:05:36.104, Speaker A: Given these three limitations, people have designed a different family of quantum machine learning algorithms that directly work on classical data. So this is what we call heuristic quantum machine learning algorithms, including quantum neural networks, quantum generating models, kernel methods, etcetera. So these algorithms has the unique advantage of being implementable on near term hardware. However, the main problem here is that there has been little formal evidence that shows that these heuristic algorithms can have a quantum advantage. So this is the challenge that we will address in this talk. In particular, one central question we can ask here is that can heuristic quantum kernel methods solve any machine learning problem that is hard classically? And our result shows that the answer is yes when this algorithm is implemented fault tolerantly. So, here is a brief summary of the results.
00:05:36.104 - 00:06:44.744, Speaker A: So, quantum kernel methods is a family of supervised classification algorithms developed by IBM and another group. So here is the two sentence summary of this algorithm. First, classical data vectors are mapped nonlinearly to a quantum state via a quantum feature map. Second, a linear classifier in high dimensional Hilbert space can be efficiently obtained by the kernel method. So our result is the following. We show that this algorithm can probably solve a classification problem, and this problem is hard for all classical algorithms. So this result can be understood as evidence of end to end quantum speedup with this algorithm here, end to end means that we just have some classical input and generally some classical output, and our algorithm needs to take care of everything in between.
00:06:44.744 - 00:07:44.224, Speaker A: Okay, so next, I will give you more details about the algorithm. Let's start with a recap of classical support vector machines and kernel methods. So, on the left hand side you can see a set of training data which is labeled by, as indicated by the color. And the support vector machine algorithm is just the convex optimization problem on the right hand side. So here is one way to understand this convex optimization. A very important quantity is what we call the margin, which for any data point essentially means the distance to a hyperplane in this picture. So it is very easy to see that this convex optimization is actually trying to maximize the minimum margin of the training set to the hyperplane.
00:07:44.224 - 00:09:12.904, Speaker A: So we can think about this algorithm as trying to put a very thick board in between the training set. So here is a slightly more complicated situation where we could have some outliers and the original convex optimization becomes infeasible. So here the trick is to introduce some additional penalty terms as denoted by the PSI variables, and this convex optimization can be understood as optimizing the margin in some other higher dimensional space. Ok, so this is still fine, but here it seems we are in trouble because this picture is clearly not separable by any straight line. So however, it's very clear that if we could map this two dimensional data set to a three dimensional sphere, then the data becomes easily separable by a two dimensional plane. So the way that people tackle machine learning problem in practice is to come up with a high dimensional feature map that maps low dimensional data to a high dimensional feature space and hope that it reduces to a situation like this. And if we have this one, we can use the convex optimization to find a good separating hyperplane.
00:09:12.904 - 00:10:31.134, Speaker A: But here is a new problem, because the dimension of this feature space could be very high, then our convex optimization could potentially become inefficient due to the dimensionality. So the trick here is to use duality. So if we look at this convex optimization and notice that here I already replaced the data vectors x with the feature vectors phi x, so it could live in a very high dimensional space. And if we look at the dual program of this convex optimization, notice that the feature vectors phi x will only appear in this inner product, which we call the kernel function. So this gives us a way to do training and testing in support vector machines with a high dimensional feature map, because the key point here is that we can replace all of the high dimensional vectors with the kernel matrix k, and we can do the same thing in testing. That is, after training. If we receive a new data example, we can compute the kernel and evaluate the classifier.
00:10:31.134 - 00:11:20.770, Speaker A: Okay, so this is known as the kernel method in machine learning, where we do not specify feature map explicitly. Instead we define this efficiently computable kernel function. And this is a very popular trick in machine learning. And here are some examples of the classical kernels widely used in practice. And of course they should be nonlinear because otherwise it is not doing anything non trivial. Okay, so what about quantum kernel methods? Well, here the only difference is that we are replacing a classical kernel with a quantum kernel. Here, by quantum kernel we mean that we are encoding data into a quantum state by a quantum feature map.
00:11:20.770 - 00:12:26.664, Speaker A: So we map this classical data x into this density matrix, which is parameterized by some unitary circuit. And this is a way to represent a nonlinear function on a quantum computer. So the reason we use density matrix. Here is that the inner product between two density matrices, which is the Hilbert Schmidt inner product, is a physically measurable quantity. Okay, so now we can see that the way to implement this quantum kernel on a quantum computer is very simple. So we just have the input zero and run this circuit uxi and then run this circuit uxj, dagger and measure, and we count the frequency of seeing the all zero output. So here we are calling this procedure quantum kernel estimation, which is highlighting the fact that this procedure is subject to a small additive error due to finite sampling because we can only take a finite number of measurement shots.
00:12:26.664 - 00:13:41.674, Speaker A: So this is why we call it quantum kernel estimation. Okay, so now let's take a look at the difference between classical svms and quantum kernel estimation. So for classical svms, we have a classical training set and we compute the kernel function for each pair of training data, run the convex optimization, obtain classifier, and when testing, we compute the kernel function again. And for quantum kernel estimation, it is actually exactly the same, except that we are replacing compute with estimate. And the only step in the entire learning procedure that requires a quantum computer is the quantum kernel estimation routine, while all of the other optimizations are performed classically. So here are two main points regarding classical and quantum kernel methods. First, intuitively we should think of quantum feature maps are more expressive than classical feature maps due to the exponentially more powerful computational power.
00:13:41.674 - 00:14:27.534, Speaker A: So we should hope that quantum future maps can recognize patterns that classical feature maps cannot recognize. So this is the hope for quantum advantage. However, we also have this finite sampling noise. That is, the quantum kernel estimation routine has this one over poly sampling noise, even if the quantum computer is fault tolerant. So this is one thing that we need to address for proving a quantum advantage. So our result is an end to end quantum speedup with this algorithm, which has two steps. First, we construct a learning problem and prove that it is hard for classical algorithms.
00:14:27.534 - 00:15:13.834, Speaker A: That is, no efficient classical algorithm can even achieve a non trivial performance on this data set. And second, we show that we can solve this problem up to very high accuracy using quantum kernel estimation. And along the way, we also need to prove that this algorithm is robust to finite sampling noise. So here is a sanity check. The learning problem that we construct could potentially also be too hard for quantum algorithms. So we need to make sure that this problem is in BQP. So the way we do this is to construct a classification problem that is as hard as discrete log.
00:15:13.834 - 00:16:13.354, Speaker A: So that passes the sanity check, because discrete log is in BQP. So one way to interpret this result is that for solving machine learning problems, quantum kernel estimation can use the full power of quantum computers. Okay, so next, let's take a brief look at how to prove that kernel methods work. So here we have this familiar picture, and the data set on the left hand side is IID drawn from a distribution. So here is the intuition to understand this picture. So after having obtained these training samples from the distribution, we look at it and we realize that this data set looks separable. And we can also efficiently find a good separating hyperplane for this data set.
00:16:13.354 - 00:17:54.894, Speaker A: And therefore, if we draw a new sample in the same distribution and evaluate on this classifier, because of concentration measure arguments, we can intuitively understand that this new example should also lie on the correct side of the hyperplane. So in statistical learning theory, what I just described is called a generalization bound, which is that if this classifier achieves good performance in training, then it should also achieve good performance in testing when it receives new examples. In particular, what we use here is known as margin based generalization bound. That is, if we manage to find a hyperplane, the black line that separates the data set by a large margin, which is the delta distance there, then the test error should be bounded by this margin. In particular, we can see that actually the convex optimization that we are doing is actually trying to minimize this upper bound on the test error. So that looks great. In the most general case, where we have this high dimensional feature space and we replace the data vectors by the feature vectors phi x, we can essentially apply the same intuition and we can similarly show that the test error is upper bounded by this training loss in the convex optimization.
00:17:54.894 - 00:18:57.934, Speaker A: So this is still saying that the convex optimization is directly trying to optimize, trying to minimize the upper bound of the test error. So here m is the number of training samples. So the key point here is that large margin observed in training implies good performance in testing. Okay, so using this tool, we can sort of go through the proof of the quantum advantage. First, we explicitly construct a quantum feature map, or a kernel function that maps classical data to a very high dimensional feature space, and crucially show that the training data is separated by a large margin by this feature map. And next we run the dual program using this quantum kernel, and we are guaranteed to find a good hyperplane. And next use the margin based generalization bound.
00:18:57.934 - 00:20:22.244, Speaker A: We can guarantee a high accuracy when testing this classifier. And along the way we need to address noise robustness, which can be obtained by strong convexity. Here, one thing we use is that small perturbations in a kernel will only cause small perturbations in the classifier, and the large margin sort of protects the classifier from this perturbation. Okay, so before concluding, I would like to mention the complexity theoretic background behind this work, which is known as quantum state generation, a very beautiful paper by Aronof and Tashma. So here, the intuition is that many problems can be solved by preparing certain quantum states and estimating their inner product. This is a general way of designing quantum algorithms. For example, sometimes we can prepare such states, and the discrete log is such an example where they show that if we could pair certain quantum states and estimating their inner product, this can be used to solve discrete log.
00:20:22.244 - 00:21:15.884, Speaker A: And we know how to prepare such states and we know how to solve discrete log. Both are in BQP. So our construction of the quantum kernel essentially follows from this example. On the other hand, sometimes we also know that we also don't know how to prepare such states, but we can establish a reduction. For example, we know that graph isomorphism, a well known problem in computer science, is in BQP if and only if the uniform superposition over all permutation of a graph can be prepared. So we don't know how to prepare, prepare this uniform superposition, and we don't know how to solve graph isomorphism in BQP. As we know they are equivalent.
00:21:15.884 - 00:23:07.614, Speaker A: Okay, so having introduced this quantum state generation problem where many problems can be solved by preparing certain states and estimating their inner product, we can immediately see that there is a similarity with quantum kernel estimation, where classical data is mapped to high dimensional quantum states and the kernel matrix is obtained by estimating their inner products. The two things look very similar, so our result can be viewed as establishing a formal connection between the two things. On the one hand, we can sort of use quantum state generation as a complexity theoretic background that supports the quantum that supports the quantum advantage of the quantum kernel estimation algorithm. On the other hand, we also know that preparing certain states, or actually certain uniform superpositions is a very powerful way to solve many problems, especially those with a group theoretic structure, which is precisely where quantum advantage lies in. This shows that quantum kernel estimation is actually a very well motivated quantum algorithm with good reasons to hope for a quantum advantage. So finally, I can mention some prospects and obstacles of quantum advantage with quantum kernel estimation. One future direction is that we could potentially improve our results to BQP complete.
00:23:07.614 - 00:24:25.204, Speaker A: That is, we can try to construct a learning problem that is BQP complete, and then solve it using quantum kernel estimation. So our current construction is based on discrete log, which is not known to be BQP complete. And secondly, and perhaps more importantly, we should try to find practical learning problems that are both challenging for classical algorithms and with a structure for potential quantum speedup. Because fundamentally we know that quantum speedup requires structure. And on the other hand, there are also obstacles for obtaining a quantum advantage with this algorithm. First, we know that constant depth 2d quantum circuits do not have asymptotic advantage due to a result of bravia tau. In particular, their result implies that if we use constant depths 2d circuits in the quantum kernel estimation routine, then that process is simulable by a classical algorithm, and therefore the entire learning procedure can be efficiently classically simulated.
00:24:25.204 - 00:25:25.296, Speaker A: And of course, our result uses deep polynomial depth circuits. Finally, we also have to admit that there are already very powerful classical general purpose learning algorithms, especially deep neural networks. It is very hard to imagine that we can actually outperform these algorithms on a generic learning problem. But as I just mentioned, we should look for this structure for a potential quantum speedup. Okay, so finally, I can give you one example in recent experiments that implements this quantum kernel estimation algorithm. So they used 17 qubits Google sigmoid device and try to do some binary classification of some cosmological data set. And the left hand side is the quantum feature map that they use.
00:25:25.296 - 00:26:08.604, Speaker A: And we can see that this is essentially the same as what we just described. And this is implemented by a shallow circuit. And they can also observe that in simulations, the quantum kernel estimation algorithm achieves similar performance as classical svms. And when they actually implement this on a quantum computer, they can observe a non trivial accuracy with 60% to 70%. So this is basically pretty much the state of the art for current experimental implementation of these algorithms. So, yeah, that's all for the talk. Thank you.
00:26:14.024 - 00:26:16.844, Speaker B: Thanks for your talk. Are there any questions?
00:26:23.864 - 00:26:26.484, Speaker A: Oh yeah, there are several questions.
00:26:26.904 - 00:26:41.024, Speaker B: Let me see one question. The question is, has HHL already showed BQP completeness of quantum matrix inverting? Why doesn't that automatically imply an exponential separation between classical and quantum machine learning via, say, discrete log?
00:26:42.884 - 00:27:35.514, Speaker A: Right. So I think HHL still uses this amplitude encoding because it assumes some input is provided in terms of quantum states. And this is fundamentally a different model from our setting, where the only input we receive is a set of classical vectors, and we need to find a way to load them into quantum computers. So I would say that this is in a different input model and in the classical setting, which is what we are talking about, sort of. It is unclear that heuristic quantum machine learning algorithms could, in principle, solve a hard problem, because the way that they use quantum computers is not necessarily the most general way. So our result is sort of solving that challenge.
00:27:37.534 - 00:27:42.874, Speaker B: Thank you. There's one more question. Could you explain the plot on the last slide in more detail?
00:27:44.014 - 00:28:20.294, Speaker A: Oh, right. Okay. Yeah. So the second. Okay, so the first figure is the quantum fusion map that was implemented in this experiment, where this is just a shallow circuit. And, of course, if you keep the depth as the same and scale up to many qubits, this is still classically simulable. And the second picture is the numerical simulation of the quantum current estimation routine.
00:28:20.294 - 00:28:52.094, Speaker A: And you can see that they're comparing this quantum fissure map with RBF, which is actually one of the examples of classical fissure maps that I just described before, so they can observe a similar performance in testing after doing some training. And finally, in this experimental implementation, the accuracy, of course, drops because of the noise, but it's still beyond 60%, which is sort of a non trivial accuracy.
00:28:53.914 - 00:29:03.874, Speaker B: Okay. It doesn't seem like there are any more questions, so let's. Thank you, Michelle. Again, I think we have some time before the next one.
