00:00:00.200 - 00:00:09.194, Speaker A: First day. So the next talk is going to be by Jotam Dikhstan is going to talk about reverse hypercontivity and sampling in high dimensional expanders. So take it away.
00:00:11.574 - 00:00:32.268, Speaker B: Thank you. Thank you for coming. So, yeah, first of all, all I'm going to say in this talk is joint work with Max Hopkins from UCSD. So. Hi, Max. Yeah, so as those said, I'm going to tell you about reverse hypercontructivity and sampling in high dimensional expanders. So this is sort of the Boolean function analysis part.
00:00:32.268 - 00:01:09.384, Speaker B: And this is like the beyond the hypercube part. Before I start though, I like prepared this cheat sheet on high dimensional expanders. Do everybody see this part of the board? Yeah, so just so that we converge to the right definitions and establish some notations. So maybe I'll go through this for a minute. Yeah, so we're going to talk about simplicial complexes. Basically these are just hypergraps with the downward containment property. Namely, if you have a set inside the hypergraph and a subset of it, then the subset is also part of the hypergraph.
00:01:09.384 - 00:01:47.122, Speaker B: One nonstandard notation that I'm going to use, xk, like this, will be all the sets x inside x of size. Exactly, k. So I know that the convention is to put here a k plus one, but it will be somehow more convenient to use this convention. Two other notations. One is quite standard. The link of the face s inside x will just be taking all the t s for t that contains s and x. So probably at least some of you have seen these things already.
00:01:47.122 - 00:02:58.664, Speaker B: And I also think that you maybe gone over this in the bootcamp still, it will be nice to give an example. So if this is sort of our simplicial complex, like these three triangles and all edges and vertices, and we look at this like s here in the middle, its link will actually be a graph, right? It will have like these two edges, a and b and c and d, because the vertex a here corresponds to the edge sa and so on and so on. Um, another thing that, uh, um, I want to notate here is the star of s, or the k star of s, which will be all k faces that contain s. So for example here, if we look at like the one star of s, this will be like all these four edges, and you can see maybe why I like to call it a star. Um, so one last thing I should define now like high dimensional expanders. Basically, high dimensional expanders are these simplicial complexes where the underlying graph in every link should be an expander graph. So this is like a non example because this is not even connected.
00:02:58.664 - 00:03:46.284, Speaker B: However, I'm only writing this down here so that those of you who know this definition already will know what I'm talking about. Essentially, for the talk today, I won't actually need this link definition. All we really need to remember is that high dimensional expanders sort of approximate behaviors in the complete complex, in the complex that has all possible sets. So this is really the essence of high dimensional expanders and we'll be more precise. So yeah, so I want to tell you about reverse hypercontructivity. And essentially this is a property of graphs that talk about sort of small set mixing inside graphs. So first, let's define the graphs that we will be looking at.
00:03:46.284 - 00:04:31.364, Speaker B: So x will be some simplicial complex. And for the first graph that we talk about, we need two integers, k and l. And this first graph is sort of the, what's called down up walk graph. But let me define this. This is a graph whose vertices are of sides k inside x and whose edges are just pairs that intersect with at least l elements. Okay, so these are all the t one s and t two s intersection size, at least l. I like to think of this as some like sparsified version of the Johnson graph, because when x is the complete complex, this is roughly the Johnson graph, maybe with equality here.
00:04:31.364 - 00:05:08.916, Speaker B: And this graph also comes with a natural distribution that one can think of as like generalizing the noise distribution in the Boolean hypercube. So this distribution of choosing a pair t one and t two goes as follows. Well, first we sample one endpoint in xk. And I should say this right away, think of x as being like very, very regular. So when I say we're sampling something out of x carry, think of the uniform distribution. It doesn't have to be like this, but it will make things simpler. Um, so first we sample one endpoint.
00:05:08.916 - 00:05:56.718, Speaker B: Then we sample a intersection inside it. So this will be an l subset of t that's uniformly sampled. And finally, we sample t two, also in xk, but conditioned on containing this intersection s. So why do I think of this as like a noise on t one? Because again, this is t one. And when we choose t two, we're saying we're keeping some of the vertices so there is some correlation. However, for the rest of the vertices, which will give us back t two, we're sort of like re randomizing over the remainder of the set. So this is the first graph that I want to talk about.
00:05:56.718 - 00:06:38.580, Speaker B: And the second graph is even more resembling the noise distribution. Because the second graph, we won't just choose s of a fixed size xl, we will actually like flip points for every vertex to decide whether or not we are re randomizing it. So for the second graph, we have some parameter rho between zero and one. And let's denote by j k row. And don't worry, we will always be able to make the distinction to be the graph whose vertices are also x k. But this time, technically, the edge set will be like all possible pairs. However, the distribution will be very non uniform in the way that I just described.
00:06:38.580 - 00:07:39.782, Speaker B: So what we do, we sampled one endpoint out of xk. Next, we will flip coins for row biased coins for every vertex, such that with probability row we keep the vertex inside the intersection, and with probability one minus row we remove it, and then we re randomize on t two. So one way to formalize this is to say that we are sampling l from some binomial distribution with success probability rho, and then we sample s inside t uniformly at random that has this size l. Finally, we output t two the same way we've done for the previous graph. So these are both graphs. I think that maybe Max also covered this in the bootcamp, that this is really sort of a generalization of the noisy graph. And before I state formally what I want to tell you today, I want to maybe just do this in a picture.
00:07:39.782 - 00:08:22.214, Speaker B: So these are, you know, one of these graphs. And essentially today I want to tell you about a mixing theorem that says that if you have here two small sets, a and b, still edges in this graph will be quite well distributed. Namely, we will always find like a noticeable fraction of edges going from a to b. So this is quite a nice property, especially because these graphs are not very good. Say, expander graphs are not very good spectral expanders. So like, the right regime of parameters to think about is maybe like rho equals 0.9. So in this case you can find like very small sets here where like 90% of the edges actually stay inside the set.
00:08:22.214 - 00:08:47.764, Speaker B: So like very bad expanders. And yet you still have this like small set mixing lemma that says that even if this is true, the remainder of the edges are quite well spread among the graphs. You can't achieve non trivial independence, let's say, due to this correlation between subsets. Let me write the theorem down. Formally, or at least somewhat formally.
00:08:50.784 - 00:08:51.072, Speaker C: The.
00:08:51.088 - 00:09:33.334, Speaker B: Theorem is as follows. Um, let's take rho to be some parameter, and let's take l to be rho k. Whenever I multiply an integer with a fraction, it will always be an integer. Okay, so I'm not going to do integral value, but let's just assume it. Um, and let x be a sufficiently good high dimensional expander where like the lambda parameter goes like exponentially down with k. So we have two parts, one for each graph. For the first graph we have that for every two subsets of the vertices a and b.
00:09:33.334 - 00:10:32.334, Speaker B: If the relative size which I'll denote just the probabilities of a and b are at least this like exponentially small sets, then the probability of sampling an edge in this JKL graph such that one endpoint is in a and the other endpoint is in b, will be at least some small polynomial in the probabilities of a and b. So of course, if this was just a complete graph or something, I could just have maybe written this here. I need to put some o of one. This like omega constant and this o constant both depend on row. But for a fixed row, I can just write this down. Yeah, for the second graph, like this is sort of a smoothed version of this graph, we get even something smaller. We actually get the same theorem.
00:10:34.394 - 00:10:34.994, Speaker C: But.
00:10:35.114 - 00:11:26.172, Speaker B: Without, or for any a and b, even without this requirement on sizes. So sort of in this graph you have like some small probability of just sampling t one and t two independently, like if you fail to keep any of the intersections. So somehow this takes care of like sets that are smaller than this. And really, even though I'm stating this for these two graphs, this is really the graph that we're going to focus on. And this is like where the heart of the proof is. Yeah, so this is called small set mixing. And before I tell you about sort of how this goes and why this is connected to sampling, I would like to maybe mention a couple of words on the history of such inequalities and why I like to call it a reverse hypercontractive inequality.
00:11:26.172 - 00:12:29.464, Speaker B: So let's think back on the Boolean hypercube. I want to maybe keep this. What should I erase? I'm going to erase this cheat sheet. It's okay. Okay, so back maybe to the Boolean hypercube. Um, so let's denote by t rho. The rho correlated noise operator, namely t rho operates on a function x, gives us another function that on input x, where x is an n bit string, it outputs the average of f over y's that are row correlated with x.
00:12:29.464 - 00:13:28.052, Speaker B: And when I say row correlated, again, like we, the randomness here is by taking x for every coordinate, flipping a coin with probability rho, we keep the value of x with probability one minus rho, we randomize. So this is the expectation here, the well known hypercontractive theorem by, I think, bonamy, and also Beckner and Gross and Nelson. Um, it says the following. So suppose we have p and q where q is greater than p. And suppose we take a correlation that is small enough with respect to these p's and q's. Then it turns out that for every function f, the qth norm of the noise operator of f is less or equal to the pth norm of f. And here, just so we recall, the pth norm of a function f is of course, the expectation of f in absolute value to the p.
00:13:28.052 - 00:14:13.464, Speaker B: And then we take a pth root. And what's important about these norms is that they're monotone. So if I take q to be larger and larger, the norm should increase and increase, or at least it should be greater or equal to the p norm. But here, what this theorem is saying is that this noise operator shrinks f so much that even if I measure it in the qth norm, which should make it bigger, it's still less or equal to f in like the original peak norm. Um, so this is like the standard hypercontractive theorem. And it also has apparently a reverse hypercontractive theorem where I can actually, like, reverse this inequality. This sounds like nonsense, but let me state it for you.
00:14:13.464 - 00:14:57.004, Speaker B: And this, I think, is due to Borrell in the eighties. And now something weird is going to happen here. Now we're going to take p and q less than one, and we're going to say that if Rho is still small enough, then it turns out that the, I'll call it qth norm of t f is actually bigger than the p's norm of f. So, you know, let's, let's process this for a minute. First of all, when p or q are less than one, this is not a norm anymore. Okay? So this is not convex. You don't have like a triangle inequality.
00:14:57.004 - 00:15:10.364, Speaker B: Still. I mean, you can define, you, you can define this. Let's, for convenience, ask f to be a non negative function. This is, of course, necessary here.
00:15:11.624 - 00:15:11.992, Speaker C: Okay?
00:15:12.008 - 00:16:31.694, Speaker B: So you can still define this and it still makes sense to maybe compare these things. And it turns out that, you know, you can, you can actually get an analogous result if you sort of reverse the arrows here and you reverse the arrows here. However, I mean, this seems quite abstract, and it actually has an equivalent two function version that is, like, much easier to understand. And to actually find out how this theorem came from. So actually, the two function version here will say that for every f and g, if we look at the expectation of fx and gy, where x is uniform over the Boolean hypercube and y is noised up, and this expectation is greater or equal to the pth norm of f times the Halder conjugate norm q prime of g. So here q prime, like formally it's q over q minus one. And I really encourage you to think about this theorem in the parameter regime where like p and q prime are some like positive constant, of course, less than one.
00:16:31.694 - 00:17:52.220, Speaker B: Okay, so why do I like this two function version? The reason is, it actually makes sense when you put your indicators of functions. So when I will put your indicators of functions, what we actually get is, is what? Suppose f is the indicator of some subset a and g is the indicator of a subset b. The left hand side of this inequality, well, it just simplifies to the probability of sampling x and y as a noisy copy, such that one endpoint is in a and the other endpoint is in b. And whereas this right hand side, uh, this will be, well, it's not too hard to follow this expression. If this is an indicator function, this p power or absolute value doesn't matter. So this really is like the probability of a to the one over p, the probability of b to the one over q prime. So if you believe me, that this is equivalent, if you instantiate this to indicator functions, then you will get that this thing is actually also the probability of a times the probability of b to the o of one.
00:17:52.220 - 00:18:35.626, Speaker B: I should say that this observation was first done by Mossel, O'Donnell, Regev, Sudakov and Steif. And they actually proved this very useful small set mixing inequality in the Boolean hypercube. And they used it for application and some communication problem with many parties later, this also found applications in like in agreement testing and like in robust social choice theorems. And it's really quite a nice theorem for the Boolean hypercube from justice version.
00:18:35.690 - 00:18:39.978, Speaker C: Oh, that's actually a good question. The answer is sort of yes for.
00:18:40.026 - 00:18:42.242, Speaker B: If you have this for, if you.
00:18:42.258 - 00:18:58.630, Speaker C: Have this for offsets, if you're willing to satisfy, to be satisfied with slightly worse P's and q primes here, and maybe also, maybe like a fixed constant here, you can do it. Actually, we also do it in the paper, just because we didn't find anywhere.
00:18:58.662 - 00:18:59.914, Speaker B: That this was done.
00:19:02.374 - 00:19:19.184, Speaker C: But maybe a better answer to your question is that, well, yes, but really, when people use this reverse hyperconstructivity. All they really care about is this. I mean, I think that in every single application, or at least the ones that I read, this is what's interesting.
00:19:19.684 - 00:19:25.864, Speaker B: But formally, mathematically, you can get back. Good.
00:19:27.244 - 00:19:27.884, Speaker C: Yeah.
00:19:28.004 - 00:20:07.008, Speaker B: So I'll continue here. Okay, so this is maybe the history, or why we call it reverse hyperconjunctivity. And now that we know this, I want to tell you something about the proof here. And the first thing I want to do is to convince you that this question of mixing is actually a question of sampling. Okay? And the reason is as follows. Yeah, so look at this distribution over here, which is the distribution on edges. Actually, it can be equivalently viewed as a distribution where we first sample this intersection.
00:20:07.008 - 00:20:19.574, Speaker B: So we treat it like a first class citizen, and then we will just sample t one and t two, sort of independent from one another. So like the equivalent distribution here is really first sampling s.
00:20:21.394 - 00:20:22.026, Speaker C: Which is the.
00:20:22.050 - 00:21:48.364, Speaker B: Intersection, and then sampling t one and t two that contain s independently from one another. So this probability that t one is in a and t two is in b, we can of course write it as the expectation over this intersection in the first step of the probability, given that t one and t two contain s. But because these things are independent, this is just really this expectation over s of the probability that over, like the relative size of a and the relative sizes of b inside the star of s. So like the picture should be, or maybe I will just like, decrypt this notation, pa given s is just like a intersect, d k star of s divided by the k star of s. And, you know, like pictorially, we have sort of xk on one side, we have xl on the other side. And we sort of look at like these containment neighborhood or this containment graph. And, you know, we have like two sets here, a and b.
00:21:48.364 - 00:22:57.624, Speaker B: And this expectation is really asking the question, how many s's c in their, like, containment neighborhood? Or like in their star, they see a and b with at least some like, noticeable or non negligible probability, because this is all that's going on here. So this sort of motivates us to ask, how well do high dimensional expanding, like, how well do these containment graphs in high dimensional expanders actually sample small sets of dimension higher? So, so with this intuition in mind, I'd like to define a very useful primitive in computer science, which is called sampler graphs. There are some definitions out there. Not all of them are exactly equivalent, but they are very similar. Essentially, a sampler graph is a graph where one side really samples sets on the other side quite well. Maybe we draw another picture. Suppose we have a bipartite graph with left or left and right sides.
00:22:57.624 - 00:24:08.122, Speaker B: And suppose we have some set a on the left who has some relative size, which we'll denote by pa. So we can say maybe that the vertex samples a well on the right if when viewing its neighborhood on the left, the relative size of a inside this neighborhood is roughly the relative size of the global a inside on the left side. So we really want that like the a inside the neighborhood would be approximately the probability of a. So this vertex is good, but I don't know, maybe like this vertex u prime is bad because it almost doesn't see anything inside a. Or maybe this u prime prime is also bad because I don't know, maybe it's like completely contained or almost completely contained in a. And a sampler graph is really a graph where like most of the vertices on the right will be like this vertex u and not these. So I think this is a nice, this is a nice definition.
00:24:08.122 - 00:25:14.330, Speaker B: It has many uses in like error correcting codes and saving randomness. So the definition goes as follows. So for a fixed graph g and some subset a in the left, let's denote by t of a to be like the bad vertices in the right. So these will be all the u's on the right side. So that inside their neighborhood either we see a with more than three halves its original relative size, or we see it less than a half. So these are like the vertices that sample a atypically and for some parameters alpha and beta, we say that g is a alpha beta sampler. If for every set a of probability at least alpha.
00:25:14.330 - 00:26:10.010, Speaker B: So this will be the set size bound we have that at most beta, a beta fraction of the vertices sample a badly. Okay, so for every a, most of the vertices will sample a quite well. So I want to give some examples for this definition. So it will maybe be less mysterious. I'll erase this, okay, so I think that the first example would sort of be the complete complex example. So let's take our graph to be left side is some universe n sized right side. I'll denote it by this notation, namely all possible subsets of n of size exactly k.
00:26:10.010 - 00:26:29.414, Speaker B: And the edges will be containment edges. So like v is adjacent to s, if v contains s. So by like standard churn of bounds, one can show that for every set a, the probability of the vertices on the right.
00:26:31.554 - 00:26:31.890, Speaker C: At.
00:26:31.922 - 00:27:33.756, Speaker B: Sample a badly is at most some exponent to the minus k times the probability of a squared. And why is this true? Well, roughly let's think of k as some constant and n as much larger than k here, like sampling a node on the right, this is roughly just like sampling k vertices on the left. Independently, I mean, of course there is some probability when we sample independently that there will be a collision, but ignoring that when k is very small, this is roughly the same thing. So this is just a churn of bound. And in fact, one can show that for graphs with right side degree k, this is essentially optimal. This is a theorem by, I think, Kanetti Eben and Odette Goldreich. So really this is the most that we can expect, and we indeed get it.
00:27:33.756 - 00:28:22.112, Speaker B: Of course, if you like to save in randomness and things like that, this is very inefficient. There are a whole bunch vertices here which like people have tried to take down. The second example that will be more modest in parameters is just that, if g is a lambda spectral expander, so it's a bipartite graph, but it can be a one sided lambda expander. You will still get, for every alpha greater or equal to zero, that g is sort of an alpha roughly lambda squared over alpha sampler, just to compare, because I don't think I wrote it.
00:28:22.128 - 00:28:22.712, Speaker C: Here on the board.
00:28:22.768 - 00:29:26.366, Speaker B: Oh, here g is like an alpha e to the minus alpha squared over three k sampler. Here you can get something that's like polynomial in the degree, whereas here you get something exponential. Still, sometimes what you have to work with is only a spectral expander, and this bound could still be useful. And in fact, in previous works of high dimensional expansion, like, you know, on Kaufman's work on agreement expanders, or dinur, Livny, Kaufman, Halcha and Taschma's work on constructing locally list decodable codes like this was mainly what was used. Spectral gap does achieve good enough sampling for some applications. The main technical tool that max and I needed to show this reverse hyperconstructivity is actually stronger sampling than what you can get from a spectral bound in these containment graphs of high dimensional expanders. Yeah, here.
00:29:26.550 - 00:29:57.754, Speaker C: Well, g is a bipartite graph, so it must be a long one sided spectral expander, because you always have like this minus one eigenvalue in a vector. This is not a spectral expansion of the complex. No, this is just like for plain graphs. This is not, this is just an example. But yeah, I'll comment on that in a minute. Okay. Yeah, so we learned about that.
00:29:57.754 - 00:30:19.294, Speaker C: We proved in the following, so let x be a two to the 97k high dimensional expander. Here you can either take a two sided high dimensional expander, you can take some multipartite one sided high dimensional expanders.
00:30:19.994 - 00:30:21.658, Speaker B: But they need to be like a.
00:30:21.786 - 00:30:58.324, Speaker C: Cut off of like polynomial, polynomial in the dimension. This is like a technical property, but this can could be extended slightly. Then the following graph is a very good sampler. So what will be the graph? So this graph will have a, let's maybe fix three parameters, I, j and k, and let's maybe fix a face r I. The left side of this graph will be the r star of eleven k.
00:30:58.624 - 00:31:00.776, Speaker B: The right side of this graph will.
00:31:00.800 - 00:31:08.804, Speaker C: Be the r star of eleven j. Edges will be of course, edges of containment.
00:31:09.264 - 00:31:19.164, Speaker B: And the sampling property that we achieve is that if you have a set size bound which is exponential in.
00:31:20.864 - 00:31:21.176, Speaker C: A.
00:31:21.200 - 00:31:37.324, Speaker B: Minus I divided by j minus I, you will get a confidence that's a small constant. This is roughly the technical tool that I want to tell you about.
00:31:39.224 - 00:31:39.584, Speaker C: But.
00:31:39.624 - 00:32:31.904, Speaker B: Let'S talk about this a little. First of all, um, like the, like the first use case that you maybe want to consider is like what happens when I equals zero and r is just the empty set. In this case, this is just xk, this is just xj, and we get this like containment graph. The rest actually follows from like the general case that I wrote here, sort of actually follows just from the fact that the star is sort of isomorphic to the link, and the link is also a good high dimensional expander. So this is not only like the first case you think about, this is like almost a general case. So I like to think about it like this. But it will be useful later to also consider like these local components, these stars.
00:32:31.904 - 00:33:22.294, Speaker B: Another couple of things I want to say about this lemma. I don't think I'll have time to prove it to you. First of all, I should say that there is a work by impaliazzo, cabonets and Wigdelsson from 2008 that showed roughly the same lemma for the complete complex. And whenever you see such a statement on the complete complex, it sort of challenges you to say, ah, you know, you don't really need the complete complex. You can do this on high dimensional expanders. So it turns out that you can. And the proof, it mainly goes through a previous sort of pseudo random result in high dimensional expanders by Rick Dinur and myself, which is called the high dimensional expander mixing lemma.
00:33:22.294 - 00:34:46.592, Speaker B: And I will just say that you can also generalize this, and we try to slightly generalize this to also other interesting hypergraphs. So you can get somewhat of a similar theorem. Also, if you look at like waxing and expander or like other, what's called like splittable trees for those who know this, for those who don't, this is not such an important comment. Yeah, finally, the last thing I want to say about this lemma is that in fact, this maybe parameter regime is not what people usually like to look at. Usually people like that the set size bound is constant and the confidence is really, really high or really, really low. If we look at beta as the probability of the bad set, it turns out that if you flip sides here, namely, you take the small side to be the side that sample and the large side to be the side that samples, like ksets versus j sets, you can also reverse the parameters here so you can get small constant as your set size bound and a large constant, sorry, not a large constant and an inverse exponent here in the confidence. And this also found applications in palliato cabinet and Vigdor zone's work.
00:34:46.592 - 00:34:58.164, Speaker B: So I don't know, maybe this generalization to sparse or simplicial complexes could also still be interesting. Yeah, any questions here?
00:35:04.504 - 00:35:32.488, Speaker C: J is like k over two. You're not getting. Yeah. So for j and k to be constant, this is, this is quite weak, right? Because this is only a constant here. You need to think about this as when j is like really small, like, I don't know, like a square root k or even a constant, where in this parameter regime, then the, what you're pronounced here from the spectral gap, is something that behaves like one over alpha to one over alpha k or something.
00:35:32.656 - 00:35:34.792, Speaker B: Whereas here, this is like an exponent.
00:35:34.928 - 00:36:06.244, Speaker C: In, an exponent in minus k minus k or minus square root k. So really the power here is when j is like incomparable to k, but then you don't have, you don't have like a polynomial decay. You have exponential. Expect to get better. You also assume a very weak pattern.
00:36:09.424 - 00:36:10.896, Speaker B: What do you mean by weak spectral.
00:36:11.000 - 00:36:17.614, Speaker C: It'S a really strong electronic expansion, right? It's like inverse exponential in K. Oh, it's good.
00:36:18.234 - 00:36:18.642, Speaker B: Okay.
00:36:18.658 - 00:36:40.690, Speaker C: Yeah. Okay, I'll say something. Okay, think of like I have zero. So this is just x k versus x j. When j is one, then this roughly achieves the parameters you get for the complete complex. So you can't expect anything for even j equals two, or, I don't know, square root k or something like that. Well, I don't know how much better this can get.
00:36:40.690 - 00:36:52.494, Speaker C: We haven't tried, or we haven't tried very hard. You need to try also in the complete complex, because better things are not known even there. So I mean, it could be true, I'm not sure.
00:36:58.914 - 00:37:49.544, Speaker B: Definitely for that case you don't have like some optimality lower bound or something. It could potentially be better. So now that we have like all the players in the proof, I'd like to give you a rough sketch of how one shows this reverse hyper contractive inequality. So let me remind what the setup is. So this theorem. So recall that we have some good enough high dimensional expander. And when I say good enough, I mean a high dimensional expander such that the previous lemma holds.
00:37:49.544 - 00:38:29.230, Speaker B: That's like really all we need. We have two sets, a and b, which are vertices in this like JKL set, subsets of vertices in the JKL graphic. And all we know about them is that they are no smaller than some one exponent in k. And for simplicity, let's fix like rho to be one over four, l to be k over four. I should say this is like a minor technicality. When rho gets closer to one. You need to introduce some more complexity to this proof.
00:38:29.230 - 00:39:15.474, Speaker B: But still, most of the ideas will suffice here. And this is like still linear in k. And what we want is to show that if I sample t one and t two in this like sparsified Johnson graph Jk Lk over four, this will be greater or equal to the probability of a times the probability of b o one. This is what we want to show. And now I want to draw this like containment graph, and it will have soon multiple layers. So I will draw it like horizontally. So here's XK.
00:39:15.474 - 00:40:18.924, Speaker B: Here we have two sets that could be like inverse exponentially small, but I'll draw them slightly larger just because, uh, just for the picture. And you know, here's x k over four. And as fan pointed out, I mean, if these sets had like some, I don't know, like a small constant size, we could have just, you know, said that like for at least like maybe half the s's over here, they would sample a and b well enough, like up to like losing a factor of half. But of course this is not true, right? Because we don't know that they're constant. Instead, we know that they, they have side, that's the most like inverse exponential in k. However, note that what we need to prove is not that this thing, this probability is lower bounded by a constant times these two probabilities. We need to prove something much weaker when a and b gets smaller.
00:40:18.924 - 00:41:15.266, Speaker B: We need to prove that this is only greater or equal to this product of probabilities to, I don't know, to the 7th power. So like the smaller a and b gets like the more room for error that we have. We don't need such a strong sampling theorem. So what we're going to do is sort of try to like accumulate more errors along the way and yet still use the sampling theorem where it will actually apply for a and b. So with this in hand, the trick will actually be that when we sample s in x, k over four, we won't sample it in a one shot, we will sample it in batches. Okay, so we think of s as like a k over four set, but we will sample it like j vertices at a time. So r one, r two, r three and so on up to, I don't know, rb, which will be the number of times we need to do this.
00:41:15.266 - 00:42:01.354, Speaker B: So j, I think of it as the batch size. And for the moment let's think of j as just being like 100 vertices or something. Later on we should optimize this. Okay, but let's think about it like that. And b would be the number of batches we need, which is of course k over four j. And the point is that when this j is indeed a constant, then even if a and b were actually this small, we will still be able to use this sampling property. Okay, because when j is a constant, and you should think of I also as a constant here, then this will be like e to the minus k.
00:42:01.354 - 00:43:45.536, Speaker B: So the trick is, again think of x. J is like way down below here, and we will sample things in xj and then we will gradually go up until we get to x k over four. So what can we say actually about, what can we say about xj? Well, for the first ride, for the first r that we sample for the first j vertices, there, we actually know that our sampling graph will be small, will be strong enough, because these are bounded by this quantity and j is a constant. So like for the first k vertices we can actually sample a and b quite well. So towards this maybe we define like this good set g one, which will be all the vertices such that a or b and their star is off by at most a half. And because we know that this graph is e to the omega k 0.25 sampler, and because we know that a and b are large enough, we know that if r is not in g one, this means that r is either in the bad set of a or the bad set of b, which means that g one has relative size, at least one, two.
00:43:45.536 - 00:44:39.144, Speaker B: So like for the first day, our vertices, half of them are good. What about the next star vertices? So now I've sampled like one subset of vertices I still haven't gone to gone all the way, but now I'm only worried about sampling the next r vertices. So I like to think about it like this. Now we're looking at x two j, we've already sampled r1. Hopefully it wasn't this like good set. And now sampling another r two is actually sampling, you know, is sampling the r1 union r two, or disjoint union from the jth of r. So this is the, sorry, the two j star of r.
00:44:39.144 - 00:45:45.614, Speaker B: So we have two j vertices j of which are inside r. And of course, provided that r landed inside this good set, we know that if we look at the star versus star graph, if we look all the way up onto the k star bar, we know that over here a and b. Well, if r was good, they may have been smaller, but they're only smaller by a factor of half. So still a factor of half, we can still use this bound. So again, there is some deterioration in parameters that you need to carefully like trace, but this star graph will also be e to the minus omega k 0.25 sampler. And so if you define the set which are all the things in with two j vertices, so that in the star we are off by a factor of at most a quarter.
00:45:45.614 - 00:46:27.562, Speaker B: So it's like half squared. Okay, then I claim that the probability of this g two is itself greater or equal to a quarter. And why is this simple? We're sampling this set g two. So, you know, first we sampled r1. If it's in the bad set, okay, we lose, but it's in the good set with probability half. So, you know, so like with probability half, r one is in g one. What about two reals? Well, here I use sampling.
00:46:27.562 - 00:47:38.214, Speaker B: Given that r1 was in g one, we know that half of the r1.02 reals inside here sample a and b really well with respect to their size inside the r1 star, which is at least half their original probability. So probability another half r1 and two reals samples and b. So this is roughly it. So now, you know, we will define also g three for three, j four and so on and up until you maybe arrive at sort of gb where b is the batch size. And these will be all sets of size k over four such that in their star we are off binomial of a factor of two to the minus b. This is like a very typical inductive proof in high dimensional expanders.
00:47:38.214 - 00:48:56.684, Speaker B: You sort of start with something global, and then you condition on a link of a star of something, and then you maybe condition on something else, and you go up, up, up all the way until you get like this recursive or induction or inductive argument. And I claim that this suffices because of course, the probability that a good event happens, which is this expectation. Well, this is just greater or equal to the probability of gb times the smallest value this thing can get, given that s is in Gb, which is the capital b. I am now noticing that I used little b and capital b, so I'm saying b a lot. I apologize. Yeah. So of course, here, the correct analog of what we are proving here, like half a quarter and so on, will also be that this GBST.
00:48:59.864 - 00:49:00.376, Speaker C: Will have.
00:49:00.400 - 00:50:05.796, Speaker B: A size that's also like inverse exponential in B. And if you can actually prove this, which we do, this is at most, sorry, at least two to the sum exponential in BApB. Oh, you know, this may be a bit technical, but all we really need to understand here is that if you use, if you analyze using this technique, the error that you incur is really exponential in the number of batches that we take. So like the name of the game here is to carefully balance these parameters. So on the one hand, you want to take as many vertices per batch as you can, so you will incur less error. On the other hand, if you take too many vertices, then you won't be able to use like the sampling lemma. But it turns out that if you use a large enough constant, say 100 -100 log pap, this argument will actually work.
00:50:05.796 - 00:51:20.560, Speaker B: So you can lower bound this by Papb to some small power. Yeah, maybe I'll just give some concluding remarks and I'll finish here. Um, so this was sort of a combinatorial proof to like a reverse hyper hypercontructive inequality that I felt that maybe had like a different flavor than like the more analytic proofs that you see for like, you know, for the boolean hypercube or other places where you can sort of do something in like a two point space and tensor eyes, or maybe use like log sole of inequalities. So I was wondering whether there was like a more analytic way to also view this proof or also view these proofs in high dimensional expanders. So this is like one question that I maybe want to pose to the audience if somebody wants to think about it. Another thing that I sort of view as somewhat a caveat in this work, which is also a caveat, like in a lot of works in high dimensional expanders, that this is really requiring a lot. I mean, you know, high dimensional expanders are very, very rare objects.
00:51:20.560 - 00:52:03.634, Speaker B: You can't just like sample one, like from a natural random distribution. And here not only you need a high dimensional expander, you need like an extremely good high dimensional expander. Like the expansion parameter is like inverse exponential in k. And you know, like in nature, sometimes you can prove high dimensional expansion, but maybe it's not like uniform in all levels and maybe it's not exponential in K. And I mean, these like mixing inequalities seem quite useful. So I mean, what can we say for other structures or even for high dimensional expenditures that are like weaker than this, like very strong guarantee? I think this is a good place to end. Thank you for listening.
00:52:10.454 - 00:52:12.434, Speaker A: Any questions for your tomorrow?
00:52:23.444 - 00:52:41.772, Speaker B: Yeah, that's a good question. I don't think it extends to any matriarch complexes. I mean, maybe you can try and tailor like the proof details to matriid complexes as well, but no, I mean, I wouldn't be surprised if you could extend this. They're very structured, right? They're like almost partite.
00:52:41.908 - 00:53:34.234, Speaker C: So for the complete arsenic now is also complete complex, what is known, like your version of another type of. So if you want. Yes, it's known. No, but talking about gentleman and for later, but in Johnson, it's not true for all. Therefore you need. That's if I can go back.
00:53:40.654 - 00:53:43.030, Speaker B: I admit I'm not sure what, what's.
00:53:43.062 - 00:53:45.046, Speaker C: Known in the biodense case.
00:53:45.190 - 00:53:47.526, Speaker B: I imagine that these techniques will be.
00:53:47.550 - 00:53:52.434, Speaker C: Weaker wherever, like more analytic techniques.
00:53:52.734 - 00:53:54.686, Speaker B: I mean, this is not, this is.
00:53:54.710 - 00:54:03.664, Speaker C: Also probably not optimal in the parameters. And like, really the advantage here is only that you can work over like sparser sets.
00:54:03.704 - 00:54:05.524, Speaker B: I don't know.
00:54:06.344 - 00:54:53.982, Speaker C: I think very interesting to me here is that you have this hypothesis that your set is loud because in many external problems it looks like the conductivity is no way of working because it somehow works for all sets and the external problems you want the sets to bring out. Yeah. And somehow the fact that you managed to prove something with less activity under the focus only when. Yeah, I'd be happy to discuss later. Thank you. Maybe I've just mentioned just for the last minute that this requirement is actually also necessary. So you can like, so not a very complicated counterexample where this, like a.
00:54:53.998 - 00:54:55.262, Speaker B: And b have sides two to the.
00:54:55.278 - 00:55:01.566, Speaker C: Minus k and they have no edit between them. Like, as far as you can, with.
00:55:01.590 - 00:55:25.152, Speaker A: Respect to that is the spectral expansion parameter and the set size the same, or do you get. So if the spectral expansion parameter is weaker, do you get anything better? I mean, do you get anything for larger sets? Oh, if you're gamma hdX, do you get something for polynomial in gamma?
00:55:25.288 - 00:55:31.004, Speaker C: Well, I mean, eventually, if they're large enough, you can get something. Just construction. But in this context.
00:55:32.864 - 00:55:34.440, Speaker A: Gamma is heated on the minus.
00:55:34.512 - 00:55:54.660, Speaker C: Okay, yeah, I guess that for minus, write gamma is two to the minus something and make it. Okay. Like, what is hiding here?
00:55:54.812 - 00:55:57.452, Speaker B: Like, the theorem that I stated in.
00:55:57.468 - 00:56:06.668, Speaker C: A simplified way sort of tells you that you are, you are a sampler with like, with error probability that like.
00:56:06.716 - 00:56:07.980, Speaker B: E to the, I don't know.
00:56:08.052 - 00:56:12.782, Speaker C: Okay, something, something plus an error, which is of the form like two to.
00:56:12.798 - 00:56:13.806, Speaker B: The k or three to the k.
00:56:13.830 - 00:56:38.724, Speaker C: Or something times lambda. And sort of. So, like, if you are a small exponential, then this is roughly like this. And once you're a large exponential, you get something that could be much larger than, I mean, even for exponentially small lambda, this could be much larger than one. So you get nothing. Yeah, yeah. So missing that, you know, sometime.
00:56:41.224 - 00:56:43.136, Speaker B: So, so again, this really only comes.
00:56:43.200 - 00:56:49.004, Speaker C: Up like in the proof of development. So if you know it from some other source, you can, you can do this.
00:56:52.104 - 00:56:53.444, Speaker A: Any other questions?
00:56:57.424 - 00:57:03.580, Speaker C: You can't push the confidence level like here, one, two, five, the conference.
00:57:03.732 - 00:57:06.068, Speaker B: Oh, you, I just simplified you.
00:57:06.156 - 00:57:15.704, Speaker C: There's a parameterized version where this is alpha, and this is maybe like e to the minus alpha squared times what I wrote, and I just didn't want to.
00:57:24.524 - 00:57:25.924, Speaker A: Okay, so let's thank it again.
