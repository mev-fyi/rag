00:00:00.160 - 00:00:41.462, Speaker A: Shop by Moses Charika from local galleries and raft cuts. Okay. Whoops. All right, well, thanks for sticking out till the end. Thanks for the organizers for inviting me. So before I tell you the problem that we set of problems that we studied, I should say this is joint work with Neha Gupta, who's here in the audience, and Roy Schwartz, who's also here. Let me tell you a simple problem that I want to leave you with.
00:00:41.462 - 00:01:32.364, Speaker A: And if there's one thing you should take back, think about this nice simple problem. I think it's very cute and it's reflective of the kinds of problems that we study in this work. Okay, so you all know about the minimum s t cut problem, right? Basic problem. Take a graph, divide it into two pieces such that s belongs to the first piece, t belongs to the other piece. If our cut is S s bar, we're interested in minimizing the number of edges that cross over from one part to the other undirected. Yeah. Okay, now let's ask the following question.
00:01:32.364 - 00:02:51.910, Speaker A: Instead of summing up all of the edges that go across from one part to the other, what if we pay attention to the edges in the cut that are incident on every vertex? Okay, so let's look at a particular vertex, v, and let's look at how many of the edges incident on v participate in this cut. Okay, what if we want to make sure that every vertex is, well cut, no edge. No vertex has a lot of its edges, participant in the cut. Ok, so that gives us the following objective. Minimize overall cuts, the maximum overall vertices in the graph of this quantity. So, simple variation on the classical SD card problem with a local objective, a vertex wise objective instead of a global objective. So if you think of the cut as being a vector that tells you for every vertex, how many edges incident on that vertex are cut, the usual objective is just summing up all the entries of the vector.
00:02:51.910 - 00:03:14.004, Speaker A: I'm saying what happens if you want to minimize the maximum entry, the l infinity norm. So how well do you think this problem can be approximated? Just curious. Pretty bad. Pretty bad. Okay. But the short answer is we don't know. But we'll, we'll give you some partial answers in this talk, okay? Because there's one problem I wanted to take back.
00:03:14.004 - 00:03:40.242, Speaker A: This should be the problem. Okay, let me tell you how we got to this question. So we actually, we started thinking about a certain problem called correlation clustering. It's a very simple problem. In this problem, you have a bunch of vertices, you have edges. These edges are of two types. You have positive edges, which indicate that the two endpoints are similar, and you have negative edges that indicate that the two endpoints are dissimilar.
00:03:40.242 - 00:04:18.414, Speaker A: So you have a graph with edges annotated with these positive and negative labels. What you like to do is to cluster the graph so that similar edges end up inside clusters and these dissimilar negative edges end up across clusters. And there may not be a clustering that actually gets everything accurately. You want to minimize the number of disagreements. Or another version is you might want to maximize the number of agreements. So just to drive this point home, here's a particular clustering. These are mistakes because they're positive edges that are actually going across clusters.
00:04:18.414 - 00:04:56.124, Speaker A: These are also mistakes because they're negative edges that are inside clusters. So it's a very nice simple problem. One thing that's nice about this problem is that the number of clusters is not fixed a priori. The goal of the algorithm is to find whatever number of clusters are appropriate for the given input. And this very nice question was introduced by Bansal, Blum, and Chawla in 2002. So it's been a while now, 15 years, and they were interested in some applications in machine learning where you have, let's say a bunch of entities that are referred to in documents. You'd like to be able to cluster them, figure out which entities are really the same person, the same entity.
00:04:56.124 - 00:05:47.368, Speaker A: Um, but the input to the problem comes from a machine learning classifier that gives you judgments about whether or not things, two things are similar or not. Okay, so, um, you know, there are various settings in which this problem has been studied. One is, um, the complete graph where all edges are labeled with similar and dissimilar. So one thing that's, that's very easy to see is if there's a perfect clustering. If there indeed is a clustering with no mistakes, it's very easy to find it. You just take the connected components of the green edges and that gives you your perfect clustering. In general, of course, you have noisy information and perfect clustering may not be possible whenever you have a structure like this, which is a triangle consisting of two similar edges and one decimal edge.
00:05:47.368 - 00:06:15.090, Speaker A: No matter how you cluster it, you've got to make a mistake. You can't put them all in the same cluster, you can't put them in different clusters. You can't put two of them in one cluster or one outside. Either way you're going to make one mistake. And in general you might have a number of such things. If a perfect clustering exists, it's easy to find it. I said that the challenge is, what do you do if there's no perfect clustering? There are two kinds of versions of the problem that have been studied.
00:06:15.090 - 00:06:42.436, Speaker A: As I said, one is minimize the number of disagreements. Another one is to maximize the number of agreements. So let me just give you a brief run through of some of the things that we know about the problem. For complete graphs, there's been a series of work starting with the Bantzalblum Chawla paper, and currently the best known algorithm is a 2.06 approximation due to Chaola et al. Very interesting question. There's a natural LP relaxation for this.
00:06:42.436 - 00:07:15.904, Speaker A: In fact, we'll see a very similar LP for our problem. There's an integrality gap of two, and the question is two the right answer, and these guys come tantalizing close to answering that question. The problem with general graphs is as hard as multi cut. Turns out, it actually encodes multi cut and similar graph partitioning problems. There's a login approximation for this due to a variety of, bunch of different authors. Um, on the hardness side, we know that it's APX hard. Okay? So we can't get beyond the constant factor.
00:07:15.904 - 00:07:59.776, Speaker A: All right, I'm not going to be saying much about the agreement maximizing agreement problem, but let me just tell you what the results are just for completeness. So for complete graphs, there's a ptas, again, from the original paper of Bernsel et al. And for general graphs, there is a 0.766 approximation. This uses sort of this SDP, a random hyperplane, rounding a slight generalization of that, and it's also known to be APXR. Okay, so that's sort of the, these results for correlation cluster. Okay, you know, this general correlation clustering problem where, you know, you have a subset of the edges, or you might have weights on the edges, actually captures more general graph cut problems.
00:07:59.776 - 00:08:37.694, Speaker A: So it captures minimum SD card, multi way cut, multi cut. So it's a fairly, it's a fairly nice framework to study these problems. For example, if you wanted to encode minimum SD cut, here's your graph. The plus edges are indeed just these edges in the instance. And to encode the fact that s and t need to be separated, you just put a minus edge with a super large weight. And so you really have to separate these guys. The question is, how many of the, of the positive edges do you need to separate? So it's a nice framework that captures a lot of the graph problems that we know and love.
00:08:37.694 - 00:09:17.556, Speaker A: All right, so I'm going to talk about these clustering problems with local objectives instead of global objectives. Going back to the first problem that I spoke about, in all these problems, we usually sum up the number of errors that we make. What if we were interested in minimizing the worst error over all the vertices? Okay, that's the kind of objectives that we'll talk about. And I'll talk about, well, if I have time, I want to talk about our results for complete graphs as well as our algorithm for general graphs. I'll say a little bit about the proof. Maybe if we have time, we'll see. I'll give you some open problems.
00:09:17.556 - 00:10:31.858, Speaker A: Okay, so as I said, usually for these problems, we think about a global objective where we're summing up the number of mistakes. What if instead we say we want to minimize the worst mistakes over all the vertices, okay, and that's the local kinds of objectives that we'll be talking about. So these kinds of problems were introduced very recently by Polio Milankovic in 2016, just last year. And they were interested in correlation clustering with local guarantees because they, you know, okay, so just to give you, just to belabor the point, here are two clusterings of the same same instance from a global perspective. These make the same number of mistakes, but this one is a little better than this one, because in this one, this particular vertex has two mistakes, whereas this one has three mistakes. Okay, so local objective obviously is different from our global objective. So Pullio and Milankovic introduced this set of problems, and they were interested in a very practical motivation.
00:10:31.858 - 00:11:25.016, Speaker A: They were interested in detecting communities in this situation where they had no antagonists. And what they meant by this was there are no vertices, there are no entities that disagree with their own cluster too much. Okay. They're also interested in using this for recommendation systems where the number of mistakes that you make for a vertex is related to the quality of the recommendations that you'll be providing to this particular the person who's encoded by that vertex. And so, in order to make sure that nobody is too badly off, you want to try to minimize, optimize this local objective rather than a global objective. What's nice about these local version of correlation clustering is that it actually encodes a whole bunch of other problems, like this Min max SD card that I defined. Min max multi card.
00:11:25.016 - 00:11:54.574, Speaker A: Min max multi way cut. So our interest on this was twofold. One was to understand the approximately of these problems, but also it seemed that these are very natural variants of well studied optimization problems that for some reason haven't really been studied before. To the best of our knowledge, people haven't really looked at these questions before. And so we were curious to see how well we can do for these. So that's the problems that we're talking about. So let me tell you what's known.
00:11:54.574 - 00:12:20.360, Speaker A: So it's known that this problem for complete graphs, and hence for general graphs, is NP hard. Polio and Melinkovic gave a constant factor approximation for complete graphs, and that's it. Completely unweighted one. All weights are the same. Yes, yes, all weights are the same. Okay. And that's the status of the problem.
00:12:20.360 - 00:12:55.092, Speaker A: Okay. They also gave integrality gap for general graphs. So, all right, so we're going to give a simpler and a better constant factor algorithm for complete graphs. I'll give you an algorithm for general graphs, which will answer this question somewhat. And I think that's all I'll have time for. Okay, so the algorithms themselves are very simple. I think if I do this properly, I should have time to walk you through some of the main, some of the key ideas in the analysis.
00:12:55.092 - 00:13:33.766, Speaker A: I just want to introduce these problems, say that these are nice, fun problems to think about. And there are some very basic questions in here that would be nice to have more people thinking about them. Okay. All right, so how do we get an algorithm for complete graphs? We're going to use an LP. We have to, this is the workshop on continuous discrete optimization with continuous relaxation. Okay, so we're going to set up a linear program which measures the number of mistakes that we make at every vertex. Okay? And in this linear program, we're going to encode the partition, the cut by a zero one metric.
00:13:33.766 - 00:14:03.524, Speaker A: Okay? So a distance of one indicates that these two guys are partitioned. A distance of zero means that these two guys are in the same cluster. So in order to measure the mistakes at u, we've got to sum up over all the edges incident on u and say which of the plus edges have a distance of one, which of the minus edges have a distance of zero. That's what we're summing up. That's the number of mistakes that we make at U. And now we want to minimize the maximum of these Du's. Okay.
00:14:03.524 - 00:14:22.094, Speaker A: Of course, these distances, duv, have to be a relaxation of a clustering. The standard way in which we do that is we say that this is a metric. It satisfies triangle inequalities. So that's a relaxation. It's a very simple. Relaxations of this kind have been used for correlational clustering. Okay.
00:14:22.094 - 00:15:03.054, Speaker A: All right, so how do we use this for our local problem. So, you know, one hurdle you run up against is that there are many algorithms for correlation clustering, many different approaches and quite a few of them have used randomized rounding. Okay, it's great, it works really well for global objectives, but very hard to make it work. In fact, seems impossible to make it work for local. Okay, so these randomized rounding approaches do very well from the global perspective, but they could have individual vertices, could be very badly off. Okay, all right, so we give a deterministic algorithm, a very simple one. Let me tell you what it is.
00:15:03.054 - 00:16:04.974, Speaker A: So we solve our LP, we get our distance function and now we're going to do the following. We're going to look at a one seven ball around vertices and pick that vertex that has the maximum number of vertices in its one 7th ball, okay? And then we pick this guy and pick a radius, three, seven, something a little less than half around it. That's our cluster, take it out and recurs. Okay, so it's sort of a curious algorithm and this algorithm actually gives you seven approximation. Okay, I'll try to give you a flavor of the analysis enough so that if you're really interested you can go back and read it or you could even reconstruct it. Okay, so we're going to take the algorithm's cost and remember there's a cost for every vertex. So you're interested in minimizing the maximum guy.
00:16:04.974 - 00:16:47.614, Speaker A: We're going to compare the mistakes that we make at a particular vertex with the mistakes that the LP made of that vertex and bound. The two show that they're within a factor of seven of each other. Okay? And if we show that we're done. All right? Okay, so what do we need to worry about? We need to worry about mistakes. And the mistakes could be minus edges that are sitting inside a cluster or plus edges are going across cluster. Okay, now minus, you know, one thing to keep in mind is the following. If for an edge the LP has already paid a lot, then we can afford to make a mistake on this guy.
00:16:47.614 - 00:17:26.814, Speaker A: Okay, so plus edges where the LP pays more than one 7th, very easy to deal with. Minus edges where the LP pays more than one 7th, very easy to deal with. It's the edges where the LP makes very few mistakes that we have trouble with and those are the ones that are really going to bother us in the analysis. Okay, so let's worry about negative edges inside the cluster. Well, remember that the radius of our cluster was little less than half. So no negative edge that's completely inside the cluster can have distance that's close to one. So any negative edges inside the cluster, the LP has got to have paid a lot for it, at least one seven.
00:17:26.814 - 00:17:43.794, Speaker A: So we're okay. Okay, that's not a problem. Okay, let's worry about the positive edges. This one is a little more interesting. So let me give you a little flavor of how we handle this again. The long positive edges are easy. The short positive edges are the ones that we have trouble with.
00:17:43.794 - 00:18:21.934, Speaker A: So I'll give you one case that you should be worried about. And this is what if you have a vertex U that's sitting outside here, it has a lot of short positive edges that are going inside this cluster. How could we possibly bound the cost that we incur for that particular vertex U? What's more troubling is that this vertex u could accumulate cost for multiple iterations of our algorithm. Every time you pull out a cluster, you could have additional cost on this guy. Somehow. We need to deal with this. All right, so here's how we deal with this.
00:18:21.934 - 00:18:51.358, Speaker A: We're going to say, well, let's look at all of these guys who are in this one 7th ball. There could be a number of these guys. So all of these blue guys here could contribute one to use cost. And this is the problem. Well, remember, the way in which we picked our initial cluster was we said, let's pick that s star which has the maximum number of guys winning. It's one seven. And this is going to save us.
00:18:51.358 - 00:19:39.014, Speaker A: Now what we're going to do is we're going to, for each of these blue guys, we have one of these purple guys. And what we'll do is we'll look at this edge uv, but also this edge uv star. Now, this edge uv star is kind of in a no man's land, okay? It can't be very small and it can't be very large. It can't be close to zero. It can't be close to one. No matter what, the LP is going to have paid a lot of cost for this uv star edge, okay? If it's a plus edge or it's a minus edge, the LP is going to have paid a large cost for this and that's going to save us. Okay? So for every mistake that we made, there's something else that the LP paid a large amount for.
00:19:39.014 - 00:20:22.386, Speaker A: Okay? So that's essentially the analysis. All right, let me switch gears and say, how do we handle general graphs? Okay? So with general graphs, we have a problem. There's a very simple example that has a large integrality gap. For general graphs, you just take a cycle consisting of positive edges, one negative edge give weight, you know, distance one over n to each of these guys and one minus one over n to this guy. In a fractional sense, every vertex has two over n mistakes incident on n. But of course, integral, we make at least one mistake. Okay, so we're going to do a simple modification.
00:20:22.386 - 00:21:03.464, Speaker A: Okay, for now, just think of all the costs as being either. If the edge exists, the cost is one. We want to deal with general weights, but let's not worry about that. Let's just take max of the lp cost and one use that as our lower bound, and we show that this lower bound is within a factor of root n of the right answer. Okay? Okay, so I'm going to tell you how you get a root n approximation based on this lower bound. Okay, so we solved the same lp as before, not complete, same with unit weights. Well, we can handle arbitrary weights, but right now just think of our unit weights for simplicity.
00:21:03.464 - 00:21:28.544, Speaker A: Okay. All right, any questions? Lower bound is LP, max of lp and one. Yeah, max of lb and one. Okay, because you're scaling with the weights as one. Yeah, because I made this assumption that the weights are one. Okay, since everyone's worried about weights, let me just tell you, you can handle general weights very easily. It's odd to have max edge.
00:21:28.544 - 00:22:08.124, Speaker A: Okay, we're going to guess the maximum weight edge that you need to make a mistake on, okay? And once you throw that in, you can come up with a bug. All right, so we solve this lP. Now, we have very, very modest expectations. We only want a root n approximation, okay? Now the only edges that we need to be worried about are the edges for which the lp pays very little less than one over root n. All the other edges we're fine with making a mistake. Okay, so if you have a positive edge whose length is more than one over root n, we're just going to remove it. We can afford to make a mistake on this guy.
00:22:08.124 - 00:22:46.718, Speaker A: If you have a negative edge whose length is very close to one, we can remove it because we can afford to make a mistake on this guy. So what are we left with? We're only left with edges that the LP length is less than one over root n, positive edges and negative edges where the distance is one minus one over root n. Okay, very close to one. We're going to work with this graph. These are the problematic edges. Now what we're going to ensure is that in fact, all these long negative edges, we're definitely going to separate their endpoints for sure. Our algorithm is going to proceed in that way.
00:22:46.718 - 00:23:25.382, Speaker A: And now we want to somehow make sure that we don't make too many mistakes on these short positive edges. Okay, so let's look at one such pair that's separated by this long negative edge. Let's look at our graph of these short positive edges. Let's ignore the LP lengths. Let's just think about the graph length. Shortest path in the graph by design. The shortest part to get from here to here is length about root n by design, because each, remember each positive edge had Lp distance one over root n.
00:23:25.382 - 00:24:14.264, Speaker A: The negative edge had Lp distance very close to one. So we just do a breadth first search, do a breadth first search from this guy to this guy and look at the lengths, the number of vertices in each layer in the first half of bread. First search, find three layers such that each of these layers has less than about root and vertices. You can show very easily that such a, such three layers must exist. And now choose your cluster to be everything up to and including the first of these three layers. Okay, somewhat weird algorithm. That's the algorithm and the claim that this gives you a root and approximation.
00:24:14.264 - 00:24:59.048, Speaker A: Okay, all right. It's not very hard to show, you know, again, you need to worry about, well, we remove this cluster and this, recurse the remaining graph. Okay, already hard to show. All right, so, you know, what do we need to worry about? You can convince yourselves that this graph and this method ensures that all the negative edges that are very close to one are never inside a cluster. And the reason that happens is because, look, the way in which we designed our cluster, we took radius of slightly less than half. There's never going to be a long negative edge in here. What you should be worried about somehow is these positive edges.
00:24:59.048 - 00:25:41.944, Speaker A: How do you make sure that the positive mistakes are no more than root n? So, well, first of all, these guys who are inside here never have any of their positive edges being cut. No problem. These guys who are on this boundary in this first of these three layers, they could have their positive edges being cut. But where do these positive edges go to? Well, these positive edges just go to the next layer and we ensure that the next layer has flu tan vertices. No problem. Okay, now what about the guys who are just outside? Well, they could get affected multiple times, right. So this guy, something out here didn't get picked.
00:25:41.944 - 00:26:26.444, Speaker A: It lost a whole bunch of its short edges. But this could happen over and over and over again. And this is a problem, right? Well, not exactly. Why? Remember, for every guy who's in this middle layer. The only vertices that it could lose as neighbors are vertices here and here. In the future, additional mistakes that you will incur for this guy are only these guys and their atmos, root and other. So for everybody who's just outside, not included in our cluster, we have a guarantee that the total number of mistakes we're going to incur is root n.
00:26:26.444 - 00:26:51.046, Speaker A: And so we're done. So that's the algorithm. Very simple algorithm. In fact, this is the best algorithm we know for this min max SD card. Okay, all right, I actually walked through the analysis. This is exactly what I said. Now, even though I made this assumption that all the distances are one, all the weights are zero or one, you can actually generalize this for general graphs.
00:26:51.046 - 00:27:26.624, Speaker A: Okay. It's not very hard. Okay, so open questions, I think this is one of the most tantalizing open questions. Just how hard or how easy is this min max SD card problem? Can you get improved approximation algorithms? Can you improve r seven? Can you improve our root n? Root n is the one that we have no idea. I have no idea that root n is really the right answer in terms of low ones. The only result we know is. Okay, so we know of hardness for this min max problem.
00:27:26.624 - 00:27:47.150, Speaker A: For general correlation clustering, it's been reported to us that there is np hardness for min max SD card, although I haven't actually seen the manuscript, so I don't really know. But that's it. So the problem is wide open. It's a natural problem. I hope some of you will want to think about it. Thank you. Thank you very much.
00:27:47.150 - 00:28:07.114, Speaker A: Thanks to the organizer. All right, any questions? We're clear. So we just have to solve the problem. All right, off to lunch. Thanks for organizing. Thank you.
