00:00:33.164 - 00:00:38.664, Speaker A: Most of them, no.
00:00:40.644 - 00:00:43.292, Speaker B: She'S not part of the program.
00:00:43.388 - 00:01:32.614, Speaker C: And furthermore, she's got to run it up today. He needs no introduction. I won't talk about myself.
00:01:34.234 - 00:02:17.254, Speaker B: Thanks, Clay. Okay, so I'm showing you. Usually, I show this picture to show people how Berkeley looks, but I show you this picture to see how the weather usually looks. So, yeah, so this is a list of neuroscientists. Maybe Otto Roessel is not known for publishing neuroscience, but he also did. And this is kind of an incidental observation, but it's nevertheless interesting. So this group of neuroscientists, they have something in common in their past.
00:02:17.254 - 00:02:57.310, Speaker B: So they all, at some point, actually operated on the air, so they had kind of a station identification. It was actually not easy to get back in the day, so you really needed the permission by the telecommunication ministry, and so you had to undergo an examination and so on. But it's very hard to get a picture of actually a real neuroscientist at the radio station. So this is King Hussein as a stand in. And so you basically had a small. You had a small table of equipment, and you could reach every point on the planet without any man made infrastructure. Also, Bart Ehrmantraut was a huge.
00:02:57.462 - 00:02:59.342, Speaker C: Bart Ehrmantrawet huge.
00:02:59.478 - 00:03:39.966, Speaker B: Thank you. I have to add to the list, this is just. I found this out kind of coincidentally, somehow, over the years, but, yeah. So what might actually have shaped the way of thinking of these people somehow? And if you read David Huber's last book, he really mentions the open source community. So he basically said he learned electronics from radio magazines, basically. And I think there's a similarity here also with neuroscience. In neuroscience, you don't have huge, consumable products and numbers where, really, industry immediately kicks in.
00:03:39.966 - 00:04:30.824, Speaker B: So there are things like the Mesoscoba genealogy and the neural pixels and some optical imaging techniques where it's really an open source hardware and open source community to learn from each other. So that's definitely one thing. And the other thing is definitely the communication aspect of neuroscience. And this, of course, has to do with brain machine interfaces. And, for example, Bill Adey, after he built his first transmitters in Australia, he built an EEG machine, but it also involves investigating communication in the brain. And if you think of the brain doing computations with distributed elements, communication is computation. So it's immensely important, and it's an aspect that I will basically address today.
00:04:35.884 - 00:04:36.660, Speaker D: It's online.
00:04:36.772 - 00:05:12.018, Speaker B: So, tali, sorry, I forgot your suffix. I don't know how they define it. I mean, this was my high school. Well, you told me about it. So this is suffixed anyway, so I will talk about four projects. One is, the first two use information theory as a tool, and the last two use more ideas from compressed sensing and sparse coding. And the first is really investigating information transmission through an LGN relay cell.
00:05:12.018 - 00:06:24.784, Speaker B: The second is then asking, how can we come up with information measures that we can use in recurrent networks to tell you something about information processing and information integration in recurrent circuits? And there was some nice progress in the last years, leading to some nice measures one can now estimate. And then the short thought part will tell you something about the compressed sensing hypothesis for cortical communication that we put out a few years ago. And this idea also led then to a brain computer interface idea, and it led to this, decoding the phase structure in hippocampal theta oscillations, which is a traveling wave, as you have seen a lot, and you can really decode it. And I think there is not many examples here, so that's kind of interesting. And maybe a model how we can do more decoding. So, yeah, information theory, actually, the right picture here shows Claude Shannon with his artificial mouse. So, Claude Shenin also was very interested in cybernetics and actually biology.
00:06:24.784 - 00:07:21.484, Speaker B: And soon after he wrote his book in 49, there were a lot of people thinking neuroscience in terms of information theory. And I just listed the most important as an earlier wave, really, in the cybernetics community. And then Rob de Ruyter and Bill Beer really let kind of the analysis of channel capacity to new heights and really did a lot of very impactful work in the nineties. Also, I would like to introduce the information geometric view of mutual information. And this field of information geometry was started by Sun Ishiah Mari, who is also very well known to neuroscientists. He did a lot of work in, theoretically, he's one of the pioneers of theoretical neuroscience as well. Let me just describe you very quickly.
00:07:21.484 - 00:08:26.764, Speaker B: So, basically, if you want to compute mutual information between two variables X and X Tilde, the way you can think about this is that you, you have a joint probability of these two variables, and you can basically ask, you can come up with a null model, which assumes independence. So the q is just the product of the marginals. And now, basically, all these null models form, in general, a manifold in the space of probability distributions. And you can look for, basically the one which has the minimal kullback lipo divergence. And this is basically exactly a definition of mutual information. I'm introducing this because we will later use it for defining integrated information. So in the first project, we collaborated with Judith Hirsch, who does these recordings in thalamic cells, basically in vivo.
00:08:26.764 - 00:09:00.538, Speaker B: She patches onto a single thalamic cell and recalls intracellularly from that cell. And you get kind of recording traces like this. So the spikes are warping and huge. But you can also see here these smaller things. And I should also say that this is worked by Shin Wong, who was a student, a PhD student in Judy's lab. Then he went actually to Terry's lab and now he's at Nirvana intel. Forgive me, this irrelevant question.
00:09:00.586 - 00:09:02.254, Speaker D: Was that a mouse or a cat?
00:09:03.474 - 00:09:36.222, Speaker B: This is. Yeah, it's a cat. Yeah, this is a cat. And actually the data I will show you are from cat, but Judith does now similar experiments also in ferret. So very visual animals, without any doubt, and mice, which some people, but we have heard very good evidence that mice are visual too. So she does now also study other species, but this is cat. And so basically what you can do with this data is you can do something like spike solving.
00:09:36.222 - 00:10:11.150, Speaker B: You can really detect these little uprises here. And there are putatives, epsps. So these are the synaptic inputs to that cell. And they are really big in the relay cell. In a thalamic relay cell, they are so big that you can even measure them extracellularly. This is the so called s potentials discovered by Kaplan et al in the eighties. And this is also, this makes this very interesting.
00:10:11.150 - 00:10:46.100, Speaker B: So potentially one could actually really scale up these studies now using some voltage die methods also. So for that reason, I think this is very interesting. But in this case, we just listen to one cell. These are the outputs so that we listen to the red cell, and the red spikes here are the output spikes, and the blue spikes are the input spikes. And now, of course, one can just do. Yeah, and so now what I also want to do is. So it's similar to what Christian told us yesterday.
00:10:46.100 - 00:11:22.316, Speaker B: You listen to two subsequent stages in the feed forward path of visual processing. But here this is an earlier stage. This is basically the output from the retina to this first waystation, which is the LGN. The receptive fields, as we already heard, are kind of centers around receptive fields like Kuflov reported in the fifties. And then there's also a temporal kernel. And so if you. So this is basically times goes like this.
00:11:22.316 - 00:12:36.144, Speaker B: So you have kind of here a temporal kernel and you have a very small receptive field which has, sorry, it's pretty dark, a center surround structure. And so you can now use the probability that it's pretty localized in spatient time to actually estimate the mutual information between a stimulus and a spike by saying, okay, what I need to know about the stimulus is basically adjust the projection. So s of t is a stimulus time, a projection operator b onto this relevant subspace of the cell. And then you basically can plug this in a formula that was proposed actually by BLX Group a long time ago. And you can actually measure the mutual information or the information rate in for example bits per spike. If you just do that independently on the input and the output, then you see, and many people reported that in the early two thousands, you basically see that the information content in the LGN spike is higher than the red milk spike train. And this also the main reason for that is that the rate is much lower in the LGN because of course there's the information process in inequality, so the information can only go down.
00:12:36.144 - 00:13:37.502, Speaker B: And so basically what Jin did now is that he, he determined, just using white noise stimuli and so on, he determined the relevant subspaces in the stimulus space and then he actually compared them. And so basically this is now just vectors in the directions of these receptive fields. And so again, the convention here is input is blue, output is red. And so you can see in this, this is just a cartoon example, but here they are not exactly the same. And so you can basically now project the kind of the output feature onto the input feature and you get basically a feature which is a gain feature. So there seems to be something encoded about the input, the visual input in the output which is not encoded in the input, which is strange. And I will resolve this.
00:13:37.502 - 00:14:40.532, Speaker B: But, and so you can now basically model the spike drains by decomposing the stimulus features of the joint pre and postsynaptic space. And then for example constructing LNP models. So here you have the linear receptive field, you have maybe a nonlinearity and some spiking mechanism. And so what comes out from this is actually that just jump here to the. So if you just stimulate with a gaussian ensemble, white noise stimulus, the black and centered cloud here is the unconditional stimulus. And you can now see that the conditional stimuli, the conditional stimulus distribution conditioned on either the input in blue or the output on red, is now a somewhat shifted cloud. And it's also that the red and the blue do not coincide.
00:14:40.532 - 00:15:05.920, Speaker B: So they are some somehow different. Yeah. So there's something I don't quite understand. So there's only one input to this dalanic cell from the resin? Yes. So, well, we do not really do cell sorting. So it's basically all the detectable inputs, but these are only these unitary inputs that we can really detect. So there are some other effects, for example, inhibitory effects on the cell.
00:15:05.920 - 00:15:11.912, Speaker B: That's true. And they are not. So you cannot detect in a relay cell. Ipsp's.
00:15:12.088 - 00:15:15.240, Speaker C: There are often more than one.
00:15:15.352 - 00:15:44.492, Speaker B: Absolutely, exactly. So this is not necessarily a spike train from just one cell, but very often it is. So it depends a little bit. We can try to do clustering of these whole cell intercellular blips. And sometimes there's a clear distinction. Sometimes it's not that clear. But this is a set of cells where there was kind of one dominant input, but it's not guaranteed.
00:15:44.492 - 00:16:18.774, Speaker B: We don't have a guarantee that it's just one input. In the top left is what is actually input to. Yeah. So this is actually for. Yeah, for measuring the spatial properties he designed. This is kind of also gaussian in the gray level space, but it's kind of taken into account that we know it's more or less centers around. But what's important here is that the change in the feature and the coding feature that you see is the bigger difference between input and output is actually in the temple.
00:16:18.774 - 00:16:41.616, Speaker B: In the. In the temple. So the correlation between the temple input and output shape is much smaller. So this is now averaged over the cell population of the 15 or 20 cells that they actually examined. And the spatial receptive fields are very similar. And this actually led people to say, oh, yeah, I mean, the LGN is just a passive relay. Nothing happens there.
00:16:41.616 - 00:16:55.874, Speaker B: But it does happen. Something here, actually, in the temporal kernel. And you can see that the. The output kernel becomes longer in time, so more memory and more biphasic. Great question.
00:16:55.954 - 00:17:21.794, Speaker C: Comment. Yeah, you asked that question about one, two and three. Everyone who cares about Khan atomics should know about the beginning of mammalian Khan atomics, which is this man here in 1983, Rajkowski and Sherman studying that problem reading. It's beautiful. Is that reconstructing a single input. Yeah, yeah.
00:17:22.894 - 00:17:55.414, Speaker B: Great. Thank you. Can you explain a little bit the scatter plot? So what I don't understand is that the gain feature, by definition, shouldn't be absent in the presynaptic signal. Yeah. And it pretty much is. I mean, the main shift here, I mean, it's getting a little bit elongated. So in the second moment, something is happening, but it's mainly shifting on this blue line, and the red is kind of here deviating above, going in this direction.
00:17:55.414 - 00:18:00.042, Speaker B: So, yeah, it's hard to see, but there should.
00:18:00.098 - 00:18:01.774, Speaker C: Is this spikes or cells?
00:18:02.114 - 00:18:44.506, Speaker B: No, no, this is the spike triggered stimulus ensemble. So basically, every time there was a stimulus that triggered a spike, this gives a dot. Okay, one cell, one cell, one cell. Okay. But yeah, let's not. Let me get through that. So what you can do now in this plane of the presynaptic feature and the gain feature, you can now basically also ask what is the information in different directions here on this plane? And you get these dumbbell shaped curve, and you can see there is this tilt from the input to the output.
00:18:44.506 - 00:19:32.854, Speaker B: And you can also see there is these sectors that are white where the information rate goes up. But then there's also the black section where actually the information rate goes down. And so this is of. And so, I mean, the biggest puzzle is, okay, where's this information coming from? The information processing inequality is violated here. There's some information about this gain feature in the output, which is not in the input. And so now the resolution of the puzzle is actually to look into the higher order structure of the spike train. So the next step is kind of to ask the question, what is actually the code? What is the language?
00:19:33.434 - 00:19:35.130, Speaker A: There's no feedback here.
00:19:35.322 - 00:20:08.130, Speaker B: There's very little feedback. I mean, this is an anesthetized animal and of course there's a lot of feedback. Synapses, as we know from the anatomy, but they are hard to see in the physiology. So all the hallmarks of these unitary epsps that we can cluster or sort, they have retinal ganglion cell hallmarks. So they look like spike patterns of retinal ganglion cells. So this is, you can think of this as a feed forward path. And.
00:20:08.130 - 00:20:48.994, Speaker B: Okay, and so the synergy is now basically the question, if you compute, what's the information in a spike pair of a certain latency, is this just two times the single spike information or is it more? If it's more, we say it's positive synergy. If it's less, it's kind of a redundancy in the code. And so this is now basically showing you the synergy as a function of this interspike interval. And you can see in the output it's pretty flat. So there's no synergy. This is a single spike code. But in the input, you see actually, for small latencies, you see that there is a significant synergy.
00:20:48.994 - 00:22:00.750, Speaker B: Now, all the 20 cells or so you see, most of the cells are hidden behind the mean red line. But in the inputs, you see there is significant synergy. And what you can now do is you can ask, okay, is there a transformation happening maybe in this neuron that actually takes sharp ISIS and transforms them into a single spike? And does this actually account for this gain feature? And the answer is yes. And so basically you can set up a model where you say, okay, the null model is just a model where you assume the firing probability is independent of the previous ISI, but then you can actually fit a model to the spike trend of the cell that really has here this increase for short, ISIS. And you can now basically model the observed psths. And so you can now see, you can now compare this model that actually has this effect. This is called paired spike facilitation has been reported by Marty Ashri and others.
00:22:00.750 - 00:22:33.342, Speaker B: So this is definitely something that people have seen before. We see it here on this one particular cell and fitted. But if you now actually look in how well you can fit the PSTh respectively of these cells. So this is time and this is basically just the PSDH size here on the y axis you can see that the null model. So the blue is the real data and the red is the models. No, sorry. So here is the model, here's the true data, and here's the null model.
00:22:33.342 - 00:22:48.794, Speaker B: And here is actually the ISI model. You can already see that the ISI modulated spiking mechanism actually does a much better job in reproducing here. The red line above. Yes. Could you go one back?
00:22:50.374 - 00:23:13.574, Speaker C: So that's an extremely sharp transition at zero there. Zero interspike interval. Are there any ideas what physiologically could cause that? I don't know, calcium in the presynaptic terminal. No, you cannot have zero inner spike interval for lymph.
00:23:14.314 - 00:23:14.914, Speaker B: Right.
00:23:15.034 - 00:23:16.338, Speaker C: Flat is the baseline.
00:23:16.466 - 00:23:17.674, Speaker B: Flat is the. Yeah, yeah.
00:23:17.714 - 00:23:20.218, Speaker C: So I mean, before zero is just the baseline.
00:23:20.306 - 00:23:21.346, Speaker B: So it's going to be flat and.
00:23:21.370 - 00:23:27.594, Speaker C: Then immediately afterwards it's postsynaptic summation plus asynaptic.
00:23:27.754 - 00:23:45.668, Speaker B: But you know, this is 50 milliseconds here. So the resolution here, what happens at zero and one millisecond ISI? Yes. Del, you know, in a lot of these paired recordings we do see really the refractory period observed, but not always. And this is then when we assume there's more than one input.
00:23:45.756 - 00:23:50.052, Speaker C: So how long does it go down to zero? You see, it's.
00:23:50.148 - 00:24:23.494, Speaker B: Yeah, yeah, yeah. I think, you know, the spy grades of salt, we could not really. Yeah, it's interesting. Okay. Anyway, but let me wrap this story up. So basically what we can now really do is kind of also reproduce these information plots and we can kind of, if you put this paired spike facilitation model in, you see this tilt of the red dumbbell, and you can also. Yeah, you can basically.
00:24:23.494 - 00:24:59.046, Speaker B: So now let me just go to that slide. So, basically, what you can see is that in the. So this is the data versus the null model. And so you can see that the data has actually higher information rate than the Nile model. But you can see if you actually use this paired spike facilitation model, you come closer here to the identity line. So it's still not a full explanation of everything. So it's still systematically a little bit below, but it follows the line pretty well.
00:24:59.046 - 00:25:52.114, Speaker B: So let me just sum that up. So, the old idea of what the LGN does is really a relay that relays actively this information, but does not change very much the language or the content. And what we can see is really that first the language is changed. So the RGC spike trains are synergistic. I can't tell you why, but you have to kind of separate, if you want to decode the code, you have to separate short ISIS from single spikes to do the best job. And the LGN spike trains, in contrast, are entirely single spike. And of course, this makes a lot of sense if you think that LGN spikes now are integrated like what Clay described to us in subfields of simple cells.
00:25:52.114 - 00:26:06.974, Speaker B: You don't want to require a mechanism that tells apart short ISIS from single spikes. You just want to sum up. So it makes sense that if synergy is there, you want to remove it before you start, really, to integrate and build more complicated receptive fields.
00:26:07.274 - 00:26:12.294, Speaker C: So it seems wasteful to me to have redundant spikes.
00:26:12.334 - 00:26:13.270, Speaker B: Yes, that's right.
00:26:13.422 - 00:26:14.918, Speaker C: Because you're going to throw them away, right?
00:26:14.966 - 00:26:41.310, Speaker B: Yeah. Yeah, it's a good question. But, you know, there is more to the story. I will not go into this, but there is also ongoing activity in the retina. There is periodic activity in the retina. And so this is not the entire story. So I just give you this as an example how you can actually, if you are able to record from subsequent stages, you can tease a part.
00:26:41.310 - 00:26:55.954, Speaker B: These two questions. How does the language change and how does the feature change? And I think that is a very interesting application of just Shannon information. There could be some sort of error correction if you really want.
00:26:57.094 - 00:27:36.544, Speaker A: So, Fritz, if you go to the next step to v one, then there are dozens of papers from the time period you were talking about before, in the heyday of information period, like Lance Octagon, Barry Richmond, all those people, and they computed the information content in different spikes. And of course, the most was in the first spike of a short sequence, but there was non trivial information in the second, and if memory serves correctly, even some in the third. And then after that, there wasn't much information. So there's something of a difference in v one, at least, by having two, maybe three spikes as opposed to one.
00:27:37.404 - 00:28:07.624, Speaker B: Well, okay, so there's another also, we also have stories. There's actually also bursts. I'm not talking here about bursts. There's bursts in Regina and there is bursts in the LGN and to elicit them. And of course, Murray has. All the early data is from Murray on that. And also he has theories on that, that this is something like, could be a wake up call, because usually you need dn activation of calcium channels to actually get the burst in the LG.
00:28:07.624 - 00:28:53.320, Speaker B: And so the idea could be if after a long period of kind of nose stimulation stimulus comes in, the bursting is actually a burst. And that's actually very interesting also in terms of communication. If you deal with asynchronous communication, you might have signals like wake up calls, handshake, and you might actually afferent copies and all these things, because it's not clear. It's an asynchronous parallel computing system. So stuff might need to be communicated that you first do not think about. But, yeah, so there's more to this. Let me leave it by that right now and just move on.
00:28:53.320 - 00:28:54.320, Speaker B: Because it's interesting.
00:28:54.352 - 00:29:04.576, Speaker C: You brought up this low threshold calcium current in the LGM, which you kind of ignore it because it produces bursts. Right. Thalamus has two single spike mode.
00:29:04.600 - 00:29:36.256, Speaker B: That's right. So, okay, so we also. And actually, Jin also did beautiful studies on that. And so the 1 minute answer to this is, you see, under naturalistic stimulation, you do see bursts. They come quite consistently because it's in these, because you have this one over abstract temporally, also in natural scenes. So for a long time, often a receptive field is not stimulated, and then it gets stimulated. These are the instances where you reproducibly get the bursts.
00:29:36.256 - 00:30:14.940, Speaker B: They are relatively rare, most of the. But, you know, more than 90% of the spikes are single spikes, but they are fired at pretty consistent temporal locations in natural stimuli, which is very interested. And then, I mean, Sheen went on and study really this calcium mechanism in detail because he had so wholesale recordings. And so we looked at inhibition that you cannot detect by just doing these unitary event detection things. We tried to reconstruct that and really figure out also what. What's the mechanism of inhibition that actually is important for this inactivation mechanism, sociologic.
00:30:14.972 - 00:30:22.904, Speaker C: Is always also interesting, given the theme that we've had. We've heard that he's now working for a hardware company that's building deep learning chips.
00:30:24.124 - 00:30:24.864, Speaker B: Right?
00:30:25.564 - 00:31:03.504, Speaker D: Yeah, but I have to put the alternate hypothesis on the table. All the evidence for siccards suggest that they're to answer a very specific question that's known before the Sicard lands. And so an alternate way of thinking about this is that in the cortex, there are multiple processes competing to read the thaumas. And so basically, it's more like a polling mechanism for these things. And so rather, as opposed to this feed forward or count, where you're just trying to preserve the information.
00:31:03.964 - 00:31:40.906, Speaker B: Yeah, I mean, if you look into the data in the physiology, to see the feedback is not easy. I mean, there was beautiful work by Faran and Marty Ashri, who studied that ozone monkey. It's very tricky to see the feedback. Also, we know of the anatomical feedback to the LGN. That's the first station where you actually have really no fibers going back. There's nothing going back to the retina, but there is two lg, and then it's. But it's actually, as a neurophysiologist, you are hard pressed to see much of much evidence for the feedback.
00:31:40.930 - 00:31:48.730, Speaker C: It depends on the conditions. I mean, if you're just giving it a spot of light, that might not be the right state to be in.
00:31:48.802 - 00:31:58.124, Speaker B: Absolutely. And all the data have been talking about was anesthetized cat. But, I mean, Faron studied a vague monkey.
00:31:58.744 - 00:32:07.764, Speaker D: Just point out that you've written one of the seminal papers that shows that the oscillations in the retina are in phase with the thalami.
00:32:08.224 - 00:32:17.256, Speaker B: Yeah, but this is not a feedback mechanism. I will not talk about this. I will not talk about this. Now let's move on.
00:32:17.400 - 00:32:30.976, Speaker C: But actually, you know, you're saying that these feedback projections have weak influence. That's not true. When the animal falls asleep, what happens is that the cells hyperpolarize and then those low threshold currents actually dominate. Right. That's where.
00:32:31.000 - 00:32:39.232, Speaker B: That's where the sleep spindles. Yes. So instead, brain state changes. Definitely. Yeah. Yeah. But I mean, doing the visual, if you're just doing visuals in general, I.
00:32:39.248 - 00:32:53.456, Speaker C: Think this is something that really is confusing in the literature because. Because brain states change even in the awake state. Right. I mean, we have different levels of activation. And so it really depends the results.
00:32:53.520 - 00:32:54.744, Speaker B: You get depending on the brain state.
00:32:54.784 - 00:32:57.168, Speaker C: Which often is not really monitored.
00:32:57.256 - 00:33:31.436, Speaker B: That's right. And this is exactly a beautiful transition. To the next stage, which is really more about brain states. And so, and this is actually worked with Daniel Toko, who is here on the audience, and also Hannah, who contribute. The slide will come up later. But so the idea is other information measures that we can, in a meaningful way, apply to just recordings of activity in recurrent networks. And there were other proposals, mainly in this context of consciousness.
00:33:31.436 - 00:34:17.820, Speaker B: So there was Tonioni, who had several proposals how to measure integrated information. They were somewhat controversial because they were not always positive. That is a problem, and also very hard to estimate for reasons just in terms of computational issues. And I will bring that up. But Suni, Shiamari and Oisumi, they actually came up in recent PNAS papers, 2016 with a pretty good definition. And this kind of involves the geometric integra, the geometric information geometry somehow. And so let me just explain that briefly.
00:34:17.852 - 00:34:24.104, Speaker C: Where are you getting these handwritten diagrams here? Are they, you know, the MRE?
00:34:24.964 - 00:34:49.654, Speaker B: No, it's not, it's. No, it's not Mre. I made this for a tall in the plane, so I had just to draw things and then photograph it. So now it's not. I wish they would have these stamps, you know, of Amari lab, 1968 also. But they don't know, you know, like the Ramonica drawings. Now, I could have faked them maybe.
00:34:49.654 - 00:35:53.222, Speaker B: Yeah, but, yeah, so, I mean, yeah, you're all familiar how you can kind of unfold dynamics that goes on a recurrent network, like what people do also in deep learning, if they want to train recurrent networks. And so basically, now you can ask how information travels through time. And so you can look at two times steps, t minus tau and t, and you can basically just measure the mutual information, or you can also partition the nodes first and then measure the mutual information. And then you have basically all these information flows between the partitions, within the partitions. But now the idea is you come up with a null model that cuts this between partition information flows. And so basically you measure the Kulberg Lieblo divergence between this model, which is the full connected model, fully connected with kind of the cut model. And this actually turns out to be abounded by neutral information, positive, beautiful.
00:35:53.222 - 00:36:30.534, Speaker B: A lot of really features that we like, and this is what we are going to use. But now there's one problem. I mean, of course, for every partition, it spits out a number. That's a lot of numbers. If you have a bigger network, you can have a lot of partitions, a and b. And so what Tony only proposed is basically, well, let's just focus on this one number, which is the minimal information partition. So you measure all the numbers of this integrated information, and now you take the minimum.
00:36:30.534 - 00:36:39.610, Speaker B: And the problem is there's a lot of numbers to compute. And so this basically kept people from going beyond toy networks.
00:36:39.682 - 00:36:41.722, Speaker C: It didn't stop Christoph Koch.
00:36:41.778 - 00:37:19.684, Speaker B: No, nothing does. So the proposal that Daniel came up with is basically say, okay, let's do what people very often do in imaging and somehow connects also to the Kriegers cotter. So he did this for spatial pattern, but let's just look at correlations. But now between time courses. So let's come up with a matrix value. All the nodes in your network. You have now kind of here a correlation matrix of just the correlations of the time courses.
00:37:19.684 - 00:38:44.304, Speaker B: And. Yeah, and then basically just apply spectral clustering and come up kind of with a community clustering in this network. And so the idea is now, I mean, so one hope could have been, okay, we just look for the best community clustering of the thing. And this already is the minimal information partition. It's not quite as simple, but it turns out that, and he tested this in many different ways, that actually you can restrict the soft space enormously, and you basically find a very good estimate for the minimum information partition and then basically a number for the integrated information. But what that now means is basically that you're looking in your network of nodes between, you look for the weakest connected community partition, and then you ask what's the information flow? And again, it's the information flow in the following sense. It's kind of the, if you measure information, how information is propagated by the network through time, you ask how much do the between connections contribute? So if you cut them, how much lowers the information? Okay, so, and now we are basically ready.
00:38:44.304 - 00:39:52.790, Speaker B: So now the beauty of this is you can now do this in networks of hundreds and thousands of nodes, basically. And so now it's time to really come up with some networks where you can test, and this is then where our friend Rossler comes in. So we actually, so what Daniel did is actually he kind of had a construction process that used kind of some Hebbian wiring mechanisms to come up with networks that have some properties of conic tones. And then on the notes, basically he used Roslaustel. So it's a couple Roslausteledel network. And the beauty of the Royslaws de Ladle networks is that there are well known parameter regimes where the data is pretty gaussian. Unfortunately, we had to kind of really adhere to terrorism prayer that things are gaussian, because in order to estimate, integrate information, that was important.
00:39:52.790 - 00:40:35.006, Speaker B: Of course, we are now working also on methods that actually can do without. So now this just shows some controls. This shows now for small networks of 14 and 16 networks and 18 networks, where he could still compute brute force. The minimal information partition that you actually get exactly, if you subtract from the estimated minimal information partition, phi, the actual ground truth, one, you get exactly zero and also the rent index. So you get exactly the right partitions. Then he looked at larger networks. Again, it's a control.
00:40:35.006 - 00:41:33.590, Speaker B: It doesn't work, that it does not work entirely perfectly, but also pretty good. So he now introduced a cut by hand. So we again knew the ground truth. And so you could basically now also see how things now for larger networks, you see that some of the values actually deviate. But so this was kind of another test. And now you can, for example, you can now ask, well, in small world networks, well, in the strogarts networks, where you have this random. So you start here with a just locally connected ring, and then you introduce these cross cutting connections here with the probability p, you can basically ask, what you know, how is the geometric integrated information going up? And you see actually how it goes up and then actually kind of plateaus here once you have reached a certain global connectivity level.
00:41:33.590 - 00:42:21.686, Speaker B: But now the most interesting question is, how does this actually really apply on data? And the beauty on this is you don't need to know the graph, you only need the time series. So basically, if I give you the time series, you can kind of apply the measure and determine what's the minimal information by partition and what is actually minimal, what is the integrated information. And so this is now Ecoc data of monkey. So it's one hemisphere of monkey. So this shows you now. So these are basically all the electrodes. And this shows you basically the result that you just get if you do naive spectral clustering on the functional connectivity matrix, on this correlation matrix.
00:42:21.686 - 00:43:07.354, Speaker B: So that's not very informative. But then if you really look for the minimum information bipartition, you actually get in the two monkeys these patterns. And they, of course, really make sense. So it really splits here, the posterior sensor areas from the anterior association areas. And so this is basically, this is kind of. I mean, Daniel is not applying this to, to really understand brain states differences, how they actually express in this measure of integrated information, many other things. But this was kind of an encouraging first result that he got.
00:43:07.434 - 00:43:12.906, Speaker C: Why is it the two monkeys are so different in terms of the quantitative peaks there.
00:43:13.010 - 00:43:35.758, Speaker B: Yeah, this is interesting. It's. We don't know. We don't know. It seems very consistent in terms that these are contiguous areas. But, yeah, definitely it's. I mean, they're all also, both of them somehow contain here some temporal lobe.
00:43:35.758 - 00:43:45.548, Speaker B: I mean. Yeah, but this is all interesting and one cannot study it. I don't have the answers for this right now. Could you do it a second time.
00:43:45.596 - 00:43:47.916, Speaker C: And give me like a hierarchy?
00:43:48.020 - 00:44:14.090, Speaker B: Yeah, absolutely. You can use this like a surgery knife, basically, now. So you could, you can now go on and do this more. Yeah, exactly. So you can now look for partitions within partitions and things like that. Okay, so now, but now this is basically just from functional data. But what can we actually do? And how can we do kind of, I would not say structural function relationships, but maybe structural dynamic relationships.
00:44:14.090 - 00:45:05.306, Speaker B: How can we study those? And this is actually something that we started. So when Rolf Coethel had his first collection of the Kokomark data, we actually did a study. And this is actually how we presented this at a post in 1997. So this was basically the Cocomark database. And then we put very simple threshold elements on the nodes. And there were some old studies then in the fifties where they stimulated with strychnine somewhere in the monkey cortex and area, and then they just made a list of the areas that went active. And so what we could show is indeed, if you use the full cocoa mark data, it is better than the nearest, just the nearest neighbor connection connectivity.
00:45:05.306 - 00:45:59.974, Speaker B: You can reproduce these pedals that these guys saw in the fifties, much better. But no one was really interested in these data of the fifties because those are very rough data in a way. And so, but now, and this is actually the slide that Hannah contributed, who is here and is Simons research fellow, and she is an applied mathematician at the University of Washington. And so basically what she is now doing is basically using the Allen Maus brain connectivity atlas, and again, putting dynamic elements on the nodes, in this case Rosler oscillators. And now you basically can run simulations in this. And I just put this, we put this in because you can now judge yourself. And this.
00:45:59.974 - 00:46:18.874, Speaker B: So this is now just Rushlaw's delay. There's no delays. It uses the real, basically this connect to Matrix, and you can now judge whether you see traveling waves or not. So we are not so sure. So it's definitely not exactly looking like what we saw from Terry, but that's something.
00:46:21.984 - 00:46:24.480, Speaker C: You know, time plays would make it more interesting.
00:46:24.552 - 00:46:25.204, Speaker B: Yeah.
00:46:27.144 - 00:46:29.920, Speaker C: Henry Kennedy had a much more accurate.
00:46:30.112 - 00:46:32.164, Speaker B: Connectome yes.
00:46:34.304 - 00:46:36.248, Speaker C: And many more connections then.
00:46:36.376 - 00:47:24.632, Speaker B: Yeah, this is interesting how the Henry Kennedy Konnicktone now changes the picture that we had from cocoa. Definitely. Okay, so this was a first foray in how can you actually just compute a number for a recurrent network? And there are still a lot of holes to fill in methodologically and also how to interpret this. But I think the channel capacity in feedforward pathways solved us very well. We should think about such measures we should explore. And I think now actually, the information theory is in place for actually solid measures that fulfill all the necessary requirements to do that. But of course, estimating information is always difficult.
00:47:24.632 - 00:48:28.264, Speaker B: So there are still a lot of issues to actually address and resolve. Okay, so this kind of finishes up the information. How am I doing with time? Well, okay, so this finishes up what I want to say about information theory. Now let's get to this hypothesis, how cortical, cortical communication could actually take place under wiring constraints. So the question is now, if it seems 50% of the volume of the brain, roughly speaking, is white metal, but this is far from every, connecting every neuron with every other neuron. So there seem to be very hard, varying constraints on cortical, cortical communication. If actually these connections between areas are actually sparse rather than dense, like what we are used now in the deep learning networks, how would this actually change the picture? And how could you actually still communicate high dimensional local patterns through such fiber projections? That's the question.
00:48:28.264 - 00:49:31.144, Speaker B: And yeah, so this is this work of Guy Charles. They are grad students in Redwood center, and Chris Hillel, a former postdoc. And so how many people here are familiar with compressed sensing? Okay, so let me just give a two minutes version of it. So if you want to compress an image like this image, how many people recognize who is on that image? Yeah, so, I mean, yeah, so you can do it this way, but it might not be the smartest way. So you can just make the pixels bigger, which kind of reduces the high frequency content. But another way, which was proposed in the early two, in 2006, is compressed sampling or compression sensing. And so the idea is you basically just multiply this image with random projections and not so many just n numbers like these n calls voxels over there.
00:49:31.144 - 00:50:35.616, Speaker B: And this gives you an entirely garbled vector. But what you can do with this vector, under the assumption that there is some sparsifying basis for these images, and it turns out that's a reasonable assumption, you can actually solve a lasso problem. So basically what you can do is you produce this garbled y vector. And now you solve a lasso problem of this sort. So you ask what is the best sparse vector? So that basically multiply it actually with the sparsifying dictionary for images, I call this Psi, and then multiply it with the sensing matrix. It actually equals this compressed vector as good as possible. And then you put an l, one sparsity constraint on the sparse vector here.
00:50:35.616 - 00:51:11.836, Speaker B: So that's, you can do that. You can use this optimization to extract the sparse vector and then you just multiply it with dictionary and this gives you a reconstruction of the image. So that's kind of the idea. So in other words, you can use structure in the high dimensional space, which is now not a dimensionality constraint, but it is a constraint in terms of sparsity. So that is the original idea. And note here that the receiver needs to know the sensing matrix. And now if you think about a cortical, cortical communication.
00:51:11.836 - 00:51:19.424, Speaker B: So you have two regions here. One has, in the superficial layers, neurons that project to another region here.
00:51:21.084 - 00:51:42.416, Speaker C: So there's the cost, there's a cost of encoding and decoding, and that's true in all information theory. Modern codes are very computation intensive, but they're better in terms of reaching the Shannon limit. And if you look at your new scheme, right, you're pushing a lot of the computation into the decoding step.
00:51:42.480 - 00:52:18.362, Speaker B: Absolutely, exactly. Yeah. And so now how do we model this? So, yeah, the idea is now that these neurons basically just subsample the high dimensional. So the idea is there is lots of local neurons here and that these neurons, with their dendritic trees subsample these local and rather sparse response petals. Think of this, of v one, for example. This is basically the matrix a here. And then this is sent to the wiring bottleneck to a receiver region.
00:52:18.362 - 00:53:07.724, Speaker B: And then basically what the receiver region needs to do is to do exactly what I just described to you. It has to solve this lasso problem. The problem is, of course, you cannot expect that this sampling matrix, which describes how the dendritic arbors here connect to the local neurons, is somehow known by this receiver region. So you need kind of a learning to do that. And so this is basically now a dictionary problem. And the question is, under what condition can a receiver region, which is just bombarded with compressed signals, learn a b matrix that actually allows to produce here sparse vectors b that are by some reasonable similarity metric similar to the, to the, to the a vector here. Yeah.
00:53:07.804 - 00:53:09.860, Speaker A: So I have a fundamentals question here.
00:53:09.892 - 00:53:12.812, Speaker B: So if, if I don't understand why.
00:53:12.828 - 00:53:20.004, Speaker A: We need to reconstruct, basically, if you know from this theory that you've passing the relevant alliance information you need.
00:53:21.904 - 00:54:00.314, Speaker B: Right. So I would not say it's reconstruction. It's basically. So you are compressing information. So basically, one fiber here contains information about many different features, and it's more a disentanglement than a reconstruction. So what I am saying is, so how this hypothesis sees communication in cortex is that you go through, so there's expansion and compression stages, but you shouldn't think about this as a reconstruction, but it's getting access to the features that are mixed together in the dense representations. That's the idea.
00:54:03.094 - 00:54:03.834, Speaker C: Right.
00:54:05.014 - 00:55:11.322, Speaker B: Okay, so basically what we examined with Chris now was, okay, under what conditions? So we know the conditions of compressed sensing. So you know that if you have a certain number of, if basically your sparse vectors have a certain dimension and your sparsity, let's just say this is now an l zero sparsity. The number of non zero elements has a certain number, then we can kind of, we know that then the dimension of the compressed vector goes like the logarithm of the ratio of these two numbers. The c constant is kind of an ortho one constant. So the good news is here you can really compress. So m can be of much higher dimension than actually y. But the question is now, under what conditions can you do the learning? And so basically what we showed in this paper is that under exactly the same conditions, so whenever you can do compressed sensing, you also can do this learning.
00:55:11.322 - 00:56:07.028, Speaker B: So what that means is you have a receiver region who is entirely agnostic about the sampling matrix, and it is able up to a permutation to actually learn the sampling matrix in the lasso problems. So that actually the reconstructed sparse vectors are up to a permutation and a linear stretch the same as the sparse vectors in the center region. And so this is basically just a demonstration of this, where we did kind of the void computation. So think of this as three brain areas. And all we did here is we started to, we trained this with image patches. So the first is a sparse coding model, basically, and then we basically just compress the sparse codes do here the learning, compress and do the learning. And you can kind of see in this void computation.
00:56:07.028 - 00:56:50.364, Speaker B: Of course, the features look all similar because nothing happens. But of course, what we left out here is the local computation that happens. But of course, the premise here is that whatever the local computation, there is still a sparse structure, otherwise this scheme would not work. So, to sum this up, it's a hypothesis, how you could do communication under varying constraints. We showed that basically when abercompressed sensing works, also dictionary learning works, you can do this. It kind of makes the assumption that the input stages of cortical regions do something like sparse coding. So it's very compatible with the sparse coding hypothesis.
00:56:50.364 - 00:58:14.354, Speaker B: And again, the overall picture is that the communication might be, might include this expansion and compression alternation in somehow this work. And this brings me to my last point. This work inspires us now to say, well, if you, if we now assume the brain could do this with what is sent through these fiber projections, of course, in lFps, you should also be able to do this, and even in challenging cases, in cases where you don't have a topographic organization of the feature. So LFP decoding is quite well, works quite well and is well understood if you have orientation columns or something like that, because what the local field potential does, it sums up signals from many cells in the vicinity of your electrode, and it kind of mixes all these signals together. But if these signals kind of tell you something about a very similar feature, no problem, you will get an orientation selectivity of your entire electrode and you're done. But in the hippocampus, this is still different. In the hippocampus we know that in a particular environment, blade cells are salt and pebbles, so nearby blazes are not represented nearby by blade cells that are nearby.
00:58:14.354 - 00:58:20.954, Speaker B: And so now let me just, so this is joined vault with hippocampus.
00:58:22.294 - 00:58:39.334, Speaker C: If what you say is true, it should be possible for you to take one of these dense recordings and extract what the, you know, right? You call it a dictionary, but, you know, whatever it is that was being encoding that. Have you tried that?
00:58:39.954 - 00:59:34.584, Speaker B: We didn't, because the data, I mean, you would really need, you would have, you'd need recordings from identified neurons. I mean, it's a little bit what christian, we could start to look in data of this type, right? So you're saying you think that you are mainly recalling from the output neurons and so that, but we didn't do that. But that would be now the test. That's exactly what needs to be done. Okay, so just briefly, the hippocampus is structured deep in the brain, often in the hierarchy. Picture on the top of the hierarchy, for example, of sensory systems, systems, also deep in the brain. But yet if you are in a certain environment, you have these cells that actually represent certain locations, certain places in an environment.
00:59:34.584 - 01:00:43.416, Speaker B: In addition, what you also have, and Terry already showed some pictures, you have kind of very strong 10 hz oscillations in this behavioral state where the animal is navigating actively through the environment. And actually the blade cells really represent the places where the animal is right now rather than doing replay kind of mental traveling. And it has been known that actually, if you recall, not only spike drains of such blade cells, but also the phase of, of this 10 hz theta oscillation, that there's this phase possession structure, which means you get a little bit of super resolution. So if the red neuron starts to fire here, it starts out later in the phase of the theta oscillation. And as the animal traverses the place filled, it gets to somewhat earlier points. And so if you recall from both, you get a little bit. Hippocampus results will still debate whether this is used by the animal because you have so many place cells, you might not need it.
01:00:43.416 - 01:01:25.668, Speaker B: So it's, it's, but it's an interesting observation. And basically what we ask now and is, yeah, and so the problem, of course, with reading out now from the local field potential place information is that, is that this is the picture. So if you are somewhere with your electrode, you kind of recall signals from currents in the dendritic trees around you. But these belong to blade cells that encode very different places. And so therefore, the entire blade selectivity of your entire LFP electrode is pretty flat and it's actually hard to decode or you cannot really decode it.
01:01:25.756 - 01:01:42.766, Speaker C: So Yuri Buzzaki recorded lfps along the hippocampus and was able to decode location using ICA. Are you familiar with that paper? In other words, it is possible to decode that even though it looks like it's.
01:01:42.870 - 01:02:38.292, Speaker B: Well, yeah, but I mean, what I will show you, it's basically. So that's exactly what we do here, too. But what we now use is, and so the question is now, if you just filter the 10 hz signals that you record from a multielectrode array placed in CE, one, ce, three of cortex, and you filter 10 hz. So you removed information that you would need to do spike sorting or detect even gamma activity. Can you use this multivariate signal to actually decode where the animal is? And the answer is yes. And so, very briefly, what you need to do for that is basically to first characterize, using just principal component analysis, the traveling wave structure. So this is now just the electrode array here on the x axis.
01:02:38.292 - 01:03:17.836, Speaker B: And these are now just the 10 hz filtered signals, basically just displayed over time. And so you see that this is this traveling wave that you have in the hippocampus. And what we do, we actually use this traveling wave as a carrier and then detect phase distortions with respect to that carrier. And it turns out that actually this signal contains, you can then decode the location of the animal. And so this just using basically linear decoding. And so this is basically the decoding. So here you see the, the continuous line is where the animal walks.
01:03:17.836 - 01:03:47.696, Speaker B: So this is just a linear track. And as time evolves here on the x axis, the animal goes back and forth. The continuous line is where the animal is. And the dotted lines is where the linear decodal things the animal is. And you can see, you can do this if you do spike sorting and use the spike signals. And you can also do this actually using these phases distortions in the LFP. Actually, if you combine both, you can even do better.
01:03:47.696 - 01:04:29.596, Speaker B: It's also interesting that actually if you just PCA, do dimensionality reduction using PCA, the variance here goes down much more quickly than actually the decoding performance. So there's definitely something in these signals which is not entirely captured by PCA. And so for that reason, we applied ICA on these phase distortions. And so it's complex ICA. And so here's what you get. So this is the animal moving in one direction and this is in the other direction. And then if you just look on these panels.
01:04:29.596 - 01:05:21.814, Speaker B: So each color here is a different independent component in this phase distortions, multivariate phase distortion signal. And it is when the coefficient is high. And here on the y axis up trial. So you can see very consistently the light blue guy goes on here and then the dark blue guys goes on and so on. And you have also a tiling you have for every location, basically in the maze, you have an independent component and you have one set of components that actually tile walking to the right and one walking to the left. And the nice thing is, now that this is a, so this is also a very robust signal. So you can actually just leave out randomly about 50% of your electrodes, even if the electrodes would move probably.
01:05:21.814 - 01:06:05.904, Speaker B: So it's a very robust signal also for doing actually brain machine interface. And so how do actually these phase distortion patterns look in the grid space? They are very distributed. So you really need, so there's no localization in the hippocampus, but there are basically reproducible phase distortion patterns as the animal visits the same location again and you can actually read that signal out. And this also works in more complicated environments. So here, different eyes again are marked by different colors. This is kind of in a resting area here, small kind of 2d area. You can also see there seems to be some fine structure in here.
01:06:05.904 - 01:07:05.314, Speaker B: So to sum up, and to finish, the interesting thing is that these waves seem to not only indicate brain states of the animal, but they, if you know how to look at their structure, they tell you detailed behavioral information. And the interesting thing is, not only this, now could help us. Now we have two different ways to do the decoding, and it could help us to really understand what is computed locally in the circuit. But there's also very nice evidence. Actually, Joanie Wallis gave a talk at cosine a week ago also, and she reported that if she does actually decision making experiments with actually visual stimuli to monkeys that have actually different values assigned to different faces, the monkey learns how to choose the highest valued face. Very, very good. And in prefrontal cortex, there are neurons that encode that.
01:07:05.314 - 01:07:30.714, Speaker B: While this computation is going on, she sees actually theta oscillations in hippocampus and prefrontal cortex coherent. And she recalls now, actually, from these both structures, it will be very interesting to see whether some of this travelling wave structure actually has to do with the long range communication. So that finishes my talk. This is the funding. Thank you. I'm happy to take questions.
01:07:34.494 - 01:08:11.292, Speaker C: So, in compressed sensing, part of your talk, there was the. There's been a lot of interest in the olfactory system because there. It looks like the piriform cortex, which, by the way, has the same architecture as the hippocampus. Right. It looks very much as if you go from a very, you know, each receptor, one type of receptor for each glomerulus in the olfactory bulb projects very broadly and seems to, you know, the single parameter cell seems to be sampling from many glomeruli at the same time. And it looks there like that might be another place where you're doing this compression.
01:08:11.308 - 01:08:39.002, Speaker B: Yes, that's true. There are some competing theories which even do not need that. So the idea is that you can actually now construct random separation planes in just this expanded space without doing anything and even relying on any sparse structure. But, yeah, I think also it would be very interesting to compare this with the theory of this type compressed sensing part. Somebody would claim that this idea of.
01:08:39.058 - 01:08:41.594, Speaker C: You know, compressing information and transmitting that.
01:08:41.674 - 01:09:35.794, Speaker B: Compressed version can also be done with an auto encoder structure. So how do you compare the two approaches? I would argue, perhaps, that the autoencoder approach is actually more biologically plausible than assuming that sparse coding is implemented somehow in the brain. You know, you can think of sparse coding as being an auto encoder going up and down, this is one way to think about, of course you don't want to think about auto encoders going up the hierarchy because, you know, as was said before, we don't want reconstruction of the same thing. But sparse coding absolutely is, you can view sparse coding as an autoencoder, where you put, I mean, not a limitation in the dimensionality on the latent representation in the outer encoder buds sparsity. But there's basically, you know, there's a reconstruction objective like in odor encoder.
01:09:39.534 - 01:09:41.434, Speaker C: So did you have.
01:09:43.734 - 01:09:54.974, Speaker A: The information geometry? You mentioned the Amari definition of distance to the independent independence manifold. And did you ever actually use that?
01:09:55.554 - 01:10:24.198, Speaker B: Well, I actually, no, we use Amari's definition of integrated information, geometric integrated information, and this uses it. And the funny thing is, before people thought, I mean, there's a very similar measure proposed by Neha Dyn. It's called stochastic interaction, but it's not the same. So it's really interesting if you think in terms of information geometry about integrated information, it helps you to really tease out what we are talking about, what you want to quantify.
01:10:24.286 - 01:10:27.994, Speaker A: Is that actually connected to the graph cut stuff that you were doing?
01:10:31.214 - 01:10:45.082, Speaker B: So the graph cut stuff, no, not directly. The graph cut stuff was really, I mean, just taking these correlation matrices and then detecting communities in them. That was not. Yeah.
01:10:45.218 - 01:11:02.494, Speaker A: Is there an information geometry approach to, because you could, you could think in geometric terms of finding some segmentation of the space, some decomposition of the space in which the information that the independence manifold is embedded.
01:11:03.074 - 01:11:27.844, Speaker B: There is a, there is, yeah. Actually takes this multi information, which is really not useful. Yes. The detail between the joint and the two subgraphs. And essentially you just force it to minimize the information and you get this transitions to disconnect the component just in the same way that I destined. And this actually, we proposed it in the minimum principle. Information principle.
01:11:27.844 - 01:11:34.984, Speaker B: Yeah, a bit earlier in the nineties already. Yeah. Yep. Great.
01:11:36.724 - 01:11:52.372, Speaker C: So I think correlation might not be the best place to start because as you know, things can be correlated without having any connectivity at all, simply with the joint input or some kind of circulating coherent information. Latent variables.
01:11:52.548 - 01:11:52.996, Speaker B: Yeah.
01:11:53.060 - 01:11:59.352, Speaker C: So, I mean, if there's a huge literature, unfortunately, it's called functional connectivity in the fMRI.
01:11:59.428 - 01:12:06.684, Speaker B: Yeah, exactly. Yeah. This is called functional connectivity. They call these correlation matrix is functional connectivity.
01:12:06.984 - 01:12:23.576, Speaker C: It's crazy, but you know, it's, that's, if that's the convention. Okay. But it turns out that people have actually gone beyond that. I mean, some. Some small groups. And if you take the inverse of the covariance matrix, they call the precision matrix, you can do better. It's not ideal.
01:12:23.640 - 01:12:24.184, Speaker B: Yeah, yeah, yeah.
01:12:24.224 - 01:12:28.404, Speaker C: But, you know, you might. You might want to take that as your starting point, see if you can see any different.
01:12:28.484 - 01:12:34.184, Speaker B: I mean, I was surprised, actually, how well it worked at all, but it's a good.
01:12:34.764 - 01:12:51.248, Speaker C: But you might be able to actually dig even deeper. But another question has to do with. It works so well with theta, but maybe. What about some of the other frequency bands? Right. We know there are traveling waves and beta, alpha.
01:12:51.376 - 01:12:52.520, Speaker B: So I'm so be.
01:12:52.552 - 01:12:52.792, Speaker C: Really.
01:12:52.848 - 01:13:04.792, Speaker B: So we are actually now talking to Pascal fries, who actually sees this phase diversity in gamma. And so we are now. Absolutely, yeah. You want to study.
01:13:04.928 - 01:13:09.584, Speaker C: You looked at the traveling wave of theta and hippocampus on, like, the 2d surface of the hippocampus, right?
01:13:09.704 - 01:13:10.200, Speaker B: Yeah.
01:13:10.312 - 01:13:17.384, Speaker C: Do you think there might be anything on the third dimension, like, going along the dendritic axis with there being traveling waves there?
01:13:19.724 - 01:13:29.984, Speaker B: I think, actually, yeah. You will also see some. I'm not sure if it's a traveling wave, but you definitely see a phase shift as you go down.
01:13:31.444 - 01:13:34.660, Speaker C: Yeah. That just has to do with the sign of the current going in and out.
01:13:34.772 - 01:13:35.140, Speaker B: Right.
01:13:35.212 - 01:13:37.124, Speaker C: But I think it's the same wave. I don't think there's any difference.
01:13:37.164 - 01:13:43.864, Speaker B: Yeah, I think so, too. Yeah. Yeah. So it's mainly going across the surface. Okay.
