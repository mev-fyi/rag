00:00:01.840 - 00:00:37.454, Speaker A: Thanks, James. Okay, so I'll talk about efficient parallel solver for SDD linear systems. At least that's what I thought I would talk about when I send in the title and abstract turns out there that turns out this can be an entire class of algorithms. There are several variations that showed up in the meanwhile, and quite a bit of it is joint work with some people here in the room. So this is joint work. This represents some joint work with Yu Chen is over there intact. Li Yan, Liu, dence Bielman and Zhang Hua Ten.
00:00:37.454 - 00:01:22.204, Speaker A: The talk is structured as follows. I'll start by describing the problem. So the Lx equal to b problem describe what makes it difficult. Then I will describe our key tool and how to use these tools to get parallel solvers, as well as other representations of the inverse of the graph plasm matrix. So the problem is motivated by large graphs, specifically that I have, I have large graphs showing up in many real data sets. For example, images, I take pixels, these are my vertices, road networks, junctions are vertices, social networks, everybody's a vertex. Friendships are edges and meshes where, uh, the space is discretized into a, into a point set.
00:01:22.204 - 00:02:12.512, Speaker A: So some of these graphs get really large, so they have anywhere between, say ten to the six to ten to the twelve vertices and edges. So it makes them challenging, some of them challenging to even store. But once we have them stored, next question is how do we analyze and how do we compute over them? How do we optimize over? So here are useful definition for representing graphs and for analyzing them is the graph Laplacian, which is a central object in spectral graph theory. This is build a matrix where every row and column corresponds to a vertex. So this correspondence means that my off diagonal entries can be interpreted as values between pairs of vertices. So every off diagonal entry I can just, I can set that equal to the negation of the edge weight. And then I put the degree of the graph on the diagonal.
00:02:12.512 - 00:02:47.754, Speaker A: So my diagonal equals to the weighted degree. So by this construction, because I put the negation on the weight, this means that my row and column sums is zero. Also, because I'm dealing with undirected graphs, I will assume this is a symmetric graph. So in this example over here, this edge of weight two corresponds to this entry of negative two on the off diagonal entry. So once I have the graph Laplacian, the natural thing to do is to solve a linear system in it. So this is the lx equal to b problem. Give a graph Laplacian l vector b, find vector x such that lx is approximately b.
00:02:47.754 - 00:03:34.812, Speaker A: And to describe efficiency, I will assume that it's m by m matrix with m non zero entries. So n is number of vertices or dimensions, m is number of nonzeros and edges. So this operation of lx equal to b. In the last ten years there has been, there has been many beautiful applications of this primitive, and this has led to what's known as the Laplacian paradigm. So if you just solve a graph Laplacian or precondition with a graph Laplacian, this is directly connected to what's known as elliptic method elliptic problems in numerical analysis and scientific computing. It lets you solve for problems such as such as problems such as heat equation or differential equations. So this is using collision modeling.
00:03:34.812 - 00:04:21.224, Speaker A: You can embed planar graphs with them. Once you iterate with a solver, you can compute objects such as eigenvectors or heat kernel of the matrix. So this is used in a lot of applications such as spectral segmentation, finding sparse cut balance cuts, and so on. If you take a solid iterate more times with it. So once you start iterated, let's say polynomial in n number of times, you get solutions to many classical graph problems as well as problems from image process. So examples include the image segmentation, which is actually related to the sparse cut problem. Here you can also use it to generate random spanning trees, compute max flow, max weight matching, as well as a variety of other graph problems.
00:04:21.224 - 00:05:27.070, Speaker A: So partly because of these applications, there's been a lot of attention given to the question that's to this core primitive in the Laplacian paradigm, which is how fast can I compute lx equal to b? How fast can I solve this? So because this is just a linear system solve, I can apply off the shelf tools such as, well, such as gaussian elimination, or that with otherwise known as direct methods to it. These methods running about n cubed time. There's been a lot of work in this area, giving runtime as approximately 2.3727. So once again, to describe running time, I will say n is an m by m matrix with m non zero entries. You can also use iterative methods. So if you believe in exact arithmetic, these iterative methods, you can show that they run times n times m, so number of dimension times number of nonzeros. If, on the other hand, if you believe in round off errors, the more popular bound is m times square root kappa, where kappa is the condition number of the matrix.
00:05:27.070 - 00:06:06.466, Speaker A: So these are methods that work for any linear system. But because we're dealing with graph Laplacians, we can also use the fact it's a graph and make gains through this graph theoretic connection. So this led to an area known as combinatorial precondition. The first result in this direction was by Vija, who gave an algorithm ran in about m to the seven fourth time. And then there were many subsequent improvements. So Bowman and Henrison showed you can get to n times m speedometer intent showed first that you can get to m to the 1.31 running time and subsequently to about m poly log n.
00:06:06.466 - 00:06:52.534, Speaker A: So this is what we call the spearman ten solver. And then there were subsequent improvements in the last five years that greatly improved the running time of this type of method, I think so. I think most of the authors on these sequence of results are in the room here. So basically, at some point everyone decided that, hey, we should make the solvers go faster. And they got running times as a form of m log squared n, m log n, m log to the 1.5 n using a different method, and then eventually to m square root log n with, with a few log factors in there. So it's fair to say that in a sequential setting, we have fairly good idea how to solve these linear systems.
00:06:52.534 - 00:07:51.572, Speaker A: But then there's the question of how do we start implementing some of these? And a very important aspect of modern architecture is that there's a great deal, a great amount of parallelism. So when I was in grad school, the machine on my desktop that had eight cores and software like Matlab, they implicitly use these parallelism, which is why. So if you look at the current solver package by Giannis, it talks with Matlab. It's because, it's because Matlab, for certain routines, Matlab is able to take advantage of this type of parallelism and get speed ups that way. So the common architectures that have parallelism include multicore architecture, Mapreduce, that's data centers. And the general idea is that if I have multiple cpu's, I can just split work or have multiple machines, I can reduce my work by splitting it among the machines. So of course the total amount of work doesn't decrease.
00:07:51.572 - 00:08:28.380, Speaker A: So that's the notion of work. But the limit which you can split up the parallel computation is limited by the maximum number of dependent steps. So this is known as parallel time or parallel depth. And it turns out that for these nearly linear work laplacian solvers, one can get algorithms at parallel lines. So the first result on this direction was by Giannis and Gary. They gave an algorithm that ran about n to the one point n to the one six time for planar graph. For general graphs, Blablock Al showed that it's possible to get to a depth of roughly n to the one third.
00:08:28.380 - 00:09:08.488, Speaker A: Cool. And I will come back to this in a moment. There are some inherent bottlenecks in these methods. So what we show in this result is that we can get solvers that run nearly linear work and polylog depth. So we get depth that's polylog n times polylog kappa and times other log factors. So one thing that I'm omitting here is that I'm generating a linear operator that approximates the inverse, even though the graph Laplacian itself is actually low rank, so it has a null space with the all ones vector. I will hide this under the rug.
00:09:08.488 - 00:09:36.584, Speaker A: I will just use inverse throughout the talk. So there are several ways of dealing with this. One is just like slightly bump up one of the entries on the diagonal to make it full rank and just claim numerical convergence. So the other details in this is that my dependency on the error. So this dependency on epsilon, it's log one over epsilon. So this means that the solver converges very quickly to the exact answer. So this may as well be exact solver.
00:09:36.584 - 00:10:11.084, Speaker A: The other difference is that I have a dependency on log kappa. So there is a dependency on the condition number of the matrix. So this algorithm is numerical in nature. But usually we can also assume that kappa is somewhere in the poly n range. And that's because you can show that kappa is less or equal to the max weight divided by the minimum weight in the graph times poly m factors. So if you assume your edge weights are have reasonable spread, you can also assume that this is essentially a polylog depth algorithm. We also give extensions in that.
00:10:11.084 - 00:10:56.284, Speaker A: This routine also generalizes to taking p powers of the matrix for nap between negative one and one. So you can extend this algorithm to other matrix functionals as well. So in this talk I would like to solve the problem of lx equal to b in polylock depth and nearly linear work. And let me just describe what makes this problem difficult. So one way to see the difficulty of linear system solving graph laplacians as well as just graph algorithms in general, is by considering these two extreme instances. On the left is a highly connected graph. You can think about it as a complete graph, except it's not as nice as a complete graph, say a random graph with about analog and edges.
00:10:56.284 - 00:11:22.234, Speaker A: This is often known as the expander. It's a highly connected graph. If you make any kind of local step, it's going to get expensive very quickly. So if you start doing, say, partial doing also divination quickly, you just get a very dense graph that's expensive. So the way that these things are usually solved is with global steps. On the other hand, you can get long paths or trees. So these are things with a very high diameter.
00:11:22.234 - 00:12:01.618, Speaker A: So for these kind of graphs, to get one end of the graph to talk to the other, you need to propagate information. Diameter times and diameter can be as big as n, so you need many steps. So each of these steps better be inexpensive. Each of these is easy on their own. Iterative methods are designed to handle this kind of densely kinetic case, and combinatorial methods like gaussian elimination. They're designed to handle this kind of long path scenario. And the main difficulty in designing solvers for graphloplacians as well as solvers in general, is that I can give you a graph that consists of a bunch of these cases stacked on top of each other.
00:12:01.618 - 00:12:42.144, Speaker A: I can take a bunch of expanders, connect them together with long path. I can throw another expander on top that's just not as heavy. I can just add graphs on top of graphs. And a solver algorithm needs to be able to take care of both of these cases simultaneously. And you don't want to multiply the cost of a global step times the number of steps you need to address one of these local problems. And this is the kind of difficulty of most solvers. You either need to look into the structure of the graph, pull out these pieces, or you need to devise methods that somehow work well for both of these methods simultaneously, well for both of these classes of graphs simultaneously.
00:12:42.144 - 00:13:34.478, Speaker A: And this is what the combinatorial preconditioning framework was designed to do. And all the previous fast algorithms, all the nearly linear time solver algorithms fall into this class. What this type of algorithm do is that they reduce the problem into three pieces. So, to spectral sparsification, tree contraction, and iterative methods, spectral sparsification. In turn, you use tree based routing, low stress spanning tree for finding the tree, and local partitioning for doing, for setting up a sparsification routine. The general idea of this framework is that I want to repeatedly reduce my graph to sparser graphs. So what I do is I take a graph, I find a graph that's sparser than it, that's similar to it, recurse on it, and recurse all the way until my graph, because it's easy.
00:13:34.478 - 00:14:36.040, Speaker A: So by easy in this framework, it means a tree. Tree is nice, because I can just do Gauss elimination on it and it will disappear. And I can do tree contraction in the middle of the algorithm. So, this is the recursive preconditioning framework, and the focus of pretty much every single result since vitreous has been improving this part of the spectral sparsification routine. Well, this part of the framework, which is a spectral sparsification routine, one way to think about it is that we're finding better and better sparsifiers that we can plug into this framework. On the other hand, the driver routine, which is repeated gaussian elimination plus iterative method that has stayed more or less the same in the last 25 years. And one way to think about this rival code is that once you unroll all the recursion, once you unroll all those, the cost savings, what is really doing? It's generating a polynomial in lg, lt inverse.
00:14:36.040 - 00:15:35.214, Speaker A: It's generating a polynomial in the graph preconditioned by the final tree. And what we're trying to generate, we're just trying to generate the inverse of this operator. So we have the graph times the tree inverse, we want to go from that to the tree times the graph inverse. And because it's a polynomial, all these methods, what they can be viewed as is that they generate this polynomial, and then they go about evaluating. And the evaluation scheme is not too different than Horner's method, which just says that I take my terms, multiply by the coefficient, add another constant term, repeat so with a degree d polynomial, where each term corresponds to a matrix vector multiplication. To get to evaluate one of these degree d polynomials, Hoenner's method takes about d log n steps. And in the Spielman ten framework, and pretty much all the subsequent ones, we get that the degree of the polynomial is roughly square root of n.
00:15:35.214 - 00:16:28.634, Speaker A: And the only reason that these routines get faster speed ups is because, uh, they do this step so well, that when combined with the size reductions, my intermediate graphs are able to be sparse, to be, are extremely sparse, and that, that's how I get the runtime savings. So, let me just expand on this part, this notion of polynomial approximation a little more. So, polynomial approximation is one of the ideas under, that underlies numerical numerical methods. What it's trying to do is it's trying to basically do division with multiplication. And the simplest form of division with multiplication is this power series. So I want to evaluate one minus a inverse. The way that I can evaluate that is by just taking some terms of this power series, one plus a plus a squared, and so on.
00:16:28.634 - 00:17:10.454, Speaker A: We know that when we sum this to, we sum up this infinite series, we get one over one minus eight. And what we can show is, because we're powering a, these terms get smaller if a is between negative one and one. If a is strictly between negative one and one, these terms, they, they just decrease in size. So you know that alpha is lesser, the magnitude of alpha is less than one. So we can, there's a notion of condition number which can just be defined as one minus the max, the max magnitude of a. This is analogous to the spectral gap. And you can show that kappa terms gives good convergence.
00:17:10.454 - 00:17:43.834, Speaker A: And because of the spectral theorem which says that symmetric matrices can be viewed as destroying operators on their eigenspaces, you can just extend this to work for matrices. So this is the reason we often talk about spectral gap. Spectral gap is just one minus the maximum eigenvalue of the normalized random walk matrix, which is just a. We can do slightly better. So this one converges in kappa steps. There's a method known as Chebyshev method. It was also described as heavy ball method in the previous workshop.
00:17:43.834 - 00:18:13.684, Speaker A: What it states is that a degree of square root kappa is sufficient. So this is a plot of a Chebyshev polynomial. These are fairly intricate objects. And what they say is that if I have condition number kappa, I need roughly a degree square root kappa polynomial. And this actually is optimal up to constant factors. This is a well known result in functional analysis for the setting of the solving linear system. There's a very nice proof of it in a paper by Orokia Sakadeva Vishnoy that I highly encourage you to take a look at.
00:18:13.684 - 00:19:24.294, Speaker A: So this is roughly optimal. And what we can also show is that for our definition of a tree. So if you restrict your definition of tree to, say, a spanning tree, there exist graphs where the condition number of the graph versus any tree needs to be at least roughly n. So if you combine this with the optimality of Chebyshev polynomials, a conclusion that's very tempting to draw is are we looking at a square root and lower bound in terms of parallel depth for these kind of methods? And you might recall from earlier that I mentioned a depth of n to the one third. So that's clearly not the case. So the question is what, what is going on in the bloodlock al result that gets around this lower bound? And this is actually a very simple fact, which is just matrices have inverses. If I compute you the inverse of my graph Laplacian, I can multiply my vector b by that inverse in log depth, but my work is fairly big, but my depth is small.
00:19:24.294 - 00:19:58.244, Speaker A: So formally, you can invert a matrix in n to the omega slightly more than n to the omega work. Polylog depth, you can multiply. Once you have that inverse, you can solve linear systems in it in log depth and squared work. So the issue with these methods is that these steps are expensive. So the actual work of invoking them, expensive. But these are highly parallel methods. And this is why a lot of linear algebra problems if you, a lot of linear algebra problems are easy to parallelize, if we are okay with say, n cubed work.
00:19:58.244 - 00:21:05.384, Speaker A: And the reason that this m to the one three, where this m to the one three comes about in a blablock ar result, is that because this is so expensive, we can only use it for fairly small sized systems. So it switches over to these inverses at a size of roughly n to the one third, and then gets the runtime too. And then we can show the runtime trades off to that. So what we do improve over this kind of parallel solver, is that if we're able to generate a sparse representation of the inverse of the graph of the plasma, then we will get a faster algorithm. So the question is, can we have a sparse approximation to the inverse of the graph laplace? And in some cases the answer is yes. So the nested dissection algorithm, this is, this was first studied on square mesh by Alan George, and then it was extended to general separable graph planar graph by lipping, rose and Turgene. And this is also the reason that, so using this kind of inverse, we had to get further gains.
00:21:05.384 - 00:21:54.664, Speaker A: So this is a result by Janice and Gary that got to the one six step this way. So what we showed here is that the standard numerical methods for solving these linear systems. So the standard driver code, that has certain limitations. And the question of getting a parallel solver is more like, more or less akin to question of can we have a good sparse representation of the inverse of a matrix? There's one aside that I should mention, is that the recent works on cut approximating, on cut approximation and oblivious routing. So these were based on the combinatorial preconditioning framework, and they can actually. So these ones paralyze very naturally. And this is by Madre, Sherman, Connor, Lee, Erwick and Sidford.
00:21:54.664 - 00:22:30.174, Speaker A: And so these methods can be viewed as, there is a way to view them as parallelizing the recursive preconditioning framework through asynchronous iterative methods. So this is fairly different than what I will talk about. So here there's another series. There's actually, it's fairly different than what I plan to talk about and also the trade offs of parameters. There are other trade off issues that show up there. So maybe we can, just, so I can, I'll be happy to talk more about that, that connection later on. But in the meanwhile, I'll describe our key tool.
00:22:30.174 - 00:23:38.284, Speaker A: So the main, so the motivating question for our key tool is that if I want to evaluate a degree D polynomial, do I necessarily need depth d? So the question is, is hoarder's method optimal for evaluating a polynomial? For those of you who are familiar with repeated squaring, the answer is no. So for example, if I want to take a number to the 16th power, what I can do is I can square it, square it again, square it again, square it again, just square four times. So repeated squaring already gives a way to work around the lower bound. And this is the fact that I will, that I will build upon in the next y. So this can also be applied to the, to the power method expansion earlier. So for the power series of one plus a sums, all the powers of a, you could represent that as one plus a times one plus a squared plus one plus a to the fourth and so on. You just take a bunch of products involving powers of two out of eight taken to powers of two.
00:23:38.284 - 00:24:19.316, Speaker A: Of course, this has a matrix version as well. Each of these terms, instead of little a to the power of two to the I, I can take a matrix to the power of two to the I. But before I continue, I need to talk about how can I get into the situation where my matrix a behaves like a, like a number between negative one and one. So this is the reduction that I will use. So I have a graph of Laplacian, I can add self loops to my vertices to make all the vertices have the same degree. So I can transform to a regular graph. And once I have a regular graph, I can just scale down the Laplacian by its degree.
00:24:19.316 - 00:24:45.832, Speaker A: So I get identity minus some adjacency matrix. So this makes my graph look like I minus eight to make it full rank. What I'm going to do is I'm just going to slightly scale down this matrix. So you just slightly scale down the edge weights. You can show that numerically. If you do this with two up to something like machine epsilon, it does not affect the answer too much. Of course, machine epsilon is not a value.
00:24:45.832 - 00:25:28.114, Speaker A: Everyone is seeing your numerical algorithm, but it just has a proof tool. And there are ways of sorting this out. But what this lets us assume is it lets us assume that a is a weighted graph where the weighted degree at every vertex is at most one. So this can be viewed, so it can be viewed as a transition matrix for a random walk, a lossy random walk. So the spectral norm of my matrix is at most one. So this is how I can reduce my problem to the case where it's more or less behaves like a number between negative one and one. And once I have this interpretation, I can start interpreting this kind of factorization.
00:25:28.114 - 00:26:45.154, Speaker A: So because a is the transition probability of a random walk matrix, a squared is just two steps of my random walk, a to the fourth four steps of my random walk. In general, a to the two to the I is two to the I steps of my random walk. So this view, this factorization, it can be viewed as computing a stationary distribution of a random block by taking one step in each power in successive powers, successive squares on my random walk walk matrix. So I take one step of the two step random walk matrix, one step in the four step random walk matrix, one step in the eight step random walk matrix, and so on. And I just do this until my power random walk matrix gradually disappears, becomes in some sense an expander, it's rapidly mixed. And for this to happen, because I'm repeatedly squaring my matrix, and because earlier I said that it takes about kappa steps for my random walk to mix, and I'm repeatedly squaring, I just need to set two to the I to kappa. So the number of steps I will need is just log kappa matrix multiplications for me to simulate this kind of random walk.
00:26:45.154 - 00:27:32.964, Speaker A: The issue though is that because I'm starting to square matrices, these things become very dense. Squaring is one of the big taboos in numerical methods because you quickly generate these very dense matrices. So the work just with this method naively is going to be n to the omega Times poly log factors. And what we're going to show next is that this mixes fairly nicely with size reductions. So this kind of method as an algorithm tool actually shows up quite often in theoretical algorithms. Well, it actually shows up in a variety of places, one of which is the multiscale methods which were described in the talk on Wednesday that was also based on squaring of the matrix. I'm still trying to understand the connections, and if you haven't noticed connection better, I'll be very happy to discuss with, with you about it.
00:27:32.964 - 00:28:23.564, Speaker A: The other, the other algorithm that looks similar to it are the poly are the log depth algorithm for shortest path. This is based on repeated doing, the mean squaring of the matrix. So repeatedly take, take the repeatedly compute two hops, shortest path distance. The routines that look closest to what we're doing are actually the log space connectivity algorithms. So this is the algorithm bi Rangel that says that in log space I can compute whether two vertices connected in an undirected graph. And in fact there's actually a variant of it known as determinist squaring by Rosman and Badan, that look almost exactly like the kind of operation I'm doing here. So what they do is that repeatedly, they repeatedly squared a distancing matrix until the spectral norm of the, of the matrix is small.
00:28:23.564 - 00:28:54.664, Speaker A: At every step they take their graph, they reduce it to a low degree graph via the use of expanders. They use a derandomized method. And what they want to maintain is connectivity in the graph. What we do instead is that we do the same kind of iteration step we do squaring. We stop when the special norm is small. But every step, instead of doing just maintain the degree to be small, we just make sure the graph is sparse. So we maintain the average degree to be small.
00:28:54.664 - 00:29:54.712, Speaker A: We use a randomized method as well. But our requirement is strong in that we want to maintain solutions to I minus a equal ax equal to b. So this is our way of getting around the lower bound described earlier, which is by just repeating squared. And in the next, in the rest of the talk, I'll describe how to translate this into algorithms. So, before I describe algorithm, let me first launch in a philosophical session of what is a numerical algorithm. So I have a routine that takes the input b, outputs a vector x, and for many numerical algorithms, if you look at the inner loops of your algorithm, it actually does not have, it does not have if statements, it has the flavor of, perform these operations on a vector, add some values together, sum up a bunch of things together, output the answer. So these, all these operations can be viewed as linear transformation.
00:29:54.712 - 00:30:44.164, Speaker A: Of course, this is under exact arithmetic, but then for the numerical round of errors, you can do the same kind of calculation as well. So because we have a linear operator z, what we can do is just write down, write it down as a matrix. So I claim that most numerical algorithms can be viewed as corresponding to a matrix, and we're just doing a matrix vector multiply involving battle. And our goal is to create a matrix z that is approximately the inverse of my matrix. So this notion of approximation is going to be very important in designing these classical algorithms, its spectral similarity. There are several ways to think about it. One way is to think about it as the eigenvalues of the generalized eigenvalue between the two operators.
00:30:44.164 - 00:31:17.024, Speaker A: Another way to think about it is through luverneur ordering, which is this positive semi definite ordering among matrices. But for this talk, all I need to use is that they obey all nice notions that you would expect from a similarity operator, in that it obeys symmetricness. So if a is similar to b, b is similar to a, it's invertible. So if a is similar to b, a inverse and b inverse are similar, and it's composable. So a is similar to b with error epsilon. B is similar to C with error epsilon. A is similar to C with error two epsilon.
00:31:17.024 - 00:32:03.884, Speaker A: That's all I will use. So our goal is to create this operator z that is close to the inverse of my graph Laplacian, using the sum and product of a small number of matrices. And the reason that this notion of similarity is great for working with is that their results that says that I can take any graph and sparsify it into one with about n over epsilon squared edges. So this is for any dense graph I can take, it does not matter how many edges I start with. All that matters is the vertex count and my error requirement. And my final graph size only depends on the number of vertices and my error parameters. So, m over epsilon squared.
00:32:03.884 - 00:33:01.534, Speaker A: Of course, if we want to do this efficiently, right now, well, currently, if we want to do this efficiently, we need to pay a few logs in both the size of the output and the work. So this actually was from the original construction of special sparsifiers by Spielman ten. And if you combine that with subsequent improvements, namely SDP based partition routine by Orochia Vishnoy, you can get efficient and parallels sparsification routines. And of course, there's also a few other tricks that we need, because we're squaring the matrix as well. So this result is actually for any graph Laplacian. But we can also check that if we square the graphloplasian, we can still implicitly construct a sparsifier very quickly. So this result has already been improved by Lianus Kudis, who gave a faster algorithm based on combinatorial methods, which is based on spanners, or even just low diameter decompositions.
00:33:01.534 - 00:33:34.112, Speaker A: But what we're going to do, we're just going to take this result and use it. So we're just going to apply this to our matrix repeatedly, over and over. So I start with, with a I create I minus a one that's similar to I minus a squared. I create I minus a two similar to I minus a one squared, and so on. I just create this chain. So this is what I would call this, the approximate inverse chain. What it does is it takes a graph, puts in more edges, but sparsifies it, so it makes the graph more connected and then repeats.
00:33:34.112 - 00:34:22.834, Speaker A: So eventually you get something that looks more like, more or less like the identity matrix. So eventually you get a graph that is pretty much uniform. So we can show that because we're squaring the matrix, its condition number will just decrease by a factor of two. But even, and even if we introduce errors, as long as we keep our error reasonably small, so less than constant, this condition still holds for a smaller value of two. So what we can show is that in about log kappa steps, we do get to mix it even with these approximations. So this leads to a chain of depth log kappa. Another question is just how do we use this chain? So recall that we want to compute approximation to one minus a inverse, which is equal to these products.
00:34:22.834 - 00:34:52.613, Speaker A: And the only condition that we have is that we have that one minus a of I plus one is approximately the same as one minus a I squared. So we don't have. So our intermediate condition doesn't even look like the factorization we have. We can't just say like I plus a I plus a is roughly the same as I plus a squared. So we can say I plus a two is roughly the same as I plus a squared. So what we do is we apply this kind of condition gradually. We apply this one at a time.
00:34:52.613 - 00:35:25.620, Speaker A: So the idea is that we use the factorization of one minus a squared. So we know that one minus a squared is one plus a times one minus a. So what we have is that we have that one minus a I inverse is roughly one plus AI times one plus one minus a I squared inverse. And here, because we have that, we have that this is similar to the next operator. We can substitute that in this. If we're working with scalars, I'm doing something horrible with matrices. I will deal with that on the next slide.
00:35:25.620 - 00:36:18.044, Speaker A: But assuming we're dealing with scalars, we can substitute with the next operator, just because the similarity notation also carries over under inverses. So this gives us a way to propagate operators back up the chain, and we just do an induction reverse back backwards after chain, because we have that one minus ad is close to one. We have operator ZD that is similar to the inverse of my last level operator. So by the inductive hypothesis, I can assume that I have an operator similar to my next level. So I've operated z of I plus one that is more or less the same as the inverse of I minus AI plus one. So what we do then is we just take this and let that be our operator. So we substitute this with our next level operator.
00:36:18.044 - 00:37:19.534, Speaker A: And what we have is that because we have that our next operator is similar to the inverse of the next level system, I can just, by similarity, I can just pull this over, I can just replace that operator with the exact inverse, and then by that identity above over there. So I buy this identity, I get that it's similar to the inverse of my current operator. So this gives me a way to propagate inverses of the chain through the use of operators. So this is for the scalar case and in the matrix case. In a matrix case, there's a big issue, which is that why I want to replace, apply approximations. The terms around what I replaced needs to be symmetric for me to claim that the same kind of error guarantee carries over. So if I want to replace z with z prime that's similar to z, I need to have u and u transpose around z.
00:37:19.534 - 00:38:08.314, Speaker A: So the part that around my expression needs to be symmetric. Another way to think about it is that whatever operation I write around z, the term needs to read the same backwards, forwards and backwards. So this is sometimes using z of I plus one at most once. But the operation that I wrote down earlier, which is this I minus a I squared. Well this is, which is this I minus a I of z plus zi plus one that is not symmetric around the operators. Clearly I have one term on one side, nothing on the other side. So the main, so once we have this setup, the main difficulty in the problem is just come up with a symmetric factorization of I minus a involving I minus a squared.
00:38:08.314 - 00:38:54.914, Speaker A: And this is the factorization we use. So this is just by some algebra manipulation, you can get to operation to a factorization that happens to be symmetric around the term involving one minus a squared. You just put one term of one plus a on each side and you can show that just by checking the identity, that this is actually an expression, the right expression. So of course this can be carried over to the matrix setting just by the spectral theorem. Again, because my matrix a commutes with identity. So I'm just working in just its own eigenspace. So I can just apply the, I can just apply spectral theorem to all my eigenspaces and what I get is I just invoke this kind of identity in my induction back up the chain.
00:38:54.914 - 00:39:48.074, Speaker A: So my induction hypothesis gives that my z of I plus one is similar to my, to the inverse of my next level. And the construction of my chain implies that this inverse is also similar to the inverse of the current level squared. So we just, so if you compose these together, you get that I have a good approximation to the current operator squared. And all we need to do is just take this approximation and substitute in for this term. So once we perform the substitution, you just get this representation of the inverse operator of my current level. And what the inductive hypothesis gives you is that you get an error that's slightly larger and just plug that error in. So you get that I have a good inverse operator of my current level as well.
00:39:48.074 - 00:40:24.134, Speaker A: So every level up every level of induction, I gain an error of epsilon. So all I need to do is just to make sure my total error buildup is not so big. So I set my error to be one over d, whereas d is the depth. So I get an error roughly log kappa of f times epsilon. So the pseudo code of this algorithm, once you unroll the way that we're substituting operators, is also extremely simple. The general idea is just, I just proceed once down the chain. So I multiply by the right side operator, and then I proceed again back up the chain on the left side.
00:40:24.134 - 00:41:04.278, Speaker A: So all I'm doing is I just compute a bunch of vectors that's I plus AI times my current vector. Last level I do nothing and just copy it over because I'm going to use identity as my inverse. And then I propagate these back up the chain by just adding my current vector plus some copy of the solution to the next vector. So this leads to something that looks extremely similar to the multi grid algorithm. So it's a v cycle like algorithm. And what we get is we get a solution from matrix vector multiplications involving log kappa sparse matrices. So we get a depth of log papa.
00:41:04.278 - 00:42:04.952, Speaker A: So we need to set epsilon to roughly one over one over the depth for the total error to be small. So the number of non zeros at every level is going to be n poly log n times log squared k something over the total size of the chain. Because we have one of these matrices per level, we get a total depth of roughly poly log n, and then a work that's also n poly log. So the new idea that we introduced here is that we can keep all the intermediate matrices sparse and it's a way to analyze these algorithms is by viewing every algorithm as its own matrix and just apply a spectral similarity and propagate spectral similarity between these operators around in an inductive manner. So this is where I thought this algorithm stopped. I thought it was kind of a. So if you asked me this, say, four months ago, I would have said this algorithm is just a fairly strange situation.
00:42:04.952 - 00:42:35.864, Speaker A: It's kind of a curious, it's a curious phenomenon. But I didn't think that this actually has extensions. And what motivated us to look for extensions of this algorithm is the problem of sampling from gaussian graphical models. So here the idea is that I want to sample from a gaussian with covariance matrix I minus a. So this is a phrase in this, in the computational statistics setting. But formally, what I need is I need a matrix such as C transpose. C equals the inverse.
00:42:35.864 - 00:43:22.436, Speaker A: So if you're okay with non square matrices, there's a fairly easy way, using the factorization of my graph Laplacian to get this. But say we want to also have a symmetric factorization. So a square factorization and the form of the, if we just use our earlier algorithm, what we get is that because we're adding an identity at every level, what we're getting is we're getting a sum of sum of products and matrices. So it doesn't have this kind of factor. It's not factorable in this sense. So what we want is just a product that approximates inverse of my matrix. So here you are assuming the attractive model, like all the a is positive entries, the general gaussian graphical model can have both.
00:43:22.436 - 00:43:57.490, Speaker A: Yeah. So this is assuming that my covariance matrix happens to be a graphloplacian. This is a fairly, this is a specialized class of graphical models, gaussian free field. Okay, thanks. Keep that in mind. So, this was also recently studied in the setting of can I get Gibbs method to perform well in parallel? This was also the setting study there, which is what motivated us to look at this from this perspective. And so the other way.
00:43:57.490 - 00:44:34.796, Speaker A: So suppose we just want a product. So recall the factorization earlier of just I minus a inverse is I plus a times I minus a squared inverse. And recall that I just want a factorization that's symmetric. So the even simpler way to factorize it so it's symmetric is just to, instead of having a full, full power here, pull a half power over onto the other side. So these matrices all commute because it's just a and identity, there's the same eigenspace they all commute. So we can get to this, this set. But then the question is what happens? So, and if we repeat this.
00:44:34.796 - 00:45:23.946, Speaker A: So if we repeatedly substitute approximations to the next operator, what we do get is we do just get a c equal to the square roots of a bunch of matrices. So we could get such a factorization. But the problem is, how do we evaluate one half power of a matrix? This in general seems like an even harder problem. But one thing that we do know is, because we know that a one is the square of a zero, we know that all the eigenvalues of a one is between zero and one. So if you look at all the eigenvalues of I plus a one, we have that they are all between one and two. So this numerical, in the numerical language is, this is a very well conditioned matrix. And for well conditioned matrices, anything pretty much works.
00:45:23.946 - 00:46:16.914, Speaker A: So you can take the Taylor expansion of the functional, you want to evaluate this well formally, it's the Maclaurin series expansion. So you get that you have a low degree polynomial approximating everything after the first term. So all the, all of these terms, you have low degree polynomials approximately. The problem still is, what about the first term? We still need to take the square root of the first term as well. And this is, this is the third solution that we can take for this problem. So this is in a recent, recent result with Chan, with Chan, Chan new and ten. So the, so the idea is just, instead of having four copies of the matrix, take half of the matrix at every step, and turns out that the factorization still gives inner terms that's easily sparsifiable.
00:46:16.914 - 00:47:22.334, Speaker A: And now, because the matrix of the eigenvalue of each one of these matrices between negative one and one half of the matrix is between negative half and half. So all my terms are now very well conditioned. So what we can do is we just use a log, log, kappa degree Taylor expansions of all these terms substitute in and we get a representation that is just multiplied together a bunch of low degree polynomials. So that gives a representation that is just a product of matrices. So this also has generalizations, because, so the nice thing here is that what we can do is we can now try to, we can show that sparsification of this term also preserves its p power operator. So if we want to compute a Pth power of a matrix for some p between negative one and one, the same factorization will also work just as putting a different power in these factorizations. So in each of each one of these Taylor expansions, in each one of these Maclaurin series.
00:47:22.334 - 00:47:58.488, Speaker A: So this, so what this seems to suggest is that this method leads to an entire class of algorithms. So it leads to a way of factoring a graphloplacian into a small number of well conditioned matrices. Yeah, just one question. So in this setup, when do you need the bionic dominance condition then? So I needed the condition because I want I minus a squared to be efficiently sparsifiable. Nice. Yeah. So the issue there is, for many classes of matrices, the square that has a higher, has bigger terms.
00:47:58.488 - 00:48:36.140, Speaker A: That's a little harder to treat. But that's, that is a very good question and that I will talk about open question next. And that's very high up on my list of open questions. So this seems to suggest we can also approximate a very wide class of functionals. So I believe there's a lot of open question that comes out of this. So as you mentioned, the question first question is, can we apply this to more general classes of matrices? All we need is the square of the matrix to have a good sparse fire for it. The sparse squaring seems to have a lot of connections to a lot of iterative methods.
00:48:36.140 - 00:49:09.754, Speaker A: We have an algorithm that looks more or less like a multi grid b cycle. So this seems to suggest that has connections to iterative methods as well as multilevel methods. So multi scale decomposition of graphs. There were two talks on Wednesday about this, and I'll be very interested in. And so I believe there are very interesting connections going on there. For other functionals. We only looked at p power matrices, but it seems that for other classes where if I have a function that can be represented as products, so if I have a function that I can apply to products, and I have.
00:49:09.754 - 00:50:10.906, Speaker A: So if I have two matrices that I want to evaluate, say the function of a times b, I can get it from a and b separately, that prime, that probably should be applicable, so that this algorithm can probably apply there. And another question is that what kind of sparsification do we actually need in this algorithm? So we're using spectral specification, and we're using spectral specification in a very strong manner, in that we need to have very small error at every step, because we're chaining together about log kappa of these errors. So one question, therefore, from a theoretical perspective, is what kind of sparsification do we need when we compose about log kappa of these sparsifiers? So to get this routine to be more efficient, actually running out of time, maybe take some of these questions offline. All right. Okay, it looks like there's even another slide that this one doesn't end. Yeah, there's a whole bunch of these end. Okay, I can probably take some of these offline.
00:50:10.906 - 00:51:13.604, Speaker A: And so some of them involve, can we get sparse fires to go faster? And can we start, can we, and what parts of this routine is, can, can be implemented? There are actually quite a bit of, quite a few subsets of the routines used here. Would already be interesting, would already have quite a bit of application consequences as well. So that's all. Thank you. Any questions? So, in multi grid algorithms, there's this interpretation that what happens is that every level in this v cycle treats different part of the spectrum, solves in different parts of the spectrum. Is there something similar going on here? It essentially addresses different eigenvalues, different brackets of hygiene. So the question is, there's this notion in multi grid algorithms that different parts of this algorithm addresses the different parts of the spectrum.
00:51:13.604 - 00:52:01.144, Speaker A: So one thing I think one place where it's helpful to look for this is in the factorization. So if you look at the algorithm in a factorized form, so this form of the factorization, every once in a while I add an identity. So you can ask yourself the question of where does this, so for a single eigenspace of the solution, where does it get fixed? And what you can show is that it gets fixed roughly. So if my ion value is kappa. So if my special gap is like, if it's distance to the spectral gap, it's like Kappa gets fixed, roughly log kappa steps in. So it gets fixed by this plus I at roughly log kappa levels down. So you can interpret this routine as fixing eigenspaces gradually.
00:52:01.144 - 00:52:40.724, Speaker A: I am not aware of algorithmic consequences. There probably are as well. I have one question. Does this mean you get a sparsifier, you get a sparse. So I give you some graph and you get a sparse graph such that this, you know, the Laplacian of the sparse graph to the minus one two is a good approximator for the Laplacian of the original graph to the minus one half. So the question is given. Oh, so the question is, if I sparse, given a graph Laplacian, can I generate another graph of placing such that they are closed under the one two power? This turns out to be just a property of the luminal ordering.
00:52:40.724 - 00:53:06.244, Speaker A: So somewhat surprisingly, this was a bit surprising to me, but turns out that if you have two graphs that approximate each other in first power, they are half power. So any power that's less than one, whose magnitude is less than one, they do approximate each other. I meant the negative one half power. Yes. So if I have a graph Laplacian that's close to another graph Laplacian. Their inverses are also approximate each other, and. Oh, so you mean.
00:53:06.244 - 00:53:13.164, Speaker A: Oh, you mean another graph Laplacian approximates the negative one half of my first Laplacian? I'll ask you in a sec. Let's finish up and.
