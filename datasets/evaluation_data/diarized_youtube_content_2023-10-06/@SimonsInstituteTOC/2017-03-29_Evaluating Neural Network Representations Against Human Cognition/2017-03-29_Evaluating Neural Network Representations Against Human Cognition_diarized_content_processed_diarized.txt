00:00:00.440 - 00:00:15.714, Speaker A: Continuing the morning session. So we have Tom Griffiths. Tom is a very well known research in the space of machine learning and cognitive sciences. He's a professor at Berkeley, and he's going to talk about comparing neural network representations to human behavior.
00:00:16.374 - 00:01:03.274, Speaker B: Claire, great, thanks, Russ. Yeah, it's been a pleasure listening to all of the talks this week. And as one of these longer talks, I've been charged with giving a tutorial on how cognitive scientists think about representation. And so what I'm going to do is give you a little bit of history, and then I'll talk about how some of the ideas from that history apply to some of the methods that we've been hearing about over the course of the week. So I'm going to focus on the one particular problem, which is a problem of categorization. So basically, the question of how it is that, you know, that that's a dog and that's a cat, and then when you see something novel like this, you're able to make a reasonable sort of inference about what category it might belong to. So the question that psychologists have asked here is a question about how it is that people represent categories like what is it that's inside your head that's allowing you to solve this kind of problem.
00:01:03.274 - 00:01:35.278, Speaker B: And so there have been a lot of different approaches that have been taken to try and understand that. As with most things, in the beginning, there was logic. And so the idea that the way that you form representations of categories is essentially that what you're doing is building up some kind of logical rule that you can use to try and then figure. Figure out whether something is a particular kind of thing. So the way you might decide whether something's a cat is by applying a rule like it's a cat if and only if it's small and furry and domestic and carnivorous. And then when you see an object, you can apply it. You can say, yes, that's a cat.
00:01:35.278 - 00:02:37.318, Speaker B: No, that's not a cat, because while it's domestic and potentially carnivorous and somewhat small, it's not furry. But then, you know, that particular rule fails when you see something like this, which happens to be small, furry, domestic and carnivorous, but probably not a cat. So the idea, though, for that sort of logical perspective was that this is just a failure of that particular rule, and there's going to be some rule that you can figure out. There's going to be some sort of combination of features and way of putting those together with logical connectives. That's going to be the right kind of rule, and people kind of had faith in that up to about the 1970s, when Eleanor Rush, who's a psychologist here at Berkeley, really convincingly showed that for many of the natural categories that we see in the world, this kind of structure, where there's some defining rule, doesn't work in characterizing the nature of those categories. And rather, she argued, the way that we should think about categories is in terms of what she called a family resemblance structure. So if you look at these fellows over here who belong to a particular family, you can see they're all sort of related to one another.
00:02:37.318 - 00:03:07.514, Speaker B: They all have some similarity to one another. But there's no defining feature that tells you that somebody is a member of that family. If you sort of focus on the color of their beards or the size of their ears or whether they're wearing glasses or not, or all of these sort of features, that you could pick out those features, none of those features alone or in combination sufficient to allow you to say, okay, this is the thing that makes somebody a member of that family, but nonetheless, you can recognize that they somehow fit together and form a family.
00:03:08.574 - 00:03:10.910, Speaker A: Maybe a threshold of features.
00:03:11.062 - 00:04:05.566, Speaker B: Yeah. So that sort of starts to go in the direction, which is then to say, what is it that you need to do in order to represent this kind of family resemblance structure? So I'm not sure what this is, but we're in cognitive science. There's like a lag here of, you know, I think historically about 30 years, and now we're starting to get that down a little bit. Having this kind of meeting is something that helps to accelerate that process. So the argument is, yeah, there's something going on here. So how do you represent categories in a way that allows you to capture the family resemblance structure, something that doesn't sort of naturally fit with clear deterministic rules? And so the first idea that people had in order to try and do this was the notion that maybe categories are represented by prototypes. So, you know, what it means that you have this particular category of cats is that somewhere in your head there's a kind of ur cat, the sort of prototypical cat.
00:04:05.566 - 00:05:04.372, Speaker B: And that's the thing that you're consulting when you're deciding that something is a cat or not. And so what you're doing is you're seeing a thing in the world. You're comparing it to your prototypical cat, your prototypical dog, and you're making a decision based on how similar it is to those prototypes. And that's the way in which you're deciding what things are cats and so this is starting to sound a little more machine learning ish, and we'll get there in a second. But so this is sort of late 1960s, early 1970s, and then a few years later, people figured out that, in fact, there's another approach that can actually explain all of the effects that had motivated people to think about these kinds of prototype models. And so this is what's called an exemplar model, where rather than abstracting away the prototype, what you do is you remember every instance you've ever seen of a cat. And when you are making a decision about whether something is a cat or a dog, you compare every, you know, you compare that new object to every instance of a cat that you've seen in every instance of a dog, and you sum those similarities together and then you make a decision on that basis.
00:05:04.372 - 00:05:05.084, Speaker B: Yeah.
00:05:06.424 - 00:05:21.240, Speaker A: Aren't these equivalent because of, like the representer theorem, I can write, you know, anything as a sum of the exemplars, or I can say there is a weight vector primal form that is represents the prototype.
00:05:21.392 - 00:06:01.914, Speaker B: Yeah. So I think if you allow yourself a rich enough feature space, right, then you can show that they're equivalent. I mean, in this case, the way that this worked in the sort of psychological literature is people said, if we look at what people do, we find that if you give people, if you're teaching people a new category and you show them this, this and this, then they're actually better at deciding that this prototypical member, which is sort of the average of all of the things that they saw, is a cat, than they are about deciding whether these other things that are sort of slightly further away are cats. Right. And so that was the argument they made for prototypes. I said, you have to be representing this thing inside your head, which is the average of all of these. And that's what makes it easier for you to make that decision.
00:06:01.914 - 00:06:23.814, Speaker B: And so the thing that motivated people to think about this other approach was to say that, in fact, if that's the effect that's motivating you to think about prototypes, you can get exactly that effect without needing to postulate some abstract prototype. You just need to have the idea that you're just comparing similarity to each of the examples is enough to mean that the average of those examples is going to be closer.
00:06:23.894 - 00:06:35.366, Speaker A: I guess my question is, do these theories make differences in predictions of different behaviors, or do they? Because this one, it seems like, is just an implementational detail.
00:06:35.430 - 00:07:26.428, Speaker B: Yeah. So if you fix a representation, then they make different predictions. And in fact, I can be more precise about that, which is that. So if you're a machine learning person sitting in the audience and saying, well, hang on, all of this sounds very familiar. There's a reason for that, which is that your prototype model, if you sort of think about this as you have a prototype for each category, and then you measure the distance to the prototype, and that's how you make your categorization decision, corresponds to a linear decision bound model. Right. Um, and the exemplar model corresponds to, uh, it's sort of a, you know, um, you can, you can show formally that it's equivalent to doing Bayesian classification using a kernel density estimator as the basis for estimating the, the density that's associated with each of the categories in the same way that this is bayesian, uh, classification using a, you know, a spherical Gaussian, a zero estimate for the category where the prototype corresponds to the mean of the Gaussian.
00:07:26.428 - 00:08:12.760, Speaker B: Does that make sense? Yeah. Okay, so, um, everybody, any other questions as we're doing, like history of cognitive psychology? No. Okay, so the psychological question that people asked is, okay, now we have these different kinds of accounts. They do make different predictions. Basically, you can get much more complex kinds of decision boundaries out of your exemplar models. So now which of these accounts actually gives the best description of human behavior? I think, you know, I think for a lot of people in the field, this question was resolved somewhat definitively by a bunch of experiments that showed that people can learn really complex stuff. So this is an example of one of these experiments where Musofsky and McKinley constructed a two dimensional stimulus.
00:08:12.760 - 00:08:34.776, Speaker B: And I'll show you what that means in a minute. But so a stimulus where there were sort of two dimensions along which it could vary. And then they constructed category a as a mixture of two gaussians. Category me as a mixture of another two gaussians. And then when you do that and you actually compute what the optimal decision boundaries are, you get something like this where anything that's in here is a b. Anything that's in here is an a. Anything that's over here is an a.
00:08:34.776 - 00:09:18.606, Speaker B: Anything that's over here is a b. Okay, so this is a kind of classification boundary that you can't produce using something like a prototype model, you can produce if you use the exemplar model. And then they had graduate students come into their lab over the course of a week, and they would sit in the lab and just classify things as a's and b's for 4000 classification trials. And at the end of those 4000 trials, they could do stuff like this where they're actually classifying stuff that's over here as b's and classifying stuff that's over here as a's. So x's are as and o's are b's, right? So they said, okay, that's it. It has to be the case that people can learn these complex classification boundaries. And so it can't be that what you're doing is just representing simple exemplars.
00:09:18.606 - 00:09:36.954, Speaker B: Here's another example of this. So here. Now, b is a mixture of these Gaussians. A is a mixture of these Gaussians. You get these boundaries where now, if you're in the corners, it's an a. If you're anywhere else, it's a b. And again, if you sit a graduate student down and have them do 4000 trials of training, you can get them to the point where, you know they can learn that that's the right classification rule.
00:09:36.954 - 00:10:43.924, Speaker B: Okay, so, one implicit point in the models I've been talking about so far is the idea that we have a way of measuring what similarity is, right? So I said, you decide whether something is a cat or a dog by measuring the similarity of that thing to your prototypical representation of a cat or a dog. What does it mean to measure similarity? So, one popular historical proposal was made by Roger shepherd, which is the idea that you could think about similarity just as something which is a decreasing function of distance in a psychological space. So the idea is that the way you're representing cats and dogs is you have continuous features. Essentially, each cat is a point in this continuous feature space. Each dog is a point in the continuous feature space. And when you see a new thing, you're measuring similarity based on essentially an exponentially decreasing function of distance in that space. And so, using that assumption, Shepard was actually able to do a lot of interesting work trying to reveal the kinds of representations that people have for complex sorts of stimuli.
00:10:43.924 - 00:11:45.516, Speaker B: So, if you assume that you know that similarity decreases as a function of distance in psychological space, you can use that to try and figure out what the right spatial representations are for a given domain based on the similarity judgments that people produce. And so this is the context in which multi dimensional scaling, which you might know more as a sort of general statistical method or a method for doing embeddings, was really developed by psychologists. And Robert Sheppard was one of the people who put a lot of effort into that as a tool for going from human similarity data. So, for example, this is human similarity data for pairs of color patches. Here, where we have light measured in terms of wavelength. So if we have people judge the similarity between pairs of colors, you can then take those, judge similarities, and you can use multi dimensional scaling and say, okay, if I know the similarity between these points, what do I have to assume about the distance between them? And from that, reconstruct a representation. It's a spatial representation of that domain.
00:11:45.516 - 00:12:39.528, Speaker B: So what we're seeing here is basically just taking those similarity data, applying multi dimensional scaling. What it does is give you back a color wheel. So you have over here violet, blue, green, yellow, round to red. So this is the sort of representation that you see whenever you click on, you know, changing the colors in a computer program. But it's the representation that makes sense psychologically because it's the way that we sort of perceive the structure of colors as revealed by something like multi dimensional scaling. So this kind of approach of thinking about similarity in terms of distance in psychological space was, as I said, popular, but not necessarily universally accepted. And another psychologist, Amos Tversky, who is famous for his work with Danny Kahneman on analyzing the structure of human decisions.
00:12:39.528 - 00:13:43.442, Speaker B: You could read Michael Lewis's recent book about this soon to be a movie about the relationship between Kahneman and Tversky. So some of his less famous, but I think some of my favorite of his work focused on this question about how we should figure out the structure of similarity. And he wrote this paper called features of similarity, which basically said you can look at the, if people's similarity judgments are based on the distance between points in a psychological space, then those similarity judgments should obey the metric axioms. And so what we can do is go and look at whether people's similarity judgments violate the metric axioms as a way of testing whether that assumption about the nature of similarity holds. And so what he did was systematically take each of these axioms and then find examples that violate them. So, for example, you can show that human similarity judgments are asymmetric. So one of his sort of famous examples, although I think somewhat dated ones, was sort of saying that people are more likely to say that North Korea is similar to China than China is similar to North Korea, or likewise that an ellipse is similar to a circle than a circle is similar to an ellipse.
00:13:43.442 - 00:13:52.470, Speaker B: Right. So the fact that you could find those asymmetries for him was evidence that there was something more complex going on than just measuring a distance in a psychological space. Yeah.
00:13:52.582 - 00:13:58.142, Speaker A: Now, have they ruled out that this is some trick due to averaging or some bad?
00:13:58.238 - 00:14:35.684, Speaker B: Yeah. So the immediate response when he did this was to say things exactly like this. So, for example, one hypothesis, which was developed by Carol Krum Hansel, is what's called the density hypothesis. So the reason why you get this asymmetry is, as you go from, say, if circle is sort of, you know, basically this phenomenon is you're more likely to compare the sort of outlier thing to the prototypical thing than the prototypical thing to the outlier thing. And so if that prototypical thing is sort of surrounded by a cloud of points, you're kind of traversing more points, you know, sort of. You're traversing more density or something like that in that region, right. As you're going out than you are going in the reverse direction.
00:14:35.684 - 00:14:50.974, Speaker B: And so there's a sort of local deformation of the metric or something like that. Right. So you can do that. All the Tversky's arguments show is that making the assumption that similarity corresponds to something which is directly related to a metric is problematic.
00:14:51.054 - 00:14:54.046, Speaker A: But this similarity is average over many people.
00:14:54.150 - 00:15:15.374, Speaker B: Oh, so this. But you can get this from individual people. Individuals, yeah, that's right. Yeah. And so another example of one of these metric violations is violation of the triangle inequality. So the triangle inequality says, you know, the distance from a to c has to be less than or equal to the distance from a to b plus the distance from b to c. So, you know, Tversky presents a bunch of similarity judgments that violate this.
00:15:15.374 - 00:15:40.058, Speaker B: One of the examples actually comes from William James. So this goes all the way back to the 19th century. One of the first scientific psychologists, so, remember, James was living in the 19th century, his sort of canonical form of artificial light was a gas lamp. So he says the gas lamp is like the moon. Seems like a reasonable sort of, you know, like relatively high similarity. The moon is like a soccer ball. Again, relatively high in similarity.
00:15:40.058 - 00:16:38.788, Speaker B: But a gas lamp is nothing like a soccer ball, right? So if you have people write the similarity between this and this and the similarity between this and this, the similarity between this and this, then you find something where, you know, these two things need to be close in that space. These two things need to be close in that space. But then these two things are sort of, you know, pretty far apart from one another. Okay, so you've now done a semester of cognitive psychology in 15 minutes. And so now what I'm going to do is just for the rest of the talk, focus on how some of these ideas about human learning are things that can potentially, you know, give us some insight into things that we care about, about machine learning and vice versa, how some of the ideas that we've been talking about about machine learning can go back and give us better insight into the nature of human learning. And so my objectives for the rest of the talk are basically three things. So the first is that these methods that are used by psychologists were developed for providing insight into a complex learning system that has opaque representations.
00:16:38.788 - 00:17:38.616, Speaker B: Right. Uh, which I think in many ways describes the properties of some of the systems that we've been talking about here. So I think one thing that you can do is actually use some of these kinds of psychological methods to investigate representations in neural networks and ask questions about the properties of those representations. In doing that, we can also explore the correspondence that exists between the things that we know about how humans represent things and, uh, the things that we find out about neural networks. Uh, and then finally, as a consequence of the fact that the sorts of models that we've been talking about are things that are very successful in dealing with complex stimuli like images and text, we can potentially leverage neural networks to get deeper insight into human cognition. So when I've been talking about these experiments on human category learning, those experiments are, you know, are using very simple kinds of artificial stimuli. So I said, when I showed you that experiment which favored the exemplar models, they constructed a two dimensional stimulus.
00:17:38.616 - 00:18:02.306, Speaker B: What a two dimensional stimulus is, is something like this. This is an example of a two dimensional stimulus. It's a Gabor filter. It can vary in the spatial frequency of these lines and the orientation of the lines, two dimensions that vary separately. That's how you construct a stimulus. So you can plot everything on a nice two dimensional space, and you can say, okay, these are the things that people are sensitive to. And when psychologists do experiments, this is what they do.
00:18:02.306 - 00:19:11.334, Speaker B: They say, I'm going to understand how human categorization works. So now I'm going to go off and construct a very simple stimulus. And the reason why people do that is that it gives you knowledge of what the features are that people might be encoding and as a consequence, a high level of precision for testing model predictions. But it's worth pointing out that this is a long way from cats and dogs, right? Cats and dogs are not simple stimuli that vary along just a few sort of simple dimensions. And if you're trying to decide, make a decision about this striped stimulus, the nature of that decision making process might be quite different than the decision that you're making about this stimulus. So the idea is that to the extent that neural networks give us better ways of dealing with things like this, they actually might give us representations that we can use as a starting point for doing better experiments with people, that allow us to have a sense for what the stimulus structure might be for things like this, and then allow us to use other kinds of resources, like, say, crowdsourcing, as a means of trying to get back some of the precision that we lose as a consequence of operating with these more naturalistic kinds of stimuli. But as a consequence, we might get a much better kind of test of the psychological models that I've been talking about.
00:19:11.334 - 00:20:05.714, Speaker B: So what I'm going to do is, with those objectives in mind, talk about models that have been applied to the two cases we've heard a lot about so far this week, images and text. And I'm going to start with images. And this is joint work with Josh Peterson, Josh Abbott and Rory Battleday. I think I saw Rory over there, so you can talk more about it with him, if you're curious. Okay, so the canonical method that we've been talking about for working with images is convolutional neural networks. At this point, I don't need to sort of say a lot about how these work and what they do. There's been a little bit of work which has looked at how some of the internal representations in these models correspond to what's going on in the human visual system, where what they're doing is essentially taking the activations of nodes from those hidden layers and then correlating them against neural activity as measured by MRI or other kinds of methods.
00:20:05.714 - 00:20:59.604, Speaker B: And there's a sense in which trying to answer that kind of question is a relatively easy problem. It's basically a regression problem where you have, on the one hand, a set of activations in the network, and on the other hand a set of neural activations. And you're trying to sort of regress one onto another in order to figure out what the relationship might be. What I'd like to do is take those representations and ask a different question, which is how? Well the sort of hidden layer representations that we find in these models correspond to the psychological representations that people have. And the tool that we're going to use for doing that is exactly the same kind of tool that cognitive psychologists have developed for trying to understand the nature of people's representations, which is similarity judgments. As I was talking about before, we have a variety of ways of thinking about how to use those similarity judgments to try and reconstruct representations, things like multi dimensional scaling. And so as a consequence, we can take similarity judgments from people and similarity judgments from these models.
00:20:59.604 - 00:22:02.228, Speaker B: Essentially looking at inner products in hidden layers of these models and then look at the extent to which those actually correspond to similar kinds of representations of domains. So I'm going to show you an example where we collect similarity judgments for animals. So in this case, these are just taking images of animals, 120 images, and we then complete that similarity matrix that I was showing you before. So basically we show people pairs of these animals, and then we get people to judge on a scale from one to ten how similar they think the images are. And so what we end up doing then is collecting about 70,000 judgments like this, uh, using Amazon mechanical Turk, so that we have a reasonable number of judgments for every one of those pairs. And then we can take that pairwise matrix and we can look at how well the human similarity judgments for those images correspond to the, um, the, the similarity is evaluated against the representations in a convolutional neural network. So to do that, we took a set of pre trained neural networks that have been applied to imagenet.
00:22:02.228 - 00:22:15.584, Speaker B: Um, and, uh, we took inner products in the, the layer which is the last layer before the output layer of those networks. So it's like fc seven and Alex nut. And then we looked at how those correlated with the human similarity judgments.
00:22:16.604 - 00:22:19.524, Speaker A: Yeah, I mean, are you asking people.
00:22:19.564 - 00:22:31.428, Speaker B: The grade similarity on a scale of one to five or what? One to ten? Yeah. Yeah. And it turns out these judgments are pretty stable. Like this is a sort of standard psychological task. We get very good correlations. I'll show you in a minute what those correlations look like.
00:22:31.556 - 00:22:43.526, Speaker A: Quick question, since you just went, you know, through bad tversky results showing this, these aren't metrics at all. How does that come in? I mean, do you find that actually in your.
00:22:43.670 - 00:23:08.024, Speaker B: That's a good question. We haven't actually dug into that. So I'll get back to that stuff later in the talk, though. Okay, so this is what this looks like. So I'm showing here, this is r squared. So this is the proportion of variance accounted for for Alexnet, Googlenet and VGG. And then we also have hog sift features which are sort of the, I don't know, like prehistoric computer vision baseline at this point.
00:23:08.024 - 00:23:39.710, Speaker B: And what we find is that you can actually get a pretty good correlation between the similarities that these models are producing and the similarity judgments that people give. But this doesn't give you a lot of insight into what's going on in those representations. And using some of these methods, like multi dimensional scaling, gives you a better sense of what's going on. So this is just taking the best fitting of those. This is, I think, for the VGG representation here, this is a multi dimensional scaling solution constructed from human judgments. And over here, this is the multidimensional scaling solution from the Network. And what you should be able to see is that there's a difference between these.
00:23:39.710 - 00:24:30.884, Speaker B: There are these clusters that appear in the human judgments, and those clusters basically correspond to different taxonomic groups. So up here we have, this is reptiles and AmpHibians, this is birds down here, this is Primates, these are some large mammals. Those same sorts of basic similarity relationships appear in the representations that are coming out of the Network. So here are Primates, here are reptiles and so on, some birds up here, but they're not pulling out those clusters in the same kind of Way. And this is made more apparent if you use a hierarchical clustering analysis. So here on the top of the human Judgments, and we've just colored these to correspond to different classes of animals, primates, rodents, wildcats, and so on. And then taking the same color assignments, applying them to the clustering solution that you get from the neural Network, you can see that it's preserving a lot of the right sort of local structure.
00:24:30.884 - 00:25:14.672, Speaker B: But what it's missing is these high level distinctions that lie between these different groups. So one way of thinking about this is that what these distinctions correspond to is basically the sort of the taxonomic representation that people have for this domain. So it turns out, not necessarily surprisingly, that if you take a network which has been trained to do the imagenet classification task, it's learning a reasonable sort of perceptual representation for those objects. It's learning a reasonable way of differentiating them into classes. But it's not learning that those classes are things that we should think about as being related to one another or being very different from one another. And that's something which stands out in the human representations. So there are a few things that you could do from here, things which we are doing.
00:25:14.672 - 00:26:39.486, Speaker B: So one thing you can ask is, if we take these networks and instead of training them on that flat classification, we train them on a more hierarchical structure. Do you get something which more closely corresponds to the human judgments? You can ask if you use linguistic similarity data and combine those with the image based similarity data, or have a joint task where you're getting both images and text, does that change the similarity judgments? You can sort of ask a variety of questions about training tasks and architecture to see if you can actually produce something which is closer to what people do. But another thing that you could ask is, is there enough information in these representations that we can actually already reconstruct what the human representations are like. And one way of thinking about that is that if you think about that last output, that last layer before the output layer in the neural network, what the network is really trying to do at that point is to construct a representation, which is useful for answering a variety of different kinds of questions that you can ask a network questions about how you classify a whole bunch of different classes of things. And so the way that you get there is you take essentially a linear transformation and then apply a non linearity. And so we can ask, is this representation actually rich enough that we could nonetheless use it as a basis for reconstructing what human similarity judgments are like? And then that would help us get to the point where we can still use these networks as the basis for doing the kinds of experiments that I'd like to be able to do, where we use more realistic sorts of stimuli. And so we took that thought literally.
00:26:39.486 - 00:27:44.922, Speaker B: So a basis for human representations in the sense of, is there a way of taking a linear combination of these vectors that actually allows us to do a better job of reconstructing human similarities? And so to do this, we actually use a similarity model that Amos Tversky had introduced. So his feature based similarity model, essentially one part of that ends up being taking an inner product between feature representations, but doing so in a way where you're assigning different weights to each of those features. And so previously what we were doing was taking this inner product, but with the identity matrix here, we can instead say, what I'm going to do is take the inner product, but I'm going to learn a set of weights on those dimensions in order to best reproduce the human similarity judgments. And that turns into a pretty straightforward kind of regression problem. We can regularize that as much as we want. We regularize it in a way that means that if we shuffle all of the features and so on, we end up getting back. So we use cross validation to determine the extent of regularization and check that if we permute all of the features, we get essentially zero correlations out.
00:27:44.922 - 00:28:38.918, Speaker B: So we're sort of, you know, we're dealing with a large number of predictors and a relatively small amount of data, but we can appropriately regularize the regression problem so that we're actually getting reasonable solutions out. And when we do that, we get, as you would expect, a big improvement in performance. And in fact, the performance that we're getting out of the re weighted network representations is pretty close to the noise threshold in terms of what we see in terms of the human human correlations. And so we can then say, okay, using those re weighted representations, what do they look like? So this is multi dimensional scaling for that transformed representation. And you can now see it's very similar to the human judgments. And likewise, this is taking that transformed representation and then doing the hierarchical clustering. And you can see that essentially, there was enough information in that representation to allow us to pull out the taxonomic structure that was missing originally.
00:28:38.918 - 00:29:20.386, Speaker B: Um, it still makes a couple of errors. So over here, uh, there's a couple of, uh, small primates that get classified in with the rodents. Uh, and then for some reason, there's, like, one large animal that that thinks there's a bird. But other than that, it's doing a pretty good job of pulling out, but not just the perceptual information, but the relevant taxonomic information. Okay, so, so far, we have that you get a reasonable correlation out of the raw models, and then we can reweight that in order to actually end up with something that's a pretty good model for human similarity judgments is images. And we've reproduced that on a bunch of different domains. So this is just the sort of equivalent thing for this is, I think, fruits, furniture, and vegetables.
00:29:20.386 - 00:30:01.458, Speaker B: And again, we can use the same kinds of methods to construct reasonable representations in those domains. And then the test is when we take those representations and then apply the same model to a new set of images, whether we can actually predict reasonable things about human behavior on those images, and we find that, in fact, we can. So this is an example of an experiment which is doing just that. So what we do in order to do this experiment is we take another set. This is for animals. We take another set of 120 images of animals that weren't used in setting the transformation that we were using. And then we apply the model to that, we get out a similarity matrix.
00:30:01.458 - 00:31:02.826, Speaker B: We perform clustering on that similarity matrix and then use that to define two or three or four clusters. And then we train humans to classify those animals into those different clusters, and we look at how long it takes them to learn that clustering and what their performance is. And so the green line here corresponds to clusterings which have been established using the similarities that were generated by the re weighted network. The black line is based on the similarities that are produced by the original network. And so we're getting generalization to a completely new set of images and allowing us to make sort of substantive predictions about performance that people will show when they're actually performing a cognitive task with those images. But I think the really sort of interesting question that we can ask here is whether we can actually sort of fulfill this promise of being able to use these representations as a way of actually conducting strong tests of psychological models with naturalistic stimuli. And so we've taken a first step towards being able to do that.
00:31:02.826 - 00:31:54.214, Speaker B: And this is actually what Rory's been working on, is so, using a data set which consists of 300,000 human judgments of whether an image contains a bird or a plane for a set of 2000 small images. So these are like c fire images. So our code word for this project is project Superman. So people have to say whether it's a bird or a plane. Okay, so what's interesting about this data set is, I think, when you think about these datasets as a computer vision problem, you're focused on reproducing ground truth, right? So there's some ground truth which is associated with each image, whether it contains a bird or a plane. When we actually get humans to make judgments, we find that there are systematic deviations from that ground truth. So this is an example of an image which ground truth is a plane, but people are pretty down, sure, as a bird.
00:31:54.214 - 00:32:12.574, Speaker B: And this is an example of a ground truth images that are ground truth birds that people are pretty darn sure are planes. And so what we're doing here is not trying to reproduce the ground truth, but trying to predict the judgments that people are making about whether these things constitute birds or plants.
00:32:14.914 - 00:32:17.802, Speaker A: On an individual level, or uncertainty, this.
00:32:17.818 - 00:32:48.758, Speaker B: Is aggregated over people. Yeah. So, yeah, this is a case where we're averaging. So I told you before that when we were using those very simple stimuli, those simple two dimensional stimuli, the story about prototype and exemplar models was pretty clear. It said that it favored the exemplar models because people can learn sort of complex decision boundaries for those simple stimuli, and that's something that could only be predicted by those exemplar models. When we look at the. The categorization performance that we see in this data set, we actually see a different story.
00:32:48.758 - 00:33:47.450, Speaker B: So with these much more complex stimuli, if we take the representations that come out of the neural network models I've been talking about, there's actually very little difference in the performance of prototype and exemplar models. So this is a case where, in fact, you can do a pretty good job of explaining people's classification performance using a prototype model, despite the fact that, you know, these are necessarily pretty complex kinds of boundaries. And, you know, if you're a machine learning person. The reason why that's the case should be pretty clear here. This is a situation where we're taking a complex stimulus, but we're also constructing a complex representation for that stimulus such that it's possible to apply a simple classification rule to decide, you know, how these things get classified. So if we actually look at the, the, um, the representations that come out of the network, um, the blue ones over here are the ones that people think are birds. The red ones over here are the ones that are planes, and they're sort of colored by the extent to which people believe that they go in each direction.
00:33:47.450 - 00:35:03.923, Speaker B: And, uh, you can see that, you know, that simple prototype model is going to be adequate in this space because you can do a pretty good job by sticking a linear boundary in here. And so I think from the perspective of psychological models, this tells an interesting story where by focusing exclusively on very simple stimuli, then the only way that you could produce complex behavior was by having complex classification boundaries, but by thinking about more naturalistic, more complex kinds of stimuli, where we can use methods like this to construct representations that are sufficiently complex that we end up getting to the point where you can actually use simple classification rules to identify them. And so I think that sort of gives you a different way of thinking about the nature of categorization. That is, something that reveals what the consequences might be of actually being able to operate with more realistic, more naturalistic kinds of tasks. Um, we can also do fun, cool stuff like, uh, this is the prototypical bird, uh, and this is the prototypical plane. Uh, so we can actually figure those out from, uh, the human judgments and try and actually estimate what the structure of those categories might look like if you use those particular models. Okay, so the second example I'm going to talk about is text, again, an example we've been hearing a lot about.
00:35:03.923 - 00:36:25.254, Speaker B: And I'm going to focus on the case of vector space models. So, um, we heard, uh, Chris gave a great introduction to, you know, vector space models and sort of set this up. So these are models that use sort of relatively simple neural networks as a means of constructing a representation of each word as a point in a vector space, and then turn out to have interesting emergent properties such as, you know, vectors, uh, vector addition in that space being a way of producing certain kinds of simple relations and simple kinds of analogies. So as soon as I start talking about the idea of a vector space model, now that you have all been educated as cognitive psychologists, the thing that might spring to mind is that we just talked about some arguments about why spatial representations might not be good for capturing the nature of similarity. And so, since I already took that semester of cognitive psychology, when I first heard about vector space models of language, that's what I thought about Mark Stivers and Josh Tenenbaum and I wrote a paper in 2007 where we showed that these properties that Tversky found for similarity judgments, you can find for semantic association, which is basically a kind of linguistic measure of similarity. So, what semantic association is, is something that you've probably done before. If I give you a word like planet, you yell out the first word that comes into your head.
00:36:25.254 - 00:37:09.770, Speaker B: That was good. Okay, so Earth is the highest frequency associated planet. So the way that these data are collected, you get a whole bunch of undergraduates to come into the lab, or you could now do this online. You show them a word, they type in the first word that comes into their head, you remove all of the swear words and inappropriate things, and then you end up with a list of the words that people produce with different frequencies. So this actually came from data from 1998, before Pluto was demoted from this list. But so the way that these are collected, this particular set of norms, has about 5000 words that were generated by giving people a word as a queue. Then those people would generate associates, and then any word that appeared at least twice as an associate would be fed back as a queue.
00:37:09.770 - 00:37:49.954, Speaker B: And so you end up with this big square matrix of q and associate relationships. And then we can go look at the properties that appear in that matrix and you see asymmetries. So, for example, people are much more likely, well, again, undergraduates, to say beer, having seen the word keg, than they are to say keg given the word beer, or snake given cobra, then cobra given snake, and so on. And these asymmetries sort of abound in these data. You can also find violations of the triangle inequality. So, for example, asteroid is highly associated with belt, and belt is highly associated with buckle. But if you show people the word asteroid, they never say the word buckle, right? So, you know, it's a violation of the triangle inequality.
00:37:49.954 - 00:38:29.644, Speaker B: So, this paper that we wrote in 2007, we argued that these properties of semantic association are potentially problematic for vector space models of, you know, lexical similarity. And at the time, that was latent semantic analysis, which was the precursor of our modern vector space models. And so, in something like latent semantic analysis, the way that you would measure the relationship between two words was using the cosine of the vectors that corresponded to those words. And that's symmetric. And a monotonic function of a metric on a hypersphere. So it obeys a triangle inequality. And so to the extent that we can show these violations, those provide evidence against those kinds of representations.
00:38:29.644 - 00:39:26.494, Speaker B: But the modern vector space models that Chris talked about, one of the nice things that have come along with that is a more sort of natural probabilistic way of thinking about what those vector space models are doing, and a better way of kind of understanding how it is that they should be estimated. And so as a consequence of that, um, we have a nice sort of probabilistic interpretation that allows us to calculate things like conditional probabilities. So you can calculate a conditional probability from a vector space model, because there's a, an implicit joint probability model for the co occurrence of two words. And so we can take these conditional probabilities and then we can compare them. So these conditional probabilities don't necessarily obey the constraints that are properties of those spatial representations. We're no longer sort of being bound by a metric, and we can look at whether those conditional probabilities actually behave in the way that people do. So what we find when we do this is that, in fact, this is something which.
00:39:26.494 - 00:40:30.024, Speaker B: So the first thing we do is actually just sort of look at overall performance. And if you just look at models that are estimated from relatively small numbers of words, so on the order of millions, topic models still outperform vector space models. So topic models are models which sort of calculate conditional probabilities quite naturally and do so in a way which does a good way of capturing the different senses of words. So if you have, say, 6 million words, the first associate that people produce has a median rank of about 17 out of 5000 different words, versus 31 for word two back or 65 for glove. But if you take those vector space models, the real advantage of those models is that they can be trained on very, very, very large sopra, which includes, say, 6 billion words rather than 6 million. And then if you train them on those very large corpora, then they produce representations that actually do better than the topic models do. So you can get the best results that we've seen in terms of predicting human semantic associations come from the glove model, which gets this number down to about 14.
00:40:30.024 - 00:41:35.994, Speaker B: And these models are producing the first associate that people produce. So the earth that you said, they're doing that around a quarter of the time, and the rest of the time doing a pretty good job in terms of producing reasonable associates. And you can also show that the conditional probabilities that are computed from those models actually adequately capture the asymmetries and violations of the triangle inequality, which we also saw in people's judgments. So it seems like these sort of modern models actually do a pretty good job at overcoming some of the concerns that we had about those vector space models originally. But there's another context in which you can start to think about those sort of spatial constraints, which is in the context of thinking about analogies. So this model of analogy, where the way that you relate king to queen is by taking the vector which you have between man and woman. This model is actually a model that has a long history in cognitive science, going back to the 1970s, where it was proposed by Dave Rumelhaut, who is one of the co discoverers of backpropagation.
00:41:35.994 - 00:42:02.506, Speaker B: And this model also relies on a spatial representation and is potentially something which we can apply the same sorts of analysis to. So as a first step, what we did was actually try and evaluate this model using some human data. So a lot of. Oh yeah, what do you mean by model here? So what are they asserting exactly this, that if you have an analogy of.
00:42:02.530 - 00:42:04.346, Speaker A: The form, internal representation.
00:42:04.530 - 00:43:04.456, Speaker B: So they say if you've got a, if you have an analogy of this form, then the way that given a, b and c, the first three things here, the way that you can find d is by, if you have these representative points in a space, you take, you know, the prototypes. No, you just have, you just have each word being, or each concept being represented as a point in a space. So in this paper, they did this for animals. Yeah, that's right, they did this for animals. They took human judgments of the similarity between animals, did multi dimensional scaling to construct a multidimensional scaling representation, and then presented analogies like, you know, force is to elephant as. I don't know what their analogies were, but yeah, so they're assuming that you've got that spatial representation and then you, and then you'll apply this model. So a lot of the analyses that have been done of models like word two Vec are focused on sort of relatively artificial relational pairs, so things that you could generate automatically, like the relationship between a country and its capital city.
00:43:04.456 - 00:44:05.058, Speaker B: So we generated a new database of human relational similarity judgments across a pretty wide range of relationship to try and understand the circumstances under which this kind of model works well. And our starting point for doing this was a dataset called the semival task, two dataset. So this consists of 79 subtypes of ten different relation types and gives us a sort of basis for systematically beginning to explore the space of different kinds of relations. So what these relations are like, there are class inclusion relations. So say the relationship between a flower and a tulip, pothole relations like a car and an engine, similarity relations like, say, simmer to boil, contrast relations, old to young, attributes, beggar to poor, non attributes, fire to cold case relations, soldier to gun, cause, purpose, joke to laughter, space time, library to book and reference, siren to danger. So these are the kinds of relations that we looked at. And as a first pass, what this data set actually contains is a set of these pairs that were generated by people where they were just asked to come up with.
00:44:05.058 - 00:44:38.542, Speaker B: They were sort of shown an example of a relation and said, okay, now come up with some other things that obey that relation. And so we could take those examples that people had generated and do the same kind of analysis that was done in terms of, you know, actually looking at what the vectors look like that are generated by those pairs. And this is what this looks like across these different types. So the first thing that you should be able to see when you look at this is that there are some types where the. So this is doing a principal component analysis of the sets of points that correspond to each of the pairs in each of those relations. And there are some types like this one. This is object state relations.
00:44:38.542 - 00:45:23.234, Speaker B: So like coward to fear, where it does seem like that linear sort of transformation parallelogram model is going to be a model which does a good job of capturing those relations. And then there are other types which are very different from that. For example, contrast or similar, where the underlying relation is something which seems to be represented quite differently geometrically. So what we did was actually then have people generate ratings for a whole bunch of these types. So this original data set wasn't ideal, because people had just sort of produced things and then rated them for prototypicality. What we wanted was actually getting relational similarity pairs. So we generated a whole bunch of things are going to get a little meta for the rest of the talk.
00:45:23.234 - 00:46:13.006, Speaker B: So we generated a whole bunch of pairs of pairs and had people write the similarity between those pairs of pairs. So what they're doing is giving us a judgment of whether, how similar the relation is that it's expressed between those pairs. And then we could look at how well we could predict those relational similarity judgments using, in this case, the cosine between the vectors, or the euclidean distance between the endpoints of those vectors. And so what we find is that there's a clear variation across different relation types in terms of how well they're captured by this model. So some relation types, like case relations, soldier to gun, or the other example that I showed you, which is like coward to fear. So, attribute relations are things that are well predicted by these models. It doesn't really matter whether you have the word two vec, model or glove.
00:46:13.006 - 00:47:17.564, Speaker B: There are other things like similarity or contrast, which are relatively poorly predicted. And so I think, you know, one way of thinking about this is that, you know, an interesting property of these models is that they reveal how a particular relation, the one that's been focused on, which is the relation of this particular vector transformation, corresponds to an aspect of human judgments. But there are many other kinds of geometric relations that exist that can be expressed in these data. And I think those other geometric relations are potentially things that give us a better account of particular semantic relations. And so I think this kind of just opens things up to starting to think about what the classes of geometric relations are that can be applied in these vector spaces that might capture other kinds of semantic relations that people perceive. And so we now have a nice data set for trying to answer those kinds of questions. The other thing that we did in terms of looking at these kinds of judgments was going back to thinking about Tversky's analysis of similarity and thinking about whether the same kind of objections might apply in the context of analogies.
00:47:17.564 - 00:48:01.688, Speaker B: And so my postdoc, Dawn Chen came up with some nice examples of things where you see the same kinds of violations. So, again, to the extent that we see these violations, they're arguments against being able to use a vector space model to capture all of the things that are going on in the way that people are assessing analogies. So here's an example. You can evaluate how good you think the analogy is. Hairdresser is to comb as picture is to baseball. Very good, right? Versus picture as to baseball as hairdresser is to comb. Okay, so you might not get the effect, because I got you to do both pairs of pairs here.
00:48:01.688 - 00:48:38.504, Speaker B: But if you present these to people, they think that this is a good analogy, and this is not a good analogy. And the reason why you guys maybe didn't get that is that basically, when you do that, people, I think it's basically a variation in sense, that if you're thinking about it in this context, you're thinking about the baseball as an object, whereas here, the immediate sense that appeals is baseball as a game. And clearly, a hairdresser at a comb is not the same relation as a picture to the game of baseball. And so we collected people's judgments for another set of 500 relational pairs in both directions. And when you look at this, you find a relatively large proportion of these that show statistically significant asymmetries in the judgments.
00:48:39.404 - 00:48:43.956, Speaker A: Yeah, I guess we discussed it offline, but. Yeah, that's an effect of priming. Right?
00:48:44.140 - 00:48:59.324, Speaker B: I don't think it's a priming effect. I think it's that it's priming in the sense that the word sense that you apply to this word is influenced by the word sense of this word. So it's a kind of slightly second order form of priming. Right. Like it's relational priming, something like that, yeah.
00:48:59.744 - 00:49:01.112, Speaker A: What frame is activated?
00:49:01.168 - 00:49:25.604, Speaker B: Yeah, that's right. So the other thing you can do is show violations of the triangle inequality. So dawn somehow was able to come up with examples that illustrate this. I don't quite understand how she did this, but here's an example. So here's an analogy. Nurse is to patient as mother is to baby. Are you good? Mother is to baby as frog is to tadpole.
00:49:25.604 - 00:49:40.034, Speaker B: Also pretty good. Nurse is to patient as frog is to tadpole. Not good. Right. So this is a. You know, I told you things are going to get a little meta. Right? Like, this is a relational violation of the triangle inequality.
00:49:40.034 - 00:49:58.362, Speaker B: In terms of, like, if these two vectors are pointing in the same direction, then this vector needs to be pointing in a similar direction. Right. So I actually showed this to Sanjeev, and he was like, well, you're being adversarial. And, you know, also, I mean, the.
00:49:58.458 - 00:50:01.666, Speaker A: Rommel heart model is saying that there's a subspace here, and there's a subspace here.
00:50:01.690 - 00:50:02.610, Speaker B: Right, right.
00:50:02.682 - 00:50:05.134, Speaker A: So there's nothing being said about that.
00:50:06.274 - 00:50:16.458, Speaker B: Well, we can try and figure that out. Yeah. So. But you can actually get, like, very large violations. So here's an example. So this is lawyers to books. This chemist is to beakers.
00:50:16.458 - 00:50:46.184, Speaker B: It's pretty good. Chemist is to beakers as librarian is to books is pretty good. But then lawyer is to books as librarian is to books is pretty bad. Right. And the reason is that, I think when you present the analogy in that form, what it does is it changes the way that you think about. Like, it basically encourages you to think about the differences between the ways that lawyers are to books and librarians are to books, rather than highlighting the similarities. So I think these examples just help to illustrate that maybe there's something more complex going on in analogy than just the vector spaces analysis.
00:50:46.184 - 00:51:50.684, Speaker B: And I think the goal here is really to say these representations give us a good starting point for starting to engage with some of these kinds of questions. But there are going to be other kinds of things that are going to come up as we start to think about how to capture some of the nuances of the actual judgments that people make here. So if we look across these two cases, I think there are some reasonable general conclusions we can make. So, first of all, off the shelf deep networks do surprisingly well in capturing human similarity judgments. So in both cases, we found that taking these models, they actually do a pretty good job of capturing human similarity judgments at a basic level. But in both cases, there's also important conceptual structure, which is still missing. And I think in the case of the images, one way to explore that is by looking at different architectures, different training regimes, and seeing if there are ways of constructing tasks that you can give to the neural network which produce a greater correspondence to what people do, maybe tasks that are better aligned with the kinds of things that people do with those images.
00:51:50.684 - 00:52:36.440, Speaker B: And in the case of text, thinking more broadly about the sorts of geometric relations that we might expect to see expressed and corresponding to semantic relations. The second is that using these representations, we can actually take them as a starting point and build pretty good representations that are at least good enough to allow us to begin to test certain kinds of psychological models with more realistic sorts of stimuli. But there are lots of open questions here. As I was saying, different ways we can think about training these networks, different kinds of data, different sets of architectures, and I think a broad range of applications of these ideas in psychology and cognitive science in terms of changing the things that we can do with these approaches. Thank you. I should say. Yeah.
00:52:36.440 - 00:52:40.724, Speaker B: Josh and dawn and Ida have asterisks here because they're looking for jobs at the moment.
00:52:44.104 - 00:52:49.004, Speaker A: Have people also talk about using other metrics than the syntax between.
00:52:50.384 - 00:52:51.604, Speaker B: In what context?
00:52:52.024 - 00:52:59.004, Speaker A: In all the contexts. Your starting assumption is, you know, you start with a matrix.
00:53:00.784 - 00:53:14.404, Speaker B: Yeah, sure. I mean, you can certainly do that. Right. So you can think about what we were doing in the first part as a kind of metric learning. Right. And we were just using the simplest variant on, you know, inner product as a measure of similarity. Right.
00:53:14.404 - 00:53:21.904, Speaker B: Which is just learning the weights on that inner product. But you could explore a more complex, you know, a more complex space of different kinds of metrics.
00:53:24.244 - 00:53:26.064, Speaker A: Like alkene or.
00:53:27.004 - 00:53:36.684, Speaker B: Yeah, yeah, that'd be interesting. Yeah. And there's actually a psychological literature which argues about that, too. So whether you want to use l one or l two basically has been most of that argument.
00:53:38.584 - 00:53:39.792, Speaker A: So in the first part of the.
00:53:39.808 - 00:53:42.736, Speaker B: Talk, you said if you reweight the.
00:53:42.840 - 00:53:45.160, Speaker A: Vectors, you actually find the hierarchical structure.
00:53:45.232 - 00:54:23.844, Speaker B: How did that work? So it's basically that the hierarchical structure is there in the human data, and we're reweighting it to try and capture the human data. So what it's showing is that I would think about it. I think there's, the most deflationary way of thinking about it is that there's just enough information that's contained in the hidden unit representations that you can build a model of it. Right. But then you can also, I think the less deflationary version of that is to ask why that information might be present. And I was kind of trying to make an argument about that. That really what the network's trying to do at that point is learn a representation which can be used to flexibly answer a variety of different questions about the images that you're seeing.
00:54:23.844 - 00:54:52.774, Speaker B: Yeah. So it just ends up being a regression problem. So basically what you end up doing is taking the Matlab of the features, and then once you've done that, then you just have a regression problem where you're regressing that directly onto the similarity judgment for that pair of objects. Yes.
00:54:55.674 - 00:55:06.178, Speaker A: I'm wondering, you show that for different networks, you got slightly different correlations. Do you see a correlation between how well the correspondence is to human judgment and how well they actually perform?
00:55:06.346 - 00:55:23.354, Speaker B: Yeah. So there is a correlation. So things that perform better generally predict better. There's not a perfect ordering, but we do see that. And the other thing that we see is the higher you go in the network, the better the correlation is. So if you look at lower levels, then you get worse correlation.
00:55:23.394 - 00:56:03.704, Speaker A: Another question I couldn't. The shannonist record, I couldn't help but think when you showed these violations of the triangular equality and symmetry, think that these things seem superficially similar to Kell divergence in the sense that if you have looking at a more specific versus more general, then you have large scale divergence one way, but small scale divergence the other way. I don't know if there's a question here. I'm wondering if there's anything. Can it be modeled as some kind of divergence or in terms of sampling or something like that? That would actually, to k, divergence explain these type of asymmetries and lack of triangle inequality.
00:56:05.044 - 00:56:38.840, Speaker B: Yeah, I mean, if you're so kl, divergence is not a metric, right? So, yeah, that's right. So, I mean, that's similar to the, like this thing I was saying, the density hypothesis. Right? Where essentially you're introducing something which is an asymmetric measure as a means of accounting for the asymmetry. And that could be a reasonable way of doing this. Sure. I mean, I think that's something you could try and evaluate empirically. I mean, the way that I think about those differences, we have a paper where we show that that just falls out of doing so.
00:56:38.840 - 00:57:29.454, Speaker B: Roger Shepard's original argument for how you derive that exponential generalization law, he actually gives a bayesian argument for it, where the argument is, what you want to do is calculate the probability that if you've got two, you make two observations, x and y, and you want to calculate the probability that y falls into a subset of the space that x belongs to. That's what it means to generalize from x to y, right? Is that you. You think that y belongs to a subset of the space which is the same as a set that x belongs to. And when you. So if you assume that you've got convex regions and an isotropic prior and so on, then you get out that exponential generalization gradient. But if you have violations of that kind of homogeneity, then you get out asymmetries. So.
00:57:31.374 - 00:57:43.206, Speaker A: I think there was a question. So, going back to this violation for triangle inequality thing. So I'm wondering why we believe it has to be a matrix, right? Every matrix has sacrificed triangle in a coil.
00:57:43.270 - 00:57:43.950, Speaker B: Doesn't matter.
00:57:44.062 - 00:57:50.674, Speaker A: LP or clean distance. But if it is a nonlinear function of the distance, let's say a sigmoid.
00:57:50.974 - 00:58:19.774, Speaker B: No, this is a weak argument in the sense that it's saying, you know, this is an argument. It's not an argument that there's something wrong with the spatial representation. It's an argument that there's something wrong with using a metric as a means of measuring similarity in that space. So the only reason to point that out is that the way that people measure similarity in those spaces is using metrics. So it doesn't have to be that way. And I think you can then explore whether there are other better non metric ways of answering those questions.
00:58:21.934 - 00:58:26.470, Speaker A: Son, the results where you have the analogies that didn't give linear relationships.
00:58:26.542 - 00:58:27.174, Speaker B: Yeah.
00:58:27.334 - 00:59:00.786, Speaker A: While I wouldn't want to claim that everything will come out linearly, I kind of wonder whether some of those examples are sort of unfair, because I kind of think you might hope to get a linear relationship where there's a clear semantic relationship between the pairs, such as sort of gender is the currency of, is the capital of. But, you know, some of your ones that didn't work were things like, you know, not an attribute of or similar to which it seems like they aren't clear semantic components that you'd expect.
00:59:00.850 - 00:59:58.654, Speaker B: No, and I think that's right. I mean, this is, all that this is saying is. I think the conclusion that I would make here is that there's a broader set of geometric relations that correspond to kinds of semantic relationships, right? So it's. I mean, I'm not sure anybody made the claim that every semantic relation is going to be encoded by, you know, a vector in this space. But, you know, I think there's sufficient reason here to believe that that's not the case. We might have believed that already, but at least we can quantify this and say, what are the kinds of relations that actually seem to be well captured by that assumption? And then here are some other kinds that don't. For those other kinds that don't, you can then think about, are there other sorts of geometric relations that do a better job of capturing that semantic relation? And a simple example is similarity, right? So similar, you know, similar as a relation is better captured not by adding a vector, but by saying, you know, these points that it's a constraint on the norm of the vector that relates to those points.
00:59:58.654 - 01:00:26.516, Speaker B: Right. And so I think you can. One of the things we've been trying to do is think about, well, how do you, if you take those two as examples of geometric relations, right. One that's a constraint on the norm, one, which is saying you want to do vector addition. How do you generalize the class of geometric relations such that you can then figure out on a semantic relation per relation basis? For semantic relations, what's the right way of thinking about this relation when you've got these points expressed in a space like this? And I think that's kind of an interesting question. Yeah.
01:00:26.620 - 01:00:39.784, Speaker A: I was going to ask a question about the triangle inequality. So you had one for vision and one for text, and it seemed like they're actually quite different because the text one, you using the asteroid belt buckle, it seemed like the problem there was that belt actually has sensors.
01:00:39.904 - 01:00:40.592, Speaker B: Yeah, that's right.
01:00:40.648 - 01:00:48.440, Speaker A: And so I guess I sort of felt that there are two different things going on there, whereas the vision one felt like there was something more fundamental.
01:00:48.512 - 01:01:25.472, Speaker B: The vision one is that it's a change in the features that you're paying attention to. Right. So I think they're actually more closely related than that in that they both rely on a little bit of. There's a little subterfuge about what happens to the B items. Right. And it's that what happens in the text case is that the B item is changing the sense in which it's being used when you're comparing a b and B c. And in the other case that I showed you, the B item is changing the features that are being used when you're comparing a b and B c.
01:01:25.472 - 01:02:07.882, Speaker B: But you can think about those as being sort of pretty parallel things. I think what they both illustrate, though, is that you need to do something which is a little more complex than just thinking about. You have a point in a space and you're measuring the distance between those things in the space. So you need to say that the way in which we're measuring distance between points can differ for different points based on the features that seem most relevant in the first case. And you need to say that there are potentially, when you have a word which could be belt or it could be baseball. Right. The way that you represent that word is going to have multiple senses, and you want to differentiate those senses and decide what sense it is when you're trying to make those comparisons too.
01:02:07.882 - 01:02:21.294, Speaker B: Right. And both of those are sort of somewhat non trivial cognitive operations that we do implicitly. And so for that reason, we don't recognize that we're doing something more complicated than measuring something like a distance.
01:02:23.414 - 01:02:27.074, Speaker A: By the way, word sensors. You can also do very simply by linear.
01:02:27.574 - 01:02:37.834, Speaker B: Yeah, but you want to. I mean, the thing here is that when people are making that judgment, part of what they're doing is making a judgment about what sense they should be making it in. And that's kind of what makes it slightly more non driven.
01:02:40.494 - 01:02:44.354, Speaker A: Can you use the human similarity data to improve the accuracy of the models?
01:02:45.294 - 01:03:42.994, Speaker B: Oh, so, yeah, this is interesting. So if we take the imagenet models, say if we take the model that was for animals, we take our transformation for the animals, and then we take the untransformed version and the transformed version, and then just train a very simple classifier to reproduce the imagenet categories that are represented in our sets of animals. We find that you get a slight deficit in performance as well, consequence of the transformation to fit the human similarity judgments. So the way that I would interpret that is that, you know, that original model is kind of overfitting the task, which is the imagenet task, whereas the representations that people have are there for doing other kinds of things. Right. And so I think the way that you might see an advantage is if you considered a wider range of tasks, for example, doing hierarchical classification or trying to have a wider range of labels or things like that. Those are the things where I think you might actually see a benefit from changing the representations.
01:03:42.994 - 01:04:10.094, Speaker B: But I think the other reason why it's useful is if you're in a context where, I mean, this relates to what Trevor was talking about, about explainability, that if you're in a context where you're using these models and you need the representations, that the model has to make sense in terms of what people are doing, then the transformations that we're learning are things that are hopefully sort of aligning those representations more directly. And so I think there'd be a set of applications where it'd be relevant to have those transformations, thank God.
