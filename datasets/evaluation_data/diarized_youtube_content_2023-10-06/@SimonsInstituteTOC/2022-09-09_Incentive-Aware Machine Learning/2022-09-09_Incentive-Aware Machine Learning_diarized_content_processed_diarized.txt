00:00:00.600 - 00:00:34.684, Speaker A: Take them. Hello, everyone. I'm hara. So I'm going to be at UC Berkeley for the year as a postdoc, and then I'm joining MIT as an assistant professor at the OR Stats group. And I'm going to talk about incentive aware machine learning for decision making. Apologies to those of you who heard some sort of this presentation during my job market last year. So I'm sure most of you in this audience know that machine learning algorithms for decision making are almost everywhere nowadays.
00:00:34.684 - 00:01:49.484, Speaker A: For example, a machine learning algorithm may decide whether we qualify for a loan or not, whether we will get probation, whether after an interview with a video interview software, we will be invited for an onsite interview and ultimately be given the job of our dreams, and also whether students who are applying to colleges will be admitted to the colleges of their choice. So, because now these decisions are so consequential for humans, people oftentimes try to change to strategize with the data that they feed into these machine learning algorithms in an effort to obtain better outcomes for themselves. So in the loan approvals example, people may try to increase the number of credit cards or bank accounts that they have, or try various ways to improve their credit history. In the college admissions example, people may try to improve their GPA, retake the GRE, pay for tutoring classes. And we've also seen this like very interesting strategizing, which is to change schools in order to obtain a better in class ranking. And similar examples of strategizing exist also in the video interview software. So, for example, people may try to dress a certain way to hide piercings or tattoos and change the way that they talk.
00:01:49.484 - 00:03:07.014, Speaker A: So what is the problem here? The problem is that if machine learning algorithms ignore this type of strategic behavior, then we risk making policy decisions that are incompatible with the original policies goal. And this is pretty much what I'm studying in my research agenda under the name of incentive aware machine learning, where we studied the effects of strategic behaviors both to institutions and society as a whole, and propose ways to robustify machine learning algorithms that are deployed for decision making. Now, every time that you hear the word incentive aware machine learning, I'd like you to keep in mind that this is a system that has three key stakeholders. The first stakeholder is the institution. These are usually the mechanism or algorithm designers who will create learning tasks for accurate prediction in an effort to obtain some goals like profit or justice. In case of the justice system, the second stakeholder is individuals. These are the data providers, and they can strategically change their data however they deem fit in order to get the best outcomes for themselves.
00:03:07.014 - 00:03:46.136, Speaker A: And then lastly, the third stakeholder is society as a whole. The actions that society can take are very broad. Society can regulate. We can create public norms and expectations and public pressure to institutions about the types of algorithms that we want to be deployed. And the goals that society has, again, are very broad. We strive for fairness, we strive for robustness to bad actors and an increase in the welfare of the population. This is a fairly broad categorization of the three key stakeholders, and I'd like to give you a flavor of the types of settings that I've worked on so far.
00:03:46.136 - 00:05:25.606, Speaker A: So, from the side of the institution, thinking about trying to suppress the strategizing from the side of the agents, I've worked on algorithms that are robust to strategizing, and we define robustness in two ways, either with the standard more economic way of strategy proofness, or a new way that has to do with adapting with the incentives of the agents. Now, whenever we talk about robustness to incentives, we assume that all of the agents that are going to come to our system behave exactly according to the behavioral model that we have posited for them. So, moving away from these very strict assumptions, I've started working on algorithms that are robust to irrationalities in these types of settings, which means that some of the agents that we're going to face are not going to behave according to the underlying behavioral model that we have now, from the societal point of view. I have worked on the societal effects of non transparency for incentive aware machine learning systems, and I'll come back to that closer to the open questions that I'm going to tackle in the future. So, to give you an idea of the flavor of results that my work has provided, I'm going to talk to you about the strategic classification model, which we worked on with my advisor during my PhD, Yulian Chen and Yang Liu. And this was disappeared in Noreps 2020. So I'm going to be talking about a repeated interaction between an institution and individuals that happens over capital t rounds.
00:05:25.606 - 00:06:05.008, Speaker A: At the beginning of each round, nature chooses a feature vector XT. This XT, you can think of it as like having the original GRE, the original application features of the agent. The learner picks a classification rule alpha T. This is linear in the D features, and the d plus one dimension corresponds to the intercept that you can have. Now, the agent observes the classifier alpha t that the institution deploys and their true data point xtyt. But they do not directly reveal xtyt. They instead decide to report something potentially changed.
00:06:05.008 - 00:07:03.224, Speaker A: So this is the x hat t of alpha T. So how should you think of this x hat t being generated? Imagine that your data point was like this thing here. So this can correspond, for example, to having like, I don't know, a gre of this, and like an in class ranking of this big. Now the way that we allow the agents to move in this setting is we allow them to have a budget of delta that they can move around. And if the classification that they are assigned to from the algorithm is minus one, they try to jump the classification and fool us. So this is the way that we assume that the x hat t is produced and we call these agents delta bounded. So what does the learner observe? Now the learner observes this report from the agent and incurs binary classification loss, which is essentially one every time that the learner makes a mistake in their classification.
00:07:03.224 - 00:08:19.358, Speaker A: Now the goal of the learner, and remember, this is what we're trying to do mathematically, we're optimizing for the learner's goal is to minimize the Stackelberg regret, which is defined as the difference between the cumulative loss of the algorithm minus the loss of the best fixed classifier. In hindsight, had you given the opportunity to the agents to best respond just so that we're all on the same page? Differences from standard binary classification. In standard binary classification, the data generating process gives birth to data, and then I receive access to that data. I put a middleman in between. So I'm saying that data generating process produces the data. This data informs the person's decision and according to the strategizing of the person, this is the data that I received. So the take home message, and I won't have time to outline the techniques, but please feel free to find me afterwards, is that if you account for this delta boundedness, you can learn to classify delta bounded myopically rational agents in a nearly tight way, which means that we have both upper and lower bounds, and in fact, the regret guarantee scales adaptively to the power of the adversaries.
00:08:19.358 - 00:09:26.084, Speaker A: This means that if the adversaries are very, very good against us, then I will incur linear regret. But this is nearly tight. So what I've done so far is I've told you about this strategic classification problem as if all strategies, all strategizing is bad. And so if we do believe that all strategizing is bad, one way to overcome it, the complete extreme, is to have obscure machine learning algorithms. These algorithms stop strategic behavior altogether, but of course they are non transparent by definition, on the other extreme, especially when we talk about algorithms for decision making, if we have transparent algorithms, public algorithms that people can probe and they can understand them, these algorithms can potentially incentivize efforts for outcome improvement. But, of course, these are prone to strategic behavior, as we've seen. I think that what lies in the middle is interpretable machine learning algorithms for new definitions of interpretability that take incentives into account.
00:09:26.084 - 00:10:40.284, Speaker A: And I'd like to give you an idea very briefly in one slide of where I see this research going in the next couple of years. If this is the current state of the research in incentive aware machine learning right now, I think that the first set of questions that we all collectively need to address has to do with what happens if the learner designs nonlinear rules, for example, rules that are that coming from weird neural networks, or that cannot be just expressed exactly by a single linear function. And then we can play around with different models for agent behavior. Agents can either understand the rules fully, or what I believe is closer to reality, they can understand, like, approximations of these rules, and then try to best respond to the approximations that they understand. Now, the second step here, despite just building the perfect models in the theory world, is to really start talking to policymakers and lawmakers and people from the law and different departments to build case studies so that we can apply these results in real life. And I can give you, like, a very brief example here, which is that I was a trained computer scientist. Computer scientists will love randomization.
00:10:40.284 - 00:11:49.494, Speaker A: Policymakers and lawmakers hate randomization. So that's, like, a concrete example about why you need to think about these cases. And if you want to hear more about that, me with Ben Endleman and Josh avid. We gave a tutorial at fact 21 where we tried to translate this, like, incentive aware machine learning literature to lawmakers. And then finally, if we answer all these things, maybe we can hope to achieve, like, interoperability that is both robust to strategizing and incentivizes honest outcome improvement. In the interest of time, I will not tell you about the second theme moving forward, but it just has to do with the way that we perceive agents behaving in our settings. And I think that incentive aware machine learning offers an avenue for thinking beyond myopia and best response in settings where we deploy our algorithms for decision making, which I believe is the correct way to think about agents, as opposed to thinking about them being completely adversarial or satisfying some exact utility function.
00:11:49.494 - 00:11:59.424, Speaker A: This is a summary of what we talked about today. Please come find me afterwards if you have any questions or comments? And thank you very much.
00:12:04.484 - 00:12:07.264, Speaker B: Thanks a lot for the Lolita questions.
00:12:09.564 - 00:12:18.104, Speaker C: Pretty nice thought. And I wonder when you mentioned that you have a nearly tight bound follow request or can you establish any lower bound for that?
00:12:18.404 - 00:12:30.724, Speaker A: Yeah, yeah, that's why it's nearly tight. Like the. In the paper, we have both upper and lower bounds. It's the two bounds diver by a logarithm, but it's tight nearly up to the logarithm.
00:12:32.104 - 00:13:11.614, Speaker D: Hi Hara, as always, a very nice talk. I have a question about the last part of the myopia and all that discussion. So in the first part, if I got it correctly, we assume that the middleman somehow is able to choose perfect to play the best response to corrupt the data. Do we have anything like limited power to the middleman? Do we have a knowledge about what is happening in that case? Is that case interesting at all?
00:13:11.734 - 00:13:59.726, Speaker A: Yeah, that's a very interesting question. I talked about just the strategic classification paradigm in this talk. But of course, incentive aware machine learning encapsulates a lot of other problems. And that's what I was aspiring to give you with this triangle view of the world. So in other cases, like for example, learning in Stackelberg security games, that is part of this family of problems, there have been different assumptions on the middleman. For example, people have assumed that people can only approximately best respond, or they can best respond according to some quantile. And very recently we've seen works thinking about non myopic agents, meaning that agents that take actions at this round right now just looking at the future ahead of them.
00:13:59.726 - 00:14:12.314, Speaker A: So not exactly best responding. There have been studies for agents behaving as no regret learners in the context of auctions.
00:14:15.094 - 00:14:18.354, Speaker B: One more question and the next speaker can set up.
00:14:21.974 - 00:15:04.954, Speaker A: I'm having a hard time thinking about interpretable machine learning algorithms that are also like incentive aware. Can you give us an example of something like that? I guess contract theory. Everything is linear and it incentivizes the outcomes that you want. So can we have an analog for contract theory for machine learning? I think that's a faster way to describe, but yeah, it's like the way that I imagine this area moving forward is by defining the axioms that we want interpretable algorithms to satisfy so that they can simultaneously incentivize some actions as we want and suppress gaming. Yeah.
00:15:06.854 - 00:15:09.758, Speaker D: What do you mean with the analogy of the contract theory?
00:15:09.926 - 00:15:36.298, Speaker A: Yeah. So in contract theory, uh, the principal, it's like standard principal agent problems. The principal announces a contract, right? And according to the effort that the agent will place. The agent will be rewarded, paid. So you basically. Are we good doing it by laptop? Okay, so, yeah, but you. But the contract is kind of, like static.
00:15:36.298 - 00:15:58.854, Speaker A: So, like, you announce the contract once, then the agent participates in this problem, places the effort that they have to place when they. Once they're done, it's over. So this is interpretable because it's just a linear rule. So I'm wondering whether something like that can exist for non linear machine learning. Rules, authority, policy.
00:16:03.554 - 00:16:04.802, Speaker B: Thank you very much.
00:16:04.978 - 00:16:06.506, Speaker A: Thank you. Okay.
00:16:06.530 - 00:16:12.514, Speaker B: For the final talk of the day will be on adaptive online learning without prior knowledge by Chen Yu Wei. We get started.
