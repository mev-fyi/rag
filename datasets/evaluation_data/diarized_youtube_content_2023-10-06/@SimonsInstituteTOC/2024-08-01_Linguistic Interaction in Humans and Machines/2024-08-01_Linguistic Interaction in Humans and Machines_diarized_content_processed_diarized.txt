00:00:00.800 - 00:00:14.250, Speaker A: Video to start my talk. I don't know. For me at least, that video was. Was a lot. I remember watching this movie as a kid. The talk will be divided into two major parts. The first and smaller part, I'm going to talk about learning and collaborative linguistic interaction.
00:00:14.250 - 00:01:02.358, Speaker A: Much of this work was done by Elaine Soro sitting here doing your PhD. And the second part, I'm going to talk about language abstraction and ad hoc adaptation. I kind of like made like a try to squeeze maybe a bit too much. So I'm not sure I'll get to everything, but it's fine. Throughout, I'm going to kind of like jump back and forth between a cognitive science and machine learning, where cognitions research kind of contributes insights, experimental designs, modeling decisions to machine learning research. And then for this machine learning research, we're going to take the methods, the models, the data and the tools and try to get insights about cognition. I have to warn, I am not a cognitive scientist, so I have a feeling I'm going to say a lot of wrong things, miss a lot of references and stuff like that.
00:01:02.358 - 00:01:30.170, Speaker A: I apologize in advance and I'm very happy to be corrected. That's how I learn. Okay, so the first part we'll talk about learning cognitive linguistic interaction. I'll introduce serial bar. It's a scenario and game we designed for kind of like collaborative interactions. And then I'll do two things with it. I'll look at like, human language change in environments with flexible utility, and how it's a bit different than what we see in some other scenarios that have been studying cognitive science.
00:01:30.170 - 00:02:22.852, Speaker A: And I'll apply it to the scenario of learning to generate instructions where the aim is to kind of like to deploy a system, put it out there, have it interact with people, and have it continually improve over time purely from these interactions. Okay, so Celia bar, it's a work that was done by Claudia Anne and Elaine during her PhD. It's a situated, collaborative game with a very important component of natural language instruction. You have two agents. They collaborate, work together in the world to complete tasks, and there is a unidirectional natural language communication between them. So one of them is the leader, and it sends instructions to a follower. It's a relatively complex scenario in what it captures, but it's also very scoped, so it allows us to do research at the scale of academic research.
00:02:22.852 - 00:02:51.060, Speaker A: This is the serial bar interface. At the top here, it's the serial bar environment. It's procedurally generated, it includes a bunch of props that are around a bunch of different terrains and a bunch of cards that are in the environment. And these are actually going to constitute what the task is in this environment. There are also two agents. There is a leader. This is the colorful figure and a monochromatic figure that is our follower.
00:02:51.060 - 00:03:33.560, Speaker A: The goal of these two agents is to pick up valid sets of cards. And a valid set is made out of three cards where according to these patterns, each color, shape and count is unique in this set. So, for example, this is a valid set of cards. They select the cards by moving on them. Once the set is selected, disappears, they get a point and new cards appear and the interaction continues. The leader, the one that is hidden behind this thing here, writes instructions to the follower, and the follower has to follow these instructions and they both move in the environments. They're both selecting cards and collaborate together.
00:03:33.560 - 00:03:56.116, Speaker A: There are several design decisions that we made in order to make sure to really incentivize collaboration here. So one agent can't just ignore the other. The first one is, there is a very fundamental observability gap between the two partners. The leader sees the complete environment. This is actually the leader interface. And this is their view. They have a godlike view of the environment.
00:03:56.116 - 00:04:17.672, Speaker A: They can do perfect planning. They can figure out what's the best a next set to collect. Whereas the follower has a very limited first person view. So they only kind of like see what's ahead of them. They can't really plan. They're incentivized if they want to optimally operate in the game to just do what the leader tells them to do. On the other hand, there is a capability gap.
00:04:17.672 - 00:04:52.480, Speaker A: So the two partners, they move in turns and they have a limited number of steps per turn. So the leader has significantly fewer steps than the follower. So the follower is really the workhorse here. And if the leader wants to get stuff done, they need to use the follower effectively in order to kind of like maximize their score. Okay, so kind of like, this is a very quick example. I'm not going to run through it all, but you can see here, this is the leader turn. They're going to collect a card, they're going to write an instruction, and then it will be the follower turn.
00:04:52.480 - 00:05:16.374, Speaker A: The follower execute the instruction. As this continues, the reader instruction is actually ambiguous. So they correct it and they kind of like, and slowly they complete the set. I'm not going to run through all of it. So we use syllabus, a number of research projects, and with the first research project we actually did supervised learning. And to do this, we collected a set of human, human interactions. I'm not going to go over that.
00:05:16.374 - 00:05:56.726, Speaker A: But shortly after that, Anna Effenberger, in collaboration with Elaine, did an analysis of this data. This is not a very careful experiment, but we saw some really interesting dynamics. We tried to see what insight it gives us about how human language and behavior changes over time in this community. So what we see as expected, and something that happens in a lot of interactive scenarios that are studied, performance increases over time. People are becoming more experts, they're becoming better at the game. So the scores of partners in the game is increasing over time. These are kind of like deciles of like that.
00:05:56.726 - 00:06:34.140, Speaker A: We split the data periods during data collection. The path that they're taking are more ambitious. They're becoming kind of like their shortest distance is longer and there is less access move. So they're being basically, they're getting higher score, they're more efficient, they're just better as far as the language. And that's where it's actually started to become interesting. What we see is that this kind of like jumping instruction length in the beginning, and then it stabilizes or becomes a bit longer, but the instructions actually deliver much more information. So you can see that the words per card event, per selection is decreasing.
00:06:34.140 - 00:07:12.414, Speaker A: And we have more cards events, pair instruction. And this is not really what we expected to see. And the reason for that is because of finding in reference games. So what we see is that the community performance in serial bar increases over time. But as people form conventions, increase their expertise about the game, they kind of like form a common ground among the community. But the language becomes more complex during this period. And this is potentially because the benefit of increasing complexity when the utility of an instruction is actually flexible.
00:07:12.414 - 00:08:15.870, Speaker A: So you can increase the complexity, get more done, get more out of your instruction. And that was different than what has been observed so far in character science, like reference games that have been studied a lot in cognitive science, where the trends that you are seeing, and this has been repeated in experiments again and again, is that lens of senses decreases over time. Language becomes simpler, because as people form conventions, they form a common ground. They need language less, so the complexity of the language decreases. But once you give them flexible utility, something that doesn't exist in reference games, they actually balance between, yeah, they need less complex language to complete each task, but because the utility is flexible, they want to increase their utility, so they'll be actually making the language more complex. So you can think about the generalization of what we see in reference games, where the statistics that you see in the language itself are actually completely inverse. Although the conclusion is the same, the conventions allow you to kind of get more done with less language.
00:08:15.870 - 00:09:13.942, Speaker A: So this is kind of like the analysis on reading human data. And then one of the problems that we actually designed serial bar for was for learning from interactions. And Nori's work in that space was looking at the problem of generating instructions. So what you want to do is you want to learn, and this is back to conflict machine learning land that given a world state and a plan, we want to generate a natural language instruction to relate to the follower. So you think about it like this. The system, the model, is going to be the leader in the game, and the follower is going to just, is going to be a human executing the instructions when we relay the task to them using natural language. Now, the question that we were focused on is, can we learn to generate these kind of instructions by simply observing human users executing instructions that the system generated.
00:09:13.942 - 00:10:07.844, Speaker A: This is a bit different than how we usually think about language generation. Usually this is done by having access to a supervised set of reference instructions. In this case that human wrote, in this case, we don't have that. The system is just going to try to generate instruction, see what happens in the world with its counterpart, and try to use that as a learning signal. So why does this make sense? If you assume that you have a language proficient and collaborative instruction follower? So this is a built assumption into the system, then what does a good instruction do? A good instruction basically relays the system intent. Well, this will allow the follower to do something that looked what the system intended them to do. A bad instruction is either inscrutable or maybe makes sense in the environment, but will lead to an unintended outcome.
00:10:07.844 - 00:11:09.240, Speaker A: So if you compare what the people did in the environment to what the system intended them to do, you can get an indication of the quality of the communication channel, of the language you generated, and potentially you can use that as a learning signal to improve that communication channel. And now, this was heavily inspired by what we see in cognitive science and psychology, where a similar signal influence human language learning to various degrees. So there is this, like, the oldest result I know is from this paper from Krause in 1966, that show that confirmation feedback allows for like a convention formation, which is a form of really rapid, ad hoc learning in interactions. And it was reinfirmed again and again in reference games. We're going to talk about reference games, the second part of the talk. Okay, so just kind of like in very high level, I'm not going to go down into technical details here. The way our learning works is we're going to initialize the system with small amount of supervised data just to kind of, like, get it off the ground.
00:11:09.240 - 00:12:03.726, Speaker A: And then we are going to get into this, like, loop of, like, deploying the model, interacting with users, getting like a training data from these interactions, aggregating this data, and retraining our models. And then we're going to deploy it again, and we're going to repeat this process again and again and again and again. Okay. The data that we get basically is made out of, like, the world state. That's what the system that observed when it generated the instruction, the system plan, the instruction that the system generated, and whatever the user did in the environment from these tuples. What we do is basically we compute rewards by comparing these two elements, the execution and the plan. If they align to each other, then we know that this communication channel functioned well.
00:12:03.726 - 00:12:44.460, Speaker A: So this gives us a positive signal about what the system do. We want to reinforce this behavior. If they don't align to each other, then we know that this communication channel didn't communicate the plan well. It might be still describing the execution well. It's kind of like a counterfactual input, but it's definitely not relaying this plan well. Okay, so from this training data, we create, we compute binary rewards, and we get this scenario where we have the inputs, the output, and this kind of binary word. This is a very simple contextual bandit learning scenario, and we can use relatively conventional learning algorithms to learn from that.
00:12:44.460 - 00:13:27.374, Speaker A: Okay, so what we did is we deployed this in a number of studies. Our longest term study is that we repeated this process of, like, deploying and retraining for 14 times. So 14 rounds of deployment. And what we measured is that the main measurement was a task completion rate of. This is the human task completion rate. So basically asking how well does the instruction relays the system intent? And the overall performance increases significantly from 45% to about 80. The dashed lines are different complexities of the instruction system generated according to how many goals are in the plan.
00:13:27.374 - 00:13:56.916, Speaker A: And you can see here that the simplest ones actually pick up the fastest, while the most complex one actually take time to pick up. Earth's mover distance is another way of measuring this, and that's when we want to go down. So it's good. A very big confounding factor in this kind of studies is how much of this is because, not because the system improved, it's just because the humans adapted to the system. The humans are really good. They're very flexible. Learners.
00:13:56.916 - 00:14:26.668, Speaker A: So we teased apart this confounding factor by redeploying the initial system in a randomized experiment with the final one. And we see it basically none of this improvement is coming from the system actually adapting to the system. So this is net, the system improving. However, not everything is rosy. We see some not so pleasing trends as well. So this is what happens to the language during this process. The min instruction length decreases significantly.
00:14:26.668 - 00:15:06.410, Speaker A: That's okay. That's not so necessarily troubling. The system is maybe getting better at communicating its intent, and it does, but it's also, we see the vocabulary almost completely collapsing to a very, very small vocabulary. And that is because this is a closed system. The system is basically training on its own output. It's learning to focus, it's learning to refine, but its language is actually drifting farther from human language, and this is not a good thing. If we get to the last part of the talk, I'll discuss how we actually address it in a more recent research project.
00:15:06.410 - 00:15:33.990, Speaker A: Okay, so this is serial bar. We actually, since then we built a new version that is more professionally engineered. No offense to my PhDs, my ex PhDs, engineering skills. We actually had an engineer rebuild this and made it much more research friendly. You can go and play with the demo and you can download it. It's all available. It has all these interfaces to access the data and customize it in a much more comfortable way.
00:15:33.990 - 00:16:03.300, Speaker A: Okay, so I'm going to switch gears now. So this was like the collaboration. Now I'm going to talk about learning from language abstraction, Adobe adaptation mostly. Basically all this part is focusing on reference games. If there are any questions before that, I can take them now or I can wait to the end. Okay, I'll wait to the end. So, okay, so here I'm going to talk about a data set we collected for testing and reasoning about abstraction.
00:16:03.300 - 00:16:52.920, Speaker A: I'll talk about how we use this data to analyze abstraction in contemporary models, how we use this, how, again, how we use data to think about convention formation as a form of generalizable conceptual alignments in humans, and then talk about convention formation, alms. And if I have time, I'll get to the coupling of comprehension and generation. But this is more a side project, ambitious goal, I would say. Okay, so let's start. So what does this look like, dog? Anybody has a better imagination? What a mask. Oh, that's cool. And that's, I didn't get, that's, I never heard before.
00:16:52.920 - 00:17:00.372, Speaker A: Others. Crab. Crab. Yeah, crab. I heard crab is good. A couch. Couch is also good.
00:17:00.372 - 00:17:15.949, Speaker A: That's good. Okay, so, yeah, dog is the first. Is the first answer. So you are definitely a demean. It's a sleeping human. Some say it's a cat, a rabbit, a pig, and so forth. This is basically a tangram puzzle.
00:17:15.949 - 00:17:50.196, Speaker A: It's been used in cognitive science for the last few decades. It's really cool just because, to show here, because different people seeing these things, really different things, and it allows you to really get insights about their ability to abstract. And then when they actually talk about it for a long time, how they agree on something and form convention. It's a very good research tool. It's very good stimuli. Okay, so this project was done by Anya G. In my lab with the collaboration with Robert Hawkins and a number of other collaborators.
00:17:50.196 - 00:18:18.300, Speaker A: And basically, these shapes are tangrams. They're puzzles. They're made out of all of them. They're made out of, like, these seven very standard geometric pieces that are just put in different ways. And they are often used in reference games, starting at least from the 1980s, to study abstraction and convention formation in humans. And both of these questions are also interesting for models. But the problem with that is.
00:18:18.300 - 00:18:37.008, Speaker A: Oh, I forgot to explain. Who doesn't know what is the reference game here? I don't know if I need to define that. Okay, I will do that. So, reference games is super simple interaction, but really interesting one. It's basically you have a speaker and a listener. They share a visual context, which is a set of images. The speaker receives a target.
00:18:37.008 - 00:19:15.310, Speaker A: They need to describe it using natural language, and the listener, using the language alone, has to select the target. If they select the target correct, the game is successful. If they're wrong, the game is not successful. And it's been a tool of research that's been kind of used for decades. But one of the problems with using tangrams in this list is the research relies on a very small set of visual stimuli, like a dozen or 16 images that are used again and again in research over the years. Not very good for machine learning. We need more data, not a lot of diversity.
00:19:15.310 - 00:19:42.046, Speaker A: So the first thing we so something. So the first thing we did is we figured out that we can kind of, like, apply the ML playbook to this problem, and we scanned the tangram book and digitized over 1000 of these shapes. So it's not only a larger number than any kind of like the previous resources, it's also vectorized representations with standardized shapes across all of them.
00:19:42.078 - 00:19:51.070, Speaker B: Yeah, Shiri, why can't you just permute and just do whatever you want. Why do you need to digitize the book?
00:19:51.230 - 00:19:53.726, Speaker A: These images are not random. They are.
00:19:53.758 - 00:20:01.310, Speaker B: But whatever you do with the tangram, you can imagine it looks like something really not always.
00:20:01.650 - 00:20:29.420, Speaker A: I mean, some of them are really hard. And as you will see, that actually, depending on the shape that you construct, it has very different properties as far as how humans annotate it. And we'll see this in a second. We actually quantify that. Okay, so in addition to just kind of like having all these shapes, kilogram also comes with linguistic annotation. So each of these 10 grams comes with a whole shape description. So this is the standard way they are used.
00:20:29.420 - 00:21:11.546, Speaker A: Just name the entire shape. We also annotated part segmentation. So tell me each of the shapes, what part it constitutes in, like dog, for example, here. And what is the name of that part? So, for example, this dog has a front leg, it has a back leg, it has a head, it has a body, and of course, it has a nice curly tail there at the top. So what did we get from that? We got over 1000 tangrams, or 13,000 of these language annotations. So for each diagram, we have ten individual annotation, and then we have a set of tangrams that is a subset that we annotated much more densely. So we get a better representation of the distribution.
00:21:11.546 - 00:21:43.902, Speaker A: So we have 50 annotations for each, and we get a pretty rich vocabulary of like 4500 token types, especially considering the fact that these are just seven simple geometric shifts. Repeat again. So people really see a lot of stuff in these things. Now we want to quantify how diverse this data is. So we created three measures of diversity for 10 grams and our linguistic annotation. The first one is looking at how diverse is how people name these things. So this we can say.
00:21:43.902 - 00:22:31.302, Speaker A: So this is shape naming divergence. And what we get is this, like really skewed distribution where most of our data is very diverse, but it has this, like, really long tail of things where people are very consistent in how they name. So on, the more consistent and the higher, in the less low diversity side, we get shapes that get named the same or very similar names, whereas in the high diversity side, we get, like, a lot of creativity. So, for example, this here can be a trophy, it can be a robot, or if you come from the right cultural background, it's Princess Leia from Star Wars. I mean, I see Princess Leia when I look at this, like now all the time. We also look at how diverse is the part naming. This is also gives us a skewed distribution, but with a much fatter tail.
00:22:31.302 - 00:22:46.300, Speaker A: We have low diversity elements where people actually name the parts very similarly for the same timegram and high diversity cases. Sometimes even with the same name for the whole shape, we get like very different names for the different parts. Yes.
00:22:46.600 - 00:22:52.744, Speaker C: Did you like select the part areas and then they were naming them, or were they selecting which pixels belong to?
00:22:52.792 - 00:23:27.112, Speaker A: So they select parts, they select these geometric shapes. They don't select pixels, but this part. So I had kind of like an animation that goes for the annotation process. But the way it goes is we show them in tango and we add them. What is this? This is called, this is a. And they fill, and then they get an interface, and they click, they select a subset of the shapes, give it a name, next subset of the shapes, give it a name, until they kind of like named all the geometric shapes in the image. And finally, we also look at how they actually segment.
00:23:27.112 - 00:24:22.920, Speaker A: So this is the more kind of like purely visual part. This actually gives us a normal distribution where low diversity tangrams are always kind of like segmented in a similar way, where high diversity one. Again, sometimes even with the same shape name, they are segmented in wildly different ways. Okay, so this is kilogram. It's publicly available. And the first thing we applied it to is analyzing model generalization, kind of like just to see what, at the time how language and vision models can reason about this, kind of like abstract stimuli. And the idea was that models that generalize the truly generalized should kind of like lift their concepts into a more abstract space and should be able to reason successfully about these kind of concepts.
00:24:22.920 - 00:24:54.070, Speaker A: Now. So how would you do it? We do it using reference games and just kind of like this abstract stimuli. It's basically following the cognitive science textbook of analyzing these kind of properties. And kilogram allows do this in scale. So now we can use models to do that, and we can even fine tune them because we have sufficient data. So how does this reference game looks like? You get ten images. You get, you get the name of the shape, a reference.
00:24:54.070 - 00:25:32.446, Speaker A: The model goes and computes a distribution over these shapes. And then hopefully what it does, it selects. The one that has the highest probability is the correct one, and that's what it selects. This is the classical reference game. You get the shape and you get a reference to it, and you just select it. But because kilogram is richer from a linguistic perspective, because of the annotations, we can expand on this and we can create four types of reference games. The next, the slightly more complex slightly more rich one is that we also, in the text, we add the part name.
00:25:32.446 - 00:25:59.700, Speaker A: So we kind of like construct a templated sentence. Instead of just a flying goose, it's a flying goose with a head, wings, a neck and a body. So much more richer information in the language. We can also do the same kind of like the more just the whole shape name. But I. The tangrams themselves now has the kind of like part segmentation. So just the visuals and naturally the next one you can guess, we can throw everything together.
00:25:59.700 - 00:26:18.850, Speaker A: So we have parts here and we have parts marked here. So now if you kind of like, think about what's going on here. So it's a flying goose with the head. Oh, you can see the head. There is the wings marked here, there is a body, and there is the neck. It's like it becomes much more easier to reason about. Well, at least for humans.
00:26:18.850 - 00:26:57.150, Speaker A: Okay, we evaluated, we did both zero shot and fine tuning. I'm not going to show the fine tuning, actually. Don't think these are interesting, because for pre trained models, we want to see what comes out of the box with a zero shot. So we tried both wild and clip. This is at the time, these were the state of the art models that were available, and we also had humans play this game. And the result, as you can see here, both wilt and clip perform slightly better than chance, but pretty modestly better than chance. So they are pretty crappy.
00:26:57.150 - 00:27:40.204, Speaker A: And what is more striking is they don't really change with the additional information. So they don't really benefit from any of the part information and the segmentations that are in the image. Humans perform much better with a partial part information. These two here, either only in the language, only in the image, they show an improvement. It's modest, but when you combine everything, you see a really significant improvement here. And this is, by the way, is kind of an underestimate of human performance for various reasons. One of them is that we actually, when we did the component analysis of human performance, we identified two populations in kind of like a, in very hand wavy terms.
00:27:40.204 - 00:28:18.196, Speaker A: The people who got it and the people who didn't get it, and the people who don't get it are not great, but the people who got it are really good. Okay, so this is what we saw back then. But, you know, in preparation for the talk today, I also went and rerun some of this evaluation on GPT four. I didn't want to give OpenAI too much money, so I just did it on 1000 interactions. I tried to be really nice to the model. I hate that we have to be kind of like, to suck up to these models, but I wrote, you're a smart and imaginative individual, and so forth. I gave them instructions, I worked on it.
00:28:18.196 - 00:28:56.228, Speaker A: I massaged the instructions for a few hours until I tried to get performance to go up. Unfortunately, the farthest I got was GPT 423 percent and GPT 4.0 at 21% accuracy for 1000 games. So not doing great. I think it kind of like, remains an open problem of what this tells about what the models learn and kind of like their, their failure modes. Okay, so this is, so this was the kind of the first thing we did with kilogram. We applied it to models to see what kind of reasoning we get there.
00:28:56.228 - 00:29:33.620, Speaker A: The next thing is we turn back and try to study humans with this larger scale set of stimuli. Again, this was led with Anya, with collaboration with Ronald Yaff from Balan University, and of course Robert. And basically this one motivated by seeing this thing in reference games. So usually the very reference gamer started. There's something called repeated reference games, where you have a block of images and you kind of go over them in random, each one in target. You do the reference, then you repeat it again and again and again and again. And this is called the repeated reference game.
00:29:33.620 - 00:30:20.650, Speaker A: And what you see in this scenario is that people really form very tight conventions on the stimuli that they see. Again and again. Descriptions become short lexical choices stabilize, so they become really consistent, the communication becoming cheaper. They basically form a really kind of like strong common ground between them. What we wanted to see is if this kind of conventions generalize beyond this repeating stimuli. So basically we call it conceptual alignment. The question is like, if we give them a new stimuli now, something that they didn't form a convention on, are they going to display some kind of like, increase in the similarity of how they think about it? Okay.
00:30:20.650 - 00:30:50.836, Speaker A: And this requires a scale of stimuli that was not available in previous work, so kilogram is perfect for it. So we constructed a free stage experiment. First, we collect pre intervention baseline descriptions. So we get a two partner, two individuals. Each one functions separately, the speaker, and we get their description for about 20 tangrams. They think they're playing a reference game, but they don't get any feedback. They're actually nothing.
00:30:50.836 - 00:31:38.276, Speaker A: Nobody is actually seeing their descriptions, we're just collecting them. Then we take these two individuals and we pair them together to a reference game, and they're going to play this repeated reference game. So that's going to allow them to really form a convention over time, get an agreement. And you really see their descriptions kind of like converging to be short, consistent and the same. They're alternating roles, so they're actually going to, someone is going to, one of them is going to come up with a description, the other will adopt it, and slowly they will just repeat the same over and over again. Okay. In the first stage, then we go back to the context from the first stage, and we have them play a reference game on these contexts, but no repetition, so they don't have any chance to kind of like form conventions on them.
00:31:38.276 - 00:32:32.212, Speaker A: We just want to see how, what descriptions they get on these things, knowing that they're talking to the same partner. So then that's really important. And then what we do is we want to evaluate the description, the similarity between them here in the pre stage to the similarity between them in the post stage. Okay? And if they kind of, like, if their semantic space got closer to one another and it generalized to new stimuli, you would expect that the similarity between them here is higher than the similarity in the pre stage. And this is indeed what we are seeing. We see that the descriptions of different tangrams grew more similar following the interaction. And this is actually modulated by the content of the interaction stage.
00:32:32.212 - 00:33:14.446, Speaker A: So if the tangrams in the interaction stage are more visually similar to what we are going to test them later, the effect is going to be stronger. So we can think about it. The way I think about it usually is kind of like, you know, you can have like, these two semantic spaces of person a and person b. We're going to kind of like bring them together, kind of like a wormhole that connects the two on the 10 grams they converge on during the interaction stage. And then the difference for other tangrams is going to be influenced by how close they are to these 10 grams. It's almost like gravitational pull for the stuff that they agreed on. Okay, how am I with time? Eight minutes? Yeah.
00:33:14.446 - 00:33:36.004, Speaker A: Okay, so I'm not gonna finish. I'm not gonna get to the last part, but that's fine. Okay, so lastly, I'm just gonna. I'm gonna do this one quickly. This is a very recent work. It's not yet available on archive, but it's gonna come there soon. We wanted to see if LLMs are actually doing this kind of like convention formation.
00:33:36.004 - 00:33:51.948, Speaker A: And to do this, we didn't look at tangrams. We already know LLM sucks at 10 grams. Fine. We want to tease that problem apart. We looked at, like coco images. So the most within distribution, a visual stimuli you can think of for these models. We want to make it easier for them.
00:33:51.948 - 00:34:38.590, Speaker A: So we have this game. So we have four images and a speaker and a listener played this game of references. We have human data from, from previous work, and we created this framework that allows us to basically test how, if LLMs are forming conventions using this kind of data. And the ideal scenario is that LLMs would form these conventions without us asking them to do so, because that's what humans do. The LLMs have been exposed to tons of human data. It's very rich with conventions. You would expect them to display that as kind of like, part, just like they display so well, so many other aspects of language.
00:34:38.590 - 00:35:12.642, Speaker A: Unfortunately, it doesn't seem the case out of the box. We tested here, like, three, sorry, two open models, edafix and lava, and all the kind of like, big tech models, GPT four, Gemini and cloud. And overall, you can see that as far as, like, one effect of conventional formation is utterances becoming shorter. This is what the human does. This is the line here. And none of the models is displaying even a similar behavior to this kind of, like, out of the box. Another measure is like, lexical consistency.
00:35:12.642 - 00:35:46.400, Speaker A: So convention is not only making things shorter, it's also just sticking to concept that you both agreed on, so you reduce the cognitive load on your partner. And again, models are definitely not doing this. You can prompt them to do this, so you can ask them to do a gray scene, to be more gracian. That doesn't help. You can ask them to. You can do it in a more explicit way, and that starts doing it. So as far as lengths, you start seeing trends that are very similar to human behavior.
00:35:46.400 - 00:36:23.380, Speaker A: But as far as w and l, which is like word novelty rate, no, not yet, because they are still constantly introducing new words into the game, which goes against the whole point of just forming a convention. So if you're curious how it looks like this is a human, this is what the human does. They shorten what they say, and they converge on a very specific choice for a specific target. And this is what GPT four does. They do shorten here because we told them very specifically to do that. But for some reason, they just have to be. I mean, it just has to be creative, right? It keeps changing what it's saying.
00:36:23.380 - 00:37:11.490, Speaker A: You can overcome this with, like, even further kind of like, explicit instruction. Like, you like being very, very specific about what you do. The unfortunate thing about this instruction that really gets behavior similar to humans is that it's absolutely not generalizable. It's really about reference games. So this is the prompt we use, and it tells them when you reach a message that is very short, don't change it anymore. It's not a general way of forming or displaying conventions. So what we learned from here is that LLMs learn a lot from all the data they're exposed to, but for some reason, and I don't know why, they fail to acquire certain types of, of linguistic behavior.
00:37:11.490 - 00:37:52.700, Speaker A: Okay, I'm going to skip that part, so I can put 1 minute to kind of like, oh, sorry. To the questions that we were asked to pose. So I'm kind of like. I'm starting with like a goal. I really want to get to the point where we build dynamic models that learn in a more smart and effective, efficient way. Especially in kind of like, I want to see stuff models that learn from data in a much more efficient way. I'm less concerned about model sizes, maybe, although I would love to be able to fit them on the GPU's in my group, but I would like them to kind of like to just be smarter about the way they're learning and they're observing data.
00:37:52.700 - 00:38:47.620, Speaker A: And I think human learnings can really inform this kind of process. This is something that came up again and again so far in the workshop. I think, I think, I hope I was able to convince you that experimental interaction designs in cognitive science are really interesting and really informative for ML research. And a question that I ask myself all the time is like, what other designs transfer well to kind of inform our research allow us to understand these models better. I think I have this especially after what we see in conventional information. I have this kind of open questions of like, what models acquire well and then what linguistic behaviors they don't acquire well because they seem to be doing some things really well and other things pretty bad. And the menu is broader than just getting a lexical meaning and compositionality as well as they do, which is pretty impressive.
00:38:47.620 - 00:39:04.450, Speaker A: And finally, and this is much more abstract and kind of like blue sky is can LLMs help us kind of like develop and test a comprehensive theory of language? All right, thank you very much and happy to answer question in time.
00:39:09.670 - 00:39:59.930, Speaker C: I have a first question, actually a super cool talk. So my question, I guess, is, I guess it's really interesting to see how bad all the actually models like both clip and VAT and LLMs, how they all failed kind of consistently at all aspects of this task. I'm wondering if this indicates, actually, like I feel when you do this task, when I do it, it's basically a generative task that just, like, you know, kind of package it. A discriminative task, because I first have to, like, make up, you know, like, there's not enough information in ten grand, right? I have to make up what it might be, then I imagine that, and then I discriminate. And if I would do this with another person, I feel like, you know, we would both generate something, and then we would kind of decide what's probably the most, like, obvious thing. So I'm wondering if that's the reason, you know, why the models are failing, because they're treating it directly. Either they're just discriminative, or they're treating it as a discriminative task.
00:40:00.010 - 00:40:33.930, Speaker A: So when we experimented with the models, we experimented with, like, a stuff that is not abstract. So that's problem. Well, for the kind of like a convention formation in the last part. So that problem is kind of like a separate as performance wise, by the way, and I kind of like a scale skimmed over that really fast. These models are actually really good, right? So the performance is through the roof, so they are able to play the game. They're just not displaying the behaviors that humans are doing. If this was 10 grams that are abstract, this would have been terrible.
00:40:33.930 - 00:41:09.356, Speaker A: Now, why does this happen? I mean, I don't know. It's very hard to tease things apart, and I'm not sure how to do it. But generally, I think that these models don't acquire concepts in any way as dissimilar to how we acquire them. And to kind of like, to reason about abstract stimuli, this is really necessary. This is the kind of like the beautiful thing about. About 10 grams. And I think so as far as, like, do the models that these architectures have the capacity to do it? They do.
00:41:09.356 - 00:41:53.026, Speaker A: So if we actually, if we train wild, and I didn't show this with all, but if you fine tune wild from this abstract stimuli, it performs almost as good as humans, basically, and I'm sure GPTs and all the new ones, if you fine tune them, they'll do great. But when we train them, it doesn't seem like we have the right objective to kind of like, to go there. And there was one of the talks that remember saying that was talking about how much of our learning and reasoning is maybe based on our limitations and abstraction is really a sense of compression, and models don't have that. So maybe we are just. They're not really pushed in that direction. I don't know. Jacob is not happy about my answer.
00:41:53.026 - 00:41:55.030, Speaker A: It's very speculative, as they say.
00:41:56.890 - 00:42:12.150, Speaker D: Yeah, so I guess a low level. Oh, Jacob Andreas, a low level question and then a high level question. In the experiments you were showing with the sort of generalization task, which is super cool, how are you measuring similarity between responses before and after?
00:42:12.610 - 00:42:36.778, Speaker A: So we have. So we have similarity at text space, that is, image space. At text space we used ADA embeddings. And at image space we use the fine tuned clip model. For both of these cases, we deployed a data collection for similarity from humans and we correlate that. And they actually, and that's how we ended up with these measures. So they actually correlate well with human perception of similarity.
00:42:36.834 - 00:42:44.870, Speaker D: And do you have like a fine grained sense of what actually is getting matched or in what ways those things are getting closer to each other in the after condition?
00:42:45.610 - 00:42:49.026, Speaker A: In the sense of like more than just kind of like just in the embedding space?
00:42:49.098 - 00:42:49.710, Speaker D: Yeah.
00:42:50.450 - 00:42:58.266, Speaker A: No, but that's a good point. It's kind of like if you have any kind of like idea to the kind of. We're writing this paper now, I'll be happy to get them.
00:42:58.338 - 00:43:25.588, Speaker D: Okay. And then the high level thing. Sorry, if I can do one more, is I guess I would be interested to hear more about your intuition for why you expected these models to show human like, trends in the experiments that you have on the slide right now. You know, just in the sense that, like, when people are doing these tasks, there's pressure for concision because they're trying to, you know, get their crowd worker bonus faster. It's, you know, energetically expensive to move your lips, things like that.
00:43:25.604 - 00:43:26.236, Speaker A: Yeah. So the whole idea.
00:43:26.268 - 00:43:37.272, Speaker D: So is it like that? We've. You've actually think there's a lot of examples of convention formation, like happening in chat logs, in the training data or like, where does that signal come from?
00:43:37.336 - 00:44:11.626, Speaker A: So models don't have the pressure. So people show this because they want to. I mean, they, you know, they both at least is that they want to reduce costs. Generating language takes time, so it's basically a cost reduction mechanism. Models don't have that because they don't care how much cost they take. But yes, this is in the data and they're trained on human data. You would expect that they acquire the kind of like the cost constraints of human data, taste in theory and this kind of behavior.
00:44:11.626 - 00:44:30.148, Speaker A: So they're not trained on reference games, but reference games is a scenario that elicits this. It's not what this is kind of like built for. References are all over the play. It kind of like convention formation and reduction in description lens is all over the place in, like, newswire and any kind of, like, text on the Internet.
00:44:30.284 - 00:44:33.476, Speaker D: Well, but you see the end state. You don't necessarily see the.
00:44:33.628 - 00:44:48.500, Speaker A: No, you see the whole thing. Like, you know, you start. You start. You start an article, you read an article. As the article progresses and you form conventions, you see things shortening and stuff like that. I mean, my student is working on it now. So we actually see it in, like, we're working now on, like, a movie scripts.
00:44:48.500 - 00:45:14.012, Speaker A: And in movie scripts, you really see this kind of conventions form. So this is more dialogue data. But in general, this is a more general phenomenon. I mean, and it makes sense, right? If I just keep this high level of verbosity that I used to introduce things in the beginning, it will be a very annoying piece of text to read. Hi. Very nice talk. Convention usually is between two people, and then you have the memory.
00:45:14.012 - 00:45:38.662, Speaker A: So if you will add the smallest episodic memory of the conversation with the other models. Oh, that's a great question. Force them to use the same words again and again. So these models that we're testing, they have the entire interaction in context. So they have. I mean, I don't know how to compare it to, like, memory mechanisms in humans, but they see all the past of the interaction, so they. So it's.
00:45:38.662 - 00:46:16.652, Speaker A: It's kind of like all sits there in context. Something else to give it in the contextual window or saving the embeddings as an. That you retrieve, like, retro model. So I'm not sure. Okay, that's. I'm not sure I completely understand the kind of, like, alternative of retrieving embeddings or something, at least not in the LLM space. So be happy to hear more about, technically, what actually we can talk about as far as, like, it's listing, like, the way we think about lamp space.
00:46:16.652 - 00:46:36.520, Speaker A: They have the entire episode. They have access to it. And what you actually see out of the box, what happens here is they're actually using this memory very well. They just keep saying the same thing. And I think this kind of, like, follows up on, like, general conclusions about transformers. They have this tendency to just copy and recycle.
00:46:40.870 - 00:47:20.740, Speaker B: I have a follow up question. Okay, well, first of all, you still didn't convince me that you couldn't just permute the 10 grams and then send it to Turk and just collect the ones that are. That have. That seem to be semantically meaningful, and then you'd have a bigger data set. But we can talk about it later. The actual question is I kind of agree with Jacob and Uli that it's almost like you're ahead, expecting these models to have some idea of a social interaction, almost like theory of mind of another person in order to be able to do this agreement procedure. And here's another.
00:47:20.740 - 00:48:34.606, Speaker B: And I wanted to throw out another idea for such an agreement procedure in some way that maybe, I don't know if you've looked at. So my question is about, how many options do you give the person who needs to decide in the reference game? Like how many? It's a fourth choice, right? Between different. Because one thing we did a very long time ago with Manuel, who I think is not here now, but in my youth, we did this thing by Carnegie Mellon, where you had someone who was describing and who could see an image, and then somebody else who was looking for that image, but the search was in the level of a full blown image search engine. So now, and the question was, the scientific question was like, how do you elicit language from the describer that is very rich but very discriminative, so that if you let somebody really find a needle in a haystack, you have to get them to the right place and ensure that they get there. And that's a different type of, kind of like, you need to get to some shared understanding, but in a richer way. And I'm just wondering.
00:48:34.758 - 00:48:40.774, Speaker A: So reference games are an image, I mean, to some extent are an image retrieval task. You can think about like this.
00:48:40.822 - 00:48:42.838, Speaker B: Yeah, but you have like maybe five options.
00:48:42.934 - 00:49:20.382, Speaker A: You have four options in this case, in the tangom case, ten options, you have a small set of options. These options are selected to be hard in the case of like the cocoa images, to actually select images that are, were very similar in some kind of embedding space. I don't remember which one. But yeah, you can think about it just like it's a miniature task in that sense. Now. So the model and what the humans are doing, they find some kind of a balance between the verbose, they find description that is long enough to discriminate in this context. And this is our call, pragmatic reasoning comes into the, into consideration.
00:49:20.382 - 00:49:36.670, Speaker A: And you know how much it cost them to, to generate this kind of like a description? It's a bit different than the retrieval, because in retriever, you don't have a notion of the entire context, so you can't write. I mean, usually the difference is how.
00:49:36.710 - 00:49:45.344, Speaker B: Many options do you have to choose from? Like, do I have four options or do I need to search through space in order to retrieve.
00:49:45.392 - 00:50:08.586, Speaker A: Yeah. So these are just like apples and oranges. They're just different. The point of this game is that it's actually really important that you see the entire context because they all the set of images what? Because you actually can do pragmatic reasoning that is really tailored to this context. Right. When you see the images you can kind of like hit the switch. Sweet spot.
00:50:08.586 - 00:50:31.510, Speaker A: I'm not going to be too verbose. I'm not going to be, I'm not going to be over a, I'm not going to be overly descriptive. I'm not going to be under underly descriptive. Right. So this is actually, it's not a bug, it's a feature and it's not and when you kind of, if you think about like a large, you know, larger scale image retrieval it's just a different, you know, it's a different scenario.
00:50:34.210 - 00:50:40.170, Speaker C: Let's take a break. 20 minutes and that's also great. Thanks.
