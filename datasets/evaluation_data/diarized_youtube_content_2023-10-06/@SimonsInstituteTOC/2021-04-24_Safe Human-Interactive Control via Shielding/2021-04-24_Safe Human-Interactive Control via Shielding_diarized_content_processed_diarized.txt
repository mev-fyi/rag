00:00:01.040 - 00:00:14.954, Speaker A: It's my pleasure to introduce Osbert Bastani from the University of Pennsylvania, and he will talk about safe human interactive control via shielding. So, yeah, why don't you take it from here?
00:00:15.294 - 00:01:16.774, Speaker B: Okay. Yeah, thanks for the introduction, and thanks for inviting me to give a talk here. And, yeah. So today I'm going to be talking about some of our recent work on safe human interactive control, and in particular, using how we kind of use a game theoretic formulation and enable safety within this formulation. So the work I'm talking about is joint work with Jeevana Inala, Jason Ma, Shin Zhang, and Armando Solar Lazana. Yeah, so the problem that we're studying is settings where robots where there's interest in deploying a robot in a place where they must interact with humans. And this includes both cooperative settings where robots work with a human to accomplish a shared goal, as well as non cooperative settings where the goals of the robots and humans might be different.
00:01:16.774 - 00:02:37.086, Speaker B: For example, in self driving cars, which is going to be a running example throughout the talk, the robot car has to interact with the other human drivers on the road and pedestrians and so on, who have different goals than the self driving car. But it's also not a zero sum game because they all have a kind of high priority for not causing an accident. And in this general setting, the key question is, how do we model the human? And this is especially important in non cooperative settings where we need to reason about the human's behavior, the behavior of a human might who might have a different goal than ourselves. And kind of a very general solution is to model this interaction as a two player dynamic game. And, yeah, here I'm going to focus on the two player case, but a lot of these ideas extend naturally to multiplayer games, just to kind of give an example to illustrate some of the challenges involved. So here I'm showing a simulation where there's an actual human interacting with the robot using via keyboard input. And basically, so the robot is shown in red and the human is shown in blue.
00:02:37.086 - 00:03:20.554, Speaker B: And their goals are both to try and cross the intersection as quickly as possible. But beyond that, their first concern is to avoid an accident. Uh oh. Yeah. And as you can see, there's a pretty complicated negotiation process that happens at the intersection where the human and the robot are trying to pass first, but simultaneously trying to avoid this accident. And to achieve this kind of behavior, the robot has to be able to plan in a way that accounts for how the human responds to their behaviors. And this is why we need game theoretic formulations of the human's actions.
00:03:20.554 - 00:04:34.024, Speaker B: So just to dive right in, I'm going to start off describing how we formulate the problem. So, as I said, we're going to formulate this problem as a two player dynamic game. And basically what this means is we have a dynamics model f, which is a function of the current state, and both the robot state URT and the robot control input URT, as well as the human control input UHT. And then each agent has this utility function, which is just a sum over the rewards that they accrue over some time horizon, capital t. And each agent is trying to optimize their utility function, taking into account the behaviors of the other agent. And as I said earlier, a kind of key aspect of our approach is that we're going to assume that the human is behaving somehow in a rational way. In particular, we're going to assume that they're acting according to a Nash equilibrium strategy.
00:04:34.024 - 00:05:20.278, Speaker B: And, yeah, so there are, there is a lot of, and humans don't always act optimally. And there is a lot of work on kind of modeling how human behavior is boundedly rational. But I'm not going to be talking about any of that today. So we're going to just assume that the human is acting optimally. And I'll go into, I'll say, and we do have some assumptions about exactly what this means, which I'll tell describe later. And kind of, given this general framework, our goal is to construct a robot controller, PI R, which maps the current state xt to a robot control input urt. And there are two kind of goals.
00:05:20.278 - 00:05:55.814, Speaker B: So first, we want the controller to reach the goal as quickly as possible. So this could just be a goal region, and then we want to reach the goal region after some number of steps, but we simultaneously want to ensure safety. So we want to stay inside a safe region, x safe for the entire horizon. And as I said before, safety is really going to be the first priority here. Our goal was really to try and construct a safe controller. So there are two kind of key challenges in this setting. The first is the computational complexity of computing Nash equilibrium strategies.
00:05:55.814 - 00:07:01.934, Speaker B: And the second one, and possibly more importantly, the second one is that we don't actually know what the human reward function is in practice. This is the reward function rh, and without the human reward function, of course, we can't possibly compute the Nash equilibrium strategy. I'm going to give some background on one approach to human interactive control in this setting. Um, and there, there's, of course, been a lot of work, but, um, this work in particular, uh, provides a game theoretic form, provides and solves a game theoretic formulation of the problem. Um, so basically, the idea to solve these challenges in this prior work, um, first they're going to reformulate the dynamic game as a Stackelberg game, um, which turns out to be significantly easier to solve. Um, and to address the second challenge, the unknown human reward function. Their basic idea is to use a technique called inverse reinforcement learning to infer the human reward function from data.
00:07:01.934 - 00:07:52.104, Speaker B: So just kind of in terms of computational complexity, as I said, they formulate the problem as a Stackelberg game. And in a Stackelberg game, unlike traditional two player game, the agents are going to play sequentially rather than simultaneously. So what this means is that the dynamics are now decomposed into two parts. We're going to assume that the robot moves first, and then they're going to move alternatingly after that. So the robot dynamics are fr of the current state and URT. And once the robot has made its move, then the human is going to observe the new state, and they're going to take action UHT in response. And then that's going to get us the next state, X t plus one.
00:07:52.104 - 00:08:52.622, Speaker B: And just to kind of give a visual illustration of this, the idea is, so the robot in red is going to go first, then the human, and then it's going to come back to the robot, and they're going to continue playing alternatingly in this fashion. And this is an approximate, it is an approximation to the true kind of system. But intuitively, if you take the time step to be small enough, this should be a pretty good approximation of the original game. So, like I said, the nice thing about Stackelberg games is that they're easier to solve. And in particular, for the finite state and finite horizon setting, we can actually generalize kind of standard dynamic programming algorithm, or backwards induction algorithms to solve Stackelberg games. And yeah, I've written the, here, I've shown the rules for the robot. So, u r t here is the optimal policy at time t.
00:08:52.622 - 00:09:32.748, Speaker B: And it's a function of kind of the, and because we're going backwards in time when we solve this, it's a function of the future cumulative reward of the robot, j r t. So that's the value function of the robot from time t, as well as the human optimal policy, UHT, which is shown in red here. And that's because the human is moving after the robot. So the robot's policy on the current step is a function of what the human is going to do on the next step. And we can solve. Yeah, so this is the equations for the robots. We also have the equations for the humans, which I'm showing here.
00:09:32.748 - 00:10:45.812, Speaker B: And as you can see now, it's a function of the robots policy on step t plus one. And we can solve this backwards in order to get the optimal solution for both the human Nash equilibrium solution for both the human and the robot. And one nice property is that this actually computes a subgame perfect Nash equilibrium, which is a useful thing to have. So one caveat is that we're actually in not infinite state spaces. So we have a continuous state space because we're thinking about a dynamical system, which means that we're actually going to have a, we can't use backwards induction directly to solve the game. And in this paper, in this prior work, basically what they propose doing is just using gradient descent on both URT, so the robot optimal sequence of actions as well as the human optimal sequence of actions, and just running gradient descent on the corresponding objectives in order to solve this. And you can basically, and the reason you can do this is because you can write down everything explicitly in terms of maxes and mins over the future actions.
00:10:45.812 - 00:11:42.212, Speaker B: So the main challenge here is computing the derivative of the argmax, and you can do this using implicit differentiation. I'm not going to go into details on exactly how that works, but it's a pretty standard technique. Okay, so that's their, the way they propose to solve the Nash equilibrium strategies. The next thing that the second challenge is how to deal with the human reward function, which is unknown a priori. And like I said, the basic idea they use is to use inverse reinforcement learning to infer the human reward function from data. And I'm just at a very high level. The idea behind inverse reinforcement learning is to try and select a reward function r, which best describes the given data.
00:11:42.212 - 00:12:48.364, Speaker B: So here, d is the data set of human demonstrations. So we just have the human play the game a bunch of times and we observe what their actions are. PI of r is the optimal policy for a candidate reward function r, and l is a loss function that compares the state action pairs generated using policy PI with the data set, the human demonstration data set d. So there are various instantiations of inverse reinforcement learning depending on the structure of your loss function, the policy class, and the m structure of the MDP. I'm not going to go into details, but just kind of at a high level. As you can see, this objective is trying to select the reward function such that the state action distribution of the PI PI of r most closely matches the observed data action pairs d. So, yeah, in this paper, they propose to gather demonstrations of human behavior and then use off the shelf inverse reinforcement learning algorithms to estimate the human reward function.
00:12:48.364 - 00:13:39.284, Speaker B: So with the human reward function and their solution strategy, you can now compute Nash equilibrium strategies to the stack over game. So they're kind of. So this is a, they show that this is a pretty effective approach, but there are some significant shortcomings if you want provable guarantees. And the first challenge is that because they're using gradient descent, it's susceptible. In general, if the rewards and the value function are non convex, gradient descent is going to be susceptible to local minima. So they're not guaranteed to find a Nash equilibrium solution. Yeah, basically, if you converge to local minima, then the human might be able to take alternate actions than the ones that you predict they'll take.
00:13:39.284 - 00:14:49.070, Speaker B: And the second challenge is that the human reward function is while they're using data to infer it, there isn't really, this is just a machine learning algorithm, and there's no guarantee that the inferred reward function is correct and you can deal with the inaccuracies due to noise, but in practice, different humans will have different reward functions. So even if you learn a reward function using a data set from some subset of humans, it might not generalized to reward functions in other settings or with other human drivers and so on. So therefore, again, we can't really guarantee that. If we can't guarantee that the inferred reward function is correct, then the human might act differently and we can't ensure safety. And our goal is really to design a robot controller that guarantees that the, the system is safe. We are also going to make a best attempt to reach the goal, but we're not going to provide any guarantees in terms of this. And of course, we can't make unqualified guarantees of safety.
00:14:49.070 - 00:15:33.736, Speaker B: So we are going to need to make some assumptions about the human reward function in order to be able to reason about what the human is going to do. And our goal is to really minimize the required assumption. So we don't want to assume that, that, you know, that we have a representative data set of human behaviors or that the algorithm finds a, you know, a good approximation of the human reward function. And then with these assumptions, we want to guarantee safety. So, so that was the background on kind of the existing approach, formulating this problem as a Stackelberg game. The way we're going to approach the problem is it's a bit different. So it's based on this idea of shielding.
00:15:33.736 - 00:16:31.524, Speaker B: And next, I'm going to go into some detail about some background on the general shielding algorithm. So shielding is kind of a. And it has other names in the literature, but it's basically a general strategy where we want to ensure we're given an untrustworthy policy, PI hat. So PI hat might achieve good performance, but it doesn't come with any safety guarantees. And we're also given a backup policy, PI backup. And PI backup might perform quite poorly, so it's not going to be good at achieving the goal. But in contrast to PI hat, we know that it can safely bring the system to a stop from some subset of states, which we denote by x recoverable.
00:16:31.524 - 00:17:15.624, Speaker B: And we refer to this subset of states as being the set of recoverable states. And just to. Oh, right. So, and the basic strategy in the shielding approach is to try and override the, use PI hat as often as possible, but then override it as necessary, using the backup policy in, in a way that guarantees safety. And so we're going to guarantee safety, but we also want to minimally override PI hat to ensure good performance. And as I said before, we're not going to provide any performance guarantees here. So just to formalize the notion of recoverability.
00:17:15.624 - 00:18:04.974, Speaker B: Yeah. So I'm just going to give an illustration. So here's a, you know, self driving robot that's trying to avoid the obstacle shown on the right hand side. And in this case, we're going to apply PI backup and a state is recoverable if in this case, the backup controller is basically just going to hit the brakes and try to bring the robot to a stop without crashing into the obstacle. And in this case, X is recoverable, since PI backup can actually safely bring the robot to a stop. So, yeah, that's the definition of recoverability. And with that in mind, the algorithm is very simple.
00:18:04.974 - 00:19:05.246, Speaker B: So in shielding, basically we're going to, the algorithm is going to use the optimal policy, PI hat. It's basically going to simulate the next state under PI hat to get X hat t plus one. So this is, it's a simulation of where the robot will end up if it uses PI hat, and then it's going to check if X hat t plus one is in the recoverable set. And if so, then the algorithm is going to use PI hat, and if not, it's going to use PI backup. And, um, the reason this works is that this algorithm maintains the following invariant. If in the current step, we're in the recoverable set, then the, in the next step, we're going to remain in the recoverable set. And so as long as we start in the recoverable set by induction, we're going to stay in the recoverable set for the entire horizon.
00:19:05.246 - 00:19:37.890, Speaker B: And it's pretty easy to show that this theorem holds. So just to give a quick proof, there are two cases. So if the algorithm uses PI hat, then the robot, sorry. Then the theorem statement follows by the red condition. So it's, you know, we've just simulated it and checked that if we use PI hat, it's going to stay in the recoverable set. So by that check, we're going to stay in the recoverable set. The second case is that we actually end up using PI backup.
00:19:37.890 - 00:20:12.574, Speaker B: Then in this case, basically we assume that we started in the recoverable set. And what this means is, and by definition of recoverability, using PI backup can safely bring the system to a stop. So basically the same must be true of xt plus one. Since we use the backup policy, high backup must still be able to bring the system safely to a stop. So again, the theorem holds and we stay in the recoverable set. And this is enough to kind of prove that this algorithm ensures safety.
00:20:13.874 - 00:20:15.854, Speaker A: Osbet, I have a question.
00:20:16.954 - 00:20:17.694, Speaker B: Sure.
00:20:18.834 - 00:20:21.774, Speaker A: How do you compute this set? X recoverable?
00:20:22.954 - 00:20:59.734, Speaker B: Yeah, so I'll say that in a. Yeah, I was just getting to that. So the challenge here is that the recoverable set is hard to compute and is often hard to compute in closed form. And there's actually a really simple strategy which is just using model based simulation to check whether X is in the set. So kind of just to illustrate this, we're first going to, as before, simulate one step using PI hat to get to X hat t plus one. And now we want to check if X hat t plus one is in the recoverable set. All we're going to do is continue our simulation using PI backup.
00:20:59.734 - 00:21:36.708, Speaker B: And if PI backup can safely bring the robot to a stop, then we know that x hat t plus one is in the recoverable set. So we have, so this property holds. If there's an accident or if we don't come to a stop, then we can, then we're not in the recoverable set. And with this check we can just proceed as before. So if we find that we are in the recoverable set, um, then we'll use PI hat. Otherwise we'll use PI backup. So, um, that's the basic idea between behind model predictive shielding.
00:21:36.708 - 00:23:03.274, Speaker B: Importantly, this does assume that we have a full model of the dynamics that we can simulate. And of course, that's not going to be true in the human, um, interaction setting, which is the key challenge that we need to solve, um, because we need to somehow model the human behavior. So next, I'm going to talk about how we actually leverage the ideas from shielding in conjunction with some analysis of the human strategy in order to adapt shielding to the human interaction setting. So just at a very high level, we're not going to be trying to compute a Nash equilibrium solution, because really, without knowing the exact form of the human reward function, there's no way that we can compute this Nash equilibrium solution. Instead, our approach is going to be to try and act conservatively with respect to a rational human. So we are going to, we're basically going to try and prove that whatever strategy we take, as long as the human is acting rationally, then our strategy will ensure safety. And basically, the key idea is that we're going to assume, the key assumption we make is that the human prioritizes safety.
00:23:03.274 - 00:24:10.634, Speaker B: What this means is that we don't need to reason about the full possible behavior of the human because they're prioritizing safety. We only need to prove that there is some strategy that human can take to maintain safety. And then as long as we know that the human can maintain safety, given a candidate action, then if we take that action, the human will act in a way that maintains safety. They might act differently than the strategy we discover, but their actions will still kind of guarantee safety. And this follows by our kind of, by our assumption that the human acts rationally. Um, so before we continue, we are, in this work, we are actually working with a simplified version of the human model. And basically, rather than in the multi step Stackelberg game, it actually alternates back and forth between the human and the robot.
00:24:10.634 - 00:24:49.990, Speaker B: Instead, in this work, we're actually considering a one step Stackelberg game. So we're assuming that the robot takes an action, and then the human takes an action in response to the robot's action. But when the human is deciding what actions to take, they don't forecast and compute every possible outcome that the robot is going to do. Instead, they're just going to act conservatively with respect to the some subset of possible robot actions. And that's kind of denoted in this, the minimization shown in red on this slide. They're going to act conservatively with respect to some subset. So I haven't.
00:24:49.990 - 00:25:43.014, Speaker B: Yeah, there's basically some bounded set here of actions that they consider, and then they act robustly with respect to that set. And this is a one step Stackelberg game. So the robot acts and then the human acts, and then that's it. Um, so this is a pretty big simplification, but it does make sense in practice because just like the robot can't really anticipate, doesn't know the human reward function, the human most likely doesn't know the robot reward function either. So it makes sense that they'll act conservatively rather than actually try to reason about the full time horizon. But then our rationality assumption just says that the human is acting optimally according to this maximin objective function shown above. So this is the model of human behavior that we're going to consider, and then we want the robot to act in a way that is safe with respect to this model.
00:25:43.014 - 00:26:26.098, Speaker B: Okay, so next I'm going to go into details about some of the key assumptions we make about both the human reward function as well as the about the human objective. So just to kind of have it here, this is the maximum objective that the human is acting according to, and we're assuming that they actually follow this strategy. So the key challenge is that, I guess there are three parts in this that we don't know. First, we don't know what actions the human considers. That's the part shown in blue. Second, we don't know the conservative set of robot actions that the human is taking into account. That's the minimization shown in red.
00:26:26.098 - 00:27:15.324, Speaker B: And finally, we also don't know the human reward function that's implicit in the value function jh. So our first assumption is about the reward function. So the only thing we're going to assume is that the human prioritizes avoiding unsafe states. And basically what we're going to say is that the human reward function is minus infinity for any unsafe state. And we're not going to make any other assumptions about the structure of the human reward function. Our second assumption is that there is some backup, backup action for the robot, denoted U hat r. And this is a action that the robot can use to come to a stop.
00:27:15.324 - 00:28:08.780, Speaker B: And importantly, we assume that the human is going to act conservatively with respect to this action. So whatever set of actions the human is considering in the min, in the objective u hat r is going to be included in that subset. And just as a very simple example, the robot might consider, the robot backup action might be to basically apply the brakes and gradually come to a stop. So you wouldn't want it to be slamming the brakes because humans often don't anticipate another agent slamming the brakes. But at least if they break gradually, the human should be able to anticipate and respond to that. So, yeah, that's kind of a canonical example of a good robot backup action. And the final assumption is on the human backup action.
00:28:08.780 - 00:28:48.512, Speaker B: Oh, sorry. The final assumption is that we're given a set of human backup actions that the human can use to come to a stop. And the only thing we're going to assume about this set is that if the human's objective value is minus infinity, so if in this kind of maximin objective, the optimal. The optimal over the objective. Yeah. The maximum objective value is minus infinity over the human actions, uh, then basically that means that all human actions are have equal objective value. Basically, the human doesn't have any idea how they can maintain something that doesn't lead to an unsafe state.
00:28:48.512 - 00:29:41.872, Speaker B: And we're going to assume that the human is going to take some action in this set of backup actions when this happens. So they're not going to select non deterministically or arbitrarily over the set of all possible actions. They're going to specifically choose an action that can bring them to a stop. And again, this can be some kind of breaking action. The reason we consider a set instead of a single action is that we're giving the human leeway to take different kinds of possible actions. So they might break at different rates, or they might kind of steer as they're breaking and so on. So just to kind of illustrate the possibilities here, I've kind of shown the reachable sets under both the robot backup action and the human backup action set.
00:29:41.872 - 00:30:29.544, Speaker B: So if the robot is hitting the brakes, gradually applying the brakes, then it's going to come to a stop after some amount of time. That's this red arrow here where the robot backup action is u hat r. And because the human backup action is a set u hat h, theirs is going to be a region instead of a single trajectory, but they'll presumably come to a stop somewhere in this region. So the width of the region is accounting for the fact that they might apply some steering angle and they might also apply different deceleration rates here. So they'll come to a stop somewhere in this cone shape. Okay, so given these assumptions, next, I'm going to go into some detail about our algorithm.
00:30:29.844 - 00:30:45.134, Speaker C: Osborne, short question. So this x unsafe or x safe? Yeah. So both the human and the robot know the same ex safe, so they have the same perception of safety.
00:30:46.794 - 00:31:06.614, Speaker B: Yes, that's right. We're assuming that. Yeah, we're assuming that they're. That's a good point. We're assuming that their notion of safety is shared and that that is a. So I think in our examples, the shared safe set is going to be that they don't want to have an accident. But in general, if the human is not.
00:31:06.614 - 00:31:22.818, Speaker B: If the human, basically, there's some state that the robot wants to avoid, but the human doesn't, that does make the problem more difficult because the human might actively, you can imagine then that the human could even act adversarially and try to drive the system into that behavior. So this is a pretty important assumption.
00:31:22.926 - 00:31:23.522, Speaker C: Right, right.
00:31:23.578 - 00:32:05.454, Speaker B: Okay, that's a good point. Okay. Are there any other questions before I continue? Okay, so, yeah, that's the backup action sets. So just to write down the human model again so we can refer back to it, that's the same policy as before. So, and so, as I said earlier, our algorithm is based on this notion of shielding. And in the shielding framework, kind of the really critical thing is to define the notion of recoverability. So, and really it's going to be the same notion as before.
00:32:05.454 - 00:32:49.290, Speaker B: So we're going to say a state x is recoverable if using the robot backup policy. In this case, just this single action you had r from x safely brings the system to a stop. So importantly, yeah, so that's just this red trajectory that I showed before. But importantly in this case, recoverability depends on the unknown human policy. So the human is also going to be taking some sequence of actions, and whether or not the robot comes to a safe stop depends on what actions the human does takes. And we, unlike before, we don't know this part of the environment. We don't know what policy the human is using.
00:32:49.290 - 00:33:52.834, Speaker B: So we can no longer simulate the system in order to check if a given state x is recoverable. And what we're going to do is leverage our assumptions about the human behavior to have established basically a proxy for recoverability. And this proxy is basically going to say that a state x is recoverable if the system can safely come to a stop after, if the robot takes its backup action for all possible human backup actions. So regardless of what the human does in this bounded action set u hat h, the system will safely come to a stop. And in this case, if the human actions, in this case that in this kind of toy example I'm showing on the right hand side, regardless of what the, assuming this is the reachable set under u hat h, then the system is in fact safe. Recover. Sorry, the robot is in fact recoverable.
00:33:52.834 - 00:34:46.974, Speaker B: Because the human will safely come to a stop, the robot will safely come to a stop, and the two of them won't collide. Okay, so one important thing to note and just to emphasize is that the human, we're not assuming that the human takes an action in the set. Uh, so this isn't just a matter of being robust to some subset of possible human actions. And they really, they might, in fact, and they do take actions outside of UA, this set. So, for example, they might take the green action illustrated on the right hand side, which is not in this set. So, but the point, the kind of key thing that we're leveraging here is that because the human is acting rationally, this green action must be better than any of the blue actions, and because the human is prioritizing safety. Therefore, if all the blue actions are safe, then so is the green.
00:34:46.974 - 00:35:26.726, Speaker B: That's the kind of basic intuition behind our strategy. So as long as we're in a recoverable state, we're going to be able to, as long as we're in a recoverable state, we're going to be able to take the, we're going to be able to maintain safety just like in the shielding, the original shielding algorithm. So just to kind of hash that out in a little more detail. So our algorithm proceeds in three steps. The first step, we're going to compute the human backup region. This is the reachable set using the human backup actions, u hat. H, and as I said, this is not the reachable set of the human, it's only.
00:35:26.726 - 00:36:11.674, Speaker B: And the human might actually drive outside of this region. The second step is to compute the robot backup trajectory. And following the shielding strategy, we're going to first take one step using PI hat, and then take multiple steps over some bounded horizon using U hat r. And then basically we're just going to check whether these two, actually two things, we're first going to check whether these two regions overlap. We're also going to check to make sure that they bring the system to a stop. If so, then we're going to use PI hat, which is the kind of preferred policy. If we can't guarantee safety, then we're going to switch to the backup action, U hat r.
00:36:11.674 - 00:37:05.174, Speaker B: So in this case, because the two sets don't overlap, we ensure safety. If they do overlap, as illustrated here, then we're going to switch to the backup action. Okay, so, yeah, and we. Yeah, one thing, one caveat is we actually perform. So I'm going to go into a bit more detail on each of these steps. So one caveat is we actually perform steps one and two simultaneously, because when we roll, when we kind of roll out the system, we need to roll out the human and the robot simultaneously. So basically we want to compute the reachable set under the back, assuming under the robot backup action you had r and the human backup actions you had h.
00:37:05.174 - 00:38:08.684, Speaker B: So we actually use abstract interpretation to do this. Any kind of over approximation of the dynamics will do, but I'm going to kind of just for concreteness, I'm going to just assume that we're given a function on an over approximation of the dynamics that operates on subsets of the state space and the action space. So the key challenge, of course, is that the U hat h is a set of actions. So we need to over approximate the dynamics in order to deal with that. And basically I'm going to assume given a function capital f delta for each agent delta, either the robot or the human, and this is going to take a subset of states, a subset of actions for agent delta, and it's going to output a subset of states. And the key assumption is that it over approximates the dynamics. So given a subset capital x and states and a subset capital U of actions, it's going to contain the dynamics little f delta of x u for every x and big x and every u and big U.
00:38:08.684 - 00:38:54.124, Speaker B: So as long as you have this, then you have an over approximation of the reachable set under actions Big X and Big U, sorry, states Big X and actions Big U. And we're just going to apply this over approximation. So first we're going to apply it to the kind of, in the initial state we're given, that's just the singleton set x. And for the first step, we're going to assume that the robot is going to use PI hat and the human will use their backup actions. So that's going to get us to the next state, x two. And then from there on out, we're going to assume that the robot switches to the backup policy. The backup action you had are, and the human is going to continue to use their backup action.
00:38:54.124 - 00:40:04.004, Speaker B: Okay, so just to kind of illustrate, so after one step we're going to use PI hat, and after the first step, then we're going to switch to using U hat r for the robot, and we're going to expand the reachable step for the human and roll out the robot trajectory at the same time. And yeah, as I said, the formal way we do this is using an abstract interpretation framework. It's a pretty standard technique, and I won't go into details about exactly how that works. So once we've computed this sequence of reachable states of the system under these two action sets, now the third step is to kind of check the recoverability problem property. So first we want to check whether the system is safe for the entire horizon, and we also want to check whether the system comes to a stop. So those are the kind of two key properties. And to check safety, we just need to check to ensure that the reachable set big xt on every step is contained in the safe set for every step t.
00:40:04.004 - 00:40:52.004, Speaker B: And to check that the system comes to a stop, we just need to check that on the final step, we come to a set of equilibrium states where the system is at rest. And again, these are also checked using the abstract interpretation framework, which I'm not going to go into details about. So if both of these checks pass, then we're going to use the robot, the optimal policy PI hat. Otherwise, if either of the checks fails, then we're going to switch to the backup action. That's our algorithm. Basically, just to summarize, we're going to roll out the system and perform these checks. Next, I'm going to talk about the theoretical guarantees that we prove for this algorithm.
00:40:52.004 - 00:41:47.524, Speaker B: And so first we're going to make a number of assumptions which I've already alluded to earlier. We're going to assume that the human follows this model of their behavior, and we're also going to make the assumptions one, two and three I described on a previous slide. And finally, kind of similarly with the shielding approach, we have to assume that we start from a recoverable state. So we're going to just assume that the human and robot are at rest at the initial state x one. So we're in an equilibrium state. Then our theorem just says that we ensure that the robot is, that the system remains safe for the entire time horizon. And um, yeah, I'm going to sketch the proof the high level is going to follow.
00:41:47.524 - 00:42:15.946, Speaker B: The high level structure is the same as the, um, shielding, the proof for the shielding algorithm. And I'll go into a bit of detail in each of these cases. Um, but the basic strategy is just that we use induction. We prove by induction that x t is in the recoverable set for each t. And because recoverable states are by definition safe, this will establish the property. And the base case just follows by assumption. Yeah, I guess it's obvious that if the human and robot are rest, then we're in a recoverable state.
00:42:15.946 - 00:43:00.714, Speaker B: So as before, we have two cases that we need to deal with the first case is that the robot uses the optimal policy PI hat. And the second case is that the robot switches to the backup policy PI backup. And in this case it's just using the robot backup action. Okay, so um, yeah, so the first case, in this case is um, the really complicated one. Uh, so, um, this is the case where the robot uses the, um, the optimal policy PI hat. And here again I've recalled the human model. I've, in this case I've separated out the, the min portion of the maximin objective and I've just denoted that by Jh Tilde.
00:43:00.714 - 00:43:58.632, Speaker B: So the, and this is the objective that the human is maximizing. And now we need to prove the inductive step, which is that if we start in the recoverable set, we remain in the recoverable step. So the proof is basically goes as follows. So since we're using the optimal policy by the kind of check that our algorithm did, we know that the human backup action and the robot backup trajectory do not overlap, because that's the check that we did using abstract interpretation. And they also bring the system safely to a stop. So first let's assume that the robot takes an action, uh, such that the objective value j tilde h is greater than negative infinity. Then kind of by assumption one, we assume that the, that the human accrues reward minus infinity for any unsafe state.
00:43:58.632 - 00:45:02.574, Speaker B: So if the cumulative, if the value function is greater than negative infinity, then the human, then the system can't possibly have reached an unsafe state. So the human action is safe for the remainder of the horizon. And because the human is acting conservatively with respect to the robot backup action. So we know that whatever action they took, if we continue using the robot backup action in this red part, then we're going to be kind of doing something that they anticipated and therefore we're actually going to realize a objective value that's greater than minus infinity, therefore we can ensure safety and therefore the system is recoverable. That's the basic argument. So that's if the human takes some better action. So this is the case where the human can take an action that's outside of the backup region because they found some action that was better than those actions.
00:45:02.574 - 00:45:52.024, Speaker B: The second case is that the objective value is equal to minus infinity. And in this case it just follows by assumption, we assume that if the objective value is equal to minus infinity, then the human takes an action that is in this backup set. And this time by our check, which is two points above, the regions don't overlap. So we know that using the backup actions can bring the system safely to a stop. And so, again, in this case, the system is going to be safe. So, um, that's the proof. In the case where the robot uses the optimal policy PI hat, the, the second case is, um, basically the same as before.
00:45:52.024 - 00:46:36.504, Speaker B: If the. Instead, the, in the traditional shielding proof, if the robot uses PI backup, um, again, we need to prove that, um, xt plus one is recoverable if we're currently in the recoverable set. In this case, the human backup region and the robot backup trajectory might overlap. But by definition, if we use PI backup from XT, by definition of recoverability, we're going to bring the system safely to a stop. And since we, in fact use PI backup, the same property must be true of XT plus one. So xt plus one must also be recoverable. If we continue using the backup policy from XT plus one, we must again safely bring the system to a stop.
00:46:36.504 - 00:47:21.916, Speaker B: So this step follows as well. And together with the previous case, this proves the theorem. So, basically, we've proven that the shielding policy ensures safety for the entire horizon. So, yeah, finally, just to wrap up, I'm going to give some, some results from the simulation experiment. It's the same one I showed at the beginning of the talk, but we have several, a variety of different results illustrating different properties of our approach. So the experimental setup is that we have this environment where the car. So it's basically a car model with bicycle dynamics.
00:47:21.916 - 00:47:55.382, Speaker B: So the control inputs are the acceleration to apply and the steering angle. And we have several different driving tasks. One of them was the crossing task that I showed. We also have a highway merging task where a robot is trying to merge onto a highway with a human in the current lane, as well as a left to unprotected left turn task. And I'll show examples of each of these in a minute. Here. For the robot, we used our approach in conjunction with an aggressive controller that drives straight to the goal.
00:47:55.382 - 00:48:41.084, Speaker B: So this aggressive controller is the optimal policy PI hat. And we basically used our shielding approach in conjunction with this aggressive controller. In particular, this aggressive controller is going to completely ignore the human. And just so if there's a human there and they don't move out of the way, it's just going to drive straight through them. But the key idea is that this policy is going to try to get to the goal as quickly as possible, and we're going to fully rely on our approach, the shielding algorithm, to ensure safety. I will also show one simulation with a model predictive control baseline it's a pretty standard MPC. It's actually going to forecast the human trajectory, assuming that they use a constant velocity input.
00:48:41.084 - 00:49:29.626, Speaker B: And then it's going to compute a plan that avoids the human under this assumption. And it basically uses cross entropy method optimization to do this. I'm not going to. The details aren't that important. Finally, we compared about simulated humans using the social forces model, which is a standard model of kind of crowd behavior, as well as more importantly, we also have some, a lot of experiments with real humans who are interacting with the simulation via keyboard input. Yeah, so I'll show results on both of these. So, yeah, finally, just to kind of, for our approach, we have a couple of parameters I need to describe, and they're basically the same ones I described earlier.
00:49:29.626 - 00:49:52.256, Speaker B: And. I see. So the first, we need to specify the robot backup action u hat r. And basically the robot backup action is going to be to break at a given deceleration rate minus ar. So they're going to accelerate at a negative rate. And the steering angle here, actually, this is supposed to be zero. So they're going to basically not apply any steering angle.
00:49:52.256 - 00:50:33.074, Speaker B: They're just going to break in a straight line. And. Yeah, the other thing we need to specify is the human backup action set u hat h. It's going to be very similar, except we give the human some leeway. So there's a range of possible deceleration rates they can apply, and there's also a range of steering angles that they can take. And yeah, this forms the set of possible actions. So first, I'm going to show some results of our controller using our controller in conjunction with simulated social forces humans.
00:50:33.074 - 00:51:34.158, Speaker B: And in this example, you'll see that the shielded aggressive controller is going to cut in front of the human by leveraging the fact that responsible human driver will apply a slight, at least brake slightly to avoid a collision. You see that the first blue car there braked to let the human pass safely before continuing through the intersection. And this shows that our approach is not overly conservative. So if it does compute that there's a way for the human to slow down and avoid an accident, it is going, it can aggressively take the right of way. I'm assuming you use it with this aggressive controller. Our second result is that the robot is going to trigger kind of. Conversely, if it can't, if it concludes that the human might not be able to stop and avoid an accident, then it's actually going to trigger, the shield is going to trigger and override the aggressive policy and break to allow the human to pass safely.
00:51:34.158 - 00:52:25.364, Speaker B: So, yeah, here the robot in red, as you can see, it, comes to a stop before continuing after the first blue car passes the intersection. And just to show that we have the same kinds of results, I think this is the video I showed at the beginning, but now you can see that in this case, this is a real human driver. And, yeah, they're going to pass the. There's this kind of complex negotiation that goes on, and this is all kind of naturally falls out of the shielding framework. So as soon as the robot is sure that the human can safely come to a stop, it's going to go ahead and take right of way and cross the intersection in front of the human. And because the human is acting, their first order of concern is to avoid an accident. They'll actually stop and let the robot go, even though they also want it to cross.
00:52:25.364 - 00:52:58.154, Speaker B: Our next example is kind of, again the opposite, where the shield is triggered to let the human cross safely. So in this case, the human started closer intersection. Closer to the intersection. So they passed first, and then the robot break slightly to allow the human to pass. I said we also had compared to an MPC, the MPC, because it. It can't account for the fact that the human is going to. Might break in response to its actions.
00:52:58.154 - 00:53:27.020, Speaker B: It's actually going to be more conservative than the human. So it's going to slow down to let the human pass, even though it probably could have crossed the intersection first if it had been more aggressive. The next few experiments are just to demonstrate the flexibility of our approach. First, we have one. Oh, sorry. Before we get there. So this is first an example with real humans where we actually did have an accident.
00:53:27.020 - 00:53:54.396, Speaker B: So here the human is acting aggressively, and it collides with the stationary robot. So I'll first show you what this looks like. So the human here didn't kind of break quickly enough, and they also steered towards the robot. So this shows that there are shortcomings. Our approach doesn't. If the human drives in ways that we don't anticipate, there are possibilities of accidents happening. But it does, at least intuitively.
00:53:54.396 - 00:54:26.484, Speaker B: To me, it feels like it's somehow the human's fault that they kind of drove towards the robot and didn't slow down quickly enough. So when there are accidents, it kind of does match human intuition about who is at fault. In practice, you probably would want to fit the. Some of the. Be more conservative in terms of the parameters you set. We set our parameters fairly aggressively to show that you can get this kind of aggressive driving behavior. But if you wanted, really wanted to ensure safety, you should set the shielding parameters a bit more concerning.
00:54:26.484 - 00:55:20.058, Speaker B: So the next two experiments just try to demonstrate the flexibility of our approach so we can. In these cases, we're going to modify the backup action sets in various ways to show that we can get interesting new behaviors. Here I'm going to show unprotected, the robot executing an unprotected left turn. This is using the parameters that I showed above. And as you can see, there's this undesirable behavior where the shielded controller, or where our robot controller, it does safely come to a stop, but it stops in the middle of the intersection, which can confuse the human driver and lead to congestion. And there's actually a pretty simple modification we can make, which is that we can just add a constraint saying that the backup policy is not allowed to come to a stop in the middle of an intersection. So that doesn't count as safely coming to a stop.
00:55:20.058 - 00:56:12.994, Speaker B: It has to either cross the intersection or come to a stop before the intersection. And as you can see here, it's going to choose to come to stop before entering the intersection, and it's going to wait for the car to pass and then cross safely. And just as another example here, we're actually going to show that we can use just a totally different backup policy where we pull over into the next lane. So, yeah, as you can see, when the human driver kind of pulls into the highway, the robot switches to the next lane in order to avoid them. And this shows that you can use kind of a variety of different backup policies beyond the simple slowing down to come to a stop. And if you are driving on a highway, it might actually not be safe to just come to a stop in the middle of the highway, which is why. So you might need to consider different kinds of backup actions.
00:56:12.994 - 00:56:45.454, Speaker B: Okay, so, yeah, just to conclude, so we've proposed a framework for ensuring safety in the context of human interactive control, where we start with a game theoretic model of human behavior, and then we leverage assumptions about human rationality in conjunction with a model predictive shielding algorithm to ensure safety of the overall system. And, yeah, so thank you all for listening to my talk, and I'm happy to take any remaining questions.
00:56:47.874 - 00:56:58.374, Speaker A: Thank you, Osper. Any questions? So if you have a question, then you can either raise your hand or just write something in the chat, and I can call upon you.
00:57:05.894 - 00:57:07.038, Speaker C: Can I just ask?
00:57:07.166 - 00:57:07.874, Speaker A: Yeah.
00:57:08.374 - 00:57:30.176, Speaker C: Okay. Thanks, Osbert, for a very nice talk. I have a question. So you're saying that the shodan approach here intentionally. You used it in a very aggressive way. And in principle, you can crank up some thresholds and the car will stop not right before the intersection, but a little bit sooner than that example. So that's possible.
00:57:30.176 - 00:57:31.204, Speaker C: Is that correct?
00:57:31.784 - 00:58:05.970, Speaker B: Yes, that's right. You can change the, I mean, there are various, I guess there are several parameters, both the robot backup policy, the human backup action set. In that case, what we changed was what states are considered. Equilibrium states. Yeah, I guess I didn't spell out exactly all the parameters, but we basically don't consider the robot to really be at a stop when they, if they're in the intersection. That's not, that's not a state where the system is at rest, which. Yeah, so you're not allowed to come to a stop in the middle of the intersection.
00:58:05.970 - 00:58:12.614, Speaker B: So, yeah, there are, there are these set of parameters you can tune to adjust the notion of safety.
00:58:13.274 - 00:58:21.766, Speaker C: Is there a way to consider also the cars that are behind the robot driver in terms of their safety?
00:58:21.830 - 00:59:17.892, Speaker B: Yeah, I mean, you can consider safety of the full, I mean, the framework is general, so you can consider safety of human drivers behind. Yeah, I don't think I have any experiments to show along those lines. But basically, the assumption, this is why we actually initially had the backup action be to kind of break sharply. And then we did have some examples where the robot would get rear ended. So the backup action has to be constructed to ensure that humans both, like driving in the crossing direction as well as driving behind the robot, will be able to anticipate and act safely. But assuming you design your backup actions in such a way, the framework can basically, the extension is just that it'll forecast the behavior of all the humans instead of just the one human. So if you have multiple humans, it just forecasts all of them.
00:59:17.892 - 00:59:28.624, Speaker B: So then the assumption would be that if the robot kind of breaks gradually and comes to a stop, then any cars behind it will also break gradually and will be able to break gradually and come to a stop as well without hitting the robot.
00:59:30.484 - 01:00:11.714, Speaker C: Just one more question, if I may. There has been a model that I've seen, I think, in 2019 about the social behavior among this robot. So there was also a game built on various rewards of the robots, not only taking into account their egoistic reward, but also taking into account, like, some social behavior. Somebody can be altruistic or masochistic and so on and so forth. So is it easy to also, and they were also solving with Stachelberg game, but several step Stachelberg. Is it also possible to easily introduce this here, this kind of game behavior.
01:00:12.294 - 01:00:44.602, Speaker B: Right? Yeah. So that would probably come into play in terms of how the optimal policy, PI hat is designed. So. And that's one thing I like about the shielding approach, is that you can design a policy to kind of achieve whatever behavior you want, and then the shielding. So it's kind of a modular system where the shield is only responsible for ensuring safety. So I know there's been. I'm not sure I'm familiar with a specific paper you're talking about, but there's been work using things like different control algorithms and even reinforcement, learning to plan in these settings.
01:00:44.602 - 01:01:02.534, Speaker B: And you can. So if you have such a planner, you can use that to generate PI hat, and then once you have whatever PI hat, you just use it in conjunction with our algorithm to ensure safety. So, yeah, that's how I would kind of structure this.
01:01:03.394 - 01:01:06.514, Speaker C: Okay. So it's very modular. That's nice. Thanks.
01:01:06.674 - 01:01:09.494, Speaker B: Yeah. Thank you for the great question.
01:01:11.194 - 01:01:53.354, Speaker A: So I have a question, a follow up question. So, in your setting, you have really one kind of human. If you started modeling some kind of incomplete information, where, let us say, the human drivers come from one of several different types, you know, maybe aggressive drivers, conservative drivers, and so on, then in some sense, your. The game solving can become more complicated. Right. Because in a way, you can try to predict the type of the human while doing your optimization. Does your thing generalize to these cases as well, where, you know, you get games of incomplete information?
01:01:54.594 - 01:02:25.796, Speaker B: Yeah, we haven't. That's a good question. We haven't thought about that specifically, but I think, I mean, we've been kind of assuming that you design the backup actions in a way that they apply to all humans. And at least as a starting point, I think that's the right thing to do. But if you do observe that different, some human drivers drive more conservatively, then you might be able to tune their backup action set over time to account for that and still ensure safety. But, yeah, that's not. Yeah, I think we.
01:02:25.796 - 01:02:41.464, Speaker B: Yeah, we haven't accounted for this kind of learning. So certainly in PI hat, like, again, it can be general, and it can. It can take into account these, these behaviors. But if you wanted the shield itself to actually adapt, I think it is possible, but that's not something we've done yet.
01:02:42.404 - 01:03:10.754, Speaker A: Okay, thank you. Are there any other questions? If not, then let me thank Osbert again on behalf of all the participants for this wonderful talk. Thank you. And then we shall meet again next Friday at the usual time. And next week, Mihaly Siana Kakis is the speaker.
01:03:11.694 - 01:03:16.334, Speaker B: Okay. Thank you, everybody for joining. Thanks.
01:03:16.414 - 01:03:17.114, Speaker A: Bye.
01:03:19.014 - 01:03:19.534, Speaker C: Thank you.
