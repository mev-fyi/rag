00:00:00.120 - 00:00:44.984, Speaker A: Thank you very much. Thanks a lot for inviting me. So I will talk about tensor decomposition and how to do that with some squares relaxations. This joint work with, based on two joint works with the authors of the union are Sam Hopkins, Tang Yama, Jonathan she and Sil, who is in the audience. So let's start with saying what's a tensor? So tensor is a very simple object, just an array of numbers where you have multiple indices. And this generalizes in a natural way, vectors. We have one index and matrices where you have two indices.
00:00:44.984 - 00:01:49.484, Speaker A: So more formally, if you have a three tensor over RT, then, you know, it's a linear combination of these kind of guys. And these are, you know, just triples of basis elements of RD. And so that's how the space of tensors, how the space of three tensors over RD looks like. And they are special tensors, which will play a crucial role in this, in this talk. And those are called product tensors, or rank one tensors, which come from a single triple of vectors in the underlying space, a, b and c here. And the coefficients of the tensor that they specify, they depend in this multilinear way on the coefficients of a, b and c in the standard basis, just like here. And maybe one remark.
00:01:49.484 - 00:02:41.802, Speaker A: So, sometimes in the literature about tensor decomposition, there's an effort to distinguish between tensor products and chronicle products and other kind of products. But the reason is a bit mysterious to me. So in this talk this year will be the usual chronicle product for matrices. We'll be treated like the usually chronicle product for vectors and matrices. Now, why should we care about tensors? It turns out that tensors are like a natural shape of data. They arise in many different contexts. In particular, if you have a multivariate distribution and you look at its moments, then this corresponds to, it's, in a natural way, a tensor over the space where the distribution is, you know, of the distribution.
00:02:41.802 - 00:04:05.946, Speaker A: And similarly, if you have a multivariate polynomial, then the coefficients of that multivariate polynomial form a tensor. Finally, if you have a quantum system that is composed of different parts, then the each state of this quantum system lives in a, in a tensor space over the states for the individual parts. So in that sense, tensors are pervasive data structures. And one sort of the final evidence for that claim is the fact that if you look at this new deep learning software library, it turns out that their very basic data structure is tensors. So in some sense, these software libraries are just libraries to manipulate tensors in simple ways. And so this gives rise to the thesis that tensors are the new matrices in the same sense, that matrices were able to tie together a wide range of diverse disciplines. You know, tensor has the same ability, okay? But it turns out that tensors have some unique advantages over matrices.
00:04:05.946 - 00:04:59.426, Speaker A: So just because they are richer, in particular, you know, there's a reason why these software libraries are not based on matrices, but they are based on tensors. So our tensors are deeper. That also means that, you know, if tensors are the new matrices, then we should, it's the time to develop algorithms for the tensor age. So you have to identify the algorithmic challenges that come with trying to deal with tensors. And here the hope is to repeat some of this dramatic success that we had for developing for algorithms for matrices. In the last couple of centuries, we've seen amazing algorithms for, which had a dramatic impact in many ways. And the hope is to repeat some of that for tensors.
00:04:59.426 - 00:05:48.226, Speaker A: And again, there will be some unique challenges in doing that, and we will discuss some of those. So the problem I want to focus on is tensor decomposition, which is arguably one of the most fundamental problems about tensors. So suppose you have a three tensor. What you want to do is you want to explain this three tensor in the simplest way possible. So with as few vectors of the underlying space as possible. And the kind of explanation that we look for is these kind of multilinear explanations. In particular, we want to find as few vectors, AI, bi, CI as possible, such that the tensor t is a sum of the rank one tensors corresponding to AI, bi and CI.
00:05:48.226 - 00:07:09.092, Speaker A: Okay? Now why would you want to do this? So it turns out that, as I alluded to before, there are some key advantages that tensors have over matrices. And the key advantage that they have in this context is that if you look at factorizations for matrices, then they suffer from the rotation problem. And what this means is that if you have some low rank factorization of the matrix, then you can generate very different looking factorizations just by transforming the vectors with some arbitrary invertible linear operation. And that means that there's really no sense in which, you know, and in a sense, you know, the matrix factorizations are, you know, very non unique. And in contrast, it turns out that tensor decompositions are often unique. Okay? And this is very important for, you know, in applications for tensor decomposition, because if your idea is to use this decomposition in order to explain the data in the simplest way, you somehow hope that this explanation has something to do with, with some truth that you're trying to find. And now if the solution is very undetermined.
00:07:09.092 - 00:07:27.140, Speaker A: Like for matrix factorization, then, you know, it becomes not clear what it means that you find a good explanation. But if you find a low rank tensor decomposition and it has this property that's unique, then the vectors that you find, they probably tell you something inherent about the data that you have.
00:07:27.212 - 00:07:41.000, Speaker B: Yes, I'm a little confused about the analogy. You say m equals ab transpose. So, I mean, isn't the right thing to compare, you know, where m is a sum of tensor products or other products.
00:07:41.112 - 00:07:53.200, Speaker A: Yes, that's the same thing. So if you look at rectangular matrices. Yes, yes. Yeah. Here you have rectangular matrices, maybe I. So I guess never clear to me if it's the columns or the rows of a or b.
00:07:53.312 - 00:07:56.272, Speaker B: So like k by n and n by k. If it's rank k, you're saying.
00:07:56.368 - 00:08:02.416, Speaker A: Yes, yes. So here the inner dimension will be rubber, and that's the same thing that you're trying to minimize.
00:08:02.520 - 00:08:06.112, Speaker B: Okay, so it's just not written in the same here.
00:08:06.248 - 00:08:20.512, Speaker A: I think the rows of a and the rows of b will be the vectors that. You're. Sorry, the columns of a and the columns of b will correspond to AI and bi in this picture. Right?
00:08:20.688 - 00:08:32.342, Speaker B: Yeah, but I'm still a little confused about why you see, these cases is so different, though, because, I mean, certainly the singular value decomposition of the singular values are distinct. Gives a pretty unique way to.
00:08:32.438 - 00:09:20.894, Speaker A: Right, but then, but then you, if you, if you talk about things, single values, then you have in mind that the vectors that you're trying to find should be orthogonal. Okay? And then you're right that if you, if you sort of know a priori that my vectors are orthogonal, then that fixes or that, that makes the decomposition sometimes unique. Okay, for matrices. And actually, then you also assume that this incorrect values are all different. And for, in the tensor case, it turns out that, you know, it can capture, you know, you don't have to assume anything about orthogonality of the, of the, of the true explanation you're looking for. Okay. And orthogonality, of course, in some context it's perfectly justified, but in many contexts it's not.
00:09:20.894 - 00:10:21.556, Speaker A: But I know. So this advantage comes with a challenge. And the challenge is that tensor decompositions are NP hard to compute in the worst case. And that means that we can't hope to have the same kind of very simple theory that we had for computing matrix factorizations or other matrix problems. Okay? But certainly, you know, it doesn't mean that we can't talk for good algorithms because, you know, there are many cases like you know, for example, the story about approximation algorithms, where we know we have NP hard problems, but we managed to find, you know, interesting ways of dealing with them, finding proof of algorithms for them. And so the hope is to do, you know, that we can do the same thing for tens of decomposition. And it turns out that this hope seems to be justified.
00:10:21.556 - 00:11:20.710, Speaker A: There's some evidence that indeed you can have very good algorithms for these kind of problems. And another nice thing is that the tractability of this problem seems to go hand in hand with the uniqueness of the decomposition. So there's some relationship, apparent relationship, between these properties, you know, when, when is attractable defined tens of decomposition, and when is it unique. And so in this talk, we will see why, how these things are indeed related. Okay? And I don't, just to give you more examples, so it turns out that indeed sort of efficient algorithms for tensor decomposition, they gave rise to, you know, provably efficient and also practically efficient algorithms for a wide range of learning problems. All the problems are listed here. And in general, you know, these unsupervised learning problems, they try to ask you to cluster, to cluster some observations in a particular way.
00:11:20.710 - 00:12:09.700, Speaker A: And that's all the best movable algorithms are. These problems are based on intensity composition. It also means that if you have better algorithms for density composition, it means that you can learn these problems or you can solve these learning problems better. Ok, so now to get some, I want to show you what is known about the algorithmic difficulty of the kind of algorithms that we have for tens of decomposition. It turns out that I want to, it's convenient to think about tens of decomposition in a slightly different way, in a sort of a probabilistic way. And this way, you know, some level, the notation is a little bit more familiar. And it turns out also our algorithms, they follow this viewpoint.
00:12:09.700 - 00:13:25.374, Speaker A: Okay? So now when I consider this problem, so it's an inverse problem, and what you're trying to do is you just have given some moments of a distribution and you're trying to find what's the original distribution. And so what's the precise setting? Suppose that we have some set of vectors, a one up to a n, that are unknown, and we are given the low degree moments of the uniform distribution over these vectors. Okay, the first k moments of this distribution, and our goal is to recover the original vectors. Okay, so this is a special case of the multivariate moment problem and reformulation of the tensor decomposition problem. And our question is, under what conditions on the vectors? A one up to a n, and the number of moments k that we observe, can we solve this problem efficiently and robustly? So roughly means even if there's some error on the moments that we observe, we shouldn't be able to solve this problem. And there's a classical case which has its origin in the 1970s in a paper by Harshman, algorithm attributed to Jenrich. And this case is when the vectors, a one up to a n, are linearly independent.
00:13:25.374 - 00:14:08.854, Speaker A: Notice that it also means that n is at most d. In this case, it turns out that if they are linearly independent, you can, without a lot of generality, assume that they're actually orthonormal orthogonal unit vectors. The reason is that, you know, if you get the first three moments, let's say, of these distribution, then the second moment matrix allows you to construct this linear transformation. And this linear transformation orthonormalizes the vectors. So you can just push this linear operation into everything that you observe. And then, you know, effectively that allows you to assume that the vectors are orthonormal. And in this case, we can solve the problem as good as you could hope for.
00:14:08.854 - 00:15:15.782, Speaker A: So it turns out that three moments, the first three moments are indeed enough. There's a very simple algorithm, spectral algorithm, based on diagonalizing matrices. And, you know, it's a really nice algorithm. I will explain it later in the talk, because we will use it as a subroutine. Now, one interesting question here is, what happens if there's noise in the, in the input? Suppose that our input is not the sum, you know, the sum of the rank one with tensors of the AI's, but there's some error tensor e added to it. And so now you can ask, when is the information theoretically possible to still solve this problem? Turns out that if the injective norm, which is an analog of the spectral norm for tensors, this is much less than one, then you can still solve the problem. You can still approximately recover the vectors a one after a nice, there is a point normal time algorithm based on some non convex gradient descent ideas, condenser power iteration, which solves the problem whenever the injective norm is at most one over n.
00:15:15.782 - 00:16:06.284, Speaker A: Okay, so there's this gap of one over n between the thing that is information theoretically possible and what we know to do in polynomial time. And in this talk, oh, this color looks different. Here we will give a polynomial time algorithm based on the sum of squares idea, which I will define later, which says that, which shows that you can solve the problem whenever some other norm, the sum of squares norm of the error is at most much less than one. So what is this norm? This norm is an efficient relaxation of the injective norm. It turns out that it satisfies that for every error tensor, the sum of squares norm is in most square root n times the injective norm. And that means that this result actually dominates. So this condition is satisfied more often than this guy.
00:16:06.284 - 00:16:53.910, Speaker A: Basically, there's a difference between these two conditions. There's a gap of at least root n. It turns out that if you know that the error is random, let's say, chosen from an independent gaussian distribution, then the sum of squares norm is the most n to the one quarter larger than the injective norm. Okay, that means that there's only a gap of n to the one four to what's information theoretically possible. And this, and there's some evidence that, indeed, like you can't, it is difficult to improve over this end to the one quarter in particular, even if the number of components is just one, then we don't know better algorithms for that. The number of components is one. So n is equal to one, and you have a random error.
00:16:53.910 - 00:17:32.632, Speaker A: This corresponds to a stochastic model for tensor PCA, which was proposed by Montanari and Richard. So this bound here, actually, we know even if n is equal to one for random tensors. On the positive side, your result is under what assumption? On n. So they are of normals, which means that n is at most d, but n can be equal to t. That's right. Just to clarify, the vectors. Exactly.
00:17:32.632 - 00:18:12.884, Speaker A: So that's a good question. So if you have noise, if you have some error in the input, then you can, you know, the amount, the accuracy which you want to recover the vectors, it depends on how large the noise is. And these thresholds that I stated here, they are the case when you can recover the vectors up to 0.9 of their mass. So these are unit vectors. If this is the case, then you can get an epsilon approximation for, for the records. Yeah, that's a good point.
00:18:12.884 - 00:18:51.120, Speaker A: Okay, so this, for the rest of the talk, I will ignore this issue about noise most of the time, whenever it's okay. But I mean, just as a remark. So there's a sense in which this algorithm plays in sum of squares. They are almost automatically robust to noise, which is a nice feature that other algorithms, many other algorithms don't share. Now, a more general case is instead of linearly independent vectors. So consider random vectors, where n is much larger than D. Okay, so in strange, why do I say that this is a more general case.
00:18:51.120 - 00:19:33.968, Speaker A: This is a more general case, because if you have n linearly independent vectors, by what we said, we can assume that they are normal. And if they are normal, you can apply a random rotation to them. And now if you apply the random rotation to orthonormal vectors, you basically have random unit vectors. So it means that there's a way of reducing the linear independent case to random vectors with n less than D. Now we ask what happens if you have random vectors, random unit vectors with n much larger than D. And suppose you only want to use the first three moments of the vectors. So this problem has been studied only very recently, and so it turns out to be quite a bit harder than the case of linearly independent vectors.
00:19:33.968 - 00:20:27.404, Speaker A: And the first result in this context is that if you have at most c times d for some constant c greater than one vectors, then there's an algorithm that can solve the problem with a running time that is exponential in C. So this is reasonable if c is logarithmic in. And another algorithm, so this is just using a spectral algorithm. And then this non convex crane descent type algorithm, it turns out that even if the rank is d to the 1.5, if you have d to the 1.5 vectors, you can show that this vector, this algorithm has some local convergence guarantee in the sense that, you know, if you start very close to the solution, then the algorithm finds the solution. But this doesn't correspond to like an efficient algorithm because, you know, it doesn't, it's not clear how to get close enough so that this local convergence plays a role.
00:20:27.404 - 00:21:40.024, Speaker A: And then the first algorithm that sort of worked for significantly larger number of vectors, actually, you know, matching the bound for the local convergence of tensor power iteration, it was based on sum of squares, and it solves the problem in quasi polynomial time. So, which is a, a bit larger than polynomial time, but still an interesting guarantee. And this work. So the question now is, can you solve this problem if the rank is some polynomial factor larger than d in polynomial time? And in this talk, we show that this is indeed the case. So there's an algorithm based on sum of squares, which works up to the same bound for the rank as before, but runs now in polynomial time. And, you know, actually precursor of that result is a result which works up to something that is still polynomially larger than d, a rank that is pronounced larger than d. And it has a nice feature that it's much faster than, you know, it is sort of an algorithm that is, that is reasonably fast, it runs in time, you know, at most d to the three point.
00:21:40.024 - 00:22:18.392, Speaker A: And notice that this is pretty close to linear time because the input size d cubed. Now, you know, going to more general kinds of vectors. So we talked about linearly independent vectors, random vectors. Now, another model that people studied is smooth vectors. So this falls into the framework of speedrun tank of smooth analysis. And here the idea is that we usually, in smooth analysis, the idea is to perturb the input and hope that you can solve the problem on the perturbed input. Now, it turns out that for these kind of inverse problems, you want to do it a bit differently.
00:22:18.392 - 00:22:59.810, Speaker A: So here the idea is that you perturb the, in some sense, the solution that you're trying to find, you perturb the vectors. And this induces, of course, the perturbation of the tensor, which the algorithm gets as input, but it's a very structured kind of perturbation. And then, so the algorithm, you know, gets this structurally perturbed tensor t and is trying to recover the vectors. And here, nice thing is that we, you know, the amount of perturbation that we apply to the vectors is extremely small. It's just, you know, if they are unit vectors, then we perturb it by some, you know, some 0.0 mil amount. We add, you know, a gaussian of, you know, some inverse polynomial length.
00:22:59.810 - 00:23:52.304, Speaker A: And this can be, you know, as small as you like. And here, the first result is that, you know, from 2007, there's a polynomial time algorithm that works if you have the first four moments, and it works up to rank at most D squared. Okay? And the nice thing is that on some level, I only care that this is larger factor larger than D. And it's a somewhat mysterious algorithm that combines, in a very ingenious way, solving large systems of linear equations and this idea of these spectral techniques from earlier algorithms. But one unfortunate aspect of this algorithm is that it assumes that you have access to the exact moments. And in particular, it's not known whether it tolerates a 0.0 million amount of noise.
00:23:52.304 - 00:24:46.134, Speaker A: And in many applications in particular, for these learning applications, you really need to be able to tolerate a polynomial amount of noise if you want the algorithm to run in polynomial time. And that motivated a later paper by Wasgerreide, which showed that if you have more and more moments, up to the first five moments, then you can have the same guarantee as this algorithm, which is called the Fubi algorithm. But in addition, you can tolerate a polynomial amount of noise. And in this talk, we will the techniques explain. In this talk, they allow you to have the combine the benefits of both of these results. It shows that indeed, four moments are enough if you want, up to this rank, and you can still tolerate a polynomial amount of noise. And again, this uses sum of squares.
00:24:46.134 - 00:25:46.822, Speaker A: And the nice thing about this part is that the key ingredient here is to interpret the early algorithm, this algorithm, as falling into the sum of squares. As a special case of the kind of sum of squares algorithm that have been proposed before. It turns out that on some level, this algorithm here, it falls into the sum of squares framework. Okay, finally, you know, let's, you know, what can we do if you don't make any assumptions about the vectors? Suppose that we have completely general unit vectors, and just for simplicity, assume that we put them into isotropic position. Okay, so now, you know, I told you before that this problem is np hard. So somehow, you know, you would expect that you shouldn't be able to do anything for if you don't make any assumptions about the vectors. But it turns out that, you know, if you're in this viewpoint in terms of these inverse problems, then it still makes sense to ask this question, because what we do is we fix the vectors.
00:25:46.822 - 00:26:39.406, Speaker A: Now we ask how large we have to choose the number of moments in order for us to be able to recover the vectors. It turns out that if you're willing to spend quite a point in time and you want to recover the vectors up to some accuracy, Epsilon, then if you get moments up to degree one of Epsilon Times logarithmic in this ratio n divided by d, then you know, you can recover the vectors. And this algorithm is based on sum of squares. And this, the techniques in this talk, they allow you to do the same, achieve the same guarantees, but with a point normal time algorithm. And this has also some applications for dictionary learning. Okay, so let me see, what's the time? Okay, great. Okay, so now I'll try to give you some of the ideas from the proof.
00:26:39.406 - 00:27:18.686, Speaker A: And so, first, let's let me explain this very first, this very classical algorithm that works for linearly independent vectors. And this algorithm due to Jenrich. So we have access to the first three moments. And as I explained before, if you have linearly independent vectors, we can actually assume that they are normal, because the second moment allows us to. Okay, so now what can we do? Well, we look at the third moment. So this is like a three dimensional array of data and of numbers. And now what we'll do is we will choose a standard gaussian g in dimension d, and we align this gaussian in this way, with our three dimensional array of data.
00:27:18.686 - 00:28:17.834, Speaker A: And we will use it to take a linear combination of the slices that is as sort of as suggested in this diagram. And this will result into a two dimensional array. Now, this is a matrix, and the algorithm, it turns out we'll just look at the eigenvectors of that matrix. This operation is called a contraction, because you contract one index using this vector g. And now, before you had three indices, and afterwards you only have two indices. And the linear algebra operation is just that. You multiply this linear transformation to the three tensor, the third moments, where you don't do anything to the first three indices, the first two indices, and you apply this linear functional g transpose to the third index and the resulting matrix.
00:28:17.834 - 00:28:56.042, Speaker A: It's not worried about exactly how this works. It looks like this. So it's a weighting weighing of the outer products of the vectors AI, and the coefficient is the inner product of the vector g and AI. And now, because we normalize the vectors, these guys will be eigenvectors of the matrix, and the eigenvalues are those coefficients. And because g was a random vector, and the vectors AI, they are orthogonal. These are actually independent Gaussians. And that means that we know with probability one, they are all distinct and they are also reasonably far apart.
00:28:56.042 - 00:29:40.904, Speaker A: And that means that if you compute the eigenvectors, that each eigenspace is one dimensional corresponding to one of the eyes. So that's great. And now the challenge is supposed to, that we want to do this if the number of vectors is much larger than I mentioned. And now the issue is clearly that even if you do this operation, then it can't be the case that each AI is an eigenvector of the matrix, because we just have d eigenvectors. But we have. So that doesn't work if you have more than d vectors. And so here's the approach, the general approach for dealing with overcomplete three tensors.
00:29:40.904 - 00:31:05.784, Speaker A: So we have this input, this free tensor, and now we imagine that we have some magic procedure that allows us to transform, compute from, given the three tensor, given the three moments, third moments, it allows us to compute the six moments of the distribution. Okay, now why would that be good? Well, if you look at the 6th moment of the distribution, you can think of it as the third moment of the uniform distribution over a one tensor width itself, up to a n tensor width itself. And now these guys, these vectors, if you choose a one up to a at random, then these vectors, they essentially look like random unit vectors in d squared dimension, you know, you know, and this, you know, this is good, because we have, you know, the number of vectors that we have here is much less than d squared. And, you know, if I choose, you know, much, you know, at most, you know, d squared vector, random vectors in d squared dimension, then they will be very close to orthogonal. And that, and in particular, these guys, they are close enough to being orthogonal for Hendricks algorithm to work. Okay, so, you know, if you just apply Henry's algorithm, you do this random contraction, then it turns out that the eigenvectors that you will get, they will be very close to these guys. And from these guys, I can recover the vectors, a one up to n.
00:31:05.784 - 00:32:36.130, Speaker A: So that's the plan. What's the weakness of the plan? The weakness is that we assume this magical procedure that allowed us to lift the third moments to 6th order moments. And to get some intuition about how we could go about constructing this kind of magical procedure, let's consider the following ideal implementation, which will not be efficient, but it will guide us to the right thing to do. And so, what would be an ideal implementation of this magic box? The ideal I want to consider is the following. Suppose we want to this magic box, it computes a distribution d over the unit sphere, which has the same third moments as the given third moments, and then just outputs the six moments of the distribution that it computes. Okay, so what we would want to show with you is that if you do this, if you do this search over distributions that have the same third moments as the given third moments, we want to show is that this will result into distribution that has whose 6th moment is close to the true 6th moments. And notice that, I mean, one key aspect of this box is that we somehow have to the existence of this magic box, it relies on the fact that the third moments determine the 6th moments.
00:32:36.130 - 00:33:33.210, Speaker A: Right? And that's by itself, it's a non trivial fact, which, which is only, you know, which will use the fact that the vectors are random. Okay, where. That's here again, sorry. So, let's prove the fact that, let's prove this claim that the six moments of the distribution that we computed in this way, they will be close to the six moments of the, of our vectors, a one up to n. So, first of all, there are some equations that follow from the fact that the, you know, the third moments of the distribution d is the same as the third moments of the vectors, a one up to a n. So, first of all, if you take the inner product of the third moments of m of the vectors with itself, then, you know, this would be a number roughly one over n. And, you know, but if this guy has the same moment as the same third moment as this, then, you know, these two, you know, these twain products have to be the same.
00:33:33.210 - 00:34:34.754, Speaker A: And this inner product, it turns out to be the expectation and the distribution of this polynomial normalized by one over n. In the conclusion of this is just some computing consequences of this equation. The conclusion is that the distribution d, it satisfies the property that under distribution d, this polynomial has expectation very close to one. And now, if you look at this polynomial, it turns out that, you know, with high probability over the vectors, over the choice of vectors a one up to n, the way this polynomial looks like, you know, as a graph on the sphere, it looks like this. So the only points where the polynomial is large are at the points a one up to a n, and everywhere else, it's very close to zero. And that means that, you know, that if the distribution d satisfies this property, that under distribution d, the expectation of this polynomial is close to one. It means that essentially, the distribution has to be supported on vectors very close to a one, up to a n in this sense.
00:34:34.754 - 00:35:24.528, Speaker A: And that means if the distribution is indeed supported on the, essentially supported on these vectors, it also means that if you look at this polynomial, where we raise the inner products to the six, this will also be very close to one in expectation under the distribution, just because if you plug in each of these vectors, you will get something that is essentially one. And now we can do this reverse calculation that we did here. Here we showed that the fact that m three is equal to the third moments of d, it implies that this is large. Turns out that this is essentially some equivalence. And then, if you go backwards, this now implies that the 6th moment of the vectors are close to the six moments of the distribution that we computed. And that's what we wanted. Okay, great.
00:35:24.528 - 00:35:57.380, Speaker A: So now we showed this claim that the six moments of the distribution that we computed is close to the six moment, the true six moments. Now, there are two remaining questions. One question is, this ideal implementation is not efficient, because pretending that we could search over these distributions. So we have to somehow find a efficient way of doing that. Second question is, you know, the kind of error that we have here. It has to be, it should be the case that Yenrich can tolerate this error, because, you know, now we, you know, we feed, you know, this kind of thing, which is only close to this. We feed it to Yanrich's algorithm.
00:35:57.380 - 00:36:35.096, Speaker A: So you know, Yanrich's algorithm better should be tolerant, you know, tolerant enough to cope with that error. So it turns out to be very principled way of dealing with the first question. Turns out that instead of optimizing over all distributions, we can optimize over a more general set of objects called pseudo distributions. And this comes from the sum of squares framework that deals to some extent with the first question. The second question can generate, tolerate this kind of error. So here the answer turns out to be no, the error is too large. So that's the problem.
00:36:35.096 - 00:37:53.628, Speaker A: So what do we do? Well, and, okay, and you can sort of see what's the problem, right? Because if you sort of remember the calculation here, we essentially showed that, you know, these, these two guys are close in euclidean distance, but, but Yenrich's algorithm relies on, on, you know, eigenvalues, you know, magnitudes of eigenvalues. And that means that if, you know, if I, and, or these spectral gaps, and it means that if, you know, if some of these, this kind of error that we have here, it can sort of contribute a small number of very bad eigenvalues, and that will be a break. Generic algorithm. In some sense, this is a problem because we don't really have any, because we think of implementing this matching box as this kind of search. We don't really have much control over what kind of distribution we get from, from this box. And here the idea is to add an additional constraint to the search, which is similar to, in spirit, to a maximum entropy constraint. And so what we want is that if you look at the spectral norm of the fourth moment of the distribution d, this should be small, you know, as small as you can hope, in some sense, this would be as small as the spectral norm of the fourth moment of the, of the fourth moment of the vectors.
00:37:53.628 - 00:38:09.946, Speaker A: A one up to. Nice. What do you mean by the spectral norm of the rank four? Yeah, yeah, that's a good question. So here we shape it as d squared by d squared matrix. But of course, I was hoping no one, David?
00:38:10.010 - 00:38:10.330, Speaker B: Yes.
00:38:10.402 - 00:39:05.550, Speaker A: Can there be very different distributions d that match the. Yes, I mean, if I just tell you that the distribution satisfies this property that matches exactly the third moments. Yes, yes. So even if I just tell you this property, you can take any, for example, you can take the true distribution over uniform over a one up to n, and then add some adversarial but small error to each vector AI, and it will still satisfy the property. And if you add an adversarial error to each AI, if we could choose such a, it's the same, you add an arrow in the same direction for every AI, and this will contribute a very large eigenvalue in every sort of resulting computation. But if the images. Exactly, there can be only one solution, then.
00:39:05.550 - 00:39:55.874, Speaker A: Okay, yeah, I don't know. That's a good question. I don't know how to exploit the fact that it matches exactly. That's a good question. I would doubt that, that it makes a big difference, because on some level, on some level, even this ideal implementation, it's not doing the right thing, because the really right thing would be to look only at distributions that are distributed over n vectors that look like random vectors. So to search over every distribution that looks like the kind of distribution that we, that we are assuming, and the fact that we are sort of looking at an arbitrary distribution over the sphere and just require the third moment's match, it leaves a lot of freedom. But with the maximum entropy constraint, with the maximum entropy.
00:39:55.874 - 00:40:40.068, Speaker A: So in some sense, this maximum entropy constraint, it doesn't really improve the errors that we have, but it turns out that this allows us to control the effect of the error for Henry's algorithm at the end. And I hope that I can show this. So, okay, so what are. So now we only thing that we have to do is we have to deal with these two kinds of questions. So let's see, with the first question. So instead of optimizing of all distributions, we optimize over pseudo distributions, and on some level. Okay, so let me just, so, serializations are generalization of actual distributions where low complexity events behave like you would expect.
00:40:40.068 - 00:41:29.984, Speaker A: They have no negative probability, and everything is nice, but all bets are off if you have high complexity events. And the notion of low complexity and high complexity events is specific. And if, of course, you see it next. So formerly, degree d pseudo distribution is just a function on the sphere and such that if you find it supported, if you sum up the values of the function, you get one. Okay, that's what you would expect from probability diffusion. But it's not a non negative function, not necessarily a non negative function, but it has a property that if you take the, if you look at the inner product of the function d with any square of a low degree polynomial, this will be non negative. So these are the simple events that have non negative probabilities.
00:41:29.984 - 00:42:22.256, Speaker A: And it turns out that, you know, if you sort of change the degree requirement here for the functions, then, you know, you get smaller and smaller sets. And if you choose, you know, if you allow here, if you allow functions of arbitrary degree. That would imply that the function capital d is non negative everywhere. So it means that that really corresponds to uniform distribution, but everything intermediate is a more general object. Now, we can formally define an expectation under this pseudo distribution d, just like we would for actual distributions. And now the advantage of two distributions is that unlike for actual distributions, we can optimize more general distributions efficiently. Particularly, there's a separation oracle, which runs in time d to the k for the convex set of pseudo distributions.
00:42:22.256 - 00:43:08.924, Speaker A: And this means that if you have some, basically every optimization problem that you would like to solve about distributions, you can solve it efficiently for distributions. So maybe that's a take home message. If you ever encounter, feel yourself, if you ever are in the position of wanting to optimize over distributions, but it's not clear how to do it, then you can try to optimize oversee distributions. You know, with some luck, it will work just as well. Okay. It turns out that this idea of distributions is also, it's a way of capturing the best known point of time algorithms for a large class of problems. So on some level, like this is the best way of best efficient way of relaxing distributions that we know of.
00:43:08.924 - 00:44:10.454, Speaker A: So one question is, every time I see this, there is never is the finitely supported assumption. You can always make the final, okay, so here I did it just because I didn't want to write integrals. Of course, if I make this assumption, it's not really convex, right? Because, you know, only sort of like limiting object will be convex. But, yeah, it will not, you know, if you, if you, I just wanted to avoid dealing with measure theory issues, but, yeah, finally supported, you know, because on some level, you can assume that these functions are really, this function capital d, you can assume that it's a degree, it has degree at most k. So it's a low degree polynomial. And then that sort of means that you can specify it as a finely supported distribution, finitely supported function. So maybe, let me skip this.
00:44:10.454 - 00:45:35.876, Speaker A: Okay, so this is a notion of sum of squares proofs, which is an abstraction of the kind of properties that hold for pseudo distributions that are true in the real world and also hold for pseudo distributions. Let's not go into that. So one thing that now what we want to do is before we said that, we saw proof that if you have an actual distribution whose degree three moment is the same as the degree three moment of our vectors, then the degree six moment is close to the degree six moment of our vectors. And if you recall that proof, actually the only thing that we used is the fact that if this polynomial has large expectation under D under distribution D, then also this polynomial has large expectation under D. And that means that on some level, we just have to prove that this inequality is still true for distributions. And this can be done. And it boils down to proving bounds on the spectral norm of these kind of matrix polynomials, which are, unfortunately, we don't have good general tools for doing that.
00:45:35.876 - 00:46:22.932, Speaker A: But it turns out that for this particular matrix polynomial, we can prove good bounds on its spectrum norm. So this addresses the first issue, trying to replace the search over distributions with search over pseudo distributions. So this means that nothing changed. Now, the second issue is assumptions, an orthogonal issue. And here we want to say that this additional constraint, that the spectral norm of the computed distribution of the moments of the computed distribution is small. This allows us to say that yen really still works. And the way we will say this is that you show that if you do this random contraction, then the top eigenvector of the resulting matrix will be close to one of the vectors, a sub I.
00:46:22.932 - 00:47:20.342, Speaker A: And, okay, so maybe, let me just show you on a high level what's happening. So if you do this random contraction, you can think about it as doing a random contraction in the direction AI and a random contraction orthogonal to the direction AI. And now that means that this random contraction, it naturally decomposes in something parallel to AI and something orthogonal to AI. It turns out that the condition that our moments are close in euclidean norm to the true moments, this allows to show that this behaves nicely. And the fact that we have this entropy constraint, it allows us to show that this orthogonal part behaves nicely. Okay, and now, you know what? You want to let me skip the calculation here. I think I lost a.
00:47:20.342 - 00:47:44.306, Speaker A: Skip all of that. So this was. Okay, so maybe just a word here. So, you know, one unfortunate aspect of this algorithm that I described is that it uses this. It optimizes yo over pseudo distributions, which is a very slow process in general. And you would like to have this also do this fast. And it turns out that, you know, on some level, what you wanted to do here is not, you didn't want to.
00:47:44.306 - 00:48:41.998, Speaker A: The only thing that you want to do is find an object here that fools Yandrich's algorithm that behaves like a true six moments to Yenrich's algorithm. And it turns out that you can hope you can do, to some extent, implement this kind of procedure in a very simple way, just as a polynomial of the input tensor, as a low degree polynomial of the input tensor. And this object here turns out to be a degree three polynomial in the input tensor. And this turns out to fool Yannrick's algorithm in the same sense that the moments of the distribution that we computed before fool Yanric's algorithm. And this gives a fast algorithm, in particular in a runs in time due to the 3.33 skip that. Let me conclude.
00:48:41.998 - 00:50:04.760, Speaker A: So we saw a way of using some squares for tensor decomposition, and there were two parts here. One was to use a proof that is so valid for pseudo distributions, which shows that the third moments determine, up to some small error, the six moments. And the second part was to show that the Jennrix algorithm can be used as a rounding algorithm for the kind of pseudo distributions that are computed by sum of squares. And then there's a way of obtaining fast algorithms from this thermosphere's viewpoint by, instead of computing two distributions, it's enough to compute some low degree matrix polynomial of the input, which, with the aim of fooling the rounding algorithm that we had here, two open questions. So one is, so what we saw is a way of decomposing random three tensors where the rank is at most d to 1.5. So this d to the 1.5 is a bit mysterious, but it seems to be like the right answer in the sense that in the following sense.
00:50:04.760 - 00:51:05.804, Speaker A: So it is possible information theoretically to decompose random three tensors, even if you do, even if you have n squared D squared vectors. But, but it seems to be that going beyond dita 1.5, you run into some inherent computational complexity issues. And it would be nice to formalize this feeling. And final thing is that, so when we talked about the smooth analysis for tensor decomposition, we saw that there's a way of doing five tensors and four tensors, and it would be great to be able to also sort of have a smooth analysis for over complete three tensors. Okay. And, you know, on some level, that would be, that would be a great strengthening of our result for overcomplete, for random overcomplete tensors.
00:51:05.804 - 00:51:13.524, Speaker A: Okay, thank you very much. Questions.
