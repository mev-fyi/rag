00:00:00.160 - 00:00:52.690, Speaker A: We will get started with our final session for today. My name is Jake Goldenfone. Thanks for being with us, and thanks for sticking around to the end of today. Before we go on, I'll just remind people we do have more sessions tomorrow, two more sessions in the morning prior to lunch, that both promised to be really interesting and fascinating. So encourage you all to come along. In our final session for today, we have Andrea Bobu, who is here at UC Berkeley. Andrea is actually part of the persistent group, the AI and humanity group, that has been meeting for the last few weeks and will continue to be meeting over the next couple of weeks, but is unfortunately traveling.
00:00:52.690 - 00:01:08.414, Speaker A: So wasn't able to be with us today. But we're very grateful that she's willing to offer up a presentation via telepresence. So, without further ado, Andrea, the room is yours.
00:01:09.314 - 00:01:11.774, Speaker B: Thank you. Can you all hear me?
00:01:12.154 - 00:01:13.054, Speaker A: We can.
00:01:13.794 - 00:02:05.674, Speaker B: Excellent. Yeah. Very sad to not be there in person, but I'm glad I get to at least talk to you virtually. So I'm going to talk a bit today about how robots can better generalize their learning by aligning the representations with the humans that they're actually learning from. And let's start off by imagining that this Jayco robot arm wants to carry my mug to the table. And so it generates a trajectory Psi that optimizes a reward function r parameterized by theta, trading off between different features, fee of the task, things like the cup orientation, or the distance to the table, or the efficiency. But as I watch it perform the task, I'm pretty unhappy with how high above the table it's carrying the cup.
00:02:05.674 - 00:02:55.142, Speaker B: And so what I can do is I can push on it to correct it to stay closer. The robot can use my correction to update its understanding of these trade offs data and the reward, and then optimize the new reward function to adapt to how I wanted the task to be done. Now, I want you to notice that the reason this worked is that the robot already had a representation of all of the things that I cared about a priori, like all of these features that I mentioned previously. But that's not very realistic. Real human environments are way too complex to be able to specify every single thing ahead of time. In fact, even we as humans sometimes don't always have the right representation right away, and children are a great example of that. So let's look at what happens when you don't have the right representation.
00:02:55.142 - 00:02:58.726, Speaker B: And this video is going to have some audio. I hope you can all hear this.
00:02:58.870 - 00:03:03.470, Speaker C: Okay, does this row have more quarters. Does this row have more quarters, or are they the same?
00:03:03.542 - 00:03:04.190, Speaker B: The same.
00:03:04.302 - 00:03:13.242, Speaker C: The same. Okay, now watch. Now, does this row have more quarters? Does this row have more quarters, or are they the same?
00:03:13.338 - 00:03:15.050, Speaker B: That one has more quarters.
00:03:15.122 - 00:03:16.314, Speaker C: That one has more quarters?
00:03:16.354 - 00:03:16.594, Speaker B: Yeah.
00:03:16.634 - 00:03:17.954, Speaker C: Why does that one have more quarters?
00:03:17.994 - 00:03:22.730, Speaker B: Because it's stretched out. Then we're going to see another one.
00:03:22.922 - 00:03:36.774, Speaker C: We're going to take water from this glass. We're going to pour it into this glass. Now, does this glass have more water? Does this glass have more water, or are they the same?
00:03:37.954 - 00:03:39.330, Speaker B: Has more water.
00:03:39.442 - 00:03:42.106, Speaker C: This one has more water. Can you tell me why?
00:03:42.250 - 00:04:13.250, Speaker B: Because that one's higher than that. That one's higher than that. So here, this child is clearly missing something crucial. He's missing the concept of conservation of certain properties like count and volume. And that's very normal for a child this age. That doesn't develop until much later, a few years later. But without understanding this concept, the child was consistently making the wrong inferences and answering the researchers questions incorrectly.
00:04:13.250 - 00:04:47.514, Speaker B: And not only that, but he was also able to explain his wrong answers with some other concepts that he did know about that were completely unrelated. So now let's see how this translates to robots. This time, I want the robot to stay further away from me. But the robot has no notion of distance from people. So now my push actually makes it stay closer to the table. So there's two crucial problems happening here. First, without the right to representation, the robot can't learn what I want, which is to stay further away from me.
00:04:47.514 - 00:05:43.486, Speaker B: But even worse, it also tries to map my input to something else that it does know about this table. In this case, similarly to how the child was latching onto all those other concepts, they explained his answers so clearly. To be able to interpret people's input correctly, robots need good human representations. But there's a question of how do we get them? And we're calling this question the representation alignment problem. How can the robot align its representation and learning with the human that it's actually interacting with? So what we've seen so far as an answer to this question is feature representations that are hand specified, that are written down by a system designer, a priori, on top of which, the robot learns a reward function from the human input, ah, theta. So we just saw corrections as that input. But there's many other types of input, like demonstrations, teleoperation, and so on.
00:05:43.486 - 00:06:30.878, Speaker B: And then it optimizes that reward to produce trajectories in the world. And the problem is that this does require this, a priori specification that is not very viable. So there's a simple solution to this. You might say you can just learn the features as well. After all, that's the main thing that the deep learning revolution has enabled us to do, and let's try that and see what happens in practice. So I'm going to show you this example where the person cares about being efficient and keeping the cup close to the table that we just saw, and away from a laptop on the table. So if we imagine the table as the ground plane spatially, the human's reward might look something like this, where the lowest reward in yellow is right above the laptop and high from the table.
00:06:30.878 - 00:07:10.134, Speaker B: High above the table. And it gets better the closer to the table and the farther from the laptop we look. Meanwhile, the robot knows about some of these features, but not about distances to laptops. So what we want this for the robot to somehow learn about this missing feature and how it combines with the remaining features and the reward function. Deep learning tells us that we can actually do both of these things at once. First, you model the missing features in your own network, and you add it to your set of known features. You collect a few demonstrations of the task from the human and use them to both extract what's missing implicitly and learn the reward function, r theta on top of that.
00:07:10.134 - 00:08:27.900, Speaker B: It turns out that when you actually do this, we vaguely learned that being around the laptop is bad, but we don't really learn the more fine grained details of the reward function. And that's because the demonstrations that the person gives are only meant to teach the robot how to do the task, they aren't meant for learning the features per se. So the method is just sort of hoping to capture that information, the missing features, implicitly, through the demonstrations. But doing that effectively requires thousands of demonstrations, which we simply just don't have in human robot interaction scenarios. So by trying to both extract salient features of the task and the reward on top of them all at once, for with just a few demonstrations, deep learning generalizes pretty poorly. And the implication is that now when we optimize the reward, we see a behavior that mostly stays away from the laptop, but doesn't really go close to the table, as I would actually do if I were to do the task. So what we have is one method that let the robot learn a reward by specifying the representation, but specifying that representation is difficult, and then another method that avoids that, but does a poor job extracting both the representation and the reward implicitly from little data.
00:08:27.900 - 00:09:11.964, Speaker B: The question is, can we somehow get the best of both worlds here. And the core idea that I'm going to be presenting is this idea of dividing and conquering the robot learning problem. Let's first focus explicitly on learning the representation from human input. So that's the orange part before using it for the downstream reward learning problem. And what this means here is that we can't just rely on inputs design to teach you the representation implicitly, like the demonstrations that we just saw earlier. And instead we need to actually design special human input ah feed that is explicitly targeted at teaching you about the representation and about the features, and it can do so effectively. And this part is the one that I'm going to focus on for the rest of the talk.
00:09:11.964 - 00:10:48.992, Speaker B: So coming back to this example where we had the deep inverse reinforcement learning, or IRL method that try to learn both the representation of the reward at once from the high dimensional raw state of the robot. What if instead we apply this divide and conquer philosophy and first focused on learning the feature by asking the human for some sort of input about it? That I'm going to tell you in a second, and only then learn the reward from reward inputs like demonstrations or the corrections that we saw earlier. So now there's a question of okay, Andrea, but how do you learn these features? Well, we can still model a feature as a neural network, and one naive solution would be to ask them for different labels representing the feature value of certain states, and then use these labels to train the network via supervised learning. The problem is that to learn anything useful, the robot will need a very large set of labels from the person, and that's way too burdensome to ask a person to do. And even worse, humans can actually be pretty imprecise at labeling states directly with real numbers. What we need here is some feature input that trades off being informative and at the same time not placing too much burden, too much sample complexity, burden on the human that's giving that input. So our idea here was that instead of having the person label states independently, we can pack a lot more information if we ask the person for a sequence of states with feature values that are monotonically decreasing along the trace.
00:10:48.992 - 00:11:48.144, Speaker B: And we call this sequence a sequence with this property a feature trace. And in practice, what this looks like is that if I want to teach the robot about distance from the laptop, I start somewhere where the feature is high expressed, so right above the laptop, and then move it monotonically towards a place where it's not highly expressed. So away from the laptop, the robot collects a few of these traces and then it can learn a feature function that is defined by them. The beauty in these feature traces, that there's a lot of inherent structure packed into them and we can actually train a neural network efficiently. So first we have the feature traces monotonicity assumption, where the feature values decrease along the states of the trace. But next, we also know that all of the traces approximately start at a high feature value and then a low on. So for two traces, what this means is that the starts, the labels for the starts have to be consistently high and the labels for the ends have to be consistently low.
00:11:48.144 - 00:12:46.798, Speaker B: And lastly, humans are imperfect, so we modeled them as noisily rational according to the Bradley Terry and Lewis Shepard model. So now we can't learn from the feature traces directly, but what we can do is we can transform them into a data set of tuples that are actually suitable for training. So the way we do that is we take each of these properties and transform them into tuples. From the first assumption, the monotonous assumption, we can take any ordered combination of two states along the trace and label them a one, which means that the first state in the tuple has a higher feature value than second. And what's special about this is that for each trace we get a number of tuples that's quadratic in the length of the trace, which is a lot of data. It's a lot of comparisons. Now, for the second assumption, we essentially want all of the starts to be indistinguishable from one another and all the ends to be indistinguishable from one another.
00:12:46.798 - 00:13:47.028, Speaker B: And that gives us equivalence to pulse with a label of zero five to symbolize the fact that these two states in the tuple are equivalent. And lastly, we incorporate this noisy assumption into a cross entropy loss for training a simple multilayer perceptron network using these tuples. Now, I want to show you some actual examples of features that we learned using feature traces from both expert users with a physical robot, aka me, and as well as with non expert users in simulation. This happened in simulation because this was happening during the depths of COVID in 2020. So we can exactly do this experiment with real people. Unfortunately, and for both qualitative and quantitative evaluation here, I'm going to show you these sampled test set of states across the robots reachable set. So this is the 3d ball of points that I'm showing you here.
00:13:47.028 - 00:14:33.790, Speaker B: And what I'm visualizing here with colors is the feature values on all of these points. In yellow you have a high feature value because the robot is closer to the laptop and then it gets more to the blue as you go farther away from the laptop. And for easier visualization, we can also project the feature ball onto a representative plane like here. So let's first look at results with expert data that are from me, indicated by this icon in the top right. When we look at the ground truth laptop feature, we see a projected ball that decreases in intensity the farther from the laptop we look. Then with our method, we're able to learn a structurally very similar feature, which is really cool. And this actually stays true for all of the other features.
00:14:33.790 - 00:15:19.698, Speaker B: We look at some stuff like distance to the table, proxemics, coffee cup orientation and so on. And keep in mind that this result, these results that we're showing you here are collected with like ten or 20 feature traces, not thousands. So this is much more manageable for a person to give. And quantitatively, we essentially looked at the learned feature value 3d balls and compared them to the ground truth feature values and just computed the mean square there between them. So here, lower is better. What we see is that with enough data, our method learns features that are much better than random random is that dotted line in gray. Then with more data it becomes the features that we're learning become increasingly better.
00:15:19.698 - 00:16:11.820, Speaker B: And our method also gets less input sensitive as we see by this decreasing standard there. And overall, these three observations tell us that we can learn good features with expert feature traces. But what about novice users? It turns out that we were also actually able to learn good features. Here the user data is in yellow, and when compared to the, they are closer in performance to the orange, which is the expert level, than to the gray, which is the random performance. That tells us that even with novice data, you're able to learn something that has some structure. Okay, so now that we've seen how to learn features, let's look at how we can use this pipeline to learn the reward. And we're going to compare that against the deep learning baseline.
00:16:11.820 - 00:17:07.648, Speaker B: From the beginning, I looked at two settings. First, there's the online learning one where the robot starts off with some features already it detects that it's missing something and expands its representation with a new feature that we learned the way I just showed you. Then we combined them all into the reward function. We took furrow feature, expansive reward learning, which is what our method is called, which first learns the feature from traces and then the reward from corrections. And we compared it against maximum entropy IRL, which learns both from demonstrations and for the setting. The human cared about two features in the representation for the reward, one of which was unknown to the robot, and we varied which of the laptop table and Proxmix feature was unknown. And in these visualizations, what we see is consistently furl is able to learn a more fine grained reward structure than Maxent IRL.
00:17:07.648 - 00:18:17.200, Speaker B: And this result is also confirmed quantitatively as well. And we also took the user tot, so the novice taught features and performed reward learning. And we kind of see the same story where yellow is closer to orange than to the baseline in gray, even though the Maxent IRL baseline used expert demonstrations that were performed on the physical robots and not in simulation. So even though our method was at disadvantage, it still performs better, because the structure that features induced is so useful. We also looked at an offline setting where this time the robot knows nothing a priori, no features, and it just needs to learn each one of them one at a time, and then learn the reward on top. And so we compared furl here, which again learns each feature one at a time, and then learns the reward with demonstrations against the max and parallel baseline. And this time we had three scenarios where the reward depended on one, two or three features, and the robot starts completely from scratch.
00:18:17.200 - 00:19:26.764, Speaker B: And when we look at the results, what we see is that for the single feature case up top, which is a simpler reward structure, both methods actually perform well. But for the other two structures, which are more complicated and maybe have more detail to them, furl is qualitatively much closer to the ground truth than Maxenti RL, the deep deep variant. And these results are also confirmed quantitatively. And now when we're looking at the same experiments with non expert features, the results are a bit more mixed. So what we see here is that yellow, which is the non expert features, it's not necessarily providing a reliable advantage when compared to Maxent IRL. And we think this might be the case because again, people did teach these features in a simulator, whereas Maxenti RL still operates with real physical expert demonstrations. So when you like, add a bunch of little noisier features together, maybe you don't necessarily get as much of the structure benefit as you get with expert features as you see with orange.
00:19:26.764 - 00:19:43.324, Speaker B: But overall, I'm very excited about our progress so far, and I really hope that this divide and conquer philosophy can result in more generalizable robot learning, perhaps even extending domains outside of reward learning as well. Thank you.
00:19:55.494 - 00:20:36.204, Speaker A: Okay, thanks so much for that presentation. I'm really interested in this idea of like the noisy human, because so much of what we've been doing seems to be trying to figure out exactly what it is a human means or a human wants, or what a human values or what a human is capable of doing in all different kinds of contexts. So I appreciate this, this thing that's connecting what we've been talking about. Does anyone have questions for Andre? Yeah, Sonia? Anyone else at this stage? Okay, keep thinking.
00:20:38.564 - 00:21:11.492, Speaker D: This is super interesting. Thank you. So I'm really curious. You mentioned that one of the factors was how high the robot arm is off the table. Now, what struck me about this and watching the robot perform was that it seemed like, well, why do the humans care if the arm is so high? Maybe the robot's teaching us something that maybe it's superfluous, because that seemed to be a preference you threw in there that I didn't. I mean, I got the laptop. That seemed to make a lot of sense, but maybe the robot had something there that we should learn from.
00:21:11.492 - 00:21:16.944, Speaker D: Like why. Why that idea that the arm should be higher? Is this an arbitrary addition?
00:21:17.404 - 00:22:06.084, Speaker B: I see. Yeah, I guess the full story of that was that if the robot is carrying, like, your precious mug, or maybe it's carrying some liquid in that mug, I guess it being closer to the table means that it risks spilling all over the place a little less. Or if it drops by accident, the mug, it's not going to shatter on the table because it's going to be closer to the table. But it's true that, in general, these examples are. We can't incept preferences into people's heads. As part of the experiments, you can't have to. But as part of the experiments, you do have to somehow make people comparable, which is really hard.
00:22:06.084 - 00:22:35.744, Speaker B: And so because of that, you kind of have to give them a script and be like, I imagine that you don't want this cup to be spilling liquid close to the laptop. Imagine that you don't want the robot to accidentally spill. Sorry, the robot to accidentally let go of the cup and shatter the cup and so on. So, yeah, we do kind of create these scenarios when we run our user studies.
00:22:38.844 - 00:23:30.232, Speaker D: So, because it seems like that there's this. I don't know if it's trade off is the right word, but the way. So the laptop, and trying to get the computer to conceptualize that you shouldn't hover over the laptop with the liquid, that it is interesting to try to think about what is implicit and explicit in the way we maneuver in our environment. And I guess that's what's tricky about this. And this is because with the robotic arm height. I mean, you could think about that maybe for, I mean, intuitively, it seems like, of course, you don't want accidents to happen or the cup to slip, but what if it's a case where I know there's some treachery that you should be a certain, you know, elevated a certain amount above the table? So I guess I found myself trying to figure out how even in setting up the experiment, trying to figure out the implicit versus the explicit, it seems like a really tricky space to be found functioning.
00:23:30.288 - 00:24:36.184, Speaker B: Yeah, I think you're asking a question of like, something that's not captured here at all. Is the communication of, like, right now, a lot of this work is human tells robots what to do, but there's no communication of robot tells human what it knows or why it's trying to do what it's doing. So I think this is not something I've explored yet, but I'm actually really excited about in the future. Once I have this idea that once you have, once these representations are aligned, you're basically, you should, in principle, be able to open the communication channels both ways, both from human to teach the robot more efficiently, and also for the robot to explain to the human what it knows and what it doesn't know more efficiently and more interpretably, if you so wish. So that's an aspect, this aspect of explainability, and maybe interpretability is something that I'm really excited to explore in the future. Thank you.
00:24:40.884 - 00:25:31.084, Speaker A: I guess I have a question. So we this morning, and I imagine you weren't, don't have the advantage of seeing some of the papers that we heard this morning, but Smith and Sarah were both describing certain projects in recommender systems. They were effectively about trying to map an abstract form of value from the human via looking at behavioral signals with respect to how they interact with content or some sort of recommendation system, is this kind is working towards a reward function or trying to determine the reward effectively the same thing, but in an embodied space?
00:25:31.584 - 00:25:32.344, Speaker D: Yes.
00:25:32.504 - 00:25:38.684, Speaker B: Yeah. I think that, I didn't see the talk, but I would think that that is the case. Yes.
00:25:39.504 - 00:26:06.504, Speaker A: And I suppose the question becomes then, is there, you know, besides kind of the obvious, is there, what, what are the differences? Or what do you imagine the differences might be in trying to map unarticulated behavior like cognitive preferences versus unarticulated human know how with respect to doing something in the world?
00:26:09.164 - 00:26:12.988, Speaker B: Sorry. Unarticulated cognitive preferences.
00:26:13.036 - 00:27:02.854, Speaker A: Well, so, yeah, so, so the recommender system one is like, humans aren't saying exactly what they want with respect to the content that they, like, they were, they're just doing something and we're trying to effectively read their minds. And then when the stuff you're doing is about, you know, embodied know how, how we do stuff in the world that we don't necessarily articulate, we don't say to the robot, you know, like when you move a cup, these are all the things that our bodies just know what to do implicitly because we've learned them from convention and repeating and action so many times. So I'm just asking you to maybe speculate a little bit about, you know, what's the, I guess, I guess it's a philosophical question. What's like, what, what might be the differences between those two kinds of, of processes of trying to understand what a human can do and what they're about.
00:27:03.314 - 00:27:47.064, Speaker B: Yeah. So if I understand correctly, the first scenario is maybe a recommender system scenario where the AI sort of watches your behavior and then learns what you like just by passively watching your behavior. And the second scenario, the one that I've been talking about, is one where the human actively takes on the role of the teacher. So the human is trying to, like, is very conscious that they are trying to teach the robot something. So in that sense, the data is different. The way you communicate it is different. The kinds of rewards that you're going to learn are very different.
00:27:47.064 - 00:29:00.868, Speaker B: The issues, I think both, maybe the second one will have the issue of, I might try to specify to teach you what the reward is, but I might myself be wrong about this reward. I might give examples of the flights that I prefer, for example, and tell you, look, I like this flight from x to y that costs a lot of money. And I like this flight from y to z that also costs a lot of money. And then you're going to learn that, okay, Andrea likes to take expensive flights, but in reality, the reason they cost a lot of money is that they were all, they were both nonstop. And so if you had seen more data of me choosing, choosing flights that were cheaper and none and nonstop, then maybe you will have drawn, you would have drawn different inferences. But there is this risk of when I'm teaching your stuff myself, you might draw the wrong inferences because I'm not giving you enough data or diverse enough data and because I myself am not thinking of all the possible scenarios that could exist out there in the world. So that's.
00:29:00.868 - 00:29:22.504, Speaker B: So I guess there is this risk of learning the wrong reward or maybe learning how to exploit the system in order to score what the robot thinks is high reward, but is not actually what a person wanted in the first place.
00:29:22.964 - 00:29:55.664, Speaker A: Yeah, thanks for that. That's really helpful. And I suppose that's maybe raises a question for the ethicists here, which is whether there's a really meaningful distinction between the way that a human value or human reward is modeled in their didactic interaction with an electronic computational system versus one that's almost trying to trick them into revealing their preferences, like a game theory model that we heard before. Helen?
00:29:56.644 - 00:31:36.358, Speaker E: Yeah, I thought, Jake, your question was, I love the question, because I hadn't seen that these discussions we've been having with Smitha about what's, they're the different vectors, let's say input one is behavioral. I mean, there are lots, but one class of them are behavioral, and the other are like, just ask the person what they like, you know, get that input. So I hadn't seen the connection between that and in this kind of work that Andrea is doing. And when I was listening, Andrea, to you saying, you know, if I don't give them enough information, they might think, you know, the, the system might infer that I like expensive flights. And then I was going to a different conclusion, which was, well, just ask the person, is it that you like expensive flights or you like nonstop flights? Like, why go through this whole behavioral observation? But the question I wanted, the actual question I wanted to ask you was, I'm also very happy with the way you prefaced the presentation because it feels like we've been having some impact on your thinking. And so just one little thought from you about, okay, the coffee cup case, we can get on board with, because maybe there's a sense of agreement about what we want. We don't want our coffee to spill on our computers.
00:31:36.358 - 00:32:01.214, Speaker E: So good on you that you're teaching that to the robot arm. But how do you see moving into more dangerous territory when you're making assumption about human need or human preference as the reinforcement learning designer? And you would say to yourself, I have to now be a little bit more cautious about this.
00:32:04.354 - 00:32:47.874, Speaker B: Yeah. So let me see if I understand the question by giving an example. Suppose that my cup is filled with water, and I'm trying to teach you the, like, cup orientation feature. So what I would normally do is I would start where the cup is upright and then take you completely, like 180 degrees, basically flipped. The problem is that if I did that with a cup that has water in it, I'm going to spill all the water or coffee or whatever. So are you asking, how do you teach stuff like this when actually giving the input is a little dangerous or it's not safe.
00:32:49.414 - 00:32:54.006, Speaker E: It's more about teaching a task where there's.
00:32:54.030 - 00:32:54.998, Speaker F: I don't think she can hear you.
00:32:55.086 - 00:32:56.514, Speaker B: Oh, sorry.
00:32:58.134 - 00:33:36.384, Speaker E: My question was moving away from that case is terrific also, but it was more about moving away from the robot arm and coffee and a slightly more complex maybe complicating it by the fact that we don't actually know that all humans share a lot of, or maybe some subsection of humans, their gender differences and so forth. But you as the designer have to, as sts scholars might say, inscribe the user when you're teaching through reinforcement learning.
00:33:37.444 - 00:34:26.403, Speaker B: Yeah. So maybe you're talking about personalization. So these rewards right now, the way I presented them, they're kind of meant to work for just the average human, but not every human is the average human. And so how do you personalize these to, like, each individual person? And that's a good question. I think part of the difficulty right now in a lot of work out there is that. So a lot of work in reward learning and just robot learning in general kind of assumes that you have a system designer or a sort of expert that is able to collect data to give to the robot. But the problem is that not everyone is the system designer.
00:34:26.403 - 00:35:14.124, Speaker B: Not everyone has the same beliefs or preferences or whatever as system designers. So ideally you would like each individual person to give input on their own as well. The problem is that a lot of the types of input that exist right now are not necessarily suitable for novice users. Like people. Regular people don't know how to program a robot. They don't know how to give this very specific type of input to teach the robot how to do stuff. And there's the danger that if they don't know how to program the robot they might do it incorrectly, they might teach it the wrong thing or they might not teach it anything at all.
00:35:14.124 - 00:35:51.684, Speaker B: So I think, I believe that a really important direction would be to come up with interfaces, come up with types of input that are just much easier for regular people to give. And I think we should start focusing on efforts on the regular person and on like trying to personalize these things to actual end users and not just assume that each person is going to be some kind of oracle that gives you the perfect data or each person is going to be the average human that's going to give you that, like just average human data. Yeah.
00:35:55.824 - 00:37:29.584, Speaker F: This is probably more of a comment, but it's just reflecting on the conversation as it's gone and it's just making me think of this kind of sort of etymological and philosophical lineage that gets a bit obscured in the english word technology. So the distinction between a kind of embodied technique or artisanal knowledge or just the ways in which humans behave and the way that we make things and interact with the world versus the kind. So that being like technique or techne versus its abstraction as technology, like the kind of epistemic superiority of, like, when we convert that into an abstraction and turn it into a sort of scientific project and then produce a sort of factory that replaces the artisanal workers, the kind of classic distinction, and there's a lot of critical scholarship that sort of is centered on kind of defending the kind of moral worth of techniques themselves as they are embodied and learned and passed along. And it's interesting in this example, I just think, because there's a kind of doubling going on where, Andrea, like your work, is in itself a kind of process of learning how to capture technique and abstract them into a kind of robot behavior, which, which creates an interesting sort of potentially an invaluted loop between what humans are doing and how they will interact with the robots that have been designed in a kind of technologization of human behavior. This is just an observation, but it's maybe the distinction that I think Helen was just getting at. That's how I kind of see that kind of distinction seems role.
00:37:32.104 - 00:37:33.004, Speaker B: Cassie.
00:37:36.744 - 00:39:10.916, Speaker G: Hi, Andrea. I had a question about the sort of the process of the machine learning that you're talking about, the robotic learning, whether or not you followed up on any of. I think there was some work done at University of Georgia, Georgia Tech, where they took the sort of human ontology model about how small children, you had the toddler in your video, and how children learn to do things and how they learn concepts. And one of the things that's built into their thinking was robots need to learn to do things with humans the way humans learn to do things with humans. That's by interacting with them over a period of time. But one thing that happens for those of us who had the great pleasure of raising children is when your kids get to the phase, when they all start going, why? Why are you doing that? Why do you say that? Why? And so asking for, like, a causal explanation, like, so that the child, as I see, is going through some causal mapping using Andy Clark's terms in an embodied sense, they're trying to figure out causally, where should I situate myself vis a vis this person, mom or dad, that I'm interacting with? I'm just wondering whether that could be sort of modeled into the process you were talking about to get a more embodied, interactive sort of questioning, like we've been talking earlier this morning, about AI being explainable to humans, but you're actually wanting humans to be explainable to the machine so that the robot gets an explanation as to why it should lower the cup rather than taking over the high route. As Sonia said, it just wants to know why it does.
00:39:10.916 - 00:39:17.504, Speaker G: It's not resisting, but it does want to know why. And so part of that process of learning, somehow building that into the system.
00:39:18.924 - 00:40:15.492, Speaker B: Yeah, I love that question. I guess in the depths of my research, I've been thinking of these features as a why? Because, I mean, I know it's not necessarily super obvious, but I'm thinking the typical reward input is a demonstration. I show you how to do the task, but then the whys are kind of the features of this trajectory that I'm showing you. There's the y of the distance to the table, or there's the y of the distance to the laptop. So I guess there's this communication through just like the computerized values that these features have. But I think ideally in the future, I would love for this communication to happen through natural language. So right now, the feature distance, the table, is just like feature number one or feature number two.
00:40:15.492 - 00:41:25.440, Speaker B: The robot doesn't really know what they mean in terms of language. So I think language is going to be super powerful because if the human could tell it, like, it's super powerful because you have this shared way of communicating where the human can tell you, it's because I want you to be closer to the table, and then the robot can translate that and be like, ah, okay, the distance to the table feature has to be, you want me to keep that lower in general. That got it. Great, I understand that. So I think, in essence, at all of these, all of these types of inputs, even with that correction, with the push, that's what I'm trying to communicate to the robot. I'm trying to communicate stay closer to the table, but I'm doing so indirectly. I'm just, like, pushing you to do that, and you're just kind of learning that, oh, you want me to stay closer to the table, but they don't ever get an explicit communication of because you're doing the task wrong, because I want you to stay closer to the table.
00:41:25.440 - 00:41:30.084, Speaker B: And I think, yeah, I think language is going to be super important for that.
00:41:31.864 - 00:41:37.724, Speaker A: Thank you so much. Let's say thanks to Andrea for the talk.
00:41:41.924 - 00:41:50.624, Speaker B: Thank you for having me I hope you'll enjoy the banquet or whatever is happening after this.
00:41:51.324 - 00:43:00.164, Speaker A: Total pleasure. Okay, so we have just like a few minutes before, while they're finalizing the setup for the reception. And I suppose we can use the time to do, like, customer feedback survey or maybe some form of taking stock of what's happened. If anyone has thoughts that they might want to express to the group around what we're doing here, what we might continue to do or could do better, or what sorts of things we might be interested in, in producing out of this, or how could some of this really quite radically, experimentally interdisciplinary sort of thinking be meaningful to others? So this is open to anyone to reflect on and to comment on. I do realize it's the end of the day and we're all a bit tired, but maybe it's also just a chance to take those questions on board and. And maybe think about them over a little while.
00:43:06.104 - 00:43:42.804, Speaker H: Yeah, I mean, this is more you have a flag for tomorrow than something to do now, but I'd love to just have some space separate from the particular panels, just to have sort of free form conversation of, where do we take this next? It doesn't have to be like, here's the paper we're all going to write, but what are some open areas for continuing dialogue and for finding collaborations or research funding, or just thinking about what a research agenda around these topics looks like? Because I think sort of to try to just find ways to synthesize the conversation and think about paths forward, however loose or precise we want to be around that.
00:43:43.504 - 00:44:09.100, Speaker A: Yeah, I think that's what hike means in the schedule, one way or another. Maybe. Hopefully, like I always had imagined, that if we are going for a walk outside together, then maybe that's the kind of thing that we might be trying to generate. You could wear a whiteboard as a backpack and then someone, well, bring sharpies.
00:44:09.132 - 00:44:11.788, Speaker H: You have a nice white shirt. We can just use your back.
00:44:11.916 - 00:44:12.944, Speaker A: Fine by me.
00:44:14.484 - 00:44:17.276, Speaker E: I do think that do we want.
00:44:17.300 - 00:44:19.164, Speaker B: To have some time? Because it's sort of.
