00:00:07.960 - 00:00:51.674, Speaker A: Thank you very much for this introduction. It's a great pleasure and honor to give this open lecture to such a diverse audience today. So I hope I can cater for the majority of people in this room. So, my title is, in pursuit of structure, why? A little randomness almost always helps. Um, the first type of structure I'd like to explain to you is the structure of my talk. Uh, so, first, I'll tell you about, uh, some very old, um, problems, um, in number theory, uh, and a not quite so old problem, uh, that looks a little less number theoretic. Um, I'll try to explain why random is good.
00:00:51.674 - 00:01:52.714, Speaker A: Uh, I think many of you are familiar with that general theme and why random looking is still pretty good, and why neither is likely to be the end of the story. Okay, now, what kind of mathematical structure am I going to talk about? So, I'll first talk about the problem of adding two primes. There's a very famous conjecture due to Goldbach, which goes back to a letter that Goldbach wrote to Euler in 1742, which you see here on the left. So, he conjectured that every even integer greater than two can be written as the sum of two primes. For example, you can write 20 as a sum of 13 plus seven or 34 as a sum of 31 plus three. I am extremely worried. And in one of my examples, there's going to be a number that's not a prime.
00:01:52.714 - 00:02:47.384, Speaker A: I did check them twice today, but it may still happen. And we do know this. We can check this conjecture computationally, and it's known up to some very, very large number just by people doing computation. But the conjecture itself, as stated, is still wide open and believed to be very difficult. Um, we know that, uh, the conjecture is true, um, for almost, um, all, um, even numbers, um, and various other weak variants, which I'll explain to you in a moment. Now, um, why is this a structural result about numbers? Because it's essentially saying that the even numbers can be represented as, um, the subset of the set of primes. That's not quite true, of course.
00:02:47.384 - 00:03:48.682, Speaker A: I see chantal with a shocked look on her face, because two is a prime. And so you can get some odd numbers in the left hand, in the right hand set, but it is approximately true or should be true. Okay, what we can prove is a weaker version of this statement, namely that every odd integer greater than five can be written as a sum of three primes. Now, that happens to be easier because you have one more degree of freedom. So, in this, as stated, this was actually proved by Harold Helfcott in 2013. So he was a member in the program at MSRI and a speaker in our first workshop on expanders in our program in January. And there's a lot of work that came before, so he improved on that sort of longstanding method, but really close this conjecture for good.
00:03:48.682 - 00:04:05.554, Speaker A: Now, I wanted to tell you about a second problem. So here you see some numbers. 1113. So I didn't make these up. So some of these are not prime. It's not my fault. 269 and 271.
00:04:05.554 - 00:04:51.624, Speaker A: So these are so called twin prime pairs. So, both of these numbers are prime, and they're a distance two apart. And many of you will be familiar with the following famous conjecture, which is a twin prime conjecture, which says that there are infinitely many primes, p, such that p plus two is also prime. Again, I have some examples for you. So. 1113, 41, 43, and possibly a few others that you can see on the left. And mathematically, what you'd like to prove is that the limit for pn plus one, which is the n plus first prime, minus pn, is at most, which is the nth prime, is at most two.
00:04:51.624 - 00:05:41.414, Speaker A: The reason I've stated it like this is because I, on the next slide, want to show you some recent progress on this. So this is work on bounded gaps in primes. So, there's another major breakthrough. In 2013, Yitang Zhang showed that, in fact, this lymph is bounded. So he couldn't show that it was bounded by two, but it is bounded. This was a huge breakthrough, because previously we only had a bound that was growing as a function event on this quantity here. Now, shortly afterwards, James Maynard and Terence Tao independently found a different approach, an approach that's different from Zhang's.
00:05:41.414 - 00:06:20.548, Speaker A: And in fact, that gives a bound of about 246 on this quantity. So it's a very explicit number. There was some involvement of a polymath project as well, in getting that number down to something very reasonable. Um, so 246 is not two, but, uh, it's much closer to two than a function tending to infinity. Um, this is a picture of James Maynard. He is the, um, uh, co organizer of our joint MSRI pseudo randomness, um, seminar, and I don't see him right now. Excellent.
00:06:20.548 - 00:07:06.068, Speaker A: I want to tell you about a third problem, which is to do with a third kind of structure that you might want to look for in the primes, and that's an arithmetic progression. So, as was mentioned by Dick in the introduction, a very famous result in our area of research is a so called green tower theorem. So these are Ben Green and Terrence Howe. This is actually a picture taken in Berkeley in 2008, when we were all attending a program at MSRI. This was on the roof of the building that I lived in at the time. It was shortly after they proved their celebrated theorem. And that just states that there are arbitrarily long arithmetic progression in the prime.
00:07:06.068 - 00:07:45.006, Speaker A: So they proved this around 2004, but the conjecture goes back to at least 1770. So I told you I was going to talk about some pretty old problems. I think this counted one. In fact, they proved something stronger than this. Not just do the primes themselves contain arbitrarily long arithmetic progressions, but any sufficiently dense subset of the primes contains such progressions. And there's a lot of work that followed this breakthrough result. There are different kinds of arithmetic structures that we can find in the primes.
00:07:45.006 - 00:08:19.030, Speaker A: Now, we can also count these types of configurations very exactly, so we can give asymptotics for all of these structures. There's a lot of work that came out of this development. So again, I wanted to show you an example. So, 713 and 19, that's a three term arithmetic progression in primes. And here's a slightly larger one. So that is a progression of length 24, which is a number. I don't even know what that number is, but it's very large.
00:08:19.030 - 00:09:14.272, Speaker A: It was found in 2007 by Roblevsky, and some longer ones are known. But when you do want to find arbitrarily long progressions. So if you want to find an arithmetic progression of length, say 50 or 60, you need to go pretty far out to make that happen. Okay, what do all of these problems have in common? So, at least superficially, they are all about primes, and about additive relations between primes. And the reason these problems are so hard, and why many of these conjectures remained open for such a long time, is that the primes are fundamentally multiplicative objects. They're the multiplicative atoms of our universe of natures. Right? Every integer is made up of prime factors by multiplication, whereas the structures that we study here, additive structures.
00:09:14.272 - 00:10:02.744, Speaker A: So we look at twin primes. That's a prime and a prime plus two. Writing an even integer is a sum of two primes. And in the case of arithmetic progressions, looking for configurations of the form x, x plus d, x plus two, d x plus 3d, all of these are additive configurations. Okay, now, um, a lot of progress has relied on understanding exactly how, um, random, pseudo random, the primes really are. So I've printed the first, uh, well, the primes up to, I don't know, 86 for you here, or rather the indicator function of the primes up to 86. So, um, takes the value one when the number is a prime and zero otherwise.
00:10:02.744 - 00:11:15.614, Speaker A: And you can see they look fairly well distributed. Looks like they get a bit sparse as you go along, but you don't see any obvious kind of structure here. So the way we think about primes is often using probabilistic models. So there's one particular model that's been very successful in making all kinds of predictions about the types of configurations we looked at on the first few slides, and that's the Cramer model basically says the following in a very naive form. So the asymptotic statistics for the set of primes p should roughly equal the asymptotic statistics of the random set p prime, where an integer n is chosen to lie in p prime with probability one over log n. Now the one over log n comes from the prime number theorem. So we know that in the interval one up to n, there are n over log n primes.
00:11:15.614 - 00:12:08.672, Speaker A: So this is the frequency with which you expect a prime. And you can naively try and model the set of primes by just choosing an integer to be a prime with probability one over log n. Okay, now this doesn't, it gives you quite a lot of things. It doesn't quite work. So for example, you could try and predict the number of asymptotics of the number of twin primes that we'd expect to have asymptotically. Using this heuristic, it gives you the right prediction, but it also gives you the prediction for the number of pairs p p plus one that are both prime. And that's obviously nonsense because there are no primes that are just prime pairs that are one apart.
00:12:08.672 - 00:12:46.654, Speaker A: The primes other than two are even. So you have to adjust this slightly. And the way you do that is by taking into account the bias that the prime have with respect to small modulo modular classes. So for example, I just said that all the primes above two are even. So that means that you're not going to find any primes in the residue class zero mod two. It's also true that no prime is a multiple of three. So you're not going to find any primes in the residue class zero mod three.
00:12:46.654 - 00:13:54.684, Speaker A: And you can exclude a lot of the natural bias that the primes have by adjusting your model, your random model in this way, by basically taking care of this bias of the primes with respect to the small residue classes. And that gives you our best predictions current predictions that do coincide with the results we've obtained so far on the problems that I sketched on the first few slides. Um, there are other random models of the primes. For example, you can use random matrices, um, to model statistics of the primes. Um, these are more appropriate when you want to describe multiplicative behavior of primes. Um, and they're certainly not the subject of today's talks. This kind of random, um, model, or this notion of pseudo randomness, um, largely applies to the first two problems I talked about.
00:13:54.684 - 00:14:44.504, Speaker A: So these are things you're interested in studying when you're interested in Goldbach's conjecture and in the twin prime conjecture. They're less relevant for studying the third type of problem that I described, namely arithmetic progressions in primes. In fact, we believe that the green tau theorem is true independently of the fact that we're looking for structure in the set of primes. Right. The Greentow theorem shouldn't really be a result about primes at all. I'll explain to you the sense in which we believe that that's true. In fact, we believe, and we know, that any sufficiently dense subset of the integers contains arbitrarily long arithmetic progression.
00:14:44.504 - 00:15:36.750, Speaker A: That's Semmradius theorem from 1975. Unfortunately, it's not quite good enough to give you the prime result. I'll show that to you in a second. 1st, I'll show you a picture of Andrei Sammerelli at the place where he I think this is the Arbel prize ceremony. Okay? Now more precisely, we know that if a is a subset of the first integers and larger than n over, let's say log log n to some small power. For some small power c, then a contains an arithmetic progression of length k. Yeah, think of k equals 2100, whatever you like, some fixed number, okay? And this small constant c depends on the k.
00:15:36.750 - 00:16:08.474, Speaker A: Now that's the result that's due to Tim Gowers. Um, and in fact, uh, you may notice that I've, uh, clipped both of these photos out of a larger one. Um, and the person in the middle may be familiar to many of you, too. That's Amy Victorsen. Um, so I could fill an entire lecture course, oops, that went wrong. Um, talking about this result and the, the methods that go into it. What I, I'm not going to do that.
00:16:08.474 - 00:17:18.378, Speaker A: I will only point out to you that the primes have density one over log n, and in particular that's smaller than this number here at the moment. So if you wanted to just apply some radius theorem to the primes unfortunately, at our current level of understanding, you cannot apply to the primes because the primes are too sparse. However, we do believe that this is not the optimal bound. In fact, we have a lower bound. So, a construction of a large set that does not contain any k term arithmetic progressions, that's roughly of density e to the minus c log n, sort of one log case root of log n, which is. Yes, which is a long way away from this number here. In fact, even in the very simplest case, where k equals three.
00:17:18.378 - 00:18:05.874, Speaker A: So, if we're just looking to find a three term arithmetic regression, there is still a gap between the upper and the lower bound for this problem. So, the current best known upper bound is due to Tom Bloom, who is a Simons fellow at the moment. And he's not here based on work of Tom Sanders, who's also currently a visitor on the program. And this lower bound is an old one. But for k equals three, there's some progress of Elkin and Ben Green and myself. Okay, now, I said we'd forget about the primes altogether. And in fact, I want to simplify the problem.
00:18:05.874 - 00:18:53.554, Speaker A: I said we forget about the primes altogether and we concentrate on the case k equals three. So we're just looking for three term arithmetic progression. And I'm going to now simplify the problem even further. Forget about the integers. We're going to move to a toy setting, which especially computer scientists will be very familiar with. So we're looking at a vector space over a finite field, and I'll explain to you what the cat set problem is. Okay, so the question is, how large does a subset a of f three to the n have to be before it is guaranteed to contain a three term arithmetic progression? Right? So if you replace f three to the n with the integers, then you have a sort of k plus three k, sub Roth theorem.
00:18:53.554 - 00:19:54.784, Speaker A: Why is this called a cap set problem? Well, a cap set, for some historic reason, is a set that contains no three ap's. So the cap set problem asks, what is the maximal size of a cap set? Now, why are we looking at f three to the n? I know computer scientists prefer f two to the n. I can't really make sense of a three term progression in f two to the n because there's only zero and one. There are versions of this problem that you can formulate in f two circumflex n, but the real analog is in f three circumflex n. And the reason I want to talk about a three term progression rather than a configuration of the form x plus y equals z, which you could have in f two circumflex n, is that I want the configuration to be translation invariant. Now, actually, if you think about it, in f three circumflex n, a three term progression is actually just three points on a line. So it's really a very simple object.
00:19:54.784 - 00:20:44.978, Speaker A: Okay, now, how large can a cap set be? I can certainly very easily construct for you a cap set of size two to the n. I simply take all the vectors in f circumflex n, whose coordinates are only zero and one. There are two to the n of them. And that set certainly doesn't contain three points on a line, because once I've taken a step in one direction, I have to take another step and then I'm out of my set. So there's a trivial lower bound of two to the n on the maximal size of a cap set. In fact, it's been known for over a decade, and this is a result of Erle, that there are sets, cap sets, that are of size 2.2174 to the answer, slightly better.
00:20:44.978 - 00:21:38.384, Speaker A: So, relatively complicated construction. And the number is a number that you can generate, and we've also known for about that long, that a capsaic can have size at most a constant times three to the n divided by n. So let me decode this for you. Three to the n is the total size of my space. Um, so the density of this set is, is a constant divided by n, so tends to zero as n goes to infinity. Um, so these cap sets certainly have vanishingly small density. And I will tell you a little bit about how you prove this particular result, um, which is due to Mechelem from 1996.
00:21:38.384 - 00:22:47.874, Speaker A: The reason I want to tell you about this is not so much because I believe that this problem is hugely important. What is important is the tools that go into addressing this problem that have been developed to improve the bounds on this problem, because they've been used in many other instances across mathematics and computer science. Okay, so what are these tools? So this is the second way in which we use the notion of pseudo randomness in this branch of mathematics. So the first way, remember, was to say that I use the idea of pseudorandomness to model the behavior of the primes, right? So I'm going to do something quite different here. So let's think about what would we expect of a truly random set in f three to the n. So if my subset a of f three to the n was truly a random set, let's say we choose the elements of a independently, at random, with probability alpha. So the expected size of our set is alpha times three to the n.
00:22:47.874 - 00:24:14.224, Speaker A: How many three term arithmetic progressions would it contain with high probability? Well, how many three term progressions are there in total? So I can choose the first point, that's three to the n choices, and the second point, which is three to the n. Again, so I have three to the n squared, many three term progressions, and how many of these lie in the set a? If I've chosen it independently, at random, with probability alpha, I'll have alpha cubed three to the n squared many three ap's, where alpha is the density of my set. And what's really surprising about this, or perhaps it's slightly counter intuitive, is that, you know, in this random set, I know nothing about its structure, and yet I can count the number of three ap's it has with almost complete accuracy. Okay, so we'd like to have a notion of pseudo randomness that replicates this behavior, and it applies to a larger range of sets than just the truly random ones. So what will work for us is we'll say that a subset a is pseudo random if its indicator function has vanishingly small Fourier transform at non trivial frequencies. So I'll tell you what I mean by their Fourier transform in this context. This is an awful formula.
00:24:14.224 - 00:25:04.844, Speaker A: First and last time, I'll use PowerPoint presentation for a maths talk. So, the indicator function one, sub a, is the function that takes the value one when x is in a and zero. Otherwise, the Fourier transform at any point t and f three to the n is just the sum over all elements x and f to the n. The interfutative function times the third root of unity x dot t, where x dot t is the scalar product in f three to the n. So x dot t here can take three different values, mod 30, one or two, and this sum, you should think of it as normalized for everything that I'm going to say now. Otherwise it won't make much sense. So I divide it by the size of the group, which is three to the n.
00:25:04.844 - 00:25:50.414, Speaker A: Okay, now, the first thing we observe about this Fourier transform is that at zero, so the trivial furrow coefficient, which I can just plug into the formula, actually find I have the normalized sum over the indicator function at x, and that's actually just the size of a divided by the size of the group. So that gives me the density of the set. And it's also not difficult to see, just intuitively, that this is the largest value that the Fourier coefficient can ever take. Right. Because there is no cancellation at all from the complex phase. So all of your Fourier coefficients of this indicator function are going to be an absolute value at most alpha, the density of the set. Okay.
00:25:50.414 - 00:26:37.908, Speaker A: And I'll call this set pseudorandom, if, in fact, all of the non trivial Fourier coefficients are really a lot smaller. So I actually want them to go to zero as the dimension of my space, little n goes to infinity. Um, okay. And I claim that if a set a is pseudorandom in this sense, that its indicator function has vanishingly small Fourier transform at non trivial frequencies, then a, it does actually have the same number of three ap's as a random set. And that's really good news. Right. So this is a notion of pseudo randomness that we want to use, um, um, when we're interested in counting three ap's, because the now pseudo random sets actually behave like random sets.
00:26:37.908 - 00:27:07.644, Speaker A: Right. And the reason for this, the proof of this is really, really easy. I'm not going to give it to you, but it essentially relies on the fact that I can rewrite the number of three ap's in a as a sum over these Fourier coefficients. So the sum over t one, sub a hat of t cubed. There's some complex conjugate signs missing, but this is roughly what it is. And this is a very nice expression to work with. And you can see that the trivial Fourier coefficient contributes an alpha cubed here, which is the main term.
00:27:07.644 - 00:27:50.368, Speaker A: And if the remaining coefficients are small, then, you know, they don't contribute anything much. And indeed, a random set, it's not hard to see, is actually pseudo random in this sense. So that's, that's good news. But more importantly, there are other, there's, there's a larger class, class of sets that are pseudo random in this sense, but are not truly random. So, for example, there are some quadratic varieties that satisfy this. And this is very important. Okay, now what you do with this, um, you can iterate this, um, or use this as a basis for an iteration, um, to get the, the upper bound that I stated for you before, namely that the cap set.
00:27:50.368 - 00:28:32.284, Speaker A: So a set with no three ap's in f three to the n is bounded above by a constant times three to the n divided by n. Okay, so this is how this goes. Um, so either my set a is pseudorandom, in which case, actually, we just saw that it has lots of these three term progressions, namely alpha cubed, three to the n squared. But hey, that can't be right, because I'm actually assuming that my set has no three ap's at all, so therefore it can't be pseudo random. And what does that mean just by definition? It means it must have a large Fourier coefficient. Um, it must have at least one large Fourier coefficient. So let's look at this Fourier coefficient again.
00:28:32.284 - 00:29:52.324, Speaker A: Um, what was it? It was a sum of these ones or zeros, um, multiplied by this complex phase, right? But this complex phase can only take three values, um, depending on the residue class of x dot t, right? T is fixed. I've got one large Fourier coefficient. Um, so if this coefficient is large for some non zero t, that means that one of these three residue classes, zero, one or two mod three must get more than its fair share of hits from a because otherwise you'd get complete cancellation in this sum. What does it mean that one of the three residue classes of x dot t gets more than its fair share of contributions from a? That means that a actually is biased towards one of the, these subspaces defined by x dot t equals zero. Okay? And that tells me that a has sort of very large density, or larger density than on average on this subspace of co dimension one, which is determined by the orthogonal complement of t. Okay, so that's great. So I have a set now that is a bit denser on a portion of the space.
00:29:52.324 - 00:30:52.884, Speaker A: It still has no three ap's. And I can just run this argument again, and I do this until I must stop because I run out of dimension or because the density of the set has got so big that it's actually bigger than one. And you trade these two things off against each other quantitatively, and this is what you get. So the reason I tell you about this in quite a bit of detail is because really, all of the solutions to these seminarity type problems on arithmetic progressions follow more or less this blueprint. There is a notion of pseudo randomness that tells you if your set is pseudo random. Actually, I can count the number of progressions or the number of configurations that I like to count pretty exactly. Um, and then you have to conclude something about, you know, when my, when your set is not pseudo random and you expect to get some structural result, and you can kind of code this up, uh, more compactly, uh, in the sort of following statement.
00:30:52.884 - 00:32:27.366, Speaker A: So really what we've done is we've sort of written the characteristic function of a as a sum g plus h, where my g is structured in the sense that it is constant and large subspaces of f three to the n, and the function h is a pseudo random error, and the pseudorandomness in this case being defined as the function h having vanishingly small Fourier coefficients. So what matters for counting three ap's is just a function g. Okay, and I can rewrite this yet again to say, really, our aim, given a set a, our aim is to find a function g which approximates one sub a up to a pseudo random error. And this is a very, very general statement that is ubiquitous in our area of research. And I'll give you a couple of examples of how this can be extended. So, the first application of this type of approximation statement is it actually can give you much stronger information about the large Fourier spectrum of a set. So, you know from Parseval's theorem that a set can't have too many large Fourier coefficients, but actually you can have a much stronger information about their algebraic structure.
00:32:27.366 - 00:33:36.368, Speaker A: And that's very helpful in many contexts. These sort of statements, as I've already alluded to, can also be obtained for other notions of pseudo randomness. So, in particular, when you start to look at progressions that are of length greater than three, you need to define other notions of pseudo randomness. In particular, we work with the so called Gauss uniformity norms. So you want approximations of this type where your function h is pseudo random in a more sophisticated sense. And that leads to the whole subject of higher order Fourier analysis, which I have worked on quite a, a bit, and a number of applications to computer science, including pseudo random generators, etcetera. And finally, you might want to obtain, in a number of contexts, including the Green Tower theorem, you want to obtain these kinds of statements, approximation statements, not just for the indicator function of this dense set, one sub a, but also for unbounded functions.
00:33:36.368 - 00:34:59.346, Speaker A: So, for the Fernmangold function, for example, which is unbounded, and which kind of subs as the indicator function for the primes. So, the fact that this function is unbounded is an indication of the set of primes being very sparse, because you have to re normalize it. So if you want to prove something like the Green tower theorem, you will have to approximate an unbounded function in this way. In computer science, that's a statement that's called a dense model theorem, which can also be used to prove impagliazzo's hardcore set, lemma, and various other types of weak regularity type statements. And in particular, I wanted to point out to people in the machine learning program that, um, techniques from machine learning, in particular, boosting algorithms, have been used to obtain these kinds of approximations. So, um, so various notions of pseudo randomness. Okay, now most importantly, from a, from a number theoretic point of view, um, you can obtain these kinds of statements, not just in f three to the n, which is a group that we're interested in because it's kind of nice and simple and it has these subspaces, et cetera.
00:34:59.346 - 00:35:45.254, Speaker A: But you can also prove these statements in the integers. And there is sort of, there's a mechanism by which you translate these results from this setting of f three to the n to the integers. It's very technical, but it can be done. Now, I have a, I do have twelve more minutes before we get to the final part of the talk. I have a brief diversion because I did mention a card game in the abstract for the talk. So I thought I should briefly talk about that in case somebody was expecting me to. Okay, so there is a card game which some of you may have played.
00:35:45.254 - 00:36:06.464, Speaker A: I see some people nodding here. Unfortunately I don't have it with me because it's in my office in the UK. But it is actually a really good game. You do need to be awake though. Okay, it's a very simple game. It's played with a deck of 81 cards. You notice that 81 is a power of three, three to the four in particular.
00:36:06.464 - 00:36:39.544, Speaker A: So each of these cards displays a number, one, two or three of shapes. Actually, I'll show you a picture. There we go. Diamond, squiggle or oval, in one of three colors, green, red or purple and filled with some kind of shading. So solid, empty, or you can't see this very well, but it's striped. Okay, so that makes 81 different cards. And the combinations of three cards is very confusingly called a set.
00:36:39.544 - 00:37:12.764, Speaker A: If for each of these four attributes, all three cards are either the same or they're all different. So I'll give you some examples. Okay, this is a set because they're all purple, they're all solid, they're all squiggles. And you have the numbers one, two and three. I'll give you another example. This is a set because they're all solid. You've got red, green, purple, squiggle, diamond, oval, and there's something I'm missing.
00:37:12.764 - 00:37:56.444, Speaker A: Oh, and two, one, three. Okay, this is not a set because the three occurs twice. Can anyone find a set? Yes. Very good. Okay, so I think many of you will have spotted that you can associate to each card a vector in f, three to the four. And so in fact, a set is just a line in f, three to the four. So it's three points on a line.
00:37:56.444 - 00:39:08.614, Speaker A: And the way this game is actually played is that the dealer puts, I think, twelve cards on the table. Table, and then people start shouting out set if they spot one. The point is, you're not guaranteed a set in any set of twelve cards that's on the table. The question is, how many cards do you need to have on the table before somebody is guaranteed a set? So, in this particular game. So in four dimensions, the answer is 20. And the capsaic problem is just the n dimensional version of this question. Okay, now that I've spent the first half an hour telling you how we use these pseudo random methods and sort of this analytic machinery to solve card games, the real point of this talk is to actually tell you about a totally non random breakthrough from last year, which has really called a lot of this methodology into question.
00:39:08.614 - 00:40:18.034, Speaker A: So, Krut, Levin, Pach surprised the additive combinatorial community in May 2016 with a very short preprint, um, that addressed the, the capsaid problem, not in f three to the n, but in z mod four to the n, which is slightly different. Um, but eventually their, their method led to a polynomial bound on the size of. So these are crude Laffenpaul. Sorry, to a polynomial bound on the sides of a cap set, which is of form 2.27,536 to the n. Um, and this was done by, um, Ellenberg and Heisweit, but it heavily relied on, on the initial ideas of Krutlev and Pach, which initially looked like they really were very specific to z mod four, um, z. But, uh, Allen, Berg and Heisweit were able to adapt them to, to f three to the n.
00:40:18.034 - 00:40:56.338, Speaker A: Now, the amazing thing about this preprint is that it's very short, and the method is really quite elementary. So I will try and give you a sketch in the last five minutes of this talk of how this goes. Remember that the previous bound that I sketched to you obtained using analytic method, was of the form three to the n divided by n. So this is really a huge breakthrough. In particular, it almost looks like the lower bound, which was of the form 2.274. I don't remember, but the numbers are. I mean, they're not identical, but this looks much more like we're in the right ballpark.
00:40:56.338 - 00:41:29.744, Speaker A: Okay. And I said that the method of proof is quite elementary, so it relies on an idea that's actually quite old and has been used repeatedly in combinatorials over the past 30, 40 years. And that's the following. You want to find a low degree polynomial. Pull. Um, which vanishes on, on the set a that you're studying. Right? So we all know that, um, uh, a polynomial of degree d can have at most d root.
00:41:29.744 - 00:41:58.086, Speaker A: So you can adapt that to various fields. Um, that statement. Um, and so it means that if you find something that's relatively small, a polynomial of relatively small degree, and it vanishes on a, then a can't be too large. Right. It's a pretty simple approach. Um, now, of course, it's not at all clear how you actually find this polynomial. And in particular, something about this polynomial has to reflect, um, the structure of your problem.
00:41:58.086 - 00:42:31.734, Speaker A: So somehow you have to take advantage of the fact that, um, your set a contains no three term progressions. Right. Um, and in practice, actually, this polynomial method is quite far from the way that the polynomial method has been applied, had previously been applied in combinatorics. And in particular, it uses a new notion of so called slice rank, which I'll explain to you on the next slide. Okay, so I have one technical slide. Okay. This is a statement that the entire proof relies on.
00:42:31.734 - 00:43:13.272, Speaker A: So suppose I have a polynomial p, which is such that p of a plus b is zero for all distinct a and b and a. And then I claim that there aren't too many elements of a for which p of minus a is non zero. And I could tell you exactly. In fact, you'll see in the proof what this means. It's not too large. Okay? So we will prove this, at least in outline. So take any polynomial, p or b, any polynomial and n variables of total degree at most d.
00:43:13.272 - 00:44:04.624, Speaker A: And we will fix for now d is an arbitrary degree, and we will fix it much later in the proof. Um, and we want to write p of x plus y as follows. So I'll write it as a sum. Well, a linear combination of monomials in x and y. Um, these monomials are indicated by m and m prime with some coefficients. And the point is that m times m prime has degree at most d, because we said p was a polynomial of degree at most, um, uh, d. But that means looking at any single one of these, um, products, um, at least one of m and m prime has to have degree at most d over two.
00:44:04.624 - 00:45:03.624, Speaker A: Right? So I can actually break this down further into a sum over monomials in x times some polynomial in Y, which depends on the monomial plus a sum of over monomials in Y times some polynomial in X, where these sums are over monomials of degree at most D over two. Right? Because I'm either here or there. Okay, now I want us to consider the matrix P of a plus b for little a and little b and a. So I claim this is a matrix of size, a size of a times size of a. So that's pretty obvious. I think we're good here. We're also assuming that P of a plus b is zero for all distinct a and b and a.
00:45:03.624 - 00:45:34.804, Speaker A: So it's a diagonal matrix, right? It's a very nice matrix. And what about the diagonal entries? The diagonal entries are of the form p of little a plus little a. So two a in f three, that's minus a. So those are exactly the entries we're interested in counting. So we want to count how many non zero diagonal entries there are. Okay. That actually just means we want to give an upper bound on the rank of this matrix.
00:45:34.804 - 00:46:19.064, Speaker A: Okay? And now you see why I've decomposed it in this way. Because p of x plus y is a linear, well, is a sum, well, two sums, but of matrices, each of which has rank one. So m of x times fm of y. You see that all of the rows are multiples of each other, and the same goes for here. So I have a sum of matrices of rank one. And so the rank of this matrix is at most the sum of these ranks. And how many elements are there in each of these sums? But that's a number of monomials of degree at most d over two.
00:46:19.064 - 00:47:31.234, Speaker A: Um, and we have, because we have two sums, so we have that the number of a and a for which p of minus a is not zero, is at most twice the number of monomials of degree at most, um, d over two. And what's that got to do, um, with the cap set, you ask? Well, the crucial observation is that the set a plus a, which I, so there's something missing in the middle. So that's the diagonal elements. So I want the sums little a and little b. I want a and b to be distinct in this sum set and minus a a disjoint, right? Because if they had an intersection, I'd have a three ap and namely a solution to the equation a plus b equals minus, or a plus b equals two c. So the disjoint, what we're going to do not in any detail, is choose a polynomial p with maximal support from all polynomials of degree d that are supported on minus a. Why do I want my polynomial to be supported on minus a? Because then I know that it's going to be zero on this set here.
00:47:31.234 - 00:48:34.316, Speaker A: Right? And then I can apply my lemma. Simple counting tells you that a supportive p is bounded below in terms of DNA, and the lemma gives you an upper bound on the support of p, and you play these two off against each other and you choose d in an appropriate way. And that's it. Now, what's really important about this? And you hopefully see this from the proof that this is a completely different approach to the iteration argument that I sketched to you in the first half of the talk. Um, and I'll conclude with, uh, some remarks and open problems. So the first one is very obvious. Um, now, is this upper bound actually tight? Um, which is correct, the upper or the lower bound we have at the moment? Um, it turns out that, um, this, um, polynomial method also solves a various related problems.
00:48:34.316 - 00:49:25.044, Speaker A: And for some of these problems, the bound resulting from this method has been shown to be tight. So in particular, this is the multicolored sum free set problem. But we don't know what the answer is to this question at the moment. Does this approach generalize to kind of counting or finding bounds on sets so don't contain any longer arithmetic progressions? So with the analytic approach, there's a huge increase in difficulty going from three to progressions to four to five. It doesn't obviously work. So people have tried this. Can you apply this in the original setting, the integers, the primes, we don't know how to do that.
00:49:25.044 - 00:50:57.384, Speaker A: What other applications does it have? So there have been, as I already mentioned, this has been applied to a few variants of this type of question, and indeed some that look less obvious. But it's also been shown to rule out approaches to improving the exponent for matrix multiplication, um, that had been, um, devised by, um, Conan Umans, um, so in their group theoretic framework, um, but the philosophical question I want to leave you with, really, have we been kind of barking up the completely wrong tree for about half a century, um, trying to refine our analytic methods, um, to get better results, if a simple polynomial proof can give it to you in a page and a half. So thank you very much for your attention. Other questions? I'll take questions over cookie, questions over cookies, if there are any left. Yes, please enjoy the cookies and participate in the questions.
