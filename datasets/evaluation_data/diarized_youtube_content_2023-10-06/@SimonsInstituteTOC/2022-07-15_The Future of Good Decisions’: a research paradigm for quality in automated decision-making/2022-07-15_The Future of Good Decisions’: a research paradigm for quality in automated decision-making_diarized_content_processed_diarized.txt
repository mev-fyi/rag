00:00:00.160 - 00:00:18.846, Speaker A: There we go. Thanks very much. It's really great to be here. And with the addition of all these lovely people at this workshop. And thanks very much to Ben for that excellent talk. Extremely clear. And I'm going to piggyback basically straight off the back of it.
00:00:18.846 - 00:00:54.942, Speaker A: This is a really unusual talk, and it's the opposite of Ben's talk, which presented some fantastic research with conclusions. I'm presenting work that it's not even in progress. It hasn't started yet. It's a brand new project, and the news about this project has been embargoed until recently, so I've literally never spoken about it. This is really exciting and big for me. So I'm a former lawyer come critical legal theorist, working interdisciplinarily on the question of what the human is for law, more or less. And obviously I realized I had to address the question of how the technological upheavals that we're facing interact with that question.
00:00:54.942 - 00:01:20.644, Speaker A: And for some time I've been teaching around some of these questions at the University of Kent, and that led to the funding proposal for this project, which has just been funded quite amazingly. It's a seven year fellowship, so I'll be working for four years with a budget of about 1.25 million pounds, which is about 1.5 right now. So that's a euro. That's the euro pounds. Worth a bit more still anyway.
00:01:20.644 - 00:02:01.290, Speaker A: And then with a separate budget for the last three years. The reason I'm being particular about those things is because part of the point of this talk is to create a potential for conversations and involvement in the project. And so I'll talk about some practical things as we go. What I want to do is three things. Reframe the challenge of ADM in administrative decisions, which is the focus of my project. I want to talk about my research design for working on administrative decision quality, in other words, what we should be doing or how we should be working on these problems, according to me, and then finally linger at the end on this research. Use of live action role play as a really important methodological key for my work.
00:02:01.290 - 00:02:57.316, Speaker A: And that, I think, is a great platform for collaborative work on these problems. So right before I submitted my grant proposal, I published a piece called automating the human and meaningful human control autonomous weapons. I'm going to skip through this pretty quick because Ben's covered a lot of the salient points, and some other discussions have, too. I concluded that in international law, this emerging norm of meaningful human control offers us a human that is not a legal concept, the human has no legal status at all. Instead, human is a floating signifier with huge symbolic capital that's co opted for political ends most of the time. In this specific context, I suggest that the human is offered to us as an a priori, non technological, reflective, autonomous agent with a legitimizing or stabilizing function. And my conclusions were that this emergence or this positivization in law of this figure of the human, at least in this setting, it has three main qualities.
00:02:57.316 - 00:03:41.460, Speaker A: First, it's fundamentally contradictory and historically significant, because what it actually does is legitimize a new techno economic political arrangement while in fact diminishing control in the name of the human. I suggest it's dangerous. It ignores risks and problems that Ben has talked about. And it's also negligent. And this is my key point, it's negligent because I think governance should do better than to simply offer this lazy figure of the human. And that led me to think that scholarship should also be doing better. We should actually be working harder on this question of can this figure of the human serve this function in a meaningful way, or is it just eviscerating our governance processes? And so my project is an attempt to shift that problematic to the administrative decision context, which, as Ben has said, is really important.
00:03:41.460 - 00:04:23.628, Speaker A: And I'll skip through this very briefly. I just want to point out a key dynamic here. So over here we've got some basics about the context which we've heard a lot about through this meeting. But I wanted to point out that how insignificant it is that scholars know that we will not really fix these problems just by sort of tweaking transparency settings or something like this is not how we're going to resolve this problem. And the scholarship is absolutely full of people using the language of epoch. Epoch will change, radical ontological difference, these kind of massive claims, and yet governments, and here there's of course, lots of legislative responses which do things in all kinds of contexts. But my interest has been in what administrative governance is doing about this problem.
00:04:23.628 - 00:05:27.584, Speaker A: And actually, these are some of the techniques that are being used here on the right. They're all good. I'm not saying they're not good, but I think they do not begin to address the problems that scholarship has addressed. And so my project frames that sort of question of like, what would a coherent response to this problem look like? I think it's significant that meanwhile, outside of the legal setting or the administrative government setting, people are talking about this. They're saying humanity is being reconfigured like our fundamental presuppositions about how we work and who we are, what is good, what languages we should be using. People know this, and yet we don't see that level of epochal change shifting through into administrative law and administrative practice, which obviously has been said, as I put up on this previous slide, affects all of us in each of our lives. Every visa application, tax determination is an example of a setting where our lives can be directly affected by these techniques.
00:05:27.584 - 00:06:08.432, Speaker A: So what I find really interesting, and what I mentioned in my automating authority paper is why is that? That not only philosophers of technology and ethicists, and it's not only these people who see this problem. It's lawyers, too. Lawyers know. A judge who's working with me on this project has published an article so desperate was he to go on record saying how deeply problematic and dangerous this situation is. And yet there's something about the kind of legal discourse that constrains the sort of response that we can offer. Not only that, but I think administrative law is uniquely constrained. It's a very high stakes field.
00:06:08.432 - 00:06:28.944, Speaker A: Every decision directly affects somebody, so we can't experiment very much with it. It's deceptively important to political health as a stabilizing kind of function. Administrative decisions have been the site of intense work, historically speaking, at technical, historical, imaginary, philosophical, theoretical, and practical ethical levels.
00:06:33.564 - 00:06:40.668, Speaker B: What you are referring to as administrative law and losing us in your talk.
00:06:40.756 - 00:07:21.614, Speaker A: Okay, if you could just quickly say, all I mean by administrative law is the law that regulates the kinds of decisions that we've been talking about. That's all we need to know. So the law that says how they should be taken, which is the law of good decisions at the moment? Well, it's not even law. It's policy about what good decisions look like, how we should take decisions, and then the law that governs, how we can review those when they're turned to be out of line. And so that's where I have to go off with that left column, which talks about the fundament. We really don't need to know any administrative law to know what I'm working on. What we do need to know is that the key principles of that administrative law are the ones that people acknowledge are in deep fundamental conflict with the way that AI and machine learning fundamentally work.
00:07:21.614 - 00:08:52.104, Speaker A: But my point is that the administrative state has been a key site of human political endeavor to institute a set of values and norms that have been worked on in very intense and very deliberate and very overt ways. And what I see now is that that sort of work is not being done. We're not adequately taking account of the way in which we need to rethink these processes and their fundamental normative foundations. So what my project does is lightly but fairly radically reframe the question of how we can evaluate the way in which decisions are understood as being taken by humans in government, with or without the involvement of machine learning. So what I've done is displaced the tasks to a different kind of level of analysis. It's not that radical, to the extent that all I'm doing is saying we need a new vision and practice of good decisions for a new techno social media ecology. So the public lawyers in the room, which is maybe one or two, will know that good decisions is also itself a kind of technical language, and that has been addressed in a certain way, typically through very human terms, of what kinds of things you have to look at, what kinds of reasoning, how carefully you need to look, don't exceed your power, all these kinds of practical guides, but interestingly, these have not been updated, despite this raft of other kinds of legislation that have emerged with the rise of machine learning.
00:08:52.104 - 00:10:23.538, Speaker A: So my proposal is that what we need to do, given the sort of ontological work and identification that the people we've just been looking at as sort of with this radical diagnosis are talking about, is we need to take more seriously the evolution, not just of law, which is something people talk about, how is law evolving? Or how is legal reasoning evolving because of the input of these kinds of new modes of reasoning through machine learning, but rather to think about the whole technical milieu, our whole life world. And not only that, but to take seriously how evaluative principles belong with a particular media world or life world. So that's extraordinarily difficult to do, however, and of course, there's no shortage of theorization about these questions. There's no shortage of sort of new norms or new rights or new kinds of principles that people are proposing. And there is also some practical change. But what my project suggests is that we need a coordinated human engineering effort to turn three keys at once, so that there needs to be some sort of solid theorization of this problem of decision making that isn't just somebody's theory concocted from behind their desk, but actually resonates with how a social and techno social world understands what's happening when decisions get made. I think that needs to go hand in hand with a coherent or integrated set of normative principles, or inculcated values that are institutionalized along with compatible practical change.
00:10:23.538 - 00:11:20.278, Speaker A: So concrete processes, protocols and decision support systems. So really this sort of task of turning these three keys, I think is a very difficult one. And the problem that I sweated over in devising my project was to think about how would you do that? I actually ended up scaling back the question to be not how do I do that, but how should people be working in a way that this might happen? Really, that's the question. So how do you work on a problem like this in a deeply holistic way? And the answer I came up with was this research design. And so it's perhaps slightly daunting, but I'll break it down for you just a bit. So I start with a theorization of what decisions are in techno social ecologies, which is not a question that those, that the theoretical frames that are working in this area are particularly looking at. So we're going to do some original work there.
00:11:20.278 - 00:12:09.566, Speaker A: We're going to integrate that with normative theory, thinking about how existing ways of thinking and reason, public doctrines of public reason decision making work, and filter that through concrete questions of doctrine and practice. Those will be tested through this live action role play mechanism through which. Which I'll talk about in a moment. Mutually informing each step of the project, culminating in an empirical study of a real decision making process in a multi sided ethnography. So the full scale how is a decision system conceived, designed, implemented, evaluated, et cetera, culminating in a realistic re envisaging of that decision making process in this new sort of pre figured way, through a live action role play. I'm going to. I had some more information on each of the sub projects.
00:12:09.566 - 00:12:52.710, Speaker A: That's not particularly important, except to note that over here I've got collaborators, and to note that this work is fundamentally collaborative. This is not going to be produced from behind a desk. This is the second project thinking about how we can reconcile human and cybernetic normativities and think about techno social evolution in terms of law. Here we have the discovery of best practices for the administrative design, including Jake Golden Fein and some other names. The only thing I want to note here, you may know some of these people if you work in this field. The only other thing I want to note is I'm working with Supreme Court judges, federal court judges, and have a mutual consultative role with the Law commission in the UK. So this is, the channels are open to make this a reality.
00:12:52.710 - 00:13:30.420, Speaker A: Whatever we come up with after seven years stands a high chance of affecting the way in which reform is being conducted. The law commission is looking at this problem right now and I'm in dialogue with them. So, multi sided ethnography. The only thing I wanted to conclude on here, and I know this is a lot, but I wanted to talk briefly about live action role play. I'm working with specialists from the UK and Italy on live action role play. And these are sort of large collaborative events where we imagine a process for decision making, and we set it up and we do it, we play it. So people, I'm talking about members of the public, academics from across the different disciplines, government employees.
00:13:30.420 - 00:14:15.024, Speaker A: We will be conducting a series of three live action role plays across seven years. And I see this as a really appropriate way to develop new normativities through this kind of iterative process. And I'm working in combination with large public institutions that do this kind of like creative AI, arts AI kind of experimentation, serpentine galleries and their labs. I want to draw attention to the creative AI lab, who we'll be working with, and then a bunch of collaborators from across the whole project, and indeed other people who want to get involved as the project unfolds. All I've got here are just some of the parameters that I know that we will be messing with as we design these systems and play around with them. I think. I mean, that's all I really have to say.
00:14:15.024 - 00:15:53.894, Speaker A: Knowing that this is quite a lot to take in, and of course, knowing that everything kind of depends on what kind of decision you're trying to model, what you're trying to do that will determine, for example, what are the kind of meaningful sites of participatory deliberation coming out of this technosocial philosophies of evolution. It might determine how we think about how decisions are becoming disaggregated in time. So through the work of designers, ends up really being the key site, perhaps, of where decisions, or how decisions are constrained or bounded, as we might say, how we define the loop of the decision that's commonly done, what kind of interface we might need, how and in different, what different ways AI and humans might be interacting through these processes. In conclusion, I would say this is an attempt to take very seriously the question of, given these kind of ontological changes that Peter was talking about earlier, and the kinds of need for contingency and deliberative space that other speakers have been talking about, how can we actually build a process for research that enables us to work in that way that also feeds a potential process for decision making as the administrative state seeks to reform itself in response to this problem, I look forward to discussions now and in the future, if you want. If this is interesting to you and you want to know more or get involved. It's the fear you naturally inculcate that lets you not have to do the hard thing. Yeah.
00:15:57.334 - 00:16:21.234, Speaker B: Right. We'll, we'll follow that usual process. So hands up. And I like Salome's approach of like when. When I'm not. You'll know that. One, two and go ahead.
00:16:21.234 - 00:16:37.304, Speaker B: It's maybe that post lunch.
00:16:38.444 - 00:16:40.864, Speaker A: Thank you. It was there, but when I started talking.
00:16:45.884 - 00:16:46.212, Speaker C: Yeah.
00:16:46.228 - 00:17:34.806, Speaker A: My question is for Connor first, thanks to both of you for really fascinating papers and for Kano, as a, you know, in a past life, I was a theater scholar, so I'm absolutely fascinated by the idea of larping your way to better legislation in some sense. So I'm wondering if you could talk a little bit more about that because, you know, usually trying to simulate the impact of real life decisions by reenacting the process of a decision leads to different outcomes than the actual decisions. Right. Because the circumstances are different and people are very, you know, humans can very well differentiate between something that's simulated and something that's. That's real. Right. So what are your thoughts about how to implement that practically? Yeah.
00:17:34.806 - 00:17:39.954, Speaker A: Great. Do you want to take multiple questions? I'm going to need my notepad.
00:17:45.454 - 00:19:08.184, Speaker C: Okay, so this is primarily, I guess, a question for carnal, but it's not intended to be a challenge and it actually relates to what Ben was talking about as well. So I guess, I know it's a shorthand and it's a unity, sort of referring to unity of something that's very disaggregated and fragmented. But when we use the word state and we talk about administrative decisions, you know, that's a kind of, in a way, like almost an outmoded from reference in itself. And I think, you know, one of the interesting things that goes on in a lot of the discourse, commercial discourse, as well as the political discourse. And here I'm just thinking of Mister Dominic Cummings, who for anyone who doesn't know this, was Boris Johnson's kind of like pseudo intellectual scientific guy who wanted to literally sort of turn the entire UK government into a kind of algorithmically governed control station, you know, and against that kind of imaginary, where the state is disaggregated into this kind of like real time simulation of the world. You have something like what, Ben, I take you to be suggesting, which is actually, this is an opportunity to think about disaggregation of democratic power, right. And a re imagination of the political that doesn't refer to the old administrative state.
00:19:08.184 - 00:19:24.334, Speaker C: And I just wondered, Ben, it's a question, like, if I picked you up that correctly, and, Connell, is that you're just using the state as a shorthand. I mean, are we kind of thinking, like, okay, maybe the state is the thing that is both being attacked from all sides in this. In this kind of project? Yeah.
00:19:26.834 - 00:19:27.442, Speaker A: All right.
00:19:27.538 - 00:19:37.726, Speaker B: If anybody sees a microphone near them, then one has mysteriously gone missing.
00:19:37.870 - 00:19:38.674, Speaker D: Okay.
00:19:40.054 - 00:19:51.534, Speaker A: Yeah, we can do that. Yeah, thanks to. This is a really great question. And so I think, yeah, you have to acknowledge, when you're using live action roleplay, you're using an irrealist mode.
00:19:51.574 - 00:19:51.750, Speaker B: Right.
00:19:51.782 - 00:20:41.044, Speaker A: You're not trying to determine real outcomes. There's other work now looking at, for example, how real citizens feel when they know that decisions have had, you know, an automated aspect to them. And that isn't what I'm trying to test. What I'm looking for are kind of sites of new kind of normative value within these kinds of apparatuses. And the idea is that we're testing multiple things throughout that process, and it's an iterative process. So the first, in fact, the laps kind of increase in realism as they go on. And it's very important to me not to begin by saying, oh, that isn't real, or that isn't realistic, or that isn't a real legal criterion, or what normative values are you trying to introduce? Those are things that reviewers of my project asked about very grumpily.
00:20:41.044 - 00:21:24.136, Speaker A: And the answer is, we cannot start there. We know that our existing frameworks aren't really working. We need to start somewhere else. And so this is the point of starting in these ecological philosophies. And so the first lap, for example, is super unrealistic. I'm doing basically a crowdsourced drone strike paradigm, so participatory in a very dystopian mode. It'll be a two hour game or three hour game with lap geniuses chaos League in Italy, and people will be able to see, what would it be like to have to take the decision to kill in a cohort of anonymous online people with who you're interacting with and interacting with AI and real time signals intelligence, what would that actually be like? And so you can see what I mean.
00:21:24.136 - 00:21:52.016, Speaker A: The aim is to look at the technical interface problems more so than solutions or real outcomes, and then that'll be iterated across different labs as I go, which we can talk about other times, but I think it's important to know. I'm working on normative imaginaries to start with. Yeah, I'm not working on what would the outcome be if we tweaked this value in this. If we backpropagated in a certain way here, that's all going to be part of it, but in a very loose way. Sorry.
00:21:52.160 - 00:21:53.084, Speaker E: Super brief.
00:21:55.784 - 00:22:34.282, Speaker A: Suspension of disbelief required for a measurable outcome, even if it's speculative scenario. But we can talk about this. Yeah, I'm not sure I understand the question about the state. Yeah, I think, Bernard, I'm forgetting about the state for a moment, strategically, to do that thing about, let's really start somewhere else and kind of build it. And so I'm very influenced by Bernard Stiegler's philosophy, who didn't get a chance to talk about, but I'm looking at it like a constructed situation. So you construct the decision first and then see how that, like, how would the state figure once. Once we've.
00:22:34.282 - 00:22:44.654, Speaker A: Once we think we know what decisions would be and what good decisions might look like within a techno, you know, in an imminently technosocial value system? Yeah, that would be my answer.
00:22:45.594 - 00:23:35.776, Speaker D: Mike was discovered. Yeah. I mean, I think you could sort of decide whether, maybe open question how much you want to incorporate the types of institutional oversight mechanisms into the administrative state. But I think it's actually quite concordant with many of the administrative state mechanisms that are already in place. I mean, something like notice and sort of like public comments on proposed rule changes in the US is an example of a form of democratic input over sort of these broader decisions. You can envision part of my sort of stage one of agency review could be a version of that where the rule would be, we're going to implement this algorithm to inform sentencing decisions, and let's have public comment about that. So really shifting those decisions upstream in a more stringent way.
00:23:35.776 - 00:24:29.652, Speaker D: And I think that actually fits in with quite a bit of what the administrative state mechanisms already exist for, and that's probably an easier path forward than sort of abandoning that paradigm. And I think sort of an additional sort of response to Fabian's question from me, because I've, you know, much of my. The prior work I was talking about was experimental research with lay people making these types of decisions. And sort of the obvious response was like, you're not working with judges. Like, what does this mean? And I think there's always the challenge with experimental work of both recognizing that it's not the field, there's always issues of ecological validity, but also there are insights you can get from experimental work or these sorts of, like, simulated role play that are different than what you could get in practice and also allow you to play with these mechanisms in a low stakes way. Right. So if we can build our scientific understanding without having to put these systems into practice, that's a lot of value.
00:24:29.652 - 00:24:42.494, Speaker D: And then always the question with experimental work is figuring out what translates into the real world and what doesn't. But I think there's a lot of value with that type of humility in bringing that knowledge to bear.
00:24:43.474 - 00:25:32.562, Speaker B: All right, so I have a few questions on tap, but I would like to interject a question first to Connell. I still want you to say the difference between administrative law and law. Like, why do you keep putting that word administrative in front? And I feel like it's a coded thing that half people in this room know the answer to and the other half of us don't. So that's my question. And then to Ben, such a clear, like, you laid this out so nice and clearly, I think what I took away is that when you. Your idea is to kind of shift the burden of proof on the interjection. Not to say, all right, we're going to interject an algorithm.
00:25:32.562 - 00:26:26.494, Speaker B: Now, what we're going to do, but rather let the institutional leaders first say, you've got to prove to me why the algorithm is needed. Is it efficiency? Is it capacity? What is it? But then is there a possibility of a recursiveness happening? Then the. The answer is, well, we'll put it in to be more efficient, but we'll always, like, have a human in the loop. What's to stop that little Gremlin coming back? So that's the whole question. You don't have to address the final bit, but it was really the tail end. What I could do is take a few more questions. So maybe Ben and Sonia, and then.
00:26:29.994 - 00:26:51.002, Speaker E: Well, I can't wait, because I want to make a connection. I really enjoyed the session, actually. I was also on Zoom for the morning session, which I also enjoyed. So I really want to make a connection of the issues you talk about by human loop to upstream before the algorithm being deployed. Right. That's where I come from. I'm a data scientist, machine learner or statistician, and they're actually myself.
00:26:51.002 - 00:27:18.194, Speaker E: My collaborative have been advocating this, making things transparent, and we make so many human judgment calls, definitely including human biases. And I wonder whether there's capacity for you to even go upstream to the development algorithm. I think I'll make a more integrated system. And some of what both of you said, I think is very relevant there, too. So I just want to make the connection.
00:27:20.174 - 00:27:26.674, Speaker B: Thank you. Oh, okay. And then we have some others that.
00:27:27.894 - 00:27:49.846, Speaker D: Yeah, thanks for these questions. I think. Yeah, I think we can continue to sort of shift upstream like Helen, how you sort of framed it is. Yeah. Really. I really think about it in terms of shifting the burden of proof away from. We'll put in the system and then we'll add a corrective to why are we putting the system in place at all? And I think as part of that, that would involve.
00:27:49.846 - 00:28:45.970, Speaker D: I didn't get into this too much, but actually having much more of that review process also includes some discussion of if to the extent we're going to incorporate human algorithm collaboration, is there evidence that that works? So the burden of proof should be that we are skeptical that human oversight or human collaboration with AI can solve those problems. To actually, the default assumption should be that human oversight doesn't work. And to the extent that it does work, then we can actually do that. And that goes to Vin's question where I think a lot of the insights for this work also I think do go upstream into the development process. And that's where that algorithm and the loop idea comes from, is that we actually shouldn't be just saying we're going to design a model for maximal accuracy and then put it in and do human in the loop. We should be designing for improving human decision making, not accuracy. And I think Shann's work from yesterday morning on remarkable AI is a really good example of that.
00:28:45.970 - 00:29:02.534, Speaker D: That often the algorithm that's going to best improve human decision making is actually quite different than the algorithm that is optimal just on purely sort of statistical measures. So we actually need to be shifting what we're even designing for from a development perspective.
00:29:03.254 - 00:29:46.972, Speaker A: Yeah, yeah, I totally agree. I mean, so the second lap that we're going to be doing in my project at the moment looks like it will be lapping the collaborative development of the decision system. Right. So what would it look like if a government agency knows now we need to make a certain kind of decision, this is going to be coming or we want to reform a decision process. Who decides how that works? At the moment, people don't even really know. And it happens very ad hoc, very in department, there's no centralized best practices, nothing. So I want to go completely in the other direction, say, let's go as far upstream as we can and get everyone who might be vaguely affected, involved and say, how do we co design this system? You can do live coding, live building, you can have sessions.
00:29:46.972 - 00:29:59.044, Speaker A: I actually want to do this. It won't be real but that's what I want to do. The interactions will be real, the coding will be real. I want to go way upstream. Bin, I think you're absolutely right. That's super important. Yeah, absolutely.
00:29:59.044 - 00:30:42.016, Speaker A: Should we answer Helen's question? Yeah, well, I mean, this is. Yeah, okay. So, I mean, I don't want to go on too long about this, but it's a big thing. I mean, the first thing is there's a huge trend in my field away from talking about law, law as just capital l law, because all law is not the same. Law is not some sort of. It's seen as a hangover of this kind of, like, theological sense of law, that there's a sort of grand thing called law, and we all interact with law. There's a huge trend in the kind of theoretical frames I'm working with to say, if we do away with that notion, we think about more detailed analytics, like control or power or production of subjectivities through technical means.
00:30:42.016 - 00:31:05.680, Speaker A: It gives us more agency and how we work on it. So this is a sort of major movement that I guess I seem to have endorsed. But more generally, I think I'm. Well, secondly, different branches of law work very differently. And I don't want to talk about all kinds of law. The way I've been teaching about sentencing, judging with my students, it's completely different. It just has nothing to do, I think, with how administrative decision making works.
00:31:05.680 - 00:31:29.784, Speaker A: It's just a very different process. It's not judges. There's no judicial society which helps to regulate how judges navigate these tools. There's none of that. It's a very different subject. Who's making these administrative decisions as a bureaucratic. Just really briefly, fascinatingly, you can find literature from the forties and fifties saying administrative decision making should be automated.
00:31:29.784 - 00:31:49.176, Speaker A: It's a core value of the administrative state that that's the sign of rationality. The more you can remove the individual subject from it and suck the discretion out of it, the better. So that makes admin law very specific context for thinking about what AI is doing in that context. Right. And how it interplays with value and what we want out of the decision. Yeah. So I'll leave it there.
00:31:49.176 - 00:31:50.164, Speaker A: All right, thanks.
00:31:52.824 - 00:31:53.696, Speaker B: Sonia.
00:31:53.840 - 00:31:54.448, Speaker A: Peter.
00:31:54.536 - 00:32:21.000, Speaker B: And is your name Jason, I think Yasun. Yasun. Okay. Okay. Same here. So actually, Kamal, right at the end, when you mentioned power, that's what is really missing in both of your topics, creation and subjectivity. So from the fucolian perspective, and I really buy his from the dungeon to the panopticon to the neolithic liberal, subject to.
00:32:21.000 - 00:32:52.336, Speaker B: Now, I take it we're in this sort of an algorithmic form of governance. It's sort of maybe another step. And what both of your presentations felt more like, if we think of Hobbes or Bentham or Gary Becker, I felt that you're more in the role of those guys trying to just come up with this new thing. And I'm completely missing where Jake, that first, say, when we're talking about power, it seems so like there's no power. There's no. I don't know. I just sort of.
00:32:52.336 - 00:33:13.204, Speaker B: Sort of baffled. You're more like, yes, we're doing this administrative, which is vivarian, by the way. I mean, it just goes back to Weber that the state is just going to be doing this sanitized thing of governance. And I just felt that you're part of that program in the way that both of you were presenting your analysis. So if you could reflect on that and. Thank you.
00:33:13.784 - 00:33:20.454, Speaker A: Maybe I'll start. I mean. Oh, sorry. Sorry. It's such a big question. I thought we should get started early.
00:33:22.194 - 00:34:14.034, Speaker F: Mine is a relatively brief question for Ben. I really appreciate the moving away from that scapegoating of the human individual, who's kind of weighing in as the decision maker. Did the AI make a good decision here? And, no, I override it and so on. And it seemed to me like you were suggesting that the institution take a role in that. And I was wondering, what does that really look like in practice? So I see that the move from an individual human oversight is good. Go to a group or something like that. But what would the group be comprised of? I mean, who would be involved, and would that differ depending on situation? Are we talking about an ad hoc deliberative council that's developed to come in and look at decisions over a period of time? Is it a one off thing? I'm just wondering, sort of, practically speaking, what that alternative looks like between the individual and institutional black box.
00:34:19.694 - 00:34:56.110, Speaker G: Okay, thank you very much. I have two related questions. The first is for Ben, and the second, I think will be amenable to both of your presentations. So, the first is that I'm very sympathetic to the findings that you described. In fact, they kind of resonate with my assumptions about what has been discovered through sociotechnical research. But I'm curious, when we say that human in the loop systems don't work, when we say we've studied it and they're not performing better, what is the kind of metric that we're using? So, in a sense, it will always be surprising. At DeepMind, we see this.
00:34:56.110 - 00:35:37.594, Speaker G: When you add a human to the mix, it's not going to work as well as some aggressively optimizing algorithm. But often the normative ask is something different for people to have an experience that they feel emotionally salient or where they believe that they got to error grievance or something like that. So does your notion that these algorithms don't succeed take into account the broad set of meanings that people might be asking for when they request humans in the loop? And then the kind of building upon that, I'm curious, if we were to try and rescue the idea of human in the loop and not just think of it as ideological wash or a kind of simplistic implementation of some kind of technical system, what could we say in defense of the notion, or what might that look like?
00:35:40.574 - 00:35:52.630, Speaker B: Here's a choice. We've got three more questions, and I love to take all the questions. We could start tea five minutes late and, you know, what would you like to do?
00:35:52.702 - 00:35:53.514, Speaker A: I don't mind.
00:35:54.454 - 00:35:56.382, Speaker D: Yeah. Happy to go over a few minutes.
00:35:56.438 - 00:35:57.870, Speaker A: But maybe we could respond to that.
00:35:57.902 - 00:36:01.014, Speaker B: Yes. Yes. Okay, let's do that.
00:36:01.094 - 00:36:06.834, Speaker A: I already think. I didn't get your sense. Sorry. I don't mind.
00:36:08.654 - 00:37:20.856, Speaker D: Sure. I'll sort of try to respond to all of that. Yeah, I mean, I think I sort of brushed, certainly brushed over some of those questions of power that I think are absolutely sort of structuring this idea, because what I see the human oversight paradigm is doing is exactly creating a band aid over those questions of injustice and sort of unaccountable power. So you have organizations, whether it's welfare agencies or large tech companies or court systems, that are essentially able to just choose to unaccountably put these systems into place to surveil, control, punish individuals. And the human oversight provides this very superficial way of saying, oh, but we are accountable. We are caring about your outcomes. And so I think that sort of gets into some of the examples of how then there's that significant amount of power for, say, a police chief to blame the, you know, a police officer for not overriding the facial recognition system when it's the police chief and the technology companies that put these systems into place to surveil, punish, and so on.
00:37:20.856 - 00:38:37.194, Speaker D: So I think that it's absolutely operating as this sort of mechanism that allows the power, powerful individuals and organizations to operate without accountability. And so I think when. And so, yeah, so, I mean, with respect to, you know, what are the goals? I am sort of looking at how is human oversight actually able to address those types of concerns around, are they identifying errors, making the system less biased and so on. I mean, I think there's this sort of open question that sort of gets at somewhat of like a european versus american approach to these things, of how much of it is just a dignitary harm, where people want a human to be involved as a sort of dignitarian salve, and whether or not that actually leads to better decisions. I'm not that sympathetic to the view that just inserting a human creates a more dignified process if the insertion of the human is not preventing discrimination or preventing errors and so on. But, yeah, so the various experimental research is really looking at both accuracy, but errors and so on, sort of looking at the other types of values that we might care about and even thinking about. Yeah.
00:38:37.194 - 00:39:05.508, Speaker D: How it's warping the decision making process entirely and actually shifting from, say, shifting to a value where maybe a judge is only thinking about recidivism risk, and not thinking about leniency or sort of liberty and so on in that exact value proposition. So I think that's really what I'm thinking about. And then sort of, with respect, maybe I'll let actually you go and, Peter, we can talk offline, just to make space for everyone's questions.
00:39:05.556 - 00:39:28.430, Speaker A: Yeah, I mean, I agree with everything Ben just said. And so for that reason, I feel justified in also semi answering for Ben in talking about the sort of approach that I think we might both be taking here. And I think the reality is the states being reformed really quickly under the COVID of darkness. This is what Inkstrom calls the quiet revolution. Right. So there's wonderful for coding critiques. They're already there, and they're not doing anything to stop this.
00:39:28.430 - 00:40:03.230, Speaker A: Nothing. They have no impact, no influence. Even if people know about them and approve of them, they can't voice or articulate them or act on them. There's just no way to do that. I mean, it's going to sound pretentious, but at a certain point, someone, you know, person like me, has to say, what am I doing? Why am I a scholar? What actually are we trying to do here? And, you know, in my case, particularly, in Ben's case particularly, look at us. You know, I had to put down a critical mantle to some degree and say, well, I actually need to help institution shape here if I can. But it's very, very different.
00:40:03.230 - 00:40:28.906, Speaker A: I mean, my theoretical paradigm comes from Bernard Stiegler. Right. So this has just nothing to do with state forms. It's the absolute radical left alternative to state forms. It's saying, how do we keep the future open? How do we depraitarianize? How do we give people the capacity, the right to work on their own political determination? Like, that's what this is. And this is what this AI problem is giving us to think about. How do we keep.
00:40:28.906 - 00:40:39.450, Speaker A: And it's not. I agree with Ben. You can't just say, here's a flesh and blood thing called a human. Let's put it here. I mean, this is just not touching the problem. It's not touching the network that's actually working. Yeah.
00:40:39.450 - 00:40:57.726, Speaker A: So it's about for Stigler and for me and for Ben, too. I think it's about what other analytics can we use to carve this problem up and get different points of entry? What other evaluative languages do? Touch. Yeah. This. The qualitative nature of these arrangements. Foucault is important. I mean, Foucault is important.
00:40:57.726 - 00:41:14.430, Speaker A: And I know, and I'm fearing the day, but the day will come when this, my project is established enough that people start critiquing it. And that's great. You know, it's great. But I want it to come out of the practice. I want people to be laughing together, saying, here's a system. And I want the people doing it to say, I have a problem with this system. Right.
00:41:14.430 - 00:41:19.434, Speaker A: The critique has to come from, for me, within the process in which everyone's empowered to participate.
00:41:19.554 - 00:42:48.202, Speaker B: Okay, so can I. I have three questions, and I think maybe asking questions, we would call it a day, and discussion can continue over the key, and perhaps one, and I don't know if anyone. That's fine. I mean, mine definitely can be taken as a comment for Connell, just a little anecdote from sort of the history of AI, in which there is actually a precedent for simulated role play, as method that starts at the very roots of AI. The 1956 Dartmouth conference on AI, where Herb Simon and Alan Newell are putting forward what's considered the very first AI logic theorist, which, if you're not familiar, it's a program designed to prove the mathematical theorems of Bertrand Russell and Alfred North Whitehead's principia. And this is sort of a little tidbit from history, is that Herb Simon used his own family, his children, and his wife and grad students to physically act out the program to see if it would work. So they wrote individual rules or subroutines and components of memory of the program onto index cards, and they assigned an index card to each person, and then the participants then physically executed the subroutine and all provided the contents of memory when called upon, which is, like, super fascinating in.
00:42:48.202 - 00:43:29.424, Speaker B: So that this is like a really innovative and experimental form of research that also can be drawn back into the very history of the birth of the field itself. And one thing to sort of consider, I suppose, is that I've researched and looked at that in the context of gendered histories of AI and whose labor counts, like, who is an inventor at that moment. And it's like you venerate Alan Newell and Herb Simon, but his wife and family, whatever, they're like, not part of it. And also the ways in which there's, like, fundamentally, like, embodied ways of knowing and knowledge making at the very beginning. And yet this is all part of a history of symbolic AI, which apparently has nothing to do with, like, the body. And it's all abstract.
00:43:33.764 - 00:43:34.504, Speaker A: Thanks.
00:43:35.404 - 00:43:43.988, Speaker E: So just a sort of a huge shout out to University of Kent because that's my alma mater. So, yes, it's rare to see people from there.
00:43:44.116 - 00:43:45.208, Speaker A: Kentucky boy, huh?
00:43:45.276 - 00:44:13.124, Speaker E: Yeah. Anyway, so the one thing that I sort of think is super, super valuable about your research, Kong, is the ethnographic side of things, which is like a huge gap. Right? Like, we have all these, like, things about, like, these. These should be the models. It's the data that should be used. And then we have, like, legal theoreticians to say, you know, these are the normative things we should think about. But we never really have anything about, like, the people who actually, like, are involved in these things and what sort of things they take into account.
00:44:13.124 - 00:44:41.744, Speaker E: And that sort of really answers, like, a very deep question about it. And the other thing is sort of like a little sort of reference to the. I think you mentioned in the slides, this evolutionary approach. So that's my sort of one concern. It's sort of clear that the dust in this area hasn't settled at all. Right? It's not being sort of. And the way I think about, like, legal decision making, like, from a theoretical perspective or like, what.
00:44:41.744 - 00:44:45.066, Speaker E: How rules emerge is that there's a lot of factual situations that keep repeating.
00:44:45.130 - 00:44:45.306, Speaker A: Right.
00:44:45.330 - 00:45:07.762, Speaker E: And then we can abstract them away to a higher level. Right. Whether in a decision or a rule. Right. And sort of imposing or imposing is a powerful word. But sort of having this theoretical view of, you know, what a quality decision is. Could it be like crystallizing, like something that's kind of not yet formed, if that makes sense.
00:45:07.762 - 00:45:09.214, Speaker E: But, yeah. Thank you.
00:45:10.494 - 00:45:18.754, Speaker B: Thank you. You're going to have a lot to discuss, including your home universities. Chris, last word.
00:45:20.294 - 00:45:41.710, Speaker A: Thank you both. Those were really wonderful, interesting papers, and there's too much to say about both of them. Yeah, one point was just for conal. I wanted to talk a bit more yesterday, but in French, work factors, they do a lot of laughing or like, a lot of this sort of like, situated, simulated simulations in the workplace and stuff. That's kind of an interesting tradition. You might. I think that's a great methodology.
00:45:41.710 - 00:46:18.092, Speaker A: And, I don't know, there's just sort of a vein of research there you might be interested in. But then, you know, other questions. I mean, that was great. I guess the interesting thing for me is, you know, the idea of the human in the loop as providing the false sense of security, you know, like that's. And that's the danger. And I agree, you know, that's always like the question of anxiety, and diminution of anxiety is always kind of like a problem, you know? But surely, I mean, I guess this is bouncing off Sonia's point, really. You know, there seems to be, like, kind of a neo weberian sort of institutionalism that's kind of resurgent at the moment.
00:46:18.092 - 00:46:41.308, Speaker A: And I disagree. Like, my boss, Mark, he's kind of into this and, like Frank Desqua, like, I think, you know, and I. And it's. It's interesting to me because as far as I can tell, I'm not a political scientist, but, you know, there's never been a more significant crisis of the institution, at least in America, than there is today, you know. And in Australia, similarly. I don't know, there's lots of research on you. We love to have a royal commission.
00:46:41.308 - 00:47:23.742, Speaker A: We're always having royal commissions into things and they never, like, do anything, you know, like, so this is what I worry about, you know, like, there's a kind of false sense of security. I agree. It's like, okay, human just ticks the box and it's fine. But there's a broader false sense of security, perhaps in terms of kind of like outlier, you know, the, you know, kind of. What's the word? I don't know, putting the question of political within, like, this sort of frame of the institution itself. You know what I mean? I think this is kind of an unfair question, you know? But you were saying, like, you think this is kind of like what the model you're proposing is more or less congruent with, like, current kind of legislative capacities and so forth. So I don't know.
00:47:23.742 - 00:47:38.290, Speaker A: I just wonder what your thoughts are on that. You know, like, is that a danger as you foresee it? How do we kind of, you know, and I. And I take the point about. Well, you know. Yeah, obviously. Yes. We also need to worry the human being outdated, but you know, the Leviathan, you know, and all these.
00:47:38.290 - 00:47:39.954, Speaker A: These are all outdated problems as well.
00:47:39.994 - 00:47:40.170, Speaker E: Right?
00:47:40.202 - 00:47:44.346, Speaker A: The state, you know. So I don't know, just your thoughts there I'd be interested to hear.
00:47:44.490 - 00:47:59.874, Speaker B: Okay, so you. Thank you very much to everybody. Fantastic questions, great provocations and cot and we'll reassemble.
