00:00:00.200 - 00:00:33.554, Speaker A: Today, the theme is to try to go up from propositional logic and look at the interaction between propositional satisfiability technique and higher level reasoning, such as first order reasoning. Or in case of. Philip is going to tell us about string constraints. So, to see, there is a whole other thing about SMT, which we're not going to deal here, but we are looking at lifting propositional techniques to higher level reasoning. And so we're going to start. Philipp is going to tell us about string constraints.
00:00:35.294 - 00:00:38.318, Speaker B: Okay, thanks a lot. How much time do I have for this presentation?
00:00:38.486 - 00:00:39.958, Speaker A: You have 45 minutes.
00:00:40.046 - 00:00:43.646, Speaker B: 45 minutes? Okay, that should work, but no one.
00:00:43.710 - 00:00:46.954, Speaker A: Will complain if you do it in 40 minutes.
00:00:48.294 - 00:01:04.748, Speaker B: Let's see how fast I can be. So, I hope you can see the slides now. And also I hope that you can actually understand me. I'm just trying to use the microphone of the computer, and if that doesn't work, well, then I can just switch to the headset. But I like it, actually, to speak without that headset. So far, so good. Okay, great.
00:01:04.748 - 00:01:42.884, Speaker B: And yeah, I think this can be very interactive. So in case you have any questions, then please just interrupt at any time. Or if, in case I make any mistakes, then you can correct me, of course, as well. Right, so this is a bit of a summary about our work on string constraints, and also about string constraint solving in SMT in general. And I should start by saying that this is really joint work with many people, so at least the people on this slide, and then probably a couple of other people that I forgot. And of course, all the actual contributions go back to the people on this slide, and all the mistakes are due to me. And so the plan of the talk is to start with a bit of an outline about string constraints and why string constraint solving is interesting.
00:01:42.884 - 00:02:32.824, Speaker B: And then I would like to discuss two algorithms that we have been working on ourselves, and the one that we implement in our tool norm, and a newer one in our tool ostrich. So, both algorithms for checking the satisfiability of string constraints quantify free string constraints. So let's start from the beginning. Why are we interested in strings? And, well, of course, everybody who has been writing programs actually knows strings. So strings are really the data type number one, and might actually be the first data type that you see when you start programming. You cannot even write a hell of a program without strings. And so this is a trend that actually has become even more pronounced in the last years, because now we have high level dynamic languages, for instance JavaScript, or also Python, where really most of the data is in some way stored as strings.
00:02:32.824 - 00:03:24.816, Speaker B: So in JavaScript anything can be seen as a string in principle. So it's completely impossible to do things like program verification for JavaScript without being able to handle strings. And so string constraint solving is also getting more interesting because these modern languages have rich built in libraries for strings, and so these libraries are themselves implemented in a different language. So like the library for JavaScript would actually be implemented in C or C internally. So that means that we cannot just take the definition of a string operation written JavaScript, but we actually have to support natively what operations we are using in the language. Otherwise there's no way to do things like symbolic execution, for instance, and right, so strings are used freely everywhere in these languages. So what are the operations that you're considering? So you've probably seen all of these well in papers or when writing programs.
00:03:24.816 - 00:04:17.008, Speaker B: So obviously when we have string variables, we can do things like concatenate the two variables to get a new string. We can also say that we want to split the string variable with some certain delimiter symbol, and then we get an area of the substrings. We can do things like varying the length of a string. We can test whether a string is accepted or is matched by a certain regular expression, and this can also be used to extract sub parts of the string. So we can have things like capture groups in the regular expression, and then we can do like more sophisticated things. We can do things like search and replace, where again the search string can be possibly a regular expression, and actually the replacement string can contain, can refer back to the capture groups, but it can actually be by itself a function that is defined to do more complex operations. We can have things like conversions from string to numbers, or vice versa.
00:04:17.008 - 00:04:48.424, Speaker B: We also might have things like encoding or decoding operations. So for instance changing from UTF eight to UTF 16 and these things. So it's a relatively complex set of operations, which makes string solving very interesting. And right, I should also mention that regular expressions are not really. Well, what programming languages see as regular expressions does not directly correspond to the textbook version of regular expressions. So we have quite tricky features like capture groups I already mentioned. But we can also have back references to capture groups.
00:04:48.424 - 00:05:42.174, Speaker B: So we can say that a regular expression matches on some string, and then later on in the regex we refer back to the same string. And this way we can actually write regular expressions that do not describe regular languages, but even context sensitive languages. And so that's a feature that exists, for instance in JavaScript, which of course immediately causes all kinds of undecidability issues for us and makes the whole thing even more interesting. So what are string operations commonly used for? So this is a list of five kind of big areas that are taken from this book from 2017 by tier and others on string constraint solving or just the use of string relevance of strings for verification and security. And so typical operations would be things like input sanitization. So we check whether user input is actually following the right format or is in some way safe and also validation. So we actually massage the user input to make it correct.
00:05:42.174 - 00:06:49.838, Speaker B: We use strings to do things like generating queries for databases, for instance in SQL, handling data in general, and also more dangerous things like actually dynamically generating code in the form of strings, which is data executed, for instance using the eval function in JavaScript, or looking at Java. We could do things like dynamically loading classes or dynamically defining methods that we want to invoke. All of these operations are potentially safety critical, but what makes strings in particular dangerous is that in some of these cases we are mixing data and code, or we are converting from data to code. So in particular when generating queries for databases, of course we actually constructing a string that contains the data that we want to use to maybe data that we want to put into a database. But at the same time we are writing a program, and that is of course a very dangerous situation. So here is one example why this can be dangerous. This is of course a slightly very much simplified example, but it is inspired by a real security vulnerability found on web pages.
00:06:49.838 - 00:07:19.326, Speaker B: This is a small piece of JavaScript code that you could find embedded on a web page. There is some input data, which is this cat string. This is data that is controlled by the user. For instance, the name of a cat that is input by the user. But in principle that is a string that contain, can have arbitrary contents. Then we are calling various JavaScript operations on the string. So we are doing things like HTML escape, which is going to replace special characters with the HTML code.
00:07:19.326 - 00:08:15.284, Speaker B: You're running the escape string which is doing JavaScript escape, so it's adding backslashes in the right places. And in the end we are using the string y and also the x to construct a piece of HTML code. So this button, and actually this button also contains JavaScript code. So actually this executable code we're constructing based on user input. And now of course this is a dangerous combination because we have this code controlled by the user, the cat name, and we're using this code to construct a program so in principle there's a danger that actually the user can influence what we're going to execute, or in this case actually what's going to be executed on the user's machine. Because this JavaScript code of course is going to run in the browser, right? So there are already complicated operations. We have the HTML escape and the escape string, but also the assignment to any HTML implicitly unescapes and some of the code.
00:08:15.284 - 00:09:18.234, Speaker B: So there is one further escape unescape operation happening, and well, you've already seen why this code is vulnerable, but I will just recapitulate what is happening. So the problem is that actually this code is not written completely correctly, and one possible attack would be to choose cat to be this string. So it starts with a quote and so on, and in the end actually there is an alert command. So why is this an attack? Well, the problem is that the generated HTML string, so the string that we are generating in the last command will contain the code that we chose the catch to be, but in the escape version. So in particular the parenthesis is going to be the HTML code. And now when we are running the HTML unescape, no, sorry, I made a mistake. So what is actually going to be.
00:09:18.234 - 00:10:10.154, Speaker B: So this HTML code actually corresponds to the single code that we had in the string. Now when we are assigning to the innerhtml field, then this is going to be unescaped, and what we get is actually a string that is just ending the createcatalist command and afterwards directly executing the alert. So the problem is really that this alert command is escaping from the argument of create catalyst, and instead it's going to be a separate command that is going to be executed. Now, alert is not going to do anything harmful, but in principle, of course this could be any kind of JavaScript code. So at this point we would actually be able to to attack the browser of the user. So now you might be wondering, so why is actually this code incorrect? Because apparently the programmer did a lot of effort to actually escape and try to make the safe. But the problem is that the escape operations are actually applied in the wrong order.
00:10:10.154 - 00:10:57.022, Speaker B: So the correct version would be to first do the escape string and only afterwards to the HTML escape. So somehow the programmer managed to swap the order of these escapes, and this is creating a vulnerability. Right. So this was just to demonstrate that actually, well, this code operating on strings is very interesting, and obviously since this has a security hole, you would like to find methods to discover such vulnerabilities. So well, one way is to look at the big toolbox of verification techniques that could be applied. And so a technique that has turned out to be quite useful is symbolic execution for analyzing a code of this kind. So we could actually try to just translate the single program path to a logical formula.
00:10:57.022 - 00:11:43.882, Speaker B: And as we will see, this formula has to talk about strings and has to mention the string operations, of course. And then afterwards we can run an Smt solver, or any kind of constraint solver in principle, had to try to find satisfying assignments, would then represent attacks. So how would this formula look like? Well, we would go through the path statement by statement and try to map each of the operations to some logical function or some logical constraint that a constraint solver could understand. So the HTML escape operation, for instance, this could be seen as a function that then a constraint solver has to understand. Similarly, the JavaScript escape would be another function. Then we have this concatenation of different strings on the right hand side. So this could be seen as a concatenation of five different strings.
00:11:43.882 - 00:12:14.494, Speaker B: W one, w two and w three are concrete strings. And the string also the right hand side also contains y and x, which were constructed in the equations above. Then we have the HTML unescape that is happening implicitly. And in the end we would like to actually search for some kind of attack. So we need some kind of target, some attack pattern that we're looking for. So this is often, again a regular expression that, for instance, expresses that. Well, the string that we're constructing is in some way it has a suspicious shape.
00:12:14.494 - 00:12:53.244, Speaker B: It's maybe not actually, and not only executing the code that was part of the original string, but it's also containing other commands. Now of course. Well, this is a logic formula. So if we had a magic constraint solver that is clever enough to understand all these operations, then we could find, for instance, the attack that I've shown on the previous slide. Right, so this is the wish list. We would like to have solvers that support basically all the operations that I've shown, and a couple of more things. So in particular, we would also like to support full Unicode alphabets, not just ASCII, because Unicode is the standard nowadays, and also different unicode representations.
00:12:53.244 - 00:13:33.306, Speaker B: Right. So what are the challenges? And actually, so this is another point that makes string constraints. Constraints are quite interesting. So there are both practical challenges and also theoretical challenges. So the practical research question could be what is actually decidable about strings? And I will show some of the interesting questions here in five minutes or so. What is the right string logic, or the right fragment that we would need for applications, but can also be solved effectively. And of course, how can we build efficient solves for these logics? And many groups are these days working on this, on these different questions.
00:13:33.306 - 00:14:11.112, Speaker B: So, for instance, or in particular, one outcome after many years of discussions, was this SMT lib format for strings. So this was released only in February last year. So it just turned one. And actually, this goes back to a proposal in already 2012. So this actually took many iterations to get done. This does not really answer the question what the right string logic is, because as the theory is defined here, it's immediately undecidable. So, in practice, of course, all solvers will look at, well, they'll consider some fragment, or they will try to solve whatever they can solve and be happy and.
00:14:11.112 - 00:14:42.934, Speaker B: Right. There are also many solvers, many groups are working on solvers for solving string constraints. So this is certainly an incomplete list. In case I missed your solver or your favorite solver, then I have to apologize. So really, the journey started with the solver hampi by Vic Ganesh, who was one of the organizers of the seminars. And since then, many solvers have been developed, including the ones in c three, the ones the string slob and CVC four, and, well, many solvers that are focusing on more specific fragments of strings.
00:14:43.594 - 00:14:45.338, Speaker A: Philipp, can I ask a quick question?
00:14:45.466 - 00:14:46.690, Speaker B: Of course, please.
00:14:46.842 - 00:14:55.722, Speaker A: What is the relationship between this and what classical thing on words equations or string equations and macros? I mean, very classical topics.
00:14:55.858 - 00:15:00.226, Speaker B: Yes, that's a very good question. I'm going to this. I'm going to get to this in two slides, actually. Okay.
00:15:00.250 - 00:15:02.334, Speaker A: It's very good. Very good. Keep going.
00:15:04.474 - 00:15:40.994, Speaker B: Okay, so the next part is about how we can actually solve string constraints. And of course, I need to go back to the classical works by on word equations. So how can we actually solve string constraints? And right here, we already have the word equations. And so let's look at some examples. And so this is a very simple word equation, and it's one of the equations that we had in the example. And, well, the question is, how could we decide whether this equation is satisfiable or unsatisfiable? Well, in this case, it's actually obvious because the equation is already in a solved form. So we can choose any values for the variables on the right hand side.
00:15:40.994 - 00:16:01.174, Speaker B: And then to get the left hand side, we just concatenate these strings. So we immediately have a closed form description of all solutions of this equation. And so. Right, you can see that in this logic, we only have a concatenation. Well, we have string variables. And of course, we also have equation equations. But it turns out that this is a very complex logic already.
00:16:01.174 - 00:16:29.764, Speaker B: So particularly also negated equations can be encoded into just positive equations. Also, disjunctions of equations can be encoded into just conjunctions of equations. And also conjunctions of equations can all be turned into a single equation. So just asking whether a single equation over Vertz and with variables ranging over Vertz satisfiable, is already extremely hard. Right? So this equation was easy. That that is promising. This is already in solved form.
00:16:29.764 - 00:17:08.244, Speaker B: How about this equation? So this is a bit more complicated, because now both left hand side and right hand side are compound terms, so they contain concatenation. But still, it's relatively easy to handle this equation. And so, the main insight that we need to have is that there are three different possible arrangements of the variables on the left and right hand side. So the first case would be that the x is a short string. So the x actually ends within the u on the right hand side. So here the upper bar is the left hand side, the lower bar is the right hand side. So in the first case, the x is short and ends within the U string.
00:17:08.244 - 00:17:46.954, Speaker B: In the second case, the x is a bit longer and ends exactly in the middle of these two concrete characters, Ab. And in the third case, the x is very long and ends within the v string. And so these are three possible arrangements, and actually all possible arrangements up to, well, that are interesting for us. And in each of these cases, we can just write on a closed form solution for the equations. So, for instance, in the first case, we can say that, well, actually the x ends within the U. So we can just split the u into the first half u one and the second half u two. And then x has to be u one and the y has to be u two and then ab and then the v.
00:17:46.954 - 00:18:07.958, Speaker B: And. Well, the situation looks similar for the two other cases. So we can split the equation, that is, that we started from. And in each of the cases, we end up with simpler equations that are in solved form. So, well, again, this looks promising. And actually this transformation has a well known name. This is known as Leed Nielsen's transformation.
00:18:07.958 - 00:18:56.576, Speaker B: And it just says in general that whenever you have a string equation, a vertical equation that contains concatenation on both sides, then essentially there are two possibilities. So either the first variable on the left hand side is short. That is this case. And there is a symmetric case where the u is longer than the x, and then, well, we can identify some common substring that we can call t, and we can use that to decompose the equation. So how about this equation xy equals y concatenated with z? Well, we can play the same trick. We can look at possible arrangements, and that gives us something like a tree of possible solutions for the equation. In the first case, we assume that the x is longer than the y.
00:18:56.576 - 00:19:28.164, Speaker B: So that means x is y concatenated with some string t, and then z has to be t concatenated with y. And we get the second case. And now we can see that in the second case, the y actually turns up as both left hand sides. And this actually gives us an equation xt equals tz. So these are the two right hand sides. And now, unfortunately, we can observe that this is actually the equation that we started from, just with variables renamed. So we ended up in a cycle essentially.
00:19:28.164 - 00:20:11.414, Speaker B: And well, this is actually what happens in general. Then we apply this decomposition rule so well. This is an important rule that is actually implemented by many solvers, but by itself, it does not decide satisfiability of string equations, because it can get into these cycles and one has to work very hard to get out of them again. So what can be done? We can just ignore the cycles and hope for the best. And in some way, this is what most SMT servers are actually doing. They typically add various additional heuristics and criteria for detecting that equations are unsatisfiable. So it's not just ignoring cycles, but there's no very clever technique either to actually identify cycles and get out of them.
00:20:11.414 - 00:20:50.172, Speaker B: One can identify fragments for which this transformation. So the splitting rule is guaranteed to terminate. And there are various of these fragments, and the details are not that important. But essentially all these fragments are designed in such a way that when we split exhaustively, then eventually we'll end up with a closed form solution. Or of course, well, we can develop these actual decision procedures. And this is, for instance, McCann's method, which is essentially applying the splitting rule, but then it's also adding an effective termination criterion, or there is a newer method called recompression. And so the problem with these methods is that all the theoreticians claim proudly that they are completely unimplementable.
00:20:50.172 - 00:21:33.646, Speaker B: And as far as I know, actually there's no solver that is really implementing these decision procedures. Although these methods have been known for many decades, in particular, the first one has been on for a long time. Okay, so what we are focusing on is essentially the second case. So we are trying to identify fragments where we don't need the full power of these complex decision procedures, but we can just essentially apply splitting and it will terminate. But then we can add various other operations that are interesting. And I'm not going to give a full definition. Well, actually I'm going to define the straight line fragment, but I'm not going to define the other fragments.
00:21:33.646 - 00:22:07.302, Speaker B: And because the definitions are a bit technical and not that interesting actually. But in principle any of these could work, right? As I said, actually many of these solvers do implement the splitting rules that I've shown. So in particular, that's be the case for C three, str and also CVC four, and also many of the other solvers, but in combination with various other techniques. Okay, so this was actually the first kind of constraint. So just the word equations. And this already turned out to be a bit of a nightmare to solve in a complete way. But of course we are not satisfied with just equations.
00:22:07.302 - 00:22:38.944, Speaker B: We want more. So in particular we want regular expression constraints, and we needed those, for instance for the motivating example for this injection attack. And so it turns out that regular expression constraints cannot be directly reduced to vert equations, but also the combination is quite well behaved. So the decision procedures for vertical equations can easily be extended to also handle regular expressions. So this is fine. But then we would also like to look at length constraints. We would also add constraints at the length of some string x, maybe equals the length of some string y.
00:22:38.944 - 00:23:17.084, Speaker B: Or in general we would like, well, arbitrary length constraints in Prescott arithmetic. And, well, this turns out to be very interesting. And in fact, the decidability of this combination of vertical equations and length constraints is still open and has been open for a very long time. And well, even more interestingly, even when we look at very simple fragments of equations, for instance, quadratic word equations, where each variable can occur at most twice in an equation. So this is very simple because they're very simple decision procedures for this fragmentation. But the combination of the length constraints even then turns out to be very complicated. So even here the decidability question is still open.
00:23:17.084 - 00:23:48.106, Speaker B: Well, and then we would like to have these more complex operations like the HTML escape. And these can in a very general sense be captured as letter to letter transduction. And I'm going to show some examples for that if the time is permitting. And well, if we had transduction, then immediately we end up in a relatively simple setting. But the things are completely undecidable. So transaction actually immediately captures everything else. So using transaction we could also represent concatenation and also equations.
00:23:48.106 - 00:24:16.150, Speaker B: So in principle we would only need transduction here. Right. And so this is kind of the spectrum of what we could do. So we have very complicated vertical equations that are decidable but complicated. And we have transaction, which is in principle simple but undecidable. And now we would like to define fragments that are interesting for solvers and interesting for applications in the spectrum. And so I mentioned these two different tools that we had, norn and ostrich.
00:24:16.150 - 00:24:56.168, Speaker B: So in norn we had a relatively simple fragment for vertical equations, so it's even simpler than quadratic. But then in addition to that, we had full support for regular expressions and also full support for length constraints. So this particular combination is decidable. And in the second tool I mentioned in ostrich, we actually added a bit of transduction, but then we could not add length constraints at the same time. So again, this combination of length constraints and transaction immediately give us undecidability. So this is a green fragment and well, there are many other interesting fragments that one could carve out here. Right.
00:24:56.168 - 00:25:31.786, Speaker B: So I think 15 minutes left. So in the presentation I first talked about Norn, which is supporting these different parts, but I think I will jump over this part and instead directly talk about ostrich, which is the much newer work. So this tool, Norn, was published at KAF in 2014. So it's already a bit older, although compared to other tools, well, it's still handling a relatively rich logic of string constraints. Right. So what nor is supporting are essentially all the parts that are a green check mark. Yeah.
00:25:31.786 - 00:26:17.914, Speaker B: So concatenation within this fragment of word equations, length constraints, regular expressions and, well, also arbitrary finite alphabets can be handled efficiently, but we don't have things like search, replace or escape operations and so on. And this is really what we wanted to solve in this second tool, ostrich. And, well, that leads me to this part where I want to talk about how to solve string constraints going backward. So starting from the end. And so this is in some way this is also one of the key steps underlying norm, but we look at it in a more general fashion here. So let's consider a word equation. And actually later in Austria, we're going to see this as an assignment of x concatenated with y to some new variable, z.
00:26:17.914 - 00:27:11.648, Speaker B: And so what we're using as the main reasoning step in ostrich is backpropagation. So we assume that we have this equation and we have some regular expression constraint about the result, about the z. And so z is in some regular language, l. And so the main step we are doing is, well, we are going to backpropagate this, and from the l we're going to derive constraints about the input variables x and y. So now this is not possible in this form in general, but it's always possible to find a finite number of cases. So we can always say that if the image of this concatenation is some regular language l, then we can find a finite set of regular language pairs l one I and l two I, so that their uni corresponds exactly to the pre image. And so this is essentially the lemma that is underlying this.
00:27:11.648 - 00:27:36.934, Speaker B: So whenever we have a regular language link, and then there is a finite set of languages, of pairs of languages, this form, so that x concatenated with y is in L if and only if x is in one of these pairs, l one I, and y is in the other regular language. And. Right, so here I should actually say that, well, these languages are, again regular languages, so we can describe them using automata.
00:27:40.574 - 00:27:50.286, Speaker A: Yes, I suspect that the lemma is stronger. It gives you probably an algorithm to go maybe from a regular expression for l to regular expressions for l one and l two, right?
00:27:50.430 - 00:28:15.954, Speaker B: Yeah, that's right. Yeah. So we are internally always working with automata, but in principle this is very simple. We just need a disjunction over the states of the automata of the automaton. Yeah, you just have to say that whenever, whenever you're looking at a run of the automaton, then of course the recognition of the x has to stop in some state of the automaton. And from that point on we're going to recognize the y. So this n will be bounded by the number of states of the automaton.
00:28:15.954 - 00:28:21.354, Speaker B: But I'm sure you could also directly do this on the level of regular expressions if you want.
00:28:21.694 - 00:28:22.334, Speaker A: Tomato?
00:28:22.374 - 00:29:07.884, Speaker B: Fine, yeah, great. Yeah, right. So this is what can be done for concatenation. And, well, there's a simple lemma that explains why this is possible, but it turns out that actually many other functions have the same property. So whenever we have some function f, and I'm going to call these functions admissible, then if you know some constraint about the result, we can back propagate and get constraints about the arguments. And in general, we are going to have, well, this finite disjunction. And so these are the functions that we call admissible, not following exactly the structure of the slides, but so we call functions admissible whenever the preimage of regular languages l can be represented in this form.
00:29:07.884 - 00:29:52.414, Speaker B: So, finite union of cartesian products of regular languages, and if you know about transducers, then you will also know that this shape is called recognizable. So a function is admissible whenever the pre images of regular languages are recognizable relationship. Right. So now I should jump back because I actually skipped all the interesting part, interesting parts, right. So what is the fragment of formulas that we are actually handling? And so these are these straight line formulas that I mentioned. So we are supporting straight line formulas, or we can also call them programs containing regular expressions and assignments. And the assignments can involve application of admissible functions to string variables.
00:29:52.414 - 00:30:43.452, Speaker B: And well, as we can prove this is a decidable fragment as well. So what are straight line formulas? Not straight line formulas are simply conjunctions of constraints where each of the conjuncts is either a regular language constraint or it's an assignment of the result of applying some admissible function to a vector of string variables and valid straight line, because all the variables on the left hand sides are pairwise distinct. And we may only use variables in constraints that are following data. So it's a bit like the SSA form of a program. We can only use variables once we have assigned a value to them and we cannot assign values twice. And here would be an example of a formula in straight line. So this formula consists, well, contains both regular expressions.
00:30:43.452 - 00:31:41.254, Speaker B: So x is an input variable and is assumed to be in this regular expression a star, b star. And we also have applications of admissible functions like reverse or replace all. So replacing all occurrences of a with b. And now the question is, well, is this conjunction of constraints satisfiable? Or in other words, is there some input x that will satisfy all the assertions? So the assertions are directly expression constraints. So to answer this, we are going to use this principle of backpropagation. So we're going to go backwards in the program and we are going to, we will try to figure out constraints for the x that we need to satisfy all the constraints. Well, so first of all, how can we do this? We know that z is in the language b and z is the result of replacing all a's with b's.
00:31:41.254 - 00:32:21.094, Speaker B: Well that of course means that y needs to be read in the language a or b because all the a's will have been replaced by the replace all. So that means we can just remove the last assignment and instead say that Y has to be an A B star. And now this can be conjoined to the regular expression about the Y, which in fact turns out to be stronger than the second one. So you can just actually remove the last regular expression. Then the Y is the result of applying the reverse operation. So reverse will just change the order of the characters. Well, if the Y is in b a star, then of course the reverse of Y.
00:32:21.094 - 00:32:47.954, Speaker B: I'm sorry. So if the Y is in b a star, then of course the x must have been in a star v, because reversing it gave us the y vertical. So we can also eliminate the last assignment and replace it with a regular expression. X is an a star, v star. And that also turned out to be the constraint that we had in the beginning. So it's a precondition. And now this last regular expression constraint is satisfiable.
00:32:47.954 - 00:33:13.262, Speaker B: And from that we can conclude. And that actually the whole program, the whole formula is satisfiable. This is easy to solve and. Right, so this is the overall idea. In ostrich, we are solving constraints using backpropagation. Here are some functions that we can support in this setting. So we've already seen that there's this lemma about regular expressions.
00:33:13.262 - 00:33:51.784, Speaker B: So we can about, sorry about concatenation. So we can immediately identify concatenation to be admissible. You can also handle things like replace and replace all, and in fact replace all, even when the replacement string is a variable. This is already quite powerful. So this is more than I think any other s and p solver apart from our solver. Ostrich can actually support and decide. We can very easily handle the reverse operation, and we can handle any functions that are defined by transducers, and those can even be two way transducers, even that can be handled here.
00:33:51.784 - 00:34:24.908, Speaker B: And we can also encode things like conversions between different formats or things like escape operations quite easily here. And. Right, of course, I need to show an example of a non admissible function as well. So for instance, a function that is converting a number from unary to binary representation. And so this is not admissible because the preimage of a regular language under this function is in general not regular, as can be seen quite easily. So here we can encode exponentials directly, of course. And.
00:34:24.908 - 00:35:04.791, Speaker B: Right, so this diagram essentially shows the ostrich procedure. I can maybe quickly go through this, but I think I've explained the main idea already. So in Ostrich we will start from a conjunction of function applications, and this is one half of the straight line program, and we will also have some regular expressions. And so these are initially called the active regular expressions because we still have to solve them. And then in each step we're going to pick one of the function applications and one of the regular expressions about the result of the function application. Then we're going to compute the preimage of the regular language under the function f. And so this will be a disjunction in general.
00:35:04.791 - 00:35:47.004, Speaker B: And so we're going to pick one of the disjuncts and focus on that and move that back to the active regular expressions. So essentially at this point we have to make a decision and then do follow a particular branch of the search space. And so the y becomes passive at this point, because we know that there is only a single function application producing the y. And so we don't actually have to propagate again for the same y. But later on we might have to check whether this y constraint is consistent with other active regular expression constraints about the y that we might get. So, right. So we pick one regular expression, we pick one equation, we compute the preimage, and then we recurse over the new regular expressions.
00:35:47.004 - 00:36:16.034, Speaker B: And each time we are adding a new regular expression. Of course we have to check whether it is consistent with expression that we already have. And if it is, then we just recurse and pick the next equation and the next regular expression. If there is a conflict, then we have to backtrack. And in the style of SMT, then we are going to compute a conflict set and we are going to back jump as far as possible and follow different branch of the search space. And. Right, so some of the steps in more detail.
00:36:16.034 - 00:36:47.340, Speaker B: So we have to check regular expression consistency. Well, that essentially boils down to checking where the intersection of regular languages, sorry. Yes, the intersection of regular languages is empty internally. This could be done by computing products, of course. So this is, well, in theory this is expensive, but in practice this is actually something that can be done quite efficiently. Then we have to compute conflict sets and those we are minimizing in a greedy way, essentially SMTO style. And also we have to compute pre images.
00:36:47.340 - 00:37:45.374, Speaker B: And those are essentially automata constructions we are applying. And so how do these look like for the admissible functions that I mentioned for concatenation, we have to do the splitting over automaton states and replace, and replace all, and can internally be reduced to later transducers. And. Well, that construction can be optimized in various ways. Reverse is very easy, because computing the pre image of reverse just means that we have to revert all the automaton transitions and unary functions defined by transducers can be handled using a product construction and projection. So we're projecting out the result to get the constraint over the input, essentially and right so this is essentially everything we need to solve the example that I started from. So this constraint can be solved within a couple of seconds by otool ostrich, of course, after we have encoded things like the escape operation in a good way.
00:37:45.374 - 00:38:42.594, Speaker B: And how do we encode escape? So it turns out that these can be defined as letter to letter transducers. So these are about machines that read some input string and produce outputs. So the automata working on multiple tracks. And so the transducer for encoding something like the HTML escape would essentially look like this. This is only a small part of the transducer though, because the actual escape will look at many more special characters. But essentially we start from the initial state, and each time we are reading one of these special characters, you're just replacing the character with the HTML code, and then go back to the hub, to the main state, and, well, that is all we need to actually solve this example. Right.
00:38:42.594 - 00:39:31.494, Speaker B: So time is running out, and there are various extensions that could be presented here. So I started from the straight line fragment, but of course it would be interesting to handle more general fragments of vertical questions. And for instance, one can observe that Vancouver can go to a slightly more expressive fragment like the acyclic fragment, where we can have compound terms on both sides of the equations. We cannot easily go to full vertical equations. And another very interesting direction is how we can add things like computing the length of strings and also porting operations like string at, which gives us one particular character in a string, or computing the substring. And so this combination of integer operations and string operations, this is very interesting. And so we had a paper about this at Aatva last year, and that is essentially generalizing the whole framework to also deal with integer functions.
00:39:31.494 - 00:40:12.164, Speaker B: Right. So then I'm actually staying within the time. So I did not talk much about norm, but I gave a description of the problem it is solving. And so, well, these are two decision procedures for particular fragments of string constraints, covering things like regular expressions, length, particular fragments of vertical equations, and also other functions that are admissible. And so in general, string slogging is a very exciting area. It's very active. There are many groups that are working on tools that are publishing each year at CAF.
00:40:12.164 - 00:40:34.024, Speaker B: They're going to be at least half a dozen of submissions about strain constraints and. Right. I also started from these research questions. And so all three of them are extremely interesting, and there's still a lot to do. So thanks a lot for your attention. I'm happy to take more questions.
00:40:34.404 - 00:40:57.752, Speaker A: Thank you, Philip. Thank you very much let me use my privilege and ask some of the few first questions and a question about the fragments. And I want to ask about two aspects of them, one like in first order logic, which are inside the bell. And we study fragments. We also look at what is the complexity of each decidable fragment.
00:40:57.888 - 00:40:58.608, Speaker B: Yes.
00:40:58.776 - 00:41:06.244, Speaker A: Is there a similar study here of the conversion complexity? And there are some result characterizing the complexity of each fragment?
00:41:06.754 - 00:41:18.674, Speaker B: Yes, of course, but it's something you would have to do actually for each fragment in particular. So I was thinking that you might be asking about this. So I prepared a slide.
00:41:18.834 - 00:41:21.066, Speaker A: Ah, very good. Unpredictable.
00:41:21.250 - 00:41:58.364, Speaker B: But now where is it? I cannot find it. Right. So these are some results for different fragments that you could handle with an ostrich. And so here the complexity really mainly depends on what kind of operations you can. And so, of course, if you only have regular expressions, the problem is going to be p space. And so depending on what kind of functions and what kind of transducers you're adding, well, you're going to get more or less terrible performance or terrible complexity. And of course, well, at some point you also end up with undecidable things like this combination of equals and replace all, of course, immediately would give you undecidable fragments.
00:41:59.444 - 00:42:38.432, Speaker A: My other questions about the fragment, and again, it's motivated by my familiarity with decidable fragments. First order logic is the question is, are these fragments useful? So in first order logic, turn out that one decidable fragment is useful, the bernation fragment is useful, the other fragments of theoretical interest only. And I have seen no application. You know, the Ackermann class, the Gedler class, I just don't know of any application of it. Now, you started by motivation for application mostly, I would say, from security applications. If we need to do analysis, do the desirable fragment match the needs that come from the application side?
00:42:38.608 - 00:42:54.436, Speaker B: Yeah, to some degree, but not completely. I think you're right. That's a very good question. Well, it's one of the research questions, really. So I think this constraint is not unrealistic. So this is actually coming from a real program. Well, it's a very much simplified program, but still.
00:42:54.436 - 00:43:28.914, Speaker B: And so this is ending up in the fragment that I've described. So I think that is one argument, actually, that the fragment is not completely useless, but it's very easy to also go beyond the fragment. So, for instance, when you do symbolic execution, when you execute an if and else statement, of course you will immediately get a positive and a negated version of the condition. The conditional and so that would immediately also give you negated equations, for instance. And so that would already push you beyond the fragment. So I think the fragment is not useless, but it's also brittle. I think you can easily cross the boundary.
00:43:28.914 - 00:43:36.154, Speaker B: But I would say that all these fragments are actually motivated by applications, so they're not just completely theoretical.
00:43:39.614 - 00:43:41.834, Speaker A: Other questions from the audience.
00:43:43.714 - 00:44:26.118, Speaker B: Philip, quick question, since you're already on this slide, you explain how do you encode HTML escape, but how do you model attacks? How do you model what exactly? Sorry, the attack. Right. Yeah, that's actually also a good question. And I think the typical answer would be that you assume that you have some kind of catalog of common attacks. So kind of, you need to have some domain knowledge, you need to understand HTML and you need to understand actually what string would be dangerous when you can construct it. And then this is something that you could, that you could define as a regular expression. But it's a good question.
00:44:26.118 - 00:44:52.894, Speaker B: So at this point, actually would need user input. It's a bit of a specification, actually. You would say that you're not supposed to be able to construct a string of this particular shape. So say if you are constructing SQL queries, for instance, you could say that actually, and whatever you want to get as a final query must only, well, it must not do things like delete, for instance, which is something that you could characterize as a regular expression. Thank you. Thanks.
00:44:59.154 - 00:45:15.822, Speaker A: Thomas, do you have any questions? I see a question on the chat from Thomas.
00:45:15.998 - 00:45:18.478, Speaker B: Okay, can you.
00:45:18.566 - 00:45:23.274, Speaker A: Hello, I should, yes, go ahead, Thomas.
00:45:23.654 - 00:45:53.712, Speaker B: Now I'm online. Can you quickly, you went over another slide where I quickly saw the class, NP aside, could you show it again please? This slide went over it very quickly. Which one was that? Actually? I don't remember it. Oh, this one? Yeah, yeah, yeah. Right, okay. Yes, right. So this is actually a whole different presentation.
00:45:53.712 - 00:46:53.046, Speaker B: So this was about, so when we have this fragment that we handle in ostrich, what kind of length constraints could be, and could we add to this fragment? And. Right, so actually, so one option that we considered was that we were investigating actually, which length constraints actually make a difference in the sense that they really require an extension of the fragment. And so it turns out that actually many of the constraints that you would add, although they talk about length in principle, they could just reduce to things like regular expressions. So for instance, if you had a length constraint like the length of x equals to three, well, that could just be turned into regular expressions with the x has to be in sigma times three and so on. And of course, then there are other constraints that you could not translate to regular expressions in the same sense. For instance, length of x equals to the length of y. And so this can be formalized as this notion of a formula being monadic.
00:46:53.046 - 00:47:32.474, Speaker B: So formula is monadic if it is equivalent to a formula over just monetic predicates, and a monetary predicate only talks about a single variable. And obviously each monetic predicate can then be directly turned into regular expression. And so the point about the NP completeness is, well, how difficult is it actually to decide whether a formula in preschool arithmetic is monadic or can be turned into this monadic form? And so, in an each card paper last year we figured out this is co EMP complete. Okay, this is also super interesting, but it will be a different presentation. Thanks. Okay, thank you.
