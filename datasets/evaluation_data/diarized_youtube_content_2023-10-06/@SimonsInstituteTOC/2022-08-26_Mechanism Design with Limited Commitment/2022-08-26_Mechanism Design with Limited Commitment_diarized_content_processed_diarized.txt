00:00:01.040 - 00:00:01.264, Speaker A: Yeah.
00:00:01.264 - 00:00:32.400, Speaker B: Okay, so welcome back. For those that did come back. So now it's aura and talking now about time with limited commitment. Thank you. So this talk will be more focused on, like, work that I have done as opposed to one last example since I've done. And it will be in its work that's joined with basilica. So let's go back to the.
00:00:32.400 - 00:01:20.444, Speaker B: So we've done a lot about how to represent information structures, and now let's think about how we represent games. So sometimes we are designers and we get to design games. Let's think about that for a little bit. So this is the slide from before. We have some private information that's in the hands of the agents and we get to choose what actions they're going to play in this game, how these actions map into outcomes. When I'm trying to think about what I can implement and whatnot, let me do it in notation that might be more familiar to people that have worked already with mechanism design, which is usually, we think that already a set of states of the world represents the information that the players have, usually. So basically what this side does is the identity.
00:01:20.444 - 00:01:54.860, Speaker B: So given theta, each player coordinates. And usually these action sets that we endow the players with will hold on messages. Because at the end of the day, what we are interested in is in aggregating this private information that they have. And because we want to use this information to determine what's the best thing we can do. So this is, in a sense, if you want to think about it through the lens of something, you know, this is a little bit like agent based learning. The information. The information is in hands of agents and you want to incentivize.
00:01:54.860 - 00:02:23.614, Speaker B: So the information that you're receiving might you have to take into account the incentives of the person that's giving it to you. And you're trying to overcome those incentives. Yes. Oh, yes. So a zero will be like the allocation. So basically, for instance, if I wanted to auction a good is who gets the good and how much they pay me. So these are the allocations that the designer is in control of.
00:02:23.614 - 00:02:55.126, Speaker B: But thank you. So basically, what we are trying to understand is what assignments of types to distributions over outcomes we can implement. Thank you. And this is because I was changing the slides last night, so forget about that. So basically, the idea. So these B ones and bns are my m one s through mns. So basically, the idea of mechanism design is we take as given the information structure, we're trying to design a game.
00:02:55.126 - 00:03:38.802, Speaker B: And as I said, the first order concern is that the information is in the hands of selfish players. And before we wanted to talk about all information structures, now we want to talk about all games. I'm going to call those games mechanisms. Why? Because someone a long time ago decided that these are going to be called mechanisms. And I don't have a better excuse on that. Now what mechanism design does, at least in static settings, is gives us a language to talk about all mechanisms via the revelation principle. So what is the revelation principle as a result? That goes very long, a long while back.
00:03:38.802 - 00:04:16.428, Speaker B: But basically what it says is the following. You can always write the following game. You can tell the players your message sets, and this is why calling them messages makes a lot of sense. Your messages sets are your set of types. I'm going to ask you to tell me your types. And let's say that I wanted to implement five. If you tell me that if I aggregate the reports of all the players and I end up with the vector theta prime, then I'm basically going to implement what I said I was going to do when the state is theta prime.
00:04:16.428 - 00:05:08.064, Speaker B: It doesn't, this doesn't mean that the true state is theta prime. It just means you told me that the state is theta prime. I'm going to trust you. I'm going to say implement PI. So basically, if I can find a game that has PI as an equilibrium outcome, if and only if I can implement PI as an equilibrium outcome of this game, where I ask the agents to give me their information and I follow blindly what they tell me. Moreover, if there is a game that implements PI, I can, without loss of generality, do it in a way where the agents will tell me the truth, not because they are not strategic, but because I will make it worth their while to tell me the truth. So I know that this sounds a little bit strange, but this is just function composition.
00:05:08.064 - 00:06:32.702, Speaker B: So basically what I'm saying is the players know their types, they can tell me messages, and as a function of their messages I will choose some a. And that's the a note that Adi was asking about. So the players are going to report something as a function of their types, and my game will take the messages and choose some a plus will be at random. And I'm saying that I know that this game implements PI. So that means that when I compose the reporting strategy with what I told them that I would do when they tell me their messages, this is exactly the PI. So this is the assumption, I'm assuming, that there is a game that implements that implements PI, which means that when I compose what they tell me in the messages with what I told them that I was going to do as a function of the messages, I get PI, but that means, and I'm pretty sure, that many of you know category theory way more than I do, that I can just ask them to tell me the truth and then do what I was saying before. And this triangle was known for a very long time in economics.
00:06:32.702 - 00:07:25.904, Speaker B: But the thing is that we didn't have game theory to formalize this. So this idea, this is called the Mount Ritter triangle, and it's a very, very old idea, but for a long time, people didn't have the language of game theory to formalize it, because the property that we care the most about is that if this was the agent's best response to this game, then telling the truth is the best response. So this is the incentive property that we want to preserve. So usually when this revolution principle type results have some equilibrium notion embedded in them. And when you move from this so called indirect mechanism to this mechanism, where you have to respect the types, you're preserving the solution concept. Does this make sense?
00:07:33.644 - 00:07:40.762, Speaker A: On the right, we're saying ID has the identity has to be the best response.
00:07:40.858 - 00:08:29.434, Speaker B: Yes, let me qualify that. The identity has to be our best response. There's a big literature by one of the authors in that paper that's got Haman Maskin, Eric Maskin pushed a lot, this idea of full implementation. So not only that, the identity, so that every equilibria of the game also implements PI. So if it was also an equilibrium, remember what we were talking about before that I might tell you to take an action, but you're indifferent between the two actions. If it's also an equilibrium, part of an equilibrium for you to take, the other action, for instance, may not tell the truth. All equilibria of this game has to implement PI.
00:08:29.434 - 00:09:41.892, Speaker B: But for today, we're going to do what's called partial implementation. So I need that truth telling is a best response. Any other questions? Okay, so what this result says is that, in a sense, this idea that we have strategic agents with information and that we're trying to learn it, the revolution principle, sort of makes that problem mute, because basically, so we're going to abstract from the decentralized learning process that ends up with the designer having information to implement the allocation. So basically, we're saying that without loss, we can focus on mechanisms where the people own information, they reveal it, but not because they are dumb, but because it's in their interest. So if it was, if what I was trying to implement, didn't give them the correct incentives to tell the truth, then I cannot implement that. So in general, you might have heard that there's distortions of the allocation or the rents that the agents receive in order to tell the truth. Now, having the revelation principle doesn't mean that all mechanisms are truthful.
00:09:41.892 - 00:10:27.526, Speaker B: So for instance, if you play the first price auction, please don't beat your values, because you might go home unhappy. And it's not necessarily that truthful mechanisms are better. So there's research showing that people, even if you're facing with a trustful mechanism, they don't understand that it's a truthful mechanism. How breaks news. But the obsession with truth telling and the revelation principle, because this will be a talk about being obsessed with the revelation principle, is that it gives us a recipe. I don't want to be thinking about this world where I'm going over message spaces and equilibrium. I want to think about this world where I have a fixed message space and where I have a set of constraints that ensure truth telling.
00:10:27.526 - 00:11:27.850, Speaker B: So the revelation principle allows me to think about a problem that's essentially an equilibrium problem into a constraint optimization problem. I have a well defined set of mechanisms, and I'm maximizing over the set, subject to constraints. So this is why we are, like economists in general, are obsessed with this, not because we think that the set mechanisms exist in the wild, but in order to know what I can do in the wild, I need to be able to write down the program. Mechanism design, I think has been very successful in the while. I took this from Anna Carlin's slides from the bootcamp in 2016, and I'm pretty sure that we can, there's even more applications. And one of the reasons is that this result makes it very easy to work with. Now let's go into something that looks more like the environments that we look at in the workshop, where things are dynamic.
00:11:27.850 - 00:12:19.732, Speaker B: So now imagine that I'm not choosing allocation only for today, but we're doing this repeatedly. So every day we hold an auction, or probably every other second or so we hold an auction. So we're thinking about repeated interactions, and we might be thinking that either the agent's private information is persistent, or it evolves over time. If I'm seeing, for instance, different requests for ad slots for different consumers, that's like my valuation evolving over time. Now, this is a very burdening area, both in Econ Cs and or, and the newest in parentheses, because actually people have been doing dynamic mechanism design since the eighties. So there was not like Myers and optimal options from 1981. Baron Vesanco, which is one of the first dynamic mechanisms in the same paper since 1984.
00:12:19.732 - 00:13:02.188, Speaker B: So people were given in theory and they just went to racist. And basically the idea now is that the designer needs to learn information that's not as in any sequential decision process. The information that the designer learns today might be relevant not only for today but also for tomorrow. So you might know that if you awarded the good to someone today, then they have a budget constraint. So tomorrow they might not be willing to be as high. You might want to get the good to another player tomorrow. The problem now becomes that the players start being concerned about what the designer learns today, because what the designer learns today, they can use it tomorrow.
00:13:02.188 - 00:13:51.080, Speaker B: So think about when you're online shopping. Sometimes you're like, should I take the most expensive products in Amazon? Well, maybe Amazon tomorrow shows you the most expensive product for the next category. And last time I was here at Simon, we had this game in the, we were sharing apartments with a couple other economists. We will open up our Uber apps. And it turns out that when I was living in LA, I had sort of told Uber that when I was on a seminar trip, I was willing to pay high prices, maybe because someone else was paying for the trip. But then Uber quoted me high prices, or like higher prices than the other people were in the same house. So you could imagine that forward looking agents, then they understand this.
00:13:51.080 - 00:14:39.260, Speaker B: So if you know that Google is going to adjust your reserve price today, tomorrow, by using your bids today, you might want to do something with your bids today. And this is in economics, this is called the ratchet effect because it can sort of unravel. You can end up in a situation where you don't want to reveal anything about you just because you're so worried about how this information is going to be used in the future. Now, if in a dynamic setting, we knew that the designer was committed to the mechanism, and by that I mean that designer set the rules. And now he learned that I really value the good high, but he cannot take it back. He already promised me that tomorrow I will get the good for $1. And now he knows that I value to get ten.
00:14:39.260 - 00:15:23.184, Speaker B: And they are like I told her, $1, so she's getting the good for $1. Then we will have again a version of the revelation principle where the agents are going to be reporting truthfully their new information each time to the designer. And we're going to be able to focus on truthful. There's a start here because equilibria, we have to start thinking about dynamic game solution concepts. But still there will be truthful equilibrium in the game. And again, we can focus world loss in truthful equilibrium of the game. And of course, what changes is how we pay the agents rents, who might be thinking about how we have to compensate them today for how we are going to use their information tomorrow.
00:15:23.184 - 00:16:28.664, Speaker B: And what happens is that sometimes these rents are high enough that the optimal mechanism, what it says is that the, even if I learned today the information that I needed, actually I'm not going to change the mechanism. So there's a huge literature in economics thinking about the determinants of price discrimination. For instance, you have a durable good that you're trying to sell, and you have buyers whose values are evolving over time. And the fact that if you want to lower the price at some point to serve unserved demand, the incentives that it triggers on players with higher valuations. That basically means that in many settings it is optimal to not drop the price. So a very classic result is that, and this is a paper from the seventies, is that if you want to sell a durable good and you have commitment, you need a lot of evolution. In a sense, you need the values to change a lot to be willing to lower prices if you have commitment.
00:16:28.664 - 00:17:10.804, Speaker B: So let's go a little bit. But the thing is that, that I think introduces a problem that relates to Ruchi's question about do we trust the designer or not? So let's think of a seller who wants to sell one unit of favorable good. He has two periods over which to sell it, and the buyer's value is going to be persistent. So if I value the good ten today, I will value it, attend tomorrow. And the optimal mechanism, as I was describing, is basically to set the same price in each period. And the price I'm going to set is the one that solves a monopoly problem in the first period. So that's the solution to the problem.
00:17:10.804 - 00:17:53.472, Speaker B: So what happens is that the designer is supposed to ask the buyer, do you want to buy the good at price p? The buyer checks and she realizes her value is less than p. Then she says no. We go into the second period, the designer is supposed to ask again for a price of p. And if the designer has commitment, he already knows the answer, because the buyer said no before. The buyer will say no now. But actually now the seller, you might think the seller now wants to think about this again, because he says, okay, I know that theta is less than p. And actually I know that everyone that has theta less than P told me.
00:17:53.472 - 00:19:05.664, Speaker B: Knowing in the first period I have a new demand function. And now that means that the designer might be like, I will offer a new price that will be lower than P because I know that everyone that's in the second period has none lower than p. And now what happens? The incentive properties of the mechanism go as cool because now let's think of the agent in period one that has a value higher than p, and now that anticipates at the moment that she says no, the price is going to lower. So now you will have some agents in period one that will say no. So if you're worried that the designer will use the information that they have learned to adjust the mechanism, then what you will lose are all the incentive properties of the mechanism. Does this make sense? Because basically the talk will be to think about how do we think about mechanism design and how we think about representing mechanisms when we're worried that the designer will react. So we'll take the information that they learn and they will do the best with it moving forward.
00:19:05.664 - 00:20:23.044, Speaker B: So yes. So no, that's great, because that means that I was confusing me, I shouldn't have separated this into slides. So under commitment, the seller should set p and not move it. But what I was trying to illustrate is that now that you know that everyone below pe doesn't buy in the first period, actually you're not happy with the mechanism anymore because you have a residual demand, and under that residual demand there's a new price that's optimal. So the commitment solution in this case requires that you ignore whatever information you learned. And actually, if you think about it from the point of view of the revolution principle, the designer in the first period learns data and now they know exactly how much you value the good and they could come back to you in the second period with a new offer and they don't. So basically the commitment assumption assets is very important in this particular setting.
00:20:23.044 - 00:21:17.804, Speaker B: And actually there's many mechanisms that have this feature. So it's not something else. I'm good. If you have dynamic ad auctions or you have repeated sales or you have procurement mechanisms, in these cases all have this feature that with forward looking agents you need to pay them so much if you want to use information in the future that you end up ignoring the information that you learned to compensate. So you distort the location a lot to compensate for the rents that you need to give them. So basically what we would like is a theory of mechanism design that does not rely so strongly on this assumption. That the designer will ignore the information that he learns in order and not optimize on it.
00:21:17.804 - 00:21:58.096, Speaker B: Now, the first issue is that there's only one way to have commitment, which is I announce the mechanism and I stick to it no matter what. There's many ways in which I could have limited commitments. And then I think here, this is one setting where enumeration is bad. So basically, there's many different ways that haven't been explored in the literature and economics. I think that the ones that we're most familiar with are short term mechanisms, which is like the setting that I showed you. If today I tell you the good is sold at p and you accept p, I cannot say, oh, actually, do you want. I regret it.
00:21:58.096 - 00:22:39.672, Speaker B: I would like to raise the price. So if I announce something today, I'm committed for today, but I'm not committed to what I'm going to do for tomorrow onwards. So this is a setting that in economics is very common because it models like sequences of government, for instance, and also all the papers that I cited before from CS and or are also in this environment. I think the other setting that is more popular is when you cannot commit even to today's mechanism. So Shen Wu presented his work with Mohammed when I was here in 2019. But there's other papers in this realm where it's basically, we run the mechanism. I see the outcome of the mechanism.
00:22:39.672 - 00:23:14.194, Speaker B: I'm like, I don't like it. I go back and I say, guys, let's do this other mechanism. And so, in a sense, these are like very opposites. What I'm going to focus today is on the first one. So basically what we're going to have is an uninformed designer that is going to interact with one agent that's persistently and privately informed. We can allow for the type to evolve over time, but it's easier to. To state the result without this.
00:23:14.194 - 00:24:00.632, Speaker B: And the designer can commit to today's mechanism, and he cannot commit to what the agent will do in the future. Sorry, what he will do in the future with the agent. And there's many examples that follow in this category. And despite this being something that you would think that people are sort of obsessed with, because it seems like a first order consideration, it's an incredibly hard problem. There's very few papers, both in cs nor and in econ that analyze optimal mechanisms in this setting. And this takes us to the second issue, which is that even when you narrow down what you mean by limited commitment, the reason we don't have a lot of results is that we don't have a revelation principle. And you can think about why the revelation principle might not hold.
00:24:00.632 - 00:24:48.974, Speaker B: At least the truth telling part is if you tell the designer your information now you know that the designer will go and use it, you might not have the correct incentives to tell the truth. So basically what, the result that I'm going to show you is how do we restore the revelation principle in this setting? And you will see that it has a lot of connections with the first stop. So it's, and it will look more like bayesian persuasion. Any questions where parameter can change daily or having a different mechanism, like is I guess, what's the definition of different ideas?
00:24:49.094 - 00:24:49.754, Speaker A: And.
00:24:52.894 - 00:25:33.188, Speaker B: So I don't think that my impression is that I could parameterize. I think that at that level of abstraction there's no difference. But the thing is that the difference is that I set a set of rules in period zero and they are supposed to be in a movable. And now the question is we saw the outcome of period zero. And I asked myself, do I like what I said? That I would say in periods one, two, three with commitment, even if you were like, shoot, now I should lower the price. You're not going to change it. That's fit.
00:25:33.188 - 00:25:46.704, Speaker B: So the thing that you said you're going to, in your series here, you're going to stick to it. With limited commitment, you could say, you know what, I'll re optimize my plan. Does that make sense? Yes.
00:25:48.244 - 00:25:51.894, Speaker A: If I can reoptimize in particular, I can keep strategies.
00:25:55.194 - 00:26:37.354, Speaker B: So the thing is, so I think that I guess this is difference between a strategy and a behavioral strategy. So at the end of the day, we're going to play equilibrium. And my strategy, I will stick to it because it will be an equilibrium. The question is, where do I question myself? So in commitment, my plan has to be optimal in period zero, and I never check again whether it's optimal in with limited commitment, you're going to check that the plan that's going to be fixed is optimal in period zero in period one, period two, and so on and so forth for every contingency.
00:26:39.014 - 00:26:57.194, Speaker A: So when you said that with commitment, the optimal strategy doesn't change the crisis, what that's saying is that even if the seller says, all right, here is my pricing algorithm, the fact that they would have to publish this pricing algorithm means that they can't actually achieve better than just keeping the price.
00:26:57.614 - 00:27:51.368, Speaker B: Exactly. Yes. So of course with commitment, you're always better off. No, and I want for this talk because it requires introducing more things that are not. Yes, it will complicate more than clarify, but we can get the same results, a similar version of the results with evolving types, no? So there could be persistency. So I think so. Elon has a very beautiful paper on repeated sales with IID types.
00:27:51.368 - 00:29:13.494, Speaker B: And then there's. So there's two ways in which commitment can bite you in this setting, or lack of commitment can bite in this setting. One is that if what you learned today is useful for tomorrow, now you might want to use it, but the other way where commitment bites, which is, I think before I read Elon's paper, I hadn't reflected on this is the, in a dynamic setting with commitment, if the agent said, I want to sell your good, the first moment you tell me, no, I don't want the good, or I don't want to participate in your mechanism, I can be like, we are not talking anymore. I might be very tempted to go and retry sending you the good, but I will be like, I don't want to listen, but with limited commitment, if I know that there's an opportunity for trade, I will keep coming back to you. So even if your values are IID, which will mean that, for instance, all these issues about the revelation principle will be mute, because what I learned today has no implications for tomorrow. There's another part where commitment will bite, which is, or lack of commitment will be, which is that now your participation constraint looks different, because you know that maybe you say no to today's mechanism and tomorrow I will come back to you and be like, okay, let's play this other game. Whereas with commitments, I won't have that.
00:29:13.494 - 00:30:24.804, Speaker B: I will say, the first time you say no to me, that's it. Which gives the designer a lot of power. Nicola, don't you have like four different types of results? I don't know. But in the other extreme, with commitment, with full commitments and IID types, I have to leave you some runs for today, but I can extract without something like expose sequential IR, I could extract all your rents moving forward, because in period t, you and I are as informed about what will happen from t plus one onwards. So I can charge you exante for your rants. So I think that that's sort of like the setting that Elon and Renato were comparing themselves against, that with commitment, you have a lot of power in that setting, and the ability to, this inability to keep coming back to the agent sort of induces something that looks like a sequential IR. But I don't know about the fault here, but it would be questions.
00:30:24.804 - 00:31:00.544, Speaker B: All right, so in order to basically tackle this question, I need to be a little bit more abstract about what I mean by a mechanism. So when, when we went over the regulation principle, I gave you like the standard result, but there's a lot to unpack. So the way we're going to. So the way to think about a mechanism, even in the Myerson setting, is the following. The mechanism has two set of messages, a set of input messages. These are what the agent reports, and a set of output messages, the signals. And this is what the principal will see.
00:31:00.544 - 00:31:49.922, Speaker B: So basically the way the mechanism works is the agent sends a message, and I hope someone finds my use of agent hilarious. And then he privately sends a message. The mechanism takes a message, randomizes over the signals and the allocation and the principle. What he sees are the signals and the allocation. So this is the public outcome. So if you think about the revelation principle that I told you before, basically what it says is that in a setting where the agent has no actions to take, not like the first lecture today. So in a setting where the agent only has private information, communication between the principal and the agent will be directly between the agent and the mechanism will be direct.
00:31:49.922 - 00:32:26.278, Speaker B: So the agent will report a type. It is real loss to think that the mechanism can make the communication observable so that each s maps only to one m. So the principal, when he sees the s, he knows the m. And the reason is that with commitment, the principal can ignore the information he learns. And then it is without loss to focus in the case where communication is truthful. So when the principal sees the s, not only he learns the agents type report, he learns the agents should type. Now, of course, this is the source of the issues.
00:32:26.278 - 00:33:29.956, Speaker B: We're trying to take this result to the case of commitment because we already discussed at length why what happens, why the agent would like to lie, if the principal will react to the information. So there's a very nice paper from the two thousands by Bestron Strauss. And what they do is they keep the assumption that communication is observable, so that when the principal sees the output message, he learns the input message. And what they show is that if you wanted to characterize the principal's best mechanism, then without loss you can ask the agent to report the type. But something has to give and it will be truth telling, which means that you lose the beautiful characterization of like now when you're thinking about optimal mechanisms, you have to carry around the agent strategy because each mechanism has a different truth telling constraint. For the agent, it's not clear what lie you want to tell. So this is, and this is very related to an observation in.
00:33:29.956 - 00:34:23.714, Speaker B: So this is the paper on complexity of dynamic mechanism design. The last section, they're thinking about repeated sales without commitments. And they have this example where you might need like many rounds of communication when there's limited commitment. So this lack of when communication is direct, it's not truthful. It's very related to this idea that you might need to communicate many rounds between the principal and the agent in one period to finalize the allocation. Now moving on in time, they have the same authors, they leave this restriction. So they're like, let's look at a larger class of mechanisms, but they look at a one period setting and then they show that we can make communication direct between the principal and the agent and make equilibrium communication truthful.
00:34:23.714 - 00:35:37.624, Speaker B: So you can think that basically. So the paper that I'm sort of describing with Vasiliki, basically what we are going to do is characterize what the set of output messages is, if you like. That's the most simplistic view. So basically, and this should be very familiar from what we talked about this morning, what we're going to show is that if you want to label your signal some way, they're going to be the beliefs that the principle holds throughout the agents type. So basically we have the recognis as the agent to report the type, and the principal will learn whatever he needs to learn about the agent moving forward. And the beliefs that the mechanism outputs, like the literal information structures that we saw this morning, are exactly the beliefs that the principal will have when he does base rule using the mechanism under the assumption that the agent is telling the truth. So basically what this says is that with limited commitments, you can always think of mechanisms that take the agent's type and maps it into distribution, or posterior and allocations.
00:35:37.624 - 00:36:52.624, Speaker B: But the result has one more simplification in this class of mechanisms that I think it's easier to do on the board. So let me be very informal and say, so the mechanism, what it says is for each agent type there's a probability of getting a certain allocation and a certain belief. And I can always do law of total probability and write this like first I will draw a belief given the agent type, and then I will draw an allocation given the belief and the agent's type. So what the result says is that in equilibrium the principal will use mechanisms where the allocation is drawn independently of the agent's type, conditional the belief. So what the mechanism is effectively doing is designing a black wall experiment, and then conditional on the belief is assigning the allocation. So when the principal sees the allocation, he hasn't learned any more than what he learned by seeing the belief about the agent style. So basically, in abstract terms, what this is saying is that if the principal wants to give each type of reagent a different allocation, well, he has to learn the agent's type.
00:36:52.624 - 00:38:12.854, Speaker B: There's no way of like, if the allocation separates the agents, the mechanism separates the agents types. And otherwise, if you want to pull types together, all those types have to get the same allocation. And because of this feature of like the Blackwell experiment and the location rule, we call these direct Blackwell mechanisms. So it's kind of good that tomorrow there will be all the talks with about David Blackwell's work, because today we have seen twice an application of his idea of how to write experiments. Okay, so just to give a simple insight of what's going on, when we started talking about static mechanism design, what we were trying to replicate is how each type maps into an allocation. We had that pie and we were trying to match that distribution. But here, because we also have to keep the principal happy with the mechanism he chose, we also need to be replicating how each type maps to the different beliefs that the principle has.
00:38:12.854 - 00:39:24.416, Speaker B: And so the first thing we did is, the first thing is like what I showed you at the beginning of the second lecture with mechanism design. And the second thing is what we were doing in the morning with information design, this belief approach. So instead of this, what the result is saying is that instead of designing the mechanism for a given information structure, what the mechanism is doing is designing both what we used to call the mechanism, the mapping from types to allocations, but also information structure. So you're designing both how Google is running the auctions and how the information that it just learned about the builders is used to update the prices. And if you think about with what we did this morning, what's happening here is that from the perspective, so we said that the principal imperial t has commitment to whatever he's doing in period. And he is concerned about two things. He has to elicit information from the agent, but also he cannot control what's happening from period t one onwards.
00:39:24.416 - 00:40:25.004, Speaker B: So the principal in PODC wants to keep the price p today and tomorrow and the day after tomorrow. But this guy over here might not be comfortable with keeping the price constant if new information is learned. But what the principal in period can do is sort of control information that the principle imperial t one does. So in a sense, the principle imperial t is designing the information structure for this guy, because this guy uses the information structure to the information. He learns to do the mechanisms in c one and so on and so forth, and also to elicit the information from, from the agent. So this is why we get, this is why we have a problem very similar to this morning. Now the question is, this morning we saw you can write information structure in terms of beliefs or actions.
00:40:25.004 - 00:41:08.054, Speaker B: Here actions are very complicated because actions are mechanisms, and they are mechanisms for tomorrow, the day after tomorrow, the day after tomorrow. And the principle from the day after tomorrow is playing a game with the agent. So you might think that there's multiple equilibria. So you're not only saying what mechanism you should choose, you should say what game, what equilibrium you should play, so that it could be very complicated. So in a sense, it's sort of magic that you can collapse that just by collapsing everything to the belief you can get this to work. But this is a setting where using actions, action recommendations will be very difficult. And that's why we use Philips.
00:41:08.054 - 00:41:10.174, Speaker B: Yes.
00:41:12.674 - 00:41:15.534, Speaker A: The agents are not learning anything, right?
00:41:17.984 - 00:42:41.080, Speaker B: No, but the principle is learning about the agent. So now you might imagine that you want to, you want to slow down the learning decomposers into a series of direct labels. Yes. So basically, if you wanted to think about what mechanisms to restrict the principle to, these are the mechanisms, and like before, you will have truth telling from the agent. So that means that now we can remove again the agent from the equation instead of thinking about what the agent will do when I show him a new mechanism. So, yeah, so basically that's the one thing that I wanted to highlight, which is that now again, we're going to have these two constraints on the mechanism, which is that the agent has to be willing to truthfully report, and that the mechanism has to match the principles equilibrium Williams. But once you have these two things, basically the agent becomes like constraints that the mechanism has to face.
00:42:41.080 - 00:42:52.724, Speaker B: The problem is that you still have the game between the principal and essence, his future self, this reaction, but that one is that one I don't think it's possible to get rid of.
00:42:56.624 - 00:43:07.014, Speaker A: Okay, so my question is, if I remember, Vasiliquius credits paper correctly, what she does is something like, she does a reduction to like using the utility space, right?
00:43:07.354 - 00:43:11.626, Speaker B: The outcome space. So basically what she says in my.
00:43:11.650 - 00:43:13.694, Speaker A: Question is this a generalization of that.
00:43:15.234 - 00:43:50.126, Speaker B: So the thing is that, so basiliki, in her paper, what she's using is the mechanisms for communication is observable between the principal and the agent. So in a sense, by looking at the space of allocations, then you don't need to worry about like how changing the mechanism changes the agent's best response. But we have, so in the setting of her, of her job market paper, we show that with these mechanisms now the principal can potentially do more. So they are not the same. Yeah, sure.
00:43:50.230 - 00:43:52.594, Speaker A: Yeah. So I'm just trying to add.
00:43:56.144 - 00:43:56.432, Speaker B: In.
00:43:56.448 - 00:44:13.912, Speaker C: The example you gave earlier, there was one. So can the mechanism not allow itself to learn anything from the first period, or is there some restriction?
00:44:14.008 - 00:44:26.394, Speaker B: No. So the print. So the principle always is the allocation. And so, because otherwise you're right, like if I don't see whether I sold the gun, I don't want to, I can restore commitment.
00:44:27.214 - 00:44:32.114, Speaker C: So the principal sees the allocation. Is there any reason to see more information than this?
00:44:35.654 - 00:45:39.650, Speaker B: So in the papers that we have worked on, no, but we are always in these settings where like sort of this idea of the ratchet effect kicks in, that the agent is worried that the principal learns things and then extracts runs. But you could imagine that maybe the information that the principal learns today is useful for both the, there's an efficiency motive, but economists like this, sort of like principal versus the agent settings. So I don't have an example of that. But you might imagine that you want to encode more information. I thought I saw another hand. So I was going to show you an example of like how you will derive this, but I won't because I have like ten minutes and it will be too much. We have a bunch of applications because the framework is very general, so we don't have restrictions on the cardinality of the set of types or the payoff functions.
00:45:39.650 - 00:46:48.314, Speaker B: So we have a bunch of applications where we have looked at what are optimal mechanisms in infinite pricing settings to sell an optimal good. Or we have a paper where we study how you design your product line. If you're concerned that once you learn that the buyer likes high quality goods, and then tomorrow you're going to price discriminate them. But in that setting, there is a concern that knowing the buyers type also allows you to personalize the goods. So there's an efficiency motive for learning information. So given that I want to do the example, let me just give two remarks and then I'll comment on a bunch of open problems in this area. So why would you think about talking about this problem in the context of data driven decision processes or algorithmic game theory? The first is that one of the reasons for the existence of this literature, even though the economic problem of like trusting the designer is very relevant is that people were hoping to make mechanisms simpler.
00:46:48.314 - 00:47:27.896, Speaker B: So, you know, you make it very costly for the principal to react to the information because the agent goes on and punishes him. So then the principle would become more, less reactive to information. So for instance, you could think about like non clairvoyant mechanisms. Those mechanisms don't use future information. So then all these issues won't be present. But if you think about the representation that we did for the problem, basically we are now very, we are describing how the principal is timing his learning about the agent. So it becomes this single agent decision process where the principal is deciding when and how he's going to learn about the agent's type.
00:47:27.896 - 00:48:16.434, Speaker B: Usually those problems from a patient setting have complicated solutions. So this literature, in terms of trying to derive simpler mechanisms, did not succeed a lot, but this was one of the, of this literature, and then more in terms of what can be done. I think that the representation that we get is very useful when you're thinking about learning algorithms. So if you think about what's happening in Yash's paper, or what solor is describing his paper with Nika. So you have these agents that are facing a learning algorithm, and all of a sudden. So for instance, let me use the example in Yashi's paper. The reservation price tomorrow depends on what I bid today.
00:48:16.434 - 00:49:47.644, Speaker B: And then I'm going to do something to my bid beyond what I would usually do, because I'm in a dynamic setting just to slow down the principle learning. And both these papers, basically the solution embeds some form of slowing down the learning of the algorithm to prevent the agent from overreacting to how the algorithm is using information, which you can do if you have commitments. But in a sense, you can think that once I set a learning algorithm, and if I didn't think ahead about how this learning algorithm is using the information, the algorithm is basically doing the best it has with information that it receives. And this is what, well, I think this is called strategic overfitting in the literature. But I think that the representation we obtain is very relevant to think about what learning the incentives that learning algorithms inducing these agents, because the algorithm will take the role of our sequentially rational principle. And now what our representation says is that if you have, it gives you, if your algorithm is patient, then it gives you a way of representing the outcomes of the strategic interaction between the principal, between the agents and the algorithm. And then you can give the Google, Amazon or whoever it is, a way to compare different learning algorithms, just by being very faithful about how the algorithm uses the information about the agents.
00:49:47.644 - 00:50:39.464, Speaker B: So let me jump to last slide. So thinking about how to represent mechanism design. So mechanisms in the case of limited commitment was a long standing question in economics. So that means that there's a lot to do, and the most glaring one is that the result that I showed you is approved for a single agent. So there's many counterexamples out there for the results. In the paper that I mentioned from the two thousands about how those mechanisms generalized or not for multiple agents, our results survives these counter examples. But that doesn't mean that we know what mechanisms are without loss.
00:50:39.464 - 00:51:55.564, Speaker B: And with multiple agents, there's this question of how to aggregate multiple the information. And there's this paper that Alex Thomas told me about helper and t, where one agent might not want the other agent to learn the information. So there's impossibility results in doing this. And Sid told me today about another paper in that area that sort of has this feature of requiring many rounds of communication. Now, more practical is how do we implement. So now we have a tool, at least for single agent, or if you have a continuum of agents, to think about limited commitment, how do we implement them? How do we implement the black box mechanism? So do we need multiple, maybe infinite rounds of communication between the principal and the agent, like the Papadi mitro paper? Or would cryptographic commitments like Pereira and Weinberg have proposed for credible mechanisms in the case of Akbar Purandi would also work in this setting? And there's like a lot of applications to worry about. So this is a very open field.
00:51:55.564 - 00:52:04.804, Speaker B: And with that, I'll say thank you, because you've been very patient with me throughout the whole morning, and I'll take questions.
00:52:13.584 - 00:52:30.206, Speaker A: So I'm thinking back to the, just the one, the two period buyer, seller. It seemed like the sort of the mechanism of there's a price posted and the buyer says yes or no. Is that like in some sense the sort of minimal communication of belief, like.
00:52:30.230 - 00:52:34.286, Speaker B: They sort of have the most, and.
00:52:34.310 - 00:52:39.154, Speaker A: What is the mechanism? And if so, what's the mechanism that would have sort of give the seller more information?
00:52:41.374 - 00:53:17.534, Speaker B: So I would say that actually, and this, and this goes back to the paper Elan was referring to from Basilica's paper, I think that the posted price has the least information. So if you will observe the communication between the, with the buyer, then deposit price has the, in a sense, it uses information more efficiently. Like if you, if you try to do something where you learn a little bit about certain types and a little bit less about other types like this truncation sort of is an efficient way to learn for the principle in terms of the agent strengths.
00:53:18.744 - 00:53:24.564, Speaker A: When you say efficient as in it is, as in it gives the seller the least.
00:53:24.864 - 00:54:04.892, Speaker B: No, the best. So basically, in Basilica's job market paper, what she was showing is that even with limited commitments, posted prices are still the optimal mechanism in a different set of mechanisms that we were looking at. But basically what will happen is that you will, your prices will decrease over time. But a big part of the construction is sort of showing that there's for any belief that you might have in the second period. Actually, the truncation one is the one that, from the perspective of the first period, leaves less rents to the agent and more value to the principal, at.
00:54:04.908 - 00:54:05.724, Speaker A: Least to the agent.
00:54:05.804 - 00:54:06.464, Speaker B: Yes.
00:54:20.054 - 00:54:23.274, Speaker C: You are still committing to what information you show.
00:54:27.814 - 00:54:32.434, Speaker B: That's why I think this is very important.
00:54:34.434 - 00:54:42.066, Speaker C: Is there any work on thinking about like, maybe I don't even need to check everything that you show me in. Sometimes I need it just to make.
00:54:42.090 - 00:54:42.654, Speaker B: Sure.
00:54:45.474 - 00:54:52.134, Speaker C: There'S some easier way to just verify approximately that principle is doing the right thing.
00:54:52.674 - 00:54:53.734, Speaker B: You mean that?
00:54:56.034 - 00:55:20.636, Speaker C: So the output of these mechanisms could be very. I need it because I'm worried about maybe there's some, like if I don't have. Is there some way to just verify that I'm somewhat certain that everything has been revealed and so I can go.
00:55:20.660 - 00:56:15.144, Speaker B: Ahead or that's exactly what had to be revealed. So I don't know for this. I actually don't know. Like, well, you might know better than I do that whether, like, even if with a mechanism, like someone runs an auction and I'm told I lost, are there ways of like, checking they run the auction correctly? I don't know. The way you phrase the question also made me think this is like for the other extreme case that I described, where you are not committed even to today's. And I think that that's what this paper is trying to address, how you can use cryptographic commitments actually to do like two rounds of verification that the principal actually did what he said he was.
00:56:30.984 - 00:56:32.608, Speaker C: And that just looked like the.
00:56:32.696 - 00:56:37.884, Speaker B: Best responding constraint and constraint that the belief should follow.
00:56:41.824 - 00:56:44.248, Speaker A: The question is that there's some similarity.
00:56:44.296 - 00:57:34.674, Speaker B: Between Docker activation and perfect project with. So I'm not sure I understand that. So our construction really relies on the solution concept that we use in the sense that we know that the principal will best respond to his beliefs and that whatever the agent was doing before was optimal. So when we change the mechanism and we make him to let roof. He still has incentives to toilet truth. So it is relying heavily on like the sequential rationality imposed by perfect basin equilibrium. But I'm not sure, I'm not sure if I understand that this is like following the representation of perfect basin equilibrium.
00:57:34.674 - 00:57:40.554, Speaker B: You mean your result is in.
00:57:43.914 - 00:57:44.274, Speaker C: You.
00:57:44.314 - 00:58:16.184, Speaker B: Obtain your result from the definition of yes. So the solution concept that we use is perfect bayesian equilibrium. So as I said before, like every revolution, principle result has a class of mechanisms and a solution concept. And you want this to be closed. So when you go to a simplifying class, you want to be still in the same solution concept. And we are relying quite heavily in that. We are like asking a lot of sequentiality, both from the principal and the agent.
