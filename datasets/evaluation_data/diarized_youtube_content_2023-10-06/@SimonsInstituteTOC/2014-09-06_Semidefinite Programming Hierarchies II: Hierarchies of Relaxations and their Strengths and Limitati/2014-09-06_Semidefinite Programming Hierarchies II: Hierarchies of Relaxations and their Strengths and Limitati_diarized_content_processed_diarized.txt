00:00:00.160 - 00:00:11.834, Speaker A: Hold them. But people are showing up. Oh, yes, yes.
00:00:20.334 - 00:00:29.946, Speaker B: Okay, don't. Have you first suggested this definition of.
00:00:29.970 - 00:00:31.930, Speaker C: Knowledge to those philosophy departments?
00:00:32.002 - 00:00:35.374, Speaker B: Ah, no. It would be interesting to know what they say.
00:00:38.194 - 00:00:41.814, Speaker A: They can't refute k denoting knowledge.
00:00:42.154 - 00:00:45.894, Speaker C: No, I think the denotation choice, I think they.
00:00:46.274 - 00:00:54.734, Speaker A: So anyway, this is David's second talk, and I guess he needs no introduction since he was introduced yesterday. So go ahead.
00:00:55.514 - 00:01:22.214, Speaker B: Thank you. In this lecture, we will use these serial distributions and the convex relaxations that they correspond to in order to design algorithms for constraints and selection problems. And in particular, that's motivated by the Uni games conjecture. And the unique games conjecture is a conjecture about the following problem.
00:01:28.374 - 00:01:34.314, Speaker A: Okay, my job.
00:01:47.214 - 00:02:33.682, Speaker B: I think it was designated, it's a promise problem that has a parameter epsilon and. Okay, so how does it work? The input consists of variables x, one up to xn, and they take values. These variables are over an Alphabet consisting of numbers from one to k. And then we are, the three markers on.
00:02:33.698 - 00:02:35.094, Speaker C: The four dime are triangular.
00:02:38.834 - 00:03:34.466, Speaker B: Is there a consistent color? We have a system of equations, and we have a system of equations in these variables. And these equations are very simple. And these equations, you know, you ask them to be satisfied. Modulo modular k, larger.
00:03:34.490 - 00:03:36.042, Speaker A: Okay, I think they don't see that.
00:03:36.138 - 00:03:58.690, Speaker B: Okay, sure. Thanks. And. Okay, and this, this prop, this instance, this input. Yeah, it depends on. Yes. Ij and c range over in j, range over one to n.
00:03:58.690 - 00:04:26.070, Speaker B: And c. Ranges can be anything between one to k, but it's. Yeah. So maybe one way to visualize it is that here you have the variables, then you have edges, and between the variables and they are labeled by numbers c, indicating. So that's a way to encode C.
00:04:26.102 - 00:04:28.214, Speaker C: Is c sub ij need not be the same for all.
00:04:28.294 - 00:04:29.594, Speaker B: Yes, yes, yes.
00:04:33.254 - 00:04:41.734, Speaker A: Why is it c I j? It doesn't depend on I and j. It depends on the specific equation, right? Yes.
00:04:41.774 - 00:04:44.646, Speaker B: Yeah. So there are different ways of, different ways of I and j.
00:04:44.750 - 00:04:45.834, Speaker C: They are all varying.
00:04:53.434 - 00:05:55.904, Speaker B: C can be one, three, five. The instance comes with a promise. And the promise is that there exists a good solution. A good solution means here there exists a solution x that satisfies one minus epsilon fraction of the constraint. And your goal is to find a solution that satisfies, let's say, 0.9 of the constraints. And what the unicames conjecture says is that this problem is empty hard.
00:05:55.904 - 00:06:48.254, Speaker B: This is the conjecture. This problem can be hard for every epsilon bigger than zero. And there's one aspect that I'm ignoring. So k here, the way I brought it is sort of part of the input. So it means that you are considering algorithms that should work for all k. But actually you're only interested in k are not too large. So k, let's say, is less than log n.
00:06:48.254 - 00:06:59.534, Speaker B: So that's sort of an addition. You can think of it as an additional promise on the instance. And then the conjecture says that even with this additional promise, it still might be hard to solve this problem.
00:07:00.034 - 00:07:06.654, Speaker A: Typically, what you just stated can't be true, right. For a large epsilon t.
00:07:11.274 - 00:07:14.386, Speaker B: Why? I mean, if Epsilon is, let's say.
00:07:14.490 - 00:07:18.930, Speaker A: One, then you know that you can't satisfy any different strings.
00:07:19.042 - 00:07:33.874, Speaker B: No, no, no, but I mean, you know, at least. You mean Epsilon greater than maybe. I mean, okay, because at least one minus s. No, but I mean in some sense. Yeah, you know.
00:07:34.814 - 00:07:35.674, Speaker A: Okay.
00:07:45.534 - 00:08:25.982, Speaker B: Right. So maybe I wanted Epstein is smaller than 0.1, just so that it's compatible with this 0.9 here. But a problem that you can't solve is maybe automatically NPM at least. Ok, and that's the conjecture. And just to get some intuition about it, with this problem, what happens if Epsilon is equal to zero? So that's not considered by the conjecture, but it's an interesting case to look at.
00:08:25.982 - 00:09:32.290, Speaker B: So then it means that you can solution satisfies all of the constraints. But in that case, just by some kind of gaussian relation or some propagating these constraints, you can find a solution that satisfies all of the constraints. So that's easy. Similarly, if k is small, let's say k is equal to two, it turns out that this problem is essentially like Max cut, because the constraint and then c can be either, let's say zero or one. And then the constraint either says that you should assign the same label, or you should assign different labels to the variables. And that's essentially next cut. In particular, the algorithm that I mentioned, or that we saw in the last lecture, with this promise, it would find a solution that satisfies one minus root epsilon of the equations, which is bigger than 0.9
00:09:32.290 - 00:09:59.194, Speaker B: for the kind of epsilon that we are interested in. Is there something else? Okay. It has really interesting implications. And so therefore, we're trying to understand if this contexture is true and what DMC is this known for every fixed.
00:09:59.574 - 00:10:00.782, Speaker A: Or is k equal to?
00:10:00.838 - 00:10:22.854, Speaker B: Yes. So in general, if k is less than two to the minus one over epsilon significantly less than that, then you can solve this problem to the one over. Yes, essentially by similar algorithm then given.
00:10:26.474 - 00:10:27.334, Speaker A: Okay.
00:10:29.994 - 00:11:48.550, Speaker B: But here, you know, here can be as large as log n. And so if n is large enough, which is sort of the kind of thing that you're interested in for this complexity things, then you decide whether it will fail. And, okay, what I want to show is an algorithm that solves this problem in time somewhat better than two to the n, two to the n would be, or k to the n would be the three bit algorithm. So it would solve the problem in time two to the n to the epsilon to the one. So one third is somewhat of the main, not the most important. So n today have an epsilon aspect. And what's maybe somewhat, how does it relate to the lecture? It relates in the following sense, that many empty hardness are just proofs.
00:11:48.550 - 00:11:50.114, Speaker B: They rule out these kind of.
00:12:01.944 - 00:12:03.684, Speaker A: Problems. Problems.
00:12:06.024 - 00:12:10.912, Speaker B: Also for promise problems, or is that question.
00:12:11.088 - 00:12:13.084, Speaker A: Yeah, I'm asking if there.
00:12:15.744 - 00:12:47.594, Speaker B: Yes, for example, if you would have here three variables per clause, and you would just try to satisfy, and you have this promise, you just try to satisfy more than one over k of the constraints, then for this problem, the NP hardness proof puts out. I don't understand that lesson. This one.
00:12:47.674 - 00:12:48.334, Speaker A: Yeah.
00:12:49.474 - 00:12:52.962, Speaker B: Do you say that people approved things? Yes.
00:12:53.098 - 00:12:55.706, Speaker A: Harness results for which they need to.
00:12:55.730 - 00:12:57.854, Speaker B: Have the values of epsilon.
00:13:01.114 - 00:13:02.538, Speaker A: This is under some assumptions.
00:13:02.586 - 00:14:10.114, Speaker B: Yes, it is. So the assumption is that NP is not a subset of. You can't have this kind of algorithm for NPR problems. It also stated that this kind of algorithm rules out many NP hardness points. Yes. So it means that these kind of reductions, which are, for example, gadget reductions from label covers of the typical kind of hardness reductions, or these promise problems, they usually reason why they rule out this kind of algorithm is that the reduction runs in linear time, nearly near linear time. And that's under this assumption roots out this kind of model.
00:14:11.094 - 00:14:14.062, Speaker D: I mean, it's enough for the direction to learn in fixed polynomial time.
00:14:14.118 - 00:15:49.452, Speaker B: Yes, that's right. Okay. And yeah, but we would see this algorithm using pseudo distributions. But the way we, the underlying techniques, they are also useful for constraint satisfaction in general, especially constraint satisfaction problems that involve two variables per constraint. Now, so we want to use these pseudo distributions and to solve this problem first, let's write this problem a bit more concisely. Let define, let the value of an assignment x be the fraction of, of equations satisfied, equations satisfied by x. And, you know, then the problem that we're trying to solve is to maximize over all possible assignments the value of the.
00:15:49.452 - 00:16:34.414, Speaker B: Simon. So that's the optimization problem that we're trying to solve. Now, one question is, where are the polynomials here? So, last time sort of relied on some are talking about polynomials. And then, so here, this. At first, it doesn't look like a polynomial, but turns out that there's sort of a generic new school transformation to make these things into polynomials. Basically, the idea is to change the cure. These variables are not.
00:16:34.414 - 00:16:49.898, Speaker B: It's not so good to think about them as real valued variables. And so we will introduce new real value variables that express in a sort of polynomials in these variables.
00:16:49.986 - 00:16:52.154, Speaker A: You want to write value of x as a polynomial.
00:16:52.234 - 00:17:22.519, Speaker B: Yes, yes. But not in. Confusing thing is that you shouldn't. So, here, the numbers one to k, you shouldn't think of them as real values, but, you know, so of some symbolic. Yes, some symbolic labels or a finite group. And so some of you want. Now you want to introduce, reevaluate variables over the reals in a natural way.
00:17:22.519 - 00:17:28.043, Speaker B: And so just write this polynomial.
00:17:30.564 - 00:17:35.812, Speaker A: So by this, you mean choosing a random equation from the set of equations.
00:17:35.948 - 00:18:08.404, Speaker B: That's right. So to be average over the, over the, over the equations. And then we want here a function that just tells us whether this particular equation is satisfied by x or not. And if you look at, if you. So here, this notation means, you know, it's one. If this is satisfied, zero. Otherwise j is equal to a plus c.
00:18:08.404 - 00:18:25.504, Speaker B: So this is one way of, you know, decomposing this one way of expressing this function. And now you can think of this here as a minus a. Shouldn't you divide this by k a minus c?
00:18:26.764 - 00:18:27.504, Speaker A: No.
00:18:29.484 - 00:18:54.804, Speaker B: So, you know, there can only be one term that is one c for all equations. I guess maybe someone mentioned that indices are allowed to vary. But.
00:18:58.104 - 00:19:01.072, Speaker C: CAn you actually explain that equation again?
00:19:01.208 - 00:19:53.814, Speaker B: What is the expected value? What is the distribution? So, here, you know, when we say, you know, we want to satisfy this fraction of constraints, so then, you know, the way we measure fractions is. So if you give every equation the same weight. But it's not like we have an equation for every player. So, that's a system of equations. You PicK a random equation, and so here we just measure what's the fraction for what fraction of equations is this? This can be either zero or one. And so this measures for what fraction of equations is this one? And so the way to think about. So why is this a polynomial? So this is a polynomial in, you know, degree.
00:19:53.814 - 00:20:54.834, Speaker B: This is a degree two polynomial in the variables. Xiaomi. So this gives just an example. But so this gives an degree of degree for functions, for the kind of functions that we are looking in general, not just for this particular function, but in general, at these kind of functions. And now we have a notion of saying, we have a notion of saying what it means that this is a degree D polynomial. It will mean that it's a degree D polynomial in these terms.
00:20:55.814 - 00:21:02.584, Speaker E: So you're going to introduce a lot of new variables. Is it going to be xi one up to x I k?
00:21:02.924 - 00:21:09.860, Speaker B: Yes. If you want to think about it from an Archbi point of view, then those are the variables that we.
00:21:10.012 - 00:21:12.932, Speaker E: And they're just going to be boolean variables. They're going to live on the hybrid.
00:21:12.988 - 00:21:37.684, Speaker B: Yes, that's right. I see. But in some sense, it's cumbersome to keep track of them, and in some sense, you don't need to keep track of them. We will mostly work with functions of this form, which is all the kind of thing, which is the kind of thing that we want to maximize.
00:21:39.184 - 00:21:41.488, Speaker A: You only allow boolean variables for x.
00:21:41.616 - 00:21:42.792, Speaker B: But now you're thinking about this as.
00:21:42.808 - 00:21:44.244, Speaker A: A real value polynomial.
00:21:46.184 - 00:21:47.964, Speaker B: Yeah. The values are arrays.
00:21:50.364 - 00:22:08.964, Speaker C: Why didn't, with degree log n polynomials, we could have just enforced that a variable xi, instead of choosing xia, takes values in a set one through k. Why did we change variables? Because we're only shooting for some exponential time anyway.
00:22:09.004 - 00:22:10.264, Speaker B: Right. So we could have.
00:22:13.164 - 00:22:18.064, Speaker C: A degree k polynomial would let us enforce that xi takes value to the separate.
00:22:22.924 - 00:22:24.904, Speaker B: So, okay.
00:22:27.644 - 00:22:29.796, Speaker C: Then we could just stick with variables xi.
00:22:29.980 - 00:23:30.422, Speaker B: Yes. But now, I mean, so this year, this year is nicer. So we, you know, you will always, I mean, these, these xia variables, they will not appear ever again. Okay, so just, so this is just to explain what it means when we say that this kind of a function has degree two or degree d, it means that you can express this function as a degree d point in these variables, but we will not have to deal with them explicitly. So it means that we will always have, we can always deal with functions of this form. And so this is just a way of specifying these functions. But if I tell you, if I ask talk about a function of this form, then, you know, I don't want to think about it as a k to the n dimensional vector with a real value for every k to the n choices.
00:23:30.422 - 00:24:32.054, Speaker B: I want to be able to represent it in a concise way. And the way we represent these kind of functions in a concise way is as a polynomial in these terms. So it's just way we will representing sufficient. So now we have, now we see that this is, you can think of it as a problem of maximizing a polynomial. And so that will allow us to give us the, it also defined what a pseudo distribution means in this case. So it's some function DD. So it's a function d on assignments.
00:24:32.054 - 00:25:34.056, Speaker B: And it's, we will, you know, represented as a DPD coil in this same sense as this. Yes, in the same sense. And now we want properties of this. And what are the properties that we want. So again, we have this notation to work with this, this function, which corresponds to, which is the same as an analog of the expectation of the function. And this, you know, it will be defined for every real valued function on this set. And so the conditions that we want about d is that it should be normalized.
00:25:34.056 - 00:25:52.604, Speaker B: And this means that if you plug in here the one function, which just means that we sum up all the values, then we want to get one. That's something that our distribution satisfies. So we want a certain distribution to satisfy this. And another property that we wanted, that we got enforced is that.
00:25:54.984 - 00:25:55.368, Speaker A: If you.
00:25:55.376 - 00:26:55.146, Speaker B: Look at the function, it is a square of a degree d over two polynomial. Then this should be, then this expression should be non negative, this pseudo expectation should be non negative. So this is for all polynomials p the degree d over two. And now, so this, so far d models a distribution over our set of assignments. And now we want to, you want to use the knowledge that we are interested in these solutions that satisfy those constraints. And how can we enforce that? So you can express this in the following way. David.
00:26:55.146 - 00:27:31.750, Speaker B: Yes. Do you want, maybe you want to think of d as the moments of a distribution, because it's sort of unnatural to expect the distribution itself to be loadingly for long because, yeah, but yes, you shouldn't think about, you shouldn't think of, you should maybe think of the low degree part of the distribution. So there's some actual distribution that you have in mind and we are looking at the low degree part of that distribution.
00:27:31.862 - 00:27:42.954, Speaker A: But I mean, does there have to be a distribution that would give these moments, or eventually you will be optimizing?
00:27:43.774 - 00:28:09.034, Speaker B: Yes, there does have to be a distribution with these moments, but the moments will satisfy all the properties that you expect moments to satisfy. And the goal will be to show that it's enough that they satisfy these properties in order to find a solution. But this is the definition of a pseudo distribution. Yes. And what you're writing next is.
00:28:13.184 - 00:28:13.616, Speaker A: So.
00:28:13.680 - 00:28:41.316, Speaker B: Here, I mean, so far this would, here, these two conditions just correspond to saying that we have a distribution over this set, over the set of assignments. And we want to say that those assignments satisfy, constraints, satisfy a lot of constraints. The constraints of our instance here, pseudo.
00:28:41.340 - 00:28:49.384, Speaker D: Distribution is a real value function that satisfies the first two properties, right? Actually.
00:28:52.044 - 00:29:05.882, Speaker B: No, actually it needs more. The probability of any low degree junta event or something should be non negative and that you can express degree polynomial just captured here.
00:29:06.018 - 00:29:28.274, Speaker D: So you're saying a pseudo distribution of degree b is some real function defined over k to them such that it's the pseudo expectation of one is one and the pseudo expectation of the square of a penalty at most d over two is non negative.
00:29:28.614 - 00:29:29.286, Speaker B: Right.
00:29:29.430 - 00:29:37.534, Speaker D: And also that they should, expectation of a monomial of degree at most d is equal to its actual.
00:29:37.654 - 00:29:42.046, Speaker B: No, no, there's no actual distribution.
00:29:42.110 - 00:29:46.594, Speaker D: So up to the second to last line, that's the definition of a pseudo expectation.
00:29:47.134 - 00:29:53.574, Speaker B: I mean, so there's a difference between sort of, I mean, so if you.
00:29:55.514 - 00:29:56.410, Speaker D: Want to say later.
00:29:56.482 - 00:29:59.042, Speaker A: Question is the new line the last line?
00:29:59.058 - 00:29:59.594, Speaker B: Yes.
00:29:59.754 - 00:30:02.774, Speaker A: That's not part of the definition of a pseudo expression.
00:30:11.994 - 00:30:31.202, Speaker B: Of degree d, you know, over. Good assignments. Yeah, maybe I should. Good solutions. Good solutions in the sense of our promise. Okay. We'd like to have a distribution, we'd like to have a solution that satisfies the one minus epsilon fraction of the constraint.
00:30:31.298 - 00:30:33.974, Speaker A: So this is specific to this specific.
00:30:36.834 - 00:30:38.294, Speaker B: Part, right? That's right.
00:30:44.274 - 00:30:45.574, Speaker A: Some linear program.
00:30:47.234 - 00:30:57.106, Speaker B: I mean, so usually in these relaxations that would be the primer that you're solving or because you're trying to, it's.
00:30:57.130 - 00:31:21.486, Speaker E: Like the dual of an SDP. I mean, I don't know, essentially distribution, I mean, distribution says, you know, given any non negative function, you get a non negative output, right? But then he's saying this is too much. So, you know, maybe this is too much to guarantee. So let me guarantee that. Just if you give me a square, which is an obviously non negative function, I'm going to hit a non negative output. And these are pseudo distributions because I don't guarantee that for every non negative function I get a non negative output.
00:31:21.510 - 00:32:06.024, Speaker B: Just for every squared. In this case, it would choose the degree to be n, then it would capture every function. Maybe that's a good exercise. If you choose the degree d here to be n, then the first two conditions just say that we have a distribution over assignments. And the second condition, this condition here would enforce that. It's a distribution over assignments that satisfy one minus epsilon fraction of the constraints. And now what we are doing to make these things efficient is just that we don't enforce these things for every degree, but only for degree up to some maybe constant or smaller number.
00:32:06.644 - 00:32:08.664, Speaker A: What is p in the last line?
00:32:13.554 - 00:32:36.186, Speaker B: That's a degree. Degree. A polynomial of degree at most, I think d minus two. So I just want to make sure that, like, all the things that we are evaluating here are degree deep polynomials. So the distributions are just another name for the moment sequences of Lacera that the industry. Yes, yes, yes. Yeah.
00:32:36.186 - 00:33:16.540, Speaker B: It says that, like the, it's just this different way of operating with those things and the things that, I mean, usually if you talk about moment sequences, then you specify the moments in a particular basis, and usually it's really the monomial basis, and that makes manipulating those things cumbersome. And so here we do it in a way that is independent of particular choice of the basis. Like an equivalent representation. Yes, it's an equivalent. Yes, completely equivalent to. Yeah, I mean, I'm not giving references in the. Maybe the way I should.
00:33:16.540 - 00:33:51.572, Speaker B: So, yeah, this is, you know, this, this idea of enforcing this condition of just over squares is due to aggregation here. So, you know, the fact that you can do this efficiently is, comes from shore and was later sort of fleshed out by parrillo and Lasse on the.
00:33:51.588 - 00:33:53.104, Speaker A: Moment problem, you see?
00:33:57.224 - 00:33:58.004, Speaker B: Yes.
00:33:58.344 - 00:34:00.564, Speaker A: So your last constraint.
00:34:03.504 - 00:34:22.524, Speaker B: Yeah, yeah. Just to make things a little bit simpler. So this, this model you would have here, arbitrary coordinates of degree n. Then this, you would say that we have a distribution over solutions that have value that satisfy exactly a one minus epsilon fractional difference.
00:34:25.344 - 00:34:27.804, Speaker A: How many levels of laser is this?
00:34:28.104 - 00:34:56.424, Speaker B: So this is indicated by the degree of the polynomial. Okay, so this is. And, yeah. What these people show is that you can compute this, this in time polynomial, you know, into the time. Enter the. Already.
00:34:57.804 - 00:35:03.444, Speaker D: So yesterday, when you were telling us about the Boolean case as opposed to this EIA case.
00:35:03.484 - 00:35:03.996, Speaker A: Yes.
00:35:04.140 - 00:36:00.596, Speaker D: So then it was clear that once you compute in time and to the degree, the part of your functioning big D, then you can compute that function everywhere, because that's just like the. Those are the Fourier coefficients of big D that you have. But, and that. So if you're given just this degree little d, the coefficients and the function big D that you can define this way will satisfy what will pseudo acceleration constraints. The higher degree terms will cancel out. But now it's a bit less clear what's going on, because these degree little d terms that you get out of solving the NASA relaxation, they're not the Fourier coefficients of dysfunction big D. They.
00:36:00.620 - 00:36:24.484, Speaker B: Are if you look at the q minus, because the Boolean variable, like the k and dimensional k, then they are. Yeah. I mean, so this is, you know, this Fourier kind of decompositions, it also works for general kind of product spaces like, you know, k to the n.
00:36:27.984 - 00:36:30.924, Speaker E: But isn't everything here secretly boolean anyway?
00:36:32.044 - 00:37:19.804, Speaker B: So. Yeah, not really, because like here, I mean, there's a constraint that I'm not, I don't want to talk about explicitly. If you would just have the variables Xia, then you would have to have the additional constraint that, you know, only one of them is one and all others are zero. Right? Because I want it to be like an assignment, like it should correspond to an assignment. And if you look at these things, then the indicators of these events, then if I look at a particular variable x, sub I, this is only one. This is one exactly for one of them. And so that would be some constraint that you would have to enforce explicitly if you would talk about polynomials in Xia, just about polynomials in Xia.
00:37:19.804 - 00:38:06.126, Speaker B: But here, because we talk about functions on this set here, you don't have to deal with these things explicitly. It's all inbuilt because we talk about these functions. So this is the object that we are getting. And you see that if you would choose degree D here to be n, then this would be an actual distribution. But if you choose degree to be very large, then it's hard to compute. And if you restrict the degree, then it becomes easier to compute. And now the goal is to just use the information that we get if it forces conditions up to degree D.
00:38:06.126 - 00:38:59.004, Speaker B: And in order to find a solution that satisfies this condition. So we will, so now we want to, so we want to use compute this kind of pseudo distribution. And now we want to use it to find a good solution for the instance. And we will use the following two properties that are useful in general. The first property is that you can condition certain solutions. And, you know, conditioning is if you first think about it just in terms of distribution. So you have some random variable and it has distributed in a certain way.
00:38:59.004 - 00:39:28.804, Speaker B: And now, you know, you create a new distribution by, let's say, conditioning the ith coordinate of the random variable to be a. So here I'm thinking of the distribution over assignments. So distribution over is this arrow just, you know, just what does conditioning mean? You know, I have this kind of distribution and now I condition it, you know, condition on the event. It's just basically some notation.
00:39:32.584 - 00:39:34.684, Speaker A: You mean you go from d to.
00:39:35.414 - 00:40:28.362, Speaker B: Yes, yes. So now we have some zero distribution d and we want to go to zero distribution d prime that corresponds to this. And you can define it exactly in the way that you would define it for distributions. Basically, you zero out, you zero out the function when xi is different from a. Just multiply this function by this, and now you renormalize. And the way we renormalize it is by the probability that this event happens. And this is, corresponds to this.
00:40:28.362 - 00:40:46.650, Speaker B: So, you know, if d here would be an actual distribution would be non negative, a non negative function, then this operation here is exactly conditioning on the event that Xi is equal to a, and we can do the same transformation for zero distributions.
00:40:46.762 - 00:40:49.722, Speaker A: Is it obvious that you don't increase the degree?
00:40:49.898 - 00:41:00.344, Speaker B: Yes. So, exactly. That's what we want to, what we want to verify. The question is, if you have a degree, d zero distribution here, what do we get out of it? Right.
00:41:00.464 - 00:41:01.204, Speaker A: You know.
00:41:04.744 - 00:41:24.144, Speaker B: So the claim is that this here is a degree, let's say d minus 20 distribution, you know, meaning it satisfies all of these conditions. Just with the degree, you know, smaller by two.
00:41:26.204 - 00:41:32.184, Speaker D: Does the degree necessarily drop like this variable might not participate in all.
00:41:32.884 - 00:42:26.304, Speaker B: Yeah, you could get, yeah, you could get lucky. But in some sense, yeah, we worry about, you know, what you say that in general, it never decreases by more than two, but does it always decrease by two? Ah, no, I don't know. I don't think so. Yeah, probably not. I mean, if, you know, if this here would be an actual distribution, then this here would still be an actual distribution. And so the reason why this is the case is because we want to. So now we want to say that for this new object, I only, you know, an outtake, you know, polynomials p, that have a little bit smaller degree, then we still satisfy these, these conditions.
00:42:26.304 - 00:44:13.054, Speaker B: Okay? And the reason why that's still the case is because what we multiplied with here, this is, this is just, you know, this is just another polynomial. And this is actually a polynomial that has small degree. You know, just, just this year, as a function on this set, it has, you know, it's a polynomial that has degree one. And, okay, so it means that if we evaluate the pseudo expectation of some function with respect to d prime, it's like evaluating the pseudo expectation under d, but of the polynomial, but of the function in addition, multiplied by this here, but multiplying by this here can increase the degree only by one. And the reason why we might lose two here for the degree is just because when you talk about squares here, then we need to treat here, we need to treat this here as, we need to treat this here as a square of a linear. And this is, you know, this here is the same as, just because this takes only value zero and one, this is the same as you know, at this polynomial which has degree two.
00:44:13.434 - 00:44:20.814, Speaker A: Just an excellent question. So everything that says degree D pseudo distribution is also a degree.
00:44:22.274 - 00:45:39.254, Speaker B: That's right. Multiplying by this year. Maybe I should just write it down. So if we look at this kind of expression, then this is the same as, and so the normalization here enforces, you know, make sure that this here stays satisfied. And the fact that we multiply it here only by a square means that all these, all these non negativity constraints and these conditions, they are also satisfied. And this is in some sense, you know, an advantage of having here this equality is. So it means that no matter what we condition on, it will always be said this condition here will still be satisfied, just maybe with a smaller degree.
00:45:39.254 - 00:45:45.376, Speaker B: But really, I mean, so, yes, so.
00:45:45.400 - 00:45:50.844, Speaker A: Just to verify what your claim is that this is a degree of the police.
00:45:52.264 - 00:45:55.680, Speaker B: Yes, just by definition, degree a, degree D minus two.
00:45:55.712 - 00:45:56.724, Speaker A: So redistribution.
00:46:12.904 - 00:47:51.272, Speaker B: So this is some operation that we can do. Another fact that is useful about zero distributions is that sort of marginal distributions are actual distributions in the following sense. So if we just, what is marginal means? If you have some distribution for random variable x, we can look at, you know, the distribution just of, let's say, two of the coordinates x I xj, and we can do the same thing, the same operation to a zero distribution and define our justice field distribution over two, over two coordinates in this way. And the claim is that, you know, if this year is, you know, at least let's say, decree, I don't know, for distribution, then this year is extra distribution. And this is also something that when you say a and b, because we've.
00:47:51.288 - 00:48:00.576, Speaker C: Interpreted these to be polynomials over the course of variables x, sub ia, you mean the family of all variables, you know, that have A's or B's in the second position.
00:48:00.680 - 00:48:04.044, Speaker B: So here, you know, B prime is now just a function that takes.
00:48:05.864 - 00:48:06.128, Speaker A: You.
00:48:06.136 - 00:48:16.824, Speaker B: Know, sort of an assignment for two vertices I and j. And so you can assign a. So a is what you assign to I, b is what we assign to j.
00:48:17.684 - 00:48:27.356, Speaker A: So what you're really saying is that if you look at v and you look at any pair of coordinates that induces an actual distribution on the values.
00:48:27.380 - 00:49:09.222, Speaker B: Of two coordinates, and it's, you know, objection. Yes, yes. And this is, you know, the only thing to verify is that this is non negative for all choices of a and B. And the reason why this is not negative is because, you know, this here is the same as the expectation of the indicator of this event. And this event here is a polynomial of it's. Actually a square of a polynomial of degree, but this is zero one. So it's great bearing doesn't change it.
00:49:09.222 - 00:50:05.714, Speaker B: So this here is the square of a degree two polynomial. And therefore, you know, by this condition that we enforced, it's non negative. So it means that for all choice of a and b, this is a non negative number. And you can also check that it sums up to one. So that means that it's a deciduous. Now we have these two properties and the question is, okay, what, what can we use it for? And the idea is that we can use conditioning in order to create a new serial distribution where the variables are uncorrelated. So enforce the variables to be uncorrelated on average.
00:50:05.714 - 00:50:54.870, Speaker B: And so what does it mean precisely? That means that if you have degree d that satisfies these properties, then you claim that they exist. A decree e minus two times r zero distribution. That satisfies where the variables are uncorrelated on average. So it means that if we pick two random variables, two variables. We pick two of the variables at random and we look at the correlation between them.
00:50:55.062 - 00:50:56.154, Speaker A: What's r?
00:50:56.574 - 00:51:03.734, Speaker B: R is a parameter that you can choose whatever you like. So for every r this is true.
00:51:03.814 - 00:51:04.814, Speaker A: It could be zero.
00:51:04.974 - 00:51:09.254, Speaker B: Could be zero, yes. But then the conclusion will not be interesting.
00:51:09.374 - 00:51:12.154, Speaker A: Oh, it will appear somewhere. Yes, as well.
00:51:16.944 - 00:52:10.554, Speaker B: Yes. So usually, usually these things will appear more than once. This is less than gamma, which is this schlock, k divided by R. Okay, so for every r, he claimed that there exists a degree v minus two, r as a distribution where the variables aren't correlated. In this sense, and I stands for yes, that's a good question. So this here is the mutual information between xi and x ray. And the way you can think about this, one way to think about this is maybe, let me make this more precise.
00:52:10.554 - 00:53:18.214, Speaker B: So this is with respect to some distribution, some pseudo distribution, this pseudo distribution, d prime and mutual information with respect to some distribution, b, two variables, is the amount by which the entropy drops if you condition one variable on the other variable. Yes, that's a very good question. So can you answer it? So the answer is already on the board. The answer is on the board. Just because, you know, marginal substitute distributions are actual distributions. That means that, like, you know, we always talking about just, you know, the joint distribution of these things, they only depend on the marginal distribution of these variables. And so it means, you know, everything is, will be defined just like for actual distributions.
00:53:18.214 - 00:53:48.914, Speaker B: Isn't that always with some particular thing? Yeah, with respect to some particular, I mean, d was a fixed. Here I wanted to fix d, and d prime was here, a particular series distribution. And here I wanted, you know, general.
00:53:51.434 - 00:53:54.482, Speaker A: So what's the relationship between d and d prime?
00:53:54.658 - 00:55:16.204, Speaker B: D and d prime. Ah, yes, yes, that's good. I forgot to write that here. So this d prime is a conditioning of pieces. You obtain d prime by conditioning d this way. So the proof will actually, you know, it will, it will not be about two distributions, it will just be about distributions, because we only talk about marginals of at most two guys. And so the proof is that we will use as a potential the average entropy of a variable with respect to some.
00:55:16.204 - 00:55:57.784, Speaker B: And notice also that here this expectation sign and this expectation sign is not with respect to, it's not a zero expectation, it's just choosing these guys at random, just choosing variables at random. We use this as a potential and we'll use the following procedure, you know, so greedily condition on event of the form, you know, x I is equal to a, you know, such that.
00:55:59.284 - 00:56:01.232, Speaker A: So this is a procedure to do.
00:56:01.388 - 00:56:09.684, Speaker B: Yes, to find the distribution such as the potential decreases the most.
00:56:17.024 - 00:56:17.560, Speaker D: Okay.
00:56:17.632 - 00:57:10.824, Speaker B: And the claim is that if you follow this procedure, then you will arrive at eventually you will get a series that satisfies these properties. Why is this the case? So let's look at what happens if you condition on a random event. On this event x I is equal to a for random I and a. So then the decrease in the potential. Expect a decrease in the potential.
00:57:12.724 - 00:57:16.572, Speaker A: What do you mean by that? By random coils? What distribution?
00:57:16.708 - 00:57:33.952, Speaker B: Yes, that's a good question. I was hoping no one noticed. So random j just means a random variable. Okay. And then, you know, uniformly. Yeah, a random index of a variable uniformly. Uniformly, yes.
00:57:33.952 - 00:57:54.704, Speaker B: And a random I means from the marginal distribution of the variable x. And yeah, we pick index j uniformly at random, and we pick a from the marginal distribution of the variable x j.
00:57:55.204 - 00:58:00.384, Speaker C: You just have two different ways of choosing I and a. One is greedily the.
00:58:03.884 - 00:58:08.716, Speaker B: I'm using j to condition and I to measure the potential.
00:58:08.900 - 00:58:10.664, Speaker A: I guess he wants to show the.
00:58:20.304 - 00:59:24.964, Speaker B: So this is the expected potential drop if you do this randomly. And if you look at the definition of this mutual information, then this is the same as the average mutual information between. And so it means that if you would condition on this kind of event for this random choice of j and a, then, you know, the expected decrease is at least that much. If you do it 3d, you will decrease at least that much. And this potential here is always non negative. Initially it's at most low kick. And so it means that, you know, this can happen at most, you know that the degrees that this quantity is larger than log a over r can happen at most r times, which gives you this.
00:59:24.964 - 00:59:32.876, Speaker B: So we have to condition it most r times to get to a point where this here is smaller. No, I suppose I'm almost okay.
00:59:32.900 - 00:59:36.516, Speaker A: Yeah, I think one or two more minutes.
00:59:36.660 - 01:01:00.572, Speaker B: So the question is, suppose now you have a distribution that satisfies where the variables are uncorrelated on average. The question is, how does it help? And there are several ways in which it helps. For example, what you can show is that if you had, if your CSP instance was pseudo random, the instance of the CSP that you're trying to solve, which I think I erased, if it was pseudorandom, in a somewhat big sense, it turns out that this condition here, if you satisfy this condition, then it will also be the case that basically all the variables are fixed. If that condition is satisfied, it will mean that all the variables are fixed. And then you can just read off an assignment, because you enforce this condition and still satisfy. It means that this assignment is satisfied fraction of the constraints. But if, you know, if it's a worst case instance, you can still, it's still useful to have this kind of condition.
01:01:00.572 - 01:02:45.894, Speaker B: And the reason why this is useful is because you know that for edges, if this here is a constraint, and you know that is satisfied with the large probability, which is the case for most of the constraints, then it has to be the case that either these variables are fixed or the mutual information is very large. So it means that in a case where the variables are not fixed in the games, it means that on edges, the mutual information has to be very large. And so it means that if you look at the mutual information for random pairs, it's much smaller compared to the mutual information for between variables that appear in a constraint together, in an equation together. And that means that this graph that's formed by the equations, it turns out that this means that the random walk on this graph mixes very slowly, and it means that you can partition the graph into small pieces and recurse. And that means that's the reason why you can use it to solve worst case instances, because either you can read off an assignment, or you can partition the graph into small pieces and reprose. Yes, there was a question.
01:02:48.514 - 01:02:49.454, Speaker A: No question.
01:02:50.954 - 01:03:21.804, Speaker B: Okay, I think consultant, thanks. That.
