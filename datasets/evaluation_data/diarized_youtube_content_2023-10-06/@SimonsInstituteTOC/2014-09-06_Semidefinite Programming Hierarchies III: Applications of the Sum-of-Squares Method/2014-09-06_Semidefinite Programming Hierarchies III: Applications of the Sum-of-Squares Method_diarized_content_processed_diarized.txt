00:00:08.840 - 00:00:38.022, Speaker A: So welcome on the second talk on SAP hierarchy. Thank you. Thank you. So I wanted to finish some of what we discussed yesterday. Into the time bitter and better under control. I made some slides. So what did we look at yesterday? So we had this promise problem unique games that had a parameter epsilon.
00:00:38.022 - 00:01:43.628, Speaker A: And so you remember you were given a bunch of equation involving two variables modulo modulo k. And we wanted to find an assignment to the variables that satisfies as many of the constraints as possible. And we were promised that there exists an assignment satisfies a one minus epsilon fraction of the constraints. And the approach for finding such an assignment was to consider a pseudo distribution that models a probability distribution over these assignments that satisfy one minus epsilon fraction of the constraints. And here, just everything that you need to know about definition of facility distribution. So it's a function defined, a rebalued function defined on assignments to the variables. And we have this notation corresponding to the expectation of some arbitrary function with respect to the density d.
00:01:43.628 - 00:02:35.370, Speaker A: And then we enforce some conditions that expect pseudo expectations of squares are non negative, pseudo expectation of one is equal to one. And this condition which corresponds to saying that you satisfy one minus epsilon fraction of the constraints. If you would do this for enormous up to degree d, that would just mean that probability distribution over assignments satisfy one minus epsilon fraction of the constraints. And now we relax this and only enforce these consistency conditions up to degree d for some d. And that means that we can also represent this density just by its low degree part without loss of generality. And that allows us to compute these things efficiently. But just, I mean, the fact that capital d is low degree can be assumed low degree is not part of the definition of pseudo distribution.
00:02:35.370 - 00:03:31.794, Speaker A: So that's not something that you have to worry about ever. And so one property that falls out of this condition is that if you condition the distribution d, prime the distribution d, and get a new pseudo distribution d prime, and sort of in a way such that d prime. So we saw that conditioning certain events preserves the condition that d prime is a zero distribution. And this condition here implies that even if you condition on any kind of event, the resulting pseudo distribution d prime will still behave like it satisfies a one minus epsilon fraction of the constraints. Actually, second, still on average. What do you mean on average? Yeah, so here we average over the equations in the instance. Exactly.
00:03:31.794 - 00:04:55.004, Speaker A: And this is equal to one minus epsilon. Okay, so that's the setup. Now we saw that by conditioning on events carefully, on a few events carefully, we can get into the situation where the variables are uncorrelated on average with each other. So if you pick two indices uniformly at random variables, then when you look at the mutual information between xi and xj, it's very small, roughly one over r when we allow the degree to drop by R. Okay, and the question was, how can we exploit this? And the reason why, for uni games, you can hope that this sort of has an interesting effect, is because if you look at this kind of a condition, you know, that Xi is equal, minus xj is equal to c, with probability close to one. Then this means that either xi or xj are fixed to a particular value, you know, that happens to satisfy, you know, fix this probability one minus epsilon to a particular value that satisfies the equation. Or the distribution of Xi and xj has to be very correlated, almost perfectly correlated.
00:04:55.004 - 00:05:47.286, Speaker A: It turns out you have to use the right measures for correlation if you want to, you want to deal with this sort of at the right granularity, but that's something that you can do. It's not too hard. Now we see that there's some tension between having small global correlation. And so this global correlation refers to that we pick two indices at random, but at the same time, if we pick two indices that participate in a constraint, then the correlation is high. So this is the local correlation. The local correlation is high, the global correlation is small. And so we ask, does it say something interesting about the instance? If we have this kind of discrepancy, and that turns out to be the case.
00:05:47.286 - 00:06:51.222, Speaker A: So what you get directly is a surprising probability distribution over real numbers on the constraint graph, which is the graph formed by connecting indices I and j. If the variables xi and xj participate in an equation of the instance. Okay, and what does it mean to have this surprising. So what do I mean by a surprising probability distribution? So it's a probability distribution over rn, you know, a real number for every, for every vertex in the graph. And it has a property that if you look at a random edge, then the random variables are very, concentrate, are very correlated in this sense. And if you look at two random indices, two random vertices, then the correlation between the random variables psi I and psi j is very small on. Okay, so we have a graph that supports such a probability distribution.
00:06:51.222 - 00:06:55.154, Speaker A: And the question is, what does it mean about the graph? And.
00:06:57.094 - 00:06:59.554, Speaker B: Is this because there's a unique solution?
00:07:03.694 - 00:07:08.086, Speaker A: Yes. Why is the claim true? That's a question.
00:07:08.150 - 00:07:09.754, Speaker B: Or because there's nasty?
00:07:11.024 - 00:08:09.224, Speaker A: Yeah, I mean, so it requires a proof. It's, yes, I will explain the proof on the next slide. It's almost non trivial, kind of, it's not true in the following sense. So here we talk about particular kind of correlation, namely the kind of correlation that says that, you know, these variables satisfy a linear equation with high probability. And here we talk now about another kind of correlation that you have two real value draining variables that are correlated with each other in sort of in the best way that you can hope them to be correlated. Right? So this, I mean, this ratio, I mean this here can never be larger than this quantity. So this being one minus epsilon is sort of almost as large as it can be.
00:08:11.284 - 00:08:17.028, Speaker B: The low global correlation, you need to do some work to move it from mutual information to this form.
00:08:17.156 - 00:08:20.924, Speaker A: Yes. And then that's the key solution.
00:08:21.084 - 00:08:23.908, Speaker B: You need to do some work to move it from there to this point.
00:08:24.076 - 00:09:02.054, Speaker A: Yes. Somehow rating this notion of correlation and this notion of correlation, these two notions of correlations are a bit incomparable. And now you want to unify them into one, into this particular correlation. So yeah, what we're doing here is transiting between correlations. And the reason why we can do that is precisely because we have an SCP solution or because, another way of saying it, because the pseudo distribution satisfies the condition that I stated before. Yes.
00:09:02.554 - 00:09:09.134, Speaker C: In the SDP setup. Should I think of this beta I as some kind of gaussian projection of average of tensorfactors?
00:09:09.714 - 00:10:15.634, Speaker A: Exactly, yes. And yeah, you see that there's actually a somewhat intuitive interpretation of these kinds of vectors in terms of series. And, okay, so why, why is this, so what does this mean? So this, I'm claiming that this means something about the graph. And what it means about the graph is that first, it means that the random walk on the graph mixes very slowly in the following sense. Even if you do this number of random steps in the graph, you see only a very small fraction of the vertices. So the distribution will be, you know, will not be uniform overtices like you would expect after, you know, after lots of set, we would expect that distribution is uniform over all vertices. But this condition implies that even after this number of random steps in the graph from particular vertex, your distribution is concentrated on roughly an n to the minus beta fraction of the vertices.
00:10:15.634 - 00:11:27.384, Speaker A: So this property called Ron invokes on the graph. And now, you know, the question is what you know, what's the combinatorial reason why the, you know, can be so derived combinatorial consequence from this slow mixing time? And indeed that's the case. And what it means is that if you take this much time to mix, then basically the reason has to be there exists a set that is very small and its expansion is very small. And the reason why that's the case. So maybe easiest to see in the contrapositive. So, you know, if this would be not true. So, meaning every small set has lots of large expansion, then, you know, that sort of gives you a way to just check that actually the random walk mixes quickly, because in every step you sort of, you know, you see, you know, if the expansion would be larger than this number, you see, let's say an epsilon of a beta fraction of new vertices.
00:11:27.384 - 00:11:57.694, Speaker A: And that means that after this number of steps you sort of see everything. Beta. Ah, yes. So beta is introduced here. So this is saying gamma. Yeah, the gamma here is from the, from the global correlation. And so if you choose gamma to be gamma, choose beta so that it satisfies this.
00:11:57.694 - 00:12:05.694, Speaker A: Just reformatressing this little bit so that we don't have locks. There are too many locks appearing here.
00:12:06.524 - 00:12:12.964, Speaker B: So this follows, because if you, if you have a short random walk between two pairs, then their correlation must be high.
00:12:13.044 - 00:13:28.760, Speaker A: Yes, yes. So that's a good question. So why does this condition imply that the random walk mixes slowly? And that's exactly what you said, what you said. So what you can show is that is the following. If you, if psi satisfies these conditions, and now you pick not just one random step, but t random steps in the graph, then the correlation between the endpoints of the random walk is at least one minus, you know, whatever it was before the correlation, it was before, raise to the t. And so that means that where does, now this french number come from? So that's a number that you need to. Okay, let me pick up a little bit.
00:13:28.760 - 00:13:35.084, Speaker A: So I want to say that the random walk mixes slowly.
00:13:40.024 - 00:13:43.364, Speaker C: So you just want to compare this one minus to the t two gamma, right?
00:13:43.664 - 00:14:36.646, Speaker A: Yeah. Right. So now I look at, you know, what's the correlation that I have left after beta divided by epsilon times log n steps. And then this here, if you choose this, this number st, this here will be much larger than n to the minus beta, which, which means that it will be much larger than this number. And that means that you know that your random walk hasn't mixed because you know that if you sort of pick two vertices according to the stationary distribution, the correlation is very small. But if you, but what we picked here, two vertices according to the Reynambault and the T sub renden walk. And the correlation was very low.
00:14:36.646 - 00:15:26.892, Speaker A: It was much larger than for the stationary measure. And that means that you have. Now what's the proof of the claim? Details are a little bit technical, so just give you some high level idea. So it turns out that what's useful, so we want to construct some of these random variables on the graph. And what turns out to be useful is to consider the following new serial distribution, which is just the product of our tensor product of our given serial distribution. And you know, this is actually a completely valid operation for serial distribution. So this will now be a new seal distribution.
00:15:26.892 - 00:17:08.060, Speaker A: And instead of being distributed over good solutions, it's basically distributed over pairs of good solutions. And because it's a product, it's independent, and, you know, it still satisfies all the conditions that we want from pseudo distributions. Now, we can define a function on this, on this new space, nonetheless, that satisfies exactly the kind of properties that we want in the foreign sense, that if, if this condition is satisfied, if xi and xj are linearly related with probability close to one, then the correlation between fi, fj under the serial distribution is close to one. And similarly, if the variables xi and xj uncorrelated in the mutual information sense, then the correlation of fi and fj under the pseudo distribution is very small. Okay. And so if you have such a function, such functions fi, then, you know, sort of for the same reason as, you know, then you can just sample a gaussian random variable with the same covariance as fi and fj. And because, you know, it's a zero distribution, like the covariance of this is positive, semi definite.
00:17:08.060 - 00:17:45.004, Speaker A: And so, you know, you can actually sample such a gush c. That would be, yeah, exactly. The xi would be any, you know, any random variable that has the same second moment. And it exists just because we have these square polynomial conditions, or because it's an SDP program. But notice that somehow, like this, I mean, this function here can be a little bit complicated. So it's not. The relationship of fi to the vectors.
00:17:45.004 - 00:18:36.024, Speaker A: Expressing it in terms of vectors is somewhat surprising, but so how does the function look like here? So it turns out that essentially the following function works. The following functions work. So the function f I, given two assignments, x and x prime will be either equal to minus one over k, or equal to one minus one over k. And in fact, what you would look at is whether the values for xi are the same, both x and x prime. So you check whether x and x prime, whether xi is equal to xi prime. If that's the case, then you output one minus one over k. Otherwise you output just minus one over k.
00:18:36.024 - 00:19:35.034, Speaker A: And there's a slight catch. And this sort of makes everything a little bit ugly. So this precise choice only works if xi has, has full entropy in the distribution d prime, which is a little bit cheating, because we obtain d prime by conditioning, and that tends to reduce the entropy. But it's only cheating in the sense that, again, you have to choose fi more carefully in the general case. But that contains the main idea. So why does this make sense? It's actually like, you can, you can really, it's sort of, you know, if you look at this full entropy case, you can convince yourself that it satisfies these conditions. So, you know, the one property that this has is that the expectation, if you choose x and x prime, the claim is that the expectation of fi is equal to zero under this.
00:19:35.034 - 00:20:51.074, Speaker A: The reason is that, you know, if this has full entropy, then the event xi is equal to xi prime happens with probability one over k. So that means that in the expectation, you know, this contributes one over k, and this contributes minus one over k. So the expectation of Fi is zero. And that means that if Xi and Xj are really, you know, if this mutual information is close to zero, which means that Xi and Xj are close to independent, then it means that, you know, this, this expectation is close to the product of the expectations, expectation of Fi times the expectation of FJ, and that's zero. This is, okay, now, why is this, why is this true? And this is true because, you know, if, if Xi and Xj satisfies in the relation that probably close to one, then basically, if Xi and Xi prime collide, then also XJ and Xj prime collide. So those two events are perfectly correlated. And that means that this inequality is true.
00:20:51.074 - 00:21:33.664, Speaker A: So that's the scale of the proof of the claim. Now, let me give you a high level view of the whole algorithm. So, all algorithms, meaning, you know, so you get it in a game instance or set of equations, and you compute the serial distribution. Now, how do you get an assignment? How do you compute an assignment? So here's the overview of the algorithm. So, there are three steps. I'm showing currently two of the steps, and you repeat the steps as long as possible. So, first thing is that, so look at the connected components of the constraint graph.
00:21:33.664 - 00:22:48.634, Speaker A: And if one of these components has a large global correlation measured in terms of just for the component, then you condition on event in that component and reduce the average entropy of the variables in that component by a significant amount. Now, because if all components have low global correlation, we can look, if in any of the components, it's true that most of the variables are almost fixed, so then we can just read off a good solution for that component. And, you know, we choose that assignment and, and, you know, it will satisfy most of the equations, most of the equations in that component. And so that's good. And we can continue. Now, if all components have low global correlation and the entropy of the variables is large, then what we saw is that it has to be the case that the random box on this component, it mixes slowly. And that means that we can cut out a set of that component that, you know, whose size is n to the beta fraction smaller than the size of the component, and where the expansion is small.
00:22:48.634 - 00:23:27.442, Speaker A: Okay, so that's the whole algorithm. Now, what's the analysis? So we can think of this as recursion depth. This may be not the right word here, but we can associate some depth in the this process. And in the sense that when we cut out a set, think of it as that new set, that small set here, we think of it as depth, being one smaller component will be great. Here, define its depth to be one plus the depth of the previous component.
00:23:27.578 - 00:23:30.094, Speaker D: How do you identify the set.
00:23:32.404 - 00:24:16.424, Speaker A: That'S, you know, if the random walk mixes slowly, then it also means that you can efficiently find these sets that don't expand. So you just do the computer distribution of the render block after the number steps. And then there has to be sort of one time step where, you know, this very portion is of the plastic is very small. And that means that you can have a set at a small expense. And maybe there's a nice picture that you can draw this if that's this component that you're looking at here. Now you pick some vertex. The random walk mixes slowly.
00:24:16.424 - 00:24:37.674, Speaker A: So it means that if you take one step, you may be distributed here once, another step, you distribute it here. Now, even after log n steps, you know, you're still distributed around a very, you know, very small part of the component. And so it means that, you know, in one of these intermediate steps, the expansion was very small. And you can compute these intermediate steps.
00:24:38.734 - 00:24:45.478, Speaker B: This first thing, if it has global correlation greater than this, that's, that's where you use this gaussian, where you're sampling from the gas.
00:24:45.566 - 00:25:50.634, Speaker A: Ah, where, where is it that you're sampling from this in some sense, the sampling is really only to prove that the Brandon walk mixes slowly so you don't have to actually sample in the process. If both of these steps fail, then you know that the global correlation is small and the local correlation is large. And then there exists this distribution of real numbers. And that certifies that the random walk makes a story. Yes. Okay, so it means that the number of times, because whenever we create a new component, its size is n to the beta fraction smaller than the previous component. It means that we can, you know, the number of times we can sort of go to smaller components, is the most one over beta.
00:25:50.634 - 00:26:45.636, Speaker A: So it means that we can sort of arrange the computation into, into a tree of depth, one over beta. And it turns out that in, because we only cut things that have expansion, most root, epsilon divided by beta in each level of the tree, the amount of number of total fraction of edges that we're deleting is almost this much. And that's the number of levels that we have. So that's, this here it tells us what's the total fraction of edges that we are removing from the, from the, from the constraint graph. And here, you know, these edges, they correspond to the equations that we want to satisfy. So, giving up on those equations and the rest of the equations, we are satisfying most of them, we are satisfied in this step. So as long as this number here is smallish, we are in good shape.
00:26:45.636 - 00:28:01.324, Speaker A: And so it means that if you choose an example, smallish, if you choose better, to be roughly epsilon to the one third larger than that. And that means that, okay, what does it mean? So it means that for the scopic correlation, that's the bound that we can tolerate. And this bound here, this here was. So the bottom deck here is, this here has to be comparable to the degree of the pseudo expectation, because if you want correlation to be correlation to be this small, we have to condition into the, we have to be able to condition and to the better times roughly. And so it means that the degree of the zero distribution should be at least into the, into the, into the better. And so that means the total running time of this algorithm is, you know, two to the n to the epsilon to the moment exponential in the degree of the zero solution agreement.
00:28:02.264 - 00:28:11.444, Speaker D: I still don't understand, what is the exact claim that you're making about being able to identify the small cuts.
00:28:19.544 - 00:28:19.880, Speaker A: If.
00:28:19.912 - 00:28:25.984, Speaker D: Something like this, I mean, you obviously don't find the exact set of expansion.
00:28:26.444 - 00:28:27.224, Speaker A: Yes.
00:28:28.004 - 00:28:29.316, Speaker D: So what's something else?
00:28:29.380 - 00:29:02.026, Speaker A: Yeah, what you find is. So look at these. Okay, so we established that the random walk mixes slowly. Now we have these distributions. And the way these distributions look like is g. So if you start at some vertex I, then the distribution looks like after t steps is like this. And now what you get is that this here, you can think of it as, that's the collision probability of this distribution.
00:29:02.026 - 00:29:10.574, Speaker A: And this is very closely related to the size of the support of the distribution. And so it means that.
00:29:12.274 - 00:29:13.766, Speaker B: It'S a tigger bound, right?
00:29:13.850 - 00:29:14.382, Speaker A: Yeah.
00:29:14.518 - 00:29:14.894, Speaker D: Okay.
00:29:14.934 - 00:30:01.740, Speaker A: I mean, the question is, so maybe to what, I mean, what function do you want to apply the gigabond to? And so what you get from this condition that the random walk doesn't mix quickly is that there exists some t in such that these two numbers are most, sorry, this here. So this, we expect this to decrease. So this here will be at least one minus epsilon divided by beta. Now what does this mean? So now if you rearrange this, you know, you can think of, if you look at the function now, v to the t minus one times. Oh, okay. Yeah. This function would be a function where you can apply the chigabong to.
00:30:01.740 - 00:30:02.824, Speaker A: Yeah. Okay.
00:30:03.644 - 00:30:10.774, Speaker D: So you're not really finding, can you just, this is an upper bound of expansion.
00:30:10.894 - 00:30:11.554, Speaker A: Yes.
00:30:13.614 - 00:30:17.314, Speaker D: What you're really finding is a set of small eigenvalues.
00:30:19.494 - 00:31:13.412, Speaker A: It's tricky to think about it in terms of eigenvalues because what's really important is that the reason, what grants us from talking about eigenvalues in the way you usually forward in the bonding of serious inequality is because we want a small set. Yeah. And the reason why you get a small set in this, if you sort of look at this function, is because, you know, this function will also have a large collision probability, relatively large collision probability. So it means that this function, you know, as a set, corresponds to a small set, because as large. But maybe this picture here is, this is really like the sets where the distribution is concentrated after different number of steps in the render walk. And you know, all these sets are small. That's one promise.
00:31:13.412 - 00:31:20.564, Speaker A: And you know, there are many steps that you can do. And that means that one step has to be a very small expansion.
00:31:20.724 - 00:31:21.584, Speaker D: I see.
00:31:25.054 - 00:31:29.806, Speaker B: Levels of the thing descends to minus two. Beta is hopeless.
00:31:29.910 - 00:32:30.504, Speaker A: Ah, yes. Okay, so that's exactly what I wanted to discuss next. So what's the. Yes, so what's maybe more general question is how could we refute, I mean, why does this not, I mean, why could you not do something similar and refute the. And, yeah, so s, as you said, why do we need this very small global correlation? Or do we actually need this? And in some sense, we indeed need very, very low global correlation. So there are examples that show that you can have very low global correlation, but still, but no good partitioning exists at all. So not just about the algorithm that you use to find the partitioning, but the examples that show that you can have very low global correlation and it's just not possible to partition the graph.
00:32:30.504 - 00:33:56.994, Speaker A: And these are somewhat intricate constructions based on locally testable codes, somewhat surprising connection. And these examples are actually not just ruling out this particular, and not just saying something about this particular approach. So it turns out that basically the same examples, they also show that if you only look at monomial moments in the pseudo distribution, then you have to, then you need to invest super constant degree, which means that you cannot hope for nominal time. And here, you know, this look here comes with some restriction. So what Popper and Sackett show is that there exists a pseudo distribution who, you know, that in terms of the monolith moments is very close to satisfying all the constraints that you want to satisfy, that you want a pseudo distribution to satisfy. And so it means that, you know, if the way you look at these monomial moments is, unless you are really exploiting that things look exactly like they're supposed to look like, which is sort of unintuitive in the context of approximation algorithms, then yeah. What their construction shows is that you need to have super constant degree, but other moments are just linear combinations.
00:33:56.994 - 00:34:28.784, Speaker A: Yes, exactly. But if you do linear combinations, then you know things that look okay in terms of monomial moments, they might look very far off in terms of after you take the linear combination, because you know the linear combination, it might accumulate errors. So basically there are these monomial moments. They are okay up to some errors, but they still satisfy that squares are non negative, for example. So it's really like squares are polynomials. No, no, the spares of arbitrary polynomials. They are non negative.
00:34:28.784 - 00:35:28.234, Speaker A: But now if you look at, you want to satisfy the constraints, and that's sort of where there's some things that don't hold exactly, but only after some help. And so this, that's sort of what motivates. Okay, and now, okay, so you might think that, okay, maybe this means that, you know, you know, so maybe it's not a problem with these monomial moments, maybe it's just a problem with the whole approach. But it turns out that if you look at the serial distribution, if you look at these things in the right, you look at these moments in the right basis. As you said, it's just the basis change if you go to other moments. But if you look at the right basis, then there's a very easy argument, a very robust argument, that shows that, you know, that these instances, these examples are not a problem. And the write basis turns out to be the I basis of an appropriate operator.
00:35:28.234 - 00:35:47.584, Speaker A: And this is what motivated this way of thinking about these SAP solutions in a basis independent way, which is what we did with, I mean, the goal of the pseudo distributions, the serial distribution viewpoint for these things.
00:35:48.444 - 00:36:25.506, Speaker C: Okay, is it obvious that the squares are non negative in the quad pocket? I mean, they give SDP vectors, yes. But for example, when you compute the expectation of a polynomial and you square it, you will sort of want to replace these moments consistently or when you like. So when you think of, when you get a term s delta t, you want to think of product of s and t right products, but then you will get errors. Once you try to substitute the inner products of the SDP vectors into this polynomial, you will have to substitute each term with an error.
00:36:25.570 - 00:36:39.854, Speaker A: Yes. Okay, so maybe, maybe one way, yeah, maybe one way to say that is that like the commutativity properties are not satisfied as you would expect, you know.
00:36:39.974 - 00:36:42.798, Speaker C: That would break the positivity of squares.
00:36:42.966 - 00:36:49.398, Speaker A: I mean, it depends what you mean by squares that you next to someone.
00:36:49.486 - 00:36:51.114, Speaker B: Wipes these things out anyway.
00:36:51.654 - 00:38:21.684, Speaker A: Yes, I mean, so this is, I mean, this is not a very, you know, this is more a question of taste, you know, I mean, it really means that if you look at monomial moments, then you have to be very careful with, you know, if you want to avoid this obstacle. But in the fact that it becomes very easy in the right basis is maybe also motivation to look at things in basis independently. So now I wanted to talk a little bit about more recent developments in the context of some squares, also at the higher levels. It's not as technical as what we did just now, and it's the following problem that I wanted to talk about. You want to find the sparse vectors in a subspace. So what you set up is you have a d dimensional subspace U of rn and you're given it in some, I know, by some basis vectors and arbitrary basis. And the promise is that the subspace contains a very sparse vector and your goal is to find such a sparse vector in the subspace.
00:38:21.684 - 00:39:25.004, Speaker A: It is actually quite possible related to the games conjecture and the problem that we talked about before. So it turns out that this problem is at least as hard as a problem called small cell expansion. And that's the correspondence is most clearly in the case when the sparsity of the vector that you're talking about is comparable to the number of coordinates of the vector. And basically a small set expansion is a problem that you need to solve if you want to review the new games conjecture. And so for a time algorithm for small expansion would be, or even time algorithm would be a major step towards refuting really exciting picture. And so that's the, was our motivation for looking at this problem. But it turns out that actually the average case version has also been solved of this problem.
00:39:25.004 - 00:40:18.660, Speaker A: And here it turns out that we can say something new. And so the average case version of this problem is that you generate the subspace in the following way. You pick d minus one random vectors, say random gaussian vectors, and one which can actually be an arbitrary case bound selection. And you look at the subspace spanned by all these vectors and you're given it some arbitrary basis, maybe a random basis, and now you define the sparse vector. So in some sense you make it, you plant one sparse vectors among p minus one random vectors. In this problem. One way to think about is some kind of benchmark problem for various unsupervised learning tasks.
00:40:18.660 - 00:41:46.740, Speaker A: So for example, one previous work from study by Spielman Whining bright and their motivation was to solve a problem called dictionary learning, which is very basic antibiotic and what the. But it turns out that the problems that have been suggested by different from my demon hand afterwards, they have a limitation, namely they only work for various sparse vectors. So the sparsity of the vector has to be small. Compare, you know, if the dimension of the, of the space is larger and which is actually, which is actually sort of not something that we wanted in the context of small expansion, because here we really wanted vectors that are sparse just compared to the number of coordinates. And the algorithm that they're proposing is actually very, very simple, very nice. You look at the subspace and you find in the subspace a vector that maximizes this ratio that has the largest infinity norm compared to the l one norm. And this is something that you can do exactly using linear programming.
00:41:46.740 - 00:43:09.104, Speaker A: The reason is that you guess which of the n coordinates of this vector, this is the maximum, and then you minimize the l one norm over the subspace with respect to the constraint that your large in the vector is large in that particular coil. So there's a somewhat non trivial sort of this kind of, this kind of very popular l one minimization thing going on over some affine subspace. And the affine subspace that you look at is determined by your guess for the, and it turns out that if you use degree 40 distributions, then you can solve this problem for a larger range of parameters. In particular you can solve it in the full sparsity range up to where the SPAC is up to a concentration of the coordinates as long as the dimension of the subspace is not too long. Here if b to b root n then here you need, you know, this thing has to be, you know, still significantly, you know, much, much sparser than a constant answer. Okay. And I don't have, so now I'm switching to the board.
00:43:10.684 - 00:43:18.204, Speaker D: So if you don't have the promise problem, the decision problem is hard.
00:43:19.144 - 00:43:22.616, Speaker A: If you, you mean antiheart or.
00:43:22.760 - 00:43:28.204, Speaker D: Yeah, decide if don't find vector, just decide if the subspace has something.
00:43:28.864 - 00:44:11.050, Speaker A: Yes, yes. This should be hard in the neutral sense. I mean I'm not sure if even approximating this in a general, even actually the kind of approximation that you want for force expansion, at least four inomial time to do that. So in terms of this problem, even with the promise it probably doesn't have a pronoun time algorithm. But for the indigenous objection, even at quasi polynomial be very innovative reviewed conjecture.
00:44:11.242 - 00:44:17.114, Speaker D: And if you relax the sparsity, so you just want a vector that is very unbalanced.
00:44:17.154 - 00:44:22.054, Speaker A: Yes, yes, that's implicitly always what we are doing.
00:44:22.754 - 00:44:23.894, Speaker D: Oh I see.
00:44:26.114 - 00:44:59.974, Speaker A: It'S still hard in this sense and also in sense even if you relax. So how would you, how would you, how can you use two distributions to solve this problem? So in some sense the right thing that you would want to do is you would want to find a vector in the subspace that maximizes the.
00:45:01.464 - 00:45:03.000, Speaker C: Can you type in?
00:45:03.192 - 00:46:39.404, Speaker A: Yes, large font. So the right optimization problem that you would want to solve some sense would be to maximize the two norm compared to the one norm. We want to find a vector that has large two norm compared to its one norm. You know, instead of just finding a vector that has large infinity norm compared to its norm? Because those basically if this condition is violated then if you do this then there will be a vector that's completely unrelated to the sparse vector that you wanted to find. But if you have this condition then that would succeed in essentially in full range of parameters. Really natural thing to do for this. So you would want a serial distribution and it would now be a function on the unit ball of the subspace u, rebalued function of the unit on the unit wall, the subspace u, sorry, rebounded function.
00:46:39.404 - 00:47:33.684, Speaker A: And so it means that now we have to think about integrals, but actually we don't have to think about integrals because these things are, you know, you always represent these things as polynomials. And then, you know, integrating polynomials of the spheres, I think, know how to do that. And, and then, you know, you have this, you would have the usual conditions, you know, that squares should be non negative. And now you, some of them want to say that, you know, that the one arm is small. And here's some point where you run into trouble, because, you know, if you look at the one norm. Yeah, actually, maybe. Let me.
00:47:33.684 - 00:48:26.564, Speaker A: So what would you expect the one norm to be of a k square vector as k spas unit vector. So we would expect k coordinates of value, roughly one over k. So you would expect this 1 be k times one over root k. We'd expect it to be root k. So now you would want to add the equation that the one norm should be root k. But now the issue is that the one norm here is not a polynomial in the sense of functions on this ball. This makes things very complicated to reason about.
00:48:26.564 - 00:50:02.234, Speaker A: Now, one way to resolve this, actually, which has been suggested in a paper on spars. So, using seminary programming for sparse principle, for sparse principle component analysis, is that you instead of. So let's look at the square of the monomer, but let's do it anyway. So this, you know, this would be the square of the one norm would look like this. And now what people have been suggesting is that maybe we relax the constraint that the one norm is small and instead just enforce that. So what you want to minimize, have the constraint that instead of saying this here is less than root, is less than k. Now, because we squared the one root, instead of saying that this is less than k, you say that this here is less than k.
00:50:02.234 - 00:51:15.094, Speaker A: So, you know, you remove the, you know, you put the absolute value signs outside, which, you know, makes things only smaller. So this is still satisfied by sparse vectors. And now we can solve this efficiently because it's a convex constraint on these numbers and efficiently checkable convex constraints on these pseudo distributions. Yeah. Unfortunately for this problem, it fails, also fails quite dramatically. And the reason is, somewhat nice reason, you can choose d to be an actual distribution and make this fail. Just choose d to be the uniform distribution over the sphere and over the, this d to be uniform distribution over the two ball of the subspace.
00:51:15.094 - 00:52:17.344, Speaker A: So now, you know, this would actually be, this would be an actual distribution. But if you compute that if u was a random subspace, then. So I guess here what I mean uniform. Yeah. Here what I mean, uniform, even uniform, just over these d minus one vectors, then it turns out that, you know, in this case, these expectations will be very small, I think. Okay, I'm not exactly sure if it's, there's a root d or trustee. But in any case, if you now look at, look at the sum, you know, this would always be, the sum here would always be, even if your distribution was just as uniform, distribution over the world would be less than n over.
00:52:17.344 - 00:54:05.684, Speaker A: And it means that, you know, unless, unless k is smaller than n over d, you know, this, this distribution looks like it's a distribution of a sparse vectors with respect to this constraint. And so that's bad. So this is basically sort of as much of a problem as here. Now, what you can do instead, which turns out to work better, is use an appropriate degree for polynomial and the right degree four polynomial, if you want to enforce sparsity, would be that you add the constraint that x for norm of x, you know, is k times one over root k to the four, which is k squared, which is one over k. Since you add the constraint that the four norm of x to the four, which is a degree four polynomial, is at least one over k. And so this is satisfied by case pass vectors. And it turns out that, you know, if you enforce the condition this is equal to one over k, then it turns out that the zero distribution will be concentrated on the vector v.
00:54:05.684 - 00:55:36.970, Speaker A: And so it means that you can basically read off the vector v just from the degree two moments. But you need to enforce this condition about the degree four moments. And the proof that this condition is recovers the sparse vector is, I guess I don't have to write something. It's in a sense very natural. So what you do is you just prove that. So if you would solve this problem exactly, exactly without zero distributions, just if you just prove that the vector that is larger, this larger phonogram in the subspace and the unit vector that has this larger phonom in the subspace, you can prove that this has to be close to the sparse vector v. And it turns out that you can prove it in a way that it is very explicit, explicit in the sense that the proof uses only the squares are non negative, and the kind of inequalities that are used in the proof only use in squares are non negative.
00:55:36.970 - 00:56:07.714, Speaker A: And now you notice that these properties are also satisfied by zero distributions. And that means that serial distributions capture this proof and therefore pseudo distribution has to be concentrated around the vector B. So that's what I want to say about this problem. Thanks a lot.
