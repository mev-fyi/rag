00:00:00.440 - 00:00:35.330, Speaker A: Just told me there is an email from Martina about feedback about this conference. So you can praise the speakers, complain about the organizers, or do whatever you want. Our first speaker is Ronald El Dan from the Watcher Institute. And right now he's just writing very slowly his title. Let me just. As a spoiler, let me tell you that his title combines three major themes of this workshop. First, low concavity, second, concentration of measure, and third, a discrete hypercube.
00:00:35.330 - 00:00:39.614, Speaker A: And all in just one ticket. So, Ronan, please go.
00:00:39.954 - 00:00:44.178, Speaker B: I didn't know the hypercube was the same, so it could be.
00:00:44.266 - 00:00:44.934, Speaker A: Yeah.
00:00:47.394 - 00:01:27.560, Speaker B: Glad to hear that. Okay, thanks. Then I guess just let me start. This is joint work with Freddie Keller, Omer Chameleon of Zaituni. And in overview, this talk is gonna be about. We know that log concave measures in RN are concentrated. And we wanna find an analogous notion of log concavity on the discrete hypercube and prove concentration over there as well.
00:01:27.560 - 00:02:07.934, Speaker B: This is the topic of the talk, so let me just start by reminding you what happens in rn. So this is going to be the motivation for us. So rn. So here's a fact. Suppose that I have a measure in rn with a convex potential. So suppose that d nu is equal to e to the minus v dx. Nu is a probability measure.
00:02:07.934 - 00:03:08.830, Speaker B: So nu is normalized to be a probability measure. And v from rn to r is. I want to make it uniformly convex. Meaning I want to require that the hessian of v is uniformly bigger than some positive constant delta times the identity. Then this is the fact. This implies several ways in which this measure is concentrated. Let me describe them in like and hierarchically increasing way.
00:03:08.830 - 00:03:59.716, Speaker B: So, the weakest type of concentration you can consider is the following. So, for every one lip sheets test function phi from rn to r, we have that the variance with respect to nu of phi is at most one over delta. Okay, this is probably the weakest type of concentration you can think of. Then there's a stronger one where you can actually say something about higher moments. And this thing is actually exponential. It's basically sub gaussian. But I don't want to get into that.
00:03:59.716 - 00:05:04.616, Speaker B: Let me just formulate a slightly stronger type of concentration in the form of a Poincare inequality. So for every smooth enough phi, we have again that the variance with respect to nu of phi is now bounded by one over delta times the expectation with respect to nu of the gradient of phi squared. So this is called the Poincare inequality. And then we really have an even stronger thing. For example, a log sublev inequality, which is the same thing where I can also add a log. So, okay, let me actually, let me not write what this is. If you don't know it, it doesn't matter so much.
00:05:04.616 - 00:05:48.164, Speaker B: In this talk, we'll be mostly talking about this and this type of concentration. Okay. And now I want to ask a question. Actually, let me, before asking the question, let me formulate this fact a little differently. It's not going to matter so much. But let me say that d nu is e to the, it's just going to be more convenient a bit to say that it's e to the plus v. And here I'm going to write d gamma, where gamma is the standard gaussian measure.
00:05:48.164 - 00:06:53.844, Speaker B: And then instead of asking for the Hessian to be bigger than, you know, delta times the identity, I changed the sign. So I really, this is equivalent to saying that the Hessian with respect to the Gaussian is at most one minus delta times the identity. Right. Because the Hessian of the Gaussian is just one. So when I wrote this, now, I didn't really change anything. Still the same fact. But it's going to be more convenient because now I want to ask, you know, what if I replace gamma? Well, what if I replace rn? What if rn becomes the discrete hypercube and gamma becomes mu, which is just defined to be the uniform measure.
00:06:53.844 - 00:08:07.252, Speaker B: Okay. So the reason I, one of the reasons, you know, it's, it pedagogically makes more sense to, to write gamma here is that, you know, with respect to gamma, even if v is just, you know, equal to zero everywhere, we still have concentration. And also if, you know, here v is equal to zero everywhere, we still have concentration with respect to the uniform measure on the hypercube. So it makes more sense to compare the hypercube to the gaussian rather than to the Lebego measure in rn. This is a measure that already has some curvature in some sense. Okay? And we're going to face several challenges when we try to, you know, take this notion and adapt it to the hypercube, because there are several real things. It's not clear how to write in the setting of the hypercube.
00:08:07.252 - 00:09:24.254, Speaker B: First of all, it's not clear what one Lipschitz functions are going to be. Second of all, it's not clear what the right analog of this thing is going to be. And maybe let me change this a little bit and let me just copy it here and just mention that in the setting of gaussian space, or rn, the right hand side here is also called the Dirichlet form, usually denoted by this fancy e of phi and phi. So this is just defined to be the right hand side one over delta. Okay? So it's also not clear what the corresponding notion of a Dirichlet form is going to be. Let me write it like that. And finally, it's clearly not clear what, you know, the analog of this thing is going to be.
00:09:24.254 - 00:10:09.134, Speaker B: Right? Any questions so far? Just, you know, to give you a taste and convince you that things are not going to behave the same. For example, in gaussian space, if I take the gaussian measure and just condition it on any convex set, I still get to keep my concentration. I only get something which is more concentrated than before. But on the discrete hypercube, conditioning on a convex subset is like conditioning on any subset. I can always take the convex hull. It's not going to change anything. So clearly things are not going to be as well behaved in gaussian space.
00:10:09.134 - 00:11:25.844, Speaker B: Okay, so here are some, here is one result, and this is going to be the first. So, case one is going to be just the case of quadratic potentials. So here nu d nu is going to be e to the v d, the uniform measure on the hypercube. And v is just going to be. And v of x is just going to be some x ax for some matrix plus some vector h dot x in gaussian space. This is a very easy case because we're always going to get another gaussian measure. But on the discrete hypercube, I guess most people here know that for some matrices a, it's very, very hard, actually, to understand what the behavior of this measure is going to be.
00:11:25.844 - 00:12:56.462, Speaker B: Okay. And I mean, in particular, it's not even clear how to understand what the covariance structure of the measure is going to be. And maybe an example to keep in mind is the so called Sherrington Kirkpatrick model, or spin glass, which is just the case that we take aij to be a gaussian centered with variance one over n, or actually beta over n, where beta is some constant. So beta is of constant order. Okay, so here, there's already a very nice result, which I should first mention by. So the first theorem is by Bauer, Schmidt and body naught. And they show the following.
00:12:56.462 - 00:14:34.268, Speaker B: So they show that if h is zero, if there is no external field, and if the operator norm of the matrix a is at most one over four, then we have a poincare inequality of the following form. So let me just see that I remember it correctly. Why don't they have it here? Okay, then we have the following thing. So we have that the variance with respect to nu of any test function, phi is at most nice over the operator norm of a. Sorry, over one over four minus the operator norm of a times. And now I have to tell you what the analogy of this thing is. So it's just going to be written exactly the same as here.
00:14:34.268 - 00:15:47.776, Speaker B: It's just going to be the expected gradient square. And the gradient is just the discrete gradient. So it's just going to be the expectation over nu of the gradient phi square. Where what is the gradient of phi? Where the ith discrete derivative of phi at x is just defined to be phi at x, where the ith bit is chosen to be one minus phi at x, where the ith bit is minus one over two, and the gradient of phi is just defined to be, you know, del one of phi up to del n of phi. Okay, so this actually looks, this is a very nice result. It actually has an amazingly short and nice proof one. Yeah.
00:15:47.776 - 00:16:05.684, Speaker B: A has to be PSD rights. Ah, yeah, yeah, sorry. Yes. The way this is formulated, a has to be PSD note, however. Yeah. So, right. Let me just write it.
00:16:05.684 - 00:17:18.263, Speaker B: No, it's e to the plus a here. It's e to the positive thing. But it actually doesn't matter so much because we can always subtract an identity from a and this is not going to change the distribution. So you know, what this is saying is just, instead of PSD, we can just say if the operator norm is at most one over eight, then we can always make it PSD and have the operator norm at most one over four. So it doesn't really matter so much. So one drawback of this result is that if we go back here to gaussian space, this Dirichlet form implies mixing for longevity dynamics. So it's actually useful to prove that concentration of this form because then you can, for example, sample from the measure using a random walk.
00:17:18.263 - 00:19:08.754, Speaker B: However, to this Dirichlet form, in the discrete case, there is no corresponding discrete time dynamics, which this implies the mixing of. So actually, in the discrete case, it's more natural to consider a slightly different type of Dirichlet form, which corresponds to the so called glauber dynamics. So the Glabro dynamics is just defined as a Markov chain such that the transition probability from x to y is the indicator that x is a neighbor of y in the discrete cube times the measure at y over the measure at x plus the measure at y. Maybe let's normalize it by one over n. So, you know, this just corresponds to generating a coordinate uniformly conditioning on all the other coordinates. And then, you know, choosing the value of this coordinate according to the conditional measure, and then to this dynamics corresponds the Dirichlet form that looks like this. So nu of x p x goes to y, and then just phi x minus phi y squared.
00:19:11.134 - 00:19:14.034, Speaker A: So, is this a sigma? Just making sure.
00:19:14.494 - 00:20:06.218, Speaker B: Yeah, this is a sigma. Sorry. Yeah, I don't know. My handwriting is. Yeah. Any other questions? So, arguably, this is a more natural Dirichlet form than this one in the context of the discrete hypercube. And the first theorem with Keller and Zeytuni is basically the same thing, but for this glauber dynamics, and also with any magnetic field.
00:20:06.218 - 00:21:40.702, Speaker B: So we basically have a poincare inequality of this form. So for every phi we have that the variance with respect to Nu of phi is at most almost the same. So n over one over four minus the operator norm of a times the Dirichlet form. So, in particular, a corollary is that glauber dynamics mixes in poly time for high enough temperature on the SK model. Ah, good question, Clement. So, Clement is asking, is there something conceptual about this one over four? Yes, there is a reason one over four appears. But first, let me just say that this is not the critical temperature.
00:21:40.702 - 00:22:28.674, Speaker B: So it's expected that this fact is true. Not up to one over four, but up to one half, actually. But one over four is when we have that. So, you know, this, this is not, this is for NEa. This is not only for the SK model, and actually one over four is the phase transition when you consider rank one matrices. So if you take rank one matrices in the all one's direction, you will see that, like, one over four is where you have. So this result is actually sharp, but it doesn't imply a sharp result for the SK model.
00:22:28.674 - 00:22:40.534, Speaker B: Okay, good. So this was the first result. But I actually, in this talk, I want to focus more on another result.
00:22:40.914 - 00:22:50.466, Speaker C: Which ron and I forgot. So the operator norm is the inverse of the temperature. The operator norm is equal to beta factor one half.
00:22:50.610 - 00:23:44.032, Speaker B: So usually when you define the SK model, you have one half here in the exponent, and you have minus v. So, yeah, so if the inverse temperature is beta, then beta is twice or one. Hold on. Yeah, beta, I think, is twice the operator norm in this language. So this is up to like one half of the critical inverse temperature. This works. Okay, so, you know, this was only for quadratic potentials.
00:23:44.032 - 00:25:29.284, Speaker B: And now the question for quadratic potentials it kind of makes sense to define log concavity. So you can still do it like this. But when the potential is not quadratic, then it's not clear at all how to define low concavity, because the potential is only defined in this case on points of the discrete hypercube. Okay, now the question is, how do we, what's the correct notion of log concavity? So I want to suggest one notion, which we'll give a non trivial concentration inequality. And this, the definition has to do with the so called multilinear extension of a function on the discrete hypercube. So let me remind you that for a function f from minus one one to the n to r, there exists a unique way. So there exists a unique function f hat, which is a function from, you know, the two to the power n to r, such that I can write f as a multilinear.
00:25:29.284 - 00:26:30.214, Speaker B: So for every x f of x, I can write it as the sum over all subsets of one to n of f hat of a times this product over I in a xi, right? This is called the Fourier Walsh expansion. Whatever. And another way to think about this. So. So I'm calling it a multilinear extension, because this function was defined on the discrete hypercube. But the right hand side really makes sense in all of rn. And not only that, it's a harmonic function in all of rn, and also it's an harmonic function in the relative interior of every facet of the discrete hypercube.
00:26:30.214 - 00:27:26.016, Speaker B: Instead of a multilinear extension, you can think about it as a harmonic extension. If I have a function defined only on the corners here, then I can extend it harmonically to edges and then extend that harmonically to two dimensional facets, et cetera, et cetera. Until I get a function in the whole solid hypercube, this formula basically gives away, this gives away to make sense of f in minus one one, the interval minus one one to the n. Right?
00:27:26.080 - 00:27:33.114, Speaker C: By the way, you said harmonic extension, but now you gave us different dynamics on the discrete cube.
00:27:34.694 - 00:28:23.320, Speaker B: I didn't give any dynamic. Ah, yeah, well, harmonic, you know, I'm just harmonic with respect to the usual Laplacian in rn, right? I didn't, you know, I didn't connect this to any dynamics yet. Good point, though, Ronan. If this related to Lova's extension, this version of. I actually don't know what you refer to by the lava extension, but, you know, I guess in, in most papers you will open on Boolean analysis. You will see, you know, this notion. Good.
00:28:23.320 - 00:29:35.784, Speaker B: So you know, it doesn't make any sense to ask about the log concavity of f as a function defined here. But as a function defined here, it already does make some sense to ask about log concavity. But on the other hand, it's pretty hard to find low concave functions on, like, when they define what they are defined like this. Because being both harmonic and log concave, those are two things which are pretty hard to satisfy together, if you think about it, right? Because like, if you're harmonic, it means that, you know, the Hessian has trace zero. But on the other hand, we want, you know, all the eigenvalues of the Hessian to be negative. So low concavity would be a way too strong thing to require. I mean, we basically get an almost empty result, luckily, in gaussian space, recall that we didn't need our measure to be log concave.
00:29:35.784 - 00:30:39.720, Speaker B: We just wanted to, wanted it to be not too log convex, because the gaussian measure already gave us some log concavity. And, you know, ideologically, the discrete curve, discrete hypercube should also give us some low concavity. And indeed, that is what happens. So we're not going to ask for log concavity of f, but we're going to just ask for semi low concavity. So we're going to define. So the measure nu is going to be called beta semi log concave. If d nu is e to the v d, the uniform measure.
00:30:39.720 - 00:31:52.904, Speaker B: And the Hessian of the log of v is bounded by beta times the identity. And beta can be positive. And when. And this, you know, the left hand side here means first take v, consider the multilinear extension, and then take the Hessian of the log. So this is, you know, so this is at x for all x in the solid hypercube. Okay? And here's a theorem. If nu is beta semi log concave, then for every one lip sheets, and by one lip sheets, we mean here one lip sheets with respect to the discrete derivatives we defined before.
00:31:52.904 - 00:33:05.960, Speaker B: So it's just one hamming lip sheets. For every one Leib sheets phi, we have that the variance with respect to nu of phi is at most n to the two minus some constant over beta, where circle is a universal constant. And let's look at the right hand side here for a second. Linear functions with respect to the uniform measure have variance n. So we definitely can't get anything better than n here. Okay? On the other hand, n square for one Lipschitz functions is just an empty result, because any one Lipschitz function has oscillation at most n. The diameter of the hypercube is n.
00:33:05.960 - 00:33:42.004, Speaker B: So any Lipschitz function has variance at most n squared. So what this is saying is, basically, you get some non trivial improvement over what you have if you know nothing about the measure. So, for example, if you think about the Sk model in low temperature, it's not going to satisfy this in low temperature, if you think about spin glasses, then you can find one lip sheets function such that the variance is going to be n squared.
00:33:42.784 - 00:33:44.444, Speaker A: Then is better at least one.
00:33:45.344 - 00:33:46.176, Speaker B: Sorry.
00:33:46.360 - 00:33:47.924, Speaker A: Is better at least one.
00:33:50.864 - 00:34:18.142, Speaker B: So, you know, if beta is. I mean, it works for every beta. But then can you. I didn't tell you what this c is, so we cannot take beta to zero. Yeah, you are right. Yeah, you're right. Beta has to be bigger than c.
00:34:18.142 - 00:35:02.514, Speaker B: Yeah, good point. Okay, so, you know, this is the weakest type of concentration you can think of, but it's still like a first step towards some notion of log concavity. And. Sorry for a stupid question. Do you really need the log in the definition of the better log concavity? Semi log concavity? No, I mean, without the log, it would. I think the definition will be just.
00:35:04.074 - 00:35:05.442, Speaker D: But you have v in the.
00:35:05.498 - 00:35:18.498, Speaker B: In the power. Right? So I'm. I mean, you're asking, will this be true if, like, I define it without the log?
00:35:18.626 - 00:35:21.706, Speaker A: Just a nitpick. Just an. In the book, it's not a question.
00:35:21.890 - 00:35:41.514, Speaker B: What. What if you look at. If you look at the one line above DNU, is e to the power v d mutual. Now, log of v is not something that makes much sense for us. Just remove the log. No, but he wants v to be bounded by something away from zero. So I think it's correct.
00:35:41.514 - 00:36:15.026, Speaker B: I mean, it's correct the way it's written now. But I'm just not following so much what you're suggesting. I mean, I think this is the. I mean. Okay, first of all, you know, I'm taking the log of v not only on the hypercube, I'm looking at the log on the entire, like, extension. So I don't want to say extend v inside. I want to extend e to the v inside.
00:36:15.026 - 00:36:20.974, Speaker B: And then, like, does it make sense?
00:36:21.844 - 00:36:22.904, Speaker A: Not too much.
00:36:23.404 - 00:36:32.904, Speaker B: What? X squared. Like. Yeah, sorry, sorry, sorry, sorry. I see. Yeah, I see, I see, I see. I see what you're saying. Yeah, yeah, yeah.
00:36:34.604 - 00:36:36.972, Speaker C: You don't. You just. You just want to get rid of.
00:36:36.988 - 00:37:05.896, Speaker B: The log there, right, right? Yeah, yeah, yeah. So, yeah, this is what I want. Okay. Yeah, yeah, got it. Now. Good. Yeah, sorry, I confused you.
00:37:05.896 - 00:38:30.536, Speaker B: Now, it's correct. I'm taking the actual density, I'm extending it inside, and then I'm looking at, you know, the log. Good. Yeah. Okay, so let me just one remark that this is related to some recent very nice results by Anari Leo Ofe and Vincent. So what they do, they also consider some notion of low concavity for probability measures on the discrete hypercube. But they basically look at, first of all, they look at homogeneous measures, and they find some notion of low concavity which is not invariant with respect to reflections.
00:38:30.536 - 00:39:16.296, Speaker B: So for their notion, somehow they have a very, like, the direction one, one, one has a, you know, is, it has a special importance. And I don't want to write exactly what they know. The notion they consider is they. Okay, I'll just say in words, they consider the so called characteristic polynomial of the measure, and they want it to. To be log concave at a certain point. And also they want all its derivatives to be log concave, and they get some point in equality for these kinds of measures. So I think.
00:39:16.296 - 00:40:15.760, Speaker B: So it seems like there is some relation between those two results. But what they do is their proof is much more algebraic, and what we do is, like, both the notion and the proof is, like, much more analytic. Oh, sorry, I also forgot to say, this is joined with Omer Shamil. So this is. So, you know, just for, for those of you who know these series of results, interesting to compare them, but it seems that, you know, the relation is more superficial, even though they do have some low concavity involved there. Okay, so, any questions about this result? If there are no questions, I do want to.
00:40:15.952 - 00:40:25.924, Speaker D: How is your result related to the previous result? And also to the case of product measures where f is exponential to a linear function.
00:40:28.024 - 00:40:42.992, Speaker B: I see. So, first of all, for product measures, you already have concentration pretty easily, because the inequalities, tensor eyes. So I'm not sure.
00:40:43.048 - 00:40:47.056, Speaker D: The question is whether you reconstruct whether you get the same result here or.
00:40:47.080 - 00:41:23.214, Speaker B: You already suffer, because I already do. Like, this never gets all the way to n, so it just gives some improvement, but it doesn't get all the way to n, and it definitely does not give up one career inequality. It only gives concentration for Lipschitz functions. We didn't try actually to push it much further. We were just looking for some kind of concentration for the more general case. So it definitely doesn't recover what we had before.
00:41:26.594 - 00:41:27.494, Speaker D: Thank you.
00:41:30.794 - 00:41:37.694, Speaker B: Okay, so let me try to give you some ideas of the proof in the time I have left. So.
00:41:47.214 - 00:41:50.514, Speaker A: So since you started five minutes later, you also have five minutes.
00:41:52.694 - 00:41:55.710, Speaker B: So how long do I still have? What does it mean?
00:41:55.822 - 00:41:57.022, Speaker A: So, ten minutes?
00:41:57.198 - 00:43:19.056, Speaker B: Ten minutes. Okay, great. So, okay, so I have this measure new on the discrete hypercube, and I want to try to explain to you how I can use, it's, you know, this low notion of low concavity. And the first step, so step one is going to be the following. So it's a reduction to something like you can think of as a small transportation between tilts. So what do I mean by that? Let's define so for any measure nu. Let's define tau sub w of nu to be the measure whose density is like the density of nu, but tilted in the direction w.
00:43:19.056 - 00:46:09.094, Speaker B: So I want d of tw new nu at x to be defined as e to the w dot x d nu x over the integral of e to the w dot y d nu y. So you can just think of it as taking the measure mu and exponentially tilting it in, you know, the direction w. Okay. And step one is going to do the following reduction. So suppose that for all w in rn and for all unit vectors theta, we have that the Wasserstein distance, which I'll define in a couple of seconds, between the tilt w of nu and the tilt in direction w plus epsilon theta of nu, when epsilon is small, is at most some constant where w one between two measures nu one, nu two is just defined to be the supremum over all one. Leipzig functions of the integral of this with respect to, of, you know, this difference, which is also, you know, the cost of earth moving with respect to the hamming metric between nu one and nu two. So suppose that if I tilt my measure in the direction w and I tilt it in a direction very close to w, then I know that I have a good transportation plan between these two measures.
00:46:09.094 - 00:47:31.196, Speaker B: Then nu satisfies the concentration as above. I'm not going to explain how to prove this step, but I am just going to say that a very similar idea appeared in a paper of boas, probably 2008 or something. I think, uh, in the context of low concave measures in some vague way. Uh, um, very vague, pretty vague way, I guess. But I mean, if you'll see the proof, this is not so, so vague. So let me just say that this, this, this notion is somewhat related to something called the h minus one norm of the measure. Okay? So this, I think this step is like more standard in analysis.
00:47:31.196 - 00:48:32.002, Speaker B: So I don't want to, you know, I don't I don't really have time to explain it so much, but this, this property that, you know, if you tilt the measure a little bit, you don't get something very far away. May I do want to give you a little bit of intuition of why this is related to concentration. So suppose I have some measure on the hypercube. Let me draw the discrete hypercube like this, which is on this side of the hypercube, it's concentrated here. And on this side of the hypercube, it's concentrated like in the antipodal direction, where I think of this direction as being one dimensional. And this direction, I'm thinking about it as the rest of the dimensions. So this is like n minus one dimension in this sketch.
00:48:32.002 - 00:50:00.394, Speaker B: Okay, so suppose that if I change this coordinate from minus one to one, then the center of the measure changes a lot. I mean, in this picture, it only changes by one coordinate, but this one coordinate, think about it as, you know, many coordinates, then this is not concentrated, the picture I did here, because these two pieces are very far away from each other. And this non concentration is going to be demonstrated in the fact that if I tilt the measure a little bit in this direction, then all of a sudden this point gets more measure. And in order to transport it back to the original thing, I have to move mass all the way from here to here, which costs a lot. Okay. So somehow concentration can be reduced to the fact that if you like, push the mass in some direction, then moving it back doesn't mean you have to go in a diagonal way, but you can kind of go in straight lines. Okay, this, I don't know if you like this intuition or not, but this is the best thing I can give you, unfortunately.
00:50:00.394 - 00:51:00.458, Speaker B: But now I want to talk about step two. So step two. And this is how to prove a fact like this. And how do you prove a fact like this? So actually there's a very nice stochastic process that gives, it gives this almost in a straightforward way. So let's define a stochastic process as follows. Let bt be a brownian motion in rn. And let me define, okay, let me first write a stochastic differential equation.
00:51:00.458 - 00:52:04.058, Speaker B: So consider the stochastic differential equation w zero is equal to w and DWT is equal to dbt plus the gradient of log f, which is the harmonic extension of the density at wt dt. So I just recall that d mu was f d mu. Okay, so you know, this, this thing makes sense and. Yes, hold on. Sorry, sorry, sorry. Something is. No, not exactly.
00:52:04.058 - 00:52:54.784, Speaker B: Sorry. Yeah. Okay. Let me just call this a sub nu of wt dt. And a sub nu is not exactly going to be the gradient of log f, but almost. So I'm going to define f of theta as the log Laplace transform of the, of the measure and u. So this is going to be the integral e to the x theta d nu of x log.
00:52:54.784 - 00:54:05.604, Speaker B: And I'm going to define the vector a sub nu at w to be the gradient of log. The gradient, sorry, of f at w. Okay. So now these equations are all well defined. Okay? And a nice fact is that if I look at a sub nu of w t and I take its limit, so it's not very hard to see that a sub nu is always going to be in the convex hull of the support of the measure. So it's always going to be minus one, one to the n. And if I take this limit to t equals to infinity, then this is always going to converge to a point.
00:54:05.604 - 00:54:54.754, Speaker B: So this is going to converge to a point to some random variable that, let's, let me call it x. And this is going to have the distribution new. Sorry. It's going to have the distribution of tau, sub w of nu. I know this is a lot to follow in the last five minutes of the talk, so you don't need to understand all the equations. The only thing I want you to take from here is that there is a natural stochastic process which samples from the measure new. So this stochastic process lives in rn.
00:54:54.754 - 00:55:04.994, Speaker B: But if I invoke this function, a sub nu, then it converges to a point which samples from tilts of the measure nu.
00:55:05.294 - 00:55:08.674, Speaker C: Is there a natural way to write this backwards in time? Because.
00:55:12.374 - 00:55:19.994, Speaker B: Probably yes, but I can't, you know, I'll need to think in order to understand how.
00:55:21.874 - 00:55:31.194, Speaker C: Only because this one is hard to couple with the measure that samples from. You know, it's hard to couple different w's with this notion. I mean, maybe it's not, but.
00:55:31.354 - 00:55:50.520, Speaker B: No, I mean, it's, it's, it's not. Yeah. So, right. So, so you've, you're one step ahead of me. So now, you know, I want to consider two tilts. So it's actually pretty easy to capital couple different w's. You just basically take the same brownian motion.
00:55:50.520 - 00:57:07.314, Speaker B: Almost. So this gives me a way to sample from every tilt of new. And in order to say that two tilts are closing transportation distance, then it's enough to find the coupling such that the two endpoints are close in transportation distance and the whole point is I know I'm out of time, so let me just say it in words. The whole point is that the way we defined log concavity exactly corresponds to the fact that this function, a sub nu, is Lipschitz. I mean, it's a Lipschitz vector field. So, I can actually start from two different tilts, run the process, and couple them in a way that they don't get too far from each other very fast. So, just by considering low concavity and using it in a straightforward way to analyze two coupled versions of this process, I can just immediately prove the type of transportation inequality I showed you before.
00:57:07.314 - 00:57:16.574, Speaker B: So let me end here. But I hope I gave you some taste of, you know, what's going on in the proof.
00:57:21.234 - 00:57:28.654, Speaker A: Thanks, Ronan, for a brilliant talk. Any lecture? Any questions to Hoennen? We have time for one quick question.
00:57:29.434 - 00:57:36.214, Speaker D: Don't you need to control the Langevin dynamic all the way to infinity in order to get your transportation?
00:57:37.834 - 00:58:09.482, Speaker B: That's true, but it's not very hard to see that the derivatives of this fun. Like, the second derivative of this function, becomes very, very small as time increases. So, in some sense, the right way to look at this is re parameterize time exponentially. Like, if you just take the uniform measure. You see that this thing becomes small exponentially. I see. Okay.
00:58:09.538 - 00:58:10.334, Speaker D: Thank you.
00:58:12.634 - 00:58:24.954, Speaker A: Okay. Any other questions? Okay, so, thank you again, Ronan, and let's meet again in five minutes, which is 1005 Pacific time.
