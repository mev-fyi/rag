00:00:00.920 - 00:00:27.352, Speaker A: Okay, thanks. When I submitted that abstract, I left it pretty vague because I figured then I could talk about whatever I felt like when I got here. And Orlean gave a great talk yesterday. That is close to some of the stuff I hadn't planned to talk about. So I kind of changed the title a little bit here. But, you know, people say, oh, I want to pick your brain on something. Well, I'm thinking about using multi armed bandits to pick a whole bunch of brains for information.
00:00:27.352 - 00:00:50.208, Speaker A: And that's what I'm going to talk about. And this is a picture of our lab back in Madison. Okay. So, really, I'm going to keep it pretty simple. I want to give you one kind of motivating kind of application to think about, and that is sort of a contest type of problem. So suppose you have n items. They could be n beers, for example, in a beer contest, n papers.
00:00:50.208 - 00:01:59.812, Speaker A: At a conference, you want to choose the best student paper or n resumes. You're trying to hire somebody, and you want to find the best person, person for a position, and you have a pool. And I'm really thinking more about large pools of people who could judge these items. And the question is, if we want to find the item out of this set of n that is most preferred by all the judges, how should we allocate the judging? And there's sort of, at some very simple level, two ways you might think about doing it. First of all, you could say, well, let's just somehow uniformly allocate our judging resource over all of the items and then collect the ratings and see which one gets the highest rating out of all those that were judged. Or rather than just sort of that uniform distribution, you might say, well, maybe we can start collecting ratings, getting judges to look at items, and as we get a little bit of a sense about what's going on, maybe we might see some of the beers are really, really not liked at all, but there's others that people are giving higher ratings. Maybe we want to stop asking judges to taste those yicky ucky beers and focus them more on the beers that might be winners.
00:01:59.812 - 00:02:44.374, Speaker A: Or maybe we want to have some of our judges look at certain papers more than others, because certain papers seem like they're really in contention for that best paper award. Okay? So that would be an adaptive allocation. So this is, I'm going to sort of formulate this problem as a multi armed bandit, and I just want to set up a little of the notation. It's pretty simple. So we have n arms. So each arm corresponds to one of the items, we have expected ratings or scores, that we get for each item, and I'm going to order them from the highest score to the lowest expected score, like this. So mu one is going to be the top item, and its expected score, or rating is mu one.
00:02:44.374 - 00:03:10.354, Speaker A: And then I have the rest of them. And I'm going to assume that there's a distinct winner, namely that Mu one is bigger than Mu two and all the others strictly larger, so that there's no issue of a tie. We'll just avoid that. Okay. We don't know the order, of course, but I'm going to keep them in this order just so that when I show pictures and so forth, we'll just always think of arm one as being the best arm. But, of course, we would never know that going into this contest. Problem.
00:03:10.354 - 00:03:41.136, Speaker A: What we can do to get information about these means, we want to basically learn that Mu one is the largest. We can get random ratings from judges. And so the ratings are going to be. So Xi is a rating for item I from Judge J. And we're assuming those are coming from a probability distribution, who's parameterized by that mean expected rating, mu I, and the. I don't know why I have two of these here. That's funny.
00:03:41.136 - 00:04:28.824, Speaker A: At any rate, the judges we're going to assume are iiD. So we're just getting basically iid samples of mu. I think of those as noisy versions of Mu I. So that's Xi is a noisy sample from this distribution, P mui. And then the natural thing we would do is collect a bunch of these ratings or scores and average them. And that would give us an empirical estimate of that, of the different score expected scores. And the idea then, of course, is to use these empirical average is to choose I hat so that I hat not equal to one which is the correct winner that we're trying to find is happening with small probability some delta, which is a small number.
00:04:28.824 - 00:05:33.438, Speaker A: So we want to be correct with probability one minus delta in picking that the first item is the one that should be the winner. Is that pretty clear? Okay, so then the way you start doing this usually is you would say, well, what can we say about our empirical means? So we're going to assume that these are subgaussian distributions, and then we can have a little concentration inequality, saying that the empirical means are concentrating around the true means. So it looks something like this for some constant c. And this is just, you know, could be turned off if things were Bernoulli or Hoeffding, if they were bounded. Or it might just be a gaussian tailbound if things were gaussian. But those are just all special cases of this. And what that's really telling us is that with probability, at least one minus delta for a particular arm and a particular number of judging, say, t, I can say that that true mean is sandwiched between these two kind of confidence limits that are defined in terms of my empirical mean.
00:05:33.438 - 00:06:10.438, Speaker A: And then these confidence bounds, things that come from the concentration inequality. So that's really simple stuff. So I just said for fixed I and fixed t, this is what we had, but we kind of want to have it not just for a particular item, but all the items simultaneously, ideally. So then we're just going to take delta and change it to delta over. Nice. So we effectively have a union bound over all the items. Now, I have confidence intervals for all items simultaneously at a particular number of samples t.
00:06:10.438 - 00:06:39.518, Speaker A: And then I also want to hold it for all numbers of samples. So maybe one sample, two sample, three sample, and so on and so forth. I don't want to say I know in advance how many samples I'm going to take. And so then I'll also divide by two t squared. So I'll take delta delta over this and that. Again, I can union bound everything, and then I'll have confidence bounds that hold for all I and all t. Okay? So once you do that, things are pretty easy.
00:06:39.518 - 00:07:19.434, Speaker A: So, first of all, let me explain, like, something that you probably would never think you would do, but let me just talk about it just to kind of get everybody going here. So let's think about non adaptive. What would you do? So, non adaptive, again, this is like this uniform allocation of judging across all the items. You just judge every item an equal number of times. So we have equal confidence. The confidence interval widths are equal across all n items. And you're just going to keep sampling every arm uniformly until you get to a situation like this where one guy's confidence interval is strictly above all the others.
00:07:19.434 - 00:08:12.044, Speaker A: Then, you know, without any confusion, at least up to this sort of confidence level of delta, that indeed one is the winner. So that's what you could do if you. That's what you would do, I guess, with this setup, if you were being nonadaptive about how you allocate the judges. And then you could say, well, when is that going to happen? Well, basically what it means is that the confidence intervals have to be small enough so that the difference between the first and second mean mu one minus mu two, which I'll define that gap as delta sub two. You have to have that the confidence intervals or two of these guys be smaller than that gap. So this is one of my, this is like one side of a confidence interval is what I have right here. And I need twice that and twice that again, which gives me this four.
00:08:12.044 - 00:09:04.480, Speaker A: And so basically it's saying you have to take enough samples so these confidence intervals shrink enough so that they're small relative to the gap between the first and second best items. And if you just then invert that, you can see that the time it's going to take is a constant times or no more than a constant times, the inverse gap squared delta two to the minus two, and then the log of n over delta delta two to the minus two. So that's how many samples per arm are sufficient in this nonadaptive setting to determine the best one with confidence delta. And then if you just say, how many total samples would that be? Well, I have to do that for all n, so I just have a factor of n there. So that's non adaptive. That's not what people would normally think to do. That's not really how you think about multi arm bandits.
00:09:04.480 - 00:09:50.454, Speaker A: So what would you do with an adaptive scheme? And this is basically what's called successive elimination. It's an algorithm that's been around for a while, even darn at all. We're the first to maybe the first to analyze it. And the idea is that, well, as soon as we see this confidence interval, save our best guy, our upper confidence bound, if this guy's lower confidence limit is above the upper confidence limits of some other arms, we can just safely discard them. So you would, obviously, you would not keep sampling guys that you already know cannot be the winner. And that's the idea. You just get rid of these, and then you only focus additional sampling on the ones that remain.
00:09:50.454 - 00:10:42.736, Speaker A: And you just keep doing this, kicking guys out as soon as their upper confidence limit falls below the max lower confidence bound. And using the same sort of reasoning that I just had on the previous slide, you would say, well, when would the ith arm be removed? Well, the number of samples you need, Ti is going to be on the order of the gap delta I to the minus two, and the gap delta I is the gap between mu one and mu I. So that's that gap between the top arms, expected reward and the ith arm. And then you have the same business here. This log n over delta delta I to the minus two. And then the total samples would be a summation of all those Ti's okay, so let me. So this, this is a little bit better than what we had in the nonadaptive thing.
00:10:42.736 - 00:11:16.716, Speaker A: And let me just show you an example that kind of illustrates this. So let's look at a case where we have basically two good arms, one slightly better than the other. And in fact, let's make the gap kind of small, shrinking with n. Let's say it's one over the square root of n. So those two are pretty darn close. And so you should think we ought to really pay most of our attention, most of our judging on those two, to tease out which one's truly better, if we want to really give the paper award to the most deserving paper. But all the other papers are pretty crummy, it turns out.
00:11:16.716 - 00:12:00.176, Speaker A: So they're all say they're deltas. Their gaps are greater than a constant, maybe one. Okay, so in this case, the non adaptive procedure that I said uniform allocation of judging would be basically order n squared, total samples you'd have to collect, versus something like order nice. So there's a, you know, potential. This is an extreme kind of artificial case, but you can see the gap between adaptive and non adaptive could be pretty large. Okay, another thing you notice here is, you know, we're getting this log n because we were doing this union bounding to get all these simultaneous confidence intervals for every arm and every time. And you could.
00:12:00.176 - 00:12:41.734, Speaker A: Well, these. I'm sorry, these are just for union bounding over all the arms. And so you could wonder, is that really necessary to. Do you have to suffer that log then? Okay, so I want to kind of think a little bit about what's really going on in this problem. Again, it kind of relates a little bit to what Orlean talked about yesterday. But suppose we were just looking at two arms like we were just talking about those best two arms in that last example. And naturally, what you would think about doing is you would take the scores you're getting from those guys and compare their empirical means, or equivalently, just look at the sum of these differences.
00:12:41.734 - 00:13:12.192, Speaker A: And if that's greater than zero, then you're going to decide one. And if it's not, you decide two. And so if the two means were actually equal, in case that the gap then would be zero. And think about this. Maybe it's just being gaussian variables here. Then you would just have some zero mean random walk like this. On the other hand, if the mean mu one is really bigger than mu two, then the delta two is greater than zero.
00:13:12.192 - 00:13:49.274, Speaker A: And you're going to have some drift. And so your random walk is going to be a walk with a drift. Okay, so let me see here. So there's the drift. And what do we know about random walks? Well, we kind of know that at least the zero mean random walk ought to stay in a kind of envelope which is given by the law of the iterated logarithm. So this random walk, the blue one, shouldn't kind of, morally speaking, shouldn't come outside of this square root of two t. Log, log, log t, which is the li l bound.
00:13:49.274 - 00:14:29.352, Speaker A: Okay. And the idea then is to say, well, at some point the drift is going to take you outside this. So probably then you would think that if there was a drift, your drifting random walk will cross that li l bound. And so the idea is roughly to say how long is it going to take? Well, it's the drift slope, which is your delta times the time. And when does that equal this li l bound, which is the magenta curve I have there. And if you work that out, then it sort of suggests that that's going to happen kind of around time. Delta to the minus two.
00:14:29.352 - 00:14:43.804, Speaker A: Log log. Delta to the minus two. So now we're seeing, first of all, we still have that inverse gap squared. That's the delta to the minus two. But then instead of log of the delta to the minus two, we have log log. So we have a double log. That's an improvement right there.
00:14:43.804 - 00:15:13.254, Speaker A: Okay. And so what I want you to sort of think then is that really maybe our sample complexity is something more like this. So it's the sum of the inverse gap squared and then a log log term and no n at all. No, no union bound over all n arms. That's sort of what we would hope for. And we think this is probably about as good as you can do it. Certainly we know it's as good as you can do when you just have two arms.
00:15:14.074 - 00:15:21.534, Speaker B: I forgot, what are you trying to achieve? You're comparing here an almost sure bound, which is a logarithm against the mean of another one. So, right.
00:15:22.714 - 00:15:33.746, Speaker A: So what I'm trying, so what I'm just trying to say is that if you squint, this is what you might hope for. This is not a you want with.
00:15:33.770 - 00:15:37.854, Speaker B: Probability, more than one half that one of them can be distinguished from the other. I mean, what is the.
00:15:38.184 - 00:16:18.900, Speaker A: Yeah, you want to eventually, with probability or confidence, at least one minus delta. You want to decide that correctly, that mu one is the best one. So it's the same problem as before down here, same criteria in this fixed confidence setting, as opposed to a fixed budget, which we also heard about yesterday. And then I'm sort of saying, based on this kind of hand wavy stuff here, this is what I hope to get to. All right. So it turns out that it's not terribly difficult to get a kind of finite sample version of the Li L. And you can get confidence intervals that then look like this.
00:16:18.900 - 00:16:58.468, Speaker A: So you have this log, log thing in there. Okay? And this holds for all t. But just for a given I, it's not a unit, it's not a uniform bound for all I, it's just uniform for all t. And we're going to use a slightly different strategy. It's still confidence bound based, but it's called a UCB style algorithm, upper confidence bound algorithm. And the idea is that we have now what I'll call these Li l confidence intervals. And the way a UCB algorithm works is it said every time you're going to take a new sample, look at which arm has the greatest upper confidence bound.
00:16:58.468 - 00:17:33.483, Speaker A: So the greatest one of these upper guys, and sample that one to try to pull it down. Okay. And what that roughly says is that you're equalizing all these upper confidence intervals. And so the picture would look something like this. And what it's telling us is that you're going to be sampling the arms that have large means much more frequently than these. The larger the confidence interval, the fewer samples you've taken. And so it sort of automatically focuses your sampling, biases it towards potential winners.
00:17:33.483 - 00:18:22.278, Speaker A: Okay. So I'm not going to go into too much of the analysis of the algorithm. I just kind of want to give you a hint of what's going on, though, is that eventually, somehow, it's very, very hard to push the upper confidence bound of the top arm down unless you keep sampling it again and again and again and again, because he's got a gap between the first and then the next best guy. And you end up, basically, this algorithm will focus all of its sampling attention on that best arm. And so our analysis is actually to sort of explain that and quantify it a little bit more. So there's sort of two key steps. The first is to show that the suboptimal arms using this UCB algorithm will be sampled first finitely many times.
00:18:22.278 - 00:19:17.054, Speaker A: And in fact, you can get a bound like this that with high probability, the total number of samples you take from arms two and larger is going to be bounded by something like what we were just looking for, what we were hoping for before, in terms of that total complexity bound. And another thing, the next thing you do is you show that no suboptimal arm is going to be sampled more than some constant number of times the total number of samples collected from all the arms. So this is just like all the other arms, some constant. And you're saying you're never sampling any one of the suboptimal arms more than a constant times all the other samples you've taken. And if you put these two things together, basically the idea is that at the end of the day, the only arm that won't satisfy this, at some point, the optimal arm won't. And then you stop. That's your stopping criterion.
00:19:17.054 - 00:19:57.366, Speaker A: The key at a very, very loose, high level is that rather than trying to control all of the arms and their confidence intervals individually, which we could have done with that successive elimination style algorithm, that's what we were doing. We're just kind of looking at controlling averages of things. And this averaging, it gets kind of subtle in the details, but basically that's why it works. You don't ever have to take a union bound over arms because you're doing this averaging. The only one that you're really guaranteeing control over is that top arm. And then the other things you're controlling are averages. And so this is, here's a movie of what happens.
00:19:57.366 - 00:20:44.174, Speaker A: This is a simulation, and you can see that equalizing the upper confidence bounds. And you can see that eventually I just keep banging on that top guy over and over. That's exactly what happens. And then the theorem that I say here is basically saying, as long as the arms are subgaussian, then you do get this inverse gap squared log log kind of sample complexity, and we don't have any reason to believe that it could be improved from that. So that's one thing I wanted to explain. Is that clear to everybody? Good. So one of the reasons we set out to do this is because we work with a bunch of cognitive scientists, psychologists and other people, and they have real problems where they want to collect information from people and they want to do it really efficiently.
00:20:44.174 - 00:21:48.672, Speaker A: And when we started implementing some bandit algorithms for these types of tasks, we noticed that some of the algorithms that were really nice in theory were very, very. Yeah, the constants were super huge. And this kind of algorithm is actually something we would probably use. It's like the constants aren't so bad, and I think they're practical algorithms. So we're sort of trying to take some of the theory, which sometimes is you can get some bounds that look good if you ignore constants, but then they end up being kind of crummy algorithms for the scale of problems that some of the psychologists are looking at. Like, if you only have 100 items and you have 1000 people on mechanical Turk answering questions, you might not be in a regime where you can kill off really huge constants, like 3000 out in front of your bounds. So another thing that they're really interested in, this is kind of taking me to the second kind of idea I wanted to convey today, is that they're often not interested in getting ratings or scores from people, because it's hard to calibrate different people.
00:21:48.672 - 00:22:28.930, Speaker A: And so instead they'll just try to look at comparative judgments. So what they'll do is they'll basically give people a choice, a forced choice between two items and say, which one do you prefer or which one's better? In some sense, that's a paired comparison, forced choice kind of experiment, very common in psychology. So, for example, rather than asking you to rate beer a or beer b, I might just say, here are two beers, take a taste of each. Which one do you like better? Okay, so you can formalize this also as a bandit type problem. It goes by the name of dueling bandits, which comes from this paper by you et al, in 2012. And the basic idea is this. So you have a bunch of probabilities.
00:22:28.930 - 00:23:00.894, Speaker A: Pij, that's the probability that arm I is preferred to arm J. Okay? And then you get samples of those, basically Bernoulli samples. From that probability, you see outcomes of people making these comparisons. Compare item I to j, and they either say better or one's better or the other. So there's one or zero. Now, things get a little trickier here. In this case, there isn't a clear way to say what you mean by the winner in this kind of situation.
00:23:00.894 - 00:23:31.000, Speaker A: And so there are a lot of different criteria. There's a Condorcet winner, which is basically saying if there's any item or arm that is probably better when compared to any other individual arm, you'd like to pick that one. That's the Condorcet winner. It generally would not exist. So that's one problem with the Condorcet criterion. It's really nice if a Condorcet winner exists, but it, it doesn't always exist. Another option is Borda winner.
00:23:31.000 - 00:24:18.204, Speaker A: That's what I'm going to focus on. And let me just kind of explain what the Borda idea is. So the border idea is that we'll define the score of each arm, or the expected score or rating to be just the average probability when I compare item I to all other arms. So I just take that average. So what this is saying is you'd like to then pick the item or arm that has the largest score on average, or largest probability on average compared to all the other arms. So it's going to win more duels on average when paired with a randomly chosen other arm. And so that's the border score.
00:24:18.204 - 00:25:18.674, Speaker A: We can simulate samples from an arm by just basically saying a sample from arm I is you take arm I, you dual it with some other arm j, where j is selected uniformly at random over the remaining arms, and that gives you a sample from this distribution. So you duo with a random other guy, and that simulates a sample from this, that's a noisy version of this score. And so once you have that Borda reduction of the problem set up, we're back in the situation where we can just work with the mus. We know how to get samples that on average tend to the mus. And so all the algorithms that we had before apply. So that Li l stuff and the UCB algorithm, we can just use it now. Okay, so that's one of the nice attractive features of the Borda approach, is that it simply reduces back to a normal kind of best arm bandit problem.
00:25:18.674 - 00:26:19.404, Speaker A: So we thought, well, is that the end of the story? You just do this border reduction, and now we can deal with dueling bandits, at least under this border criterion, just using all the other things we already know about. And we started to wonder if that's really true. Is this the best we could do if we want to find the border winner, which would, again be the guy that has the largest score here, which I'm going to call mu one again, is this the best strategy to do this reduction and just go from there? So, to think about that, I'm just going to kind of look at two examples. So these are two different matrices of these pijs, these dueling probabilities. And basically, in both cases, the first two arms, which are the first two columns here, are the two top guys, and one is the best. So it's the same in both cases. We have large probabilities in those two columns in rows, three quarters roughly.
00:26:19.404 - 00:27:12.516, Speaker A: But there's a difference here. So why is one better than two? Well, he's just slightly better than two when dueled against arm three. So he's got a little epsilon edge on arm one. He's also got sort of an edge on arm, sorry, he's got an edge on arm two. He's got an edge on arm two in this case, too. But now that epsilon rather being concentrated just in one dual, is sort of spread out over a bunch of duals. And so somehow our thinking was that this would make things, this situation here would be easier, because maybe if we could figure out who are the top guys and then just focus our dueling on those, and if it's revealed which one's better just by one dual arm three, in that case, maybe we can capitalize on that structure.
00:27:12.516 - 00:27:43.120, Speaker A: And so it turns out you can if you know the probabilities up to a permutation. So let's just say I gave you all those probabilities. I know that there is this three four plus epsilon somewhere in here. I just don't know exactly which arm it is and so forth. Then how could we capitalize on that information? I'll show you in a second. But what we're going to see is that the total number of samples required in this case to find the border winner is sort of like order n. And the total number of samples in this case is going to be like order n squared.
00:27:43.120 - 00:28:24.354, Speaker A: So there's a big difference, even though if you look at the board of scores, which would just be averaging across the rows here, say the border scores of the first two arms are the same in both cases. So both these P's basically end up with the same border scores, which are those mU's. But one problem is much easier than the other, is what I'm trying to say. So let me just kind of step through how we would, what you might do here if you knew this matrix up to a permutation. You know, that's the problem you're dealt with. You just don't know which arm is which. So what you might do is you'd first dual each arm with log and randomly chosen other arms.
00:28:24.354 - 00:29:11.410, Speaker A: And what that's going to do is that's going to give you enough information to basically determine that arms one and two are the best, because these guys are going to sort of look like their border score is going to be more like three quarters, and all the other guys are going to have a border score that's more like one half. So you've got a pretty big gap between those border scores. You do a small number of random duals and you can home in on these two guys are the only real contenders. Once you found that out. Now you know that these two guys are these columns, you just don't know where the three four plus epsilon is. Is it arm three? Is it arm four so on and so forth. So what you're going to do then is you'll dual arms one and two against each other arm, something like one over epsilon squared times.
00:29:11.410 - 00:29:51.400, Speaker A: And that gives you enough of a kind of epsilon accurate estimate of all of those other sort of partial border squares, if you will, to deduce that indeed this one is bigger than this one. And I can finally then decide that arm one is better than arm two. And that's what gives me this calculation right here. I have to do that with every arm. So I have to do that n times. Okay? So that's where that order n comes. If we tried to do the same thing with the other case, we would again say dual each arm with log n other arms chosen uniformly at random.
00:29:51.400 - 00:30:18.804, Speaker A: That would again tell us that one of these two is the best. And now you could say, well, once I know that, I know, I could just compare these guys to any other arm, say arm three, because they're all going to be the same story, just slightly better for arm one in every case. But then, to get the level of accuracy, I need to distinguish these two. I have epsilon over n. I swear that I get n squared over epsilon squared. Sorry. Yeah, 1 minute.
00:30:18.804 - 00:30:42.090, Speaker A: Okay, good. I'm almost done. So that's just sort of an argument for kind of like an upper bound. But you can use Fano's inequality and show this is a lower bound tool. Okay, so some problems are easier than others. What's the key idea? Somehow there's some sparsity lurking around in here. Most of the duals are not informative for distinguishing the big, the top two here, whereas all of them are sort of informative in this other case.
00:30:42.090 - 00:31:24.874, Speaker A: And so we want to exploit that sparsity. And this is the idea. I have 1 minute. So I'll just say that again, the board of gaps, the board of scores. We could do what we were doing before with the lil, but we want to know, could we adapt to sparsity? The long story short is not agnostically, and if you use some of Orielian's very nice work, you can actually prove a lower bound that if you want an algorithm that's going to give you this delta confidence for all p matrices, regardless of whether they're sparse or not, you're not able to adapt to sparsity. You really have to suffer something very close to this. You can't really improve using sparsity.
00:31:24.874 - 00:31:55.326, Speaker A: So really you can't be adaptive to sparsity agnostically. But if you start off with an assumption of sparsity, you can do better. So what's the idea? Let's suppose that the best arm is differentiated by any suboptimal arm by a small subset of, say, at most, k duals. Then the idea is basically like that successive elimination I was talking about. But instead of just eliminating arms, you also eliminate duals as you go. And you start to focus in on what are those really discriminating duals. And if you do that, you can prove some nice results.
00:31:55.326 - 00:32:23.714, Speaker A: But basically you can prove you're able to improve on that basic border reduction under this sparsity assumption. And this is just simulating for that p one matrix that we were just looking at. This is using this, what we call sparse border algorithm. The slope is one. So n to the one. Basically linear growth versus n squared, that's the growth. If you use that li l algorithm, the border reduction I spoke of, and then this is just showing you that it actually works on some data where we simulate this process.
00:32:23.714 - 00:32:35.166, Speaker A: So that's it. I'll stop there. Here are the students. Kevin's here. He's sort of the lead on a lot of this research. So if you have questions, you can talk to him. Matt was another student who meets a student in Sebastian Bubek at Microsoft Research.
00:32:35.166 - 00:32:35.374, Speaker A: Thanks.
