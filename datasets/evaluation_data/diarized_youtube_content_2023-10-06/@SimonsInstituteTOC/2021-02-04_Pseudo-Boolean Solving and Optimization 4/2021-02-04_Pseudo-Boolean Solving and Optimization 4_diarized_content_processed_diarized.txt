00:00:03.600 - 00:01:09.894, Speaker A: Welcome to part four of the tutorial on pseudo Boolean solving and optimization. This is part of the bootcamp for the program satisfiability theory, practice and beyond at the Simons Institute for the Theory of Computing. Previously, in part one, we defined what a pseudo Boolean formula is. It's a collection of zero one integer linear inequalities. In part two, we discussed pseudo Boolean solving, which is the problem of deciding feasibility of such pseudo Boolean formulas. Is there a zero one assignment satisfying all of these linear inequalities? And we discussed method of solving this by using ideas from conflict driven closed learning Sat solvers. Either you could rewrite your pseudo boolean formula to conjunct in normal form to Cn's and run a CDCl solver, or you can keep the pseudo Boolean formula, but try to lift the conflict driven techniques to native pseudo Boolean reasoning.
00:01:09.894 - 00:02:39.296, Speaker A: Then in part three, we added a linear objective function and you want to minimize so you want to find a solution that satisfies your formula but minimizes this linear objective function. Discussed different ways of doing this using ideas from Max sat solving, for instance. And we saw some examples of experimental results for different implementations. And in these experimental results we saw that apparently mixed integer linear programming solvers are also fairly good at sudo Boolean optimization problems, providing some fairly stiff competition. And so in this fourth and final part of the tutorial, we'll look a bit more closely at these mixed integer linear programming solvers, how they work and what they do, and whether we can maybe borrow some of these techniques to improve our pseudo boolean solvers, and maybe export some pseudo Boolean solving techniques potentially to make a mixed integer linear programming solvers better. So a big caveat here is that I am not certainly an expert on mixed interlinear programming. So you should view this as a very biased survey, a personal take of this material, as I try to learn it enough to make use of it in my own research.
00:02:39.296 - 00:04:11.844, Speaker A: So I'll try to tell you a little bit of my understanding of mixed interlinear programming and integer linear programming ILP, and then tell you about some ideas for combining pseudoboolean solving and MIP techniques. Why this is not maybe so easy to do, but I'll also tell you about some ideas that seem to work. I'll report on a case study on some research that we have done on this, and then, as in the previous parts of this tutorial throughout, I'll try to highlight a number of open problems and research directions. So what is a mixed integer linear program we have an objective function summing over j a j xj for variables xj. We want to minimize this subject to a bunch of linear constraints AI j xj summing over j less than equal capital AI. And if you have watched the previous tutorials, you know that in the pseudo boolean setting we love normalized form where we always want greater than but it seems that MIP people tend to write their programs with less than or equal. So in this part of the tutorial will defer to MIP standard and use less than or equal some of the time.
00:04:11.844 - 00:05:40.234, Speaker A: So the variables are, some of them are integer valued, but there are also potentially some real valued variables. We think of all variables as being non negative, and as we already said, we have linear constraints and a linear objective function, except that if there are no real value valuables, then we have an integer linear program. If in addition all variables are bounded between zero and one, we have a zero one integer linear program and a zero one integer linear program that is just a pseudo Boolean formula. And if we have a vacuous objective function, say summing over j zero times xj, then that's just a pseudo Boolean formula decision problem. And we can feed such problems to MIP solvers, but they seem to be best for optimization problems where the objective function can really drive the search. Two interesting difference compared to the SAT and sudoboolin communities is that firstly, in the sat community it seems that essentially the best solvers are in academia, or at least even if the big corporations maybe have their own solvers, they're not fundamentally different. They're not doing something fundamentally better than what we know in academia.
00:05:40.234 - 00:06:38.004, Speaker A: However, in the mixed interlinear programming community, this is not so closed source commercial solvers such as CPleX, Groby and Express are arguably much better than the best academic solvers. Although a solver like SCiP is still it's academic. It's not fully open source, but you can at least read the source code and understand what it does, but it's not as good as the best commercial solvers. This also, of course, is challenging when you try to evaluate and run experiments because you would like to run experiments with the best solvers. But it's hard to interpret these results because you have no idea what these solvers are doing. Or maybe sometimes you have some idea, but not really enough. So this is a challenge when running experiments and evaluating MIP solvers.
00:06:38.004 - 00:07:23.234, Speaker A: Another difference, which is more about solving technology, is that at a high level, you can say that for satin and pseudo boolean solvers, the search phase where you make decisions and propagations. There's a great emphasis on speed. You try to make your decisions as quickly as possible, you unit propagate as quickly as possible. Just go fast, fast, fast, and then, until you hit a conflict, and then you back up, then you go slow. You do a sophisticated conflict analysis. You really, you're willing to invest some effort in this, because you want to learn a good constraint that will guide the search going forward. In MIP solvers, it's pretty much the other way around.
00:07:23.234 - 00:08:31.532, Speaker A: Mixed integer linear programming solvers can invest lots and lots of time and effort into decisions, how to set variables to values, or how to decide on bounds for the variables. And in comparison, the backtracking part is not so advanced. So at a high level, what a MIP solver does, it does some preprocessing of the input. It's called pre solving, and then it repeatedly solves linear programs that gives estimates of what the solution might be, and this is called branch and boundaries. Then you can also add new linear inequalities that are known as cutting planes. And they are valid, so they preserve all the solutions to your original problem. But they can be used to rule out infeasible solutions found by the linear programming solver.
00:08:31.532 - 00:09:30.390, Speaker A: And so they tighten the search space. And then there are lots of heuristics, in particular, heuristics for how to solve, because in contrast to, you can think of sat and pseudo boolean solver as going depth first, always in your search tree. But this is not so in MIP solvers. You can grow your search tree in a much more varied way, maybe sometimes go deep, and sometimes you go more doable, like breadth first search or some kind of combination. Okay, so what is a linear programming relaxation? Well, you just keep your mixed interlinear program, and the only difference is that the integral variables are relaxed to be real value. So this means that you can solve the problem fast. Now it's just linear programming.
00:09:30.390 - 00:10:18.712, Speaker A: This we can do in polynomial time. And you get a linear programming solution, which we'll denote as x star. And this clearly gives a lower bound, because you're optimizing over a bigger search space now. So you could potentially get a better minimum. And the lower bound you get is definitely a lower bound on what you could achieve over the smaller domain. It could happen, though, that this solution happens to be feasible for the original MIP, and if so, you're happy, then you have an optimal solution. As a side note, the LP solver that you're running is I said we can solve this in polynomial time, and we know that you can solve linear programs in guaranteed polynomial time using these new sophisticated ellipsoid based methods.
00:10:18.712 - 00:11:34.546, Speaker A: Typically, this is not what you'll be using. You'll use the old and trusted simplex algorithm. The reason for this is that you will have many, many LP calls on very similar problems where you just change some variable bounds and so you can reuse earlier work and do so called hot restarts to find a solution that might be very close, just a few pivots away from where you were earlier. And this is something that is supported by simplex, but not by these more advanced modern LP solving methods. So in branch and bound, you somehow choose an integer valued variable Xj in some intelligent way and some bound b some integer. And then you solve the mip with the added constraint that either XJ should be greater than or equal to b, and that gives you one subproblem and the other sub problem is that it should be smaller than or equal to b minus one. And in this way you can get a tree of sub problems, which you can then solve in the same way.
00:11:34.546 - 00:12:39.904, Speaker A: And you can prune away a sub problem or a node in your branch and bound tree when you find out that the LP is infeasible, or for instance, when you have you're keeping track of the current best solution, the incumbent solution. And if you solve an LP bound in a node and already that LP bound is larger than your incumbent, then you know you don't need to explore this node or the search tree. Any further branching decisions. As you can see above, they're made on variables in general. You could also imagine branching on general linear constraints. This is potentially much more powerful, but it's also difficult because it's difficult to figure out what kind of integer linear inequalities you might want to branch. On a side note for theoreticians, in its most general form, this branching on general linear inequalities corresponds to the so called stabbing planes proof system.
00:12:39.904 - 00:13:47.604, Speaker A: And a big question in proof complexity is to understand the power of this proof system. It's very powerful, much more powerful than the cutting planes proof system that we have been talking about before. But again, this assumes that you can branch over general linear inequalities, which in practice is not done. So the cutting planes method is something that we should talk about carefully. One problem when you talk to people in the MIP community is that the words we use on the theory side maybe, and on the sat side, and some of the words they use, for instance, when we talk about cutting planes, the cutting planes proof system, the cutting planes method, we typically mean different things, so that's important to keep in mind. So what is meant with the MIP cutting planes method is basically this. You solve an LP relaxation of your problem, and again, if your solution happens to be feasible for the MIP, then you're happy because you found the optimum.
00:13:47.604 - 00:14:52.144, Speaker A: But otherwise you add some in some way you generate a constraint sum over j b j x j less than b. That is valid for your original problem, but is violated by the solution that you found, because this allows you then to rule out the solution. And next time you call the LP solver, it will have to find some other solution. You repeat this over and over again. What could these cut rules be? Well, for those of you who have watched previous parts of this tutorial, the rules we had in suitable solving, the division rule and the saturation rule are examples of cut rules that would cut away fractional solutions. Two linear inequalities in fact, that is why it's so important during pseudo boolean conflict analysis that we use division or saturation, because that's the only way we can get truly boolean reasoning. And we'll talk in a minute about some other cut rules.
00:14:52.144 - 00:16:14.110, Speaker A: So the branch and cut method then is that you run this branch and bound that we just talked about this branch and bound method. But in each subproblem not only do you call the LP solver, but you repeatedly take the solution, add a cut, and then solve an LP. Again add a new cut so that you tighten the polytope describing your problem, and get rid of more and more non integral solutions for an interior linear program. Okay, so let's see an example of a cut, which is a knapsack cover cut. Suppose that you have a constraint sum over j in some set index set I aj xj less equal capital a. And let's assume here that all the a's are integral, they're positive integers, and that the xj or boolean variables, so binary variables. Then if we find a minimal cover of this index set, so sum set c such that if I sum up all the ajs in c, I exceed a, but any element in c that I remove and I sum over the rest and I don't exceed a, that's a minimal cover.
00:16:14.110 - 00:17:41.274, Speaker A: And then if you think about it, you realize that if the constraint here at the top of the slide is supposed to be valid, then in particular, when I sum up over the j's, I cannot pick more so this is a typo, I think so this should sum up over j in C, and I cannot sum more than at most c minus one of them or I would exceed the bound. Sorry for this typo. So, going back to cutting planes, how would you derive this? Well, you would in normalized form. This constraint is summing over j and I aj xj bar greater or equal minus a plus the sum of all coefficients. And you can weaken this to remove everything except c, and then you can divide to get a disjunctive clause that says that for one of the negative literals have to be true. So another quite interesting and intriguing and harder to understand. Cut is what is called mixed integer rounding.
00:17:41.274 - 00:18:13.184, Speaker A: So I'll go fairly fast and won't explain. So you can hit the pause button in the video and look at it. It's an ugly expression, or at least confusing and hard to understand expression. Suppose I'm stating it for a normalized pseudo boolean constraint. AI li greater equal a. So all the AI's and a are positive integers. Li are literals over zero one valued variables.
00:18:13.184 - 00:18:53.124, Speaker A: And suppose we do a mixed integer round and cut with divisor D. That produces the following constraint. Every li gets the following coefficient. So we take the minimum over AI modulo D and the degree of falsity. A modulo D plus AI divided by D rounded down times a modulo d. Summing over this this is what we do for every li greater than equal. And on the right hand side, we get a over D rounded up again times a mod d.
00:18:53.124 - 00:20:00.954, Speaker A: And somehow the magic and the mystery here is in this min term, where you can choose the minimum between AI and a mod d. So it's easier to look at a small example if we do a mere cut for the constraint, say x plus two, y plus three, z plus four, w plus five, u greater or equal five. Then the meerkat says that, well, x plus two, y plus two, z plus three, w plus four, u greater or equal four. So if you look at this, it basically says, in terms of weakening, it says that you can weaken z from three to two. You can also weaken w from four to three, you can weaken u from five to four. And if we did this with weakening, then normally the five here would drop to a two. But the meerkat says that no, as you weaken, you're losing at most one unit here.
00:20:00.954 - 00:20:49.824, Speaker A: So it's maybe instructive to compare this to standard division that we've talked about before. If we just divide by three, then the dividing by three, five divided by three, and rounding up gives it two. So let's divide by three and then multiply by two so that we get the same degree of falsity on the right hand side. And if you do the math, you see that standard division by three and multiplication by two produces the constraint two. X plus two, y plus two, z plus four w plus four, u a greater or equal four. And if you compare this with the mere cut, you see that the Mercut is stronger. Sorry, here is the mere cut.
00:20:49.824 - 00:21:48.928, Speaker A: It's stronger because we have the same degree of falsity here, but all the coefficients here are less than or equal than their counterparts here. So this is a stronger constraint than this one. And this is a general fact that anything you can do with the division rule, like we find in the theoretical computer science literature, the proof complexity literature, you can do with mere cuts. And it's actually an interesting question, and I don't know the answer to this, how the theoretical computer science the proof complexity cutting planes division rule compares to the mixed integer rounding cut rule. This would be interesting to understand better. So next, just a few words about presolving. That's a topic for a completely wholly separate talk that goes for many other topics in this whirlwind tour of MIP solving.
00:21:48.928 - 00:22:23.196, Speaker A: Um, interestingly, it is important for performance, but my impression is that it's not as crucial as for CDCL solvers, where it's like nobody in their sane mind would, would run a CDCL solver. I think without preprocessing for MIP, it's certainly important, but it's not a life or death question. As far as I've seen, some simple techniques that are used. If. If you figure out that a variable is fixed, you can just substitute the value. There's some normalization of constraint. For instance, if there's an integer constraint, you can find the greatest common divisor.
00:22:23.196 - 00:23:13.320, Speaker A: On the left hand side you can divide, and then round. On the right hand side you can probe by tentatively assigning binary variables and propagate to see what you find out. You can. By dominance, you can remove constraints that are implied by other constraints to shrink the instance. And already in part one of this tutorial, I talked about additional material on mixed integer linear programming, in particular for presolving. There is more details in the talk by Ambrose Kleitzner. And here you have the link to this talk again, mixed interlinear programming solvers also do conflict analysis, and it's analogous to what's being done in conflict driven clause learning.
00:23:13.320 - 00:25:00.574, Speaker A: Sat solvers, in fact, I would argue that it's too analogous in the sense that and in contrast to native pseudo Boolean solvers, what you do is you operate on reason clauses that you extract from the constraints. But if you remember in part two of this tutorial for pseudoboolean solvers, they actually do the conflict analysis directly on the linear constraints themselves. But MIP solvers don't do this. MIP solvers only operate on clauses that you can somehow derive from the constraints, and this leads to an exponential loss in power. One simple example of this is a bit stupid example, for reasons that I will explain presently but still instructive, is look at the pigeonhole principle, which says that xi j means that pigeon I flies into hole j and we have constraints for every pigeon I saying that it gets a hole j, but we have n plus one pigeons and we have only n holes, and for every hole we have the constraint that at most one pigeon flies into this hole. And if we write this in the natural pseudoboolean form, but do a conflict analysis where we only extract clause or reasons, then it's not so hard to see that this encoding and the clausal encoding have exactly the same propagation properties. So for an outside observer, if you're doing your conflict analysis on clausal reasons, you can't really tell whether the input is these are these linear inequalities or the CNF encoding, which in turn means that if you're running on the CNF encoding, we have an exponential lower bound in the seminal paper by hockey.
00:25:00.574 - 00:26:09.734, Speaker A: So this shows that if you're doing conflict analysis but on clauses extracted from your linear inequalities, then this can lead to an exponential loss in power. So why is this a stupid example? Well, it's a stupid example because this is also the LP relaxation of this problem is infeasible. So if you're a met person, a valid complaint is to say, well, I agree that if I were to run conflict analysis on this, this wouldn't be so great. But I will never run conflict analysis on this, because at the root node node of the MIP solver will solve the LP relaxation and will immediately determine that the problem has no solution. This is a fair point, and I can just say that there are other more interesting benchmarks where it seems that you run into similar problems where we see that pseudoboolean solvers perform really well, but MIP conflict analysis seems to suffer from that. It's too close to resolution and suffer from the same exponential lower bounds that resolution do. How do you then branch well you can look at.
00:26:09.734 - 00:27:28.630, Speaker A: So you know already that you've run your LP solver, you have a solution x star. And if you think about it, it would be nice to find a variable xj such that when you branch on xj being larger than the value in the fractional solution rounded up or smaller equal than the fractional solution rounded down if both of these subproblems provide good increases in your lower bound. So here comes a difference, as we talked about before, how you're willing to invest in a MIP context a lot of time in making decisions. So one thing that MIP solvers could potentially do, as far as I understand, even early on in the search, you consider all variables and you look at the possible branching decisions and you solve all the sub LP's and look at what the gain is, and only then you decide how to actually branch. So from a sat solving or sudo boolean solving point of view, this is absurdly expensive. This will take tons and tons of time, but you certainly do get a very good decision. Another technique is to not quite do this.
00:27:28.630 - 00:28:18.078, Speaker A: So this is let's look ahead or a strong branching. One other thing you can do is that you can compute estimates of what these gains might be, not by actually solving the LP's, but using information from the past branching history. And this is known. The technical term for this is that they're using pseudo costs and later on in the search. So maybe you do some some look ahead search in the beginning and later in the search you can use these statistics to already look back and use pseudo costs instead, which will be faster but less accurate. And there are also other statistics that MIP solvers keep about the variables to really make intelligent decisions. Another difference from SaT solvers and sudo boolean solvers.
00:28:18.078 - 00:29:12.830, Speaker A: As we said, you don't go depth first, there's a flexibility in how you're going to grow your search tree. You could definitely go depth first, and this will make your repeated simplex calls really cheap because you're just adding one bound at a time and then solving calling simplex again so you don't need many pivots. Or you can grow your search tree in a more adaptive way. You can focus on improving the lower bound, which is known as best bound search, and the lower bound. If you're a MIP person, you talk about dual bounds, or you can focus on improving your upper bound or your solution, which many people will talk about as improving the primal bound. And this is known as best estimate search. But even if you do best bound search or best estimate search, you can combine it with so called depth.
00:29:12.830 - 00:30:20.878, Speaker A: First, search plunges where you suddenly go deep for a while to exploit the gain that you get from being able to run simplex quickly with hot restarts for related problems. You have heuristics for how to find good solutions and improve them to improve the primal bound. So these are primal heuristics. Let me just give one concrete example that is known as relaxation enforced neighborhood search. Again, the assumption is that you've run your LP solver, you have a LP solution x and you look at it, and for all the integer valued variables that have integer values already, you fix those and then you look at integer valued variables with fractional solutions. You reduce the domain to either rounding this value up to the nearest integer or rounding it down. So all of these variables now have a domain size of two.
00:30:20.878 - 00:31:08.114, Speaker A: And now you solve the new subproblem where the fractional solution variables have domains of size two. And this is just one example of something that is referred to as fix and MIP local neighborhood search heuristics. Interestingly, if you look at it with Sat or pseudoboolan glasses, this is actually turning your problem into a zero one integer linear programming subproblem, which you could imagine. Then if you have a really good pseudoboolean solver lying around, then maybe you could feed this problem to a pseudo boolean solver. And there is lots and lots more. We haven't even started talking about everything that MIP solvers do for large problems. You might need to decompose them.
00:31:08.114 - 00:32:18.168, Speaker A: The buzzwords are branch and price and column generation. Another important concept is benders decomposition, where you sort of divide the problem up into the integral part and the real part, and somehow solve these parts somehow independently, as it were. So I wasn't planning to get into this, so maybe I should just try to get out of it and refer you to the literature for a better explanation. But these are important techniques, especially for large problems. Symmetry handling can be important. You can do it via graph automorphisms, somewhat similarly I think, to how it would be done, for instance in some SAT solvers or the commercial solvers would have more syntactic dedicated symmetry detection. You can rewrite your problem using extended formulations.
00:32:18.168 - 00:33:32.168, Speaker A: Introducing new variables and constraints and extended formulations is a beloved topic in the theoretical computer science community. So there are lots of interesting results there. Parallelization is important for MIP solvers to run on parallel architectures. How to use restarts, many other aspects. What about correctness? Well, so the fastest MIP solvers use floating point for efficiency reasons, but that means that you might run into rounding errors. So what can you do about this? Well, you can try to do exact MIP solving, but such solvers tend to be significantly slower and they don't incorporate the full range of state of the art approaches for solving MIP programs. So this means that actually the commercial state of the art solvers can sometimes give wrong results, maybe because of rounding errors, maybe because of bugs.
00:33:32.168 - 00:34:43.014, Speaker A: And this is just a known fact that some sometimes MIP solvers will claim that problems will return a solution as feasible that is not feasible, or sometimes it will claim optimality when in fact the solution is not optimal. And people in the know apparently know how to feed like evil problems to MIP solvers to increase the probability that they give wrong answers. When this happens. This is fairly well documented in the literature, but it seems that it somehow happens rarely enough that these MIP solvers are very useful anyway, or quite often. Maybe you don't care about exact optimal solutions, you're happy with good enough solutions, and maybe MIP solvers provide this, but it certainly raises the question of how you would get provably correct results. Like we know from the SAT community that proof logging is important to have certified results from solvers, and this is just something that MIP solvers don't provide. Although as I said, there is a need for it.
00:34:43.014 - 00:35:41.244, Speaker A: There has been some work on providing proof logging for MIP solvers. Seems that, well, it seems like it hasn't been successful enough to actually be incorporated in practice, let's put it that way. In contrast to, I think the proof logging techniques for Sat solving that are in fact used in practice, a challenge of course, compared to Sat solving. I think one reason why proof logging for SAT solving might be easier is that SAt solvers are in some sense a monoculture. You have the conflict driven learning paradigm that dominates everything else in MIP solving. You have a much wider diversity of techniques. How can you capture all of them? Is it even clear what a proof should look like? What's a good format? And the more advanced the reasoning gets, the harder it gets also to generate the proofs on the fly without incurring too much overhead.
00:35:41.244 - 00:36:48.098, Speaker A: So this is another like definitely a good open research problem for MIP solving, and also for sudo Boolean optimization and for Max sat solving to be sure. How to provide proof logging other interesting questions that I think are worth thinking about that are related to MIP is how to find out how to do better. Branching on general integer linear constraints. Again, as I said, this is related to the stepping pain plains proof systems. I complain that MIP conflict analysis is not very smart. An obvious question, at least for zero one integer linear programs, which is a big and important subclass of MIP problems. Whether we could borrow ideas from native pseudoboolean solvers to improve on the conflict analysis, there's lots of room for improvement for providing a rigorous understanding of MIP solver performance.
00:36:48.098 - 00:38:20.804, Speaker A: I think it's instructive here to compare with the state of the art in SAS solving. Somehow, maybe accidentally, we have had these parallel developments in SAT solving on the one hand, and in computational complexity, in proof complexity in particular, on the other hand, where it turns out that proof complexity almost unintentionally provides explanations for what Sat solvers can do and cannot do, and provides, for instance, lower bounds on, say, the resolution proof system or the cutting planes proof system tells us what we can expect from, say, CDCL solvers or cutting planes based pseudo boolean solvers. And based on such results, we can also develop interesting families of theoretical benchmark formulas, sometimes referred to as crafted benchmarks, and computational complexity results for them, where we know that these benchmarks might be hard or easy for solvers using such and such techniques. So it would be interesting to have something like that for mixed interlinear programming. My understanding that it is basically just missing. And then perhaps the main reason why I'm standing here and recording this video is, since MIP solvers in fact seem to be so good, could we maybe steal the best ideas from them to make our sudo boolean solvers better? So this is what I want to talk about next. And this is also the final topic or final subtopic in this prerecorded tutorial on sudo Boolean solving and optimization.
00:38:20.804 - 00:39:32.752, Speaker A: Okay, so combining sudo Boolean solving and mixed integer programming, what are the pros and cons of the different paradigms? Well, one thing that I'm trying to pitch that you've heard several times now if you're watching these videos, is that pseudo Boolean solders really have a sophisticated conflict analysis using the cutting planes method here. I don't mean the cutting planes method as described in branch and cut earlier in this part four of this tutorial, I mean the cutting planes method as in the cutting planes proof system. In the proof complexity literature, which is something related but slightly different. Anyway, based on the cutting planes proof system, you can get very sophisticated conflict analysis that is exponentially stronger than what CDCL sat solvers provide. But we know from experiments. It published in the literature that sometimes pseudoboolean solvers just perform abysmally bad. Well, we know that they're bad on CNF formulas, but more interestingly in this context, because MIP solvers also tend to be bad on CNF formulas.
00:39:32.752 - 00:40:31.210, Speaker A: But we know that MIP solvers are fantastic when the LP relaxation is infeasible. But we have examples of pseudoboolean problems where the LP relaxation is infeasible, but the pseudo boolean solver seems to run forever, not pigeonhole principle formulas. They're typically solved very fast. But there are other instances, other benchmark families which seem to be really, really hard. So mixed integer linear programming solvers, as we've seen now, they have very powerful search do a lot of smart thinking before branching exploiting information from LP relaxations, which is something that sudo Bulano solvers just don't know about. They have a rich variety of cut rules, not just division or saturation, but mere cuts and other cuts. But on the other hand, the conflict analysis is not so great.
00:40:31.210 - 00:41:38.594, Speaker A: It's basically just SAT based conflict driven clause learning. And it's not clear since they're so good at other things like why would the conflict analysis, comparatively speaking, be so weak? So the proposal then presents itself to merge pseudoboolean solving and mixed interlinear programming to get the best of both worlds, the SAT style conflict driven search and conflict analysis and MIP style branch and cut and lp relaxations, and design a new solver to rule them all. So how would you do this? Let me tell you one way of doing this, and try to explain why this might not be so easy. So one idea would be to I'll think, since I'm coming from the SAT side, I'll think of the pseudo boolean solver as being in charge. It's another interesting perspective. You can think of the MIP solver as being in charge and then using a pseudo boolean solver as a subroutine. That's a very interesting perspective.
00:41:38.594 - 00:42:34.584, Speaker A: We won't talk about this in this talk, so this will be a separate talk. So let's think of the pseudo boolean solver being in charge, but doing just as a MIP solver would do. Call the LP solver to solve LP relaxations. Okay, for instance, you could use it as a preprocessor, but using it only as a preprocessor is clearly not sufficient, because it could be that the LP relaxation is feasible, but we know of formulas where it's feasible. But there's sort of backdoors to LP infeasibility. That is, you just have a small, if you just set a small number of variables, then the residual pseudoboolean formula turns infeasible even as an LP. Still, such benchmarks seem to be exponentially hard for pseudo boolean solvers.
00:42:34.584 - 00:43:32.892, Speaker A: So you somehow need to really integrate the LP solving inside the pseudo boolean search. So the other extreme would be, well, why don't you consult the LP solver before making a decision, just as a MiP solver would do? And the answer is, well, you can't because you don't have the time. So pseudoboolean solvers, just the SAT solvers, are based on this very very quick alternation of decisions and propagations. And solving an LP relaxation is just a completely different time scale. So if you solve an LP relaxation at every, every time, it's time to make a pseudo Boolean solver decision, then the pseudo Boolean solver will just grind to a halt and all the time will be hogged by the LP solver. So you don't want to do this. So somehow you need to balance the time you set aside for pseudo Boolean solving and LP solving.
00:43:32.892 - 00:44:16.954, Speaker A: That's one challenge. Here's another challenge. Suppose that you run the LP solver and a dream scenario would be you're somewhere in the middle of your search. The pseudo Boolean solver is happy, thinks it's doing good, but then you call the LP solver and the LP solver says, well, given these decisions that you've already made, dear pseudo Boolean solver, the residual LP is in fact already infeasible, so you can safely backtrack. What should we do if the LP solver says this? Well, we should clearly backtrack. I mean, if there is no real valued solution in this subpart of the space, then clearly there's no Boolean solution. But a pseudo Boolean solver can't just backtrack.
00:44:16.954 - 00:45:35.534, Speaker A: What can it do? Well, it only knows how to do conflict analysis on a violated pseudo Boolean constraint. And the problem is of course there, there is no violated pseudo Boolean constraint because if so, the pseudo Boolean solver would have initiated conflict analysis. So the LP solver sees a conflict that the pseudo Boolean solver doesn't see, and so it can't do conflict analysis. More subtly, this efficient LP solver is using floating point. So when it says that there is a conflict, the PB solver can't just backtrack because maybe this is a rounding error and the pseudo boolean solver must maintain sound reasoning. So how can you combine this with using an LP? That might give floating point buggy results? A third interesting question is okay, suppose that we run the LP solver and we do some kind of branch and cut, so that when the LP relaxation is feasible and the MIP solver or the LP solver finds a solution, then what a MIP solver would do, it would generate cut constraints. So we can do this.
00:45:35.534 - 00:46:57.904, Speaker A: Should we tell the PB solver about this? Should the pseudo Boolean solver get access to these constraints? And in the other direction, the suitable solver is learning new constraints all the time from conflict analysis. And if you look at these constraints with the right classes, these are in fact also kind of cut constraints, a particular kind of cut constraints. This can be made somewhat formal and the question is well, should we pass on these cuts from the pseudo boolean solver to the LP solver to tighten the LP polytope? And how so? We have a recent paper joined with Ambrose Gleichner, who's heading the SCIP team in Berlin. Scip is arguably the best academic MIp solver. So we have a paper where we tried to integrate a pseudo boolean solver with an LP solver and cut generation and everything. So I'll tell you some of our ideas, how we did this and what we see. So we interleave LP solving within the conflict driven pseudo Boolean search.
00:46:57.904 - 00:48:05.242, Speaker A: But importantly, we keep track of the total number of pivots done by the simplex algorithm, and we enforce that this should never exceed the number of pseudo boolean conflicts. Or rather that when we're thinking about whether to call the solver, we will never issue an LP call if the number of pivots exceeds the total number of conflict scenes. So this is like a proxy for balancing LP time and PB time. Once we call the LP solver, we also have a limit for how many pivots we allow in a single LP call. Because it turns out that if you run the LP solver until completion, then some LP's can in fact be pretty hard and the LP solver is operating on a different timescale. So maybe the instance will time out without the LP solver ever returning to the PB solver, and we don't want that. So then we just, we have a hard limit.
00:48:05.242 - 00:49:30.054, Speaker A: So we abort after, say P pivots in a single call. However, this call is like wasted because the clearly the LP solver did something but didn't reach any results and we don't want to waste resources. So whenever this happens, we double the limit. P what? This means that in the future we'll get fewer LP calls, but we have a greater probability of the calls we actually make running until completion. And it seems that this is a novel contribution to how you would balance LP time with the other techniques used in the solver. Because the idea so I didn't say this, but I should say the idea of somehow incorporating LP relaxations in, say, a Sat solver or a sudo solver, or even a constraint programming solver is certainly not a new idea. People have tried this, but a challenge that has been hard to deal with is apparently the fact that how do you avoid the LP solver to starve the other solver? And this is how we make sure that the LP solver can never starve the pseudo boolean solver.
00:49:30.054 - 00:50:27.546, Speaker A: So what should we do when the LP solver reports that the LP relaxation is infeasible? Well, it turns out that this problem has in fact been solved. So you can use Farca's lemma, which says that when this happens you can there exists, and not only exists, but you can find explicitly, efficiently, a linear combination of constraints that is violated by the assignment that the solver currently has. And since this is a linear combination of existing constraints, a positive linear combination of existing constraints, it's valid to derive it. So we can derive this constraint and add it to the database of constraints for the solver. And then we say, ha, we have a conflict. Look, pbsolver, we have a conflict here. And then, okay, then the PBS solver knows what to do.
00:50:27.546 - 00:51:19.374, Speaker A: It can trigger conflict analysis and backtrack. Importantly, when we compute this linear combination, we do so with exact arithmetic so we don't have any rounding errors. On the other hand, if there were rounding errors, one thing that can happen is that this linear combination that should be violated by the solver assignment turns out not to be violated after all. We would not expect this to happen very often, but it could happen. Well, if so, we just say we determine that this was a false alarm and we ignore it, and we continue the Sudo Boolean search. No harm done. Maybe we missed out an opportunity on an opportunity to backtrack, but if so, what? We try to compute a constraint justifying this backtracking and we failed.
00:51:19.374 - 00:52:21.760, Speaker A: So we have to continue the pseudo Boolean search. When the LP solver does not detect infeasibility, but instead finds a solution to the LP relaxation, we explore adding cuts as a MIP solver would do, and we share also these constraints with the Sudo Boolean solver to hopefully improve the Sudo Boolean search. We've also explored to some extent to use this LP solution to guide the PBSearch like you can use it to maybe decide which variables to decide on or how to set the face. And we've also in the other direction. As mentioned, you could imagine having the pseudo Boolean solver passing on additional learned constraints to the LP solver. We've done some work on this as well. So since I mentioned Farkas lemma, let me just be clear on what we need.
00:52:21.760 - 00:53:30.564, Speaker A: So let me state like a very special purpose pseudo Boolean Farca's lemma, which is basically just Farca's lemma for zero one integer linear inequalities phrased in pseudo Boolean language. That's all that it is, nothing fancy. So what it says is that if we have a pseudo Boolean formula consisting of constraints c one up to cm and we have a partial assignment rho such that when we apply this partial assignment rho to the formula f, and this is this notation, this is the residual formula f. After we've applied rho and simplified, suppose that you take this formula, you take the LP relaxation of it. Suppose that it's infeasible. Well then there are positive or non negative integer coefficients so that you can find a linear combination of these constraints such that this linear combination is in fact violated by rho. Or in pseudo Boolean language, if you take this linear combination, this constraint, and you look at the slack under Rho, then it's negative.
00:53:30.564 - 00:54:09.844, Speaker A: And this is nothing new. It has been observed before that you can do this and that the Farca's constraint that you get in this way is a valid starting point for Sudo Boolean conflict analysis, as we discussed before. So the point is to somehow to do conflict analysis combined with LP relaxations. And if you're a MIP person, you can say, well, we already have this. MIP solvers already combine LP relaxations with conflict analysis. SCIP does it. Some closed source commercial solvers do it.
00:54:09.844 - 00:55:11.894, Speaker A: So what's the point? Why should we reinvent the wheel? So it's important here to understand the similarities and differences. I talked a bit about this before, but let me return to this again and to describe in MIP language, as it were, how Sudo Boolean search and conflict analysis works. So what's Sudo Boolean search? We make a decision for some free variable to assign it to either zero or one. Once we've done that, we propagate all assignments that are immediately implied by some linear constraints. And if we get such a propagation, then we check if there are any other constraints that are now implying other propagations. And in order to we describe this in part two of the tutorial, we use this notion of slack to decide propagation. But this is just slack is just a formal definition for saying, well, a linear constraint propagates an assignment if setting the variable to the other way would provably violate the constraint.
00:55:11.894 - 00:55:36.258, Speaker A: So you do this until saturation. If there is no contradiction, you go back to step one, make a new decision, and then propagate. So Sudo Boolean searches this cycle of decision and propagations until you get a contradiction. You get a constraint that is violated. Then we do conflict analysis. Now the fun thing starts. So what do we do? This is a somewhat simplified description, but it's close enough.
00:55:36.258 - 00:56:15.440, Speaker A: So we, so we have a conflict. So we look at the conflict in constraint c chronologically. We look at the last variable in c that was assigned to the wrong value, leading to c being violated. That was because of a propagation. So we find the reason constraint responsible for this. Let's call it r. Then we apply a cut rule to r to generate a constraint that propagates x to a Boolean value.
00:56:15.440 - 00:57:06.572, Speaker A: Not only so, the reason that r propagated was that the pseudo Boolean solver knew that x had to be zero or one. But we change in the or cut constraint. We changed it so that even if the solver would have been a real valued solver, it would still have propagated all the way to zero or one. And there's a systematic way of making sure that this is so. And this is covered in part two of the tutorial. Good. So now once we have this, we can, since Rcut is still propagating x to one value and c would like x to have the other value, it's not hard to see that x actually appears with opposite signs in Rcut and c.
00:57:06.572 - 00:58:13.652, Speaker A: So this means that we can take a linear combination with positive coefficients of r, cut and c so that x vanishes. So let d be the smallest such integer linear combination. Now you can prove, if you think about it for a little bit offline, that if we take the current truth value assignment, the current trail of the pseudo Boolean solver, and we remove x from it, then this new constraint d is actually still violated. So this is this invariant that we have in conflict analysis, that the constraint that we have derived is violated by the decisions made before this point. So we look at d, and if we like it, if it satisfies our termination criteria, and if it is assertive, then we say oh, let's learn this constraint. Otherwise we say that d is our new constraint on the conflict side. So we set c to d and go back to one again and find the next variable in reverse chronological order that was propagated to the wrong value.
00:58:13.652 - 00:59:30.628, Speaker A: We find the reason constraint for that and generate a new rcut and take a linear combination of r and c, which is now d, and keep going until we get a d that we like. When we get a d that we like, we learn it, meaning we add it to the database of constraints that the solver maintains, and then we back jump. We undo further assignments that the solver had made up until the first point where d is no longer violated, and then we switch back to search. So this is how Sudo Boolean conflict analysis works, if you want to describe it in MIP language. How does this compare to what SCiP and the other MIP solvers would do? So SCIP has the same propagation as pseudo boolean solvers, but in addition it has the powerful but slower propagation coming from solving LP relaxations and figuring out things that follow from such LP relaxations. But the conflict analysis is weaker in the sense that we don't perform the conflict analysis on the reason constraints. Instead, we extract disjunctive clauses from the reason constraints and perform the conflict analysis on those.
00:59:30.628 - 01:00:44.514, Speaker A: And now if you pull together your standard papers on cutting planes on resolution and the paper making the connection between resolution and conflict driven clause learning, proving that conflict driven clause learning conflict analysis is in fact just deriving making resolution derivations, then it follows from this that conflict analysis on extracted disjunctive clauses is exponentially weaker than conflict analysis on the linear constraints themselves. So that is why explains what are we doing differently now. So we're having more stupid propagation potentially, except that we're also using a little bit an LP solver now. So we're improving. We're going in the MIP direction when we get information from the LP solver and the conflict analysis is better than what a MIP solver would provide for the corresponding zero one integer linear program. Another point to make is that again, SCIP uses floating point, whereas all the reasoning step in a pseudo boolean solver would be exactly. So there's no rounding problems with rounding.
01:00:44.514 - 01:02:10.638, Speaker A: So this thing that I described, now, we implemented it. We took the roundingsat sudo Boolean solver and combined it with the LP solver supplix from ScIP and then we can just run we can just run the LP solver with inside the sudobool solver without anything else. Or when it finds feasible solutions, we can generate Gomorrah cuts. We can also share these Gomorrah cuts with the sudo boolean solver. We can also in the other direction, share learned pseudoboolean solver constraints with the LP. Here are some plots on what happens so if we take the collection of hard knapsack benchmarks collected by Pissinger and we run the state of the art pseudo boolean solvers naps and sat for j, then we run roundingsat and we run roundingsat with LP integration plus Gomorrah cuts plus also the PB solver. Sharing learned constraints with the LP solver and see what happens.
01:02:10.638 - 01:03:09.674, Speaker A: We see that although these knapsack problems are sudo boolean optimization problems set for J and naps struggle, rounding set is better when you throw in the LP solver. It gets even better when you take the gomory cuts. Yet more amazing stuff happened. And even sharing learn constraints from the PB solver with the LP solver also helps. You don't get all the way up to skip, but you close a large part of the gap. So on these particular benchmarks, it definitely seems that combining pseudo boolean solving with linear programming is a no brainer. Then we also tried on running on some sudo boolean competition benchmarks, some zero one introduction programming instances from Miplip.
01:03:09.674 - 01:04:43.316, Speaker A: Again just running sat throwing in the LP Solver plus Komorei cuts plus sharing learn cuts from the sudo Boolean solver side with the LP solver and we run on decision problems and optimization problems from the latest sudo boolean competition in 2016, and from some zero one integer linear programs, decision versions and optimization versions that we have adapted from Miplib translated to the sudo boolean format. So these are not quite the original Miplib instances. You have to see the paper for more details. But anyway, so what can we see? Well, for all kinds of problems, we see that either the original MIP solver or the original pseudo boolean solver is best. However, when for suitable indecision problems, we see that SCIP is actually pretty terrible and rounding sat is pretty terrible for optimization problems and for MIP problems in general. And here you see that when you combine rounding sat with an LP solver, you interpolate much more nicely. You see that in fact, on decision problems you're very close to beating the MIP solver.
01:04:43.316 - 01:05:45.114, Speaker A: Here on suitable decision problems, the MIP solver gets beat decisively, and it's very close for the MipliB decision problems. For optimization problems, even though rounding set cannot beat skip, there's a clear improvement. And then you can see, interestingly, you see that on these types of problems, you don't gain too much from these Gomorrah cuts and learn cuts. But it's clear that in general we're much better than. I mean, these two columns represent the pseudo Boolean state of the art, and we're clearly improving fairly significantly on that. So what are the conclusions? I said that we wanted to reach the best of both worlds, did we not? So clear in the sense that here are still the best. So we're not the best of both worlds.
01:05:45.114 - 01:06:22.814, Speaker A: Should have been in this column or this column, maybe, but it isn't. But arguably, as I said, it's like almost it's the maybe second best of both worlds. It's a well rounded performance. We're never much worse than the best solver, and that is nice. So it's like a well rounded, good performance overall, always competitive with the best solver. And definitely, I would argue, fairly dramatic improvements for optimization problems compared to previous sudo Boolean solving state of the art. But again, SCip is a hard to beat mip solver, and the commercial solvers are even harder.
01:06:22.814 - 01:07:12.020, Speaker A: Interestingly, we can see that adding LP solving courses causes a performance loss on PB decision instances. This is here go down from 1472 instances solved within the timeout limit to 1453. So what happens? It turns out, in particular, the satisfiable instances, the results get worse. Why is this? Well, it turns out that if you measure the search quality in terms of the number of conflicts until you reach your satisfying assignment, the conflict count actually went down. So in that sense, somehow it seems that the search is better. But the search is also slower, so it doesn't pay off in terms of running time. The solver gets more intelligent, but it's also slower.
01:07:12.020 - 01:08:40.744, Speaker A: And this gets back to this theme that we talked about at the outset of this tutorial, the trade off between inference power and inference speed, here apparently being a little bit dumber, but faster is better, as we saw, sharing these Gomorrah cuts and learn cuts in general seems not to be so helpful, except for the knapsack benchmarks, where it's like absolutely crucial. Also, I think that, honestly, this is like, I don't know of any pseudo booleans over doing this before. I'm not so sure that we did this in the best way possible. In fact, I'm pretty convinced that there are lots of smart things that could be done. So maybe the first thing to do would be to look at this more closely and optimize how the sharing is done, and maybe we would see better performance. One thing we did look at is to try to estimate how useful different types of constraints are. Like how useful are the regular constraints learned from sudo Boolean conflict analysis? How good are the Farca's constraints? Can we see any difference between regular constraints learned from regular conflicts compared to constraints that we learned from these LP initiated conflicts where the LP solver saw in advance that we're heading towards a conflict and gives us a Farca's constraint that allows us to trigger conflict analysis.
01:08:40.744 - 01:09:37.214, Speaker A: How would we measure usefulness? Well, we don't know. What we can measure is how often are these constraints used in conflict analysis. So this is not necessarily usefulness, but it's at least usage, and hopefully tells us something interesting. So at least I find it surprising that, or maybe not so surprising, I don't know. But the Farca's constraints turn out to be really useful. Like the Farca's constraint tends to be much more used in conflict analysis in the future than a regular learned constraint would be for optimization problems, not necessarily for decision problems, but so these are the Farca's constraints that are returned by the LP solver. But then remember that we trigger conflict analysis from these constraints to learn a new constraint, just as a pseudo boolean solver would do for any conflict.
01:09:37.214 - 01:10:54.348, Speaker A: And these constraints seem to be much worse than regular learned constraints, and I don't know why. However, there's also a big spread in the usage measurements, but I think it would be interesting to study this and understand what's actually going on. So future research directions well, fine tune the heuristics, think about how to improve the cut generation based on our particular setting with pseudo Boolean solving to get more intelligent in how the sharing of constraints between from the pseudo boolean side to the LP side should be done. Also, one important thing is that we have a very, in some sense a fairly static allocation of time by counting number of pivots that the LP solver gets compared to the number of conflicts that the pseudo boolean solver gets. Maybe if the LP solver is producing amazing results and deciding infeasibility over and over again, maybe we should allocate some more time to it. Or on the contrary, if it never does anything useful, then maybe we should give it less time. We haven't explored this but it's a very natural thing to try.
01:10:54.348 - 01:12:01.596, Speaker A: I think we talked about which constraints are useful, Farca's constraints, or constraints learned from Farca's conflicts compared to regular constraints. I think there's more room for understanding what's going on here. Also, to what extent like gomory cut constraints are useful for the pseudo boolean solver? The MIP solver uses lots and lots of information from the solutions to the LP relaxations. Could we use this information more intelligently also in a pseudo boolean solver? Since we're now solving linear programming relaxations? Presolving is something preprocessing is not done in pseudoboolean solvers. Why don't we do that? Maybe we could use MIP presolving. Maybe we could use mixed integer rounding cuts instead of the division we're using in pseudo boolean conflict analysis to improve it. I think this would definitely be worth looking at.
01:12:01.596 - 01:12:35.738, Speaker A: To the best of my knowledge, this has not been done. And as I mentioned before, in some theoretical sense we know that mere cuts are never worse than division cuts. They could be better, but we don't know how much better. This would be interesting to understand. So now we have combined an LP solver with a basic sudo boolean solver. What about we talked about in part three of this tutorial, we talked about core guided search. We talked about implicit hitting set algorithm.
01:12:35.738 - 01:14:17.562, Speaker A: Maybe those could be combined with an LP solver in the same way? A comment on the pseudo boolean solver so it seems, based on these results that I showed here, that rounding sat with linear programming integration, or also, as we discussed in part three of the tutorial with core guided search seems to provide state of the art performance, improving significantly on what has been available previously for pseudo boolean solving. Although again pure MIP solvers are also good and often much better. But roundingsat tends to be much better on unsatisfiable instances or proving optimality than on actually finding solutions. So running on satisfiable instances. So it would be very interesting and I think important to improve the search in sudo boolean solvers. And I believe that the sophisticated cutting planes based conflict analysis and pseudo boolean solvers should be exported to MIP or at least to zero one interlinear programming solvers. And maybe we could go beyond zero one integer linear programming solvers and even solve zero one mips in such a way that the PB solver would be responsible for the integral part, and then whenever it needs to deal with the real valued variables, it would call an LP solver that takes care of that.
01:14:17.562 - 01:15:05.564, Speaker A: It would be interesting to see if something like that could be made to fly. So with this, we have actually reached the end of the whole prerecorded tutorial, all four parts. So trying to sum up in one slide sudo Boolean optimization is a powerful and expressive framework, although we limited it to linear sudo boolean optimization of style. Still, you can capture lots and lots of problems with this. And then once you have these problems formulated, you can attack them with sat solving or max sat solving. Or you can use what I've referred to as native pseudoboolean solving, that is, using pseudo boolean reasoning based on the cutting planes proof system. Or as we talked about in this part of the tutorial, you can use mixed integer linear programming.
01:15:05.564 - 01:16:34.644, Speaker A: I think it's fair to say that these approaches have complementary strengths. So I think it's an interesting research direction to explore how to combine them to exploit synergies and how to come up also with new solving paradigms. I think there are some highly non trivial challenges in how to come up with and design algorithms, and even more so, like coming up with a sophisticated algorithmic idea is maybe not so hard, but then implementing it to run blisteringly fast in practice is an important and tough challenge on the theoretical side, as I've been trying to say today, especially for mixed interlinear programming solving, but also earlier for these other paradigms, Max sat solving, for instance, there are lots of theoretical questions where we don't really understand what's going on. There's lots of room for computational complexity theory to contribute here, I think. But in this intersection in between different areas, I believe there might also be quite a bit of low hanging fruit if we come together. If people come together with different perspectives and look at these problems, then maybe it's possible to make significant progress just by pooling ideas from these different research areas and different communities. And personally, regardless of anything else, I just find that these problems are really, really fun to work on.
01:16:34.644 - 01:17:51.004, Speaker A: They're an excellent source of fun problems, both applied fun problems and challenging theoretical problems. So I invite you to work on these problems. I'd be happy to talk to you if you're interested in this and have listened on the tutorial this far. And with this, I thank you for watching. And this is the end of the tutorial. So what comes now is just as promised, several times in these different prerecorded tutorials, I'll just flash through the references, but you should also be able to find the references in the PDF file with these slides that you find on the Simons Institute Institute webpage. As should be clear from here, there are quite a few references for further reading, with a focus on applied papers, and for some of this material, especially for the pseudo boolean solving part.
01:17:51.004 - 01:19:57.994, Speaker A: You could also look at the SAT handbook, the pseudo Boolean and Cardinality constraints chapter, and also the proof complexity chapter, which has some of the latest news regarding how to design conflict analysis algorithms based on cutting planes. So I'll keep flipping through these references. I guess we're roughly halfway, and as you can see in particular there are a lot of max Sat references, so I think there's a lot of material in there Max sat solving where you can ask how these techniques can be lifted from conjunctive normal form to the pseudo boolean format and maybe be made stronger thanks to this. There are also a fair number of classic SAT papers like the Cerens and beer paper on minimizing learned clauses. Doing something like this for pseudo boolean solvers would be really nice. And this is the final slide with references. So thank you again for watching this far, and if you have any questions after having watched these videos, please do feel free to contact me.
01:19:57.994 - 01:20:01.474, Speaker A: Shoot me an email, I'll be happy to talk. Thank you.
