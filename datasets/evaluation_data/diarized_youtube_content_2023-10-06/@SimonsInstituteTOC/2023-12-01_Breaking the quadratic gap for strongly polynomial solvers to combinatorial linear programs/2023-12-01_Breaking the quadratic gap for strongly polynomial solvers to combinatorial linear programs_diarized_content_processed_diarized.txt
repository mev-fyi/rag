00:00:12.360 - 00:00:45.092, Speaker A: All right, I think we can get started even if we maybe have some strike words. So let's. Welcome to Penta. Thank you, Rasmus. I'm going to speak about strongly polynomial algorithms for subclasses of LP. And let me start with a slightly provocative chart. And that is, how hard is it actually to solve fractional multi commodity flow problems? So, there are sort of three different regimes.
00:00:45.092 - 00:01:23.578, Speaker A: Low accuracy, high accuracy, and solving it. Exactly. And it seems to get harder the more accurate you become. But then once you become exact, suddenly become simpler, which is maybe a bit confusing. But what I'm plotting here on the y axis is the relative hardness, and this relates to solving general LP in the same accuracy regime. So, what I mean by that is, if you want to solve multi commodity flow to low accuracy, then this is relatively easy with first order methods. There are many results on that.
00:01:23.578 - 00:02:29.486, Speaker A: If you want to go to the high accuracy regime, that would mean your running time should depend on your error logarithmically. Then suddenly you can show that general LP can, in weekly polynomial time, be reduced to these multi commodity flow problems. This was shown by ItA in the seventies, and just last year by Ding, King and Zhang. It was shown that you can do that in essentially linear time, and the size doesn't blow up. There's a slight caveat that was sort of exploited by Jan van Denbrand and Zhang in the same year, where they showed that you can do a little bit better if your graph is dense. But once you go to the exact regime, then it suddenly becomes easy relative to general LP, because you can solve multi commodity flow problems and strongly polynomial time. And this is due to them being so called combinatorial LP, where your constraint matrix is integral.
00:02:29.486 - 00:03:34.480, Speaker A: And it's a still big open question whether you can solve general LP and strongly polynomial time. So, essentially, the gap that you see here in running time is exponential relative to General LP. Okay? And my talk is roughly here, I care about solving multicommodative law problems or my general combinatorial LP in strongly polynomial time, and I want to do it as fast as possible relative to the high accuracy solvers that are very hard up here. Okay? Now, I haven't introduced the problem itself, but in this audience, I can quickly go over the definition of mincast flow problems and multicommodity flow problems. As I said, I will speak about more general combinatorial linear programs, but they will be quite useful as running examples throughout my talk. Okay, so for the minimum cost flow problems, what do we have? We have a directed graph on this. Directed graphs, the edges have capacities and cost, and the vertices have certain demands.
00:03:34.480 - 00:04:31.184, Speaker A: And our task is to route exactly the demands throughout this network, subject to obeying the capacity constraints and minimizing the overall cost, the minimum. The multi commodity flow problem is essentially the same, except that you now route multiple flows for multiple demand vectors b. So in our case, I have a second demand vector b bar that you have to route through the same network. These instances would be essentially separable if I wouldn't link the two flows by the edges that they share. So they share, yeah, so they have to together obey the capacity constraints. And it turns out that this problem is as hard as general LP for weekly polynomial running time. Okay, so these are the two running examples that I will use in the talk.
00:04:31.184 - 00:05:08.030, Speaker A: So what happens for these different accuracy regimes that we saw in the, begin building the graph that I showed in the beginning. So if you look at an error epsilon, so I want to find a solution to an LP that is almost optimal. It is almost feasible. Then the low accuracy regime would mean I have an algorithm that does so in running time that depends polynomially on one over epsilon. Classical methods would be these here that achieve that. Then since the seventies and early eighties, we have high accuracy methods. These are methods that only depend polynomially on the logarithm of my error.
00:05:08.030 - 00:05:53.630, Speaker A: These are ellipset methods and point methods. And if you want to solve an LP, exactly, so you don't allow any error. Then usually, unless you are in the bitcomplexity model, you would go to the simplex method, which doesn't have any running time guarantee. And then there are specialized interior point methods and proximity based solvers. And my talk will fall into the category of specialized interior point methods today. Okay, let's speak a little bit about computational models to make clear what I'm working with, but many people work in is the standard bit complexity model, which would be weekly polynomial time algorithms, where you have a running time that is polynomial in the number of integers. So the size, the dimension of your problem, and the bit encoding l of your problem.
00:05:53.630 - 00:06:34.786, Speaker A: If you want to drop the bit complexity dependency l, then you end up with strongly polynomial time algorithms. So you have only poly n elementary operations comparisons and basic addition multiplication and so on. And there's a, yeah, okay. There's the additional requirement that you want the space of the algorithm to be bounded. This can actually be a problem. For example, if you want to solve repeatedly linear systems, essentially you can only do that to poly lock def naively without your bitcomplexity blowing up. And then there's another model where you essentially completely get rid of the bit complexity dependency, and these would be real rum or BSS models.
00:06:34.786 - 00:07:23.826, Speaker A: So there we operate on real numbers. We're given an oracle that can add, subtract, and so on these numbers. And in this regime, we want to have only polynomially, many in dimension, many iterations to solve NLP. My result will fall into this category of the real RAM model is polynomially there. Let me mention that usually it is possible, if you have such an algorithm, at least for LP, to make it strongly polynomial, with the caveat of losing a few polynomial factors in running time. So there's the thing that for weekly polynomial algorithms, you could somewhat always round the numbers that you have to keep the bitcoin vaccine under control. You can't really do that in the realm model, because your model would become too powerful.
00:07:23.826 - 00:08:28.080, Speaker A: But instead of rounding, you can do something for LP, that is, express your current solution as a convex combination of vertices, which is doable. But again, this will cost you some polynomial factors and running time. Okay, now, what is the state of the art for linear programs for weekly polysolvers versus strongly polynomial solvers? So we can solve LP very, very efficiently in the weak polynomial regime. So since the seventies and eighties, we have these high accuracy solvers, which can be turned into exact solvers if you allow running time that depends polynomially on the bit complexity. And in fact, we can do it now from Kohen Lee Song, Jan Brand made it deterministic, and then Weinstein and Jiang showed that depending on what the true value of omega is, they can be potentially faster. We can solve it now in M to the omega Times L. However, if you look for strongly polynomial time algorithms, it is still a big open question whether there exists.
00:08:28.080 - 00:09:07.840, Speaker A: But general LP is strongly polynomial time algorithm. And as yin you mentioned yesterday, even for the special class of mark of decision processes, we do not know yet. Okay, what we do now is for combinatorial LP, meaning my matrix is integral and the entries are not too large. We can then solve in strong polynomial time. This was due to taros in 1986. And crucially here, the dependency is only on the entries in the constraint matrix. It doesn't matter what b and C are, they can be essentially irrational numbers.
00:09:07.840 - 00:09:51.136, Speaker A: If my model allows me to add, subtract real numbers, any real numbers, then you can do it in strongly polynomial time. And another big class where you can do it in strongly polynomial time, is if your constraint matrix a has at most two nonzero entries in every column. So this was a result by Daniel, Cedric, Neil, myself, and lazy from this year. But yeah, this is not what I'm going to talk about today. And this sort of unifies many special subclasses. I mean, it was known for mincus flow, for example, that you would of course have one and a minus one, but you could have arbitrary real numbers in these columns. And then feasibility, for example, was known, but not optimization.
00:09:51.136 - 00:10:32.366, Speaker A: And dual feasibility was also known, but not optimization. So yeah, this is sort of a unifying result there. So what my talk is going to fall into is a faster, strongly polynomial term algorithm in this regime. So you have a constraint matrix that is integral and the integers are not too large. Okay. For that, I go a little bit into condition numbers. Um, so one way of defining sort of complexity of an LP is defining this condition number kappa a.
00:10:32.366 - 00:11:11.458, Speaker A: And that is the subnorm, or the maximum subnorm over all bases of this matrix here. Okay, so essentially you, you bring a matrix in basis form. You look at the sub norm, and based on that condition number, it is possible to solve LP. Yeah, independency logarithmic on that condition number. And for example, if you look for a flow matrix, then the bases correspond to trees. So your matrix a b to the minus one a has this identity block corresponding to the tree edges, and then the non tree edges. The columns correspond to the fundamental cycle.
00:11:11.458 - 00:12:03.408, Speaker A: So every edge here corresponds to an edge here. And there exists a unique cycle with your tree. And what these columns here sort of indicate is whether the tree edges do appear in that cycle. And if they do, then in positive or negative direction. Okay, and sort of this concept generalizes to general Lp that intuitively this condition number can be described as follows. Say you want to augment flow or you want to change the value of a variable by one, and you want to do that with a vector f that is supported on a subset big f on the variables. And then the condition number is sort of the smallest number, such that you can always do that with some flow f bar that is also supported on the same set routes.
00:12:03.408 - 00:12:38.848, Speaker A: Exactly one unit along that edge and the supremum norm of that flow is at most kappa. Okay, so this example here essentially shows that for flows, this kappa has to be one. This is sort of equivalent to saying that flow matrices are totally unimodular. But it turns out why multi commodity flow is so hard is that kappa is exponential. And that can be seen by a very simple example. And that is, let's say we have the first commodity here, and I want to route one unit of flow from here to here. And I can do that, say, around the cycle here.
00:12:38.848 - 00:13:22.406, Speaker A: This is necessary for the first commodity to preserve the demands. But now what could happen is that I violate the edge capacity on this arc, and say, on this arc as well. So I have to fix it. And I fix it by using the second commodity and route one unit of flow in the other direction so that the total difference in capacity here is zero. Now, when I do that in the second red commodity, then I have to again fix the demands. And if I do that, then what I end up with is a flow of two over this arc here. Now, this gadget, I can repeat again, I have to fix this with the first commodity and I keep going.
00:13:22.406 - 00:14:36.196, Speaker A: And if I keep doing this, then it turns out if I just want to change one unit of flow over this arc here, I'm going to be forced to change an exponential amount of flow on another arc. And yeah, this is sort of at the core of the hardness of multicommodity flow. And yes, I will sort of show how you can, how we can solve these problems somewhat faster than none before. Okay, so what is the state of the art for certain graph based LP and for general LP? So for. This stands for single source shortest path. For the single source shortest path problem, there was a breakthrough that last year, shared best paper at Fox 22, that both single source shortest path and mincus flow can be solved in nearly and almost linear time tilde of M. However, if you want to solve it exactly, meaning strongly polynomial time, then single source to this path is Bellman Ford order mn since the fifties.
00:14:36.196 - 00:15:45.154, Speaker A: And mean cost flow is a result by Jim Olin from the nineties that you can solve it in mn. So you have this linear gap of factor n. And now, very recently, a few weeks ago, Jeremy Feynman put a prepaid on archive where he claims to solve single source shortest path in strongly polynomial time in mn to the eight over nine, which would mean the first progress in about 70 years on this. So if that turns out to be correct, then this would say that, okay, the gap between single source path, high accuracy and exact is smaller than linear. But yeah, let's say until a month ago or so, the gap between high accuracy and exact for these graph problems was linear. Okay, now, what is happening for general LP in the regime where, let's say we do not have many more variables than constraints, then you've seen already these high accuracy results where you can reduce the optimality gap by a factor of two in M to the omega. The omega is the fast matrix multiplication exponent.
00:15:45.154 - 00:17:18.624, Speaker A: And if you the reason why there was an l in the last time I showed that result was that you have to do that, you have to decrease the gap by a factor of two to the l to be able to round an optimal solution. It turns out that if you want to do that in the real model, so we don't allow bit complexity dependency, then you can still do it as sombertades in polynomial time in the dimensions and the logarithm of the maximum subdeterminant, I've shown the slide before. The maximum subdeterminant is sort of an upper bound on this kappa condition number. And we showed that this polynomial factor can, by black box use of these high accuracy solvers, be turned into a running time, that is m to the omega plus two times the logarithm of that condition number kappa. So in general, if you have a high accuracy solver with running time t, then you can turn that into an exact solver with running time t times m squared times logarithm of condition number. So we see that the gap for general LP between high accuracy and exact solvers would be quadratic, while it, at least until recently, is linear, or was linear for graph based problems. So the natural question would be to ask, is that quadratic gap necessary if you have more complicated linear programs, or should it also be a linear gap? And yeah, I can show that, in fact, you can reduce that gap to a linear factor.
00:17:18.624 - 00:18:12.882, Speaker A: Okay, so you can solve an LP exactly in the real Ram model, in M to the omega plus one times logarithm of the condition number, many iterations. So my algorithm itself is an interior point method, and this is sort of different to the previously fastest algorithms, where I mentioned that previously, you would need to, in a black box way, invoke interior point methods. But it turns out that if you want to speed up these algorithms, you have to open the black box and do something very non trivial with these interpoint methods to make them faster for exact solvers. Okay, we have seen already talked about interior point methods. I will just use the standard log barrier. So let me quickly go over this. We saw a talk by and by Sally on this.
00:18:12.882 - 00:19:04.930, Speaker A: So what you have is you define a barrier function, which tries to push you away from the boundary of your feasible region, and then you initialize with a point that is near the analytic center of the feasible region. And then you drop this parameter mu to zero. And once you drop it to zero, then this optimization problem is the standard optimization problem, the standard LP. And from you going to infinity, you essentially forget about the cost factor. So this is why you end up from you roughly being infinity at the, at the analytic center, and then you follow the path approximately to the optimal solution. Okay, so this is all well known. A particular interior point method would be the predictor corrector path following method.
00:19:04.930 - 00:20:08.514, Speaker A: Let me mention that modern methods that are the most efficient usually do not follow this predictor corrector regime. But however, if you want to solve an LP exactly, then you have to perform, at least in some sort, predictor steps that reduce your gap more efficiently than these standard pessimistic methods are doing, where you reduce the gap by a factor of one over root n in every iteration. Okay, so what is happening? You start somewhere near the central path. You do predict a step which is essentially trying to look for the gradient for the tangent on your path. You follow the direction until you are too far away, and then you correct and go back and you keep doing this. And standard analysis show that you will need order root m, many iterations to reduce the gap by a factor of two. Okay, let me now come to the limitations of these shortstep method, which can be easily exemplified by a very basic, very simple lp.
00:20:08.514 - 00:21:10.844, Speaker A: Here, my polytope is just the infinity ball in two dimensions. My central path is going to be the origin, and I give the objective that is minus theta for some very small theta and one interaction in the y coordinate. Okay, now what happens? It is easy to see that the optimal solution is going to be the right bottom corner. However, if you follow the central path, what is happening if that theta is very small is that initially you will roughly follow this line here. So you make a lot of progress in the objective value, but you do not make progress in reducing the distance to the optimal solution. And the theta here, I can, in the real model, choose as small as possible. So why is that not a problem in the bitcomplexity model either? In the bit complexity model, theta is zero.
00:21:10.844 - 00:21:50.480, Speaker A: And if theta is zero, then my cost vector is just the vector zero one. So my optimal solution is down here. My path would be just the straight line down here. So I can find this in one iteration, in one step. And in the bitcomplexity model, if theta is not zero, then it's lower bounded by two to the minus bit complexity or so. So my angle is at least two to the minus bit complexity. In fact, that tells me that I only need to reduce the gap by a factor of two to the minus bit complexity, which you can do with proportional to l many iterations and therefore you can find document solution in l time, poly and many iterations.
00:21:50.480 - 00:22:28.060, Speaker A: However, if theta is not bounded by bit complexity, then any shortstop method, Theta could be super, super tiny would need to do every initial step method would fail to find an optimal solution efficiently. Okay. Yes. So this is sort of how to overcome these limitations. I will explain a bit later. Let me now give an overview of what is out there for exact ipms that do overcome this issue. So there was the first interpod method by Stephen Vavasis, and he knew ye in 1996.
00:22:28.060 - 00:23:20.180, Speaker A: And there were follow up works by George lan, Montero and Tsuchiya that showed that you can design a tier point method that solves lp exactly within this many iterations. And each of these iterations can be computed in strongly polynomial time, and every iterations requires order and many linear system solves. We sped that up in 2020 where we showed that m to the 2.5 main iterations suffice. And just last year we had a result that is somewhat universal for path following a sharepoint method. And that says, okay, here we have an a sharepoint method that will only require at most m to the 1.5 many iterations as any path following sharepoint method that you design.
00:23:20.180 - 00:24:39.412, Speaker A: Okay, so for people that care about fast algorithms with high accuracy, this m to the 1.5 factor may be very unimpressive, because, I mean, in the end, root m, many should suffice, right? But then again, if you want to go to the exact regime, you can design Lp's, where any inferior part method would require exponentially many iterations, two to the dimension. So, yeah, for that regime, this sort of shows the limitations of interior port method and essentially tells you that if there's a strongly polynomial time interior port method out there for a certain problem, then this one here is as well. Yeah. What does path following mean? Does it mean shortstep, or would your method that you're presenting also count as path following? So, path following, I would call any Interpol method that follows approximately the central path of some barrier function. Okay, so there are these potential reduction methods that don't necessarily follow a path. We cannot say anything about these, we cannot compare to these.
00:24:39.412 - 00:25:28.254, Speaker A: But if you follow to some exponential factor, even some barrier induced central path, then this algorithm is as good as your algorithm, and you assume that the barrier is self concordant. Yes, yes. Good. So sort of to hell explain, without explaining the interport method, the underlying geometry of how many iterations you're going to need. I will quickly speak about the so called straight line complexity defined as follows. A crucial notion for that is the so called max central path. And that is the following.
00:25:28.254 - 00:26:17.954, Speaker A: I just draw a two dimensional diagram. The x axis is the optimality gap. The y axis is the, say, slack variable or the variable xi. And now I'm asking you, look at the sublevel set of your polytope where my gap is at most g. And subject to that, my curve is going to be the point that maximizes the I've coordinate. Very simple, very related to what shadow simplex does. And if you project your polytope onto the two dimensional space spanned by the objective vector and by the coordinate xi, then this curve is exactly the subset of the, of the boundary of the polygon that you will get.
00:26:17.954 - 00:26:58.962, Speaker A: Okay? And now the straight line complexity is the following notion. The blue curve here is the same as we saw in the last slide. You just scale this curve by a factor of two. And now what you do is you look into this tube between these two blue and light blue curves, and you try to find as few as possible linear segments in that curve. And it turns out that the number of iterations of your interior point methods is determined by the number of linear segments, number of green linear segments that you, that you get here. Okay, let me. Yeah.
00:26:58.962 - 00:27:53.094, Speaker A: So there are very basic properties of that curve. If you want to show that thread line complexity is small, you need to do a lot more advanced things. But one thing is clear, for example, is that the number of pieces that you would require is at most the number of pieces that your maximum path has. So the number of line segments of your shadow. And it turns out, for example, for the maximum flow problem, the number of these segments is only two. So this would, this immediately gives you that interport methods are strongly polynomial for maximum flow. Okay, what we show in our paper this year is that, in fact, if you saw the result that you can deconsolve LP's in strongly polynomial time whenever a matrix contains at most two non zero entries per column, and it turns out that the straight line complexity of these variables is going to be strongly polynomial.
00:27:53.094 - 00:28:31.414, Speaker A: And this is why. This is how we show that you can solve LP with at most two non zeros per column in strongly polynomial time. Additionally, we show that, in fact, the straight line complexity is bounded by a linear term. So m times the logarithm of a condition number. This result here, the first one, that is about, I guess, 20 pages or so, but the second one is just one page. The proof of this fits on one page. So now the question is, let us be optimistic and take this threading complexity bound and plug it into this result here.
00:28:31.414 - 00:29:08.814, Speaker A: And let's see what we get in the running time. So if you optimistically say that amortized, you can implement every iteration in m to the omega minus one half. This is exactly the amortized running time that you get for high accuracy solvers per duration. Then the overall running time would be the number of iterations times the cost per iteration. And what you would get is m to the omega plus two times log kappa. So this would match the result that we had three years ago. But I did promise you that we get this down to n to the omega plus one.
00:29:08.814 - 00:29:54.390, Speaker A: So new ideas and new insights will be required to drop the running time by this linear factor. Okay, good. Again, before I go into the ideas of how to make interior point method exact, let me show another pathological example. We've seen one before where we had the l infinity ball and where the angle of the objective was very close to a hyperplane. But here's another example that is a flow example, and there will be how we can overcome these issues that if an angle is very close to zero. So it's the most simple instance of a Schrodinger's path problem that you can imagine. I give you two vertices, a root r.
00:29:54.390 - 00:30:42.522, Speaker A: I use r here because I want to use s for the slacks and the target t. And I give you two parallel arcs and they have slightly different costs. So the first arc has cost one, the second arc has cost one plus theta. And this theta is super, super tiny, much smaller than one. Okay, now it is easy to see that the optimal solution to this would be just send the flow over the first arc because it's cheaper. However, what is the interior part method trying to do? If you have a large perimeter mu, then the optimum is given by the minimizer of the feasible flows of this function here. And because theta is so small, the cost doesn't really matter whether I send it over the first arc or over the second arc.
00:30:42.522 - 00:31:18.086, Speaker A: However, for this logarithmic term, I would really like to balance out the flows of these two arcs. So the problem is now that it's going to happen that if I'm on my central path, or near my central path, I'm going to send one half unit of flow over the first arc, or approximately one half unit, and I'm going to send one half over the second arc. And this will not change. I mean, I can arbitrarily arbitrary multiple factor of mu. I can reduce my gap. I will not see changing behavior of that. So what I need to do is I need to go through the dual side.
00:31:18.086 - 00:32:05.610, Speaker A: So what is the dual of this problem? I want to minimize the sum of the slacks subject to these constraints. So these are sort of the, so the s's are the reduced cost. It is easy to see that the optimal dual solution is given by, of course, the first slack being, being zero, because we want to route flow in the optimal solution over the first arc. And the cost or the slack in the optimal solution on the second arc is going to be theta. But now my path on the primal side, on the flow side looks like this. The x axis is the gap, the y axis is the flow that is sent over the arcs. So if we have very high gap, if we start somewhere close to the elliptic center, then both flows are going to be roughly one half.
00:32:05.610 - 00:32:55.816, Speaker A: And I would need to reduce the gap by a lot to finally get to the gap theta. Finally, the interior pot method would learn which arc is actually going to appear in the optimal solution. So here we will see that at some point, once my gap is theta, the first flow will go to one and the second flow will go to zero. So the question is, how can I accelerate my interior point method to go to this point theta efficiently? Okay. And if I look on the dual side, what is happening here is the following, sort of. You have the complementary behavior, because flow times slack is always going to be constant. You have that all the slacks are essentially scaling down to zero until you hit again the gap theta.
00:32:55.816 - 00:33:28.354, Speaker A: So it is easy to see again that, okay, for the first slack, you really just go down to zero. And the second slack is a slight shift of the first slack. So in particular, until I'm here at theta, it really scales down as well. So what I suggest as an update, you do that manually. You don't do that by computing the standard update steps. In sharepoint method, you would not touch the flow on the primal side because that's the right thing to do. And on the dual side, you would set them by hand to minus the first leg.
00:33:28.354 - 00:34:09.930, Speaker A: And it will turn out that this is in a good direction. And you can, will be able to reduce the gap until you are here down at theta. And you can from any point on the central path in a single iteration, reduce your gap to the theta here. Okay, so what you have achieved once you've done that is that the second slack is roughly where it's supposed to be in the optimal solution. The second slack is roughly theta on the primal side. Still nothing has happened. But now you can use the slow step once you are at this point, the slow step, to make more progress on the central path, because now slowly, the first flow will reveal itself.
00:34:09.930 - 00:35:06.793, Speaker A: It will go down, will go up, and the second flow will go down to zero. And then you do a second update step by hand that really pushes this flow to exactly one, this flow exactly to zero, and this like exactly to zero. So this is sort of, you need to do two steps by hand to be able to turn the standard RPM analysis into an exact solver. Okay, this is a very simple example, again, the most simple example that you can imagine, and it turns out that roughly this strategy works for general LP in the following way. The predictor step, that is sort of the gradient on the central path, is given by the following system here. So you want an update step, delta x, that is primarily feasible, which means it lies in the kernel of your matrix. You want a dual update delta s, which lies in the image of a transposed.
00:35:06.793 - 00:36:15.394, Speaker A: And you want in the third step here. These are the equations which tell you that you want to reduce the optimality gap. And then you choose some step size alpha and go in these update directions to reduce the gap. Now the question is how large can you choose alpha so that you don't go too far away from the path? And standard results are that you can pick alpha proportional to one over root m, and therefore your gap reduces by this factor. However, you can do much better in an optimistic scenario, and that is you can reduce alpha such that your residual gap is going to be something that is proportional to the Hadamard product of your primal update in local norm x times dual update in local norm s. Okay, this is sort of a two line proof that this works out. And this is something that we exploited in this toy example on the last slide where we essentially said that delta x, delta on the flow was zero.
00:36:15.394 - 00:36:38.698, Speaker A: So in particular, can reduce, reduce the gap by a lot. Okay, so far so good. Is that l two or another? It doesn't really matter. I think it should be l two because I mean, yeah, it should be l two. The gap between the two norms. Of course, that most root. Nice.
00:36:38.698 - 00:37:13.264, Speaker A: And if you can reduce the gap in two norm, then you just do slow steps afterwards. Not your concern with the choice of norm, because you wanted to also get the fastest, stronger body than algorithm. Okay, I think there, it wouldn't matter in the end, but the. Okay, if you want to stay in l two neighborhood of your path, then it's l two norm. So I guess. Yeah, but I use the algorithm is l two. Okay? Right? So, we have this linear system.
00:37:13.264 - 00:37:59.406, Speaker A: We have this linear system here that we're trying to solve. And it turns out that there's a very nice interpretation in terms of, you could say, electrical flow. Or for general Lp, it's just an l two regression problem. And that is, if you're roughly centered, then this delta X update is just the update in delta x norm, such that the residual norm x plus delta x is minimized, subject to the local norm x. And why is that intuition important? It's important for the following way, for the following reason. Let's say we have this Lp here, where my central path, okay, we have the cost function here that is going upwards. My analytic center is somewhere here.
00:37:59.406 - 00:38:42.082, Speaker A: My optimal solution is down here. And let's say I am somewhere here on the path. And what is happening is that the slacks on the blue arcs on the blue edges is super small, while the slack keeps getting smaller and the slack on the red edges is slightly increasing. But it's essentially constant up to constant factors. Okay? And we want to accelerate on the central path, then you shouldn't solve the standard l two regression problem, but you should do it in a more principled way. Namely, what you do is you look at the layers. Namely, you order your variables by their value.
00:38:42.082 - 00:39:12.074, Speaker A: And now you see a gap in the slacks between the third variable here and the fourth variable here. And now, what you do is the following. This is what Vervas and J did in 1996. 1st, you sort of solve this l two regression problem by only caring about the variables with small x value here. You would be able to set this to zero. Namely, the optimal update direction would be exactly this point here. And then you fix the update in l two.
00:39:12.074 - 00:39:48.182, Speaker A: This is what these equations are. And what you do now is you solve it on the first set of variables, again, subject to fixing what happens on the second. And this will bring you, in this instance here, to the optimal solution in a single step. It doesn't always bring you to the optimal solution, but this procedure will always make you combinatorial progress in a certain sense. So, let us generalize this whole concept to more layers. We just saw two. But what you can do is whenever you see a gap between consecutive variables, just create a new layer.
00:39:48.182 - 00:40:09.714, Speaker A: And now you have this primal, so called layers, least squared step that does the following. You first solve. You go down from the last layer to the first layer. You first solve the l two regression problem on these variables. Fix them. Then you solve it on this, fix them and keep going. And you do the opposite thing on the dual side.
00:40:09.714 - 00:40:44.536, Speaker A: Recall that we have that x times s is roughly mutual, so the order is exactly reversed. On the dual side, l one has now the smallest stack variables. And now what you do here is solve the l two regression problem here on the dual side, fix the variables, do it here, and keep going. This is what you do, and this is what your update step is going to be. Delta x Delta S. And in the analysis they show that in fact you get at most n to the 3.5 log kappa, many iterations.
00:40:44.536 - 00:41:32.384, Speaker A: And the idea behind this is the following. You need to make some sort of combinatorial progress that tells you that this step, this Lls step doesn't need to be done too often. And an important property that they exploit is that the central path is essentially monotone up to a factor of m. So that's, by that I mean that if you look at the optimal solution x, then each entry is at most n times as big as the entry that you've seen at any point on the central path before. And the same thing on the dual side and depicted. This can roughly be interpreted as follows. Recall the max central path, that is maximize the I've coordinates subject to have a certain gap.
00:41:32.384 - 00:42:25.894, Speaker A: Now, if I scale this graph not by two, as it did the last time, but by m, then it turns out that the central path is going to be always in this tube. Okay? So it's not going to be monotone. It can do some, some funky stuff. It can go a little bit up, then down a little bit up, but it's essentially always decreasing. Okay. And this is crucial because of the following year. Show that after a single step you arrive at some parameter gap, parameter mu bar, so that there exists some layer in your layering and variables I j in that layering, such that the ith coordinate has essentially converged, meaning that the optimal solution is at most the poly m factor away from your current value.
00:42:25.894 - 00:43:30.456, Speaker A: And again, this means that I'm essentially converged because I'm monotonically decreasing. Essentially monotonically decreasing. And there's also a dual variable j, where the optimal solution is, yeah, only a polyfactor away from my current value. And because of the fact that the product x times s is spontaneously decreasing with mud, you will have it that if you only decrease mu by this factor here, which is only exponential, which means in log world, which interpod methods live in, you can do that in poly time, you have that the value of xi is going to be much bigger than the value of xj because again, sj is converged, so xj has to go down. In particular, if I is so much bigger than j, then they will never appear on the same layer again. Because recall, we chose our layers subject to their values roughly being the same. Okay, this is combinatorial progress.
00:43:30.456 - 00:44:07.474, Speaker A: You learned that in J will never appear in the same layer again. Such pairwise information you can learn at most career many times. And this will give you the running time, that is poly m times algorithm of condition number. Okay, so this was all the was in D in 1996. But it turns out that you don't have to make your layers separated by such a big gap as they do. So what they did again, is you build a layer. Whenever you see a gap of again here, switched to delta, which is maximum sub determinant, but doesn't really matter.
00:44:07.474 - 00:45:04.830, Speaker A: Whenever you see a gap that is that big, then you create a new layer. It turns out you don't have to do that. It is enough if layers that are by index two apart are separated by a polynomial factor in delta, that allows you to create layers, essentially whenever you don't need to be lucky or you don't need to wait until a big gap appears. Here, I only want the red part to be separated by a gap to the gray part and the blue part to be separated by the light green part, again by polynomial factor. So it's a much finer layering that you will build, and you also have to build a new update step that I will go over very quickly. So what we've done before was we set these here. We essentially solve the l two regression problem on these coordinates while not caring at all about what they are doing here.
00:45:04.830 - 00:45:43.864, Speaker A: But now the problem is I don't have that the gray part and the blue part are separated by a big gap anymore. In particular, if I just solve the l two regression problem here while ignoring what's happening on the blue part, then I run into problems because the gray part influences the blue part by far too much because I don't have separation. I need to be careful in my update. And the care that you take is that you don't ignore what is happening on l one and l two. But you make sure that your initial update is in a space that is safe for you. And that is, the space is fd find here. Let me not go into details.
00:45:43.864 - 00:46:18.924, Speaker A: And in the second step, you look at the orthogonal space to the space in the first space, and you perform the update here. Very high level, bit too much algebra, I think. So let me skip over it also. What you need is a stronger convergence result. So here, this is what we saw in j, that there exists some layer for which I and j exist, where I converge on the primal side and j converge on the dual side. It turns out that you have a much stronger convergence result as well. And that is the following.
00:46:18.924 - 00:47:25.088, Speaker A: Perform one step of this finally square step where we don't look for separation. And then there exists a variable I and ball around this variable, meaning that I look at all variables j that are only a poly factor in kappa away from xi. And then there exists a variable j in that ball with the same properties as a versenti head. Okay, so that essentially tells you that you learn combinatorial information quicker. And also what you can show is, or what is easy to see it is that there are also combinatorial events happening for all other variables k that are in your layer b or in your ball b. Because essentially either the x variable in the xk variable will either follow xi, meaning it also converges, or if we follow sj, which means it goes down on the primal side. So xk would go down, but then it will never be layered together with variable I.
00:47:25.088 - 00:48:28.870, Speaker A: So intuitively, if you have a layer of size l, then you learn l many pairs very quickly. So that is sort of the summary of that. And it will turn out that you can also have longer step sizes. So recall that in general, for general lp, it is a big question whether you are able to show that you need less than root mn iterations to decrease the gap by a factor of two. But it turns out that the step sizes that you can take is proportional to the size of the layer that you have intuitively, why you can only take by your step sizes bounded by root m is the the following. In every iteration of an interior part method, you project the all ones vector onto some subspace, and the step length is proportional to the supremos norm of that projection. Now, the all ones vector has norm, root m, so the supreme norm can be up to root m.
00:48:28.870 - 00:49:04.070, Speaker A: And this is why you get your step size. But it turns out that if you have layers with a certain gap, then the step size that you can take is inverse proportional to the size of the layers. So, theory, yeah, it tells you that you can. The larger the layers are, the less progress, or the larger layers are, the less progress you can make. But you learn more combinatorial information. So this is the trade off that you can exploit. And the same holds if you don't have separation.
00:49:04.070 - 00:50:13.628, Speaker A: This is what you can show. You can show we can also make these steps that long, even if there's no gap, only if there's a gap between layers that are index two apart. Okay, so combining sort of these two insights, meaning I find combinatorial events between two variables quicker and I have larger step lengths, you sort of get this result, okay, in root l or in root cardinality, many iterations, I can learn cardinality l, many combinatorial events. And what is sort of left to show is that not only can I take longer steps, I can also compute these steps faster in amortized running time than m to the omega minus one half. And for this I will only give high level ideas why you can do that. In the following, you perform some matrix multiplications, sorry, matrix operations that make the turn your constraint matrix into one that corresponds to your layering in the following way. So let's look again at our layering.
00:50:13.628 - 00:51:14.402, Speaker A: We have that the values of my variables are separated whenever the index is at least two apart. And now you essentially do some partial gaussian elimination where you want that it's an upper triangular matrix. And the nice property is now that if you get your matrix into this shape here, then it's almost a diagonal matrix, because these entries up here are not zero, but they are tiny. And they are tiny because you made sure that there's separation between these variables here. Now what you do is, with these rows is, okay, it makes only sense if someone, makes only sense if the row norm is one. So these really don't matter much. And it turns out that if you, okay, that doesn't turn out, but what you have to do is you have to make these blocks in rows orthogonal.
00:51:14.402 - 00:51:55.722, Speaker A: And I don't mean you make the whole thing orthogonal, that would be too expensive. But you only make the parts where they meaningfully intersect orthogonal, because this year's epsilon, they are almost orthogonal on this part. And it only really matters what's happening here. And you make them orthogonal, you maintain their orthogonality throughout your algorithm. And the important bit is that the IPM performs projections onto this matrix here. And if you have orthogonal rows or block orthogonal rows, then your projections become much, much cheaper just because. Yeah, I mean, yes, projection onto orthogonal matrices are trivial if they are block orthogonal, the cost is proportional to the size of the blocks.
00:51:55.722 - 00:52:39.658, Speaker A: So that's make, that makes the iterations much, much cheaper. Okay. Right. So what I've done here is sort of for the image of the transpose of your matrix. So what I maintain is sort of matrices that are a decomposition of your image space of your rescaled image space that is orthogonal. You can do the same thing with matrices v on the kernel of your matrix. And now the analysis of the algorithm that you would perform here is a little bit different to standard analysis of robust PM.
00:52:39.658 - 00:53:40.832, Speaker A: What normal robust PM would do is they sort of define the potential of your error of your centrality arrow phi, that is roughly supreme norm of the arrow per coordinate. And then you show that even after long steps you can bound that error after a certain by a constant, say. However, we will not be able to do that here because we want to make larger step sizes. So we will not be able to bound the total error. But what we are able to do and what is necessary and sufficient is essentially whenever you have an error here, phi, this error can be attributed to the subspace. So this error decomposes onto the primal and dual orthogonal subspaces. So in particular, what you can do is you bound for every of these subspaces, the error, that is the projection onto that subspace of the gradient of your error.
00:53:40.832 - 00:54:44.882, Speaker A: And you can show that this is constant, and with that everything is fine. You can show that you have a faster robust rpm, again very high level, but this is how you can design the improved running time. So let me summarize. We had so following tartus framework in 86, we found a combinatorial algorithm that runs into the omega plus two. It turns out that Vovasis and GE and follow Burke showed that with an IPM you can achieve again poorly running time, however not very efficient. And the new result is that you can actually get this down to the best known running time for exact LP for combinatorial linear programs. And the tools are that you need compared to a was Ng, larger step sizes, you need better layering, you need stronger convergence results, and then you need the classical tools, that is even lazy updates and then all, you know, dynamic matrix inverse.
00:54:44.882 - 00:55:39.514, Speaker A: This year is all sort of almost plugged in from CL's 2019. And yeah, these things all combined and carefully mixed. You, you get the overall running time still open is the following what do you do about sparse matrices? So you've seen on this last slide that I had to manually made my subspace orthogonal, so I cannot exploit any sparsity because I have to have expensive matrix operations to make, to make and maintain orthogonality. So that is unclear. Then what can you do for wide matrices? All of these arguments sort of rely on some self duality, which you will get by the standard lock barrier. And it's not clear how you would be able, for example, to plug in the lee, silver barrier to turn, essentially, maybe a root or. Sorry, there should be an m here to turn a root m into a root n.
00:55:39.514 - 00:56:06.392, Speaker A: And then, of course, what is. What should be the true gap be between strongly polynomial algorithms and high accuracy algorithms. Because even for. I mean, now we see that maybe for single socialist path, the gap shouldn't be linear, but sub linear. Is that same thing true for general combinatorial p? Who knows? There are certain barriers to that. But. Yeah.
00:56:06.392 - 00:56:38.224, Speaker A: Okay. Yeah, thank you. And. Yeah, that's it. I'll ask some questions to get us started. So, can you just remind us what, like, what is combinatorial LP here? Like, what's. Yeah, essentially, all matrices that are integral and don't have entries that are larger than exponential.
00:56:38.224 - 00:57:06.044, Speaker A: Okay. And then this delta is that the condition number that you should be the largest subdeterminant. So all the results would hold for the more general condition number that is Kappa. But it's good intuition to think of the largest subdeterminant of your constraint matrix. Okay. But I can have, like, I can have, like, unbounded. No, I cannot have unbounded entries here.
00:57:06.044 - 00:57:22.768, Speaker A: So if you. If you want to. Okay. That's sort of the reason why it's. It's better to think of Kappa, because for Kappa, you could even irrational. You could have random data where you not necessarily would think of bit complexity bounds. It's really about this condition number kappa.
00:57:22.768 - 00:57:53.084, Speaker A: Yeah. So the norm of a b to the minus one, a, the maximum of that. Okay. And then, now just channeling Santosh and Richard a little bit, because they have a paper on doing, like, bit complexity analysis for Cl's, which Cl's does not do. Do you need to do something like this? Yeah, on purpose. I wanted to go through the. Where I don't have to bother with that.
00:57:53.084 - 00:58:25.044, Speaker A: However, if you want to make this work in the bit complexity model, then you lose a lot of running time. And that is because, essentially, I don't want to have dependency on b and C. I don't want to like to have dependency on the bit complexity of b and C. And what all these algorithms by Santosh, Meerdat and Richard would do is, you know, you need some sort of rounding. You need to access the bits. I don't want to do that. I can do it without it by just representing everything I do by convex combination of vertices of my polytope.
00:58:25.044 - 00:59:20.870, Speaker A: However, that is expensive. You need to do some carradori type decompositions, and it's unclear how to speed this up. Yeah. So I don't have. I'm not allowed to round things, and that's the problem. So this nearly orthogonal property, does this kind of make this lattice reduction then by, like, when you define something as being nearly orthogonal, is it like so small that you're looking for some kind of basis in there? Does that make this a lattice reduction? Oh, I'm not sure about relations to lattice reductions here. So what I skipped over sort of, is that either you are able to maintain this authority composition or your layering becomes invalid at some point.
00:59:20.870 - 00:59:57.378, Speaker A: And if it's invalid, then you also learn combinatorial event. So there's sort of an additional layer of complexity that I try to omit during my talk. Okay, so you're not really checking to see if this is like NP hard? Oh, no, all I'm doing is, is essentially gaussian elimination, and on top of that I do some orthogonalization. That's it. One quick question. So is the final algorithm is like deterministic at the end? Yes. Okay.
00:59:57.378 - 01:00:17.134, Speaker A: Yes. All right, let's thank Bento again. And we have a break. And we continue with usual at 130 minutes.
