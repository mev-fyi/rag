00:00:00.320 - 00:00:04.794, Speaker A: Okay, so thank you for the organizers for giving me this chance to give a talk.
00:00:06.654 - 00:00:08.154, Speaker B: Everyone who's an organizer.
00:00:10.934 - 00:01:34.560, Speaker A: Actually, the motivation for this work is partially your work with John. So, it's a paper by John Hunsten and Frank Sotheil in 2011 that they certify, it's like a program package, and the paper coming with it, it certifies square systems, polynomial system. N polynomials in n variables using alpha theory. So, in general, alpha theory developed by Steve Smale and Mike Schub gives a formula using derivatives of the functions and the coordinates of your approximate point that you and you plug it in. And it gives a proof that from your point Newton method quadratically converges to a root of the system. This only works for simple roots. And in fact, you also certify that between your approximation and the exact root, there are no other roots of the system, and you can also use it to give a bound on the distance from the exact root.
00:01:34.560 - 00:02:38.890, Speaker A: So what's new about this paper of my understanding of it is that they give a very easily computable bound for the alpha function in case of polynomial system, and they implement it, and they use it for certifying polynomial systems. So, to get from square system to over determined systems, it's a big jump. There are a lot of difficulties. So, there is an alpha theory developed by Didier and Schub for over determined systems, but we couldn't use that. And the reason is because that would certify fixed point for the Gauss Newton iteration. But those fixed points do not correspond always with roots. And the other big jump in the notion is that really consistency of over determined systems is not a continuous property.
00:02:38.890 - 00:03:41.266, Speaker A: We are not interested in this talk of certifying whether you can perturb your input system and then find the root. We really want a root of the input system. So, for example, if you don't know the coefficients exactly only in an interval of your input system, the question doesn't even make sense. So this simple system, if you don't know the exact value of a, you cannot decide whether it has a root or not. So that is the reason that the question that we're trying to answer only makes sense over the rationals. You can also extend it for algebraic extension, but that eventually reduces to the rational case. So, in the same paper of Frank and John, they actually studies this same question, and they give a certification algorithm to certify that something is not an approximate root using alpha theory.
00:03:41.266 - 00:04:58.234, Speaker A: So basically, the idea is the following is that they take two random square subsystem of the over determined system, they are just random linear combinations, and they use homotopy method to solve them. And then they, with alpha theory for each of these random square system, they could certify that the point that they want to certify is an approximate root. If they find that for one of them, it's not case closed. It was not an approximate root for the full system. However, if they are, you can certify that the corresponding roots are not equal by getting more and more accurate approximation of the roots until using each of these square system, until you have this inequality. So here we don't know the roots, but with alpha theory, you can get a bound on the right hand side. And this would prove basically that the corresponding root cannot be the same point.
00:04:58.234 - 00:05:44.336, Speaker A: Yeah. So this is using office theory to prove that your point is not an exact, not a root, an approximate root, to certify the opposite, that you are actually near an exact root. They basically propose, to use a priori worst case lower bounds for rational polynomial systems, which are known to be positive over a disk. And in the paper, they conclude that these known lower bounds, in the worst case lower bounds are too small to be practical. So is the point.
00:05:44.360 - 00:05:45.112, Speaker C: We were wrong.
00:05:45.248 - 00:06:22.816, Speaker A: No. So here is the simplest version of this theorem. Go back to Kenny in 87, and he gives, you can see that these lower bounds are, well, the size of them are exponential. So the magnitude is double exponential. And a very simple example shows that in the worst case, you can actually get that. So here this system is n plus one, polynomials in n variable. The height of this polynomial is at most two.
00:06:22.816 - 00:08:03.740, Speaker A: The degree is also two, and they don't have a common root. But the last or the extra polynomial can have such a double exponentially small value on the roots of the previous polynomial. So yeah, using worst case lower bounds, we cannot hope to get too much better. So what we did is that I looked at these papers of, oh, I forgot to say, there is in the audience several people who actually work. This is a lot of work has been done to finding these lower bounds and extensions and different cases. So what I found, I may be not completely right about it, that most of them reduce the problem to the univariate case, either by considering rational univariate representation or other kind of rational representations, which are all really computable objects, like these papers use computable objects, and then they use worst case height bounds and univariate root separation bounds to give the worst case lower bound. So the main idea of this work is that why don't we compute those objects for our system instead of using these very pessimistic lower bounds.
00:08:03.740 - 00:08:58.873, Speaker A: So that's the main idea. And so the goal and the results, what we achieved, there are many choices to do that, as we figure that basically what you have to do. So first of all, as I mentioned, the certificate that we want to give is for a property that is not continuous. So the certification necessarily has to have some exact computation. You will have at one point decide to decide something x equal to zero, a number equal exactly to zero. So it we cannot hope to use a fully numerical algorithm. So what we propose here is a mixed hebrew symbolic numeric method, which is better than the worst case one.
00:08:58.873 - 00:10:05.180, Speaker A: So most of the you could go just using symbolic methods, then you would get the worst case bound. But what we're trying to prove that mixing symbolic and numeric techniques with not too much extra computation compared to the homotopy continuation method, you can actually also compute a certificate. So we take homotopy continuation method as kind of something that you can do. So we assume that for a square subsystem you can compute all the root, and then a little extra computation gives you also a certificate for the over constraint system. And then the second part of the talk, I will talk about how to reduce the certification of singular roots to the previous case for the over determined case. So that's the outline of the rest of the talk. So first, the over determined system, and I'm not going to give you the whole algorithm, it's very simple.
00:10:05.180 - 00:10:45.064, Speaker A: As I said, there are many choices. So basically what we try to avoid is there is literature. You could use, for example, the LL algorithm to create a rational representation. We try to avoid that, because that doesn't parallelize. We wanted to find an algorithm that just like homotopy method, you can compute with many processors parallel. So this is maybe the simplest such representation that you can do. There might be better ones, but we did our complexity analysis for this particular simple representation, and I show it on this toy example.
00:10:45.064 - 00:11:34.592, Speaker A: So this is three equations in two variables, and it's corresponding really already to the double root case. So it's the intersection of this circle with a parabola. And we are interested in this, in the two double points. So the third equation is just the determinant of the Jacobian of the first two. So the first step is to use homodopy method for a random square subsystem, same as in the paper of John and Frank. And we have four single roots with no multiplicity. And then the second step is plugging back or use your favorite method to exclude the roots.
00:11:34.592 - 00:12:26.304, Speaker A: That's surely not going to satisfy your over constraint system. So among the four, two will be candidates of satisfying all the three equations, and we get some numerical approximation. So the main point here that these approximations we cannot hope to find exactly, they are from an algebraic extension. In particular, the second coordinate will be one four, but the first happens to be not one. It is a proper algebraic extension. However, what we do instead is we combine them and get from these two two roots. They will form a rational component, and we can find a representation which then have rational coefficient.
00:12:26.304 - 00:12:56.740, Speaker A: So basically we use what is called the rational univariate representation. Again, you could use your favorite representation. Rational representation. The first part is a so called primitive element. It's a linear combination of the coordinates that separates the root. This is the defining equation of your primitive element. And then these ones are just interpolate the coordinates in the primitive elements.
00:12:56.740 - 00:13:39.994, Speaker A: It's a very simple representation. And so the first one is not the true exact. It's just what you get from the first approximation. Then you have to use some technique to refine the accuracy. And we give several techniques, basically based on Hansel lemma, you can do it or just brute force interpolation and Newton's method, until you get the exact one. And this is where you check with the exact arithmetic that something is zero. Yes.
00:13:39.994 - 00:14:21.404, Speaker A: So checking whether you found the right thing is an exact computation and can be easily done. And then finally, now once you find the exact representation, you just use for certify, for now, this square system to certify the roots, in other words, from approximate roots, we construct the approximate rational representation, and that one we can find actually exactly by refining it. That's the main idea. It's reconstructing a rational representation from approximate data.
00:14:22.024 - 00:14:23.364, Speaker B: Is there ru r?
00:14:23.864 - 00:15:23.334, Speaker A: Yeah. Yes. So it may be a subset of the, the original set, but when you choose these points, you have some, these approximate points, you have some control to include the one that you want to certify to start with. So the main results, what we proved with this very simple algorithm, are the complexity results. So the first one is that each iteration can be efficiently parallelized. So where we use an arithmetic parallel complexity model, so it will be the square of the logarithm of the depths of the polarization with polynomially many processors. And the second one, and this is like a simplified version, is basically the main point of this, doing all this.
00:15:23.334 - 00:16:11.224, Speaker A: So in the best case, when we actually find a rational representation, the algorithm is polynomial in the output size and not the worst case size. And that's why. So basically you gain when your system is better than the worst case. So it's not a degenerate case. Then you can actually find it much faster than using either symbolic methods or the worst case binds that John and Frank uses. And then finally we also study the worst case. When you actually fail, it happens that you fail, and hopefully you never get into that situation.
00:16:11.224 - 00:17:00.670, Speaker A: Using the method of disproving of John and Frank, you can actually detect when you chose the wrong point. So hopefully you will never get in this point. But this is actually according to the worst case that other people use or what you would get with only symbolic method. So the moral is that you get an early terminating algorithm if you combine numerical and symbolic techniques. So the second part is to use this in certifying ISO singular points. So there were several people mentioning these algorithms or these techniques. So here the problem is similar.
00:17:00.670 - 00:18:00.848, Speaker A: We are given a point and polynomial system, which is not going to be well constrained on the under constraint, over constraint. And we want to certify that the point is an approximation of an ISO singular point. And instead of defining ISO singular points, I just give this example. So we do not require that it's an isolated root of the original system. So for example, if the original system consists of this single polynomial, then the origin is a non isolated root, but it's an isolated root of the deflated system. So here, the jacopian matrix at the origin, we have ranked zero. So you just add the one times one submatrices and the determinants of them appended to the system.
00:18:00.848 - 00:19:03.574, Speaker A: And I will show you. So using this isosingular deflation, you deflate it. And then for the deflated system, you have an isolated simple root, then you have an ISO singular point. We will use the same similar deflation methods that other people use, and you have seen today several times, and I recall this, to determinant of deflations, there are again other choices. We happen to choose this one because it has some properties that we like. So basically, you append to the original system subminers of the jacobian matrix, and you find those sub minors. By having an approximation of your ISO singular root, you can detect the maximal nonsingular minor, and then you just take the minors that contain that.
00:19:03.574 - 00:19:44.462, Speaker A: So you can make this not too many, so you can make it just polynomially many and you can iterate this. I think this method would not work if you don't assume that the number of iteration of this deflation is too big. Maybe one or two times you can iterate. The degrees of these polynomials are decreasing heavily. But in theory it's possible to. So what we like about this, that you get an overconstraction constraint system. It's a rational system.
00:19:44.462 - 00:20:37.928, Speaker A: You don't use really your approximate root or root coordinates anywhere. And the number of variables did not increase. So the heaviest part of the computation, the homotopy method, will have the same number of n variables and just the degree of the polynomials increased though. But other, there are other methods, maybe this is not the best, but for our purposes, to demonstrate that the algorithm works, it's suffice to use this one. And then you just use the machinery that I talked about. You have now an overdetermined system that has your original singular point. Now is non singular for this system, and you can certify it.
00:20:37.928 - 00:21:15.964, Speaker A: It's an over determined system. So let me finish the talk with examples. So the first one is called the Capres system. It's one of the benchmark problems that people try to use, and we try to find examples when actually the roots are not rational. So you really need to find the rational representation. The roots alone wouldn't give a certificate. And so this system has 24 non like simple roots and eight multiple roots.
00:21:15.964 - 00:22:22.524, Speaker A: And each multiple route has the Jacobian with rank two. So the deflation appends for three by three sub determinants. And then we get an over determine system that now have the eight roots. And then we can compute this rational univariate representation which certifies them. The second example is, well, it's called the cyclic n system because you cycle around the variable in these products. And again, this is considered hard to solve because of the high number of singular roots and also the symmetries in the solution. And again, it has, for n equal four, it has eight singular isosingular points.
00:22:22.524 - 00:23:16.654, Speaker A: And then using this just one step of the deflation, we deflated them. And then you can certify them with the over constraint certifier. So this is again the rational univariate representation that you can compute. Exactly. And then finally for the cyclic nine system, a little bit modified, have now 54 regular and 54 double roots. And then these double roots actually split up into three subsets, depending which sub determinant of the jacquard matrix is nonsingular. And then for each of them you can find the rational representation.
00:23:16.654 - 00:24:09.106, Speaker A: So just what we are doing now. So the first two here is what I talked about. So well, the over determined system and the ISO singular point certification. What I talked about, the implementation of the over determined system certification algorithm is under construction. Our student tulai is working on it right now in preparation with a collaboration, also with Bernard Morin. We are working on also trying to certify the multiplicity structure. Not only the root itself and its coordinates, but the multiplicity structure.
00:24:09.106 - 00:24:20.734, Speaker A: So we have a. We are working on a paper on it and we haven't yet started to work, but that's in the plans to deal with higher dimensional components. Thank you for your.
00:24:30.144 - 00:24:43.104, Speaker D: Yeah. My question is not actually about Geronimo, but about these bad examples. So you showed us an example where you have a very small separation between rules.
00:24:43.224 - 00:24:43.924, Speaker A: Yeah.
00:24:44.504 - 00:24:45.376, Speaker D: Can you show?
00:24:45.440 - 00:24:46.044, Speaker A: Sure.
00:24:57.944 - 00:25:16.324, Speaker D: My feeling was that all these examples with very bad separation are very degenerate in some sense. So if you look at this example, it looks like it's not degenerate at all, because all the roots seem to be isolated and simple.
00:25:18.644 - 00:25:19.384, Speaker A: So.
00:25:21.324 - 00:25:34.144, Speaker D: If you look at what happens at infinity. I don't remember this very well now, but I think if you look in projective space, then the system is actually not so nice. So you have.
00:25:34.644 - 00:25:43.024, Speaker A: I forget what happened, but in the general case you would have this kind of situation.
00:25:43.424 - 00:25:53.244, Speaker D: I would like to know if it's known that for a non degenerate system, then is it true that you have better bounds than.
00:25:54.424 - 00:26:06.964, Speaker A: Well, you have to define what is non degenerate. Because here, these are already special systems that you want to certify, right? You want to certify over the turbine system, which are consistent. So then within them.
00:26:08.844 - 00:26:10.020, Speaker D: But if you.
00:26:10.052 - 00:26:11.424, Speaker C: I guess in this case.
00:26:14.324 - 00:26:21.064, Speaker D: Yeah, if you. If you have only isolated solutions in projective space and no multiplicities.
00:26:21.484 - 00:26:25.436, Speaker A: But if your job uses certified polynomials that somebody gives you, you don't have that.
00:26:25.540 - 00:26:46.816, Speaker D: No, I understand that what you do is probably more efficient. But I'm just curious whether better bounds than this are available in non degenerate situations. It's true that to hook up the systems. It's a hard problem, right? It's a hard problem to hook up this.
00:26:46.960 - 00:26:47.764, Speaker A: Oh, it's.
00:26:50.664 - 00:26:57.644, Speaker D: Probably true that they are very special in some sense. You are not aware of any renters around?
00:26:58.744 - 00:26:59.684, Speaker A: I don't know.
00:27:00.084 - 00:27:00.636, Speaker C: Well, here.
00:27:00.700 - 00:27:01.812, Speaker D: Let's talk about that later.
00:27:01.908 - 00:27:03.108, Speaker B: What does Charles have to say?
00:27:03.236 - 00:27:25.184, Speaker C: I have two questions. My first one is on the isosingular deflation part. Of course, we discover the rank that we use to form them from using the approximation in the numerically approximate solution. And it's possible to get that rank wrong. Would your procedure realize that? And then.
00:27:29.544 - 00:27:53.466, Speaker A: Well, okay, so your question is that here, right, that we assume that we know the maxima minor? Yes. No, I don't think so. I think you would get into the situation. Okay, so if you don't get the.
00:27:53.490 - 00:28:12.890, Speaker B: Rank right, then you haven't deflated it and it'll still be singular. And so then your Newton method won't converge. So you'll see that if Agnes flips one way or the other, you start with some approximation of the h, I think, earlier.
00:28:13.042 - 00:28:13.934, Speaker D: Keep going.
00:28:17.894 - 00:28:30.514, Speaker B: So you start with some approximation of the rur, and if everything is nonsingular, then you'll converge to this one with rational points. If you get the rank wrong, you won't see the convergence here.
00:28:31.054 - 00:28:34.834, Speaker C: Okay. And is there a test for how?
00:28:35.574 - 00:29:28.544, Speaker A: So this is what I mentioned, that, yes, this is. So to exclude the wrong choices with this method, unfortunately, you have to use these worst case bounds. But what I mentioned, and that's just a heuristic that this is the case that actually alpha theory handles more efficiently by the method of John and Frank. So hopefully, combining the two ways of seeing whether you chose the wrong point or wrong rank, you could use then John and Frank's method in alpha theory to detect it before you have to go through all these computation to certify and figure that you did the wrong choice.
00:29:29.244 - 00:29:40.844, Speaker C: And then my other question was, you have to put some roots together in order to get the right. So, is that a combinatorial problem, figuring out which roots belong together?
00:29:41.624 - 00:30:23.894, Speaker A: So there are many options for this. So here, this situation is actually a nice situation, analogous to computing the GCD of two polynomials. So this subset actually is cut out by one of your given polynomials. So by simply substituting into your input system, you will in practice see which are your points. There are other ways to do it. If that's not the case, you can actually compute using lll basis reduction. If you just know the coordinates or approximate coordinates of one point, you can find the irreducible component.
00:30:23.894 - 00:30:36.804, Speaker A: That would be another way. But in our case, we actually know all the roots, and it's cut out with an extra polynomial, which we also know. So. So it's a fairly easy case.
00:30:38.104 - 00:30:39.840, Speaker D: I think we probably should.
00:30:39.992 - 00:30:40.864, Speaker A: Thank you.
