00:00:00.960 - 00:00:20.154, Speaker A: Craig Tull is going to tell us about a spot suite for data management and workflows in the x ray light sources that are probably, I would say, even more so than astronomy. The largest data producers in our field. Removing the LHC.
00:00:21.494 - 00:01:17.852, Speaker B: Thank you. Thank you for inviting me. I am Craig Tola. I work in the competing research division of the Lawrence Berkeley National Laboratory and head up a small group called the Physics and X ray Sciences Computing Group. We have projects involved with high energy physics, specifically a few experiments at CERN and China, and the CERF facility in South Dakota. We're also working with x ray light sources, primarily the advanced light source at Berkeley lab, but we've also worked with the AP's at Argonne, nsls two, and Brookhaven, and the lcls that you'll hear about this afternoon from Amadeo. So let me just start by saying a few words about not light sources, about data intensive computing in general.
00:01:17.852 - 00:02:06.872, Speaker B: So, the archetypal data intensive experiment that Peter just mentioned is the large Hadron Collider at CERN. They are taking many petabytes of data every single year, about 15 petabytes of data each year. I like showing this old slide because it's very old. You'll note that the stack of data is in CDs instead of DVD's or anything else. And the Concorde was still flying at that moment. So this is more just to emphasize not only the sheer volume of the. The data coming off of the experiments of the LHC, but also to illustrate the amount of time that has actually been sunk into trying to solve these kind of problems by the scientific community at CERN.
00:02:06.872 - 00:03:08.442, Speaker B: But there's always somebody larger. I mean, the LHC is producing many petabytes of data a year, but that is dwarfed by things like Facebook and Google and even the amount of email that's being sent around. One of the challenges, of course, that we face in the scientific community is that we're trying to do much more coherent work with much less manpower and, you know, scientific and effort and funding than these big commercial entities. And that implies that that requires us to really be innovative and creative in our data solutions. So, staying on the Lac experiment just a little bit more, the Atlas experiment is one that my group has been working on for quite some time. We're responsible for the software, the infrastructure, and framework that is used for doing the simulation and analysis. This is a big data project, but like a lot of these experiments, it also has other large, at scale challenges.
00:03:08.442 - 00:04:07.638, Speaker B: There are thousands of people over many, many years that are supposed to be interacting with this software, and the software itself is many millions of lines of code for an experiment like atlas. And those pose their own challenges dealing with that. But I am mostly going to focus on the data challenges today for light sources. So one last slide is just that. In addition to serving the needs of an atlas experiment at the LAC, we also have a lot of work being going on in trying to use these large data producers to do work in neuromorphic computing and machine learning and quantum computing and other things. A very rich body of computing science research that can be done when you have these very large and rich data sets. So, switching gears back to the light sources.
00:04:07.638 - 00:05:24.796, Speaker B: So the basic energy sciences is the office of DOE, which has facilities at many different laboratories, mainly, but not only light sources, but also molecular found are the nano facilities and the neutron sources at Oak Ridge. And these are instruments that are being used by tens of thousands of experimenters, researchers every year to explore almost every area of science and even things like art and culture. And it's an expanding area where they're trying to get a lot more users, scientists coming through right now. I think for the ALS, more than a third of the experimenters that are coming through the facility are first time users, which means that they're not coming in with a priori knowledge of how to do the experiments, much less how to do the computing and to interact with the software and the data volumes. This is a theme I'm sure we're hearing in every talk, are exploding rapidly for the light sources. The reason is that the light sources themselves, the accelerators, are getting brighter with higher luminosity. The detectors are getting faster with higher resolution.
00:05:24.796 - 00:06:14.378, Speaker B: The beam lines are automating. Right now, one of the constraining factors that John alluded to, sort of a different but related reason, is that many of the experiments are done in a way where samples have to be manually put into a beam line, sealed, vacuum sealed, and sort of isolated from the experimenters. And this can just take some time. But some beam lines are actually putting in automated robots, which will change out samples. And so that kind of human induced, injected latency is disappearing. And real time processing, real time, even real time simulation, are emerging as critical capabilities that scientists are looking for when they're going to these facilities. The ALS is a synchrotron light source.
00:06:14.378 - 00:07:28.974, Speaker B: This afternoon, you'll hear about a free electron laser light source. That's a linear accelerator, but this is a circular accelerator with many interaction points and many sort of radiation points of various descriptions. And each of these beam lines of which there are about 40 or so in the ALS has at the end of it, an end station, an instrument that is targeted at a particular type of technique, experimental technique, and usually focuses on a certain sort of category or group of scientific goals. What this means is that we saw the five versus from John this morning. It's always who you talk to, whether they're three, four, or five. But the variety of the beam lines, the data coming off these beam lines, is very heterogeneous. One of the larger challenges for trying to address this as an ensemble problem, we have real spaced data, such as tomography and microscopy, where the image that you actually get off the detector really looks like the sample you've put into the beam.
00:07:28.974 - 00:08:36.104, Speaker B: We've got inverse reciprocal space data sets, such as scattering or diffraction, where it's not possible to directly invert the data back into the structure of the sample itself. But you have to do some calculations, maybe some iterative Monte Carlo techniques. And then we have spectroscopy, which is not usually very data intensive, but it can be very cpu intensive, because to do the inverse calculation, it takes a lot of compute power. And then there are also some hybrids. Psychography is a new technique that's being used a lot of the light sources, and it has both very high volume, but it also has a lot of inverse capabilities, characteristics that need computation. So sort of the worst of the best of both worlds, I should say. So the typical experimental science workflow at these beam, at these light sources in the past is that the scientists would come to the beam line, they would conduct an experiment.
00:08:36.104 - 00:09:54.414, Speaker B: The data would actually be written typically onto some kind of a hard drive or thumb drive or some other hard medium that the person would take back to their institution, do some research at their home institution, publish a paper, and then the data would effectively be lost. Maybe it's not thrown in the trash, but perhaps it's collecting dust on the show, sitting on a Concorde high stack of CDs. So we envisioned something a little more modern and automated, where the beamline user would come to the do the experiment again. But instead of doing everything manually, data transferring, et cetera, there would be a data pipeline going to some kind of a HPC center, a data center, a compute center that could actually do some analysis, prompt analysis, basically in real time, do some comparisons, maybe with simulations or some other sort of way of doing the analysis, and then report back to the user with some web based interface that they could then immediately use to sort of, in real time, inform the next iteration of their data. So of their experiments. So this is the vision that we came up with. And it was.
00:09:54.414 - 00:11:10.214, Speaker B: And so spot basically corresponds to these various areas in the web interfaces as well as the workflow that is executing on the, on the HPC system. And this is. And this was a vision that we started working on. And as soon as we started talking with the scientists, we realized that another constraint that I mentioned in another challenge that I mentioned in the, in the previous slide is that the automated, the real time, the monitoring of in situ, time resolved experiments for both real time QA and for informing this next step in the iteration of your experiments is vitally critical. This is an example of sort of a ceramic matrix with fibers in it that's used, that's being studied to research materials for high temperature, high stress materials and things like jet turbines. This is, you know, inside of a metal container here that you cannot see. The only way you can really see whether or not, as you increase the temperature or change the pressure on the sample, whether or not cracks are forming.
00:11:10.214 - 00:12:42.316, Speaker B: And something interesting has happened is you have to do some kind of reconstruction, you have to do some kind of data analysis before you can actually move on and know whether or not your experiment is finished or whether nothing interesting yet is happening. So the combination of the high volumes and the real time component of it really led us to a sort of a collaboration between the computing research division, my own research, the advanced light source nurse, and Esnet as computer facilities in the molecular foundry, which provide some of the theoretical and algorithmic components. It's a fairly straightforward and by now probably fairly familiar architecture. You take data at the beam line, you generate raw data from the detector and some metadata from either that acquisition system or the beamline or the users themselves. We initially started suitcasing this up into an HDFI file at the beam line, transferring it over across the network. Now, some of the beam lines that we work with actually generate hdfs directly. Instead of our needing to wrap it up in sort of a post data acquisition phase, transfer it across the beam line, it hits Nursk facility, it immediately goes onto the file system, archived to the tape system, and scraped to our metadata database, a MongoDB database that holds provenance and metadata for the experiment.
00:12:42.316 - 00:13:10.320, Speaker B: And then there's an automated process that launches a complex or simple workflow on the machines and automatically executes the sort of the standard amount of analysis that would then be exposed back to the user, both by querying the database or looking at the images directly from the.
00:13:10.352 - 00:13:18.646, Speaker A: Analysis with the real time queues, or do you reserve queues a reserved queue.
00:13:18.710 - 00:14:22.848, Speaker B: We've actually done it three ways and we're doing it now in a combination of two ways. So when we first started this, so this is sort of an LDRD project for those who are not at the laboratory. That's a laboratory directed research and development grant which is largely designed to sort of test out ideas and develop prototypes in order to sort of start a seed project for a longer term project. So when we first started out we were basically reserving compute power at NERSC and so we had some reserved number of nodes that we could launch our jobs on. Then we started using a RabbitMQ based system where in fact we're launching a worker job on a node. That worker job is sort of launched by the conventional nurse queue, which can actually take a long time, you'll see in a little bit. And then third way is the real time cues.
00:14:22.848 - 00:14:47.286, Speaker B: And so we're sort of, we're trying to mix the real time cues and the worker job approach by sort of using the standard cues for feeding these worker jobs. And then if there's any dynamic resource requirements we can immediately increase that with the real time queues following that.
00:14:47.350 - 00:15:05.874, Speaker C: Going to the device, you showed the als with lots of targets off it when it's operating. There's a question about the shared central resources. So when it's operating, how many targets are basically sharing that particle beam?
00:15:06.034 - 00:15:45.246, Speaker B: So let me see if I can find the. Yeah, so this page over the work day or. Well everybody is sharing the same beam inside of the storage ring here. And I mean the, basically what you're, because you're, I mean this is an electron beam inside the storage ring. And what you're pulling off is x rays which are generated by some kind of bremstrawling, some kind of, you know, so there's these, there's bend magnets and there's wigglers and oscillators and whatnot that actually force the beam through sort of an acceleration that emits bramstrolling. And so effectively all of these can be running at the exact same time. Right.
00:15:45.246 - 00:16:08.790, Speaker B: It's a little bit different than what you'll hear this afternoon, I think, from the LCLs where in fact they basically are, they're only dealing with one or two beam lines at a time running off their accelerator here. It's very common for, I don't know what the percentage is, 90%, approaching 100% of the beam lines to be operating at the same time.
00:16:08.982 - 00:16:15.742, Speaker C: So you can get the photons you want to a target from the same electron.
00:16:15.838 - 00:16:48.554, Speaker B: Exactly. Exactly. That's very typical of a synchrotron x ray light source. Go through all my silly animations here. Okay, so I'm going to just, you know, change gears once more a little bit. I'm kind of interspersing the Hep experiences and the form of kind of a backstory for some of the work that we've done on the light sources. So another experiment that I've been involved in for quite a few years now is the Diabe experiment.
00:16:48.554 - 00:17:36.074, Speaker B: It's a neutrino oscillation experiment at the nuclear power plant outside of Hong Kong, about 60, about 55 km northeast of Hong Kong. There are six reactors in this facility, and it's one of the most powerful nuclear power plants in the world, and consequently one of the most intense sources of neutrinos in the world, which is why we were there. And so this is a 50 50 sort of collaboration between the US and China. It was the first major experiment that really had such a. An even distribution between the, or share between the two countries. It's a antineutrino. It's an inverse beta decay reaction where you're detecting, you have an antineutrino coming from the reactor.
00:17:36.074 - 00:18:26.580, Speaker B: It interacts in this detector vessel. It generates a positron, which immediately annihilates because it's antimatter, and it also generates a neutron, which takes some time to thermally equilibrate and release some light. And so the signal of the prompt electron and the positron and the slower neutron at particular energies is the signal that you're doing. But of course, we don't have the power to the compute power at the experimental site to do the analysis. So what we do is we ship all the data from the experiment either directly to Beijing and then subsequently daisy chaining it to Nursk or directly from the site to Nursk. And this is our data flow for. It has been our data flow for the last five and a half years or so.
00:18:26.580 - 00:19:29.056, Speaker B: We've been running pretty steady state for quite a long time. We even have a sort of a manual disk transfer and some alternative paths because we have a lot of network interruptions. We've had network interruptions from earthquakes, from typhoons, and even from ships dragging their anchors across the sub oceanic network cables. But we had this system, which we adopted for the ALS as well, called SpAde. SpADE is a data transfer and orchestration package that we developed for an experiment called ice cube at the South Pole, where the telemetry is even worse than it is from China. And you can see that we have features in the data transfer that are well known. There's a Friday calibration every Friday, so you get a little bit of a different data transfer pattern than the steady state that you see during a typical experimental day.
00:19:29.056 - 00:20:31.040, Speaker B: You also have some network outages and problems with machines, etcetera. Our system automatically recovers from that. And this relay and direct mode that I alluded to earlier, you can even see the effects of that when you actually start using the direct mode, which means you're transferring data to Beijing and to nursk at the same time, rather than daisy chaining it. So this data comes to Nursk, which is our us data center, and it gets automatically processed by something we call the keep up production process, and immediately put into a web data, a science data gateway portal for analysis by the scientists. This is, you know, sort of the inspiration for the spot suite is this automated system that then has a web. Web interface, a web portal interface to the data. The physics for the science for the Dai Bay experiment, like the LHC experiment, is relatively simple compared to the something like a light source.
00:20:31.040 - 00:21:04.700, Speaker B: With a light source, you might be shown. You might be studying grapevines one data. Understand how grapevines respond to drought. You might be looking at a ceramic to sort of understand its thermal and stress properties. You might be looking at the growth of dendrites inside of a lithium ion battery another day. So every day brings you a new scientific challenge. Whereas diabay, it's a neutrino oscillation experiment, and that's pretty much what you're looking at every day, every week, every month, which makes it a lot easier to deal with.
00:21:04.700 - 00:22:00.626, Speaker B: But we took a lot of the same concepts that we applied to Daibay and some other high energy physics experiment and are trying to apply it to, are applying it to the light sources. So, in Daibay, this was a real huge success. We started running in December of 2011. Not a very jolly Xmas for me and some of the other guys on the software side, because, you know, really, the Chinese don't really observe Christmas that well, so we had to kind of live with that. But we got our first theta one three result, the very first measurement of a non zero theta one three in the world, in history. Within two months of data, are able. Even though we're remote and across an ocean, we were able to have full access to the data and complete parity with the Chinese.
00:22:00.626 - 00:22:55.544, Speaker B: We observed antineutrinos. This signal here, within 24 hours of the detector turning on, and we saw the antineutrino deficit, which told us that we actually had a real scientific measurement within seven days and then announcing within just a few months of starting up was, was quite a success. And it's an important measurement. It's one of the 20 or so fundamental properties. Theta one three is one of the 20 or so fundamental properties in the standard model that you cannot deduce theoretically you have to measure experimentally. And so along with things like the Higgs boson mass, it's important to measure those because otherwise the standard model is, is just a bunch of equations with no real predictive power. So just this last month we succeeded in reprocessing all of the five years of data we've taken so far.
00:22:55.544 - 00:23:39.344, Speaker B: This is about 700,000 files and about 700 terabytes of data. We had to adopt some or develop some new technologies. One is very much similar to what we've been doing with the. That I just described for the ALS is a RabbitMQ driven task dispatch method that allows us to send jobs to any machine that we actually have access to, authorization to run on. We had to containerize our application. So Linux containers are of course sort of a. I think of them as a thin virtual machine, but it's in essence a way of sort of wrapping your own application in its own environment.
00:23:39.344 - 00:24:43.584, Speaker B: So it runs in well understood and sort of in an image that you can run basically on any machine that any Linux based machine. So at nurse that's the PDSF cluster, the JGI cluster, the Edison machine and the two versions of the query machine. And then we had the self contained partitioning scheme which bundled together our worker nodes that I mentioned before. Same idea, rollback date, frozen offline database which defines all of our calibrations and conditions and some slave and master processes and monitoring to sort of keep the system going. So this is really a big success. We were able to run in four days on Edison and Corey what would have taken ten weeks of running on the high energy physics class cluster at Nersk. So this was, you know, there was a high concurrency of jobs for this.
00:24:43.584 - 00:25:18.818, Speaker B: I mean typically we would be much. I mean the PDSF ATP cluster is an order of magnitude smaller than this. Even if I had 100% of the system and I can't. So the number of concurrent jobs was very high. It was limited by the amount of ram available on it. And we did notice a big difference between the Haswell and Edison nodes on quarry. So the quarry comes in two flavors and its older flavor is the Haswell intel chips.
00:25:18.818 - 00:26:47.206, Speaker B: And the faster or the newer is the Knights landing, which is a significantly different architecture. But like I say, the end result, the bottom line is in four days we were able to do ten weeks of processing, which I think was very, very gratifying for us and really astonished some of my colleagues. So back to spot suite. So this started out as an LDRD process or project. It's turned into what I call a pseudo production process, largely out of necessity because the scientists at the beam lines found it so useful and sort of vital to their through work that they really demand that it kind of keep going and be up twenty four seven at least up as much as their beam lines are. We started with a number of technologies from high energy physics and the ALS, but also and with sort of a select number of beam lines, the microtramography beam line run by Doula Parkinson's saxwax scattering beam line that Alexander Hexamer is in charge of and the micro diffraction beam line that Tamura is in charge of. We've worked on multiple facilities, although ALS is our main one, and we're partnering with a number of Oscar projects as well.
00:26:47.206 - 00:28:24.050, Speaker B: So pseudo production running means we're running 24/7 really 24 six, because Mondays is usually time off since it's been turned on. Up until yesterday, probably 05:00 when I did the query against our database, we had over 420 data sets stored, a little over 3.4 petabytes of data on HPS and disk. And we've executed 6 million jobs automated on Nursk we have a web portal interface just like we did for the Verdiah Bay, that has interfaces for data monitoring and control, job monitoring, job introspection and inspection, data browsing and even high quality remote visualization of datasets. And this is all backed by some of the metadata and provenance information as well as the workflow. So just as a sort of an architectural sort of representation of what this system is, there's the web portal interface which is on top of certain presentation templates that we define for different types of data. It's true that you have the real space and the inverse space kind of data sets, but you also have, you know, a lot of commonality where in some sense they're just images, you can treat them a lot the same way we have the metadata database and provenance database in MongoDB.
00:28:24.050 - 00:29:18.814, Speaker B: I'll speak a little bit about that later. And we have this task queue and workflow management system which includes the RabbitMQ model and real time queue model that I mentioned before. Data management and resource matching is basically how we get the, the data from the beam line to the facility and then have it manage the catalog and warehouse and move things off onto hpss for curation. We have the API that is based on the Nursk web tool interface. It's a restful interface that Nursk has developed and we've extended. And then under that, there's a lot of sort of APIs that nurse can, theoretically, at other compute sites. So, in fact, we've run at OLCF.
00:29:18.814 - 00:30:19.180, Speaker B: There's nothing there that's sort of long term and persistent. And underneath that are the actual algorithms that people want to execute on their data that we can kind of compose into a relatively sophisticated workflow. So this is a spot architecture, and sort of the Nursk architecture involves networking, of course, data storage devices, both disk and tape, the compute facilities, and then a lot of sort of ancillary systems and services, such as data transfer nodes and science data gateways and database servers and whatnot. And we basically use in spot everything that nurse provides. We use the. I can't spell, obviously. We use the compute resources, obviously, both the normal and real time queues.
00:30:19.180 - 00:31:13.144, Speaker B: We use both the global file system and the HPSS tape storage system. We use the Newt API, the rest API that nurse provides, along with an API that we've sort of developed as an extension to newt. We're using database services that nurse provides and maintains and that we actually design and populate and the science data gateway interfaces, which is basically the web portal that scientists use to get to the data. And I think that one of the important things we learned is maybe we knew it ahead of time intuitively going into this, is that you really can't do this kind of a big, integrated, comprehensive system other than at a facility like nurse, because you really need a little bit of everything, or a lot of everything, in our case.
00:31:13.804 - 00:31:15.904, Speaker A: How big is the Mongo database?
00:31:16.644 - 00:31:44.998, Speaker B: I should know. It's terabytes. I can't remember. But there are collections in there that are in the many millions of entries. I mentioned that there were something like six plus million jobs that have been launched, and each of those jobs have sort of a start stop entry in the collection. So there's a dozen million or something.
00:31:45.126 - 00:31:47.598, Speaker A: Is it just running on one node or is it.
00:31:47.646 - 00:32:41.024, Speaker B: It is just running on one node. I don't mention this anywhere else in the talk, so I'll just take a moment. So Nursk has been talking about the spin system, which is, I think, inspired by the cloud resources that Amazon and Google and others are sort of developing. And we will probably want to move our database, Ana RabbitMQ, and most likely the science data gateway out to those spin interfaces. That's the first step for moving those not just from a physical Nursk server to a virtual Nursk server, but also potentially spinning those out to the cloud, to the Amazon Web services or something like that. Nursk is a great place to do it. Like I say, you can't do this sort of thing without all these capabilities.
00:32:41.024 - 00:33:31.464, Speaker B: One problem with using this is that Nursk has downtimes just like the ALS has downtime. And those downtimes aren't necessarily, well, they're not at all coordinated. So what that means is that if on Monday the ALS is down, but the nurse goes up, and then on Thursday nurse goes down by the ALS is up, you've got two down days for the, you know, what I call the super facility, the combination of the two facilities. So the downtime for a super facility is in essence the union of the downtime of all the individual components in that, in that super facility. So this is the kind of resources that we draw upon at NERSC. One of our early successes. I mentioned the automated beam lines earlier in the talk.
00:33:31.464 - 00:34:23.184, Speaker B: This is a beam line that. This is the scattering beam line at 733 at the ALS. This was the very first run of their robot. They'd just gotten it in. This robot is going fairly slow because the safety people of the ALS won't actually let the people run it at full speed, so for fear of safety problems, but it still eliminates the human latency in the system. One of our colleagues, Alessandro Cepe, was actually on a train from Zurich to Zurich, rather, while he was doing his experiment. He was able to, on his smartphone, just access the spot website and say whether or not the data that they were getting was good, whether they needed to go back and redo an exposure or whatnot.
00:34:23.184 - 00:35:43.390, Speaker B: And he was really very effusive in his praise. He said it was actually not any different than being at the beam line at the ALS, which is exactly what we had aimed for, and which just tells us that remote experiments are now a reality at using these kind of systems at the ALS. So let me just take a moment to describe this term super facility, which actually I coined, not Kathy, but the idea here is that it's a seamless integration of two or more user facilities. So the DOE spends something like 50% of our, of its budget every year on facilities. And those range everything from tokamaks that John described earlier to accelerators and high energy physics or nuclear science, ion sources or compute facilities, et cetera. So the idea really is to meld a data producing facility with a data consuming facility in a way that is seamless and sort of hidden to the end users, the end scientists. So scientists used to come to the light sources, and they would get light, they would get x ray photons out of the accelerator.
00:35:43.390 - 00:36:32.524, Speaker B: They had to deal with that however they wanted to. They bring their own instruments and their own samples and their, you know, whatever data analytics they had, though they wouldn't have used that term at the time. Nowadays, all the light sources, in essence, produce kind of standardized data for the users. You know, TIFF images for some beam lines, HDF five images for other beam lines, or HDF five files for other beam lines, other formats as well. Hence the heterogeneity that I mentioned in my one slide. But I think they. But they still have to deal with these data in a somewhat bespoke manner, in the sense that they take the data back to their home institutions.
00:36:32.524 - 00:37:26.806, Speaker B: They might have the expertise, they might have the computing barrier, they might have the postdocs that are sort of familiar with both the technique and the data analysis to do the work, or they might not. What we really want to aim for with a super facility concept is that they really all the, all the processing and analytics that can be done on the fly as the data being taken are done. So that, in essence, the scientists, the researchers are going away from their experimental experiment not just with a disc full of images, but with real knowledge about what they're an insight to the science that they're trying to do. So, from photons to data to real knowledge, is the evolution of the super facility concept. So, to test this, we did a number of demos some time ago. I'll describe one of them. So this is organic.
00:37:26.806 - 00:38:35.926, Speaker B: Photovoltaics are sort of an interesting technology. They are the kind of photovoltaic materials that could be printed on a printer at very high speed. They can be printed onto flexible substrates, etcetera. And they're still a little bit early in the process, in the sense that their efficiency is not as high as, as other types of technologies, but they're very interesting for the kind of future devices that people are wanting to make with the photovoltaics. One of the problems with studying these in the laboratory is that when we actually, when the original technique was to use what's called spin casting, it's basically you have a substrate, the spinning very fast, you drop some liquid photovoltaics on the spinning disk, it would sort of spread out by centrifugal force. And that was the sample that you would study in your beam line. The problem with that is that's not the way that manufacturing actually happens.
00:38:35.926 - 00:39:52.104, Speaker B: The manufacturing happens with some kind of printing mechanism. And it turned out that the printed photovoltaics and the spincast photovoltaics, it just exhibited different characteristics. So studying the spin cast was not really applicable to the end result of what you were trying to understand. So at the 7.33 beam line, this is a saxwax beam line, is a very typical looking beam line, where you have the x ray beam coming in and hitting a sample, and then x ray detector here at the end of the. At the end of the instrument of the beam line. And so the idea was that we would do the data collection, transfer the data to Nursk in this demo, do some on the fly calibration and visualization via the web portal, then transfer it to Oak Ridge, Oak Ridge in Tennessee, do some real time simulation and analysis of the, the data, extract the structural model of the underlying scientific experiment, underlying scientific sample, and then feedback through the web portal to the scientists so that they could actually go to the next.
00:39:54.444 - 00:40:01.396, Speaker A: Comment on how long they're typically taking data for on an individual object.
00:40:01.500 - 00:40:49.944, Speaker B: Right. So it depends very much on the experiment. So for this particular experiment, you'll see some data in a moment that's only like ten minutes or something per sample. So, in essence, that's dictated by. It's a bit constrained by the capabilities of the beamline data acquisition system, but it's also dictated by the drying time of the underlying. Of the solution that you're trying to study in terms of the organic photovoltaics. The tomography folks, likewise have a spinning, rotating sample, and you take many different projections at thousands of angles, potentially more.
00:40:49.944 - 00:41:45.374, Speaker B: And those are, again, limited a little bit by the data acquisition system. And the capabilities of that are largely sort of constrained by the amount of energy you want to dump into the system and the rotating speed of the turntable. So again, for them, ten minutes, 15 minutes for a single sample is common. Now, there are other types of. But then you might actually have an experiment, which is several exposures of a particular sample. Then you change the condition of the sample, you know, put some stress on it, put some flow through it, whatever, and then do the same thing over and over again. These in situ, time resolved experiments where you're actually looking for an evolution of the system to some known or unknown state or development of a feature or.
00:41:45.374 - 00:42:19.694, Speaker B: Yeah. That you're trying to study, basically. So, John, mentioned the near real time idea of the fusion analysis. We have the same near real time sort of concept where that real time is kind of, you know, minute. That near real time is minutes between one exposure, one experiment and the next. But when you ask the scientists how fast do you want your results out? They say seconds. We haven't got there yet by any stretch.
00:42:19.694 - 00:43:14.750, Speaker B: But you give them, you know, you change their numbers from, you know, months or weeks to, you know, days and hours, and then they want minutes and next seconds. So it's the nature of success. So, in any case, this experiment, this demo was done, like I said, taking data, the ALS, transferring it to nurse, transferring some subset of it to Oak Ridge for the GPU cluster, doing some calculations, transferring information back to Nursk and displaying it via the web portal. There's a lot of things that we had to do to sort of make that possible. And the experiment, like I said, wasn't spincasting, but we actually used a slot die printer to print it onto, print the ov, the solution onto a silicon strip. The x ray data would produce. I have to do a little trickery to get this movie to work.
00:43:14.750 - 00:44:00.434, Speaker B: There it goes. And you'd get this as a time resolved, or the time evolution of features in the scattering. And you'll see that. You'll see a couple things here. One is that there's only a certain level period of time that the information is actually, the pattern is changing, and it's mostly constrained to a single, a single place, spatial sort of region of interest in the scattering pattern. And this is transferred, and the simulation, the sort of reverse Monte Carlo approach of doing data analysis, was executed to get results on the Oak Ridge machine. Turned out that this is much, much slower than it needed to be for the real time system.
00:44:00.434 - 00:45:18.274, Speaker B: But it was a good demonstration of the, of the overall principles. And one of the things that was slow is that if you transfer the entire data set over and then do the analysis on the entire data set, it's not so much the data transfer that's the bottleneck, but rather you just end up doing a much, much more intense reverse Monte Carlo simulation analysis than if you narrow it in on the region of interest where things are changing and when things are changing. So in this deduce project that I'm on, we've been trying to look at that, and the original idea was, over time, things are stable until something changes and they're stable again. You saw that in the diagram, the animation I just showed of real data. And there's only so much of the scattering angle, which is actually interest in this scattering angle or in this scattering pattern. You can actually see the shape of that slot die printer here as a shadow of the x rays. But the real work, the real information is, the real data of interest is down here in the corner at the very right in line with the beam.
00:45:18.274 - 00:46:27.944, Speaker B: So the idea for this deduce was to sort of try and find that something, some way of measuring this period, this phase transition period, and constrain it down to a region of interest both in time and in space. And indeed, we've done some work on that and done some processing, which is very simple and very fast. We can do it at the beam line. As you see, it looks almost exactly like the fake curve that I imagined it would. And we are in the process of trying to integrate that with the front end of the data pipeline so that we can constrain or prioritize at least only the data which is of interest, instead of the entire data set. That will actually reduce both the. I mean, if we're willing to throw away data which we aren't in the first phase, it'll reduce data or data network bandwidth required, but it can also, it would also reduce the amount of cpu required to do the analysis.
00:46:27.944 - 00:47:24.210, Speaker B: So I use the word, the term workflows a lot. And this is what we sort of envision one of the ALS workflows being. It's, you know, it's a directed acyclic graph of nodes that correspond to processing components and connections between them. Some of these are parallelized, some of these are sequential. There's, you know, there's a lot of underlying complexity to this to deal with the parallelism in particular. But one of the things we learned early on is that not every single workflow is, needs to be run for the earliest results. So initially we didn't have this last, this bottom line, which is what we call the fast tomopy line.
00:47:24.210 - 00:48:28.560, Speaker B: We were in essence, taking all the data, processing it through tomographic reconstruction, generating images from that, and then displaying those in the web pages. That was actually fairly slow, slower than we needed it to be. And so we take this fastoma PI, which takes a sampling of the data and does a lower fidelity but much faster reconstruction, and that way we can get the results out to the users in much faster time. Indeed, shown here, there's a kind of plot that I generate a lot, which is stages of the processing versus time. The different nodes here are, the different histogram lengths here are just indicative of how long a particular stage executes. Orange is Q wait time and green is execution time. So it used to be that in order for a scientist to really see the first indication of reconstructed data, they had to wait until this stage, which it's way too resolution, low resolution to, to read what that number is.
00:48:28.560 - 00:49:05.478, Speaker B: But it's in the many tens of minutes. And by going to the fast homo PI, we reduced it to two minutes or so once the data actually starts processing, this is a big improvement. It's not the seconds that were being requested, but it's even a little uglier than this. I just wanted to show a couple of things. So this is sort of six different iterations of the workflow for this one beam line. And what you can see is that it started out being relatively simple and slow. We made it more complex and parallelized it, and then had to deal with the Nursk q wait times.
00:49:05.478 - 00:50:02.334, Speaker B: We were talking about the real time versus the RabbitMQ approach to solving those problems. And then finally, you know, got beat a lot of those demons and problems, and then ended up injecting the fastoma PI down here at the bottom. But as you can see, one of the aspects of this is that the analysis actually, although it's stable for a particular period of time, it evolves over the history of the beam line. The other thing that evolves is sort of just the end to end system. We have new hardware coming in, we have new beat network coming in. We have changes in software. So in the EXWP project, we've been doing some change point detection and anomaly detection of the data transfers between the ALS and nurse so that we can understand when things are behaving as they should be and when we might actually have to address a problem.
00:50:02.334 - 00:51:15.954, Speaker B: It is a, you know, in the super facility concept, it is sort of the responsibility of the infrastructure, software and infrastructure, hardware, computing hardware to really be up as often for the same period of time as the accelerator is. So being able to detect these problems and respond to them in real time is one of the critical aspects of making this concept work. You'll see that for this, the big change points are the horizontal blue lines. The sort of model that's a data driven model, statistical model of the, of the data transfer is this red line. And so measuring the real data transfer versus the predicted data transfer is the way we actually detect anomalies. And indeed, here's an anomaly that corresponds to a nurse network performance that drop that we detect in their log files. And we see that in our, our data, even if you don't have the access to the email of problems outages at Nursk.
00:51:15.954 - 00:51:58.514, Speaker B: But it's not just network transfers that need to be monitored. In essence, we had an outage. These are both in September. This is September 7, 2017, and this is 2015, I think. But in September last year, we had an outage where everything just kind of ground to stop, but we couldn't figure it out because the data transfer looked fine. This compute on both ends looked fine. Turns out that the raid controller for the NFS file server that was down at the beam line had been misconfigured in a reboot or something like that.
00:51:58.514 - 00:53:14.232, Speaker B: And so data duplication times from one machine to another machine on NFS basically went from tens of seconds to many hundreds and sometimes thousands of seconds. So diagnosing this problem was effectively a manual process. And we want to get to the point where these sort of anomalies and patterns are sort of flagged for us, even without presupposing that we know about what parts of the end to end pipeline might misbehave. So I'm running out of time. We're going to integrate with the elk stack and elk services at Nursk so that we can actually get both information at Nursk and information from our applications and data pipeline that will allow us to sort of do even machine learning on the data that are stored in the elastic search database. This is a big system that takes already many billions of measurements per day. At Nursk, I've got just a few slides, and then maybe I'd skip over them very fast.
00:53:14.232 - 00:54:15.198, Speaker B: But it's not just a data. The ALS and all these data processing pipelines are really big cpu needs as well. And when you have something like real time access and start talking about the super facility concept, you very quickly start having scientists imagine new ways of using the facility. The one that enumerated here is often the scientists will come in, they want to have simulated their experiment beforehand. They want to analyze it afterhand. But even during the experiment, they would like to have some, what they call a digital twin, which is a simulation that's running concurrently with the data taking. So you can actually understand whether or not the experiment is behaving as you expect it to, if not respond to it, and do some either analysis or tweaking of the experimental setup.
00:54:15.198 - 00:55:04.500, Speaker B: So in closing, I'll just say that the super facility concept is still. That it's a concept. It's gaining a lot of traction at DoE headquarters. I think in large measure because this combination of data source facility and computer facility is really the way to go. Because when the ALS and other light sources began, it was really individual desktop machines that were doing the analysis. The data flow and complexity of the data volumes and rates and the complexity of the calculation have made it have outpassed that technology. And they really need the high performance computing capabilities that something like nurse can bring with that.
00:55:04.500 - 00:55:07.064, Speaker B: I'll stop and ask for questions.
00:55:11.444 - 00:55:55.256, Speaker A: Okay. I actually want to ping you with an idea first. So super facility always been thought of as many places working together to handle the issue. You said, which is my downtime is fine so long as it's your downtime as well. What about the idea of proposing to doe that Argonne Oak Ridge and each devote Iraq to a mini super facility to test some of these ideas out? What would be the minimum amount out to make it useful for, say, the workflows that you're doing?
00:55:55.400 - 00:56:49.306, Speaker B: So this is a good idea. And I would throw in there even something like cloud resources as well, because in the past, the constraint against running on Nursk and on Oak Ridge and on Argonne machines was the heterogeneity of the underlying architectures. Containers help to sort of alleviate that to some degree. Right. But it's still, I think especially for the Argonne blue gene architecture. I don't honestly know if you can run an image on that or not. The Oak Ridge GPU based machine, you could run images on that and you could even utilize something like the cade's resource at Oak Ridge.
00:56:49.306 - 00:57:58.126, Speaker B: I think this is really the right way to think about it though, is that Nursk has these downtimes mostly planned, but also sometimes unplanned. And they don't make any kind of explicit effort to coordinate with the ALS or with the LCLs or some of the other facilities, data source facilities. And I don't think it's practical to expect that because that would then imply that the ALS and LCLs coordinate our downtimes, or, you know, maybe even with diabetes or other facilities and the combinatorics gets too high. The combinatorics on the compute side don't get so high. If you've got three facilities where you can say, oh, well, we're going to, we're going to go down, you know, the last week in February and make sure that the other two facilities have some capability. So I think it's a, it's a good way to go. I have, we have talked about using cloud resources for failover a very small amount of computing, not the entire workflow, but maybe just the prompt reconstruction that I was describing before.
00:57:58.126 - 00:58:22.134, Speaker B: If we could send off a small subset of data to the Amazon Web Services and run the fast Tomo PI on those, you get the fast turnaround reconstruction results, but you just have to wait for the main data facility to come back for the full results or for full access of the user to the data.
00:58:22.954 - 00:58:24.294, Speaker A: Other questions?
00:58:24.714 - 00:58:42.558, Speaker C: So, super facilities, I've heard a lot of people talking about them, but I've never really heard a good definition of what one is. Can you name two or three technologies, features, components that we don't currently have that would make a super facility equity come into existence?
00:58:42.646 - 00:58:49.474, Speaker B: Right? So, yeah, I coined the phrase initially. And it's sort of been.
00:58:51.094 - 00:58:52.334, Speaker C: It's a great name.
00:58:52.494 - 01:00:08.766, Speaker B: It's been sort of, you know, adopted by many people, sometimes meaning different things, right? So in my mind. So the ALS Nursk Esnet is sort of an archetypal sort of super facility where you have the data producer, the data consumer, and the data transfer. Now, Esnet, ALS and Nursk have existed for decades, right? That doesn't mean it's a super facility. What's really required to make those in, you know, those disjoint, discrete Doe facilities into a super facility is the infrastructure, the software infrastructure, that actually, you know, ties these things all together. So we have many of the components of it. We are still struggling with things like federated identities between the different facilities, some way of, some feedback loops so that you can actually understand other than somebody typing in. Monday, February 26 for the downtime for a nurse, some way of sort of doing that in an automated way.
01:00:08.766 - 01:01:19.090, Speaker B: We have a lot of work, as you know, on the exof side for instrumentation of applications and the hardware to sort of do monitoring and understanding of the end to end workflow and whether it's behaving properly or whether it's failing. There's a lot of, I think, common ideas that many people in high energy physics experiments and light sources and genome sequencers and whatnot are utilizing. I put up my spot architecture. You know, it's a bit of an implementation specific architecture. I think that some kind of more abstract reference architecture that is the super facility is really what DOE researchers need to kind of come to some agreement on. There's other reference architectures for things like hierarchical storage systems and whatnot, that if we could, you know, adopt something like that, then people at Argonne and at Berkeley could actually talk about which pieces are corresponding to which other pieces. Right now the boundaries are not the same.
01:01:19.090 - 01:01:34.042, Speaker B: And so it's often the case that it's impossible to understand whether you want to evaluate something that someone else is utilizing or whether it even matches into your system. That wasn't a very precise answer.
01:01:34.218 - 01:01:47.170, Speaker C: So do you see it? For instance, the facilities like Nursk, providing Amazon Web services type APIs to access and control the capabilities they provide.
01:01:47.282 - 01:02:29.822, Speaker B: Right. So one of the things that nurse did that I thought was really good was this newt. Newt API, which is this restful interface to the underlying resources. And that was good because they did that early. Right? I mean, supercomputing facilities have historically operated on the model of you, a user logging into the machine through some gateway node, obviously into interactive nodes, and then running on it, submitting jobs to the batch queue, and then going away and coming back later to look at the results for these automated systems, these distributed, automated, workflow driven systems. That doesn't work. Right.
01:02:29.822 - 01:03:15.632, Speaker B: I mean, you really need to, you really need something that can, you know, interact with, communicate with your software infrastructure and run on a 24/7 basis. So this brings to mind another one, in addition to things like federated identity and other cybersecurity related issue, which is Nursk is considering multi factor authentication as a. To enhance their cybersecurity status. Enhance or degrade. I'm not sure which is the right verb. But that also is something that has implications for a non human sort of actor interacting with the system. If every single data file I sent across needed a multifactor authentication, the system would grind to a stop.
01:03:15.632 - 01:03:31.384, Speaker B: We've talked a lot with the nurse people, and they are both very aware of and accommodating of this kind of, of this kind of use case. So I'm certain that what we'll end up with is going to suit our needs.
01:03:32.084 - 01:03:41.284, Speaker A: Fingers crossed. All right. With that, let's thank Craig again. Back at 130. Thank you very much. Sure.
