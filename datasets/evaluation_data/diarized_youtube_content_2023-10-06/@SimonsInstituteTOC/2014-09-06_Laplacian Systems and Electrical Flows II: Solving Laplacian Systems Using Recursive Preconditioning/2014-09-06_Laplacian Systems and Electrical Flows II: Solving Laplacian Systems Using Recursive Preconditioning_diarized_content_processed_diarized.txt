00:00:02.640 - 00:00:34.594, Speaker A: Thanks. So, yes, this is the second talk, and what I'm going to do now is talk about how to use some of the machinery from last time and then combine it with a bunch of graph theory in order to actually get fast solvers for Laplace and systems. So. And by the way, just to see where this is situated. So, next, the next one, I'm going to kind of sometimes start over and not use anything from the first two talks and get another approach. But I think that this one. So this one is really strongly connected to last lecture.
00:00:34.594 - 00:00:53.692, Speaker A: If people weren't here for the last one and have questions, feel free to. Or if I just kind of assume things I shouldn't, feel free to, like, wave your hands and ask questions. Yeah. Quick question about the last lecture. Right, the Chebyshev polynomial stuff, is that related to momentum walks? I mean, where. Yeah, that's actually a really good question. The answer is yes.
00:00:53.692 - 00:01:56.326, Speaker A: So, um, the question was, there's these iterative methods in the sort of just straight context optimization world, not linear system solving, but just broadly speaking, I give you some convex function that you want to minimize, and I give you some properties and some promises. And there's this, you know, the gradient descent analysis gives some value that depends on some parameters, Lipschitz constants and strong convexity and whatever else. And then there's this beautiful paper by Nesterov that shows that you can take a square root of something there, and it is actually the same square root for a reason that I think I'll answer offline. Roughly speaking, if you apply Nesterov's method for. This is the momentum things you were talking about, right? Am I answering the right question? So, it turns out if you apply those to minimizing the quadratic function that I wrote down last time, you actually do get the same behavior and you essentially get the Chebyshev polynomials. So, Nesterov's method does give a three term recurrence relation that isn't quite Chebyshev. It's like slightly conservative Chebyshev.
00:01:56.326 - 00:02:34.524, Speaker A: So if you were to take Chebyshev polynomials and just kind of back off the parameters a little, that actually is what you get if you were to work out what Nesterov's method does. So it is. But maybe I'll answer offline exactly how that works. Roughly speaking, Nesterov's analysis, just that sometimes if you look at what it's doing, it's using more or less the fact that a strongly convex function is boundable by a quadratic in one direction. By a line on the other. And the quadratic, the strong convexity constant is exactly the condition number that we're writing down. So it does actually match over.
00:02:34.524 - 00:03:11.362, Speaker A: But maybe I'll answer more about that offline other questions. Okay, so let's get started on this one. So last time I talked mainly about generality, sort of every. I think everything was before 1970 that came up in last lecture. Today we're going to jump forward to the two thousands, and I'm going to specialize the methods from last time, which were almost entirely for general systems, to solving Laplacians. And there's actually a very long history on this that I can't do justice to. And so what I'm going to do is just give a super short summary of it, just to say where what I'm talking about is coming from.
00:03:11.362 - 00:03:57.144, Speaker A: And then I'm going to post some links just so that you actually have the full versions of what I'm saying, and so that I don't, I don't know, inadvertently slight dozens of people. And so I'll. Or, yeah, so I'll post some links online and I'll find out from the organizers how to link to my links. Okay, so the very short list of what's happened in the past on flossy and systems was, you know, there's a lot of work on solving various subclasses because they come up a lot in practice, but the ones that have come up a lot in certain practical settings have certain special structure. You know, they came from triangulations of low dimensional objects. And so there were a lot of special cases that have been solved very efficiently. In particular, if you hear the word multigrade, it's almost entirely referring to solving certain classes of laplacian systems.
00:03:57.144 - 00:04:39.594, Speaker A: And then, of course, you could do general direct solvers like ASCII delamination or Strassen's algorithm, or whatever else you want. There's general iterative solvers, which we talked about yesterday. And then for graph Laplacians, that was sort of where the numerical analysis toc kind of line got crossed a bit. And in the theory of computing community recently, there's been a long line of work that's led to nearly linear time algorithms based on recursive preconditioning. And that's what I'll talk about today. This line of work was, to my knowledge, started by Vidya, who put in place sort of the general structure of using the combinatorial properties of a graph to construct preconditioners. And this was continued by a lot of people in this room and others.
00:04:39.594 - 00:05:40.654, Speaker A: And it led eventually to the first nearly linear time algorithm by Dan Spielman and Xiang Wa Tang, which really started, I think, a recent spate of papers and that built on a whole string of intermediate papers that I'll post some links for. And then this talk, I'm going to talk about a paper that followed it, which was by Gary Miller, Yiannis Kutis and Richard Peng, which very much sped up the linear algorithm, the nearly linear algorithm, and also vastly simplified it. It was based on some of the same ideas, but it included some very beautiful ways of redoing a good chunk of the structure to really simplify what was going on and lead to an algorithm that actually will work on a real computer. So, you know, the stillman Tang algorithm, they were really shooting for the theoretical result. We didn't try to optimize the numbers of logs and constants, but they were large. You know, it was 30 ish log to 30, and constants that were very similarly large. I'm sure that their method could have been pushed down a little bit if they really tried to optimize the number of logs, but it wouldn't have been a small number.
00:05:40.654 - 00:06:18.054, Speaker A: And then this has two, and then Gary, Richard and Giannis had one, and now more recently, they've gotten it down to a square root of, to a half of one. So, okay, so subsequently, this has been sped up further, and I'll post the links to it. The one I picked was one that has, I think, a nice trade off. It's a, a very reasonable number of logs, m log squared, n, and it actually has a very clean global structure that doesn't, it sort of doesn't have a lot of detail on top of the main ideas, you know, it really. So I think that's what I'm picking for today. And then in the next one, I'll talk about some algorithms that were based on a different framework. Okay, so, moving back to math.
00:06:18.054 - 00:07:06.404, Speaker A: So, last time, just to remind you where we were, we said that the key parameter that defined how quickly you can solve a linear system is the condition number, which is the ratio of biggest to smallest eigenvalue. And we gave an iterative method whose iteration bound was proportional to the square root of it times log one over epsilon to get an epsilon approximate solution. And if you. The nice thing about it was it was just doing matrix after multiplies as its main time comp the main time per iteration. So each iteration took o of m time, where m is the number of non zeros and entries in a matrix. So it wasn't n squared, you know, just it was an m, because multiplying by sparse matrix is actually much faster than multiplying by a dense one. And the first question was, is this enough to solve Laplacians in nearly linear time? And I said briefly at the end, no.
00:07:06.404 - 00:07:51.288, Speaker A: Before we get to the sort of real reason why no, let me just mention a little technicality that we should deal with, which is just that. I guess strictly the way I define this, the condition number is infinity, because Laplacians have a kernel, right? We know that the all ones vector is in the kernel of Laplacian. So for a connected graph, you have a kernel that is of dimension equal to the number of connected components. So throughout the rest of everything, I say all graphs will be connected, in which case you have this one dimensional kernel coming from the old vector. And that certainly is not a good running time to take the square root of infinity. This is a really not big deal. The point is we know where the kernel is, and if you look at, and we don't even really need to, as long as we know it's a very simple kernel, it's the all one vector.
00:07:51.288 - 00:08:40.654, Speaker A: Morally, just think we're working on the orthogonal complement of the all ones vector. Everything we did yesterday is just applying polynomials to matrices. So you don't even really need to know where the kernel is. As long as I feed you a right hand side vector that actually has a solvable linear system, you can essentially just think everything we're doing is just working on the complement of the all one vector, and it doesn't send any real issues. So really you should think Lambda two is the thing that's going to govern the min eigenvalue. Now note that we already have a result that's not super obvious, which is that we have a, you can solve the linear system very efficiently on expanders, right? And the reason for that is that each iteration takes linear time. And if you look at what is an expander? An expander is something where the ratio of its biggest to smallest eigenvalue is a constant, right? So we've now actually argued that if I were to just try to solve.
00:08:40.654 - 00:09:18.544, Speaker A: I'll remind you a little bit about definitions of epoxians on the next slide. Very briefly, this is just sort of the high level culture. But note that for expanders the condition number is constant, and so the number of iterations is just something like log one over epsilon. By the way, a quick terminology thing. I'm going to say today, if I ever say nearly linear or put a tilde over something, it means I'm neglecting log factors. And also I'll sometimes just write poly log when it might be one log or two or something, just because I don't want to get, I'm not really too concerned with how many there are, and I think it will cloud the picture. So I'm going to be a little imprecise with some of these things today.
00:09:18.544 - 00:09:49.970, Speaker A: They shouldn't. Hopefully it won't cause confusion. If it does, just stop me. Okay, so this says that you can solve expanders very quickly. And the real problem is that the opposite of an expander, a graph that has very small lambda two, is going to immediately break this, right? So if I give you lambda two, very small. If I give you a graph with sparse cuts, something's going to go wrong. And so I guess the exemplif thing that really exemplifies this is paths, where I said yesterday, the condition number is n squared.
00:09:49.970 - 00:10:37.842, Speaker A: Right. The smallest eigenvalue is one over n squared. And that means that we have n iterations because it's the square root of that. And that's a really not good algorithm for what we're going to do. Any questions on that? Okay, so now the first question is maybe we can just improve our analysis of something or construct better polynomials, because remember, you know, the path is structured, and I don't know if I gave you a well structured instance, maybe instead of using Chebyshev polynomials, you should do something else and pick a polynomial that does something good and you strongly can't. So I actually claim that no straight iterative method of the form I talked about yesterday has any hope of solving that, and it's for a very, very simple reason. The simple reason is just, if you look at what we're doing, every step, we're multiplying by the matrix.
00:10:37.842 - 00:11:04.776, Speaker A: And, you know, the Laplacian is just the identity plus the adjacency matrix. And so after k iterations, we're looking at a polynomial of degree k in the Laplacian. And you'll note that that only knows about paths of length k, right? So just let's talk adjacency matrix for a second. Each time you multiply by a, you get another, you get paths that are one longer. So if you look at a to the hundred, you only get paths of length 100. Right. And so in particular, any polynomial I ever hope to write down.
00:11:04.776 - 00:11:40.280, Speaker A: If I give you a path of length n, it needs n iterations. You know, to look at polynomials of degree n, just to get anything from the left side of the path to the right. So really there's just no hope of making better polynomials because you have to somehow, you know the answer. If I put something, one end of the path of the other is actually supported on the whole path. And so you really just need to, somewhere along the line, write a polynomial that can touch paths of length n. So you just need degree n no matter how well you did. Yeah.
00:11:40.280 - 00:12:17.590, Speaker A: When we say path, do we mean generally graphs with high diameter? Yeah. So this was one example, but in particular, anything with a diameter, you always need at least the diameter. Yes. So this means that we can't just hope to abstractly fix all these problems without talking about maths a bit. Now, you might notice that paths are not very hard. The reason they're not very hard is they're very easy linear systems, and you can just do them by hand, by direction. And what we'll see today is that sort of the motivating picture, which doesn't, it kind of gets hidden in the cleaner analyses, and I think it's more in the motivations Gilman Tang had.
00:12:17.590 - 00:12:51.958, Speaker A: Thanks. Somehow the more recent ones have managed to get around it. But I think just a nice high level picture to keep in mind is that pads are easy because you can do them by hand. Expanders are easy because they're very well conditioned. And a variety of graph decomposition things that have appeared in the last couple decades will say that in one way or another, all graphs can be thought of as being sort of a bunch of expanders glued together in a tree and recurse somehow. You can partition graphs into expanders and then collapse them and partition and collapse. Morally, you can think of all graphs as in some sense being some combination of trees and expanders.
00:12:51.958 - 00:13:11.334, Speaker A: And hopefully you can combine those two I'll make. Now, it's just good to keep that in mind, because it's going to get hidden in what I'm about to say. But it's a good design paradigm to have it in mind for a lot of fast algorithms. Yeah. Confused with the diameter. So for an expander, you show that constant number of iterations are enough, though the diameter is log n. Oh, sorry.
00:13:11.334 - 00:13:27.784, Speaker A: But you need to get an error of one over n squared or something to get an answer. The condition number is constant. The epsilon that you need to get a good solution should be like poly one over poly. And so. Yeah, that's a good point. Thanks. Other questions.
00:13:27.784 - 00:14:18.134, Speaker A: And in fact, you could get a constant error with a very low power. It just won't be popular. Okay, so, and just kind of more generally, this maybe is this jumping off point for some of this recent line of work was that it's not just that lines are bad and expanders are good, it's that there's really a very strong connection between the conditioning of these Laplacians and the cut structure. Luca's talks didn't just show Uf cheeks inequality that says cuts are directly boundable in terms of the condition number of these things and vice versa. But it's actually more than that. It's that the structure of the eigenvalues, how many big ones, how many small ones, how big they are, is very strongly related to all of the cuts in the graph. Somehow, each cut in the graph presents one bad eigenvalue or something like that.
00:14:18.134 - 00:14:49.846, Speaker A: And once you. And when you. And in order to sort of understand the linear algebraic structure, you have to understand where the sparse cuts are in a graph, and that's kind of the main roadblock that these methods have to get around. Okay, so now we're now stuck with, I guess, an assertion that there's something good that can happen and then a promise that nothing. I told you last time, guess so. What I want to talk about now is just a general method, my last general method before I get to Laplacian, which is preconditioning. That's going to be how you can hope to solve the line even, or solve things where, you know, they have bad condition number.
00:14:49.846 - 00:15:32.768, Speaker A: And, you know, just broadly, the question is, what do you do to solve things where Kappa is big, where it's badly conditioned and it's not totally hopeless? You know, here I wrote down a linear system with Kappa is a million. That one iterative method will in fact take a very long time on, but especially because if you want to solve A Symmetric Linear System, you even square it. But the good news is that I actually can look at this. I can kind of Eyeball it and say the reason it's badly conditioned is just because this DiaGonal is really badly behaved. If I just do d inverse times it, I can now look at it and get something that's actually really well conditioned. So morally, if I understand why something is badly conditioned, I can hope to fix it. And so you'd think that here, modules and caveats.
00:15:32.768 - 00:16:09.466, Speaker A: I'll tell you in the next slide, the fact that I was able to look at this Eyeball and do a Simple Transformation to make it well conditioned said that. Now, instead of just solving something with a I'll just solve something with d inverse a. Now I've got a nice well conditioned system. So more generally, preconditioning just says I want to solve a poorly conditioned system. And suppose I know a positive definite matrix M, that's very similar algebraically. So something where m inverse a is close to the identity in some appropriate SeNSe, but where I'm able to, one way or another, easily invert this approximate matrix. So I have a Simpler matrix.
00:16:09.466 - 00:16:56.378, Speaker A: On the previous one, it was a diagonal matrix that has some algebraic similarity to the one that I want to solve, but is nice. It's one that I can invert by a simpler method, and then you can just instead solve M inverse A equals M inverse B. That might be right this time. And the, that's actually maybe a major Win, like, and here morally, we're going to say we understand graphs by where their cuts are, and this is going to let us understand how to kind of make something that gets rid of the bad eigenvalues. So, okay, there's a Couple complications that I'm going to just very quickly mention. And by the way, today is going to be sketchier than last lecture in the sense that I'm going to start as the lecture wears on more and more aggressively omitting details. This one's a mild omission, but when we get to the solvers, they really are quite complicated.
00:16:56.378 - 00:17:27.194, Speaker A: The Spielman Tang line, I think, is something like 150 pages if you concatenate the contents. The one I'm talking about today, if you put the numerical analysis in, I think, still is in the 60 to 70 range. So I'm going to start omitting details, but I'm going to hope to convey the picture. So, ok, main questions. One is where do we get this preconditioner? The other is that you don't really want to compute an inverse a because that's complicated. Right? Again, we don't want to do full inversions. And the third one is that I told you everything for symmetric matrices, and I made probably a not symmetric matrix.
00:17:27.194 - 00:18:05.328, Speaker A: So the short answers are where do we get m is? I have no idea. So in general, it's a hard question and it's a problem specific question, and this has no general answer. You should think of this as saying a preconditioner is when you understand the structure of your problem, you can hope to use it. So there's not, this is a non trivial question and one that will instantiate for specific classes the expensive to compute M inverse, a, same thing we did yesterday. Don't compute M inverse a. We're only doing matrix vector multiplies. So you have to be able to multiply by M inverse and a, which amounts to solving linear systems, not inverting matrices.
00:18:05.328 - 00:18:36.518, Speaker A: And the non symmetric one. It's really not a huge deal. And let me tell you roughly why it's not a huge deal. Roughly why it's not a huge deal is that it's not a symmetric matrix, but it's pretty close. And the reason is that you can define a symmetric matrix. So the idea is that if I take M to the negative one, if I assume M is PSD, I can take a square root, I can make M to the negative one two. And then note that by similarity M to the negative one a and M to the negative one, two a, M to the negative half have the same eigenvalues.
00:18:36.518 - 00:19:32.934, Speaker A: And if I multiply a string of these, if I apply a polynomial to M inverse a, or the other one n to the negative one two a, M to the negative one two, they're pretty much the same. The same stuff happens in the middle, and it's just a conjugation on the beginning and end. And so morally, everything we did yesterday should work fine under the appropriate change of variables. And a little more precisely, if you look at whatever polynomial we did last time applied to m inverse a times m inverse b, and just if you apply the same polynomial in the analysis we did gives the same kind of bound, it gives a bound with QK applied to Lambda I on the A norm. And the only crucial thing to note is that actually when you write it out, we are using that, we're using the a norm here. So when you write things out, you'll see that the fact that I can kind of start playing around with M's and A's and commuting them in various ways, when you write this out, you'll see that it gives a reasonable bound in the a norm, not a reasonable bound in the l two norm. And this is one more reason why the A norm is the right thing to work with.
00:19:32.934 - 00:20:10.324, Speaker A: Okay, so short version was, don't worry about it, not a huge deal. Rewrite the analysis from yesterday, essentially verbatim, and everything goes through. And really, it's because m inverse a is only barely not symmetric. We know it has nice eigenvalues. We know it's related to a symmetric matrix, and so we're in good shape. Okay, so now let's get to, I have difficulty with this because, for example, I know that if it is not symmetric, the pronged vector could be exponentially off. Yeah.
00:20:10.324 - 00:20:38.476, Speaker A: And then the convergence would be a huge problem. Right. So the point here is that we actually don't have that. So, you know, you should think that what we're really doing is running the whole algorithm on m to the negative one two, a m to the negative one two, which is symmetrical. And then the eigenvalues of m is to negative a is symmetrically. A is symmetric. Yeah.
00:20:38.476 - 00:20:59.734, Speaker A: Right. So everything. Yeah, a is symmetric and it will depend on the eigenvalues of this, you know, this matrix here, which may. Symmetric, yes, that was all I was. Inverse is. Thanks. Sorry, I should have been more clear.
00:20:59.734 - 00:21:34.074, Speaker A: This is under the assumptions from before. Plus it's under a is symmetric in PD. Okay, thanks. So now where do preconditioners come from? And again, it's really art more than science, because it is problem specific. We're trying to outperform general linear system solving, and the basis for the algorithms that we'll have to graph the classes is going to be to really understand the structure to get a class of preconceptions. So just, I want to give you a very quick reminder. Somebody asked me just to throw in, in case you missed previous talks, what is the Laplacian? I'm going to do this at lightning pace because the last couple days have been about them.
00:21:34.074 - 00:21:51.192, Speaker A: But just to remind and fix notation and remind you, if I give you a graph, it will be a weighted graph. Laplacian matrix is D minus a. So D is the diagonal matrix of weighted degrees. A is the weighted JSON C matrix. L is d minus x. So that was my definition. Looks like this.
00:21:51.192 - 00:22:22.804, Speaker A: Right? So diagonal entries are the weighted degree. You have negative on the non diagonal, you have negative whatever the degree, the weight of the entries. And we'll see over and over that it's often better to think about this as the corresponding quadratic form. Like really for most things that one will do, the more natural object to look at is x transpose Lx. And in that case, it's the sum over all the edges of the weight of the edge times xi minus xj, quantity squared. And that's the one that we'll work with. Okay, so this is what we're going to solve today.
00:22:22.804 - 00:22:38.012, Speaker A: We're solving systems. Lx equals b. These are L's. Okay, I think I'm going to. And so if there's questions about this later, just let me know. But this is fixed notation, what we're working with. In particular note we are not using the normalized Laplacian we're using the regular one.
00:22:38.012 - 00:23:18.582, Speaker A: So the eigenvalues are between zero and something that's like theta of D, not theta of one. We're not scaling everything down. Okay, so let's talk about preconditioners and linear systems. Our goal is to solve all x equals b. And the first question is, why should we hope for this to be easier than solving all linear systems? We're not going to solve all linear systems in linear time. That would be a better talk. And so, one good reason why Laplacians are nice is that they're what's called weakly diagonal dominant, which means that in every row, if you take the diagonal entry so diagonally, diagonally dominant means that it's greater than or equal to the sum of all other entries in the row.
00:23:18.582 - 00:23:51.530, Speaker A: So, flipping back to the slide here, it's actually exactly equal. Right? Like the four here is exactly equal to the sum of those other three numbers. Essentially, by definition here, the weak, just to instantiate the terms weakly, is that it's greater than or equal to, not a strict inequality. And for graphs, it's actually a strict equality. It's like the least diagonally dominant weekly that I put down in the system. And why is that helpful? The reason it's helpful is that it actually tells us where the null space is. So, in particular, we know exactly what the kernel of our matrix is.
00:23:51.530 - 00:24:32.164, Speaker A: And a general principle that I think is good to keep in mind when trying to precondition stuff is what we're trying to do is crudely approximate the inverse of a matrix multiplicatively. In particular, one over zero is a very bad multiplicative approximation. And so if you want to have, excuse me, any hope of solving a linear system or preconditioning a linear system, you really strongly better know where the null space is. And you should do that. Certainly, checking if your answer is infinity should be no, shouldn't be harder than solving a whole linear system. And so, for going. And what's nice about weak diagonally dominance is that you can actually look at it and write down the proof that says that there's no other kernel of your matrix.
00:24:32.164 - 00:25:16.880, Speaker A: To immediately look at it, eyeball it, you know the current. And in particular, we also know that you can buy all the cut stuff we've talked about, eyeball it, and know not only where the kernel is, but where the almost kernel is, where the fan eigenvalues are. And that's related to cuts. I should mention that everything we talk about today can be extended to solve any weakly diagonally dominant system, not just Laplacians. And it's really a straight black box reduction. Negative off diagonals are basically graphs with self loops. So if I just, if I keep my non diagonal entries negative, the only difference between laplacian elite diagonal dominance is that here we have exact equality to, you know, the row exactly summed up the diagonal entry and it could sum up to less than or equal.
00:25:16.880 - 00:25:52.528, Speaker A: That actually just makes the problem easier. You can sort of think self loops and it's a pretty easy reduction. The slightly harder one is if I allow myself to put positive entries in the non diagonal entries. So if I, instead of making negatives on the non diagonals, make arbitrary numbers whose absolute values sum up to something less than diagonal, that there's also a reduction, but it's not completely trivial, involved making two copies of the graph. So it's a simple black box reduction to write down, but it requires some creativity, I think. Goes back to Grimbahn. Is that right? Just Nikhail's talk.
00:25:52.528 - 00:26:18.662, Speaker A: I don't know. Did the kill do the positive offense? No, no, he said, I mean that's the same as the, when he took two graphs and put them together, right? Oh, the two lifts. The what? I don't think that's exactly the same. What was it called where you two copies of a graph and then you start switching the edges? Yeah, I don't think that's exactly the same, but I think it's, it's identical. Yeah. Double cover, right? Is it double, it's just a double cover. Identical to that.
00:26:18.662 - 00:26:27.410, Speaker A: Right. So it's got the right to parsity structure. A little careful because there's no graph. There's no graph. If you put the positive entry, there's not. No positive anything, but. Yeah.
00:26:27.410 - 00:27:11.320, Speaker A: Okay, so Gary, the reduction looks like is a sparsity pattern of a double cover with the appropriate sign. So, okay, so good. Now let's kind of move to the core of this, which is how do you use sparsifiers to precondition? Now Gary, you're right, you're right about the structure. Yeah. So what do you think the possibility of going to positive, positive, semi definitely to all PSD. So all PSD is all matrices, because you can solve ax equals b, you can just multiply by a transpose on both sides and reduce all matrices to PSD matrices. And then, so that would, I think you need more ideas, right? Like I think that there is a strong difference between the two.
00:27:11.320 - 00:27:47.130, Speaker A: I mean that would. What's the strong difference? I mean, in particular, you can't even so I think my guess, the kernel answer is a pretty good one, which is that one major difference with these matrices from general matrices is I can guess whether the matrix is invertible very easily. I can just check if the graph is connected. And for a general PSD matrix, I don't know how to do that. I would say that if you could give me a fast algorithm for just verifying whether a matrix was invertible or very close to an end, then make it a little bit robust. So make it. Then you'd probably end up with a faster algorithm.
00:27:47.130 - 00:28:18.994, Speaker A: But I don't know how to do that for a general matrix. So the diagonal dominance is, that's really, I think, where the win is. It's that you can check where the kernel is very efficiently and understand where the eigenvalues are. But then if you have a positive definite matrix, then it's easy to have a promise that it's positive. Well, so you really don't just need the promise, you need to know why is I guess my assertion that you need to be able to eyeball it and know where the big and small eigenvalues are. But here we know where the cuts are. If I promise you it's positive definite, you really don't know how to exploit that.
00:28:18.994 - 00:28:54.288, Speaker A: If I give you a really good reason for it, then you can hope to design and recognition. So, okay, so here's the jumping off point for what common charts will come in. McKill showed that any graph can be sparse. So he showed that there exists a graph h for any graph g with n poly log n edges, so that you can approximate the graph with pse matrices. And he actually showed o of n. But I'm going to not talk about those because they're not fast enough for what we're going to use. So I'm going to backtrack to the effective resistance sampling and then polylon results, because we're shooting for nearly linear time algorithms.
00:28:54.288 - 00:29:20.894, Speaker A: So the o of n U, as far as fire construction, would take too long for what, what we're going to try to use them for. And by the way, just my notation here, that's less than or equal to, it's going to be the same thing that I think people have used before in this talk, which is PSD. I write a little slightly curved, less than or equal to. It just means that if I subtract them, I get something. PSD. Yeah. So the linear time algorithm only applies to graphics, right.
00:29:20.894 - 00:30:02.544, Speaker A: It's not a generally diagonal dominant thing. Right. So the idea is we're going to solve graph Laplacians and have a reduction to that case. So like the previous couple slides ago, the assertion is that they give you a black box algorithm for Laplacians, then general diagonal dominant matrices can solve a problem of twice that size and solve it. So it's a reduction, but I'm not going to work with that general ones, I'll just reduce that make sense? So I made a black box reduction, and I'm just going to solve the thing I reduced to. I'm just going to solve the Boscians today and then assert that there's a reduction from general to this case. Just a lifting that Nikhail talked about.
00:30:02.544 - 00:30:40.930, Speaker A: All you do is you take your original thing and you view it as a lifting, right? Yeah, you put, it's a two n by two n matrix. You put negative the right hand side and positive the right hand side on the right, and put the positive ones in one block and the negative ones in the. Not too long. Okay, so this is our point. And now what's nice about this is that this, I claim, implies that the sparsifier is a good preconditioner for the original graph. And why is that? So what exactly does this PSD thing do? It says if I were to multiply both sides by lh to the negative one two. And again, a couple more apologies.
00:30:40.930 - 00:31:27.536, Speaker A: I'm going to write negative ones and negative one two instead of pseudo inverses today because I'm completely ignoring the all ones factor. None of these matrices have inverses because they have kernels. But just think, everything I do is on the complement of the all one's vector, and it makes my notation less messy. Okay, so all of the eigenvalues of this, so the lh negative one two lg lh negative one two. If I just multiply that whole thing through one left and right, I get that all the eigenvalues are between one minus epsilon and one plus epsilon. And so in particular, this is really good news, because it says that if I use the sparsifier as a preconditioner, I get a relative, I get a condition number that's one plus or minus epsilon. And so if I were able to handle the sparsifier, then I'd be able to handle the original graph.
00:31:27.536 - 00:31:52.626, Speaker A: So this does almost tell us something. It almost tells us that you can use laplacian linear systems for all graphs as if they're sparse. And all you do is multiply the number of iterations by a log factor that's already a non trivial assertion. Right. It says you can solve dense graphs in a time using a logarithmic number of iterations of a sparse graph. So you've already saved a factor of almost n. Yeah.
00:31:52.626 - 00:32:29.122, Speaker A: So is there a way to see this as removing sparse cuts or something? So, this one is more. You're understanding the sparse cuts by solving the original graph. So the idea is that it's more that the sparse cuts in H and G are basically the same, because it's the promise that a sparsifier gives you is that they have the same cut structure. And so the way you're sort of removing the sparse cuts here is very aggressive. You're doing it by exactly expanding everything about the sparsifier, and it has the same cuts as the original. Does that make sense? Yes. Okay.
00:32:29.122 - 00:32:54.824, Speaker A: So now all we've done is multiply the iteration by a small factor. Of course, you know, we need to find it efficiently. So you started with a matrix that has mostly zeros and minus ones. And the LH could have bigger numbers. Right. They're not going to be that much bigger. So if you start with an unladen craft, they'll still be poly polynomials, and so it's not like you'll lose some logs.
00:32:54.824 - 00:33:25.014, Speaker A: So, you know, when you do these sparsifier constructions, you're never going to make the size of the numbers grow, grow by more than a polynomial factor. So it's not going to ever really change anything inside of numbers. You can probably make the number of logs, I just said somewhere between zero and one. But it's not. The sparsifiers don't blow up the numbers too much, is the point. But. Yeah, you're right, it is a real concern, but not one that the point is just the sparsifiers don't, you don't ever need to use numbers that are much bigger than the ones that you're in the graph.
00:33:25.014 - 00:33:55.760, Speaker A: Also, you can define age efficiently. I think Kheel briefly talked about the older ones to get it. But, like, the nice one that I kind of wanted to talk about is the effective resistance sampling algorithm. And that's what's going to motivate what's in this, the construction video today. The unfortunate thing about effective resistance sampling is that the way that you compute the effective resistance is just by solving a linear system. And so it's really not great to have a linear system solver as a prereq for a linear system. So we can't quite use it.
00:33:55.760 - 00:34:48.493, Speaker A: And that's our first. Like, the way that one does affect the cleanest specification algorithm I know of is you compute effective resistances by solving a linear system, and you can't use that as to solve linear systems. Yes. So then the convention is to say that the preconditioner, like a, if I want that, you say that lh is the preconditioner and you use the ability to solve linear systems in it. So I believe that the way that, so the way you, if I have a and B and a and B are similar to each other, then you have to solve a inverse b type. But you say that a preconditions b and not that a way introduce it is not actually confusing because otherwise it might become damaged. Right.
00:34:48.493 - 00:35:30.092, Speaker A: So the idea is that if I can solve linear systems for LH, then I can use that. Then we know that this has good eigenvalues. So if I were to look at instead of solving, so we know that I can use it as a preconditioner for solving things from the original graph. And then what I need to be able to do is in each iteration, I need to be able to solve the new systems in LH and to multiply by the original graph, because I need to be able to compute the right hand side. Sorry. Each iteration still involves multiplication by LG, right? It does, yeah. But then LG may not be sparked.
00:35:30.092 - 00:36:04.240, Speaker A: So, so you can't ever hope to do better than the number of edges in the original graph because you need to be able to compute residuals. So I guess I should have been a little more careful what I said here. That statement seems to say, you know, you can solve it as if it was sparse. So you can solve it as, you can solve it as if it's sparse in the sense that if I. So this degenerates a bit in the case of a number of iterations where it's a near linear algorithm. Imagine that linear system solving was, you know, required more than a constant or log number of iterations. Then somehow I guess you're right, I should have been a little more careful.
00:36:04.240 - 00:36:41.596, Speaker A: Here it says that you've reduced solving to multiplication by the original matrix than solving a simpler matrix. So, okay, so there is a, so the earlier paper, Stillman Tang paper, constructed. As far as how you're not using effective resistances, but more by hand, I think we feel calculated about that. But instead we're going to do something that for our applications is vastly better, and we're going to kind of dodge the problem entirely. And for that, let me tell you what we'll do. We're going to construct much sparser matrices. So, you know, I said sparsifiers are useful here.
00:36:41.596 - 00:37:31.620, Speaker A: But I guess if you try to unwind what I said, it doesn't really hold one because the sparse bar gets under the edges n log n over epsilon squared. But, you know, you still have to solve a linear system in something with n log n of x on squared edges. If I started with a sparse graph in the first place, I haven't gotten anywhere for a linear solve. Now, what we're going to do here is we're going to say, what's nice about these iterative methods is they actually work pretty well with pretty bad approximations. If I gave you a much cruder, I don't need one plus or minus epsilon, I could get away with log n. And then I just use that log n times and good things happen. So the question we're going to ask is, what if I want to do a much weaker approximation? Can I instead use a much simpler graph? So instead of shooting for n log n over epsilon squared, I'm shooting for something very small, and then hope that I can then wrap that in an iterative method to make up for it.
00:37:31.620 - 00:38:03.494, Speaker A: And what we'll show is that actually you can do this. This is a component of something like this is in the Spielman tang one and in the Kutus milder Peng one, and essentially every preconditioned system that will show up. I'm going to do the Kludis motor Peng construction, which makes a much cleaner software. And so what we're going to show is that you can get a graph with n minus one plus m over k edges, so that the condition number is o tilde of k. So, ok, like k poly, like k. And this they call on the paper incremental parsifi. Now.
00:38:03.494 - 00:38:38.142, Speaker A: So the way to think about this is think, I have to have a spanning tree. If I have a disconnected graph, I'm really in trouble. And this says that I take a spanning tree plus a very small number of edges, m over whatever I want k to b, and then I lose a k in my approximation. So why is this sometimes better? Well, the main points, if I give you a, what's m? M is the original number of edges. So that. Yeah, I guess I was going to apologize for m being not n later and deal with it. But one of the problems, of course, is if your graph is dense, this may be bad.
00:38:38.142 - 00:39:15.074, Speaker A: That was four bullet points down. But, yeah, so I think if you have a sparse graph, this is a big win. If m is dense, you have to deal with it. And why this is sometimes better is imagine m is imagine you're starting with a fairly sparse graph. Then this could be very small. Think of it as could be square root of n. And now the real point here for that is that for a lot of algorithms, in particular, linear system solvers, morally, if I give you a tree plus k edges, solving problems on it is something linear time, and then a graph that's really of size o of k.
00:39:15.074 - 00:40:10.600, Speaker A: So if I were, and I'll say this in more depth in a bit, but you know, morally, if I have a path, I can sort of collapse the path to an edge. And the real core of the problem is kind of encapsulated by the graph I get when I take all these long paths and collapse them. So if I give you a tree plus a small number of edges, you can reduce the problem to a graph to in linear time, to something that really is like a graph whose number of vertices and edges depends more on the number of edges you've added to the spanning tree, not the number of edges, not the whole graph. So this is going to operate like something that's a size m over k, give or take. And then we're going to hope. So what we can hope to do is, for example, imagine thinking of k as root n and n is like n. You can hope to reduce the problem to get a condition number of root n with something that's one plus root n and then minus one plus root n edges, and then collapse that to a size root n problem and hope that something good happens.
00:40:10.600 - 00:40:39.614, Speaker A: So that's the high Level PiCture. You can boost it to much smaller problems. I'll skip the details, but give points. So what I'm gonna do is I'm gonna sort of start with the main, main points, then I'm gonna start iteratively refining them. So the super high level structure is the following. Graph approximations give preconditioners so we can use combinatorics. If I give you a graph with a tree plus a c edges, I can solve linear systems in that object in o of n time to sort of collapse the paths plus linear systems on graphs of size o of c.
00:40:39.614 - 00:41:20.260, Speaker A: I can then use incremental sparsifiers to get preconditioners of the form tree plus c edges. So, and then we'll talk about how to do that in a little bit. And then it's going to be using low stretch standing trees plus resistance sampling. And then the idea is to use this inside a recursively preconditioned solver so I'm not going to do the root n. I just said before, what I'm going to do is I'm going to approximate a graph. With a graph that's like poly log n small, a couple log n's smaller, and then I'm going to shrink it by log n, log n times. So think that I'm going to have like n over log n over log squared nn over log cubed n with different constants than what I just said.
00:41:20.260 - 00:41:50.112, Speaker A: And I'll have log n levels of log n of Recursive preconditioning and put that inside. So then I'll use each level to precondition the one before it. Okay, that's the super high level structure. And now let me fill in a little more about each of those pieces. So again, the really high level point is that I can reduce a graph to a much smaller graph at the expense of having to solve more linear systems. And so what I'll do is I'll, the log n's are, because I did a root n, I'd have to solve too many linear systems. And so this is just to make the recurrences work.
00:41:50.112 - 00:42:39.756, Speaker A: And what we'll do is precondition graph by simpler graph by simpler graph by simpler graph by simpler graph, and then use our iterative methods to buy our way out of the fact that we've simplified our graph. So let me start with how you simplify really sparse systems. So my assertion of tree plus c edges can, you know, is basically a graph of size c plus some linear time stuff, I'm going to kind of draw a picture morally. It's just that leaves and paths are easy. I'm running a little slow, so I'm going to make my picture a little bit of a crude picture, but roughly the idea is if I told you to solve a linear system on, I don't know, something plus some, plus a tree hanging off of it. So I think this is anything arbitrary, and then I have a tree or a path hanging off of it. Then you can, you basically can do back substitution to collapse the leaves up to the base so you can solve it by a direct solver.
00:42:39.756 - 00:43:24.070, Speaker A: More generally, if I were to make these cycles, if I give you, if I give you just any internal path, any sequence of edges, that or all of degree two or one, then you can do what's called partial Trotsky factorization. You can basically, in linear time, figure out how to what will happen on the path and collapse it to a single edge. So just think you can do this by direct solvers. It's a pretty reasonable exercise. You know, it's just that solving things on lines is super easy. You can do it by hand and it's just back substitution so you can kind of collapse the paths down to edges in nearly linear time. Is that reasonable to people? I'm happy to write this out if people want me to, but maybe I shouldn't because I think the rest of it is the more exciting part.
00:43:24.070 - 00:44:04.974, Speaker A: But the point is just that paths are easy and the only way the rest of the graph interacts with a path is through sort of what comes in and what comes out. And so you can collapse the path after an edge and the cost of one linear time computation. So this is what they call paper greedy elimination. And this is what lets us say that actually if you get a tree plus a small number of edges you can make the number of vertices you shrink down, something that's like the number of additional edges you've added. I have a small comment that could explain why this works. The average degree in vertices three is a little less than two. Therefore you must have either long path or many leaves.
00:44:04.974 - 00:44:41.714, Speaker A: Yes, right. Sometimes that's. Oh, I guess I should have said, oh, I should have said two comments. I should have said two comments. One is that you can collapse paths and the other like that I didn't say out loud, but should have was there are a lot of paths, you know, and that actually is exactly the proof, which is that if you look at the average degree, if I give you a tree plus a small number of edges, you have to have a lot of degree, one or two vertices, a factor of two. So this is good news. And now let me give you the high level of preconditioning structure.
00:44:41.714 - 00:45:21.424, Speaker A: You're going to make a chain of graphs, there's going to be two kinds, sort of a's and B's. I think one of them is sparsifying, the other is this collapsing operation. So I'll have a chain of graphs and they'll have the following properties. I'll just say notation wise, Ni and mi are the number of vertices and edges in AI. AI plus one comes from Bi by doing this greedy elimination. So bi is going to be something that's tree plus small number of edges, and AI plus one is going to be the thing you get by collapsing pads and leaves. And then the bi's are going to be a sparsification thing using the incremental sparsifyers I'll describe shortly.
00:45:21.424 - 00:45:50.416, Speaker A: So the bi's are good approximations of the AI's with some parameter kappa that we hope to make some. So the setup is you start with your whole graph. You're then going to get an incremental sparsify. We're going to try to get rid of almost all the edges. We'll get down to tree plus small number of edges. Then you're going to take the tree plus small number of edges and collapse it to a graph of many fewer vertices, but a fairly dense one. Then you're going to sparsify it and you're going to collapse, then you're going to sparsify, then you're going to collapse.
00:45:50.416 - 00:46:30.650, Speaker A: And that's our sequence. And what you're going to want to have happen is that stuff has to fall off fast enough that this doesn't, well, that this actually works quickly. And so what you're going to want is that the number of edges shrinks fairly quickly between different days, so that the process of sparsifying and collapsing does something useful. And what you're going to want is that the ratio of the number of edges in particular. So we're going to talk about edges more than vertices here you want, the number of edges is going to fall off at least at a root kappa rate, where Kappa is this parameter. Um, you know, Kappa is a function of n is the parameter that we want to make small. So you want to say that the sizes shrink very quickly.
00:46:30.650 - 00:47:04.324, Speaker A: And the other, my assertion is that if I give you this, we're actually going to be in really good shape. You can get an algorithm that will solve Laplacians up to an epsilon error in the a norm in time that is like the starting point number, you know, the, sorry, the size of the original graph, the size of the final graph, q. So let me tell you where that was. So the m sub d is the size of the final graph. This is just Duke outsuit elimination at the end. So after you do all this recursive preconditioning stuff, you have to solve your base case. So you pay for that with a n cubed and just make that small.
00:47:04.324 - 00:47:39.602, Speaker A: And then you have a square root of this condition number. Yeah, sorry. So the mi, is that mi as in the number of edges in the original graph, or is that the c from the four? So mi is the number that is in the ith graph. And think a one is your original graph. So m one is the size of your starting graph and md is the size of your last graph. And you're gonna hope for, we're hoping for something linear in the size of the original graph. Think that Md is like log something or not really, but think it's very small.
00:47:39.602 - 00:47:56.364, Speaker A: It's gonna be small enough that this doesn't kill your running time. So you solve the last one by hand. So the first step is really taking your original graph and shrinking it, right. Shrinking all the time. Right. Right. So this is a sequence of graphs that interpolate between your graph and a really simple graph, sort of little by little.
00:47:56.364 - 00:48:33.832, Speaker A: Does Kappa have anything to do with condition number or just a u function? Well, so the, it's going to be. So Kappa will be the condition number of using beta as a preconditioner for alpha, for b is a preconditioner for a, because here it's just a function here. Well, the hope. So here it is a condition I haven't yet said how to solve the recursive preconditioning system. It will be the condition number of the systems you'll solve. And the hope is that you can get it to be small enough that you don't pay for it a lot. So in the next couple bullet points, I'm going to tell you what you do with this chain.
00:48:33.832 - 00:49:15.694, Speaker A: But yeah, it's going to be the condition numbers that show them. And this is the proposed running time. You should think kappa is something involving condition numbers and iteration time. M one is the starting point, mv is the base case, and the log one over epsilon is the log one over epsilon. So the basic idea of how you use this conditioner, this chain to do everything is as follows. The idea is that I can solve even if you know the MDQ, which could only make your life. That's the exact expression that you had when you wrote this expression, right? So the root of the first graph.
00:49:15.694 - 00:49:59.314, Speaker A: So the hope, the root kappa n is not the condition number of the first. So it's, remember, kappa of n is a function. It's not, sorry. It's not the condition number of the Laplacian, it's the kappa of n that appears in my approximation sequence. So then I'm coming back from the previous question, so how should we interpret this function? What does this have to do with condition number? So, okay, so what is polylon? Think of it as polylog n. And it's going to be that we're going to do the incremental sparsification construction to get this is going to be graphic incremental sparsifiers, and we're going to do that so that it's how well b approximates a. So that when I recursively precondition with it.
00:49:59.314 - 00:50:34.522, Speaker A: It's the condition number of the preconditioned system. And so we're going to make this polynomial again. Yeah. It means that in your approximation before, when you were sparsifying, you want that kind of. Right. And so is it the case that for this to happen, you must start with a spouse? So I'll get there, hopefully, yes. As stated, yes.
00:50:34.522 - 00:50:56.784, Speaker A: We need one more idea, which is to make that not happen, I think morally, yes. And one of the main innovations in the paper I'm describing was to not have that be a problem. So I haven't yet. So I'm going to fill the details in iterations for now. That is a legitimate worry, like how we get this chain. I have not yet told you. I just want to say that if I give you this chain, something good happens.
00:50:56.784 - 00:51:36.344, Speaker A: And the idea is, how do you use the existence of the sequence of graphs to solve everything? The idea is that I can solve a system for bi by solving a system for AI using this elimination argument. The b's are these very sparse graphs. And I can solve a system for AI by solving this number of linear systems for bi. Right. So the point was this condition number is how well b approximates a. So how many, how many preconditioned iterations I need to solve to use b to solve a. And so what I'm hoping to do is reduce solving a linear system in a to solving root kappa of animal systems in b, where b is hopefully smaller.
00:51:36.344 - 00:52:15.856, Speaker A: So those are the two assertions I've already given you, and this almost gives the right recurrence. So the idea is that now what's going to happen is that each step is going to reduce a system with m non zeros, mi non zeros to a system with square root of kappa ni. So the number of linear systems that we solve ignore the login and repson for a second, and not for any legitimate reason, just ignore it for a second. The root Kappa n, we're going to reduce each step. So ignoring the log on over epsilon, the number of iterations I need is root kappa n. So I have to solve root kappa n linear systems. And these smaller objects.
00:52:15.856 - 00:52:41.824, Speaker A: And the smaller objects are of size m over root kappa n. So I've reduced solving something of size m to solving things of size m over root kappa n. Copy, you know, that many copies of m over with m over root kappa n entries, and that gives the recurrence root process. So that actually. Right. So the idea of the number of non zero x of b becomes the size of a. And now I've actually gotten something that expands that to the recurrence I've given.
00:52:41.824 - 00:53:23.124, Speaker A: So that almost is good enough. Right? So again, the idea is that I've got these graphs that have fallen off in size very quickly, but I have to do a bunch of them. I reduce solving one system to solving polylog systems that are of size higher up, smaller, and that gives us our recurrence. The reason I've lied to you is this log one over epsilon, which is. I don't want to talk about it right now because I'm running a little slow. But roughly, the point was, is that if you had to pay the log one over epsilon at every stage in this process, we're trying to get, in order for this argument to make sense, we're trying to get Epsilon to be one over poly N. And then you'd get, you'd multiply this by, like, log n to the log n power, which would make a very bad linear system solver.
00:53:23.124 - 00:53:59.098, Speaker A: And the idea is that this is where my yesterday promised that we need to use Chebyshev, the fixed polynomial, instead of conjugate gradient, which is adaptive. It was really for this, which is going to say that actually what we can do is essentially only tag along over Epsilon on the outside of the whole process, because we have a fixed polynomial in the a's and everything is linear. And let me not draw on the numerics there, and I'll come back to it offline if people have questions. But that was where we're using it. It was that, you don't want to have to pay, pay the amplification for the error at every step of this recursive preconditioning, or else you'd get a very bad solid time. Okay, so this is the setup. The setup is the graph showing very quickly.
00:53:59.098 - 00:54:29.102, Speaker A: And so, if we can prove this, we're in really good shape. So now let's see if we can prove it. And the idea is we want to construct these incremental sparsifiers, and we can try to use effective resistance sampling, which, au priori, doesn't really work. There's two real problems. So the first is we can't compute the effective resistances. And the second is, it doesn't give the right answer. So the reason it doesn't give the right answer is exactly the question that you were asking a few minutes ago, which is that graphs don't have to be sparse.
00:54:29.102 - 00:54:59.674, Speaker A: Right. And so if M is big, then m over k is, you know, possibly much bigger. Than n, and this didn't help us at all. So the idea is to deal with, the first issue is that we're going to not use the resistances, we're just going to use a bound up. So Nikhil showed in his talk that if I just give you, if I sample things more than I'm supposed to, it's just as good. If I get some lower bound on the effective resistances, that's fine. I just might, I might be doing overkill, I might be taking more edges than I need, but I'm not breaking anything.
00:54:59.674 - 00:55:36.044, Speaker A: So we're just going to have to bound them upper bounds on the effective resistances. It will just result in oversampling and taking too many things. We have to just make sure we can do that in a way that doesn't cause too many edges. And then the idea here is what we're going to do is use a spanning tree to bound the effective resistances. So let me talk about that and I'll come back to the too many edges. So the idea with oversampling, you know, here, this is just, I think Nikheel actually expanded this out, but the comment I just made, what it really said was that. So again, we're going to use effective resistances, which are one over the weights of the edges.
00:55:36.044 - 00:56:17.728, Speaker A: Nikhil showed that if I sample with probabilities proportional to the taus, then I get a decent approximation. And the setup was that each sample will take independently samples of each edge with probability tau over the total. And the number of samples is what dictates how sparse a graph we get that's dictated by t. And it will work exactly as long if we use lower bounds. But we have to make sure that t isn't too big. If we're too crude in our upper bounds, then we'll just, we could, I could just make them all one, you know, we'll get too many edges. So that's kind of our main goal.
00:56:17.728 - 00:57:01.512, Speaker A: And the idea, rich, I stole some slides from Richard Peng, is that we're going to bound the effective resistances with a spanning tree instead of computing them. Exactly. So we don't want to solve a hole in your system. We're just going to get a crude enough approximation to do things well. And the Rayleigh monotonicity law, it's an effective resistance assertion, which just says that if I take edges out of the graph, if I cut wires, I only make the effective resistance bigger. If I take away options, I can't make the optimization problem have a better answer. And so what we're going to do, and this seems like it's too aggressive, it really does, is we're going to take this whole complicated graph and we're just going to pick a spanning tree and we're going to bound the effect, use that to bound the effect.
00:57:01.512 - 00:57:40.128, Speaker A: We're going to erase all the edges but a tree and use the tree to bound. Then on a tree it's super easy to compute effective resistances. It's just add up the stuff on half and we're going to use that to bound everything. So. Right, because if I give you a path, it's just, you add up the resistances. So now what does this tell us? It tells us the sample probability we're using for edges is just the weight of the original edge times the effective resistance of this long path that I've replaced it with. You know, morally I'm taking an edge and I'm, when I erase an edge, I'm replacing it with this whole path in the tree.
00:57:40.128 - 00:58:06.420, Speaker A: And we'll define the stretch to be the ratio of the original length to the length of the path that I'm trying to replace it with. And that's the probability that shows up in the effective resistance. So I'm starting to more aggressively sketch because I'm running out of time, but I'm happy. So, but let me kind of, I want to get to the main points. So, okay, that's the stretch. The stretch is how big the ratio is between the edge and the path we're routing it along. And entrepreneurial could be very big.
00:58:06.420 - 00:58:36.702, Speaker A: If I give you a cycle, then every edge has stretch n. The trick, ignoring that for a second. The trick is that we're going to use the effective resistances on a tree, but try to choose our tree so that this isn't too bad. And the main combinatorial point here is going to be that not all trees are created equal. So the total number of edges that we're going to end up sampling, if we think about the probabilities that we computed before, is going to be proportional to the total stretch. Right. Edges get proportionally sampled, proportional to their stretch.
00:58:36.702 - 00:59:02.234, Speaker A: So the total number of edges is proportional to the sum of all the stretches. And not all trees are equal. So if I give you a grid, for example, this is a bad Spanning tree. So this tree has total stretch n to the 1.5. And why is that? It's that if I take an edge here, the stretch of this edge is this whole path. And so those are rooted. That's bad.
00:59:02.234 - 00:59:34.500, Speaker A: This is good. This is stretch n, log n. And the main theorem we're going to use is a theorem that says actually every graph has a very good Spanning tree. So the theorem is that given any graph, I can find a graph whose Spanning tree has total stretch and log n. So what does that mean? That. And I can find it very quickly, I can find it in linear time. So what that means is that it says if I pick, if I look at the average for each edge in the graph, you know, morally, an edge could be stretched as much as n.
00:59:34.500 - 01:00:22.740, Speaker A: And in a cycle, one of them has to be. This says that actually I have to get some edges stretched a lot, but I can find a tree. So the average stretch of an edge is just log nice, which is very strong feeling. And if I have time, at the end of my next talk, I'll sketch the construction. It's a very nice ball growing construction plus some work. So that is now going to say that we have this nice, this is where the combinatorics come in, is that, this says that this graph actually gives us a good understanding of the combinatorics because it lets us get a graph where now all these stretches are only, only logarithm owned by the here that tilde is hiding log factors or. Yeah, and it's, this says we can now get a graph where sort of the tree is very well capturing the length of the paths.
01:00:22.740 - 01:00:50.382, Speaker A: So now again, we're back to this too many edges issue. The total stretch is m log n. That's still not n plus a small number. But the idea here is that we're being too precise. So to fix this, we're going to notice that we can tolerate a condition number of k, right? K was the, you know, the number in the. We wanted to get n minus one plus m over k. And we wanted a condition number of k for this.
01:00:50.382 - 01:01:18.246, Speaker A: And we're shooting, you know, we're doing the effective resistance thing, which had one plus or minus epsilon. So we're just being way too aggressive in our goal. So here what we're going to do is we're not going to shoot for one plus or minus epsilon, we're going to shoot for something worse. And the way we're going to do that is we're going to scale the tree up by k. And this was a beautiful idea in this paper. So what it said was why? We're going to start with the tree and we're going to try to add edges to it. The tree is smaller than the graph and so we can just keep it.
01:01:18.246 - 01:01:41.462, Speaker A: But then we're going to get stuck with this total stretch of m log n. What we're going to do is we're already going to try to lose the factor of k. So don't keep the tree, keep k times the tree, which is much bigger. And that already now is covering a lot of the, of a lot of the graph. Like it's really a much bigger object than we started with. And this is going to let the effective resistances go be much better because now the tree is k times larger. So now we'll have the same non tree edges.
01:01:41.462 - 01:02:25.390, Speaker A: But every. But now I claim that the effective resistances are going to fall off by like a factor of k. So the idea is that first note that when I just scaled up a subgraph by a factor of k, my new graph is within, you know, it's bounded below because it's a subgraph and bound it above because I've scaled it by a factor of k. So I've only lost a factor of k in my approximation. But that's it. And now the tree edges have stretch one and the off tree edges are getting their stretches divided by k because I've made the tree edges that you're routing them over k times larger. And so the upshot was that now I've gotten total stretch instead of being m is m over k plus some logs.
01:02:25.390 - 01:03:05.520, Speaker A: The logs are the total stretch. And that's a major win, right? So now this is our big win here was that by scaling stuff up, I've allowed myself to divide all the stretches by k, which makes the number of edges I need go way down, right? Because now the total stretch is like m over k log n. So the number of edges I need to sample is m over k poly log m, which is exactly what we wanted, right? So the condition number of k, we get exactly what we started. We get an approximation ratio of k with n minus one plus m over k. Parallelogram edges. So let me just kind of gesture this a little more before I move on. What was the point? The point was if I give you a subgraph of a graph, it's smaller.
01:03:05.520 - 01:03:32.478, Speaker A: I could get the left hand side type inequality because I just removed edges. If I scale edges up, it makes the graph bigger. But if I scale edges by a factor of 1000, I've only made everything at most 1000 pounds bigger. So when I scale the tree up by a factor of k, I've made the tree. I've at most, you know, gotten a k approximation. But then when I try to sparsify this graph using the tree. Now the total stretch has fallen off by, well, the tree stayed the same.
01:03:32.478 - 01:04:00.264, Speaker A: The tree still stretches itself by one, but each off tree edge now gets divided by the length. Instead of routing it over an edge of say, length one, you're routing over an edge of length k. And so you have a denominator k in each of the stretches. So that lets you divide the number of the probability. So each edge that you sample is an off tree edge gets sampled with a one over k factor. Better lower probability, which makes the number of edges fall off by a number k. And then that exactly gives us what we promised.
01:04:00.264 - 01:04:30.138, Speaker A: So that now actually puts everything together. So now we have exactly gone. So I did not follow this at all. Are you increasing, keeping the number of vertices the same but changing the number of edges? Or are you changing the weights without changing the edges? Or is there resistance? Right. So think of this as two steps. Start with our original graph, original graph that we're trying to spark. We're going to try to construct the, we're trying to construct this incremental sparsifier.
01:04:30.138 - 01:05:06.154, Speaker A: So we're trying to construct a graph with n minus one plus m over k polylog and edges, that is a k approximately of our graph. The way we're going to do that is we're first going to take our graph and make it and blow up some edges. So take the tree and scale them, right. What's the edges mean? So take every, we're going to fix, take a nice low stretch spanning tree and take all of their weights and multiply them by k. Okay, so now we have a new big, yeah, right. So now the resistance over those edges goes down because they're now, so think the stretch, the stretch over. If the stretch was 100 when I rounded it over an edge of length one, that means the path was of length 100.
01:05:06.154 - 01:05:40.266, Speaker A: If I now route over an edge of length 1000, then it's now 100 over 1000. It's now 0.1. So what I've done is I've scaled, I've made a new graph that now has, by blowing up this tree. Now the stretch over all the edges over edges in the tree is much smaller, so the effective resistances are much better. And then if I sample on that graph, you know, all these off tree edges are now getting sampled with one over k times smaller probability. Does that make sense? And so that lets us, that's where this m over k is coming from. Yeah.
01:05:40.266 - 01:06:20.542, Speaker A: So the point is this gives us exactly what we need for a chain of preconditioners with kappa is poly log n. Okay, so this results with all the details I swept under the run to all the numerical analysis and the construction of leisure spanning trees. Putting that all together, you actually, this is exactly what we said we needed for this gives us exactly our change of preconditions, right? It exactly said that we can get with the kappa being. This is the stretch in our spanning trees. And so the upshot was that now when I put that back into my previous recursion theorem, we get the promise of saliva. So again, the high level structure and then the details. High level structure.
01:06:20.542 - 01:06:59.980, Speaker A: Take a graph, eliminate almost all of its edges. So think m all but tree plus m over polylog n edges. I claim that this incremental sparse fire construction lets you do that with a. But your new graph approximates your old one within a polylon k factor. Given that you get this chain of repeatedly doing this, where you have to solve a bunch of linear systems that are much, much smaller. And that is what gives us the recurrence that we're solving. Then, to do that, the main construction was to construct a graph that these incremental sparsifiers, a graph that's almost a tree, but is a pretty good approximation.
01:06:59.980 - 01:07:34.674, Speaker A: The way we did that was we scaled up some edges to make. We had a low stretch spanning tree, which made each, nothing too long, scaled stuff down, scaled the tree up so that now the effective resistances are, are very small. And then did sampling, did just effective resistance sampling to get this very sparse graph. And that was it. And putting that together gives us an M polylogn algorithm. So, okay, that was a little compressed at the end, but actually the full structure of an algorithm. In my next talk, I will give a algorithm that doesn't use any of this, and I can give a full proof for that will work in.
01:07:34.674 - 01:08:17.044, Speaker A: Okay, thanks. I suggest we take other questions offline and we reconvene at 1140. Sorry for going from this over. We started late. All. Yeah, I.
