00:00:06.520 - 00:00:38.398, Speaker A: Hello. It's an honor to be speaking here. Yeah. As Peter mentioned, today's talk is a joint talk with me and Nikhil. It's about sort of a survey on our semester long program in geometry of polynomials. What I'm going to talk about is basically this method called the polynomial paradigm and how to use it in designing and studying algorithms and other stuff in physics and combinatorics. So let me start by going back like 200 years ago.
00:00:38.398 - 00:00:58.034, Speaker A: So let's say we have a univariate polynomial with real coefficients. All polynomials I'm going to talk about today will have real coefficients. So let's say I have this polynomial p. I can look at its root in the complex plane. Here are the roots. I can also look at the roots of the derivative of P. And here are the roots of the derivative of P.
00:00:58.034 - 00:01:36.598, Speaker A: Now, Gauss asked this question that if you look at the convex hull of the roots of p, it includes the roots of p, prime the roots of the derivative of P. So this conjecture was proved by Lucas a few decades later. And basically, we can see this as a history of this. So the thing is, you can see here that there's interplay between doing operations on a polynomial and its roots. Okay? So what we're going to see is generalizations of this in many settings. So you can, in one level, extend this. So let's say you have a univariate polynomial.
00:01:36.598 - 00:02:05.114, Speaker A: You can look at it in three ways. One way is to look at it as a sum, writing the polynomial in terms of the coefficients. If I have a degree n polynomial, I'm going to have n coefficient, n n plus one coefficients. I can look at it as a product. A degree and polynomial will have n different roots. So I can look at the polynomial as product, or I can look at the polynomial just as a function which maps complex number to complex numbers. Now, I have these three representation, or three ways of thinking about the polynomial.
00:02:05.114 - 00:02:54.910, Speaker A: I can ask these questions like, what happens if I change one to the other? If I change coefficients, what happens to the root? Or what happens to the function? And so on? Or you can ask, what happens when I differentiate. If I differentiate the function, what happens to the roots or the zeros? So Garth Lucas theorem was like an example of such a thing. But in general, we are interested in multivariate versions of these questions. When you have multivariate polynomials. Now, it turns out that if you have a multivariate polynomial in general, you cannot. It's not easy to relate these questions or to answer these questions, but the main ingredients that we're going to see a lot in this talk is that when you have multivariate polynomials, when they have nice zero free region, then you can argue about coefficients, roots or function values. So here I show you two examples of such a thing.
00:02:54.910 - 00:03:38.228, Speaker A: In the first example, you see a univariate real rooted polynomial. The roots are all real, so there are no roots in the upper half complex plane in the right. You see another example where we have a bivariate polynomial where there are no roots in the positive orphan. So here are two simple examples, but we're going to see more interesting example as we go into the talk. So what is a polynomial paradigm? It says that assuming you have polynomials with nice zero through regions, you can relate coefficients to functions and zero. So you can study the interplay between these three objects. So this was something that mathematicians have been thinking about for decades.
00:03:38.228 - 00:04:35.512, Speaker A: But what has happened recently over, say, the last 15 years, is that there are many settings in which you have, say, a mathematical object, like a combinatorial object or a probability distribution. And what you can do, you write a generating polynomial out of that object. Then you study the generating polynomial using this sort of polynomial paradigm, you prove theorems or arguments about the generating polynomial, and then you pull back that statement to study the corresponding probability distribution. So the thesis we want to sort of propose here is that, you know, whenever you, you have a discrete object, you want to study its global properties. One way you can do it is that you can encode your object into a polynomial which has nice zero free regions. Use tools from here, and then maybe that gives you a way to study global properties of your object. Okay, so let me put this in context.
00:04:35.512 - 00:05:10.518, Speaker A: We're going to talk about four applications today. So I'm going to talk about the first two, Nikhil will talk about the second, the last two. Uh, so in all these applications, we're dealing with some polynomial with a nice zero free region. In the first application, we see the interaction between, uh, the polynomials and probability distributions. We see that assuming you have nice zero free regions, there are some closure properties for the polynomials. So we'll deal in this case with so called real estate polynomials. That will give you some nice class of probability distributions, and you will see how you can use that to study traveling salesman problem as an application.
00:05:10.518 - 00:05:50.744, Speaker A: In the second application, in the second example, we'll see a different class of polynomials which are log concave in certain parts of the r to the n. We can use that to study spectral gap of random logs. And then we'll see as application that you can study counting problems for matrix, and Nikhil will talk about the caddy cell signal problem applications to study skull physics. Okay, so let me start with the first one. Uh, so, okay, so again, what do I want to talk about? I want to talk about how you can relate polynomials to probabilities and use that to study probability distributions. So let me start with some very simple setting. So let's say this is Berkeley.
00:05:50.744 - 00:06:10.464, Speaker A: It's going to rain tomorrow at probability p one. It's going to rain the day after tomorrow. With probability p two, you can write generating polynomial for each of these events. Z one, p one plus one minus p one. This is the generating polynomial for this. This is the z two. P two plus one minus p two is the generating polynomial for the other one.
00:06:10.464 - 00:06:47.488, Speaker A: So basically, each of these events, or you can think of it as so called Bernoulli random variable, happens with probability b one. So we have two distributions. I can study the joint, yes, the generating polynomial, I'm not going to define formally, but it's basically you. You look at every set in your distribution. You look at the generating monomial for that set, the product of Zi's for the set times the probability of that set. Okay, we're going to see examples as I go forward. So, good.
00:06:47.488 - 00:07:22.342, Speaker A: So now that you have two different polynomials, you can study the joint distribution of these two random variables by treating them as independent random variables. So I can study the joint distribution by taking the product of these two polynomials that I just showed you. So, for example, this would say that the probability that it rains today and tomorrow is p one times p two. Okay, so we have this. Now let's see, how can I generalize this to n. Let's say you have n independent Bernoullis B one up to bn. You can similarly multiply their generating polynomial.
00:07:22.342 - 00:08:14.376, Speaker A: You get such a polynomial, and it turns out that this is a real rooted polynomial, because each of the terms here is a linear polynomial with a real root. So I get the product is also real root. Now, the nice fact is that the converse of this is also true. If you have a univariate polynomial which is real rooted and has non negative coefficients, then there exists independent Bernoulli random variables such that their sum is distributed according to the coefficients. The probability that the sum is equal to k is exactly ak over the sum of the coefficients. Okay, so in particular, it turns out that the prior of the PI for the bi for the Bernoulli is a function of the ith root of the polynomial. Now what you see here is an example of a correspondence between probabilities and polynomials.
00:08:14.376 - 00:08:39.188, Speaker A: Whenever you have rear rooted polynomials correspond to Bernoullis and, like independent Bernoullis, and so, and vice versa. Now you can ask, okay, enough about Bernoullis. Bernoullis are like, these are independent. We know everything about independent distributions. Can we go beyond independence? So let me show you how we can use multivariate polynomials to go beyond independence. So I want to talk about random spanning tree distributions. Let's say we have this graph.
00:08:39.188 - 00:09:10.536, Speaker A: Let me remind you that the spanning tree is a subgraph which is connected and it has no cycles. So here the graph I showed you has four different spanning trees. By removing different edges, you get these four spanning trees. You can study a uniform, say, distribution on these spanning trees. So I pick each of the probability one quarter. Let's say I label the edges as follows, like 12345. Now you can write down the generating polynomial of each spanning tree by taking a monomial corresponding to the edges that appear in that spanning tree.
00:09:10.536 - 00:09:38.854, Speaker A: So here I'm going to have z one times z two, times z three times z five, and I'm going to multiply by one quarter. Because the probability of choosing these three is one four, you sum them over. That's a generating polynomial of a random spanning tree distribution. Okay, good. So now we know how to go from this to a polynomial. Now let's study the operations you can do on probability distributions and see what they correspond to in the polynomial language. So I start with a simple operation called projection.
00:09:38.854 - 00:10:17.506, Speaker A: So what is projection? It's like you zoom in, you let's say I zoom in on the edges adjacent to d, and I forget, I ignore the rest of the edges. Okay? I just want to see what is the projection of my distribution. And the edge is adjacent to d. So what this corresponds to in the polynomial language, if you think about it, it's nothing but substituting one for the edges, not for the edges that I ignore, for the edges that I don't zoom into. Okay? So just substitute with one, and you get this polynomial, substitute one for z one and z two, and you get a generating polynomial of the projected distribution. So here is one operation, let me tell you another operation. Let's say.
00:10:17.506 - 00:10:49.164, Speaker A: Now you want to study the rank sequence, so called probability distribution, which is in this case would be the degree distribution of d. What is the distribution of d? That's the probability that is equal to two, or it is equal to three. So what do I really want? I really want to look at this polynomial. I don't want to treat this as different variables. I just care if like two edges or three edges or four edges adjacent to d is sample. So what that really means is that I really want to symmetrize it. I just want to treat all variables the same, and I just want to look at the power of that variable.
00:10:49.164 - 00:11:16.166, Speaker A: Okay, so this operation is called symmetrization. It gives me this polynomial. And what does this tell me? This tells me that the probability that the degree of this vertex b is equal to two is equal to one half the coefficient of x squared. The probability is three, is equal to one two as well. Okay, so good. So now you can write this long table. There are many more things I haven't written of correspondence between probability distributions and polynomials.
00:11:16.166 - 00:11:53.854, Speaker A: Projection corresponds to substitution, rank sequence corresponds to symmetrization, conditioning corresponds to differentiation. I didn't talk about correlation corresponds to entries of the Hessian, of the log of the polynomial, and so on. Now, the point is that, let's say I want to study probability distribution. Suppose I have a class of polynomial which are closed under the operations in the right hand side. Then I would get a nice class of probability distributions which are closed under the operations in the left hand side. So that's what I'm going to do next. I'm going to define a class of polynomials which are close on the right, and that would give me a nice family of probability distribution.
00:11:53.854 - 00:12:28.870, Speaker A: Okay, so let's go to that. So this is the first class of polynomials we're going to see. So we say, so I want to define a so called real estate polynomial. So let's say you have a polynomial p, again with real coefficients. Now, one thing to remember is that when you have multivariate polynomials, then the roots are not, as I said, are not, are no longer distinct. There would be a continuous, there would be continuously many roots. So in particular, if you have a degree two polynomial like this, you can look at its roots, for example, in r squared, and then you would get curves.
00:12:28.870 - 00:13:13.240, Speaker A: If you have a degree three polynomial like this, you can look at its root in r cubed and you get like surfaces and so on. We say a polynomial which is of degree d is really stable if every line pointing to the positive orient cuts these roots at exactly d points. Okay, so here you see that every line pointing this way cuts exactly the set of roots at exactly two points. Similarly, that's a degree three polynomial. Every line pointing to the positive word cuts these set of roots at exactly three points. So that's called real stable. Now, the real zero three region of this polynomial, you should think of it as a part of c to the n.
00:13:13.240 - 00:13:46.200, Speaker A: But I don't want to get into that. It turns out that a polynomial is really stable if and only if it has no roots in the upper half complex plane. Okay, so roughly speaking, having no roots in the upper half complex plane corresponds like having like regions with no roots in the complex plane corresponds to like intersection number of the lines in r two or r three or r 2d. Okay, so that's really what is going on. That's really the nice zero free region for this polynomial. Now that we have this polynomial, let's study their properties. So here are some simple properties.
00:13:46.200 - 00:14:28.064, Speaker A: First of all, any univariate real estate polynomial is real rooted. Second of all, if you have a real state polynomial, it is closed under many operations such as substitution, differentiation, or symmetrization. And one thing important to us is that random spanning theory distributions are real stable. So the proof of these statements are not complicated. For example, the one, the closure for differentiation just follows from the Guass Lucas theorem. I told you at the beginning, many other proofs are just calculus or some limiting arguments, so they're not so complicated. But let's see what we can get out of it.
00:14:28.064 - 00:14:53.076, Speaker A: Okay, so let me, let me show you an example. Application to random spanning trees. So here is a simple lemma you can prove with what I just told you. Let's say you have a random spanning tree distribution and you have a vertex v, such that the expected degree of v is somewhere between 1.5 and 2.5. Then I want to prove that the probability that the degree of v is two is at least one 8th is at least a constant down the way from zero. So there's nothing special about 1.5
00:14:53.076 - 00:15:15.158, Speaker A: and 2.5. As long as this is more than one and this is less than two, you get a constant here. Okay, so let's see how we can study such a question. So let's start from the random spanning three polynomial for this graph, let's say. So we said that random spanning three polynomials are real rooted, sorry, a real stable. So you get a real state polynomial. Now I want to zoom in to the degree of d, let's say.
00:15:15.158 - 00:15:43.830, Speaker A: So I first do projection. I substitute one for every edge which is not the neighbor of my vertex. So I get this polynomial, which is also real stable because of closure under substitution. Then, because I just want to look at the degree distribution, the rank, the degree distribution, I just symmetrize. I just count how many edges neighbor of d are sampled. So I get a univariate polynomial like this, where coefficients describe the probability distribution. Now this polynomial is real stable.
00:15:43.830 - 00:16:25.024, Speaker A: But we said that any real stable univariate polynomial is real rooted. So this is a real rooted polynomial. And now you can use the fact that for real rooted polynomials are just sum of independent Bernoulli. So there are independent Bernoulli's b one up to b three such that the sum has the same distribution as a degree of my vertex. So what does this say? This says that even though random spanning tree distributions are not uniform, sorry, are not independent, there are correlation between the edges. Using this machinery, you can turn your question into a question about sum of independent Bernoulli random variables. And now you can use a lot of tools developed in probability theory to study such a question.
00:16:25.024 - 00:16:53.894, Speaker A: Okay, good. So now we can ask. We can ask, okay, who cares about like spanning trees with degree two? Why is this interesting? It turns out that it's interesting because of traveling seismic problem. So let me just very briefly remind you what it is. Let's say you go to San Francisco for the first time. You want to visit some places and you want to take the shortest tour. So maybe here, maybe this is the shortest tour that visits all of the places that you see.
00:16:53.894 - 00:17:27.464, Speaker A: So it's really a hamiltonian cycle. Now if you think about it, what is a hamiltonian cycle? It's really a spanning tree where every vertex has degree two. I mean it's really a spanning tree like plus an edge where every vertex has degree two. So we should be interested in spanning trees where you have many degree two vertices. So now we seem to getting somewhere. So you can, so basically you can formulate an argument starting from a TSP instance. You can use tools from convex optimization to construct a random spanning tree distribution where the expected degree of every vertex is equal to two.
00:17:27.464 - 00:17:59.814, Speaker A: Then you can use the lemma I just mentioned to say that, okay, whenever you have expectation two, the probability is equal to two with at least a constant probability. The degree is equal to two with at least a constant probability. So you get that the constant fraction of vertices will have degree equal to two. So for example, here are two examples in the left. This is a torus graph on like about 25 vertices. And the vertices with degree two are colored in red. So as you see, a constant fraction of vertices have degree two.
00:17:59.814 - 00:18:46.584, Speaker A: And here I'm doing the same thing for a complete graph. Again, I'm choosing a random spanning tree of a complete graph where the expected degree of every vertex is two. And again you see a constant fraction of vertices have degree exactly equal to two. So the point is, using this machinery, you can prove that whenever you sample from such a random Spanit distribution, you have a lot of, maybe 30%, 40% of the edge of the vertices will have degree equal to two. Now, this part is not so trivial, but it turns out that this helps you to, the fact that you have many degree two vertices helps you to say that you can augment a tree into a TSP two of a small cost. Okay, so this all comes down to this theorem we proved a few years ago that improved the crystal fitting. So let me give you the context.
00:18:46.584 - 00:19:30.022, Speaker A: So the thing is, there is this simple algorithm by Christopher back in 1976, which gives a three half approximation for TSP, and it's a longstanding open problem to improve it. What we did with Sabrina Singh was that we improved the Christopher's algorithm, like by Epsilon by constant, Epsilon, for a special case of TSP, called graphic TSP. And that's exactly the way we did it. There are many other follow up works that I'm not covering here, but basically the point is we can use polynomials to study algorithms. Okay, so, okay, so, so far I covered the first thing. So I said that how you can use closure properties of polynomials with nice zero free region to talk about the structure of probability distributions. And we use that in the application for TSP.
00:19:30.022 - 00:20:09.736, Speaker A: So in this case, the nice zero free region was the upper half complex plane. Now let me go over the second application, which is how to use log concave polynomials to study matrix. Okay, so let me first tell you what matriids are, and then tell you how polynomials are related to them. So, matriids are, this are these abstract sort of objects. They, they are defined to generalize notion of linear independence, you get for vectors. So let's say, let's say you have a set of vectors in a finite, in any field, it could be a finite or real or complex field. We say a set I of these vectors are independent if the corresponding vectors are linearly independent.
00:20:09.736 - 00:20:29.064, Speaker A: So, for example, if you look at this picture, the vectors three and five are linearly independent. So this is an independent set. But four and five are dependent because they point to the same direction. So this is not an independent set. So this is set of independent sets. Bases are maximal independent sets. So here we have five bases.
00:20:29.064 - 00:21:13.394, Speaker A: One is orthogonal to everybody else. So it's in all of the bases, but the rest of them, okay, you get 123124 and so on. So you get five bases. Matriarchs are basically sort of invented about 70, 80 years ago to sort of generalize many things we see like many of the objects we see in combinatorics, and now we see also in computer science, like spanning trees, matchings, linearly independent vectors, and so on. Let me tell you one application of a starting matrix, and then tell you what we do. So here is one application of using matrix in application in coding theory. So let's say Alice want to send a message to Bob.
00:21:13.394 - 00:21:37.914, Speaker A: She's going to code it. What are set of feasible codes? You can think of them as follows. There is a parity check matrix m, and the set of feasible codes are exactly the kernel of m. It's a set of vectors x, such that m of x is equal to zero. So, Alice want to send this message to Bob. Now, suppose this channel is noisy. Every bit will be dropped with some probability p independently.
00:21:37.914 - 00:22:30.606, Speaker A: Okay, so here, for example, the first and the third bit are flipped, are dropped. What you want to see is that what is the probability that we can recover the message, that we can recover the code, you know, assuming that every bit is dropped, the probability p independently. So if you think about it, it turns out that you can recover the code if and only if the columns corresponding to the removed bits are linearly independent. So, in order to compute the recovery probability, all you need to do is to count the number of linearly independent set of columns. Or in other words, if you look at the corresponding matrix, the number of independent sets in that matrix. Okay, so now you see, there are some applications for doing counting problems on matrix, for example. But there are many other applications I'm not going to cover.
00:22:30.606 - 00:22:58.974, Speaker A: So let me tell you a little bit of history. Edmonds study like optimization over matriids. In seventies, he defined this object called matriid based polytope, which is the convex hull of the indicator vector of the basis of the matrix. So here my matriid has five bases. I'm looking at the indicator vectors of them. For example, 11100 corresponds to the base one, two, three. Okay, so you get this, you can look at the convex hall of this.
00:22:58.974 - 00:23:35.696, Speaker A: In general, you can have exponentially many bases in the num, in n, in the number of elements. So you can have exponentially many vertices, faces, and so on. But it turns out that even though this polytope is big, you can do optimization. You can, for example, test if a point is inside or outside this polytope, and you can optimize convex function over this polytope. So this was a contribution of matroid, sorry, Edmonds. Now, Michael and Umesh here asked in 1989 whether you can study random box on these matrix based polytopes. So what is that? Let's say this is the matrix based polytope.
00:23:35.696 - 00:24:03.474, Speaker A: So, see, the edges here correspond to swapping elements, like the edge between these two correspond to swapping the element two and three. So if you give me a base, I can easily figure out its neighbors in this matrix based polytope. So I can just treat this polytope as just a graph. Let's say I start from here. From this vertex I can run a random walk. Each time from a vertex I can go to one of its neighbors and so on and so forth. So I can run this random walk.
00:24:03.474 - 00:24:55.260, Speaker A: And then if you do a so called metro police filter, you can just run it such that it converges to a uniform distribution. And the question is, does this converge rapidly to the uniform distribution or not? If it does, it allows you to sample bases from a matrix and do counting problems for matrix. Okay, so here is what we proved with anari quique here somewhere, and Cynthia wins on that. Basically, for any matriid, if you look at the Markov chain and vertices of the matroid base polytope, it converges rapidly to the uniform distribution, like a time n squared, or even less if the rank of the. Make sure it's smaller. So yeah, basically we managed to answer this question of Mikhail and Vazirani after some number of years. The main tool that we use for this.
00:24:55.260 - 00:25:15.446, Speaker A: Oh, let me say, let me say one thing. So I haven't said what it means to converge to the uniform distribution. Let me be imprecise here. Instead, I'm going to show you a simulation. So what is the simulation? I'm showing two graphs. On the left you see the graph of a sort of a cycle and like 80 vertices. On the right you see the graph of a matrix based polytope.
00:25:15.446 - 00:25:58.244, Speaker A: For some matrix, the distribution is color coded here. So if two vertices have sort of similar color, it means that they have a similar value in a distribution. And the point is, if you see the graph on the right side, you got to the same color at like seven steps. In the left, even at like 30 steps, you don't get to the same color. Basically, the point is, even though the graph on matroids can be exponentially big in the number of elements, it will converge to the uniform distribution in time logarithmic in the number of its vertices. Whereas for a cycle, for example, it takes time like polynomial in the number of vertices. So if you have exponentially many, it would take exponential time to converge to the stationary distribution or the uniform distribution.
00:25:58.244 - 00:26:24.350, Speaker A: Okay, so I won't have time to tell you about much about the proofs. I just want to say how the polynomials are useful. Now here is how they are useful. I told you the spanning tree polynomial before. It turns out that the spanning tree polynomial corresponds to a determinant polynomial of some of positive semi definite matrices. That's a well known fact. Now, there's a fundamental fact in optimization that whenever you look at determinant of sum of PST matrices, the log of that function is a concave function.
00:26:24.350 - 00:27:01.752, Speaker A: So log of determinant is a convex k function. What we proved is that, okay, matroids generalize the spanning trees. So in a similar sense, if you look at a generating polynomial of a matrix, as opposed to generating polynomial of a spanning tree, so what the generating polynomial of matroid, just, you just sum over all of the bases. The product of zi is in the base. If you look at the generating polynomial of the matroid, this polynomial is locked concave over the positive orthon. So you just generalize this fact in optimization. And again here, the zero free region corresponds to the positive orthon.
00:27:01.752 - 00:27:12.214, Speaker A: You won't have any roots in the positive orthan. In fact, the log of your function would be concave in the positive. Okay, I'm going to stop here, hand it to Nikhil.
00:27:23.314 - 00:28:25.508, Speaker B: Okay, I'll take that as a hello. Clapping for me as well. So, okay, so I'm going to continue the story that Cheyenne started telling. So let's just recall, I mean, it hasn't been so long, what he said, he showed that we can understand some rather complicated probability distributions, like the distribution on spanning trees of a graph, or distribution on basis of a matriid, by encoding these exponentially sized distribution as coefficients of some multivariate polynomial, then observing that these multivariate polynomials, they're not just some arbitrary polynomials, they have some nice big structured zero free regions. And using this fact to go between the coefficients, zeros and other properties of polynomials, to answer these questions. So what I'm going to do in the rest of the talk is I'm going to build on this a little more. I'm going to explore a different way in which you can use polynomials to do combinatorics.
00:28:25.508 - 00:29:01.782, Speaker B: So what I'm going to do is I'm going to show you some, some instances where you can encode very interesting combinatorial information directly in the zeros of a polynomial as opposed to the coefficients. So that's one difference from what Cheyenne talked about. He was always encoding things as coefficients of a polynomial. And now we're really going to start caring about what the values of these zeros are. So you can view my part of the talk as sort of more quantitative version of what Cheyenne did. I'm going to present two applications. So one is something I call the graph skeleton problem, which essentially reduces to a problem about controlling eigenvalues of some random matrices.
00:29:01.782 - 00:29:52.286, Speaker B: And I'm going to show how this can be understood by actually throwing away a bunch of information and turning the matrices into polynomials and exploiting the interplay between the coefficients and zeros of these polynomials. And this is the longer story. And then finally, I'm going to briefly discuss sort of long going research program in statistical physics about approximating partition functions of certain statistical physics models. And again, the broad theme is the same. These polynomials are going to have some zero free regions which are now going to have maybe a little more complicated structure than before. And this is going to allow us to move freely between coefficients and roots and exploit some surprising phenomena there. Okay, so what's the graph skeleton problem? Okay, so here's a sort of motivation for this problem.
00:29:52.286 - 00:30:56.144, Speaker B: So this is a map of the power network of California, apparently from Ca dot Gov. And okay, here's a unfortunate situation that could arrive. Suppose there's a budget cut, and you can only keep half of the power lines for some reason. So you have to delete half of the power lines, but you get to choose which half you want it delete. And so what would you want to do? You'd want to somehow delete the ones that are redundant, and you'd want to keep half of the lines which preserve half of the functionality. So what you'd like to do is, can you do this in a way which preserves half of the functionality of the electrical network? And one reasonable way to make that mathematically precise is to say that, well, this is, you can, I mean, okay, I don't know how reasonable it is, but you can think of this as being a resistive circuit. And then the question is, is there a way to choose half of the link such that whenever you put a potential difference across the edges of the network, the energy dissipated in the subset that you chose is about half of whatever it was originally? Okay, that's one way of phrasing this.
00:30:56.144 - 00:31:35.322, Speaker B: Or maybe you don't care about electricity, maybe you're interested in roads. So that's a map of the highways in California. And suppose you could only keep half the roads. Again, could you choose a subset that retains half the functionality? Here, the appropriate definition of functionality is that for every cut. So that means for every subset of terminals in this road network, if you look at the boundary of that subset, so if you look at the roads leaving that subset, the weight should be about half of what it is in the original graph. So this is some problem which could hopefully not in the near future arise. What I'm going to show you is a mathematical problem which includes both of these as special cases.
00:31:35.322 - 00:31:58.974, Speaker B: This is what I'm going to call the graph skeleton problem. So I have a graph g. So to define the problem I need to define the notion of energy on a graph. So the energy of a vector of potentials on a graph is simply the sum of squared differences of the values of the potentials across the edges. So here's a graph g. It has some number of vertices. These red numbers are potentials on the vertices.
00:31:58.974 - 00:32:29.610, Speaker B: And so then the energy is just a sum of squared differences across the edges. So this contributes one, this contributes one, this contributes two, you add them all up, you get some real number, and this is the same as the usual physical notion of energy in spring and resistor networks. So this is the graph skeleton problem. So suppose, so the input is a graph g, finite unweighted undirected graph. And what I would like to find is a subgraph of this graph. So that's an edge subgraph. So same set of vertices but different edges as shown in this picture.
00:32:29.610 - 00:33:04.872, Speaker B: So the green set is a sub graph of the original graph. And I would like it to have the property that for all potentials x. Okay, the energy in the subgraph is between 40 and 60% of the energy in the original graph. Okay, so this is, the ratio of these energies is bounded between two numbers that are close to a half, 0.4 and 0.6. And the tricky part is I want this happen for all x simultaneously. So for all possible electrical loads, of course there's nothing special about 0.4
00:33:04.872 - 00:33:35.040, Speaker B: and 0.6. I could replace these by any number one minus q and q. And this number q measures how good a skeleton this is. So a graph that has this property, I'm going to call it a skeleton and the skeleton is going to come with a parameter q. So let q of s be the smallest q so that this is true. Okay, so if q is one, that's horrible, because every sub graph of a graph clearly carries between zero and 100% of the energy of the graph. If q is one two, then that's optimal.
00:33:35.040 - 00:34:29.015, Speaker B: That means I exactly split it 50% and anything less than one minus epsilon is good. If epsilon is some constant that doesn't depend on anything else, then I'm going to consider that a non trivial skeleton of this graph. I split the graph into two pieces that are in some sense functionally into one piece that's functionally equivalent to the whole. Okay, so the question I want to look at is, does every graph have a good skeleton? Okay, any questions about the statement of the definition of the problem? Okay, so why would you want to do this? There are a bunch of reasons. So first, is this electrical motivation? Maybe you're interested in finding sparse approximations of electrical networks. It turns out this condition also implies the condition that came in the road network problem. So if I have a graph s, which is a skeleton of a graph g, then it turns out that for every cut c, so c is just a subset of the vertices.
00:34:29.015 - 00:35:09.054, Speaker B: The weight of edges crossing the cut in s is about half of the weight crossing in e or q. If q is the quality of the skeleton, there are algorithmic motivations. So it turns out if you can do this, you can use this to solve an asymmetric version of the traveling salesman problem. This is work due an Ariana based current. And finally, this looks like a very combinatorial problem, but it's very close to a problem in mathematical physics called the Kadasen singer problem, which grew out of work of Dirac and von Neumann. And I won't define it here, but it's some linear algebraic problem that's very close to this. And it turns out if you can solve this graph skeleton problem, it's pretty close to solving this other problem.
00:35:09.054 - 00:35:29.050, Speaker B: But that's the problem. And so we're interested in the question, does every graph have such a skeleton? So let's look at some examples. So here's the easiest example. It's parallel paths. So it's four parallel paths on six vertices. And it's sort of, it's quite trivial to find the skeleton of this graph. Right? So for every edge there are four copies.
00:35:29.050 - 00:35:59.924, Speaker B: Let's just take the skeleton to be two copies of each edge. So then, you know, clearly it's easier to check this. The energy function for this graph is literally half of what it is for the original graph. That's easy. There is one thing that's interesting about this parallel path example, which is if you try to choose this skeleton randomly, it doesn't work. So here's a random subgraph, and this is not a good skeleton at all. And you can see that by.
00:35:59.924 - 00:36:26.890, Speaker B: How does this work? Yeah, so you can see that by looking at, for example, this cut is completely saturated and that's not good, right? It's supposed to carry about half the weight of the cut. This cut is completely empty. So in fact you can prove that a random subgraph is not a good skeleton. It's a poor skeleton. It has this q equal to one with high probability. So even though it's easier to find a good skeleton here, they're actually quite rare. They can be exponentially rare.
00:36:26.890 - 00:36:56.266, Speaker B: For example, okay, here's a slightly more interesting example, a dense random graph. So this is, so this is a graph in which every node is included with some constant probability. So it's connected to an average constant fraction of the other nodes. And here it's not so clear how to choose your skeleton s. So if I force you to try to find a deterministic algorithm, it's not at all clear. I mean, it's not at all clear to me what to do. But here the opposite thing works.
00:36:56.266 - 00:37:26.152, Speaker B: It turns out if you choose a random subgraph. So just include each edge with probability half in this subgraph s. Then you can prove using concentration arguments. So essentially if you look at any cut, so let's say you look at this cut, well, you know the edges in this cut that are included in s, that's sum of independent random variables. It's very concentrated. And this is good enough to prove that actually you get this energy preservation property. And a random subgraph is actually, it's a good skeleton of a random graph.
00:37:26.152 - 00:37:49.454, Speaker B: Okay, so this is easy, but for a completely different reason from the, from the path example. And now let's look at a hard example. It's a sparse graph, okay? So that this is a graph in which every vertex is connected to a constant number of other vertices. So here the degree is four. And again the goal is to find the skeleton. Again, it's not clear what to do deterministically. I mean, you can try all sorts of clever things.
00:37:49.454 - 00:38:25.368, Speaker B: This is quite a symmetric graph and maybe you might be able to come up with something, but random certainly doesn't work. So for example, here I've chosen a random subgraph of this graph. Okay, can anyone see why this is a poor skeleton? It has an empty card. Yeah, so S has two connected components. So certainly it's a horrible, you know, it's not, it has q equals one. For this reason. Um, okay, you can choose another random subgraph.
00:38:25.368 - 00:38:59.664, Speaker B: This one's connected, but now this one is bad. For another more complicated reason, it turns out there's some electrical flow where this subgraph carries 100% of the energy. So this random one doesn't work either. You can keep going and you'll go for a while, and you won't find anything because you can prove that with high probability. If you choose a random subgraph of large constant degree graph, you're going to get something that's a bad skeleton that's going to have Q close to one. Okay, so it's not even clear whether a skeleton exists for this graph. And finally, since we're getting to harder and harder examples, here's an impossible example.
00:38:59.664 - 00:39:19.298, Speaker B: So, this is a dumbbell graph. This is two cliques joined by a single edge. And it's quite easy to see that this does not have a non trivial skeleton because of this middle edge. So if you put that in s, you're gonna have 100%. If you don't, you're gonna have 0%. You're never going to get close to one, two. So this actually answers the question.
00:39:19.298 - 00:39:50.238, Speaker B: It's not always possible. There are some graphs for which there is no good skeleton. And the theorem whose proof I want to discuss is this theorem that we proved, Marcus and Spielman, which said that if G is a bridgeless graph. So what the theorem says is that that's the only obstruction. So, if you give me a graph that does not have a bridge edge, there's a quantitative definition of bridgeless in terms of effective resistances. But I won't get into it now, because I want to focus on the polynomial aspect. So if G is a bridgeless graph, then it has a good skeleton.
00:39:50.238 - 00:40:34.814, Speaker B: In particular, if it doesn't have the bridge edges, then you can find some subset so that you get a q equals 0.6. Quality skill. Okay, now, this is where polynomials come in. So I'm going to explain the proof, and it uses this polynomial paradigm. The first step of the proof is to encode all subgraphs of a graph as polynomials, and in a way, so that the quality of a subgraph, whether or not it's a good skeleton, shows up as being one of the zeros of this polynomial. Okay, so we turned this into a problem about energy and things like this, and a problem about polynomials with certain zeros. And then, even though we're interested in the zeros, we're going to average the polynomials coefficient wise.
00:40:34.814 - 00:41:03.848, Speaker B: This is where this interplay between coefficients and zeros comes in sort of surprising way. And this coefficient wise average, even though we know a random, you know, a random subgraph, is not a good skeleton. This is actually going to somehow allow us to deduce the existence of a good skeleton. Okay, so let's start encoding. So this is, this is, this is not very hard. So let's go back to our problem. It was a problem about preserving this energy functional, right? So e g of x is summation x I minus x j squared.
00:41:03.848 - 00:41:47.184, Speaker B: Well, that's a quadratic form, which means it's certainly x transpose lx for some matrix l, which turns out to be the graph Laplacian. It looks like everybody in this room already knows what that is. But anyway, I prepared this for general audience, but it doesn't matter what it is, it's some matrix. So the energy is a quadratic form of some matrix. The condition we're interested in is finding an s so that the energy of s is sandwiched between these two copies of energy of g. Now the point is, since it's a quadratic form of a matrix, this is equivalent to an eigenvalue problem. So by representing this as matrices and using the Quran Fischer theorem, this condition is actually equivalent to the property that the eigenvalues of the Laplacian of s times the inverse of the Laplacian of g are contained in interval one minus q.
00:41:47.184 - 00:42:13.346, Speaker B: Q. Okay, basic linear algebra. Okay, now that is a problem. That is a property of a polynomial, right? That's just the same as saying the characteristic polynomial of this matrix ls lg inverse has all of its zeros in this interval. I'm not doing anything fancy. It's still basically linear algebra. And then, now I want you know, this is some information about the lower and upper bound on the zeros.
00:42:13.346 - 00:42:57.038, Speaker B: I can just multiply this polynomial by the characteristic polynomial of the complement of s and get a one sided condition. The characteristic polynomial corresponding to s has zeros lower bounded by one minus q and upper bounded by q. If this new polynomial p's, which is the product of the characteristic polynomial, and the characteristic polynomial complement has zeros bounded upper bounded by one over q. Okay, so that's it. So now I've reduced this to a problem about polynomials. Given a graph, I want to find a subset s such that the corresponding polynomial has largest root bounded by q. Okay? So the idea now is to consider random polynomials and average them.
00:42:57.038 - 00:43:20.984, Speaker B: Okay? So let's, let's see how this looks. So this is our first example of the path, the parallel paths. This is the empty subgraph, and it's characteristic polynomial, just x. I'm sorry. Corresponding polynomial is x to the two n and all the roots are zero. So the circle is, these circles are the zeros and this is the function. Okay, here is another, another.
00:43:20.984 - 00:43:48.140, Speaker B: Here's a random partition. Doesn't look too good, right? And I plotted the corresponding polynomial in the same plot. So this is now this yellow polynomial, and it has zeros, you know, elsewhere it has a zero at zero, but it also has a zero at one corresponding to these bad cuts. That doesn't look like a good polynomial. Remember, we wanted a polynomial like this with all zeros bounded away from one. Okay. Um, let's look at another one.
00:43:48.140 - 00:44:16.964, Speaker B: Let's, let's plot it on the same graph. And now you see that, you know, you see that as functions, these can, these things can have very different signs at any, at any given value of x. Right? Let's do it again. And so the zeros have a very clear meaning. It's not as clear what the function itself is representing. But I can just go on plotting like this. I do this a bunch of times.
00:44:16.964 - 00:44:48.096, Speaker B: So these are now the polynomials corresponding to a bunch of different random subgraphs. And what you see if you do this for a while is that there are going to be lots of cancellations. This is huge at this point. This is small, et cetera, et cetera. And if you keep doing this for a while, if you just literally average everything that you see, you get something that looks like this. This is sort of the punchline that you had all these crazy polynomials that were blowing up all over the place. Some of them had horrible roots.
00:44:48.096 - 00:45:15.544, Speaker B: Most of them had roots that were close to one and zero. But when you average them, all the stuff with the bad roots seems to cancel out, and you get a polynomial that looks like this. In fact, the roots of this polynomial are bounded, I think, between 0.3 and 0.7, or something like that. Okay? So we did this, like, after doing this encoding, we did this brainless thing. We just averaged all the polynomials over all partitions, and we got a polynomial that looks good, but it doesn't correspond to a partition.
00:45:15.544 - 00:45:39.564, Speaker B: Okay? But you might argue that, well, this is a very symmetric example. You know, maybe there's so much symmetry that you get these nice cancellations. But now let's run this on our hard graph, right? This is our sparse graph that we were having trouble with. This is the polynomial corresponding to this partition. Okay, it's something. All these polynomials are monic, so they go to infinity after one and so on. Let's plot another partition.
00:45:39.564 - 00:45:57.794, Speaker B: Okay, let's plot another partition. And now this last. So the first two I plotted were both bad. They both had roots at zero and one. But the last one I plotted is actually good. You can compute its roots, and they're actually bounded away from zero and one. They're 0.9
00:45:57.794 - 00:46:27.034, Speaker B: and 0.1 or something like that, this particular one. Now, the magical thing that happens is if you compute the average over all possible partitions of this, you get a polynomial that looks like that. I had some technical issues, and I couldn't compute it in time because there are two to the, I don't know, some large number of partitions of this. I couldn't compute them all the time, but I know that you get something like this. So again, you have this phenomenon that I'll explain in a moment. I'll explain why I know.
00:46:27.034 - 00:46:53.574, Speaker B: Again, you get this phenomenon that all the polynomials with large and small zeros cancel out. So you have these two surprising phenomena. You're adding up all these polynomials. First of all, you're adding up a bunch of real rooted polynomials. You're always getting something with real roots that should already be surprising. And the second surprising thing is that not only are you getting someone with real roots, all the stuff you don't want is somehow canceling out. And we can turn this into a proof.
00:46:53.574 - 00:47:32.530, Speaker B: So here's one lemma. For any graph g, the expected polynomial has real roots. And if the graph is bridgeless, the largest root is bounded by 0.6. Okay, so this theorem articulates this phenomenon that these bad polynomials cancel out. Won't have time to say much about the proof, but it uses a lot of what Cheyenne was talking about. So the idea is to view the expected polynomial, which is the univariate polynomial, as a restriction of a certain multivariate polynomial, which is real stable, which has a nice zero free region, and then analyze log q inside this multivariate zero free region. So the zero free region now is more interesting.
00:47:32.530 - 00:48:28.718, Speaker B: It's some other convex cone, but it's the same sort of theme. So that proves that when you average these polynomials, it somehow finds finds a good one. And then there's another lemma, which I also won't prove, which says that, well, once you find this good polynomial, there's some topological reason why there has to exist a specific polynomial, a specific graph which has its roots upper bounded by the root of the average polynomial. So the interesting thing here is the expectations inside the largest root operation. Anyway, if you combine these, you get this theorem, and the punchline is that somehow you're interested in the zeros. You know that the average zeros are actually bad, right? I showed you that. But if you average the coefficients, you get these cancellations, and somehow this, even though this is an average like statistical object, it finds these polynomials that have good zeros.
00:48:28.718 - 00:48:59.884, Speaker B: And there are various open directions about understanding this in terms of random matrix theory. We still don't understand it. We know how to prove it. I don't personally understand it very well. Okay, so that was one story. The other story which, okay, I don't have much time to talk about. I'll spend a minute on it, is an old story from statistical physics, and I want to include it because of its historical significance and also because it connects many different strands of research.
00:48:59.884 - 00:49:26.474, Speaker B: So this is something about the ferromagnetic easing model. So if you don't know what the ferromagnetic easing model is, it's probably hard for. It's probably, well, I don't know how. Well, I can explain it in the next couple of minutes, but I'll try it. So a caricature of it is the following. So it's a probability distribution on subsets of a graph. So you have a given graph g, such as this graph here you have all configure, you have a set of configurations, which is a set of subsets of the vertices of this graph.
00:49:26.474 - 00:50:07.908, Speaker B: Then you have a probability distribution that's parameterized by two parameters, beta and lambda. So beta is an interaction parameter, and. Okay, and the probability distribution is that the probability of a set is proportional to beta raised to the size of the boundary, times lambda raised to the size of the set. Okay, so what does this mean? So here's a particular configuration. So what this means is the probability distribution favors configurations with small boundary, because beta is less than one, and depending on what lambda is, favors configurations that either have many vertices or few vertices. So this is a particular type of probability distribution on a graph. Here's another configuration.
00:50:07.908 - 00:50:51.308, Speaker B: It's going to have probably a lower probability, at least if you've in terms of beta, because it has a much larger boundary. Now, the normalizing concept of probability distribution is a polynomial. If you add all these things up, you get a polynomial, which is called z of z, the partition function. And it turns out that derivatives of the log of this polynomial, again the same type of object, correspond to physical observables. I won't say any more about that. And therefore, physicists are interested in the location of the zeros, because existence of zeros implies a phase transition in the statistical physics model. So the first theorem in this line of work, and really one of the prototype type theorems in this area is the theorem of Li and Yang.
00:50:51.308 - 00:51:50.724, Speaker B: What they proved is that in this model, when beta is between zero and one, all the zeros lie on the unit circle. So they proved that this interior of the disk is a zero free region. So is the exterior of a disk and one. So this theorem is one of the oldest theorems proved many decades ago. And one very cool story that's been going on is that now there's a computational exploration of this phenomenon started by work of Barbienock and others, um, which says that, well, you know, this, these fa, these, these zeros correspond to phase transitions, but they also tell us where we can approximate, like on a computer, the polynomial efficiently. So, for example, there's a line of work showing that you can approximate the partition function of the easing model model efficiently, that is, in polynomial time away from the, sorry, quasi polynomial time away from the zeros. And again, this has, this boils down to the fact that the logarithm of the partition function has a rapidly converging power series into inside the zero free region.
00:51:50.724 - 00:52:39.052, Speaker B: Okay, so that's all I want to say about the easing model. Those are the two things I wanted to say. And roughly, what was the summary of the talk? Well, you can encode all this data, all this interesting combinatorial data in polynomials, which then have these three interfaces of how to access them, coefficient zeros and function values. And in general, you can't do anything right. All polynomials is an incredibly rich, wild set of objects. You can't, you can't expect to have nice relationships between these things, but if you know something about existence of a nice zero free region, usually a convex cone, and perhaps something about concavity of log on that region, then you can easily move between these two representations and you can get nice theorems and algorithms out of it. So I guess I'm already over time.
00:52:39.052 - 00:53:04.134, Speaker B: So I'll just end with the slide listing. There are connections to many other areas that are, people are working on intensely in the program that I didn't discuss. And one question is, we have all these arrows. Which of these can you actually implement in polynomial time? So the example, remember, I had to fake that polynomial because I couldn't compute it. There's a good reason for that. I couldn't do it in polynomial time. And so there are algorithmic questions there as well.
00:53:04.134 - 00:53:54.284, Speaker B: Questions, yep. I don't think everything goes away. So there's a definition of analogous to this real stability for analytic functions not having zeros in the upper half plane, a lot of the properties of this being preserved under various linear operators. That also holds. I don't think it's been explored from this combinatorial perspective, usually because we're used to dealing with finite objects. But it's a good question. I mean, I think it's a very good question.
00:53:54.284 - 00:54:03.844, Speaker B: Any other. Yep.
00:54:04.184 - 00:54:06.088, Speaker A: So what do you actually mean by.
00:54:06.136 - 00:54:08.096, Speaker B: Making these arrows, like, algorithmic?
00:54:08.160 - 00:54:10.408, Speaker A: Because, like, can they actually use, like.
00:54:10.456 - 00:54:33.164, Speaker B: FFT, go from coefficients to. Yeah, so, okay, so what I really mean is, usually in practice, these polynomials are, they're np hard or sharpie hard to approximate to compute. Exactly. So we're always working with approximations. And so once you only have approximations, these questions become very different. You can't just do interpolation. Right.
00:54:33.164 - 00:54:47.824, Speaker B: Or something like that. Plus, they usually have many variables. So anyway, you would need many points to do interpolation. So really, the question is, when you restrict your attention to these special families, do you get some nice computational features out of it? I don't know if Cheyenne wants to say more about that.
00:54:49.124 - 00:54:52.012, Speaker A: The only thing I wanted to say was that right now, it seems we.
00:54:52.028 - 00:54:54.084, Speaker B: Can go from coefficients to functions for.
00:54:54.124 - 00:55:03.044, Speaker A: Some of our many of these, but other directions, you may want to assume more about the polynomial.
00:55:18.344 - 00:55:38.376, Speaker B: Yeah, so you're right. So what I did was with the expected polynomial. Right. Your question is, what about the expected maximum root? Well, let's go back to this example. Right. So this says that with high probability, the queue for this is one. Right.
00:55:38.376 - 00:55:59.524, Speaker B: So in particular, the expectation is one. So the expected maximum root can. That's the point. The expected maximum root can be horrible, but the maximum root of the expectation somehow cancels out these bad events. Hello. All right, thank you.
