00:00:00.160 - 00:00:32.664, Speaker A: I think we're good to get started. So, our first speaker for this session is Mark Embry. Was that you think we talked about deterministic? Mostly, yeah. Thank you very much. I appreciate the opportunity to come and speak. I'm a little bit of an outsider to this community, coming from traditional numerical linear algebra, but I'm certainly an enthusiastic admirer of Rand and LA. And I'll try to make a few connections here throughout my talk and maybe entice you to look at a problem or two that, that maybe your algorithms and skills could be particularly helpful for addressing.
00:00:32.664 - 00:01:30.214, Speaker A: Motivation for talking on this subject really comes from some conversations with Petros in the past about convergence theory for Krylov's subspace methods and some of the ways these algorithms come to be used in recent developments in Rand NLA. So with that, let me get a setting here. We're thinking of a, a large square sparse matrix, potentially non hermitian, and I have complex entries there, so we don't have to worry about making any special exceptions for complex eigenvalues and eigenvectors. But, of course, if you like to live in the real world, you're welcome to do that. We're imagining that all eigenvalues of a are too expensive to compute, although there are some settings in which we do want all the eigenvalues of a. For example, if we want some statistics about the distribution of the spectrum, some problems in quantum mechanics. So we're looking for some small subset of the spectrum, let's say m eigenvalues, and often these are eigenvalues that are on the right end of the spectrum, or the left end of the spectrum, or largest in magnitude, and so forth.
00:01:30.214 - 00:02:49.182, Speaker A: And so the technique we'll use are a variety of algorithms we might apply to this, but many of them fall in the framework of projection methods, which I guess you could say is a kind of sketching where we're going to look at the operation of a on a subspace that has very low dimension, larger than the number of eigenvalues we're trying to compute, but smaller, much smaller than the ambient dimension, n. And so we hope that we compress a down to this low dimensional subspace, and that some few number of those eigenvalues of the compression of a onto the subspace are accurate approximations of the eigenvalues of a. And so everything will then come down to how we choose that subspace v. And so, as I say, mostly this is the deterministic case, but I will address a few rand NLA issues as we, as we go along. So I had a couple questions yesterday. Do we even need to care about non symmetric matrices? Why is the non symmetric eigenvalue problem relevant? Well, of course, if you're interested in SVD calculations or many problems from quantum mechanics, the symmetric case is fine. But there are many applications, particularly coming from a variety of different discrete or continuous time dynamical systems, where we're very interested in computing the some eigenvalues of a, of a non symmetric matrix and understanding some of the spectral perturbation theory behind that.
00:02:49.182 - 00:03:25.642, Speaker A: So, a few examples for you here, and one plot just to show you what the complex plane looks like. If you've not visited there very often, I encourage you to dip your toes in. We're looking for eigenvalues of a pretty small matrix dimension 381,000. In serious calculations for 3d fluid flows, we might be interested in a matrix that's 1000 times as large. So this is something of a toy problem, even though it's quite large. And here I'm looking for, I think I've computed 1600 eigenvalues in this example, most of which are a number of which are on the screen there. Yeah, I thought we were only interested in pseudo spectra.
00:03:25.642 - 00:03:44.158, Speaker A: Well, I show you pseudo spectra here. That's what those colored lines are all about. Are we looking for eigenvalues or are we looking for. We want both. We want both. So in this case, the rightmost eigenvalue is very stable to perturbations and it's perfectly legitimate to compute it. I have no faith in the accuracy of these eigenvalues, but I don't care about them so much.
00:03:44.158 - 00:04:16.774, Speaker A: I care about that rightmost one for understanding a fluid stability question. Okay, so an overview of projection based eigensolvers. So that's the setting we've had before. Obviously the power method is the simplest version of this. Just take powers of a applied to a vector X, a one dimensional subspace, easy to implement, but it can be slow. Of course, very little storage in this community. Of course, we've seen the great success of subspace iteration powers of a now apply not to a vector, but to a block of vectors, where now we have larger dimensional subspaces.
00:04:16.774 - 00:05:22.972, Speaker A: Those matrix vector products can be very quick to apply. We build large subspaces quite fast, and it also has the ability to get repeated eigenvalues if we have multiplicity. But there's a richer world out there of Krylov subspace methods, where now, instead of just looking at, say, one fixed power applied to a block of vectors, we're going to look at the aggregation of many different powers of a, which will give us the ability to work in polynomials of a applied to X. And if you doubt the wisdom of Krylov methods, you might just think if you had the choice to, say, apply a to a block of ten vectors. So in other words, ten first iterations of the power method, versus looking at the first ten iterations of the power method applied to one vector, you might expect that that by the time you get ten iterations of the power method, you're really learning quite a lot about a dominant eigenvalue, much more than you would from the aggregation of one iteration applied to a bunch of different vectors. So the high powers matter a lot. But that's lots of applications of a block.
00:05:22.972 - 00:06:25.962, Speaker A: Methods are trying to get the best of both worlds, where we'll apply powers of a to blocks of vectors. And that's a technique, obviously that's popular in this community, particularly for SVD calculations. Joel was showing some nice calculations with this yesterday to a few of us. The trick is, how do you benefit? How do you balance large subspace dimensions with block sizes and trying to strike the right balance to get fast convergence. But even within the single vector Crelov world, which I'll focus on today, there are lots of different options that maybe this community hasn't yet explored fully. So, for example, I'll say a few words about restarting Krylov methods, where, after we've maybe done an initial calculation, we try to improve the starting vector by running a polynomial filter in a against the starting vector to get faster convergence by getting a better starting vector. An intriguing method that I'm pretty fond of at the moment is where we take a polynomial transformation of a and apply Krilov subspace based on that polynomial transformation.
00:06:25.962 - 00:06:59.444, Speaker A: This is a way to get very high powers of a. I'll show you an example or two of this later. So we can really get very large powers of a this way, or we can do something even more radical, like transform a entirely, turn the spectrum inside out by inverting a or even taking a rational transformation of a. And these are popular algorithms, essential algorithms in engineering computations. The shift invert Creelov. So when you run igs, you're either running restarted Crelov, or you're running some version of shift invert Creelov. Same for svds.
00:06:59.444 - 00:07:48.514, Speaker A: Okay, a few preliminaries that will help us with some of our convergence theorems that follow. I'll assume I have some number of distinct eigenvalues. For each distinct eigenvalue, I can construct this complex contour integral. Even if we have a symmetric matrix, we can still define this contour integral around a little contour that contains that single eigenvalue and no others. And it turns out that that contour integral is actually a projector, and its range is the invariant subspace, the eigenspace, and possibly some other stuff. For non symmetric a associated with the eigenvalue. If you have a symmetric matrix and the eigenvalue has a simple unit eigenvector, this contour integral is just the outer product of the eigenvector with itself.
00:07:48.514 - 00:08:36.384, Speaker A: But in the non symmetric case, things can be a little bit more interesting. In particular, while Pj is a projector, there's no need that it be orthogonal. In other words, it's not a symmetric projector, it's not a nice, the norm of the projector can be larger than one, but together these projectors give us a resolution of the identity. So I'll have a little bit extra notation here. Pg will be the sum of the projectors associated with the eigenvalues. I want ug, g is for good, is for the invariant subspace I'm trying to compute, and m is the number of, well, the dimension of the invariant subspace, the number of eigenvalues, if you will, counting multiplicity, and then the b is for bad. We have the complementary spectral projector, the complementary invariant subspace, and then the remaining dimension.
00:08:36.384 - 00:09:06.176, Speaker A: So just have that notation down. And the way that we're going to measure convergence is not by looking at the error in an eigenvalue or even the norm of a residual. If we put an approximate eigen pair in and apply it to a, we're going to measure angles between subspaces. It's a very geometric way of looking at the problem. And so that's also we can define the angle between two vectors. Of course, we're now defining the angle between two subspaces. This came up, of course, in the last talk.
00:09:06.176 - 00:09:59.306, Speaker A: We're doing lots of those calculations in the plots that follow to just draw convergence curves, many angle calculations. And this measure, which we call the containment gap, is really a way of assessing how m dimensional subspace compares to a k dimensional subspace. So assuming k is larger than m, so we have a chance of approximating all the vectors, we're looking for the best approximation to our m dimensional subspace from that larger subspace. So every place you see a delta, just think of it as the sine of the angle between the best approximating subspace and the invariant subspace in particular. Notice I'm not saying how we're going to extract eigenvalues from this subspace, so that could be another issue we'd have to have to think about. This is what typical convergence curves look like. In the non symmetric case, there's some warm up period where the Crelode space just has to get big enough to even approximate all k vectors.
00:09:59.306 - 00:10:48.474, Speaker A: So there's a kind of plateau there. And then there can be a dip where some special properties of the starting vector and the orientation of the invariant subspace to its complementary subspace causes a kind of dip, a fast but slowing convergence. And then the method settles down into some pretty steady linear convergence rate. And sometimes you'll see it dip down faster at the end. So this curve will actually accelerate, and we'll talk about why that might happen. So this is the kind of thing we'd like to come up with an upward bound for. And a lot of the theory that I'll be describing today goes back in its roots to work Yusuf Saad did in a couple of beautiful 1980 papers, subsequent 1983 paper, but a couple of papers that Chris Beatty and I wrote with John Rossi and Dan Sorensen.
00:10:48.474 - 00:11:27.364, Speaker A: Ultimately, we're going to bound convergence based on the product of two constants, which I will actually tell you what those constants are. And then a polynomial approximation problem. We're trying to approximate one by the product of a polynomial that's fixed. It has as its roots the m eigenvalues that I'm trying to compute times another polynomial that I'm free to choose of degree k minus two times m. Remember, I'm computing m eigenvalues. So now for this bound to hold, I'm assuming I've built my subspace out to at least two m. So that's already maybe a little bit of a gap in the analysis here, but it turns out to work out cleanly that way.
00:11:28.024 - 00:11:29.576, Speaker B: You're oversampling by, mister.
00:11:29.600 - 00:11:49.176, Speaker A: Oversampling by twice just to start getting convergence. Exactly. Okay. Yeah. On your previous slide, are you saying that you're getting machine epsilon accuracy in about 150 iterations for the angle. For the angle, for the angle between the subspace. Well, I didn't tell you how nice this problem is.
00:11:49.176 - 00:12:09.144, Speaker A: This isn't. That's very nice. Yeah. The examples that I'll show for illustrative purposes are going to be, have nice gaps and eigenvalues and so forth. Okay, if I went back to the problem I showed you in an earlier slide. Things are much more difficult for real problems like that. But I want to be able to just show you some nice cartoons almost of what the convergence looks like.
00:12:09.144 - 00:12:36.846, Speaker A: So you can scale this to your problem. Add a zero or two there if you like. Yeah. Ok, so what are the constants going to be? The first constant c one will tell me how my starting vector is oriented toward the invariant subspace. My second constant will depend on the orientation of the eigenvectors to one another. And then I've told you about the polynomials. We'll have this set omega b, which is some set that contains the bad eigenvalues I don't want to compute.
00:12:36.846 - 00:13:17.974, Speaker A: That's the set on which the approximation problem is posed. And I've told you then about this polynomial alpha, kind of a minimum polynomial for the desired eigenvalues. Okay, so let's, first of all, I had a caveat here, that if the subspace was reachable, we have to obey some ground rules, which I'll go through quickly. If you don't have a component, if your starting vector lacks a component in the eigenvector you're trying to compute, even if we just look at the power method, you get no action from the eigenvalue lambda one that you're trying to compute. It doesn't show up. It could be any value. You have no hope of computing that particular direction matrices, but not necessarily for non hermitian.
00:13:17.974 - 00:13:19.366, Speaker A: That's true for non hermitian.
00:13:19.430 - 00:13:22.234, Speaker B: No, we can't talk about optimal.
00:13:23.154 - 00:14:19.954, Speaker A: So this I'm assuming, I'm not assuming a Jordan block here, but yeah, that decomposition holds in general, so you won't see the direction you're not trying to compute. But a different problem arises if we have multiple eigenvectors that are linearly independent associated with the same eigenvalue. Simplest case would be an identity matrix, where if we just try to build the Crelov subspace, you run out of gas after just one step, and you exactly recover the eigenvector. We call this happy breakdown, but it may be if you think of your eigen basis of the identity as being columns of the identity, that vector x maybe doesn't look like any of the vectors you're trying to look for. Here's maybe a more striking example. Two Jordan blocks, a two by two Jordan block in the left, and a three by three on the bottom right. And I have a vector x here where I can choose any component I want in that second slot.
00:14:19.954 - 00:15:05.564, Speaker A: So in particular, I might try to compute the eigenvector 0100 by making this starting vector lean really closely toward that eigenvector. And it turns out that regardless of c, I'm actually going to compute the eigenvector zero, zero, one, and I get a three by three Jordan block. So even regardless of how closely oriented that starting vector was to the two by two Jordan block, I always get the other Jordan block, if you will. So this is one of the quirks of these Krylov methods. If I wanted to get the two by two Jordan block, I'd have to. There's only a set of measure zero starting vectors that can find it, so I have to be a little careful in the presence of this multiplicity. Ok, so I think that's enough.
00:15:05.564 - 00:15:44.124, Speaker A: Yeah. When you move to the block methods, though, you can identify higher dimensional high values with multiplicity. Yes, yes. On a set that's not measure zero, is that right? Exactly, yes. So if I had, if I were building out a basis of, say, suppose I just think of this example, right? That's an artifact of a single vector, right? Single vector. If I think of just the identity, even that simple example with a block of two vectors here, I would run out of gas after a two dimensional subspace where repeated eigenvalues are nasty. If you.
00:15:44.124 - 00:16:36.596, Speaker A: Yeah, like the identity, yeah. Okay, let's think. Now, assume we're in the case where we don't have some unlucky starting vector, we're gonna actually be able to attain some convergence. How does this procedure go? How does the starting vector influence things? So if I just look at a single eigen, look for a single eigenvalue, I have this old bound from sod which we can kind of express in this way, and now the leading constant is one over the norm of the spectral projector applied to the starting vector. In other words, the component of the starting vector in the eigenspace I'm trying to compute. And you can see as the starting vector is oriented further and further away from that space, this constant gets larger and larger and larger. We expect slower convergence and for subspaces, again, single vector iterations.
00:16:36.596 - 00:17:23.552, Speaker A: But now looking for k, for m eigenvalues, we have another kind of constant that looks pretty similar, that looks at the orientation of the worst vector in our Krylov space of dimension m, the size we're looking for, and we find the vector that has the best orientation toward the bad subspace in the least orientation toward the good subspace. So it's a nice way of seeing what this constant c one should be. We can compute it using a generalized singular value decomposition. And if we look at an experiment where we take starting vectors that are oriented with a subspace of dimension eight hermitian case here of increasing angle. So start very, very close. Our convergence curve is fast. As we get farther away, we slow down.
00:17:23.552 - 00:17:44.336, Speaker A: Eventually we get a plateau before we converge. And if we look at what the error bounds would be, there, this constant c one is shown as the black dot, and as it gets high ten to the 6th. That's a scary constant. But we see that that gives us kind of the right height to our convergence curve to capture this rightmost blue convergence.
00:17:44.400 - 00:17:49.416, Speaker B: So mark, this has only to do with the starting vector, not with the eigenvector condition number for the particular.
00:17:49.560 - 00:18:10.374, Speaker A: No bakes that in, because these spectral projectors measure the conditioning of the eigenvectors, yes, this constant is very subtle in that way. These are not orthogonal projectors. Those are the spectral projectors there. It's unavoidable in the bounds. It's unavoidable in the bounds. It's a feature and not a bug.
00:18:10.754 - 00:18:15.214, Speaker B: There's a sensitivity of the sensitivity.
00:18:15.594 - 00:18:49.504, Speaker A: Okay, what about the eigenvalues themselves? We should probably look at those quickly here. This is, I think, sort of the most boring part of the bounds. We need polynomials that approximate a function well over the unwanted part of the spectrum. If you're in the hermitian case, you come up with an interval that contains the unwanted eigenvalues. This is the famous gap condition that we've been looking at before. What's the separation of lambda M from the rest of the spectrum? Can use Chebyshev polynomials to get the convergence. If you have in the complex plane, you use potential theory.
00:18:49.504 - 00:19:33.864, Speaker A: So if we have some unwanted eigenvalues and some well separated nice eigenvalues, we bound the unwanted eigenvalues by a continuous set. We can formally map it to the disk that causes the wanted eigenvalues. Also to move, do a conformal map of the exterior of the disk. Everything on the disk is nice. We know how to put interpolation points that are give us optimal approximation on that region if we're working on the disk, and then invert the conformal map to go back to the domain we care about. And we know where to put optimal interpolation points to interpolate that function one over alpha to get a good approximation. So Green's functions give us some way of getting convergence rates, that linear rate that I showed you earlier.
00:19:33.864 - 00:20:01.304, Speaker A: But if the convergence is slow, as often happens, think about whether you're solving the right problem or not. Here's an example. If I have a just think of a symmetric case. Again, I want to compute a single eigenvalue. Then the convergence rate gets worse. If I imagine lambda one and lambda two being locked down. But as the dimension gets large and I get more and more, eigenvalues spread further and further and further apart.
00:20:01.304 - 00:20:28.270, Speaker A: This is exactly the situation that happens when we discretize a differential operator, an unbounded differential operator. And as we get better and better discretizations, lambda n gets bigger and bigger and bigger, and so the convergence rate slows. So here's an example, just a discretization of the Laplacian in one d. Two hundred fifty six. Pretty slow convergence. That's ten to the minus one. This is an example I could use.
00:20:28.270 - 00:20:53.854, Speaker A: If you want big numbers, we could have big numbers. Yeah. If I improve the discretization of 512, convergence slows and it gets even worse, the better and better I get. Those are calculations and not bounds. So the discretization is better, the convergence is worse. We can see what's going wrong if we go to the Krylov subspace method itself. What's that? Uniform discretization.
00:20:53.854 - 00:21:41.588, Speaker A: But if I think that I'm really applying the Krylov method to an operator, the unbounded operator minus the second derivative with Dirichlet boundary conditions, and I pick some vector that satisfies those boundary conditions, and I try to generate my Krylov subspace, it fails at the first step when I apply my operator to f, and usually it won't be in the domain of the operator anymore. So it makes no sense to apply the operator twice. So the functional analysis is saying this is the wrong way to solve this problem. Well, how do we do it? Well, we should invert the operator. And if we think of what the inverse of the Laplacian is, well, it's going to be a compact operator. Back in high school, I learned about constants of integration. Right? And then if we pick those constants so that we stay in the domain of the operator, we're golden.
00:21:41.588 - 00:22:11.304, Speaker A: And now look at the convergence. This is in mathematica. Now, there's no discretization. I can run Lengsho's in mathematica, and we get what we call super linear convergence, that we get fast convergence rates. And this is in the eigenvalue error, because we've applied the operator to, we've applied the method to the right operator, the inverse. And so, if your convergence is slow because your eigenvalues are bunched together. Maybe you want to do something to turn the spectrum inside out.
00:22:11.304 - 00:22:42.962, Speaker A: So that's what we call shift invert Arnoldi. That's what Eigs does when you ask for smallest magnitude eigenvalues of a non symmetric problem, for example. Another thing we can do, a poor man's spectral transformation, is not to invert a, but to apply some polynomial in a. This is what we call polynomial preconditioning. We can use min res. I had to honor the method from Michael Saunders in the front row here. We can use minres to construct the polynomial PI that we want to use.
00:22:42.962 - 00:23:35.446, Speaker A: And here's just an example where we have a bunch of eigenvalues, which you can read across the horizontal axis that are bunched together on the left part of the spectrum, maybe have a little bit of a gap, and then bunched together again down here. And as I look at different polynomials of increasing degree, I can read the eigenvalues of PI of a off on the left here through these gray lines. And what we see is that as the polynomial degree increases, I'm separating out these eigenvalues here. They're much more separated on this axis than they are on this axis. And now I can run the Krylov method on PI of a and potentially get much faster convergence. Okay, what about eigenvectors? Well, they come into the bound through this constant c two. And if you're only interested in the symmetric case, your c two is always equal to one.
00:23:35.446 - 00:24:25.726, Speaker A: And if you're interested in non symmetric problems, then you need some way of handling that departure from normality, or the departure from orthogonality of the eigenvectors values. And there are various choices you can make for sets in the complex plane on which to bound the norm of f of a. So the eigenvalues of a, the numerical range, the epsilon pseudo spectra. So now we pose the approximation problem on sets whose size grows, but we have better control over the constants. And so we have a variety of different bounds here that depend on that departure from symmetry. In the context of r bounds, this f of a is really f of a constrained to the unwanted invariant subspace, and we just apply the bounds in that setting. The constant here for the numerical range is conjectured to be two.
00:24:25.726 - 00:25:16.054, Speaker A: That's known as Crusier's conjecture, and it had been 11.08 for many years until Crouzet and Palencia chiseled it down a little bit last year. Okay, this is, I think, an acute plot of how the power method is affected by angles between subspaces, angles between eigenvectors. So I set up a little three by three matrix with two entries here, alpha and beta, that I'll turn on in turn. So if I leave them both zero, I have a symmetric problem, and I run the power method to compute the eigenvector, u one, the dominant eigenvector. And I see the power method starts here, and it bounces around and it's converging just like I'd expect. But now, if I turn on alpha and I add some coupling between these two eigenvectors, look at the power method.
00:25:16.054 - 00:25:56.314, Speaker A: Contrary to what we normally expect, that actually makes this common direction, u one and u two. They're almost aligned. That makes that more magnetic. It reels, all those power method iterates in. And so, paradoxically, a departure from orthogonality can sometimes improve your convergence. However, if it's the other eigenvalues that are coupled together, then you can get something that's a little nastier, and the subordinate eigenvectors can be the magnets that slow down your power method convergence. So there's a lot of subtle business in terms of angles between eigenvectors that you don't get to enjoy if you live only in the simulation world.
00:25:56.314 - 00:27:00.056, Speaker A: What I'd like to end on is talking about really a crucial technology that makes Eigs work, this idea of restarting. And it's also a humbling aspect of the problem from the non symmetric perspective. Suppose I have some matrix, complex matrix matrix with complex eigenvalues, and I want to compute those five rightmost eigenvalues. Here's how Eigs works. We first compute some approximate eigenvalues, these black circles, none of which is a particularly good approximation of any of the eigenvalues, but many on the periphery of the spectrum, as we expect from Krylov methods. And what Eigs is going to do is it says, I want five eigenvalues. So I'll just assume that the five rightmost of these approximations are the best, and I'll use the other bad approximations as roots of a polynomial that I'll use to filter out my starting vector.
00:27:00.056 - 00:27:36.818, Speaker A: So I build this polynomial. Its magnitude is large in the gray area and small in the lighter area. So many of those unwanted eigenvalues are now damped out by this polynomial that I apply to my original starting vector. And so, to use him's language from the last talk, if you fix your dimension k, and you do a standard Krylov method in approximate eigenvalues. You're doing a kind of sketch and solve, right? And you hope you get accurate eigenvalues. I would argue that this is a kind of sketch to precondition approach. You're doing some compression of a, you're learning about a.
00:27:36.818 - 00:28:21.824, Speaker A: You're then using that to start an iteration that will filter repeatedly, because, of course, now you've got a better starting vector. And once you're addicted to that, you've got to keep going until you drive the convergence down. Now, weird things can happen. You can make it fail for non symmetric cases. In the symmetric case, we have interlacing, and that's crucial to so many convergence bounds for eigensolvers, for the hermitian case or symmetric case. So what that says is that the iteration gets larger, iteration count increases, we approximate the eigenvalues, the red lines here from the inside out, and the eigenvalues can never bunch up. The approximate eigenvalues can never bunch up in the ends of the domain.
00:28:21.824 - 00:28:46.314, Speaker A: In the non symmetric case, all bets are off. There's no sense that the eigenvalues, well, that they're even real. All the eigenvalues could be real, but these approximate eigenvalues could be complex. They can be outside of the bounds of the extrema of the spectrum. They can bunch up in weird ways, all sorts of nightmare scenarios. Here's the worst of all. The eigenvalues of a are just these numbers here.
00:28:46.314 - 00:29:03.120, Speaker A: Read from left to right. Here's k equals 123-4567. You can make monsters like this all day long using a beautiful construction from Jurgen Deutener, Tebbens and Chard Moran. So, life can be bad, but usually it's not.
00:29:03.272 - 00:29:05.984, Speaker B: So that was because the nerve folding part was used, right?
00:29:06.024 - 00:29:30.052, Speaker A: Yep. Yeah, you could see there's something hidden here in that last column, right? Nothing looks bad until you see that. Okay, so I'm running out of time. I just want to introduce you to a cool set of problems here in the last few slides. Suppose I want to just. This is my rand NLA component of the talk. I take a three by three Jordan block, and I'm going to compute two approximate eigenvalues.
00:29:30.052 - 00:30:14.434, Speaker A: Now, obviously, the eigenvalues are zero. There's no sense of what interlacing would be if the eigenvalues are only zero. But I know that the approximate eigenvalues must always fall in this set, called the numerical range, which, in this case, conveniently enough, is just a disk. And so those are two disks. I'm going to do an experiment where I compute, I think, 10,000 approximate eigenvalues for the left and right, so 10,000 complex subspaces. Every time I do an experiment with one of these two dimensional subspaces, I'll sort the left approximation and the right approximation to zero, and I'll see if there's any pattern. And sure enough, what you see is that the left approximation always falls in this kidney shape and the right in its mirror image.
00:30:14.434 - 00:31:02.140, Speaker A: And indeed, you can draw a line and you can prove using a trace argument from Russell Cardin, that you, you can never get a ritz value sort of left of this line or right of this line. So in particular, I'll skip these examples where two matrices with the three matrices with the same numerical range but very different approximate eigenvalues. Maybe here's the bound to say that there's a kind of interlacing by looking at eigenvalues of the hermitian part of a that allows you to get some strips in the complex plane where your different approximate eigenvalues have to fall. Fall. So it's not as strong as we would like, but it's at least some movement toward an interlacing theorem in the non symmetric case. Okay, it holds for singular values as well. Maybe just some closing thoughts.
00:31:02.140 - 00:31:38.360, Speaker A: Polynomials are better than powers. We really benefit from Krilov methods, as opposed to just subspace methods that just keep one power of a block. Methods hold additional property promises beyond even what I've shown here. Get large subspaces quickly, but you have to find some way to maintain linear independence and handle some of those numerical considerations. Restarting makes these engineering computations work, and if there's a way, they're harder to analyze. But I think that's the secret technology that could help with some of these tricky data science problems shift, invert. You can see what it can do to a problem.
00:31:38.360 - 00:32:14.094, Speaker A: Take something that converges glacially and turn it into something that converges superlinearly if you understand something about where your problem comes from. Finally, non hermitian problems matter. They're solved every day, particularly for interest in dynamical systems. And even though I've sort of caused question about the theory here, I and ARPAC work remarkably well on these problems. The fact that the technology is 20 some years old now and still in use is really a testament to its effectiveness. And I'd love for random matrix theory to tell me something about the eigenvalues of these compressions in the non symmetric case. So if you have any ideas I'm all yours.
00:32:14.094 - 00:32:32.034, Speaker A: Thank you. So we have time for a couple questions. Yeah. So in the version where you run cryloid from the inverted matrix. Yes. Is this something you can. How do you.
00:32:32.034 - 00:33:09.720, Speaker A: Yeah, that's the trick. Right. So there are things called inexact methods that might then use a krylov subspace linear solver like minres, to apply the inverse operator and do it. Exactly. Then the question is to what tolerance need it be applied? So now you think you're kind of only fuzzily applying your operator in this case. And Valeria Simoncini and Danielle Shield have a nice review paper, I think, on this topic, looking at accuracy of an exact creel off methods. Is there a block restarting version? You can imagine how to do that.
00:33:09.720 - 00:33:21.848, Speaker A: Yeah. It's not an AR Pac, for example. Tougher analysis. What's that? Would analysis be even harder? I think a bit. I'd have to think through a little bit of this.
00:33:21.936 - 00:33:29.148, Speaker B: You didn't talk about deflating and recycling Kryno spaces that people in probabilistic numerics are interested in that.
00:33:29.276 - 00:33:37.452, Speaker A: Yeah. So recycling is, you know, you have a series of problems to solve and you have a good Krilov space, and you'd like to sort of build on that wisdom.
00:33:37.548 - 00:33:39.724, Speaker B: That's one way to maybe explain the restarting.
00:33:39.764 - 00:33:39.940, Speaker A: Right.
00:33:39.972 - 00:33:41.884, Speaker B: If you add that to the Krylovsky.
00:33:41.924 - 00:34:26.586, Speaker A: Even in Saad's 1980 paper, he says, basically, he has an explicit restarting scheme where he says, this is basically like block. Lang shows and proposes that idea. Do you have any comments on the feast method? On the feast method? Well, when I said mostly we use subspace projection, feast methods are contour integral ideas that are pretty interesting. You get a Honkell matrix approximation of your compression of your matrix, something Serkan guerrishin and I, our former master's student, Michael Brennan, are very interested in, in the case of nonlinear eigenvalue problems. Some really beautiful ties to system realization theory there. Yeah, it's any class of methods. Let's actually make that the last one.
00:34:26.586 - 00:34:30.554, Speaker A: We'll take the rest off. So thanks again. Thank you.
