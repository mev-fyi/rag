00:00:00.240 - 00:00:00.496, Speaker A: Right.
00:00:00.502 - 00:00:25.874, Speaker B: Thank you, David. Let's move on to the third talk of the sessions. Sorry for delaying this. Both the speakers had to leave and couldn't be part of the discussion, so that's why we chose to take questions. I hope, Negan, you didn't mind. So, yeah. Without further ado, let's have Negin from the Sloan school of Management at MIT talk about her work on online markets.
00:00:27.034 - 00:01:07.464, Speaker A: Thank you so much, Shipra. So, first and foremost, I want to thank the organizer, and in particular Shipra, for the invitation. And thank you guys for attending the talk today, I'm going to talk about online learning with offline greedy, via offline greedy algorithms. And I'm going to talk about some of the application in market design and optimization that fits our framework. And this is a joint version of my great quarters and Joshua, my PhD student, Francisco, Susan and Ashwin. Okay, so I'm gonna start with motivating this work. Can you guys see the second slide? I think that there's a lag.
00:01:11.324 - 00:01:17.744, Speaker B: No, we cannot see the second slide yet. I cannot. At least, I don't know about others.
00:01:18.294 - 00:01:41.954, Speaker A: I think. Yeah, I think that you guys cannot see it. I don't know exactly why. So let me stop sharing and start sharing again, because there might be some issue. Okay. So hopefully you can see it now. Can you?
00:01:43.994 - 00:01:44.774, Speaker B: Yes.
00:01:47.194 - 00:02:42.676, Speaker A: Okay, so then, so I'm going to talk about. Let me motivate this work, and by talking about decision making marketplaces. So, if you think about marketplaces, they have to make certain decision repeatedly over time. So think about online retail stores for them, like, on a regular basis, they have to decide about what items they want to offer to customers in order, for example, to maximize their market share. So another example is product ranking. That in product ranking, the problem is that how we are going to display the products on online platforms in order, for example, to maximize consumer engagement. And it turns out this is an important problem for the online platforms, because as a human, we tend to look at the products that are more visible to us, and as a result, in what order products are being displayed is going to influence the customer's decision.
00:02:42.676 - 00:03:31.824, Speaker A: And as another example, we have reserve price optimization. So think about running second price auction in display advertising, market to sell ads. And in that scenario, one of the important parameters that needs to be optimized in order to maximize revenue is reserve prices. So for all of these problems, we face few, like, fundamental challenges. So the first challenge is that in these decision making processes, we have a lot of uncertainties. So think about the storm planning as a retailer, I don't know the customer's demand in advance. So for product ranking, I don't know how customer, how patient the customers are going to be and how many products they end up looking at to make a purchase.
00:03:31.824 - 00:04:08.854, Speaker A: And then for reserve price optimization, I don't know the advertiser willingness to pay. And on top of that, the environment under which, down the line environment under which I'm making a decision can be time bearing. So for assuming planning, demand can change over time. So maybe before COVID people had different demand function. I know after COVID people are actually interested in buying different things. And for product ranking, the same thing. There may be customers before COVID would like to spend more time on online platforms, did not mind actually spending a lot of time browsing.
00:04:08.854 - 00:04:54.254, Speaker A: But then after COVID, maybe they'll get busier. And for price optimization, we have similar challenges. But these two challenges are not the only challenges that marketplaces face. There's another important challenge, and maybe the most important challenge here, is that let's assume that I don't have any uncertainty whatsoever, then the underlying offline problems that we need to solve in these decision making processes is going to be anything hard to solve. So roughly speaking, this is the case because I have too many options to try. So when I'm offering a solar planning, my options is the number of sets that can be exponentially large. So given these motivating examples and given these main challenges, here are two main research questions.
00:04:54.254 - 00:06:02.454, Speaker A: We want to understand how to design learning algorithm for such combinatorial and timeline environments. And in particular, we want to know if we can transform offline algorithms to the online algorithm with sublinear approximately, can we transform offline algorithms to get good learning algorithms? And the answer to this question, and based on this work, is that yes, otherwise I wouldn't talk about it. So the answer is yes, because for a large class of offline problems that admit a robust greedy algorithm with constant approximation factor, we are going to be able to do this transformation. And in fact, for all of these three examples that I talk about, there exists a greedy algorithm, and all of these greedy algorithms are robust. And I'm going to explain what we mean by being robust, and also they give us constant approximation factors. So in the rest of the talk, what I'm going to do, I'm going to use assortment planning as the running example to illustrate our technique. But then our technique can be applied for other examples that I talk about product ranking and reserve price optimization.
00:06:02.454 - 00:06:51.018, Speaker A: Okay, so let me start with preliminary, and I'm going to walk you through the offline problem. And remember that my focus right now is going to be on just simply assortment planning. So what's the problem? I have end products and my goal is to choose set s with cardinality at most k that maximizes market share, which is just simply the probability of purchase. And I'm going to use this f to denote the market share or the demand under set s. And for many, I think for those of you who know about choice modeling. So for all random utility choice model, this f function is simply a monotone and submodular function. So as a result, my offline problem can be simply be maximizing a monotone submodular function with cardinality constraint.
00:06:51.018 - 00:07:50.620, Speaker A: So this is the offline problem that I have here. And then the offline problem admits a greedy algorithm with the approximation factor of one minus one over a. And this is a, I'm sure that most of you know about this result is a classic result by non historical that they design the algorithm with these approximation factors. But let me walk you through this algorithm, because later I'm going to build intuition based on this greedy algorithm. So the way that this greedy algorithm works is that it has case of problems at every subproblem is going to choose a product Zi that maximizes the, that maximizes the marginal market share, right? So that's, and then this is the, what I mean by marginal market share. This is the market share that you have at the chosen set when you add products I, and this is the market share at the previously chosen set due to the difference in the marginal. And I'm choosing a set product Zi that maximizes the marginal market share.
00:07:50.620 - 00:08:31.636, Speaker A: And after that I'm adding Zi to my set. And at the end I'm returning s of k, which is going to be the final set I'm going to show to the customer. And I think like one of the nice thing about this greedy algorithm is that it actually builds a solution stage by stage on top of each other. And that's what I mean by the greedy algorithm. So that means that when I'm like doing building a solution stage by stage, I do not go back and revisit my decision, I just build a decision on top of that. And that's kind of my, the definition of greedy algorithm in this way. Okay, now what about the online problem? So in the online problem I have trons.
00:08:31.636 - 00:09:08.248, Speaker A: And in every rom t there is a nature, I will call it adversary, and it's going to choose a monotone submodular demand function f of t. And note that this, there's a index like t here. That means that this demand function is time varying. And also this demand function is not unobservable. And that's indicate that the decision maker needs to make a decision under uncertainty. So after adversary chooses this demand function, the algorithm is going to choose a set s of t and then obtains a market share or reward of f of t at set s of t. And then the algorithm is going to get some feedback afterward.
00:09:08.248 - 00:09:52.812, Speaker A: And we focus on two different feedback structure. So full information, which means that algorithm is going to observe the entire function and then the bandit feedback structure, which only means that algorithm only are going to observe f of t at the chosen set s of t. And we have resolved for both of these settings. And of course, bandwidth feedback structure is more challenging. But then to simplify the discussion and today's talk, I'm just going to focus on the full information feedback structure, okay? So, and then this process would continue like from every run, adversary chooses something algorithm to the set and get some feedback. And we have that process up to run capital t. Any questions? Okay, there's no question.
00:09:52.812 - 00:10:42.048, Speaker A: So then let me talk about our goal here in this work. So, we want to minimize regret with respect to gamma times opt. Gamma is the approximation factor, and opt is going to be the maximum market share that one can get if he or she had the benefit of the hindsight. So this is if I know all this, the demand function f of t in advance, and I get to choose one single set having this knowledge, this is the best maximum market share that I'm going to be able to obtain. And then I'm calculating the regret by comparing what I'm getting, which is this summation with gamma times opt. And remember, gamma is approximation factor. And the reason I have this gamma here is because the offline problem itself is so hard and I can only get a gamma fraction of the optimal.
00:10:42.048 - 00:11:35.184, Speaker A: That's why when it comes to the regret, I'm also focusing on approximate regret, which is a very common notion in the, in the learning literature. Okay, so I think now this is a good time for me like to summarize the contribution and main result. One of the nice thing about this work is that we have a general framework to transform offline grid algorithm to a low regret online algorithm via Blackfell approachability. And we have result for both full information study and banded feedback instruction. For full information setting, we get o of a square rooty gamma regret. And then for the bandwidth feedback structure we get t power of 210 and for the maximizing monotone set submodular function, which is the running example for the full information, we can match the best prior bound in the literature. And then for the banded feedback structure we improve it.
00:11:35.184 - 00:12:28.856, Speaker A: Improve the best prior bound the literature. But I think, I don't want to say that this is kind of the main contribution of this work, because the main contribution is the fact that our framework has a wide range of applications and can be applied to a variety of settings. So here's the list of application that we try in our paper and we, we believe that there are more. So we apply to product ranking problem like reserve price optimization, non monotone set submodular function, non monotone strong doctor submodular function and non monotone weak doctor submodular function. So and then in all of these cases, either we matches the best prior bond or improve that, or we are the only one like that provider bond for this problem. So let's look at the results. So for the full information setting, we always get in terms of dependency in t we get a square root t dependency.
00:12:28.856 - 00:13:20.594, Speaker A: And for the bandit, so we get to the power of two third dependency if our decision set is discrete, for example for product ranking. So ranking something discrete, we get to the power of two third. But whenever like the decision set is become continuous in that scenario, I think there are some questions here. Oh, so good question. Are there any lower bonds for these problems? I think like as far as I know, for example for, I'm not aware of like in terms of the dependency on t, I think for full information, square is tight for the banded feedback structure. I don't know if it is tight, but then as you can see, this is kind of like in majority of cases we outperform the best known bond, the literature. Thank you for your question.
00:13:20.594 - 00:14:19.154, Speaker A: So then for the continuous case, because for example, for price optimization, we need to, we need to also do discretization and discretization is going to, because of discretization error, our regret, one is going to get a little bit worse. We get to the power of four fifth. And couple of things that I want to highlight is that if you think about the banded feedback structure, the bandit feedback structure, capture more realistic scenarios, but then you don't see, and it's also more challenging to attack. But we don't see a lot of prior results in the literature and we are very excited to see that. We are very excited to see the result in the literature, to contribute to the literature by providing for the bandit back instruction I think for some reason you guys don't see that. Again, I think my video cut is not there. The sharing thing.
00:14:19.154 - 00:14:22.434, Speaker A: Sorry for the comments.
00:14:25.654 - 00:14:31.074, Speaker B: Are you using iPad or using iPad? Yeah, sometimes it freezes.
00:14:34.414 - 00:15:27.114, Speaker A: So now I think I also want to talk about related work and kind of to put into context to say how this work contributes to offline to online transformation. For NPR problems, I think the first and foremost, there's this negative result in the literature saying that for general combinatorial problems, you cannot get sub linear regret. Okay, so. And that kind of justifies that why in this world, we focus on a class of NP hard problems, that they're amenable to some greedy algorithm with approximation, with constant approximation factor. And there's also like a nice work by column van Paul 2005 and Dude 2017, that they provide the algorithm based on follow the perturb data. And their approach only works if you can solve the offline problem efficiently. But for our case, this is definitely not the case, because we are dealing with NP hard problems.
00:15:27.114 - 00:16:35.764, Speaker A: And for, and then there is a great work by Kakaday told that they show that if they, if the NP hard is, and they can handle NP hard problems, but then for them, it's very crucial to have linear rewards, which is, again, this is not the case even for the maximizing sub modular function. There is also a large body of literature and combinatorial learnings. And then for combinatorial learning, I think this literature is pretty mature, but the focus is on linear reward structure, which is not the case for us. So our contribution is that we focus on NP hard problems with a nonlinear reward structure, and then we handle both bandit and full information setting. And we were able to transform offline grid algorithms with their online counterpart that gets sub linearly grip. Okay, so now I think we are getting to the exciting part of the talk, like talking about high level ideas and presenting the algorithm. Okay, so to explain the high level ideas, I'm just gonna first revisit the greedy algorithm, present some of the properties of the greedy algorithm, and then based on that, I'm just going to tell you about the high level idea and present our algorithm.
00:16:35.764 - 00:17:18.520, Speaker A: So, if you remember, grid algorithm has k sub problems where in subproblem I, we choose a product zi that maximizes the marginal market share. And delta if. And I just introduced new notation just to make everything simpler. Now, I'm going to say the fact that the greedy algorithm chooses such product is equivalent to this payoff vector to be greater than equals zero. So let me walk you through this kind of parse this vector for you and look at the first element. So if you look at the first element, this delta f of zi is the marginal market share at the product that the greedy chooses. And delta f of leg one is the marginal market share of the product one.
00:17:18.520 - 00:17:53.806, Speaker A: And this should be of course greater than equal zero because the difference between these two, because the greedy algorithm chooses the product zi that maximizes the marginal market share. And then the same thing, following the same logic, you can say that all these like elements in this vector should be greater than equals zero. And then this is equivalent to say that the greedy algorithm chooses the product Zi that maximizes the marginal market share. So, but there is a, like a minor issue here. Like it's nice that I wrote this condition. Later you will see this why this is nice to write it as this in this form. But there's a minor issue that this vector payoff is not linear in the greediest decision zi.
00:17:53.806 - 00:18:38.144, Speaker A: And I'm just, I need, you're gonna find out why this is important. I'm just gonna do it like a simple trick to make it linear. And then after that, talk about another property of the greedy algorithm. So the trick I'm going to do is that for subproblem I, instead of returning the product Zi, I'm going to return a distribution theta I over the end products that I have. And then what is this distribution? Let's say I only have one product, only one product that has the highest marginal market share. And then in that distribution I just simply put the mass of one for that product and put it like zero for the rest of, okay, nothing major is the same thing. But then, now let me write this vector payoff in terms of theta I.
00:18:38.144 - 00:19:48.928, Speaker A: So if I do that, if you think about the yellow part that I highlighted, this is the expected value of the marginal market share at the greedy solution, right? And then again by the same logic, I know that because greedy algorithm go with a solution that maximizes market share, this vector payoff that I defined is going to be again greater than equals zero. And why I did that, because now all of a sudden the vector payoff is linear in the greediest decision theta I, which is a nice property to have. So now let me also talk about a very nice property of this greedy algorithm. And in fact, I'm going to show you that discrete algorithm is robust to local errors. So what does it mean? Let's say I have an errorless system and for every subproblem I and every coordinate j, I managed to make sure that this vector payoff that I have at the solution that I'm returning for every element j is going to be greater than equals zero. Then by non Hester's result, we know that if you satisfy that, you are going to get one minus one over the approximation factor. So that's what I mean by errorless system.
00:19:48.928 - 00:21:09.340, Speaker A: Now let me add a little bit of error to the system in a sense that instead of theta, I'm going to replace it with this noisy version theta I tilde, such that if you look at the payoff vector at the theta I tilde and look at like any j coordinate, this, this may not be greater than equal zero as I wish for, but then with a little bit of error, this epsilon, with a little bit of like error, epsilon is still gonna be greater than equals zero. Okay? So that means that I satisfy this condition that I have over there like kind of approximately with some error. So if I guarantee that I satisfy this condition, so then the greedy algorithm, the solution that greedy algorithm is going to return is not going to be too far from gamma approximation. It's only going to be epsilon times k away from the, from the, from the gamma approximation, which is a great thing because, why this great? Because greedy algorithm needs solution on top of each other, right? So I want to make sure that if one of the, in one of the stages I have some error, so the error does not propagate, it just simply adds up together. And it turns out that this greedy algorithm already has that property. In the greedy algorithm, this greedy algorithm, the local errors do not propagate, which is quite nice. But that's not all.
00:21:09.340 - 00:22:02.046, Speaker A: In fact, like this crazy algorithm is amazing that in a sense that it's extended robust. So think about like a noisy run of this algorithm over trons. And let's say that for every run I have like a noise and I returning some noisy version of theta it, and then this, the noisy version satisfy this in quality, that if I add up this vector payoff across all the time periods and look at the coordinate j, with the little bit of error, which is a function of t is going to be, this is going to be greater than equals zero. Let's assume that I satisfy this condition that with a little bit of help, with a little bit of error of t, I satisfy this inequality. Then in that scenario it is guaranteed that I can satisfy this inequality. And what is this? This is what your algorithm gets. This is a gamma times what the optimal gets.
00:22:02.046 - 00:22:44.474, Speaker A: This is kind of the benchmark because s can be any set, including the optimal. And this is kind of the regret part. This, how far you are from the, how far you are going to be from the, from the benchmark. And at a high level, it means that if the aggregate error over t runs for every coordinated is still a small, the algorithm is still going to do well. In other words, the error again is not going to propagate if the algorithm satisfies this property. We are going to say that this is extended robots and not every greedy algorithm is going to have that property. But there, as we are going to, I'm going to show you there are many greedy algorithm that has that property.
00:22:44.474 - 00:23:40.966, Speaker A: I think this is maybe the most important of the most important slide of this talk. I'm going to talk about our high level idea. So remember, for the offline problem, for every subproblem I, I wanted, I return a distribution theta I such that this vector payoff at the theta I and this delta I have become greater than equals zero. So that's what I wanted to do. Now for online problem, of course, I don't know the demand function f of t. And still I need to make a decision. What I'm going to do, I'm making, I want, I'm interested in making sure that in subproblem I, I return some kind of noisy version of theta I such that if I'm adding up all this vector payoff and look at coordinate j with a little bit of help, like of error t with a little bit of error, I'm going to still be sat in a to show that this is going to be greater than equals zero.
00:23:40.966 - 00:24:21.914, Speaker A: Okay. And why this is good? This is good because if I make sure error of t in the order of square root t by extended robustness property, it is automatic that I'm going to get a square root, the gamma regular. Okay. So isn't it amazing, just by satisfying, making sure that every subproblem I can be done with kind of manageable error in the order of a square root t, by extended robustness, I automatically, I'm going to get a square root t gamma degree. And that's kind of the whole idea. But I think there's a main question here, how I'm going to design an algorithm for every subproblem that with the error of t in the order of square root t. And the answer to that question, the Blackwell approachability.
00:24:21.914 - 00:24:47.654, Speaker A: So we are going to use Blackwell approachability to handle that. And let me walk you through the Blackwell approachability. And I'm sure that some of you have seen that it's a two player zero sum game with a vector value reward. In every runty the play do players play their actions. P one obtains some reward and then p two obtains a negative of that. Two things that are important, reward is vector valued and reward. Vector is going to be by a fine.
00:24:47.654 - 00:25:34.042, Speaker A: So, in the Blackville game, p one wants to approach a convex set and p two does not want this to happen. And we say convex that is approachable, meaning that player p one can get close to that. If there is this for player p one, there is a strategy that no matter what p two does, the distance between the average vector value reward and set s is going to be less than g of t, where g of t goes to zero as t goes to infinity. So that means that I'm just as a player one, I get closer and closer to set s as t goes to infinity. And, but of course, not every target set is going to be approachable. And there's simple things that simple tests that you can do to see if the set is approachable. I'm not going to bore you with that.
00:25:34.042 - 00:26:14.164, Speaker A: But what is important is that if set as is approachable, you can approach it with this rate one over square root of t. And there are algorithms that allow you to do that. And I'm going to call this algorithm algebra, is the Blackfell algorithm that helps you to approach this target approachable set with this rate of one over a square root t. Okay, so let me go back to our high level idea. So remember, I wanted to make sure that error of t is going to be in the order of a square root t. The way I make this happen is that by letting Alex B algorithm, this Blackwell algorithm, handle every subproblem. Okay, so this is the way I'm going to do that.
00:26:14.164 - 00:27:04.204, Speaker A: And if this is the case, I need to define some like black field game. For every subproblem I, which is which is doable, player p one is going to be the algorithm, player p two is going to be the adversary, which returns the delta f. There is a vector, payoff vector, which is exactly which is by affine due to this, because of the transformation id. And that's the reward vector. In the Blackfield game, I have a target set which is the positive ordnance, because I want to approach that set which is approachable. And then if this is the case, the algorithm allows me to approach that set with the rate of one over square root t, which is correspond to error term in the square root t, right? So I think kind of is magical. So then I would able to, by handling every subproblem with the black field game, I would be able to get error time in the order of a square root t.
00:27:04.204 - 00:27:45.374, Speaker A: So let me put everything together. Every subproblem is being handled by a bilateral game. So I have algo b one, algo B two, etcetera. So after that they make a decision, pass their decision through the greedy algorithm, and then after I'm getting a feedback, I pass the feedback to them, they obtain themselves. And then I know by this, I know the error term for the RB is going to be in the order of a square root t. And then because of the extended robustness, if I, if I have this, consider all the error terms of different RB algorithms, these Blackfell algorithms, the total regret one is going to be k times that, because the error does not prove again. And kind of, that's the gist of the algorithm.
00:27:45.374 - 00:28:53.684, Speaker A: And in fact, we can go beyond assortment planning. If I think, if I have a greedy algorithm that is extended robust, meaning that the error does not propagate and it's Blackfell reducible, in the sense that for every subproblem I can define a Blackfell gain with biafine vector payoff and with approachable target set. In that scenario, I'm going to be able to get a square root, theoretically. And it turns out that for many of these problems that I talk about product ranking, there are greedy reserve price optimization, monotone set, sub modular function, and for continuous sub modular function there exists algorithm that has this old property, extended robustness rebalak reducible. And then by applying our framework to these settings, that's the result we are getting. And let me highlight that I think the result that we get for continuous modular function is quite interesting, because for weak doctor, there was no result in the literature prior to this work. And then for the strong doctor, we significantly outperform the existing one, even in terms of the approximation factor.
00:28:53.684 - 00:29:36.808, Speaker A: So we get one half approximation factor and which is the best thing, and then they get one four. Okay, so let me wrap up. So today, talk about transforming offline grid algorithms to their online counter points using Blackfell approachability. So we need the greedy algorithm to be extended robust, so that when it builds a solution on top of each other, the error does not propagate too much. And that assumption is satisfied by many greedy algorithm. And we want the greedy algorithm to be Blackfell reducible, so that for every subproblem we can define a Blackfell game and let the Blackfell game handle that particular subproblem. For full information studying, we get a square root to regret.
00:29:36.808 - 00:30:10.814, Speaker A: And for the bandit, we get to the power of two thirds and we show that our framework is flexible and can be applied to many application including product ranking, razor price optimization and submodular optimization. We are hoping that people can find more application. And thank you so much for listening to my talk. This is the link to the paper and this is my email and I would be delighted to take any questions. Thank you. Thanks again.
00:30:10.854 - 00:30:48.034, Speaker B: That was very interesting. I think I have one quick question, or maybe not so quick, I don't know. We have a 1015 minutes for discussion. Since the other speakers couldn't join for the discussion today, my question is like can you give a quick insight into like what was greedy, what was the property of greedy algorithm that was crucial here? Or is it any algorithm that processes, any offline algorithm that processes one input at a time? Is that like a local kind of nine? Is that.
00:30:48.614 - 00:31:37.514, Speaker A: Yeah, I think what you need, yeah, I think the main thing for the main property, there are a couple of properties that we need. So the main thing is the robustness, because if you think about the gritty algorithm, it has like several like sub problems. But you go from subproblem one to two, etcetera, and then up to like k. And then for, for such a creative algorithm, let's say that in the online learning you are not going to have handle the subproblems accurately. So whatever like decision you make in that subproblem might have like some error in a sense that you are not going to exactly follow the thing that you want. That's another, you want to make sure that the error you are making in every subproblem does not propagate and become like too large. And that property is not going to be satisfied for every greedy algorithm, for example for digestion.
00:31:37.514 - 00:32:42.752, Speaker A: So you are not going to have this robustness to local errors. So the first like main property we need is robustness to local errors. The other thing is that the, like when I was walking you through this vector, so when I, when I walk you through this, so when I walk you through this like a high level idea. So one thing I need to have is that for this subproblem, I need to define a black belt game for that. And then, so, and that's, that's also important because I need to define some biafine vector value pair and then I need the, I need to make sure that they said that there's a convex set, which is approachable in all of our application, the convex set, which is simply positive or so as long as you have these two properties, robustness to local errors and making sure that every subproblems can be handle by a black belt game. You can define a Blackville game for that. And yeah, you can handle it.
00:32:42.752 - 00:32:48.088, Speaker A: A problem by Blackville algorithm, you can apply our framework. Does it answer your question?
00:32:48.216 - 00:33:26.234, Speaker B: I think like my question is, so these, these things you need on top of greedy, right? But what if I replace greedy, but, and give you these two properties? So I, if I like, is there something about greedy that you're using? For example, if I have another algorithm like some kind of gradient descent or let's say Frank Wolf or something that is local update, kind of incremental update kind of method that can be applied online and you could ensure these two properties. Like is there something crucial about greedy that you're using here?
00:33:26.354 - 00:34:02.014, Speaker A: Good question. I think one thing which is, I haven't looked into the details, but I think one thing which is crucial here is that the number of subproblems we need to solve for the offline problem should be also bonded. So I don't know, like for example, if you apply to the gradient descent or mirror descent algorithm, if you know exactly how many steps you need to take for the offline setting when you don't have any uncertainty to reach to the point, to the minimum that you want to get in that scenario. Yeah. So you can apply our framework. There's nothing special about the, the greedy algorithm. Any local improvement is going to be good.
00:34:02.014 - 00:34:18.670, Speaker A: But then I think we have a dependency on the number of subproblems. So you need to have that. So if you know that the number of subproblems for the offline problem is bonded. Yeah, you can, you can apply that. As for all of our studies, somebody.
00:34:18.702 - 00:34:19.638, Speaker B: Else had a question.
00:34:19.766 - 00:34:24.112, Speaker A: Yeah, go ahead. Yep. Hi.
00:34:24.208 - 00:34:26.232, Speaker B: No, no, I was just saying somebody else had a question.
00:34:26.288 - 00:35:17.706, Speaker A: Yeah. Okay. Yeah, I had a query with respect to the federated differentiable online settings and how can the biases be corrected in them? Okay, so for federated distributed learning. So every, there is a, every like node is going to make a decision on its own. And then I think that probably our framework can be applied there. And because you are going to have limited steps that you are going through and then let's say you are going through those steps in a sequential fashion, like for example, node one, make a decision and then node two, etcetera. And if you do kind of your loss function is robust.
00:35:17.706 - 00:35:57.752, Speaker A: Like if you, for example, node one making a decision and then overall, and then node two also make some like maybe a bit like a noisy, noisy, noisy decision. And overall, if that error does not propagate and gets too big, it just adds up. I think that this framework can handle that. That's kind of like my guess. Okay, thanks. First, like, it depends on the specific problem that you want to also make sure that the, the problem that every node is solving, this subproblem, think about like the, what are every node solves like a one sub problem. So you want to make sure that this subproblem is also black, well, reducible.
00:35:57.752 - 00:36:24.464, Speaker A: You can assign a black belt to game to that. So I'm sure there might be like some application on the federated learning that satisfy that, but I haven't looked into that. That's a good question. Yep. Okay, thanks, I got it. Any other questions?
00:36:31.124 - 00:36:34.100, Speaker B: Sudeep, do you have a question? Your mic is on.
00:36:34.292 - 00:36:40.314, Speaker A: Oh no, I was prompted to unmute, so I don't know why that happened. I had no questions.
00:36:41.374 - 00:36:42.110, Speaker B: Okay.
00:36:42.222 - 00:37:36.534, Speaker A: I think like you maybe go ahead. I think like Chipra, that you asked some question about the mirror decision. I think this is interesting. One thing I want to say in all of the application that we consider the sort of subproblems that we need to go over, like in the offline problem is deterministic and it's known ahead of time, probably like in the mirror decent algorithm, you maybe you know that with high probability, with this number of steps, you're going to kind of converge to the solution that you want. And so maybe it needs a little bit of work, how to take care of the randomness in the number of steps that you need to make in order to reach, reach your solution. Because for us, every great algorithm we consider has a fixed number of like subproblems and then. Yeah, but then potentially can be extended.
00:37:41.114 - 00:37:54.890, Speaker B: Right? Like I think like, yeah, it will be interesting to see if any incremental algorithm, something that processes one part of the input at a time, not necessarily in a greedy way, but, but there could be other ways to.
00:37:55.042 - 00:37:55.586, Speaker A: Yeah.
00:37:55.690 - 00:37:59.410, Speaker B: To, to have an offline incremental algorithm and.
00:37:59.522 - 00:38:01.234, Speaker A: Yeah, that's a good question.
00:38:01.274 - 00:38:16.122, Speaker B: Yeah, greedy has this nice structure that it's natural to take it online because. Yeah, yeah, right. Do you have any insights on like future directions or things you would like to work on in this?
00:38:16.298 - 00:39:49.744, Speaker A: Yeah, I think like this is an area which is kind of, I think has a lot of potential because the setup papers there is not, uh, is not huge. I think like one of the one thing that I'm interested is that is there any another class of algorithms that not necessarily greedy, but any other white class of algorithms that we can do the transformation? And also another thing that I'm interested to look at is that is there like a black wall, black box algorithm that just, instead of me looking at the details of the algorithm, that's what I did here, right? So I look at the like algorithm, the details of the algorithm, try to define some pay off vector for that and map it to some black real game. Is there like some way that I can look at this problem in a black box, black box manner that I just give you, like you give me any algorithm and just tell you that how you can convert it to the, to the online version instead of just looking to details of the algorithm? I think that would be also an interesting direction to pursue. And also like what you mentioned is also interesting to see that at some point I thought about it, but then I don't, for some reason I kind of stopped. But yeah, I think if you, if there are some like gradient based type algorithm that you don't know how long it's going to take you to a stop, but it has some kind of gridding nature because you improve your solution a little bit by a little bit at the time. So then can you say something about the designing of the algorithm?
00:39:58.544 - 00:40:07.404, Speaker B: Right. If there are no more questions, I think we are towards the end of the session, unless someone would, someone has any last questions.
00:40:11.224 - 00:40:12.648, Speaker A: And then I want to say thank.
00:40:12.696 - 00:40:16.848, Speaker B: Thank you for all the three speakers and. Yeah, yeah, go ahead.
00:40:16.976 - 00:40:34.674, Speaker A: I just want to say this paper is under review and I appreciate the feedback and then, because. Easy to improve it. Yeah. If you guys have questions or comments, feedback, just feel free to reach out to me and yeah, we'd be more than happy to receive those feedback. Thank you.
00:40:36.454 - 00:40:49.554, Speaker B: Yeah, thank you very much. Yeah, it has been a very interesting session and I'm sure you'll enjoyed it as much as I did and hope to see you in the next session in half an hour.
00:40:49.854 - 00:40:51.494, Speaker A: It's not all halls.
00:40:51.534 - 00:40:52.134, Speaker B: Thank you everybody.
