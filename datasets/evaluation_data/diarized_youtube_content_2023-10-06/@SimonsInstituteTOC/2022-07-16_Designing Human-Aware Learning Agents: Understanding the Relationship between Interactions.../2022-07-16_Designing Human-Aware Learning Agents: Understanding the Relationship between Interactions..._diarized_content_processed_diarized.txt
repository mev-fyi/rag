00:00:02.280 - 00:00:03.108, Speaker A: This works.
00:00:03.694 - 00:01:00.734, Speaker B: Hi, my name is Yuchen and I'm currently a postdoc at Stanford working on interactive robot learning. And also I graduated from UT Austin last year. And it's interesting we're in the same panel, but yeah, I actually was part of that reviewing committee for two years. And yeah, if you're interested, I can talk about my experience there. Yeah, anyway, that's not going to be my talk today. So today I'm going to give you a brief overview of what I work on and then zooming into a particular project that I think is going to be of interesting for the discussion within this workshop. So I work with robots, and industrial robots have revolutionized manufacturing in the past 50 or so years, and they are often tasked to work in a fixed environment and on a single and very specific task.
00:01:00.734 - 00:02:16.562, Speaker B: So when we think about bringing the same level of automation into human centered environment like hospitals, hotels and our own homes, new challenges arise. So the first one being that now the environment we expect these robots to work in are going to be unstructured, dynamic and diverse. How do we tackle this? Let's give them the ability to adapt and to learn. After deployment, we think about the problem of robot learning, considering three different entities, the human user, the robot and the environment. The robot takes actions that change the state in the environment and receives consequent observations from the environmental team. So since it's becoming possible for a robot engineer to start program the robot for every single possible scenarios the robot's gonna need to work with, we have a human user come in mind. So they usually come with the task they want the robot to do in mind in order to actually allow the robot to adapt to the environment.
00:02:16.562 - 00:03:37.062, Speaker B: After deployment, we can leverage this human user to provide data to the robot. And traditionally this is in the form of demonstrations. Traditional learning from demonstration methods allow a user to program robots by providing kinesthetic demonstrations, which is physically moving the robot, or teleoperate robot, like using some controller or nowadays even VR to control the robot. However, I would argue that providing these kind of demonstrations is a high effort form of teaching. And when robots nowadays are faced with non expert human users, this kind of high effort form of teaching is not no longer user friendly. So the research I've been working on focuses around this question of how can we enable robot learning from low effort form of human teaching? And in order to do that, we think about this human in the loop robot learning problem by enabling the robot to take an active role in its own learning process. So enable this interaction between the robot and the human user.
00:03:37.062 - 00:05:06.634, Speaker B: So the robot can provide queries to the human user and receive feedback such that it makes better use of the human user's effort. And formally, the problem of robot learning is often modeled as Markov decision processes, where the robot has a set of possible states and a set of possible actions it can take to move in between these states. The state transition dynamics dedicates how the robot moves from one state to another given an action. Lastly, we really have this notion of a reward function that actually defines the task the robot needs to learn. The robot has a policy that maps states to actions, and the goal of the robot is to learn a policy that, in expectation, maximizes the accumulated reward over time. So when the human user comes into picture in practice, oftentimes we don't actually have this reward function from the environment, and rather this reward function is somewhere in the human's mind. We make this assumption that what the human desires can be captured by some underlying reward function, but it doesn't necessarily mean the human actually knows the reward function.
00:05:06.634 - 00:06:01.796, Speaker B: Now, the goal of the robot becomes to learn this policy from interaction with the human, who actually may not even observe this real function themselves. In a recent survey paper, with collaborating with colleagues in Carnegie Mellon, we are thinking. We are trying to argue that in order to analyze the effect of an interaction design on the learning outcomes, we need to think about how interaction design affect the training data we generate for these robots. We argue that there are two pathways. One is direct, which is the optimal informativeness of an interaction query. Assuming we can receive expert feedback, this is what we designed the interaction to be. So, as an engineer, we proposed an algorithm.
00:06:01.796 - 00:07:21.024, Speaker B: We know what kind of questions the robot can pose to the human and in what form we're receiving these feedback, and we can optimally compute what's more informative given model parameters. However, there is another pathway that once we design this interaction, it has an effect on the human user, and that effect is indirect and oftentimes much more complex than what we can imagine at the time. We designed the algorithm. For these two reasons, it's not easy to actually design a perfect interaction that will work with any user at test time. So the work I did throughout my PhD tried to tackle the problem of learning from low effort human teaching under both pathways. Under the first one, we think more about how we can generate informative queries. Because this is direct, we can just find a metric we want to optimize, and then think about how we develop an algorithm to actually optimize for that magic.
00:07:21.024 - 00:08:20.214, Speaker B: And in the direct pathway. We need to think about how this interaction design actually has an effect on the human user who's going to actually interact with the robot. And from that end, I've proposed different forms of interactions that the user can use to interact with the robot. So for the rest of the talk today, I'm actually going to focus on just one project that's on the idea of learning from implicit human feedback. So this is collaborative work with Chiping, Alessandro, Peter, Scott and Brad. And at that time we were associated with UT Bosch, and some of us with Bosch. So the idea of natural implicit human feedback is that humans naturally react when we watch other agents perform tasks.
00:08:20.214 - 00:10:01.824, Speaker B: And for a learning agent, when it sees other humans react to its performance, can it actually leverage their reactions to learn something about its own performance? So we proposed this learning framework, evaluative mapping for effective task learning from human implicit use showed it as empathic. It has two stages. In the first stage, the robot learns a mapping from implicit human feedback to a fixed set of task statistics we designed. And the important part of the first stage is that since we designed the task, we need to incentivize the human user to actually align their internal evaluation of the task with what we specified the task to be. So once we do that, actually, to give you a concrete example, in our user studies, we just tell our participants that their payout for participating in our user study is going to be proportional to the performance of a robot. So that's the how we incentivize our participants. And once we have that, we can actually record data and learn this mapping of reaction features to task statistics, so that at a test time, given a deployment task without any task specifications, we can just learn from observing the reaction of a human.
00:10:01.824 - 00:11:27.550, Speaker B: So this is the model we instantiated the empathic framework with. And more specifically, we were looking at learning from human facial actions. So we record videos of our participants and then we extract the features like their facial action activations, and as well as the frequencies of the feature movement in the horizontal and vertical directions. So those will correspond with head motion of like knot and shakes. And with this model, we were able to learn a mapping in a simulated task and then transfer that to this robotic trash sorting domain. And yeah, hopefully there is also sound. So that's it.
00:11:27.550 - 00:12:21.144, Speaker B: So with this model, we learned we were actually able to rank the trajectories in this particular robotrash sorting domain. So all the ones color green actually resulted in positive returns like the one I just showed. So it sorted the crack object and the one with red is sorted. The wrong object, the ones with yellow, it just didn't end up picking up any object. So we see that by just looking at the facial reactions of these participants, we were able to tell the ones color green should be ranked higher than the ones colored yellow or red. So from this project, actually, we realized there are so many challenges just in designing effective user studies, and I'd like to share some of that with you all today. I think that's more relevant to the discussion of this workshop.
00:12:21.144 - 00:13:49.794, Speaker B: The first one being that human subjects are very complex and their behaviors shift when even a single factor of the condition changes. And it's well known that when subjects are aware that they are part of study, their behaviors may become different from when they are naturally interacting with robots. So this is kind of like conflicting with what we initially targeted for, because we actually want to be able to learn from natural human reactions. But once we put humans in this study room and putting a camera in front of them, they start to become aware of how they're doing is going to be used. I don't know if that was the reason that they were so reactive to our agents or that's the reason that they wasn't behaving very naturally, but we see a lot of challenges of basically deploying this model outside of the laboratory lab settings. Another issue is that while we tried our best to incentivize our participants, oftentimes they are not as invested in the task or they don't care about the robot performance as much as we want them to do. And we also learn this by trying to do the same task through mechanical Turk like during the pandemic.
00:13:49.794 - 00:14:54.874, Speaker B: And it turns out mturkers are just trying to maximize their throughput of tasks so that they can maximize their payout. At the end of the day, they sometimes don't even pay attention to the tasks we were showing them during the study. And last but not the least, is the fact that humans are highly adaptive, and the interaction dynamics between the human and robot may change over long term interactions between them. This is basically saying that our model learned today, may not be applicable in the long run, because as a society, we are getting more and more familiar with robots, and our reactions to their behaviors can change over time. You probably were surprised the first time you saw a roomba moving, but not like 50 times, too. So with that, I'd like to mention two future directions. I'm good on time, and the first one being developing foundation models for robots.
00:14:54.874 - 00:16:05.744, Speaker B: So I've been talking about this with one form of low effort teaching and all of that needs the robot to do a lot of learning ahead of time. And that's the idea where we adapt existing large scale language and vision models for the robot to do semantic understanding so that hopefully one day it can start taking natural language instructions and ground that to its own actions, so we no longer need to provide physical demonstrations. And yeah, following that direction, we actually are interested in how do we do online adaptations and personalizations after the robots are deployed. But actually, give me one more minute. I'd like to end on a note on human babies. I recently became a mom and just realized how helpless human babies are when they are born. And by some research estimation that human mom would have to gone through pregnancy period of 19 to 21 months if the human baby is to be born at the same developmental stage as a chimpanzee newborn.
00:16:05.744 - 00:16:30.854, Speaker B: So some biologists, swiss biologist Adolf Portman hypothesized that human beings are born so early because they need to embed their development and processes in interactions. So learning from interactions is an important part of human intelligence. And yeah, I'd like to just on that note, thanks.
00:16:39.994 - 00:16:47.854, Speaker C: For questions. Let's get the mics happening. You can keep that mic on.
00:16:56.404 - 00:16:56.948, Speaker D: Thanks.
00:16:57.036 - 00:17:58.632, Speaker C: My question's for you, Chen. You started off by talking about how robotics and automation had revolutionized manufacturing in the last half century or so, and then said that the challenge is how do we adapt that to an environment that's unstructured, dynamic and diverse in domestic spaces and all that. And they got me thinking that it's not a state of nature for labor to be structured, stable and homogeneous. That's a design choice and a managerial and political strategy that doesn't just happen by accident. And so I'm wondering if in moving, I mean, what you're talking about is automation essentially, if we're moving automation into domestic spheres, is there a chance that, that certain design values, managerial strategies, political choices, get imported over there? So rather than the robots simply adapting to a static state of nature, which is more diverse and different, that we're actually changing the nature of the space in which those robots or automation are inhabiting. Does that make sense?
00:17:58.728 - 00:19:19.518, Speaker B: Yeah, take that. Now, should I answer? Oh, yeah, yeah. So, yeah, that's an excellent question. So, for me, I think more of it as, should we adapt robot to human environment, or we should actually adapt human to robots and, or, like, more relevant to the autonomous, like very close, like, should we actually build infrastructures that they operate more smoothly or we actually make them to work with what we have currently? So I think, like, if possible, things should go both ways. If you look at how Amazon have these home robots, they're trying to sell to a selected set of customers, and there they are trying to filter what kind of home you have, whether you have glasses at large, window at home that the robot cannot perceive or how big is your floor, and whether you have small bumps that the robot cannot rollover. So they're doing these things already, like by trying to filter what the target deployment domain is. But what I'm trying to argue is that at the end of day, your robot is going to interact with a human that's going to use it.
00:19:19.518 - 00:20:19.948, Speaker B: And since at that point the robot might be tasked to do something that the human have specific in mind, they need these learning abilities to adjust to their user's preference. One example I could give is even for autonomous driving, this is really just speaking up from personal experience. Me and my husband, we drive this Tesla, and actually whenever we turn on the autonomous driving, we each have a different way of trying to. Different ways of feeling uncomfortable with it. It tries to drive in the middle of the road, which actually is nice. But for me, I prefer to drive a little bit closer to the left because I feel like I react faster to the end I'm closer with. But my husband actually has this preference of driving to the right because that's much, he feels like it's much likely to collide with oncoming tasks from the other side.
00:20:19.948 - 00:20:30.536, Speaker B: So users have drastically different preferences with autonomous very close. So, yeah, I don't know if that actually answered the question, but that's, yeah.
00:20:30.600 - 00:20:55.962, Speaker C: Talk about it after, perhaps we might take actually probably still one at a time. Okay, great. Yeah. Thanks for two really great papers. My question is for you, Chen as well. So just in your reply right now, you mentioned, you know, should we adapt robots to humans, or might we need to adapt humans a little bit to robots? Right. It's a back and forth process or like two directional process.
00:20:55.962 - 00:21:31.998, Speaker C: So I'm wondering if you could talk a little bit more about how would it ever be possible to disentangle reactions of humans towards robots from natural reactions? Because in the scenarios that you described, you see people in these very contrived artificial scenarios and you talked about that. Right. But, you know, how can you ever disentangle this? Like, how can you ever produce a data set of natural reactions in an artificial setting as a baseline for then, you know, building onto. Does that make sense?
00:21:32.166 - 00:21:34.114, Speaker B: Yeah, yeah, I think.
00:21:35.814 - 00:21:36.174, Speaker E: So.
00:21:36.214 - 00:22:27.102, Speaker B: That, I don't want this to sound like, unethical, but like I feel like we really need dataset in the natural settings, but the problem is that we don't already have robots interacting with robots from on a daily basis. That's the target deployment setting. So, like, some of this, we might need to come from studying human interactions. So by just understanding how human pickup signals from each other, maybe in a, like, more theoretical and generative way, you can build these more, how do I say, like, parameterized models. By understanding our own interactions and use that for the robot. And once we have that robot, we can start collecting, like, data sets. That's more natural.
00:22:27.102 - 00:23:14.254, Speaker B: Like, once it's already deployed in interacting with the human. And there, like, there is still this controversy of, like, whether we should actually let the human know the ways the robot is processing their data and if. Whether the human is going to be comfortable with that or not. But, like, at that point, I feel like that's going to be the new norm. Like, that's basically the way we're interacting with our cell phones and the way we're interacting with tvs nowadays and those, like, at some point, that's going to become more or less stable and all. And also, if we make robots personalizable and adapt to individual differences, at that point, we can start to rely on the data we receive. So that's the hope.
00:23:16.354 - 00:23:51.444, Speaker E: So my question is for Serena. So using the machine learning and the applications, of course, anyone who's in admissions could see the burden of going through that. And I know you didn't have the full justice of time to really present the case, but was it that the quote about being shocked? Did they not do anything, like, take 50% of the applications and try it with a new procedure and just kind of have any test of how the performance was versus the unassisted human by not giving it the ranking? I'm just incredulous by this case.
00:23:52.504 - 00:24:12.010, Speaker A: Yeah, that's an excellent question. So I want to preface by saying that I think probably going to the original paper is, like, better source material and, like, full details of that. But they. And, yeah, maybe actually, you can. Might even know about the. I mean, my view of it is only from the lens of that paper. I wasn't actually inside the room when any of the decisions were made.
00:24:12.010 - 00:24:51.854, Speaker A: But in the paper, they do talk about ways that they tried to go about that validation process. So I think they did do some kind of test where they, like, split up some of the applications for, like, just human review and then some for, like, human plus the. The, like, grade review. And then I think they compared those somehow and looked at, like, oh, compared to, like, the just human review, was grade, say, under predicting, or was it over? You know, where were the errors? Like, were the errors in the region where grade was predicting, like, really low? Or were they where it was predicting time? So I don't remember the exact, like, numbers that they came up with, but that is, like, a kind of study that they did back in. Like, I think this was, like, in the early times when they were just putting it out.
00:24:52.784 - 00:24:54.752, Speaker B: But it also seems like just when.
00:24:54.768 - 00:24:56.888, Speaker A: You give a ranking that, like, with the other.
00:24:56.976 - 00:24:57.872, Speaker B: Some of the other half of the.
00:24:57.888 - 00:25:01.856, Speaker E: Ranking itself might start to really inform the decision making.
00:25:01.920 - 00:25:03.364, Speaker B: I just wondered if they.
00:25:03.744 - 00:25:05.344, Speaker E: Anyway, absolutely fascinating.
00:25:05.424 - 00:25:06.008, Speaker F: Thank you.
00:25:06.096 - 00:25:27.284, Speaker A: Yeah, that's also totally an excellent point. And at some point, like, there is, like, I think there's something to be said about the fact that, like, we don't. We won't ever actually know the real counterfactual there. Like, what actually would have happened if this wasn't there? And also, things are changing over time. So, like, some of these things are, like, something that I wish we could answer, but, like, I don't know that we can now.
00:25:28.224 - 00:25:29.404, Speaker C: Thanks, Todd.
00:25:29.944 - 00:26:23.984, Speaker G: Yeah, I'm back to you, Chan. Although I think this question may also have implications for Serena, and I'm picking up on Lee's question, trying to relate these talks to the theme of our cluster, which is great to hear about the technology. Let's now focus back onto the human and assumptions about the human. But it isn't just like the human user. That's a very generic characterization, and it's very rare in real life that it's simply the user. Even that term is it just wipes out so much that's important from a political, ethical point of view. So back to Lee's question about labor.
00:26:23.984 - 00:27:27.464, Speaker G: If you're a worker in a factory and this is your livelihood, and the factory owner says, I want you to sit just so. Or like you're an Uber driver, because, you know, that's another case. We want to learn from you how to automate vehicles so we can put you out of a job. The owner says, sit just so and don't move because we're observing your behavior so that the robot can learn. Well, now you're in a home, and with the best of intentions, Yuchen, you're saying, let's adapt this environment, but it's all a market, right? So we know that if a family is told, do this just so so our robot can learn, you're probably not going to get cooperation. So now when you and the motive that you give. Your motive for doing certain things, or, Serena, your motive for doing certain things.
00:27:27.464 - 00:28:17.604, Speaker G: And I had said a similar thing yesterday. Fabulous. This is why you're doing the work you're doing. But I want to ask both of you to think a little bit more about what it could mean if that user isn't, like, a free agent, maybe a teacher who? Or an admissions person. And actually, we're going to use this algorithm to just, you know, get rid of your jobs completely and so, etcetera. So this is. I want to ask you to please think about it or tell us who should be thinking about this, your best position, because, you know, the tech.
00:28:19.464 - 00:29:09.960, Speaker B: Yeah, I can take this first. So I completely agree that we should be actively thinking about this, these questions, and especially when we actually go about deploy these robots, there should be discussions around what happens if something goes wrong. The same with autonomous driving. So at the policy level, when new tactical knowledge gets deployed, we should discuss the harms of it. But I also want to take the view that jobs are being taken. For sure. But if you think about it, computers used to be jobs, like, there used to be people actually doing this computing for the purpose of, I don't know, other projects, they are slowly replaced with the technology.
00:29:09.960 - 00:30:13.584, Speaker B: But that's not mean that there are just a lot of people out of jobs. We adapt the society that adapt over long period of time, over decades. And these people get trained to do other tasks, and they may have new jobs, like maybe tally operating a robot, in this case, teaching them how to do these tasks. And as a matter of fact, a lot of countries like Japan, they are actually suffering from the lack of labor. They have so many elderly that need to be taken care of that they just don't have enough young people. And that's why they have the densest, like, robot, actually population right now, as nowadays, they are developing these technologies so that they can automate home care for their elderly. So I think, yeah, that it is a problem that we should constantly think about the harm we bring as we design technology.
00:30:13.584 - 00:30:31.092, Speaker B: But humans are highly adaptive at the same time. And oftentimes when we look back, we see that as a society because we are here and you are here, we are adapting. I don't know if that actually.
00:30:31.268 - 00:30:32.924, Speaker C: Serena, can I ask you to respond?
00:30:33.084 - 00:31:03.664, Speaker A: Yeah, of course. Yes. Thank you so much for that great point, because I want to. Man, I wish I had more time today. But actually, a lot of what we do try to say in our paper is that a lot of the machine learning for education papers that we studied, I think they exactly suffer from this issue of, like, not humanizing, like, the. Or not thinking enough about, like, the depth of the different, like, people in the system. So one, I'm going to point out, like, two examples that I wish I could have talked about or like that, you know, we'll have in more detail in the paper.
00:31:03.664 - 00:32:20.906, Speaker A: So one is that, like, in a lot of these, like, automated systems, they tend to want to, like, you know, summarize numerically a lot of, like, inherently qualitative or human traits that, like, you can't, you know, really just, like, put an. It's like trying to, like, rate a painting on a scale of one to ten or, like, rate somebody's, like, life story on a scale of, you know, zero to one. Like, that doesn't really. That kind of, like, quantitative summary of these, like, complex and qualitative elements or human pieces is, like, not something that we should take for granted or, like, you know, we should be careful, like, when those things kind of get suggested in the form of these kinds of, like, quantitative or technological choices. And I don't even know if, like, even with the best of intentions, these things might just kind of accidentally get suggested. So then another thing that I would. That I also want to point out is that in the, like, translation between from these, like, quantitative predictions to interventions, there's, like, one kind of thing that, uh, repeatedly came up in our interviews was that it's important to, like, keep, like, the best success comes from actually, like, incorporating human operators in the process or, like, designing with their kinds of different strengths.
00:32:20.906 - 00:33:21.724, Speaker A: So, um, one example was that, like, when they had. When chat bots were deployed in, like, an advise, an advising system at a university for. For somebody we interviewed, they actually, like, they found that if a student just kind of used that chatbot, like, the student doesn't know what they don't know. So, you know, they could only answer, like, very specific small questions or, like, ask, you know, specific kind of almost logistical questions. But when they actually had the advisor involved in the process and the advisor could have these, like, you know, in depth conversations with the student and kind of, like, elicit what kinds of needs the student really had, then the chat bot was actually able to, you know, assist the advisor in pointing out, like, oh, you know, maybe the student might struggle with, like, these classes or something, and then the advisor could come around and, like, provide a better recommendation. So I think that the human element in, especially education systems, is, like, extremely important and essential to think very critically about and carefully.
00:33:25.464 - 00:34:10.214, Speaker E: Great. Thank you so much for both of these talks. I found them fascinating, and I look forward to talking to you both about them more. My question is for Serena, actually, and it's not the type of question that I normally find myself in the position of asking, but I think for a lot of people in this room, the idea of, there are these unintended consequences of deploying this algorithm. It ended up potentially exacerbating exactly the kind of societal problems that a lot of us think very carefully and deeply about in our work. The fact that in 2020, UT Austin stopped using it is like, the first step of a success story. Like, good job you're no longer using the algorithm.
00:34:10.214 - 00:34:46.735, Speaker E: But I would really. And I know you sort of ran out of talking about the second half of your talk, but it kind of strikes me as, like, well, what now? Because the underlying pressures that drove them to adopt the algorithm to begin with are all still there. You know, I have dear friends of mine who are in postdocs in Michigan that there are. They accept three people a year, and they get 900 applicants. Like, and if an algorithm isn't formalizing that problem a little bit, that's not to say that humans aren't, like, whoever's at the bottom of the pile at the end of the night when you're sleepy and, like, hungry, like, you're going to be grumpy or. And have low blood sugar, and then they're arbitrarily. So I don't know.
00:34:46.735 - 00:35:05.933, Speaker E: Like, I'm glad that they're no longer using the potentially biased algorithm, but the underlying pressures to deal with that problem and to try to systematize it and to try to formalize, like, a fairer way of doing admissions. Clearly, all of that, those pressures are still there. So I would just love to hear you reflect on, like, where do we go? Like, what's the next step?
00:35:07.033 - 00:35:43.996, Speaker C: Thank you for that question. And, Serena, while you think about that, we've got a few more questions, and I have one that actually relates directly to that. So I'm going to add it, and then we'll take the remaining ones. Yeah, that. So my question is about precisely this kind of the way in which these automated systems put pressure on and also expose value preferences of institutions. So it's interesting that in the AI, as you're describing, what we're seeing is the need to articulate clear goals, and those somehow become public through this process in a way that they might not have before. So the question.
00:35:43.996 - 00:36:21.048, Speaker C: So this is interesting, and you had a quote from a student saying, you know, or a preference expressed by students saying, I would not want my application to be processed this way. So my question is, have these models, or have you done any thinking about how you would, what kind of new pressure that is, and how you might engineer for those preferences, too. So how do you articulate a public value? What kinds of public value might be there, and what kinds of public value might systems like that try to engineer in Ben, you're saying your questions on the same topic. Okay, let's add it.
00:36:21.096 - 00:36:21.624, Speaker A: Let's add it.
00:36:21.664 - 00:36:23.644, Speaker C: Sorry, everyone, we're diluting the questions.
00:36:24.304 - 00:37:09.254, Speaker H: Yeah, I really think they're just building off of one another. But, yeah, I'm interested in this. Like, it almost gets to the level of your methodology of comparing the goals stated in ML papers to the goals that sort of come naturally out of interviews with education practitioners, where there is sort of a nice connection that you can make between, for example, the optimization like, objective function and a goal as articulated by an education practitioner. But I'm really interested in how that formulation might break down or might not always, um, work as a, as a comparison. Like, to what extent.
00:37:11.314 - 00:37:12.082, Speaker D: To what extent.
00:37:12.138 - 00:37:52.654, Speaker H: Are educational goals, like, not, uh, like are, is it difficult to boil down educational goals to an objective function and sort of, as Salome said, like, where do we go from there? Like what, what do we do with that? Um, I think education experts are often, like, looking at the particular needs of students and in much more communication with students. And so the sort of like, impersonal approaches to machine learning might not be fitting. And is the answer just refusal or is there anything we can do with sort of. Yeah, the kind of scale and current state of education that we find ourselves in.
00:37:56.274 - 00:37:57.834, Speaker C: Solve the world's problems now.
00:37:57.994 - 00:38:39.204, Speaker A: Oh, boy. I mean, I think all three of you just articulated, like, my entire research agenda for the next slide. Because I think these are, I mean, first I wanna say that, like, I think our paper brings out a lot more questions than answers. So, for sure, like, I don't have great answers for a lot of things you're saying. Cause I'm, and I'm really thinking about those things. I wanna point out, like, let's see. One thing I did want to, what I think is cool is that it seems like you've keyed in on this, like, issue of goals and how those goals, you know, interact with our technological solutions and the way that we approach building these, especially specifically machine learning technologies, because that's where I.
00:38:39.204 - 00:40:03.886, Speaker A: Yeah, focus. So there's two things, two directions that I've been thinking about recently. In that, in this like, kind of translation gap between the goals and the technologies. And the first is that, like, the first is this kind of, we have this bias towards thinking that there's just one goal or one objective to optimize in, especially in machine learning systems. I mean, the way these things are built is you optimize an objective function, maybe subject to some constraints, but, like, there's one objective function. And I think that this kind of, like, this oversimplification is more common than we think, or like, it is quite common, but in practice, like, in reality, there's really, like these very complex environments with a lot of different needs and a lot of different goals that can always be, can't always be quantified so easily. So, like, one that's, that's a kind of one issue that I'm, like, thinking a lot about now is like, how do we, how do we go from just like trying to optimize one objective to, like, working within these, like, complex multi goal environments? The second thing is that, like, maybe the goals that we have access to are not just a subset of, again, this broader ecosystem of different goals and objectives.
00:40:03.886 - 00:41:31.980, Speaker A: So even if we are, even if we do consider optimizing over a subset of some number of goals that say, I know about now, is optimizing these really the best move, or is there some, I mean, because if you like, in all these machine learning papers, they're just like, oh, how do we get to optimality? What is, you know, we want like Pareto optimality or just different notions of it, but like, is that really what we want to do, even if we, like, expand our number of objectives? Or is there like something else that we should be thinking about? Like maybe we want to. Yeah, and that's kind of like an open, like, question that I've also been working on. So I don't know if that quite answered your question of that's, I think your question of, like, what the, just because we remove a technology doesn't mean, like, the problem doesn't go away. Like that is like, I totally empathize also with the fact that, like, that isn't the question that I would usually ask either. But, like, I think it's definitely, like, at play here, especially with education, where like, it's like, you know, chronically under resourced in a lot of, you know, different places and areas. So I think before we can answer that question. Well, though, there's like, okay, I think there's still a lot of work that needs to be done in the technological space to find where these gaps are and where it's like, where it's kind of failing people in order to come back and be able to actually improve things.
00:41:31.980 - 00:41:37.744, Speaker A: So I don't know. I guess it's just an ongoing, iterative thing, and I don't have a great answer to that question.
00:41:38.724 - 00:41:39.060, Speaker B: Right.
00:41:39.092 - 00:42:01.986, Speaker C: So we've got three further four now for the question. We're going to take Peter, Julia, and then Chris and then JD, in that order. Can we keep the question super brief? We're now eating into break time. We did start a little late. I feel like everyone's okay with that. Go. This is a bit of a comment as well as a question.
00:42:01.986 - 00:43:08.240, Speaker C: I think that it seems that one of the presumptions that's kind of behind this is that the objective function, whether it's for purposes of machine learning, how to engage with humans, or in this education context, is the objective function seems to be at the level of the provision or the production of certain kinds of behavior. So in education or education about providing education, how fast the students get through and all that. But there's also purposes of education, and those purposes of education extend out over a long time horizon. So if you're asking what's the purpose of education? Is it to ensure the kids get grades? Is it about degrees? Is it about getting jobs? Is it about employment? Or is it about opening up entirely new kinds of opportunity pathways that wouldn't have existed for them otherwise? And can we devise objective functions that address those longer term purposes? Because it gets back to a lot of the other questions that we're talking about here, which are we've got a human technology world relationship that's developing over time. It's evolutionary and it's recursive, and we're transforming ourselves in that process of deploying these technologies. We're not human beings. We're human becomings.
00:43:08.240 - 00:43:27.644, Speaker C: We're becoming moment by moment changing as we interact with these systems that we're deploying. And it's according to whose values and with what timing. So the question is, it's not just an issue of sort of which harms are going to be incurred by human beings, but whose values are going to be shaping our human becoming.
00:43:33.154 - 00:44:08.184, Speaker F: Super, super interesting stuff. I just want to sort of piggyback off the previous points and really say something real quick. So the point on employment, right. This is such a fundamental point, like in economics, they distinguish between structural, frictional, and sort of cyclical unemployment, right? And structural is when you sort of technology changes and you're no longer employed, right? Buggy whip drivers or whatever, right? I'm sure they exist still somewhere. But the point is, the importance of employment is not just something. It's the first time you meet someone, you say, I am Ilya, I am a doctoral student. It's literally the second.
00:44:08.184 - 00:44:28.750, Speaker F: It's such a big part of the identity that's just saying, hey, maybe there could be things that we can come up with. These are very tenuous things. People don't understand how important it is. Second point, there's a lot of philosophical work on this. Anyway, the second point is optimization. I was thinking about your example. It's such an important example.
00:44:28.750 - 00:44:57.984, Speaker F: Like, I can imagine that your paper, let's say in like 20 years, there is. You get the perfect admissions algorithm, right? All of the. All of the challenges have been addressed. Would I still feel comfortable in the knowledge that some algorithm is deciding my graduate school admission? Even if there is a perfect algorithm that addresses all the issues, it still is. There's something about it that sort of. I think a lot of AI and ML researchers don't actually like address or like, keep in the quiet. And it's like an elephant in the room.
00:44:57.984 - 00:45:14.920, Speaker F: I was. I was like, I love Nietzsche. And he has a quote in the spoke there, too. He says the best way to preserve something is to destroy it. It's a weird quote, but what he says is, you know, once we sort of tackle problems, we're actually sort of. We're actually improving the things that underlie them. Right.
00:45:14.920 - 00:45:23.604, Speaker F: We're saying, okay, these are the things that can be done to improve. You're perpetuating the systemic injustice of the hierarchy of the educational system. Anyway, that's. That's sort of like a general sort of thing.
00:45:24.704 - 00:45:30.544, Speaker C: And we'll take. Chris, we might have to take these as comments and think about it after the break.
00:45:30.704 - 00:46:31.424, Speaker D: That's right. So mine was piggybacking off what Salome said before anyway, but I was just wondering if you've been following the fuck the algorithm protests in Britain. It sort of came out in Covid, disrupted the sort of school year for school leavers in high school. And so because people couldn't sit the exams, a level exams, they had to sort of use this algorithm to kind of, you know, this is what your grades would have been, you know, and one of the key variables that was used was a historical performance of schools that people have come from. And so obviously, you know, people who've come from historically very privileged schools, eton and whatever, were, you know, tended to do very well by the algorithm. And people who came from underprivileged schools did very badly, you know, and, you know, it's really like, stupid algorithm, you know, and it. But led to these huge protests.
00:46:31.424 - 00:46:49.824, Speaker D: And I'm just saying it's interesting, you know, sort of the way these things work as a kind of way of presencing these kind of, like, latent sort of class tensions, social tensions, gendered tensions, particularly that you're drawing attention to, and racialized tensions as well.
00:46:50.844 - 00:46:51.180, Speaker A: And.
00:46:51.212 - 00:47:18.252, Speaker D: But I think it's sort of a challenge. You know, simply, it's difficult to kind of really optimize for this. It's a big ask to try to resolve these tensions within an algorithm. But it's just something interesting for me to, you know, how do you kind of, like, absorb as you already are, you know, kind of responding to those kinds of critiques and how, like, that kind of tension plays a part in the way you're modeling these sort of problems? Yeah, there's something interesting, Chris.
00:47:18.308 - 00:47:26.014, Speaker C: And lastly, JD. And then keep it brief, JD, and we can talk about it over a couple of jobs.
00:47:27.474 - 00:48:11.034, Speaker I: I really enjoyed these talks. I think, really my question is the same as Peter's and Chris's, so it'll be short. What is the alternative to using machine learning models in graduate admissions? Right. Is the alternative, as Solomon suggested, like somebody who's exhausted, looking at the end of the pile, sort of having their own biases being coded here, in a sense, what is the use of these algorithms in particular ways? Tell us about the values that people have. It feels like what they're telling us is that we don't know what we want. And a lot of the problems that you're describing are in many ways wicked problems with many stakeholders, and you can't make everyone happy and all this stuff. Graduate admissions is no exception to that.
00:48:11.034 - 00:48:28.754, Speaker I: It strikes me that the question that we're battering around the edges of is actually far deeper than just is this an appropriate use of machine learning? But is it, what do we actually want here? You know, I don't have an answer to that question, obviously.
00:48:29.694 - 00:48:46.074, Speaker C: Yeah. These new technological developments, I think one of the great value of them is they're forcing a whole lot of difficult questions in a lot of different ways, and I think that's a great thing. This has been a wonderful panel and a really excellent discussion this morning, and please join me in thanking our wonderful speakers.
00:48:49.374 - 00:48:49.974, Speaker B: Thank you so much.
