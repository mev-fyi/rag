00:00:00.080 - 00:00:42.106, Speaker A: This week's session of the synthesis workshop at Simons. So today's session is going to be on algorithmic aspects of synthesis and some interesting applications. Our first speaker is Professor Laura's Dantoni, who is an assistant professor in the computer science department at the University of Wisconsin in Madison. Laura's has worked on many aspects of formal methods and programming languages, including program synthesis, verifiable machine learning, and topics in automata theory, and also personalized education. And today Lauris is going to focus on the synthesis side of his research on synthesis with guarantees.
00:00:42.290 - 00:01:23.768, Speaker B: Laura, hi, thanks for the introduction, and thank you for having me here. Yes, today I'm going to talk about essentially most of my students chin happening, whose dissertation, which has been about how to add some interesting guarantees to program synthesis. So let's dive right in and let me tell you what I mean by guarantees in program synthesis. So I'm going to focus the conversation on syntax guided synthesis. But a lot of the techniques that we present are general. I'm going to use cyguses and something that many of you know, where essentially we have a specification where we are looking for some program that has certain property. And here I'm really meaning p of x y.
00:01:23.768 - 00:01:56.368, Speaker B: So this way it speaks on the side. But I'm looking for a program p of x Y that takes for every input x and y returns something rather than x, something rather than y, and is either x or y. And I have a grammar of terms I'm looking for. So in this case, I have essentially linear integer arithmetic expressions with if the analysis. And I would like my synthesizer to give me a program that satisfies the specification. And it's in the search space. So in this case, the obvious one is if x rather than y, return x, otherwise return y.
00:01:56.368 - 00:02:38.890, Speaker B: Of course, this is not the only program that satisfies the specification here. There is this other one, for example, that has two if analysis, and in this case, the two are equivalent and they both satisfy the specification. And one is arguably better than the other. And while I'm talking about this, well, people have used cycles and synthesis in general for a lot of things. Here is just a slide where it shows some applications of Cygus. And a few years ago, with chin happening, we say, oh, maybe we can use Cygus ourselves for one application. And we were working on inverting, on building a synthesizer that could invert string transformers.
00:02:38.890 - 00:03:18.684, Speaker B: So these are things like, I give you a base 64 encoder. Can you automatically synthesize the base 64 decoder for me? So a string a problem that takes a string and outputs another string, I want to compute its inverse. And we were able to solve this problem by basically building a lot of smaller problems at the character level that we were giving to cycle solvers. So you don't need to understand what I'm doing here, but I'm inverting essentially some program, and we formulated and give it to a cytosolver, in particular the e solver. And we got this pretty good program. And again, you need to understand. But then when we gave the same sigus problem to a different solver, we got a very different program.
00:03:18.684 - 00:03:54.040, Speaker B: Okay? So we got a huge program that had a bunch of fifth analysis, and it was very complicated. So I argued that program synthesis is essentially a bit unpredictable. In this case. Essentially I have a search space. And what happened is that in the set of solution spaces, so in my search space, there were multiple programs that satisfy the specification. So I call the solution space the set of programs that satisfy the specification. And what happened here is that we were lacking a way to specify which solutions were better than other, and then synthesize the best solution according to this metric.
00:03:54.040 - 00:04:26.730, Speaker B: So in this case, we wanted the program at the top, but instead sometimes we got the one on the right. So here is like my Forrest Gump quote is like, synthesis is like a box of chocolate, you never know what you're going to get. And so that's one of the problems that we have in synthesis. Another problem we have is that in terms of unpredictability, is that sometimes maybe synthesis problems don't have solutions. So, for example, if you have a search space that doesn't contain any program that is in the solution space. So the two are disjoint. Most solvers are there.
00:04:26.730 - 00:05:06.068, Speaker B: What they will do is they will basically run forever, time out or not terminate. Ideally, you would like to be able to produce a proof that indeed there is no program in the search space. So you want to be able to automatically, the same way we are very good at finding programs, we want to be able to prove that there are no programs. So, and this kind of started this direction of chin happens dissertation about how can we add some guarantees to program synthesis. And we worked on a number of things. So the first thing we worked on is this ability to prefer a solution when there are multiple ones. So, synthesis normally takes a specification, a search space.
00:05:06.068 - 00:06:02.276, Speaker B: And can we also have a preference of what solution we want to synthesize? Can we automatically prove when there is no solution? And some work I'm not going to talk about today. But maybe, can we generate a program that satisfies instead of fully satisfying a specification, maybe satisfies the probabilistic version of it, like the program is correct on 99% of the spec, or is correct with a certain probability. So let me talk about the first project, which is essentially this one of adding some quantitative objectives. So the problem here again, I have, essentially I have my specification and I have my search space, and a cytosolver will synthesize one program in this search space that satisfies the specification. And in this case, we have two programs, and we want a way to prefer the first solution. Like we don't like the second one, the second one seems silly. Here we like the first one.
00:06:02.276 - 00:07:08.828, Speaker B: So how do we do that? Well, the natural way to do that is to basically add some kind of quantitative costs to the program synthesis problem. So in this case, what we did is let's just, instead of having a normal context free grammar that describes sets of terms, why don't we make this grammar weighted so that we can assign naturally cost to terms? And so we call these Q cycles for quantitative cycles. And essentially the main change is that now, instead of just having a grammar, your grammar comes with some costs. So what does it mean to give away the cost per program? Well, if you look at the program here, essentially what happens is just you can take the parse tree of this program, and whenever you use a production that has a non zero cost, in this case, I assume that everything that isn't marked at zero cost. So in this case, I have two productions that cost one or the other one cost zero. So basically this program is cost two. The point here is that the programmer, when specifying their synthesis problem, can provide this weighted grammar to me.
00:07:08.828 - 00:08:08.948, Speaker B: Okay, so as part of the specification, the user of the synthesis technology can provide costume on top of having a qualitative specification, which is what we want the program to do, we can have a quantitative specification that says I want a program with a certain cost. So, programs now have weight costs. In this case, we have the top program is cost one, which is smaller than the bottom program that has cost two. So let's see what is then a synthesis guided synthesis problem with quantitative objectives, a q size problem. So we have formula as before, we have something that we want to be true for the program we are trying to synthesize. Now we have a weighted grammar, so grammar with costs, and we have an objective we want to achieve. In this case, let's say we want to minimize the cost, and so we don't only want to find the program that satisfies the specification and is in the language.
00:08:08.948 - 00:08:56.290, Speaker B: We also want that program to be the best one, meaning that we want that there is no better program, there is no other program in the search space that also satisfies the specification, and it has smaller costs. So this is a natural optimization objective. Okay. Based on the weights I assign in my grammar, I want to find the best program, the one with the smallest weight, that also satisfies the specification. But how do we do that? So, adding this cost, it, clearly it wasn't that much of a contribution. We have to figure out a way now to build solvers that in this new domain of basically cybers problems with quantitative objectives, we need to build solvers that can solve these problems. So I'm going to show you, essentially, we have devised a meta algorithm, an algorithm that can be used by any existing cyber solver.
00:08:56.290 - 00:09:27.838, Speaker B: And I'm going to show you with an example what the algorithm does, and hopefully that will make sense. So, this is a Q cybers problem. So I have a quantitative objective. So I have a specification, I have a weighted grammar, and I want to find the program of minimal cost. Okay, that satisfies the specification. So what can I do? Well, I can start with something that I already know how to solve. So I already have to find, I already know how to find a problem that doesn't have the best cost.
00:09:27.838 - 00:10:08.134, Speaker B: So I can take a side solver, ignore the weight, and just give the same problem. So I'm going to give you a specification, I'm going to give you a grammar, and I'm going to say, give me any program you want in this grammar. So there is no cost here. Okay, I just have the grammar, and let's say the synthesizer returns this particular program. Well, now I can ask, can you do better than that? So, I'm going to basically restrict my grammar to only accept programs of better cost. So in this case, and this is what we have in our paper, we have an algorithm for doing this. But I think this example kind of gives away a lot of the idea behind algorithm.
00:10:08.134 - 00:10:52.756, Speaker B: I'm going to build a grammar that basically uses new non terminals to represent different costs. So in this case, I'm trying to get a grammar, g less than two, that represents all the programs in, all the programs in G that have cost less than two. So how can a program that starts with start have cost less than two? Well, it either has cost zero or it has cost one. And how can a program have cost zero? Well, it either is a composition of two programs of cost, zero or so on and so forth. So now the interesting thing is that this program is again a cybers program. So this program does not use any weights. The weights are part of the non terminals.
00:10:52.756 - 00:11:23.564, Speaker B: There is no cost, it's just a normal grammar. I just introduced new non terminals, and now I can call again a cygo solver and the cygo solver, maybe this time we return this program, which happened to have cost one, and now I can keep going. So if I started with whatever cost, I can keep going and keep going and keep going. So the cool thing about this is that I built a solar for the Q Slygos problem. So the problem with quantitative objectives using cygo solver. So this technique can be reused by any other side of solver. It's effectively a form of linear search.
00:11:23.564 - 00:12:13.504, Speaker B: And the theorem is that if you give me a weighted grammar without negative weights, so only with positive weights, I can always build for you such a grammar that g less than c, that accepts all programs of cost less than c. And actually, in the paper, we generalize this result also to multiplicative weights. So if you have probabilities, you can also use the same technique, and also for bounded negative weights. So if your negative weights cannot go below a certain threshold, we also have a way to handle some negative weights. So essentially, it's fairly general, and the whole theory is given in terms of certain monoids and whatnot. So you can support a number of interesting weights that appear in practice. In fact, let me show you what's interesting.
00:12:13.504 - 00:12:54.388, Speaker B: So I showed you how to do minimization, doing linear search, how to minimize the cost. But you can actually also put constraints that you want cost to be, let's say, greater than three. Why? Because I talk about context free grammars. But in Saigus, effectively, in all the benchmarks, we consider, I would say most benchmarks, you would only need regular three grammars. And so if you can do g less than four, you can then complement that grammar and for free, get all the programs of cost greater than three. And similarly, you can compose cost using intersection, because regular three grammars are closed under all operations you want to have. And in fact, you can even compose multiple costs.
00:12:54.388 - 00:13:42.912, Speaker B: You can have multiple metrics, you can have a cost, one that would say is, I want to minimize the number of if analysis, and maybe I want to minimize some probability. So essentially, you can compose multiple costs together. And so we implemented this. So the idea of our implementation is very simple. We just have a Q Cygos implementation, and then we create these intermediate problems and use different solvers to solve them using Cygus. And we were able to basically take a number of Cygus benchmarks, add various objectives like minimize the number of if analysis, minimize the solution, minimize some probability. Probabilities are very useful in, in synthesis because you can use them to assign, people have used them to train models of how likely a certain program is.
00:13:42.912 - 00:14:22.384, Speaker B: And then you can use this to guide the search. So you want to find the program with the highest probability. And you can also, as I say, compose weights where, for example, you want to first minimize the number of if analysis, and after that you want to find the program, the smallest program of all, the ones with the fewest ifth analysis. And you can see on the right the problems we generated. And the summary here is that essentially you can, this works. We were able, for many of these benchmarks to start with a program that the first program that the cycle solver returned wasn't the best one. But then by doing these iterative updates, we were able to find a better program.
00:14:22.384 - 00:15:21.042, Speaker B: And the interesting thing though is that only for 14 of the 16 benchmarks we could optimize, we could prove optimality, meaning for two of these benchmarks we weren't able to show that you couldn't do better. Okay, so just to summarize, we basically, we have this solvent that iteratively builds Saigus problems. And he says, can you do better? Can you do better? And at some point we weren't able to show that you could not do better. Like basically we weren't able to solve the next problem. Let's look at this. And this brings us to the other question I mentioned at the beginning, which is, remember, I got here to the point where I say, is there a program of less, less of cost less than two? And we found this program here. We found the if then else's q side, because now we ask, is there a problem of cost less than one? And the grammar for less than one is the one you see here is basically is only linear terms without if.
00:15:21.042 - 00:16:18.006, Speaker B: The analysis and the function we are trying to compute here is the max function. And there is no way to implement the max function using a program in this grammar. So there is no way to say x plus y plus n or ax plus by n that can implement the max function. And so basically this brought us to the problem that these cycles we were encountering, cytos problems that didn't admit a solution. And this was essentially the other problem I mentioned at the beginning of the talk, which is we want to be able sometimes to prove that no program exists, we cannot find a program that has a solution. We cannot find a program for this particular problem. So this is the next project that happened, work done, which was basically, how can we prove unrealizability in syntax guided synthesis? And so here the problem is, I have an unrealizable problem.
00:16:18.006 - 00:17:24.304, Speaker B: So I need to output a proof that there doesn't exist a program in the search space that does what I want. And because the grammar is infinite, this is an undecidable problem. And Sanjit, for example, has interesting results on this, on this domain. So before starting and showing our technique, I'm going to make a simple one assumption is that I can use, instead of trying to prove that the whole problem is unrealizable, I can focus on examples because it's a little easier. Why can I focus on examples? In many cases, something like cgis, an algorithm that basically gives you examples as part of the specification, will be enough to show realizability. So for the program we have here the max, if you use these four examples, it's already enough to show that already, not only there doesn't exist a solution to the problem across infinite for problem, already you cannot find the max of these four pairs of numbers using the grammar we had before. So I'm going to just focus on examples.
00:17:24.304 - 00:18:05.574, Speaker B: And that's okay, because if I prove unrealizability over a finite set of examples, it means that the problem is also unrealizable in general. So if you can, if the problem cannot be solved over four examples, there is no way I can solve it over all the infinitely many examples. And now I can give you the outline of our algorithm, which is very simple. I'm going to give you one algorithm first, and then we're going to try to improve. So the algorithm is basically, is just a reduction. So I'm going to take my cycles with examples problem and reduce it to a reachability program analysis problem. So essentially I'm going to build a program that has an assertion, and the assertion will always hold if and only if the problem is analyzed.
00:18:05.574 - 00:18:34.854, Speaker B: Okay, so I have a problem, a problem, a cygus problem. I'm going to build a program, and an assertion in this program is true. It always holds if and only if the problem is unrealizable. So let's look at the construction. So first of all, what does it mean for an assertion to always be true? Well, basically it's a reachability problem. Can I make the assertion false? And what we're going to use in a reduction, we're going to use non determinism a lot. Okay, so this is something you're all familiar with.
00:18:34.854 - 00:19:25.176, Speaker B: So I'm not going to explain what non determinism is, but the point of this, the reason I'm excited about this is because why do I want to solve a different problem? Well, because it's a problem that someone else has already solved to some extent. So there are many tools for proving reachability or for proving that an assertion is always true, so we can rely on those tools to try to solve our own problem. So here it is. The whole construction is very simple. So essentially I'm going to build a program that takes my inputs, it runs my inputs non deterministically to one of the programs in my grammar, and then it's going to check that the specification I'm trying to enforce is false. So essentially this program, this assertion is always true. If no matter which program I run my inputs through, I will not satisfy the specification.
00:19:25.176 - 00:20:28.660, Speaker B: So this is summarized here. So I'm going to repeat it. This program structure, it has three, three blocks, takes the inputs, runs them non deterministically through a program in the grammar, and then checks, have I not solved the problem? So if I can never solve the problem, it means that the problem is unrealizable. And now you kind of can imagine what this looks like. So my assertion at the end is basically I'm just specifying, hey, are, have I solved the problem? Am I satisfying the specification? So in this case, I'm just checking, is it true, sorry, am I not satisfying the specification? So is it true that I'm still not satisfying the specification? Is it true that for all these four inputs, you, for at least one input, you incorrectly computed the max? And now the other bit, the non deterministically drawn, is effectively kind of like a sketch. So I'm going to have a program that non deterministically chooses what to do, and it basically mimics, oh, sorry, these are shifted by one. It mimics the grammar and say I'm going to non deterministically pick this production.
00:20:28.660 - 00:21:13.654, Speaker B: So in that case I just return zero, I'm going to non deterministically pick this production. In that case I return one. If I pick x or y, I return the corresponding component of the example and for the sum, and basically I non deterministically choose a left child, I non deterministically choose a right child, and, and then I sum them together. So basically this is a non deterministic program that non deterministically chooses a program in the grammar and runs it. So there is an issue with this that kind of makes it different from things like sketch. The problem, first of all, is that we have an infinite amount of non determinism. So every time we call this function, we can make a different nondeterministic choice.
00:21:13.654 - 00:22:03.930, Speaker B: And that causes a big problem, because essentially when you run this function on an input, you get an output of zero, which is the output of some program in the grammar. If I run this function again on a different input, I get the output of some other program in the grammar. And this doesn't guarantee that the two programs are the same. So if I do it this way, my tool is incorrect, because it might tell that essentially it allows different non deterministic choices to run different inputs on different programs. But what I'm interested in is finding whether running the inputs on the same program always results in the correct result. And the way you fix this is that you basically, instead of running one input at a time, we can run all the input examples together. So basically you kind of run them pairwise.
00:22:03.930 - 00:22:59.618, Speaker B: And you can see here, for example, when I non deterministically pick a sum, I get the left side, the right side, and then I pair wireless. So we implemented this and we use basically our QC benchmarks to generate new benchmarks that are analyzable. And essentially these are the ones. When I asked you before, can you do better than this? Can you find a better program? The answer was no, because there was no better program and we were able to solve a bunch of them. So I'm not going to bore you with the details, but one thing that didn't work is that when the grammar was very large, these programs were resulting in very big unrealizability, very big reachability programs that existing tools cannot handle. And so to fix this, we built a new tool for proven analyzability. And I'm almost running out of time, so I'm going to just show you the high level idea.
00:22:59.618 - 00:23:26.874, Speaker B: But the high level idea is very simple here. So this is a specific technique for proving analyzability instead of trying to use a full blown reachability tool. So consider this grammar here at the bottom, and I have two examples. I want f of one to be five and f of two to be six. That's my specification. And my program can only generate things of the form of two forms, basically x from one or x two. Let's look at x one.
00:23:26.874 - 00:23:57.620, Speaker B: So if you look at x, you will see that basically what it generates is an expression of the form. It generates a bunch of expressions, and each of them is of the form two lambda, x plus one. Okay? Where lambda is, can be anything. So now, if you. If I tell you this, it's easy to see that there is no solution. Because if no matter what lambda I put here, I'm in trouble, because the thing on the left here is always going to be an odd number. But on the right, I want something that is even.
00:23:57.620 - 00:24:31.474, Speaker B: So, there is no way that anything in here will result in a solution. And similarly, this one generates three lambda, and you get a similar problem. So altogether, they generate this type of expressions. So, if I were magically able to figure out that this is the case, I could prove that this is analyzable in an analytic fashion. And this is the entire idea of this other work we've done. Essentially, from the grammar, we generate a set of equations that describes all the possible outputs. This grammar, the programs in this grammar, can produce.
00:24:31.474 - 00:25:08.744, Speaker B: And then by solving those equations, we find all the outputs. And now, if the output we care about, five, six, is not in that set, we have proven our elizabeth. And there are two ways to solve this. One is basically, you can encode with inductive relations, you can encode the problem of solving the equations. And we have done these using off the shelf solvers. Or, and this is the part that perhaps you're most interested in, in the particular case of linear integer arithmetic. You can show that these equations can always be solved and result in what is called a set of semilinear sets.
00:25:08.744 - 00:25:56.686, Speaker B: And this is quite exciting, because, basically proves that for conditional linear integer arithmetic, size over examples is actually decidable. You can always prove analyzability or. Or the problem is solvable and you can find a solution and the tools kind of perform differently. I'm not going to show you too much, but to conclude, basically, we can add a number of quantitative and interesting guarantees to synthesis. And I show you how to do it for Saigus. But one thing we are working on is going beyond Saigus, because Saigus is only interested in expressions and the synthesis problem, or expressions, but people are interested in many problems. So I want to conclude the talk with a pointer to our new paper, where we extend syntax guided synthesis to arbitrary domains.
00:25:56.686 - 00:26:15.134, Speaker B: So, basically, we introduced a framework called semantics guided synthesis, that is hoping to be what Saigus is for SMT expression semantics guided synthesis. It will be that problem format, but for any synthesis problem. And so I'm over time, so I'm going to stop.
00:26:16.634 - 00:26:21.094, Speaker A: Thank you, Loris. Are there any questions from the audience?
00:26:24.034 - 00:26:35.330, Speaker C: Hi, Lauris here is Raz. Thank you for the nice talk. A quick question. If I understood correctly, for QC, you consider the synthesizer itself to be a black box, right?
00:26:35.482 - 00:26:35.930, Speaker B: Yeah.
00:26:36.002 - 00:26:39.954, Speaker C: So would some algorithms be a better fit than others?
00:26:40.074 - 00:27:12.194, Speaker B: That's a great problem. So I guess Qcygos, there are two things. There is the format and the algorithm. So the Q cycles algorithm, we do black box because this way it was easy to use. But if you actually do something like what Amina does with meta sketches, where you incorporate the problem in the loop, or what sketch does. Also with this linear search, you can reuse the facts you learn at each step and improve your search. And actually we've tried in cengas, we also tried incorporating weights and it's a little tricky.
00:27:12.194 - 00:27:39.494, Speaker B: This linear search doesn't give you good inductive facts. The solvers that use these numeric objectives, if the objectives are not tied in interesting ways to the structure of the program, then the solvers actually struggle to use them to reuse lemmas. But I think you're right that in some cases it would be possible to do so basically improve the search so that you don't just black box call and you try to learn facts.
00:27:40.034 - 00:28:08.708, Speaker C: But I would say that even in the case of metasketches and the sketch system, kind of the algorithm itself, the inductive synthesizer, is a black box. In both cases it is sort of a set solver kind of thing. But I was thinking, at that level, if you used some properly structured enumerative solver, then you might be able to get those costs bounced for free.
00:28:08.876 - 00:28:30.726, Speaker B: Yeah, actually. Well, okay, I didn't understand if that's the answer. The answer is no, because in all the benchmarks we have, there are infinitely many programs at each tier. So you will never be able to prove that you've explored everything below. So that's in meta sketches you can do that. But in what we have, if you want to prove that there are, that's what happens with the example. I show the best solution at one if then else.
00:28:30.726 - 00:28:41.954, Speaker B: But there are infinitely many solutions with zero if then else. So you can enumerate like kind of dovetailing and eventually get something, but it will not tell you that you found the best.
00:28:42.534 - 00:28:46.166, Speaker C: Oh, I see. So just that unrealizability part is the.
00:28:46.350 - 00:29:06.582, Speaker B: Yeah, I mean, even if you find a program, if you, if you do dovetailing you, at some point you find a program of cost ten, you still haven't exhausted anything of cost less than ten. So it might be that you haven't found there was a problem of cost six, it was just not in your enumeration order. So in some cases you can do that, but in general, you cannot.
00:29:06.678 - 00:29:21.414, Speaker C: Right, but I see, I see. But infinitely many programs with zero, if the analysis would be also infinitely large. Right, correct. I mean, their cost would be zero, but it is sort of an unnatural cost.
00:29:21.534 - 00:29:42.334, Speaker B: Yeah, yeah, exactly. So that's why we have those multi objectives as well. So if you want, you can compare, compose them, and that helps. You're right. For some subcases, that's what we started with. For some subcases, you can do that and enforce a total order that allows you to. Basically, it's an enumerative set that you can give an algorithm, but it's very, it's very interesting.
00:29:42.334 - 00:29:46.394, Speaker B: Like extracting that from the specification is a very interesting question. If you can do it.
00:29:47.174 - 00:29:48.142, Speaker C: Thank you, Loris.
00:29:48.238 - 00:29:49.034, Speaker B: Thank you.
00:29:50.494 - 00:29:51.582, Speaker A: Thank you Loris.
00:29:51.718 - 00:29:53.194, Speaker B: Thank you for sharing.
00:29:53.774 - 00:30:14.854, Speaker A: And let's move on to our next speaker. So suprate, please go ahead and project your screen. So I'm delighted to welcome our second speaker, Professor Supathik Chakrabarti. Supratik is a professor at the Department of.
