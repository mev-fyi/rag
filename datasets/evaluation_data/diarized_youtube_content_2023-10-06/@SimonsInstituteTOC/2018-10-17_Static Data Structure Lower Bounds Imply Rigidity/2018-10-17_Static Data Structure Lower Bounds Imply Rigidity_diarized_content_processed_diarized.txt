00:00:00.120 - 00:00:01.414, Speaker A: Today is by Sasha Golovne, who's going.
00:00:01.414 - 00:00:04.914, Speaker B: To be talking about connections between static data structures and data systems.
00:00:13.894 - 00:00:50.064, Speaker A: Thanks for the introduction. Yeah, my name is Sasha Glaminov, and today I would like to talk about connections between data structure lower bounds and circuit lower bounds. So, matrix, and this is based on a joint work with Zev Dvir from Princeton and Omri Weinstein from Columbia. So I will first define static data structures that we are working with. So, yeah, we're going to be working over an arbitrary field, f. For simplicity, you can think that this is f two. And in a data structure problem, we have n inputs and m outputs, and typically m is a polynomial of n.
00:00:50.064 - 00:01:28.548, Speaker A: And when I ask you to solve static data structure problem, I'm basically asking you to come up with two functions. One is preprocessing function. It takes all the n inputs and outputs some number s of memory cells. And the other function is query function. It will be just answering my m output queries. So it takes whatever you pre computed in the first step and outputs all the query values. And since you don't want to lose the input information, s is usually greater than n.
00:01:28.548 - 00:01:50.068, Speaker A: For any non trivial function, you need at least ten cells to store the input. So, pictorially, it looks like this. You have basically a depth two circuit. You have n inputs. You preprocess them into a larger number of cells s, and then you want to be correct on all m outputs. And m is huge. It's.
00:01:50.068 - 00:02:10.256, Speaker A: I know, n to the hundred. There are two parameters we care about. One is space. How much space you store to answer all my queries. And we denote it by s. And the other one is t. It kind of time you spend for each output.
00:02:10.256 - 00:02:47.330, Speaker A: But by time here, we only mean how many cells you read. So there are two important parameters, s space time. Does it make sense? Okay, so every data structure problem has two trivial solutions. One is when you store answers for all output queries, you spent a lot of memory, you spent m cells of space. But then query time is trivial, just one. The other trivial solution for every data structure problem is when you store only input. But then for every non trivial problem, you have to spend a lot of time.
00:02:47.330 - 00:03:00.898, Speaker A: Spend time because your output might depend on all the inputs. Yeah, and. Yeah. So in this setup, like the inqueries are part of the function. Right. They fixed it. Static data, data structure problem.
00:03:00.898 - 00:03:34.234, Speaker A: In the very beginning, I'm telling you, I'm going to be asking these m questions. Please answer them correctly on every single input. Yeah, so, and as always, random data structure problem is super hard. It requires space almost m and time almost n. But again, as always, we want to find an explicit function which is hard for data structures. And the best what we know is Larsen 2012. Its t must be at least log m over log s over n, which, let me parse it for you.
00:03:34.234 - 00:03:55.882, Speaker A: When s is linear, when. When you want to store only linear space, then we know that c must be at least logarithmic. And when s is polynomially super linear, then we know nothing. We only know that c must be super constant. Oh, constant, yeah. This by Sigel from. Yeah, yeah.
00:03:55.882 - 00:04:24.728, Speaker A: It was reproved like four times. Yeah, yeah. The first time it was produced in late seventies, I guess, basically, yeah. Okay. And, okay, so this is our setup here. And. Yeah, and so one can think about data structure as a depth to circuit.
00:04:24.728 - 00:04:57.436, Speaker A: And in this work, we want to understand connections to circuit lower bounds, or matrix ergis. So let me define you what I mean by circuit. Luckily, Sascha already more or less defined all what I need. So, in the circuit world, we normally work with an inputs and linear number of outputs. Not necessarily, but it's usually the case. And here, by circuit, I mean Sasha defined directed acyclic graph where fanin of each gate is two. We want to prove, ideally super polynomial circuit lower bound.
00:04:57.436 - 00:05:39.880, Speaker A: So at least super linear best. What we can do is roughly three, no, five, and depending on how you define circuits, and as we usually do in such a case, we just say, okay, let us try to solve a special case of the problem. We solved a bunch of special cases in the eighties, and we seem to have a hard special case. Hard special case, which we cannot solve, is when we restrict the depths. For example, if we restrict the depth to be, say, half log n, then since fanin is two of aggregate, your output does not depend even on the whole input. So every non trivial function cannot be computed by such a circuit. But we also know functions which cannot be computed by circuits of depth.
00:05:39.880 - 00:06:02.028, Speaker A: 1.9 log n or 2.9 log n. Depends on, again, how you define circuits. But this is kind of cheating, because it's not like we're proving superlinear lower bounds for the circuits. We're just saying these depths is not sufficient to compute our functions. We don't know how to use depth restriction in our circuit lower bounds, except for one thing.
00:06:02.028 - 00:06:38.714, Speaker A: In 1977, Vellian said that here is a way which can possibly help us of using depth restriction. So the result is based on the beautiful work of Erdos, Graham and simmered and Sascha showed it like it's been 40 years since well improved it. So we know many ways to present Velen's result. Sascha showed it what it means in terms of depth three circuits. But I want to make it more similar to data structure settings. So I'll present Welland's result in the language of depth two circuit. It says the following.
00:06:38.714 - 00:07:17.504, Speaker A: You have n inputs, m outputs, and you pre compute just a little of information, epsilon n. Actually, you can make it n over log log n, much smaller than in the case of the data structures. And clearly this is not sufficient to compute all outputs. So you have to allow outputs to depend on inputs as well. So, but they depend only a tiny bit. Each output can only see n to the epsilon bits of information. So what Vellian proved is that if you can prove a lower bound against this model, then you immediately get a super linear lower bound for log depth circuits.
00:07:17.504 - 00:08:02.162, Speaker A: And the goal of this, we basically want to say that these two pictures look similar, the one for data structures and the one for log depth circuits. And actually, in the case when all functions, data structures and circuits are linear, we prove that they are equivalent in some sense. So yeah, we have to restrict our attention to the linear case. We say that. So we assume that all m output functions are linear functions of the input, just some linear combinations of the inputs. And whatever you compute here and here is also linear. Actually, it was shown by Jukner and Schninger that it suffices to assume that only this layer is linear.
00:08:02.162 - 00:08:59.044, Speaker A: Then, without loss of generality, without any loss of the parameters, you can make the middle layer linear as well. So we're working with linear functions now. And then the result of valiant can be stated as follows. Say you want to compute circuit which computes multiplication by matrix m. That is, it takes a vector x from f to the n and it outputs m times x. Then any circuit of linear size and logarithmic depth which computes such a linear map just from this can be written as the following sum a very sparse matrix a, which corresponds to dependence of the outputs on the inputs plus product of two matrices. One is this sparse transformation and the other one is whatever you compute in the middle layer.
00:08:59.044 - 00:09:38.482, Speaker A: So it's the sum of a sparse matrix plus a product of two matrices. We can forget for a second that C is sparse. Later on, if time permits, I'll show you that it might be useful to actually remember that C is sparse. But for now let's forget and then we can just write it as a sum of sparse matrix and matrix of small rank. It has small rank just because it's a product of two matrices of small dimension. And this list is the definition of rigidity. We say that matrix is say epsilon and comma t rigid if it cannot be written as a sum of a C sparse matrix and a matrix of low rank.
00:09:38.482 - 00:10:37.404, Speaker A: By C sparse matrix, I always mean a matrix which has at most c nonzero elements in every row. My sparse will be always, I'm going to be talking about rectangular matrices and sparsity always means the number of nonzero elements per row. And, yeah, and basically if you find a rigid matrix which is for rank epsilon and it requires n to the delta for arbitrary constants epsilon and delta registry, then that's it. You prove a superlinear lower bound in this setting. But the best what we can do is far from this. For example, for square matrices, we can only get constant rigidity here. And Allan Panigrahi and ichanin showed that if you consider rectangular matrices of size poly n by n, then you can do slightly better than you can do log.
00:10:37.404 - 00:11:02.444, Speaker A: Nice. Okay, and now for a second, let's return to the data structure, to the data structure question. This is the best one. If we insist on epsilon n, right? Epsilon, yeah. If you don't mind having sub linear rank, then we know better. Yeah, we'll get to this later, hopefully. Yeah.
00:11:02.444 - 00:11:58.094, Speaker A: So let's get back to the data structure question here. We also want to consider linear functions. That is, all our m outputs are linear functions and you only compute linear functions here. Then whatever compute here can also be written like you compute for every input x, you compute m times x, where m can be written as follows. This is whatever your output layer computes some matrix a times matrix b, which corresponds to whatever computing the middle layer. And a is c sparse because each of your outputs depends only on c elements and b is small. So m is of size, m is of size, m times your space and b is s times n.
00:11:58.094 - 00:12:39.326, Speaker A: So if you want to find a problem which is hard for linear data structures, just find a matrix which doesn't have such a decomposition. And what we're saying this work is just this decomposition in data structures and the one in rigidity. They're related. Okay, so I've given like enough definitions. Let me show you an example of connection between data structures and rigidity. And if you like it, then I will give two final definitions from which we can just collect all the results about how equivalent Gcc and data structures are. So let me show you an example of such a connection.
00:12:39.326 - 00:12:53.874, Speaker A: So I want to say, is it clear? Any questions? So if we have a matrix. Yeah, I have a question. Yeah. In order to get such matrix m, all we need is to take a matrix of n higher than s, right?
00:12:59.954 - 00:13:03.634, Speaker B: S is bigger than n. Oh, I see it.
00:13:03.674 - 00:13:37.336, Speaker A: Sorry. Sure. Yeah, it's n by n matrix, right. And s is. Okay, so other questions, we want to prove the following. Say we have a matrix which is not rigid, which is not rigid for rank epsilon n and some parameter sparsity t. Then we'll show that there exists a matrix of the same size such that its column space intersects a lot, the column space of M.
00:13:37.336 - 00:14:17.850, Speaker A: It spans almost all column space of your matrix m, constant fraction of it, and that's easy to show. We assume m is not rigid, so there is a decomposition as follows. M is sum of sparse and lorank. Now let me define a linear map such that its kernel is exactly the column space of M. I can always easily do this. I just say I take any basis of m, say that you output zero on this basis, and then I extend this basis to the basis of the whole space, and say your identities are extended by linearity. So it's easy to construct such an l whose kernel is exactly a column space of this.
00:14:17.850 - 00:14:47.776, Speaker A: Now let me apply this l to this equation. L times m is l times a plus l times b, but l times m is zero, because I defined l this way. So l times a equals minus l times b. In particular, rank of l. A is rank of lb, but b is a low rank matrix. So even when I apply the transform the linear map l, it's still low rank. On the other hand, the rank of ace is large.
00:14:47.776 - 00:15:29.894, Speaker A: For example, a is a sum of I can always, without loss of generality, assume that M is full rank matrix. Otherwise it's even easier for me. So M is a sum of matrices like full rank and low rank. So its rank must be, must be large. And how can it happen that you apply this transformation to matrix of rank almost n and get a matrix of Frank almost nothing. This means that the kernel of L intersects the column space of a, but kernel of a is exactly the column space of M. So this means that intersection of column space of a and m is huge, it's almost n.
00:15:29.894 - 00:15:47.784, Speaker A: It reduces it from almost n to almost nothing. And a is a sparse matrix. I just found a sparse matrix which spans almost all column space of your not rigid matrix. Have a question?
00:15:48.444 - 00:15:50.844, Speaker B: It doesn't say in the lemma that.
00:15:50.884 - 00:16:12.244, Speaker A: A is part, sorry, of course, yeah, I want a which is. Sure, sure. I'm sorry, I meant to say that a is sparse, of course. Yeah. Otherwise it doesn't make sense. Yeah, I always can find a sparse matrix which generates almost all my column space. And this parse matrix is exactly the matrix you give me in rigidity decomposition.
00:16:12.244 - 00:16:45.504, Speaker A: Yeah. And now let me prove that a data structural arrow bound implies some rigidity. We can prove stronger results later. But let me show you a result of this form. Fix c. Like what? Don't fix c for every matrix m. It's either computable by a data structure with linear space, like really small linear space and time t, or it contains a rigid submatrix.
00:16:45.504 - 00:17:38.034, Speaker A: It has like a good, it has a good rank parameter, and it also, it loses only logarithmic factor in the rigidity. In particular, if you give me a matrix which cannot be computed by a data structure, then it contains a rigid submatrix. Moreover, this can be made explicit. This matrix m prime belongs to a reasonable complexity class, to a complexity class where we want to prove rigidity or circuit level bounds. And how do we process theorem? Now it's easier. Does a statement make sense? If you give me a matrix which cannot be computed by a data structure, then it contains rigidity. So here's the matrix, and I'm just going to be recursively using this lemma to prove the theorem.
00:17:38.034 - 00:18:13.614, Speaker A: So I look at the matrix you gave me, m by n matrix. If it's already rigid, I'm done. I proved that it's rigid. Otherwise, I know that if it's not rigid, then there is a sparse matrix which generates a huge chunk of the columns. Let me call this m prime. I know that m prime is some product of the matrix a times b, where a is t sparse. Just by the previous lemma, we know that a times b, where a sparse, is exactly data structure.
00:18:13.614 - 00:18:43.134, Speaker A: So I have a data structure which correctly computes every single output on these inputs except for some fraction of the inputs. So now I just recursively apply the same procedure to the remaining inputs. I'm saying, okay, let's look at this matrix. If it's rigid, I'm done. I proved that it contains a rigid submatrix. Otherwise, I come up with a data structure for this input and I repeat it just like log n times. In the end, I have a tiny matrix which I can solve by a trivial data structure.
00:18:43.134 - 00:19:21.666, Speaker A: So either my algorithm at some steps is a rigid matrix which I'm happy about, or I construct an efficient data structure which I'm also happy about. Good. Okay, then I'll just give two definitions and they basically set the stage for all our results. And all the results follow in just like one line each. Let m be a matrix of size m by n and see some sparsity parameters. Then we define, we call this Paturi pudlak dimensions. They were defined by Paturi and Pudlak in 2006.
00:19:21.666 - 00:19:50.514, Speaker A: We just use a different form of them. Yeah, they define it for columns and for some reason needed for rows. And it seems important for some reason here. Yeah. So give me a matrix then. Outer dimension of this matrix. Matrix for this sparsity parameter t is the minimum width of a matrix of a sparse matrix whose column space contains the column space of your matrix.
00:19:50.514 - 00:21:05.608, Speaker A: What's the smallest width of a sparse matrix which contains your matrix? And the inner dimension is kind of the opposite. It's you give me a matrix and I'm looking at all sparse matrices of the same size. What is the largest dimension of their column space intersections? And it's not hard to show that actually outer dimension exactly corresponds to data structure. Complexity of the problem problem can be solved by St data structure if and only if the outer dimension with sparsity parameter c is at most s. The inner dimension exactly corresponds to a strong version of rigidity matrix is strongly rigid if and only if it has small inner dimension. And after this, all what we are doing is we are just proving like tight inequalities between inner and outer dimensions. And they give us the correspondence between rigidity and data structural arrow bounds in both directions.
00:21:05.608 - 00:22:26.164, Speaker A: So the theorem which we just proved basically says that if your matrix has large outer dimension, then it contains a submatrix with small inner dimension, sparsity parameter slightly smaller. And this implies that if you have a data structure lower bound, like for any, for any query time t and space n times one plus epsilon, then you have for example, a rectangular rigid matrix with a parameter better than we know. Moreover, we can stack these rectangular matrices together and get a square matrix with a rigidity parameter better than we know. So any improvement on the current data structural hour bound would imply non trivial result in matrix rgdc. We can also do there is one line proof of the following inequality. Whatever, whatever matrix you consider a sum of its inner and outer dimensions is at least two n. N is again the rank of the dimension of the matrix.
00:22:26.164 - 00:23:54.542, Speaker A: This implies, for example, that if you have a rigid matrix, then you have like in the other direction, then you have a data structural outbound. Indeed, a rigid matrix means your inner dimension is small. This inequality says that if your inner dimension is small, then outer dimension is large, and outer dimension is exactly the data structure complexity. So we have the correspondence in the other direction as well, and we can do other things. For example, if you have very strong data structural bound, for example, if for linear number of outputs you have data structure lower bound of n times one plus epsilon for the query time n to the epsilon, then you also get a super linear circuit lower bound. For this, you just need to show that valiant result. It says that every small circuit gives you the following decomposition, where a and b are sparse, both of them, and if you keep in mind that both of them are sparse, then you can see that actually this means that the column space of m is generated by the column spaces of a and b, which is, which exactly matches the definition of outer dimension.
00:23:54.542 - 00:24:22.304, Speaker A: This says that every matrix computed by a log depth circuit has small outer dimension. So if you have a data structure lower bound, this means you have large outer dimension, and this also implies a circuit lower bound. Yeah, that's it. Yeah.
00:24:22.424 - 00:24:24.164, Speaker B: The dilemma that you hit there.
00:24:26.744 - 00:24:27.080, Speaker A: Does.
00:24:27.112 - 00:24:40.924, Speaker B: It give one to valiant statement, a partial converse or something, that if you have a matrix which is not rigid, then the linear transformation given by that has simple circuits, a small circuit.
00:24:42.104 - 00:25:17.630, Speaker A: So, okay, first of all, we prove a stronger statement here. This just has, I think, simple proof which I wanted to present. We prove a stronger statement. We don't lose anything anywhere, and we prove it in two directions. We actually say that matrix is strongly rigid if and only if this holds. I don't know how to, I don't know how to construct a circuit from this. For example, I don't think it's known that any non rigid matrix can be computed by a circuit, right? So I don't think this is exactly like the other direction of valiant thing.
00:25:17.630 - 00:25:23.906, Speaker A: But we have equivalence here. Matrix is strongly rigid if and only if this.
00:25:24.030 - 00:25:39.654, Speaker B: And I had another question. So we know of matrices which are sort of semi explicit, which are very rigid, right? Do they imply data structure lower bounds? In this semi explicit setting, for instance, the entries are like square root of primes.
00:25:40.394 - 00:26:14.724, Speaker A: Sure. Yeah, this correspondence does hold, but again here, same as in circuit lower bounds we're talking about. Yeah, this holds, but we know this bounce in data structural arrow bounds as well. You can always take algebraically independent elements and say you cannot compute anything. Yeah. Do you know anything about the tightness of the log n factor? So I just showed you the worst case parameters. We don't always have this log n only for some setting of parameters, but where we do have it no, we don't know how to improve it.
00:26:14.724 - 00:26:23.064, Speaker A: Any other questions? Okay, let's type out.
