00:00:00.160 - 00:00:21.594, Speaker A: Hey, welcome, everyone, to day two of our AI and humanity workshop. The first panel this morning, we'll be starting off with Sarah Sen, who is at MIT, and I will be keeping time. Everyone has 15 minutes. I'll give you a five minute warning.
00:00:22.054 - 00:00:38.298, Speaker B: Great. Okay, great. Just checking if everyone can hear me. Okay. All right, thank you. Good morning. Today I'm going to be talking about the design and governance of human facing algorithms.
00:00:38.298 - 00:01:26.384, Speaker B: And just to give a bit of background, my name is Sarah, and my home department is computer science at MIT. And my work generally focuses on kind of two components of data driven algorithms. One is kind of the theoretical underpinnings of the design of algorithms, and the other side is the auditing and governance of them. And today I'm going to be talking about a few works, and they're in collaboration with some amazing people, Andrew Elias, Alexander Madhuri and Manish Raghavan. So when I'm referring to human facing algorithms, what I'm referring to is algorithms that directly affect or interact with the decision subject that is human and. Okay, I can speak a little louder. Yeah, maybe move my parallel.
00:01:27.524 - 00:01:28.264, Speaker C: Great.
00:01:28.724 - 00:02:04.256, Speaker B: And I think I don't need to motivate the reason why we need to scrutinize these human facing outcomes to this audience. But two reasons that we might be interested in doing so are that they're increasingly common. Right. They have this kind of very uniquely wide and deep reach into our lives, and that's only going to escalate with time. But the second, which is what I'm going to focus on today, is that we weren't quite, quite prepared for human facing algorithms, both on the design side and in terms of governing. So on the design side, we kind of rushed into this era of big data. We took algorithms that were designed for other applications, and we co opted them for human facing applications.
00:02:04.256 - 00:02:53.456, Speaker B: And what that's meant is that we've had a lot of unintended consequences come out of the design side, and we're kind of catching up on the governance side. So I'm asking today, how can we better design and govern these algorithms? And due to the shortened time, not really going to cover this broadly, but I'm going to focus on two approaches or two works. The first is what we call trust and recommender systems, and this is going to be on the design side. And the second is what we call the right to be an exception, and this is going to be on the governance side. Okay, so trust and recommendation. I think a lot of us are familiar with recommendation platforms, but effectively, what these platforms do is they generate a set or an option, a set of options for each user. And so when I'm talking about recommendation platforms, I'm talking about Netflix, Yelp, Facebook, Tinder, and even Grammarly.
00:02:53.456 - 00:03:40.496, Speaker B: So Netflix is going to recommend you tv shows or movies. Facebook's going to recommend you content, Yelp is going to recommend you places to go, Tinder's going to recommend you individuals to date, Grammarly, is going to recommend different ways of maybe phrasing your sentence. And our focus today is going to be on the trust between a user and their platform. And I just want to emphasize that we're not focusing on trust that a user has in another user, because that's kind of a different field of study. We might ask, well, why do we care about trust? And the short answer is, because humans, not just platforms, are adaptable and strategic. So we often give credit to platforms for being able to adapt to maybe changing data, changing times, and then to be strategic to optimize for their own objectives. But we don't give that same amount of credit to humans, at least on the design side.
00:03:40.496 - 00:04:33.014, Speaker B: Um, but humans have the ability to perceive, they can develop mental models of how they think a system works, and they can then adapt and strategize how they respond to that system. And this is a problem for platforms. Why? Because algorithms weren't designed to be human facing. Most algorithms and recommendation are going to follow this general kind of framework. They're going to assume that users have a fixed set of unknown preferences, and that these preferences aren't necessarily adapting or changing with time. And what they're going to do is they're going to then probe the user for certain feedback, like ratings, or they're going to observe you on different platforms or different sites, and they're going to use that to kind of fill in or infer those unknown fixed preferences. But like I mentioned before, in reality, the data is not IId missing uniformly at random, et cetera, all these assumptions that we commonly assign to users in recommendation.
00:04:33.014 - 00:05:18.766, Speaker B: So actually going back a little, maybe just to give a little bit of context, what we did was we kind of took these inference algorithms that were designed for different applications. So they started off being co opted from the days when we were using algorithms to estimate the location of a submarine or a missile. What we're doing was we were maybe sending out some radar pulses and then be receiving data back. But each radar pulse isn't going to be strategic. One compared to the next is going to behave relatively similarly. And so we developed this sort of language and tools to deal with this kind of data source. But when it comes to recommendation platforms, users are the data sources, right? They're main data sources and they don't have fixed preferences.
00:05:18.766 - 00:06:02.874, Speaker B: They can adapt and they can strategize. And the strategic behavior is going to violate some basic assumptions, um, that we assume in recommended, uh, recommendation. So we have that recommendation algorithms kind of weren't designed for this two way adaptability and strategic behavior. And the question is, well, what do we do next? What our work is doing is it's kind of asking algorithm designers to maybe rethink this framework for the sake of users and for the sake of their own algorithm. The punchline here is just going to be that trust is going to help align the interests when you have two strategic parties. So what kind of phenomena are we talking about here? I'm just going to use two silly examples to help kind of ground our discussion. You might go to Facebook, you might see a link, you might actually be super curious about clicking on that link, but you don't do it.
00:06:02.874 - 00:06:27.254, Speaker B: Why? Maybe you think last time I clicked on a link, Facebook sent me or recommended me a ton of things with this content and I don't want that. Or maybe this is actually a link to an article that has an opposing view to your own and you don't really want to see more of this type. You're just curious. You want to read what the other side thinks, but you don't want to see more of that in the future. But there's no way to tell Facebook that. So you strategize, you don't click. Maybe you open an incognito tab and you kind of search it on your own.
00:06:27.254 - 00:07:13.784, Speaker B: As another simple example, let's say you're trying to protect your privacy. So maybe you're on YouTube and you like this video, you watch it, you like it, but you don't know who YouTube is sharing that information with. And so you strategically adapt and you say, I'm going to downvote this video even though I like it, just to kind of like make, mess up YouTube's recommendation a little bit. And what we're arguing is that this is kind of a self defeating cycle, right? You as a user are going to strategize. If you don't trust your platform, that's going to corrupt the data, which is foundational to the recommender systems algorithm. So the platform then is going to realize that they don't necessarily trust their users and their clicks and they're going to take more extreme measures. So they might, for instance, say, I don't believe your clicks anymore.
00:07:13.784 - 00:07:43.144, Speaker B: So I'm going to look for data that's more in line with my assumptions. I'm going to track the way you scroll down your feed and see if you linger on certain pieces of content. And that's going to be my new proxy instead of your clicks. But users are going to realize, catch on to this, they're going to realize that this is happening. They're going to strategize yet again. Platforms are going to take more extreme measures. And really who's winning from this? This idea that maybe we need trust to kind of align the interests and maybe stop this cycle is not new.
00:07:43.144 - 00:08:44.220, Speaker B: Hardin political scientists wrote a book in 1991 about trust as encapsulated interest based on this idea that you trust someone when you trust that they're going to act in your interest as well. And so the idea that we're trying to build on is when you have two actors like a user and a platform who are working so closely together, you need some kind of guiding forces. You have this sort of like implicit contract, but you need some kind of guiding forces. And the correct notion of that here is going to be trust. So we've kind of tried to force algorithm designers to think about their algorithms, assumptions, but can we actually change anything concretely? And what we offer alongside this is a formalization. This formalization, just very bare bones, is an alternating game where you're going to have a platform deploy recommender system. So that's set of recommendations, maybe the user interface that comes with it.
00:08:44.220 - 00:09:49.910, Speaker B: In response, the user is going to maybe click on some recommendations or interact with that recommender system. Platform is going to observe how you behave in response to recommender system and they're going to deploy another recommender system. So we're going back to step one, step two again, and so on and so forth. And what this game theoretic model allows us to do is, one, to explicitly account for the fact that users have their own agency, and two, it allows us to actually quantify kind of what trust adds and maybe what you can call maybe the cost of distrust. And so for example, using this formalization, we were able to capture both examples I talked about before and kind of pinpoint what goes wrong in these scenarios. Just a quick note that we don't assume rationality in this game theoretical model. And just the last point is you might ask, why does formalization matter? We're going to argue that formalization does really matter in this context because without the algorithm designers are going to continue deploying algorithms, and if we don't account for this human agency, then they're going to continue to ignore the role of that.
00:09:49.910 - 00:10:20.974, Speaker B: And that's going to be to the detriment of themselves and the users, because they're going to argue, look, why should I change? Why should I account for trust? Why should I account for societal concerns? Because in reality, I'm doing the best that I can do. I've proven it. And this is saying, well, look, actually you didn't account for something. When you account for something, that's not necessarily going to be true. So let's maybe rework this co opted algorithm. So without much preamble, I'm just going to jump into the second one in the interest of time. And this is called the right to be an exception.
00:10:20.974 - 00:11:00.004, Speaker B: This is an ongoing work with mini Shragaban. And so any feedback is super welcome, especially from this audience, since we're kind of stepping a little outside our comfort zone for this. Okay, so let's start with an example. Let's say you have an individual who comes in for healthcare or medical care and they have the symptom, I'm going to call it symptom green. And most often the symptom is associated with some common illness. But in some rare but extremely fast acting fatal cases, it's the only symptom that shows up in some sort of fatal disease. You could take, maybe think of two approaches you could take in this scenario.
00:11:00.004 - 00:11:47.544, Speaker B: One is I could treat this individual as the average of statistically similar individuals, like in my dataset. Or I might say, can I rule out exceptional cases by running a test, running a simple test and saying, thank you. That just running a test and saying, look, you don't have this, it's going to be the common illness. We kind of have this intuition here that there's this push and pull between the cost of running a test, maybe the inconvenience, but that it should be balanced against the risk of harm to this individual and how much that information, that test can give us. And this kind of push and pull, this balance, the way that we kind of intuitively think about this problem, is what's going to be argued in the right to be an exception. And so one thing I'd like to point out is that exceptions are natural. They're inevitable.
00:11:47.544 - 00:12:15.744, Speaker B: They're always going to arise. And we're paying particular attention to data driven exceptions for two reasons. One is because statistical averages are so fundamental to machine learning. Anytime you see probability, it's likely that they're doing some sort of expected value, which is effectively an average anytime. You see accuracy. Accuracy is an aggregate measure. So I guess the main point I'd like to make here is that anytime you have an average based school, but you have a heterogeneous population, you're always going to get exceptions.
00:12:15.744 - 00:12:22.184, Speaker B: The second is that data driven rules are non intuitive. Yeah.
00:12:24.484 - 00:12:25.984, Speaker C: There'S always error.
00:12:26.284 - 00:12:29.424, Speaker A: Are you using the word error, exception and error?
00:12:29.884 - 00:12:32.380, Speaker B: I am not. That's going to require a slight more.
00:12:32.412 - 00:12:56.742, Speaker C: Nuanced session, so I'm going to take it. If it's okay, you can take it offline. So data driven rules are non intuitive. You know, you can think of the autonomous vehicle example. Let's say you have ten fatalities per million without autonomous vehicles, and then you have one fatality per million without harmless vehicles, and you ask, oh, that's so much better. That's great. But what if it was these ten individuals who were the fatalities and without autonomous vehicles? And then all of a sudden it shifts.
00:12:56.742 - 00:13:29.472, Speaker C: It doesn't reduce it, but it shifts to a different individual when you add autotonous vehicles. And we don't really kind of have intuition for who that individual is going to be that's going to be harmed by this, a driven change. And this non intuitiveness is why we need to kind of pay special attention here, because I'd argue that in kind of more human facing applications, human, we know the limits of human cognition and we know where exceptions arise, and so we know kind of how to account.
00:13:29.528 - 00:13:56.918, Speaker B: For those, and that's what often our legal systems are doing. So the goal here is to provide legal protections for individuals who, through no fault of their own, fall through the cracks when it comes to data driven decisions. I'm a little bit running out of time here, so I'm going to go through this super quickly. There are a bunch of reasons exceptions arise. This is kind of a little bit more for the technical audience. It can come from sampling bias, modeling, capacity distribution shift, partial observability, and many more. And the point I'd really like to make here is just that.
00:13:56.918 - 00:14:50.518, Speaker B: It's not just that you lack enough data about an individual. There are multiple sources that can cause this, the right to be an exception. I'm not going to be able to do a full treatment here, so I'm just going to identify the three main factors which are harm, individualization and uncertainty. And so risk of harm, you can look to write to reasonable inferences, or write to contest AI or the proposed AI act of the EU, for kind of, um, is to kind of ground this, uh, thought process. But risk of harm provides a measurement stick. It kind of says, look, when should I be pursuing action on behalf of my right to be an exception? Individualization is this kind of natural thing that comes out of, um, if I'm using an aggregate rule, I'm going to have exceptions. So maybe the problem is I need to individualize my decision making process more to, um, the individual of interest.
00:14:50.518 - 00:15:31.358, Speaker B: Maybe I'm not capturing all the relevant information, and so it shifts their attention from aggregate to individual. But the one that actually is, I think, really, really important is uncertainty. And uncertainty captures kind of these inherent limits. The challenge is this notion, very common notion in computer science, that with enough data, with enough computation, as t goes to infinity, as n goes to infinity, that you can estimate anything. I'm going to argue that actually ground truth isn't always possible. So the reason uncertainty matters, you could argue that skipping through a little bit, maybe it's just I should individualize my decision. If my decision isn't individualized enough and the risk of harm is high, then I don't the place decision.
00:15:31.358 - 00:16:08.968, Speaker B: And I'm going to argue that's not enough. And the reason why is because you have different types of uncertainty that arise. One is epistemic, and this is uncertainty that is reducible. You can add features that will eventually maybe refine your algorithm, refine your decision. So this results from lack of knowledge, but there's also aleatory uncertainty, and this is irreducible knowledge that comes from just inherent unknowability. So individualization is basically going to tackle epistemic uncertainty, but it's not going to tackle aleatory uncertainty. And I'm just going to end on a simple example, which is, let's say you're using a data driven assessment to inform parole decisions.
00:16:08.968 - 00:16:48.274, Speaker B: The approach here is I'm going to predict whether or not someone recidivates, which is the outcome, by comparing them to individuals. In my training data set, that's effectively, you know, you might be reweighting, but that's effectively what's happening. One of the problems we can identify here is that we know that, you know, doing so washes out the details that make a defendant unique. And then we might want to individualize, we might want to add features, and I want to make it a bit more refined. But there's this second part, which is that what we're effectively doing is we're having an individual be judged based on the actions of others and not their own. So we're kind of stripping them of their moral autonomy. This kind of like a moral determinism that's happening here.
00:16:48.274 - 00:17:07.214, Speaker B: And this is what Aleator gets at. And I will conclude there. Thank you. Yeah, you're good.
00:17:11.154 - 00:17:14.714, Speaker A: All right, so next up, we have Peter Hershock from the east west center.
