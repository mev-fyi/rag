00:00:00.680 - 00:00:41.778, Speaker A: Let's start. Good morning everyone. It's a pleasure to be here again with all of you at the next session of our workshop on program synthesis. Our first speaker today will be Rishabh Singh who works at Google Brain. Rishabh got his PhD at MIT and after that he worked at Microsoft Research and became one of the very early pioneers in in using machine learning for program synthesis, or vice versa, program synthesis for machine learning. Both of these actually, and I know he has been doing exciting things recently at Google Brain. Some of those he may be able to tell us about.
00:00:41.778 - 00:00:50.534, Speaker A: Some of them perhaps not yet. But regardless, we really look forward to seeing your recent results. Rishabh, thank you for joining us.
00:00:53.574 - 00:01:34.694, Speaker B: Thanks a lot Ras, for the kind introduction and thanks for inviting me today. I was thinking about just talking about some of the more recent work we've been working towards, neural program synthesis. So let's get started. Great. So basically we've seen in last few years lots of tremendous progress in search based program synthesis, trying to scale up synthesis to tackle larger and larger search spaces. And we've had lots and lots of successes in terms of both constraint based synthesis, deductive reasoning, specialized algorithm and even enumerative search. We can now scale up with lots of new ideas.
00:01:34.694 - 00:02:47.644, Speaker B: So that's been actually quite, quite good. But one challenge still with these techniques is that they can't really, even for some of the modest complex programs, these techniques, the search base quickly becomes too large and we're not able to scale to or even generalize to larger classes of programs. So to tackle this challenge, actually we've been looking at if you were to step back and think about how human programmers. So that's actually an interesting example where we have very strong evidence that human programmers, we can write very complex programming programs. We can write, we can take specification in many different formats and ambiguous things and then generate code that satisfy the specification. And if we look back a little bit more about how we are able to solve a lot of these problems, there's some notion of some logical reasoning we have to do, we have to understand what the problem is, how to solve it. Then it's also the case that not we don't actually begin with being a super programmer to begin with, we first learn, we go through courses, we learn programming, read books.
00:02:47.644 - 00:03:51.654, Speaker B: Then over time actually we get better as we write more and more programs. And finally, actually, nowadays especially, we don't really learn, we write programs from scratch. For many of these cases, we actually use samples from lots of other places. So some of the work I'll talk about today. We're trying to see, can we get inspiration from this mechanism of how human programmers are able to take complicated specifications in like programs? Can we build systems that can leverage similar type of reasoning, similar type of mechanisms to help scale up these synthesis process? And also, one thing I wanted to also contrast a little bit with some of the techniques we'll be seeing today compared to some search based methods. So I was trying to figure out some ways where we can contrast the two sets of techniques. So the first thing is basically what I'm calling intuition here, which is more about trying to write programs without needing to enumerate.
00:03:51.654 - 00:05:12.440, Speaker B: So that's actually one difference we can see when we, as humans programs, we don't really search for all possible programs and then right one. But actually we have this intuitive reasoning behind the scenes that we use to write programs. The second thing is we, unlike search based systems that are typically static, we have one system that we use for synthesis again and again, whereas as human programmers, we are not static in the sense, as we learn new things, new concepts, as we write more programs, we get better over time. So they also have this characteristics of improving over time. Then also, as we saw earlier, actually as human programmers, we can write understand specification in many different formats, which is unlike typically search based systems that take specification in a given format. And then finally, actually, we also have this capability of when we are trying to write a bigger program, we don't just solve the whole problem at once, we break it down, break the problem down into subproblems, and then we write programs for sub problems and then we learn to compose them. Yeah, and then finally, actually one more characteristic which is interesting is this notion of being completely correct.
00:05:12.440 - 00:05:55.888, Speaker B: So typically search based systems, we try to generate programs that are exactly correct with respect to specifications. But I'm sure many of us have experienced this as well. It's very seldom the case that we write the correct program in first go. We actually typically write a program, we find issues with it, or actually, then we correct it. So also this notion that it's okay to make mistakes and then learn from mistakes rather than generating full program sequence. So for this talk, I'm hoping to give very brief overviews of some of the techniques we are using and try to connect it to some of these ideas as we go in the talk. Great.
00:05:55.888 - 00:07:16.516, Speaker B: So let's start with this first contrast about intuitions versus enumeration. And essentially what I mean here is that rather than using lots and lots of compute, lots of resources to search for a program, what if we take a step back and say I'm going to take limited resources, because that's how actually, as humans also, we have some evidence that we don't really enumerate programs when writing. So how do we bring that type of notion in generating programs? So for this actually, and also for other systems, I'll use an example like this. So there's a system called Smartwill now in Google sheets, which allows users to write in proper examples of the desired thing they would want their data to be transformed to. And then it learns programs in the sheets language that are consistent with the examples. So one general philosophy we'll see actually to learn programs in various different settings is this framework where the basic idea is that we're going to use lots and lots of data to learn this intuitive style reasoning for generic programs. So the basic idea is something like that.
00:07:16.516 - 00:07:52.334, Speaker B: We start with the programming language, let's say for smartphone, we start with sheets programming language. Or for Tensorflow, it could be tensorflow language. Then we're going to generate lots and lots of data in this, in this programming language by sampling random programs. And then we generate specification for that. In this case, it would be imperfect samples. And then we have paired data set, we have input examples and programs for which we're going to train these models. And hopefully these models are going to capture some of this intuitive reasoning to perform the code generation and testing.
00:07:52.334 - 00:08:29.534, Speaker B: So what we do here, we start with this programming language and we generate random programs. So here a couple of random programs we just sampled. We generate random inputs. We can run these programs on the inputs to get outputs. And now we get this pairs of examples saying here are input of examples and here are programs corresponding to those examples. And then at test time, the hope is if the models have learned the right thing, they can generalize to even real world data that users want to perform in such cases. So that's the overall framework.
00:08:29.534 - 00:09:20.264, Speaker B: Now we have lots of choices of how are we going to implement the architectures to import these specifications and programs. And here's one example that we used before, which was more of an encoder decoder architecture, where we first have incorporate sample encoders. And in this case these are simple, let's say lstms that can treat inputs and outputs as sequence of characters. So we first encode input using a bidirectional scheme, which then is used to condition the encoding of the output string in the example. And then we finally have a decoder that learns a distribution over programs conditioned on both input and output encoding. So this way we can encode one example. Similarly, we can encode multiple examples user is providing.
00:09:20.264 - 00:10:42.556, Speaker B: And finally we do some type of cooling to learn a consistent distribution of programs that is consistent over all the examples. So there are multiple choices we can have here. This was one particular encoding we used, and it turned out actually it worked quite well. It got about 92% generalization accuracy, which was in fact even higher than the hand coded version we had earlier, like a manual search and gotten, and which is kind of quite, quite interesting to see that first, at least these domains, the models, are able to learn this form of intuitive reasoning, to be able to generate programs for unseen examples of testing. And one more side note here in the experiments we were trying was we can also contrast this approach a little bit with this notion of differentiable programming where we make, we take these programming languages and make it all continuous differentiated, so we can do gradient descent directly in the program space. But one thing we found that actually doing gradient descent in the continuous space wasn't as effective as actually using the learning to perform this kind of intuitive search over symbolic space. So that was a kind of side experiment.
00:10:42.556 - 00:11:37.914, Speaker B: But it's interesting to see that actually it's important to have symbolic grounding in a programming language rather than making everything continuous. Now, the second component here is actually, we want these systems to improve over time. I won't have too much time to go into too much details here, but even in the previous system, actually, we will see that since we are training these models again and again as new data is coming in, or we are generalizing them to slightly different dsls on top of what they already have. So we do see this type of transfer where we don't have to train these systems again and again from scratch. We can just leverage what we have trained and which captures this notion of as humans. Also, when we learn programming, we don't just forget everything we have learned in the past, we can keep building on top. We'll be happy to talk about it later in the q and a if this time about this as well.
00:11:37.914 - 00:12:23.338, Speaker B: Now, the next part was understanding multimodal specifications. And for this I'm going to use this example called TF coding. So TF coded is the system where it's similar to smart fill, where users can give in to examples of tensor transformations they want to perform in tensorflow. But the interesting part here is that in addition to examples, they can also specify their intent in natural language. So here's an example taken from stack overflow, where a user specifies a task they have in English. And then they also give an example to clarify what sort of tasks they have in mind. And actually this was a question for which there was no answer for, I think, almost two days.
00:12:23.338 - 00:13:38.174, Speaker B: And when we ran the system, actually this answer was generated by TF coded. And there are many such problems for which the f coder can generate exam generate answers which are, which are quite similar to how human experts would write. So the basic idea of the system is that we have a natural language specification, and then we have some interrupt examples, and then we give this type of specification to the system. And then the system comes up with a program that is hopefully that's guaranteed to be consistent with the examples and also likely consistent with the natural language specification. So the basic architecture is similar to robust film we saw earlier, where we have, we're going to build these models that can ingest examples to distributions over programs. One interesting thing here is that we also have natural language specification in addition to examples. But the nice part about having these learning based mechanisms is that for these actually we can just have one more encoding and we can keep the whole pipeline similar to as before.
00:13:38.174 - 00:15:16.990, Speaker B: So that's the nice thing about having a learning based system, that we can add new and new modalities or specifications just as easily in our synthesis process. Now, one challenge, unlike input examples, is that we can't synthetically generate lots and lots of natural language. We can, but it won't be very realistic and it won't be, the distribution won't match the kind of distribution we see at this time from users. So for learning a language to API distribution part for this work we leverage the documentation that these APIs have, and then we do some TF IDF based cosine similarity to say, given a natural language statement, which APIs are most likely to be likely to be relevant and useful. So once we have these two distributions from examples in natural language to APIs, we can then perform this type of intuitive search we were looking at earlier, where we have programs of different APIs, a composition of different AP's, and we can guide them using, using these weights and distributions. And again here also in some of the preliminary experiments we were doing, we did see that not only by adding this learning component we can improve search based systems, but it can also be much faster compared to just doing search creates. So this was an example of multimodal specifications.
00:15:16.990 - 00:16:25.244, Speaker B: Now let me quickly talk about last two contrast points I had. One was about like we typically don't solve these problems in one go, we usually break down the problem into smaller parts and then learn to compose. So for this I'm going to use an example for later programming. So here the basic idea is given, given a similar problem, let's say the problem we were looking at smartphone or robust film before, where we have input examples, and I would like a program that is consistent with the examples. So in those previous works, we were trying to go directly from examples to programs or distributions over programs. But here we wanted to explore actually, can we have this additional bias in the architecture where we force the model to perform some type of decomposition and then solve some problems and then learn to compose again. So just to give a concrete example, let's say this was a program some spreadsheet user wanted to perform.
00:16:25.244 - 00:17:28.946, Speaker B: And if you look at a program, actually the program gets first number and then gets first capital, second capital, third capital. So if we look at the program, there's a lot of similarity in the sense this notion of get capital is kind of getting repeated again and again three times, and then there's a constant string as well that is repeated. So rather than having the model generate full programs from examples, we now have this additional bias, which we're calling it kind of latent, latent, discrete latent tokens. So now what the model does, it first maps the examples to this discrete latent space where each latent token is trying to capture a subproblem. And then for each individual sub problem, we can ask the model to generate the concrete program in the language we would like to. We would like to generate. Now, one challenge with this approach is that we don't really typically have the intermediate latents when we are, when we are training these models.
00:17:28.946 - 00:18:14.988, Speaker B: We just have imperfect examples and programs. So for that we came up with this architecture where we first have, during training, we have all these low level programs and we try to quantize them. We train these discrete vaes that try to come up with a discrete latency which tries to explain the low level programs. It's trying to learn commonalities or common subproblems that are present in lots and lots of programs. So that defines our latent space. Then what happens is we add this latent space as an additional additional mechanism for the decoder to decode before decoding the full program. So we start with examples.
00:18:14.988 - 00:19:17.304, Speaker B: Then we first map the examples to this latent space that has been learned using discrete ways. And then we use this discrete latent space to, to again go back to the final concrete program domain using another set of decoding. So the nice thing about this approach is that in addition to giving shaping the intermediate latency, we can also now perform two level intuitive search. So rather than just performing this intuitive search towards the end of the decoder, now we have one more space which I'm calling this discrete latent space. And the hope is that in that space the model can perform reasoning at the decomposition level. It can learn many different ways to decompose the problem. And then the lower level of search can actually do concrete symbolic generation of programs.
00:19:17.304 - 00:20:42.584, Speaker B: And yeah, again, in some of our initial results we saw that having this discrete bottleneck in the middle that captures some of these higher level subproblems allows the model to learn programs of much longer lengths as well as now we can actually perform reasoning at two levels, one at this higher level of extraction and then one at the lower level. So now let's say if I want to search for a program given a budget, let's say I can only do 100 tries, so I can, I can distribute my hundred cries to say let me do ten crimes at the, at the higher level, and then let me do ten cries at the lower level so we can, we can distribute this budget. But yeah, this is one way where we can impose more structure in the native space to encourage these models to learn, learn to decompose and solve these problems more, more efficiently. Great. So let me come to the final point on can we go beyond just forcing the synthesizers to generate always correct programs? And for this I'm going to use this example of differentiable fixing. And the basic idea is actually quite simple. So let's say I'm given some examples like in smartphone and I generate a program.
00:20:42.584 - 00:21:52.094, Speaker B: Now let's say this program is. So my initial task was to get first capital and last word. But let's say the model has learned to get first capital and second word, which works for most of the inputs, but for second input it doesn't produce the right answer. So now, so usually what would we do in previous approaches is we would generate lots and lots of programs and we would just do this type of generate and filter approach. We would run the programs and see which ones work and return the one that works back to the user. But if you think about how humans might go about fixing this problem, is that actually we can look at the output that the model has produced and then use that output and current program to come up with an improvement to the previous program. So rather than, rather than discarding that program and starting afresh, we can say that I have a good starting point and let me come up with changes to this program that will fix the output.
00:21:52.094 - 00:23:11.444, Speaker B: So in this case, what would happen is we would feed these examples and the original program to differentiate will fix it. And it would then say that rather than using second word, I have to use last word to, to fix this mistake. And in terms of architecture, it's similar to robust fill in the sense we start with some encoding of the examples, we map it to a latent space, and from that we can generate a program. We then run this program on the examples, and now we get tuples of original examples and the new output that the program has produced, and we feed it back, we encoded back these three tuples back to the latent space, and now we decode it again. So the hope is that since we are mapping all of these tuples back to the same latent space, in theory, we can apply this fixer step multiple times, so we can keep applying the fixer until we get to the right. And here again, actually in the experiments, we saw that having this differentiable fixer mechanism allows the model to not only make changes. So typically, if you look at beam search, beam search would typically make changes towards the end of the program.
00:23:11.444 - 00:24:24.084, Speaker B: But now with differentiable fixer, we can fix mistakes even in the middle of the program or before, as well as it can make many more changes to make the model correct of program correct, rather than making small changes. What beam search does. Yes, I guess briefly, I just wanted to show you some ideas about how we can take inspiration from how human programmers write programs by breaking down the problems or doing this type of intuitive reasoning or fixing mistakes. And here are some pointers to some of the work we are trying to, we have been doing in this space to see which techniques might, and which types of architectures might we might be able to build to perform these types of human like reasoning. But I think these are just some very early steps. There's lots and lots of interesting problems here, both in terms of what are the best architectures to encode specifications and programs. How do we build architectures that can perform logical reasoning, and also in terms of feedback? As humans, we learn from textbooks, we learn from great teachers who teaches programming.
00:24:24.084 - 00:24:52.414, Speaker B: How do we incorporate even richer feedback than the kind of feedback I showed today, which was very low level, usually just from data? And then finally, how do we apply these techniques to go beyond these small custom languages, but to do more real world languages? There's lots of, lots of exciting directions. Great. So this was briefly some things I want to talk about. I'd be happy to take some questions or any other questions people might have.
00:24:53.674 - 00:25:28.354, Speaker A: Thank you, Rishabh, this was great. Thank you for the nice talk. We do have exactly five minutes for questions, so people should just type them in the chat. Or better yet, turn on the microphone and speak up. I could start. Rishabh, could you tell us a bit more about the subproblem identification? That was the part four. I'd like to understand a bit better how.
00:25:28.354 - 00:25:48.012, Speaker A: Yeah, this was the interesting slide, how you actually identify the subproblems. Is it relying on the fact that the output includes parts of the input or is that not a requirement for the latent scheme to work?
00:25:48.188 - 00:26:47.464, Speaker B: Yeah, I think. And actually, again, there's actually lots of ways we can capture this problem. So in this case, in this domain, in this domain of text processing, there's some overlap in inputs and outputs. So it makes it easy to see the decomposition in some other domains. It may not be as apparent, but hopefully, hopefully the architecture that we're using to come up with subproblems, hopefully that's going to generalize as well. So the basic idea of the basic assumption we're making here is that we have these low level programs we want the model to generate. Now, rather than generating each low level program token by token, what if actually we knew about some concept? So we make, we are making an assumption that there exists some form of higher order, higher order functions we can think about, or sub functions that are generating these low level programs.
00:26:47.464 - 00:27:36.954, Speaker B: So this latent space is capturing that assumption that can I map my low level programs into this higher level abstraction such that, can I explain low level programs as compositions of these higher level labels? And in this case, actually, yeah. So we did see some interesting aspects after training that, for example, token 36 is this high level latent that captures this notion of capitals, even though there's different lower level programs. First capital, second capital, third capital. But it's capturing some notion of capital followed by Dot. So yeah, so it's one way these models are learning to capture some higher order reasoning for these low level, low level tokens.
00:27:38.334 - 00:27:45.070, Speaker A: I see. And those, those latents are learned completely using an auto encoder style or.
00:27:45.142 - 00:28:02.444, Speaker B: Yeah, yeah. So in the beginning, we train, we train this auto encoder. We have a discrete bottleneck there to ensure that the auto encoder learns like a discrete token space. And then they are also fine tuned little bit during training with examples as well.
00:28:03.464 - 00:28:34.744, Speaker A: Thank you. Other questions? Okay, if we do have 1 minute left, perhaps we should move to the next speaker. Thank you Rishabh, for the nice talk. This will be all recorded and so other people will be able to record a posted seed later. And thanks so much.
