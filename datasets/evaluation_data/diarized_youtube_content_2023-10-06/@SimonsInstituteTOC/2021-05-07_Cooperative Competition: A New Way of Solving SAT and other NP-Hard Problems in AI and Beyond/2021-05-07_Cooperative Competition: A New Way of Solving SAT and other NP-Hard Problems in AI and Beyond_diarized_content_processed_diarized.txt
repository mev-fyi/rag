00:00:00.080 - 00:00:29.256, Speaker A: For a couple minutes, but let's go ahead and get started. We have. It's a pleasure to have Holger Hus speaking today. He's at both Leiden and UBC, as I understand it. If that's correct, he's a professor of machine learning at Leiden, at least according to his webpage. So that's great. I was also noticed, I didn't notice before, that he is the author of music notation software and has done written various things for music notation.
00:00:29.256 - 00:00:41.724, Speaker A: So that looks very interesting. At any rate, it's a pleasure to have him here, and he's speaking on cooperative competition, a new way of solving Sat and other NP hard problems in AI and beyond. So over.
00:00:42.624 - 00:01:16.470, Speaker B: Thank you very much, Sam. It's a pleasure to be here. Of course, in reality, like many of you, I would much rather be really in Berkeley. Right. I was very much hoping that would be possible again in the near future. But of course, this event is another virtual one, so hopefully many of you are not too sad to hear me speak like this. Last time I was at the Simons Institute, I had a great time, and I was also kind of mortified and petrified because I was looking around the audience, and there were three touring award winners right there in the audience.
00:01:16.470 - 00:01:52.686, Speaker B: And that made me aware that that was a pretty high stakes talk. So today, I can't see you all. I have no idea whether there's any touring award winner here, but I see a few friendly faces, which is very good. I do want to make a big disclaimer, though. I mean, this institute that's hosting me today is called Simon's Institute for the Theory of Computing. So I hope that what I'm going to tell you is, will be of interest to people that are intrigued by the theory of computing and dedicate their careers to that. But I'm not making any claim that I'm contributing to the theory of computing or leveraging the theory of computing at all.
00:01:52.686 - 00:02:38.454, Speaker B: The talk that I'm going to give is. Is actually quite empirical and practical in a sense. But to the extent that theoreticians sometimes like to be inspired by practice and look at, you know, what their findings might mean for more practical settings, hopefully you will find something useful here nevertheless. But certainly it's not a theory talk. So let's talk about competition. This picture here was taken at the Summer Olympics in 1956, and you see a very typical situation. You see one guy at the top of the podium there, he looks mighty happy, and then you see a silver and a bronze medal winner, and they look sort of, maybe not quite as happy, right? Although certainly they did well.
00:02:38.454 - 00:03:52.334, Speaker B: And so competition is something that we enjoy as entertainment, but we certainly also value in science as a driving force that compels us to give our very best, right? And then when we succeed and we make the top of the podium, we are happy. And if we just narrowly miss, or not so narrowly miss to make the top of the podium, then maybe we are inspired to work harder next time around. So in sad solving, which is what this workshop is about, competitions have played a very important role for a few decades now. But not just in SAt. There are many other areas in computer science where solver competitions have really moved the needle, have helped us improve the state of the art and actually gain evidence, present evidence for these improvements. So problem solving competitions are great, aren't they? They assess the state of the art in solving a problem such as sats, so we know exactly how good we can be, and we don't have to rely on big O notation or something like that to tell us which algorithm is better. We can actually run them and see what the constants are.
00:03:52.334 - 00:05:03.104, Speaker B: They do provide incentives for improving the state of the art. We want to make the top of the podium so we work hard, our students work hard. They do provide guidance to practitioners and users of solvers. If somebody who isn't an expert in sad solving wants to use a sad solver for something, I bet it doesn't take long for them to find the sad solver competition pages, and particularly the result pages, and that might inspire them to prefer one solver over another. But there are problems, and the problems are that the results of such competitions are often statistically questionable, that the winning solvers are not always truly best in class, and as a result, neither reflect the true state of the art, nor do they provide a proper incentive for improving that state of the art. And those four or three and a half facts I call the inconvenient truths about competitions. Now, don't misunderstand, I'm not against competitions at all, but I think it's very important to recognize them for what they really are in order to do even better than the current setup allows us typically to do.
00:05:03.104 - 00:05:47.362, Speaker B: So I don't ask you to take my inconvenient truths at face value. I want to convince you at least a little bit that I do have a point there, actually three and a half points. So let's walk through this quickly. So my first point is about assessing competition outcomes. And the question there that I want to ask is how robust are the outcomes of competition, such as SAT competitions, to modest changes in the benchmark set. Because, you see, if they weren't robust to modest changes in the benchmark set, then, of course, the competition wouldn't be meaningless. But it wouldn't generalize very well, well beyond the narrowly defined setting of that particular competition.
00:05:47.362 - 00:06:35.634, Speaker B: And therefore, they would be much less useful, I think we can all agree, than if these findings did generalize, if they were this kind of robustness. And so the idea that a few students of mine and a colleague had a while ago was to statistically assess this existing competitions with respect to this question using resampling methods. And here's what you find. So, this is the 2014 SAT competition. So you can guess, we started doing this a while ago. And what you see here is on the x axis, the coverage metric, which is the metric that you maximize in order to win the competition. So, lingaling here is the solver that won the sequential set plus unsat track that year.
00:06:35.634 - 00:07:18.446, Speaker B: And then next was this swdiasby a 26 and so on. And so you see immediately a bunch of things. So the official competition results are indicated in red here. And what you see is that the winner actually had a bit of an edge, but then there were three solvers that were almost tied. And then it's an incredibly tight field up to around here, where then all of a sudden, you start getting into stragglers. Right? But now, if you use resampling, to be precise, bootstrap resampling, you can actually create confidence intervals around these point outcomes of the competitions. And that's what these blue bars are.
00:07:18.446 - 00:07:53.744, Speaker B: They're 90% confidence intervals. Bootstrap. Confidence intervals. And what you can see is that these overlap so much that we really shouldn't be too sure that the statistical. That the difference is between gold and silver and bronze, and actually the fourth and fifth and 6th and 7th all the way up to beyond the 10th. We should not have any faith that these are statistically significant at all. And in fact, if you look at how often a solver actually wins over 10,000 bootstrap samples or gets the same rank, then you get a very clear picture.
00:07:53.744 - 00:08:17.684, Speaker B: And that is that lingaling here, it does win in a significant number of these samples, but not even half of them. Right. And in the middle here, these solvers, the ranks that they got in the official competition, are almost completely arbitrary and not robust. What is actually very stable are the bottom ranks. Right. And if you look back, that makes perfect sense, because clearly this is highly significant. Huh.
00:08:18.864 - 00:08:20.364, Speaker A: Olgr, can I ask a question.
00:08:20.784 - 00:08:22.000, Speaker B: Yeah, of course. Yeah.
00:08:22.032 - 00:08:30.364, Speaker A: So, going back to your confidence bands, it does seem like those bands are all the same width, is that correct?
00:08:30.804 - 00:08:32.652, Speaker B: And, no, they're not.
00:08:32.708 - 00:08:33.664, Speaker A: They all seem to.
00:08:35.684 - 00:08:55.752, Speaker B: This one is actually a little narrower. So the fact that they all have roughly the same width, that's just an indication that, interestingly enough, these solvers sort of have the same statistical stability with respect to small permutations, actually resampling of the underlying benchmark sets. But you can see differences. Right. This is much narrower than this one here.
00:08:55.948 - 00:09:07.684, Speaker A: But looking at that stack on the right there, that's very close solvers. It looks like the competition red dot is right in the center of the band. Always. Is there something going on there that I don't understand?
00:09:08.184 - 00:09:30.098, Speaker B: Yeah, that just means that it's actually not plus minus standard deviation, if that's what you mean. Right. And if you look very closely, this, for instance, is not really in the middle of that band. Right. But it turns out that the competition outcome wasn't atypical, so it wasn't just luck. Right. But it is true, actually, that there are many better outcomes and many worse outcomes as well.
00:09:30.098 - 00:09:59.276, Speaker B: That's just the way this cookie crumbles. But it's a good observation. I think the reason why it's relatively symmetrical is also because we did take 10,000 samples, so it's a pretty big sample in there. Okay, let me go on. The second point is assessing the strength of solvers rather than the degree of tuning. And that is an argument that has been made by many people before it goes like this. We know that solver parameters have big impact on performance, and different values are good for different types of instances.
00:09:59.276 - 00:10:54.108, Speaker B: So when you actually launch a solver in a competition, you can never be quite sure whether its parameters are chosen well for that particular competition. Now, what you don't want is that a solver basically doesn't make the podium just because it was unlucky in terms of parameters, where somebody else had a lucky guess in the parameter, and that's the unfair tuning problem. So if somebody had maybe a better idea of what kind of instances would come up, then maybe they would have tuned themselves a little bit more to them. And again, it wouldn't generalize very much if we looked at different benchmarks. And so the idea here is to use automatic configuration. So sort of a very uniform and principled and explicit mechanism as a key ingredient of the competition itself. And then, you know, if indeed this is a concern, then we will see that if you configure all solvers using the same computational resources and the same protocol, you will get very different results.
00:10:54.108 - 00:11:35.154, Speaker B: But if it's not as big a deal, then maybe you get the same results as in the official competitions. And we again started looking at this in 2013 and ran two competitions called the configurable SAT solver competitions. Together with some students and Kevin Dayton Brown at UBC, we applied automatic configuration with a uniform protocol and budget to all submitted solvers. We had multiple high performance configurators, even to make sure that we don't get misled by a configurator that's particularly effective for certain solvers. And then we ranked based on the performance of the configured solvers per instance set. And of course, we didn't configure on the final competition instances, but on some training set. Right.
00:11:35.154 - 00:12:05.540, Speaker B: So here's lingerling. Once again, this is 2013 configurable SAT solver competition. And what you can see is default performance of lingerling. And each of these markers here is one instance and then the after configuration that performs. And this time it's time for solving the instance. So lower is better. What you can see is that for many instances, there is a speedup between anywhere between two and 100 fold, and sometimes even more than 100 fold that you get by configuring the solver.
00:12:05.540 - 00:12:35.412, Speaker B: And that's lingering. That's a solver that is expert configured to start with, but just not for these particular benchmarks. The average running time on these sat encoded software verification instances, so pretty realistic instances, actually drops from 3.32 to 0.16 cPU seconds through configuration. And it raises from rank four to rank one on industrial Sat and unsat. So in other words, and this is configuring all the competitors as well.
00:12:35.412 - 00:13:08.224, Speaker B: So it turns out that lingerling, actually unconfigured, wouldn't have won that track of the competition. Configured it easily wins it. That's interesting to know. And that's not just lingerling. Here is clasp a very different type of Sat solver as many of you know, on SAt encoded and problem, very different kind of SAt instance as well. In the 2014 configurable SAT solver competition. Here, the part ten value, which is again a cpu time measurement, drops from 705 to five cpu seconds, and the rank rises from five to one on crafted SAT and Unsat.
00:13:08.224 - 00:14:02.662, Speaker B: So in other words, if you didn't configure, you would have had somebody else on the podium, which arguably deserved a little bit less to be there, just because they would have made the podium due to unlucky choices in default parameters on clasp in this case. The third point is about leveraging performance complementarity. And this is something that I should point out. The workshop is called 50 years of satisfiability. And one of the things that happened in these 50 years is not just that we got amazingly good solvers and lots of progress there, but also, we learned. And sad, actually, we learned this arguably first in all of computer science, that by actually running more than one solver, you can typically get better performance, or at least considering more than one solver. And examples for these sorts of techniques are parallel algorithm portfolios, or, in machine learning ensembles, algorithm schedules and pipelines, where you run one after the other.
00:14:02.662 - 00:14:45.118, Speaker B: And, of course, for instance, algorithm selectors. SAt is sort of famous for that because that's, to my best knowledge, at least, the first major problem in which it was shown that it's practical and not just a theoretically nice idea. This is where you use a feature extractor to drive a selector that then selects quickly a given algorithm, a given SAT solver that's supposed to work best on the instance that you actually have. Okay, so let's just. And, of course, we know that these things are very good for SAt solving. And, in fact, they really do define the state of the art rather than individual monolithic solvers. And that has been shown in competitions as well.
00:14:45.118 - 00:15:27.344, Speaker B: So, in summary to this part, competitions have helped us advance the state of the art in solving AI problems such as SAT, but also AI planning, machine learning, and so on. So competitions certainly play an important role. However, they can suffer from noise and low statistical significance. And I've shown you examples for the SAT competition here. But we've done the same for machine learning competitions, computer vision competitions, and found exactly the same phenomenon. They typically involve manual, out of context tuning, which also is a bit of an issue in terms of generalizability of the observations. And they're mostly focused on single solvers and broad spectrum performance, which is, again, not what you might really care about most when you apply these things in practice.
00:15:27.344 - 00:16:07.764, Speaker B: And as a result of all of these factors, competitions often don't reflect the true state of the art. And so there's still a good incentive for us to do better with our algorithms. But looking at that and going like this won the competition that got gold in the sad competition, therefore, truly is the world's best sad solver. That's a tricky judgment call to make. And because they don't reflect the true state of the art, they also typically don't provide an effective incentive to improve the true state of the art. And this is where we get to this concept of cooperative competition, which I think is actually a very interesting one. The idea is there, in a competition, we have multiple competitors.
00:16:07.764 - 00:17:06.444, Speaker B: Anyway, by definition, it's a boring competition if there's just one competitor, and we measure on an absolute scale, and that's it. So competitions thrive from having a broad field. So why don't we always leverage the strength of all competitors rather than just trying to designate a winner? Why don't we switch to the goal of maximizing team performance, quote unquote. And why don't we reward contribution to team performance? And this is what we started doing a couple of years ago in the so called sparkle challenges. So here we built a competition platform that we call sparkle because it makes everybody sparkle a little bit more than if you just have brown, silver and gold, you'll see in a moment why that is. And solvers are submitted to that platform. That platform then fully automatically builds a state of the art per instance selector based on all the solvers that have been submitted.
00:17:06.444 - 00:18:23.644, Speaker B: And we then assess the solver contributions to the overall performance of this selector based on relative marginal contribution, which is a pretty obvious concept. I'll talk a little bit more about it later. That had been used in some of our earlier work and many other pieces of literature. Also, what we found useful and inspiring to do is to have a concurrent assessment and solver development phase, so called leaderboard phase, before the actual real run of the competition. And then we give credit for contributions to selector performance, and that goes to the component solver author, so that if you make a good component that brings the team a lot further, that selector a lot further, then you win. So this is the equivalent of going from this, where the guy on the top of the podium has gold and smiles and everybody else doesn't smile, really quite as much to having a gold medal and cutting it up and giving everybody a slice of the gold medal that reflects the contribution that they made to their team. Everybody together, defining the true state of the art.
00:18:23.644 - 00:18:55.680, Speaker B: So we did this in 2018 as part of the Floc Olympic Games. It was closely coordinated with the SAT competition. The 2018 SAT competition was launched in March 2018. There was a leaderboard phase over ten days in April, and then final results about a month later, 19 open source solvers were submitted and four closed source or concur solvers. This is not much worse than the main SAT competition. So a pretty broad field once again, and many of the solvers were actually solvers that also participated in the main competition. If you're interested about details.
00:18:55.680 - 00:19:30.214, Speaker B: Here's a URL. You can even click it later when the slides are online. How did we do this? Well, of course we needed training and testing sets. We can't actually build the selector based on the ultimate competition instances. That would not be proper methodologically. So we constructed a training set consisting of over 1000 instances from 25 families, all from previous SAT competitions and races, as you would do if you wanted to see how well your solver performs that you're about to enter into a new competition. Of course, you know, you tried on old competition data.
00:19:30.214 - 00:20:18.594, Speaker B: And then we had a testing set which was 400 instances from 23 families that was identical to the testing set of the main track of the 2018 SAT competition and unknown to us until the end of the leaderboard phase. So even us as competition designers didn't know that set. And in particular, we couldn't have an informed training set construction. And this will come back later. So we then split the training set into a core training set and a validating set. We did that basically by randomly selected family selecting families, and then basically that left us with a slightly larger core training set and a slightly smaller validation set. Based on this, we constructed 100 per instance selectors using autofolio, which I'll say a little bit more in a moment.
00:20:18.594 - 00:21:28.594, Speaker B: Selector construction method from the literature, we trained on the core training set and we chose the one of the 100 selectors with the smallest part two score, which is just the official SAT competition score every time, penalized by a factor of two on the validating set. Right, that's what we use the validating set for. So why autofolio? Autofolio actually is arguably, or was, at least at the time, the most powerful automatic solver selector building framework. It is an automatically configured one. So it actually has the ability to search a large space of possible per instance algorithm selector construction methods and use the best for a given context. It's based on a very well known flexible per instance algorithm selection framework called class 402 coming out of the ASP literature, and it leverages state of the art general purpose algorithm configuration, namely the smack method that is quite widely used. So what it means is that it's a cutting edge and robust algorithm selector construction method that we've integrated into the sparkle platform.
00:21:28.594 - 00:21:39.710, Speaker B: How do we assess solver contributions? Well, that merits also a little bit of thought, because that's what we're going to ultimately reward in our scheme. So, given a set of solvers, let's.
00:21:39.742 - 00:21:51.484, Speaker A: Call that set question if it's not that time. Yeah, this is wondering, to configure these solvers that are submitted, do they provide an API to you that they have to conform to or do you actually.
00:21:51.524 - 00:21:52.412, Speaker B: Oh, good question.
00:21:52.548 - 00:21:53.904, Speaker A: Parameters and so on.
00:21:54.404 - 00:22:25.544, Speaker B: In here, we don't configure solvers, so I should have made this very clear. We configure the solver selector, but the individual solvers are used exactly as given in the competition. We'll get back to that at the end. We could configure it, but it would make things a lot more complicated. It would be a good idea, by the way, but we didn't do that. So we just took individual solvers as in the normal SAT competition, but we then configured a selector so the configuration only happened on the selector, not on the solvers. Thanks for asking.
00:22:25.544 - 00:23:37.412, Speaker B: Okay, so how do we assess solver contributions given a set of solvers in a per instance selector? Based on that set of solvers and the instance set, we define the absolute marginal contribution, which essentially is the reduction in part ten score that you get when you take out a particular solver. Because these part ten scores are running times. We do this in a multiplicative way and then we take a log so that we get nice numbers. Then what we really do is we normalize that so that everything sums to one to get relative marginal contribution. So that way we can then cut up our gold medal more easily, right? So that somebody who gets 15% basically better have a relative marginal contribution of 0.15. So here's what happened here. We have the first leaderboard, the last leader board, the final performance on training set, and the final performance on testing set, which was never seen before and actually even unknown to us organizers back here, okay? Unknown to us organizers, because we took it from the main track of the set competition and these guys knew it, but kept it secret from us.
00:23:37.412 - 00:24:09.054, Speaker B: And these are the part two values. So low scores are good because it means fast solver performance. So this is the single best solver, right? So the very best solver on the data that we have, you can see there was a bit of an improvement between the first and the last leaderboard and then another improvement on final training. That's because some people chose to not reveal their solver before the leaderboard was closed. So they never competed in training, they only competed in the actual race. Right. And then on final testing, it was a lot worse.
00:24:09.054 - 00:24:43.184, Speaker B: And that was because the guys who organized the SAT competition, they thought, let's challenge these other guys and make a final testing set. That's a lot harder than anything that has ever been seen in the previous competitions and they did succeed in that. You can see that it's, you know, on average these instances are, you know, almost twice as hard as they were at this point. So now let's look at the virtual best solver. This is the best solver. If you could for every instance pick the one solver that solves it best magically, somehow. So a perfect oracle.
00:24:43.184 - 00:25:07.044, Speaker B: So of course this is a lot better than the single best solver. And you can see that on the last leaderboard, it's even better. On final training it's yet a little bit better. And on final testing it's still significantly better than the single best. Right. And so this is the gap into which we want to get with a realistic selector because, you know, this would be a perfect selector. Nobody knows how to build this.
00:25:07.044 - 00:25:37.672, Speaker B: So this is what our sparkle selector could do. Very close on the first leaderboard to the, to the virtual best. Still pretty close here. Still pretty close here. And better than the single best, but quite a bit worse here. How did that happen? Well, turns out the final testing set was not just much harder but also really different in terms of the types of instances. And therefore, of course, the selector performance didn't generalize as well as one would hope, essentially because these instances were too different.
00:25:37.672 - 00:25:50.812, Speaker B: But still, we're doing better. We're closing part of the gap. Right. But now what's very interesting is this. This is standalone versus relative marginal contribution. So standalone is part two. So low bars are good.
00:25:50.812 - 00:26:35.594, Speaker B: These here are the winners of the competition, right? These would be the winners of a competition, of a traditional competition where you just look at the performance of an individual solver. This is the vbs, a ton better. This is the sparkle selector. And you can see that the difference between gold, silver and bronze is much smaller than the difference between that and these ones here. So now let's look at the relative marginal contributions. And the shocking thing is the best solvers don't have the largest marginal contributions, but even stragglers have sizable marginal contributions. Right? So including these guys into a selector makes a lot of sense.
00:26:35.594 - 00:27:02.410, Speaker B: This guy here is a ton better in standalone performance than any of these guys here. But including that solver doesn't give us any advantage. Completely dominated by something else. Okay, so now let's look on testing set. I told you the testing set is very different. And now you can see the difference between silver, gold and bronze there, not unlike previous competitions that I've shown you before, is very, very small. And then there's a bit of a middle field and then there's a big gap.
00:27:02.410 - 00:27:31.404, Speaker B: And here's the rest. Right? Notice what I've shown you before was the 2014 competition. This here is 2018. So four years later, qualitatively it looks exactly the same. Now, the bbs still is a lot better. The sparkle selector is only a little bit better than the gold medal winner. But look, the difference between this and the difference between gold and silver is much, much larger, right? So in other words, this one is actually significantly better than those two.
00:27:31.404 - 00:28:21.304, Speaker B: Okay? And again, on the testing set, you see the marginal relative contributions over here. And by the way, you read this as a percentage scale here, you can see that it's not the case that the best solvers contribute the most to performance. So this is interesting. And just to put it into numbers where the SP's is 4740, the VBS 27 ten, the sparkle selector actually closes 14.3% of that hypothetical gap, which is not great, but also really, really good, especially compared to how small the gap between gold and silver is in here. So here's the official results from our competition, which is now ranking according to marginal contribution on the testing set here. Crypto mini set actually wins.
00:28:21.304 - 00:28:50.402, Speaker B: And that is the one that also has standalone rank one, but only very narrowly. But its relative marginal contribution is only 12.97%, so they only get like 13% of the gold medal. The second one happens to be in this case also the same, but with less than 10% now. But now look at the third one. This has a relative marginal contribution, almost as high as the second, but it's ranked only 11th in terms of standalone contribution. And it continues to go down like this.
00:28:50.402 - 00:29:32.558, Speaker B: Right. There are also examples such as this one, which is rank four in terms of absolute performance. It only ranks six with seven point something percent in terms of the relative marginal contribution. Now an interesting question is what would happen if we could train on instances that are actually from the same families as our testing instances, which for practical applications is a lot more relevant. You use sat for solving software verification. Well, chances are the instances you're going to see are similar to previous software verification instances rather than similar to whatever n Queen's instances. So this, I would argue is a much more realistic setting.
00:29:32.558 - 00:30:07.396, Speaker B: So what we now did is we took the testing set of 400 instances from 23 instance families and we split that set into 50 50 training and testing set for each family. Right. And so then what we get is a very different result. We can now close 55.7% of the gap because of course our testing instances now in a proper, supervised, machine learning way. They are actually drawn from the same distribution in a sense, as the training instances. It's easier to see this in pictures.
00:30:07.396 - 00:30:50.428, Speaker B: This is what this looks like, again, qualitatively very similar on the new testing set. This is the virtual best solver, the perfect oracle. This here is the sparkle selector. You can see now we are actually in the middle of that gap between a perfect predictor, which, unfortunately, we don't know how to make, and no predictor whatsoever, just choosing the best. And notice how big the difference is between the gold medal winner in a traditional competition and the real state of the art performance we get here. This is a difference that's almost as big as the difference between sort of the end of this winning field and then the stragglers. And again, what you can see is that the relative marginal contributions are very different.
00:30:50.428 - 00:31:28.274, Speaker B: And here you can actually see a case where the biggest contribution comes from, actually the last one in this first leading field, but, you know, a solver that. That would have never even gotten close to the podium in a traditional competition. And you can see that even those stragglers back there jointly, they actually contribute more than the first two together. So this is interesting. We were wondering, of course, whether this is another case where, you know, sat can sort of trace, that can. Can basically blaze the trail. And we tried the same for AI planning, which is another domain in which competitions have a very long history.
00:31:28.274 - 00:32:02.330, Speaker B: So AI planning, they've had competitions as long as the SAT community, actually, and they pay a lot of attention to them as well. And so here we choose a setting that's similar to a very prominent track of the international planning competition IPC series called the Agile track. I don't want to explain it in detail, but it is one of the tracks that people really like competing in and pay a lot of attention to. We launched it in June 2018, actually. Sorry, June 2019. That's a typo here with the leaderboard face this time of four weeks. So we wanted more leaderboard and final results.
00:32:02.330 - 00:32:50.514, Speaker B: Then at the ICAHPs conference, ten open source planning systems are submitted. That's because not quite as many people work on planning, on at least that kind of planning than on SAT. Again, you can find the details on the competition website here. But just to point out some of the difference to our SAT challenge, we had a longer leaderboard phase, 30 versus ten days. We had a stronger notion of instance families, because in planning, this comes naturally. They have so called planning domains, always had this from the beginning, and a large number of diverse domains for training. So 52 domains actually versus 25 for satisfying, fewer competitors because there's just fewer people who make state of the art planners and a different performance metric, part ten versus part two because that's what they like to use.
00:32:50.514 - 00:33:22.384, Speaker B: So I'm just going to walk you through the pictures here. So you can see this is now just on testing set, not training set. You can see that these are the standalone solvers. This is the virtual best, the perfect selector, and this is the actual one. And you can see how much of this gap we actually close. So we now close the major part of this gap. And again, the relative marginal contributions here you can see the stragglers don't contribute, but for instance, this number six solver here contributes actually way more than the number three solver.
00:33:22.384 - 00:34:09.394, Speaker B: So qualitatively very similar to what we've seen before. And again, there's a nice improvement over time. So you can see through the leaderboard things get better, and not just in terms of standalone performance, but also in terms of the final performance of our red sparkle selector, which are approximations of this tool, state of the art and the blue lower bounds on the state of the art. And then you can see that this time this generalizes very nicely to testing, which is because there is this widely accepted and very relevant notion of instance families. And the testing set was in that sense more representative of the training data we had. We're getting closer towards the end of this talk. So let me try to summarize a bit.
00:34:09.394 - 00:35:33.364, Speaker B: What are the advantages of these cooperative competitions? And by the way, why do I call them cooperative competitions? It's quite clear by now, right? The way you win this competition is not by being better than anybody else. The way you win this competition is to basically fill a gap that nobody else can fill, right? So you get the maximum slice of your gold medal by making the maximum contribution to team performance. And that is an inherently cooperative act, right? You're still competing, but you're competing by maximally increasing team performance and therefore your improvement is actually relative to everybody else, right? If you change the team members, then your contributions will also change. And so the advantages of this is, first of all, it makes it easier to gain recognition for specialized or niche techniques, right? That could never win a competition old style, right? And this is as much as, you know, if you have a football team and everybody is a great forward player, that will be not a good team because, for instance, you also need a good goalie. But if you have a team that's just goalies, they will also not do well. And it's essentially the same here, right here you can shine by being a good goalie. This also better reflects and makes accessible the state of the art.
00:35:33.364 - 00:36:31.540, Speaker B: If somebody looks at a sparkle challenge and says give me the state of the art, we can, because what it is, it's the sparkle selector. Now this isn't to say that you couldn't potentially make something even better, for instance, if somebody comes up with a better method for building selectors based on given solvers. But certainly it's much closer to the true state of the art than is a single best solver. As I've demonstrated. It also leverages synergy, complementarity, and these meta algorithmic design techniques that have been pioneered to a certain extent in SAT. And what I mean by meta algorithmic design techniques is specifically algorithm selection here, and secondarily, algorithm configuration as used to make the best selector in this case, but now are being used basically everywhere. And finally, because this captures better, more accurately, although not perfectly, the true state of the art, it also provides a better incentive to improve the true state of the art.
00:36:31.540 - 00:37:14.604, Speaker B: Because the true state of the art is a team performance, not a solo performance. There are some further benefits. A setting like that makes automatic solver construction much more accessible than it would be otherwise. The people who submitted solvers to our competition didn't need to know the slightest thing about how to run autofolio. They didn't know how to make a per instance selector, right. We did that for them in a completely transparent way. It also promotes and ensures best practices in the use and evaluation of such techniques, and it gives you improved reproducibility.
00:37:14.604 - 00:38:26.284, Speaker B: And it does that because the selector is not some handcrafted thing, it is built by the algorithm so you can look deep inside and understand precisely what's happening. So what's next? We're not done here. And actually, Sam was very, very quick in picking up on something we haven't done yet, which is we use automatic configuration, but we don't apply it where we really should, namely to the individual solvers. We did that in the CSSC series, but we didn't do it here because there are some challenges associated with that, not just in terms of requiring more time in terms of running the competition, but also in terms of making the configurator as accessible as we currently make the selector to competitors. Of course, we could extend this to parallel portfolios, which would probably be a wise thing, given that if you really wanted to do sat solving in practice and wall clock time matters, real world time matters. You would, of course, run more than one solver in parallel. Maybe we should also look at optimization of solution quality so we can apply this to problems such as scheduling problems and machine learning problems, where running time isn't typically the thing that you're most concerned about.
00:38:26.284 - 00:39:03.100, Speaker B: Also, there is still an issue. We assess contributions in a pretty reasonable way. But if you think about it carefully, it's not perfect, right? Because, for example, if you insidiously submit the same solver twice, but mask that so that it's not obvious that you have two copies of the same solver, both of them will get relative marginal contribution zero. Right? In other words, you can wreck somebody's success in the competition by copying their solver and submitting it. Now, in reality, we've watched out for that. And, you know, with ten or 19 submissions, you can. And there is telltale signs that something like this happens.
00:39:03.100 - 00:40:20.806, Speaker B: But it would be nice to automatically recognize and deal with adversarial behavior. And there is a little bit of a sort of, you know, applied game theory challenge here. And then finally, and from my perspective, perhaps most exciting, it would be nice to have mechanisms for deeper collaboration, to go from cooperative competition to what I would call competitive cooperation. And one way of doing that would be to say, well, you know, maybe Sam has a good solver and I have a good solver, and we've already found enlightenment in feeling that we shouldn't sort of just force anyone to pick his or mine, but we should, you know, have a per instance selector that tells people, for this instance, use sams and for this instance, use mine. Right? But maybe it would be even nicer if somebody else, like, say, Stefan here could take Sam Solver and mysolver and tweak it a little bit and make his own variants. Right now, I would love that. But of course, if those then make big contributions, I'd like to be acknowledged somehow, right? And mechanisms that allow that, right, that allows sharing and reuse of components and give fair credit for that would fit very nicely into the context of this overall intellectual effort that I'm describing here.
00:40:20.806 - 00:41:01.394, Speaker B: And then, of course, from a purely pragmatic point of view, it would be nice to have further sparkle challenges, especially after we've realized some of these extensions that we're currently working on. And we are indeed pretty close to finishing the first of these we're working hard on the second. The third we haven't paid too much attention to. But it's also, I think, not that hard. And these ones we've started to think about pretty deeply. So, let me wrap up in a nutshell. Competition is useful in SaT, in AI and elsewhere, right? Competition makes us give our very best when we want to solve a problem, and that's useful.
00:41:01.394 - 00:42:21.444, Speaker B: But success in competitions, as in the real world, should actually depend on teamwork. And we know that because the best solvers are typically not built by a single person, but by a team of scientists. Most impactful papers are not written by a single author, but by a team of authors, and are better for it. So why, in a competition, should we have a single solver that is winning gold? Right? So that's why I think we should go to cooperative competition, where we compete for contribution to team performance, where everybody does well by having the team do maximally better. This is enabled by meta algorithmic design techniques such as automatic configuration and automatic selection, which have to a large degree been pioneered or at least developed in SAT, and now are being made accessible broadly in the sparkle platform that my group is developing. And finally, I also think that taking this perspective, we better leverage human creativity and diversity of perspective, and also the complementarity of individual solutions to challenging problems. And that's pretty important, because this morning when I woke up, I saw that SpaceX had just managed to, for the first time, not only land their starship prototype, but also not have it explode a few minutes later.
00:42:21.444 - 00:43:31.798, Speaker B: So I think it's pretty difficult for us to overestimate the impact of that, because fully reusable access to space will really be a game changer, and possibly, as Elon Musk likes to say, even a game changer for the species. But let's not kid ourselves. This planet that we're looking on here, actually straight down to the North Pole, is the most habitable and the best place that we know of in the universe, and certainly the best one accessible to us. So we better take good care of it. And to do that, we have to protect things like this, which actually is the Arctic ice cap, and that thing is shrinking, and that's not good for us. And so it is in this sense that, I think, in computer science and beyond, we should really pay close attention to what Alan Turing already said as early as 1948, the search for new techniques must be regarded as carried out by the human community as a whole. And in that sense, really, it's about high time to switch from all out individual competition to cooperative competition in science as much as in our other endeavors.
00:43:31.798 - 00:43:40.058, Speaker B: Thank you very much. That is the end of my presentation. I think that should leave us with quite a bit of time for questions. Thank you.
00:43:40.066 - 00:43:59.650, Speaker A: Very much. That was a very nice talk and very stimulating. And I appreciate the comments on the earth at the end as well. Very nice theme. We have plenty of time for questions. I have a list of my own, but they're already starting to stack up in the Q and a window. So we'll go to those first and we'll see what we have time for.
00:43:59.650 - 00:44:10.296, Speaker A: I encourage everyone else to post questions to either the chat window or the Q and a window. So the first one is from Philippe Bartek. Can you see this, Oliver? But I'll read it anyway.
00:44:10.360 - 00:44:12.392, Speaker B: Yes, I can actually see it.
00:44:12.568 - 00:44:21.364, Speaker A: So he asks, have you considered using contribution measures other than, quote, relative marginal contribution, unquote, what are their advantages and disadvantages?
00:44:22.104 - 00:45:23.602, Speaker B: So the answer is, we have actually, Kevin Layton Brown, myself and students that we jointly work with have considered Shapley value, which is a well known concept from cooperative game theory and a variation of it that we introduced called temporal Shapley value. That sort of pays a little bit of attention to precedent solvers that were there before others came. And it would doubtlessly be good to use to sort of cut up the gold medal based on these, because they address some of the issues that I pointed out with respect to, for instance, masking the contribution of others. The problem is these become computationally very expensive. And so, in Sparkle, we didn't implement that yet because we couldn't find out a way to compute this reasonably accurately within reasonable time. But certainly it's something we're exploring. And I do think that a better way of assessing contributions than relative marginal contribution is a very nice improvement to our overall framework.
00:45:23.602 - 00:45:26.934, Speaker B: So I agree with you, Philippe, that that would be good to do.
00:45:29.174 - 00:45:40.754, Speaker A: Good. Matias asks Matthias Florey, when are solvers dissimilar enough to avoid the adversarial problem? Are solvers from hacktrack dissimilar enough?
00:45:42.294 - 00:46:30.054, Speaker B: That is a fascinating question. I like particularly the second part of the question, right, because it is something that I've always wanted to look at, and we haven't done it yet. So, Matthias, unfortunately, I don't have the answer for you yet, but realistically, I should just tell one of my students to run that experiment. My suspicion is some of the solvers that we dealt with here that form this tightly packed field. Let me actually go back to ground this a little bit in figures here. As many of us in SAts solving know, many of these solvers here are actually not that dissimilar. Right? So many of them are based on the same underlying ideas.
00:46:30.054 - 00:47:24.226, Speaker B: And so what you can see is that certainly for them, this masking problem doesn't seem to be too harsh. Right. It's difficult to tell it just from this figure, but at the very least, things like this could be potentially knocked out because of this. But then you look at solvers that are very closely related to these, and you see that they actually do get very sizable relative marginal contribution. So based on that, I suspect that even in the hack track, it's not a huge issue. But it bothers me, not because this adversarial setting, I think, distorts our results, but more because I think, in part here you see me arguing that a traditional competition gives us the wrong incentives, and this competition still gives an incentive that's sort of manipulable a little bit too easily. Right.
00:47:24.226 - 00:48:05.954, Speaker B: Even if the manipulation doesn't happen and isn't likely to happen, perhaps accidentally or in something like the hack track, the fact that it's possible really bothers me. So I do think that, first of all, we should have a better answer than I currently can give you, Matthias, to your question number one and two. And secondly, we should really find a mechanism that makes this question redundant, where we know that the incentives are right and that we are protected against adversarial attack. Ah, yes. And here's Armin with a long comment. This is interesting. Yes, I've seen that, Amin.
00:48:07.694 - 00:48:29.434, Speaker A: Let me just summarize it. It's too long to read, but I'll just say Armin's talking about subsampling and their paper from 2000 implants 19, I believe it was, showed that ranking is stable relative to subsampling. But I'll let you take the rest of that over.
00:48:29.974 - 00:49:01.344, Speaker B: Yeah, Armin and I have a friendly collegial disagreement about that. So just as Armin doesn't trust our results too, too much, I don't trust his results. Here are their results too, too much. I think we need to look a little bit more deeply into this, what's really happening here. But I would be. I think it's probably an artifact of the way stability is assessed in this work versus ours. What I would suspect is that in reality, there are situations where stability exists.
00:49:01.344 - 00:49:28.516, Speaker B: Right. And I mean, I have to tell you honestly, of all the competitions we looked at, it's a little bit beyond the scope of this talk. But let me go back to this figure. Of all the competitions that we looked at in terms of stability, the SAT competition is the worst. In particular, that competition that I was talking about here. Let me just find the picture. That competition here was perhaps particularly bad.
00:49:28.516 - 00:50:28.984, Speaker B: Right? On the other hand, what you have seen that this extremely close field here, it happened again in 2018, and that is what Armin is referring to here. And so therefore, I would be extremely, extremely surprised if you had statistical stability in the rankings. That's much better than here, just given how close the performance is in the 2018 standalone competition. Again. So what I would say is that, yeah, probably by teasing out from the benchmark set, by having a benchmark set, that sort of avoids that which other competitions have managed better than at least some sad competitions in the past, you avoid this phenomenon. But regardless, the most important message is not, did this happen in this competition, and did it again happen or not in 2018? The most important thing is that before the work that Armin mentioned here and the work that we did actually before that likely inspired some of this. It is an issue, right.
00:50:28.984 - 00:50:56.620, Speaker B: The stability of competition results needs to be actually evaluated. And then if we find that it's stable and we can trust the results, that's great. And if we find it's unstable and we can't trust it, we've learned something useful, too. But it's simply not acceptable to just assume that it's stable and then not study it. Right. And in that sense, the work that Armin mentions and ours both make the same important point. And that is, it's important to assess the statistical stability of competition outcomes.
00:50:56.620 - 00:50:59.944, Speaker B: And sometimes it's stable and sometimes it's less so.
00:51:06.024 - 00:51:39.464, Speaker A: Well, Armin's not a panelist. I could promote him if he wishes, but he can also type back an answer if he wishes. Why don't we move on, though, and we come back to this issue as people raise it? Paul Beam asked a question in the chat window. Paul, do you want to speak up? Or I can read it for you. Let me go ahead and read it. So one of the items that the usual SAT solvers are measured on in the SAT competition is the number of unique instances solved. Do you have comparisons with that and relative marginal contributions? Is that where much of the benefit of low performing solvers is coming from?
00:51:40.564 - 00:52:18.124, Speaker B: Yeah, that's a very insightful comment. So, you know, all these studies, this one here, and also what we did later, we didn't use this, although I agree with, with Paul that, you know, this is a better measure that uniquely solve instances. Right. However, what we wanted to do is we wanted to compare this with the competition as it stands today. And there they uniquely solved is not the way gold, silver and bronze are adjudicated. Right. Having said this, I think uniquely solved is in some sense, hmm.
00:52:18.124 - 00:53:48.334, Speaker B: It is in a similar way, perhaps giving you an impression of of how large the relative marginal contribution ought to be, or could be, as the VBS tells you with respect to how good a selector could be. However, I firmly believe that if you want to assess contribution to team performance, why do it in such a roundabout way? Even if you end up capturing some of the same things, why not capture the thing that you really care about, which is not uniquely solved instances? It is contribution to team performance, and absolutely, somebody who has a large swath of uniquely solved instances should be expected to contribute more, probably in a formalizable way. That should give us a bound on relative marginal contribution. But why not measure the real thing? Now what I find interesting about Paul's question is, and this is maybe a question that a theoretician could answer on the spot, and I unfortunately can't, although I have my suspicions, maybe uniquely solved instances in a very crisp and precise sense does give us a bound on relative marginal contribution, at least to a perfect selector. I think that's quite possible, and if it does, then of course maybe it deserves to be shown in these plots. So Paul, if you can see immediately how this is done, I'd be very curious, and otherwise, I'll take it away after this talk and give it a little bit of thought.
00:53:50.524 - 00:54:17.744, Speaker A: Okay, next question was from Chiarin Rakrish, sorry if I'm mispronouncing the name, has a practical suggestion to make this be more widely usable. But they write maybe this work could be used for a website that lets people upload a few of their instances and be told, quote, if you want one solver, use x configured as y. If you want more, dot dot so that non experts don't just pick the competition winner with the default configuration.
00:54:18.874 - 00:55:09.462, Speaker B: Yeah, I think that's a very nice idea. And in fact, we're currently writing a paper about sparkle and cooperative competition, so we hope that this will make it into a special issue on benchmarking of a journal. That's, interestingly enough, in the area of evolutionary computation. But that's another area that, not unlike sat, it's full of people who take benchmarking pretty seriously and have reasonably high standards. So we're going to put it there just because that's the next deadline that comes up that we feel is suitable for this kind of work, and they are actually an idea that's not so dissimilar from this will turn up. Although I do like Karen's idea that is even a little bit different from what we had in mind there. So, yeah, that sounds really interesting, by.
00:55:09.478 - 00:55:38.864, Speaker A: The way, I should say, for all these questions, a chance to do follow up questions. I don't mean to just pick them off one at a time. In the lieu of anyone speaking up more, I'll just keep going on through the questions Mohasi malaria asks. Is it an issue that Sat is a very low level, unquote, problem? Wouldn't the cooperative approach be more effective in setting that leverage problem formulation in a higher level language? That is, before encoding the CNF, what is your experience?
00:55:39.924 - 00:56:45.662, Speaker B: So that's exactly why the second sparkle challenge was on AI planning, right? Because, I mean, I tell you honestly, the reason we went to SAP first is because it's a community in which my team and I had a lot of experience with competitions, because, you know, we had submitted individual solvers and we've configured solvers and we had submitted selectors. So we knew a lot about that competition series. And it's always nice to try new ideas sort of with your own community. But when it worked there, the obvious question was, wouldn't it work even better in something that hides less of the structure of the problem that we're solving? Because these 25 families of problem instances, many of them are highly structured instances, as you all know. It's an insightful question that is being asked here. What if we can exploit the structure more explicitly? AI planning is one of these areas where people have encoded into sat and solved as sat and actually gotten very good traction. But then there are still people who write planners, and for a good reason, because the planners can be even better in many cases.
00:56:45.662 - 00:57:41.858, Speaker B: And what we found, and that's exactly why I presented these results today. And I would expect similar results for other sort of richer domains like QBF and SMT and, you know, ASP, where we haven't run that yet, but I think it would be very similar. I think you will get qualitatively very similar results, but. But even better. And it will be even better because in the SAT competition, you have this notion that, yes, families exist, but ultimately nobody is taking care that you have the supervised learning scenario proper, in that ultimately, the instances that you put as a challenge can make a reasonable claim of being drawn from the same distribution as the training instances. And that really is different in the planning community. And it's different in many other communities where the recognition that there's different types of instances and they're very different, and you shouldn't sort of really mix them together quite as readily as we do in Sat is much more developed.
00:57:41.858 - 00:57:53.654, Speaker B: So yes, I agree that this is even better for problems where the structure is more explicit than it is in Sat, at least in CNF SAT.
00:57:58.214 - 00:58:08.114, Speaker A: I missed a question from the Q and a window earlier. Zod Sawaf asked, how is RMC relative marginal contribution defined?
00:58:08.854 - 00:58:46.094, Speaker B: Yeah, I apologize, I went over the slide pretty quickly. But, you know, here it is. So what it is is absolute marginal contribution. And that is just think of that as the performance difference. The only reason it's the log of this ratio here is because running times sort of more naturally live on a log scale. And what we're dealing with with here is running times these prior scores. So it's really the difference in performance that you get the hit in performance that you get when you knock out one solver, and then you normalize this so that basically your gold medal pieces add up to a full gold medal.
00:58:46.094 - 00:59:06.354, Speaker B: That's what relative marginal contribution is. So I apologize for the math on this slide, but there's not much behind it, actually. It's really just the relative drop in performance as you knock out each individual team player. Right. I hope Ziad, that clarifies things a bit.
00:59:08.094 - 00:59:17.102, Speaker A: So this would mean you get credit for only if you're the best solver on a particular input instance, right? If someone is always the second.
00:59:17.198 - 00:59:56.728, Speaker B: No, not quite. I mean, this is why I very carefully answered to Powell earlier. Right, the thing is, the selector isn't perfect, right? We want to deliberately capture an attainable state of the art, not just give a bound, right? And that means we have a real algorithm selector. And that means that even if you're not the best on any instance, the selector might still choose you. Because in a machine learning sense, in a supervised machine learning sense, you might be a safer bet than the guy who really is the best. Or you just lack the features to, you know, figure out in advance the guy who's really best. You see what I'm seeing, what I'm saying, Sam? Does that make sense? Yes, it does.
00:59:56.728 - 00:59:58.984, Speaker B: Yes, absolutely. Yeah.
00:59:59.104 - 01:00:10.784, Speaker A: And just. I won't. Just for the benefit of people who are maybe not seeing the chat window, there is a reference in the chat window from Philip Bartek to a paper of Holgers on finding these things. Very good.
01:00:10.824 - 01:00:27.904, Speaker B: Yeah, indeed. And again, I apologize. I went over this a little quickly, so thank you very much, Philip. It is exactly the paper. Actually, it's not the paper that I'm referencing here, it's another paper. But I think there must be two papers where we. Where we introduce this.
01:00:28.604 - 01:00:52.764, Speaker A: So, good find and let's see. Karen Mcreesh has another comment quote. Now the grumpy question. As a solver author, I frequently get annoyed by configurators and portfolios because they produce really good results and won't tell me why. So I don't get any clues on how to design even better solvers. Explainable ML is allegedly a thing. Now, does it work for algorithm selection?
01:00:55.704 - 01:01:50.980, Speaker B: Okay, let me push back a little bit on that. So first of all, let me say it's true that currently a lot of meta algorithmic techniques are black box, but you know, we can probably do better. You know, there's now an increasing emphasis on explainable machine learning. Of course, we could restrict ourselves to machine learning models at the heart of the selector that are explainable, right? We could use decision trees rather than random forests, and we could use very small decision trees and boom, the choice of selector or the choice of algorithm made by the selector all of a sudden would be explainable. And I think that would be an interesting thing to do for configuration. You can do similar things for configuration. Actually, there are approaches have been out for a few years now that are called parameter importance analysis techniques, and they allow you to basically assess the impact of particular parameters to the configuration success, so to speak.
01:01:50.980 - 01:03:16.182, Speaker B: I'm a particular fan of something called ablation that is inspired by ablation methods that have been used in the experimental sciences, where you basically isolate the impact of individual factors, not unlike you isolate the impact of individual solvers. Here in terms of RMC, actually quite similar. And so I wouldn't say that these things necessarily must be black boxes, but it's true, whenever you bring heuristic techniques to the table to solve an NP hard problem in practice, you end up with algorithms that are not fully comprehensible to us in the sense of why do they work well when they do, right? That's the very nature of a heuristic algorithm, at least from my perspective. And I also think, philosophically speaking, it's the nature of our current understanding of NP hardness, right? So in some sense I would push back also in saying to you, even if you build the SAT solver from scratch, you don't really understand that solver either, right? Because it's full of heuristics. And yes, you've experimented with them, but they will never cease to surprise you. And here I would actually challenge Armin to contradict me and maybe tell me that he understands exactly when lingaling does well and when it doesn't. But I have a feeling that even he will admit that occasionally his own algorithms surprise him, as they tend to when we solve NP hard problems.
01:03:16.182 - 01:03:56.984, Speaker B: So once again, summarizing, whenever we deal with NP hard problems such as SAT, I think our solvers tend to surprise us and are not fully explainable, and that shouldn't lead us to lean back and make them even more opaque. We should acknowledge that maybe we can't fully understand and comprehend and explain them. And secondly, the meta algorithmic techniques, if we just use them indiscriminately, can make things more opaque. But there's no reason why we shouldn't aim for doing something better and actually making them help us understand. And so the final little comment, sorry, this is a little long winded, but I think it's actually maybe interesting. My final comment is this, if we look at results such as this one here. Give me a second.
01:03:56.984 - 01:04:42.024, Speaker B: I would actually argue a very natural next step is to look at which are the instances on which this solver here actually does well. And that gets very close to Powell's comment earlier. Let's look at uniquely the instances that are uniquely solved by this solver, or that are best solved by the solver, or for which this solver is better than is amongst the top three. And that might tell us something about the solver in the instances and actually give us more insight than if we just run this by itself. Right. And so from that perspective, I think we can turn this around and also saying this sort of ensemble view can help us understand a little bit better which technique works well when. And that's at the heart of understanding our solvers better.
01:04:44.244 - 01:05:14.424, Speaker A: Yeah, you've mostly. Thank you for the answer. You mostly answered a question I was going to ask, but let me ask it anyway. But you don't maybe have to answer it again, is it seems to me you could use this analysis to cluster your solvers according to these solvers do well on the same types of problems. And maybe you could understand somewhat better the taxonomy of solvers and their performance, and maybe understand what contributes to the success of a solver. Yes, just by looking at which solvers are similar to each other and which ones duplicate each other and so forth.
01:05:14.844 - 01:06:07.644, Speaker B: So now you get a very short answer, and it is, yes, you're right. Let me just add one sentence to it, which is that, you know, we have started doing that not in this setting, but closely related to it, with our work on temporal Shapley values that we published a few years ago, where we take a more detailed look, a more sophisticated look at solver contributions, but not in this competitive setting. And we do that not just for SaT, but also for, you know, even better understood. So for algorithms that we truly understand a lot better, like sorting algorithms. And there we see this relationships, these relationships between algorithms that come out of this temporal Shapley value analysis, which is quite fascinating. That doesn't go all the way, Sam, to what you're suggesting, but it follows the same intuition. So you're absolutely right.
01:06:10.184 - 01:06:43.204, Speaker A: Thank you. All right, last call for questions or comments. If there's nothing else, we'll wrap things up. Thank you again for a very nice talk and stimulating discussion. We have lots of questions and comments, and there's a. For those of you, there's a link to the Gathertown website, if people would like to go talk a little more informally afterwards online. It's in the chat window, and I believe this is open to anyone who clicks on that link.
01:06:43.204 - 01:07:01.012, Speaker A: Is that right, Antonina? It's not just specific to Simon's participants. Anyway. You can click and find out and. Or Simons participant. So, workshop, or simons, participants. Okay, so you're welcome to come gather a little bit at gather dot town on the link in the chat window. So thank you again.
01:07:01.012 - 01:07:03.612, Speaker A: I'll clap on behalf of everybody and.
01:07:03.708 - 01:07:16.814, Speaker B: Well, and thank you for having me, by the way, in case you wonder what the little thing down here is, that's the complexity monster, the little guy that we're fighting when we're solving SAP, just in case you wondered. We're having fun with him.
01:07:18.634 - 01:07:21.546, Speaker A: Okay, very good bye.
