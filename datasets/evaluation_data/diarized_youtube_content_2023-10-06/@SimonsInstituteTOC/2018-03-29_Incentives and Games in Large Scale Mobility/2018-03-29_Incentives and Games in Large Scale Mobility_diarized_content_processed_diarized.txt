00:00:01.080 - 00:00:22.318, Speaker A: Our second talk today is also focusing on traffic mobility. And here we're looking as Jane set up Alex's talk about routing games. Alex, thank you very much.
00:00:22.406 - 00:01:08.996, Speaker B: Peter, thank you very much for the invitation. I'm very excited to be here. And hi to our millions of viewers online. I heard we have millions of people watching live today I'm going to talk about inference and controlling routing games in the specific context of transportation, though routing games, obviously a much broader topic that applies to many networks, Internet in particular. First thing I'd like to do is to talk generally about transportation as a system, which is the framework that is used for what we do here. And so if any of you has taken any introduction to controls class, Caltech and many other books, many other places, including Berkeley, use that. That's the way you learn to think about a controlled system with an actuation system, sensors, and then a controller to go with it.
00:01:08.996 - 00:02:18.294, Speaker B: And so the way I'd like to walk through the transportation system today is in the same fashion to give you a sense of what's been done on the modeling and control side in each of the different boxes and the work done in inference in some of them. So the core, obviously is the system dynamics, which is pretty complex to model, whether you're looking at something as crazy as this, which happens nearly every day in LA or Thanksgiving traffic. The point here is there's been probably half a century of modeling work done on transportation systems. Anything from coupled PDE systems to queuing systems to even simpler models that somehow work. We do and everybody does, relies on, and we've contributed to ourselves over time. Then there's sensing infrastructure, which in the case of transportation traditionally has not been very widespread, mostly for cost reasons. And these are things you've seen on the road many times when you drove through different places, California or elsewhere, loop sensors, cameras, radars, and many other things.
00:02:18.294 - 00:03:15.050, Speaker B: Then you have control infrastructure, which also you're familiar with. If you drive in most states of the US, obviously traffic lights in cities, but also for freeways, things like metering lights like this, things like metering for bridges, things like special use lanes, which is a way to control traffic, you can enter the lane or cannot enter the lane, or you can enter for some prices or things a bit more sophisticated, such as variable speed limits or speed advisories. And so if you think about, for example, a fairly well instrumented corridor like the I 210 in LA, that infrastructure is still a fairly small size. You have a total asset inventory of about 450 lights, traffic lights and meterings, maybe 50 other types of wayfinding sites that can help. And that's essentially. So maybe a total of 500 is all you have to manage. Hundreds of thousands of people living there, going through that corridor every day.
00:03:15.050 - 00:04:21.444, Speaker B: So that's, you don't need to be an expert in control theory to figure out that this is a very small amount of actuation when it comes to controlling the system. Now, on the control side, there's two important blocks, which are estimation, now cast inference, and then demand forecast, which is essentially trying to predict what's going to come into the system, which you can conceptualize as follows. So typically, this is a screenshot of a system we're building for LA. If you're trying to do inference, you're essentially coloring a 2d plot where the color level indicates. Now, so this is time and this is place, essentially the speed on the freeway. And that's the most simple, simple graphical way of characterizing what inference is. So as the time evolves, until now, you estimate what traffic is and then forecast is more done for demand, which is because it's much harder to forecast the state, but you can at least forecast demand statistically.
00:04:21.444 - 00:05:08.276, Speaker B: That's essentially saying, well, there's going to be a peak demand in the morning rush, there's going to be another peak demand in the evening rush, and something in the middle. And there are fairly good statistical models of these things. And so what you do on it, essentially, and that's the job of any traffic management agency, is based on estimation and forecast to try to optimize over the corridor, which you could conceptually think, well, if your goal is to have a free flow, means everybody going to the speed limit, you would essentially want to paint this in green. It's not true in general, because things never stay green. I mean, if the freeways were decongested all the time, more people would start to drive. That's called induced demand. But just conceptually think about optimization.
00:05:08.276 - 00:05:14.424, Speaker B: That's trying to do your preferred column pattern here, which is what you're trying to achieve.
00:05:14.724 - 00:05:17.384, Speaker C: The location is on the line, the.
00:05:17.884 - 00:05:27.874, Speaker B: Location would be post mile. So say you start from a location on a freeway, you declare zero, and then the freeway is 15 miles long. That would be 15 miles.
00:05:28.854 - 00:05:31.366, Speaker A: Okay, general case. Yeah, it's a network, right?
00:05:31.430 - 00:05:48.594, Speaker B: It's a network. So this is a link by link representation. And every link you click, and so in the tool we're building, every link you click and you can see this. So I think this has been referred to as a time space diagram for 50 years by traffic engineers. I mean, probably many other names in different, different fields.
00:05:50.534 - 00:05:56.622, Speaker C: You mentioned the economic point that if you really cleaned up the roads, and.
00:05:56.638 - 00:05:59.790, Speaker B: This is what we're going to go to towards the second part of the talk is induced demand works on different.
00:05:59.822 - 00:06:09.434, Speaker C: Timescales, because people, it feels like the objective you want to optimize to might be non trivial. Because if you optimize to a trivial objective, you might just.
00:06:10.534 - 00:07:03.234, Speaker B: Exactly. So this is a nice teaser for about 30 minutes from now, but ask me again in 30 minutes because then we'll go into conspiracy theory, which is. So ask me again about the conspiracy theory in about 30 minutes. So this essentially feeds an operator, because in most of the systems, even though these are not safety critical as you would have in aviation, you still have an operator. And so this is a picture of the Le District seven common center that we're building this decision support system for, and La Metro, which is the MPO, Metropolitan and public organization agency in charge of this jurisdiction. There's always two agencies, the state and the NPO. And so the point in building this is that like, actually you were hinting you can't optimize for anything.
00:07:03.234 - 00:07:54.084, Speaker B: And in fact, you have to be very careful what you optimize for, mostly because there's a lot of jurisdictions in charge of the geography, so they inevitably will always have conflicting interest. I mean, you can see if there are many cities along the corridor. Not all of them have the same objective in terms of where they want cars to go. If you have a state in charge of the freeway, but an MPO in charge of the arterials, how you balance traffic between the two is not obvious. So for those of you who have done optimization, in a sense here, the feasible set is a set of acceptable procedures that all the stakeholders agree to define what you optimize over. Essentially what you do is you write a concept of operation, which is a set of pre agreed rules that all the stakeholders said it's ok to do. And so that's, in a sense, how you define your feasible set for your optimization program.
00:07:54.084 - 00:08:49.238, Speaker B: And in the present case for LA, for the I 210, we wrote it on behalf of the stakeholder involved in the coil. So let's talk a little bit about inference problems. So, inference problems mostly have to do with either forecast or demand or state estimation. So in the case of demand forecast, one of the things, and you probably saw some of it in James talk earlier, is with the explosion of the use of cell phones that are ubiquitous. Even if you don't query every phone's gps track, which gives you position within a few meters by just querying the cell tower it's talking to, which any operator does. You have an approximate positioning, which can be on the order of 500 meters wrong. So clearly not good enough to do traffic flow estimation or speed estimation, but good enough to do demand estimation.
00:08:49.238 - 00:10:04.802, Speaker B: And demand estimation is saying, well, from your neighborhood where you live, say, within a block or ten blocks, to your work location, say, the middle of the financial district, there's x thousand people living at eight in the morning and arriving at nine in the morning. And so the type of the work that has emanated from the community over the last ten years, and we've done some of it, is essentially creating algorithms that can do this type of inference. And it turns out that you can do this quite efficiently with simple quadratic programs that essentially model an approximate routing of the people along their way. Because if you look at the records you have, you can essentially track the person from cell tower to cell tower. So as you drive in your car in the morning, your phone switches off from cell tower to cell tower, which gives you an approximate, which gives a telecom company that owns your cell phone plan an approximate sense of your routing. And then with simple quadratic programs that can be accelerated with isotonic regression, what we can show is that we can get really good inference of demand patterns. And that's something which, from a theoretical standpoint, is not that hard.
00:10:04.802 - 00:10:37.722, Speaker B: But I think from a practical standpoint, is essential. Because if you think about five years ago or ten years ago, the only way to get to people's routing was essentially by census. People would knock on your door, ask where you work, give you a bunch of questions that would be updated every ten years, and that's how they would be doing demand forecast. Over the last ten years. With this explosion of this data, essentially a lot of companies have enabled the collection of data that can really change the way this inference is done. And to give you a sense of what the data looks like, this is a set of cell tower records. So just focus on the purple here.
00:10:37.722 - 00:11:20.174, Speaker B: The purple is, the blue are a bunch of vehicles, and the purple is one specific vehicle that happens to be traveling through La. And the points along the trajectory are not the vehicle's position, the position of the cell tower it's talking to, which is why we give a little jerk here. That's because it's been handed off to a tower that is off the freeway. But the point here is, you can see from that trajectory, you can get an approximate person's trajectory. And if you bundle this to company like at and t probably has 30% of the population. In California, that means you can get approximate origin and destination from people for 40%, which then you can scale to the entire population and you can see interesting things. This is probably a FedEx truck that is doing deliveries.
00:11:20.174 - 00:11:57.360, Speaker B: So it's kind of going around a part of the city delivering packages. This is someone going to work, stopping along the way, probably to do some shopping and then going home. And this is another person traveling. And so the type of work you do with this is you take this data for the proportion of the population you have, which is high. And essentially you solve the quadratic programs I've shown before, which is saying, trying to minimize an error functional between your measurements and what your model predicts, subject to a constraint, which could be a model. And in our case, it's a game theoretical model. And that gives you, for each origin destination, a flow split.
00:11:57.360 - 00:12:47.628, Speaker B: So say the model will say that from the 3000th vehicle an hour that leave this origin and go to this destination, 50% use this route, 40% use that route and say 10% are kind of spread in the less red routes here. That's what the model, that's going to be a graphical way of explaining what the model gives you. And so the interesting types of things you can do with this are as follows. For example, questions that you would want to ask, and you can at least answer in simulation, is what should be the cell tower density, so that you can have a specific set of errors. So obviously you'd like to have an error which is as small as possible. And that problem is going to expose combinatorially, because you can imagine the number of roads between any origin and destination is exponential in the number of links. So you have to make choices.
00:12:47.628 - 00:13:24.736, Speaker B: Google will typically give you three routes, usually from your origin to your destination. So depending on whether you constrain your program by a number of routes, which is three to 50, you can see the error is getting better. If you give 50 possible routes, obviously your search space is bigger. So when you optimize, then you probably get closer to reality. And so that is something that you can factor in. And so if you plot the relative error between your ground truth and your prediction, what you can see is obviously a decrease. The more cells you have, the more precise you can track people, but also the more you allow them to take routes, then the bigger the program gets.
00:13:24.736 - 00:14:08.718, Speaker B: But so it's more expensive computationally, then the lower the error. So these are interesting tradeoffs to explore if you're trying to predict routes of people at an approximate level, which is essentially what you use to feed a more precise model for local traffic. So this is not good for traffic control, this is really good for forecast of demand. And that's really important because the second part, so that's this box here is very important because obviously another part of controlling traffic is understanding what's happening now at post mile 12.7, because there's an accident and that's a very different problem. That's a local problem, and that's also something which we've worked on and I'll talk a little bit about in the next couple of minutes. So here the problem is now local.
00:14:08.718 - 00:14:35.336, Speaker B: So we go back to this time space diagram. This is time, and this is postmodern, going back to your question. And so typically, if you want to control traffic, any controller will need some state input. So inference of the state. And that's in a sense the state. So here's an example of an accident that happens, that creates a big traffic jam, that creates a backpropagation of a shockwave, and then the freeway never really recovers. And then this is the evening peak.
00:14:35.336 - 00:15:27.280, Speaker B: Of course, you never measure that because you would have to measure every vehicle, everywhere. So say you start with nothing, so just shaded it and say that this is what you want to reconstruct. If you were able to sample the data at fixed location, like with sensors, by counting cars, which is commonly done in California, that's essentially saying you can measure the data on these axis. Sometimes there's a failure sensor goes down, so you don't have data, then it kicks in again and you measure the data. If you have a good sense of what data is at midnight, because usually freeway is empty, except maybe in la some places, then technically you could also initialize the inference here. And then if you were able to sample some vehicles, because now you have access to their precise GPS location, then that means you can sample the data along some trajectories. And sometimes you don't transmit your position all the time.
00:15:27.280 - 00:15:55.590, Speaker B: So say you have a fast track reader or easy pass in the east coast. You just check in at specific places, like at the tolls and other readers along the way. So you can. So this vehicle here at this time is the same as this vehicle here. So that's a good graphical summary of the type of data that you get when you do traffic estimation. And for those of you, and I know there's a few folks from LBNL who work in oceanography. I mean, essentially this is lagrangian sampling because you're sampling along a particle.
00:15:55.590 - 00:16:38.906, Speaker B: And this is allurian sampling because essentially it's sampling in a controlled volume. And these are terminology that progressively are migrating to transportation, but they are mostly from the physical oceanography and physical sciences. And so in doing data assimilation or inverse modeling, typically you have a constitutive equation. So if you're hydrodynamicists, usually you'll use Navier Stokes equation. Or if you're doing estuary in environments, you'll do shallow water equations. Or if you're doing atmospheric boundary layer, you'll do different versions of Navier Stokes equation here. The PDE that you routinely use in traffic is the Hamilton Jacobi equation, where the Hamiltonian is a specific function that encodes the level of congestion of the road.
00:16:38.906 - 00:17:17.987, Speaker B: And that's a function that was postulated in 1935 by someone named Bruce Greenshield. And that has been since then called the fundamental diagram. In classical hamiltonian mechanics, it's called hamiltonian in conservation laws. It's called the flux function. It has a lot of different names depending on the mathematical subdomain. So the point here is that you're doing inference or you're doing data assimilation, which is essentially, again, trying to reduce a functional error subject to a constraint. And the constraint is this Hamilton Jacobi equation, which is nonlinear, which is, um, which has non smooth solution, and which is essentially the, the model of the plant that you're trying to have here.
00:17:17.987 - 00:18:14.416, Speaker B: And so if you're measuring that function at a given location with the allurian measurement, that's essentially saying you can sample the function at a given location at all the times. If you are, um, postulating that the function is, for example, empty freeway at midnight, that's saying you can tell that's the value of the function on that line. And then the interesting thing about the lagrangian measurement is that it's always the same car you're following. So if people don't pass each other too much, because m happens to represent the number of the car on the freeway, if you just number people by the time they pass a reference point, essentially it's saying the function is constant along a lagrangian trajectory. And that was something which was kind of known since the second. The guy who invented this is an engineer from Kartrans called Karl Moskowitz. That's why we call the function m in kind of remembering Moskowitz, because he had no idea about pD's.
00:18:14.416 - 00:19:01.788, Speaker B: I don't think he was even trained in numerical analysis, but somehow figured out that there was a function that was conserved and discovered the Hamilton Jacobi equation without realizing it was Hamilton Jacobi equation. So it's kind of a very interesting story from a practitioner standpoint. And it turns out that you can sample that function along a car. And so if you follow that trajectory, it's saying that Moskowitz function is constant. It turns out that function satisfies the Hamilton Jacobi equation, which is another historical interesting thing, because the existence and uniqueness of the solution to that equation only goes to, it's essentially 1982. And that's a very famous paper of Condyle, Evans here at Berkeley, and Lyons, who later the Fields medal. And that's the viscosity solution.
00:19:01.788 - 00:20:30.446, Speaker B: So it's something that is still kind of recent in mathematical timescales. So here the inference game is essentially trying to reduce an error functional subjected to this constitutive model and whatever data you can measure. And so part of the contributions we've made over the years is pose these problems come up with existence and uniqueness proofs of the solution, because the lagrangian data creates extra complications that are not in the classical viscosity solution. Then, leveraging an interesting formula called the Luxop formula, it turns out you can solve this equation semi explicitly in a one dimensional case, and you can integrate boundary conditions, and initial conditions and internal conditions, so these conditions into the problem formulation, which is interesting too, because it's not something you would have a typical Cauchy problem. And the resulting thing you can get from putting all these things together is you can pose the data assimilation problem as a convex program, where the problem is convex in the parameter data that you don't have, which could be either the noise you model in the data, or the uncertainty in the model. And so this took several papers to put together. But it's really nice because it gives a generic framework for data assimilation, which systematically leads to a convex optimization program, as long as you model your error functional as a quadratic error of some of the parameters.
00:20:30.446 - 00:21:39.506, Speaker B: And so heavily relied on the first book we wrote about ten years ago now on viability theory. And now we're writing a second part specific to the solutions here, because it turns out this framework has also had a lot of different applications in control as well. So, to give you a sense of how it works, if you were able to sample every, if you were able to sample 2% of the people on the freeway every 3 seconds, so imagine every 50 cars, someone sends you data every 3 seconds, this is what you would see. So this is a GPS track sample on that small stretch of freeway, and the color indicates the level of congestion, which is the speed here. Essentially, you can see that with 2% of traffic, you can pretty much see what's happening. Obviously, companies like Google have something much closer to 100% because of either the Android phones, which have it in the os, or the people who have iPhones who use Google apps also send their locations. But what's important to realize is that none of the public agencies in charge of traffic have access to that data, because that data is not shared.
00:21:39.506 - 00:21:56.494, Speaker B: And so if you buy data at a very, very small percentage of the population, and some companies sell that, that's more likely what you'll get. So here, I just decimated the previous data set to only keep a few. And it's obvious from this plot that it's much harder to reconstruct traffic.
00:21:57.054 - 00:22:03.286, Speaker A: Can you get some analysis of the error in your reconstruction as a function of how much data you've collected?
00:22:03.390 - 00:22:15.246, Speaker B: So an analysis of the error as a function of how much data we collected. So it would be a statistical guarantee on something based on, or it doesn't.
00:22:15.270 - 00:22:28.916, Speaker A: Have to be statistical, or even just some l two. L infinity. L1. The error between what my algorithm for produces and what my true solution is as a function of how much data I collect.
00:22:29.100 - 00:23:10.184, Speaker B: So we've never quantified this as a function of how much data we collect, because we don't know how to solve that problem. But the one thing we have done because, I mean, like, explicitly, I mean, obviously we do it empirically, and we can show convergence plots empirically, but we don't have any explicit bounds. But. Okay, but the one thing we can do is we can actually characterize as a convex feasibility problem. The following problem is like, if we can, if we assume the error level, so say, for example, robust. So take the maximum possible error in the measurement, is that much. And if we assume the model parameters of.
00:23:10.184 - 00:23:16.524, Speaker B: Yeah, well, I know it's more. Yeah, I know it's not exactly what you're asking. It's like robust versus.
00:23:18.604 - 00:23:19.764, Speaker A: Stable versus robust.
00:23:19.804 - 00:23:46.564, Speaker B: Yeah. And so that is something we can pose as a convex program. Now, giving you an explicit bound of convergence, I think that's a question many people have asked, and I don't know how to do it, not yet, maybe in ten years, but we do. But we, but I don't have them in this talk. But we do have a bunch of nice plots that show convergence, obviously, of the estimator as you increase the data. They're just empirical. Empirical.
00:23:46.564 - 00:24:41.944, Speaker B: And so I think the interesting perspective on this is that that question might actually become moot in a few years. This is an interesting curve which shows the convergence or the increase in cell phone ownership in the US population. But if you think about the world of transportation in five years where people have multiple cell phones and your car itself is connected, in a sense, inference might become useless because at least some agents, like Google, like Apple, like maybe even Uber one day will have enough data to just collect data for enough people that you don't even need a model anymore. And I think this is a really interesting transition because people have spent the last century in modeling traffic, and it turns out that you probably will spend the next century just watching traffic and pulling the data.
00:24:44.804 - 00:25:11.224, Speaker A: So if you go to like, you know, the full IoT car situation where telematics is just built in and it propagates into the fleet, are there any qualitative advantages to that? Rather than robotics on the passenger, the cargo to provide limited amount of information versus telematics level?
00:25:11.344 - 00:25:24.724, Speaker B: So I believe. And for people online, the question is, is there an advantage of people providing data through their personal cell phone versus the car itself or the vehicle itself through IoT paradigms, sending it instead?
00:25:25.264 - 00:25:26.792, Speaker A: As that becomes more common, as it.
00:25:26.808 - 00:25:52.450, Speaker B: Becomes more common, it really depends what you're doing. If you're doing traffic management, the latter is better because what you're interested in is the vehicle. The rest is irrelevant. But if you're doing demand management, because now you're trying to push people from transit to car or car to transit, or you're trying to infer what people are doing as people, then the other one is better, because what you care about is the people. In particular, if you're interested in ridership, you would like to know how many cell phones are in a vehicle, because that's an indicator of how many people.
00:25:52.482 - 00:25:56.148, Speaker A: Are in a car, that would probably be a telematic.
00:25:56.276 - 00:26:52.898, Speaker B: Okay, so then, okay then. Okay, so let me just skip a few things so that we stay on track. So let me summarize, in a sense, one of the things that we did was to build one of these first systems, and this is now ten years ago, we had a, our ten year anniversary this month. And essentially what we did in 2008 was to look, this is what the website with traffic information would look like then. And we built one of the first systems that could essentially create something like what you have on your phones through Waze or Google or any other app from probe data, it was launched here from Berkeley. And so these, to give you a sense of what that data. Looks like this is a plot of 500 vehicles sampled at 30 seconds interval in San Francisco.
00:26:52.898 - 00:27:26.700, Speaker B: This happened to come from the cap spotting feed we were getting at the time. This is the data that Jane probably described in the previous talk. So this is what you measure if you have about 0.5% of the people sending their data every minute. And what's interesting is that even with 500 vehicles at this very, very low frequency, you get a lot of information to give you a sense of the magnitude. Uber has, I think, 40,000 registered drivers in San Francisco, and each of them is collecting that data at the 1 second frequency, I assume. And that's just by watching the car moving on the map.
00:27:26.700 - 00:28:11.732, Speaker B: You can see it's roughly updating its position every 1 second. So they must be at least collecting that position every 1 second. And so that's interesting, because you can see that we're progressively entering an era in which these estimation problems, like you were asking, might become moot. If there is enough data that you can just look and almost literally do it vehicle by vehicle. But we shouldn't forget that there are lots of places in the world, including the US, where that data is still not ubiquitous. And so I think a lot of these companies that have built systems like the one at this crib have essentially built an architecture just like the one we used at Berkeley for the last 1010 years, which is like, you gather all your feeds and these are all the data sources I showed before. There's a bunch of filters, the models.
00:28:11.732 - 00:29:19.396, Speaker B: I spoke a little bit about the Hamilton Jacobi equation, then the predictors, which we solve as convex programs, and that essentially provide traffic information. And so looking from a historical perspective and technology perspective, we started working with this problem about ten years ago, at the time with Nokia, which was the giant telco cell phone provider. Since then, obviously a lot of other agents have captured that market today, Google and Apple being the two major collection of this data. And I think it's fair to say that our problem not only has become very mature from an academic standpoint, I mean, there's been tons of work done, both in modeling, estimation experiments, and even data quality. But it's also becoming mature from an industry standpoint, because I think this is, in a sense, there is not much more money to be made in this field. Most of the companies will provide traffic information and sell it are struggling, and those who give it for free are just doing it because they can bundle their products with it. But what's interesting is the consequences that this ubiquitous technology has had on society.
00:29:19.396 - 00:30:24.534, Speaker B: Because if you think about, I don't know how many of you drove to Berkeley this morning, or if you're driving back to this other school on the south of the bay in the afternoon, but the point is, whenever you query shortest travel time, usually what the app does is it will give you one route and it might give you two other routes that are nearly as good. And try it now, try it later. Everybody has done that before. What's interesting is that if you look at this diagram, what it actually does is it changes the dynamics inside the plan by adding a feedback loop that is unique to you. Because you're going to follow an app, you're not going to follow directions from the government or directions from previous experience, or do what the government thinks you're doing, you're going to do what the app tells you to do. So essentially, now you're closing the loop inside the plant, and everybody's closing a different loop because everybody's using a different app for different purposes. And if you've been on any control theory, you know that having a model of your system in your controller that is different than in the plant is a really bad idea.
00:30:24.534 - 00:30:57.842, Speaker B: It's just like putting a jet fighter autopilot into a 747 is not going to create a happy landing. And so first thing about this is like, well, maybe this is an academic problem, maybe it's actually not that bad, but it is actually really bad. And let me show you why. These are interesting snapshots. I'll go through really fast of statements made by politicians or elected officials about the fact that, oh, we're going to team up with this app or that app. And because people are getting the shortest path, traffic is going to improve. Of course, if you know game theory, you're already laughing.
00:30:57.842 - 00:31:51.946, Speaker B: I can see people laughing because pretty quickly what happened is not exactly that, but people noticing that some problems happen in their neighborhood due to through traffic. And there are several things at stake here. But one thing at stake is obviously what happens when you query the app and you take a shortest path, is you're playing a selfish game in which you're doing selfishly what's best for you. There's nothing wrong with it because, I mean, who likes to be stuck in traffic? But what's interesting is that what ended up being a non problem initially is a problem if you have a very large proportion of the population using this, because that means now everybody is playing that selfish game. And the information that used to not be there now enables you to play that game. So at some point. It got really bad to the point that people started to react.
00:31:51.946 - 00:32:39.228, Speaker B: So first it was kind of funny, people shaming people, saying, you should not come to my neighborhood, and posting signs. Then it got a bit more creative, putting fake detour signs to confuse people so that they would not go through their neighborhoods. There's a few neighborhoods who started to pay senior citizens to walk with a phone open along the road to send fake measurements to the app to spoof the algorithm. There's actually manuals to do this. You can find them online, how to spoof the algorithms. And then cities started to get involved and figure out, well, it's like the resistance, it's better when it's organized. So cities started to figure out, this happened in Fremont not so long ago, but this happened everywhere, that if you start to uncoordinate the traffic lights or put a lot of restriction in your traffic, you're going to make it impossible for people to go through your neighborhoods.
00:32:39.228 - 00:33:07.234, Speaker B: There's many ways you can do that. So one way is to put street bumps, which slows down traffic. So huge increase of street bumps in a lot of cities, or stop signs, very good for that as well. Turn restrictions, more societal left turns. That's the proper way to call them in traffic engineering. And so the point here is that what you see now is a lot of cities making their own traffic worse so that the algorithm stops sending more people through their neighborhoods. So it's crazy, right? This is the.
00:33:07.234 - 00:33:39.334, Speaker B: I mean, who can think about making their own life miserable so people don't come to their neighborhoods? But that's essentially what's happening to the point that now that things have gotten political, this is, has reached the desk of many elected officials. This was one of the forecasts that finally happened because ironically, the first lawsuit happened against so Citi suing a company for sending more traffic their way. And that's kind of where we are today. And I think it's not unlikely that maybe in the next one or two years we'll see a class action, probably in the US.
00:33:40.154 - 00:33:50.226, Speaker A: Do some of these apps actually send people down routes it doesn't know the answer to just so it can gather data, figure out whether they're short routes or are they only using data that they require because people have to.
00:33:50.250 - 00:34:00.226, Speaker B: Oh, now we're into conspiracy. Now we're into the. This is a part of conspiracy. I did not talk about. I did not plan to talk about. This actually happened. This is a really good question.
00:34:00.226 - 00:34:28.222, Speaker B: This happened. This used to be a common practice of the company ways when they built their maps for historical reason. Maybe I'll take a five minute digression and cut five minutes of the talk, but at least I can answer that. So ten years ago, when Waze. Does anybody know the story of Waze? The reason why Waze exists is in some parts of the world's map change every day. Waze being an israeli company, obviously in Israel, the maps change every day based on what's happening. So people wanted to find a way to find what's the map today.
00:34:28.222 - 00:34:52.746, Speaker B: So the only way to do it is to send people to measure whether the road is blocked or not or whether there's something on the road. And the way Waze did it at the time is, for those of you who were running the app about ten years ago, is there were goodies. So it's just like a Pokemon go. And if you drive through the goodie, you make points in this game, and then you get better virtual status and that virtual ranking. And that's the way they used to do it. Now, this was explicit. People were.
00:34:52.746 - 00:35:40.034, Speaker B: Now you could wonder, why would people waste time and energy and fuel to do it? To gain virtual. Okay, that's a different story. Actually, I have someone on record from that company saying that during a few years, I think the mileage of the users and the gamers was increasing by 25%. So people were driving 25% more to gain virtual status, polluting the environment, increasing congestion, just to drive through goodies. Anyway, that era is behind us. I don't know, honestly, today, if a company like the ones we're talking about would dare to send someone just to measure traffic, because I think if this was found out, I think this would be really bad. So I don't know if it's happening for real, but I know it happened in the past, explicitly in this virtual game that Waze created.
00:35:40.034 - 00:35:45.822, Speaker B: So this is a part of. Yes, there've been some even more dangerous consequences.
00:35:45.878 - 00:35:48.846, Speaker A: Recently. When we had wildfires here, Waze were.
00:35:48.870 - 00:35:50.494, Speaker B: Sending people down streets that were empty.
00:35:50.534 - 00:35:52.046, Speaker A: Because they were on fire.
00:35:52.190 - 00:36:17.794, Speaker B: Yeah, yeah. For those of. Since you didn't have a mic, is, like, specific companies sending people into a fire. Yeah. And in fact, there have been some lawsuits of people against one of these companies for sending them to dangerous neighborhoods. Like, your router sends you through a neighborhood which is faster, but you get attacked along the way. And so there are a few lawsuits on record where people have done that, and it is a real problem.
00:36:17.794 - 00:36:33.554, Speaker B: I mean, it's funny because specifically, these dangerous neighborhoods, these companies learned very fast to route around them, and I don't know which country it was in, but, yeah, I mean, this is. The fire is a problem, the danger is a problem. There's a few issues of that nature.
00:36:33.894 - 00:36:41.034, Speaker A: Finally, what are the terms of service that a user, what promise is actually made? What's the trade?
00:36:41.814 - 00:36:56.466, Speaker B: So the question is, for the viewers online is what are the terms of service? I think nobody read them because everybody clicked. Yes, yes, yes, yes, yes. So I think you consent to everything, including being killed, probably, but frankly, I don't know because I never read them myself.
00:36:56.570 - 00:37:01.334, Speaker A: But then you assume that the benefit to the driver is saving time.
00:37:02.114 - 00:37:44.044, Speaker B: Yeah, and I don't know if that's ever stated in the terms of service. Probably not, because I assume then you could sue people or sue the companies for missing your plane because there might have been a faster route, but I assume it's probably somewhere. Frankly, the terms of service keep changing and I've yet to find someone who ever read terms of service for these apps. I should probably do it. So, just to give you a sense of the magnitude of the problem, these are the places in the US where this has, or in California where this has popped up in the media. So you can imagine if there is a newspaper article and it may actually made the front page of the New York Times on, on December 28 this year, last year. It's crazy.
00:37:44.044 - 00:38:19.294, Speaker B: Front page of the New York Times. I mean, who would have thought? But essentially, if you see, if this is hitting the news at this frequency, it's really happening everywhere. And so there've been a lot of questions, and I think it's really good. So I probably won't have time to go through the rest of the talk. Maybe the two things I'd like to do is to show a movie that shows the influence of information on routing and why more information, if not managed properly, is not necessarily helpful. And then I'll say a little bit about learning and inference, and then I will probably stop for today. So the movie here is essentially a concatenation of two movies.
00:38:19.294 - 00:38:51.114, Speaker B: This is a simulation. In the top movie, it's a micro simulation that means it simulates every vehicle in the network. And in the top movie, what it shows is what people do. If 20% of the people have the information, 20% of the people will do what's selfish for themselves. And then here you could view it as what happened ten years ago when nobody had information. So people would just follow their usual patterns. So let me just play the movie and narrate the movie as it plays.
00:38:51.114 - 00:39:19.166, Speaker B: So as you zoom in you'll see an accident happening. So an accident is a blockage. So, say a truck spills over, three lanes are blocked. It'll take Caltrans 1 hour or 2 hours to California highway patrol to empty the freeway. In that meantime, the app kicks in immediately, gives you your shortest path, and now you have 20% of the people who have the option to leave the freeway. Usually the app doesn't know that the accident has been cleared. It can measure it later, but it doesn't know how long it takes.
00:39:19.166 - 00:40:09.072, Speaker B: So by the time the accident is cleared, you still have a lot of people leaving the freeway, and a lot of the routers have not updated. And even if they have, this has now created a bottleneck because a lot of people are trying to leave the freeway. So it's interesting, the first negative externality is that by giving that information, you've now clogged the freeway because a lot of people are trying to leave. That has actually created additional congestion upstream from the freeway. But what's interesting is that now all these people are on the secondary road, which does not have the capacity to process them. The intersections do not have the capacity to process the external traffic, and these were never designed for it, and the traffic signals are not timed for it either. So now you have all these extra vehicles that create additional inefficiency in these parallel networks.
00:40:09.072 - 00:40:57.074, Speaker B: And there's essentially a consequence of couplings that create further clogging of the freeway in the other direction. And now you've even made the traffic worse in the other direction. So this is purely empirical. There's models for it. But what's interesting here is that each of the steps I just narrated, the accident, the bottleneck creation due to the rerouting, the bottleneck removal, the residual congestion, and so on and so forth, all the things I just narrated, they're all things which are modeled quite well. There has been a ton of work over the last century to model them in independence, and fixing them in independence is something people know to do. But what this information has created is essentially a concatenation of them, which makes it nearly impossible for it to be controlled.
00:40:57.074 - 00:42:05.212, Speaker B: And so this is just illustrative of what information has created. And maybe to finish the talk on a slightly more theoretical note, what I'd like to do is to give a sense of what happens when, essentially, this loop that I talked about starts to operate at large scale, and the system starts to learn how to react to these selfish behavior. So I'm going to just show this curve. So this is a result of a model, modeling traffic as a Nash Tackleberg game in which you have selfish users who do the best thing for themselves, and you have other users who just do things they would do otherwise. And what you see is that if you plot the evolution of travel time as a function of the proportion of people using the app, ten years ago, when nobody was using the app, 0%, you had a pretty low travel time on the arterial roads here, like on this little stretch, the green roads, and you had pretty high travel time on the freeway. And the more people are diverted, essentially, the lower the freeway travel time gets. That's normal because people leave the freeway.
00:42:05.212 - 00:42:27.516, Speaker B: So technically it kind of helps. But of course, the first roads get congested, so it gets much higher travel time. Then people start to use the second road, and it also gets congested. And at some point, the travel times equalize. And for anybody who has studied economics, then you'll recognize that's a convergence to a Nash equilibrium. And that's why all the claims that the elected officials made about, you know, if you give people selfish information, it makes traffic better. Well, no.
00:42:27.516 - 00:43:14.004, Speaker B: John Nash got a Nobel Prize for proving the sim about half a century ago. And so what information has done is it has enabled people to play that gigantic Nash game in which they are able to compete for a SIM commodity and do it selfishly. And so today, the situation in which we are is essentially a multiplayer situation in which anybody can use any app they want, and any app will route people selfishly based on their belief of what's best for their customers. And maybe they will split the flows, maybe they won't. We don't really know what's below the hood. And that system essentially operates as a system of black boxes that are not coordinated and obviously are all competing against each other. So it's kind of a bunch of closed feedback loops on everybody using their phone.
00:43:14.004 - 00:43:47.562, Speaker B: What's interesting is that system evolves over time. And so that system, as more data comes in, the companies that route you, they learn over time how to route you better. If you've been using Google or Waze or Apple for the last five years, you've noticed the app has changed. The routes they give you have changed. That's because the algorithm is learning over time. So that is something that can be modeled as a repeated game. So the game is essentially an instantiation of that system I've shown at the beginning, and the learning is month after month.
00:43:47.562 - 00:44:57.456, Speaker B: The app learns how to route their contingents better. And so there is a very nice repeated game formulation of it, and the one question that we should all collectively ask ourselves is if every day the apps are splitting flows according to what they think is better for people, and at the end of the day, they discover how well they did, because they can measure everybody, what learning algorithms should they use, so that overall things improve. So you can view the learning algorithm as an evolution of what the system split the next day should be as a function of what you learned from the outcome of the game the previous day. And so, the type of work we do now is trying to study these learning algorithms to say, are there anything we could say about them? And so far, the depressing conclusion we have is that most of the models we find all end up inevitably converging to Nash solutions. Which means even if people are continuing to play and the system is evolving, because the system is never really at equilibrium when it converges, it still converges to Nash equilibria. And so, just skipping to the three main results here. So I'm just going to go, I'm going to skip all the equations, but just show the three results here.
00:44:57.456 - 00:45:59.428, Speaker B: Is that depending on the assumption you make about the way the apps learn, if you make almost no. If you make almost no assumption on the way they learn, you just assume it's a no regret learning strategy, then you can't even show that the system converts. So it's essentially saying that if the algorithms put together by the companies to learn over time how people wrote themselves, don't have specific assumptions, then you don't even know that the system is stable. And there have been known gridlocks created in many cities by these apps. And you could view this as an early instantiation of this lack of stability of the system. If you make further, more restrictive assumptions on the way the app learn, then you can prove that you have essentially a form of convergence. It's almost your convergence of these distributions, which is encouraging, and in even more restrictive assumptions, where you assume that the apps learn in the form of a mirror descent, then you can prove convergence rates of the way the distributions evolve over time.
00:45:59.428 - 00:46:22.124, Speaker B: But all these convergence, in the best case, still converge to a Nash equilibrium, which is not good, because for anybody who has studied economics, then you would know that in most of the system, there's a price of anarchy, and it's non zero, which means, essentially, yes, you converge to a Nash equilibrium. It's better than instability. But still, they're not at system optimum where you would like to be. And so, at the core of this.
00:46:22.204 - 00:46:50.186, Speaker C: You were kind of implying in these situations that these companies are only looking at current state of the roads. When you were showing that, zoom in, and you walked us through that bad situation, it looked like a not even very sophisticated model would be able to make predictions of the dynamics into the near future. Do you think that these Nash equilibrium results will hold if the companies do make more of a predictive or use more?
00:46:50.330 - 00:47:24.012, Speaker B: This is a very deep question. Yeah. So the question for the viewers are, if I try to summarize, the long question is, can we say anything about the impact of the quality of the forecast of this company on what it would converge to if these companies were doing forecast? Indeed. So it's a very deep question, because if you think about all the. I'll just list five or six different ways they do it. So ten years ago, essentially, the way they would compute travel time is posted speed limit. Okay? So when there was no information, you pick the speed limit, and that's your forecast model.
00:47:24.012 - 00:48:08.312, Speaker B: Then a little bit later, they move to historical forecast. Okay, that was a bit better. Then about ten years ago, they moved to Nowcast, which is essentially you're measuring what you have now and you assume it doesn't change, and that's your forecast. Then they moved to statistical forecast, which I think is still where most of the state of the art is now. So you measure nowcast. You know that when nowcast is this, then most likely forecast is that, and that's your forecast. And then the real way to do is to really do a flow prediction, which is also contingent on doing demand.
00:48:08.312 - 00:48:23.824, Speaker B: Doing a flow prediction is called the dynamic traffic assignment. That's essentially having a forecast model that is initialized with the nowcast and may be fed with some statistical forecast for demand. I could bet you that no company today is doing that, because it's super hard.
00:48:25.404 - 00:48:34.292, Speaker C: But really, my question is not, I believe they're not doing that, but the real question is, if they did do that, would that make things better or worse or the same?
00:48:34.428 - 00:49:10.106, Speaker B: And this is very interesting. So I don't know. And this needs to be studied because essentially, now, when you essentially have a controller that is controlling your plant with a version of the future, so it's like doing NPC with a version of the future that can only be as good as your future. So I assume that with some guarantees on that future, maybe you can prove that it's better. Maybe, but I don't know. And so that's an open problem. That's a problem we're working on.
00:49:10.106 - 00:49:43.190, Speaker B: That's a problem to which I don't know the solution. And that's a very important problem because everybody's going to forecast a future, but they only have partial information because Google does not know what Apple drivers will do at vice versa. So even then, there is a game in predicting the future. So you can see why this problem is extremely complex. And this is open. I mean, if anybody has any idea on how to solve this, I would love to talk, because I think that's one of the holy grail of cracking this problem. And you can also imagine why this is hard to convey in a newspaper.
00:49:43.190 - 00:50:21.974, Speaker B: I mean, obviously. So when people assume that if you. I mean, the notion that the quality of your forecast and the non cooperative nature of your forecast with partial information is going to affect the outcome, it's complicated. So, summary of the talk at the core of this problem is selfishness, which is we all want to get to our destination faster. That creates this dynamics loop I have created here. That is part of this bigger system, which essentially is the way it runs. And that bigger system itself gets abstracted by the app providers that learn over time how things work.
00:50:21.974 - 00:50:59.184, Speaker B: And the reason why that learning is important is that part is done. I mean, 100% of the population statistically has a smartphone, if you average the fact that people have more than one connected object. But what's changing constantly is demographic growth. We will reach 50 million people in California by 2050, I believe. And then obviously locally, like when I was a student at Stanford, there was a small startup called Google. And we just found in the newspaper that if they piled up all the buildings, it was in the news yesterday, it would be a tower which is 854 meters high, I think, with 120 floors. So now there's probably 40,000 people on the campus.
00:50:59.184 - 00:51:20.116, Speaker B: And that has completely changed the dynamics of traffic. There's a factory of Tesla which grew millimeter overnight with 5000 people. In the next three years, there'll be 18,000 units open in Treasure island in the middle of Bay Bridge. You can imagine what it will do to traffic. So the point is, this is when the agent kicks in, right? Because the agent, over time, learns. Every month there's x thousand more people. And that's where it learned.
00:51:20.116 - 00:51:50.722, Speaker B: So these problems are really important because this is the way the system learns over time. And we have to understand how it learns if we want to manage it better. So I'm going to stop here because I think we're at the end of the talk, but I'd be very happy to answer any question. And again, I want to thank Dick and Simon S Institute for inviting me here today, and Peter, for organizing the session. Thank you, Josh.
00:51:50.818 - 00:52:08.058, Speaker A: So, satellite imagery, I guess, now we're able to get telemetry down at unit timescales on some parts of the world. Are you using that? Does that help? Obviously, there's clouds, so that's an issue. But is there some extra ground truth you get from that on some days?
00:52:08.226 - 00:52:44.894, Speaker B: Yeah. So I spoke with them, DLR, the Deutsch Luftenram for Germany, because they were able to reprogram a satellite to orbit over California about ten years ago. Low orbit, I think, and I'm not an expert in telemetry. They were showing ten years ago that they could capture 70% of the moving vehicles from low orbit imagery. There's an issue of real time and everything, but I believe that. I believe that this could provide probably the best data there is for ground truthing, assuming you have good visibility. So it would be really interesting.
00:52:44.894 - 00:53:16.514, Speaker B: I think this is probably the only way to get global ground truth, because as soon as you start to probe people with GPS, then you always have the issue of who is transmitting and who is not transmitting. I think if we got access to that kind of data, I think it'd be really great. I heard, and you probably know a lot more than I do, but I heard some large corporation acquired a satellite company some time ago where they could see their competitors trucks shipping. So if that can be done, I assume we can do a lot more good things.
00:53:20.774 - 00:53:38.154, Speaker A: If you had a system that was able to beat the Nash equilibrium, presumably one company that can give advice to all drivers, it seems almost inevitable that you would have two drivers in exactly the same circumstances who would have to be given different recommendations.
00:53:38.234 - 00:53:38.894, Speaker B: Yes.
00:53:39.474 - 00:53:42.050, Speaker A: And one might turn out to be better than the other.
00:53:42.122 - 00:53:42.378, Speaker B: Yes.
00:53:42.426 - 00:53:44.434, Speaker A: Which raises a whole fairness issue here.
00:53:44.474 - 00:54:24.782, Speaker B: Absolutely. Okay. So the question for the viewers is, if you had a company that had 100% of access to everybody's driving, and two people were going to the exact same location, from. From the exact same location, in practice, they would be potentially given different recommendations, one be better than another, and then the whole issue of fairness. So. And the thing is biology, probacar is very involved in this, and I think what you will probably show, or has shown throughout this workshop, is that people will not give half an hour of their life for a free cappuccino because it's not worth it, given your consulting rate. And so then the question of fairness becomes who gets the information and who gets the good information.
00:54:24.782 - 00:55:25.494, Speaker B: And there have been a few companies over the last couple of months which have started to make very timid statements that some people might get better recommendations than others, and that's for the greater good of society. But obviously it's a very bad value proposition, so I don't think they can push it. The truth is, the only way to make sure that people get better or worse for combination than each other, which is the only way you can systemize optimized, because if you want to reach system optimum, someone has got to pay the penalty for the greater good. And that in terms, if you measure in terms of trial time, so you have to compensate the fact that you have incurred that penalty by something else. Free goodies are not going to work, but access restriction is likely to be the thing that is going to work. And so, for example, if you look at the one on one corridor today, this corridor is completely asphyxiated by the growth of economic activity because of phases. Facebook, Google and all the other big companies, the only way they can save travel time reliability is by making either access restriction a bus lane or tolling, which is problematic because of the gas tax.
00:55:25.494 - 00:56:16.174, Speaker B: And so I think what you see happening around the world is true in Singapore, is that tolling or access restriction is probably the only way to make sure that now someone is going to take the penalty, because some people don't want to pay the toll. And if you make the toll high enough, certainly they will not pay the toll. Or if they cannot get ten people in a vehicle because it's a smaller vehicle, they won't be able to access the lane. And so now that's how you offset the penalty in travel time, with money. It's very hard to do with a carrot, because carrots are hard to make work, but it's easier to do with sticks. But sticks have issues, because now going back to your first problem about fairness, well, if you have a tax on a specific corridor, who is going to be affected by it? And that's a whole other dimension of the problem which policymakers have been struggling with for a while, and that is not easy to solve.
00:56:18.914 - 00:56:34.394, Speaker A: With regard to individual drivers motivation, travel time has been the sole one here. Could you smooth out, could you affect effects that equilibrium by offering another currency, perhaps risk?
00:56:34.774 - 00:56:37.542, Speaker B: So this is very interesting. Give it up to the individual to.
00:56:37.558 - 00:56:39.314, Speaker A: Make those trades and hope that the.
00:56:40.254 - 00:57:21.162, Speaker B: Absolutely very interesting question, if you go to the airport in general, you don't want the expected shortest time, because expected value doesn't do anything for you to miss your flight. You want the path that maximizes your probability of on arrival time, on time arrival. That's called sota stochastic on time arrival. There's a very amazing group at Cornell led by Samita Samara Nayake, who is working on that problem. Former graduate of Berkeley. And instead of minimizing your expected value, you maximize your probability of arrival on time. By the way, this is super hard to convey to people on an app.
00:57:21.162 - 00:58:14.514, Speaker B: So even though everybody probably in these companies realize that's the right thing to do, I don't think any company will be able to even pull a product where they can explain to someone that concept. It's very, very hard to convey simply. But the interesting policy question that you raise is what people really care about in many cases is reliability. Like, if you're going to Google from San Francisco, you want to be able to not miss. You want to be, you don't want to miss your meeting half of the time because travel is unreliable. And what's very interesting is that who benefits from a carpool that is becoming a high occupancy carpool lane? Like the fact that if suddenly the carpool lane become a bus lane on the 101, now travel time becomes reliable if you're driving the bus, because we have not saturated the number of buses on that lane. So I think there's still growth before it saturates.
00:58:14.514 - 00:59:24.936, Speaker B: And today, if you look at who is lobbying for this, and if you think of who is even in favor of this, you'll find that Uber and Lyft have both made statements in favor of high occupancy lanes. Because if I'm a company like a TNC, what I would like to sell is travel time reliability. And the way to sell it is to push occupancy, which restricts access and pass the cost to customers. The more customers in the vehicle, the more the cost gets absorbed by the number of customers. So it's interesting, because I think the way we're going to see this TNC work in the future is they will push for more legislation that would advocate for higher occupancy vehicle lanes, which is a way to push travel time reliability. As Uber, I would like to be able to provide an Uber service from San Francisco to the airport that is guaranteed 13 minutes, if it's guaranteed 13 minutes, because there's always five people in the pool and the pool gets to access a special lane, then as a company, have an amazing product. And so I think we're going to see this happening over the next couple of years in many cities, like essentially, these companies are going to start advocating and advocating because they provide a good and interesting product for people that doesn't exist there.
00:59:24.936 - 00:59:31.480, Speaker B: I mean, when I go to the airport in the morning, it's, you know, you always have a little bit of an adrenaline rush if there's an accident. You don't want that.
00:59:31.552 - 00:59:35.884, Speaker A: So you'd offer to not count the driver as part of the hov requirements?
00:59:36.064 - 00:59:47.944, Speaker B: Well, that's another issue, but I think as long as you get a hov four, then it becomes moot. But you're right. I mean, because today the driver technically is a passenger. So that's a loophole. It needs to be fixed.
00:59:49.724 - 00:59:58.948, Speaker C: So I think you already answered this. So I'm just repeating, but why do the companies, not themselves, enter into a treaty obligation?
00:59:59.116 - 01:00:01.806, Speaker B: Into a what? A treaty. A treaty? Yeah.
01:00:01.830 - 01:00:14.278, Speaker C: Why don't they have a treaty organization to get all their passengers where they're going more efficiently? I think you already answered it because it has something to do with having to differentiate customers.
01:00:14.406 - 01:00:46.926, Speaker B: I think there is probably several levels of answer to that question. First, it's even striking that they don't do it internally. Like, there are probably countries where there's only a single routing app that people massively use, and even that company would not do it. Because I think the first level of answer is that it's going to be super hard to convey to the public. Going back to your question that someone is incurring a penalty, because the only way to have a treaty is that some people have to pay the price that you have to offset. So I think that's the first level of answer. Then the second level of answer is that, I mean, it is the prisoner's dilemma or the notion of a Nash game.
01:00:46.926 - 01:01:07.988, Speaker B: Anybody who deviates from a Nash equilibrium is worse off. So, I mean, how do you make sure that these two companies, who are main competitors and how you enforce and can you subpoena? So there's probably several layers of reasons that need to be overcome, why that has not been done. They're sequential, but you've got to start internal. Yeah, they're sequential. You're right. Internal and external.
01:01:08.156 - 01:01:09.596, Speaker A: All right, why don't we thank Alex?
01:01:09.700 - 01:01:10.244, Speaker B: All right, thank you.
