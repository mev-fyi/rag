00:00:00.080 - 00:00:37.064, Speaker A: It's a great pleasure to introduce Dee, who's going to with us. From the usual joke here is the school across the bay or the farm? We have all these little insulting ways to talk about Stanford. I will tell you my joke, which is what's the difference between Stanford and Berlin? So do my Stanford print. For the answer to this Stanford is clean, safe and boring. Berkeley is the opposite. But we are very welcoming to our Stanford friends because some of our faculty also came from Stanford.
00:00:42.964 - 00:01:00.766, Speaker B: Hello everyone. Thank you for being here. I know it's late afternoon and I know you are here to listen to my talk or Scott talk. Thank you for that. Md. I want to first ask a question. How is the workshop so far for you? Okay, great, great.
00:01:00.766 - 00:01:30.220, Speaker B: So the workshop is on large language models. Apparently I'm not going to talk about it today. What I'm going to focus on is to share my thoughts on people and the real world uses. I think Dan already opened this up by seeing that the next phase of natural language processing may be more real world problems. So hopefully I can share some thoughts here. A lot of issues. Yesterday Nicholas was also talking about attack and risk.
00:01:30.220 - 00:02:29.404, Speaker B: For me, I kind of approach it from a social domain. This is earlier this year. I'm sure if you can imagine that questions like this will get dodged by large language models because it's not a good action. And then at that time, instead of doing very automatic optimized way of making the fail, we were thinking about what would be some different reasoning structures for social or controversial scenarios. So a simple phrase like think step by step is going to give you all the potential drama of a lot of questions. We actually have a lot of cool examples in the paper and apparently this is not working today, so you do not need to give it a try. Other things I want to mention is that we have been talking about a lot of progress in NLP, but apparently there are also arising issues reported with those models.
00:02:29.404 - 00:03:46.724, Speaker B: I do not need to remind you again, but just reading those news headlines we can see that the models seem to still have troubles in dealing with issues such as culture, heat, speed, biases, and those issues really hinder the practical adoption of large language models in the real world. So another thing I want to mention is that this is not only just about biases. If you think about a value which is a very high level social construct, apparently if you use models trend, let's say on a western centric corpus for per training, alignment, etcetera, the values may get alerted to reflect the us culture. If you apply it to a slightly different document or different region. So apparently a lot of issues seem to merge from the people side. When I think about a lot of the specific phenomena we see so far around all sorts of instances we can imagine around the low resource language bias, fairness, hate speech, harms, limited social understanding. The generalized issue are actually about our models do not have a good understanding or awareness of underarms and groups, culture, ideology, norms, social norm, common sense, social context, etcetera.
00:03:46.724 - 00:04:24.896, Speaker B: And then taking all together, the common theme I think is the human factor. Here. Human factor is underrated today because it's not like a math function you can optimize. It's really hard to operationalize, it's really hard to capture. However, it's very essential and necessary to be well studied to push either large language model or NLp to the next level. So I was wondering whether we could have a different view towards this space. So people, this is a term I'm going to introduce today from large language models.
00:04:24.896 - 00:05:26.960, Speaker B: In the past few days we have been looking at the research around the reasoning, benchmarking, robustness, verification, grounding, interpretability on the people said there are also a lot of very exciting questions that we can take a look at. There are social factors, there are culture, norm, value, fairness, privacy, interaction, trust and impact. All of them seem very important. We all agree. How should we get started there? So my research view, or my action for the next step is that we need to think about when people are interacting with large language models. How should we approach from there and move forward from there and then specifically, very selfishly related to my own research, there are three keywords I want to kind of emphasize and then I will give very quick highlight of case studies to support each of them. So, socially aware, human centered and positively impactful.
00:05:26.960 - 00:06:15.574, Speaker B: What does that mean? When I think about the space of large language models or NLP, I think there has been a long standing focus towards mainstream populations. But we think about human people. There are speakers who do not speak mainstream languages. There are low resource languages, dialysis that exists in almost every corner of us and also the world. There are vulnerable populations who probably need assistance in terms of language understanding and a lot of other applications in terms of culture, ideology. It comes to the question of whose culture and value are represented in this process. Norm and context is going to affect how we understand a specific sentence, especially when it comes to interaction with large language models.
00:06:15.574 - 00:07:43.054, Speaker B: So in order to kind of formulate this space, my colleague and I, we kind of introduced this very practical and simple framework to think about how or what type of social factors are involved when we use language to interact or to communicate. So when you think about speaker, who is a receiver, what's their social relation, in what context, guided by what type of social norm, culture, ideology, and for what type of community goal here. The other thing I want to kind of emphasize today is that we already see a lot of amazing applications you can perform. Use NLP systems, AI systems, large language models. I'm advocating for more human centered design thinking into this process in terms of how we can incorporate domain users and stakeholders into different stages of large language model research. If you think about problem formulation, data collection, model training, evaluation and deployment, especially with the recent emergence of AI agents, I also feel we need this kind of mixed initiative interaction, not only human in the loop, but also how human can benefit from machine assistance and a lot of cool stuff we can work on in terms of human AI interaction for better coordination, collaboration. That is how we can leverage the intelligence from both parties.
00:07:43.054 - 00:08:43.718, Speaker B: Last but not least, the last keyword I want to share today is that there are a lot of objectives in the community that we can take. We used to think that accuracy performance is the main direction. There is a single number that we are all cheating for here. I want to mention that there are lots of other objectives we need to think about, not just focus on performance. If you think about a large language model and the people interaction, there are lots of social beneficial applications and knowledge domains like policy, mental health, education. A lot of those, not only we just use them as a tool in this process, we are going to produce knowledge, insights, and also translational impact from human AI interaction. And the other dimension is that there are also a lot of influence of large language model on people, and we see not much, or I guess not a lot of research in this direction.
00:08:43.718 - 00:09:52.884, Speaker B: If you think about misinformation, think about allocation harms, how large language model could boost productivity, how it might affect trust, and how it may even influence the creativity of our human beings as a whole. So that's a very high level abstract. Introduction I want to use three case studies to go through what I mean by those very fuzzy and high level constructs. First, I'm going to talk about how large language model work when it comes to social context. And the second is that I want to make large language models understand different type of Englishes so they already understand English. What do I mean by various Englishes here? And the third, which is a very exciting part that I really want to get to today, if I have the time, is about how we could actually build multi agent systems powered by large language model for social skill training such as negotiation, conflict resolution, positive thinking. Any questions so far? Great.
00:09:52.884 - 00:10:40.508, Speaker B: So first, when large language model meets social context, why I was thinking this way? Well, again, I come from a traditional NLP training background. I also get a lot of training in HCI. When I think about these problems, there are actually two split world in front of us. Sometimes when we think about large language models, we need to go beyond accuracy. If you look at the left aspects, there are accuracy, performance, efficiency, scalability, a lot of terms we can list. But then on the other side, if we think about interaction, there are usability, engagement, interaction, human factors. As I mentioned earlier, is this an easy adoption process? The answer is no.
00:10:40.508 - 00:11:25.040, Speaker B: There are a lot of unseen data. We need to make it adaptive to users and context. We need explanation who to hold accountable for errors, and also a lot of unpredictable model behaviors that we probably don't see in our validation process. On the other direction, if we think about this is just application question, why don't you just build a model for it and solve it? The reality is more difficult than that. It's really hard to articulate what can be done, what cannot be done. If you think about the technical feasibility, we always make a lot of assumptions towards okay, here is a gaussian distribution, here is a uniform distribution, et cetera. Real world probably doesn't work in that way.
00:11:25.040 - 00:12:03.234, Speaker B: There are also prototype challenge and a lot of open feedback that we don't know how to put into our math equation. So we kind of want to approach it from a relative comprehensive view. When I mentioned about social context. So earlier this year, we were thinking about what if we provide a lens or evaluation framework for this type of models in social context. And social context is another very vague word. In most cases we call it social media, or we call it other type of things. Here we want to make it more grounded in our science field.
00:12:03.234 - 00:13:04.484, Speaker B: So we take a look at psychology, political science, literature, history, sociology, linguistics, and then what do we do? Is that we're going to get all the well studied or very well represented tasks in each field and then use them as a test bed to see how current models are doing for these different dimensions. As you can see, we have emotional recognition, humor recognition, platonists, empathy, ideology, detection, agenda setting, framing, power relation, social role, data features, a lot of those. This is not our focus if you look at a benchmark. So for each of them, we kind of gather resources we have from the community. There are already a lot of such resources available, and you can see that those tasks are not simple. They are not simply just a sentence classification. Some of them may involve utterance conversation, some of them may involve a lot of like document level understanding.
00:13:04.484 - 00:13:42.140, Speaker B: And when you have such a rich context, you can ask many, many, many research questions, beyond the scale, scale law, beyond domain utility, beyond the functionality. I'm just going to mention maybe one or two of them today. So what it turns out that is that we need to run this like a giant evaluation in terms of all the social tasks or social context we can think of. And apparently we want to make a comparison here. So if you look at the blue boxes, I know it's really hard to read numbers. That's not the goal here. We also have the red boxes here.
00:13:42.140 - 00:14:13.704, Speaker B: So the blue boxes are all the current open source models we can access. Many of them are a very large scale, and then the right boxes are the open eye models. We also have this box of fine tuned models here. Many of those numbers are not only centralized in terms of gt four. And it turns out that some of the open source models, especially when the output is short and very concise, they can still do a decent job even in this kind of zero shot prompting model.
00:14:14.284 - 00:14:15.684, Speaker A: Excuse me, can I interrupt here?
00:14:15.724 - 00:14:16.224, Speaker B: Sure.
00:14:16.364 - 00:14:30.776, Speaker A: Explain one of these numbers, or just to get a sense of what you mean by this data is this. So suppose there's that rule of ideology, what does that number mean? Or comparing different models.
00:14:30.960 - 00:14:32.416, Speaker B: So let's take a humor.
00:14:32.520 - 00:14:32.896, Speaker A: Yeah.
00:14:32.960 - 00:15:08.576, Speaker B: Okay. If you look at the line of humor, basically we have traditional use the test set, right, for a humor. What do the numbers represent, their accuracy or f one score? The score, yeah. In percentage or. No, just a score range from zero to 100 if you take the humor here as one example. So we have, let's say standard test set to categorize whether something is a humor or not. And then you can basically calculate how well a model is doing compared to ground truth on a test set.
00:15:08.576 - 00:15:56.712, Speaker B: This is how those numbers are calculated. So as you can see, if you use fine tune the model, which we use a roberta large here, you can get something like 73. And then if you use like flying ul two, which is a very big encoder decoder kind of model, open source, that you can get something like 56.8. In this case, if you use gp four, you can get something like 61.3. And then for many of those tasks, we still see a difference in terms of fine tune models versus this kind of zero shot prompting. This is a classification. So basically you have a lot of sentences some of them are humorous, some of them are not.
00:15:56.712 - 00:16:00.204, Speaker B: You ask the models whether they have humor in it.
00:16:04.984 - 00:16:15.076, Speaker A: The nature of most of these tasks, are they binary or multiple choice kinds of tasks, or do they have a more continuous gradation of quality?
00:16:15.220 - 00:16:33.184, Speaker B: Yeah. So some of those tasks are like a battery classification. Some of them are multiple choices. Some of them even have multiple categories. And then the space could range from two to four. And some of those event argumentation, as you can see here, extraction task actually has more than 20 labels.
00:16:36.624 - 00:16:37.248, Speaker C: Okay.
00:16:37.336 - 00:16:50.344, Speaker A: But I mean, probably somebody might argue that here, the ground truth thing has been done by a certain set of people and culture can have an impact on what you regard as funny or not.
00:16:50.464 - 00:16:53.024, Speaker B: Yes, I'm going to go there for a vision task.
00:16:53.104 - 00:16:57.760, Speaker A: We all agree. But what is funny or not funny can be very culturally.
00:16:57.952 - 00:17:19.284, Speaker B: Exactly, exactly. So we are going to. So I wasn't going to have people looking at that giant table of numbers. That seems not respectful. So what I'm going to do is that let's take those numbers and put it into some questions we all care about today in terms of how large language model empower individuals and what those numbers mean.
00:17:20.744 - 00:17:53.154, Speaker C: Just elaborate on that. You know, sometimes averages don't convey much information. What, you know, information is conveyed by the outliers. So, for example, if you say, you know, is the following offensive? Well, you know, maybe that's not, that doesn't convey much information, but if you say, look, extremely offensive and you, you know, people, people usually agree on what's extremely offensive. But if the model disagrees, then you. That has a lot of information.
00:17:53.854 - 00:18:47.514, Speaker B: Yes. Yes. So many of those tasks, they are designed to kind of capture this level of categories or spy charm. So you can imagine that if we really want to differentiate that, you can have like a scale of one to five to rate the degree of offensiveness, etcetera. And it's true that for the very extreme cases model may have a very confident prediction, but then somewhere in the middle, the prediction may not be very accurate. So I guess one of the takeaway earlier was, or at least from the prelate, which those models seems to do very well in terms of different tasks. And then we were thinking about something a little bit different here because we have been talking about that.
00:18:47.514 - 00:19:33.716, Speaker B: Okay. Large language models can replace human in many, many ways. And then especially related to social context tasks. One of the very challenging thing is that you need to have those resources to test a very sophisticated construct. So what we then did is that now let's take whatever the best performing model from my giant table and use that as a data annotator. This could be GPT four, this could be flung t five model, et cetera, et cetera. And then we are going to let it do the annotation for us and then calculate what would be the agreement between human and the large language model and then compare that to what would be the agreement between a human and human here.
00:19:33.716 - 00:20:25.324, Speaker B: So as you can see on this page where I just take a few numbers for utterance level task like emotion stance, misinformation, the f one score is what we can get from the best performing model. And then if you look at the agreement here in terms of annotation agreement utilizing large language model as a data annotator, we seem to get a really good performance in terms of the flaccid kappa here. So something around 0.6, which is good. And this is even better compared to when we use average human for human human annotation agreement. Yes. Could you expand more on why people would think, because if human agreement is low, maybe some things are not as concentrated.
00:20:25.324 - 00:21:40.354, Speaker B: So there's this kind of artificial standard. Yeah, so that's a good insider. So one of the things is that this is the page with the really good annotation. Let me finish the next two page and then we take it together to answer your question. So if you see this kind of trend for the task I just mentioned, like emotion stance, which are widely existed in many of our current exploration, which we may have a lot of data because pre training covers a lot of political or other kinds of domains. When it comes to tasks like ideology detection, semantic change, humor, power relations, we see that the agreement tend to drop to fair here and then for very kind of subtle task, as some of the question was asking, like implicit hate speech detection or toxicity prediction or persuasion detection, we see that the agreement tend to drop significantly between human and large language models, while the human human agreement is a little bit high.
00:21:40.894 - 00:21:48.390, Speaker D: So what humans, are they, are they careful workers or careless workers? Are they actually disagreeing or they're just careless?
00:21:48.502 - 00:21:48.886, Speaker C: Yeah.
00:21:48.950 - 00:22:32.934, Speaker B: So because we cover like more than 30 tasks here, it's really hard for us to do the annotation for each. So right now, the human human annotation agreement actually come from the original paper. When the author offers this data set to the public, they need to do this kind of annotation check to make sure the data is quantified, something reliable, so the numbers actually are taken from those papers. You can imagine those as, I guess at least experts or people who are trained to be well qualified for the task. Also, I want you to kind of pay attention to this. So this is like on dialect. We can see that it's actually extremely low in terms of performance.
00:22:32.934 - 00:23:58.934, Speaker B: So one thing we have been thinking about is that if we cannot simply let g four being the annotator here, is there a more kind of collaborative framework we can introduce? So one thing we are exploring right now is in terms of this human AI collaboration framework, where is it possible that we can design some mechanisms to estimate the expertise of those large language models? So this could be done by existing methods, such as self evaluation. Ask GPTN to judge whether a response from the previous version is a good annotation, or you could create many, many kind of different prompts to quantify the same construct, and then measures uncertainty there as a way to judge the expertise there. And this could also lead to some sort of change in terms of how we think about the work allocation. When we have a task where we have certain budget for annotation or accuracy, we have different type of data points we want to annotate, because fine tuned models may still give us the best number when it comes to the task. And then how would we assign this kind of data notation to large language models and to humans, and how we could actually take it all together as optimization problem.
00:24:00.994 - 00:24:02.282, Speaker C: Can I ask a question?
00:24:02.338 - 00:24:08.534, Speaker A: Sorry?
00:24:08.994 - 00:24:16.464, Speaker C: Would it be natural for your large language model to output probability instead of just a decision? Zero one.
00:24:17.644 - 00:24:24.292, Speaker B: So in our case, you mean with regard to expertise explanation, right?
00:24:24.468 - 00:24:36.020, Speaker C: Yeah, with respect to all the fates that we listed in the previous slides. So instead of saying what percentage, right.
00:24:36.052 - 00:25:48.088, Speaker B: So should we count it as score or like more like probability associated with it? For some of those models we actually can. For some of those we just can get a response. So basically we frame each of the choices as something like a multiple choice questions, and then let the models pick which type of thing with regard to the confidence here. So some of those you can actually frame the confidence as a further question to those large language models. So basically ask them to self evaluate, like from a confidence level one to ten, how would you rate your confidence to a current question? I'm probably going to skip this kind of like skidding law, but basically we observe something very interesting, and a lot of those I don't have great answers. So apparently for the Flan T five family models, we see that as the model parameters increase, they tend to do better in terms of those social understanding tasks. And the way that comes to this is from ada to text aventi zero, zero, one we also see this kind of like skating to some extent.
00:25:48.088 - 00:26:00.450, Speaker B: However, for this kind of social understanding task, we actually didn't observe this kind of jump in terms of a perfect skating law here. And in fact there's even a drop in terms of gt four performance.
00:26:00.592 - 00:26:12.274, Speaker E: Yes, I missed in the previous. And when you look. So these the harder tasks, is there also higher human to human disagreement as well? Do they track?
00:26:12.734 - 00:26:42.726, Speaker B: We have human agreement for some of those. So for this page, as you can see, implicit hate speech detection. So this is a task that we have a lot of hateful content, but they do not involve explicit words, et cetera, et cetera. So the heat treat is actually latent. And then for human, human, you can have something like 0.55 in terms of flexic kappa here, but then in terms of large language model and human, they can only achieve 0.19.
00:26:42.790 - 00:26:49.406, Speaker E: No, I guess what I'm saying is that. So here the models are not doing very well.
00:26:49.510 - 00:26:50.310, Speaker B: Yes, yes.
00:26:50.422 - 00:26:55.190, Speaker E: And previously, are the human to human agreements also worse than before?
00:26:55.302 - 00:27:14.726, Speaker B: No, it's comparable. Yeah. So there seems to be. I think there was one of the earlier question. When the task gets very subjective and require a lot of context or even background, you see that although human and human can agree very well with each other, human and the model cannot achieve something plausible here.
00:27:14.830 - 00:27:18.754, Speaker E: But if the task is subjective, why don't you see human to human disagreement?
00:27:19.774 - 00:27:52.700, Speaker B: That's a good question. I think in many of those annotations you actually do not just like take a sentence out of nowhere to give it to annotators. You wanted to have a very in depth understanding of what's going on. So in general, when we give annotators the annotation, we give them a context so they kind of understand that. Or if I say like implicit hate, I want to kind of understand the hate speech related to election. You probably want to have people who are familiar with us culture to do it, rather than someone who come from a different background.
00:27:52.772 - 00:27:55.004, Speaker E: But the model doesn't get that context.
00:27:55.164 - 00:27:57.572, Speaker B: The model currently does not get that context.
00:27:57.628 - 00:27:59.196, Speaker E: That sounds unfair to the poor model.
00:27:59.260 - 00:28:03.304, Speaker B: Yeah, we need a better prompting. Yes.
00:28:03.604 - 00:28:20.832, Speaker C: I'm going to ask a totally non human question. So you talk about these scaling laws, but one is kind of notoriously trained or just a stupid amount of tasks similar how they like compare.
00:28:21.008 - 00:28:54.654, Speaker B: Yeah, I actually put it as a limitation in my slides. So you are right. Like in the training of plan, there are actually amazing amount of tasks and some of them actually do have overlap with what we are testing here. So there is definitely a leakage issue here. But when we think about like a social context or social understanding capability measurement, we try to gather more and more kind of non overlapping tasks. So maybe a better thing we should plot here is that instead of the overall performance, we should put those like a non overlapping task as a way.
00:28:54.774 - 00:29:07.234, Speaker C: I guess maybe what I'm trying to understand is, is there claim here that as it gets more model parameters in the general web, it gets better at social understanding or that it gets better at remembering the tasks that we solved around social media.
00:29:07.674 - 00:29:58.670, Speaker B: Yeah. So two levels. First level is that if I just look at this figure, basically what I mean is that for flan type of models, when their scale gets larger, they tend to be better at a social understanding task. The level two is that in order to better provide explanation here, we really need to understand other kind of like leakage issue here. That this part, I don't have a greater way to quantify this moment. That's what. Okay, so a lot of questions, and I think this is also something very tricky when we think about, when we deploy those models to kind of do a lot of real world tasks, because if we think about constructs like humor or platinum or a lot of those constructs, it's actually very subtle and require expert taxonomies.
00:29:58.670 - 00:30:47.712, Speaker B: Maybe we need a context, maybe we need like a better assumption statement. And also the size of the target label space is kind of uneural compared to most of the tasks we are dealing with. And also a lot of like a temporal grounding in terms of knowledge, which seems a limitation here. And for many of those, I want to share it as an overall pattern, as we are still working on it. And there are a lot of issues we cannot tease out in this process, such as the leakage issue, and also a lot of the causality of whether this is a real kind of reflection of what's happening. But despite that, I think there are still kind of insights we can kind of take, take as a takeaway. So there is this kind of new paradigm, which is very exciting.
00:30:47.712 - 00:31:58.344, Speaker B: I didn't go into the detail of how human AI can collaborate to do annotation together, but this apparently seems to be a new paradigm of how we should think about a lot of uneural tasks and how we should approach them. And this also opens something around the simulation, which I'm going to talk about next. So the second part is, I want to switch a little bit since it's about people, and English right now seems to be the main language we are communicating with large language models. However, if we kind of read a lot of the content on the side and also pay attention to a lot of the research. In fairness domain, apparently current language technology seems to struggle when it comes to dialects or language that are not spoken by the mainstream population. And we kind of mentioned this in some of the earlier work when talking about the pre training, et cetera. Apparently a lot of the english dialysis data actually get filtered out from the pre training as low quality based on their publicity.
00:31:58.344 - 00:33:11.088, Speaker B: And this is one of the triggering for why those models probably cannot work well. The other thing is that because english dialects and the standard market English are so similar, when you simply combine them together, they might harm performance because some of the dialect features are probably going to be washed out by the large amount of standard American English. We have. We have a lot of empirical studies in terms of is this a real issue and is this a real issue for large language models on the right? You can see that when you plot the relationship between model size and their ability to recognize dialogue features, you can see that those numbers are actually not as impressive as we can imagine. F one score yearly is something around 20. Even with Ul two, we can get something like 50, but that's still not enough if we really want to use it in a context to see is this a dialogue? Should we treat it differently, etcetera. So my group has been building efforts in terms of how we can actually teach those models to understand a lot of the low resource dialects here, and we call it a multivalue effort.
00:33:11.088 - 00:33:45.006, Speaker B: With this kind of resources, you can actually do data stress test. You can take any existing NLP systems and perturb the data to see whether it works for a low resource dialect. We actually look at a lot of english dialects here. There are 77 of them. So what I'm showing you is only five of them. So we cover African American English, Chicano English, Indian English, blockchain English, and also Singapore English here. The process of how we build it is actually quite straightforward.
00:33:45.006 - 00:35:00.384, Speaker B: So we take our linguistic literature and try to first do this kind of synthetic transformation where we are going to translate a lot of the well known features for dialects so that we can convert a standard american English sentence into a specific dialect, like Ave here. This is a very well known feature associated with black English. So if you think about she don't have a camera, this will become she doesn't have a camera, it will become she don't have no camera in Ave. As you can imagine, this is actually going to cause issues to models in terms of how to deal with negation after the first stage, we actually work with native speakers that we recruit from all over the US to kind of validate and rewrite it into something that's both linguistically profound and also socially accessible. You may wonder, does it really matter? Well, if you look at this figure, take a very simple model like Roberta and take a very simple task, sort of not ambiguous task here, like sentiment. We saw that the performance dropped from 94.6 to 92.4.
00:35:00.384 - 00:35:51.770, Speaker B: This kind of job is consistent. If you look at a lot of the tasks we have access to, or we did a perturbation. And apparently for task like question answering, when there are a lot of spoken questions, we saw that the performance is even bigger. If you simply just fine tune with a lot of those type of data, the disparity still remains. This is just unlike African American English. If you think about Indian English or Singapore English, or applied to English, there are two different degree of job in terms of performance on something that we can claim we solve the task like conversational question answering. A lot of the cool work these days has been built in terms of how we can actually do efficient adaptation of large language models to more Englishes.
00:35:51.770 - 00:36:49.368, Speaker B: So hopefully the tool that we are building around the value can actually help develop this temp of dialect adaptation. So this is building upon many of the efficient fine tuning work that Colleen and Sacha mentioned earlier. The key idea is that you can actually use a very small amount of parameters to learn something specific to the task and also learn something specific to the dialect and then use them in a plug and play way for better adaptation. Just to give you a little bit of highlight of the performance. So this is what we show earlier in terms of sentiment analysis. If you only design this kind of adaptation with 0.04 percentage of tunable parameters, you can actually mitigate the gap well for some tasks and such mitigation is also consistent for a lot of other application here, just to quickly summarize this.
00:36:49.368 - 00:37:19.704, Speaker B: So I think when people think about dialect or low resource languages, a lot of times there may be like speech factors or accent factors that involve. So currently our work does not cover the speech aspect. And then also since a lot of the work are rooted in linguistics, we don't cover more recent lex converations or undocumented the variations here, I guess before I move to the third part, are there questions?
00:37:20.004 - 00:37:37.924, Speaker C: Yeah, really interesting, the way that you seem to construct the dialect variations. You translate questions that originally in standard American English into different dialects for testing. This is the distribution of the actual question. The language has changed.
00:37:38.004 - 00:37:40.028, Speaker F: Do you think that in the real.
00:37:40.076 - 00:37:48.004, Speaker C: World, across the different dialects, you actually have distribution shift of the types of questions themselves?
00:37:48.544 - 00:38:35.042, Speaker B: Yes, that's true. So there are a few considerations here. First is that when we want to kind of justify, there is a language disparity, the first step we want to make sure is that for something we have in common, or we have studied a lot to see what's the correspondence performance there, if we switch the language. So that's one of the design choice of why we go with this, so that you can see the number on standard American English and on other low resource language. So that's one of the design choices. And you are right. Like, in fact, in the process of working with people, we actually have a procedure called a participatory design approach, where even during the very beginning, we actually sit down with native speakers to see how we actually can do this research in a more responsible way.
00:38:35.042 - 00:39:34.334, Speaker B: They talk about that we don't use our dialect to read the New York Times in this way because maybe some of our data set has New York Times articles. You also get to know a lot of nuanced insights here. Like, okay, this usage is probably from last century, and now we are using some other different cool terms, or cool morphosyntax, et cetera, et cetera. And then there will be generational differences, there will be regional differences, a lot of those. So the space is actually huge. What I want to emphasize here is that if we just take a share the task and we can see the disparity clearly, hopefully there will be more and more people working on this problem. Okay, finally, I want to talk about something very exciting, and hopefully we can switch to a little bit of different mode in terms of social and people, and also how large language models can play a role here.
00:39:34.334 - 00:40:21.714, Speaker B: That's very exciting. So this is actually not something new starting from last year because of the impressive capabilities of those models. Despite that they may struggle with a lot of the subjective or nuanced social understanding. You can actually use them for a lot of cool applications. For example, one work from Adam actually used large language models to simulate humans and as a way to explore whether we can replicate human subject studies. There are also work that tried to build a village with different large language model powered agents so that they can interact with each other for very cool applications. So a lot of work.
00:40:21.714 - 00:41:33.310, Speaker B: And then, in fact, this is definitely related to human, right? Like large language models stimulating humans. And it's there because a lot of greater benefits we can achieve from this process. First, it's going to reduce your cost because you do not need to probably recruit a lot of people in order to test a prototype. You can pre validate a study, you can customize your participants for a lot of counterfactual aspects. You probably can reduce additive concerns. The issue might be evaluation is really hard, adaptation is really hard, representation is very hard. So I want you to keep all of those in mind in terms of thinking about when we simulate humans or even use them for different type of agents, what would be the issues here? So what I'm going to talk about is that because we see a lot of the cool applications you can perform on top of large language models, you can actually create a simulated environment with different roles there powered by large language model so that they can help human to do something.
00:41:33.310 - 00:42:10.354, Speaker B: Here we call this social skill training. And then in most cases we are going to have two agents. One is what we call AI partner that basically representing digital as users who you can practice with for different topics. There will also be an AI mentor. This is another large language model, empowered mentor who can provide feedback to help people improve their skills. So turning it into something more straightforward. Let's say I want to learn negotiation, I want to learn how to think positively.
00:42:10.354 - 00:42:43.372, Speaker B: Then I'm going to practice with this AI partner in a simulated environment. And in the process I also want to receive feedback. So there may be another expert AI mentor we can build to kind of mediate this process or help me learn this process. So think about you have a coach who's sitting with you for you to kind of practice your conversations. And this kind of social skill training can be very generic. Again, this is work in progress. So I welcome all sorts of feedback.
00:42:43.372 - 00:43:24.570, Speaker B: But high leveling, this is the process of how we approach it. We first sort of like design AI partner for people in those high risk conversations. And then we are going to operationalize expert knowledge into computable strategies. And then we are going to create an AI mentor by all sorts of conditional generation, planning, contacts, memory, etcetera. Last step is that we are going to integrate them into a simulated environment for users to learn safely. So what does that mean if we put it into different contexts here, think about therapy. Mental health is such a big issue.
00:43:24.570 - 00:44:01.752, Speaker B: We don't have enough therapists. There are a lot of people who want to learn how to be good supporters to each other. So the user here is noble's counselor who want to learn or enhance their therapy skills. And then the partner here is going to be a digitalized patient, and then the mentor here is going to be a therapy coach. Or if you think about conflict resolution, there will be people who want to handle different conversations and then the partner is going to be digitalized partner. And then a coach will be confident resolution experts. So you can also design similar situations for education and negotiation.
00:44:01.808 - 00:44:31.744, Speaker A: Yes. I have a question which is, I mean, this is a setting which could also apply to any other tutoring setting, right. Like teaching students how to solve physics problems. So can you say, and wooden, can you say what you think? Are there special reasons you prefer this domain versus teaching physics or biology or whatever? Because this has some additional complications. It could be a bug or a feature.
00:44:32.484 - 00:45:01.776, Speaker B: Yeah, yeah, exactly. So I think that's a great extension of how we can actually further leverage this framework. The way how we. Or how I pick this domain is that first, this is something I'm super excited. I think like social scale, social intelligence are something that we want models to help people with. And also a lot of those skills are actually not something you can easily learn in real world. Think about negotiation or conflict resolution.
00:45:01.776 - 00:45:55.556, Speaker B: You actually take courses or MBA classes or a lot of those, and those are not available for people who probably cannot afford it. And then. So part of the choices for what we focus on here is we really want to kind of make skill training, a lot of the key skill training, accessible to everyone. So that was one of my design choices. But with regard to teaching physics or teaching like other subjects, I think you will need a very grounded knowledge from that domain to sort of help ground it. Otherwise it may be a little bit challenging here we also need a grounding, but because of the very expressive nature of those large language models, you can actually do a relative good job in terms of practicing those corner cases for people. So in previous studies, there have been different results that there are different biases embedded in LLMs.
00:45:55.556 - 00:46:37.006, Speaker B: And like, let's say you have a woman negotiating the work and then the model has embedded biases. How would you deal with this case? Yeah, that's an excellent question. So the way how we approach them is. So the system I'm going to show you later, we actually make sure that, let's say you want to practice your skill in negotiation. You have someone in mind, you are going to customize what would be the specific role you are going to interact with. So we give users full control in terms of the explicit attributes you want to practice. Maybe female feel less empowered in those positions, so they really want to practice with a senior colleague, et cetera, et cetera.
00:46:37.006 - 00:46:54.094, Speaker B: So you can actually customize it in the interface, then the digitalized patient or HR or partner will be switched to a different Persona. So in this way we make sure that you can actually practice forces that you do not feel safe to do in real world.
00:46:55.474 - 00:47:32.250, Speaker F: Yeah. So I wanted to mention a different bias, which is that people often kind of treat solutions that come from computer system as kind of like more correct or sort of like more trusted. And so I'm wondering if in say a situation like conflict resolution, this can create problems where you're like, okay, like I said the things that the system told me I should say and the other person got mad at me anyways, it must be their fault and I don't have to worry about it because endorsed my response as opposed to them being like more willing negotiate.
00:47:32.402 - 00:48:11.422, Speaker B: Yeah, this is also an excellent question, so. But I think this is not the first time you face this. If you think about like coaching centers or a lot of the educational centers, you learn specific skill there. There's no guarantee that the scenarios you are going to see in real world will replicate what you have been taught in school. Right. So in this case, I guess what I mean is that there is going to be a distributional shift potentially in terms of what you are practicing here, in terms of what you are going to do in real world. Right now, the practice, the best practice we take is that you make every of those kind of potential issues explicit to users.
00:48:11.422 - 00:48:41.396, Speaker B: So when they practice with the system, they know that this is not going to be represented outside the world. They need to use this in a more contextual way. But the system is designed in a way to teach them how to learn those keys skills. So it's not actually about the exact wording you are going to learn in terms of conflict resolution. We actually take this literature from business school in terms of power interest framework. So you're actually learning strategies, not specific wording.
00:48:41.500 - 00:49:04.404, Speaker F: Well, I guess what I'm imagining is, because the model is queryable, people might use it also in their specific situation, right. They like, oh, imagine this hypothetical situation, what should I do? But actually it is like the actual situation they're dealing with and they're just directly getting advice from the model. Or do you think like for some reason that could happen with this system?
00:49:05.184 - 00:49:30.658, Speaker B: Yeah, I think that that thing can definitely happen. So in that case, I would wish that the user, when they use the system, they kind of customize the digitalized partner in a very specific way so that to kind of minimize the potential shift in terms of all sorts of scenarios. Yes, yes. That's what I'm going to show, I.
00:49:30.666 - 00:49:34.974, Speaker A: Mean, just your time. Let's give a good idea.
00:49:36.874 - 00:50:17.386, Speaker B: Let's just skip. So the current system we build is that you actually, let's say Alex is someone who want to learn this kind of support skills. And then without this type of system today, Alex may see something really bad, like just sounds really bad to a very overwhelming sentence. And then with this type of systems we are going to build, Alex may respond in a very supportive way. And then in this process, you will have two agents we introduce here. Jane is actually going to be AI partner. And then the care system here, which is our AI mentor.
00:50:17.386 - 00:51:18.750, Speaker B: Because of the sensitive nature, we actually need to fine tune a GPT-2 model to this very specific mental health therapy domain. So we have this offline model in terms of providing feedback. This is what is being deployed in the system. So as you can see, the blue boxes are actually from the person who want to practice, and then the member is actually this kind of patient who have issues. For any of those scenarios, you are going to recommend the potential strategy this person is going to take, and this user can actually take any of those messages and then further editing them to form their response. And a lot of the log analysis, since we kind of keep track of how people interact with the system, it actually helps around 84 percentage of the time. If you deep dive into what they like, what they don't like, people like the fact that they remind them of strategies, inspire response, increase confidence.
00:51:18.750 - 00:52:39.236, Speaker B: They don't like the fact that it may disrupt thinking or may limit the response or take time to read. Again, we do a lot of this kind of like very in depth interview with participants when they use the system. As some of them mentioned, that this is a great tool for listeners who are new or who are not feeling comfortable with the topic. I also want to just quickly show this type of thing since the earlier version is actually with GPT-2 in fact, you can actually do this with this kind of GPT four and you have much more powerful results. The way how we teach is that we are building this mixed initiative agent framework where we want people to practice difficult conversations and how to learn how to handle those difficult conversations. I'm going to skip the details here, but with this kind of system, compared to current training materials or the current training fashion, individuals who actually learn through our kind of multi agent systems are actually 40 percentage less likely to use aggressive strategies, is three times more likely to proactively propose solutions to deal with conflicts. This is basically what I want to cover today.
00:52:39.236 - 00:53:13.584, Speaker B: So I really hope that I at least convinced at least one of you that social understanding is actually very challenging for models at this point. And then there are lots of objectives or questions we could think about beyond what has been focused today. And just to kind of take it back, there are lots of issues related to people such as culture, value, norm, fairness, privacy, trust and impact. And I hope that there will be more and more exciting research in the space of human AI interaction. Thank you.
00:53:21.824 - 00:53:45.640, Speaker G: So like some of the strategies that you are coming up with, do you cross reference that with your training corpus to see that if it is derived from like valid sources or kind of combined from multiple sources? I'm kind of going into more like a topic modeling. But these are pretty critical decisions.
00:53:45.672 - 00:53:49.748, Speaker B: Right, sorry, you are referring to the skills field training or the first one.
00:53:49.836 - 00:53:52.664, Speaker G: Like the way you were kind of giving the strategies.
00:53:53.444 - 00:54:30.380, Speaker B: They are grounded in literature. So for example, for the therapy skills, we actually work with psychiatrists to leverage framework like motivational interviewing strategies, which is a framework of how therapists are trained. So we basically operationalize all the possible strategies there into a specific, like something like a dialogue act. And then we basically teach people in terms of specific strategies. Similarly with conflict resolution, there are expert taxonomy you can leverage from business contacts. And then basically we teach those concepts to people.
00:54:30.492 - 00:54:31.044, Speaker A: Yes.
00:54:31.164 - 00:54:48.508, Speaker G: Do you also see like multiple strategies that combined in an hallucination setting, like do you see kind of these kind of things and how do you protect or how. I mean, is there some sort of metric that you are looking to figure these out?
00:54:48.676 - 00:55:16.824, Speaker B: Yeah, so that's a great question. So right now, in the current process of planning, at each step we actually only focus on one specific strategy, which means that we are going to generate all possible sentences. Each sentence contains one strategy. So although you cannot cover all strategies in one sentence, but you actually can have a lot of choices. So user can actually explore which one to use and then have more counterfactual reactions?
00:55:18.364 - 00:55:18.700, Speaker C: Yeah.
00:55:18.732 - 00:55:49.702, Speaker D: So I have a question regarding to evaluation of this social skill training for these people. It seems like currently the evaluation is based on after taking the social skill training from the large language model, you ask them like how happy you are with the training and some kind of mini test, but like it doesn't necessarily reflect whether that people is actually going to be more skillful when they do it at practice. So I wonder, what do you think about that evaluation? And especially like the evaluation, the effect of that training will not be immediately available directly after the training.
00:55:49.798 - 00:56:14.798, Speaker B: Yes, excellent question. So, evaluation at several levels. So today I think I basically skip the offline corpus evaluation. That's the standard evaluation we do. Like, basically, let's say if you have a corpus, how well you can identify strategies. We focus on the level two, that is at this user exploration stage, how well that works in this kind of lab study environment. We have stage three that is more long term evaluation and that take a long time.
00:56:14.798 - 00:56:56.394, Speaker B: As you can imagine. For example, for therapists who use these systems, we actually have access to patients who interact with them. So we are going to quantify their self efficacy or confidence throughout the next three or six months. We're also going to quantify, like, for people who are willing to interact with people who use these systems, do their mental health change, etcetera. So there are actually different levels. We're also going to open this up for students in, like, psychiatry and also business school to kind of try these systems in their classroom teaching to see how well it compares to what you use today, like textbook or a lot of other devices.
00:56:56.774 - 00:56:58.126, Speaker D: Yeah, that sounds super exciting.
00:56:58.230 - 00:57:00.594, Speaker A: Thank you. Last question over there.
00:57:02.454 - 00:57:46.016, Speaker B: Another question I had. Thank you for the first task that you mentioned about annotation. A lot of these models are not stable and like, they're changing all the time, and there is no reproducibility, basically, with these models. How do you deal with that when you are using these models for annotation and then human evaluation coming into play? Yes. So, yes, models are changing and the results are changing. So I think we are in this like a very interesting stage that everyone is chasing the ball of large language models to do something. I think right now, the practice we take is that you still sort of have the best performing model at a given time step.
00:57:46.016 - 00:58:18.178, Speaker B: So you go with that. And stability is a key issue. Very interestingly, I think the recent models are kind of getting stable in terms of how you do different prompting and then temperature effects, but not to a great extent. And then we do have different ways of quantifying their uncertainty. When it comes to an instance, let's say, even with slight perturbation to the prompt or to the environments, the models decision change dramatically. Then that's probably not a best use case. For model to do the annotation.
00:58:18.178 - 00:58:26.054, Speaker B: You need to kind of defer it to humans so you can actually quantify this kind of stability or expertise to some extent.
00:58:26.954 - 00:58:40.234, Speaker A: Thank you. Next we have Scott. And since we don't have time for a break, can everybody just stand up and stretch.
