00:00:00.120 - 00:00:08.182, Speaker A: That sounds fine. I'll just. I won't. I'll resist the temptation to shout. Let's see if this is working. That is working. Thanks very much for the invitation.
00:00:08.182 - 00:00:44.798, Speaker A: Happy to be here. And I'm not sure what the interests are of everyone in the audience, so I've got a bit of a grab bag of different things to speak about today. The original title did reference perceptual representation, learning, adaptation, doing things across domains. I'll cover all of that to some degree. I mostly start with can you see that? I hope you can see that it's a very light, very weak laser. It fits nicely into my bag. So I'll talk, and this is more of an empirical talk, more of a talk on computer vision and vision and language.
00:00:44.798 - 00:01:34.940, Speaker A: So there's some nice dovetailing with the previous speaker, but I'll talk about domain adaptation from an empiricist's point of view, from a computer vision point of view. Although there are connections to theory and. And the theory of domain adaptation, I'd be happy to discuss those. I'll highlight some of our ideas and results applying vision to the application du jour of autonomous driving and discuss some of the philosophies and methods therein. And then I'll hopefully have enough time to go through some of our latest work on vision and language. I think a meta point that excites me right now in the field is this confluence of techniques that are bringing together not only vision and robotics, but vision and language. And it feels a little bit like an AI renaissance, and that's quite nice.
00:01:34.940 - 00:02:31.284, Speaker A: And when do I have to end? Is it 1010 55 1055? Okay, great. So no matter where you are, across the entire spectrum of that renaissance, whether it's vision or language or robotics, deep learning has been quite successful. But deep learning has limits. And I don't need to speak to this audience about where deep learning is super successful, nor do I probably need to speak to this audience about where it's where the limits are. The limits really are at the boundary of learning from few shot examples, having good models of predictive uncertainty and adapting to new environments and domains. And that last third, all three of which I think are essentially still mostly open problems. And that third one is what I'll talk about now, how do we have deep models? Despite all of the successes of deep learning, we still.
00:02:31.284 - 00:03:17.482, Speaker A: I'm missing my motivation slide, which I probably don't need for this audience as well, of, you know, you can train good models of pedestrian detectors if you actually. Let me just unhide that slide, if I. Oh no, I deleted it. I had to give a shorter version of this talk yesterday and I decided to delete the things that were hidden. But I have this slide of a notional autonomous vehicle being trained in Menlo park to detect pedestrians and drive around. And then I have a nice picture of Boston in the snow. Right? And if you train your pedestrian detector or your car detector with enough training data with a large enough deep architecture, they work pretty well right now.
00:03:17.482 - 00:04:05.124, Speaker A: But it's still not clear. When you take that data and bring it to a new data set, a new environment, your mileage may vary. And that's still the Wild west, and that's the kind of thing that we want to overcome. And so it's, well, so it's well understood that if domain adaptation is a classic problem statement now reincarnated with deep architectures, you might have some source domain with a certain set of tasks and labels here, bikes, chairs and backpacks. And I would be using some sort of traditional convolutional network here to perform some classification tasks. But now I drive to a new city, I get a new camera, things change. I don't know exactly why, and I don't have an analytic model of how they've changed.
00:04:05.124 - 00:05:22.256, Speaker A: I just get a new consumer or a new new environment, and I still want to detect, perform the same tasks here. I'd like to classify backpacks and chairs, but when I train on all this very nice, clean data scraped from the web, and I test on images from a mobile phone or a mobile robot or something like that, we can show arbitrarily poor performance curves, even with the best, maybe not arbitrarily before performance, but significantly poor performance, even with the best, highest capacity, fully supervised learning paradigms. Now, if you happen to have labels in your target domain, that makes your life much easier, and there are a lot of things you can do. And so it's quite an interesting and active area right now of specifically building few shot deep learning algorithms. That's not what I'm going to talk about today. I'm interested in the case where you have no labels, no task labels in the target domain mean, so I can't just apply fine tuning, I can't apply any of the recent few shot learning algorithms to do this. And I'm also not gonna, I'm gonna assume that I don't have any specific labels of pairs between the domains.
00:05:22.256 - 00:07:09.394, Speaker A: That's also a very good thing to have. And if you have an environment where you have that, the problem isn't so severe because you can essentially do metric learning, or as it's called, siamese network learning in the deep learning literature, and optimize your representation to say these two points, these two images, are coming from different domains, but they should project to the same point in my representation, and that's easy to do. So what can we do when we don't have any labels in the target domain or any explicit constraints about pairs of things that they should actually map together to achieve domain invariance? Well, both in theory and in practice, the methods in the literature have converged, at least from my perspective, on this notion of minimizing discrepancy. And here's a cartoon version of what that looks like. Notionally, I have some source domain marked in red where I'm trying to distinguish phones and bottles, and whether I'm using an SVM or using a deep network, I'm notionally going to think of learning a hyperplane of some form or a notional classifier, depicted by this line here, which learns to classify one class from another. Problem is I go to a new domain, the blue domain, and everything's just a little bit different, actually, and it could well be in the representation that I learned to solve the source task, the points that would project to points in this representational space that are quite far away from where you would like them to be. And in fact, the distance between the domains might dominate the distance between the classes, and you could just get everything wrong, as you would in this example.
00:07:09.394 - 00:08:17.332, Speaker A: So what can you do when you don't know anything about this new environment you're in? Well, you'd like to say the assumption underlying the idea of minimizing discrepancy is that I would like to make decisions in a representation that's invariant to some domain transformation. So I don't know what the domain transformation is, I just know that I'd like to be blind to it. So that means I'd like to minimize the discrepancy between these domains or distributions. And I should also preface this with in my work and in the work in domain adaptation and in the vision literature, we take a very broad notion of what domains are and what domain transformations are, or dataset bias. It could be anything, it could be different distributions, it could be different labeling functions, it could be different modalities, it could be different feature dimensions. So we take a very broad view of what discrepancy could be in terms of what we're trying to overcome or tackle with these methods. So what do you want to do? You would like a representation which doesn't see a difference between these domains, it's invariant or blind to that bias.
00:08:17.332 - 00:09:15.314, Speaker A: And so let's just optimize for that. That leads to this notion of minimizing discrepancy in the representation, and a series of methods in the literature over the years that maybe start off with the idea of let's align the means of these distributions. I don't have class labels over here, but if I want to buy into this constraint of minimizing discrepancy, I can say I should at least optimize the representation so that the mean of the blue points and the mean of the red points are the same. Because actually I do have labels in the target domain, right? I actually, I don't have task labels, but I do have the domain label, right? I know these are blue points and these are red points. So I can establish this constraint that the means should be aligned, and that leads to one class of methods that's been been successful over the years. Maybe means aren't enough. Maybe I can align the moments of the distributions.
00:09:15.314 - 00:10:46.670, Speaker A: That's another set of techniques. And in this contemporary era of adversarial network learning, an even more powerful and very state of the art approach is to say, I'd like to have a representation such that even if I tried to train a high capacity discriminative classifier, it couldn't tell the difference between these distributions. Right? So that leads to this latest set of techniques that we and others have been working on, on adversarial domain adaptation, where you're simultaneously trying to learn a good old fashioned supervised classifier in the source domain, perform some task in the source domain, but you have some additional domain, or ultimately multiple domains, where you don't have labels there, but you'd like to have a representation where you can't tell the difference between the domains. So we're simultaneously trying to train a domain classifier, also a convnet in our cases, because we're using images and we penalize inversely according to the success of this. So we have an adversarial loss, just, you know, in many, we developed this independently of gans, but it very, very much shares the same philosophy of we want to train one representation to perform a task and another representation to fail at something, or not be able to tell the difference between something and jointly optimize these representations. So that's how that goes. If it works.
00:10:46.670 - 00:11:38.070, Speaker A: Well, you get this notional cartoon here of a representation where you can't separate the domains, but you can still separate the classes, and everything looks good in cartoon form. More concretely, we actually construct two additional losses on top of a traditional classification loss. I mean, if you have labels, we will use them. But all of our experiments that I'm going to talk about today have no labels in the target domain. And so we start off just by training a domain classifier. This domain classifier predicts the domain label. And so we try to learn a model that say, can I actually distinguish the two domains? And in the first iterations, it'll be very easy to, because the default representation that one might have learned will lead to very different projections, say, at the higher levels of a classifier.
00:11:38.070 - 00:12:40.772, Speaker A: And here we're just showing Alexnet for illustration, but it applies to other architectures as well. And then the innovative trick is to say, let's now add a loss called, which we call the domain confusion loss. It's related to other work that I'll cite on the next slide, where we actually penalize or we have a loss, which is essentially the cross entropy of the domain label, the cross entropy of that with the uniform distribution. So if you're fully confused, you're very happy and you take no loss. But if you can actually predict the domain label, you take a loss and you back propagate that through the representation to learn a better representation that actually confuses the domains. And in this earliest version of this work in ICCV 15, we actually had shared weights across the two conv nets, the convnet that you would run on the target domain and the source domain. And as you'll see in the next slide, the most recent work relaxes that assumption.
00:12:40.772 - 00:13:28.184, Speaker A: So this just formalizes this a little bit more. We have two different losses. First we learn this domain classifier loss, and then we define the performance of the domain classifier, and then we define the domain confusion loss, and we iterate between these two in the same style of optimization that Gann models currently do. So we have a most recent result. Actually, this is accepted at CVPR 17, which we call adversarial discriminative domain adaptation, which takes these ideas. And I'll also say that these very, very similar work was developed by Gannon and colleagues. He called the technique gradient reversal.
00:13:28.184 - 00:14:49.012, Speaker A: We had a particular, we had a different and slightly more stable, in our opinion, loss function that we defined in our domain confusion work, but those were definitely cotemporary. Also, there was an interesting model called Cogan that came out recently that just showed if you had two gans with coupled architectures, sorry, the coupled learning but unshared weights. We had previously used shared weights. It actually worked quite well in some cases, didn't work in others, and it used a gann style adversarial loss function. And our most recent work, basically, but it used a pure generative framework. And if you're actually just trying to do classification tasks, which is what we are doing in domain adaptation, you ultimately get better performance with a discriminative loss or a discriminative base model. So our latest work combines all of these techniques into a single framework where we do still have a discriminative front end, essentially, Alexnet, and we have unshared weights and an adversarial loss, where we have a discriminator and a classifier that we train the classifier and then we train the discriminator to not be able to tell the difference, and we have a GaN loss here.
00:14:49.012 - 00:15:03.864, Speaker A: I still think there's some mileage future work that actually happening right now of further integration of GAN models. Some of the results that are coming out in the last months are quite exciting, but this is the published state of the art that we have.
00:15:04.884 - 00:15:12.560, Speaker B: Could you say a little bit about what are the gains from domain adaptation here? So you use less labeled data in.
00:15:12.592 - 00:15:39.172, Speaker A: The new domain, no labeled data. You have no labeled data, zero labels in the new domain. So can you do something with unlabeled, but you assume you have enough unlabeled data. You have unlabeled data in the new domain, zero, zero labeled data in the new domain. So the only thing you can do is enforce the constraint that you can't tell the difference between the domains. So my colleague Alyosha Efros, I mean.
00:15:39.268 - 00:15:49.744, Speaker B: It'S maybe okay between Palo Alto and Boston, but like, otherwise, is that a reasonable thing, that the representation would be exactly the same?
00:15:50.484 - 00:16:26.310, Speaker A: I think it's reasonable. I don't know if I want to make a broad theoretical claim. I think there are theorists who do make claims of those forms. I can't recount them off the top of my head. Mary Armoury and I have a, I'm tomorrow going to the algorithms in the field PI meeting because we have a joint project, and I'll get the latest and greatest of what he's been doing on this. So I think I can give you the intuition. Actually, the intuition of this comes from a poster way back in, I think, 2010, Alyosha Efros and Antonio Toralba had a precursor to some of these ideas.
00:16:26.310 - 00:16:47.674, Speaker A: We had also been doing them independently. But I like the way they expressed the idea. They actually were really focusing on dataset bias. And they were saying, you know, we're sick and tired of training on Caltech 101. Like today, the standard is Ms Coco. If you're running on MS Coco and you're a vision person, you must be good. But it's just as much of an arbitrary data set, in my view, as Caltech 101 was, and before that, whatever.
00:16:47.674 - 00:17:28.732, Speaker A: And so Alyosha and Antonio, who had gotten stuck on an island for a week because of a volcano that prevented them from flying, had a poster that was just the two of them, no students. And it said, if you can play the name the dataset game, there's a problem here. So invision had become this baroque thing of one set of techniques worked on Caltech 101, another set of techniques worked on UIUC faces. So they put pictures up of all. And it wouldn't really work so well if you trained one method on one data set and tried on the other. I mean, some datasets were better than others in this regard, but the punchline and the title was the name the dataset game. They actually had researchers come up and they just showed you pictures from the data set.
00:17:28.732 - 00:17:55.274, Speaker A: Can you tell which data set this is from? And the really good researchers, the ones who got all their papers accepted, could tell you in a heartbeat, oh, that comes from that data set. That comes from that data set. That comes from that data set. That comes from that data set. And their point was, if you can play that game well, you've overfit to these data sets. You've been spending too much time finding the idiosyncratic properties of the data that aren't properties of the world. Right? They're not properties of cars.
00:17:55.274 - 00:18:25.310, Speaker A: They're properties of the images of cars that happen to have been collected that year by that researcher in that campus or something. So that's the same story of, you should play the name this dataset game and lose. That's another way of stating what we call domain confusion. You shouldn't, in your representation that you're going to ultimately make a decision on, be able to classify or detect what domain sample comes from. There was a question in the back.
00:18:25.502 - 00:18:52.106, Speaker C: Is this just to do as well as you possibly can on those datasets? Because so yesterday there was this panel discussion, and I think one of the topics that came up was sort of like, well, you know, you can pre train stuff on Imagenet and then, you know, turn it into, like, you know, change domain cluster, this medical dataset. And sure, you have to fine tune, but it sort of still helps so I don't know, isn't that like a kind of example that maybe, maybe we are learning something that's like, not like overfit to one specific dataset or.
00:18:52.210 - 00:19:24.194, Speaker A: Yeah, I mean, this doesn't say you shouldn't pre train and that pre training or that transfer learning is quite successful in deep representations. They certainly are, but you still can try and minimize the loss you might suffer from domain shift by trying to minimize the representational discrepancy between domains.
00:19:25.934 - 00:19:30.074, Speaker C: How do you say which domain share these common features?
00:19:30.534 - 00:19:31.488, Speaker A: Which domain.
00:19:31.606 - 00:19:35.224, Speaker C: Cars and buses don't share the same preset of features. How do you decide?
00:19:36.164 - 00:19:47.180, Speaker A: We don't do feature selection? That's the underlying deep architecture learns. So far in this story, we're just using straight up convnets for representation.
00:19:47.212 - 00:19:54.108, Speaker C: Learning about the initial images, the source images, I mean, you're imposing a prior, but the domains are related, right?
00:19:54.276 - 00:20:20.964, Speaker A: The tasks are the same. So there's an assumption that it's the same set of tasks in the two domains. There are chairs, there are. So I should pop up and say this is all part of the greater family of multitask learning, or transfer learning, except transfer learning. And multitask learning usually just refers to tasks. But in general we're taking the. And some people say transfer learning is orthogonal to domain adaptation.
00:20:20.964 - 00:20:44.058, Speaker A: Some people say it's all the same. There's a meta story, but you could have a set of tasks and try and transfer to a new set of tasks. Similar techniques would actually be very useful. But here we're saying we have the same tasks in two different places. I'm doing car detection here and I'm doing car detection here, but I might have a completely different camera or even sensing modality in the back.
00:20:44.226 - 00:20:50.210, Speaker D: So sorry to ask you questions about other people's work with this name of the data set. I think it's usually Sam.
00:20:50.242 - 00:20:50.870, Speaker A: Chris.
00:20:51.042 - 00:21:33.634, Speaker D: So sure, if you show me, not me, but presumably you. A picture from Caltech one. If a car from Caltech 101, a card from Imagenet, you'll be able to know which one it is. But if I show you, if I trained in Caltech 101 and I trained in Imagenet and then I classified the car from just. I took a picture on the street here and if I showed you the mistake, here's a picture I took and this is, which was incorrect, would you or other researchers be able to tell whether this is a mistake made by something trained in Caltech? One or one, or something trained on imagenet? Because that really gets, I feel, towards what you're doing here because using different data sets maybe is fine as long as my classifier is the same. But if you can tell mistakes in the classifier, then you really means that.
00:21:33.714 - 00:22:11.834, Speaker A: I would use all of the supervised label. I'm not suggesting to segregate your supervised data at this point. Every supervised label you have from all domains, there are. I think there are. There's probably something more subtle and sophisticated you could do than what I'm about to say. But to a first approximation, it does seem like a good thing to do is to take all of your labels from all of your domains and pool them together and train them jointly. So if you have labels from, if you take all your Caltech data, all your mnist data, whatever, all your data from Mscoko, I would train on it.
00:22:11.834 - 00:22:20.074, Speaker A: I'm not saying to only train on one domain and then test. I'm just saying if I give you a brand new domain with no labels, what do you do?
00:22:21.374 - 00:22:22.910, Speaker D: Okay, we'll talk about the.
00:22:23.022 - 00:22:36.034, Speaker B: So actually, yeah, so think more about, I mean, it's not clear that. So let's say that two data sets, right? There's a lot more buses than this and a lot more cars than this, right? You'd be able to sell them apart, right?
00:22:37.254 - 00:22:38.998, Speaker A: That's right. There's both the.
00:22:39.046 - 00:22:43.638, Speaker B: So somehow the data sets have to be really aligned for this to work.
00:22:43.806 - 00:22:53.874, Speaker A: The underlying representation that it performs, the classification in, has to be aligned. This is adding the constraint of domain confusion to representation learning.
00:22:55.974 - 00:23:00.110, Speaker B: But if there are like different number of cars and buses and cats, that's.
00:23:00.142 - 00:23:04.914, Speaker A: One of the aspects of data. You know, class imbalance is one of the aspects of data set shift.
00:23:05.964 - 00:23:08.344, Speaker B: And then what would this do?
00:23:08.804 - 00:23:27.220, Speaker A: It would learn representations that are less sensitive to that. Although this model doesn't specifically have a component that deals with class imbalance, this more focuses on appearance, distribution shift, or representation shift.
00:23:27.372 - 00:23:34.134, Speaker B: You see, the class imbalance may not even be in the labels in the background and so on. So it seems very strong.
00:23:34.514 - 00:24:30.394, Speaker A: Yeah, well, and I didn't, I think this is, I'm just telling you where we are on the research trajectory. I'm not sure, I'm sure that this isn't done, and there's no good theory here, so please create it for me so I can work with it later. We don't even have predictive models of deep architectures to a first approximation, so that would be a nice thing. Um, so we would normally just, uh, indeed pre train on the source data, then adapt using the methods that I've just described, uh, based on unlabeled target images. And then we would be testing the target data using the target adapted target architecture and see how well it does. And this works well, certainly for classic problems like with, which might have small, uh, domain shifts like going from MNIst to USP's digits. And so you can kind of see what the performance would be if you did nothing.
00:24:30.394 - 00:25:36.834, Speaker A: Just run the classifier that you train on MNIST and test it on the different domain, and you can see how the performance of different techniques improve, including our technique. Actually, the Kogan model works really well on very small domain shifts, larger domain shifts, or even a modest domain shift like going from street view house numbers to mnist. We train on this, but you test on this. That's where things start to not be completely trivial. And the Kogan method had trouble converging in our experiments. We're interested, as I said, in broad views on this domain adaptation problem where you don't necessarily even have the same dimensionality of a feature space in the different domains, you might even have completely different modalities. So we're excited even about the problem statement where you train on one sensor and test on another, which seems a little crazy, but it actually speaks to the robustness of these condnets that I can train classes just on intensity images and then just take the raw signal from a depth sensor, maybe do a little bit of a hack to make it three dimensional.
00:25:36.834 - 00:26:21.244, Speaker A: And there's a baseline level of performance which is called source only, which means I've trained on intensity imagery and I test on a completely different modality. But there's enough similarity in what the convnet learns that you actually get non chance performance. So Oracle performance training the convnet on the depth image would get you 46%. This is probably map or 0.46 map. And if you just ran the source classifier on the target data, you'd get 0.13. And so we kind of nearly double the performance with this just very, very, either weak or overly strong assumption of domain confusion.
00:26:21.244 - 00:26:57.634, Speaker A: There's still a lot of work to be done to achieve oracle or human level performance on this data. I can probably learn what chairs look like and then figure out which of the chairs in the depth image with a high degree of accuracy. So that's the first slightly more theoretical or generic version of the problem statement. And I have 15 minutes left. Okay, good. So I'm going to mention how we apply this in autonomous driving. Then I'm going to talk about end to end autonomous driving, and then I'm going to hopefully save ten minutes for language and vision, because especially in light of the previous talk, that'll be fun.
00:26:57.634 - 00:27:35.460, Speaker A: I mean, this domain adaptation story originated from language, by the way, the original papers on domain adaptation came from that. So there's also a language and vision adaptation line of work that I think is interesting future work. So autonomous driving is very exciting. Lots of people are throwing more money at it than they have since. And so it's always good to get underneath that whenever you can. And there's sort of two big philosophies of how people think computer vision should be used in autonomous driving. It's sort of like whether you've taken the blue pill or not.
00:27:35.460 - 00:28:45.178, Speaker A: So the classic view is vision is to sense the affordances of driving to find the cars, the curbs, the road surfaces, so on and so forth. And we do state estimation for you, and then you're going to have a good controller that you may be right with some rules or do some magic from control theory. And that's great, except unless you've taken the blue pill and you really believe in deep learning right now and you want to give up, just as you gave up all of the things that you've given up in computer vision, thinking that, well, I must have a level of intermediate parts that I understand in between my pixels and my higher level object like representations. I really don't believe I should have an arbitrary bottleneck that's manually specified or written as a set of rules. No matter how great an introspective AI agent or real intelligent agent I am, I'd like to learn this whole thing end to end. Even the representation of the bottleneck between state estimation and control. And whichever you care about, I'm happy, right? I actually believe in both of them.
00:28:45.178 - 00:29:32.134, Speaker A: I believe the real answer is you have to do both at the same time. But certainly I'm happy to be a traditional vision person and just say this deep learning revolution is in service of state estimation for control. But as I said in the talk so far, we need to solve the problems of how to make these state estimators have predictive uncertainty robust, to be able to train from few shot examples and be robust to new environments. And if I suddenly drive into a new city, I should be able to do better than I would if I. If I just use a naive model. And we are also believers in this line of research as well. And so I'll show you what we've been doing there.
00:29:32.134 - 00:30:01.354, Speaker A: So, first, let me just say we, of course, taken the techniques that I've just described and applied them to these state of the art driving affordance models, which are fully convolutional networks. We have two great data sets. We have a consortium at Berkeley called Berkeley Deep Drive, which has about a dozen companies, and some of them provide data. This is data. We have data from Mapillary and a company called Nexar. I'm a consultant for Nexar. Full disclosure on that.
00:30:01.354 - 00:30:40.754, Speaker A: So, part of this story is having enormous data sets, both for training things and labeling them, but also showing these domain adaptation stories. How many people already know what an FCN is envision, or know what cityscapes is? Okay, wow. I haven't given a talk where I get to do this in a while, and so I should have had the precursor slide. But everyone knows what Alexnet is. Everyone knows what deep learning is. Everyone knows what a convnet is, and Alexnet. Okay, so Alexnet and Convnet would give you a label for the entire image, right? If I want to say this is a toaster or this is a bus.
00:30:40.754 - 00:31:39.604, Speaker A: And in the last few years, including work that came out of my lab at Berkeley, similar to other work that in the past, many, many years ago by Jan and others, we said, let's do that at every pixel. So we call that a fully convolutional network. So we're not just labeling the whole image, we're labeling each pixel in the image, which you'd think is prohibitively complicated, but it turns out not to be. It's only, like, twice or three times the computation of putting a label on the whole image to have architectures that actually label every single pixel in the image. So the folks at MPI and at Domlar got very excited about this for work in autonomous driving, and they had defined a challenge called cityscapes, where they. I think this is ground truth, and this is output, where they labeled every pixel in the scene. With category labels, it's a fixed set of categories.
00:31:39.604 - 00:32:10.914, Speaker A: They think it's relevant to driving. So blue pixels are cars, dark red are bicycles, and there's pedestrians and lights and whatever. Actually, this is probably not ground. This is probably output, because there's some errors in there. But the fact that I couldn't tell shows you these methods when trained and tested on the same data set, actually remarkable. And the Daimler folks were blown away like envision. In general, everything got twice as good when you started using well designed, fully supervised, deep learning architectures.
00:32:10.914 - 00:32:47.766, Speaker A: So that's what cityscapes is. Everyone understand the problem statement and the architecture that. So if you take Alexnet and replicate it at every pixel. That's essentially what an FCN is, except you can have some architectural things that make that more efficient, which, of course, one does. But when you then just take one of those data sets that I mentioned, we're collecting around San Francisco and elsewhere, we might call it the San Francisco dash cam data set. Instead of these Daimler images that were collected in the cityscapes data set, we just have some new images which I think should be labeled. Your performance can be pretty poor.
00:32:47.766 - 00:33:25.340, Speaker A: Like, this looks really beautiful. Like, so beautiful. I would stop working on the problem, but if you just take the model that you trained on this data set and tested it on this new image, you get all this crap here. You've got like a car flying up in the tree and you've got lots of really weird mess in the front of the image. Actually, this is very confusing. Unless you go back and look at the data set, I hope you can see, every single image in the cityscapes data set has a Daimler logo right there, because it's the same car that collected the entire data set. And so there's a hood and there's a logo, there's a emblem right there.
00:33:25.340 - 00:34:05.068, Speaker A: They were smart enough to manually mark out that from the loss and exclude that from the loss function. But obviously the network you learned still is a little messed up down there. The kind of thing that it's hard to model ahead of time, or there might be issues like that. And you'd like to have methods that are invariant to these kinds of domain shift. So you could certainly play the set name, the dataset game and know when you're looking at a cityscape's image here just by looking for that logo. So what we do, of course, as an aside, it's also very surprising what happens when you put a picture of a tunnel into that cityscapes model. There's no tunnel in the training data set.
00:34:05.068 - 00:34:29.096, Speaker A: And so this is just a placeholder for me to remember every time I give this talk. I'm just very frustrated. I don't have results that show deep learning with predictive uncertainty. The model shouldn't be outputting such a bad stuff here. Not only should we adapt to the new domain, we should know that we have a don't care mask here. Yeah. Question.
00:34:29.096 - 00:34:41.016, Speaker A: Five minutes. Five minutes. You told me. Oh, boy. Well, so this works on fully convolutional networks. Domain adaptation. You might train on one season, test on another.
00:34:41.016 - 00:34:56.704, Speaker A: This is simulated data. You'd like to see this ground truth. This is source only this is what we get. It's really not perfect. It does work on some real images. It goes from errors like this to errors like that. More of that, more of that, more of that.
00:34:56.704 - 00:35:19.180, Speaker A: And you can see that on the web. Then the blue pill is let's not learn driving affordances. Let's learn end to end driving, which is wonderful. It's not a new idea. It goes back to the same era of convnets. This is the Alvin project. Then Jan and colleagues had RC cars that they were driving around.
00:35:19.180 - 00:35:41.554, Speaker A: And this has been reinvented by Nvidia. Quite significant, successfully. And at Berkeley we have RC cars that learn. So this is learning from demonstration. It's just behavioral cloning. You give it examples of how to predict a steering angle from a certain video and it can then do that. There's lots of interesting issues here I could describe.
00:35:41.554 - 00:36:20.508, Speaker A: This is Carl Zipzer's work, training them independently, but they still managed to not hit each other. And what we've been doing is that was all great, but it's like you learn on a single vehicle and test on a single vehicle. And I'm not really going to be excited about end to end learning unless it's learned from human lifetimes of driving. And you can't do that on a single vehicle very easily unless, I mean, Google's been doing it. But you can do it if you can crowdsource. And as I said, we've been crowdsourcing these data sets from a variety of different providers. So the key idea is really just to defines self driving as ego motion prediction.
00:36:20.508 - 00:36:59.296, Speaker A: I'm not going to predict the actual steering angle of a vehicle. I'm going to predict the trajectory angle of the vehicle or even the future trajectory over a short time window of the vehicle. And once we do that, we can now just go ahead and innovate a little bit with adding a recurrence to a fully convolutional model, do a little bit of and play both games at the same time. A privileged learning story. I'm going to have a loss on the future ego motion and a loss on segmentation on affordance labels or semantic segmentation if you give it to me. So we'll play both sides of that story. The really great thing is the diversity of our data set.
00:36:59.296 - 00:37:18.802, Speaker A: These are the videos that we have. We can also do slam here. I won't describe this, it's more of a computer vision hack. Turns out off the shelf methods for slam all failed on that data. We did semantic filtering to make it work. And I'll show you a video rather than these slides. So here we're showing videos where we predict.
00:37:18.802 - 00:37:44.606, Speaker A: We're showing a video where we're not doing actual autonomous driving. I guess I should be clear on that. I need to go over here and skip to about halfway. This is still in the same kind of spirit as a language model. Language model. You train on a bunch of strings and you see your perplexity on or predict and see your perplexity on the held out data. We're doing the same thing for driving.
00:37:44.606 - 00:38:24.314, Speaker A: We're taking held out sequences from this dash cam data set and seeing if we can predict what humans would do. The red is the trajectory angle from the human, and the green is what our model predicts the human's about to do. And so it's pretty reasonable. And whether or not this successfully drives a car straight out of the box or we need to do some tricks, it's still clearly exciting progress. In this case, whenever you have multimodal prediction, shouldn't be conditioned on the intention of the driver, right? This is not conditioned. Yeah, this is like a character model in a language model. We're just saying what next character could appear.
00:38:24.314 - 00:39:13.522, Speaker A: It doesn't actually have a model of what the human's thinking and typing, but indeed, this would only be really useful if you actually combined it with a route planner or something. All right, well, so I want to have a few moments to talk, so I'm going to take five minutes. Okay. So there's a lot to go I could go through here at the highest level I'm interested in. Half my group is working on this, the other half is working on this. And we're interested in the fusion of language and vision, and in particular, how we can get explosive, explainable models and models that exhibit compositionality and learn to reason about that composition, reason to form compositional structures, or compositional structures to answer and explain questions. And so a lot of.
00:39:13.522 - 00:39:33.046, Speaker A: We're very interested in explainable AI. Now, we have a new DARPA project on this, and we've been already working on this for several years. Notionally, I just want to. We've been doing it in the space of visual explanations. We've been working, as others have been as well, on going from images to captions. So that's what you might call image descriptions. Tell me some words.
00:39:33.046 - 00:40:20.578, Speaker A: Tell me a story about an image. That's image descriptions. And it turns out, for those of you who aren't familiar, if you just take one of these visual cnns and take even a generic recurrent language model. It was remarkable that you could jointly train these things and you would get something, something much better than any of the previous methods we've had for such tasks. And there's a whole line of work pushing on image descriptions. We're interested in explanations that are not just text conditioned on an image, but also text conditioned on an image and a task. So I would like to have a story about why I just did something, maybe either in bird classification or medical image classification, or even driving if the car just decided to turn left.
00:40:20.578 - 00:41:00.824, Speaker A: I'd like it to tell me why. And there's these two broad approaches we're exploring, and others as well. One, we call it implicit, where I just treat the explanation as a rationale that's provided as supervised data. When I ask somebody to label an image, they also tell me their story about why they labeled that image. And I train a language model to similarly predict, given a black box that's actually doing a classification task, given that image and a classification decision, let's tell a story or predict what story a human would have told given that image and that thing. And we have a previous paper on that. I'm not going to have time to go through it.
00:41:00.824 - 00:41:33.216, Speaker A: Related is visual question answering. How many people already know about visual question answering? Good. So instead of just predicting a label, I actually have an image and a question and I have to give an answer. We had used an attentive model to do very well on this task last year. And the attentive, the attention. I'm just going to focus on the attention part of this model right now because it's sort of beginning to get at explanations. It's what was the model looking at when it made the answer, when it gave the answer.
00:41:33.216 - 00:42:14.368, Speaker A: So here it's going to be the same image for three questions. What is the woman feeding the giraffe? I'm kind of more excited about the fact that it's attending to the right thing as much as getting the right answer and it changes its attention based on the question, what is her shirt? It looks over here. What is the hairstyle? It looks over here. Just remarkable that a real simple architecture could give you this kind of intuitive performance. It makes a mistake, and you can see that the mistake is partially due to the lack of spatial resolution in the architecture. I'm going to skip ahead. And so now we have models that not only generate text for the explanation, but they generate this kind of attention.
00:42:14.368 - 00:42:47.540, Speaker A: What is the person doing? Skiing. And it was looking here to answer skiing. And it can also generate text that's conditioned on both the image and the answer. So you get different explanation text for different images and different attention models visualizing what the model was doing, doing. And I'm going to just have to skip ahead of this. I thought I had ten more minutes than yesterday, but nope. The other main major theme of explainable AI for us is what we call explicit, which is actually trying to unpack what the specific model was doing.
00:42:47.540 - 00:44:09.464, Speaker A: And to that I'm going to point to work that we're doing, and led by really Jacob Andreas, who's working with a bunch of us here at Berkeley on neural modular networks, where we don't have a single homogeneous network to solve a task. We have a model inspired from language and from vision, that's explicitly compositional, to set up a set of network modules to process an image and answer a question. And if you can do that, if you know that to answer the question, is the bus empty? You should really create a little parse tree and a little set of routines that say, look for the people, look for the bus, overlap them, and see if there's anything there. That would be great. This came originally from work that Jacob and I and Dan Klein did on compiling using parsers neural modular networks to answer specific compositional questions. I'll refer you to the previous work at ACL and CVPR if you're interested in that. And the idea is that a neural modular network basically takes a sentence, compiles it into a parse tree, creates a set of neural modules corresponding to this semantic parse ground that we can then jointly train across the data set and ground the meaning of red and all the other terms in the vocabulary.
00:44:09.464 - 00:44:46.322, Speaker A: And this has been pretty exciting. I've just got two more minutes, so I'm going to say the real state of the art here is a new data set that Facebook and Stanford put out called clay clever, where it really stretches the state of the art for. And, Chris, you're on this. No, no. It pushes the state of the art of this compositionality. We're very excited about it. So our latest work shows that you can now take these neural modular networks, solve the clever kinds of questions, and importantly, learn in an end to end fashion, the parsing.
00:44:46.322 - 00:45:20.514, Speaker A: So we're no longer using a sort of side parser to generate these neural modular plans. We have an end to end model that essentially learns to reason if we're going to over glorify it. It learns a layout policy and then grounds the layout policy to answer these questions. It works really well on these clever problems. It works as well as previous methods on regular VQA data. The regular VQA data doesn't have that much compositionality in it. It turns out that's why the clever people created this clever data set.
00:45:20.514 - 00:46:10.994, Speaker A: So it. Let me see if I can just show one example here. So here's an example of how many other things are of the same size as the green metal ball, the green matte ball. And it first grounds each of the terms here, saying, or it compiles a plan that says, first find something, then relocate something and then count it. And it grounds that the thing it should find is the green matte ball. And the relocate has the extra parameter of size and then count is grounded on the word thing. So there's an attention map over the words for each of the functional units, and the images flow through the functional units.
00:46:10.994 - 00:46:52.836, Speaker A: And here's an example reasoning, an example visual routine that this method learns to output for. Does the blue cylinder have the same material as the big block on the right of the red metallic thing? And it grounds like first it knows that it should find something blue cylinder and then it should find a big block and then it should relocate that, and then it should compare them. And it does in fact get the right answer in these examples. So we have pretty good performance on this data given some baselines. And this is very new work. And hats off to Stanford for creating this and Facebook for creating this dataset. That's it.
00:46:52.836 - 00:46:53.124, Speaker A: Thank you.
