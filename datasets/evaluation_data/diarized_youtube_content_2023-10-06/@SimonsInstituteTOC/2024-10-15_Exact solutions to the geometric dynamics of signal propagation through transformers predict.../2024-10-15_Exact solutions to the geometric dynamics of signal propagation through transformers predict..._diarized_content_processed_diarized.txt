00:00:02.320 - 00:00:33.275, Speaker A: All right, welcome back, everyone. So for the last speaker of the morning session, we're very happy to have Surya Ganguly from the. From Stanford. So he sits in this kind of exciting intersection between neuroscience, applied physics, computer science. And he's worked on a whole lot of kind of questions around sort of getting better controlled scientific understanding of various types of phenomena in deep learning anywhere from things like scaling, law, training, dynamics, and so on and so forth. And today he'll tell us about how to kind of predict the trainability of transformers.
00:00:34.535 - 00:00:56.589, Speaker B: Okay, thanks. Yeah, it's always great to come back to Berkeley. I did my PhD here. I had a lot of fun here. Okay, so, yeah, I'm going to talk about. Since this is a Transformers workshop, I'd figure I'd talk about transformers. So I'm going to talk about this one paper and background material for this paper called Geometric Dynamics of Signal Propagation Predict Trainability of Transformers.
00:00:56.589 - 00:01:30.249, Speaker B: What's interesting about this paper is none of these people are machine learning people. They're actually quantum information or string theory. And I managed to corrupt them just as I got corrupted back in the day. From string theory here at Berkeley to neuroscience and ML. Okay, so just at a very high level, what I'm going to talk about. So the transformers are complicated architecture where you have normalization and then an attention layer, the normalization and MLP layer. You also have residual connections and so forth.
00:01:30.249 - 00:02:30.499, Speaker B: A bunch of tokens propagate from the input to the output of this layer. And before you train the system, you have to initialize it, right? So there's many initialization hyperparameters, for example, the strength of the attention path versus its residual path. So these two parameters, the strength of the MLP path versus its residual path, the variance of the attention weights, the variance of the MLP weights, and so forth. The question I like to ask is how should we choose the practical version of. The question I like to ask is how do we choose these initialization parameters at the beginning of training to enhance performance at the end of training? But at the same time, there's a scientific question, like transformers, again, are a complicated architecture and can we understand anything about them? Now, once the transformers trained, it's extremely complicated because the data has seeped into the transformer in ways that we don't fully understand. But at initialization, you have a random transformer. And so you might ask just a very basic math or science question, how does a random transformer behave? Right.
00:02:30.499 - 00:03:18.315, Speaker B: And of course, this is relevant at initialization. In particular, how do signals or information flow through a random transformer. And we answer this question by deriving an asymptotically exact dynamic mean field theory. And our theory actually matches experiments on actual actual transformers. And again at a very high level, summarizing what I'm going to talk about, we find two different order chaos phase transitions in the signal propagation through the transformer. One for the forward propagation of tokens as they propagate through many transformer layers and one for gradient back propagation. And then we find that the final performance, we show that we can actually predict the final performance in a trained transformer as a function of how far one initializes from these two phase boundaries.
00:03:18.315 - 00:03:52.305, Speaker B: And we find that good performance only occurs if and only if initialization is at the intersection of both edges of chaos, the edge between order and chaos, which we'll call the edge of chaos. Okay, that's a super high level description of what I'm going to talk about. So just outline going down a little bit lower level of granularity. This is the sequence of things that I'll derive for you. So of course the transformer has two components, the attention and the mlp. So we're going to start with the mlp. And this is based on older work that I did a while ago.
00:03:52.305 - 00:04:29.803, Speaker B: Just asking, how do signals propagate through MLPs? Right. So we can characterize this by thinking about how two inputs propagate through an mlp. So you put in two inputs, you can think of them as two points in some N dimensional space, where N is the dimensionality of the input space and they'll propagate through. Right. You can think of them as two particles that propagate through the system. And it turns out there's two phases of the propagation. If the variance of the initial weights are small, then the small linear weights will contract the space and the two particles will attract each other and they'll merge.
00:04:29.803 - 00:05:06.565, Speaker B: And that's I call the ordered phase. On the other hand, if there's large weight variance and the linear weights will expand, expand space and the two particles get further apart from each other, but the non linearity will fold space, you get expansion, folding, expansion, folding, and that's in different dimensions. And that's the hallmark of chaos sensitivity to initial conditions. And the nearby particles will repel each other and they'll expand away from each other. Gradient propagation also has two phases. A phase where gradient back propagate gradients either vanish or explode. The phase boundary for the back propagation is exactly the same as the phase boundary for the forward propagation.
00:05:06.565 - 00:05:59.545, Speaker B: We can also show that if the closer you initialize to the edge of chaos, the deeper an MLP you can train. Then we're going to move on to the recent work with this as background. The recent work is conceptually similar, just mathematically more complicated because you have to deal with attention. Here we ask how do N tokens or particles propagate forward through attention and the mlp? And we find again an ordered and chaotic phase for the forward propagation where now the nparticles collapse to a straight line. That's called rank collapse, which has been observed experimentally. Then we also have a chaotic phase where the N particles are repulsive and they actually expand until they become the vertices of a regular N simplex. These regular N simplexes naturally emerge through propagation through random transformers.
00:05:59.545 - 00:06:41.285, Speaker B: And I'll show you how that happens. For gradient back propagation we also get two phases vanishing and exploding, but they have a different phase boundary in hyper parameter space than the forward propagation phase boundary. So then for deep transformers to train, well, you must be at the intersection of these two edges of chaos. Okay, so I basically just told you all the take home messages for this talk, but now I'll explain to you why these take home messages are true. Okay, so, and I should say as we're doing our work, we came across these two papers that are really very elegant. Philippe I think, talked about some of this stuff. So joint work from Philippe and Uri & Co.
00:06:41.285 - 00:07:26.861, Speaker B: Where they also they think about these end tokens as a system of interacting particles propagating through iterated self attention layers. And they have some beautiful mathematical analysis proving various theorems about this propagation. We're physicists, so we are less rigorous, but we come up with asymptotically exact equations that match numerical experiments quantitatively using our usual physics, intuition, dynamic mean field theory and techniques like that. And our results are consistent with theirs in the situations where the results overlap. But we can handle the mlp. So we can add the MLP back in and not just look at iterated self attention. And it turns out the MLP is essential in preventing collapse.
00:07:26.861 - 00:07:52.551, Speaker B: All right, okay, so let's start with the mlp. And I actually spent the bulk of my talk on the mlp, again because the math is easier and it's so conceptually similar to the transformer. And then we'll talk about the transformer at the end. Okay, so this is old work from this paper with a great set of colleagues. And so, okay, so here's the basic idea. Let's consider a random mlp. So it's Just the classic vanilla mlp.
00:07:52.551 - 00:08:54.675, Speaker B: And we'll imagine that the weights are randomly initialized so that the var, so that the variance of the weights is some constant divided by the number of presynaptic inputs, right? And then the biases are also random in order one. And these are Gaussian distributions that we normalize the variance by one over the number of inputs. So the total input current, when you sum over all the synapses, is order 1. As the width becomes large, the weights and biases are acting on an equal fashion to control the activity of the postsynaptic neuron. The basic questions we'll ask is how do signals propagate through the system? And to sharpen that question, we'll ask, if you put in a single input, does its norm or length grow or shrink, and how fast? Then, if you have a pair of inputs, do they become more similar or different? And also a smooth manifold, we can handle that too. How does its curvature and volume change? The reason we looked at smooth manifolds is because we're interested in the expressivity of random deep networks. And I'll talk about that in a little bit at that time.
00:08:54.675 - 00:09:44.633, Speaker B: Okay, so here's the basic idea. So imagine that this is the activations that feeds through a nonlinearity, feeds through the weights, and then feeds through the output. We can write down what's called an order parameter, which is just the length of this vector, okay? Because everything's random, its length will concentrate about a deterministic value as the width goes to infinity. And we can write down a recursion relation for this deterministic value as it propagates from one layer to the next. So, for example, if ql minus 1 is the length of the input in layer L minus 1, we can write down, we can derive this recursion relation that transforms QL minus 1 into QL. And roughly what this is doing is it's integrating over the Gaussian distribution of weights, multiplying by the variance of the weights and adding in the biases.
00:09:44.769 - 00:09:45.485, Speaker C: Okay?
00:09:46.505 - 00:10:03.965, Speaker B: Anyway, this is a. And this is a. Sorry. This DZ is just an integral over a standard normal variable. So a random variable 0, a Gaussian random variable with 0 mean in unit variance. Okay? So this is a perfectly simple deterministic recursion relation. We can plot the output as a function of the input.
00:10:03.965 - 00:10:05.871, Speaker B: And this is what we get.
00:10:06.023 - 00:10:06.795, Speaker C: Okay?
00:10:07.895 - 00:10:23.595, Speaker B: So this is QL as a function of QL minus 1. And so this is an iterative map, right? And this curve is shown for three different values of the weights Right. The variance of the weights.
00:10:24.175 - 00:10:24.919, Speaker C: Right.
00:10:25.087 - 00:10:49.957, Speaker B: And you can see that basically it's an iterative map where essentially it goes to a fixed point. So, for example, if you start, let's say the variance of the weights is four, or standard deviation is four, you start here, so you go up here, up here, up here, up here, and then you end up at this fixed point. So essentially the length just grows and stabilizes to a fixed point.
00:10:50.061 - 00:10:50.785, Speaker C: Okay.
00:10:52.765 - 00:11:12.245, Speaker B: And that's actually what happens in numerical experiments. So this is theory and simulation, and they all match up. So that's great. Yeah. So no matter what your initial input is, it'll stabilize to a certain sphere, and the radius of that sphere grows as the variance of the weights grows.
00:11:13.025 - 00:11:13.845, Speaker C: Okay.
00:11:14.145 - 00:11:35.863, Speaker B: All right. Now, more interestingly, let's think about the geometry of two points as they go through the network. We can write the geometry of two points can be characterized by a two by two matrix of inner products. Right? The diagonal elements are the length of the norm, and we've covered that already. So we're interested in the off diagonal element. Right. Normalized by the norms.
00:11:35.863 - 00:11:39.647, Speaker B: Right. So this is the cosine angle between the two points, essentially.
00:11:39.831 - 00:11:40.555, Speaker C: Right.
00:11:42.855 - 00:12:30.705, Speaker B: And so how does the cosine angle between two points change as it propagates through the network? We can also write down a recursion relation for that, and it also depends on the length. So if I know the geometry of the two point of the two points in terms of their cosine angle and their individual lengths in the previous layer, we can compute a recursion relation for what the cosine angle will look like in the next layer. Right. And we can analyze that recursion relation. And now this recursion relation is quite interesting. It shows a phase transition as a function of the, say, for example, the standard deviation of the weights at a fixed standard deviation of the biases. Okay? And you can see the phase transition as follows.
00:12:30.705 - 00:12:38.173, Speaker B: Right. Imagine that the standard deviation of the weights are small. So that's this blue curve.
00:12:38.349 - 00:12:39.105, Speaker C: All right?
00:12:39.805 - 00:12:44.645, Speaker B: Then this recursion relation has a single fixed point at a cosine angle of one.
00:12:44.805 - 00:12:45.469, Speaker C: Okay?
00:12:45.597 - 00:12:56.817, Speaker B: So, for example, if you're at some non zero cosine angle, you start here, you go up, you go back to unity, you go up, you go back to, and you trace that recursion relation and you go to this fixed point.
00:12:56.961 - 00:12:57.725, Speaker C: Okay?
00:12:58.985 - 00:13:47.111, Speaker B: But then as you start increasing the variance of the weights, you'll find that this iterative map falls below the unity line, which means, but it still intersects the point. But now this Fixed point of the recursion relation is now unstable. So for example, if you start at a high value of the cosine angle, you'll have to drop and then go left, drop and go left and you get driven down to a small value. So that's basically saying as the weights become larger, the two points, if you start similar to each other, they'll diverge from each other. And that's how you see that mathematically here. And now you can see the dynamics of the cosine angle as it propagates, as the two points propagate through the layers. The solid curves are theory, the dots are experiment.
00:13:47.111 - 00:14:35.931, Speaker B: And you see the phase transition, right? In the stable case or the ordered phase, everything gets driven up to one. In the chaotic phase, things get driven down to some non unity value, the cosine angle gets driven down to some non unity value and it happens relatively fast within about 10 layers or so, right? We can characterize this phase transition by the slope of this line at right. This is the, this is the linearization of the, of the iterative map here. And basically you could think of this as a stretch factor, a local stretch factor. If the stretch factor chi 1 is less than 1, nearby points come close together. If the stretch factor is greater than 1, nearby points are driven apart. That's for a discrete time dynamical system.
00:14:35.931 - 00:15:34.157, Speaker B: If you write chi 1 as e to the lambda something, then lambda is like a Lyapunov exponent in a continuous time dynamical system where positive Lyapunov exponents indicate chaos and negative Lyapunov exponents indicate order. So now how does Chi1 behave as a joint function of the hyperparameters? So here's the standard deviation of the weights, here's the standard deviation of the biases, and there's the phase boundary where chi 1 equals 1, right? So here the stretch factor is neither stretching nor shrinking. Chi 1 is sort of one. So it's an isometric propagation. And so here you have the ordered phase where things are contracting to things are contracting. And here you have the expanding phase where things are chaotic. And again the bias tends to order the inputs independent of what the inputs are, right? Two inputs coming in.
00:15:34.157 - 00:15:46.077, Speaker B: If there's a bias, both inputs will go closer to the bias. So that's why the larger the bias, the larger the variance of the weights needs to be to transition from the ordered phase to the chaotic phase.
00:15:46.181 - 00:15:46.701, Speaker C: Okay?
00:15:46.813 - 00:15:52.597, Speaker B: That's why this phase boundary has the shape that it does in this phase plane of hyperparameters.
00:15:52.701 - 00:15:53.385, Speaker C: Okay?
00:15:54.005 - 00:16:31.313, Speaker B: So Again, just a large sigma W relative to sigma B. You're in the chaotic phase, correlations go to zero. There's actually an intermediate phase where the correlations go to a finite value and then there's an ordered phase where the correlations go to one. Okay, so that's what's going on in the system. Now, to have some fun and also to get our expressivity result, which we're interested in at the time, we asked, like, how does a smooth manifold now propagate through a random mlp? So we can characterize again, a smooth manifold by the matrix of inner products between all pairs of points on the manifold. So imagine a simple circle. This has this geometry.
00:16:31.313 - 00:17:15.315, Speaker B: Or we could compute its autocorrelation function. How similar are two points on the circle when they're separated by an angular separation delta theta? And we can write down recursion relations for these correlation functions. And we can see that in the chaotic phase, in the chaotic phase, if you have a certain wide autocorrelation, the autocorrelation sharpens and sharpens and sharpens as it propagates through the system. And you can see that, for example, from Principal components analysis, where a very smooth circle becomes more and more tangled as it propagates through the system in the chaotic phase. You can also see a kind of a fun thing in a T sne plot. This is a T sne plot of the circle as it propagates through multiple layers. And you can just see you get more and more wiggles.
00:17:15.315 - 00:17:58.515, Speaker B: You can ask, is the number of wiggles exponential in the depth? And the answer is yes, we can formalize that using Riemannian geometry. I'll skip the details of this. We were quite interested in this at the time, but it's not as relevant for this talk. But basically, we can characterize what's called the Grassmannian length of the circle as it propagates through the network. The Grassmannian length you can roughly think of as the number of turns the circle makes in a metric induced by the ambient metric in the embedding space of the neural network. We can show that the Grassmannian length grows exponentially with depth. And we derive recursion relations for the curvature and Grassmannian length.
00:17:58.515 - 00:18:45.855, Speaker B: And we can our recursion relations match numerics. So, for example, the solid curve is our theory, the dots are experiments. And so basically, unlike linear expansion, deep neural signal propagation can exponentially expand the length. It doesn't change the Gaussian curvature of the circle. And you get this exponential growth of grassmannian length. And the circle will eventually become space filling as it winds around at a concentrate of curvature to explore many dimensions. Okay, why did we go down that rabbit hole? What we're able to show is that we proved a theorem that said that if you had a shallow network as opposed to a deep network, we showed that the Grassmannian length of a circle in a shallow network could never grow faster than the width of the shallow network.
00:18:45.855 - 00:19:08.245, Speaker B: But the Grassmannian ranks in a random deep network could grow exponentially with the depth, which automatically showed that shallow networks couldn't efficiently approximate any random deep network in the chaotic phase unless it were exponentially larger than the deep network.
00:19:08.435 - 00:19:09.025, Speaker C: Right.
00:19:09.145 - 00:19:31.929, Speaker B: So that. So that was a way of getting a very simple expressivity result. Okay, but coming back from that, let me skip the summary. Let's go to. Okay, now, what are the practical implications of understanding the signal propagation? Oh, yeah. Oh, in the limit of infinite width. Yeah.
00:19:31.929 - 00:20:10.375, Speaker B: And the simulations that we did were like, at width 100, it's a usual stat mech thing. N equals infinity is as good as n equals 100. Yeah. So, okay, now what are the practical implications of this? So we can show. We showed in a later paper that the closer you initialize to the edge of chaos, the deeper a network you can train. So what you're looking at here is a heat map of deep neural Networks trained on CIFAR 10. This is the variance of the weights, Right? So large variance is chaos.
00:20:10.375 - 00:20:19.315, Speaker B: Small variance is ordered, a finite variance, depending on what the biases are. But a finite variance is the critical. Is the critical variance.
00:20:19.435 - 00:20:20.175, Speaker C: Okay.
00:20:20.595 - 00:21:21.343, Speaker B: This vertical axis is the depth of the network that we're training. The heat map is the final performance, with red being good performance and black being poor performance. This is like classification accuracy in CIFAR 10. And you can see that the closer you are to the edge of, the closer you initialize to the edge of chaos, the deeper you can train the network while still achieving good performance. If you have a very deep network and you initialize away from the edge of chaos, you cannot get good performance. My colleagues at Google actually extended this to convolutional networks, and they had a really nice result where they showed how to train 10,000 layer vanilla convolutional neural networks without any tricks. Just a really good initialization with no tricks like residual connection or batch norm or layer norm or any of the usual tricks that were required to get deep neural networks to train just by carefully thinking about the initialization.
00:21:21.343 - 00:22:57.169, Speaker B: And also, Andrew nicely showed that for these structured weight matrices, it's also really important to initialize them correctly. So there might be interesting things to do about signal propagation through structured matrices. Okay, so, okay, so now let's go on to transformers, right? So this is this paper. So we've kind of covered this MLP business, right? Now we want to embed it in this larger system of attention and residual connections and all that stuff, right? So now the question we'll ask mirroring what we did in the MLPs is how do N tokens propagate through deep transformer networks at initialization where the key query value and MLP weights are random, right? Okay, so now the problem is that you have the attention layer, right? And in the attention layer, N tokens are coming in and they all interact with each other, right? You can't just isolate the signal propagation to a pair of points. You really have to think about interactions between all N. Choose two interactions between n tokens, right? Okay, so the methods of dynamic mean field theory are to take a high dimensional stochastic dynamical system and try to derive order parameters that are low dimensional and write down recursion relations for those order parameters. So if we have N, choose two angles that we need to keep track of, that's going to be hard to do.
00:22:57.169 - 00:23:07.809, Speaker B: So the trick that we do is we just start with a permutation symmetric initial condition. So we imagine that this is the matrix of dot products between n tokens.
00:23:07.937 - 00:23:08.737, Speaker C: All right?
00:23:08.921 - 00:24:29.865, Speaker B: We assume that all the tokens have the same squared norm, and that's the diagonal of this matrix, and that's Q. And we assume that the dot product between any pair of tokens is the same for all pairs. And that's, and we'll call that P. Okay, so this corresponds to tokens sitting at the vertices of a simplex, right? Where they all have the same dot products and so forth with each other and with themselves. Then we're going to be asking how does the simplex change its length and its angles? Now the second thing is, if you start with a permutation symmetric particle configuration and you propagate it through a random transformer, if the dimensionality of the embedding space is large enough, then with high probability, the resulting point cloud after propagation through one layer will remain permutation symmetric, okay? I mean, there'll be some fuzziness as the width gets smaller and so forth, or the embedding dimension gets smaller. But so based on that, we can operate in the limit of infinite embedding dimension and derive a recursion relation for P and Q. In one layer to P and Q in the next layer.
00:24:29.865 - 00:25:31.585, Speaker B: Okay, And I'll skip all the math of that. It's more complicated than the MLP case, but the details in the paper, then what we find is we find again two phases. Again, it's the variance of the MLP weights that control everything. When the variance of the MLP weights are small, just like in the MLP only setting, you get an ordered phase where the tokens are attractive and they actually collapse. So the simplex kind of collapse now to a straight line. The normalization doesn't allow them to collapse to a point, the normalization just yet, the normalization prevents that. So then the next best thing they can do in the ordered phase is collapse to a straight line, right? Actually, because the norms are all the same, they will collapse to a point, but if the norms start off differently, they'll collapse to a straight line.
00:25:31.585 - 00:26:23.855, Speaker B: Okay? Now on the other hand, if the MLP weights are large relative to the attention weights, you'll get a chaotic phase where now the N particles repulsive and they'll converge to a regular N simplex where P goes to a fixed point P star which is less, sorry, P over Q goes to fixed. So P over Q is the cosine angle. P over Q will go to a fixed point P star over Q star which is less than 1. So it will be a regular simplex that isn't a line. And you can see this here. This is the dynamics of our derived recursion relation in a two dimensional space spanned by P over Q. So P over Q is against the cosine angle and then the token norm normalized by the dimension.
00:26:23.855 - 00:26:47.711, Speaker B: So this is a nice coordinate system where everything occurs between 0 and 1 here and 0 and 1 here. This is the vector field. And you can see that in the ordered phase there's a single fixed point. Here there's no other fixed points. So no matter where you start, you always have to end up here. This is an example trajectory for a 16 layer transformer. And now this is a comparison of theory and experiment.
00:26:47.711 - 00:27:22.197, Speaker B: So this red curve is the dynamics of the token norm as a function of layers. And that's what our theory predicts. This is the dynamics of the token angle or cosine angle as a function of layers. And this red curve is what our theory predicts. This blue histogram is what we get numerically from simulating an actual random transformer with 16 layers. Relatively large input dimension, I think 100 or 200 or something like that. An embedding dimension you can see that the histogram converges roughly to our theoretical predictions.
00:27:22.341 - 00:27:22.725, Speaker C: Okay.
00:27:22.765 - 00:28:00.021, Speaker B: So this is a theory experiment match. Okay. Now what happens as you move to the chaotic phase where the variance of the weights of the MLP become large? Then you get a phase transition in this, or you get a phase transition signaled by a bifurcation in this dynamical system where the fixed point corresponding to a cosine angle of 1 corresponding to collapsed line, that fixed point loses stability. It's no longer a stable fixed point, and a new fixed point appears that has a cosine angle that's not equal to one.
00:28:00.173 - 00:28:00.949, Speaker C: Okay.
00:28:01.117 - 00:28:31.205, Speaker B: And it's the only stable fixed point in the system. So the dynamics converges to the stable fixed point. And you can see that, you can see that here. Right? So here you saw that the token angle converges to one. Here the token angle converges to something that's not equal to one corresponding to a non collapse simplex. And again, theory and experiment match here. Okay, now there's something important here is that in transformers we focus so much on the attention.
00:28:31.205 - 00:28:45.655, Speaker B: I mentioned that you only get non collapse if the MLP weights are large. So if you just kill the MLP by setting the variance of weights to be zero, so you just have iterated attention, you always get collapse.
00:28:45.815 - 00:28:46.447, Speaker C: Right.
00:28:46.591 - 00:29:29.973, Speaker B: The intuition behind that is the following. In a random transformer, each token is trying to pay attention to its neighbors, right? And then it essentially moves slightly closer to its neighbors on average. So the system of particles in a random transformer, as you iterate through just self attention, is like a system of gravitational particles that all attract each other and you get gravitational collapse. And it's actually consistent with what Philippe and Yuri were seeing, but in a much more rigorous way for iterated self attention. Yeah. Oh, by chaotic, I mean if the particles are close to each other, they'll diverge.
00:29:30.149 - 00:29:30.917, Speaker C: Right.
00:29:31.101 - 00:29:43.885, Speaker B: And. But when they diverge, their norm will stabilize and their angles will stabilize, but the whole system will be kind of rotating chaotically.
00:29:44.345 - 00:29:44.993, Speaker C: Right.
00:29:45.129 - 00:30:00.885, Speaker B: Despite their being stable fixed point. So it's like the fixed point is a statistical property of the chaotic evolution. The statistics of the evolution is stable, but the detailed positions of the points evolve chaotically and are sensitive to initial conditions.
00:30:02.225 - 00:30:03.049, Speaker C: Yeah.
00:30:03.217 - 00:30:08.645, Speaker D: How does P relate to P star? Is the idea here that P would be near one initialization?
00:30:10.585 - 00:30:43.775, Speaker B: Oh, so P is just the. Yeah. So in the chaotic situation, I guess the way to understand it is P star is a property of the hyperparameters. P star is, you roughly think of it as the cosine angle that any simplex will converge to as it propagates through the system. And that's only a property of the hyperparameters of the system and P. So P star is a function of the variance of the weights of the mlp, the variance of weights of the attention and so forth.
00:30:48.355 - 00:30:51.091, Speaker E: Manifold that becomes a space fitting one.
00:30:51.283 - 00:31:01.437, Speaker B: We haven't looked at manifold propagation through these systems. Yeah, it should. It should. Yeah, exactly. Yeah.
00:31:01.541 - 00:31:05.945, Speaker E: I was wondering if you're assuming batch normalization here and expecting normalization.
00:31:06.725 - 00:31:10.745, Speaker B: We don't have batch normalization of spectral over issues. We do have layer normalization.
00:31:15.205 - 00:31:21.715, Speaker D: Do these results still hold if I set the biases to be uniformly zero or do they require devices to be non zero?
00:31:22.535 - 00:31:53.415, Speaker B: These are okay for the mlp. Yeah. So we did these simulations where the biases are zero. But what happens is if you turn on the biases, you just have to make the weight variances that much larger to get to the chaotic regime. Okay, so this shows that our theory for signal for forward signal propagation matches simulations. Now we'll discuss implications of that. But actually first we need to talk about the back propagation of gradients through the transformer.
00:31:53.415 - 00:32:49.449, Speaker B: Here the story is clear for any system, for any deep network, you always have either a vanishing phase or an exploding phase where the back propagated gradients either vanish or explode, respectively. And it also happens when the MLP weights control everything. Actually, when they're small relative to the attention weights, the gradients vanish. And when the weights are large relative to the attention weights, the gradients explode. Okay, so again, just if you kill the MLP by not having any weight variance in the MLPs, you just have iterated attention, you get vanishing gradients. Okay, so now what we can do is we can derive the analog of Lyapunov exponents for both the forward and the backward signal propagation. So for that, I mean, let's look at this collapsed fixed point.
00:32:49.577 - 00:32:50.325, Speaker C: Okay?
00:32:50.865 - 00:33:52.775, Speaker B: So, so this fixed point, I can linearize my recursion. Again, my theoretically derived recursion relation, I can linearize it about this collapsed fixed point, okay? And I can look at the eigenvalue, okay? In order for it to be stable, both eigenvalues have to have absolute value less than 1 in discrete time, right? So I can take the maximum eigenvalue, right? And write it as E to the lambda, right? And therefore I better have lambda be negative for it to be stable. That lambda is the Lyapunov exponent for forward propagation. Okay, so again, just to summarize if the Lyapunov exponent is negative, forward propagation is in the in the ordered phase, and this fixed point is stable. If the Lyapunov exponent is positive, the forward propagation is unstable. Sorry, the forward propagation is chaotic and this fixed point is unstable. So you're in this regime, okay? So you're in this type of a fixed point.
00:33:52.775 - 00:34:30.581, Speaker B: We can do the same thing for backward propagation. If the backward Lyapunov exponent is positive, then gradients, backpropagate gradients explode. If the backward Lyapunov exponent is negative, then backpropagated gradients vanish. That lambda can be the exponential rate of either growth or decay of the backpropagated gradients. And again, the forward propagation lambda is the exponential rate of approach or departure from the fixed point. All right, so now what I've done here is I've computed the Lyapunov exponent as a function of the attention weights and the weights of the mlp.
00:34:30.773 - 00:34:31.541, Speaker C: Okay?
00:34:31.693 - 00:34:59.165, Speaker B: This is theory. And this is the Lyapunov exponent for the forward propagation, which we call the token angle exponent. And the Lyapunov exponent for the back propagation, the gradient size exponent. And you can see that it's positive here, negative here. There's a phase boundary here. And it sort of makes sense. Attention always likes to order the tokens and cause gravitational collapse, so to speak.
00:34:59.165 - 00:35:13.169, Speaker B: The MLP weights, if they're large, they like to cause chaos and repulsion. So the larger the attention weights, the larger the MLP weights need to be to counteract the gravitational collapse of attention through the chaos of MLPs.
00:35:13.337 - 00:35:13.929, Speaker C: Right?
00:35:14.057 - 00:35:26.255, Speaker B: And that's why this phase boundary is going up this way. Interestingly, the gradient back propagation has a different effect where basically the phase boundary is going down.
00:35:26.595 - 00:35:27.335, Speaker C: Right.
00:35:27.995 - 00:35:35.387, Speaker B: Okay, now this is the. This is the numerically computed token angle exponent from simulations.
00:35:35.531 - 00:35:36.027, Speaker C: Right?
00:35:36.131 - 00:35:45.049, Speaker B: And you can see that there's a really nice qualitative match here, okay, between both for the forward propagation from here to here and the backward propagation from here to here.
00:35:45.137 - 00:35:45.825, Speaker C: Okay.
00:35:45.985 - 00:36:31.175, Speaker B: All right, so now that's great. But now what does this have to do with the tradability of transformers? Okay, so what we did is we now actually train transformers. We had one gpu, and so we did a vision transformer on a food dataset, which other people have actually done as well. So we're not in some random space, but we trained a vision transformer. It was like 16 layers or so. And we initialized it using various initialization hyperparameters. And for Each initialization hyperparameter, we computed the corresponding Lyapunov exponents both for the token angle and forward propagation and the gradient for back propagation.
00:36:31.175 - 00:37:12.465, Speaker B: And we computed the test loss at the end of training. This horizontal red line corresponds to chance performance. You can see if you just compute performance as a function of the token angle. You see that you must have the token angle be close to zero. Sorry, The Lyapunov exponent be close to zero, but that's not sufficient. You can still get bad performance if the Lyapunov exponent alone for the token angle is close to zero. If you compute performance as a function of the gradient Lyapunov exponent, it has to be close to zero to get good performance.
00:37:12.465 - 00:37:55.615, Speaker B: But that's not sufficient. You can have bad performance also. But if you plot the performance as a joint function of the Lyapunov exponent for token angle and the Lyapunov exponent for gradients, you see that at 00 you get the best performance, and away from that you get worse performance. So this is kind of a histogram of performance averaged over initialization hyper parameters that are in a restricted region of Lyapunov exponents. And then you can actually predict the final performance as a function of the two Lyapunov exponents. And we just fitted these coefficients here. So this is the max of the two Lyapunov exponents in absolute value with some weights plus a constant term.
00:37:55.615 - 00:38:22.997, Speaker B: We learned these three constants. That's it. So, as you can see, there's a pretty good correlation between. So this max function roughly measures how far you are from the intersection of the two phase boundaries. Right. So here you're further away from the intersection of the two phase boundaries, and the further away you are, the worse your final performance gets. So remember, these are properties of the initialization on the horizontal axis, and this is the final test loss on the vertical axis.
00:38:22.997 - 00:39:01.775, Speaker B: And so we can connect the initialization to the final training performance quantitatively. So again, the upshot is you have one phase boundary here, and you have another phase boundary here, and you better be at the intersection of them to get good training performance. Okay, and I think I'll stop. So again, this was the outline of the talk, and now we've covered the outline, and so hopefully it should make sense. And I'm happy to take questions. Thanks. So suppose I cook up some arbitrary neural network.
00:39:01.775 - 00:39:04.451, Speaker B: Is there a systematic way of finding.
00:39:04.483 - 00:39:07.575, Speaker C: These good initializations short of reinforcing it?
00:39:10.915 - 00:39:39.349, Speaker B: In principle, you could just analyze forward signal Propagation and backward signal propagation. Find the phase boundaries and find the intersection. That worked for MLPs, it worked for transformers. Right. In principle you could always do that. And if you have an architecture that's large and IID random Gaussian weights, you can usually use the techniques of StatMac to reduce the signal propagation of individual inputs to the signal propagation of inner products of their inputs. And you get a deterministic recursion relation.
00:39:39.349 - 00:39:44.385, Speaker B: So it's actually a turn the crank thing. Yeah.
00:39:44.485 - 00:39:48.241, Speaker D: Your analysis. How crucial is Gaussianity? So if you initially.
00:39:48.353 - 00:39:56.289, Speaker B: Yeah. Gaussian is not too crucial. IID random weights is what's crucial. Yeah.
00:39:56.417 - 00:40:06.545, Speaker D: So I guess this is very cool and I see how it would be like useful in practice. But I guess also in practice for large scale pre training one issue is like low precision.
00:40:06.705 - 00:40:08.205, Speaker B: Yeah, yeah, yeah.
00:40:10.035 - 00:40:11.495, Speaker D: I guess is that just.
00:40:14.635 - 00:40:44.625, Speaker B: So the low precision impacts the gradient computations. Right. And you get these loss spikes and gradient norms blow up. This doesn't address any of that. This just makes sure the initialization is well conditioned. Now something to explore is if the initialization is well conditioned, does it remain well conditioned as you train? And so who's to blame for the lost spikes? Right. Is the initialization to blame or is the SGD trajectory after the data has seeped into blame and so forth.
00:40:44.625 - 00:40:49.765, Speaker B: So I think those are open questions. Those are very interesting things to work on actually.
00:40:50.425 - 00:40:51.273, Speaker C: Yeah.
00:40:51.449 - 00:41:01.925, Speaker E: So you said that the attention layers collapses by those variable points. Is it because of the softmax or with something similar happen for linear retention?
00:41:03.755 - 00:41:30.175, Speaker B: That's a good question. I'm not entirely sure. I have to go back to the equations and check. We didn't do it for linear attention, but my intuition suggests that it should collapse for linear attention. You might get a growth because you don't have the normalization that comes from the softmax. But my sense is it would still be attractive with iterated self attention. Linear self attention.
00:41:30.175 - 00:41:32.975, Speaker B: But I'd have to check that I have to do the calculation.
00:41:34.555 - 00:41:39.651, Speaker E: So when you solve the system using the matrix assumption.
00:41:39.723 - 00:41:40.411, Speaker C: Right.
00:41:40.603 - 00:41:54.899, Speaker E: Is that. Is that assumption badly general? Like. Like in the sense that you know, is that what is forcing you to kind of get the result where everything is just a dilation just because of the. So you have the equivalence property, right?
00:41:54.987 - 00:41:55.615, Speaker B: Yeah.
00:41:55.955 - 00:41:59.475, Speaker E: Start off in one, one setting. They will end back again.
00:41:59.555 - 00:41:59.947, Speaker B: Yeah.
00:42:00.011 - 00:42:10.603, Speaker E: And you get that with the tools matrix too, Right. Not just with this matrix. And I'm just wondering if the assumptions around the matrix drive some of the results.
00:42:10.779 - 00:42:11.587, Speaker B: Yeah.
00:42:11.771 - 00:42:16.455, Speaker E: Can change the. You can only expand and contract the space at this point in time.
00:42:18.115 - 00:42:18.851, Speaker B: Yeah.
00:42:19.003 - 00:42:22.825, Speaker E: So driven my intuition.
00:42:23.805 - 00:42:31.925, Speaker B: Yeah. So here's the exact recursion relation for the length. And what you see is that the non linearity of the network plays a role.
00:42:32.085 - 00:42:32.861, Speaker C: Right.
00:42:33.053 - 00:42:43.517, Speaker B: So the matrix plays a role, but the non linearity also plays a role. And they both interact in this recursion relation to drive the quantitative dynamics of say the length. And also for the cosine angle, non.
00:42:43.541 - 00:42:45.805, Speaker E: Linearity creates that contraction.
00:42:45.925 - 00:42:47.231, Speaker B: Exactly, exactly.
00:42:47.373 - 00:42:48.411, Speaker E: Converge to a fixed.
00:42:48.483 - 00:42:49.707, Speaker B: Exactly, exactly.
00:42:49.851 - 00:42:56.307, Speaker E: It actually makes that function convex. And then when it does, it does the contraction. You contract to a fixed point.
00:42:56.411 - 00:42:59.931, Speaker B: Exactly, exactly. Otherwise you won't. Absolutely, yeah.
00:42:59.963 - 00:43:02.219, Speaker E: That kind of forces you a certain way.
00:43:02.307 - 00:43:02.571, Speaker B: Yeah.
00:43:02.603 - 00:43:22.781, Speaker E: And there are also some very interesting things, relations to control theory, wherein you basically there are poles within the unit circle that have to show up. Now what you're saying is that because you. Forward propagating and back propagating. But again you actually have to have both the poles and zeros, both people in the unit circle in order for things to converge.
00:43:22.973 - 00:43:28.493, Speaker B: I'm not sure that. That you can map forward to poles. Yeah, yeah.
00:43:28.589 - 00:43:33.105, Speaker E: That corresponds to the. You know, in a traditional system.
00:43:34.325 - 00:43:35.365, Speaker B: Yeah, right.
00:43:35.405 - 00:43:37.973, Speaker E: And you're saying that the poles have to be within the unit circle.
00:43:38.069 - 00:43:39.313, Speaker B: Yeah, yeah.
00:43:39.489 - 00:43:40.881, Speaker E: Pointing out the sign of it.
00:43:40.953 - 00:43:41.521, Speaker B: Yeah.
00:43:41.673 - 00:43:52.045, Speaker E: Because of the. Of the gradient descent. Very rooted by propagation. The zeros of the system are actually becoming the pros of the system because you kind of.
00:43:52.465 - 00:44:09.125, Speaker B: Yeah. At a high level. I, I see what you're saying, but the, the problem is the recursion relations for back propagation are just different from the recursion relations for forward propagation. They're related, but they're not identical. So there's not an inversion between them. Which is why for the transformer, the phase boundaries of the two systems are not the same.
00:44:10.945 - 00:44:14.145, Speaker A: In the interest of time, let's take the rest of the questions offline and let's thank Sir.
