00:00:01.200 - 00:00:10.734, Speaker A: All right, so we're really excited to kick off this afternoon, giving some more back about robotics. And Andrew Toss is one of the co organizers of this workshop.
00:00:10.774 - 00:00:14.206, Speaker B: It's going to robot wanting to do.
00:00:14.350 - 00:00:44.390, Speaker A: Great. Thanks, Emma. Yeah. So I am excited to tell you a little bit about the work that my group's been doing on getting robots to be able to learn how to do things from people. People. Just since it's a broad audience, a little bit of my background is I did my PhD at the media lab at MIT, working with Cynthia Brazil on cute little robots like this one. And then I went to Georgia Tech and had a great group of students, and we were working on these robots that I'll have a lot in my talk.
00:00:44.390 - 00:01:41.646, Speaker A: And now I'm at the University of Texas at Austin in the ECE department, building up a new lab. So my lab is called the Socially Intelligent Machines Lab. And we are generally interested in getting robots out into the world, interacting with people in human environments. And so this is just a smattering of the kinds of robots that we think about and the kinds of human environments and the kind of interactions that we think about. And our main premise is that we're interested in these social robots, where social, to us, means any robot that has to interact with a human as part of its functional goal. So it's going to be out in human environments, interacting with people as part of what it has to get done. So one of the things that's true about these human environments is that a lot of the successes of robots are in these structured environments where you can learn something or model something and do it over and over and be successful.
00:01:41.646 - 00:02:29.686, Speaker A: But human environments are messy and dynamic, and pre programming the exact thing ahead of time is going to be hard. So we like to think about how robots are going to be able to learn how to do the right thing on the job out in the world from end users. So that's sort of our premise. And so one example that motivates this is in the manufacturing domain. So people are interested in getting robots to be able to be flexible coworkers in these manufacturing spaces. So currently, you see robots in manufacturing doing building airplanes or cars or things where you can set up the process, and it's the same process for a very long time. But if you're a small, small or medium sized manufacturing operation, you may want to have the robot do one thing this week and another thing the next week.
00:02:29.686 - 00:03:57.864, Speaker A: And so this is this motivation for having robots that can learn how to do new things on the manufacturing line and so the one example that everyone can be familiar with is you want to have a robot that's going to clean your messy kitchen, that is hopefully not as messy as the images you get when you google messy kitchen, but every person's home is slightly different. So I can't, and I can't sit in my lab and program exactly what the robot should do when it's going to clean up your kitchen. You have to interact with the end user and get some part of your parameterization or some part of your skillset on the job from end users. So this is what we like to think about, is, how do we get end users to come into our lab and teach our robots how to do stuff? And what are the right interactions for these end users, and what does their data, what is the input that they give our learning algorithm? What does it look like, and can we change anything about the interaction? Can we change anything about the algorithm to kind of make these two things mesh a little better? So I'll tell you a little bit about some of the results that we have. So we're interested in what the data looks like and whether we can facilitate better demonstrations. So here's an example from quite a while ago. But this is our robot, Simon, and somebody's teaching this robot how to close this box.
00:03:57.864 - 00:04:44.832, Speaker A: And this is a pretty standard approach in robot learning from demonstration, is you would get a demonstration from a teacher as to what you're supposed to be doing. But this is after this person has had a whole practice session about how to use this robot and how to teach it. They're still having trouble figuring out how to move the joints. And it wasn't until one of my grad students at the time, Maya Checkmec, came and try moving it this way. And so the problem is that the robot was recording that entire thing. And so the demonstration goes, okay, start recording. And now I'm recording the whole thing.
00:04:44.832 - 00:05:39.614, Speaker A: And I recorded all of this stuff of messing with the arm. And so that's not ideal in terms of input space for a machine learning algorithm that's gonna then try to recreate that whole thing. So what we've been looking at is an alternative demonstration where we just get keyframes from the person, and so it would look a little bit like this. So here's the same skill. Record frame. So each time he says record frame, we take a snapshot of the state, and I'll get into the details in a little bit about what is actually recorded. And here you see that, you know, he's kind of having similar sorts of issues, figuring out exactly the kinematics that he's like to put the robot into to get this skill done.
00:05:39.614 - 00:06:28.520, Speaker A: But the benefit is we're not recording all of this. We're kind of letting the user figure out what they want to, what state they would like to record and then recording it. So this is a demonstration technique that we've made use of. That is a good way to get people that aren't very good at demonstrating skills to have better data. So the trajectory information, you know, good teachers are better at providing trajectories. If you know exactly how you want the robot to do something and you're good at providing it, then taking all of that trajectory information is useful because we get velocity information so we can communicate the speed. And whereas these keyframe demonstrations, you're stopping every time, so we don't get any information about what the desired velocity is at those stopping points.
00:06:28.520 - 00:07:08.544, Speaker A: But, um. But if, if instead, if you're giving me trajectories that have all of this extraneous information, I would much rather throw away some of that velocity information, infer that later and take just the keyframe demonstration. So we've been making a lot of use of keyframes. And that's going to be the input demonstration type for, um. For most of the rest of this talk. And a lot of, um, students that are in my lab have kind of taken and developed different aspects of what we might do with this information. That is, keyframes, where we're thinking about keyframes in the visual space, keyframes in the motor space, keyframes in the haptic force feedback space.
00:07:08.544 - 00:07:53.488, Speaker A: And one observation I'm going to tell you about. I'm going to focus mostly on, oh, I don't have a picture of Barish. I'm going to focus mostly on one student's work, Baresh Atgun, who's looking at exactly how we should learn from keyframe based learning from demonstration. And one of the things that really motivated Barish's work on keyframe demonstration is this early example that we saw when we were still using trajectories. And so I'll show you this really old video of the PR two. And here we're still closing this box, and the PR two is recording a trajectory. And this person has this teleoperation device.
00:07:53.488 - 00:08:22.744, Speaker A: So it's controlling the PR two via teleoperation. And it's recording the whole trajectory. And you're trying to get the PR two to close this box, which is even when you're sitting right next to it. This teleoperation is very hard to do. It's really pretty annoying. But he finally gets the box closed. And because this was really a hard demonstration task, we asked people to watch their demonstration and decide whether to keep it or throw it away.
00:08:22.744 - 00:08:46.604, Speaker A: And so this person is watching their demonstration, and this time, on the first swing, the robot closes the box. But then, because the robot's just playing back the trajectory, it continues to do this thing. And you ask the person, was that a good input? Do you want to save it? Please say no. Please say no. And they say yes. They say, yeah, sure, that's great. So this is the problem that we have.
00:08:46.604 - 00:09:50.840, Speaker A: This person thinks that was an amazing demonstration of closing the box, and we have to build an algorithm that can deal with learning how to close this box with that kind of terrible data. So this motivated Barish Atkins thesis work on thinking about this problem of people that are good at maybe not so good at showing you how to achieve the skill, but are actually very good, or everybody is pretty good at showing you what you're trying to do. And so achieving the skill is what we think of as the goal. And exactly how to do it is what we think of as the action. The showing me that I'm trying to close the box is pretty easy to show, but showing me exactly how I should move my seven degree of freedom arm may be hard for some people to do. And so in this work, we split those two things up, and we think about how we can build two separate models, because we know that one of, for some people, some of this information might be noisy and not good. So we're going to treat these as two separate learning problems.
00:09:50.840 - 00:10:42.580, Speaker A: And on the one hand, I'm going to build an action model that's telling me how to execute some skill. But then the objective or the goal is something that I should be using to actually monitor that execution. Here's what we'd like to have. I think that was successful, a little hard to see. So we'd like to have a robot that can execute something and then monitor that execution and say, I think that was successful, or I think that I failed. And that's going to be what we actually want to get out of these two pieces of information, the what to do and exactly how to do it. And so in this framework, we have keyframe demonstrations that are coming in, and we're going to build an action model out of the motor data, and we're going to have an external sensor.
00:10:42.580 - 00:11:28.104, Speaker A: In this case, we're using a camera, a depth camera that's looking at the visual scene of the workspace and building a separate goal model that can monitor that execution and lead to our whole process. And so we call this goal based learning from demonstration. And now what we really like about key frames now is that these are not just an easy way for people to provide robust information, but these are actually salient points in the skill. So it's actually great information for a goal model because people are saying, this is an important point. That's an important point, and this is another important point. So you're sort of giving a nice demonstration of what the goal trajectory is through the space.
00:11:28.444 - 00:11:30.812, Speaker C: So these sort of intermediate key points.
00:11:30.868 - 00:11:32.956, Speaker D: Are they just sort of very approximate?
00:11:33.140 - 00:12:03.744, Speaker A: Yeah. And even. Yeah. So, and across different people, they might be slightly different. And, you know, for a single skill, somebody might give four key points, another person might give, and it doesn't matter so much as you'll see, because we're going to build a hidden Markov model that if there's two points that are close enough together, they'll kind of get thrown into the same hidden state and it'll be fine. And the goal monitor sort of indicates whether the goal has been achieved. And also these intermediate.
00:12:03.744 - 00:12:30.694, Speaker A: Yeah, so I'll get into the details, but. Yeah, but those are two separate models. So we have two separate temporal models. One is doing stuff and the other one is monitoring. But they're both learned from the same demonstration. Yeah. What is it that makes something a keyframe and does that generalize across tasks? Yeah, that's a great question.
00:12:30.694 - 00:13:11.110, Speaker A: We haven't done that, and I think that it's going to be different for different tasks. But even in the same task, we don't see people. There's some agreement on if you're pouring something, there's going to be a keyframe right before you turn, and then you don't. And so there's some things that are common across everyone, but then some people just divide up the skill a lot more, and some people forget. They'll, like, go here, go here, and then you don't have a very. And if you don't have that intermediate point, you're going to start pouring, like, in the middle. Um, so, uh, so those are some of the things that you see that are different across people.
00:13:11.110 - 00:13:54.268, Speaker A: But I think that's a great question, whether there's something, uh, that makes for a canonical keyframe. Um, okay, so I'll tell you a little bit about what the exact data is that we're using as input in this case. Um, so we have the motor data is the end effector with respect to some target object. And so, you know, we have the, the key frames that are going towards this object, and we might have a couple of different ones. If you're coming at the object one way versus another way, the motor data is going to be transformed with respect to the target object. And this is a 3d position and a 4d rotation. So we have seven dimensions in our motor data.
00:13:54.268 - 00:14:52.516, Speaker A: In the object data, we have a depth camera that's looking down at the workspace and extracting features, mostly related to features about the bounding box, and then also features about the surface normals from the depth camera. And so we take both of these feature vectors and pass them through and build different models. We are using the same algorithm, but we build two different ones. So we're using hidden Markov models. And in a sense, what we get out of the hidden Markov model, after we've learned is sort of this canonical keyframe. So you can think about, if you take several demonstrations and you build a hidden Markov model, then these hidden states do sort of represent the canonical keyframes that are represented in your data. And the difference between the action and the goal model is that we have different emission spaces of these states.
00:14:52.516 - 00:15:48.554, Speaker A: So in the motor space, this is a multi dimensional, it's a seven dimensional gaussian, and in the visual space, it's much bigger dimensions. And so we'll take multiple demonstrations of keyframes and use the Baum Meltzer algorithm to kind of build this in a very pretty standard way. The difference that we do is we do add some notion of prior states and terminal states. So we want to be able to think about whether these are the states that you're going to be starting in, and it has to reach this state in order to consider the skill is finished. So we have some notion of the likelihood of different states being starting states and terminal states. How do you decide on the number of states versus the number of key frame points? They're different in all the different demos, maybe. Yeah.
00:15:48.554 - 00:16:53.604, Speaker A: Do you decide that there are four states, or is that we just use bayesian information criterion to come up with k? And I think, yeah, there's, yeah, so we use bayesian information criterion. And when we go to generate, we do look a little bit at the, that we're generating at least as many as the, at least as many keyframes as the minimum number that you saw in the demonstrations. Okay, so when we go to execute, we can take our learned action model and we can sample a path from prior states to terminal states. And then we are. So here's where we're. When we're sampling the most likely path from prior to terminal, we're going to favor path lengths that are close to the average number of demonstrated keyframes. And then once we have a path, I look at the emission space of these states.
00:16:53.604 - 00:17:38.746, Speaker A: And if I'm trying to generate the canonical trajectory that my action model is representing, then I'm actually going to just take the true means of these emission spaces. And I'm taking those as my keyframes. And what we do is a fifth order spline between them to create this trajectory that is sort of smooth and has minimum jerk and goes between the means of these states. And so now I have this trajectory that I've decided is the thing that I want to do based on the action that I learned. Now I can talk about how I can use the goal model to monitor that. And so here's our. We've sampled our action model.
00:17:38.746 - 00:18:50.254, Speaker A: We've got our four keyframes, and we're splining between them. And now I've got this action model up here. And what I'm going to do is, while I'm executing the action, I'll take a snapshot of the visual space that coincides with each of these keyframes. And I can ask now I can take this as a sequence in the goal space and ask my goal model, like, how likely is it that this sequence of visual, of my visual space represents a success or a failure in the goal? And so this is just kind of a typical question that you would ask the, hmm, is this a likely sequence for this model? The only difference is that we do make sure that the. That the last frame represents something in a terminal state because otherwise you could get a really likely sequence, but you only kind of halfway did the skill. So this is the whole system. And so now we can ask, how well does this work with data that we get from people? So we had eight people come and try to teach this robot how to close a box and how to pour something into.
00:18:50.254 - 00:18:57.476, Speaker A: From a cup into a bowl and start here. Okay, this is what they did.
00:18:57.540 - 00:19:08.744, Speaker B: Go here. Go here. This go here. All right. And here.
00:19:09.524 - 00:19:41.184, Speaker A: Okay. So that is what the demonstrations look like. And again, we're taking from this one or from the single set of demonstrations, building these two models. And we then after the person leaves, we execute their model, their learned model, five times to get a sense of, well, how successful was the execution? And then while we're executing their action model five times, we can test the goal model and say, well, how successful is the goal model? It's saying whether or not the action model was good or bad.
00:19:41.884 - 00:19:43.092, Speaker B: Here's what I learned.
00:19:43.148 - 00:19:49.194, Speaker A: So, this is what Barish had to do a lot of times for every user.
00:19:57.494 - 00:19:59.114, Speaker B: I think I failed.
00:20:01.574 - 00:20:14.744, Speaker A: At least the robot knows that it didn't do well. Barish had to pick up a lot of macaroni because, as we talked about, people are not very good at modeling exactly how to do things, but they're pretty good at modeling the goal.
00:20:14.894 - 00:20:28.756, Speaker C: So it's the main thing that, like, the brain box that has a different orientation, is that the most salient observation that made the detection possible, that made.
00:20:28.780 - 00:21:38.682, Speaker A: The action possible, made the detection of failure possible? Well, so it's hard to say exactly, because this is a particular user in the study. So, of the eight users, this is one user's exact skill that was learned from several demonstrations of the skill. So, that goal model is based on whatever variance they showed in that set of four or six, or. I can't remember exactly how many demonstrations it was, but, you know, we're building a single hidden Markov model from that person's data, and so the exact dimensions that they demonstrated variance in are what are going to get captured in that hidden Markov model. But what we see is that across people, and I think that might be my next slide, but across people, of those eight people, the close the box skill had about. Had less than 60% execution success, but more than 90% monitoring success. So, the hidden Markov model that you build from somebody's demonstrations are not very good at generating the skill successfully.
00:21:38.682 - 00:21:58.174, Speaker A: But that same set of demonstrations gave you a good description of the goal. And so, same with the pouring skill. Only about 75% accuracy on actually doing the skill, but more than 90% in determining whether you were good or you were succeeding or failing at doing it.
00:22:00.214 - 00:22:08.646, Speaker E: So, if the robot can sort of decide whether he failed or succeeded, so can you build a loop that he can sort of start teaching himself?
00:22:08.750 - 00:22:31.780, Speaker A: Yes. Yes, we can. We're going to do that next. And so we see that some teachers are better at than others at teaching actions. So some people are good at doing actions and goals, but on average, people are better at teaching the goal than teaching exactly how to do it. And so we want to bridge this gap by adding some autonomy. Yeah.
00:22:31.812 - 00:22:32.468, Speaker F: Just a quick question.
00:22:32.516 - 00:22:33.012, Speaker A: Sure.
00:22:33.148 - 00:22:36.788, Speaker F: Just to be clear, is the goal a state, or is it a trajectory?
00:22:36.876 - 00:23:11.342, Speaker A: The goal is a trajectory. The goal is a trajectory that is recognized in a very kind of standard activity recognition sort of way with this hidden Markov model. Yeah, sorry. That was a good point. So now we want to self improve based on the fact that we have this objective function that we're pretty confident in, but we have this action model that we're pretty unconfident in. And so what we're going to do is execute based on our action model, but with some variance. And then we can use our goal model to tell us when we've executed in a good way or a bad way.
00:23:11.342 - 00:24:10.454, Speaker A: And so we then can have this self improvement signal that's going that we're adding a link in our model. So how are we going to execute with variance? Well, we're going to do the same sort of thing that we did when we were executing our action model originally. But remember before we just took and splined in between the exact means or the canonical version of what our action model is representing. But now instead, we're going to take this multivariate gaussian and use that as a sampling mechanism. So we're going to sample both the state path through the hidden Markov model and the actual emission of each state, which, remember, each of these emission spaces is representing a seven dimensional pose of the end effector. So I'm going to take. And instead of doing the canonical pose that this action model represents, I'm going to kind of perturb some of the poses in a particular way.
00:24:10.454 - 00:24:10.878, Speaker A: Yeah.
00:24:10.966 - 00:24:15.786, Speaker C: So they're just position, effectively, you don't have velocity in any of these?
00:24:15.890 - 00:24:39.114, Speaker A: No. Yeah. So it's just the position and orientation of the end effector with respect to the target. Yeah. Could you sort of try to perturb in a way that you think might be better as opposed to weight? Yeah. So, yeah, so I'll tell you a little bit about we. I think there's much more we can do to sample this space more intelligently.
00:24:39.114 - 00:25:20.978, Speaker A: I think we're not doing the best right now. We have what I'll show you in a second. We do it kind of randomly, but weighted by the mixture of gaussians. And then what we do is we want to sort of sample broader as we want to sample further away from the mean. When we are either bored and we feel like we want to expand, we either have a good model already and we want to broaden kind of the applicability of that good. We want to see like, ok, I have a model that I can execute five times and it always succeeds. Let me see if I can perturb that execution and still succeed.
00:25:20.978 - 00:25:53.350, Speaker A: And so you kind of want to broaden your skill or it's a terrible skill. I'm failing all the time, so I want to explore farther away from the mean because I need to fix it. And so we have an adaptive sampling method. That's what I learned. So each rollout of the skill is an execution of the skill by the action model. And I think that was successful. And an evaluation of that execution by the goal model.
00:25:53.350 - 00:26:31.690, Speaker A: So that's one roll out of the system. So that's a positive case. And then in an episode we would do five rollouts. And so in this case we had two, that the goal model thought were successful and three that it thought was a failure. And currently we just take the positive examples and put them back in with our demonstration data. And then we rebuild a new action model that now has our original demonstrations, plus these two more that we've seen that are positive examples. Clearly we could be doing more by trying to take advantage of these negative examples.
00:26:31.690 - 00:26:32.062, Speaker A: Yeah.
00:26:32.118 - 00:26:35.634, Speaker E: Are you also trying to get a better prediction?
00:26:36.734 - 00:26:54.710, Speaker A: So we do take and use the well, so we're using the prediction model to label this. So it's not actually training model. This is not actually good training data for the prediction model because we've used the prediction model to train it.
00:26:54.902 - 00:27:16.758, Speaker E: Well, my guess is that sort of when you go inside your model, not all good predictions are equally good and the bad prediction. So you should be able to compute a lot. I'm wondering, I think there is probably, there's like a real number and you have a threshold, so you can take sort of better things to distinguish.
00:27:16.846 - 00:27:43.116, Speaker A: So, I mean, yeah, we haven't done anything with that. The only point at which we do update the prediction model is if we do have a human that's there watching this process and they are agreeing or disagreeing with these labels, then we can use that to update our goal model. But that's the only time that we've done that. But it's true. Yeah, we do get a likelihood. It's not just a binary. We get a likelihood out of that prediction.
00:27:43.116 - 00:28:11.464, Speaker A: So we could use that in some way. I'm not sure what the ground truth would be in that case. And so one thing that we do is we forget the original user data after we've seen enough positive examples because, you know, some, some of the original demonstrations of what action to do were actually so bad that it's, it's ruining your, you know, it's pulling your model in a bad direction. Yeah.
00:28:12.124 - 00:28:30.612, Speaker C: Looking at the particular task, is it true that as a human expert, you really want, on the last keyframe, the cup to be centered, and you don't really want to perturb that particular keyframe. So is there a way for, say, the human to provide that kind of safety?
00:28:30.708 - 00:29:24.654, Speaker A: Right. So in this work, we don't do that. We're looking at other ways in which you would kind of directly communicate some of these constraints like the one you're talking about. And then you can try to provide these more kind of high level constraints to a planner even, and just say, actually the only thing you care about is that you have to go over here like this in your last step and then all the way through, you have to maintain those constraints. So we're looking at that as kind of a different, in a different way. I think this sort of model is one where you could try to infer those types of constraints, but we're not doing that in this work in particular. So we do have, like I said, this adaptive sampling, where depending on the ratio of success over the last few, the last few executions will expand how risky we're being in our sampling behavior.
00:29:24.654 - 00:30:15.374, Speaker A: So here's some examples of success and failure of closing the box and pouring. And we found that the adaptive sampling. So in both cases. So the first test that we did of this was just my grad student, Barish was provided a purposely failing action model for both pouring and closing the box. So he did something that was kind of close to closing the box, but doesn't really do it, and then used this exploration to fix the model. And so we found that, yes, it was in fact able to fix the model. And yes, you can get there faster if you do this adaptive sampling instead of just randomly sampling.
00:30:15.374 - 00:31:14.304, Speaker A: And the question that we have after this is whether or not we can do this with real people's data. So some of the people that we saw, these failing action models from our first user study, would we have been able to fix their models. And so that's what we did in this last experiment that I'll tell you about. So in this case, we're kind of putting it all together. And the only thing that we're adding is this bit that we were just talking about where now while the robot is doing its sampling, we can have the teacher actually provide labels to the goal. We can have the teacher provide labels during execution and during recognition to provide this verbal feedback as to whether or not the goal model was correct. Okay, so we had twelve people come in and teach the robot three skills.
00:31:14.304 - 00:32:00.468, Speaker A: Now they're going to close this box and open a box and pour. And they did five demonstrations. And during the demonstrations, the robot sort of executed the intermediate learned model after one, three, and five demos. So after the first, it's just going to kind of repeat the demonstration that you just gave. But after the third and the fifth demos, it does give a sense of what the current state of the action model is. And then the person hangs around while the robot does one rollout or one episode of the sampling process. So they get to watch the robot try to explore for five executions, and the goal output is verbalized.
00:32:00.468 - 00:32:24.054, Speaker A: So the robot says, I think I failed. I think I succeeded. And the human can say, yes, you're right, or no, you're wrong. And then the person disappears, and we do the self improvement process while they're gone for nine more episodes. So this is the whole experiment. And here's start here.
00:32:24.554 - 00:32:40.906, Speaker B: Okay, go here. All right. Go here. Yes. And here. That was.
00:32:41.090 - 00:32:43.414, Speaker A: And so then show me what you learned.
00:32:44.444 - 00:32:55.224, Speaker B: I will, gladly. That's it.
00:32:57.284 - 00:33:09.504, Speaker A: So here, the robot's still asking, how did I do? Because we're still collecting the demonstrations to build the goal model. But then when we're actually doing the roll out or the samples, try it yourself.
00:33:10.274 - 00:33:24.370, Speaker B: Here it goes. How did I do? I think I succeeded. You did. Yes, you did.
00:33:24.522 - 00:34:15.862, Speaker A: So we're testing our goal model, but also getting a label from the person as to whether or not our goal model is correct or not. Okay, so we did this after for each of the people. And so this would be the same. This would tell us, you know, how well did the models do using just the information from the human and just sampling with the human and the close the box results were. This is a little bit hard to read, but on average, it was kind of similar to what we saw before, like monitoring success. So the red bars are close to 90%, but the execution success on average was not as good. Some people's execution success was really not good at all, and some people learned it.
00:34:15.862 - 00:34:50.274, Speaker A: So there's a lot of variance in how well the action models did. And so this was, some people got it right away, and some people didn't. Pouring results were slightly more successful. Um, and open the box was, you know, very hard. Almost no one had a model. If you look at the blue bars, almost no one had a model that was able to execute after this interaction. So what we are going to do from this is take and see if it's possible to use our self improvement to fix some of these models that didn't turn out very well.
00:34:50.274 - 00:35:16.504, Speaker A: So we'll take just some case studies of a couple of examples from a couple of people from the close the box skill and a couple of people from the open the box skill, and I'll just show you the results of a couple of these case studies. So here's participant three's close the box initial example, or this is the initial model that after they left, this is what the robot was able to do.
00:35:22.024 - 00:35:23.544, Speaker B: I think I failed.
00:35:23.704 - 00:35:43.484, Speaker A: So it says, I think I failed, which is correct. And so here's an example of the sampling. So it slightly perturbed the action model and achieved a success.
00:35:44.764 - 00:35:45.664, Speaker E: Try again.
00:35:56.724 - 00:36:35.288, Speaker A: You can see that kind of perturbed it in a different direction. So now, so this is sort of what the sampling process looks like. And so the robot just kind of playing around, and the space in which, you know, the action space in which the robot is sort of perturbing and sampling and playing around is actually very small. So it's almost surprising that after this process, you get a skill that consistently succeeds. And so here's with this participant, the final model, and you can run this model five times in a row and it'll succeed every time. And this is a model that started off. You run it five times in a row and it failed every time.
00:36:35.288 - 00:37:12.856, Speaker A: So for this. For this person, we had, after all the demonstrations, zero success. Meaning I run it five times, it fails five times after the sampling with the person. So just after five samples, I run it five times, it fails every time. But then after my rollouts, in about episode four or in about episode two of the rollouts, I can have a model that I run it five times and it succeeds every time. Um, so I'm going to. I have a couple other examples.
00:37:12.856 - 00:38:10.134, Speaker A: So this gives you a sense of actually how much the end effect, how much the, um, the end effector mean. Uh, through my action model is changing both in terms of the translation and the rotation of the end effector. And it's surprisingly little that you have to change. But I'm going to skip ahead to show you the final result of each of these. We did five of these case studies where we take a single person who had a completely failing model and use their goal model to fix their failing action model. And in all the cases we had, eventually, over the nine episodes of self learning, we end up with an action model that is fully successful. So what's exciting to me about this is that we didn't put anything into.
00:38:10.134 - 00:39:03.324, Speaker A: We didn't give the robot any information about the objective function. Right? So we're doing this refinement process with an objective function that was taught by people. So we're refining from the same set of demonstrations, we have an action model that fails. But we learned enough about that action that we can create an objective function to train that action model to be better. And so what we saw in this experiment is that we were able to do that in at least five of these cases that we tried. Some of the other cases that we didn't try, the people either already had a good enough action model that there was nothing to improve, or they didn't have a good enough goal model for us to do it. So there's still some things we have to understand about how to get the best goal model out of people, but we think this is really promising, and I'm happy to take more questions.
00:39:10.744 - 00:39:20.868, Speaker D: How well do they generalize to slight variations of the task, like maybe a slightly bigger box or like a heavier cup of something or lighter or.
00:39:20.996 - 00:39:43.860, Speaker A: Yeah, so the perturbations that this action model can handle are moving around the target object, essentially, because our state is a transformation of the end effector with respect to the target. But, yeah, so we don't have much generalization beyond that. Yeah.
00:39:43.892 - 00:40:03.392, Speaker D: My second question is, has. Have you thought about, or has somebody thought about, instead of teaching a trajectory per se, rather teach the physics of the actual goal state versus the start state? Like, if you teach the robot that applies pressure to the bottom of this lid, it'll close, and then it can maybe figure out their trajectory.
00:40:03.488 - 00:40:03.840, Speaker A: Right.
00:40:03.912 - 00:40:09.872, Speaker D: So the human isn't teaching how to use your arm. It's what do you want to use your arm to do? What you said at the beginning.
00:40:09.928 - 00:40:33.774, Speaker A: Yeah, I mean, I think that's part of the trade off between what do you want to. How do you. I think somebody. I can't remember. This has come up earlier in the week, and I'm blanking on which talk it came up in. But, you know, the reason we want to do these demonstrations is it's actually hard for us to decide exactly what to write down like that, you know, so we might write down that. I think it was Anka's talk.
00:40:33.774 - 00:41:20.196, Speaker A: We might write down that we want to apply this pressure at the lid and in this direction, and that may not be all that you need to write down. Right. And so demonstrations are kind of a stand in for doing the empirical work of, like, actually writing down the control function. And so it's this trade off between, like, oh, I can just give a few demonstrations versus, oh, I can think really hard about the physics and write down the control, write down the differential equation. And so that's kind of the constant trade off in this learning from demonstration space. To build on that, maybe I missed it. But it seems like one limitation of learning from demonstrations is that there might be a better optimal way that people don't know how to do because they don't really understand the physics of a robot.
00:41:20.196 - 00:42:17.204, Speaker A: Yeah, there's another way that the robot could do it much better. Yeah, yeah. And that gets a little bit Jerry's question, too, where? So I think that, you know, currently we're using, we're starting from the action model still and saying like, oh, well, you know, this action model must be in the basin of attraction of what's optimal to do. And I just need to use my objective function to search around a little bit. You know, if you throw away that assumption and you say, if I don't believe that this action model is anywhere near the basin of attraction of what to do, you could still try to glean from the objective function, like what are the core constraints of the task that this objective function is trying to communicate. And so that's a step further that we haven't done exactly, but I think it is encoded in the information that we have.
00:42:19.344 - 00:42:47.490, Speaker F: I'm wondering about the trade off, your design choice to say that the goal is a trajectory versus the state. It seems like maybe there's a trade off there, that if I said the goal is being in this state, then I'm kind of being agnostic about the procedure you use for achieving that. Whereas if it's a trajectory, I'm not being agnostic. But on the other hand, I'm helping you by giving you a whole sequence of. Yeah, yeah.
00:42:47.522 - 00:43:33.748, Speaker A: It's interesting. For the goal, we are explicitly only giving these keyframes. So we're assuming that something about the way that you've decided to divide up this skill is important and is salient in the goal space, but that is a design choice. And I think it is true that some keyframes are superfluous. Right. And you should not necessarily think of them as hard constraints. And this, if we are taking and converting this goal model that has this set of intermediate states and we actually convert that to constraints, we're going to obey those constraints, and that may be too restrictive.
00:43:33.748 - 00:43:49.272, Speaker A: But sometimes treating just the end state as the goal, we might miss something. Depending on the task, you might miss something that you meant to do. So I think it's a great question, like which ones are crucial versus which ones are just things that I told you to do because it was on.
00:43:49.288 - 00:44:08.604, Speaker C: The way to follow up on this question. When you look at the actual data that you have, if the human teachers have variability, I would imagine in the first few keyframes, you see large variants, but at the very end frame, like, everybody has to get the kind.
00:44:11.734 - 00:44:28.726, Speaker A: Yeah, yeah. And that. So in other work, we followed up on that, too. To drive the kind of information that you might try to get out of people, you might say, if you see a lot of variance, maybe that's not even a constraint. It's like not even a thing. Just don't even worry how you start. And you can ask somebody that.
00:44:28.726 - 00:44:51.004, Speaker A: If you see a state that has a huge amount of variance, you just say, does it even matter where I start? And so we've used the kind of variance that you see in these models to drive active learning questions like that. Like, should I just throw this whole constraint away? Or, you know, if I see a really tight variance, I can ask, does it have to be this? And then you know that it does.
