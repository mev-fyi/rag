00:00:00.120 - 00:00:05.662, Speaker A: Diego will tell us about site tight, semi definite program relaxations for polynomial optimization.
00:00:05.758 - 00:00:52.588, Speaker B: Okay, sang Brundt, sang the organizers. So, as you can see, the topic of my talk is about tighter relaxations for polynomial optimization. Well, it's not specially about hyperbolic polynomials, but, however, if we are optimistic, we can think hyperbolic polynomials as a special case of polynomials, then we apply. Okay, so, so what, what is the biology? Well, this is a polynomial optimization, is. You can see the, this question. So, suppose we have what, first of all, minimize effects. So, this is called the objective, and we have the constraints.
00:00:52.588 - 00:01:20.248, Speaker B: Well, in life, we might have both, I mean, equalities and inequalities here. I mean, for convenience, let's consider the inequality case. So, I mean, of course, in this talk, we assume everything are polynomials. Okay, so, this is called the polynomial optimization. So an x is n dimensional vector. Okay? So, but as you can see, this one is a special case of nonlinear program. So that means non linear program is very broad.
00:01:20.248 - 00:01:45.774, Speaker B: So minimize nonlinear function subject to all kind of constraints. So, it's a special case. But, however, since we have a strong properties here, everything is a polynomial. So we want better, we want better conclusions. So, especially for polynomial optimization, we want some kind of the problem to be solved globally, for example, to find a global minimizer. But of course. Well, the problem is why you can use this, Xiaomi.
00:01:45.774 - 00:02:08.474, Speaker B: That'd be hard. Actually. It's difficult. So that is. But what I'd like to mention that, I mean, for polynomial optimization, I mean, also, also for nine year program, to find the local minimizer is still hard. It still might be hard. So, actually, what I mean, if many conference, many people, they get excited when they are.
00:02:08.474 - 00:02:44.558, Speaker B: Okay, I find local minimizer, though, it's over, it looks like so. But, however, that is not true. So, to find log minimizer, I mean, in competition, maybe in practice, maybe easier, okay. But theoretically is still difficult. So, polynomial optimization exists. Huge, many methods. But the one I like most is the one that is proposed by lasalle, that is called the moment so's hierarchy, or is called lasalle hierarchy.
00:02:44.558 - 00:03:07.942, Speaker B: I don't know what is the number. So it's so, I mean, usually that means. Well, you can actually say the two versions. One is a moment version, one, the dual version, the sub squares. So the, basically, you, how do you represent a polynomial that is non active on a set that use, for example, use positive sense, using positive sentences. Or there's another one we minimize. For example, we convexify this.
00:03:07.942 - 00:03:32.504, Speaker B: We minimize the linear function over the convex car, over the vector from monomials. That is another version called momentum version. But interestingly, these two versions are dual to each other. So that's why you should be all quoted the moment as our hierarchy. And for the moment has a hierarchy. Actually, it has actually one nice properties. The first one is asymptotical convergence.
00:03:32.504 - 00:04:05.856, Speaker B: So that means here we are super hard. There is a condition for the Archimedes condition. So that means the quadratic module generated by these constraining polynomials is Archimede. I mean, this kind of something that is essentially to require the feasible set is compact. So that is, after that, that is, I mean, the cell produce a symptom convergence. That means we apply a sequence of relaxations and each relaxation gives a low bound of the optimal value and the lower bound converge to the true minimum value. And the proof actually use polynomials, positive sensors.
00:04:05.856 - 00:04:53.672, Speaker B: That means the polynomial is not actually has a representation using sum of squares, but actually, well, actually we have better conclusions like this finite convergence. Actually, that means you consider this as a non linear program, so we can write down the optimality conditions. We have first order conditions, strict complementary and second order conditions. So under some, for example, second order sufficient conditions, and the strict complementarity, then this hierarchy, the moment srV, finally, finitely many steps. So that means if the relaxation order is high enough, then the problem is solved exactly. But here we assume the SDP is solving exactly. But if the SDP is not solving exactly, then, well, we cannot do anything.
00:04:53.672 - 00:05:34.688, Speaker B: There's no conclusion. So, and beyond this, what I'd like to say that is you usually, I mean, to solve this optimization, usually, I mean, when we apply the relaxation, we can get a low bound, but also, in life we are usually, we are more interested in the minimizers. So the minimizers can also be extracted. So that means can be actually the truncated moment problem. It can be solved by singular value decompositions and cholaski and so, but how? It's also a subject. I mean, I mean, when we talk about the polynomial optimization, we must make some reasonable assumptions. For example, in this case, we need to assume the number of minimizes is finite.
00:05:34.688 - 00:05:56.150, Speaker B: So you have the, if the set of minimizer is you finger design, I don't know. It's very difficult. I don't know how to handle it. So these are the advantages, actually. Well, but of course there are also some disadvantages. So the first one is as a computational compact computation expense. We have, it depends on the order, the relaxation order.
00:05:56.150 - 00:06:26.074, Speaker B: So the relaxation are also the degree. So when we apply a representation for the sum of squares type representative, so we insert, set up a degree. So, so that degree called the relaxation order. So if the usual one, we applied the lowest order relaxation. It works very well. But however, theoretically, to solve the one future, we need the reduction order to be high. I mean, well, I would say it depends on the problems, but theoretically you can be very high.
00:06:26.074 - 00:07:07.178, Speaker B: So, this is the computational issue. Another one is about theoretical disadvantage. So, that is discovered by Klaus Schweider. So he always have very amazing results. So he, so he proves that if the dimension of the feasible set is three or higher, okay, so for this, so this case, then there always exists some bad polynomial. So they always extract some, some polynomials, some bad polynomials, that this hierarchy will fail to have finite conversion. For example, we consider for the cubic, I mean, for the cubic case and for the cubic dimension case.
00:07:07.178 - 00:07:33.514, Speaker B: So minimize, for example, minimize the masking polynomial over the unit ball. So it does not have finite convergence. Okay, but how about, what if we use the software like gropley poly, you will converge in two steps. So the reason is a numerical issue. So, because using a multiple polynomial, you probably know degrees six, right. If we extract the linear moment, you give ten to the negative, then very easy. But however, that is, that is not mathematical true.
00:07:33.514 - 00:07:52.674, Speaker B: Okay, so, but numerically, we don't see that. So that's why. Okay, so these are the, I would say yummy. The lowest order, let's say, usually is often tight. So it's actually is for the most of time, it works very well. But however, you all these kind of things util for the worst case. That is hard.
00:07:52.674 - 00:08:35.594, Speaker B: Okay, now what can we do? It looks like the polynomial is done. Well, actually, what actually, when we think about actually, there is still a lot of issues that we need to consider. So the first one was a symmetrical convergence. For example, we need the Archimedean condition. So the quadrant essentially requires, the feasible set is compact. So if the feasible side is, is unbounded, for example, minimize the polynomial in the entire space, how can we do so then? In this case, then we don't have convergence. So the natural one is, as we observe, the computational cost depends heavily on the degree.
00:08:35.594 - 00:09:00.694, Speaker B: So that means on the relaxation order. So this usually what in the competition we often want. We need to try our best to avoid high degree relaxations. Okay, we need to avoid that. But there is an issue for fixed relaxation order. For example, the lowest one. What is the best relaxation that we can construct? So, that is kind of usual.
00:09:00.694 - 00:09:35.094, Speaker B: We cannot avoid. But how you practice, we can, we should try our best. What is the best one? That we can do so also, that is why also another way concept is about the convergence. So that means the moment that is actually is proved to have finite convergence. If there are some, I mean, we still need the Archimede condition, okay? Plus some optimatic condition. So especially about the strict complementarity or the second order sufficiency conditions. So, I mean, these conditions are from non linear programming.
00:09:35.094 - 00:10:17.646, Speaker B: So if they are, you are satisfied, then, okay, we can prove they have a finite convergence. But how is it fair that what can we do? So, here is the question. The third concept is about cell radically. So, can we construct a new hierarchy that always have finite convergence for all the objectives? So that is, I mean, this one is a biosol like, so, I mean, is actually what the topic of today actually is. I'd like to, to present some work about these three issues. Well, first, let's review, I mean, let's review the polynomial optimization. So, let's say we consider, first, we consider this as a general nonlinear program, okay? We don't use any properties of polynomials, but we consider as a non programming.
00:10:17.646 - 00:11:02.922, Speaker B: So let's say what can you. So in linear programming, if you take any optimization course, then you will see that is a KKD condition. So what KKD is that means, is called can rush target condition. So that means you suppose we have a local minimizer, okay, local minimizer U. So if we have for example, the linear independence constraint and quantification condition, so is what Lic QC. Then we have this, then we have Lagrange multipliers that, divided by this, that means the gradient of the objective is a linear combination of the gradients of the constraint functions, okay, and the coefficient lambda lambda runs through lambda called lagnology multipliers. And of course, well, you can see if the constraint is active, then lambda may be positive.
00:11:02.922 - 00:11:26.956, Speaker B: But if the x strand is inactive, that means lambda must be zero. So that means the product must be zero. So it's called as a complementarity condition. And of course here was the inequality constraints. Then Lagrange multiplies are required to be non negative, okay? Required to be non negative. But of course, if we have equation constraint, then lambda can be positive negative, okay, it doesn't matter. So, well, here is l I c q.
00:11:26.956 - 00:12:00.038, Speaker B: So actually this condition can be weakened, is called MF condition. That means you can be weakened, but you should have. But actually where is different. But how was the difference is not that much. Okay, so you, so that is according, okay, so that means from linear programming. So, so this is what for the active constraints, if the lambda are positive, then we said you have a strict complementarity. Yes, complementarity is meant for every I.
00:12:00.038 - 00:12:22.456, Speaker B: The lambda is positive, CI is positive. Well, if CI is positive, the lambda must be zero. So if CI is positive and active, you don't care. So I mean, you say suppose all the constants are inactive. So that means all CI are positive. So that means we still see the strict common magnitude holds. Or if we have equation constraints, then lambda.
00:12:22.456 - 00:12:44.458, Speaker B: There is no sign that we still say the strictly complementary. So my only case about inequality constituents that are active. So this, so these are called the standard ones from the nonlinear problem. So, okay, now let's see. Kgeti condition, actually. Well, you can read the condition is, I mean, it's simple, just say so. You'll see.
00:12:44.458 - 00:13:15.840, Speaker B: So, so what the condition is. So, I mean, these are the equations, okay, so you can see if x. So for kd condition. So if x, if the value of x is null is given, then to determine the lambda. That easy, right? That is a linear program actually, right? You want to find lambda, satisfy these equations, plus this non electrons is a linear program. Or if for example under say I c u c, then we just need to solve a linear equation. That's it.
00:13:15.840 - 00:13:30.672, Speaker B: So easy. But however, yeah, we can do more actually. But how is that? Yeah, the difficult optimizer. We want to find x. We don't know the value of x. So that's the difficulty. Right? So you see, what can we do? Actually, there is actually these actually is a mathematical question.
00:13:30.672 - 00:14:10.566, Speaker B: So is suppose what we suppose, I mean, f and c, is that all given polynomial functions, but here we don't know x. Okay, now the issue is we want to set. So here's a question to how can we represent like function multipliers in terms of x. So how do we interpret the lambda in terms of the critical point x? So here the question is, does there exist a polynomial p p one through p m? Sub that is lambda one us p one us p m. So that this one is true for all critical points. So if x is not a clear point, of course we should forget it. I mean, that's not possible.
00:14:10.566 - 00:14:42.624, Speaker B: But how is, well, you can see us here for axis given, you can see this one, this equation is linear in lambda. So we can what we might nature to think, what if we have linear equation in lambda? Came to, we just reverse it, right? So we get the rational representation that is automatic. And it's obvious, true. So, but how is, okay here, I mean, is, I mean, in computation, especially in optimization, we don't like, for me, I don't like the rational function. It's not a function. Okay, I want a polynomial. Okay, I want a polynomial.
00:14:42.624 - 00:15:08.380, Speaker B: So it's doesn't can be expressed lambda as a polynomial, functions over the critical parts. So, but if this one is true, some, you can see that's a lot of, there are big advantage. You can see. So, first, the KGD conditions can be simplified. For example, we don't need lambda is a plug in what? So we eliminate variables. So that is one advantage. Another one is we also can use the sign condition.
00:15:08.380 - 00:15:31.316, Speaker B: So that means lambda, they are non negative. So that means we can pose new conditions. So important, normal optimization, the more constraints, the better. I mean, that's bad, but we want less, as less as possible, because you cannot handle the general case. But however, in polynomial optimization, we never need to worry about that. If you have more equation or more constraint, that's better. So we can apply stronger relaxations.
00:15:31.316 - 00:15:58.354, Speaker B: So that, so how you do. Actually, that is actually, that is possible. So how can we do. So that means, so the main result, so I'd like to present is that, so, okay, so this is a kt. So here is the first conclusion. Is there always exist polynomials so that the lag launching multiplier can be expressed as a polynomial function for all critical points. But how about where is the free lunch of any sum assumption? The option is very general.
00:15:58.354 - 00:16:31.704, Speaker B: So is, so the constraint polynomial is non singular. I mean, in a major geometry, people like non single radio is assumed that actually, but this is, but is a very big assumption. Okay, and what we can do, of course, once we get this representative, we just practice in. Okay, so we can, we can remove, I mean, for this equation, we can remove lambda, okay, and then we also get this new inequality constraints. And then for g, once we get more, more conscious, that's better. We can apply the moment. So as the hierarchy, we can get more, that means we get tighter relaxation.
00:16:31.704 - 00:17:01.346, Speaker B: So that's the basic idea. So important optimization. Okay, we should never worry about more constraints. The more the better. Okay, how do we do that? So actually what k is issue, we observe this. So, I mean, the KD condition can be summarized as this, a single equation. So if we, while you support x is given, the lambda can be determined by this equation, you'll see the first one is, you know, is about the gradients, but.
00:17:01.346 - 00:17:22.072, Speaker B: Okay, well, so here, the tricky here is that we need to apply the complementary condition. So that means you lambda, you know, the product of the inequality constraints. And lambda is equal to zero. So we just put them together. So we get a new, anyway, we get this new matrix equal to Cx. Okay, so Cx. So what is.
00:17:22.072 - 00:17:45.000, Speaker B: Well, first I'd like to introduce the definition of non single value. Nonsense. Actually li sec over the complex space. Okay, that's the point. So what sequence is that? Means we consider this. So that means this constraint interval is non singular if the rank of this matrix is full rank. So this one has m columns and it has m plus n rows.
00:17:45.000 - 00:18:16.702, Speaker B: So the number of rows is always bigger than the number of columns. So the one is always equal to m. So this, so the non singularity requires, the length of this matrix is equal to m for all complex points. So this means that if you have, say actually this y actually is equivalent to l I sequel c over the complex space. So what is that you say? Suppose all, for example, you suppose all the constraints are active. So that means the constraint functions are zero, the diagonals are zero. So when, now in this case, when is this guy have fallen.
00:18:16.702 - 00:18:45.190, Speaker B: So that means the radius of the constraining functions are linearly independent. But of course, if the diagonals are non zeros, then it doesn't matter because it's automatically fall rank. So that means, so this one is equivalent to, is equivalent to the ICU. But how about the issue I would say is stronger than that. Everybody requires over the entire complexes space. Okay, so this is a non singularity. Well, when is non singularity.
00:18:45.190 - 00:19:05.722, Speaker B: So how can you represent the lambda? So here is a trick. So you say this is a magic. See the same age symmetrics. Okay, so here you for exist a matrix polynomial Lx. So Lx is the matrix polynomial. Okay, there is none non rational functions. Okay, so that means if Lx is the left inverse of C.
00:19:05.722 - 00:19:46.636, Speaker B: So that means you multiply Lx in the left Kim's identity matrix. So if this one is true, then we are happy. So why you will see, this is cx times lambda equals the gradient of f, right? You multiply l from the left, then it's cancelled. So you give identity. So you see the lambda can represent it as a polynomial function over the critical points. So, so how you do? So that means, another question is when does there exist? So when does that exist? A polynomial says that is left inverse of c. So actually there is actually, the conclusion is very clear.
00:19:46.636 - 00:20:44.332, Speaker B: So this l exist if and only if this, the top of the constraining polynomials is non singular, but the non single. Actually, you'll see, actually there's basically that in the h vac if you have a generic system of polynomial equations, then you know it's non singular, right? So this condition is a genetic condition. So, and the proof actually is not too hard, actually, that is, we apply positive synthesizer because it just, because if, because we know this, this is always full rank. So that means if you consider, if we apply load reductions, element reductions for each time, the third column is, cannot have common zero. So that mass, that means the constant one belongs to that idea and we can apply that recursive. So now let's see some examples. So, let's say we consider the hypercubic congruence, so, hypercube, so you defined by any quadratic inequalities.
00:20:44.332 - 00:21:13.644, Speaker B: Okay, we have, and then we have this, actually c, this is a Cx user. This is a gradient. I mean, this is the constraint from. And if you do it, actually, what this guy is, if you multiply this one is identity. So I mean, I would argue to remark that for two express lambda, we do not need to know the active set. If you know, to need to know the active set, that's not convenient because we don't know what constraint are active. But here we don't need to know that.
00:21:13.644 - 00:21:53.820, Speaker B: So, linear constraints, linear constraints is, is similar. But I mean, the conclusion is the same for the, if all the constraint functions are linear, then, but we still need this assumption. This condition is non singular. I mean, in this y, what this is something like that means if we consider the polyhedron, that means it requires polyhedron is non generated. Well, but I would say this condition is stronger than that, because we require that, we require the non dangerous over the complex space. But how, but he only care about the vertex of the feasible region. Okay, so, so if this guy, well, for the linear case, actually we can get stronger conclusions.
00:21:53.820 - 00:22:22.564, Speaker B: That means the l, I mean, under this assumption, the l a obviously exists. But how we can bound the degree? We can bound the degree. So degree is m minus the length of a. So what is m is the number of the linear constraints. So, so if the, but they still issue a lot of linear constraints, then it's bad. So the degree is high. So these are the linear constraints and similar, for example, because as an example, this constraint simplex, simply that means everyone is non negative.
00:22:22.564 - 00:22:51.984, Speaker B: Summation is at most one. Okay, and for this is the cx. And then you can verify this guy's identity matrix. What if lx is this one? But how? In this case, you'll see, in this case, m equals m equals m plus one. So the subtraction equals one. So the degree is what? Okay, now let's do the optimization so that it's so, okay, now here is a polynomial optimization. Okay, we can, now here we need a subject.
00:22:51.984 - 00:23:17.052, Speaker B: We need make a subject. Okay, the constraint polynomial is non singular. So that means what then? But the order is that they exist. Lx, that is from the left, okay, that is l. Then we can express lambda like in this form. Okay, well, in this case, then we need to be very careful. So when we minimize the polynomial, actually with psychic condition is not necessary if the feasible set is unbounded.
00:23:17.052 - 00:23:43.460, Speaker B: So that means the minimum value, the infinite value is, may not be achievable. Actually, in our first paper, actually, we mentioned this issue, but in linear programming, the people don't worry about always assume it is a tube. But however, for the polynomial optimization, if we don't know, actually, this question is very difficult, so easy. But here I need to assume it is achiever. It is not achiever. I don't know. I mean, it's supposed to be rather hard.
00:23:43.460 - 00:24:07.476, Speaker B: I don't know. So is that if you achieve that easy, then we can write down the KD condition, but the condition that usual. Okay, we write down, that means, okay, these are the original constraints. These are the original, all these KKD conditions that return the conditions. We are slow eating. So as I mentioned, in polynomial optimization, we never worry about more constraints. The more the better.
00:24:07.476 - 00:24:31.116, Speaker B: Now we add more. We add more, because if we apply more, that means we can get a better relaxation. So we get it, because what the issue is now in the says, I mean, if we non singular, then lambda can be expressed as polynomials. Let me just throw it in. Everything is a polynomial, okay? And then we do it again. Then we apply the moment so's hierarchy, so we can do it again. So, so these are the constraints.
00:24:31.116 - 00:25:03.870, Speaker B: Actually, this is here, I'd like to, I mean, I guess I assume all of you guys are bored with so's version here. I'd like to present the moment version. So, moment version, that means what? You linealize everything, okay? You line the polynomial, then you have this, and then you have, I mean, for equations, we have equations, okay, we have linear equation constraints for the polynomial inequalities that we have localized the matrix, to be positive, PSD. So these are the localized matrix. I'm not going, I'd like to skip the details. I mean, it's not a fun. So now what is, these are directions.
00:25:03.870 - 00:25:41.344, Speaker B: That means we have more constraints that we just apply it. So that is, so what is the convergence? Well, I'd like to say, the major conclusion is that, so actually this is one, actually, let me say. So if the quadratic model of the constraint is Archimedes, that, you know, then this sequence always have finite convergence. So that means we, but here that means, okay, the subject, the, the quadratic model of C is archimede, but however, there is no assumption about f. Okay, f can, can be very bad. Can be very good, can be bad. Can we have, can we have for example, infinite, many mini methods, always tight.
00:25:41.344 - 00:26:11.786, Speaker B: Okay, but so actually here, why you gotta map, why do we need the argument condition? Because the reason is, because here I don't use the pre ordering. If we use pre orders and this conception can be removed. Okay, so that means you have, you consider here we have several polynomial inequalities. So if use the pre ordering, so we don't use cross products among them. But here we, I don't use that. So that, to avoid that. So that means I need the, the quadratic module due to the Archimedes.
00:26:11.786 - 00:26:49.402, Speaker B: But how, let's say this candidate can still be weakened. For example, actually I only, for example, if we consider this, the KKD set, for example, the kd set, if this set, for example, is finite or is some other properties, that means you can buy third one is 30, is a clean condition, okay. This is a clean condition. But however, this subject can still be weakened. Okay. And also, I mean, also there's another problem here. What I mean is what, because this, the minimum value is not, now how do we, how do we know this value is tighter than that? Where that means what we can apply flat extension or flat truncation.
00:26:49.402 - 00:27:06.974, Speaker B: I mean, that is, we consider the minimize of this guy we apply to consider as a subsequence of y. And then you check the run condition. Actually there is a result. If this, if the set of minimizers is finite, then we are happy. If the study is not fine. I don't know. I mean, it's no conclusion.
00:27:07.094 - 00:27:16.006, Speaker C: So Jerome is, this is the variety, does this sort of assumption mean that the variety is going to be finite.
00:27:16.070 - 00:27:29.830, Speaker B: Or is something, it's bounded. Actually, I don't, well, actually something can be, we can do this. This one is bounded, just bounded. Yeah, that's finite. Why just bound it is. Okay. Actually this kind is Archimede.
00:27:29.830 - 00:27:45.942, Speaker B: If you consider this as a set instead of constant, if you assume this, mine is alchemy and then. Good. But here, I mean, here, let me write it because this one is clean. Okay. Is only depends on second the c. But here, if I, here that means depends on f is not what you find. So.
00:27:45.942 - 00:28:07.378, Speaker B: But yes, you're right, we don't need to find it. So if this one, we consider the order constraint, the other constraint in polynomials here, if the quadratic module is like a medium, that's the same conclusion. True. Okay, the same conclusion, true. But here then. But how was it? I mean, the thing is here is that it depends on us, even attractive. So not verifiable.
00:28:07.378 - 00:28:29.844, Speaker B: So that is young application is not convenient. But how does that conclusion is very clear. So we only need to suffer the constraint polynomials is architect. Okay, for example, we minimize the polynomial over the hypercube. So you always have finite. If you use this hierarchy, then you always have finite common. So no matter the object is good or bad, it doesn't matter.
00:28:29.844 - 00:29:11.780, Speaker B: Did I answer your question or is that is asymptotic? No, no, that's not true. That's because of shadow. Is that if the dimension. No, that's not true. Does it not have a cube? I don't know. Because I would say in my cc is the standard hypercube, right? I mean, that is. That is what Boolean? I don't.
00:29:11.780 - 00:29:22.714, Speaker B: That would be good. Boolean set. Okay. Yeah, you need study is finite. Then is. I mean, Monica Lord has a long walk. That is obvious, right? That is.
00:29:22.714 - 00:29:38.006, Speaker B: So this is. Let me say this is. I would say this condition can be weakened. But how this is neat. It only depends on the constraints. So now let's see some examples. So this what I want to say, okay.
00:29:38.006 - 00:29:55.450, Speaker B: The example being very careful, you may choose a random polynomial. That said, original x is already tied. So there is no advantage. But how here, choose some special polynomials. Okay, so this one is. You can see this one is mosquito. So this guy is what the reason is that the mosquito polynomial is.
00:29:55.450 - 00:30:09.434, Speaker B: Has infinite minimizers. Okay, that's better. So that's why. Because I want to make sure is detectable. So that's why I add this coercive term. Okay? That's the reason. So that is why me after everything is satisfied and do it.
00:30:09.434 - 00:30:25.896, Speaker B: And then you see the bound actually for the k equals three or four, the direction. I mean if you don't, because this side is unbounded. So this guy is the classical moment. That's how the dictionary is infeasible. What do you mean? Not infinite? Is unbound from below. Okay, it's Omar from. But how he goes five.
00:30:25.896 - 00:30:45.076, Speaker B: Actually, the numeric issue, I'm not sure is really infeasible. Not because it is very big, because of a numeric issue. I cannot tell. But for this one, looks like if we apply the log long chain multiplier, then it works faster. Why? I mean, numerically. Well, here everything is about numerically, because we can. We don't know.
00:30:45.076 - 00:31:01.844, Speaker B: The Sb cannot be sold. Exactly. I mean, there is so easy. But how actually, the. So, this is one example. So let's see, there is another one. So, this guy is called something like Robinson polynomial, but I replaced x I squared by x.
00:31:01.844 - 00:31:20.490, Speaker B: Okay, so that is so. But here, this is non negative, right? Here is some what, these are nonlinear constants. I want to the exam to be more interesting. So, this is l is just do it. And still exists the minimum value. Okay, this is a comparison. I mean, for k equals two, where k was two, there is no advantage.
00:31:20.490 - 00:31:46.770, Speaker B: But for k equals three, then is big. Okay, so, I mean, how equals three is tied numerically? Okay, so let's say y. So this. Well, actually, this one, if you see the Rasnik's paper about the survey of some squares. So, this polynomial is very interesting. Actually, this polynomial is non active, and it has a lot of minimizers. It has a lot of.
00:31:46.770 - 00:32:06.146, Speaker B: If you have. Why, if you are interested in test on performance on polynomial, to mean this polynomial is very good. It's non negative and it has a lot of minimizes. And here, I mean, here I even, I added a coercive term here. So I still have this many. If I don't have this, I don't know, maybe it's infinite money. I don't.
00:32:06.146 - 00:32:20.496, Speaker B: I didn't think about it, because this bond polynomial is very tough. And then if you just read, of course, the feasible region is non compact. But how. We have a cursory term here, and if you just apply it. So we got this. We have, actually, we have eleven minimizers. Well, it's not symmetric.
00:32:20.496 - 00:32:31.336, Speaker B: Okay. It's not. It's not the meaning, the set of minimizer is not symmetric. So you love it. So it's not even. So, you see, these are comparisons that we have. Well, if we.
00:32:31.336 - 00:32:41.264, Speaker B: If we don't use lambda, okay, then we have this. If you use lambda, then you converge very quickly. So easy. Okay. Second one is always. The time is also less. Is that bit less? So easy.
00:32:41.264 - 00:33:06.384, Speaker B: So the reason is, as I mentioned, okay, in polynomial optimization, we want more constraints, okay? The more the better. So, actually, don't worry. Even if you see that computational time actually is less, when we add more constraints, the computational time is less, not more. So. Okay, let's see this one. This one. What is this polynomial? Yes.
00:33:06.384 - 00:33:24.544, Speaker B: Is also from last week's paper. So this is not some square is not next, but not sum of squares. Anyway, for this constraint. So it's what we can verify that l exists. So you can express it. So we do it so that you, the minimum value is zero actually. But we can verify exactly.
00:33:24.544 - 00:34:05.640, Speaker B: So, but how you. For numerical performance, we have this, okay, so you'll see, converge very fast. So, but you can see the time wise does not cost more, you see, or even less, I mean, but not too much because why we have, you still have the polynomials on it. Okay, now let's summarize this. So the conclusion is that, so let me summarize lambda here, the bh, you need to use the tricky use of lambda. Okay, we express lambda as a polynomial function, okay, over the critical points, but how are there is that subject. So we assume the constraint, the set of constraint polynomial is non singular, but this assumption is a genetic assumption.
00:34:05.640 - 00:34:39.828, Speaker B: So use one. Then we use this expression, we just do it. Okay, we get more constraints as a new constraint. We have finite converges on the sum condition, but these conditions are the c, for example, if the physical study is quite archimedes. Okay, but not, I guess I assumed all. Okay, if you don't know, then concern me. Okay, I have legit.
00:34:39.828 - 00:35:04.354, Speaker B: Okay, so, yes, you are not happy about this. Okay, I can explain it. Okay, we have enough time, so use, okay, so it's, uh. Okay, so we just use the expression, we just plug it in. Okay, and so, well, I'd like to mention that. So that means the construction, use the KD condition recognition. Why? Depends on the objective.
00:35:04.354 - 00:35:23.884, Speaker B: So sometimes this is not useful, but sometimes it can be not convenient. So there is also disability. I mean, there is no free line. Okay, so it's in my. So depends use the objective. But on the other hand, we minimize something, we should use the object function. And if you don't use the how to minimize, right, that is also another argument.
00:35:23.884 - 00:35:52.996, Speaker B: So there is another issue. I'd like to mention that actually, I guess many of us have spent a long time how to construct a uniform SDP. That's all optimal polygon optimization. So we spend a lot of time, actually, that is not possible. So it's by the, by the result of cross straddler. So I should keep again even for the second of non aggregate polynomials, that is not some squares, that there is no STB representation. So that means we cannot for, even for that case, we cannot solve it.
00:35:52.996 - 00:36:02.144, Speaker B: So, but however, you can use the objective, there is possible. So that is, anyway, so that's it. So I finish my talk.
00:36:06.724 - 00:36:37.584, Speaker A: Thank you very much. I think we have enough time for questions, so let me try the boring group method one more time. Time being european, now I'm going to put my european hat on and try some good old fashioned age discrimination. Anybody age 33 and under, you can ask, this is chronological age. So if you are 33 under or you think you're 33 and under, you can ask a question to Xiaoang. And I have already one question in mind that you can ask him how old he is.
00:36:44.664 - 00:36:50.324, Speaker B: Actually, I want to remember.
00:36:52.304 - 00:36:55.648, Speaker A: Anybody want to ask about the definition of non Archimedean?
00:36:55.776 - 00:36:59.284, Speaker C: What's the definition of a non Archimedean? Quadratic modulus.
00:36:59.984 - 00:37:03.276, Speaker B: Please define non archimedes.
00:37:03.440 - 00:37:03.756, Speaker C: Sorry.
00:37:03.780 - 00:38:06.174, Speaker B: Archimedean is harder. So Archimedean is Archimede. So that means is we have polynomials. This guy is Archimede. If there exists a polynomial p, and I mean all these guys are sum of squares, okay? They exist, the sum of squares, sigma, zero, sigma, one through sigma. So that, so this set is compact. That means there exists one plutonium type of representation such that this single inequality, this single inequality gives a compact set.
00:38:06.174 - 00:38:40.514, Speaker B: Oh, I mean, actually that's the equivalent condition. Or this one is equivalent to there exists some scalar r minus the summation of squares of exercise belongs to, belongs to the quadratic module. So that means this guy is, this equals this form. So in other words, so is this some r minus, some minus the sum of squares of x that can be represented this way.
00:38:43.214 - 00:38:51.184, Speaker A: Now that we know how old Jia, Wang and Santhia are, anybody 44 and younger interested in asking a question?
00:38:54.244 - 00:39:06.668, Speaker C: So nist, can you go back to the theorem you had with f where you that used this archimedean, you said that if the quadratic module is Archimedean.
00:39:06.796 - 00:39:08.264, Speaker B: So this is only one.
00:39:11.364 - 00:39:26.768, Speaker C: If the quadratic module is Archimedean and no other assumptions, then for every objective function, every objective polynomial, does that say that? Then the semi algebraic set is a projection of a spectrohedron.
00:39:26.816 - 00:39:46.760, Speaker B: No, because I, you see the contract here? That means use the, you look at the constraint here. It depends on f. So the UC is 100, it depends on fucking k. Depends on. So it depends on f. Yeah, it's a k depends. So that means the feasible circuit depends on f.
00:39:46.760 - 00:40:01.324, Speaker B: So that means it's easy. Otherwise, otherwise it's close strand, we have trouble with zero. So it depends on epsilon mass. Construction depends on the objective.
00:40:01.784 - 00:40:07.704, Speaker C: But for a given semi algebraic set, you know, you only need so many polynomials to define it.
00:40:07.824 - 00:40:42.516, Speaker B: So define part here the issue here is minimize here that you use to minimize the, to minimize the fun. Is that different? You mean by you only to care about a subset of it, right? So, but here is, obviously this one is only a sufficient, it can be vegan, but however, is neat. So that is. So the reason you, for, the reason for using archimedes is that we don't use the pre ordering. So that means because we have, we might have several inequality constraints. So we don't use that. We don't use that.
00:40:42.516 - 00:40:59.104, Speaker B: That means we need, use algebraic condition. But how. Actually there is another version of the theorem I did not present. So that means if we use, if I, we are allowed to use the preordering, so we don't have assumption. So we can remove this assumption, so that.
00:41:00.944 - 00:41:12.488, Speaker A: Irrespective of chronological age, anybody is invited. Yes. Put in our representation.
00:41:12.576 - 00:41:36.832, Speaker B: In fact, you can see the sum of square weights in the root in alpha. Yeah. Yes. As polynomial in x, which behave like range multipliers. Yes, exactly. In fact, you do more because you, you would explicitly take at the optimality condition. Yeah, yes, but it's a huge, I mean, the advantage is that there is a.
00:41:36.832 - 00:41:55.254, Speaker B: Well, if we can think that. Yeah, that's because there is uniformity. We bound for that. But if, but if you use, if you use the original polynomial representation, there's no uniformity. We bound on that. So that's why we, in the first case, we have, we have only a symptom, convergence. So that's the difference.
00:41:55.254 - 00:42:11.534, Speaker B: So, I mean. No, this is pro, for example, minimizes the mosquito polynomial over the unit ball. It does not have finite convergence. So if you prove that. But numerically that's different. Okay. Numerically, you converge in two steps.
00:42:16.274 - 00:42:27.894, Speaker A: Okay. Any others? Not. Then we'll thank the speaker again. The next talk is at 330.
