00:00:00.240 - 00:00:13.354, Speaker A: Program is Professor David Donahoe from Stanford University. So I believe everyone in the room knows David and his achievement. So I will not spend more time on the introduction. Let's just start this exciting talk. Thanks, David.
00:00:33.794 - 00:01:05.034, Speaker B: Maybe really hard for me to get started. Nancy. And you have welcomed us into your home and family. I'm so sorry. And we really appreciate the friendship that you've shown over the years. I love the pictures in the slideshow. The picture of Madeline with you is very touching.
00:01:05.034 - 00:01:54.608, Speaker B: And you introduced us to her, and then we saw her as we went around Berkeley and would talk with her and interact, and it was just very touching to feel the extended family. We were invited, say, to passover Seders. We were invited to family weddings at the brazilian room. And so many more moments. But also, there's a lot of professional moments that sometimes by random chance, I happen to be around for those. And you may not be aware of all of these, but. Well, okay.
00:01:54.608 - 00:02:54.890, Speaker B: When you were elected to Nas, there was a party at the department here in Evans hall, same thing with MacArthur Fellowship. But actually, we were in Israel and saw you get your honorary degree from Hebrew University on Mount Scopus, looking down over the Dead Sea. Yeah, that just happened. Also, I had the good fortune to see you get an honorary degree at Etiha. And I guess there was the Nightingale, or whatever it's called, in Princeton. These are a lot of important professional moments, and somehow they've all happened in your life, but they've also happened in my life, and it's the fact that I've been able to be part of that really accumulates over anyway, my life. And so I also have to say thank you.
00:02:54.890 - 00:03:51.742, Speaker B: Well, to you for having all these professional successes. But a number of people in the audience made these things possible, so. And then there are people who are not here or no longer with us that I won't be able to mention. But, you know, as we saw, John Xing enabled the Princeton moment, and Peter Bohlmann enabled the ETH moment, and Lisa and all of her sidekicks have enabled this moment. And this is going to be very memorable to me, stacking on all the other moments. So. Okay, so this is kind of how I first met Peter, pretty much, I would say, around this age, with this kind of demeanor.
00:03:51.742 - 00:04:45.522, Speaker B: And he was always very calm. And I love what Lisa said, that he always assumes the best about people. It just feels like exactly that's who he is. And that's part of the magic, as we'll see that he's got a tremendous cast of collaborators and that wouldn't be so without the kind of psychological mojo that he puts into his relationships. So I really treasure the moments that he was counseling me in my early career. And I know there are many other people who had similar moments and were sort of a loyal band on your side. Another thing is that doesn't come from nowhere.
00:04:45.522 - 00:06:04.304, Speaker B: I think it's kind of important to realize that there was an earlier generation that had also very positive attitude. Part of it was what comes from being at the pinnacle. Berkeley in those days in many dimensions was so far ahead of so many other places that it was a kind of, well, we just heard about supremacy, but it was a kind of qualitative superiority that, you know, couldn't have been anywhere before, except maybe, I don't know, in math under David Hilbert, something like that. You know, I just love the look here of David Blackwell because it sort of indicates the alertness, confidence and so on. And you can see that Eric Layman is very kind person. And you can see Peter is young with this energy, and he's just a very positive person. Okay, so let's talk about Peter's scholarship.
00:06:04.304 - 00:06:52.104, Speaker B: You're at about 1000 citations per year in Google Scholar. Every year you've been alive. So that means you didn't get, although you started quite early, you know, various things have produced that and will continue to produce citations over time. And it was very interesting. I went through at this point so many dozens of papers, and there are a lot of papers in your profile I didn't even know about. And my favorites that I think are just incredible are sometimes not even visible. Like, they're just tiny little citations compared to some of the others.
00:06:52.104 - 00:07:54.544, Speaker B: So I just thought I'd go decade by decade. And you can see here, even so, this is in the Google scholar era, and there's a lot of things that have been erased because of Internet. And yet here are these papers that were written in the sixties by Peter, still being cited today at rates that would be significant for many members of the audience had they just been written now. And some of these are on robustness, but there's a great deal of variety here. And so I'll know more about the robustness story, but other people will know about lots of other things. And then we go into the seventies, and there are things on nonparametric density estimation. There's a data analysis project on sex bias and graduate admissions that became a total classic.
00:07:54.544 - 00:08:34.614, Speaker B: There's a variety of papers on robustness and so on. Then we get into the eighties. There's a canonical paper on the bootstrap, there's the foundation of the subject of semi parametric estimation. There's the squabble with Dr. Cox and George Box. There's a great foundational paper of Peter with Yaakov Ritove, which rocked my world when it came out, and so on. So there's a lot there.
00:08:34.614 - 00:09:28.034, Speaker B: But then it continues in the nineties, and I'm not an expert on all of these papers. That's actually the thing that's very striking about this. I thought I knew Peter. I realized I know a fraction, maybe a fraction of a fraction of Peter, and it's a lot of work to keep up with the scope of what he's been doing. So in the aughts this continues, and he's moved into high dimensional statistics and network models. And then in the last decade, I guess there's a reissue of his book with Shell, which is getting a fresh, huge collection of citations in that decade. He's also working in comp bio and getting tons of sites associated with that.
00:09:28.034 - 00:11:02.462, Speaker B: Maybe this is a reissue of the original work of Bickel, Goetz and von Swett that came out associated maybe with memorial volume, etcetera, but it's still a lot of sites. And then it continues into this decade. And I just heard about some work of his that I plan to cite. So there's going to be future citations. This is the curve over time, so that the bars here are indicating decade by decade. What's going on? Okay, safe. So you can see here total citations scaling up.
00:11:02.462 - 00:12:25.424, Speaker B: So 16,000 in the teens decade that we recently left. If we can just look at publications issued per decade and the max citations of any publication, it's all heading up and it's accelerating recently. This is like really, really impressive. I wanted to say that despite all of this, I have a lot of favorite papers. There's a lot of things that Peter has done that are impactful, but some of it is not high on the list of citations at this time. The world is interested in other things at this moment, but I thought I'd just mention in my own life what's been important. So, Peter, from his early citations on robustness that I mentioned to you, was, I guess, invited to participate in a Princeton robustness year, which issued the book on the left on which he's co author, and he got to interact with John Tukey, who was my undergraduate advisor, and about as different from the Berkeley way of doing things as could be.
00:12:25.424 - 00:13:37.010, Speaker B: And somehow Peter got along with him, collaborated, and something useful came out of out of this. This book, which, you know, I have a well thumbed copy, sort of described the first, you know, computational experiment on a whole bunch of conjectures that reached sort of a high scale and was trying to implement a whole paradigm of doing these things. I had never seen that before. I'd never seen anything like it in journals, so I found it fascinating. Now, was Feinhall there when you were there for that year? Okay, so there's Fein hall, the math building. Because of the success of that book, I guess the math department at Princeton and the stat group got funded, PDP Eleven, that was running the first Unix outside of Bell Labs. And by my sophomore year, I was the manager of this computer.
00:13:37.010 - 00:14:28.392, Speaker B: And so the success of that book, which put Princeton in such high esteem on robustness, got them this computer, which got me the job, which got me into statistics. So there are levels of impact that are happening behind the scenes here that you can't simply see from the citations. So my job was to write stat software. There was nothing at the time. So I wrote a stat package which was called ISP at the time. And what made it a useful package? Let's see if I have the right things here. Well, these are a bunch of papers that I learned as a sophomore and just like, you know, rocked my world and, you know, review articles on robustness.
00:14:28.392 - 00:15:16.294, Speaker B: And this paper is very important for maybe what comes next, characterizing asymptotic variants of lots of estimators and comparisons with variants of the mean and so on. Okay, but this paper, for those of you who think Peter, is not applied. So. So this paper is what I think made ISP possible. Namely it allowed to compute robust estimates. This is stat package, right? And the Princeton robustness study has just been done. So this stat software better be able to do something right in that dimension.
00:15:16.294 - 00:15:59.524, Speaker B: So what it did was based on this, or obviously it had various options, but this is one of them. And I remember carefully reading this paper, implementing it and so on. And I was just a wild junior at this point, and no one was telling me what to do. So I just did it. I implemented it and the software worked. And before I knew it, stat group at Princeton was distributing it all around to the unixes that were being proliferating. So eventually the Berkeley people, I think, with Jim Reeds, started using that.
00:15:59.524 - 00:16:59.980, Speaker B: So I think the popularity of the package was that it could actually do this. You couldn't at the time, really get robust linear model calculations, but you could through this package, which was by just happened to implement this because some crazy junior read this paper, then Berkeley took it over, and the Statlab was the worldwide distributor of the package, which they renamed BLSs. And there was a staff here that was maintaining it, improving it. They went far beyond anything I could ever have imagined. But a key thing is that in that staff, a person was Ross Ihaka. Ross was a founder of R. And so, like, what really happened here is the computability through this paper spawned a package which spread around.
00:16:59.980 - 00:18:37.848, Speaker B: Then it led to actually being able to pay staff, and it led ultimately to foundational work that gave us R by bringing more talent into the field. First just showing them a stat package that an undergraduate could have thought of, and then let's do better. So those are some consequences you may not have thought of, of your work, is just the ability to compute something can go through a series of chain reactions. Okay, well, a series of canonical papers that I became aware of as a graduate student and after is listed here, and they're all not about robustness. But the key thing that you have to know is that the first one has an important discovery of Peters. At about the same time, I think Boris Levit and Alfio Morazzi had something touching on this, which is the calculation of Bayes risk in a normal mean estimation problem with squared error loss is isomorphic to the calculation of the asymptotic variants of the kind that you would do in robust estimation. This came out of the fact that in the sixties, Peter had papers both on Bayes risk problems and on robustness.
00:18:37.848 - 00:19:51.876, Speaker B: He knew both of these things, so he could make that discovery. Now, the connection is deeper than, just, say, the exact value of the Bayes risk tells you the exact value of an asymptotic variance. It's actually that the exact optimal Bayes rule is rigidly connected to the exact optimal scoring function that you would have to be using to get the correct asymptotic variance according to this formula. So there's an actual isomorphism between robustness, granted in a scalar univariate problem, and Bayes risk for one observation, in that scalar univariate problem. And then once you understand that, or Peter understood that, you can do several problems all at once, and he does a few of these. Just speaking on behalf of Ian. Like Ian later has papers where he figures this all out in the Poisson case and so on.
00:19:51.876 - 00:20:57.080, Speaker B: A lot of other people at the conference might have been influenced by this. I'm just mentioning things I know personally, so if I'm forgetting to mention you, please, you know, we're trying to get somewhere in referring to some things, I probably can't refer to everything out of this, actually, there's a last paper here. So John Collins was here yesterday, but he had to go early, and I was speaking to him about this. So I think if you look at the third one of these, which appeared in Sankhya, it kind of summarizes everything that's needed from the others, if you can decode it. And there he's trying to just discuss the problem of, we have families of distributions, and we want to minimize the Fisher information over that. This comes out of a paper. It's not really a paper.
00:20:57.080 - 00:22:22.594, Speaker B: It's a problem that was posed in Siam in the problem section by Colin Mallows, who's a very distinguished statistician, applied mathematician at Bell Labs back in the day. And Peter has written papers on the Mallows metric, and he's written papers on solving this specific problem, and so on and so forth. Let's minimize the Fisher information over mixtures of distributions. So Mallows had an idea for one such problem and talked about, it's frustrating that he couldn't solve it, and here's a guess, and so on and so forth. So Peter and John's paper looked at that and several others, and they described that there's an interesting discreteness property of all such solutions. And they spoke about the connection to finding the least favorable distribution for over classes of priors. And they spoke about the connection to the Hoover problem of finding the least favorable distribution for contamination and robustness problems.
00:22:22.594 - 00:23:16.942, Speaker B: So they identified this as being really central to central problems in statistics, and they characterize things. Now, I spoke to John yesterday, and by the rules of whatever was true, whatever, how one played the game of publishing in those days, you might know something numerically about this problem, but you could never publish a thing about it. It would. There was just no outlet where anything, just silence. Unless there was a closed form, symbolic expression, then there's nothing. It doesn't exist. So what? One cannot reduce the closed form and prove, therefore, one must be silent, were the rules of the day.
00:23:16.942 - 00:25:26.000, Speaker B: I don't think that we have those rules anymore. In fact, I'll discuss how they've changed a little bit later. Okay, how are we doing for time? Okay, so I just want to, you know, so coming out of this, I'll just say it's influenced my own work in a couple of ways. And I don't, I can't list them all and so on and so forth, but in some way, I've worked on the problem that's associated with his first paper here, in comparing the minimax MSE, minimax Bayes risk over a restricted parameter space between a linear estimator and nonlinear estimator. And Ian Johnstone and I have looked at this particular problem as well. Peter completely defined the whole problem of what if you have a need to estimate really well at one given point, then how much does that hurt your risk everywhere else that's somehow isomorphic to what if you have a prior distribution, that's a sparse prior that has a great deal of mass at one point, and then the other mass is spread out in some way? This, which might not seem relevant to noise removal of sparse signals at like a course level, is like isomorphic to it. So the work that Ian and I did is kind of teasing out the consequences of that.
00:25:26.000 - 00:26:56.746, Speaker B: So it goes on and on. There's a lot, a lot that one could pull out of that line of work, and, and just not that much time now. But I wanted to say something that came out of those three papers for me anyway, which is a family of work that was done with Andrea Montanari and PhD student at the time, Arianne Maleki, who's now on the faculty at Columbia. And this was the development of an algorithm called AMP, which essentially what the algorithm showed is that conceptually, if you under sample, by not getting enough measurements, it's the same thing as complete data, but just adding noise. Of course, the measurements have to be collected in a special way, but that's exactly what compressed sensing is about. And so it was just transforming only collecting 10% of the data. You need to getting a significant amount more noise on a completely observed signal.
00:26:56.746 - 00:28:49.434, Speaker B: That's what this paper was about. Once you understand that, then you understand that if you have sparsity and you can do noise removal using some of the ideas that are kind of implicit in that work, then it would be possible to recover something. The unusual thing is the notion of phase transition, where in under sampling, you can do computational experiments that will show you that for a given amount of sparsity, which could be measured in some way on this axis, there's a certain amount of incompleteness, or under sampling, such that up to a certain point, you're going to, I'm assuming, noiseless data to begin with, but very under sampled up to a certain point, you can exactly recover the underlying object. So we knew these curves by, you know, experimental means, and there was combinatorial geometry and so on and so forth. But in this work, which includes Ian at this point point, what we found is that if you look at a curve of the minimax risk as a function of the non zeroness parameter, which is a thing that Peter first defined in the paper in the churn off best rift. Then, depending on the estimator you're using and what the rules of the game are, these curves actually gave you the previous curves. There's an exact isomorphism between the curves.
00:28:49.434 - 00:29:49.078, Speaker B: There's just a deterministic rule that turns, once you know this, you know the other one. And one thing that we found interesting is this red curve here, which is one of the things that Peter wanted, like his paper with John Collins wanted. Well, it defined. So we computed it, and we computed the solution of the mallows problem numerically and computed this curve. And by the old rules of publication, that has no status, it could never be published. Ah, but we turned it into science. Namely, we then went out and made measurements experimentally of actual algorithms and showed that they exactly fell along these curves.
00:29:49.078 - 00:30:48.626, Speaker B: So then it's a prediction, and it's solving, in this particular case, a non convex optimization. But today we would say it's actually fully rigorously solving it, as long as you're within this regime. Okay. And I could tell you more. This is all in the paper that I cited with Ian and Andrea. Now, at about the time that paper appeared, so Noureddine and Peter, Bickle and Bean, who I don't know, issued two papers in PNAs. And these were fantastic papers where they're doing m estimation in this high dimensional regression limit.
00:30:48.626 - 00:31:52.256, Speaker B: These are great papers. I just want to point out by citations, they're way down in the Peter oeuvre, even for the fact that they're less than a decade old. But, you know, if I were given 100 votes, I would give 100 to each of them, or 50, or whatever I'm allowed. So, Andrea and I understood this in a different way, maybe, or maybe the same way. But we understood that what we had done with approximate message passing and this equivalence between incompleteness of data and denoising was equivalent by the same isomorphism that Peter first discovered. Namely, the problem in robustness. In this case, robust regression, is isomorphic to a problem in Bayes decision theory.
00:31:52.256 - 00:33:23.864, Speaker B: In this case, noise removal from sparse signals. And the other thing that is kind of amazing is some things that in Peter's original one dimensional problem were a little bit like gaps, where there was a difference between the Huber contamination model and the kind of model that Colin Mallows was mentioning that disappeared, like, because there's some gaussian extra noise that we had seen in the Bayes side. Of the equation and that Peter and Noureddin had seen in the robustness side of the equation. So just because of that, we could write this paper which is just using the approach by approximate message passing to instead of doing a denoising compressed sensing type problem, it could be done. Do it for robust regression. Then, just to come back to Peter's, the first paper that I ever listed here was a paper that Peter wrote where he's calculating the asymptotic variance of the Huber m estimator and various other kinds of estimators. His annals of statistics, 1965.
00:33:23.864 - 00:35:31.112, Speaker B: And something that was very important in Peter Huber's mind was that the Huber m estimator could, its variants could never blow up in the same way that, say, a Humpel estimator or many other canonical estimators, you could contaminate them to make the variants blow up. But in this high dimensional limit, that's no longer true. I came home from, I think Peter's degree ceremony at Et Ha decided, hey, at which we had lunch with Peter Huber and Effie and said okay, that's what is going on here is that actually there's a relationship that Uber knew many years ago, but now in the high dimensional limit there's a different relationship, which is almost as simple, but that shows that the variance of the Uberm estimator can be infinite outside of a kind of phase transition. So that sort of tied together a package of my PhD advisor Peter's early work. Like a lot of things coming together, Peter's isomorphism between Bayes decision problems and robustness problems. Like, I know this is all sounding very vague, but it's a long, long story to get to every detail in the package. And then there's some other work which I've been doing a lot with, for example, Ian and other authors that's all inspired by this, and I don't have time really to get to it, I think.
00:35:31.112 - 00:36:24.144, Speaker B: Right? Isn't that true? Okay, so I've shown you these site counts and we've looked a little bit at the past. What about the future? Well, we're off to a good start in this decade. And maybe if we impute, we're about a fourth of the way through the decade. And if we impute, you know, by factor of four, then we'll see that you're at least at the same place. That's at least the current rate, which is unbelievable across seven decades. Acceleration, monotoning. Like, my mind is so blown.
00:36:24.144 - 00:36:54.470, Speaker B: So at this point, we're at a place where Peter has biographies written about him, like this one by Indian Statistical Institute for a talk that he recently gave. And you can, you can go read it. And it looks. It looks pretty like, you know, complete. That's. That's like a complete story of a scientific career. It goes on and on.
00:36:54.470 - 00:38:00.452, Speaker B: I'm just showing you the first page now. So recently, David Cox passed away. And in the journal significance and actually very recent edition, the COVID has this quote from Sir David Cox where he says, in a sense, the only thing that matters is if you can look back when you reach a vast, vast age and say, have I done something reasonably in accord with my capability? If you can say, yes, okay. My feeling is, in one sense, I've done that. Many people would agree. So the thing is, Peter's already at the stage where he can say that he's not actually, by my book, at a vast, vast age. I feel that the health and vitality seem to be there from the outside and maybe a good omen.
00:38:00.452 - 00:39:30.508, Speaker B: I find the following thing helpful. So you've seen this picture last night. There's Peter Bickle, and he's standing next to Ingram Olkin, who was spry well into his nineties. And there's Ted Anderson. Ted's widow is my neighbor. I just want you to realize that Ted lived quite a long time, and even thanks to, I think, Zhang Xing, published posthumously, 99 years after he was born, that seems to suggest that being a statistician, active in publishing and continuing in the life that we lead can be correlated with achievements, even into fairly extreme old age. So when I look at what's going on here, I'm reminded of the story of Euler, who actually was able to, again, possibly publish even after he had passed away, because the editorial staff took time to publish in those days.
00:39:30.508 - 00:40:21.924, Speaker B: So things were appearing for years after he had passed away. And I expect no less from Peter. I don't want to put pressure on in the sense that I don't want to say, you have to compete with any of these people, so on and so forth. I do feel it's a very good indicator to stay active, and Ted was definitely very active and involved and so on and so forth. So I certainly hope that that will be the case. Now, there's another Nancy. I just.
00:40:21.924 - 00:41:33.154, Speaker B: So there's another thing, which is that it's very important to be in a relationship. And I think Nancy's just, you know, the perfect partner for Peter, and he's very lucky to have her. I think that there are some good indicators for longevity also based on that. So at some point, the state of Massachusetts decided that the oldest living couple in Massachusetts was an eminent statistician, Herman Chernoff and his wife Judy. I think this came out about a year ago, and, you know, Herman was very active and so on. Judy very involved in the community person, very delightful. So those are good indicators.
00:41:33.154 - 00:42:55.324, Speaker B: So being a couple where there's an active statistician who's made penetrating contributions throughout their career, that's also a good indicator to reach the status of the oldest living couple in Massachusetts. I'm kind of hopeful that the oldest living couple in California, if you continue to live here, will one day be you two. Now, an enthusiasm of Herman and Judy was to have the department have dances. In fact, my wife and I were at a dance about nine years ago at Harvard after a seminar there where Herman and Judy danced together. So if they made it to the senior couple of the state of Massachusetts, maybe part of the ingredient is not just statistics, but their dance interests. Now, Peter and Nancy have always been dancers, and I hope you'll keep dancing through life as time goes by. That's all.
00:43:04.184 - 00:43:18.024, Speaker A: Thank you, David. That's really touching. So any comments, things you want to share? Let's thank David Yadun and all of the speakers for the program again.
