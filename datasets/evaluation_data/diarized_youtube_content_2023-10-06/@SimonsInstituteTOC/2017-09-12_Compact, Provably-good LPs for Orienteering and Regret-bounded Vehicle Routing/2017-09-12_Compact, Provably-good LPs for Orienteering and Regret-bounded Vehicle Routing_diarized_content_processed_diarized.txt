00:00:00.560 - 00:01:09.574, Speaker A: Our first speaker of the afternoon is Catania Swami, who will tell us about proving good LP's for volunteering and rider rocking problems. Thanks indeed, and thanks to the organizers for inviting me here. So, yeah, I'm going to be talking about some vehicle routing problems, which is something that I've been interested in for some time, and in particular designing approximation algorithms for these problems. So, one of the things that kind of hits you when you're thinking about approximation algorithms for vehicle routing problems is we don't have too good an understanding of how to effectively use LP's and LP based machinery for these problems. So, in this talk, I'm going to talk about two such problems, orienteering and a related problem called regret bounded vehicle routing, for which we now have a reasonable LP ways of effectively leveraging LP's. And this is joint work with Zach Frickstad, the University of Alberta. Okay, so let me start by defining the orienteering problem.
00:01:09.574 - 00:01:45.424, Speaker A: So, the problem is as follows. We are given some metric space and a starting node, or root r, and we are given some rewards on the nodes. So PI of v is going to denote the reward of v, and I have some. And we'll always assume that the reward of the root node is zero. We're given some length bound. And what I want is to find a path starting at r that visits as many or collects as much reward, subject to the constraint that the length of the path is at most p. So PI of p is always going to denote the reward.
00:01:45.424 - 00:02:15.144, Speaker A: And there are two versions of this one in which I'm just told the starting node are and the version. So this is a potential solution. And if I did my arithmetic right, it collects a reward of 24. And there's the version that's called the point to point problem, where I'm given both the start node and the end node. And again, it's the same thing. I have a budget of b and I want to collect the maximum reward. And both these problems are np hard.
00:02:15.144 - 00:03:00.052, Speaker A: So, just, just to make sure we're on the same page. So, and this is a maximization problem, alpha approximation means that I get where alpha is larger than one, means that I get reward, at least the optimum by alpha. Okay, so this turns out to be a fairly fundamental vehicle routing problem. I mean, it's, it's fairly well motivated. I mean, it's been, it can be motivated from the perspective of, I mean, planning parts for vehicles, subject to, I mean, you have limited petrol, or, I mean, there's a robot with limited battery. It's also been used to plan travel itineraries. And something that's a little less immediate is that it's this procedure.
00:03:00.052 - 00:04:26.020, Speaker A: Orienteering comes up as a procedure in designing approximation algorithms for other vehicle routing problems, like minimum latency problems, and also if you want to solve these vehicle routing problems computationally. So one, one kind of general framework is that you kind of write a huge lp, a set covering style LP, or what we call a configuration LP, which has variables or parts. And if you want to solve this computationally, using kind of a branch cut and price method or cutting plane method, then the subroutine that you need to solve, which is called the pricing problem, turns out to be a orienteering problem. Right? To elaborate a bit on this portion here, let me also define the second problem in the title, which is this regret bounded vehicle routing problem. So again, the input is the same. We have a root r, and I mean, we are in the sort of typical vehicle routing world where these nodes, or clients, we want to visit them, and we want to have some measure of the goodness of a route, which could be, for example, minimizing client delays. So if I were to think of, for example, the path version of TSP that David was talking about, that that could be equivalently phrased as find one path that visits all the nodes and the objectives to.
00:04:26.020 - 00:05:33.466, Speaker A: So usually it's stated as the length of the path, but I can think of that equivalently as minimizing the maximum client delay. So maybe this is a solution involving two routes. But one issue with this objective is that, is that it doesn't take into account that, I mean, there are clients that could be closer to the root or the starting node and some clients that are further away. And you would kind of expect that clients that are closer should face a lower delay than clients that are further. So, I mean, in this solution, the sort of the red, unhappy faces are unhappy because they're kind of closer to the root than this other guy, which is farther away. But they're, if I think of the delay as the length of the path that's used to visit them, then although they are closer to the route, they are suffering a larger delay than someone that's further away. So this is like saying that although I'm in Waterloo and there's a package that's being shipped from Toronto, it takes me more than the time that it takes for that package to go to China, which is, I mean, I'd be pretty dissatisfied with that.
00:05:33.466 - 00:06:21.594, Speaker A: So this is a kind of a source of dissatisfaction. And so what can be, what's a measure that can, that sort of takes that into account. So, one measure, which is called the regret in the vehicle routing literature, is. So I want to compare the waiting time of a client versus the least possible waiting time, which is the shortest part distance from the depot to the note, by waiting time, I'm always going to denote that by, if I'm using a path p to visit the node that's cp of v is always going to denote the time, the length of the portion of the path up to that node v. And I'm going to use DV to denote the shortest part distance from the root to that node. So I'm interested in. So the regret is the difference between these two quantities.
00:06:21.594 - 00:07:23.844, Speaker A: And so what I could ask, one version of the problem is that I'm given some regret bound r, and I want to find routes that the minimum number of parts that I need so that I can visit everyone and ensure that everyone faces a regret of at most r. So this is the regret bounded vehicle routing problem. Any questions about the definition of this problem? Orienteering? Yes, why don't you just add a pass from root to every vertex? Well, I want to minimize the number of paths that I'm using, right? So yes, I could do that and that would ensure zero regret, but we would like to do better. Okay, good. So here's one way of attacking this problem. So my parts in this talk are always going to be rooted. So I may not always say rooted, but they're always going to start at the root.
00:07:23.844 - 00:08:05.924, Speaker A: So define the regret of a path to be just the regret of the end node. So again, that's just the length minus the distance of the end node from the root. So in orienteering, in previous works on orienteering, this was called the excess. I'm going to use the term regret because that's what the sort of literature on vehicle routing uses. So here's a formulation of the problem. I can write down a simple set covering formulation. So if I look at the collection of all parts that have regret at most r, so those are the parts that I can use to cover my nodes.
00:08:05.924 - 00:08:55.346, Speaker A: Now I want to find, I have a variable for each such path, I want to minimize the number of parts that I use, and I want to ensure that every node is covered. So an integer solution here is exactly what we want. So this is a simple standard set cover lp, right? So my sets are these parts and I want to cover all these nodes, right? So now the set cover thing is useful because it immediately tells us that, well, we could potentially get a login approximation. What do I need to get a login approximation? I need to be able to run the greedy algorithm, which means that I need to be able to find, at every step, the set that covers the maximum number of new nodes. One question. Why do you consider only the end node of the path and not. Okay, so, I guess.
00:08:55.346 - 00:09:14.886, Speaker A: Yeah, good question. So, it turns out that regret only increases as you move along the path. So if I bound the regret of the end node, then all nodes have regret at most r. So, good question. Thanks. So. Right, so, for the greedy algorithm, I need to find a path.
00:09:14.886 - 00:09:43.370, Speaker A: The greedy step is to find a path that covers the maximum number of new nodes. And this is exactly orienteering. So, orienteering. If I had something for orienteering, I could immediately use standard set cover machinery and get a login approximation. In fact, I can do much better here. So, this LP was, in some previous work, we showed that it has a constant integrality gap, so it's a structured setting so we can exploit that structure. But now.
00:09:43.370 - 00:10:09.644, Speaker A: So how does this give a constant approximation algorithm? Well, we need to be able to solve this LP either exactly or approximately. And you can do that in various ways. I mean, you could use the ellipsoid method, in which case you would look at the dual. And it turns out the dual separation problem is also an orienteering problem. Or you could use multiplicative weights machinery. And there again, the problem that you need to solve. It boils down to an orienteering problem.
00:10:09.644 - 00:10:11.056, Speaker A: Yes.
00:10:11.240 - 00:10:20.728, Speaker B: Your previous answer just confused me. So I don't see why the grid just increases over the length. I mean, if you could imagine a path that sort of loops back to the root, then wouldn't.
00:10:20.776 - 00:10:26.776, Speaker A: So I'm not. Okay, so I guess I'm not counting. I'm not thinking of parts ending at the root. So.
00:10:26.840 - 00:10:33.762, Speaker B: No, no even ending at the root, it just ends at a point that happens to be closer to the root than the next to the last point. And.
00:10:33.938 - 00:10:38.050, Speaker A: Yeah, but if you try the upper. Good. Yeah, good. Okay, thanks.
00:10:38.162 - 00:10:38.854, Speaker B: Yeah.
00:10:39.194 - 00:11:13.972, Speaker A: Okay, good. Great question. So, I could think of solving this approximately in a sort of, so that I get a theoretical guarantee, but. And also, again, if you wanted to solve this computationally, a column generation procedure would involve solving this pricing problem, which is orienteering. So hopefully that gives some sort of evidence for. Yeah, this orienteering turns out to be a problem that comes up in various settings. And so what do we know about this problem? Or what can we get for this problem question.
00:11:13.972 - 00:11:48.304, Speaker A: You're staying in the undirected metric space I'm staying in the undirected metric. Yeah. So, yes, regret actually can be viewed as an asymmetric metric, but it's a special kind of asymmetric metric. Let me not confuse things. We are saying it's an undirected metric space, and all our problems are in this undirected metric. So here's what we get. So we get an LP relaxation for orienteering, rooted orienteering, and an LP rounding algorithm that gives a three approximation.
00:11:48.304 - 00:12:40.854, Speaker A: So this is the first LP based guarantee. So, and that's how it fits into the theme of this talk, into the theme of this workshop. Now, this is not the first constant approximation. So there are previous approaches and very nice ideas in these approaches, that all these previous approaches use dynamic programming in a nice clever way. So, the previous approaches used dynamic programming to find together some sub parts which are stitched together using another dynamic program, and the sub parts in turn are found using another DP or using a KMST algorithm. And this was pioneered by Blum et al, who gave the first constant factor approximation and then improved by nickel and others and by Chandra and. And others.
00:12:40.854 - 00:13:34.660, Speaker A: So here. So this DP approach is nice, but it's what we are going to get is a simpler, cleaner algorithm. And I think more than just this specific problem, I think what I feel is promising is that the ideas may lead to strong lp's for other vehicle routing crops, which is, I think, which would be great in order to sort of get a better understanding of how to design algorithms for this problem. So, in particular, using what we do here gives improvements for the regret bounded vehicle routing problem. And I mean, as often happens, LP based insights we expect are versatile and will carry over to other variants. So there's some evidence of that. But we'd like much more, and I should say here, that we don't end up improving the approximation.
00:13:34.660 - 00:14:20.224, Speaker A: So, Chandra and others give a two plus epsilon approximation, which is still the best known factor for this. But I do think that, I mean, I do think that our analysis of the LP integrality app is not tight. I mean, I feel that we should be able to get a two, but we don't know how to get there. So this was for rooted orienteering. For the point to point version where I'm given both end nodes, one can actually reduce this to a version of rooted orienteering, losing a factor of two. So this is, we call this the regret version of rooted orienteering. The difference is that instead of a length bound on the path, instead of bound on the path length, we are bound on the regret of the path.
00:14:20.224 - 00:15:24.084, Speaker A: So I find this somewhat intriguing in that usually when you go from an unrooted problem to a rooted one, there's a, you tend to violate something in the constraint, like the length of the path that you're finding. But here we don't lose any such factor. And I think understanding better what's going on here may have some further implications. So these two results combined also give you an LP for point to point orienteering with a factor of six integrality. And finally, for the regret bounded problem, we again get LP's with small integrality gaps. So compact is always going to refer to polynomial size, polynomial number of variables and constraints. And these two LP's are actually quite different.
00:15:24.084 - 00:16:18.362, Speaker A: So one is just using insights that we get for orienteering. You can get a, you can sort of. So, as I mentioned, if you look at the LP for the set cover LP, and if you look at the dual, there's an orienteering problem hidden there, and one can kind of use this LP, relax the LP relaxation that we have, apply that for the dual, and then work things out, and that gives an LP for the regret bounded vehicle routing problem. And we also have a better LP which exploits the structure of the problem. Both actually give improvements over the previous constant factor, and the sort of the current best thing that we have uses this, an orthodox LP gives a 15 approximation. And I should also mention here that, I mean, the set covering LP is a huge LP, so solving that takes a lot of work. And here we have compact LP.
00:16:18.362 - 00:17:00.364, Speaker A: So although this was not our primary motivation, they do give substantial improvements in running time. So in this talk, I'm going to focus on the orienteering result, and hopefully I should be able to describe pretty much all of it in the remaining 15 minutes, ten minutes. So here's my LP for orienteering. So I'm going to bidirect my graph. So just replace every edge with two bi directed edges. I'm always going to view my path as being directed away from the root. And let's say we know the node on the optimum path, that's at maximum distance from the root.
00:17:00.364 - 00:17:52.196, Speaker A: So let's call that w. So that need not be the end node as is indicated by this picture, but it is the maximum distance node on the optimal path. So I'm going to use two sets of variables. One to indicate whether a node lies on this path, that's z, u, w, and these xas, there's always a superscript w to indicate this guess here to indicate whether I'm using an arc on my path. So my objective is to maximize my reward, which is just this expression. And then I have some constraints that kind of encode that I have a path because I'm looking for a path. I know that in degree is always going to be, well, it's either going to be equal to the indegree, or for the end node it's going to be one and the out degree is going to be zero.
00:17:52.196 - 00:18:37.696, Speaker A: We have a metric space. So I'm not thinking of repeating back, I'm just thinking of using shortcuts and using every edge at most once. And I want to make sure that my guess things are compatible with my guests. So I'm not going to visit nodes which are at further distance from my guest node w. I wonder, that's encoding my length constraint. And then I have to encode the usual kind of thing that's done in TSP, that everything is connected, or rather connected from the root. So if I look at a set s which separates a node u from the root, then what this constraint is saying is that if your lp is saying that you're visiting that node, then it better be that you have some arcs that are being used to visit that.
00:18:37.696 - 00:19:25.180, Speaker A: That's what this constraint is. And this is just encoding that I'm actually going to visit w, right? Okay, so these I'm going to call sort of a vector x which has these in degree, at least out degree constraints. I'm going to call that a pre flow. And this can be viewed as a relaxation of a rooted path. And I said that we are looking for a compact lp. Now these are exponentially many constraints, but there's a standard kind of trick that we can use that. I mean, this is kind of saying that the min cut under this, under, in this graph with capacities given by Xa is at least something.
00:19:25.180 - 00:20:03.004, Speaker A: You can replace that by flow constraints so you can get an extended formulation which is compact. Okay, so that's the LP again. Now some basic sort of sanity check here. My constraints, it's not hard to see that the constraints ensure that no node is visited more than, to an extent larger than one. That would be bad. And also the constraints also ensure that. Indeed, although I have not explicitly said this, I don't visit nodes that are not compatible with my guess.
00:20:03.004 - 00:20:46.952, Speaker A: So that's just a basic thing we would expect as a sanity check. And although I'm not going to get into this in detail, let me just mention that we don't actually need to guess this node w. There are ways of avoiding it. We get a larger lp, but, but that can be done. Okay, so what's the idea behind rounding the LP solution? So here's the key insight. And this is a great tool to actually sort of take home regarding when you can express that a pre flow can always be expressed as a packing of arborescensis. So my x was a preflow and the key insight is that, so for example I could have that as my pre flow.
00:20:46.952 - 00:21:47.090, Speaker A: Then I can always express this as that dominates a convex combination of what I'm going to call just out arborescenses rooted at r. So each of these structures is a directed tree that's rooted out from r. It's not spanning, it spans some nodes but not necessarily all the nodes. And so what this picture is saying is that you can always take x, express it as obtain a collection of arborescences with some weights such that that x is at least these weighted combination of these characteristic vectors. So let me say that more precisely. So this follows from some nice arborescence packing results of Bungensin et al. And the formally what we are saying is that we can take this x, get a bunch of trees rooted out from r and corresponding weights.
00:21:47.090 - 00:22:39.834, Speaker A: The weights add up to one. If I look at any arc and the total weight of the trees containing this arc, that's at most the Lp value of that arc. And on the other hand, if I look at the total weight of trees containing a node u, that's at least the extent to which the LP visits that node. And from these it also follows that each of these trees contains my guess node w and none of them contains any node that has larger distance than w. Right? Okay so what's my goal now? And these things can be obtained in polynomial time, right? So now my goal is to show that given the structure I can utilize this to get at least opt by three. Opt is the LP optimum. So here's the algorithm.
00:22:39.834 - 00:23:18.826, Speaker A: So I'm going to get these pairs and now what I'm going to do is I'm actually looking for parts but I'm just going to do this doubling and shortcutting. Convert every tree to a root w path and I lose a factor of two but not quite two. So I don't have to double the edges on the tree that lie on the root to w part of the tree. So I get the savings of dwell, which is going to be important. So if I look at the regret of this part, so the regret again was the length minus the distance of the end node I get a regret of. So if I look at the. Sorry.
00:23:18.826 - 00:24:29.448, Speaker A: If I look at the gamma weighted lengths, that adds up to at most twice my budget, minus the distance of w, and so the regret adds up to at most twice b minus dw. Right? Okay, so, and also, what I know is that if I look at the gamma weighted reward that I collect, that's at least optimal. So now let's, for simplicity and being really, if I was really lucky and I didn't have this factor of two, and if it turned out that all my parts, not just the gamma weighted regret, but actually each path had regret at most this quantity, b minus dw, then I'd be good. Because if the regret is b minus w and it's an RW path, it has length at most w, and then I'm getting their average reward is at least opt. So there's got to be some path which is actually going to be opt. So if I were really lucky and each of these parts, I'd regret it most this much, then actually I'd be getting the optum. So I'm not going to be that lucky.
00:24:29.448 - 00:25:15.334, Speaker A: But so what I need now is a way of converting something which gets regret twice, kind of my target, into the bound that we would like. So if I had a bound of b minus dw, then that'd be great for us. So here's what we are going to do. And this is also kind of a standard tool that's used in previous work that if I have, if I'm shooting for regret r, and I get beta times that regret, I can always break it up into rooted parts, one plus beta rooted parts, each having the right value of regret. So now I'm just going to apply that, the target that I'm shooting for. And I mean, this is just a picture that shows that. So I'm just going to apply this using this as my target.
00:25:15.334 - 00:26:03.392, Speaker A: So when I do this, I'm going to get a total weight of three in my paths and each part is going to end at some node u. But because of my guess w, I know that this, the distance of u from the root is at most the distance of w. So the two things together mean that I get a feasible collection of parts. The gamma weight of these parts is at most three, and I get a reward of at least top. So that's my factor three and that's the algorithm in the proof. And so, to wrap things up, um, so what I showed you is a three approximation. Um, using this, this LP, this is the first LP based guarantee.
00:26:03.392 - 00:26:39.654, Speaker A: We get results for other. For the point to point version and regret bounded vehicle routing. Something that I, uh, that we can do is that we don't actually need to solve the, I mean, there's a combinatorial algorithm that gives the same guarantee the arborist sensors that we need can be obtained combinatorially, and in fact, you only need two arborist sensors. So this, so we get a call. So one can get a combinatorial three approximation. And what's open is nailing down the exact integrality gap. So it'll be great to actually show.
00:26:39.654 - 00:27:35.104, Speaker A: So I believe that two is the right answer, and it'll be great to actually show that we do have a, I mean, it's easy to get a lower bound of four third on the integrality gap without actually working hard at all. So I guess one could look for a stronger lower bound. And two sort of generalizations of orienteering that I would love to solve are. So one's called deadline TSP, where every node has a different deadline by which you need to visit in order to collect its reward. And then there's a sub modular generalization of it where the reward is a sub modular function of the node set of the path. And I mean, so it'd be great if the ideas in here can be useful, but it's also clear that you need some substantial new ideas. And that's all I have to say.
00:27:35.104 - 00:27:41.404, Speaker A: Thank you. Question.
00:27:43.344 - 00:27:59.754, Speaker B: Yes, so, a question I've asked you many times, but does this give you any progress on if you wanted to minimize total regret, let's say you just have one, you have one vehicle, and now you want to optimize total regret.
00:27:59.874 - 00:28:43.514, Speaker A: If you, if you have just one vehicle, then, then, yes. So we actually have a two approximation for that that you can also obtain from some other work that. So this, this paper by Choudhary et al that has a 3.59 for minimum latency, shows that you can get a tree whose cost is at most the minimum cost path that visits. So they don't quite get this. But for the problem that you're asking, you can get a spanning tree of length at most the smallest hamiltonian path. And so that actually gives you a true approximation for the regret.
00:28:45.374 - 00:28:47.174, Speaker B: Not maximum regret, total regret.
00:28:47.254 - 00:29:17.324, Speaker A: Oh, some of the regrets. Sorry. Okay. No, I don't know about that. Yeah, I mean we have some, I know how to get a log by log log, but I don't know how to get a constant. Yeah. Any other questions? Any insight into directed oriented? So, I mean, this bungie thing will still work, but I guess the.
00:29:17.324 - 00:29:57.734, Speaker A: Yeah, so I mean, the question is, how do you convert a tree to a path? So maybe if you, if you had some, if you had added some extra constraints, talking about sort of the order of. So the short answer is yeah, I don't know. But I think this, maybe you could, you could write a more sophisticated LP that has kind of ordering variables between the nodes so that you can kind of encode in your LP that you have to pay a certain cost. Yeah, but I don't. Offhand. I don't know. Yeah, I guess that would be.
00:29:57.734 - 00:30:04.274, Speaker A: Isn't that a special case of some modular. No, I don't know. Yeah.
