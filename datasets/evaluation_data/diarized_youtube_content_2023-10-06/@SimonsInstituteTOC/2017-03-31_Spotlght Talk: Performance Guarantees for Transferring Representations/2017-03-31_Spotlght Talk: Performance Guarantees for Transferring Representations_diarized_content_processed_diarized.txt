00:00:00.800 - 00:00:15.434, Speaker A: You guys are definitely the true believers. You've survived all the way to the end of Friday. You've stayed on beyond the coffee break. So appreciate you all being here. My name's Daniel. I'm from the Australian National University. I'm a PhD candidate there.
00:00:15.434 - 00:00:43.466, Speaker A: I'm visiting the US for a year as part of the Fulbright program. I'm a Fulbright postgraduate scholar at Carnegie Mellon University, where I've been working with Nina Balkan. So this is joint work with her. I'm going to be presenting a talk about performance guarantees for transferring representations. And the work that I'm going to be showing you is the basis for a poster at Iclea coming up next month as well. By the way, it takes 17 minutes, just like the other spotlight talks, even though we don't have. That's right.
00:00:43.466 - 00:01:11.842, Speaker A: I'm not planning to go all the way to 05:00 p.m. Don't worry. Right, so this is the kind of use case that we might imagine. We've got Mister curious over here who has some really great cat and dog photo albums, and he wants to build a binary classifier. And miss Lightbulb over here says, well, there's all these really fancy neural network models on the Internet. Maybe you should try and use them. And Mister curious says, yeah, but my cats and dogs are really special.
00:01:11.842 - 00:02:08.974, Speaker A: I'm different to them. And this is sort of the question that we want to ask. Can you learn something from one task and transfer to the other? And many other speakers have sort of touched on this topic, um, throughout the week, but I guess I'm interested in, uh, can you, uh, say anything from a kind of more formal perspective about when transferring a representation from one task to another helps you. So this is the sort of model that, um, that we have in my case. So you're trying to learn, uh, some function from an input space x to a, an output space y, and maybe you've got a scarcity of labeled data for your target task. So on the other hand, you have a source task, and you suspect that the two tasks are related in the following way. You think that there's some shared representation, f mapping x to some feature space z that crosses the two tasks, and then you want to learn some specialized classifier from z to y that's separate for each of the tasks.
00:02:08.974 - 00:02:44.776, Speaker A: This is pretty empirically successful for a range of applications. In natural language, you might consider this f function to be a word embedding function. Typically, um, you know, you might be able to use one of these embedding functions with no further fine tuning on the target task. Um, in computer vision, you might have a pre trained neural network, and then maybe you would be fine tuning the weights on the target task. Um, so there's kind of various possibilities. Clearly, this procedure doesn't always work. If you pick an arbitrary source and target task, it doesn't always work.
00:02:44.776 - 00:03:47.274, Speaker A: So the question is, can we say anything about, can we find some cases where it does work? So let's just introduce some notation. So capital f is a class of such representations, and capital g is a class of such specialized classifiers. And our sort of default hypothesis class is basically function composition g, circle f, all of the hypotheses that can be composed that way. And in general, we would expect to have in our analysis more source points than target points for our analysis to be interesting. Okay, so at a high level, this is where, where we're going to go. On the left hand side, you can see our sort of base case, which is on the target task, we're going to consider all possible representations, capital f, and hopefully there's a good one, little f somewhere in there. But the problem is, if we've got a scarcity of labeled data, maybe we're looking at too big a space of representations.
00:03:47.274 - 00:04:58.448, Speaker A: On the other hand, what we could do in the center is we could maybe learn some f hat estimated from the source task, fix that in stone, and then hopefully it's nearby, some good little f representation. And then on the right hand side, maybe what we can do is one better than that, we find some neighborhood around the little f hat, which we call big f hat. And hopefully we can construct the neighborhood such that maybe we get superior generalization ability relative to using capital f. But on the other hand, we've still got the good representation, little f, hopefully somewhere in our neighborhood. So we want to try and understand this from a statistical learning theory perspective. So I guess some of the work on like weight transfer and neural networks, it's focused on things like, you know, how do you, how do you initialize a non convex optimization in a good place? That's not the kind of question that we're going to look at here. It's more a question of, can you restrict your hypothesis class in such a way that you get improved generalization on the target task? Okay, so the first case we're going to consider is the following.
00:04:58.448 - 00:05:52.264, Speaker A: We're going to learn some hypothesis on the source task and we're going to try and extract the representation function that we learned. And while in general we might not be able to extract this, if you think about the kind of case of a neural network, if you know the weights of the network, then maybe you can extract this representation function that you've estimated. And then what we want to do is fix that. And then we're just going to conduct empirical risk minimization on t over this smaller hypothesis class, which is all the specialized classifiers composed with this fixed representation. And we're going to do this empirical risk minimization. And now we'd like to understand, can we upper bound the risk of the hypothesis that we learn out of this for the target task in terms of the source task empirical error. So we're trying to get some sort of generalization error bound that moves between source and target tasks.
00:05:52.264 - 00:06:46.872, Speaker A: We get this result, I'm just going to step through it. So suppose you've got some transferability function omega. This is going to be quite abstract at this point, but I'm going to give you an example where it is a little bit less abstract in a moment. And suppose that you've got this following transferability property, which basically says, if I do well with this representation, little f hat on the source task, then that gives me some sort of guarantee about how well I could possibly do with that on the target task. And that's quantified through some transferability function omega. Okay, well, if I've got that nice property which doesn't hold in general, but, but if I have it, then I can make some remark about the generalization performance on the target task. And what does this depend on? Well, it depends on the source task error, the empirical error, hopefully that's small.
00:06:46.872 - 00:07:25.638, Speaker A: This basically says, how well does my source task hypothesis generalize over my full class hypothesis class? Capital H. And hopefully this term will be small if my source task has a lot of training examples. So MS is large. So hopefully then also my transferability function doesn't grow too quickly. So hopefully it's like linear in its argument. So all of this term hopefully is small. And then I just have to generalize with my target task training examples over a smaller hypothesis class, which is parameterized by the Vc dimension, um, capital g.
00:07:25.638 - 00:08:01.138, Speaker A: So Vc dimension being basically a measure of complexity of a hypothesis class. So hopefully what we're able to do is say we've restricted our hypothesis class such that on the target task where we don't have as many examples, we're working with a small hypothesis class. On the source task, where we have a lot of examples, we're able to generalize better. So hopefully we then get a better generalization error bound than we would have been able to from a, from learning on the target task alone. Okay, now can we try and make this a little bit more concrete? An example? Yeah, good question.
00:08:01.186 - 00:08:06.346, Speaker B: So when you do the target task, do you optimize both g at I maybe I missed the obvious.
00:08:06.410 - 00:09:21.172, Speaker A: Right, so in this case, no, you fix on the target task, you fix the representation that you learn, and we'll consider another case where you fine tune it in a moment. So suppose you're comparing these two scenarios. So on the target task you could either learn a two way neural network from scratch that's on the left hand side, or maybe what you can do is learn it on the source task and then fix the first layer of weights, don't touch them, and only optimize the second layer of weights on the target task. And in this case, then if we make certain assumptions about slightly simplified neural network architecture and some distributional assumptions about how the source and target distributions are weight, then we can say, yeah, we've got it. We're going to come up with a definition of what this omega is. And I guess the key intuition here is we want the source task risk to reliably indicate the usefulness of the representation, little f hat that we learned. So what we don't want is we learn some weights on the source task, and then somehow the badness of those weights with respect to the target task is like hidden from the source task error.
00:09:21.172 - 00:09:57.808, Speaker A: We don't want them to be able to hide anywhere. So the way that we do that is make distributional assumptions about the source task distribution and also the magnitude of the upper level weights that we use in this example. Okay, I'm going to just sort of formalize that. This is basically specifying our neural network function class. We're going to assume that there is some shared representation that does exist. So we're sort of saying there is some good shared representation of find. If there's no possible existence of this, we're not going to get very far.
00:09:57.808 - 00:10:26.384, Speaker A: But then the question becomes the little f hat that we estimated on the source task, whether that particular representation is good. We're going to make these distributional assumptions. It's basically creates some sort of symmetry between the source and target distributions. And if we do all of that, then we can say, yep, we can quantify what this transferability function actually is. And so it's kind of an example of trying to instantiate the first theorem. Yep.
00:10:26.764 - 00:10:30.884, Speaker B: So g circle f means you apply g first or you apply f. Sorry, sorry.
00:10:30.924 - 00:10:46.738, Speaker A: You get your data you apply f to, to it. So that's like a mapping from your input space x to your feature space z, and then g. Then you apply g. So f is the representation. That's right. F is the representation. Yep.
00:10:46.738 - 00:10:49.334, Speaker A: You should read that as function composition. Yep.
00:10:50.634 - 00:10:55.378, Speaker C: Are you using any particular definition for what you mean by neural network? Is it just densely connected?
00:10:55.546 - 00:11:14.114, Speaker A: Right. So it's like a feed, so it's like a two way, a feed forward neural network. And there's some restrictions about. So in fact, it's quite a limited class because we've made a restriction on the magnitude of the upper level weights in the network as well, which basically creates this kind of nowhere for the bad representation to hide kind of condition. Yeah.
00:11:15.734 - 00:11:27.160, Speaker C: Is there actually a distinction between having fixed representations versus fine tuning them? Because couldn't you kind of formulate the fine tuning inside of whatever network you're putting on, on your fixed representations?
00:11:27.232 - 00:12:04.570, Speaker A: I mean, right, so in this case, if you're moving from, basically you're optimizing two layers of weights, you're only optimizing one layer of weights. You're working with a different hypothesis class now. Yeah. Okay. This just maps the kind of improved generalization performance that we get. The green line is this transfer learning setting. Okay, so there's another situation where you could be, instead of fixing the representation, you only just look in a neighborhood.
00:12:04.570 - 00:13:06.314, Speaker A: The problem is that that neighborhood might have the same vc dimension as your overall hypothesis class. So what we, and then our vc dimension based bounds won't necessarily capture that reduction in sample complexity. Okay, so another option would be to start considering, um, stochastic hypotheses. And the reason that we want to do that is we can, uh, apply a, uh, a Pac Bayes result, which basically says the generalization performance of a stochastic hypotheses, uh, depends on how close it is in terms of KL divergence compared to a prior hypothesis. And maybe what we can do then is use our source task to construct a prior and a restricted stochastic hypothesis class. And then we can apply one of these Pac Bayes bounds. And we want to find one of, I guess, a subclass of representations that's small enough in the sense that all of our KL divergences within that class become small, so we can upper bound them in terms of some new transferability function omega.
00:13:06.314 - 00:13:56.334, Speaker A: On the other hand, we want it to be large enough, uh, as a representation class to still capture some good representation. There's no point gaining a lot in terms of generalization error. And then on the other hand, um, not actually having a sufficiently expressive hypothesis class, and then in that scenario we can gain a different generalization error bound. This is basically builds on a Pac Bayes bound. And uh, hopefully this term here, all of this term will be small if we've learned well on the source task. And basically it means that then the generalization error on the target task is going to be more tightly bounded because we've more tightly bounded the KL divergence between the posterior and the prior for any hypothesis that we learn. Again, this is rather abstract.
00:13:56.334 - 00:15:12.374, Speaker A: However, I'm not going to go through this example, but there is an analogous neural network example where we can define essentially a neighborhood around the weights that we learned on the source task. And we can say only look at weights that are close to those weights that we learned on the source task. Now, instead of having that hard constraint on the weights that we're going to transfer across and saying we're only going to walk in this little neighborhood, what we could do is soften this to a modified regularization penalty and basically say let's penalize the target task for weights, for diverging from the source task. And in a deep neural network, what you could expect is that maybe that penalty will differ for the different layers. So maybe if envision, for example, we expect the lower level weights to be very close if they're doing common tasks like edge detection, whereas if you've got higher level concepts at the high layers, we don't actually mind if those weights diverge. And so we might expect the parameter that controls how much we penalize divergence between source and target task weights, lambda one. We might expect that to be a decreasing function as we go up through the layers of the neural network.
00:15:12.374 - 00:16:20.092, Speaker A: We set up a task to try and test this out. Basically we took a multi class classification task, one for images and one for text, and we sort of made it into a binary classification task just by grouping, um, half of the classes into the positive class, half the class into the negative class, and then the target task was similarly constructed, except that the, uh, the, the disjunctions of labels that we used on the target task were different to those for the source task, and they overlapped in terms of, uh, uh, through a measure of gamma. So basically if gamma is one, the overlap in the classes between the source and target task is identical and the tasks are identical. And what we did in these cases is we had a lot more sourced task training examples compared to target task. The results are here basically on MNIst. The best performing results were using that regularization penalty that I just showed you. Um, that's, that's this line here.
00:16:20.092 - 00:17:25.274, Speaker A: Uh, and then the other lines are a base case where you're learning the target task, um, from scratch. Uh, this is where we're, we're just transferring the lower level weights in the network, uh, and fixing them. And then this is where we're transferring the entire network and doing no further learning on the target task, um, when gamma's one, you can see that just learning the source task is the best because the tasks are identical, as you'd expect with newsgroups that we found that the fine tuning didn't do quite as well. The tasks are designed so that basically there is a shared representation baked into the tasks because they're disjunctions based off the same shared representation, which is the correct labeling. I think what happened on newsgroups was that the lower level weights were starting to encode disjunctions of newsgroups directly. Because newsgroups, um, can be encoded through like a sparse, um, weight vector in, in terms of like, you don't need that many different kinds of words to characterize a newsgroup. And so for that reason, I think, um, the weights were a bit less transferable in the newsgroups case.
00:17:25.274 - 00:17:59.590, Speaker A: So in conclusion, I'll take your question. I'll just finish up and I'll take your question in a second. In conclusion, it's a step towards a theoretical foundation for transferring representations both with and without fine tuning. As you can see, the kind of restrictions we make, they're relatively restrictive. So this is definitely one of these situations where there's a lot going on empirically and it's hard to really prove exactly why this is working in a more general case. But this is a first step. Thanks for your time.
00:17:59.590 - 00:18:48.586, Speaker A: And yeah, I'm happy to take questions. So can you please explain the task again for analysts? Right. So the task is you divide up the digits into two groups. So it's like 1357 and nine would be like the positive class, and then like the other digits to the negative class. But then for the source and target task, the groupings are different. So the idea would be like in your network, hopefully you can encode, the idea is you can encode the individual digits in the lower level of weights, and in the upper level of weights you could be encoding disjunctions over those digits. Yeah, that was kind of the, that's what the network would ideally do.
00:18:48.586 - 00:18:51.014, Speaker A: Yeah. Yeah.
00:18:52.354 - 00:18:57.618, Speaker B: I'm wondering, have you ever tried to do the fine tuning without the verbalization does it work as well?
00:18:57.786 - 00:19:27.228, Speaker A: Right, so I haven't directly compared that and I guess. Yeah. And that would be interesting as well. Yeah. So I think there's two perspectives on fine tuning. I was kind of looking at it from, as I said, more of a generalization learning theory perspective. But I think there's a whole other perspective, which is like more of an optimization perspective, which is like, I'm not really so worried about generalization, I'm just trying to like optimize some non convex function better.
00:19:27.228 - 00:19:41.708, Speaker A: So I think there's sort of two, and there's been some research that's looked at that there's kind of like a regularization effect and an optimization effect. They're kind of like two distinct effects of basically transferring weights in neural networks.
00:19:41.876 - 00:19:47.864, Speaker B: I guess I'm just asking. Okay. I guess, you know, even without regularization, probably still works. It's possible, I guess.
00:19:48.424 - 00:19:53.848, Speaker A: Right. If you. So in terms of, if you initial, if you initialize the weights. Yeah, yeah.
00:19:53.976 - 00:20:01.976, Speaker B: Do you do early stopping? Like, because, you know, often if you don't do regularization, I think you stop early, especially if you have small data to do that.
00:20:02.080 - 00:20:07.776, Speaker A: Right, so. Exactly. I agree. There are other approaches to regularization that I haven't explored here.
00:20:07.880 - 00:20:12.056, Speaker B: But in your experiment, did you do early stopping? No, you just turn it until convert.
00:20:12.120 - 00:20:13.904, Speaker A: Yeah, yeah, yeah.
