00:00:00.120 - 00:00:37.048, Speaker A: Hi. We're very happy to have our next speaker, David Cox from Harvard. Great, thanks. So, Wolfgang helpfully laid out the dichotomy between industry people and academics and experimentalists and computational people. And if you're wondering which one I am, the answer is yes. So, this is. I'm going to mostly describe work that happened in my lab at Harvard and work by Bill Lauder, who's actually out in industry doing a startup, because that's what everyone does these days.
00:00:37.048 - 00:01:15.314, Speaker A: But I recently also took a gig as the director of a new MIT IBM collaboration, a quarter billion dollar AI institute. So, if you're interested in that, come and find me, and then I'm going to tell you about some deep learning work we've done. But then I'm also going to tell you at the end, if I have time, about some experimental work that it's inspired in my lab just to kind of reinforce this idea that there's sort of a loop that we can be driving between models and experiments. So we all know that deep learning has kind of a maybe kind of connection to biology. So we have units, and they have synapses and connections between them. Okay? That's part of it. That's where the artificial neural network part comes from.
00:01:15.314 - 00:01:48.002, Speaker A: And then they're deep. And, you know, we know that perceptual hierarchies, for instance, in the primate are. Are sort of deep hierarchical systems, and deep CNN sort of capture that. But really, when you get right down to it, that's been most of the interplay between these two. And I think a lot of us here are sort of trying to think, well, how can we go back to the brain and get more inspiration? But then also, how can we use the deep learning to actually help us understand the brain a little bit better? So, again, this virtuous loop, you're going to hear this, I think, again and again and again. And that's where we're coming from. So it turns out that deep learning.
00:01:48.002 - 00:02:22.662, Speaker A: So I'm interested in perception. So, a different part of the stack than the previous two talks. I'm interested in how we perceive objects and make sense of this sort of terrible, complicated flow of information coming in through our senses. And it turns out that by accident, deep learning systems, convolutional neural networks, turned out to be the best model for doing this. So people have built all kinds of computational models of how visual processing worked in the ventral visual pathway in primates. But it turns out that when you just started training deep nets, you just took the internal representations, compare them to actual neuronal population responses. And that's the best model.
00:02:22.662 - 00:03:01.002, Speaker A: So here, this is a paper from my former PhD advisor Jim DeCarlo's lab. And basically what you see is those are sort of model fit quality for different kinds of models that came before, including some of my own. And then basically once we have deep nets, those explain the data, fit the data way better than anything else. And this has been going on. And basically the bottom line is, the better the models get on imagenet, the better the fit seems to be between what the representational space looks like in the deep net and what the representational space seems to look like in the population. So this led to this idea that, well, maybe vision's just tapped out and this is a solved problem and maybe we don't need to worry about this anymore. The answer is deep nets.
00:03:01.002 - 00:03:35.306, Speaker A: There's a problem because all the work we've done so far is all looking at static representations. And really our visual systems are built for dynamic situations. And it's even worse than that because even if you show a static stimulus, neuronal populations will not produce static outputs. And this is one of the first things you notice when you're a graduate student sticking electrodes into the brain of the monkey, which I did for five years. If you show a static picture of a monkey face, you get these sort of transient responses. And in fact, the only way to drive a sustained response in an it neuron at the end of the ventral visual pathway is to show a dynamic stimulus. So this is very much at odds with how CNNs work because they have no intrinsic notion of time.
00:03:35.306 - 00:04:02.196, Speaker A: You put in an input, you get an output. It's a static thing. So something is clearly going on here. And there's all kinds of interesting rich dynamics here where sometimes it's sustained, sometimes it's not, sometimes it looks like it might be oscillating. So we don't really have a good picture yet of what that's all about. And to that, and that part, which seems really salient, isn't actually captured by simple cnns. And it gets even weirder than that too, because there's this great experiment by Carl Olsen's lab where he showed successive presentations of image, image, image, image.
00:04:02.196 - 00:04:30.836, Speaker A: And in some cases the second image, the b image, was predicted by the first image. So b four always happened after a four. And you saw this a couple hundred times. And b five was always seen after a five showed a couple hundred times. And then basically what you see is you don't get a response to b four. If it's preceded by a four, does that mean that the cell just doesn't like b four? Well, no, actually it will respond to b four, but only when it's preceded by a different image. So there's some kind of higher order temporal tracking that's going on.
00:04:30.836 - 00:04:58.904, Speaker A: But again, cnns just don't capture because they don't have any intrinsic notion of time. The other thing too, which I think is actually a salient problem for deep learning, is that most of the data sets we have to train these things. You need tons and tons and tons of data. If you want to train a dog detector, you need thousands and thousands and thousands of dogs and thousands and thousands and thousands of things that aren't dogs. And that's just not the way we learn. I don't sit down for my daughter and show her, dog, dog, dog, dog, dog, cat, cat, cat, cat. That's just not the way it works.
00:04:58.904 - 00:05:24.434, Speaker A: In fact, we can just do that experiment right now. Does anyone know what this is? Raise your hands if you do. We have a few electrical engineers in the audience. Even though you've only seen this for the first time, is it predictable present in this image? Yes. How many in this image? Two. How about that one? Yeah, but it's a little weird, right? So even though you only saw one example, you were immediately able to determine what was there. You were immediately an expert on this kind of object.
00:05:24.434 - 00:05:30.854, Speaker A: And that's called one shot learning. And if you need more evidence that deep nets don't quite work yet. Can anyone say what this is?
00:05:31.834 - 00:05:32.346, Speaker B: Marcel.
00:05:32.370 - 00:06:04.808, Speaker A: Justin. It's actually Merritt Oppenheim. It's called luncheon and fur, but everyone agrees it's a cup, a saucer and a spoon. State of the art deep net says it's a teddy bear. And then it gets worse than that, even when things are very clearly the right object. So this is the state of the art r CNN detection correctly detects this as a bird, but you just put a few objects in that don't belong and it starts calls this a cat, says that's consistent as a television, but starts calling that a cat. So there's something about, there's something brittle and adversarial examples.
00:06:04.808 - 00:06:44.396, Speaker A: We can go on and on and on about ways in which deep nets really aren't quite solving the problem yet. And I and other people think that unsupervised learning is an important piece of this. So how can we, without labels, gather up a lot of structure about the world and build representations that are really good? And then that can feed into things like reinforcement learning and all that kind of stuff. So this is just a different part of the stack I think we all agree on. There's multiple kinds of learning happening here, but the kind that I'm really interested in and have been for a long time, is this idea of temporal learning. If you just look at the environment and let it play out, the environment is almost always showing you its structure. So you look at a person doing a tennis serve, you can see how they're articulated, you can see how they're put together.
00:06:44.396 - 00:07:12.816, Speaker A: You can see how the shadows move around on an object that's just all played out in time. You don't need supervision. You can just observe and learn a lot about the world. So that's what we're interested in doing. And, you know, brains from the neuroscience literature seem to be exquisitely tuned for these kinds of temporal statistics. So this is one of my favorite stuff in psychophysics, where basically they took faces and then they rotated them and then had subjects to passively watch these. But as a little trick, sometimes they would morph the face as it moved, and people generally didn't notice this.
00:07:12.816 - 00:07:44.326, Speaker A: But if you ask the same different later ones that had morphed and they had seen them morph a few times, they would incorrectly sort of associate them as being the same object. And then you can even go further. This is something I did remember my PhD. You can even have present a peripheral object and then have, while the subject is saccading, you flip it out for a different object before their eyes land. Their eyes land on a different object, and then you ask them to do same different on peripheral versus fovea, and they'll make incorrect associations. So it seems like the brain is constantly collecting up these sort of temporal associations to make sense of the world. So we have experimentalists.
00:07:44.326 - 00:08:21.634, Speaker A: This might be happening, but we don't have really, models for it now. There have been prior attempts, so slow frictional analysis is an attempt to sort of extract those signals that are moving slowly. And this has been influential, but it hasn't sort of unleashed, you know, revolution in how we do things. But it's an interesting and important set of ideas. But what I want to do is look specifically at prediction as an unsupervised learning rule. So, basically, the idea that can we just use the idea of predicting what a future frame is going to look like to help us build better representations, and I'm going to make the argument that brains are particularly adept at prediction. I'm going to use this person who's this person.
00:08:21.974 - 00:08:22.470, Speaker C: Serena.
00:08:22.502 - 00:08:39.346, Speaker A: Serena, yes. Not Venus. Serena. You guys are experts. So anyway, so she can do tennis. Serve at 207 km/hour that's 57 meters/second that means the ball traverses the court in 400 milliseconds. About now, the latency from the retina to the primary visual cortex is about 60 milliseconds.
00:08:39.346 - 00:09:18.544, Speaker A: So that means that v one is operating 3 meters in the past. And if we go all the way through the ventral visual hierarchy, then we're talking about 170 millisecond latency. In a human, it's about 9 meters. So there's a very deep sense in which if you think you saw saw Serena Williams tennis serve, you couldn't have, because your brain, the sheer pipeline latency of your brain was way in the past. So one suggestion that's been made is if you're returning that serve, a big part of what you're doing is you're looking at the windup and you're predicting where the ball is going to be, and that's where you're putting your racket. And this idea has been sort of. It can be shown there's a little bit of nuance here, but this is called the flashlag illusion.
00:09:18.544 - 00:09:55.514, Speaker A: So if you look at this dot right here and then watch the sort of clock hand going around, you see how there's like a little flashing straight line. Now, is that line behind in front of or lined up with the clock hand behind? Behind, of course. And the rule with all optical illusions is if it looks like it's not lined up, it is lined up. And if it looks like it is lined up, it's not lined up. So this is perfectly collinear and the sort of classical interpretation, and there's some nuance here, is that basically this line has to go through the full pipeline latency of the visual system. But this tracking line is predictable. So your perception just sort of puts it where it really is in real time.
00:09:55.514 - 00:10:29.636, Speaker A: And then there's another weird thing. Does everyone kind of see that it looks like it's tilted as well? Just hold that thought. All right, so we wanted to get into this. So we were working in sort of machine learning computer vision. And right around the time we were working on this, we got interested in this idea of future frame video prediction. So basically, the idea is that if we see a sequence of images transforming, the goal is to basically predict what the next image looks like. And this is now that there was a little bit of work around the time we were doing this, and it's a much bigger field now.
00:10:29.636 - 00:11:01.784, Speaker A: Just to acknowledge that there are other people working in this area. And we did some very simple things that you would do in deep learning. And again, this sort of fits in the same spirit of what Matt was talking about, where you just want the simplest possible models to start with and just see how much your problem statement sort of, sort of, it makes the problem happen. So basically what we did is we took an auto encoder type architecture and we just wedged a recurrent neural network, in this case an LSTM, in the middle, and then asked. And then we also had a fancier version with Gans. Back then there were only four gann papers, so it seemed exotic. Now there's like 4000 Gann papers.
00:11:01.784 - 00:11:41.580, Speaker A: But anyway, so basically what we discovered is we could build these sort of generative networks that could actually basically sort of render faces. And this doesn't seem so surprising anymore. But at the time I was shocked how well this worked. And there's some details about whether you use the GaN or not and whether the ear appears or not. But the basic bottom line is this is a future frame video prediction problem, where we see a sequence of frames and then we predict what the next frame might look like is actually a tractable thing we can do with neural networks now and then. The interesting, and of course it works on all kinds of different faces, including faces you haven't seen before. But the interesting thing here was we were really trying to look at what kind of representations do we implicitly induce when we train a network to do this kind of future frame prediction.
00:11:41.580 - 00:12:45.672, Speaker A: So all we're training, we're training the network with backprop, but the loss is coming entirely from how well does it reconstruct a future frame that it hasn't seen before? And the interesting thing that we find is if we kind of just take that internal representation and just sort of pipe it off and just ask how well can we decode things, other things like the identity of the face? What we find is if we do a simple 50 way face recognition task where we need to tell which of 50 people it is, and we only get to see 1234. So on views of the person, what we see is if we start using these predictive networks, they're able to do a little bit better with a little bit less data. And this is what we're trying to get towards. Can we get representations where we can do more with less training data? And because we're sort of extracting some sort of, sort of deep part of, you know, some sort of deep structure of the image. Now, this was back in 2015. And then, you know, we also discovered that, you know, for instance, you could internal with into these the representations that we learned. You could also find directions that you can move around in that would do things like make the face more male or more female.
00:12:45.672 - 00:13:20.612, Speaker A: These are less surprising now that gans are around and we can do all kinds of fancy things with them. But what we really were driving towards, just to get more to the punchline, was not this sort of very simple autoencoder with an RNN in the middle, but really getting towards this idea of predictive coding, which came from neuroscience. So, the basic idea here, this is a paper from 1999 that popularized predictive coding. But the idea actually goes back about a decade earlier. And the basic idea is this, we start with an input. We have a feedforward signal. We try and predict away the input, and we subtract that off and we only send forward the differences.
00:13:20.612 - 00:13:49.400, Speaker A: Now, the original idea here was an efficient coding idea, like, let's try and reduce the number of spikes we have to send, because if we subtract away the things we already know, then all we need to send forward is the difference from that. So what we did was basically to take. Yeah. In that standard paradigm, it really depends on what we think the signal is. If I'm looking at the outside and I'm trying to estimate position of a ball, and predicting where the ball is going to be is a different story than if I'm trying to estimate the velocity of the ball.
00:13:49.432 - 00:13:49.600, Speaker B: Right.
00:13:49.632 - 00:14:13.124, Speaker A: Absolutely. And then in how many examples do we actually know what it is that we're trying to predict? I mean, really, for sure, right. That this is only doing that same. Right? Yeah, yeah. That's a great meta question, and I think we can discuss that. Like, what are we estimating in this case? For the purposes of this, we just took the simplest path. I mean, again, this is sort of the modus operandi, is take the simplest possible thing and then see where you get with it.
00:14:13.124 - 00:14:48.674, Speaker A: So here we're actually going to generate whole frames. So we actually want to confabulate and sort of imagine what the future frame's going to look like. So you could imagine in that flashlag illusion example, your percept, in that case is actually an imagined thing. It's like you're actually what you're perceiving at a given moment is a fabricated future state. So we're going to just take it at the pixel level. But I agree, if you were interested in different things, you might use different targets for your prediction. But what we did basically was to take the classic idea of predictive coding and instantiate it in the simplest possible way we could with deep networks.
00:14:48.674 - 00:15:19.942, Speaker A: So basically what we do is inputs come in here, we have a running prediction of what the input should look like, and then we subtract them off to get an error map, and then we send that forward to the next layer. And then we have a recurrent layer at each stage of the hierarchy that's trying to build up this prediction. And it can get feedback just the same as in the brain, and it can also get local recurrence. And then you basically just subtract off. And there's different ways you can do the subtraction, it doesn't much matter. And then you get these errors and you only send the errors forward through the network. So one nice thing about this is at time one, this is a CNN, because nothing's come through.
00:15:19.942 - 00:15:48.200, Speaker A: So the extent that CNNs are a good model for the ventral visual pathway, this is the CNN on the first time step. And then the other thing that's nice about it is it's also a classic generative model. So if we put something in on the top, it'll render down into an actual predicted image here, so we can see what the network is seeing or perceiving at any given moment. And we started off by calling these particular coding networks and then we maybe deep prediction, no, that's not a good name. Predate? No. Bill Net? No. Can't believe it's not Alexnet.
00:15:48.200 - 00:16:19.510, Speaker A: We ended up with Prednet because there are rules, there are rules to how you do this and you have to get them right. So we're calling these prednets for better or for worse. And what we found is that these networks that had the recurrence at every layer were able to, our previous ones were only able to get sort of one degree of freedom in a rotating object. Now we could actually get as many degrees of freedom basically as we wanted in these synthetic objects. So here are examples. And basically the gist for all these I'm going to show is this is the actual sequence of incoming images. And then what's next with below it is the time shifted prediction.
00:16:19.510 - 00:17:07.406, Speaker A: So you can compare the prediction to what actually came. And you can see on the first frame you get these weird potato things because it doesn't know which way it's going to go. But then once it knows which way it's going to go, it locks on. And then you start getting pretty good predictions and it can do this with faces it hasn't seen before and all that kind of stuff. But again, the reason we're doing this is not for future frame prediction per se, but to see if we can actually learn good representations, because the idea is, in order to predict what's coming next, you have to implicitly know lots of things about the structure of the object, how light works, how shadows work, all that kind of stuff. And what we find is basically if we try and build decoders again in the same sort of spirit as that face recognition task, where we just sort of peel off the representation and just do linear decoding, we discover that we can decode lots of different parameters of the image, like how fast it's moving, what the starting angle is. We can also look at things about principle components of the identity.
00:17:07.406 - 00:17:49.602, Speaker A: So it sort of fits with this idea again, that by virtue of learning good predictions, you learn good representations that are useful for lots of things, which is kind of what you need to have in an unsupervised or a semi supervised learning sort of setting. We also did a face recognition version of this again, and what we found is that these prednets, at least at the time, were performing on par or better than the best semi supervised learning algorithms that were available, called ladder networks. So you might wonder what happens if we put in things that aren't faces. What does it do? It actually does something pretty reasonable. So this is a network that was trained on faces and we put in this image of a top and something came out pretty ok. It doesn't always turn out ok. Here's like a little toy car, and you can see it's desperately trying to turn it into some kind of face.
00:17:49.602 - 00:18:25.622, Speaker A: It's like this munch scream kind of trying to happen there. But the bottom line is if you train it on complicated things and enough variety, you can get networks that can do pretty good predictions on just about anything you want to do. And of course just about anything you want to do is cars, right? Autonomous cars. Like at least in this year we did this work. That's the thing you want to be doing prediction on, because that's where all the money is. So we took my student bill, without any prompting, took some car mounted camera data sets, so we trained it on the kitty dataset, which is in Germany. And then what I'm showing you here is the test set, which is the Caltech pedestrian data set.
00:18:25.622 - 00:18:45.686, Speaker A: And then this is what the predictions look like. So they're not perfect, they're a little bit blurry. If we put a gan on it, we can make it less blurry, sure. But we're really just focusing on sort of learning what kind of representations we can pull out of this. And a couple interesting things here. One is it seems to implicitly know a lot about perspective and flow. It knows that things need to expand outwards.
00:18:45.686 - 00:19:17.480, Speaker A: It knows that things that are far away need to, you know, change less. It also knows to infill road. Here, if we look at other examples as well, you can see things like, it knows a little bit about occlusion. So this car is going to occlude this golf cart. It knows that that one should go in front. It knows, you know, it knows all kinds of things about, you know, implicitly about the flow of content in the image. And again, what we're trying to do here, it's interesting to do it, to look at, um, future frame prediction.
00:19:17.480 - 00:20:08.464, Speaker A: Um, and we can also do farther out future frame predictions as well. If we can go out as much as five frames, it gets blurrier, but it's, it's still okay. But again, the goal here is to say, well, by virtue of learning how to predict as sort of a surrogate loss, can we learn how to decode other things that might be useful? And in a car, one of the things you might want to learn about is what's the, uh, steering angle of the car? And what we find, again, in the same way we can take the representation, peel that off, put it into a linear decoder, and we can discover that without any prior training on these steering angles, this can actually outperform a system that was purpose built to decode steering angles. So this is from comma. This is a startup that's doing autonomous cars. They had a reference CNN on their data set and then basically the prednent, just by learning how to predict the future, also implicitly learned what the steering angle of the car was because that was kind of what you need to do to do the task. Yeah.
00:20:08.464 - 00:20:11.214, Speaker A: Representation seems to come from not one.
00:20:11.374 - 00:20:12.822, Speaker D: Not just the recurrence, but also the.
00:20:12.838 - 00:20:56.660, Speaker A: Fact that you're training it on continuously sampled images. How much of the improved representation is due to the prediction as opposed to merely the sequence of training images? Right. So we have, so we've done predictive versions and we've also done sort of just a auto encoder style. And the prediction outperforms. The details of that are all in the paper is how much additional, how much of a better representation do you gain by adding the predictive component as opposed to just having, just having the training data that is densely sampled from whatever image or sequence of images you have. You mean specifically for this steering angle version. So the way the comma one works is it takes in frames and it puts them to a CNN.
00:20:56.660 - 00:21:24.584, Speaker A: And so it has the temporal component, but it doesn't have any notion of prediction. Wasn't sort of trained to do prediction. And what we see here is these were the comma reference cnns, so how well they performed. And then this is with different numbers of input frames to the decoder. And you can see it can do quite well with relatively little data. So it seems to be a useful thing. But again, what we're trying to do is trying to drive towards how might this be a principle that the brain could use to organize itself? Yeah.
00:21:24.704 - 00:21:27.404, Speaker B: When you're doing logs, what do you use for the irisome?
00:21:31.204 - 00:21:56.574, Speaker A: When we're doing rollouts, what do you mean? Oh, into the future. So there you need, basically what we do is we re inject recursively the predictions and then we fine tune that. So basically, if you take the prediction, put it in again, get the next prediction, put it in again, put the next prediction in. You can get. It starts to get blurrier and blurrier over time. But that's basically what we're doing.
00:21:57.354 - 00:22:03.378, Speaker D: Is the prediction right now about the next frame?
00:22:03.546 - 00:22:29.336, Speaker A: Does it matter if you do, say, next to five frames? Yeah, I very briefly showed we can do five frames ahead. The predictions just get murkier. It makes a little bit of a difference. It gets murkier because you don't know which way the car is going to go. I mean, realistically, what we should be doing and what we are doing now is probabilistically we should actually be putting out a probability distribution of all the possible outcomes. That's hard, but that's what we're doing. And, you know, there's a lot of interesting sort of wrinkles in that.
00:22:29.336 - 00:23:03.142, Speaker A: But even just with the straight up, let's just come up with a mean outcome, because there's a problem that if you don't know if it's going to go left or right, then you don't want to split the difference and get blurry. What you'd really like to do is have a system that produces samples that are sort of the correct distribution of actual outputs you could have. And again, that's something we're working on. Something brain probably does, too. Of course, then we want to actually, since this is a neuroscience talk, we want to go back and look at. Does this do something that explains something in neuroscience that we didn't understand before. Well, here's something that everything does.
00:23:03.142 - 00:23:28.960, Speaker A: So it turns out you get gabors. Every neural network you train on, anything, no matter what you do. It's a rule that you'll get gabors out and orientation tuning. But more interesting than that is this notion that I sort of flagged earlier, which is if you put in a static input, the brain doesn't give you a static output, it gives you a dynamic output. So there's usually a delay, there's a burst of activity. And then you have this sort of activity that falls off. And sometimes you also get off responses.
00:23:28.960 - 00:23:50.334, Speaker A: So when the stimulus goes away, you get a fresh response. And it's probably not a great surprise. But prednets do this as well, and it's not hard to see why. So this is the average of units in the error representation of the prednet. It's not hard to figure out why it's doing this. Basically, what happens is, on that first hit, it can't predict anything when the image first flashes on. So you get a big bunch of activity.
00:23:50.334 - 00:23:59.798, Speaker A: But then as it locks on and learns to sort of explain away the data, then it goes down. And then when you get the image off, that's another sort of surprise. And then you get another burst of activity. Yeah.
00:23:59.846 - 00:24:11.008, Speaker D: Sounds like you're running this off frames. And the real world doesn't provide frames. If you had sprinkled the pixels randomly, like Toby Delbruck's event cameras or something, would you get the same if it.
00:24:11.016 - 00:24:29.976, Speaker A: Were true continuum for a true continuum? So it's true. We're doing this in discrete time because we're talking about lstms and things like that, convalest. So we can't accommodate continuous time. But I don't think that's. Personally, I don't think that's a huge difference.
00:24:30.120 - 00:24:39.774, Speaker D: We know the brain loves temporal discontinuities. And if you hand it frames, you're just overloading it with edge detection. Whereas I look outside, I don't get any temporal edges.
00:24:40.274 - 00:24:54.042, Speaker A: Well, I mean, these aren't like different frames. It's like continuous frames. So it's a relatively smooth progression through the space of images. And when you have nothing and then you put on something, that is the kind of discontinuity we're talking about. It is in discrete time. It's not like this. Yeah.
00:24:54.042 - 00:25:43.130, Speaker A: So what we did is we literally put the image up blank screen, put the image up and then take the image off. And so that's like the standard sort of primate experiment version of this. And basically the predna does exactly the same thing, has exactly the same dynamics that you would find in primate visual cortex. So this is a bit of circumstantial evidence, at least that part sort of matches up with expectations originally. Actually, particular coding was designed to explain a phenomenon called end stopping, which is if you have a bar that's oriented the way a v one cell likes, and then you make the bar longer and longer and longer and longer, the response will go up to some point, and then once you make it longer, actually the response gets suppressed. So it's like too much of a good thing. The longer the bar is, that's in the orientation that it likes, it actually reduces the response.
00:25:43.130 - 00:26:33.884, Speaker A: And true to form, the prednep version of predictive coding also has this sort of n stopping response, which is a nice thing to have, but it goes further than this. There's also this notion of surround suppression. So if you have a stimulus which is a dot, and you make it larger and larger and larger and larger, v one cells will like it better and better and better and respond more and more and more up to a point. But then beyond that, they'll start being suppressed, as if there's a suppressive surround around the response. And what we find is that the prednet also has this quality. So if you look again in the bottom layer, the response goes up to a certain size of a dot stimulus and then it's suppressed. But interestingly, Rick Bourne's lab at Harvard back in 2013 found that if you cool downstream visual area, so if you call v two and then record in v one, so inactivate feedback connections, you actually would find that these surround suppression effects were themselves suppressed.
00:26:33.884 - 00:27:26.124, Speaker A: They go away if you take away the top down feedback. And this is a way easier experiment to do in a network because you can just turn off the feedback connections and see what happens. And lo and behold, you get almost exactly the same pattern of feedback. So in the prednet, to the extent there's surround suppression, it's happening as a topic, top down feedback phenomenon, just the same way it seems to be happening in the primate. And then as we march through these things, we also have this interesting sort of sequence learning effects that you see in the end of the ventral visual pathway. And lo and behold, if you show a prednet, these things. So the prednet's just trained on natural images like car videos, and then we're going to show it these stimuli that are basically the same as what you would find in these experiments in an untrained prednet, you get these sort of funny effects where you flash up an image and it takes a little while to sort of predict it away.
00:27:26.124 - 00:28:19.130, Speaker A: But then as you train these sequences so that they're expected, you get sharper and sharper transitions from between these images. And then what happens is basically the same exact result. So if b is predictable from a in serial presentations, and again, this is trained on cars, and then we just subsequently show it a few of the comparable number, hundreds of examples, then b will be suppressed if a explains away b. But if we show that very same b with something else that's not a, that doesn't predict it in front of it, we get the exact same result that the Carl Olson and company got. So it's able to track with almost exactly the same number of trials, some of these interesting sequence learning effects with no extra machinery needed to be added. And then you might also ask, okay, that flashlag illusion. So you might remember this clock's going around, this bar is flashing.
00:28:19.130 - 00:28:41.834, Speaker A: It's actually lined up, but it looks like it's lagging behind. So you might ask, well, does the same thing happen with the prednet? And the cool thing about generative neural networks is you can see what they're experiencing because we can just visualize the error layer. And so this is what it's seeing. Now, there's a caveat here. Does anyone know what the caveat is? Yeah. Yeah. So you're getting a double dose.
00:28:41.834 - 00:28:56.066, Speaker A: So the network is seeing it, and then you're seeing it on top of what the network is seeing. So if we really want to do this, we need to. We need to look at freeze frames and we can see exactly what the network's seeing. And there it is. There's that weird lagging. And there's the tilt too. Right.
00:28:56.066 - 00:29:03.122, Speaker A: That was weird. There wasn't a good. There wasn't a satisfactory explanation for that tilt before, and it just sort of falls out of the prednament.
00:29:03.138 - 00:29:07.094, Speaker B: Yeah, there's something I'm missing. Because the flashes are predictable too. Right.
00:29:07.134 - 00:29:08.598, Speaker A: They're just predictable on a longer time scale.
00:29:08.646 - 00:29:10.234, Speaker B: So you have temporal uncertainty.
00:29:11.534 - 00:29:23.630, Speaker A: Yeah. Yeah. So it's the same thing with the human version of it. Right. Like it's predictable, but there seems to be some window beyond which it's effectively not predictable. That's right. There's also weird things.
00:29:23.630 - 00:30:02.468, Speaker A: Like you see these weird rippley things happening, and I don't know what those are, but they're really cool and I like looking at them, and we'll figure out what they're for eventually. And then I love the scientific community and I love arxiv. So we posted this and a group in Japan picked it up and started using it. So they basically wanted to see these illusory motion stimuli. They wanted to see what would happen if they showed them to prednis. So the way these work, hopefully you're experiencing the illusion right now. This is a static image, but it looks like these are, are sort of, these ones are sort of rotating and these ones are not.
00:30:02.468 - 00:30:52.950, Speaker A: So this is a classic sort of illusory motion sort of stimulus. And it turns out if you train a prednet on these sort of rotating, so it seemed real objects rotating in the world, or synthetic images of propellers and things moving in the world, and then you compute the flow vectors on the actual predictions. If you just let a prednet look at these static images, the prednep actually produces flow vectors that are consistent with the illusion. So when humans see the illusion, the prednet has optic flow that looks like what we see. And when humans don't see the illusion, there's no optic flow. So this is interesting, especially because the usual explanation for these things has to do with sort of epiphenomena, about the relative latencies of visual responses. And now, that might still be true, but this at least gives you another possible explanation, which is that you have a system that's trained with a predictive loss.
00:30:52.950 - 00:30:57.194, Speaker A: It's going to naturally have some of these biases that come from the sort of statistics of the world.
00:30:57.574 - 00:30:58.278, Speaker B: Good question.
00:30:58.366 - 00:30:59.014, Speaker A: Yeah.
00:30:59.174 - 00:31:03.358, Speaker B: So how far ahead do you, and what's the frame rate and how much does that matter?
00:31:03.406 - 00:31:05.194, Speaker C: I would say that frame rate's too small.
00:31:05.934 - 00:31:08.022, Speaker B: You don't learn anything because it's ridiculous.
00:31:08.118 - 00:31:38.476, Speaker A: Yeah. Yeah. So with all things neural network, you know, like, we just hide all the. Like the parameter search that went into it, there is indeed so the failure mode in general, when it can't, when it doesn't learn. So if we have, like, the hyperparameters wrong or whatever, is that it predicts the last frame, that's usually our benchmark against which to see if it's working. So we take the reconstruction loss. If you just said it's the same as the last frame, and then compare how much we improve the error, and then you're right, if it's too herky jerky, then it has trouble learning.
00:31:38.476 - 00:31:42.504, Speaker A: So I don't have a good quantitative answer for you, but there is indeed a sweet spot.
00:31:45.424 - 00:31:46.204, Speaker B: Ish.
00:31:47.424 - 00:32:10.696, Speaker A: Yeah. I mean, it depends on how fast you run the frame rate. We were running these at ten frame frame rates, so that's kind of. Yeah, about 100 milliseconds. It depends on how fast things are moving as well. So, anyway, so I promised that we would get back to a neuroscience experiment. So I'm going to tell you about a story about an actual neuroscience experiment that the neuroscience lab, the wet lab part of my lab, did in response to the computational work that was going on in my lab.
00:32:10.696 - 00:33:03.614, Speaker A: So you might remember that we could read out the steering angle from a self driving car using the internal representations in the prednet. We also had the idea, well, wouldn't it make sense to actually take the efferent's copy of the steering wheel, all the odometry of the car, if we could just feed that into the representation, surely the network would do a better job of predicting. So if I'm trying to predict how the world is going to change, if I know that the wheels turn this way, I can make a better prediction about how the world's going to change. I might also be able to do cool things, like if. If I could sort of fictively imagine, like conditionally generate. If I turned the wheel like this, what would the world look like? So this is something we started doing just because it made sense from a machine learning standpoint. And no big surprise, when you don't have an eference copy, which is basically the sort of vanilla prednet, you get certain convergence over time, so you get down to some mean error and with training epoch.
00:33:03.614 - 00:33:42.560, Speaker A: But if you include this extra information, unsurprisingly, the network converges faster and comes up with a better result. So that's all fine and good, but if this were true, and this is what was happening in the brain, that would imply that in visual cortex, we should have signals from motor cortex, right? They should be there, and if they're there, we should be able to decode them. Even in the dark, potentially. So my student Greg thought this was a great idea. He was doing 24/7 recordings in visual cortex in a rat at the time. So you just had boatloads and boatloads of data. So we just said at lab meeting, well, hey, why don't you just put them in a dark box so there's no light, completely light tight, and see if you can actually.
00:33:42.560 - 00:34:19.936, Speaker A: And you have an accelerometer on the animal's head anyway, because you know the minute you're putting electrodes in, you might as well put an accelerometer in there, too. And then so we can record from many tetrodes. So we have 16 tetrodes, 64 electrodes, and we can record local field potentials, and then we also have these accelerometer signals. So there's a hypothesis that comes from the computational work that says, well, we should be able to decode these efference copy signals in, you know, potentially even in v one. And it turns out that that is exactly what we can do. So these blue bars are in complete darkness in visual cortex, so there's no visual stimulus. They're not getting any visual optic flow.
00:34:19.936 - 00:35:12.674, Speaker A: But we can decode a little bit of sort of six degree of freedom information about which way the animal's nose is pointing. And then, interestingly, if you inject mucamol into area m two, which is the putative place where these motor signals, efrence copies would be coming from, this is all still new, so don't get futzed if this ends up not holding up, because we still need to get more animals, at least in the first animal we did, we can actually abolish that signal. So if we take away motor cortex signals, we actually can't any longer decode the position of the animal's head in three space. So this is a case where we actually built a model. We built a model that was useful in its own right. It helped explain some things about neuroscience, more or less along the way, without having to fit anything. And then by looking at it, we can make predictions, which then led to experiments that we could do to maybe learn something new about how the brain is organized.
00:35:12.674 - 00:35:13.574, Speaker A: Yeah.
00:35:14.314 - 00:35:43.550, Speaker C: Are you proposing, or could you distinguish whether this group projection from m two to v one was creating kind of a positive image of what was supposed to be happening visually in the animal at that moment, or a negative image which is supposed to be cancelling what was supposed to be coming in more in the spirit of this predictive coating. So one is kind of imagery, positive imagery. The other one would be corollary discharge to inhibit the neurons that should be activated. Now, which is it?
00:35:43.742 - 00:36:19.164, Speaker A: We can't tell. And in electrophysiology, we just have some tetrodes recording from some places, and we're not exactly sure where. We can't reconstruct the image to fully disambiguate that. We also have two photon imaging going on in my lab. So there is an idea that we could go and be getting hundreds of cells that are thousands of cells at a time and have a prayer of actually doing those decoding experiments. And that's work that we have that's that's ongoing. But right now, we're just at the stage where we say, can we decode any information? And the answer seems to be yes.
00:36:19.164 - 00:36:46.988, Speaker A: I agree. There's a lot of questions that come downstream of that. Predictive coding implies certain kinds of correlational structures that we can start to look at. We also have the ability to image synapses in the two photon image, the synapses separately from the cell body. So we can actually look at feedback synapses specifically and ask how the information that they contain is different than the cell bodies in L2, three. So these are all experiments that are ongoing. But I don't have a good answer for you.
00:36:46.988 - 00:37:02.780, Speaker A: But that's the right kind of question. And that's the right kind of question we can ask when we have these kinds of models that can guide our experiments. So, anyway, so this is just my pitch. So, apparently a DeepMind, it's a circle. For me, it's just a. Or it's just a ping pong. I don't know.
00:37:02.780 - 00:37:18.250, Speaker A: I like the circle better. Maybe I'll change. So, people are picking this up, including neuroscientists, and we love that. If you want to pick it up, there it is. The code's all free. I'm told it's not hard for other people to get working. So that's great.
00:37:18.250 - 00:37:48.438, Speaker A: That's a testament to Bill's hard work to share things. So I just want to try and get us back on schedule. So, this is my lab. I just wanted to acknowledge everyone, particularly Bill Lauder and then all the people who funded this stuff, and then just one shameless plug. So, I've just started as the director of this institute. If you're interested in AI and neuroscience nexus, the kinds of stuff that I'm showing you here and you're interested in doing that in industry, find me during one of the breaks. I'd be happy to talk.
00:37:48.438 - 00:37:49.874, Speaker A: Great, thanks.
00:37:56.754 - 00:37:57.574, Speaker B: Sorry.
00:37:59.914 - 00:38:25.074, Speaker E: So, first of all, could you rule out vestibular signals in your experiment? Because it's well known that the vestibular subject to cortices, and if you polyze the mice, it won't move as much. And second remarks is that it's well known that there are posterior signals about mobility activity. In the one, it's not surprised.
00:38:30.534 - 00:39:18.420, Speaker A: So most of the mod. So the motor modulation, there's a whole cottage industry of mice on trackballs when they run versus when they don't run. And basically, there's more information about whether they're running or not than there seems to be about the visual system that's sort of like the first approximation, but a lot of the. There's been a sort of a dichotomy of people who either think maybe this is a predictive processing kind of thing versus people who think it's more of like an arousal, sort of like when the animal's running, they're on, and when they're not running, they're off or something. And you can probably imagine which one, I think, is the more likely scenario. So the idea that we can actually get information about the actual direction the nose is pointing, I think, is interesting and a little bit surprising and feels at odds with the idea that it's just sort of an on versus versus an off kind of transformation. Now, in terms of vestibular signals.
00:39:18.420 - 00:39:44.886, Speaker A: Yeah, I mean, we've talked about the experiments aren't pretty, but you can imagine what they're like. It's like, oh, I'm just moving the rat in the dark. Here I am with my rat moving in the dark. Sometimes you have to do the science, and it's not glamorous. So we are interested in those experiments. It doesn't sort of go against the general theme here. Wherever you get the signals from, you'd be crazy not to use them.
00:39:44.886 - 00:40:00.142, Speaker A: I think the fact that we see some reduction in them when we knock out m two suggests a little bit that maybe it's more of a motor eference copy. But it's entirely possible that vestibular system. Vestibular inputs are getting through to v one through m two.
00:40:00.278 - 00:40:04.190, Speaker E: M two. Does that have effect on motor. The motor behavior of the mice?
00:40:04.342 - 00:40:24.502, Speaker A: So m two is. I mean, it's sort of like frontal orienting fields in a monkey to a crispy. It's like an orienting area. Yeah. So as near as we can tell, it doesn't seem to. So we've done sort of all the marginal comparisons, so you'd expect it to have some effect. It turns out you can actually just scoop out all of m one, all of motor cortex.
00:40:24.502 - 00:41:06.354, Speaker A: My neighbor at Harvard, Benso Olivsky, did this experiment where you basically just carpet bomb all of motor cortex, and the animals can still do all kinds of complicated motor tasks. So that's alarming if you study outer cortex. But from our perspective, it's actually a good thing because it means, as near as we can tell, we're not changing the statistics of the movement. One of the things we're moving towards is trying to get more quantitative predictions about what would the actual predictive coding, sort of subtractive signals look like versus the others. Having a model at least gives you a prayer of being able to ask those kinds of questions. But I agree, it's complicated, not the end all, be all. And also in monkey and primate, it.
00:41:06.354 - 00:41:19.874, Speaker A: There were some beautiful studies that were kind of lesser known where they showed that in complete darkness there were saccadic eye movement signals present so you could decode when saccades happened. So these ideas aren't new. It's just they're sort of driving us in new directions.
00:41:22.534 - 00:41:55.754, Speaker E: Yes. I just wanted some clarification. So when you're doing the decoding, for example, when you're predicting a steering wheel or when you're predicting the illusion and stuff, then you are decoding from the output layer, I guess, like where you see the internal representation. But when you're comparing that to the neural signal, which actually reflects the aerosols, for example, if you're showing the. If you're comparing that to the end stopping signals or the transient dynamics to the unexpected frame, then you are showing the error signal. Right? Am I correct?
00:41:56.454 - 00:42:33.412, Speaker A: Yeah. It turns out if you look at them, they almost all look the same, which is disheartening for people who want to disambiguate different populations. I mean, at least when you marginalize them this way, the e neurons look like pop and then off response. But so do the activation neurons and so do the recurrent neurons. There are recurrent neurons that are sort of sitting there and like in Matt's talk, they're ones that are doing the things, you know, they have to be doing. But there are ones in the recurrent layer that also have the sort of transient dynamics in them as well. So the long story is like, if you were sticking electrodes into a monkey brain like I did for five years, you know, you wouldn't know which ones of these you were getting.
00:42:33.412 - 00:42:35.652, Speaker A: And they all look surprisingly similar.
00:42:35.788 - 00:42:42.784, Speaker E: But if you want to decode and then reconstruct the stimulus, then you have to decode from the a, not from the e, right.
00:42:44.304 - 00:43:16.970, Speaker A: I mean, so in the cases where we're doing decoding, we're actually decoding from the R's. But I mean, you can decode lots of things from lots of places. The R's, we thought the R's made a lot of sense because they're the ones who should be holding information about context and representation. But, I mean, there's obviously a lot going on as you get further along, though. This is a CNN in the static case on the first time step because basically there's nothing to cancel out. This is all zero so you just, just go up through like a CNN. So you would expect there to be sort of different levels of representation of higher level and lower level features.
00:43:16.970 - 00:43:33.754, Speaker A: So anyway, long story short, it's complicated. The simple fact of training it in this predictive mode is going to affect all of these weights, so it's going to induce representations even in these a's. So the bottom line is you can pretty much decode from anywhere. This is bad news for neuroscience.
00:43:33.794 - 00:43:34.138, Speaker B: Right.
00:43:34.226 - 00:43:39.218, Speaker A: You can decode from anywhere and all of them qualitatively look very similar in their dynamics. Yep.
00:43:39.386 - 00:44:13.186, Speaker B: So I'm thinking about the relationship between what you're doing here and the stuff that I've been working on. But honestly, really, the ideas that were put forth by Hochride originally about supervised learning and that work would suggest that this system should be able to do something even richer, which is to do something like visual system identification. So in the phenomena that you've talked about, you look at the first frame and you can make predictions about what's going to happen because you live in this world. But there are situations where I can't predict this. Observing the dynamics helps me predict the dynamics. So if I, like, let me say.
00:44:13.250 - 00:44:14.842, Speaker A: Like, I see a dog and I.
00:44:14.858 - 00:44:31.728, Speaker B: Notice that it's limping, it's seeing the limping that lets me predict what it's going to do. Or someone with a fused joint or something. A recurrent net should be able to do that kind of system identification on the fly. Are there any, like, experimental phenomena that you know that are like that, or have you.
00:44:31.896 - 00:44:56.048, Speaker A: I don't know, but I agree. And I think we're all kind of using the same rocket fuel. Right? It's like lstms can do a lot, and these are sort of two separate instantiations of, like, recurrent nets can, can actually do. Do a lot. Yeah, exactly, exactly, exactly. And there is this, you know, even after you've fixed things and it's running, you know, it has state that's that it's accumulating. So it can do, I think, system identification.
00:44:56.048 - 00:45:08.734, Speaker A: So, I mean, this isn't, this isn't fit exactly into your. What you're talking about. But we also had it doing things like predicting balls bouncing around and things like that. And it works. The balls bounce off the walls.
00:45:08.894 - 00:45:14.354, Speaker B: I'll let you move on to the next question. But I'm talking about predictions that can only be made by observing the dynamics.
00:45:17.614 - 00:45:42.394, Speaker A: Yeah. And if we had these have electrostatic repulsion or something, you could imagine them learning dynamically. This is the new rules. So, yeah, and this is the kind of setting I would probably prefer to do it in, rather than the other one, because you can't control the natural world as easily. But I think it's really interesting. I mean, there are limits to the capacity of this thing to learn, so it can't. You know, if you have very complicated movies and things, it doesn't do a great job of predicting the next frame.
00:45:42.394 - 00:45:52.114, Speaker A: So we've far from crack the nut, but it just feels like we're kind of moving maybe a little bit in the right direction. That's all we're planning. Are we out? One quick last question.
00:45:53.094 - 00:45:57.318, Speaker D: This sounds so much like the project I spent 20 years of my life working on.
00:45:57.406 - 00:45:59.354, Speaker A: I've had so many people tell me that.
00:46:02.054 - 00:46:22.234, Speaker D: I started as a postdoc, my NIH postdoc. It ultimately got published at nips as unsupervised pixel prediction. It's all that stuff. I was so proud of generating end stop. But that was in c, and that was a long time ago. And it was like this. It was just trying to predict a moving dot on toroidal boundary conditions.
00:46:22.234 - 00:46:23.498, Speaker D: Eight by eight pixel grid.
00:46:23.546 - 00:46:24.258, Speaker A: The simplest.
00:46:24.306 - 00:46:33.082, Speaker D: Stupid thing. No recognition of anything. It was only doing the motion continuity in time. I wrestled with it. It was awful. It worked kind of.
00:46:33.098 - 00:46:34.602, Speaker A: Okay. It wasn't a brain.
00:46:34.778 - 00:46:39.010, Speaker D: I kept doing it while I was working as a coder and an algorithm guy and stuff in Silicon Valley.
00:46:39.042 - 00:46:41.498, Speaker A: So it was always my side project for decades.
00:46:41.626 - 00:47:00.076, Speaker D: Went from C into Java. I worked at redwood neuroscience, took Lawrence Wiscock's feature analysis, threw away a year of code, redid it. But ultimately, what happened was, I realized, I ultimately gave a presentation here that it worked with just using matrices and just the algorithms. Forget the neurons.
00:47:00.140 - 00:47:01.144, Speaker C: It was that hard.
00:47:01.484 - 00:47:39.824, Speaker D: Then I threw away even the matrices, and finally got a zero parameter model where I just assumed a volumetric 3d computational medium, interstitial and intracellular space. So if you assume brains just do 3d computations intrinsically, it turns out everything comes out. But I don't have the recognition part, so I understand, I think, or I think we understand continuity in time, and it's just what you say, but trying to build it on top of deep learning, which was built around essentially n dimensional hyperspace learning, when all we have to do is 3d continuous learning. I guess it would be easier if you threw away the deep learning stuff and started from scratch.
00:47:39.944 - 00:48:17.470, Speaker A: No, it's not easier if you throw away the deep learning. This is the softkey and fits paper. Is that the one the softge I think we cited you, but I think the tools, the thing that's nice about deep learning, and maybe others have this intuition about it, it's nice to be able to say, here's our. I'm not saying presto magico deep learning. What I'm saying, I think we've gotten to the stage, but that's all abstraction now. What we can say is, if we just optimize future frame prediction, what else obtains? And I think that's the right framing for using deep learning to sort of. It's not that we're doing neurons or that they are neurons.
00:48:17.470 - 00:48:41.274, Speaker A: This has backprop. Clearly, there's going to be a couple of suggestions on how we could do it better. It's more like if you just optimize this one thing and deep learning lets you very effectively optimize that one thing, this is what comes out of it. So that's kind of the mode we're thinking about it. And could it be reduced to some other simpler thing? Sure, but we just wanted to have the tools that take us to that point. I think we're out of time, unfortunately. One more question.
00:48:41.274 - 00:48:49.266, Speaker A: One more question. All right. I'm trying to understand. Predictive coding creates kind of the idea.
00:48:49.290 - 00:49:06.454, Speaker C: That creates anxiety in my head, because you have these two competing ideas. One is that the neurons represent something. They represent this or that feature of an object. And this is the kind of work you did in your thesis. And your advisor certainly talked about what this neuron or that neuron represents. And then it's. And then in the next moment, the neuron doesn't represent a thing.
00:49:06.454 - 00:49:12.518, Speaker C: It represents, you know, in fact, it should. It should not fire at all if it's correctly representing the thing.
00:49:12.606 - 00:49:13.502, Speaker A: And so I'm trying to picture how.
00:49:13.518 - 00:50:06.266, Speaker C: The same neuron can be both representing something and then not representing something. And the question is, maybe the answer is in that. Your last picture, when you said sort of on the first pass through your model, basically the representational kind of thing is getting passed forward through these error units, which are the only things passing anything forward, which is a pretty strong constraint model. But then later, those same units must be passing forward something that means something different, which is the non predicted part, only those are two very different things at different times, and so are you. Is that the correct interpretation? Which is that sort of. There's a time multiplexing where maybe a first pass through the ventral stream is the representational pass and could lead to successful recognition of an object. And then later that thing is somehow shut off because everything is predictable or what have you.
00:50:06.266 - 00:50:10.122, Speaker C: Is there a time multiplexing of two very different quantities being passed forward?
00:50:10.218 - 00:50:59.554, Speaker A: So people have suggested time multiplexing in the responses before. So there's a paper, I think it was from Miashta's group. I could be wrong about that, where they're basically claiming that the initial pop of activity had different information than the latter part. I would say when Jim and I have said in the past that this neuron represents that thing, we're taking time like spike counts within a hundred millisecond window. After 100 milliseconds after onset of stimulus, that same neuron is categorically not doing that. 100 milliseconds after that, there's this weird phenomenon that a lot of the neurons are shut down. So I think this idea that these neurons, the firing of these neurons, signal that without any further additional context, I think that kind of has to be wrong at some deep level.
00:50:59.554 - 00:51:28.684, Speaker A: The fact that these things are so dynamic, I think at the end of the day, all the system cares about is that it makes the right call and initiates the right behavior. I don't think there's any requirements that there be labeled line neurons that signal features. So this is an example where you have neurons that will look that way if you look at them through that lens. But I think the reality of the situation is quite a bit more complex. And I agree it's unsettling, but we should probably confront it. Thank you.
