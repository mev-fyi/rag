00:00:01.440 - 00:01:27.324, Speaker A: Good afternoon. Thanks for coming to my talk. My talk is about obtaining a fester STP solver algorithm for certain class of rounding algorithms which appeared recently. So basically some of the for example, for famous problems for graph parchment problems such as maximum cut unit James independence sparse cut, we would like to get good approximation algorithms. And one promising way of doing so is by looking at hierarchies, basically STP hierarchies and such. And recently there has been a couple of rounding algorithms using basically moment based STP hierarchies like Lachar or Parildo or So's hierarchies. And all of these algorithms, all of these rounding procedures, most of them share a similar structure and we would like to exploit this structure so as to speed up the total rounding time.
00:01:27.324 - 00:02:28.124, Speaker A: So first let me give a quick overview, a quick overview of these algorithms using HTTP hierarchies. Various two CSP problems independent set partial coloring sparse has got minimum linear arrangement finding sparse vectors in random subspaces. All such problems found various. For all such problems, various results were found using Lacher hierarchy or so's hierarchy. Basically, given in addition to the input number of rounds are these hierarchies produce a solution of size n to the r. Therefore, just writing down the STP relaxation takes time n to the r. Hence running time has to be at least n to the r of any rounding algorithm.
00:02:28.124 - 00:04:44.558, Speaker A: And most of these algorithms are basically give some approximation guarantee when the archae value or some art mosaic expansion of the underlying graph has some large gaps. So as I said, most of these algorithms with couple of very interesting exceptions share the same framework. Basically, given an all round of solutions, they choose a set of, let's say seed nodes s and then they sample. Basically the STP solution assigns a probability distribution over these guys or the seed nodes, and they choose a labeling for from s and then extend this labeling to other nodes by simple conditioning. So basically by just looking at this, we can see that the running time of the rounding algorithm actually takes time two to the arc times poly n, right? Therefore it can only look at most the solution part of the solution it can look at most is two to the r poly n, but the solution has size n to the r. Therefore this naturally raises the question, can we pull the rounding algorithm? You know, we will, we want to feed it a partial and most probably not even like corresponding to a feasible part of a, not even corresponding to a portion of a feasible solution, and basically just, basically just fully running rounding algorithm. So in this work, basically we give such a STP solver which gives a just good enough feasible solution for the rounding algorithm.
00:04:44.558 - 00:05:56.770, Speaker A: Basically, rounding algorithm cannot tell the difference. And before moving on, I want to point out one small assumption which is standard for stps, we assume strict feasibility and yeah, main question is, can you pull the rounding? So now let me provide a little bit overview of how the rounding works. So basically, the rounding algorithm requests first block. Think of it as the thing of the whole solution has a giant support, let's say size and two to the n, and the rounding algorithm first requests to read a small portion support, s zero, let's say. So, rounding algorithm looks at the requests. This solution, it reads this solution, and based on this, adaptively, it requests, and requests the next portion of the solution to read s one. Basically, it enlarges the support and it wants to look at the new support and pay off.
00:05:56.770 - 00:06:42.374, Speaker A: Based on this, it asks for another support, s two, looks at the support and so on at the end. Basically, after, let's say, k iterations, the rounding algorithm is left with a portion of the solution with support on skin, and then it does whatever it is to obtain a rounding. But basically this summarizes, except this summarizes, except one algorithm I mentioned. This summarizes all other algorithms in the introduction slide. So, is there any question so far?
00:06:43.674 - 00:06:46.306, Speaker B: Can you give a concrete example of this? For instance, what?
00:06:46.410 - 00:07:26.464, Speaker A: Yes, coming. Yeah, ali, what's the exception? Oh, exception, yeah, exception is basically the, okay, so the only exception is basically finding sparse vectors because they don't like the way they condition on a random guy ocean. So the support grows unboundedly, I mean, grows like basically you cannot find it in a small support. Am I.
00:07:28.804 - 00:07:40.754, Speaker C: Different type of around? It's also a fixed number of rounds rather than something that grows with some parameters.
00:07:44.054 - 00:09:37.414, Speaker A: So let me give a simple example to this kind of rounding algorithms. Basically, I will just look at the problem of finding k cliques and basically any hierarchy that's worth the paper it's written on, you know, in K rounds will always be integral for this problem. So basically you, let's say we pick Sheryl Adams hierarchy, and intuitively it assigns probabilities to subsets of size less than k, right? We like we sold it for k rounds. What we get is basically probabilities on subsets. Basically these probabilities tell us how often does this subset appear in a clique as a whole? Like how often does all of them together appear in a clique? And then here is like surrounding algorithm, it finds a node with nonzero probability, takes it inside the seed set, and then now it looks for another node which has like a non zero probability of appearing with s together, and then adds this node to the seat set s and then iterates for k times. And basically as long as this subset has size less than k, there will always be a solid, there will always be a non zero, there will always be a u with nonzero probability of appearing together with s. And therefore, and by construction, the s we chose is a clique.
00:09:37.414 - 00:10:24.414, Speaker A: Therefore this is a simple rounding algorithm for k clique problem. And Pablo, does this make sense? It is a very good reason. I like show this example, because it also highlights the best of what we can. Basically it tells us what we can hope to achieve. It effectively gives us a lower bound on the running time of a solver. So the way I described algorithm rounding just takes n square time. But assuming a exponential time hypothesis, we know that you cannot find k fluid in time less than n to the k.
00:10:24.414 - 00:11:06.604, Speaker A: Therefore, the corolla as a corollary solver needs to spend at least time n to the k. Basically the number of outer iterations, the number of times the rounding algorithm gets to look at the current solution and then request another new support. To read that number, you cannot. The running time is n to the k, and to do that number of iterations, and to the number of iterations you need, we need at least that much time. So now let me, any questions so far?
00:11:07.664 - 00:11:12.564, Speaker B: I'm confused. I mean, for the click case, right? I mean this relaxation is indexed by clicks.
00:11:12.664 - 00:11:15.148, Speaker A: Yes, so, but you know, like you.
00:11:15.156 - 00:11:17.692, Speaker B: Are searching this ship but you are.
00:11:17.708 - 00:11:41.624, Speaker A: Hoping that you don't like you can find something without looking at the whole solution, like by just looking at a very few monomers. Can you do better? And the answer is no. But at least it says that you cannot. Like that says from a solver what to expect. The solver cannot do any better than that, assuming exponential type of hypothesis.
00:11:43.694 - 00:11:44.634, Speaker B: Okay.
00:11:50.614 - 00:13:16.340, Speaker A: So basically these rounding algorithms are parameterized by two, are parameterized by two numbers. First, it does, let's say, k order iterations, the number of times they request a new block to read, and each time the ratio of the new block size of the new block they read to the old one is bounded by, let's say two to the r. And basically this corresponds to r times. This corresponds to, this corresponds to r times k number of rounds for the given hierarchy. So if they given these two bounds, k iterations and two to the r, then we should, basically we show that you can find an SDP solver which fools the rounding algorithm in time two to the r times k squared times n to the k. And as I said, the number of rounds used will be r times k. Therefore the solver will take time n to the r times k, therefore.
00:13:16.340 - 00:13:27.644, Speaker A: So we are able to shave off like a vector r from n and instead make it two to the r. Confused?
00:13:30.144 - 00:13:40.244, Speaker C: If the number of exercise grows by a factor of m in each iteration, then it's like m to the k, right?
00:13:41.504 - 00:13:43.684, Speaker A: Yeah. So two to the r k.
00:13:45.464 - 00:13:53.104, Speaker C: Yes. So within the naive solver running time and to the m to the k, or where does the r come from?
00:13:53.184 - 00:14:37.130, Speaker A: Yeah, like, I mean, basically. So, I mean, think of it this way, right? So let's consider the so's hierarchy. You like find a set of nodes, and then you request all monomials for all subsets of those guys. So you request two to the r guys, and then you extend it, you add like r new seeds. So now you are looking at a set of size two to the r, and you request like monomials for all possible subsets. So that's two to the two r and the ratio is two to the r. Now, but does that answer your question?
00:14:37.202 - 00:14:38.290, Speaker C: I think so.
00:14:38.482 - 00:14:41.314, Speaker B: Okay, so I'm confused. What is r in here?
00:14:41.394 - 00:15:05.014, Speaker A: So r is like, both are parameters of the rounding algorithm. Usually these rounding algorithms take form like if arch eigenvalue does something, or like expansion of size sets of size n over r is at least something, then some, something happens.
00:15:05.094 - 00:15:07.798, Speaker B: Should I think of r as the epsilon sometime, or.
00:15:07.886 - 00:15:23.356, Speaker A: It's not really, you should think of r is like, basically, I mean, so, I mean, r is like you should think of like, you know, think of it as Logan, for example, is it.
00:15:23.380 - 00:15:29.372, Speaker D: Fair to say that at each iteration, you add our new variables to the base variables that you have?
00:15:29.428 - 00:15:35.116, Speaker A: Yes, yes, yes, yes, yes, yeah. And then you are looking at all subsets of those variables.
00:15:35.180 - 00:15:42.904, Speaker D: Subsets of those variables, which is true to the Rho by a factor of two to the r. Yeah. So initially you had nothing. Then you have r variables, then you have two r variables.
00:15:42.944 - 00:15:50.440, Speaker A: Initially you hit some. Initially, that's like initially maybe you hit something fully n. So it's a running.
00:15:50.472 - 00:16:34.584, Speaker C: Algorithm that has k that reads k are variables, but it does it in k round of productivity. So it reads like a bunch of r variables, and then based on those, it reads another, and based on those, it needs another argument. So in assumptions s of I, whatever, you can always assume that s of I is simply two to the I times r. Yes. So the second condition is kind of moot. So you can ignore the second condition, just say it's rounding that does k iterations, each one reading our variables.
00:16:38.044 - 00:17:01.744, Speaker A: Usually. For example, I don't know what like if that, like, I mean, so for example, on a graph like hypercube, usually you need r to be log n. So if that gives a reasonable range of r, like. But the usual thing is, of course rubber can be anything usually.
00:17:02.204 - 00:17:18.996, Speaker C: So you try to take advantage of less adaptive. Say that in a click thing, fully adaptive, you say r equals one. Then you really don't gain anything from your thing. But in other problems this is much less adaptive.
00:17:19.020 - 00:17:21.332, Speaker A: And then you can gain, yes, for.
00:17:21.348 - 00:17:30.672, Speaker D: Example the CSP algorithm, or just two CSP's in your algorithm in which r is everything, right? You just sample everything at once. There is no doubt, actually there is.
00:17:30.688 - 00:17:33.804, Speaker A: Like a, you need to do that sum one over epsilon times.
00:17:35.544 - 00:17:38.724, Speaker C: K only depends on the epsilon. And so you get like.
00:17:41.864 - 00:18:50.582, Speaker A: And in fact I want to mention one other thing, like for example, one of the other CSP algorithm, two CSP algorithm br is, can also be made in parallel, like so that also fits in this framework. So yeah, basically, yeah, so just compare this with the nice solver which takes time n to the rk and the lower bound which is n to the k, which corresponds to like full adaptive case. And. Yeah, so what's the main idea? Well, the main idea is basically we want to do this recursively. We are going to write a recursive ellipsoid algorithm, separation oracles, or the checks for the feasibility are going to be like recursive calls to deeper levels to another ellipsoid solver. Basically. And for example, what I mean, what do I mean? Let's say we know s zero.
00:18:50.582 - 00:20:06.914, Speaker A: The first portion algorithm is going, algorithm wants to read and we are, sorry, we are running the ellipsoid solver. It gives like some solver, some feasible solution for s zero, right? And then basically what we do is we call the separation oracle, which basically looks at s zero, runs the rounding algorithm which now requests another portion of the solution to read, which is s one. And then the separation oracle now wants to find a feasible solution on s one, which is consistent with s zero. Basically the part of the s zero which was fixed in the before in the current code is now a constraint. And the separation oracle now uses another ellipsoid algorithm to find a feasible solution in agreement with s zero. And then it finds one, let's say. And then it again calls the separation oracle now, which calls the rounding algorithm rounding algorithm says okay, I want to now locate like solution on support s two.
00:20:06.914 - 00:20:55.374, Speaker A: Maybe, you know, like the way we chose the first s zero, we made a mistake and it's impossible to complete it to s two. In this case maybe like we will, in this case we will run into a problem here, we won't be able to extend it anymore. And then basically here we won't be able to get a feasible solution. And then what we would like to do is basically out of this, convert out of this, try to get a separating hyperplane which is supposed to be like a violated constraint on the support of s one now. And if we can do this, we will be like almost done. So does it make sense?
00:20:56.774 - 00:20:59.190, Speaker C: And so what, can you just work.
00:20:59.222 - 00:21:42.582, Speaker A: Out the recursion like, yes, I will. So here is like in pictures what solver look like. First we get s zero, and then we get some feasible solution. And then rounding algorithm says I want to look at s one. And then suppose we were able to guess a feasible s one in like consistency with s zero. And then rounding algorithm says I want to look at s two now. And then turns out it's not possible.
00:21:42.582 - 00:22:32.054, Speaker A: Turns out it's feasible. What does this mean? Basically, each ellipsoid algorithm is going to encounter a lot of separating hyperplanes of this form on the support s two, right? So like basically we have all these hyperplanes and somehow we managed to conclude that this was infeasible. And for example, in the case of stps, this will be just a simple volume volume check. If the volume became small enough, then we declare that we are infeasible. And then what we want to do is basically try to find the separating hyperplane on the support s one. Only. The problem is the previous hyperplanes.
00:22:32.054 - 00:23:42.088, Speaker A: The previous constraints which rendered the problem infeasible were only full support of s two. They were like going outside of, they were like they can adversarially be chosen so that most of their mess is on s two minus s one. Therefore. But we want to be basically, we show that it's still possible to construct a separating hyperplane with support only on s one. This is the main technical part of the paper, and if we can do so, that means we are now we have a new constraint for s one, this green bar, and then we can recurse. So we went back in the higher, we went back to the current, and now s one in its ellipsoid procedures adds this constraint and then tries to solve again. And it will most probably so it will find another feasible solution.
00:23:42.088 - 00:24:18.022, Speaker A: And then make a recursive, recursive call. Since we updated s one, this time the rounding algorithm might choose to read some, something completely different. Right. And this is like, so this is the major obstacle. So basically now it chose to read s two and then maybe it again turned out to be infeasible. So, and whatever we tried, it turned out to be infeasible. We get all these three green constraints.
00:24:18.022 - 00:24:50.554, Speaker A: A couple of states later we declare this was infeasible. So f one is not feasible in one. And then we do the same. We can do the same process to convert these three green bars to only support s zero. Then we can move to the parent in the recursive call and the separation oracle, which like the first call to ellipsoid solver invoked, will declare this is a violated constraint.
00:24:51.524 - 00:25:00.412, Speaker C: So you make a poly and Oracle call the next level at each step. I'm sorry, you make poly and Oracle calls to the next level at each step.
00:25:00.588 - 00:25:13.868, Speaker A: Yeah, most probably. Yeah, yeah, yeah, yeah, yeah, most probably something like actually poly and to do something. Yeah, so, yeah, so I'm confused about.
00:25:13.916 - 00:25:25.794, Speaker B: What specific structure in here you're using. I mean, sometimes it's always true that if you have a, like a convex optimization problem, you can think about it as a projection on the first k coordinates.
00:25:25.834 - 00:25:26.170, Speaker A: Yes.
00:25:26.242 - 00:25:34.534, Speaker B: Right. And you'll have this kind of structure, but what are you actually using? Somehow the structure of the rounding method or.
00:25:36.154 - 00:25:36.794, Speaker A: Go ahead.
00:25:36.874 - 00:25:53.704, Speaker B: I mean, somehow for any kind of convex problem, you can think about a project on the first k coordinate and you can ask, can I extend the partial solution to k plus one coordinates? And somehow, I mean, at least what you're describing in here would apply. But you're using more than that, right?
00:25:53.744 - 00:26:21.308, Speaker A: Yeah, basically using the rounding algorithm as a separation or a rounding algorithm, you know, just gets to look at it adaptively chooses, but in the end it only looks at the feasibility of the portions it shows. So we are only trying to like find a feasible solution for that procedure, which might be like the rounding algorithm.
00:26:21.356 - 00:26:25.164, Speaker B: Gives you a way of extending a solution from k and k plus one.
00:26:25.204 - 00:26:25.784, Speaker A: Yes.
00:26:31.204 - 00:27:05.344, Speaker C: So suppose the algorithm was completely not adaptive. It only read one, it only read like the first set of k whatever things, and then, sorry, I should say our things and then made the decision. But maybe for some reason it only works when, when these are our projection of a much better solution. So how do you rule that out?
00:27:08.924 - 00:27:11.584, Speaker A: I'm not sure if I understand what you mean.
00:27:12.964 - 00:27:17.744, Speaker C: Is everything trivial if the algorithm was completely not adaptive? If k equals one.
00:27:21.764 - 00:27:47.694, Speaker A: Actually not much because, yeah, it can just choose to read something random, right? So yeah, it can. So basically whenever you update, like the solution at first level, still the browning algorithm will try to read some other portion. So basically that doesn't make the problem any easier.
00:27:47.814 - 00:27:49.350, Speaker C: So if k equals one, there is.
00:27:49.382 - 00:28:22.596, Speaker A: No so, but in this case k starts from zero. So like, okay, in k equals zero, then basically there is nothing to do, right. You know, the first portion algorithm needs, that's fixed. I mean algorithm, basically because the algorithm says I want to read s zero, but it doesn't, that s zero is fixed. So basically then you can just run ellipsoid algorithm, right? So what happens is algorithm, you know, algorithm is happy if it sees something, pst on that support.
00:28:22.700 - 00:28:29.140, Speaker C: So it might not be enough that it's, PSD wants it to be a projection of some bigger thing that it might not be.
00:28:29.212 - 00:29:01.688, Speaker D: So let me put it this way. So let me tell me if I'm completely wrong, but suppose I have a set s which your algorithm decides to read. So non adaptive algorithm will randomly pick some s of size k and would want to read that. So then you would say on the set s you would solve for all subsets of these guys using a normal SDP solver to do the k. And then for the others, you don't need anything for all subsets of those guys. You just need singleton vectors for them.
00:29:01.776 - 00:29:02.096, Speaker A: Yeah.
00:29:02.160 - 00:29:10.184, Speaker D: So then you just have n minus k additional vectors to put into your SDP. So you get two to the k plus n minus k variables and then you solve an SDP result.
00:29:11.284 - 00:29:36.066, Speaker A: Yeah, but the thing is, you can skip the initial step like you can, you know, those additional things still, you can just think of them as like a big giant family of like n times two to the k variables. Actually n. Choose two times k to the two to the k variables and then just do the solver on them. And it's basically, oh, oh, in that.
00:29:36.090 - 00:29:41.814, Speaker D: Case the guarantee is still n to the k. So it's, the guarantee is trivial for a non adaptive algorithm.
00:29:42.874 - 00:29:44.134, Speaker C: K equals one.
00:29:44.474 - 00:29:53.282, Speaker D: But what was the, it was two to the, it was two to the r square. R times k square times n to.
00:29:53.298 - 00:29:56.642, Speaker E: The k. So if k is one, it's very good if k is.
00:29:56.658 - 00:30:02.606, Speaker D: Oh yeah. So you can't think of n to the k, right. You just need poly n. You can't afford n to the k. More variables.
00:30:02.710 - 00:30:14.126, Speaker E: Yeah, no, I think what Ali was suggesting is let's not think of what variables you're reading as coming from the n variables. Big SdP solution should have n choose.
00:30:14.150 - 00:30:15.782, Speaker A: K, n to the n by two.
00:30:15.798 - 00:30:17.982, Speaker E: To the n to the k. Times are variables, overall.
00:30:18.078 - 00:30:18.430, Speaker C: Yes.
00:30:18.502 - 00:30:29.344, Speaker E: Unknown solution. The algorithm reads some subset of these values, n squared of them. And let's say it's adaptive, non adaptive, one shot. Let's say it reads n squared times.
00:30:29.384 - 00:30:30.712, Speaker A: Two to the k. Yes.
00:30:30.848 - 00:30:48.832, Speaker E: And then those are the only variants. This is subsidy reads. And then you run, you run the rounding scheme on an arbitrary solution. If your algorithm doesn't perform well, then you find a separating hyperplane, you run ellipse. Basically it reduces to ellipse on the projection onto those variables.
00:30:48.888 - 00:31:19.194, Speaker C: So you're saying you run the algorithm and the only thing it does is just, it gives you a bad answer. So you somehow able to convince to convert this bad answer into a separating hyperplane. And doesn't, no matter how you analyze your algorithm, no matter what kind of CRMs you used in the, because you somehow had some CRM that magically proved that the algorithm works. And you saying that you, whenever the algorithm fails, basically always a separating hyperplane.
00:31:23.104 - 00:31:41.640, Speaker A: I mean, so I think, I mean, so let's think about the so's hierarchy, right? It only looks at this support. And basically the property of the rounding algorithm is if it sees something, PST, that it's happy, it works perfect.
00:31:41.712 - 00:31:43.784, Speaker C: No, the property of the rounding algorithm.
00:31:43.824 - 00:31:49.774, Speaker A: Is that it's assumption, though. Actually, I should have mentioned it like before, because all of these algorithms do work like that. If they.
00:31:49.814 - 00:31:52.966, Speaker C: Okay, so you have a special property on the algorithm.
00:31:53.030 - 00:31:58.686, Speaker A: Yeah, yeah. So these algorithms, actually, yeah, I'm sorry, I should have mentioned it earlier. I'm sorry. Yeah, yeah.
00:31:58.830 - 00:32:09.782, Speaker C: So you have a very special property on the analysis of the algorithms is that these particular algorithms have an analysis that if the local solution, if the local thing is someone nice locally, the.
00:32:09.798 - 00:32:20.696, Speaker A: Algorithm still outputs a good solution and everything actually, like all the known algorithms. Yeah, except BKs, works like this.
00:32:20.760 - 00:32:23.960, Speaker C: Okay, okay. But this is an important restriction. Okay, now it makes sense.
00:32:24.032 - 00:32:26.616, Speaker A: I'm sorry, my mistake.
00:32:26.680 - 00:32:28.040, Speaker B: Sorry, can you say that again?
00:32:28.152 - 00:32:50.860, Speaker A: So these rounding algorithms, you know, like they just want to see something locally feasible in the sense that the portion that they read, sk, let's say the final portion they read, if it is PST, principle minus, you know, it corresponds to the principle of a matrix. If it is PST, they are hippy, they will run perfect, they cannot distinguish otherwise. Yeah.
00:32:50.892 - 00:32:52.548, Speaker B: Just consistency of things.
00:32:52.596 - 00:32:56.664, Speaker A: Yeah. And PST. Yeah. Those are enough to make these algorithms run.
00:33:00.764 - 00:33:02.004, Speaker E: Isn't that always true?
00:33:02.084 - 00:33:12.830, Speaker C: I don't think so. I mean, you could have, like, I mean, you could have a, that says if what you see is a projection of something much bigger, that was good, then you will be successful, right?
00:33:12.862 - 00:33:18.950, Speaker E: So then if you're not successful, then you're outside this convex set. So there exists a separating hyperplane, but.
00:33:18.982 - 00:33:21.094, Speaker C: Maybe you cannot find it from being unsuccessful.
00:33:21.174 - 00:33:28.646, Speaker A: So basically, actually the thing is, yeah, like he also wants the separating hyperplane to be supported only on the subset. So it will be supported on the.
00:33:28.670 - 00:33:32.874, Speaker D: Projection, because this finding the separate in hyperplane is that another SD.
00:33:33.924 - 00:33:59.820, Speaker A: So it's actually not an STP, but I will come to that. So, but I want to mention one thing. So basically, if the rounding algorithm can like just work as a separation or a, you know, it sees something, and if it is not good enough for it, it can say here is a separate violated hyperplane, then everything works fine. So in that sense, if you have a theorem that your algorithm can magically check, but still, if it is able to find the hyperplane, always good.
00:33:59.892 - 00:34:05.156, Speaker C: Okay, so you want some constructive proof of correctness for the rounding algorithm.
00:34:05.340 - 00:34:55.074, Speaker A: Yes, yes, yes, precisely. Yeah. So now let me move on to the abstract setting and formally, like here, formally make everything formal. So let's say we have a giant universal set. Uh, think of it as like a size n to the arch, or two to the n size subset. And then for every subset of this universal subset, we have a convex set case of s, which corresponds to the relaxation constraint, to relaxation on this support. Basically, you know, in So's hierarchy, it will be basically whether the principal minor corresponding to s is PST or not.
00:34:55.074 - 00:36:01.136, Speaker A: And then of course, what we, the other properties like relaxations should get stronger if the feasible set of s and feasible set of t should be like larger than the feasible set of S union t. And finally, just a notation, I will use PI as a projection matrix, and PI sub s has a projection matrix onto support S. Okay, so here's the rounding algorithm template. Basically, the rounding algorithm starts with some empty solution, say, and then it keeps track of the solution on support Si. And then it first calls its own separation oracle, which is part of the input, by the way. So it calls a separation oracle to check if it is feasible or not. So if it is not feasible, then it asserts infeasible and then returns a violated hyperlink.
00:36:01.136 - 00:37:10.494, Speaker A: Otherwise, if it is feasible, then it requires, it finds another, it finds it requests a new support to read. And then basically we locate the new support and then do this for k iterations. And at the end we just round using yknow sk. So basically all these algorithms, given a sequence of y's and s's, they like work fine. If each y is feasible for its own convex set and these seeds are chosen with respect to the, using the seed selection procedure of the rounding algorithms. And finally these solutions are consistent, then these algorithms are, will find a good solution. And yeah, this, this is the formal definition, formal requirement of what we want from the rounding algorithm.
00:37:10.494 - 00:38:34.428, Speaker A: But the thing is at the end, you know, the vice that made a rounding except might not be part of any feasible solution at all. And that's the point. So basically, how, how can we fool the rounding algorithm? As I said, we are going to use a, we are going to construct an ellipsoid algorithm which acts as a separation oracle. Also, basically it should be able to output a certificate of infeasibility. What do I mean by this? So given a separation or equal for some convex set k and then a fine subspace h, the algorithm should either be able to find the x which is inside this affine set and inside the inside k, the feasible solutions. Basically something that's consistent with what has been seen so far, or it should be able to find a separating hyperplane or violated constraint C of the form like inside only span of PI. Basically it should only be in the support of this affine subspace such that it separates the interior.
00:38:34.428 - 00:38:48.514, Speaker A: So it separates convex body k and h. So basically we have an h, we have a convex body and we want to find something parallel to h that separates k and why?
00:38:48.974 - 00:39:00.798, Speaker D: So for sk plus one you will find something, some point of sk plus one or something k, sub sk plus one, which is not in such that the projection is not in the span of Sk.
00:39:00.926 - 00:40:06.208, Speaker A: Yes, yes. So this is what I will refer to by inside with certification. So, and then given this, actually the rest is easy, we just compose this in a recursive way. Basically recursively, we compose this ellipsoid with certification for the next recursive call and then return its result. Any questions so far? Okay, so basically, now the final part of the algorithm is how to find the separating violated constraint on the restricted support. This is the final portion of the algorithm. Now as an input we are given a fine subspace PI, which is, which is like this.
00:40:06.208 - 00:40:40.078, Speaker A: And then our goal is either to find x inside in the intersection or some separating hyperplane. And let's say the convex body has no intersection. So existence of c is immediate. Right? This is just this. That's trivial from the emptiness of the intersection. But the problem is how do we find it, for example. Now let me mention one other thing.
00:40:40.078 - 00:41:29.116, Speaker A: If s was a polytope, then basically there is a variant of ellipsoid algorithm due to transcribers which manages to find this. So basically if we were dealing with polytopes, then everything was easy. Actually, I mean, not easy. That algorithm is quite beautiful and quite complicated, but at least it's something we know how to do. But the problem is, and this is true for like hierarchies like Sherry Adams, because when we condition or something, the rest is a polytope. But in our case, this is contained in a spectra header. So there is no such a procedure like GLS is never going to work.
00:41:29.116 - 00:42:23.332, Speaker A: So basically we want to compare, we want a procedure which like given a weak separation oracle constructs another separation oracle. So basically here is the picture. So again, we are at this point where we cannot find any feasible solution. And then basically, you know, we are just trying to solve the. Yeah, this is basically this, p is the constraints returned by the next level, also separation oracle. And we concluded that just by looking at this the problem was infeasible. But our main interest of region is k, right.
00:42:23.332 - 00:43:23.954, Speaker A: And then what we want is basically trying to find a hyperplane that goes like this. So what we do is basically just because we are dealing with weak separation anyway, we just shrink by a very small amount. We just minkowski subtract a small ball from p and then basically we can just show that the closest point of this new polytope is actually parallel to, is basically parallel to h. Therefore, therefore justice projection gives us a good. So justice C gives us the separate constraints we are looking for with the restricted support and. Yeah, so basically this is the final separation oracle. And.
00:43:23.954 - 00:44:26.844, Speaker A: Any questions so far? So this completely algorithm actually. So basically, now given this, for example, what we can like, things we can do are basically we can achieve like what's known in time two to the arc poly n. Basically this means, you know, like everything that we can do things with log n rounds in polynomial time now instead of quite polynomial time. So this, in conclusion, basically the main procedure we used was a separation oracle to a weak separation oracle. And we can do the roundings in polynomial time even if r is Logan. But the majority takeaway point is actually this. This only shows the weaknesses of roundings we know.
00:44:26.844 - 00:44:47.304, Speaker A: So basically this is the whole point of this talk. So because I mean, basically if you can, there must be something more to exploit in terms of rounding algorithms there. And so that concludes my thought. Any questions?
00:44:53.844 - 00:45:17.764, Speaker C: Questions. I'm wondering, I don't know if Aram is here wondering if it's related. This kind of weakness in rounding is related to the weakness in rounding, where you can transform it into an epsilon net argument. Although your epsilon net algorithms still takes the same polynomial time, it doesn't save in time, right?
00:45:18.584 - 00:45:33.164, Speaker A: Yeah, not usually. I don't know. The transformation is not the fly net doesn't start with the SDP. It's just a completely different method that happens if you have the same performance.
00:45:35.144 - 00:45:38.924, Speaker B: Here, you're not using the global constraints, you're only using the small.
00:45:40.844 - 00:46:01.420, Speaker A: I mean, we would like to use the global constraints, but. I know. Yeah. We end up using. And in this sense, like the recent algorithm of boss and Kerner and David is actually seems to beat this scheme. So it's the only example. We know that.
00:46:01.420 - 00:46:07.544, Speaker A: Yeah. Thank you.
