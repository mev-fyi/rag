00:00:00.360 - 00:00:18.914, Speaker A: Our next speaker is Vidya Muthukumar. She is a PhD candidate at UC Berkeley and a Google Research fellow in the reinforcement learning program. Vidya?
00:00:21.414 - 00:00:24.646, Speaker B: Oh, I was muted. Can you hear me now?
00:00:24.790 - 00:00:27.214, Speaker C: Yeah, perfect.
00:00:29.354 - 00:01:16.384, Speaker B: Okay, thanks, Prasad, for the nice introduction. I'm a Google research fellow at the theory of RL program this semester, and I'll be joining Georgia Tech as an assistant professor next year. So today I'll be talking about some recent results we obtained on the generalization error of over parameterized linear models, particularly for classification problems. And we'll actually compare these to results that are already known for the case of regression. So, this was joint work with Misha Belkin, Daniel Su, Adian Narang, Anand Sahai, Vignesh, and Mark Zoo. We can go to the next slide. So, the overall motivation here comes from this very nice recent set of empirical results showing the so called double descent curve in the test error for regression and classification problems.
00:01:16.384 - 00:01:36.276, Speaker B: So, for example, in the over parameterized linear model, we can see that in, in the modern regime, where the number of parameters is much larger than the number of training points, if we pick a particular kind of solution that perfectly interpolates this data, it's doing better and better in terms of the test error as we over parameterize more and more.
00:01:36.460 - 00:01:38.044, Speaker C: And as some of you may know.
00:01:38.084 - 00:02:17.066, Speaker B: This is very much, it seems to be very much at odds with some of the classical statistical learning theory, which suggests that things should only get worse as we add more and more parameters into our model. What's very nice about this set of results obtained by Belkin et al. Is that it recovered some of the trends that we see in over parameterized neural networks in the much simpler setting of the linear model, making that much more amenable to some nice theoretical analysis. We can go to the next slide. Thank you. The basic setup for studying these questions is the over parameterized linear model.
00:02:17.230 - 00:02:22.154, Speaker C: Um, and what that means is our output is, uh, a function of some.
00:02:22.194 - 00:02:29.714, Speaker B: Linear combination of d dimensional features. So, in the case of regression, this is very simple. This is just the linear model with.
00:02:29.754 - 00:02:32.730, Speaker C: Additive, uh, say, sub gaussian noise.
00:02:32.922 - 00:02:37.786, Speaker B: And in the case of classification, we're using what is called the random classification.
00:02:37.850 - 00:02:40.482, Speaker C: Noise model, where the label is, uh.
00:02:40.618 - 00:03:05.814, Speaker B: Could be equal to the sign of a linear combination of these features with some probability and with some non negligible probability. The label could also flipped. So, in this high dimensional regime, the number of features is greater than the number of samples. And we're also assuming random training data and an important quantity that will pop up again and again is this covariance matrix or the second moment matrix of the data.
00:03:06.514 - 00:03:08.214, Speaker C: So we can go to the next slide.
00:03:11.274 - 00:03:19.378, Speaker B: And in this over parameterized setting in general, we can consider the setup of linear equations from input to output.
00:03:19.546 - 00:03:23.386, Speaker C: And because the setting is linear, we.
00:03:23.410 - 00:03:26.530, Speaker B: Actually have infinitely many interpolated solutions here.
00:03:26.562 - 00:03:29.574, Speaker C: That is, we can satisfy this with equality.
00:03:29.874 - 00:03:34.562, Speaker B: And we're going to be interested in particular in studying the minimum tunarm interpolation.
00:03:34.618 - 00:03:35.894, Speaker C: We can go to the next slide.
00:03:36.994 - 00:03:42.690, Speaker B: For both regression and classification problems, and also the max margin SVM for the.
00:03:42.722 - 00:03:45.694, Speaker C: Classification problem, if you can go to the next slide.
00:03:47.944 - 00:04:09.264, Speaker B: So our main result was to analyze the classification error both for interpolation and SVM solutions, and show that this is actually a much easier problem than regression, in other words, that exist high dimensional regimes in this problem for which classification generalizes well, but regression does not. So I don't really have time to go into the details of all of these parameterizations.
00:04:09.304 - 00:04:12.080, Speaker C: But the main broad takeaway is that.
00:04:12.112 - 00:04:20.256, Speaker B: The properties of the covariance matrix really matter. For this problem, there needs to be some effective prioritization of some low dimensional features.
00:04:20.440 - 00:04:22.272, Speaker C: And the sense in which we have.
00:04:22.288 - 00:04:58.344, Speaker B: A separating regime is that for some particular parameterizations of this model, we can show that the limiting test error for regression does not go to zero, so it stays up at something like one, but the classification error actually goes to zero and we can go to the next slide. So the main takeaways are, as we just saw, classification can be dramatically easier than regression in even an asymptotic sense for these interpolated solutions. And a key technical component here was that we also showed that the SVM and interpolation are exactly the same in high dimensions, suggesting that training loss functions may not make much of a difference.
00:04:59.324 - 00:05:03.824, Speaker C: And I have just one more slide, but I can zap up here.
00:05:04.424 - 00:05:04.968, Speaker A: Thank you.
00:05:05.016 - 00:05:08.720, Speaker C: Just things on my mind are examples and logistic regression.
00:05:08.912 - 00:05:10.024, Speaker A: Thank you, Vidya, thanks.
