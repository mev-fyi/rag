00:00:07.400 - 00:01:16.164, Speaker A: Welcome to this tutorial on Sudo Boolean solving and optimization, which is part of the bootcamp for the satisfiability theory, practice and beyond program at the Simons Institute for the Theory of Computing. I am Jakob Nordstrom from the University of Copenhagen and Lund University. So this prerecorded part of the tutorial has four parts. Part one is on pseudo Boolean preliminaries, and then in part two I'll talk about solving of decision problems, then in part three about optimization problems when we're optimizing some objective function. And then there will be a fourth part about mixed integer linear programming, and the video will be divided into these four parts. And so this is the first part on sudo Boolean preliminaries and I'll talk about what Sudo Boolean functions and constraints are, what pseudo Boolean solving and optimization means. Give the formal definition and then give some references for further reading.
00:01:16.164 - 00:02:29.524, Speaker A: So what is a pseudoboolean function or a constraint? What does pseudobool mean apart from being a cool name? Well, it's simply what theoreticians would refer to as a function over the Boolean hypercube to the real. So the inputs are boolean variables zero, one valued, and the output of a pseudoboolean function is a real number. And these pseudo Boolean functions have been studied since the 1960s in operations research and in integer linear programming. We are interested in two restricted versions. First, where the functions f can be represented as polynomials, and in fact, throughout this tutorial we'll be even more restrictive and consider functions that are representable as linear forms. And it turns out that this is a rich formalism for expressing different kinds of optimization problems. So many problems can be described as optimizing some linear objective function under linear pseudo Boolean constraints.
00:02:29.524 - 00:04:14.332, Speaker A: So why would we do sudo Boolean optimization when we have sat? After all, the name of the program is the SAT program, but we also want to go beyond, and one reason to go beyond Sat to Sudo Boolean is that it's a richer format than the conjunctive normal format used in SAT. For instance, if I want to write down that the sum of six boolean variables is at least three, then in pseudo Boolean format I can just write down this inequality. Whereas if I write it as clauses, then I need this big chunk of clauses where we have just to make sure we're on the same page regarding notation. So these are disjunctive clauses. X one or x two or x three or x four is the first clause, and then all the clauses are anded together, and it can be verified that a Boolean assignment satisfies all of these clauses simultaneously if and only if their sum is at least three. Okay, so not only is the pseudo Boolean format more expressive, but the kind of reasoning that we can use, suitable boolean reasoning turns out to be exponentially stronger than the resolution based reasoning that underlies conflict driven clause learning, which is the dominant paradigm in SAT solving. And yet, the sudo boolean format is close enough to sat so that we can benefit from all the developments that have happened in sat solving since the turn of the millennium.
00:04:14.332 - 00:05:22.300, Speaker A: And since sudo Boolean problems are just another name, in fact, for zero one interlinear programs. As we will see, this means that we can also use ideas from zero one interlinear programming and try to exploit synergies. And in fact, a lot of what this tutorial is about is how to combine ideas from sat from zero one integer linear programming and maybe other areas to construct good solvers. So what is a pseudo Boolean constraint? Well, throughout this tutorial, a pseudo Boolean constraint is going to be a zero one integer linear constraint. So it's a linear sum of literals, that is variables or negated variables, but the semantics is that x plus x bar. So the negation of x is equal to one, and the a's are all integers, and then the bowtie is some kind of comparator. So greater than equal, less than equal, or equality, or strictly inequality and variables.
00:05:22.300 - 00:06:18.224, Speaker A: As we said, take Boolean values. We think of zero as false and one as true. In fact, we're going to, without loss of generality, use normalized form, where constraints are always written as greater than or equal constraints, and all coefficients are non negative integers. And also the constant terminal on the right hand side is a non negative integer. And when a constraint is written in normalized form, this constant term is referred to as the degree of falsity or just the degree of the constraint. So the clauses that we have in a conjunctive normal form CNF formula are just pseudoboolean constraints. We sum up the literals in the clause and require that at least one of them should be true.
00:06:18.224 - 00:07:19.524, Speaker A: We just saw a cardinality constraint. That's another special type of pseudo Boolean constraints that generalizes clauses. And then of course you can have general pseudoboolean constraints like x one plus two x two bar plus three x plus four x four bar plus five x five greater equal seven, say okay. And in fact, it's not hard to see that this conversion to normalized form is is without loss of generality. For instance, if we have this constraint. Suppose we have the constraint minus x one plus two x two minus three x three plus four x four minus five x five, and we want this to be strictly negative. Then, in order to turn this into normalized form, we can first make the inequality non strictly strictly smaller than zero just means less equal minus one, and then to turn it into greater than or equal, we multiply by minus one.
00:07:19.524 - 00:08:19.878, Speaker A: And in the next step we want to make all the coefficients positive. And we do this by replacing a negative literal minus l by minus one minus l bar. So if we do this, we get an x minus two times one minus x two bar plus three x minus four times one minus x four bar plus five x five greater or equal one. And now we collect all the constant term and shift them to the right hand side. We get, in fact, the boolean the pseudoboolean constraint we had before x one plus two x two bar plus three x three plus four x four bar plus five x five. Great three equals seven, and it should not be too hard to see that this will work in general. Also, if we have an equality, we just replace that by two inequalities, a greater than equality greater than equal, and a less than equal equality.
00:08:19.878 - 00:09:41.234, Speaker A: And then we do this transformation for both of these constraints. So in view of this, we will always think of of constraints as being in normalized form. But we can also freely allow ourselves to use syntactic sugar when convenient. So if we have a linear form AI li, where the sums of the coefficients is m, then a strict greater than inequality is just greater than or equal with the a plus one instead of a and less than equal equality sum AI li less equal a. We just flip it around and get sum AI li bar greater equal the total sum m of the coefficients minus a, and then similarly for strictly less than and as we already said for equality, we replace it by two linear inequalities. So in the rest of this tutorial, whenever it is convenient, we will write the linear inequalities in whatever form is convenient for us. But whenever it matters, we explicitly assume that all constraints we're operating on are normalized.
00:09:41.234 - 00:10:59.694, Speaker A: Another aspect that is worth commenting on is that we restricted our attention to linear constraints. But if we have nonlinear constraints, say we have a polynomial sum of AI mi for I goes from one to k greater equal a, where the m I's are monomials, then we can in fact write this as a collection of linear constraints. We can linearize we can introduce fresh variables y I for each monomial mi and write the linear sum AI yi greater equal a. And then we specify for each yi that yi is one if and only if all literals in the monomial are one. So this is possible, but we won't talk more about this during this tutorial. We'll for the rest of this talk, we'll only consider linear constraints. Some convenient notation that we will use should be fairly obvious, but just to make clear what it means, if we have two constraints, then we can take their sum.
00:10:59.694 - 00:12:10.634, Speaker A: If we have a constraint in a linear form, then I can just add this linear form to the constraint. Given a positive integer, I can multiply a linear constraint by that positive integer. And so we'll use this notation. And again, assuming that these resulting constraints are normalized whenever this matters, some further notation that can sometimes be convenient. And I mentioned this also just to make sure that we are familiar with how constraints work and how we can play with them. If I have a pseudo Boolean constraints c sum AI li greater equal capital a, where I use this dot equality throughout the talk to denote syntactic equality and the sum of the coefficients again is big m, then the negation of this constraint is is also a pseudo Boolean constraint. In fact, it's the same thing as the normalized form of sum of AI li strictly less than a.
00:12:10.634 - 00:13:47.934, Speaker A: I can also have so called reified constraints where I have a new variable z, and I can write down constraints, saying, for instance, that if z is true, then the constraint c holds. And this would, if you think about it, this would be the constraint capital a times not z plus some AI li greater or equal a, where a is the degree of falsity. So, and in the other direction, I can say that the constraint implies that z is true for a fresh variable z, by writing down the constraint m minus a plus one times z plus sum of AI li bar greater equal m minus a plus one. And if you just take a minute offline, you verify that indeed these constraints mean what the notation suggests that they should mean. And in fact you can do the calculations that if I take any constraint c and not c, and I add them up, then I get contradiction, which is saying what we know, namely that both constraints cannot really be true simultaneously. So contradiction is zero greater or equal one. The reification constraint saying that the constraint c implies z is in fact the same thing as saying that not z implies not c, which is also easy to verify, and it seems natural.
00:13:47.934 - 00:14:42.184, Speaker A: And the way we can play with refined constraints is that if I have derived now that z is true, so z greater or equal one, and I multiply it with the degree of the constraint c, and then add this to the constraint z implies c. Then indeed I get c back. And in a similar way, if I take the constraint saying that c implies z and I add the constraint c to it, then I get that z is true. In fact, I get that z times the degree of falsity of the negated constraint is one. But clearly that implies that z is one. Okay, so I'm saying this just to play around with the constraints to make sure that we're on the same page. So a pseudo Boolean formula now is simply a conjunction of pseudoboolean constraints.
00:14:42.184 - 00:15:56.110, Speaker A: And so sometimes we can write this as the set of constraints. Here I write it down as a conjunction of the constraints to make explicit that we, in order for the pseudo Boolean formula to be satisfied, all constraints should be satisfied. And pseudo Boolean solving is the problem given a pseudo Boolean formula to decide whether it is satisfiable or whether it is feasible is another term that is used for this. A pseudo Boolean optimization problem is where we have, in addition to a pseudo Boolean formula f, also an objective function, which I'll tend to write summing over iwili and we want to minimize the subjective function. So whenever we have a pseudo Boolean optimization problem in this talk, we always want to minimize the objective. And this is of course without loss of generality, because if we want to maximize, we can just negate the objective function and then minimize that instead. I said that sudo Boolean optimization allows us to express many problem in a natural and concise way.
00:15:56.110 - 00:17:08.284, Speaker A: And just to give some examples of this, if g is an undirected graph and we have a weight function on the vertices v, then weighted minimum vertex cover is simply asking for a vertex cover. So we have a variable for every vertex. These are the vertices we're choosing to be included in the COVID So a vertex is in the COVID if xv is one, and for every edge we want one of the endpoints in the COVID But then we want to minimize the cost of the COVID So summing over the COVID summing up the weights of all these vertices. Another problem that is similarly easy to express is weighted maximum clique. So in a clique we want to pick as many vertices v as we can with the restriction that if uv is not an edge, then we cannot pick both u and v. So under this restriction we want to maximize the sum of the chosen vertices. Or since we're insisting on minimization, we just add a negation in front.
00:17:08.284 - 00:18:16.174, Speaker A: Finally, another problem slightly more general that will be relevant later in this tutorial is weighted minimum hitting set. So here we have a collection of sets living in some universe, and we have again a weight function on all the elements in the universe, and we want to find a hitting set. So a hitting set is just a set that has a non empty intersection with all the sets in our collection. And for such a hitting set we're looking at one of minimal weight. So summing up all the weights of the chosen elements in the set, the weight should be minimal. It's not hard to see that this is just a generalization of vertex cover, and the pseudo Boolean optimization formalization of this is again just we're minimizing the linear sum of all the variables. One variable per element times its weight under the restriction that for every set at least one element in it should be chosen.
00:18:16.174 - 00:19:15.722, Speaker A: And it's worth noting that in fact all of these examples that I gave are even more restricted than general Boolean optimization problems. In fact, here we're asked to optimize a linear function subject to just a CNF formula, because all the constraints we see are just clauses. So already this is quite an expressive framework, although in general, of course it might be that the constraints should also be general pseudo Boolean constraints. So what we will talk about in the rest of this tutorial is first we're going to talk about pseudo Boolean solving and optimization. Basically what I mean by this is that we're going to start with Sat solving. And inspired by Sat solving, we're going to take these conflict driven techniques and try to lift them to the suitable boolean setting. And this is what most of the tutorial will be about.
00:19:15.722 - 00:20:24.278, Speaker A: Then we'll also talk about the very closely related topic of Max Sat solving. And then in the final part of the tutorial we'll talk about integer linear programming, or more generally mixed integer linear programming, or MIP. And very roughly one can say that an interesting difference between these approaches is that for sat solving and pseudoboolean solving you are always doing boolean reasoning and you're focusing on finding integral solutions. So zero one solutions and trying to zoom in on the optimal one. In some sense, interlinear programming or mixed integer linear programming tends to operate from the other direction. You relax the problem and find optimal non integral solutions, and then from there you try to search for good integer solutions nearby, as it were. But for all of these settings.
00:20:24.278 - 00:21:59.224, Speaker A: A basic trade off that we will see several times during this talk is the trade off between inference power versus inference speed, where some solvers will optimize being very, very smart, though this takes more time and other solvers sacrifice intelligence, maybe for speed, and hoping that raw speed will compensate for being slightly dumber with your inference. Although this will be a somewhat long collection of videos, it will still be only a very, very selective survey of this area. So some references for further reading will four chapters in the Handbook of Satisfiability that is just out. The second edition is being printed and distributed as we speak, and it's a fantastic book. In particular, I can recommend chapter seven on proof complexity and sat solving, which contains a lot of the material I'm going to talk about. Also chapters 23 and 24 on Maxsat, and chapter eight also contains material on pseudo boolean solving for mixed intergeliner programming. There is nothing in the Handbook of Sat, but a good starting point might be these papers that I'm linking to here.
00:21:59.224 - 00:23:00.564, Speaker A: And then for Max Sat and mixed interdeprogramming, I can also recommend the tutorials the Maxsat tutorial at Ikai 20 last year, and for MIP there's a very nice tutorial by Ambrose Gleichsner at a workshop in Oaxaca a couple of years back, and those will give more details on Maxat and MIP, which are topics that are not so well covered in this tutorial. So with this, I conclude the first part of the tutorial on Sudo Boolean solving and optimization, where we just surveyed the preliminaries. Next up we're going to talk about how to solve Sudo Boolean decision problem, how to determine whether Sudo Boolean formula pseudo Boolean formulas are satisfiable or feasible, and then later on we'll talk about different ways of optimizing.
