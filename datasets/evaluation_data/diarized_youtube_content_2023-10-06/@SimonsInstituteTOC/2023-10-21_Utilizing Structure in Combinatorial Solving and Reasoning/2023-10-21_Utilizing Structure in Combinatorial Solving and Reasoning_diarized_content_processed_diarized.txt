00:00:00.680 - 00:00:55.724, Speaker A: Thanks very much for the introduction. Maybe as a disclaimer in advance, this is about combined complexity stuff, and it touches the surface of circuits. But I guess regarding some discussions on Wednesday, I think it's still very interesting that is. Okay, so I guess we all can agree that like in the last couple of years, there has been significant progress in solving commodore problems in practice. And one of the frameworks that one could use there, besides a satisfiability, is programming or logical programming, where we model problems by means of logical programs. And solutions to this programs are called answer sets. In fact, it's a quite generic framework used both in academia and industry.
00:00:55.724 - 00:02:05.554, Speaker A: So there are already two university spinoffs. One is from Calabria, one is from University of Potsdam in Germany. So what we can do there is like model common royal problems like optimizing shifting plans or assigning machines products in different production phases to machines, such that the overall production phases optimized. We can also do like certain logistics problems, like in a classical warehouse scenario where we essentially have goods that we need to bring from a warehouse to a dispatching system, such that this can be done via say robots that kind of transport shells from one location to the other such that they do not collide. And we want to do this efficiently. We can also do train scheduling and so on. So what do these problems have in common? Well, they can be modeled as a coronatorial problem.
00:02:05.554 - 00:03:17.370, Speaker A: Now the question is why? And like in which cases can resolve them efficiently? Well, actually the answer is not fully settled, so it's still not clear, like whether they give you a set instance or a logic program and ask you can this be solved efficiently? In general, what we know though is that the instance structure might help. And this is exactly what this talk will be about. So it's about Trivith, one of the most cited commodore invariants. So like in general, we know that problems on graphs or like arbitrary structures are quite hard. But if you look at the special cases like bound to triveth, then most of problems become easy, or many of them become easy. And trivia is usually defined in terms of treaty compositions, and the 3d composition can then be used to obtain so called FPT results. So we are focusing on algorithms that run in time polynomial and the instant size.
00:03:17.370 - 00:04:37.714, Speaker A: But additionally the runtime also depends on the value of a function depending only on the parameter tree with k. In a sense, we want to restrict the hardness or the combinatorial explosion to this trivis parameter, which is in contrast to the general case where we cannot confine hardness right a priori so the question now is how do these f of k look like? And if you have specific f of k's, maybe can we do better, can we improve? So kind of as an intuition, these f of k's give you the so typically the way these algorithms work is like bundles dynamic programming on the tricomposition, where we essentially compute for each node the table. And then in the end we need to like modify these tables from the leaves to root. So it's a bottom up computation. And when we arrive at the root of the decomposition node and there is non empty table, we know there is a solution, we can actually construct these solutions. So this f of k kind of gives how hard or what we expect in terms of runtime for each of these tables. Kind of the subproblem solving effort and trivia kind of gives you like the largest sub problems that we encounter kind of during these computations.
00:04:37.714 - 00:06:16.032, Speaker A: So how do these functions look like and can we do significantly better? And then maybe as a side question, does this imply consequences in practice or like depending on different f of k's, do we still expect solvability in practice? So what I will show in the next couple of minutes is a decomposition guided reduction, which is a parameterized reduction that gets a treaty composition. For now we can just assume a polymer time reduction, and the purpose of this reduction is given an instance and the corresponding treat decomposition reduced to another problem such that we immediately get a treaty composition of the resulting instance. So it's kind of a point over time reduction, if you want, for this talk where the instance we construct wider reduction has a tight relation to the treaty composition. Aristotle like a functional dependency dependency. With these reductions we can show specific random upper bounds for several problems relevant to AI and knowledge representation reasoning. Here we will mainly focus on specific f of k values for logic programming. We accompany this by corresponding lower bounds under the exponential time hypothesis, where we provide a brief glimpse into how one can show lower bounds for for problems harder than the polynomial hierarchy, and then what we just trigger under the exponential time hypothesis.
00:06:16.032 - 00:07:06.964, Speaker A: And finally, we still show how despite certain, say scary f of k functions, we can still efficiently utilize true to some extent in practice. So what we can do is we can do a table right specific decision problems. You can also do counting problems there non classical complexity results and corresponding random dependency. What might be interesting here is like the difference between tight Asp and normal Asp. These are specific fragments of logic programming. Both are MP complete, but the true dependency is expected to be different under the exponential time hypothesis. So for normal Asp we expect it to require slightly more effort despite both the same classic complexity.
00:07:06.964 - 00:08:11.360, Speaker A: Yeah, and then there are different extensions of logic programming and harder fragments that give larger trivia dependency. Okay, so back to Trivitz. I guess most should be familiar now with Triveth, since this appeared already several times in the course of this workshop. So tree width kind of gives a way to capture the tree likeness of a given graph, or essentially shows how close a certain graph is to being a tree. So what is a tree decomposition or joint tree? We also had this rest several times. So three decomposition of a graph is just an arrangement of the graph into a tree, where each node of the tree called back, it's assigned vertices of the graph, such a few three properties. All the vertices of the graph for which we can be decomposition have to be captured by a tree, so they have to appear at least one back, every edge has to be covered.
00:08:11.360 - 00:09:08.772, Speaker A: So two endpoints of vertex are covered, every edge has to be covered, so the twain points of each edge, you have to be together at least one back. And finally, connectedness is important for us to design nice algorithms on top. So if you restrict the tree to a certain vertex, the result is to be still connected, which you can check here for all the vertices. Most importantly, the width of a three decomposition is the largest back size. And finally, the three width is kind of the best width we can hope for. So like smallest width, more potential 3d compositions transcends kind of tells us if you want to use true width, this is kind of the largest subprocon size we expect when we solve no problem. Any questions so far? What's the capacity of finding the optimal criticality? You can do this in, it's np hard, but you can do this in single exponential timing a bit.
00:09:08.772 - 00:09:54.004, Speaker A: There are like two approximations. I think it's into the k time. Yeah, you can also compute it optimality into the k squared or something. It's the latest result from this here or something. So, but usually, I mean if you are just interested in working with a 3d composition, you can use efficient heuristics. So what's the decomposition value reduction? We have been given a treaty composition of a graph representation of an instance, and we want to compute a destination instance or an instance of the target problem. And the idea kind of is that we use this decomposition to construct the resulting instance.
00:09:54.004 - 00:10:49.252, Speaker A: And then one can do a function dependency between source corresponding source pack and destination pack. So in the end we immediately get the 3d composition as a result, which function depends on the instance on the tree decomposition of the instance which allows you to immediately show certain width increases or decreases. Yes, and this was exactly the question that was already asked some seconds ago. Thanks very much. So, we can heuristically compute 3d compositions and we can also approximate them in same explanation time. Okay, so what, how do we use these decomposition guide reductions to show up about. Well, the known reductions before from ESP to set.
00:10:49.252 - 00:11:24.266, Speaker A: Why do we want to reduce to set? Well, for SAT, we know that it can be solved in sig exponential timing for Asp. Assuming, if nothing is known, we want to show this result. Right. So the known reductions before essentially didn't quite take care of the tree with so. And one can show examples that all of these reductions essentially massively blow up the tree within the worst case saying instance size large. So what we want to do is we want to do a reduction from AsP to sat there by just slightly increasing the trivia. So from k to k log k.
00:11:24.266 - 00:12:11.664, Speaker A: In general, before we're going to do this, let's briefly revisit answers that programming. So, a logic program or an answer set program is a set of rules which are essentially just implications of this form, where we say we have a positive body and a negative body of each rule. And essentially this means that whenever we set the positive body to true, all the atoms there and none of the negative body, then also the head is set to true. So this is the one thing, the one side of the semantics. So we require satisfiability in the sense of an implication. But in addition, we require something else. We require that everything we set to true or everything that we have in our model is proven.
00:12:11.664 - 00:13:13.146, Speaker A: So what does this mean require that exists a rule for every such a with a in the head, such that all the positive body atoms are in m and none of these negatives are in m. Actually, there's a little bit more to it. So we should also exclude cyclic proofs, because the, the idea of this formalism is essentially that you require witness for everything you derive or set to true. So if you have like a cyclic proof that just depends on itself, you again, just don't have a witness, right? You require everything you derive, that you have a justification for everything you derive. And I think this is one of the simplest formalizations of Asp decision complexity. Is Mp complete since we can do sat, I mean, try the part of SAT satisfiability. Okay, so how do we do this? Now, let's look at previously briefly an example.
00:13:13.146 - 00:13:52.524, Speaker A: Consider the program PI, where we have like, not b, so require a if we have not b, require b if we have not a, require b if a and not c, and require c if not d. There are two answer sets of this program. One would be ac and the other one would be Bc. Essentially this can be seen as kind of a guess between our choice between a and b. This one is never applicable because here we require that if D set to false and c is set to false. So this one is essentially irrelevant. So essentially we just have a guess between a and b and we require to have c if we do not have b.
00:13:52.524 - 00:14:42.414, Speaker A: Or in other words, all of these are justified because a is justified by this rule, c is justified by that rule, and in this case b is justified by this rule, and c is just in fact. So these are all the answer sets. And in order to utilize true width, what we need to do, we need a certain graph representation. Here. Let's say we are satisfied with the primal graph where we have as vertices the atoms of the program, and we place an edge between two atoms whenever they're corresponding between two vertices whenever the corresponding atoms appear together in at least one rule. So this would be like a structural representation of this program. And now in order to do a decomposition guided action, we need a three decomposition.
00:14:42.414 - 00:15:32.234, Speaker A: We can approximate it efficiently and the goal would be to kind of guide our reduction along this tree decomposition from the bottoms to the leaves in order to augment get a new three decomposition where we have sort of provability of an atom up to a certain node. So essentially we have like a linear blow up in the back size. If you compare predecessor back compared to the successor back. Okay, so how do we do this? We got proof ability of our atoms along the tree decomposition. What we can do is first of all we check whether all the rules are satisfied. This is just the implication part. So for each rule using only atoms of a bag, we just essentially write down the implication as a logical formula.
00:15:32.234 - 00:16:26.314, Speaker A: Then what we need to do is we need to define provability of an atom up to node t. Which means whenever there's a rule in a bag containing x in the head, then either we have provability in this node, which is if there is in the rule, we have all the positive body atoms and none of the negative body atoms, or we have provability already below an ode in t and we do this for all the elements of the bag. And finally we just have to say whenever we set something to true. So whenever we derive something we require that it's proven in the end. But do we mean by in the end whenever we have a bag with x in it, where it doesn't occur anymore from there on. So this is actually all that's needed. One can easily transform this into a proxy monitor reduction by replacing implications here by equivalences.
00:16:26.314 - 00:17:28.194, Speaker A: So how does this look? On our example, we take the program and corresponding three decomposition. We construct the formula as specified, essentially just transforming the implications into CNF, which is trivial, right? Defining proofability of an atom up to a certain node, and in the end enforcing whenever we set an atom to true, then require that it's proven up to the last note. So here, like in t two, a is forgotten, because here it appears for the very last time. So whenever I said h two, require proof of the a after t two, and we almost immediately get the corresponding treaty composition with nice properties. So here we see it's linear, and I promised something like k log k, right? The point is, this was just for tight programs, because I didn't consider cycles so far, what I mean by cycles. As already mentioned, we do not allow psychic dependencies, positive cyclic dependencies, for approve. So this would be in that case.
00:17:28.194 - 00:18:35.054, Speaker A: Okay, so in order to address this, let's just assume such a program where we have psychic dependency among the positive body atoms. Essentially we cannot prove any of these positive body, any of these atoms, because essentially we cannot start, right? The proof of a require depend, the proof of b depends on the proof of a, which depends on proof of c, and so on. So it's, this program only has an empty answer set as a solution. So in order to treat these cycles, and this is essentially what gives this blow up, what we need to do is we need to kind of partially order atoms. So in each bag we locally order the atoms and ensure that this local ordering is essentially consistent with neighboring treaty composition nodes. So we require like log k additional bits for each 3d composition node and guide this information on the 3d composition. If one carries this out, cares this out carefully, we obtain the mentioned result.
00:18:35.054 - 00:19:36.252, Speaker A: However, the reduction is not parsimonious, because essentially we have like a partial ordering and we do not know global position in this ordering. So the current suspicion is that counting answer sets for normal programs is expected to be harder than the decision problem. However, there is no formal proof for it, so be happy if somebody would be interested in that. So we also did some practical experiments. So we implemented reasoning over logical programs. We also considered the problem, say formalism. And interestingly, what we compared to, we compared just like the regular way of encoding provability with provability guided along a treaty composition and these n versions are the vanilla versions without using true with or treaty composition.
00:19:36.252 - 00:20:50.606, Speaker A: And the other variants are essentially extensions where we use different variants of these decomposition guided reductions. So it seems that even these knowledge compilers in our settings seem benefit from the information of the three decomposition. This is a takeout takeaway from this slide. Yes, with this we can also do some other special fragments where we essentially kind of bound, say the cyclic dependency of positive cycles in the dependency graph of programming. So the question now is like can we significantly improve or can we do better than in time due to the k log k times poly in n? And so like can we solve significantly better in time significantly better? And the answer is unfortunately we can't, or at least it's very unlikely. So this brings us to decomposition guide reductions for using lower bounds. So what's known is that like these cyclic dependencies are kind of the reason for hardness of logic programs.
00:20:50.606 - 00:21:25.826, Speaker A: So there is a known result. So if you do not use additional auxiliary variables, you cannot translate normal EsP to sat without an exponential blow up. Why? But this really uses prohibits auxiliary variables. So for trivial it's actually slightly different. So the underlying reason for hardness in a sense is that we do not see these positive cycles in the worst case. So it's like they might be completely spread across the whole 3d composition. It's similar to like following a magnifier glass along a very large cycle or many cycles.
00:21:25.826 - 00:22:57.154, Speaker A: So we do not see these cyclic information all at once. So this kind of prevents an algorithm from analyzing the cyclic dependencies and bunk and show hardness or the two to the k log k lower bound by reducing from a bob and called disjoint paths where we have to find vertex disjoint path between pairs of source and destination vertices. And the idea is that, well we know a bound from this case, so cannot do better than with f of k to the, and the challenge now is to reduce from disjoint path problem with just a linear true width increase. And again we can use a decomposition guided reduction for that and ensure that the true width increase or the width increase is actually just linear. And the idea is there to do reachability with reachability vertices, but not for each pair because this would already blow up the width probably to guide along the decomposition whether you already have chosen for a specific vertex, an edge, an outgoing edge, and in the end we need to link those pairs together in a certain order, since the order is not clear in advance. We also do this along the 3d composition, because there we know that this order that has been captured in the treaty composition doesn't blow up the treat width. So with that we obtain the corresponding lower bound.
00:22:57.154 - 00:24:23.610, Speaker A: Another question would be can we maybe generalize this technique or can we use decomposition guide reductions as kind of a tool to show lower bounds for higher level problems? Ni and Kr, they appear naturally there. The obvious candidate, or one obvious candidate is deciding the validity of a quantified boolean formula because there we know an algorithm which runs in alpha exponential time in the tree width with truth on the top. So it's two two two height l truth on the top. And what we showed is that you cannot significantly improve that exponential time focuses and the proof goes by a decomposition guide reduction from quantified pooling deciding the leader of a quantified pool in formula with l quantifier blocks to increasing the quantifier block by one. Plus this increase of quantifier block allows you to significantly compress the true width so we can compress the true width from k to lock k at the cost of an additional quantify block. And the reductions that have been done so far before were like reducing from three set using ETH. So under ETH you cannot expect or ath implies that you cannot solve three set in time better than single exponential in the number of variables.
00:24:23.610 - 00:25:08.574, Speaker A: The previous attempts were reducing from three set to like say qz two. But the new trivia is like logarithmic in the number of variables of the instance I started from, this approach is slightly different. We reduce from k from three set with truth k to qz two with truth log k. So we have like a relation here. And why is this more beneficial? Well, I would just say it's simpler because with the same technique, with the same idea, we can just reduce from q l to q l plus one and essentially doing the same reduction. Whereas I have honestly no idea how to reduce three set with two with k from to qz three. So three quantifier blocks and making a log log n.
00:25:08.574 - 00:26:10.884, Speaker A: This idea essentially doesn't easily generalize to that one. So the advantage here is that we can actually do induction over it and obtain a lower bound for all the cases. So maybe as a so I did a weird figure just to show maybe as a disclaimer, this is kind of a simplified illustration of how other reduction goes. Essentially the idea is as follows. We start with the treaty composition with log k truith we cannot in a bag refer to more than one element of the original bag. So we use concept of pointers where we just refer to one element of the original bag. And in the end these pointers allow us to kind of make a new reduction that gets rid of connectedness of the trivet.
00:26:10.884 - 00:26:46.750, Speaker A: So whenever we introduce a certain vertex of the graph, so we haven't seen it so far in the 3d composition, we make a new copy. So we have all these copies. And so essentially we don't have connectedness anymore, right. And we can make sure that there is only like a constant number of introduces in each 3d composition node. So these are like constantly many. And then with these pointers and the additional quantifier block, we kind of repair connectedness again. So it's like first of all we destroy the tree decomposition property.
00:26:46.750 - 00:27:47.132, Speaker A: So getting rid of connectedness, adding copies, and then using this additional quantifier block, we enable to synchronize these copies such that in the end the copies are in line as before. So it consists of three parts. The guest part is the thing that happens when we introduce the variable propagate part happens between neighboring bags, ensuring basically that we repair connectedness. And to check parts, ensure that we can actually check corresponding clauses or three CNF clauses if we still have time. I can show a brief example which actually doesn't use clauses, but terms. So we have like exists for all QBf with these four terms, as before, we can do a primal graph representation. So this would be a primal graph.
00:27:47.132 - 00:28:35.398, Speaker A: In order to make it simpler, we can construct a 3d composition, say again via the two approximation, and we construct a 3d composition where we only have like one, we only see one term in each node. This just makes it simpler. And then the resulting reduction kind of looks like this. So we have these copies. So if you compare one block from our source instance and destinations, we have these copies for the universal ones. Of course, when we do copies, we need to shift it to the next existential block, because in the end we want to ensure that these copies have the same value, right? So you cannot just universally quantify both copies. We have the bit variables to ensure that these pointers are in line, right, so that we prepare connectedness.
00:28:35.398 - 00:29:24.696, Speaker A: And a pointer also has values. So it's like you can point to a certain element of the original pack and the target also has a specific value, right? True or false? So how does this look like the guess part? It's quite simple actually. So whenever wt one is true and we point to w in t one, then also the value of t one, the pointer for t one has to be true. Same thing for the false case and same thing for the other copies. So this is actually quite simple. The propagate part is not much harder, essentially ensuring for neighboring three decomposition bags that we can synchronize those. So whenever the value in t four is set to true and we point to w in t four and we also point to w and t three, then also the value of 43 has to be set to true.
00:29:24.696 - 00:29:55.484, Speaker A: Same thing for the negative case. Right. The check part is a bit more involved. For checking a three dnf we need three pointers for each of the terms, right, for each of the elements of the term. And we also need to slightly add up propagation there. And we also need to pass the information of whether we already satisfied this formula along the treaty composition, um, which is just constant overhead. Um, yeah, so in the end it looks something like this.
00:29:55.484 - 00:30:30.634, Speaker A: Basically this thing just copied three times and some satisfied with selection. In the end we can show that like the trivia is logarithmic. So we need something like twelve log plus 13. This is just because of technicalities, I guess would be interesting on whether we can do significantly better there. Just get rid of this, just use three times, whatever. It's because of the synchronization between neighboring bags. Yeah, it's just linear blow up in the lock.
00:30:30.634 - 00:30:59.704, Speaker A: In the end we can show the lower bound by just using induction. Just use the reduction from sat. Just apply it as long as you need to get there. Quite simple. It also works for planar structures. So like Q planar set, one can show with this decomposition guide reductions by using just very restricted decompositions that it also works for bandwidth and cut width parameters. Reasonably we extend this to feedback vertex size and tree depth.
00:30:59.704 - 00:32:07.254, Speaker A: But for the incidence graph there is a small say, maybe not so nice thing. For the primal graph we require a really long clause when we generalize this technique. So it would be also interesting to fix this in case for the primal graph. Any questions so far? So the nice thing now is that we can just use the result, right. So instead of reducing from three sets for truth k and reduce making showing a lower bound for a problem that's expected to be slightly higher in this f of k, say two or threefold, we don't need to make a new parameter that's like log, log log or something in n, but we can just reduce from q problem. Of course, depending on what this reduction does change the trivia, but you don't need to overcome exponential barriers. So to say all the time.
00:32:07.254 - 00:33:52.754, Speaker A: We can do this for different problems in AI and Kr classify according to hardness for trivia maybe to give a bit of a summary here. So just to kind of visualize decomposition by reductions. So if you take the trivia on the x axis or largest sub problem size, and like largest sub problem size here kind of, and like Subaru solving effort f of k on the y axis, then what these decomposition guide reductions allow is to kind of go from one spot here. So kind of decrease the sub problem solving effort at a cost of an increase in larger sub problem size and the other way around which we did for trivia. So go to harder fragment that is like smaller from k to low k as we've seen for the QVF reduction. Still have some minutes or don't I? Then maybe to like summarize this or to ask the question of what do we do with these classification results? Can we still utilize them in practice? The thing is, we can use true width, but what we observed is that kind of an abstract version of true width would be ideal. So pick a subset of the vertices of your graph and take the induced subgraph and then we add additional edges to this graph, namely whenever a vertices in the set are connected to the same connected component of the graph of the induced subgraph without s.
00:33:52.754 - 00:35:01.128, Speaker A: So this kind of ensures that we see larger sub problems later in one bag of the 3d composition of the resulting graph. So in other words, what we're doing is we're doing like the simplified graph representation where we can easily do dynamic programming along, but each bag is large. So we have like larger sub problems in each bag, but we do not use dynamic programming on these larger instances. But we use cs hut solver, say any other existing solver. And we call this concept nested dynamic programming, since you can actually apply this decomposition abstract abstraction phase decomposition phase in a nested way and then use other solvers, maybe circuits actually knowledge compilers for these large subproblems, say, and this is a system called Nasthcp which works quite well for counting. So what we did is we tested a series of counting instances. So counting the numbers of assignments of a position formula and projected counting, which is expected to be double exponentially.
00:35:01.128 - 00:36:01.786, Speaker A: The truth we did, I would say, reasonable limits on the wall clock time and main memory. And to show some results. What's quite interesting here is that we use sharp stir as a sat solver. So these are called cactus plots, maybe just don't go into details here, but like being lower and more towards the right is better, solves more instances faster. So we use this one as a sub solver in order to arrive kind of here. So it's kind of a hybrid tree width based solver by combining ideas from roundra's complexity and other techniques from model counting. What's quite interesting is that the widths that we work with are really large, like 200 something, um, just because like we do not materialize solutions.
00:36:01.786 - 00:36:43.798, Speaker A: And we just have like a combination of classical CDCL based, uh, techniques with truth based approaches. And it also works for projected counting. Um, there we also have like a, we have a benefit if we parallelize. So this is like the parallelized version. This is a single core version. We see it's kind of competitive, right? It's more effort, I guess, also because of computing abstractions and stuff, but it's kind of competitive with what has been up there. And the interesting part is like, we still use kind of large decompositions for this works for many other frameworks and extensions.
00:36:43.798 - 00:37:43.504, Speaker A: It's kind of a generalized principle. And just to note, there are also other hybrid approaches available. So to conclude, we have seen decomposition guide reductions, the application for up and lower bounds. We showed that ASP might be actually harder than SAT, if you believe in the exponential time hypothesis, at least provided kind of an idea on a low bound methodology based on quantified boolean formulas. Currently we're focusing on non crown DSP. Yeah, there is the interesting circumstance that you can actually simplify queries, kind of just because the formalism is already quite high. And second level hierarchy, which allows you to simplify queries such that can essentially, we do not need to evaluate the whole body of a rule in as once, but we can actually split the bodies.
00:37:43.504 - 00:38:23.656, Speaker A: What's still open is the result for sharp ESP, the lower bound. I still expect that it's double exponential, to be honest. And I can give some arguments about that. One can still think about lower bounds for other parameters, and the concept is quite general. Right. And it would be quite nice to have theoretical models for using high tree with in practice, because obviously high tree width might not be too bad in practice. Just depends on how you use it and how the specific 3d composition looks like.
00:38:23.656 - 00:38:32.524, Speaker A: Thanks very much for attention. We have one question.
00:38:36.964 - 00:38:49.852, Speaker B: I think it was in the table that you show without the complexity. Also the disjunctive fragment of Asp. Does disjunction add any complexity? Can you do the same things if you have disjunction in the heads?
00:38:49.948 - 00:38:54.636, Speaker A: What do you mean the same things? Destructive sp is harder, right?
00:38:54.820 - 00:38:59.824, Speaker B: I mean your reductions with the boundary, would they work if you have disjunctions?
00:39:00.294 - 00:39:02.014, Speaker A: Yeah. Why wouldn't it work?
00:39:02.094 - 00:39:03.554, Speaker B: I don't know. I'm asking that.
00:39:04.414 - 00:39:31.130, Speaker A: It's really just a general principle. You just do the reduction guided along the lines of the 3d composition. And in fact, you can do a reduction from two QBF to disjunctive Asp. They take care that you do not mess up the truth in the process. So it's really kind of a generic principle, I would say. Thanks for the question. Right.
00:39:31.130 - 00:39:37.394, Speaker A: Thanks, Marcus, again, thanks, sun.
