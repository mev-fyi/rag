00:00:00.160 - 00:00:07.954, Speaker A: Right. Well, hi, everybody. Welcome to the last talk of the workshop. So you've made it.
00:00:09.534 - 00:00:10.126, Speaker B: Okay.
00:00:10.190 - 00:00:46.430, Speaker A: And last but certainly not least, we'll have, like, Vasilis Ganis from MSR giving talk. So, Vasilis, of course, he's an expert in all of the diverse areas of the workshop. So learning in games, machine learning in economics, econometrics, and so on and so forth. And also, he's, of course, a lead organizer of the whole semester on learning games. And today he'll be giving a talk on influence on auctions with quick assumptions on information. Thank you very much, Vasike. And the floor is yours.
00:00:46.622 - 00:01:38.070, Speaker B: Thanks, George, for the introduction. So, yeah, I'll be talking about this paper, which is about inference of fundamentals and counterfactuals in auctions when we don't want to take a strong stance about what information people are receiving before bidding in an option. One motivating example and the data set that we're actually using in this work is the auctions for oil drilling rights. So there's this open source dataset that you can also play with. It's publicly available where you get the data from auctions for drilling rights in Louisiana and Texas. And here we're going to restrict ourselves attention to. We're going to restrict our attention to auctions among only two bidders.
00:01:38.070 - 00:02:06.824, Speaker B: So there are 584 auctions among two bidders for drilling rights in Louisiana and Texas. These are oil companies that want to drill for. For oil. And what we observe for its data point is a bit per acreage. So every, all tract has some area in terms of acres. People submit some cumulative bid, we translate it into bit per acreage. So this is roughly the distribution of bids that you observe in this data set.
00:02:06.824 - 00:03:16.174, Speaker B: And the typical approach in this auctions for oil drilling rights is to think of the auction as a common value option. So there is this unknown common value, which is how much oil is in this tract. And we're each trying to gather information, maybe with different technologies that different companies have before the auction. And then we're going to submit a bid, and the winner wins and drills for oil, and then discovers what was the value for that item that was sold. And what we want to understand from this data set is what is the distribution of this common value in this population of auctions that were run in these two states. So, on average, how is this value of all tracts distributed? Maybe we can use this information to set appropriate reserve prices or maybe redesign our auction. So we might want to understand this distribution of the common value or some summary statistics of that distribution.
00:03:16.174 - 00:04:24.390, Speaker B: So how do we go from this distribution of bits to the distribution of common values is the main question. Now, in order to be able to do that, you need to make assumptions that will connect the distribution of values that your bidders know when they submit a bid into the distribution of bids. But this mapping from the distribution of values to distribution of bids is also heavily dependent on the types of signals that each player receives prior to bidding in the auction. So maybe one oil company has different type of drilling of sort of like inspection technologies than others, or they know something more and they've inspected and found some more information about the item. And these technologies are typically secretive technologies. We don't know. Maybe they're proprietary, so we don't know what they're using, so they receive these signals that we don't observe in our data.
00:04:24.390 - 00:05:10.374, Speaker B: Neither the auctioneer observes when running the auction. And the mapping from the distribution of values to the distribution of bits is heavily dependent to this signaling structure and this information structure that happens prior to the auction. So, if you want to have some more mathematical formalism, we're going to assume that we have access to data from a common value option where value is drawn from some unknown distribution. PI. PI will denote the density. So with b, we're going to denote the profile of bids that are being submitted. We'll assume that the players have quasi linear utility, so their utility will be this unknown value minus their bid when they win.
00:05:10.374 - 00:06:01.404, Speaker B: It's going to be a first price auction, which is typical in these procurement auctions. And prior to the auction, each player receives some signal si, which is dependent on the unknown common value. And so it's drawn from some distribution d of v, which depends on the unknown common value v. And then based on that signal, the bidder submits a bid which depends on that signal. Of course, it doesn't depend on the unknown common value, because neither the players know the unknown common value, they only know their signal. And we assume that the data that we observe is an outcome of some Bayes NASA equilibrium of this incomplete information gain. Which means that in expectation, if I condition on my signal, my bid is the best response to any other bid that I could have submitted.
00:06:01.404 - 00:07:24.404, Speaker B: And throughout the talk, we'll assume that bids and values are discretized. So there's a finite number of bids and a finite number of values that players will have, which will go into the technology that we'll will develop. So, our goal is to perform robust inference on this distribution of values without assumptions on the signal. Structure that the players received prior to the auction, because that's something that's that we do not know. It's proprietary, and we don't want to make some strong assumptions and then make non robust, sensitive inference that is only valid under those assumptions. Of course we're going to be conservative, but we want to be sort of robust, all of those the things that we do not know and cannot model. So then, if we want to do that, the question is, what are the joint distributions of values and bit profiles that can arise at an equilibrium of the game under some signaling scheme? Um, if from theory, we could, from, you know, equilibrium theory, we could pinpoint what is the relation between what is, you know, this set of joint distribution, and we could pinpoint what is this conditional distribution of values conditional on bids.
00:07:24.404 - 00:09:13.974, Speaker B: And let's denote that with some density x. Then we can combine it with the observed bid distribution, let's call it phi, to get what is the, the value distribution, right, just simple marginalization. So we're hoping from our auction theory to tell us what is the space of feasible axis that will arise in this game under some signaling scheme. So then the main question is, what is the space of possible such joint distributions that are consistent with Bayes Nas equilibrium under some signaling scheme? And luckily, before our work, there is this beautiful theory by Bergamot and Morris that exactly characterizes what is the space of such join distributions that are consistent with the Bayes Nas equilibrium for some signaling scheme. Namely, what they do is they define the notion of a Bayes correlated equilibrium, which is a joint distribution over values at bid profiles, such that if I take an expectation with respect to that joint distribution and I condition on my bid, I don't want to deviate to any other bid than the one that was designated by this joint distribution. So it's a bayesian version of a correlated equilibrium, a particular one. There are many versions of how you can extend correlate equilibria to games of incomplete information, and this is one definition.
00:09:13.974 - 00:09:28.194, Speaker B: But the beauty of this definition is that it coincides exactly with a space of joint distributions that could arise at some Bayes NASA equilibrium for some signaling scheme of this original incomplete information.
00:09:29.894 - 00:09:38.030, Speaker C: So is there a parameter over a v that is incorporated with psi? Does the margin of psi on v equal to that prior?
00:09:38.062 - 00:09:39.326, Speaker B: Yes, exactly. Yes. I see.
00:09:39.350 - 00:09:41.878, Speaker C: So for every prior, there is a set of Psi.
00:09:41.966 - 00:09:42.526, Speaker B: Yes.
00:09:42.670 - 00:10:18.052, Speaker D: So there is this extra consistency also a requirement. So this is the main, you know, like the main observation that we're going to also base our inverse sort of like exercise. And the main contribution of this work is to use this beautiful machinery of Bayes correlated equilibria and apply it to the econometric question and develop an informationally robust inference process that boils down to solving a stochastic linear program. So the main observation, main contribution of.
00:10:18.068 - 00:11:29.314, Speaker B: This paper is that if you're searching for the space of all possible value distributions that are consistent with the observed distribution of bids, and such that the observed distribution of bids is a bayesian asset equilibrium under some signaling scheme, then equivalently you can ask what is the space of possible value distributions that are consistent with the observed distribution of bids, and such that the joint distribution is a Bayes correlated equilibrium. The two questions are basically identical. And so then you're looking for such joint distributions that correspond to base correlate equilibrium. So the SVADs satisfy all of these best response inequality constraints for every bit of a player and every deviating bead. And then the extra thing that happens is that you want this joint distribution to also be a product of the observed distribution of bids and this marginal distribution, this conditional distribution of values conditional on bids. And so the only unknown over here is this x thing that we do not know. Phi is something that we observe in the data.
00:11:29.314 - 00:11:51.678, Speaker B: This is coming from the rules of the option. And so the X that we were looking for, the space of x's that we were looking for, is exactly the space of x's that satisfy those inequalities for every I bi and bi prime. I guess you're taking it out of the picture here, right?
00:11:51.726 - 00:12:06.422, Speaker D: Yeah, it's because of that, the theorem of this theorem that you don't, if you're just looking at how values and bits correlate with each other. And that's the abstraction at which you.
00:12:06.438 - 00:12:07.674, Speaker B: Want to look at correlations.
00:12:09.124 - 00:12:22.060, Speaker D: This theorem exactly gives you the space of all possible sub correlation structures. And you don't need to actually, you know, like for every correlation structure, you can find a signaling scheme that will support it and vice versa. And that's what this gives you. So you don't really need to argue.
00:12:22.092 - 00:12:24.532, Speaker B: About the explicit signaling scheme.
00:12:24.548 - 00:12:25.148, Speaker C: It's implicit.
00:12:25.196 - 00:12:26.548, Speaker B: There is some kind of construction of.
00:12:26.556 - 00:13:23.734, Speaker D: A signaling scheme that will support the correlation that you, if you read their paper. Yes, it's in the proof. It's something like, you receive a signal that you repeat or something like that. Something. It's like, it's like a relation type of. So you receive a single touch with. All right, so as I said, this is something that comes from the rules of the auction, let's call it q for simplicity.
00:13:23.734 - 00:14:32.734, Speaker D: This is something that you observe in the data from the bit distribution, and this is what x needs to satisfy. And then once you satisfy those constraints, you can also back out what is the distribution of values? It's another linear constraint that will give you the distribution of values. And so this is sort of like, if you want to take one thing away from this paper, is that you can do econometrics that are robust to inference, to robust informational assumptions by solving this linear problem. So this is a linear program where x, v conditional and v are the end PI of v are the variables. So because we assume that values and bits are discrete, they're going to be as many variables as the number of values and the number of bit profiles. And here the number of values and the space of all possible common value distributions is the space of feasible solutions to be silky. And so the identified set of value distributions is a set of physical solutions to the silp.
00:14:32.734 - 00:14:37.334, Speaker D: So that's the main framework.
00:14:38.554 - 00:14:40.322, Speaker B: There are several, given that, you know.
00:14:40.338 - 00:14:45.386, Speaker D: Like this set of feasible solutions to the value distribution is given by an Lp.
00:14:45.530 - 00:14:51.924, Speaker B: There are many other nice things that you can do with it. And so, you know, the other contribution.
00:14:51.964 - 00:15:17.044, Speaker D: Of the work is to give you several other things that you can do with this observation. So let me give you some other things that you can do. One is that because your feasible set is just a polytope, you can minimize or maximize any linear objective of that density. And so you can find upper and lower bounds to any moment of that distribution by solving a linear program.
00:15:22.504 - 00:15:23.104, Speaker C: Is PI.
00:15:23.144 - 00:15:26.592, Speaker B: No, PI is not known. PI is what you're trying to reverse.
00:15:26.608 - 00:15:50.710, Speaker D: Engineer in the original, when you're going the forward direction, the agents are like the Bergman and Morris work is more like, I give you some PI. What is the feasible joint distribution? As you'd expect, it's this joint distribution and also the marginal PI. Here we do the reverse. We marginalize on the feet on the distribution bits. And, yeah, the other question I was.
00:15:50.742 - 00:15:51.314, Speaker B: Like.
00:15:53.094 - 00:16:04.278, Speaker C: How much is this sensitive to the fact that we don't have perfect knowledge of, let's say, b? If I jiggle this mp, what is the error in my estimation?
00:16:04.366 - 00:16:08.594, Speaker D: Yeah, so that's the inference part. I'll talk about some results on the inference part.
00:16:09.954 - 00:16:11.242, Speaker B: In the end, I'll also mention that.
00:16:11.258 - 00:16:40.154, Speaker D: I think a big open question is finding better inference procedures for subsellps, and you'll see what we have. There's been some recent progress on that topic that I haven't followed fully. And I recently seen an archive, I believe, even on next week's causality estimation reading group that's happening on Thursdays, there's going to be one paper that relates to this that Pira Semenova will be presenting on partial identification.
00:16:41.334 - 00:16:41.758, Speaker B: Yeah.
00:16:41.806 - 00:16:48.246, Speaker D: So that might be a relevant reading group document. So this is, we're still in the.
00:16:48.390 - 00:17:48.232, Speaker B: Population regime where we observe the distribution of bits, right? And all I was saying you can do these nice other identification exercises. You can find upper and lower bounds on the identified set. You can exactly find the identified set by solving two linear programs. Another nice thing that you can do is you can ask counterfactual questions. You can say, what if I change the mechanism? Or what if I change the rules of the game in general, and I care about some objective like revenue or welfare, can you give me some upper and lower bound of what that objective would look like in this new game? If I assume that the players are going to have some other signaling scheme in that new game, and they're going to play some bayesian equilibrium. So these now boils down to exactly adding an extra set of constraints and coupling with the original set of linear constraints, and it's going to be a bigger LP. But still you can formulate this upper and lower bounds as another solving again, two linear programs.
00:17:48.232 - 00:18:25.934, Speaker B: These linear programs are going to now have base correlated equilibrium variables for the new game subsidy. There will be a base correlated equilibrium of the new game. And what you want is to optimize some objective, sorry, not optimized to characterize what is the value of some objective that depends on values and bids. So it could be revenue, could be welfare. That distribution of joint distribution should be a base correlated equilibrium. These are these constraints of the new game, which is given by Utilde. And this base correlated equilibrium should marginalize to the PI of v, which should also satisfy the consistency requirements that you get from your original game.
00:18:25.934 - 00:19:15.074, Speaker B: And so all of these are now just linear constraints. And you can solve these two lp's and get up and the identified set of some counterfactual quantity when you change the mechanism. So you can ask questions. I am running a GSP. What if I was running a first price auction? What is the revenue that I should expect to get? Or I'm running a first price auction. What if I was running a second price auction? What revenues do I expect to get? Given the data that I have from a first price auction, or given data from a generalized second price auction, what should I expect my revenue to be in a first price auction without making assumptions of what signals players receive either in the original game or in the new game. So, yeah, good question.
00:19:15.074 - 00:19:45.348, Speaker B: So in this counterfactual that we had in our work, we were allowing also the signaling scheme to change between the original and the counterfactual. You can think of it as I'm doing something this year and I'm going to deploy a new thing next year. My advertisers are going to get some new signals or whatever. So that changes. But it's also, in many scenarios, might also be interesting to us. What if I couple the information, the signaling scheme. So I constrained the signaling scheme of the bidders to be the same in the new and the old mechanism.
00:19:45.348 - 00:20:29.786, Speaker B: And this was a question addressed by follow up work. Following up to our work, Bergman, Brooks and Morris addressed exactly that question. And so also that even that counterfactual, you can write it as a linear set of linear constraints. But basically what happens here is you need to view the whole thing as a single game. And now you have a base correlated equilibrium which submits jointly a bit in this game and in this other game it should be, it's joint base correlated equilibrium. So that's why now you're going to have variables that correspond to joint the value and the bid in the new game. And so there's going to be some more intricate coupled constraints.
00:20:29.786 - 00:21:11.214, Speaker B: How this variable enters in the new Bayes correlate equilibrium and in the old Bayes correlate equilibrium. But you can still use all of this machinery of LP's to address question too. Other things you can do is you can add some constraints on safe constraints on your density, as long as they fall into some linear set of constraints. For instance, you can add lipstickness by saying that the density between two values is not going to change my maps. This is another set of constraints. You can throw it into DLP and optimize subject to such lip synch constraints. This is if you want to sort of narrow a bit your identified sets.
00:21:11.214 - 00:22:23.512, Speaker B: Another set of things that you can do is assume that your density follows some parametric family. So what if I assume that the distribution of values is some tricated normal or some log normal distribution with some parameter? Then I might want to find what is the set of feasible parameters that are consistent with the observations. So now I'm not looking about a non parametric density, I'm looking about a single parameter. I'm trying to find what are the parameters that are consistent. And then you just add here the constraints for any theta that is a candidate parameter that you want to test whether it is consistent or not, you add the density here for Theta and you check whether this linear program is feasible. For every candidate theta, you have a checker which is a solution to a linear program, a feasibility, you know, feasibility of a linear program that checks whether that parameter theta is feasible. So if you have a two parameter family, you can construct a grid of those parameters and check everything, and you're going to include or not include a parameter based on this linear program.
00:22:23.512 - 00:23:19.664, Speaker B: So this is more computationally intensive because you're going to solve a linear program for every grid point. But you can also do that, and we do that also in experiments. Another thing is, you might worry that because you're making no assumption at all about the signals of the players, you might be getting very wide identified sets. And so in practice, you might want to restrict a bit how informative the signals that the players receive are. So you might want to say that I don't know exactly what signals they receive, but I believe they receive some signal that is quite, at least this much informative of the common value. And the more informative it is, the more you're going to be connecting bids to values and the tighter the identified set is going to be. Of course, there's limits of how you can do that while still maintaining the LP structure.
00:23:19.664 - 00:24:04.556, Speaker B: One example that you can do is first you need to augment the whole framework by adding what is known as these minimal signals. So, apart from the signals that we do not know, you can also extend the framework of bergamot and Boris and add the notion of a minimal signal where players receive those minimal signals too. And on top of that, they might receive something else. And still you have the same type of base correlated equilibrium constraints. But now, on top of conditioning on the bit bi, you also condition on this minimal signal Ti of every player. And again, the signal is also going to be part of the fundamentals. So you also need to marginalize the signal.
00:24:04.556 - 00:25:06.964, Speaker B: And once you augment the framework with these minimal signals, you can now do things like saying, I believe that the minimal signal is going to be within epsilon of the common value, and then it just changes the support of those linear constraints, that the signal that the player receives should be in the support that's consistent with the common value, and you get tighter linear constraints. So that's one way you can do that. Another way you can do it is to say that I believe that I impose that these minimal signals will follow some, uh, parametric family. So, you know, at least the players receive some normal distribution centered at the common value and with some variance sigma squared. And on top of that they might receive something else. And then, you know, you again get a linear, an extra linear constraint, which here q is a known coefficient and PI of v is the thing that you're trying to identify. Okay, so this is a way to incorporate informativeness constraints.
00:25:06.964 - 00:25:49.744, Speaker B: So these are all the population quantities, all of the identification strategies. And then you reasonably, you ask, now, I don't have this population limit, I don't have this distribution phi of the, of the bid profiles. I only have access to n samples. In our OCS data, it was 500 roughly samples. So we observe n samples of those bit vectors. And the standard approach is to replace this expected constraint with an empirical analog of those. So, you know, if you view this expected constraint is basically an expectation, where here I have an indicator of whether this random variable b was equal to the bid profile, small b.
00:25:49.744 - 00:26:35.594, Speaker B: So my constraints, my population constraints are of this form, I have some expected value of some quantity that involves some randomness. Omega should be less than or equal to zero. And what I'll do when I have samples is replace these expectations with empirical averages. So I'll now solve the empirical version of those constraints. But in order to account for the fact that I have error in my empirical objective and in my constraints, I will add some slackness to my constraints to account for that sampling error. So you're not going to check whether they are less than or equal to zero. You're going to check whether they're less than or equal than some threshold sigma n, which is going to decay to zero as n goes to infinity.
00:26:46.014 - 00:26:47.474, Speaker D: Yeah, but if they're various.
00:26:49.254 - 00:26:49.998, Speaker B: You know.
00:26:50.126 - 00:27:02.622, Speaker D: It'S just gonna be a, you're still gonna get concentration. You're not looking at the conditional, you're not looking at, you don't care about like super normal sort of consistency of the density. Right.
00:27:02.758 - 00:27:04.790, Speaker B: So you just care about these averages.
00:27:04.982 - 00:27:09.634, Speaker D: Which, yeah, it's more benign than learning the density.
00:27:12.174 - 00:27:20.574, Speaker C: I guess what I mean is like this vector, it's very rare. So you trust this XVP.
00:27:23.874 - 00:27:24.330, Speaker B: You might.
00:27:24.362 - 00:27:27.730, Speaker D: Not, but then also you're, it's not going to enter in the PI constraint.
00:27:27.762 - 00:27:29.734, Speaker B: Right. That rare.
00:27:30.234 - 00:27:36.762, Speaker D: What you also care is about averaging over the, right, so it's not going to affect that much the pipe when.
00:27:36.778 - 00:27:38.334, Speaker B: You marginalize on values.
00:27:39.834 - 00:27:44.130, Speaker C: Yeah, I see that. I guess I would be more comfortable if you told me, like with this.
00:27:44.162 - 00:28:24.638, Speaker D: Many big vectors I can get, if you want to go the route of concentration equalities, you can say so, you know, let's look at the non parametric inference, where you don't make any parameter you care about some moment of the distribution. So you minimize over x this empirical objective. This is just PI of v, right? I took the PI to consider putting in the objective, and this is the empirical objective. What you can show is that as long as sigma n and epsilon n are roughly log, the number of bits.
00:28:24.766 - 00:28:25.794, Speaker B: Beat vectors.
00:28:28.054 - 00:28:58.402, Speaker D: That you have over delta divided by the number of samples, then the set that corresponds to ln minus epsilon n and un plus epsilon nice is going to contain a superset of the population lu with probability one minus delta. There are also some, you know, some constants here. You know, what is the magnitude of the utilities? Is it going to be a constant in front of it?
00:28:58.418 - 00:30:21.378, Speaker B: That's typical with concentration equalities, but not many more constants. And you know, here, of course, you know, you're going to get exponential number of bit vectors if you have many bidders, but it's inside the log, so it's still going to be polynomial in the number of bidders. And, yeah, so that's one result that you can get the number of bid, the number, the number of bid profiles in the support of the distribution of bids, all possible bid profiles, right. So if you have, let's say for every bidder, you allow 20 discretized bids and you have five bidders, this is going to be 20 to the five. And similarly, if you, if you want to do like parametric inference, so you're saying I assume some parametric family and I want to check whether some parameter theta is in the identified set. So I want to give you a set which is a superset of the true identified set. And then you're going to solve exactly the same types of inequality constraints with a slackness, you're going to have this also sort of like equality constraint with the slackness.
00:30:21.378 - 00:31:23.454, Speaker B: It's going to correspond to two inequality constraint plus and minus, which boils down to an objective of a quantity of this form. You're trying to find all of the parameters theta such that a minimum over X and Max over all possible constraints, all of those constraints. This empirical analog of the constraint is going to be less than or equal to some sigma n. So you're trying to choose sigma n such that when you solve this, you get a set that is a superset of the identified set. Again, you can show that if you choose sigma n to be log, the number of parameters. So if you do a grid over the parameters and you check all of those, again, number of bit vector profiles, number of common values. If you discretize the common values, if you choose sigma n to be that, then with probability one minus delta, what you're going to get is going to be a superset of the population identified set.
00:31:23.454 - 00:32:57.274, Speaker B: However, so these are finite samples of guarantees with high probability. But what we observed when we tried to apply this in the data is that typically these types of sigma n choices, the slackness choices that come out of concentration inequalities, led to very conservative, were quite large, such that the estimated identified set that you're going to get here is going to be a very large part of the space. So almost all part of the space is going to be your identified set for small samples, right? Because you, you know, you're very conservative on how these empirical averages deviate from their mean. And so we wanted to also develop subsampling based approaches which are going to be more, going to be less conservative, but we'll of course, only have asymptotic guarantees and not finite sample guarantees. And so here's one thing that we applied in the data and worked quite well in experiments. Any of the real data was to develop a subsampling based uncertainty based on some techniques, some generic techniques, also in econometrics, where your goal is to find what is the distribution of this worst case moment violation among all the parameters that are in the identified set. So we're here now, we're still in the parametric case.
00:32:57.274 - 00:34:33.069, Speaker B: So say we have like a two dimensional, you know, like a two parameter family with some parameter, say, being the mean and some other parameter being the variance or whatever. And our goal is to find what is the set of mean and variances that are consistent with the observed data. To do that, you need to say, if you look at the population identified set, what is the typical violation of the inequality that you expect to have in the samples for all points that are in the identified set in the population limit, all those points should have a violation that's less than or equal to zero. But in finite samples, there will be one of those points that will be violated. So, okay, so here, this would be the empirical average, not the expectation. So you want to look among all of those points that are in the population identified set, what is the, the, say the 90 percentile of the, of the violation of this moment inequalities when you go to finite samples in the worst case over all those parameters in the identified set. So if you could get a handle of the distribution of this random variable, again, you know, I want to be careful so you replace this with the empirical average.
00:34:33.069 - 00:35:34.204, Speaker B: And so this now is a random variable where the samples are the random variables, and you want to get a hold of this distribution of this random variable. If you could, if someone gave you the distribution of this random variable, then you could choose the, say, the one minus delta quantile of that distribution of violations as your slackness parameter, and you'll be getting asymptotically nominal sort of coverage with probability one minus delta. So this is an asymptotic theorem that comes from prior work. And so then one thing that you can, that you can do to get a handle of that distribution is subsampling. You can construct an initial estimate of that identified set by say, solving the min max problem with no slackness. This is sort of like an optimistic estimate of the identified. So it's going to be very tight.
00:35:34.204 - 00:36:28.066, Speaker B: So you find the sets theta that satisfy these inequalities without any slackness on all of the data. And this is sort of your estimate, your initial estimate of this identified set. Then you start constructing some samples of your data, and for every subsample, you go over that original identified set and you find the maximum violation, like the violation in the LP, the violation that would make DLP feasible. So for all possible parameters in that set, you solve the LP and you find its solution. You know, like in order to satisfy the LP on that subsample, for all the, for these parameters, you need like a slackness of epsilon one, epsilon two, epsilon three, epsilon four. That's what you're going to get. This tau is going to be what you're going to get from every subsample.
00:36:28.066 - 00:37:26.824, Speaker B: The worst case violation of those inequalities for those parameters in the original theta hat. And then you can use the one minus delta quantile of those quantities over the subsamples. That's going to be a proxy for the, those quantities going to be the empirical distribution of those quantities over subsamples. Assuming the subsample size is little of the original sample size, is going to be a good proxy for how this random variable over here is distributed. And so one minus delta quantile of this empirical distribution is going to be consistent with that quantile that you care about. And then if you want to be more accurate, you can then go back and recompute the identified cell with this slackness. This is going to give you a wider set of parameters, and then you can go back and repeat this, but this wider set of parameters until you get some convergence where you know, by doing that, your identified set is not enlarged more and more.
00:37:26.824 - 00:37:42.144, Speaker B: So that is the subsampling based process. And this is, these are the two things that we applied in the data. First, you know, we did some synthetic experience. Sorry, vasily, may I ask one question?
00:37:44.164 - 00:37:45.180, Speaker D: Yeah, go ahead.
00:37:45.332 - 00:38:12.512, Speaker B: Go ahead. Sorry, I'm trying a little bit to understand so I have the maximum loss over some functions. Okay. I am trying minimization. I'm trying to understand if that problem is max mean of a convex concave function, the internal thing, min x over max j is just an LP. Right, right, right. I agree.
00:38:12.512 - 00:38:24.656, Speaker B: I agree. Because here is a grid, right? So this, you go over those points in the grid of theta. Oh, it's just a grid. You know, you'll see some grids in the next slide. Right. You just points. Right.
00:38:24.656 - 00:38:37.746, Speaker B: And you solve an LP for all of those points on every subsample and you just take a maximum over those. Right. So I'm. This whole thing is a. Yeah, okay. Thank you very, very much. Yeah.
00:38:37.746 - 00:38:40.974, Speaker B: So here is one, you know, like, who is. Sorry. Yes, sir.
00:38:42.274 - 00:38:45.994, Speaker D: Basically, Theta has depends on your data set here.
00:38:46.114 - 00:38:51.202, Speaker B: I'm not sure a problem of correlation between the fact that theta hat depends on the data set.
00:38:51.258 - 00:39:14.434, Speaker D: But there is like the s is subsampled from the same data set. Yeah. This part is not very rigorous. And most of the theorems are asymptotic. So I don't have something clever to say about that. Ideally, you would want to sample split. But yeah.
00:39:14.434 - 00:39:28.834, Speaker D: So I think one of my question is getting more robust and formal guarantees for these types of subsample procedures. There are, you know, like if you go to the literature, there are symptom consistency rates.
00:39:29.214 - 00:39:29.550, Speaker B: Not.
00:39:29.582 - 00:39:34.034, Speaker D: Right, asymptotic consistency properties, but yeah.
00:39:43.014 - 00:40:38.840, Speaker B: So first we start with some synthetic experiments to understand how these different, you know, uncertainty quantification approaches work. Over here. The first thing we try to do is, okay, how much do you gain or lose in terms of like, identification power when we, when you don't assume any parametric distribution? So each one of those pictures is a different parametric distribution. So we generated data where we drew the value from some parametric distribution, like a gaussian or poisson or binomial. Then we calculated some base correlated equilibrium for the bidders, some signal which corresponds to some signaling scheme. We then drew bit profiles from that base correlate equilibrium. And we asked, what is the identified set given this bit profile? So here we are in the population limit.
00:40:38.840 - 00:41:21.508, Speaker B: So we actually calculate, you calculate the base correlated equilibrium in terms of exact distributions. And that's what you then feed into your inverse lp program. And so these are population quantities. So you see here that the brown is. If you really, if you also assume that the, in the inverse problem that your distribution is gaussian, you'll get an identified set. That is this brown thing in terms of like the identified set for the mean and the standard deviation of the distribution of values. If you don't assume you go after a non parametric specification, then you're going to get the green thing.
00:41:21.508 - 00:41:48.284, Speaker B: So the identified set is going to be quite larger, but not that large. You don't get everything without making the parametric assumption. And the same thing is what we depict in the other ones. So it can buy you a lot of stuff if you make a parametric assumption and it's correct as well. Specify. But of course, you might have misspecification problems. The other thing we wanted to understand is how conservative are the concentration equality approaches.
00:41:48.284 - 00:42:38.088, Speaker B: And so over here you see that if you. So with brown is the parametric identified set that was also showing in the previous slide. And with green is the non parametric estimated set. When you add the slackness that are coming from concentration equality. So if you want the rigorous finite sample guarantee, this is the set that you're going to get the green one even when n is equal to 1000, and you need to go to n equals roughly 1 million samples to get something that's sort of reasonable looking. And then we also wanted to look at how subsampling behaves, which is going to be a bit more, much less conservative. So it might not include the whole set.
00:42:38.088 - 00:43:18.882, Speaker B: That's going to be much tighter. On the left, again, you see the same figures as before, but in a different sort of like xy space. But it's the concentration equality type of figures. Over here you see the subsampling based approach where with green is the estimated identified set. When we have 100 samples, we use a subsample of size 25, and we draw 50 subsamples in that process that I described. And so we get a Matz titer confidence set, which includes the brown thing, which we want to include. And similarly for n equals 500, we get even tighter.
00:43:18.882 - 00:43:59.230, Speaker B: Over here you see that sometimes you might be losing the brown thing, you might be losing some of those points. So this is a grid. Over here we've gridded those parameter points. And so you test every one of those, you might be missing some of those grid points from the brown. So you're less conservative, but you're much tighter. And with 99 confidence sets, you typically always include the truth. Similarly, if you project on the mean of the distribution, you get that you always include the true mean also.
00:43:59.230 - 00:44:47.624, Speaker B: Then you know, signal constraints can buy you something. This is the population identified set with and without signaling informativeness constraints. So here is the more stringent one where we assume somehow that the readers receive signals, that is, with one within two gridded bitpoint, gridded points from the true value. So epsilon equals two means like they get a very informative signal. Epsilon equals 20, they get a very uninformative signal. And we see that you get the point identification when you assume very informative signals, and you get weaker partial identification if you don't make that assumption. And going back to the OCS data, these are the identified sets that we were recovering.
00:44:47.624 - 00:46:04.144, Speaker B: So with Brown over here is sort of like the zero tolerance set. So this is sort of like what I was saying is the, the optimistic theta hat, where if you solve, find the points that satisfy all of the constraints with less than or equal to zero, and with green is what you get after the quantile process, where you enlarge it by taking subsamples and then resolving the LP. But with this larger slackness, and again, you can get some benefit by adding signaling constraints. This is a much coarser grid due to computational constraints, but if you add some informativeness constraints, you can get tighter identified sets. We have several extensions to private value options. We've applied it also to sponsor search data, and the whole framework applies to general games of incomplete information with discretized action and type spaces, so you can apply to your favorite game of incomplete information. One noteworthy application we did was in sponsored search, where this was a private value exercise where we looked at trying to recover the mean of the distribution of the private value of every bidder.
00:46:04.144 - 00:46:59.984, Speaker B: For instance, we looked at twelve keywords and the top three bidders on those keywords and tried to find confidence sets for the mean of their private value. The main reason we did that was to try to understand do we really get humongous confidence sets or is this very robust but quite conservative inference strategy giving us something meaningful? And you see that you don't get like sort of null sets that are between zero and 20. So the numbers here are normalized, lie between zero and 20, and you get quite informative sets for each of those bidders and keywords. And you can even get somewhat informative sets if you do counterfactual evaluation. So we did one exercise of counterfactual evaluation where we're saying, suppose that you were running among the two top two bidders of the auction. Now a first price auction with only one slot. What is the revenue in that auction? So that's the counterfactual we did.
00:46:59.984 - 00:47:42.154, Speaker B: And you get bounds on the revenue. Some bonds are quite uninformative. Again, this should be ranging between zero and I believe. So you get here some keywords where the revenue bound that you get is between zero and ten, which is too uninformative. But you get conservative information of what your revenue could be under if you switch to the first price auction. And if for some of those keywords you get definitive information that is going to be better than your current revenue under GSP, you're safe to switch to that, assuming that bidders will respond. So that was the work.
00:47:42.154 - 00:48:39.344, Speaker B: The main conclusion is that we offer an inference process for fundamental and counterfactual quantities, that is robust information assumptions in general, games of incomplete information with discretized action and type spaces. In a manner that can be phrased as a solution to a stochastic linear program. You can incorporate many constraints and variants, such as shape constraints, parametric distribution, and signal informativeness. We developed a concentration equality based approach to finite sample inference and a subsampling based approach with asymptotic distribution. Guarantees of coverage I think one main open question is getting better uncertainty set methods that are as tight as the subsampling approaches in the asymptotic limit, but with better and provable finite sample guarantees. Thank you very much. Thank you.
00:48:39.384 - 00:48:39.568, Speaker C: Thank you.
00:48:39.576 - 00:48:42.404, Speaker A: Vasili. Questions from the audience.
00:48:50.784 - 00:48:51.764, Speaker C: Could you.
00:49:00.244 - 00:49:01.504, Speaker D: This is what I meant.
00:49:02.004 - 00:49:03.668, Speaker B: For example, one question is if you.
00:49:03.676 - 00:49:13.036, Speaker D: Just do bootstrap sub sampling, can you prove finite sample rates of the bootstrap distribution and how it converges? Or do you need to?
00:49:13.220 - 00:49:14.756, Speaker B: Maybe you can do like some smoothing.
00:49:14.820 - 00:49:51.884, Speaker D: Of the min max objective that would make bootstrap work and then appropriately take some limits, or whether you maybe want to change a bit your criteria to not be very unstable, like a min max objective, so that the bootstrap will be consistent, and then while controlling for the balance. So showing that some form of bootstrap works in this setting, and giving finite sample weights on how the distribution of the bootstrap converges to the true distribution.
00:50:02.314 - 00:50:36.244, Speaker B: Most of the talk I think, was about common values, and then in the end you said it worked for independent private values and I didn't know where kind of the math changed or like did it go as far back as the Bayes correlated equilibrium? Or. Yes, it changes from the very beginning, but with more notation. Like instead of just having a single value, you have a vi for every bitter. You can also show that you can decompose the LP to be only constraints for the Vi of a particular bidder. If you have independent private values. So then it becomes a similar LP as the base correlate equilibrium. But you have a Vi for every bidder separate and a joint distribution of b two p.
00:50:36.244 - 00:51:53.854, Speaker B: And more generally you can do it for any incomplete information game where you have some minimal signaling structure t, you have some fundamentals, a fundamental vector v, which maybe some of them, you know, like some of that vector, the players observe like in a private value, they observe their part of the value vector, or some they don't observe. And so there is an, you know, like what extra information they observe is part of the signaling stream that you're robust to. So that's the general, the generality of the framework. Yeah. Question. So when the number of parameters is too large to afford a grid search, do you have any idea of what you could do? Could you do like for instance, optimization by doing implicit differentiation from, I would suspect that then more like quasi bayesian procedures would be things that you, that would work where you do some sort of Monte Carlo, where this is your criterion, this min max thing, and you do sampling and you know, like some maybe language or whatever, you know, dynamics, to sample from that, some form of like that would give you a rough approximation of the identified set. We try to traverse the space and sample from the space.
00:51:53.854 - 00:52:18.244, Speaker B: But of course, now your objective will be an LP, so it might be a bit complicated evaluating your objective. Right. Your objective will be this Min Max criterion. So the thing that goes into your quasi bayesian likelihood is a min max thing, so that, you know, but that would be my, my go to.
00:52:23.064 - 00:52:41.244, Speaker C: So are there any conditions, back to the question I asked at some point during the talk, like are there any conditions under which, like if you saw this LP, you have confidence in, from samples, you have confidence in the mapping from bit vectors to the x function.
00:52:41.284 - 00:52:51.732, Speaker B: The x, the x thing. Oh, that you have consistency in the x. Yeah, I've never explored that. I haven't looked at that at all because, yeah, for the quantities that you care, you don't need consistency.
00:52:51.788 - 00:52:55.124, Speaker C: I understand, but in principle you might care about that quantity.
00:52:55.244 - 00:53:33.618, Speaker B: I don't know, but that quantity is, the x is like super partially identified, right? That whole mapping, it's all possible, it's a whole polytope in some sense. You know, it's a very, unless you project it into some dimension, it's a very complicated. So first, partial identifier is not a single function. Right. So it's a space of all possible sort of functions or whatever, discretized functions. So it's a space of, if you think of, for every bead you have a vector, that is the number of values that you have. And so that is the dimension of your polytope, the dimensionality of the space.
00:53:33.618 - 00:53:51.586, Speaker B: And you're trying to find the polytope in that dimensional space. I don't know. It's a very high dimensional object. And the beauty is that you don't want to characterize that very high dimensional polydop object. The quantities that you care about are simple projections of it. Right?
00:53:51.650 - 00:54:03.178, Speaker C: Yeah, I don't, I guess, you know, in the sense of like imagine I see people bidding and I want to understand what's the probability this oil field has a lot of stuff underneath.
00:54:03.226 - 00:54:10.210, Speaker B: Yeah, but I think for that, this type of exercise won't give you anything because you're not making about what signals they receive.
00:54:10.242 - 00:54:11.578, Speaker C: Yes, I guess that was my question.
00:54:11.626 - 00:54:23.134, Speaker B: Yes. So it's not about sampling, it's more like about even in the population, the assumptions you're making are not strong enough to give you particular, to give you information about those types of quantities.
00:54:26.554 - 00:54:36.374, Speaker A: Very nice talk. So in your work, you've also done other papers, work about econometrics with learning agents. Is there any way to combine these two frameworks here? And.
00:54:38.254 - 00:55:16.814, Speaker B: Yeah, I mean, one, one interesting topic would be to, if you go to base course correlate equilibria, they have some connection to non regret learning in incomplete information games. And so you could potentially try to extend these things to inference under base course correlate equilibria would be, which would correspond to what if my agents play no regret, like some contextual bounded algorithm where the context is their signal and you don't make any assumption about, well, algorithm, and that would converge to the space, of course, correlated base course correlated equilibria. And then you're doing inference under that.
00:55:16.974 - 00:55:20.554, Speaker A: But is there like a similar characterization theorem for that class?
00:55:23.934 - 00:55:48.954, Speaker B: Yes, I mean, we have a paper with Jason Hardline and Eva Tardos, some Europe's 2017 that gives you the notion of base course correlate equilibrium that corresponds to those limit distributions. And it would be an, you know, I can't recall on top of my head, if it has any constraint that won't be a linear program. I believe it will help. That's something to check.
00:55:51.294 - 00:55:57.274, Speaker A: Okay, that's it. Let's thank Vasily and all of the speakers once again. Thank you Vasily. It.
