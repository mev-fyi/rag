00:00:01.200 - 00:00:58.474, Speaker A: Great. Thank you very much, Gary. So as you can see, obviously the topic of this talk is going to be related to the previous talk, and it's actually great to be talking after Maru, because their paper, I still remember first reading the diffusion waveless paper back in maybe 2007 or 2008, and how excited I got about it. And some of the ideas that led to this work come directly from trying to make that sense of that paper and trying to put it to work and figuring out possible alternative approaches. It's important. That was really the first, as far as I can tell, it was the first paper which put these classical ideas of mult resolution analysis in the discrete setting in a mathematically, mathematically rigorous and well justified manner. It's also for the same reason it's slightly intimidating to be talking out tomorrow, especially since Ronnie Kaufman had been looking at this sort of stuff for about 30 years, whereas I've been looking at it for more like three.
00:00:58.474 - 00:01:57.744, Speaker A: So, Mauro, please interrupt your game, your job at the appropriate moment. It's important to say that this work is joined with two students, Nedelina Teneva, who is at Uchicago and she's my student, and Vikas Garg, who was at the TTI, the Toyota Institute in Chicago, and now is at MIT. So all the credit is shared between the three of us. So this workshop on this whole program really is about spectral graph theory, and we're talking about something which is still related to graphs and the form of harmonic analysis. But in some sense, it's in contrast to the classical spectral approach. So classical spectral approach says that you start with something like the graph Laplacian and you diagonalize it, right? So you start with this linear algebra operation of diagonalizing the Laplacian, looking at its spectral decomposition. And then if you have functions on the graph, then you expand your function in a basis, in the basis formed by these eigenvectors.
00:01:57.744 - 00:03:01.994, Speaker A: So this whole approach has a rich, has very successful history in theoretical computer science in learning problems also used for dimensionate reduction. So some of the proponents of this are sitting right in the audience. I don't need to proselytize about spectral graph through to this crowd in particular. It has been hugely successful and also, and also conceptually extremely important in trying to, in learning how to handle graphs and find their structures. The one thing to notice about this spectral approach is that in some ways it is the analog of Fourier analysis. Because what does Fourier analysis do? If you are a physicist and you think of Fourier analysis, for example, and you see the connections to the momentum operator and quantum mechanics. Or if you just look at the matrix of the Fourier transformation that is actually made up of the eigenvectors of a given operator.
00:03:01.994 - 00:04:15.634, Speaker A: So this is very similar to that sort of approach. You have an operator describing the structure of your space, your graph, in this context, and you look at its eigenvectors, and then you express everything else in terms of those eigenvectors. Okay, so there are very many parallels between Fourier analysis, theory of Fourier analysis, and spectral graph theory. In some sense, it's just Fourier analysis on graphs. So it shares all the strengths of Fourier analysis, the beautiful theory, but also it shares some of the computational difficulties associated with Fourier type approaches. In particular, when we teach spectral graph theory to students, we always say how the important thing is that the eigenvectors are different eigenvalues separate the local versus the global structure on the graph, right? That's the, that's the whole point that the low eigenvalue eigenvectors somehow capture the global stuff and the high frequency eigenvectors capture the local stuff. But then why is it that even the high frequency eigenvectors have global support? They spread over the entire graph, right? They're dense, and their support covers the entire graph.
00:04:15.634 - 00:05:25.932, Speaker A: So, from a computational point of view, especially when you start looking at large matrices, obviously this is problematic. Why should I have to compute these massive eigenvalues? If I'm interested in my relationship to Mauro and some social network, why do I need to worry about how we're both related to somebody in Mongolia and in our world? It usually happens that when something computationally becomes problematic, that's often an indication that conceptually, there is also some kind of gap there. So that's a sign of something really going on. The spectral approach certainly captures the structure of the graph, but it captures it in a particular way. And maybe that's not the optimal, especially when you're dealing with really massive graphs, which have intricate, maybe hierarchical structure that you want to both describe and exploit. Okay, so although the Fourier approach certainly separates different scales, because the different eigenvectors correspond to different kind of frequency waves on the graph, essentially it's still a very flat thing. You have these five vectors which spread over the entire graph and nothing.
00:05:25.932 - 00:06:06.136, Speaker A: And there's no any of the notions of localization. And many of the combinatorial aspects of the underlying graph are somehow lost in the process. In contrast, multi resolution analysis. So this is multis in general, this is just a very simple orthogonal wavelength transform. So mult resolution analysis, x decomposes functions in terms of wavelets, where the wavelets are at specific scales. So these are the wavelets, these are my basis functions now. And there's explicitly a scale associated to each wavelength.
00:06:06.136 - 00:07:01.434, Speaker A: So at each scale l, you have a certain number of wavelets which capture structure at that particular scale. Okay? So this kind of decomposition makes this multi resolution nature of the analysis very transparent. You really do have wavelets which correspond to the small scales and the medium scales and the large scales. And their support reflects this fact. So the high frequency wavelets which capture local behavior of your function, really only have small support. They only localize to small parts of your space or your graph, whatever domain you are working on. Okay, so this was the majority, the major trend in applied mathematics of moving from the Fourier picture to the mult resolution picture in the last 20 or 25 years of the previous century.
00:07:01.434 - 00:08:00.344, Speaker A: So at a slightly more abstract level. So this is when people talk about wavelengths, they usually think about some things like the wavelets on the real line or maybe wavelets on euclidean space. But there is something more abstract going on in the background, of course. And the way to think of multi resolution analysis and wavelet transforms is this sequential process, this iterative process of taking your entire space of functions on your space, which in our case is going to be the graph. So I use l, two of x here. That really depends on the context with this, let's just say l of x, so some appropriate space of function on the graph, and then doing this repeated splitting of taking down the high frequency parts, so forming a basis for the high frequency parts of my space, and then kind of throwing that out, saying that this part of the space captures what happens at this level of resolution. And I set that apart and set this aside.
00:08:00.344 - 00:08:38.500, Speaker A: And what remains? So we have this direct sum of the analysis space and the. And wavelet space. So what remains? I again split off the high frequency part of and so on. So it's a repeated process of always taking the high frequency parts and kind of setting them aside as you go through this process. And actually, this is how fast wavelength transform actually work. They explicitly go through the sequence of transformations. You're going to be literally kind of peeling away the parts of your function at different levels of resolution until at the end, you have something which is the most global, smoothest level.
00:08:38.500 - 00:09:20.892, Speaker A: And that's what's captured by the final scaling functions, these phi l functions. Okay, so this is the underlying kind of abstract picture behind multi resolution analysis. So the question is, how do you define smoothness in the first place? Okay. And in harmonic analysis in general, this is just defined by an appropriate self adjoint operator. So Mauro's approach was all motivated by diffusion, which is not a surprise, because that's the most natural notion of smoothing on anything. So it's a smoothing on the euclidean space or a smoothing on graphs. There's some notion of gaussian, or if it's not gaussian, it's some sort of diffusion.
00:09:20.892 - 00:09:49.430, Speaker A: It satisfies attractive invariance properties. And then you kind of bootstrap from that. So you define the space. Again, this is a problem as you define the space in terms of this operator. And then you can replace questions about the geometry of the space with questions about the operator itself. And as you go through this process, I mean, your spaces get smaller and smaller, right? Because you keep putting stuff aside. So you are restricting your operator into smaller and smaller subspaces.
00:09:49.430 - 00:10:29.836, Speaker A: So you are also compressing the operator at the very same time. Okay, so to proselytize slightly more, why is this good? And why do you, do I feel that this is the sort of thing that really should make sense on graphs? Well, real graphs are exact, have exactly this sort of multi one. I would expect them to have exactly this sort of multi resolution structure. Right. So people are interested in social networks and information networks and so on, and interested in communities within those networks. Well, you would expect communities to appear at different scales. So there are smaller communities, like people in this room.
00:10:29.836 - 00:11:27.084, Speaker A: And then we are selected in multiple ways, and then we form bigger communities based on, we're all scientists or computer scientists, applied mathematicians and so on, and larger communities and so on. So this kind of hierarchy of different scales is what we kind of feel should be there in real world graphs, and which should be one of the organizing principles, one of the ways in which we can capture the structure of the graph and exploit. But at the same time, it's something that is not necessarily trivial to peel off. And the other thing about it is that it, so one expects this structure of communities and metacommunities and meta metacommunities and so on. But in real settings, it might not be just, if it was just one hierarchy like that, we would run hierarchical clustering and it would be end of. That would be the end of the story. But if we look at a social network, for example, then people cluster based on their profession or their jobs.
00:11:27.084 - 00:12:17.814, Speaker A: They also cluster based on where they are from and their political predilections and so on. So there are really multiple hierarchies which somehow interact. And that's what makes the problem of exploring this sort of structure challenging. And it's also what makes hierarchical clustering sometimes a risky process, because hierarchical clustering can be unstable, so small changes in the graph can really do totally different clusterings. And if you have the interacting system of hierarchies, that's exactly the sort of situation which might arise. Okay, so finally, this sort of multi resolution analysis is not just a way of modeling the graph, but as Maurum stressed several times in his lecture, it's also a way of setting up the right basis for doing something on the graph. For example, learning on the graph.
00:12:17.814 - 00:13:42.636, Speaker A: For example, if you want to learn some function on the graph, like in machine learning, we try to do all the time, and you want to use a sparse estimator, something lasso like, then we want the appropriate basis. And this sort of multis might as an attractive proposition from that point of view. So it's both a way of modeling the graph and modeling functions or stuff happening on the graphs. And finally, it's also potentially a way of coming up with fast computational methods which exactly exploit this hierarchical structure. So, at the high level, harmonic analysis is all about this kind of correspondence that if you are to study a space, that the study of a space and the study of the structure of functions on that space are two things that are closely interconnected, and it can go both ways. So really understanding what your space is about, so really understanding your graph, helps you figure out how to, say, learn functions on the graph at the same time, understanding the structure of space on the structure of the space of functions on the graph, maybe that's the way to understand what's really going on in the graph itself. This is the hearing the shape of a bell type of problem, if you wish.
00:13:42.636 - 00:14:27.864, Speaker A: Okay, so the nice thing about the fact that Maurice spoke just before me is that I can quote him from his papers. And, I mean, you can't really put it any better than this, saying that the interplay between the geometry of sets, function spaces on sets, and operators on sets, is a classical theme in harmonic analysis. So this is a bug that you only get once you. Once you really absorb this, then it's going to be with you. And this is very much the spirit of the last talk and the present talk. So the question is then, how do you do this? So how do you actually define appropriate mult resolution on a graph? And this has been a little. This has become somewhat of a fashionable area.
00:14:27.864 - 00:15:32.164, Speaker A: Well, there were precursors, of course, in graph compression and so on. But I think a major momentum actually came from Ronnie's and Mauro's paper. And then there were several other approaches, which are mostly complementary. So, treelets, for example, which I will be referring to several times in this talk, spectral graph wavelets, which Mauro already mentioned in his comparison slide, the tree wavelets of Gavis, Nadler and Kaufmann. And finally, I think that our approach based on multi resolution matrix factorizations is the latest addition to this list. So, the key thing here is that I think what we're really adding to this is that we make this very strong connection to the problem of mult resolution on graphs, to factorizing matrices, saying that these two problems are again, essentially the same thing. Okay, so, recently an overview paper has come out by Schumann and others, including van DER Greince, summarizing this whole area.
00:15:32.164 - 00:16:37.790, Speaker A: One interesting, one commonality between these different approaches is that I think the majority of people whose names mentioned on this slide are either at Yale or were graduate students at Yale or were postdocs at Yale. But I was never at Yale. Yeah. So, to try and formalize this problem and figure out how to put wavelets on graphs, let's go back to the foundations of multiple. So what is multis really about? So, the most classical setting is of course, mult resolution on the real line. And it was Mala who formalized axiomatically what the, in that case, the sequence of v spaces and w spaces is supposed to satisfy. So, in the most classical sense, in the canonical case, to put an orthogonal multi resolution on the real line, you set up a sequence of nested sequence of these V spaces in such a way that it's an infinite sequence.
00:16:37.790 - 00:17:40.618, Speaker A: So their intersection is the, is just the zero function, their union is essentially the whole l two r. So it's dense in l two r. And then how? The crucial question is, each of the spaces is shift invariant. So by integer shifts, you stay inside the space, and you can already see the beginnings of the multi resolution structure, because the amount that you shift with, so the stride depends on which space you're in. So as l increases, then the steps, then the shift step increases by powers of two. And finally, the crucial thing is that the way that two successive spaces are related in the sequence is that if a particular function is in VL and you compress it by a factor of two, then you get, then the function is going to be in vl minus one. Okay, so this is why this spaces get smaller and smaller, because they contain their span by kind of dilation of the same basis.
00:17:40.618 - 00:18:40.724, Speaker A: And the interesting thing is that defining the moltresolution in this way, essentially is equivalent to giving an explicit basis to the V L and W L spaces in that sequence. So, in particular, these axioms, I think, essentially, but they essentially imply that you can actually form an orthonormal basis for each of the VL and WL spaces by just taking a single special function called the mother wavelet, and another function called the father wavelet. And just looking at dilates and translates of these two functions. So this is a very neat system, right, where you have an orthonormal basis for each of these spaces. The spaces are nicely related to each other. These wavelets are related to each other in this very simple way by translations and scalings and so on. So this, this has led to this whole industry of mostly in the nineties, of actually coming up with such systems, which have good computational properties and nice analytic properties and so on.
00:18:40.724 - 00:19:51.290, Speaker A: So, in fact, the first such system was actually described almost 100 years ago by har. But, but later on, smoother wavelets were introduced, such as the more let wavelet here, and then Dauberschi's famous wavelets, which have compact support and finite moments and so on. So there's a whole choice of how you actually set up such a transform, which is both computationally attractive. So it's easy to do the fast wavelength transform and has nice analytic properties. Okay, so the question is, if we have this nice, well established theory in harmonic analysis, how much of it transfers to the discrete case, how much of it transfers to graphs? So, looking at it this way, by splitting these v and w spaces, this idea still very much makes sense on graphs. So there's a clear notion of smoothness on graphs because there's a notion of diffusion, right? That's the ultimate way of looking at the ultimate smoothing operator on a graph. So something is going to be smooth on a graph if diffusing it a little bit doesn't change it very much.
00:19:51.290 - 00:21:15.174, Speaker A: However, if diffusing it a little bit kills it or totally changes it, then that means that it should go into the w part of the space. So this business of recursively splitting the space of functions on the graph is very natural on graphs, just as on the real line, or maybe even more so. Okay, now, the notion that this basis function should be localized in both space and frequency, that's, again, very critical and very natural on a graph. This is what I was alluding to before, that if we want to analyze the relationships between people in this room, then we should be able to do it mostly by looking at basis functions, which are localized to people in this room, as opposed to telling us about people elsewhere in the world. Okay, now what's the third property of orthogonal wavered transforms? Namely that you can actually do this computationally, that there's this basis on each of these spaces, and it's easy to transform from one basis to another, to another, to another. That's again a natural criterion, and maybe, and that's something that we definitely want to retain in the graph's case as well. However, the trouble is in trying to define notions of translation and dilation on the graph, because now we are not on a field, right? We can't just multiply.
00:21:15.174 - 00:22:13.376, Speaker A: We can't just multiply the argument by something to scale, and we can't just translate by subtracting some multiple of the stride length in the argument of this function. So this is the tricky part. How do we make sense of translating things on a graph and dilating functions on a graph and actually the spectral graph wavelengths? The Hammond van DER Geen paper that Maurer alluded to is a very interesting approach, because they say that, of course, on the discrete space side, we can't really do that. The fundamental idea of the paper saying that we can't reduce it in the time frequency domain, but we know exactly how to do this in the frequency domain. So that's how we are going to define translation and scaling. We're just going to do first a spectral transform. We're going to do essentially a Fourier transform and then translate and scale there.
00:22:13.376 - 00:22:42.828, Speaker A: And that's how we define our basis. But anyway, so that's a digression. The question is how to define an appropriate mult resolution without an explicit notion of translation and dilation on the graph. So these are the general principles of multi resolution analysis on a discrete space. Kind of according to me. Okay, so we want a sequence of spaces, these V spaces, which are a filtration of the space of functions on X. This is a finite space.
00:22:42.828 - 00:24:10.058, Speaker A: It's just an n dimensional euclidean space such that they filter functions according to smoothness with respect to this operator. Okay, so this ratio should increase at a specific rate. At the same time, we want these spaces to have a basis, or rather their complements to have the basis, this wavelength basis, which consists of functions that are localized. So you can formalize this this way in terms of a distance on your space, which you certainly have on graphs, or you can just formalize it in terms of space sparsity of these wavelengths. And finally, the third criterion is that the transformations between the subsequent bases should have specific sparsity patterns, which allows you to do them efficiently. So the question is, on a general graph, how do we set up a system which more or less satisfies these properties? And the approach I advocate in this talk is to change the whole perspective and view this as a pure matrix problem. So instead of looking of forgetting about the fact that we actually come from a graph, just look at something like the diffusion operator on the graph, regard it as a matrix, and then regard this as just a matrix factorization issue, because essentially that's what's happening.
00:24:10.058 - 00:25:11.340, Speaker A: Right? So here is your, here is your operator now in matrix form, and you look at, you push it through this sequence of transformations. So these are just like basis changes, right? So basis changes at the same time, you're reducing the size of the space, of course, but let's forget about that for the moment. As a basis changes, literally what's happening is that you're just multiplying this matrix on the left and the right by orthogonal matrices. And we want the sparsity constraint that these orthogonal matrices are in some sense sparse, or not just sparse, but local. So particularly if these are rotations, then they affect particular like vertices on the graph, or particular sets of coordinates, and just do a local rotation on those and don't mix with other vertices which are far away. So we're going to have specific constraints on the cues. And finally we're going to have the constraint that these w spaces are split off.
00:25:11.340 - 00:25:58.464, Speaker A: So effectively this a matrix is going to be successively compressed. So these Q matrices are effectively going to get smaller and smaller. So the key, well, I'm not sure if it's called an observation, because if you, because it's a fundamental thing, but the connection isn't have hasn't really been pressed in the literature. The fundamental fact about our approach is that it makes this very strong connection between multiples and multilevel matrix factorizations. So I'm going to look at multi resolution analysis just as a process of producing a factorization like this. So this is going to look very much like numerical linear algebra. Ok? And here is this pattern of the matrices getting smaller and smaller.
00:25:58.464 - 00:26:26.458, Speaker A: So if you insist on looking at it as the full matrices, then what happens is that they just become more and more like the identity. So the effective size of the matrix gets smaller and smaller. So these rotations are localized to subsets of the original coordinates. Of course they don't have to be the first, I don't know, m coordinates. That's why I stuck in this permutation matrix. So it has to give me the freedom of figuring out which coordinates I throw away as wavelets. But in terms of a diagram, this is what happens.
00:26:26.458 - 00:27:16.100, Speaker A: You have these Q matrices, which are smaller and smaller, and they additionally have sparsity constraints on them. Okay? So our definition of multi resolution matrix factorization is a factorization of this form. You take the matrix and you write it as this core matrix, which has a special form multiplied by these local rotations, and they're conjugated by these local rotations on both sides. Okay? So this, this is a matrix factorization, but it's a little bit more tricky than, say, an eigen decomposition. You have to specify more things about it. You have to specify, first of all, what class of local rotations these cues come from. You have to specify how many of them you allow to appear, and you have to specify at what rate the dimensionality decreases.
00:27:16.100 - 00:28:27.946, Speaker A: So this is a, there are several degrees of freedom here, which makes this a little bit more tricky. It's a much more general thing than just principal component analysis. On the other hand, as we will see, it's quite revealing about the structure of the original matrix, and it has this fundamental thing of compressing the matrix, because ultimately what's happening is that a is compressed into this form where just a tiny core plus a whole bunch of things on the diagonal. So if there's one slide which is key to this talk, then it is this slide I want you to bear in your mind, because this is exactly what we're going to be doing to matrices, okay? So since this is a factorization, the question is whether you can do it to every matrix. Of course, you in general, the answer is no. So we have this notion of multi resolution factorizable matrices, which can be reduced to this form over some curlicue and over some sequence of dimensions and so on. Okay? Now deciding whether a particular matrix is in this class or not is a tricky issue because you have this combinatorial, optimized combinatorial problem in the background of how do you choose the rotations and so on.
00:28:27.946 - 00:29:10.282, Speaker A: So this is not going to be easy to decide. On the other hand, in a practical setting, what we want is approximate matrix for factorizations of this form. So in practice, there are algorithms, again to try and come up with a reasonably good matrix factorization, which incurs as little errors, as little reconstruction error as possible. Now, the thing I haven't told you about is what these local rotations look like, and they're kind of the simplest thing that you can imagine in this context. So we looked at two different types. One of them is just a local rotation, a so called k point rotation, which only involves k coordinates. Okay? So it takes coordinates I one to ik does a k dimensional rotation there.
00:29:10.282 - 00:29:39.764, Speaker A: So it's an element of Sok, and all the other diagonal entries of the matrix are one. Or it's a combination of such k point coordinate k point rotations. So this is what we call parallel mmfs, because they involve lots of these rotations in parallel. So computationally, these things happen in parallel. Okay, now this thing, of course, this is a, this is one of the standard buildings of blocks of numerical linear algebra. So in the k equals two case, for example, this is called the Givens rotation. It's just the rotation of two coordinates.
00:29:39.764 - 00:31:05.808, Speaker A: And this is, I think, probably the first true numerical linear algebra algorithm to be invented. The matrix for the diagonalization of symmetric matrices by Jacobi in 1846 just produced one of these multi resolution matrix factorizations in terms of givens rotations, except that he wasn't throwing away coordinates, and except that he had, well in the limit, an infinitely long list of these rotations where we're going to stop at a certain point. So the question is, how do we find this factorization? And as I said, instead of really trying to go for the best one, we're going to try and at least get one which has relatively small error. So, error is measured in terms of reconstruction error, which is the same as the Frobenius norm is the difference, the Frobenius norm difference between the original matrix and its factorized form. And then we need to optimize this over the choice of these sets and which are the, like the active sets. So this is, corresponds to the choice of which coordinates we throw away, also the form of the final core matrix and the rotation. So the reason that this is a challenging problem from an optimization point of view is because all these things are going on at the same time and it's a multilevel problem, right? So you first decide about the first level, then the second level, the first rotation, second rotation, and so on.
00:31:05.808 - 00:31:57.956, Speaker A: And all these levels in general interact. However, we might as well simplify it into a sequential process by treating each level greedily. So this reduces to the problem of finding the optimal rotation and the optimal kind of combination of coordinates at each level so as to kind of get the matrix as close as possible, well, to kind of process the matrix to come closer to this, to this core diagonal form as much as possible. So currently we are working on. So we've got a deterministic strategy, but of course it's expensive because you need to solve this combinatorial problem of grouping the different coordinates. We're currently working on the randomized strategies which would really make this scale up eventually to something like complexity of n log n. These slides I'm going to skip over is a little bit technical.
00:31:57.956 - 00:33:14.940, Speaker A: They just tell you that kind of as you would expect, what happens is that you do one of these rotations and then you figure out which of the coordinates that you were involved in the rotation are supposed to go into the wavelength space and those coordinates really don't get transformed anymore. So whatever error you incur at that point, whatever residual you have on that row or column of the matrix you are stuck with. So this is what makes the optimization problem viable, is that you can decompose the error level by level. So I'm going to skip the actual formulae for these rotations and rather concentrate on the conceptual side of what's really happening. So the simplest case is when the rotations are just two dimensional, so k equals two, and after each rotation one of the coordinates becomes a wavelength, because then essentially this induces a tree on your original coordinates or on the vertices of the graph, if you wish. So in terms of the graph language, this is if you take two vertices and you replace them by, as Maurer said, actually by an l, two combination of those two vertices. So this is a very simple type of graph reduction where you take bunches of vertices and reduce them by single vertices.
00:33:14.940 - 00:34:07.232, Speaker A: And effectively what you get is a higher vocal clustering. I think what's interesting is the higher order case when you do the whole process more gently, if this is the case, for example, with third order rotations, where after each rotation you only throw away one coordinate. So conceptually, what this really corresponds to the way this works is that imagine that you have a graph which does have some sort of more or less transparent hierarchical structure. So it has this sort of clustering structure that there is a bunch of vertices here and a bunch of vertices is here, multiverse is here. But these clusters form a metacluster. And there's another metacluster here and here and here. So what's going to happen is at the first level, MMF is going to do a transformation on these individual clusters.
00:34:07.232 - 00:35:01.323, Speaker A: So say there are ten vertices in this cluster. The mega is going to give you, well, it's going to give you ten wavelets which are localized to these ten vertices. But it's going to form a distinction between those wavelets which capture the local structure within this cluster, and those wavelets which capture the overall structure of this cluster. So say maybe six of these wavelets are just going to be local. They go in the W space and you forgot about them, and you're left with just four wavelets, which are then combined with the four wavelets from here and the four wavelets from here and so on. Okay? And then you have twelve wavelets here. But again, you do another level of MMF and those twelve are reduced to, I don't know, two wavelets describing the global structure of this metacluster, which are then going to be combined with a certain number of wavelets from that other cluster.
00:35:01.323 - 00:35:53.558, Speaker A: So it still has this hierarchical feel to it, but you are not making necessarily these hard assignments. So after one of these rotations, for example, so sorry that the U's have been replaced by U'S. So it's just all the slide. Then some of the resulting wave that's after this rotation can go into different places and next level up. So it's not just a tree anymore, it's more like a linear combination of trees. And this is why I say that this MMF structure is able to capture more kind of interleaved, interacting type of hierarchical structures in graphs, not just a single hierarchical clustering. Okay? So this thing has a number of applications because it's such a general way of looking at graphs and such a general way of looking at matrices.
00:35:53.558 - 00:37:03.454, Speaker A: So first of all, you have this, you find this hierarchical sparse basis, you're also clustering, it also gives you information about community structure, but you can also turn it around and actually use it to generate graphs, right? So if you want to generate graphs with this sort of structure, then you can use the algorithm itself to generate graphs for you, which is a separate industry. And in the process of running NMF, it compresses the matrix. So just as a compression graph compression, or matrix compression algorithm, it's also, I think, of independent interest. Finally, it provides a basis which you can use for stuff like lasso. And overall, the structure of that it finds in the matrix is a good starting point for all sorts of linear algebra operations and solving systems of equations and so on. Because once you have this structure, for example, multiplying vectors by matrices in this MMF form is very fast. You just have to perform each of the, each of the q rotations one after the other.
00:37:03.454 - 00:38:15.214, Speaker A: Okay? So it's crucial, before I run out of time, to emphasize the connections of this to all the other things that have come before it which kind of have a similar flavor. So, for example, diffusion wavelengths, right? It starts with the same sort of idea. You start with diffusion, and you use it to compress the graph itself. The difference is that, as Maura said, in diffusion wavelets, at least in the simplest numerical implementation of diffusion wavelets, instead of forming these q rotations independently, you really form them from the individual diffusion functions on your graph. So you orthogonalize diffusion on the graph. Essentially, you do a rank revealing QR, and that's what's going to give you your next level basis. And the risk of that, it's connections to, to classical approaches and harmonic analysis notwithstanding, is that ranked revealing Qr densifies your wavelet elements, right? So diffusions start at nice and local, but you have these nice and local functions, and you keep orthogonalizing them to each other.
00:38:15.214 - 00:39:10.308, Speaker A: They become, as you orthogonalize to more and more wavelengths that you have already removed, they become less and less local, they become more and more dense, whereas in our case, we have explicit control over the density pattern of the q matrices. Okay? So that's the crucial difference between diffusion wavelets and multiple factorizations. Now, there's also this thing called treelets, which actually is just a special case of mult resolution factorization. So this, the authors of this paper, Lee, Nadler and Bossman, made an explicit connection, just like us, between Jacobi's method for matrix factorization and wavat analysis. However, they really concentrated on just the two by two case. So the k equals two k is the Givens rotation case. And the other fundamental difference was that they were coming from the statistical angle, and they were really imitating Jacobi's method.
00:39:10.308 - 00:39:40.712, Speaker A: So the way, instead of solving the optimization problem that I sketched, which was trying to reduce overall reconstruction error, they were just going after the largest object angle elements of the matrix at any one time. So it's not really a matrix factorization method. The objective function is not about finding a factorization with small error. It's about something else which is related, but it's still not the same thing. And also, it's. They only considered the k equals tokes, so they were only. So they really always had this kind of hierarchy.
00:39:40.712 - 00:40:54.108, Speaker A: So it was like a hierarchical clustering and finding a basis combined, whereas our method is just a generalization, which is more flexible. But I think what's important to stress is that looking at harmonic analysis on graphs in this way and looking at matrix factorization in this way allows us to tap into a really rich literature in applied mathematics focused around these sort of hierarchical decomposition ideas. Which has been one of the driving forces of applied math in the last 30 years, really. So, for example, in the realm of partial differential equations, there's this thing called multi grid, where you look at the problem on different scales and you solve, and you can't just solve it on one scale and refine, you need to propagate errors back and forth between the different scales. But the fundamental idea is still the same, to discretize a problem at different scales and let the different scales communicate. So use locality at the low scales and use compression at the high scales. NMF gives rise to exactly the same sort of structure.
00:40:54.108 - 00:42:08.292, Speaker A: So it's kind of a starting point for algebraic multigrid, if you wish. If that's the sort of problem that you want to solve, then there's fast multiple methods. So if you efficiently want to compute a kernel, the sum of a kernel between a large number of particles, and the way you do it is that you hierarchically cluster the particles and say that the effect of nearby particles in a particular particle, here you need to take individual particles into account. But if something is further away, then you can just look at the aggregate effect of the whole cluster. So this is again very similar to how NMF hierarchically clusters the dimensions and reduces and compresses the problem. And finally, explicitly in the world of numerical linear algebra, well, largely motivated by these PDE applications, there's a whole bunch of different major decomposition methods of increasing complexity, like h matrices. So here hierarchical matrices and h two matrices, and hierarchically semi simple matrices, which combine similar ideas of block diagonalizing matrices and then doing rotations and then block diagonalizing again, but it's not quite the same.
00:42:08.292 - 00:43:34.632, Speaker A: So it's more like making a connection to these ideas rather than just plugging in one of these established things in applied math. So finally, we also did some, we also tried to find deeper connections to the theory of harmonic analysis. So how do we know that what we get out of this is really a wavelength? So it's localized and it has something to do with the smoothing operator on our space, but does it really qualify as a wavelength? So this is something that sometimes keeps me up at night. But there are standard tests for that, and one of them is this Hoelde criterion, which essentially says that if you have a function which is, which is smooth in the sense that it doesn't change much between points which are close together in the metric of the original space, then the high frequency wavelet components are supposed to be small. And usually this criterion is formatted in the context of what is called spaces of homogeneous type. So you can have a harmonic analysis which is going to obey this sort of criteria as long as the underlying space is somehow nicely behaved, where space of homogeneous time roughly corresponds to the fact that there is some loose notion of local dimensionality. So as you grow balls, the volume of the balls approximately essentially grows with some power of the radius.
00:43:34.632 - 00:44:42.976, Speaker A: And the question is, what are the analogs of this on a graph or a matrix? So what we came up with this is a little bit too technical to parse in the remaining 90 seconds was a slightly more complicated notion of homogeneity related to taking sub matrices, essentially forming graphs from submatrices of the original matrix, and then looking at the eigenvalues and bounding the eigenvalues of the normalized adjacency matrix away from both zero and one. So this says something like, you know, the, the rows and columns in the submatrix are neither two orthogonal nor two parallel. So if this sort of thing is satisfied, then in fact we can come up with the holder type criteria for the decay of these wavelength coefficients. So there is a connection to the classical theory in harmonic analysis. But I really want to understand it better. And finally, so we tried it out. At the moment, the experimental evaluation is a little bit limited because these ideas are fairly new still.
00:44:42.976 - 00:45:41.976, Speaker A: And I haven't scaled it up to running it on matrices of size order 100,000 or a million vertices and so on. But we just tried it out on these standard small things and we kind of see what you would expect, that if the underlying thing does have a hierarchical structure, then we do well and we do better than competing methods such as tree luts. Right? So this is the tiny thing of, I think this is the karate club graph, and this is a graph taken from genetics. And we see that we do better than trilots in particular, which are natural competitors. Since it's just special case of our method, it's not entirely surprising because we optimize for reconstruction error, whereas treeless does something else. Okay, but on a slightly. And we also saw that we also tried it out on graphs which have this very rigid structure.
00:45:41.976 - 00:46:53.300, Speaker A: So one attempt to produce graphs with a hierarchical structure, kind of fractalized structure, is the chronic product graphs. So reconstructing that we're really good at that, because essentially we go after the same, same sort of structure. So whereas nice trim methods which randomly sample rows and columns from the matrix don't do so well, we get a really small error. On the other hand, at the opposite extreme, when the matrix is random, so there is no hierarchical structure, then we don't do well at all, whereas Nystrom can do better. So, as a first shot experimentally evaluating this method, I was pushing this compression case because it's such a crucial issue in machine learning problems at the moment, just how to take large matrices, large kernel matrices, for example, and compress them to make them more manageable. And often that arises in the context of graphs. So, kernels on graphs, or graphs like things like gram matrices induced by the gaussian kernel, and we find that compared to this fairly standard methodology of taking random rows and columns, we in fact do better.
00:46:53.300 - 00:47:35.870, Speaker A: Now, there are two caveats here. First of all, we retain more information, right, because we factorize the matrix into this core diagonal part. So there is a core which is equivalent or analogous to what the Nystrom methods do, but also we have these diagonal components. There's more information there, but computationally doesn't come at an added expense, because typically what you want, the reason you want to compress in the first place, to do something like invert the matrix, and the diagonal matrix, is very easy to invert. The other thing is that this is still the version of the algorithm which does the exact matching at each level. So in fact, computing the factorization is fairly expensive. There are new wall clock times reported here, but on that front, we would not be doing so well.
00:47:35.870 - 00:48:25.882, Speaker A: But the randomized version, I think, is pretty, as long as the structure is there, it's pretty natural to come up with a randomized version which is going to make this scale, well, the size of the matrices. Okay, so let me just summarize. This is the, this is the last slide. So this multi resolution factorization is a new type of matrix factorization, which has more complicated structure than classical factorizations, but it is more revealing about the structure of the underlying matrix or the underlying graph, if the matrix is actually the adjacency matrix or the diffusion matrix on the graph. So in some sense, it leads to this generalization of rank. Instead of looking at a global eigenvectors, we are extracting structure at multiple different scales. And in terms of, say, an analogy to topic models, this would be an interesting comparison.
00:48:25.882 - 00:49:16.672, Speaker A: Whether the actual wavelengths we find similar to the diffusion wavelets case makes sense in terms of the semantics of the underlying problem. So NNF exploits hierarchy, but does not enforce a single hierarchy. So it's not just the hierarchical clustering, and then something on top of that, it can capture these intersecting hierarchies. Now, this is a bit of an exaggeration given that we've done a week or two worth of experiments. But it seems like at least in many various, these test examples for compressing matrices, many have actually does really well. Now, regarding the algorithmic side, everything related to NNF is a fundamental local operation. So all the rotations are local, all the optimizations we do are local.
00:49:16.672 - 00:50:46.172, Speaker A: So as long as we can find nearest neighbors, essentially nearest neighbors in this matrix fast, and we can update this table of nearest neighbors fast, then NNF should be, at least empirically, it should be an algorithm which scales with order n or order n log n in terms of actually computing the factorization. And once you have the factorization, it's going to be a building block for various numerical linear algebra operations. So once you know that your matrix is of this very structured form, then you can multiply such matrices by vectors easily, you can invert such matrices, and you can even do more ambitious things like actually run something like multi grid on a matrix of this form. And NMF to kind of taps into all these different communities. So there are clear connections to diffusion wavelets and creel, also things like multiscale SVD decompositions that have come up in machine learning. But it also taps into this world of structured matrices and algebraic, multigrid and fast multiple methods, and a whole bunch of other things, other ways in which people have tried to extract hierarchy from problems in applied math and exploited somehow, but somehow in the world of data, this has not been, this is still a relatively, relatively small stream of research. But ultimately, as you know, we are in an age where the amount of data is growing at an intimidating rate.
00:50:46.172 - 00:51:07.604, Speaker A: And at the same time, parallelism is the only way that we can process that data. So this, so some sort of hierarchical approaches which can exploit locality in data is ultimately going to be the only way that we're going to be able to move forward and look at larger problems. So maybe NMF will help with that. Thank you.
00:51:14.744 - 00:51:35.394, Speaker B: Yes. Good. So I can ask you something like, in the simplest case, when k is two, does it like the way you find all those cues? Does it reduce to exactly hierarchical clustering, where I embed all the nodes in, say, the eigenstates?
00:51:37.814 - 00:51:46.284, Speaker A: Well, it depends on which type of hierarchical string is. The question is, how do you find which to which two coordinates to join? And then.
00:51:47.464 - 00:51:57.944, Speaker B: Yeah, that's actually my question. Right. How do I intuitively understand the cues you find in which, like the sequence of cues, are they merging points? In some ways.
00:51:58.064 - 00:52:42.974, Speaker A: In some ways they are merging points, but they are not merging, but they are merging points explicitly so as to minimize this residual or reconstruction error, which is not the same as in hierarchical clustering. And also, I mean, you're merging points, but how do you merge points? Right? So one way you can merge points and you have two points and you can repeat. Do you replace them by their average, or do you replace them by. By something else in between them, where you have multiple points and you replace them by the median or something? Here we take this rotation. So it's like it's the average and the alt, actually a bit similar to diffusion wavelength. So that's a very specific way of reducing the graph, and it's not typically how people do it in Iowa.
