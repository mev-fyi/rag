00:00:00.240 - 00:00:45.024, Speaker A: Let me then start by introducing our first speaker, Vijay Ganesh, who is also an organizer. You may know Vijay, the person behind Maplesart, which was award winning, still award winning solver at the competitions. Or you may know him as the person who proved his abilities of some interest in series. Vijay is a University of Waterwood, his PhD at MIT, and just one thing to say, he has a paper with 1400 citation, which is, to put it in perspective, 400 more than Kukra Kaup. Okay, without further ado, Vijay, the floor is yours.
00:00:46.284 - 00:01:25.252, Speaker B: Okay. Small mistake. I did my PhD at Stanford, and I was working as a scientist at MIT, but that's okay. Thank you, Antonina, for your kind introduction. So, today I will be talking about perspectives on the practice and theory of sat solving. And in fact, I will start by talking about perhaps the central question in SaT and SMT solving, which is the unreasonable effectiveness of these solvers for many classes of industrial instances. And at the same time, these solvers happen to be slow on other classes of industrial instances, such as crypto.
00:01:25.252 - 00:02:18.828, Speaker B: And that's the theme of the entire workshop. And at the end of my talk, I will go through the set of talks that we have scheduled already for the first nine weeks to give you an overview of what you're going to learn and interact with the speakers, the kind of speakers that you would interact with. And then I will talk about structural view of industrial instances, our current best understanding. And what are the pitfalls in this kind of research. Then I'll present a view of solvers as combinations of proof systems and machine learning heuristics, some separation results for solver configurations, leveraging solvers for theory, open problems and directions, and then workshop talks. So, let me jump into it. Let me start with the context and motivation, namely the central question in solving.
00:02:18.828 - 00:03:23.504, Speaker B: So, why are solvers efficient? They don't have a right to be efficient. If you were to believe the predictions from worst case complexity, however, they are efficient, which means that there is clearly a gap between the map, complexity theory and territory. And the goal here is to bridge that gap. But not only is our goal to bridge the gap, deepen our theoretical understanding, but also, perhaps from my perspective, equally interesting is to come up with better solver design that will last a long time. And fortunately, complexity theorists have developed a variety of tools that we can leverage in order to bridge this gap, one of them being parameterized complexity. And as I mentioned earlier, there are classes of instances for which we know solvers on which solvers take a long time. So, what is the reason as to why solvers take a long time on such classes of instances, and many of these questions have been open for a couple of decades.
00:03:23.504 - 00:04:34.184, Speaker B: And this question is important not only in the context of SAT, but it's actually a general question. And to the extent that I know many other this question is relevant in the context of many other formal reasoning systems, such as CP solvers, QBF solvers, model checkers, SMT solvers. So this is basically an endless amount of work and a very large class of interesting problems to solve. And of course, this extends to many other settings where practical algorithms seem to defy theoretical expectations set by worst case analysis. And many researchers, including Richard Karp and others, have suggested this is one of the most important problems in the context of algorithm design and complexity. So how do we break this question down into smaller questions? So I'm going to assume that most of the speakers are already quite familiar with CDCL solvers and also familiar with proof systems and the connection between solvers and proof systems. And I urge you to go and listen to the excellent bootcamp talks by sambas and others if you haven't already done that.
00:04:34.184 - 00:05:59.794, Speaker B: So with that background, how do we break this question down? So the context in which we'll be working is that it's known that CDCL solvers are polynomial equivalent to general resolutions, the famous result by at Sirius, Ficte and Thorley in 2011 on independently by Pipachi, Sabat and Darbic. So while this equivalence is great, meaning that now I can lift lower bounds from proof complexity onto sat solvers, the trouble is that it doesn't quite answer the question as to why solvers are efficient on some classes of instances. It also doesn't address the question of the crypto instances that I mentioned earlier, because all the lower bounds that I know of in the context of resolution are crafted instances, if you will. They're kind of artificial instances. So can we break this question down into smaller questions of why solvers are efficient? And when you run a solver on an instance, and let's say the instance is unsat, and the solver runs quickly on that instance, we can make two conclusions from that. One is that the unsat formula has a short proof for otherwise it would not end quickly. And the second thing is that the proof search was easy, meaning that the proof was short, and the solvers are able to find the short proof.
00:05:59.794 - 00:06:53.964, Speaker B: Now, observe that this is not necessarily always the case, but it is the case for industrial instances. So then the question becomes what kind of formula, structure or parameters, and range of values to these parameters result in short proofs for unsat formulas. And why is it possible for CDCL solvers to find such short proofs efficiently? That is, they are automatable under appropriate parameterization. There's been a recent interesting result that says resolution is not automatable unless p NP. So, just to reflect on this point a little bit, if you haven't heard about automatability before, the idea is this. So imagine that in a proof system for a particular class of formulas, you do have happen to have short proofs. It is still possible that there does not exist an algorithm under appropriate assumptions that would be able to find these short proofs in time, polynomial in the size of the proof.
00:06:53.964 - 00:08:07.358, Speaker B: So therefore, the fact that industrial sat solvers are able to find these proofs efficiently means that we have to somehow be able to account for that through the lens of, say, parametrization. That's one possible way we could account for it. So, making this a bit more concrete, what would be the kind of target conjectures that we should aim for? So I'll state these conjectures, and then the entire research program is aimed at, you know, we work backwards from these conjectures, and it's aimed at proving these conjectures. So the first set of conjectures is we want to have parameterized upper bounds for the CDCL algorithm. So one possibility is, you know, the traditional parameterized complexity upper bound like two to the function of parameters times polynomial and the size of the input for appropriate values of real world parameters. So it's very important that these parameters be real world. And what I mean by that is that these parameters should capture structure witnessed in industrial instances, for otherwise it's not that interesting.
00:08:07.358 - 00:09:10.422, Speaker B: It's good to have such a result, but it doesn't quite explain what's going on in CDCl sate solves, or doesn't even attempt to explain. The other possibility is that you may have something similar to an ETH style result, meaning that it's two to the one minus one over k times n, just like in the case of ppa z, except now instead of k, which is the clause width, in the case of ppa z, we have some real world parameters, or some function of real parameters up there. So it's like two to the epsilon, and also we want to pare it down into not just parameterized upper bound on the running panel of the solver, but also proof search and proof size. We want better, practically inspired lower bound for resolution, for example, crypto. Why is crypto hard? And we want better understanding of solver configuration. So observe the following. A solver is a collection of heuristics, and I can switch these heuristics on and off, for example, restarts.
00:09:10.422 - 00:10:17.476, Speaker B: And every time I switch them on and off, we do witness dramatic change in performance in the case of certain heuristics. And why is that? And this is like, and you can play this endless game, and you can have a very large number of solver configurations, and you can ask the question, is the CDCL sats over with, without restarts, a strictly weaker proof system relative to CDCL sats over with restarts? Or is it probably the case that restarts only influence proof search? Or maybe both? We don't know the answer as yet. And of course, all of this has to lead to better solver design, especially stronger proofs, proof systems, and better ways to sequence, select, and initialize proof rules. All right, so now I'm going to talk about what we know. I'm going to just give you an overview of what we know about parameters and empirical understanding of solvers. We have parameters. So, given that this problem has been well known for some time, theorists and practitioners have proposed a variety of parameters in order to explain what's really going on.
00:10:17.476 - 00:10:58.836, Speaker B: Parameters such as tree width, community structure, backdoor, etcetera. So the goal here, or the kind of, the premise, is that the industrial instances have some structure that is exploited by solver. And the goal here is to then explain the part of the solver in terms of these structural parameters. And the goal of the empirical work in this context is to either rule out certain parameters or rule in certain parameters for further theoretical analysis. But as I will state in a few minutes, we can never conclusively rule out a rule in anything. So I'll make that clear in a minute. So we want to have these parameters.
00:10:58.836 - 00:12:14.704, Speaker B: We want to discover parameters that withstand a battery of well designed empirical tests that are primed for further analysis. But the problem with empirical analysis is it can never be conclusive or complete, because experiments are inherently contingent on many factors, such as sample size, solver setting tools used, et cetera. So because of all this, you have to kind of take it with a grain of salt. But we can, I think, say things like, based on the work that we have done, hypothesis a is stronger than hypothesis b. So what are the properties that we expect these parameters must possess? The first property must be that we must be able to witness the structure that we are parameterizing in real world instances. So in other words, parameters must be able to differentiate, we should be able to differentiate between industrial instances random instances, crypto instances, et cetera. Just by looking at this parameterization somehow, for example, build a machine learning classifier for appropriate range of values for these parameters, we should be able to show that exists a correlation between these parameters and solver running time.
00:12:14.704 - 00:13:16.300, Speaker B: Put differently, parameters should be useful as features in empirical hardness models. And finally, we want to be able to generate instances, scale these parameters and witness the runtime behavior and validate our hypothesis in this manner. And if you pass these battery of tests, then we pass it over to theorists, or work with theorists to prove meaningful theorems. So, have we ruled out certain parameters? One could say that is a yes, at least for some parameters. So in the 1990s, there was a lot of excitement with the clause variable ratio parameter, which I'm sure all of you have heard of. So it's just the ratio of the total number of clauses in the formula divided by the total number of variables. And it was with seen that for three sat, there's a phase transition phenomena when the CVR is 4.26
00:13:16.300 - 00:13:58.874, Speaker B: for randomly generated formulas. By randomly generated formulas, I mean that the clauses are generated uniform at random. And also there was, it was seen that there is an easy, hard, easy pattern here exactly at the phase transition. So the phase transition is where to the, let's say to the left of the phase transition when the CVR is less than 4.26. Most of the formulas are satisfiable to the right, most of them are unsat. And further, the hardest instances are exactly at this transition. But it was later shown that this does not quite hold up under scaling studies, and it also doesn't lift to industrial instances.
00:13:58.874 - 00:14:42.564, Speaker B: Yet another parameter which is very natural and very nice is tree width. And the key insight behind tree width is you take the graph of a formula and check how tree like it is. And the hypothesis here is that the more tree like it is, the easier it would be for the solver to solve such instances. So it's a really cool idea. In fact, many theorems have been proven with regard to related concepts such as branch width and path width, which are polynomially related to tree width. And the graph that I'm talking about here is the variable incidence graph of the boolean formula, where variables correspond to nodes. And if two variables are in the same clause, then they're connected by an edge.
00:14:42.564 - 00:15:29.712, Speaker B: It was shown in 2011 that tree width does not correlate with solver runtime. That is to mean that we expect that when tree width is small, the formula would be easy to solve. However, it was shown that easy formulas actually have large tree width and hard formulas. There exists hard formulas that have small tree width, so this doesn't quite cut it. And path width and branch width also have been proposed that, as I mentioned earlier, and they are polynomially related to tree width, so we can also rule them out for similar reasons. Yet another class of parameters are backdoors. And here I'm citing a thesis by my student where he worked on checking whether backdoors are indeed explanatory.
00:15:29.712 - 00:16:39.464, Speaker B: And backdoors were introduced by Ryan, Williams, Selman and Gomes to the extent that I know back in 2003 as a possible explanation for these power of dplll solvers, but we were able to show that there are instances that have small backdoor that are hard to solve and large backdoor that are easy to solve. Yet another class of instances, yet another class of parameters is community structure. Parameters that have been proposed, and while they're empirically very interesting, it's quite easy to show that you can easily construct formulas that have good community structure, and yet we can prove that it would be very hard to solve. And you can also show empirically that they're hard to solve. So yeah, so the bottom line here is that the parameters are intuitive, easy to work with theoretically, but they fall apart upon contact with reality. So here are some parameters, to the extent that I know, have not yet been ruled out, and many of them have worked on. I've worked with many of these parameters.
00:16:39.464 - 00:17:32.243, Speaker B: So one of them is merge, and I will talk about it in a minute. It's a generalization of the idea of subsumption. Yet another parameter is hierarchical community structure that we came up with recently, which is a variant of community structure. And the good thing about the hierarchical community structure idea is that it does not suffer from at least kind of the obvious counterexamples that one could think of. The other parameter that also we're considering is power law, or more generally, the degree of variable occurrences. We know that the degree of variable occurrences is highly unbalanced in the case of industrial instances, but not so in the case of random and crypto instances. Also, what do I mean by highly unbalanced? It's that few variables occur very often, but most of the variables occur rarely or very few times relative to these highly occurring variables.
00:17:32.243 - 00:18:27.464, Speaker B: So I'll focus on merge for a moment, and there is a resolution proof system called merge resolution. And the basic idea behind this merge resolution proof system is that it is biased towards performing resolution over clauses that share literals. And the idea is that, imagine you have two clauses. One is alpha or xn and the other one is negation of xn or alpha. For the moment, just forget those dots in there, and then you're going to derive alpha, which is strictly smaller than the input clauses. So this kind of merging of literals is kind of nice because you are guaranteed to get shorter clauses provided you have two non unit clauses input with the property that I mentioned. But even if they don't overlap completely over time, a lot of merges are likely to give shorter clauses.
00:18:27.464 - 00:19:57.162, Speaker B: And a particular form of merge is the subsumption resolution, wherein the derived clause subsumes one of the input clauses. And we have noticed, we have observed that such subsumption resolution occurs a lot in the case of industrial instances, although more thorough work needs to be done there. So I just kind of told you how merge can enable you to get shorter clauses. I'm not going to belabor that with this particular example here. Let me skip over this and kind of get to the kind of instance scaling experiment that we performed in order to validate our hypothesis emerge has something to do with the performance of CDCL sat on some class of instances. So here in this experiment, what we did was we generated about, I think, 500 or so pseudo industrial instances using a popularity generality popularity similarity generator, which is believed to generate instances similar to industrial instances and attempted to keep, while keeping all other aspects of the instance the same, we would vary the merge parameter. Now how do we do that? What we do is we use transformations that allow us to either increase or decrease the number of overlaps between resolvable clauses.
00:19:57.162 - 00:20:53.254, Speaker B: We define a metric called mergeability, which simply counts the number of overlapping literals in a formula between resolvable pairs. On the y axis, you can see the number of merges deviating from the mean. So zero is the mean, and as you go towards the right you increase the number of merges. As you go towards the left, you decrease the number of merges. And what we notice is that during the run of the solver, when you have an input formula that has a lot of merges in it, the average learn clause size is much smaller compared to when there are few merges. And further, we also performed this similar experiment with solver runtime, and we see a dramatic decrease in solver runtime as the merge ability metric is increased. So this gives us confidence that this particular metric might be relevant.
00:20:53.254 - 00:22:04.720, Speaker B: And additionally, as I mentioned earlier, subsumption resolution seems like a very is a rule that gets applied again and again in the context of industrial instances. So perhaps this might be one of the metrics that would go into a theorem to prove appropriate parameterized upper bounds. Now, so, so far I've talked about parameters and not referenced internals of the solver. And here I'm going to shift gears a little bit, and I'm going to think about how can I leverage ideas from theory in practice? And theorists have modeled solvers as proof systems. And I started thinking when I first started working in the context of SAT. This is great, but how do I leverage this? Because if you were to open up a Sat solver and look at inside of it, it's a veritable soup of heuristics. Over time, I came up with this idea that one could model solvers as combinations of proof systems and machine learning.
00:22:04.720 - 00:23:08.418, Speaker B: And this is actually a very nice combination, because is solvers generate a lot of data, and machine learning literature is full of optimization heuristics that we can leverage in this context. Just to reiterate that, solvers have proof search systems, and it's a well known abstraction, but we modified a little bit by adding this element of machine learning heuristics. And I'll make it a bit more precise what I mean by adding machine learning heuristics. But I want to contrast that for a moment with SAT solvers that are non symbolic. That is, they don't do any kind of symbolic reasoning, for example, once based on purely deep neural networks like neurosat. And we know that these solvers don't scale very well. So the obvious way in which you can use the proof system abstraction for solvers to improve them is to add stronger proof rules in your solver.
00:23:08.418 - 00:24:01.114, Speaker B: And that's kind of the basic idea behind CD Clt. But the method that I proposed was that if you think of a solver, it's essentially a decision procedure. But internally it's an optimization procedure where the goal is to construct the shortest proof possible. And one way to construct short proofs is to optimally sequence or select proof rules from a set of rules. And we can use machine learning towards that end. So, just to remind you of the CDCL sat solver, how it works, it takes as input and input as Boolean formula performs boolean constraint propagation on it until it reaches a fixed point. Either it correctly decides the input, or it cannot make progress.
00:24:01.114 - 00:24:58.734, Speaker B: Checks whether the formula is unsatisfying under the current partial assignment. If not, it branches on a variable and keeps propagating until it gets a conflict. So there is a process here where you are using BCP, followed by branching until you get a conflict, and when you get a conflict, the conflict analysis gets triggered. So there is another process. So there are two processes going on inside the solver, and this suggests the following idea. You could think of this as reinforcement learning. So you have an agent, which is the decision heuristic plus BCP, that performs propagations and construct a partial assignment until such time that you get a conflict, at which point it gives it to an environment, and the environment then reasons about what's going on and returns back to the agent a reward in terms of a learned clause.
00:24:58.734 - 00:25:54.526, Speaker B: And then the branching heuristic can then use that to make better decisions going forward. So we modeled this as a reinforcement learning problem, in particular, multi unbanded problem. And we leveraged algorithms from the reinforcement learning literature to design the LRB branching heuristic, which is the basis for maple sand. And since we introduced this in 2016, many, if not most of the winning solvers are based off of maple. Sad. And subsequent to that, we have come up with six other heuristics aimed at different aspects of the solver and machine learning heuristics and other people have adopted and adapted these ideas. Another result that I would like to talk about here there is a chat.
00:25:54.526 - 00:26:12.780, Speaker B: You have five more minutes. Okay. I may take a couple of more minutes. That's okay. Understanding CDCL solver configurations. So there is a big open question. We know that CDCL Sat solvers with restarts are polynomial equivalent to resolution.
00:26:12.780 - 00:27:02.884, Speaker B: But the question is, does this equivalence also hold for CDCL without restart? So this question remains open. But we came up with some weaker results. So, the first theorem we proved was consider the following configuration of the CDCL Sat solver. So you have CDCL conflict clause learning with an asserting learning scheme, backtracking non deterministic variable selection, randomized value selection, and restarts, and the same configuration without restarts. And we were able to show a separation on a class of satisfiable instances we refer to as ladder formulas. And similarly, we were able to look at two configurations which are much more realistic. So CDCL, with back jumping vsits, variable selection, phase saving value selection, and restarts in the same configuration without the restarts.
00:27:02.884 - 00:28:12.074, Speaker B: And here we were able to show a separation, again, exponential separation, where restarts is more powerful than without restarts on a class of unsatisfiable instances referred to as pitfall formulas. Just to give you an intuition behind theorem two, the way this proof works is that without restarting, the solver gets stuck in a large space where the only way it can get out of it is by making constructing an exponential proof. But if it performs restarts, it's able to get out of that much more quickly. So that's the basic idea. Now here I'm going to propose some directions in which we can. Instead of applying theory to practice, how about we go the other direction? Can we apply practice to theory and solve some interesting questions and complexity theory using SAT solvers? And I can identify a few directions that might be quite interesting. One direction that I find particularly interesting is solving concrete circuit complexity questions using solvers.
00:28:12.074 - 00:29:00.214, Speaker B: Ryan Williams posted the the above problem of some variant of the above problem as a polymath problem on discord. And here the question basically is, what's the minimum number of multiplications needed for three by three Boolean matrix multiplication? Is it 19, 2021 or 22? Right. So that's the open problem. And there are innumerable such problems that one could solve using solvers. Another direction is that practitioners have come up with new proof system, like drag based proof systems, or more generally, what I think moraine used to refer to as interference based proof systems. And these are inspired by practice and can be proved lower bounds for them. Yet another direction that might be very interesting is the following.
00:29:00.214 - 00:29:42.722, Speaker B: So if you think about the work of a proof complexity theorists, part of the work is to come up with conjectured classes of instances that might be hard for a proof system. It's no easy task to come up with such classes of instances. If you had a solver that could be a tool that the theorists could use to rule out. So they might say, oh yeah, I know, this class of instances might be great, why don't I run it on the solver and see what happens? And if the solver is able to quickly show that these formulas are unsat, then you say, oh no, this is not working. I may not have to go back to the drawing board, try something else. So such a tool doesn't exist. And I'm thinking of making such a tool.
00:29:42.722 - 00:30:14.812, Speaker B: It would be great to collaborate with the theorists to know what they really want so we can build a tool to their specification. Finally, solving combinatorics problems. Yes, that's what we're saying. I think Marian talked about quite a bit, and I have also worked in this place where in this space where we combine sat solvers with computer algebra systems, just like in DPllt. But we have DPL cas to solve a variety of problems in common networks. So open problems. I already talked about the first three.
00:30:14.812 - 00:31:06.094, Speaker B: I'm going to now briefly talk about the last few. Can we come up with a tighter upper bound on the BCP algorithm to the extent that I know there is no complexity. Theoretical analysis of the two watch literal scheme maybe I'm missing something, but I at least I haven't read it from a practical point of view. More machine learning heuristics for solvers like can we come up with a machine learning based clause learning scheme as opposed to the first UIP, which is kind of a hard coded or a fixed heuristic? Would be nice to have more of a dynamic heuristic. Yet another sovereign feature that I know that users like would be given an encoding of a problem. Can I give some feedback to the user saying this is why the encoding is not working. You should try to fix this aspect of the encoding.
00:31:06.094 - 00:31:57.224, Speaker B: For this we need to have a theory of encoding that will give us a way to think about good and bad encodings, and then we can encode that. We can code it up in the solver and give feedback to the user. For example, consistency is one idea that, you know, if your encoding is you can somehow detect that the encoding is not consistent, are consistent, then you can give some feedback to the user about it. Another kind of class of instances that are hard for solvers is multiplication circuits, analyzing multiplication circuits. So it would be nice to have solvers that are better at this task. And now I will conclude with giving you an overview of the workshop talks for this entire 14 week workshop. And here is the first, I think, about nine or so of the scheduled workshop talks.
00:31:57.224 - 00:33:03.564, Speaker B: So the first week today is kind of giving you some context and direction and open problems like David is going to talk about using structure as a way to improve solver performance, and Lauren is going to, I think, talk about negative results. I don't know exactly. I don't recall exactly what he's going to talk about next week is going to be theory and practice of structure of SAT instances by Stefan Zeider, Jody Levy, Ralph Rothenberg followed by theory and practice of encoding. So this is how can we use ideas that CP people are very familiar with, but bring it into the context of SAt? But Oliver Coleman, Sharon Makrish and Maria Banay March 3 is going to be a proof complexity toolbox. So here we talk about automatability and lifting ideas, which are quite a few papers in stock and fox in recent years. And this would be by Robert, Robert, Susan Desirende and Joan Ozuremake. March 10 would be recent advances in proof complexity of solvers.
00:33:03.564 - 00:33:53.804, Speaker B: So Mark vinials, Noah Fleming and Ian Lee would talk about some of the restart results I mentioned, as well as complexity theory results with regard to branching heuristics. March 17 there'll be non CDCL solvers, so solvers such as pseudo boolean solvers and other solvers that have rich preprocessing in processing techniques. By Marianne Houle, Jakob Nordstrom, and Joey Zhang March 24 Maria and Daniel will talk about lifting ideas from the CDCL setting to the theorem prover setting. March 31 is a public lecture. The Richard M. Karp public lecture and April 7 would be machine learning for solvers. And we have still, I think, another six or so weeks left after this.
00:33:53.804 - 00:34:04.404, Speaker B: So if you have any suggestions, please do send it our way and we'll be more than happy to take that into account. So with that, I'll end my talk. And thank you for your attention.
00:34:05.344 - 00:34:12.604, Speaker A: Thank you so much, RJ. Anybody has any questions, comments, suggestions?
00:34:16.784 - 00:34:17.964, Speaker C: I have a question.
00:34:20.424 - 00:34:21.164, Speaker A: Please.
00:34:21.544 - 00:34:39.754, Speaker C: So, you know BCP is really, in some sense horensat, right? Is some sense what horensat you can think of. BCP is essentially Horan sat unit propagation. Horan Sat. Yeah, Horan sat, we know is peace. Is p time complete?
00:34:40.814 - 00:34:41.454, Speaker B: Yeah.
00:34:41.574 - 00:34:46.554, Speaker C: So when you say complete analysis of BCP, what exactly are you, are you thinking about?
00:34:47.174 - 00:35:13.494, Speaker B: I'm talking about the analysis of the two watch. Literal scheme. So it is efficient for some reason. But it would be nice to have maybe an amortized analysis of it. For example, it would be nice to get an upper bound, essentially, which is less than n squared or n times m. That's what I mean.
00:35:14.034 - 00:35:16.434, Speaker C: Okay. More refined analysis.
00:35:16.514 - 00:35:17.490, Speaker B: Yeah, exactly.
00:35:17.522 - 00:35:19.454, Speaker C: Got you. Yeah, got you. Yeah.
00:35:22.514 - 00:36:06.352, Speaker A: More questions, comments? Actually, just to say something quickly, Vijay mentioned that on February 24 we will have encoding showcase what it is. If you've had experience with some interesting encoding of an interesting class of instances that you don't mind sharing, something very short, 510 minutes, please let me know. And we are looking for interesting. For interesting and showcase of interesting encodings. Okay, more questions, comments. Before we go into next speaker, can.
00:36:06.368 - 00:36:20.260, Speaker D: I shortly ask about reinforcement learning representation of the problem? Yeah, you mentioned before. So what is the objective there and where are we optimizing?
00:36:20.372 - 00:36:42.144, Speaker B: Right, so the objective there is what we refer to as global learning rate. So the goal is to maximize the number of conflicts you learn by minimizing the number of decisions that you have to make. And it seems like a very good objective to optimize for. Empirically, we have shown that this actually helps a lot and that's the objective that we optimize for.
00:36:43.684 - 00:36:55.984, Speaker D: And practically how many of these instances that you need to give to the proof system how many of them you need to learn properly.
00:36:56.484 - 00:37:06.944, Speaker B: Yeah. So this is more of an online learning method. So it's not like an offline method. So. Yeah.
00:37:08.604 - 00:37:09.316, Speaker D: Okay.
00:37:09.420 - 00:37:12.724, Speaker B: So therefore you don't need to train it offline. Sorry.
00:37:12.884 - 00:37:14.504, Speaker D: When do you start improving?
00:37:15.524 - 00:37:53.622, Speaker B: Well, it's. You can think of it as. So there are many kind of online methods that don't require you to train ahead of time. And these are particularly useful in these kind of settings where you have to make quick and cheap decisions. So. Yeah, and it so happens that, you know, these, the method that we came up with has this very localized behavior that matches the kind of instances that we have, the instances that have these localized proofs. Of course, we don't have a proof of a theorem to do this connection, but this is the intuition we had.
00:37:53.768 - 00:37:57.294, Speaker D: Okay, I see, I see. It's localized. Okay. Thank you.
00:37:58.634 - 00:37:59.574, Speaker B: Thank you.
00:38:02.834 - 00:38:05.282, Speaker A: If there is nobody, may I ask a question?
00:38:05.418 - 00:38:06.186, Speaker B: Yes, please.
00:38:06.290 - 00:38:32.144, Speaker A: So you. You did talk about your practice to theory direction and use set solvers to find combinatorial objects without doing all, without doing as much of the hard work as we usually do. So what would be good examples that you are thinking of in particular for lower bounds for theories?
00:38:35.204 - 00:39:11.836, Speaker B: So, for example, kind of generating classes of instances. And if you had an idea that you think that this class of instances could be a lower bound for resolution, you could generate it using the generator tested against a solver. Play with the knobs in the solver to see if there is an easy way to solve it. And if there is an easy way to solve it, then you can rule out that class of instances. Right. So that's the value of such a tool, but for that, you know. Yeah.
00:39:12.020 - 00:39:18.388, Speaker A: So this is also related to the polymath problem that Ryan posted about finding.
00:39:18.436 - 00:39:47.174, Speaker B: I think it's a little bit different. So in the polymath problem, there, there's a specific concrete circuit complexity question. How many multiplications? What's the minimum number of multiplications? You need to solve the three by three Boolean matrix multiplication problem, and then you can encode that as a query to the solver and ask, is it 17, eight. Sorry, 1920 or 21? So that's a little bit different from the other idea, but I can take it offline though. Thank you again.
