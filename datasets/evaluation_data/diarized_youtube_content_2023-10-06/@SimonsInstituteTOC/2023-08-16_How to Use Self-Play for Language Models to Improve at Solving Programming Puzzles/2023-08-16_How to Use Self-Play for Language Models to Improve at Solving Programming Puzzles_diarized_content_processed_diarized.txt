00:00:00.120 - 00:00:20.462, Speaker A: California next week if you want to meet. And I want to keep you all awake. And I think it's super interesting. I'll be excited, and I'd love to have questions throughout the talk instead of at the end. So feel free to interrupt, criticize, make jokes, whatever you feel like. I wanted to. You know, I know it's very late, and I think that'll help people enjoy the talk.
00:00:20.462 - 00:01:05.906, Speaker A: So I'm going to talk about some work where we're using self play, which was talked about in the panel yesterday. People were kind of wondering, oh, maybe we can get some, generate some of the data ourselves and in a specific context of programming, programming puzzles. But really I'm interested in programming in general, and I'll talk a little bit about that. And this is joint work with some interns, Matt Bowers, Al Shuster, and Ashwin Khalid, as well as some colleagues, Patrick Kalasak and Alex what was up? So I'll start by talking a little bit about code generation. Umesh asked me to talk a little bit about what I think, you know, where that's going. And I'll talk about these programming puzzles, which for a theorist is just basically an NP problem. So for the audience here, it may be actually really easy to explain.
00:01:05.906 - 00:01:56.310, Speaker A: Usually I have to spend a lot of time with, and then I'll talk about using self play to get the computer, get the language models to improve themselves using only themselves and a Python interpreter, or, you know, the ability to evaluate code. It could be in any language language. Okay. And then I want to talk also kind of philosophize a little bit about, you know, is this a good thing to be doing? Ilya, yesterday was, you know, kind of, you know, making us all be worried about what's coming next, the dangers. So isn't this kind of exactly in the direction of the singularity? Not that this would lead to it itself, but it'd be contributing to the body of work in this direction. And, you know, should we think twice? But before doing that, and I've spent a bit of time thinking about that, so I want to talk about that. So, first of all, why are we studying code generation? Why do we want these language models to generate code? So there are many reasons.
00:01:56.310 - 00:02:16.486, Speaker A: One might be. It's fascinating, it's interesting. It's a grand challenge to solve competitive programming problems. That would be amazing, but that might not be satisfying enough these days. Of course, it could be a useful tool. So people are using spreadsheets, but they don't know how to write macros or they want to make, you know, somebody has a great nonprofit. They want to make a website.
00:02:16.486 - 00:02:42.490, Speaker A: They don't want to hire somebody. Or programmers are using this already all the time to help them in, you know, write their code and GitHub, copilot. And moreover, there's a bunch of tools that we anticipate building that people have talked about here. You know, scientists want to build tools. Well, of course, there's, you know, chat, GPT, other language models. These are built with code. So if you can make the computer write code, then potentially they can make better, better versions of catgpt.
00:02:42.490 - 00:03:31.584, Speaker A: The scientists can make their better models for their projects. There's a very inspiring application I've seen already that OpenAI contributes to, which is helping blind people who basically talk to them about what they're looking at. There's auto GBT, which is just doing random stuff on the web. There's proving theorems. So in my opinion, instead of me trying to prove some really hard math theorem, p not equal NP or something like that, I think the fastest way we could get there is by getting an automatic theorem prover, which might have a better chance of doing that. And that's written in code. And so if we can get the computer to write code, which can then write these language models and write these and also write these theorem provers, we might be able to prove something faster than we expected.
00:03:31.584 - 00:04:20.618, Speaker A: Who knows? But it's still kind of scary. So maybe the type of coding problem I want to talk about today is sort of algorithms. Can we get computer to design algorithms? And I'm claiming that algorithms are going to be a great test then to think about, can we make AI that's beneficial? And what if this AI is super intelligent? I think algorithms is a great place to play with this for several reasons. First of all, you know, improving algorithms can be beneficial. You're saving energy. Potentially, you can help with the environmental problems. It's a great test bed because, you know, we all know designing efficient algorithms requires thinking very similar to proving theorems.
00:04:20.618 - 00:04:53.536, Speaker A: It requires formulating hypotheses, proving, or, you know, at least convincing arguments to yourself at a minimum of why they should work. It's similar to kind of the scientific, the same kind of things you would have to do in science. You might formulate hypotheses and do experiments. If your algorithms are building language models, then you have to do large scale experiments. So it's got a lot of the same ingredients that other domains do. It's got some of the dangers, too. You could have, your algorithms could go out of control, or you could have an evil person using your algorithms or manipulating your system.
00:04:53.536 - 00:05:20.906, Speaker A: I think it's a great domain for these things, and it doesn't, you know, it's something that we understand pretty well, so we can monitor what's going on with the algorithms. We understand algorithms. There's not a lot of moving components. It's much faster to iterate than in a scientific domain where maybe you're making a compound. Every time you make a compound, you have to go and spend a month doing an experiment. Here, we just run some code and see how fast it is. And then is that a good algorithm? And so basically, it's very easy to evaluate.
00:05:20.906 - 00:06:06.880, Speaker A: So we have the language model evaluation. And the thing I want to talk about today, which is also very exciting, is it's a domain which is really well suited for self improvement. Can we get these language models to write better code on their own? And maybe we can run it in a sandbox, or at least try to run it in a sandbox. It's a relatively easy domain to monitor. That's my feeling about why this domain might be a good domain, given that the world is moving in this direction, that this might be a good domain to try. Things we might put out, put out of work some algorithms or AI researchers, which seems cosmically fair, right? Like somehow it's good that maybe the first people, not the first, but among the first people to lose their jobs due to AI will be AI or algorithms researchers. You know, we'll see what that's like.
00:06:06.880 - 00:06:45.816, Speaker A: And before Oscar de IQ. Good question. Well, I'm not, yeah, my company's already IPO, but some other companies. Yeah, that's a good question. All right, so, and there's other issue, other things that I think are worth discussing that don't arise as much in code generation, which is another reason to maybe consider code generation as a test bed for the AI, because it's not, it doesn't have these problems or doesn't have as many problems that plague other areas. So when you try to release Chachi Pt, it's good. There's been a lot of RLHF, which has made it better, but it still has problems.
00:06:45.816 - 00:07:11.710, Speaker A: I don't know if you're aware, but if you go to Chechi PT and you say, look, the nurse married the doctor because she was pregnant. So this is like the example that we had earlier with. Anyway, there was Sanjeevan. Yeah, Sanjeevat, the nurse, married the doctor because she was pregnant. So who's pregnant? Who knows? Right? Okay. But GPT says she refers to the nurse. So the nurse is pregnant and it's not a one off.
00:07:11.710 - 00:07:48.464, Speaker A: You can do it over and over and over and nine iron or ten times. It will be confident that the nurse is pregnant, not the doctor. And you can ask it, how do you know that it's not the doctor who's pregnant? And it'll give you some reason. It'll say it is biologically impossible for a male doctor to be pregnant. So not only does it think that it's the nurse who's female, it doesn't even consider the possibility that the doctor. Wait, which model was this? This is GPT four, temperature zero. It's like the very best model that I know.
00:07:48.464 - 00:08:21.670, Speaker A: And these things are used. It's not just a hypothetical like LinkedIn. I went to a meeting at LinkedIn, or they have a blog post even. They're using these models carefully, they're adding prompts and stuff, but they have to face these kind of issues. They're using it in their profiles and you don't want have somebody's profile be affected by their gender. So there's still work to be done here. In fact, there was a nice blog post where they did an evaluation on this winnow bias data set, which is similar to what Sanjeev was talking about, measures racial bias, gender bias, all kinds of biases.
00:08:21.670 - 00:09:01.554, Speaker A: And there's still a lot of bias, even in GPT four. So there are techniques that have reduced them, and it's great research that's going on, but we still have a lot of work to do, and these things are out in the wild. So, you know, you have to think twice. Should you be releasing these things and what are the consequences in code generation? We don't have as much of this. This does happen. I've generated millions and millions of lines of code in way less than, well, at least, like maybe one in a million of line of code. Will you actually run into something like this? You will run into some offensive words, but it will be the code it'll generate will be like, here's a, you know, offensive word detector, and it'll generate code with a bunch of offensive words in it, which seems reasonable and not as bad as these examples.
00:09:01.554 - 00:09:32.142, Speaker A: So it does happen. But I just recently, today is the first time I found the model that didn't do it. Claude two. Claude one fell into the trap here, but Claude two says the sentence is ambiguous as written. The pronoun she could refer to either the nurse or doctor. So I was really happy today that the new version of Claude from anthropic doesn't have any data. So Stuart Russell has this example of humanity is on a bus and we're heading towards a cliff and you, what should we do? And there's sharks and.
00:09:32.142 - 00:10:14.502, Speaker A: No, no. And so, you know, should we be working on this? It's a really interesting question. My thinking is that the limiting factor is maybe the hardware. So I'd rather see the problems emerge today on today's hardware than ten years. You know, if we didn't do anything, if we just did nothing and waited 1020 years and we have really fast hardware, the problems would emerge and, you know, everything would advance at such a fast rate because we could iterate so quickly on the fast hardware that we might be, you know, basically it would be like speeding up the bus. It'd be like, oh, yeah, we're on a. Let's pause and wait for the buses to get much faster and then we'll go, and then the bus will be going really fast and we won't know how to steer it.
00:10:14.502 - 00:10:28.746, Speaker A: So that's a theory. It's not. I'm not confident about that. But that's my working theory right now. Sorry. I think I feel like I missed something in your logic here. Like, it sounds like you don't want to study problems that you're worried are biased, but.
00:10:28.746 - 00:10:47.802, Speaker A: Oh, I think people need to work on the bias problems. Those are super important, and I'm sorry if I wasn't clear about that. Those are super important problems, but they don't show up in code. They showed much less in code. Yeah, sure. Why are we studying code? No, no, no. So people need to study those problems, but, like, what they're doing of releasing those models.
00:10:47.802 - 00:11:13.772, Speaker A: Like, I feel more comfortable about releasing a code model than a general model, like than GPT four, which has these gender biases that, I don't know. It's not clear to me whether it's a good idea in terms of studying is one thing and then releasing is another thing. So maybe the question is, where's code and where's gender bias in this analogy? Like, what is the sharks and what's the cliff and what. So did you say that?
00:11:13.788 - 00:11:14.100, Speaker B: Yes.
00:11:14.172 - 00:11:15.620, Speaker C: Consistent from one slide to the next.
00:11:15.652 - 00:11:48.034, Speaker A: Oh, sorry. You're not a language, an analogy I'm going to try to draw. I will have gender bias in this slide at the end. If it weren't for the sharks, we could just speed over the cliff, no problem. But I don't have a great answer. But just to be clear, I think it's super important to work on the gender bias. I'm just saying as a test bed, if we want to see if we want to figure out what's going on and which technologies to push forward, I somehow feel like pushing.
00:11:48.034 - 00:12:14.030, Speaker A: Not that we shouldn't work on gender bias, but working on language is one option. Working on code, you don't face a lot of those problems and you do face some of the other problems. So it gives us a chance to kind of anticipate some of the control problems that you might face. Whereas the stuff on language, like maybe we shouldn't even be releasing that until we get the gender biased stuff done. So that stuff, but the code, at least it's not stuck in that stage. Does that make sense?
00:12:14.102 - 00:12:16.954, Speaker D: So the code could be approving the loans for homes.
00:12:17.894 - 00:12:45.968, Speaker A: No. Yeah, so that you definitely have to think about a good, very, very good point. So code, just releasing a model is not enough. You're right, it very much depends on when it. Where it's being used. The code might not have feature, it might not have like, you know, it might not be choosing the weights of the features, but it would still be generating code that somebody could use. And I don't mean to say that there aren't discriminatory aspects to code generation.
00:12:45.968 - 00:13:12.392, Speaker A: I just think they're much less. It's a little bit of a sidetrack, but let me start with some history of program synthesis. So I actually have a patent from 2012 on generating a program. Well, it's Microsoft patent. So if anyone here tries to use computer to generate a program, maybe they own Microsoft money, I don't know. We got a patent. I got a cube.
00:13:12.392 - 00:13:42.050, Speaker A: I don't think we'll ever use. Exercise that pen. But we were doing. My point is, back in 2012, we were thinking about machine learning and programming, and people were doing stuff don knows well with code generation. But basically we couldn't really do it from language back then. So we were doing these kind of simple examples where we would have an example input and an example output, and maybe we're trying to count how many times each person occurs in this list. And we would generate a program.
00:13:42.050 - 00:14:23.274, Speaker A: This is what we could do because we couldn't really describe. It was hopeless to try to describe in English clearly what we want to do and get a program to work. And that went into, there's a nice system by Sumukhovani and to generate, that's actually today in excel. You can use Excel and it'll do some of the annoying macros for you. So it was a fun field back then. But then there were these breakthroughs, as you all know, and the codex language model, which is a version of GPT-3 that's fine tuned on code and a bit smaller, can finally do things. And so they gave these examples where basically you can describe a problem in English.
00:14:23.274 - 00:15:26.850, Speaker A: I think this exact problem has been shown in this workshop. And it'll generate code, it'll complete this, and it'll generate some correct code based on the english description. I don't know if you read the example, and, but it was kind of interesting, if you read their blog post, how they describe this system. Is it the system that translates natural language to code? So it's not a problem solver or a theorem prover, you know, reasoner, it's just a translator and these types of tasks, a lot of coding is translating, you know, in English, what you want to do. You, you have a clear plan, and you just describe it in English, and then it turns, it sends it code, which is super useful. But I guess, again, when you think about all the different uses of code generation, one of the reasons this field has kind of had a hard time is there's so many different people interested in different aspects. There's no one data set like imagenet for code generation, because some people care about being useful in spreadsheets, and a lot of people care about being useful to programmers today, other people, I'm interested in the general reasoning ability of these things.
00:15:26.850 - 00:16:05.254, Speaker A: So for me, a translator is not actually that amazing because it's like translating from English to French. Sure, you could translate from just a code I'm interested in. Hey, can this thing think? Can it solve problems? So that's just kind of one of the interesting things about this field is that there's many different angles. So in that direction. One thing that got me excited was there was a paper called Alpha code by DeepMind where they were able to solve some competitive programming problems. And that's kind of amazing. It would give like a whole, you know, english problem from code forces and, you know, all this examples, and it would output some non trivial code.
00:16:05.254 - 00:16:39.814, Speaker A: And so sometimes with, you know, millions of generations, but they had a quite elaborate system, and it's really impressive. And it, you know, was better in some metric than people who logged on one once a month. I don't know, their rating was okay, so it wasn't amazing, but still, it's like, amazing that something's happening here. There is some thinking going on in solving these problems. Feel free to interrupt, as I said, so. And, you know, today, we all know there's this GitHub copilot, which a lot of people use to program some statistics. You know, they're, they give tasks, they compare people.
00:16:39.814 - 00:17:22.596, Speaker A: According to some surveys people or studies, people are a lot faster when they're using GitHub copilot. They enjoy it more. They find their work more rewarding. So it's not as much that, you know, people are necessarily getting fired as that they are accomplishing more, which may eventually, of course, lead to people getting fired, but it may be more gradual process. Okay, so, I mean, there's a lot of studies of how people are using these systems and people are using them. All right, so first I want to talk about what these puzzles are. It's a long introduction, and then try to convince you that they're very similar to, or very interesting, I should say.
00:17:22.596 - 00:17:53.016, Speaker A: And they're related to programming and algorithms design. And then I want to talk about how we can get the computer to self improve it. So what are these puzzles? So let me give you an example. So an example will be, it's like a Sat function, like a Sat formula, but it's a function, it takes a certain encode and it takes an input, and it returns true or false. So you could have a boolean is input, and then you just have a Boolean expression, like in set. But here we have, you know, a function. So hello plus the string equals world.
00:17:53.016 - 00:18:28.816, Speaker A: And the solution is also a program in the general setting. But sometimes the solution can be really long or something, complicated thing, and it outputs an input which makes this function return true. So, yes, when you concatenate the string hello with the string world, and this function returns true and you've solved it. You know, there can also be an optional input here. So we have some parameter here. So in this example, can anyone say what this code is trying to do? The set function that says length of s is n, set of s equals three. Yeah.
00:18:28.816 - 00:18:59.352, Speaker A: Like if the unique characters in this. Great. So what would a string be that satisfies that? Like a bunch of. Yeah, so this is a great way, actually, for people to, for people even. I teach my kids to code this way. Just learning, you know, to understand and parse these things teaches you about Python. This length of set of s means there's three unique characters in the string ABC followed by a bunch of C's is.
00:18:59.352 - 00:19:21.920, Speaker A: So, yes, so you got it. Okay. So we did a user study. Programmers really enjoy solving these kind of puzzles, but as you can see, it's not translating from english to code. It's given a set formula of some kind or set program, find an input which makes this return true in some bounded amount of time, and then. Okay, so this is what a puzzle is. Here's two puzzles, and we can use these as in a few shot example.
00:19:21.920 - 00:19:46.926, Speaker A: We give a few examples, usually five examples, to a language model, and then we give it a new puzzle, and we say language model. Here's a new puzzle. What is this puzzle asking for? Yes, it's a divisor. It's a factoring problem. Give me any non trivial factor of this number, which is equivalent to factoring. So look how short this puzzle is. And this is basically factoring.
00:19:46.926 - 00:20:02.066, Speaker A: So the puzzle is short to describe. Of course, the solution might be very long and complicated. In this case, this is not a very long number. So you can. But in general, a puzzle can be short. Solution could be hard or impossible or long. So the good thing is that.
00:20:02.066 - 00:20:02.690, Speaker A: Wait a minute.
00:20:02.722 - 00:20:03.514, Speaker D: Could a solution.
00:20:03.634 - 00:20:25.852, Speaker A: Sorry. Could a solution to that puzzle just hard code a factor of that? Again, it could. So is that unsatisfying? It doesn't seem like programming. I mean, either you found it. I mean, it seems like there should be a distinction, you know, like, is the computational effort in the solving of the problem puzzle or in the code output by the solution? Right. Yeah. So the question is, like, where do you.
00:20:25.852 - 00:20:52.976, Speaker A: Where do you time the person from the time that they got all the puzzles until the end of. Until they submitted their solutions? That should really be the time, right. Your system should be timed from when it gets the puzzles until. Yeah. Any other questions? Great question. And that relates to something? I'll say in a second, but so, just to be clear, you don't need a human to evaluate the solutions. You know, you get a solution, maybe it's wrong.
00:20:52.976 - 00:21:08.376, Speaker A: Divide n, but n divided by two. That's not a good. That won't work for this number. It would work for other numbers. And then you run it, and you see, does it. You know, does this thing happen? Does it hold that the. If you plug this output into this input, you get true, and the answer is no, and you get an error.
00:21:08.376 - 00:21:47.608, Speaker A: So you can try again. You can run your language model, and it sees the pattern here, and these things do work, and they see the pattern, and this is what it'll actually generate, especially GPT four. Now, it'll just generate a for loop, which will correctly factor this number, and you run it. It works in less than a second, so you're ready to go. So these language models do understand this weird representation, which is good news for that representation, and people understand it as well. And, you know, some puzzles are really hard. If you take one of the RSA factoring challenges, there's a $200,000 prize that nobody, as ever claimed, and I think they might take it away.
00:21:47.608 - 00:21:50.432, Speaker A: Nobody's factored this number. There's a bunch of numbers that are left.
00:21:50.528 - 00:21:53.408, Speaker C: It's interesting that that code that was generated went from two to n and.
00:21:53.416 - 00:22:11.700, Speaker A: Not two to square root of n. Right. Which would have been like anybody, I think. Yeah, but given that you, if you assume that there's a solution, that's good enough. But, but there are version. There is a version I'll talk about in a second where that would be helpful. So we have a data set running time.
00:22:11.700 - 00:22:31.860, Speaker A: So we have by default, a 1 second timeout for all our problems. But we could have problems with different timeouts. You know, you could add an objective, which is, you know, not only do I want you to solve these problems, but I want you to solve as quickly as possible. But as Scott says, like, you might want to start the clock when they get the problems. Not.
00:22:32.052 - 00:22:33.508, Speaker C: I meant like a timeout.
00:22:33.596 - 00:22:34.292, Speaker A: Yeah.
00:22:34.468 - 00:22:36.164, Speaker C: As a function of the length of the input.
00:22:36.204 - 00:23:19.234, Speaker A: Yes, you could do that. It's not in our data set, but it's a good idea. So we have a public data set of these programming puzzles on GitHub, and there's so many puzzles, and I wrote most of them in the training set, at least. And they're all kinds of classic chess puzzles and a bunch of new puzzles and a bunch of puzzles taken from code forces or IOI and games and all kinds of stuff. The initial version, I have to say, was too easy because I wrote it before, or like, before I had ever tried GPT. So I was like, there's no way classic synthesis techniques will solve and come anywhere close to five. But then it was pretty good.
00:23:19.234 - 00:24:05.664, Speaker A: And so we had some hard puzzles. So again, just to show that there was that initial example of this crazy code versus problem, it can be written as a single puzzle. This is just one instance, but I feel that this instance, often you can concoct an input, like factoring one big number, factoring the RSA, that single big number, that would be impressive. I mean, it doesn't mean that you have a general factoring algorithm, but if you did factor that number, you probably came up with some clever ideas. Similarly for this, I feel like this input is, you know, capturing a lot of the difficulty and richness of this, and the computer can give it millions of attempts, like they did. Codex will eventually. The good thing is that each try, you can just try them yourself.
00:24:05.664 - 00:24:51.604, Speaker A: You don't have to like submit it or anything, you just run it on the interpreter, right? So you just keep running until you get a solution. So generally the definition of a programming puzzle is a function that takes your solution y and potentially an argument x, and it just runs it in some timeout bound tip and sees if you get any suspension. Very simple, it's NP. And so, you know, towers of annoy has a very simple description. And this is really getting at Scott's point of, you know, in this version, the number of disks is hard coded to eight. So you could literally just submit the solution of moves, which is like 500, whatever moves or 1000 that you need to do to solve this. You could just have that hard coded as a list.
00:24:51.604 - 00:25:43.598, Speaker A: But it makes much more sense to solve this with a program. So the computer will solve the program. Why do you hard code a number into this statement of the puzzle? Like why not just force it to come up with the general solution? Yeah, so this would be, the general solution is like you could have, you could try to force it to solve towers of Hanoi for 1234-5678 910. Yeah, and that, and so we're, I've gotten a lot of pushback on that and so we're adding that to our data set, this general thing, which is a unit test. So for factoring, you can either have hundreds of numbers or you can have, so basically you have to submit a program. Now it's going to be run on either a fixed set of things or random set of things. But see, let me answer the question, why? So if you think about it as like, oh, I want something useful.
00:25:43.598 - 00:26:19.708, Speaker A: I want something which programmers can use tomorrow, which is similar to what people are doing in coding and is a good data set for evaluating the coding ability of something that makes a lot of sense. This is much more like a code, a unit test that they actually use. If your goal is just to get something that does reasoning and solve complicated problems, I feel like this is just almost as good. And it's simpler. You don't need to parse and understand this extra stuff. There's something called Project Euler, I don't know if anyone has used that, but it's something that a lot of people learn to code and it's similar. Give you one factor, this number, do this thing on this number.
00:26:19.708 - 00:26:48.726, Speaker A: It's just easier for people to define the problem and evaluate it. You don't have to have a randomized mechanism and stuff. So both of them make sense, I think. I know people are tired and so I'm not going to go through all the details of our evaluation and everything, but basically what we find are that smaller language models do worse, bigger language models do much better. And so it seems like a reasonable way to evaluate language models. You can also use it. We also have comments, we also have english description of each puzzle.
00:26:48.726 - 00:27:20.208, Speaker A: So you can see that. Yeah, if you do tell the language model this is what you're trying to do in English, it does a bit better. What's k? K is the number of times you run the language model. So usually we generate, you know, so when we submitted this, we were actually doing like 100 or 1000 attempts per problem. Nowadays people using this data set have pushed towards the k equals one regimen because it's much cheaper and the models are getting much better, so they actually have a chance to solve it with one. And if you give it too many attempts, it could solve, you know, tons of problems.
00:27:20.296 - 00:27:22.164, Speaker D: And this is all about zero temperature.
00:27:22.504 - 00:27:34.176, Speaker A: When you do k equals one, you usually do zero temperature. That'll usually work best, but when you do 100, you have to have a higher temperature. Seven, I have a question. Yeah.
00:27:34.240 - 00:27:39.124, Speaker D: Is the model able to like run debugging loops where it may be.
00:27:41.144 - 00:28:10.174, Speaker A: Like when we're debugging? Good question. We in our evaluation did not do that, but that would be super interesting. Now they have, for example, like Scott was using before the, you know, the code interpreter built into OpenAI. So when we are using this to evaluate language models, we're just doing it in a few shot setting. But you could build all kinds of, you know, complicated scaffolding around the language model and have it debug itself. And I think that'd be super interesting. Yeah, great question.
00:28:10.174 - 00:28:36.586, Speaker A: And this is used in the evaluation. So GitHub copilot, I won't say how, but they use this to evaluate the new models that they get to make sure they're still working well, because every time they get an update to their model, they want to see that it's doing well. And this is one of the ways that they evaluate. Also, we're adding to this data set because we put it online. We didn't really think. But there's a problem pervasive in machine learning is that all the data sets are being compromised. They're all just online, publicly available.
00:28:36.586 - 00:29:09.684, Speaker A: Even if you don't make your data set publicly available, somebody else, even if I, you know, I don't know what I would do, but if I only emailed it to people. They would, you know, some of them could say, oh, I solved these problems and post their solutions online. Right. So this is a major problem throughout all of the community in language models. So what we have is I got a. I hired an ImO gold medalist. He has a gold in Imo and Ioi, the two, both the programming competition and the math, and we wrote 129 puzzles.
00:29:09.684 - 00:29:36.170, Speaker A: And so we have some of these puzzles that we just are not putting out. So if you want to evaluate on those puzzles, you have to email me and we'll do it in this way that you can't possibly. It's interesting question how I can make sure that you can evaluate your model and our puzzles without you stealing our puzzles. Do you actually. Sorry. Do you actually use crypto for this? No, I don't have. I don't know, like, I don't know if there's any facet of crypto that would make that practical.
00:29:36.170 - 00:29:48.414, Speaker A: I see. So basically, you just email you, and then you email the puzzle and we'll watch you. I got it.
00:29:48.874 - 00:29:58.194, Speaker B: I have one suggestion. Would it be possible. Would it be possible that if anyone wants to get those private tests, they need to contribute one back?
00:29:58.274 - 00:30:26.032, Speaker A: So one time you have to double the data set every time. I like it. I like it. This private data set, so far, I've tested it on a lot of models. Compared to the public one, it's very correlated, which tells us that probably the models are not over fit to these public data sets. And it's harder, which is good, because we need harder puzzles. It's aside, but it's kind of interesting.
00:30:26.032 - 00:30:50.948, Speaker A: So that's all about puzzles for testing? Yeah. Yes. Yeah. But I mean, do you really, like. There's different motivations. One motivation is just to see if it can reason. I feel like somebody who can factor that RSA number faster than the current state of the art has come up with a breakthrough.
00:30:50.948 - 00:31:33.594, Speaker A: I think that's another definition of breakthrough that might be more useful. I don't know. You could argue, but I mean, just knowing whether the exponent, if you know in the algorithm, when you know, I've done a lot of algorithms. When you do algorithms, you care about the exponent, right? Like, is it three or two? But actually the constants matter too, in real life. So I don't know, you could, you could, you could have a data set or you could evaluate this in different ways, but it would be kind of hard to have a, like a pure exponent data set that would look at the scaling law. Try to figure out if it's an n squared or n cubed algorithm. Would you prefer an n squared algorithm or an n squared cubed algorithm based on the concept? I don't know how to implement your idea, but it's a good point that that's traditionally how it makes you think about how we should evaluate algorithms.
00:31:33.594 - 00:32:22.332, Speaker A: Is the game here to test new language models or to test prompting schemes? Or it could be used for whatever. This is just an evaluation methodology. But unlike other ones which require you to translate from English to code or which might require both the translation and some thinking, this one is just pure reasoning. A programmer who speaks Russian and knows no English can look at our puzzles and solve them almost as well, except they won't understand very well. Whereas a normal code evaluation, when they do the IOI, they have to translate it to all the languages that. What does superhuman mean in this context? We have some puzzles that are open problems in math that, like, we have no idea how to solve. They might be solvable, they might not.
00:32:22.332 - 00:32:55.088, Speaker A: We have some that we know are solvable, but we have no idea how. The factoring problems right now, you know, the ones we have nobody solved would be an example of that. But there's many neat math problems that collapse conjecture and stuff that can be encoded as a tiny puzzle. Superhuman super. It also has a typo, but it's super. In order to encode the co ops conjecture, do you need to, say output? A proof of the co op?
00:32:55.136 - 00:32:55.784, Speaker C: No, no, no.
00:32:55.864 - 00:33:07.096, Speaker A: It's the opposite. It's the non co op conjecture that is a output account. Yeah. And then it could just be a cycle that could be checked. Right. Okay. Yeah.
00:33:07.096 - 00:33:29.856, Speaker A: So that puzzle may or may not be separate. So we've separated solvable. So we've separated those puzzles. I have a big fight with somebody. I wanted to put in a for Maslas theorem puzzle, which I don't know if I believe from. So, you know, the proof, it's like, you know, hundreds of pages of proof. You know, if you give me three numbers, you know, I think I would believe that more than hundreds of pages of proof.
00:33:29.856 - 00:33:53.054, Speaker A: Anyway. There's a set of puzzles, but that one, no, it didn't make sense. Anyway. Okay, so this is the part that I'm excited about, but I'm also super excited about all these questions. And I hope it's okay if I, like, eat into my question period and just let the talk go. So what's really exciting about these puzzles, or one of the things is that they're really good for self play. So we've all heard about in chess.
00:33:53.054 - 00:34:29.848, Speaker A: Initially, we were learning from experts in chess and go, and then the computers got much better by just playing against themselves, knowing the rules of the game, they could just, even without any human data in chess, they could do much better. And we want to do the same thing in coding. That's a natural goal. Initially, we were learning from code on GitHub, but maybe we can get these things to do much, much better through self play. And the idea is that when you train on human data, you're only usually only as good as the humans whose data you've trained on. If you want to get better than superhuman, if you want to get superhuman performance, you got to do something else. You can't just train on the human data.
00:34:29.848 - 00:35:16.440, Speaker A: So if you want to get superhuman performance, you need to have some notion of correctness and so self play as possible with these puzzles, because we have an absolute notion of correctness, unlike you wanted to generate english programming problems, right? It's been done. People have tried, but, like, who knows what the right answer is? The person who generates the problem, you know, has to have. If you wanted the language model to generate a problem, sometimes it'll generate the correct solution, but sometimes it won't. You know what I mean? Like solving the problem is you have to solve the problem as well. Here you can generate it, and it may or may not be solvable, but so the pipeline we have very simple, and I think this could be improved on, but basically, we just take our training, we take our handwritten puzzles, we divide them into training and tests. We show the training. I'll show you how.
00:35:16.440 - 00:35:54.156, Speaker A: We show very simple process. We show the language model, the training puzzles, only from which it generates a bunch of synthetic puzzles. It could generate millions if you want. And then it solves those puzzles. Okay. It tries to solve those puzzles, maybe 100 tries each one. And then we just run the Python interpreter to see, ok, are these correct solutions? And we take only the puzzles that have correct solutions, verified correct solutions, and then, so now we have this synthetic set of a bunch of synthetic puzzles with solutions that we know are correct, and then we can fine tune the language model and get it to iterate and get it to work better.
00:35:54.156 - 00:36:17.496, Speaker A: We've only found like one or two loops to be helpful. So it's interesting research question for the future to how to make this continue to go up. Are you going to show an example of the puzzles that it generates? Yeah. So first, how do we generate the puzzles? So we just give it a bunch of training puzzles. Literally without any solution. They're like, oh, here's a couple of our puzzles that we made from the training set. And then it'll just start spitting out.
00:36:17.496 - 00:36:31.014, Speaker A: It'll continue the pattern. It sees a pattern. Language models, you know, non chat based language models give us pattern. It generates a bunch of puzzles. These are actual puzzles. It would generate none of them. All of them are so nice.
00:36:31.014 - 00:37:06.958, Speaker A: But this one, for example, it's just an equation. I just gave that to illustrate a very simple puzzle. And then we solve these puzzles again with the same approach. We give our same few shot examples or five examples or whatever. Then we stick on one of these new puzzles and we ask the language model, what do you think would come next? And it tries a solution. It tries a hard coded solution, maybe it's wrong, and you try again, do it 100 times, and maybe it gets a better solution because these things are not very good at, to solve this equation here. I mean, this is basically like, even though it's a trivial puzzle, it shows an understanding of the meaning of plus here.
00:37:06.958 - 00:37:26.402, Speaker A: The meaning of plus is the solution to this equation is basically, the meaning of minus is the opposite of plus. And it does that. And it doesn't have to do the calculation like it human would solve it. So you get the puzzles, you get the solutions. This is another puzzle here. What it's asking is kind of, it's an okay puzzle. It says like, what does it want? It wants a string.
00:37:26.402 - 00:38:17.560, Speaker A: It wants a string, which is a palindrome, okay? That's what this weird notation is. And it has 4600 copies of each of these strings inside there. Hello there you. And the puzzles are very like, you know, human like, okay, so, and then, you know, it'll solve it this way. Now it seems like you'd be able to learn much better if you weren't doing things for a single constant, but you'd found a solution that worked for, you know, half a dozen different integers, because then you've got high probability, you actually got a general solution that works. So I agree that if you can solve these puzzles for general sets of inputs, it's just sometimes. So for example, let's look at this example.
00:38:17.560 - 00:39:08.568, Speaker A: So you want a palindrome that has each of these strings 4600 times. There's actually kind of a bunch of annoying edge cases here, which is like, what if one of these strings is a palindrome? Then you need a different solution. What if one of these strings contains another string? So what I think might happen, it might be harder to generate the puzzles that you are talking about, because these puzzles, if you generate a puzzle with one instance, it may have a solution, it may not. But if you try to, it's just harder to design a good puzzle of the type you're talking about, because you need to have kind of like a bunch of tests that are all passed by the same algorithm. Does that make sense? So, from a generating point of view, I think that's harder, but I think it's a super interesting question, and I'm hearing the feedback from everyone, which is everyone wants problems that take multiple inputs rather than just one.
00:39:08.616 - 00:39:10.120, Speaker D: So in the loop that you showed.
00:39:10.152 - 00:39:13.384, Speaker A: About proposing a problem, and what's the.
00:39:13.424 - 00:39:18.896, Speaker D: Incentive for the proposal to pose a problem, which is difficult enough but not impossible.
00:39:18.960 - 00:39:44.480, Speaker A: Yeah. So right now, it's just trying to copy the distribution of human type, human type of puzzles. Actually, our loop is even worse because we're using the same model to generate and solve. So it's getting fine tuned. That's not very principled. I think it's a great RL problem. I think the problem of, like, how do you come up with good puzzles? But now we have a meaningful definition of a good puzzle, which is one which is going to make it to learn, so that eventually, on the test set, it will do better.
00:39:44.552 - 00:39:54.880, Speaker D: So in other cases where self play has been successful, there was this loop of, like, the competitiveness, where exactly the two sides of the engines are competing with.
00:39:54.912 - 00:40:10.954, Speaker A: Right. So, yeah, so the thing that's obvious that comes to everyone's mind first, but is, is maybe not the only thing that matters is a hard puzzle. Like one that's solvable but really hard. But that's not necessarily interesting. Right. Like, I come up with a bunch of factoring puzzles. They're all really hard.
00:40:10.954 - 00:40:56.934, Speaker A: Maybe they're not, as a group that educational. So. But we do at least have a definition of what good puzzles are, which are educational, so that when the model is fine tuned in these, it'll do well on future test puzzles. So unlike, you know, people have always asked, like, oh, I want to a computer that will pose a theorem, a new interesting theorem. Well, how do you define interesting theorem? Good luck. But here, at least, we have a metric for evaluating useful theorems. If you know useful puzzles, if you propose puzzles and solve them, and then you use that as training data for your system, and it gets better at solving the test puzzles, then I claim that those training puzzles as a group were useful, interesting, or whatever you want to call them, educational, at least not in ours.
00:40:56.934 - 00:41:11.958, Speaker A: That's future work. I think we were just. Yeah, no, no, yeah. Thank you. Thank you. Yeah, no, no, we didn't, we don't have that in our pipeline where there's nothing interesting for our language model to generate puzzles. This is where, and I know of one person who's working on this in print.
00:41:11.958 - 00:41:37.550, Speaker A: But, like, there's a lot of interesting questions here of how to do this. Is there any gradient feedback in this? Just a quick question. No, the only thing that's happening is we're filtering for correct, correct solutions and then fine tuning. So math, the model doesn't change, but even this works. Oh, and here was some fun. Scott was asking about puzzles. So some of the more strange puzzles, like this was an interesting, like, I was like, wow, look at that puzzle.
00:41:37.550 - 00:42:22.094, Speaker A: And it solved it, too. And so then I did some, you know, detective work to see where these numbers came from. And I found that there was this advent of code website which had this beautiful, like, english problem, which was essentially like some discrete log problem. And basically the best thing I could find, or I assume that the model done. I never saw this code anywhere online, is that it basically, like, translated this english advent of code problem into a programming problem and spit it out. Who knows? Like, we don't have the training data slides. Yes, this part has the, this puzzle does contribute, and you do see a lot of funny things.
00:42:22.094 - 00:42:52.030, Speaker A: So let me give you even funnier example. That's my hypothesis. I don't know because I don't know what it was trained on, but I could not find this code, this like particular python code online, anything like this. I tried many variations, but I could find this. If you just search for these constants, it's a giveaway. So maybe this code exists somewhere inside OpenAI's training data that I don't have access to, but maybe it just translated it from here to there, which isn't that hard.
00:42:52.142 - 00:42:53.914, Speaker D: There are probably solutions to.
00:42:56.334 - 00:43:38.524, Speaker A: Yeah, yeah, yeah, yeah, yeah. I didn't even notice it. What is that puzzle asking for? I think it's a discrete log, some kind of discrete log thing. I can share it with you later. Here's a funny one. So it says, give me a list of length three, that has all the numbers between one and ten in it and adds up to 100. So these really show like, the lack of common sense that the models have, but in a very easy way for us to make precise and formal.
00:43:38.524 - 00:44:01.196, Speaker A: Right, compared to sort of natural language and stuff. So this is kind of a fun way to really see what they're doing. And then you have all kinds of funny strings that it encodes. So. Okay. But many of the puzzles are very boring and like, half of them were unsolved, weren't solved by the language model. We don't know whether they're solvable or not.
00:44:01.196 - 00:44:41.230, Speaker A: We just discarded them. We discarded the ones that we, we couldn't go. So then it's late, I'm not going to give you all, I'll skip the detailed analysis. Basically, across a range of models we tried, it was like doubling the accuracy. This is like a tiny model, and this was done, these experiments were done a couple of years ago. So it was actually GPD Neo 3 billion parameter model, which is small by today's standards, but it's still basically, you could fine tune it on human data, which is the handwritten puzzles that I did in the solutions, or you could fine tune it on its own puzzles and stuff, and it would get much better. And I have a bunch of numbers, but I think people are tired, so.
00:44:41.230 - 00:45:24.580, Speaker A: But generally it just improved performance like we hoped not to GPT four levels or anything, but it definitely worked. So why did the performance kind of plateau? There's an interesting property here. We tried to investigate a little bit. You can use the embeddings, the GPT also provide open amp, provides embeddings of these puzzles. You can give it just text and it'll give you some bedding. And you can kind of see that the puzzles that were generated by smaller models were less diverse, they were sort of more concentrated, and the puzzles that were generated by a big model like codex were more spread out, and the human ones were even more spread out. The takeaways is that, and I'm kind of untied.
00:45:24.580 - 00:46:02.552, Speaker A: So the takeaways are you can use self play to generate synthetic data like we were talking about yesterday. Are we going to run out of data? Well, maybe, but maybe we'll find other ways to generate interesting, meaningful, useful data. Maybe the model's understanding of English is fine, maybe it's understanding of reasoning is not good enough yet. And so if we just want to dive down into the reasoning ability of these language models, is this great? Maybe, I don't know. It's a little concerning. These things are basically np problems in some sense. It's related to alignment.
00:46:02.552 - 00:46:39.384, Speaker A: Whenever you can specifically define your goal as a program, and you say, look, this is what I want from you language model, and I can evaluate it by machine and say, this is correct, this is incorrect. That's what a puzzle is. So it's sort of, in some sense, the definition of whatever is perfectly alignable, for better or worse. And the test has to run quickly, of course. Will this eventually lead or help? Will gptn eventually write gpTN plus one? I don't know. It's kind of concerning and scary. But I do think that code, if you compare to other domains, is a domain where we can move forward relatively quickly.
00:46:39.384 - 00:47:22.032, Speaker A: And it's a very good question whether we should. I don't know, but it potentially could be good because maybe at least one. The argument, I can put it this way. The best argument I can think of for why we do want to move quickly in code is we want to see these problems sooner than later. As soon as we see a system doing something really bad, we can start to address it. As Scott was saying, many times it's hard to work on these alignment problems before they actually start to come up. So, you know, maybe we want to see these problems today with GPT four rather than waiting ten years and seeing it only with GPT six or seven.
00:47:22.032 - 00:48:02.666, Speaker A: Maybe that's the argument. I don't know. There's another argument which I think Stuart Russell and other people would say, which is this is just contributing to the advance of these systems, you know, which could have terrible effects. I'm not claiming that working on this is better than if your goal is to, let's say, help AI with safety and fairness. Obviously you might be better off working on the discrimination problem or working on the safety problem. So I'm not saying this is the most useful thing to do, but I'm not sure it's a bad thing. And actually I spent a lot of time thinking about, should I present this here? Am I helping or hurting? It may be overthinking it.
00:48:02.666 - 00:48:52.462, Speaker A: Maybe we're. Some people think we're really far away. And this is. Sorry, what is an example of a problem you think can come up from this coding challenge? Well, I mean, the fear is that these things, like an example I have is I was writing code that would run code, okay? So I would tell me gp four output code that can run code in the sandbox. And then I'd have to do this passive 100 trick where I'd have it generate like in some complicated loop, and one out of a thousand times it would turn the sandbox from sandbox equals true, to sandbox equals false. So not from some evil motivation, but just it's doing that kind of thing because, well, by chance, or maybe that would be a successful strategy for it. And I'm putting it in some process here where it's survival of the fittest kind of the evolutionary pressure is to escape the sandbox.
00:48:52.462 - 00:49:20.270, Speaker A: If that's going to get it more. Even when you put a comment, do not change this to false. Still change it to false one in a thousand times. That's not very scary. This model today is not very scary. But if you imagine a much more capable model, I guess the fear is that people talk about is that that thing will write code that would escape the sandbox and copy itself onto. Do you have more takeaways? Because maybe if so, you should finish the talk for more questions.
00:49:20.270 - 00:49:41.320, Speaker A: At this point, the only last I have is my bus. Like, so, you know, we're on the bus. The bus is heading towards the cliff. We don't want to have, first of all, we don't want to have offensive, you know, we don't have the AI of today or people saying all kinds of hate speech and bad things on the bus. So we want to. We definitely important problem to work on that. I do anticipate that we might run into some problems along the way.
00:49:41.320 - 00:50:02.664, Speaker A: It's not like all or nothing in my mind. And those problems can slow us down and make us realize, oh, we need to improve our bus, our steering ability. And, you know, then the bus is going to head towards the cliff. And my hope is that we will turn the bus and steer it away from the cliff and hopefully AI and us together will work.
00:50:12.524 - 00:50:31.660, Speaker B: Thank you for the great talk. And also thank you for making the talk very engaging. At 05:00 p.m. I have a question. Also comment. I think also circle back to Shah's question on gender bias and the code generation. I don't think you can separate the fairness and the code generation.
00:50:31.660 - 00:50:45.276, Speaker B: In fact, the more I think about it, the more I feel like any kind of fairness questions can be formatted as a code generation question. So for example, the one you gave about nurse and doctor can literally write.
00:50:45.340 - 00:50:46.652, Speaker A: A function to do it.
00:50:46.708 - 00:50:53.796, Speaker B: Right. If I have a very adversary intention, I could say something like, find all the people in us who are good at sports.
00:50:53.860 - 00:51:26.276, Speaker A: Yeah. So there are people who have said, you know, like, classify is smart and takes a function, you know, that is race, and then it'll spit out code and it's offensive and it's terrible. In our data set and in our synthetic test, that's very, like, I haven't seen that particular. It's very rare. I don't know why, but it's not often generated. Like, you can get it to do things like that, but it doesn't happen as naturally as it does. I'm not saying it doesn't happen, I'm just saying it happens much less frequently than you do.
00:51:26.276 - 00:51:30.944, Speaker A: See, in plain natural language, which is full of gender bias.
00:51:31.764 - 00:51:48.036, Speaker B: I agree. I want to push back by saying that part of the reason could be that a lot of the code benchmark are more about numbers, integers, etcetera puzzles. Right. But if you talk about the deployment in real world, there will be companies who are going to use it for all sorts of actions.
00:51:48.100 - 00:52:34.044, Speaker A: So that benefit gets back to sort of like, why are you doing, you know, what is the point if the point is to sort of become useful as a tool? I 100% agree with you that if people are using this and they're writing code, which is, you know, in that direction of causes their system to give biased results, that's terrible. And they definitely need to address that. But if your goal is to sort of work on algorithms and these examples aren't coming up, then you're not as plagued by that problem. I'm not saying it's not a problem, but I agree with you that, like releasing this, I'm not trying to say they shouldn't work on making sure that their code generation models are not biased.
00:52:37.244 - 00:52:58.864, Speaker D: Thank you again for the great talk. So quick question. So, with regards to chess, we worked out self pair a long time ago. Some of the smartest people worked out what is missing in terms of what is conceptually missing in the analogy to self playing chess with respect to self improving llms.
00:52:59.994 - 00:53:41.452, Speaker A: So great. So let's go back to that slide. The analogy slide. So I think basically in chess, you have the rules of chess, which are very well defined and easy to understand, and there's a notion of a winner and a loser. And so the goal is to, you know, you have two players, and the goal is to come up with a strategy which, pitted against some set of players, does really well, beats the world champion in particular in coding. What are the rules? I mean, there's rules of evaluation of code, but what's the objective? And so we made that precise here because we have a set of puzzles. And so basically we do, we have created a game of sorts.
00:53:41.452 - 00:53:53.850, Speaker A: So we've tried to close the loop for puzzles. It doesn't work for coding in general, though. So our solution is not a solution that's necessarily going to make it much better at coding in real world tasks. Is that what you're asking?
00:53:54.002 - 00:53:55.866, Speaker D: Yeah, but can I just ask a.
00:53:55.890 - 00:53:56.762, Speaker A: Quick follow up question?
00:53:56.858 - 00:54:04.338, Speaker D: So if you have to instead work with theorem proving, if you just take the analogy from code generation to theorem proving.
00:54:04.386 - 00:54:24.216, Speaker A: Exactly. We could do that here as well, where we would take. We would write down, you know, some hundreds of theorems, some of which we know the proofs to, maybe some we don't, or some of them, but they don't have to be necessarily hard. But you'd write down a bunch of them, and then you would take. Divide them into training tests, and your goal would be to get a theorem prover using the training set that does better on the test ones.
00:54:24.280 - 00:54:24.480, Speaker B: Right.
00:54:24.512 - 00:54:32.752, Speaker D: But would you still be missing, uh, because. Because the rules of reduction are very explicit in. In theory improving.
00:54:32.808 - 00:54:33.000, Speaker A: Right.
00:54:33.032 - 00:54:34.576, Speaker D: Like, you have some limited numbers.
00:54:34.600 - 00:54:38.664, Speaker A: Very similar. It's very similar to, uh, solving these puzzles. Each puzzle is a.
00:54:38.704 - 00:54:47.266, Speaker D: But you think the self play, like, the entire loop is still not. We still haven't closed the loop on self play and improvement here. Yeah. Is that right? Am I hearing.
00:54:47.330 - 00:55:30.574, Speaker A: Well, I mean, if you have a static set of theorems or puzzles or one of those which are np basically, then I'm not saying I've solved the problem, but I've put it down as a game where the objective is clear and you're welcome to, you know, basically, I didn't do the rl that you need to do, but I. But I think at least the game has been defined per se. Do you see what I mean? Maybe I should write it out more clearly. The game is do well in the test set. You're given these trained puzzles and as much computer time, whatever you want to try to get better at this game, but test time is going to come. And the test puzzles, you know, is what your evaluation does. That make sense? Love that talk.
00:55:30.574 - 00:56:01.818, Speaker A: Thank you very much. So many questions are trying to still. So, first of all, your case where you did some detective work on the numbers concepts that showed up made me think that there's many, many algorithms described online in various different forms. Have you considered just trying to trawl the web of those and trying to construct it from that? Yeah, you could. You're saying, like, prompt GPT four to take a programming problem and convert it into a puzzle. It's a good idea. We have not done that.
00:56:01.818 - 00:56:34.694, Speaker A: And, okay, so, relatedly, if you use a programming language other than Python, like, for example, let's say you use a logic programming language, like functional programming language, then you're starting to get towards some kind of inductive reasoning where you're basically trying to come up with, you know, a theory that explains data. Like we haven't. Yeah, this, this you could do prologue puzzles. You could do puzzles which are about fitting data. We could have done, like, the symbolic regression. You could probably turn a lot of those problems into puzzles as well, right? Like from the previous topic. Those could be great, make great puzzles.
00:56:34.694 - 00:57:40.346, Speaker A: So, yeah, there's a lot of. I think you're getting the intuition that there's a lot more that could be done with this. So I had a question about the application. You referenced towers of Hanoi in there, and this is a commonly used small children psychology. So I'm sort of curious, like, is towers of Hanoi as like, a type of code being currently used or post used for defining superintelligence as somewhere, perhaps as a modified version somewhere else in polynomial hierarchy? Can you use it to demonstrate intelligence or something? Commonly used intelligence test. Would you, or are you using it currently as an intelligence test? Oh, yeah. I don't have an opinion on defining intelligence, but this is a measure, and you could think of it as intelligence, that the puzzles are grouped by tags, like a program capture.
00:57:40.346 - 00:58:09.054, Speaker A: So you have, oh, this puzzle you solve with a dynamic programming or recursion. And so it does give you sort of a categorization of the ability of a model broken down by types of solution. I don't know if that's where you're going, but, you know, some problems require recursion, some problems require dynamic programming, some require that. And we have puzzles that fall into that. We have some puzzles that play games that'll play. Your puzzle is to beat this bot and connect four. And, you know, so I hope that's your question.
00:58:09.054 - 00:58:38.204, Speaker A: Sorry, me, you walk over and we're late on time. So. So you gave that one example of the. The impossible public, and it was interesting because all of its individual conditions were coherent and all this stuff, but it was the interactions between them. Was that a consistent problem, was when it made impossible problems, it just didn't understand how they related to one another. Yeah, I don't know. I mean, there one explanation is that, yeah, it's lacking common sense.
00:58:38.204 - 00:59:12.518, Speaker A: That's what I would think is happening here. I suppose another explanation is maybe it doesn't even understand what we want from this framework. Typically, we put in, like, 440 puzzles into the prompt. In order for it to understand what we want from it, it would have to understand that all these puzzles are solvable. So another explanation is, oh, it didn't even understand the rules of the game. So it's not that it's lacking this common sense, but you will see the lack of common sense on the solutions as well, where it puts in a thing, which is obviously not, but with the chatty models you can have, it'll talk through its reasoning, and I'm sure this is going to work. That didn't work.
00:59:12.518 - 00:59:42.364, Speaker A: Oh, I'm sorry. That didn't work. So, working off of this, did you ever see what would happen if you included examples like this that were impossible and say, these are not good puzzles, and see how responded those things? Could it extrapolate like, oh, these are things I don't want is incompatible? I don't know. That's a great question. And also, I don't know if after it generated this puzzle, if you asked it, like, you know, if you, if you did that before or after, maybe it would generate each puzzle and then say, yeah, I can, this one is bad or this one is good.
00:59:44.584 - 01:00:19.092, Speaker C: So I gotta admit, I hate puzzles. So on a personal level, not these kind of puzzles, but. So I was trying to understand during. So you're saying solving puzzles might get us close or seeing the ability of Chachi PT or these kind of models, large language models, solve puzzles. It gets us closer to understanding intelligence in a way that we can quantify differently than with language and answering problems. But it's still restrictive in some sense. Like, we're telling what the rules are exactly.
01:00:19.092 - 01:00:50.720, Speaker C: And then they're supposed to solve it. Exactly. And it's in P and we're all happy about it. So is there some way to give a mathematical puzzle where it's not so obvious? Obvious in the sense, not that it's, okay, so here's my example. Suppose you give some of your mathematical puzzles, and there's a bunch of solutions, okay? And then you give them, like you had this thing where you've also example solutions in order to there. But you always give it the lexicographically smallest solution, right? But you don't tell them. That's what you do.
01:00:50.720 - 01:01:11.810, Speaker C: You give a solution to a puzzle for which there's a bunch of solutions, okay? And for each one of them, you give the lexicographer. So there's some hidden, so there's some intelligent, there's something clever to understand that you haven't explicitly spelled out. What? Well, no, but it's not a puzzle. See, what I hate about puzzles is they tell you what the rules are and you must adhere by it.
01:01:11.842 - 01:01:14.962, Speaker A: Oh, I don't like, you don't like the np, you don't like the, you.
01:01:14.978 - 01:01:20.234, Speaker C: Have all people that I can't break the rules or I can't at least.
01:01:20.314 - 01:01:41.524, Speaker A: Apply some personal, I mean, one of the things that I'm hoping here is that constantly what happens. We say, you know, we're going to study chess, and we're all excited. We figure, like, when we solve chess, we must have learned something fundamental about intelligence. And then we solve chess, and you know what? It doesn't generalize. Right? And then, oh, God, we did that. And, you know, then we did alpha fold. And they're brilliant ideas, and they do generalize to some extent.
01:01:41.524 - 01:02:39.340, Speaker A: I mean, I'm not saying they don't, but here again, I'm optimistic. Like, if you were to solve these things and get the ability to write long correct code, would that be useful for you to write a novel or something like that? I mean, I guess I'm just hopeful that the ability to solve these problems as well as humans, which means sometimes writing long correct code that is right, involves all kinds of reasoning and thinking that would be useful in other domains. But I admit that I'm probably making the same mistake that every other, you know, problem that eventually succumbed and then generalize is doing. It's not really an answer to your question. I mean, I specifically was anti what you're saying in deciding this data set. I wanted the objective to be as clear as day, and that way you can generate your own puzzles, and the rules are clear. I hate those math tests, which are like, what's the next number in the sequence? And I can give you an explanation for, hey, but maybe, you know, but those are the more common type of intelligence tests.
01:02:39.340 - 01:02:47.224, Speaker A: I don't know. Okay, I think maybe only one more question and not try the audience patience too much. Appreciate it.
01:02:47.884 - 01:03:02.384, Speaker B: So can we design some kind of local reward, for example, based on the number of tries to solve that puzzle to define whether it's good puzzle or not puzzle. This is one kind of reward we can consider and failure, this is a global reward.
01:03:04.564 - 01:03:05.060, Speaker A: Yes.
01:03:05.132 - 01:03:15.974, Speaker B: And the other thing is that whether we can use human knowledge along with digital connections manifesting to guide the line of developing.
01:03:18.154 - 01:03:19.282, Speaker A: What's the last question like?
01:03:19.338 - 01:03:30.934, Speaker B: Use. Use, like more kind of traditional puzzles from human and to guide the line like the human at.
01:03:33.154 - 01:03:47.694, Speaker A: Yeah, I mean, you could definitely. Yeah. All right, thank you very much. Okay, thanks, Adam. Thank you for inviting me to the sense of I could have been there.
