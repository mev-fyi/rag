00:00:00.840 - 00:00:04.274, Speaker A: Welcome to the last day morning session.
00:00:04.654 - 00:00:24.754, Speaker B: And I learned from Morris that we shall all say, please subscribe to the YouTube channel. Exactly.
00:00:25.634 - 00:00:27.170, Speaker A: YouTube channel. Yeah.
00:00:27.322 - 00:01:06.774, Speaker B: What? Simons, like the talks. All right, so waking us up with a little bit of a discussion, with maybe some controversial thoughts, maybe not. We'll see. Feel free to interrupt me, as always. And I can start by saying that clearly I won't have the answer to this. Don't hold your breath. But this is sort of the context, because as you or some of you know who I talked to before, I worked a lot in the applied context.
00:01:06.774 - 00:02:23.692, Speaker B: And the question there, in a way, is always this one, can we do this? And how is that relative to what we do currently? Or people get carried away in the discussion and forget to compare it to what they're doing currently. That's another element of this. So the applied context in which I have had most of these discussions, although not all of them, is that of profiling the unemployed. And I say profiling with a little bit of a hesitation because the EU AI act just told us no more profiling. So in a way, people working in that space might need to use different language in the future. I don't think it will change what they actually do. The motivation there is that unemployment agencies, at least in Germany, but also many other OECD countries, they try to streamline and make the process more efficient on how to allocate people who fall out of employment, or have been out of employment for a long time, into two things, either a welfare benefit program of sorts, you know, or some interventions to reskill them or to assign them to new jobs.
00:02:23.692 - 00:03:14.744, Speaker B: Right? So those are the two prediction problems, if you will. And in Germany, the context is that these are really very localized activities. So every community has that little job center, and I mean really small communities, and might just be a sign outside and one person sitting inside. Right? And then you walk in and tell them your story and then they do something. And it's not hard to imagine that that's very subjective, depending on how well the people are trained who work in these job centers. And it can be incredibly inefficient. Plus, what is the history of experience they have in consulting with people that come in and need job when they make these recommendations? So it's very sensible to think, well, let's use the millions and millions of people who have gone through this process and the many decisions that have been made to learn.
00:03:14.744 - 00:04:25.908, Speaker B: Right. In the localized level, there has been the first wave of attempts to solve this problem was to make rule based decision guidelines. If they have been without a job for this amount of time, then this kind of training program might work and the like, but not hard for this community to imagine that that can be quite inefficient still. And so the hope really is, and I would say still is that we could solve algorithmically or with the knowledge of data, any of these problems or all of them, that it is more efficient, effective and objective. There have been a lot of countries in which this is a selection of countries of which I know this has been tried. I'm sure there are others. The most recent case is Austria, and that's sort of a little bit like our, our compass case in a different application setting, but hugely disputed in the press, interestingly, not so much for the actual algorithmic decision making, but for the fact that data is used in privacy.
00:04:25.908 - 00:04:46.298, Speaker B: Privacy, privacy. It's Europe, after all. So we're still Europeans. We're still having the history we have. That was what killed it, and then it got revived and then it was modified. So this is our prime example. But in Germany, the system is quite similar to Austria.
00:04:46.298 - 00:05:50.244, Speaker B: And in the ten years that I worked in, well, partially worked in the Institute for Employment Research, which is basically the research arm of the employment agency, the discussion how to deal with this problem came on in waves. It feels like every five years, when new directors come in, there's another attempt to get this going, and then it dies again and so on. So this is for context, right? We discussed this week a lot issues with that pipeline. And where's Emily? There you are. Emily was kind enough to shorten my talk a lot or allow me to focus about something else, because she already told us with this beautiful graph about all the decisions that have to be made, sort of in the middle section. Right. What made me smile was, you know, seeing that little disk here, not because I'm so old that this at some point was an innovation compared to what we used before, but because it still starts with this.
00:05:50.244 - 00:06:59.304, Speaker B: So many decisions that happen before the data get on that disk, that that is easy to overlook. Right. And so I thought I focus on these other two sets where a lot of people are also involved that make decisions that influence this process, and that might be good to have on one's radar or to, when developing these systems, talk to these people ahead of time so that you can avoid some of the problems. Right. So that's the goal here. These questions, or the three questions that I want to ask our answer, well, I guess, pose to us here for discussion, is the larger scope on what we are using to guide the prediction in the context of the question of the talk, right. What is the algorithm using? What would be the human using when she makes that decision? And likewise, what affects our perception at the end, whether this is fair, whether this is okay, whether this is acceptable, because both of these are key to structuring the middle part correctly.
00:06:59.304 - 00:08:28.434, Speaker B: And then a little bit of a discussion on can we improve the whole thing so that maybe we make some progress in using this correctly. So for the first segment, what are we using to guide the prediction? There are a couple of things I want to point out. One is, is clearly we're using guiding principles, right? And I had a hard time deciding whether I put them at the front or at the end because it's sort of relevant for both of these. But I decided to put them on the front mostly so that I don't forget, I will say the least about them. But what is interesting, and last night we discussed this a little bit. It seems to me, or I guess you had that experience too, moving to Germany, that our lens on justice is a little different or on fairness is a little different, because I learned that yesterday what the roots could be for that. But Morit's hypothesis was that in the absence of a civil rights movement and sort of a clear group of people that you try to protect or, you know, promote, it is a little less clear what it is that we are looking for when we try to promote fairness.
00:08:28.434 - 00:09:24.510, Speaker B: Yet everyone wants to promote fairness. And it's not that we don't use this term, you know, I mean, that could be a good new stereotype about the german setting, but that's not the case. Right? I mean, we do try to do it, but it's, you never quite know what, what people have in their head when they talk about this. Right? Are they talking about in the labor market context? Are they talking about that this job programs should be allocated such that all job seekers eventually have the chance to find a job? Or should they be allocated so that it is relative to the contribution they made to the unemployment benefit program, the number of years they have been enrolled and contributed to it, or if the allocation is made based on their regional and socio demographic background characteristics or whatever. Right. And each of these decision rules would fall into a different philosophical corner of what the guiding principle is. Right.
00:09:24.510 - 00:10:28.466, Speaker B: And what is interesting about that is that you would obviously evaluate your algorithm quite differently depending on what principle you're using. And in the labor market context, we've seen this and discuss this at length, I guess in that paper, what these different decision principles would mean for the algorithm as we plan them. But what is interesting is, again, human versus machine with these individualized local and regional job centers. We don't have this discussion. It's popping up now because we are using, or want to use a machine, and all of a sudden we notice, wait a minute, we have never had that conversation, what it is, what it's trying to do here. And so in that sense, if nothing happens with implementing these algorithms, that's a useful outcome, at least in practice, that I'm watching happening among these labor market organizations and presumably in other contexts, too. But this is the only context I can speak for.
00:10:28.466 - 00:11:27.270, Speaker B: And this also led us sort of to think that it might be helpful that we, you know, when we talk about these models and the decisions and where the problem arises, that we sort of separate, I mean, at least with respect to these ethical principles, that we separate that out, you know, from, you know, the actual prediction tasks and then the decision tasks that we do using these models. And obviously there's a bit of an interaction. Yeah, yeah, go ahead. Yeah, you're saying. So let's say we use an algorithm or whatever procedure we use and predict within the labor market, somebody's going to be an example, a good employee, and then whether this is actually. Right. So there is, there's a, there's a step here that often prevents us from doing the things exactly how we want to do this.
00:11:27.270 - 00:11:50.994, Speaker B: Right. I mean, we can't support a quarter of a person. There are other constraints about the world that guide what kind of suggestion we can do in this localized labor market context. You could imagine, well, the prediction would be this person should really have this kind of job, but it would require a move from southern Germany to northern Germany. And so, you know, it constrains the actual implementation of that decision. Right. So there are a lot of things.
00:11:50.994 - 00:12:16.780, Speaker B: Number of jobs out there and a number of people. True. Yeah. I mean, some of it, I mean, some of it you could fold into the prediction task. Right. You could do this all stratified or what have you. But at least when we looked at all the documentation, how these decisions are implemented, it seems like there's a lot of sort of adjustment to the real world happening in the very end.
00:12:16.780 - 00:13:31.064, Speaker B: I don't know if you want to add something to that, but that's basically what we noticed, right? That it doesn't stop there because there's a lot of where the world is moving as we do this and the situation changes and therefore the implementation will be quite different or can be. Okay, so this is the principle, is one element that we use when we plan to use these algorithms. The bigger one and more sort of in my wheelhouse is the data we are using to do this. And in this particular labor market context, the data usually comes from the Social Security Administration contributions, right? So people get employed. And so we have excellent data also available for research for everybody since 1972 who is not self employed or a civil servant. So everyone else who is in the normal Social Security system, we have records from 72 onwards on a daily basis, where they work and how much they earn. Right.
00:13:31.064 - 00:14:18.946, Speaker B: Those two values are phenomenally good because that's what people later on use to get their pension, and they're making sure that the records are okay. Everything else in that database, not so much. I'll come to that in a moment. But the problem is that obviously, well, move to this thing. The problem is that we have this, and since 72, a lot has happened in Germany, right? And certainly when I moved in 2000 from Germany to the US, it was a very white country and very few immigrants. And that has changed quite a bit, you know, like the refugee waves being won, but also just regular labor movement. And so for certain groups, we have a lot fewer records and shorter histories.
00:14:18.946 - 00:14:59.274, Speaker B: Right. And the labor market has changed quite a bit. There was a big dynamic in how the labor market composition is what kind of jobs. So the amount of data you have to learn from is quite different by the various groups, but also by the kind of tasks you're trying to predict. And that is a problem, is no news to this community here. Many of you have seen this graph before. This is from the various facial recognition systems and the notion of the hypothesis that these systems are not so great in predicting certain shades of skin tone for females because there are fewer training data available for those, or have been.
00:14:59.274 - 00:15:33.940, Speaker B: It was interesting to see that for me because we have, in the survey context, done a study where we recorded the vocal interaction between an interviewer and a respondent. And when an interviewer, I mean, these days no one picks up the phone anymore. But back in the days when you did and actually had surveys on the phone, the interviewers often would not dare to ask you for your gender because it feels a little awkward when you call someone. And there's sort of the notion that you should recognize the gender by talking to them. Right. By their voice. Right.
00:15:33.940 - 00:16:09.164, Speaker B: And when you hear the recordings of these interviews, you hear some interviewers who or institutions who force people to ask about gender to say, oh, I'm sorry, I have to ask that. But, you know, Omar, are you male or female? Right? And. But what we did is we had for, don't remember, roughly 5000 or 10,000 people. We had the estimate of the interviewer. And then later on the self recording, right, I made a self report of the gender and we saw the exact same effect. You know, african american females were misclassified by the interviewers at a much higher rate. Right.
00:16:09.164 - 00:17:06.708, Speaker B: And the linguists told us, well, it's, you know, different vocal cords, you know, like different use of language, different, you know, so. But by and large, fewer training data for the interviewer. And so they're just not used to hearing the sound and classifying the sound makes total sense. Right. It's not, I mean, looks like racism, but you could argue it's also just lack of training. So that was interesting for me to see that the visual and the audio kind of had the same effect and should have brought you those data. But anyway, the point I want to make is that while we have increasingly many streams of data that we are using for these prediction tasks, some of them are really flawed, you know, and I mean, I had to smile when the german government during COVID said, oh, let's use fitbits and measure heart rates and what have you.
00:17:06.708 - 00:17:43.528, Speaker B: And I'm like, let's not do that. Who is it that has a fitbit? Right. It's probably those that suffer the most from COVID Anyway. So we have a lot of holes in the data. And I think, you know, as statisticians, we, or can I make that claim? Avi, we mostly think about, we're more likely to think about holes in the data as they come from entire units missing or entire answers missing. Right. But the lack of quality in the data has also different aspect, which is we often have very poor measurement.
00:17:43.528 - 00:18:36.298, Speaker B: And even if people report us, whatever group we assign them to, that reporting might also be flawed. And so both on the measurement side and the unit side, and here's one example for the acronyms. This is the national Longitudinal Survey of Use. This is a big survey in the US that has been done by the Bureau of Labor Statistics for many, many, many years. It's actually a new cohort in the making right now. And this is the current population survey, which is the largest data collection that's used to sort of get unemployment figures and whatnot. The reason I brought this rather old picture from hurricane and others, what you see here is the number of people in certain age groups age zero to 35 in this case, that were identified as being present in the household when the interviewer called and tried to make an interview.
00:18:36.298 - 00:18:59.836, Speaker B: Right. With the completely random sample, the exact same design. And this line here, the NLSY you see here, it has sort of a weird dip compared to the red to the other one. Right. This survey has a screen that says, I'm looking for people age 15 to 21 in the introduction. Yeah.
00:18:59.900 - 00:19:07.264, Speaker A: Isn't it the requirement for an LSY also that they remain participants for multiple waves?
00:19:07.664 - 00:19:24.856, Speaker B: This first camera equipment, this is just for initial recruitment. It's not drop out later on. Yeah. So that's another problem. But what I want to point out is here we have seen this in other surveys. So the health and retirement surveys, they call up and say, we are looking for people between, you know, 55 and something rather. And then you see the dip at age 55.
00:19:24.856 - 00:19:45.410, Speaker B: Right. I mean, this goes just to the misreporting the respondent. One easy way to get out of a survey is to say, oh, no one here in the household fits that. Right. Rather than just say, it's like, I don't want to do the interview. Or if you have more sophisticated ways, you can hide it a bit and then do entire household rosters and then you have dropouts and whatnot.
00:19:45.442 - 00:20:04.878, Speaker A: But to believe with the point. But I really think it's just the young people that are dropping out of the multi wave survey or that are not participating. And couldn't that just be explained because they're sort of the requirement that they're present for another year or two when they get called up and they're saying this is a survey over so many years.
00:20:04.966 - 00:20:05.518, Speaker B: Absolutely.
00:20:05.566 - 00:20:07.094, Speaker A: You know your kid is about to move out.
00:20:07.174 - 00:20:07.654, Speaker B: Yeah. Yeah.
00:20:07.694 - 00:20:09.918, Speaker A: This is age 19 or it's 17.
00:20:10.046 - 00:20:10.390, Speaker B: Yeah.
00:20:10.422 - 00:20:13.662, Speaker A: They'll say no because my kid is not going to be around in the year.
00:20:13.758 - 00:20:39.366, Speaker B: 100% agree that is an effect. We see. This screenshot of data is at the screening stage before they even know that this is a multivariate thing or before they even. This is not the number of respondents. This is just whom they found at screening. They recorded that because we're trying to figure out why is it that we don't find these people? And so they looked at this earlier phase of the recruitment. But you're absolutely right.
00:20:39.366 - 00:21:18.966, Speaker B: I mean, later on they drop out and whatnot. So what you're describing is the kind of unit non response problem that we see because they drop out at a higher rate. This one is my clearly failed attempt to show the measurement error problem, that even if we find them initially, they might not tell us who they are and then not be surveyed or they report it later on. I mean, in a different survey, you might just misreport belonging to a certain group because you don't want to reveal, you know, like, have you ever had an abortion? Nope. You know, and then you're not in the abortion group. Right. I mean, that's as easy as that, right? The more sensitive it gets, the more likely it might be that I just misreport.
00:21:18.966 - 00:21:56.940, Speaker B: Pretty easy. And for that reason, you know, the, the survey field of social scientists so excited about all these other data streams we have. But it's not necessarily getting better with other data streams because then we collectively make up the feature out of that stream. And who knows how good that is? Or actually, we know that it's often not good. One problem that we have, whether it's the misreport or the people missing entirely, is that how do we get information about the rest? A common vehicle is to use proxy reports. I believe you mentioned that also. And it can sometimes be successful.
00:21:56.940 - 00:22:51.548, Speaker B: I wanted to just briefly show you what we did in this big Covid survey that we did worldwide. There we recruited with the bias sample via the Facebook platform, people into the survey, asking them about how they do with COVID And then one question was, do you personally know anyone in your local community? Sort of use a social sensing mechanism rather than asking only about people themselves and what we saw later on. And apologize. This is way too small for this screen here, but basically what you see here is from May to December in black, the actual Covid waves in all these countries. This is starting with Spain and ending with Tanzania here. And then each of these variables that we had in the prediction model. And we saw that this addition of asking about the community really improved the prediction because of the bias sampleness.
00:22:51.548 - 00:23:29.604, Speaker B: So a combination of measurement and data collection can help you mitigate these problems, but you have to think about the problems prior in order to get there. So now, turning back to the human right, we still have a setting where someone might not have enough data. I said that at the very beginning. The problem is we already know a lot about human judgment, and we know that they use heuristics to make judgments. Right. And, I mean, I'm sure that many of these biases are familiar to you. They have been discussed in the social psychology, in other literatures for so long that they've become common knowledge.
00:23:29.604 - 00:24:59.988, Speaker B: But I think it's still fascinating to me how often we forget that in pretty much every situation, this can arise. Right. Christopher and I recently did some experiments in the labeling context, where we had people rate hatefulness, or whether tweets are hateful or show offensive language, and you clearly see that the autumn has a big effect. If you see less hateful tweets at the beginning, then you rate the subsequent ones differently than when you see more hateful tweets at the beginning. This is a very simple experiment, but the point is that these humans in the job centers, they also saw someone else right before on that very chair and made a decision about that person. And so these subsequent policies we often don't talk about, or at least I see the folks that are trying to implement the algorithms don't talk about when they try to evaluate the performance of the algorithm compared to the human in the survey context. And thanks to Emily, we discussed this yesterday, and I thought, let me throw in this graph after all, because I was so excited about your talk, Emily, when you showed us the first attempt at building a framework in making sure at least we know where to look for the errors.
00:24:59.988 - 00:26:07.140, Speaker B: And as I shared with you in the survey context, we've done this for a long time along the pipeline of survey data production to look at the error sources. And in our case here in general, I think it might have been a bit of a disservice to always just call it error and not separate it out into variance and bias at each of these steps, because they have met at each of these steps. And there are different mitigation mechanisms possible depending on what it is. I do think it could be useful to have for the algorithmic decision making context something along those lines. In particular, think about the data as the input stream and how the data collection really can affect this, because we are talking of the multi group context here and we rely so heavily on the demographic data. I thought it could be useful to point out simple things that it that are easy to a not know or b overlook when demographic data are collected. Here's an example from the 2010 census compared to the 20th census.
00:26:07.140 - 00:27:09.238, Speaker B: And those people who work a lot with census data know that the census tries to improve the way they ask their race and ethnicity questions from decade to decade to sort of keep up with the wording, right? But what is maybe a little less known is that also the way the data entry happens changed over time. And so, for example, when people wrote in mexican, black and colombian in their response in 2010, this would have been coded as mexican and colombian because they just didn't take more characters. In 2020, you would have seen all three. Who would have thought that it's sort of simple data collection elements that if you're knee deep into theoretical computer science, how would you know, right? I mean, do you have enough papers to leave? You don't need to read these methods reports. But that's later. The multiweek groups that we are comparing to, right. And I mean, this is just a simple example and it gets, of course, even more complicated.
00:27:09.238 - 00:28:02.024, Speaker B: Right now there's a big discussion in the context of the DEI diversion, equity and inclusion initiatives. Remember last year at APO, the current census director, Rob Santos was at a panel discussion. They were really honing in on the fact. It's like, well, what are we doing there? When we measure income or other categories and say that's how we measure wealth, right? I mean, we go by dollar amounts that you earn, but maybe in certain communities that's not the currency that creates value. Maybe it is the number of social connections you have, maybe it's, you know, whatever you're standing in the community. So who are we right to classify and say, these are the groups that matter? And that is a discussion that we had throughout the week. I think it could be helpful to sort of rethink what measurement are we using with the data that happened on that disk that you have there.
00:28:02.024 - 00:29:10.170, Speaker B: Okay, so that's the first set of things on the input stream that I wanted to point out on the output stream. Clearly, even if we were to be able to quantify quality of the fairness of the human versus the machine and all of that with the input stream, with what happens in the decision making process, then there's still sort of a question on how do people perceive it later on because they're not going to look under the hood and they don't care what you tell them. And so we thought we asked the people why not? And we did this using a vignette study where we had scenarios like this one here. Local employment agency has developed a computer program for assigning support measures to job seekers. This program uses data about persons past periods of employment and unemployment, as well as information available on the Internet, staff members, blah, blah, blah, compare the information, makes a decision whether you receive support or not. And what we did in this vignette study that we changed the context. So this unemployment context is one.
00:29:10.170 - 00:30:03.524, Speaker B: We did this for the banking context, we did this for prison context. I mean, yeah, I guess sentencing in for job seeking. This is job seeking for the unemployment benefits setting. And then we measure varied what action is taking, whether the action is assistive, was supportive, or whether it's punitive. We vary what type of data is used, whether it's just analog data that belong to the organization or it's, you know, external data sources that you can find and link to, and whether the decider is a person or an algorithm or the combination of the two. And, you know, I mean, someone said that this week. It's like amazing how, well, I think, avi, you said that how cheap big tech companies can get bad data.
00:30:03.524 - 00:30:42.622, Speaker B: This is not cheap data, and it's actually pretty good data. This is a huge, what we call a probability based sample in Germany. It is a multistage recruitment where first regions in Germany are randomly selected, and then within each region, either random samples from the municipal registry or from a random walk. Not the computer science type, but actually people walking with the. On the. Through the streets and writing down every fifth household and then recruiting people into the survey. The recruitment happens face to face with the 50 minutes interview of four, anyone in the household age 16 to 75.
00:30:42.622 - 00:31:30.818, Speaker B: And then, of course, we have, you know, not everyone stays on these panels forever, so they are refresher ways to get the numbers back up. But at least the recruitment process is pretty good for what you can get in terms of surveys. Right? So that just, this was not an mturk. We pay $0.25 for like 100 clicks survey. And so what we did is with this group of people who, they were exposed to these vignettes with the different context, the different actions, the different data types and the different actors and looked, you know, how do they rate fairness? How do they rate acceptability of this process? Right. And what you see in this graph is this should be readable.
00:31:30.818 - 00:31:53.994, Speaker B: Right? Yeah. That sort of up here, this is the pure. Only the algorithm decides. The middle part is both human and algorithm decides, and then solely the human. And what was interesting for us is this first column here. That's the banking context where we know, well, many people in Germany also know that this has been done. Right.
00:31:53.994 - 00:32:20.120, Speaker B: I mean, we are quite used to loan processes, or we call it shufa, this credit scoring element, that something of that type is at place. Yeah. Internet versus no Internet. Yeah. So this is the data type, whether it's the offline data that the agency has. So the bank has about you or the employment agency versus other data that they might find from you on the Internet. Right.
00:32:20.120 - 00:32:43.554, Speaker B: Admittedly, maybe not the best wording, the way we put this in the vignettes, there's always the tension. How much can you explain briefly before they stop reading what the prompt is? And so that's what we settled on. But that was the difference between these two data types, just for one line. Explain it. Yeah. Algorithm assisted knowledge. Yeah, yeah.
00:32:43.554 - 00:33:40.054, Speaker B: So let's take the banking context, right? So this would be should you get a loan or not? Right. And we tell in the vignette that a human with the assistance of an algorithm, right. Is deciding whether you get the loan. Help me. What was the punitive phrasing? What did we tell them? They get fined for something. Right, I remember. Anyway, so whether it's an assistive action or a punitive action, and then whether they, in that process Internet data or external data sources from the Internet are used versus the data that they have from past transactions at the bank or the actual data in the employment context, it would be increased welfare, health versus, you know.
00:33:40.054 - 00:34:13.769, Speaker B: Yeah. Okay, fairness rating. Who's giving the rating? So the rating, these 4000 people that participated in the survey when we did it in 2021, they gave a rating on a five point scale where they said that it's not fair at all, you know, somewhat fair. So it's a five point rating. And what you see here is sort of the average fairness rating of the people who were exposed to the vignette. Not everybody. We randomized the vignettes to the people.
00:34:13.769 - 00:35:07.354, Speaker B: It would otherwise be too hard of a task. But we had about 1500 ratings for each of the scenarios, I believe. And this is the average rating. And the gray boxes here, this is the prism context because we, we thought it was too absurd. The scenario that would have created the additional punitive statement, I mean, that decision making, I think it was a bail and we didn't want to go in the other direction was for the german context, just not feasible. We tried it out in the pre test and people were like, what are you asking about? Do they know what's concrete myself the line is using? Or they just know that this is a human and the machine together. That's all they know.
00:35:07.354 - 00:35:36.536, Speaker B: It's just a perception. It's just exactly. It's just a perception. And here too, I mean it's interesting when you're trying to measure perception, right? How much information do you give them? Sure, we could have set them down, walk them through the algorithm, right? We voted for, we voted for a setting that's closer to what's happening currently, right. People don't know, right. These things are in place. You read about it in the news, but you don't know how the algorithm works.
00:35:36.536 - 00:36:20.854, Speaker B: But if you have some expert to explain. So this is very different. So these are the people who would be affected by the decision. It's not the ones that make the decision. This would be anyone you pick from the street who is unemployed, walks in the unemployment office, and now is faced with the situation. Is this just the human judging me or is the human working with the computer and then gives me a suggestion? I'm just saying, like, those are people who will be affected, but supposedly that they were trying to be used. Oh, totally.
00:36:20.854 - 00:36:36.690, Speaker B: I mean, it could be entirely different. Entirely different. But that's not the world we live in, right. I mean, right now we don't have. Well, words will record them now that you have a YouTube stuff coming since Wednesday. But yes, of course we could explain. Hopefully we'll get there.
00:36:36.690 - 00:36:59.474, Speaker B: And maybe at some point, now that, you know, the last language models are in the news every day, maybe this is already different. We're actually repeating the study right now as we speak so that we have, you know, in two year intervals and measurement of this perception. Right. And, yeah, this is. This is not a trait. This is a perception moment in time in 2021. Absolutely.
00:36:59.474 - 00:37:49.468, Speaker B: The reason I'm showing this and not honing in so much on these actual numbers is that I think what's interesting here is that the perception is so different depending on the context. Right. This is not in the EU regulation context. We are talking about this almost context free and are about to make the same mistake. Let me say that as we did with GDPR, when we judge data to be sensitive or not sensitive or, you know, like, context independent, I mean, Helen is not here, but I'm channeling my in on this and Baum. And this is, you know, like, we got to look at the context and in some settings, it, you know, it seems reasonable to do and people are okay with it already. Now you have a question.
00:37:49.516 - 00:37:57.980, Speaker A: I maybe lost the thread, but are the respondents prompted on, like, what fair means? Or is it just sort of, you just say, like, is this fair or not fair?
00:37:58.092 - 00:37:59.076, Speaker B: The latter. Yeah.
00:37:59.100 - 00:37:59.356, Speaker A: Okay.
00:37:59.380 - 00:38:00.984, Speaker B: Just the latter.
00:38:02.164 - 00:38:02.828, Speaker A: Yeah.
00:38:02.956 - 00:38:19.080, Speaker B: You can remake that. This is sort of the kind of conversation you would have at a patti when you briefly talk about this. Right? I mean, this is. This is not an exam. This is not. We're not testing knowledge. We are not just reaction to this.
00:38:19.080 - 00:38:20.044, Speaker B: You know.
00:38:26.064 - 00:38:37.114, Speaker A: People are very worried about algorithms making all decisions alone, but they prefer algorithms and humans together over humans in certain contexts.
00:38:37.194 - 00:39:09.310, Speaker B: So in, or in certain contexts more than in others. So I think the way you phrase it was our conclusion, too, from the study. And the sneak peek at the new data still seems to confirm that, but it still varies by context. Right. So, I mean, it varies here, but it also varies here. Right. And, you know, we did all the statistician do modeling to see relative effects and holding setting into control and whatnot.
00:39:09.310 - 00:40:09.954, Speaker B: And what we do see, indeed, that the combination of human and machine is generally seen better across all the contexts, that they really don't like this in the job in person setting. But there are some interesting interactions going on and the other. So baseline here would be the banking context and everything else is perceived more negative. Those are the coefficients from that multilevel ordinal regression model that we had here. Because we are in this multi group context here, I want to also show that it does differ a little bit by demographic groups, how this is perceived. We had strong hypotheses on certain demographic groups, including foreign nationality, that more vulnerable groups would be a little more alert and sensitive to that. We didn't find these strong effects in the way that we thought they would be there.
00:40:09.954 - 00:41:07.746, Speaker B: But depending on the context, we saw a few effects on age and gender groups in this perception. The context specific piece. And I couldn't help but to actually bring some research with Helen, because I really feel strongly about this EU regulation discussion and how in the GDPR context, we overlooked the context and we overlooked how people's perception might change, to your point, over time. And as they learn, we might see this differently. And we did try a similar study in not perception on fairness for an ADM system, but willingness to share data. We had in 2019, asked in a similar quality survey whether people are willing to share data about themselves to combat a pandemic. Right.
00:41:07.746 - 00:41:22.896, Speaker B: So 2019, and then. Nice, you know, as like if economists would want to have it, then something happens and then you can look afterwards. Just a second. And then we did the study again. Right. And we saw. So this is 2019 and this is 2020.
00:41:22.896 - 00:41:53.252, Speaker B: Right. And we saw clear shifts from not acceptable to share the data to more acceptable, both in the longitudinal setting and the two cross sectional sets of data that we used. Right. And to me, this just speaks strongly. It doesn't also, not just philosophically, but also empirically, just does not make sense to discuss this without the context. Yes. I was curious if you could elaborate a little bit on what you just said on how you think that this should impact legislation.
00:41:53.252 - 00:42:27.586, Speaker B: Because it's not clear to me that legislation should simply align with perceptions that if we have very positive perception of something happening, then that means that it should be allowed. Agree. That's. And thank you for allowing me to make that clear. I didn't mean that we should even more than we already do make policies based on opinion polls. This is, if you take that away, that would be the wrong takeaway. That's not what I'm trying to say, but what I'm trying to say and what we've seen in the GDPR context is that we have very strong data protection regulations in place right now, but they ignore the context.
00:42:27.586 - 00:43:19.426, Speaker B: And that for certain purposes it really would be a lot better for everybody if data could flow more naturally from one data holder to the next. Take a medical context, one doctor to the next. You sign a lot of paperwork for that information to be able to flow right now in Germany, right. In a way that it really has harmed us during COVID and in general it's very secular. Similarly the algorithm decision making, it has a bit of a context discussion, but more the context. Is this a decision about a human versus not a human and not in which context, right. Is this a decision where, you know, in an hour from now I need to know what I, what health decision I make because you're about to die or is this a situation in which, I don't know, I'm bad with coming up with examples on the fly, but you know what I mean.
00:43:19.426 - 00:44:22.472, Speaker B: And this sort of context, and Helen Nissenbaum's argument in the privacy or data sharing context as well is that you have to know not individual perception but sort of general underlying norms because if you do violate the norms that will cause disruption in society. And I think the same is true here too in the ADM, that's the point. Does that make sense? So another element aside from the norms is how good are these predictions? Someone, I think, Emily, you were asked, right, what's the r square of these models? And I believe you couldn't pull that up out of your head in that moment. If you were a social scientist you would have said 0.2 because every paper you read in your undergraduate sociology classes, sort of .2 prediction, r square. And turns out that's still true.
00:44:22.472 - 00:45:21.034, Speaker B: And in mass coloration study that Matt Selgonick organized using data from the fragile family challenge or the fragile family study, they use data on roughly 4000 people, 12,000 variables, so lots of information over time from birth to age nine and then had hundreds of teams challenged to predict an outcome at age 15. Okay, using whatever technique you might want to use, crystal ball regression, deep learning, whatever, right? Take your pick. And this is what they found, you know, the best. I mean like no matter what was used there, you know, it's 0.2 or less the r squared, right. The predictive power of these models is just very low. Right.
00:45:21.034 - 00:45:56.192, Speaker B: And I think that's something to chew on a little bit. Sorry. In this case, predicting outcomes at age 15 about the children point to all this, right? I mean, there were all kinds of like. So these are the different outcomes. This is predicting GPA. This is predicting a material hardship. This was predicting education.
00:45:56.192 - 00:46:16.356, Speaker B: This was predicting job training layers. But the models that went in ask where for, but you're predicting what happens to them at age 15? Yes. Correct. Does somebody have a job at age 15? Job training? What kind of job training they are going into?
00:46:16.480 - 00:46:17.864, Speaker A: Oh, and this is in the German.
00:46:18.364 - 00:46:19.108, Speaker B: This is us.
00:46:19.196 - 00:46:19.732, Speaker A: This is us.
00:46:19.788 - 00:46:25.584, Speaker B: This is us. This is us. This is us. It wasn't mean for a 15 year old to get laid off.
00:46:26.564 - 00:46:32.020, Speaker A: That happened. His parents. Parents. Oh, it's a family service.
00:46:32.172 - 00:46:33.344, Speaker B: Family Soviet.
00:46:33.724 - 00:46:45.756, Speaker A: So this is not meant as a statement about the models. This is meant about, like, the data and the underlying labels that, like, the nature is, like, unpredictable here. Like, there's a lot of uncertainty.
00:46:45.860 - 00:47:17.674, Speaker B: There's a lot of uncertainty. And so, I mean, the models were good, you know, they did, you know, I mean, they did the whole gambit, you know, how you would these do this modeling. This is certainly state of the art modeling that went in there. Some of them might have been very poor. You know, I guess the takeaway here is that even if we try very hard, you know, and we solicit different talents and we have thousands of variables and many of them are measured good. I mean, this is a good survey. This is compared to your average Internet data, very good survey.
00:47:17.674 - 00:48:06.396, Speaker B: Right? But of course this, you know, again, world happens, noise happens, you know, and it's just theories aren't that good. I mean, it could be any of these, you know, it's. They try to dissect what could be the reason for these poor predictions. But it's not that one thing jumped out like, this is the key problem. But I mean, the world is complex, right? And this is just a hard prediction problem. And so coming back to my job seeker question, like, which job I should go into, that thing is a hard problem, you know, and if I look at these various, I mean, those aren't data that we are using that have 12,000 variables to pick from. You know, they have beginning, end date of my last job and when I'm german national and how old I am and what money I made and the company size, and that's about it.
00:48:06.396 - 00:48:37.604, Speaker B: And okay, for a long time series, but it's just very cool. And so we can algorithm away here and it could still be a very poor prediction. And in which case, hesitation on side of the population that this should be actually used. Understandable, right? Yet the question is the human, is the human any better? Probably not, right? I mean that's. And so for me, it's always the comparison, okay, what are we doing then? Maybe this should all be lottery. I don't know. Right? I mean, yeah, sorry.
00:48:37.604 - 00:49:58.534, Speaker B: I think that's a very reasonable question to ask. I think something that I think of a lot of the time, when I think about a similar question is how expensive is each decision making process, right? Like if we're trying to reach some greater goal outside of that era model to make the decision that humans could also make, or is there like to achieve a fairness goal by allocating? Yeah, I love that you bring the cost element in. I think that's very interesting in the survey context, you know, good high quality data to collect, very expensive. Right? And everyone is like, oh, let's use all the big data sources. But you're just shifting the money from the expensive interview, the cheap interviews to the expensive data scientists that then try to. Like right now I feel like in the prediction context, we're in a similar stage, right? I mean, yeah, you can hire the McKinsey's and, you know, get them to help the employment agency to create these models or, you know, improve the process altogether. There are a lot of different ways, but it's difficult to decide sort of the cost.
00:49:58.534 - 00:50:51.088, Speaker B: I mean, so the actual cost of making that decision, but then also, of course, the cost of the decision, what happens afterwards. Right? If it is, that's why we did this differentiation between the punitive and the assistive decision. Because I feel like as long as it's helping, and so you're either the same as before or helping, then maybe we don't need to think about it. Although your story showed me that maybe we do need to think about the helping with the student salaries. Even assistive actions could be seen as unfair if only some other people get that. Sorry, my question is a little bit related to, but is it a good thing to see that we cannot predict from the past or bad thing because Modis presented something that able to predict perfectly, but that was a sign that we couldn't intervene really well. And here we cannot predict really well.
00:50:51.088 - 00:51:18.922, Speaker B: Maybe that means we can intervene real well, and maybe that means that prediction has a really good value here. So I'm asking, is it a good observation to see we cannot predict something? So for the social that's a good discussion to have. So I love it. Right. And feel free to jump in more, so if you have a thought on that. But I. My initial reaction to this, like, for the social scientists, we never were bothered by the fact that we can only predict 20.
00:51:18.922 - 00:51:42.640, Speaker B: I mean, that the r square is not higher than 20% because it was, all the interest was in sort of relative effects. Right. Your parents education, what does that do to your own outcome? And, you know, is it moving you up a little bit? Yeah. Okay. Effect is small, but, you know, it was all about the relative effects of these things. Right. And so, so, yeah, I mean, it depends on what the goal is of what you're trying to do.
00:51:42.640 - 00:51:52.444, Speaker B: If you're evaluating the effectiveness of a certain intervention. Right. Or other covariates, you know, relative to each other, then that might be less important.
00:51:53.144 - 00:52:47.434, Speaker A: One remark about this data set. So to the extent that there was any signal, it was in four features, and the features were all about the mother. And so it was basically also an example of something where whatever signal there is was already there before the did anything. And so I think here, the thing that made this so kind of hard to. Made it look so bad in terms of predictive accuracy is this time gap between six and 15. What this is sort of like ignoring. And that's maybe like a little bit of an issue I have with the takeaway from this paper, is that there could still be a lot of things that are socially sort of constituted and determined in sort of in the data or like, in these variables that we're looking at.
00:52:47.434 - 00:53:08.954, Speaker A: Right. And so I think this shouldn't be some kind of feed, this takeaway, right. The takeaway could also be like, we know exactly what constitutes hardship, and, you know, it's these environments. Right. It's just sort of like, you don't see that when you're trying to predict GPA at age 15 from GPA at age six.
00:53:09.254 - 00:53:30.204, Speaker B: Absolutely. That's why, for me, the focus and the core focus of the tape is, too. And that's why I started out with the data. Right. I think it. I mean, we use for these algorithms and prediction on the data we have, right. And not necessarily thinking about that very problem that you just described.
00:53:30.204 - 00:54:04.746, Speaker B: And, of course, in a setting where that's why it works well on the Internet platforms, it's like, okay, I see this, this second, and I click the next second. And those are strong, strong predictive links. Right? And a lot of the social science data, the better ones, are so expensive to collect that between data collection and release there's often a year or two. Right. You aren't really having a big gap. I mean, it's not as big as h nine to then, and it was such big gap because they wanted to have data in there that weren't released and no one has seen before and whatnot. And it would be very interesting to see indeed for closer.
00:54:04.746 - 00:54:32.076, Speaker B: But that's why I led with the, you know, my somewhat flippant comment on all sociology papers feel like they have a 0.2 r squared even in other settings. We see that. Right. It is. It's just, you know, and yeah, most data collections have this big gap between collection and action that you're trying to do in the employment. Even the administrative data often don't, are not made available, you know, that quickly.
00:54:32.076 - 00:55:02.968, Speaker B: I mean, digitalization, if you're not in Estonia, everywhere else, sort of a little bit slow, you know, to get to the data and. But I mean, it's a word of caution that you really have to make sure that, you know, those data are useful and relevant and some of the historic ones might just not be. Yeah. So thanks for pointing that out. I agree with you. I'm definitely in the camp of, this is worth my work doing, not in the camp of we shouldn't do any prediction mode. Right.
00:55:02.968 - 00:56:07.344, Speaker B: It's really just we should think about that whole pipeline. And so I'm closing out just with one, two comments on that infrastructure. In order for, for the data to be used more faster, more easily, and with all the variables available, it is important to get environments in which we can do that and not wait forever until they are anonymized and available to the public. And what have you again here, too, in full support of the open data movement, but it slows things down for certain settings and it often doesn't allow to look at measurement inconsistencies and errors in the data and what have you. Creating environments in which you can do this safely, but on the actual data right when they arise would be a necessary infrastructure piece. And there have been initiatives that do that. We started this coleridge initiative that has basically set up, well, a lot of states data pooled together and can be analyzed that way in a safe environment.
00:56:07.344 - 00:57:33.048, Speaker B: And what we're trying to do in that is to follow these new guidelines from SCN on the transparency. All the decision, like basically back to Emily's talk, all the decisions that have been made so that you can access the code in addition with the data and understand that. So my takeaway is not at all. Predictions are poor, don't do it or follow the polls, but we need to look at the data and we need to just think about that data space in conjunction with the algorithms if we want to move forward. So, thank you. So I have a question somebody here asked about. Do you think that if there was an explanation why some decision for sure, you know, I do think that a lot could be done differently how we ask that, right? And how, I mean, I started out with these guiding principles.
00:57:33.048 - 00:58:12.654, Speaker B: I mean, we don't even agree. What does fair mean, right? Is it everyone gets the same, everyone treats the same? Is this really nebulous? And in this case it was on purpose because that's how we commonly talk about it and that's how people discuss about it. And our focus was really just the difference that a context made, you know? But yes, in sort of, if you're trying to actually get at what people perceive as fair, that would be a different type of study and I would issue a different instrument. And I think there's a lot of interesting work to be done as we move towards explainability. You know, it's like, when, when does it, I mean, maybe it's fair when I can look inside now, can understand what happens, right? Or whatever.
00:58:13.794 - 00:58:57.024, Speaker A: I have a question. So you framed this very nicely in the beginning, and I like this a lot. In some sense, human versus algorithm is a bit of a straw man. And the actual question is we're always talking about institutions and we're really interested in like, what happens to institutions as we increase the degree of rationalization or the degree of bureaucratization and the degree of sort of predictive technologies that are used within the institution to make decisions. So I've actually always been wanting for sociology to give us the answer, how we should think about this and how we should evaluate what happens as we dial the knob within the institution. Do you have any thoughts on how to study that?
00:58:57.064 - 00:59:44.420, Speaker B: Rather than just like, yeah, it is a straw man that I got from Omar because he said, why don't you give a talk with that type of knob? It was a little bit like that, but I thought, actually it is good because the discussion is always, should we do this or do we do the human? I mean, we are discussing it in these terms, but I agree with you. What I didn't get to do is talk about what I think happens. And that's why I brought in the infrastructure a little bit at the end is that a lot has to do with trust, right? Trust in the institution. And the humans in the institution, they're just seen as part of the institution. And so there's a lot of, trust me, there's plenty of room on these job centers not being good enough. But by and large, there's trust on what they do. And then, of course, as you change that bring in technology that changes.
00:59:44.420 - 01:00:00.364, Speaker B: And then the question is, who's building the technology and the like? So I think that would be very interesting to look at. I haven't seen convincing studies yet, but it would be around trust that I would study this to give a short answer to your question.
