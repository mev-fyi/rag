00:00:00.520 - 00:00:52.214, Speaker A: Yeah, well, you see, it's probably with mini crash course by Ashwin, Niagara on information theory. And then there will be a break, and after that, a couple talks of applications of information theory to hamiltonian complexity and also black holes. So we're starting with Ashwin. Thanks so much. So Umesh charged me with a very daunting task of, you know, trying to compress a vast and interesting research area into, you know, down to one and a half hours. And I've been spending the better part of the last few days just thinking about, you know, how I could even do it if it was possible. In fact, you know, I first started out listing all the things I thought were interesting.
00:00:52.214 - 00:02:04.424, Speaker A: And then I think over the past 12 hours, I've just been hacking away at it so that I could fit it into something comprehensible, so that it would be more of a course and less of a crash. So what I intend doing, let's see, is it coming on, is presenting a view of quantum information theory from the point of view of a user, rather than a researcher that's steeped in it. And it's also intended for the user, as in someone, as Umesh, you know, said, might not necessarily want to dive deep into it, but would still like to understand the basic principles. So what I. Okay, maybe I just have to use this works. Okay, that's better. So I'll start with some illustrative applications.
00:02:04.424 - 00:03:45.844, Speaker A: I just picked three applications which were loosely related to the theme of the special semester here. So I'll go over these applications, point out where in these applications is a crucial use of information theory, and then I'll go back and try to fill in all the concepts that you need to understand, or we need to understand to see how these concepts actually are put to work. So I'll go over the mathematics of quantum information, and this would probably be really the crash course part of this talk. And I'm hoping that most of you are familiar enough with these basics so that this, my maybe 1015 minutes summary of it, would be sufficient to jog your memory. And then I'll go on to talking about entropic quantities. Now, I should really say that, unlike what you might imagine, quantum information theory is not a theory that has been fully developed and sits out there for us to learn from, but in fact, it's being invented as I speak. So there are new concepts that are being introduced, and especially in the last ten odd years, that have come to play an important role in recent developments.
00:03:45.844 - 00:05:06.684, Speaker A: But I won't have time to go into all of that, I'll fall back on trusty entropic quantities, quantities that were first devised with perhaps specific information processing tasks in mind and later turned out to be useful in many more contexts. And finally, I'll try to summarize what we have learned and perhaps point to a few questions that still remain to be answered. Okay, so let's move on to application one. This is a well known application in cryptography. It's that of privacy amplification. Here we have two agents, Alice and Bob, if you wish, who would like to generate a random key that is known only to them and no one else, so that later on, they could use it in cryptographic protocols. So imagine that they've actually gone through an initial phase of some kind of a protocol whereby they managed to share n uniformly random bits x.
00:05:06.684 - 00:06:46.084, Speaker A: Now, the problem with this string, or with using it directly as a shared cryptographic key, is that potentially an eavesdropper has secured some information about it, and this information could be in the form of a quantum state. So imagine that Eve has stored some quantum state y, that is correlated with this classical string x. Can they still, is there any hope of they still being able to distill secure key out of x? Now, in the classical scenario, where the eavesdropper is entirely classical and y is basically some random variable that's correlated with x, this is a well solved problem. In fact, what they can do is the following scheme. Alice can pick a uniformly random seed z, which is independent of x, and then send this over to Bob, the second agent. And both of them have in mind an extractor, a randomness extractor in the theoretical computer science sense, either. And they both compute the output of the extractor on their rho key x and the seed z.
00:06:46.084 - 00:08:45.194, Speaker A: Now, classically, provided the seed is long enough and the extractor is suitable for min entropy of x, given x, given y, we can ensure, using strong extractors, that this key is actually nearly uniform, even given the information that Eve has. So Eve had originally y, that was sort of garnered from x, and then later on also saw the seed z that was sent out in the open. So even given both of these things, both of these quantities, this output of the extractor would be nearly uniform to e. Now, the problem with using the scheme in the quantum case is that this state y is, you know, is a quantum state that could be measured, could be observed in one of many ways. And the way that eve measures this may depend on the seed z on the seed zone, right? So that, um, the, the final random variable that eve has, you know, representing the information about x now is actually could be potentially highly correlated with this output of the extractor. So a question is, well, are there extractors e that are suitable even against such quantum adversaries? Now, this problem has been around for a long time. In fact, it's an integral component of quantum key distribution protocols.
00:08:45.194 - 00:09:41.724, Speaker A: And we've known that, for example, using two eyes independent hashing, we could, in fact, obtain secure keys. The downside? Yeah. Is there something you can say to give me a better sense of what an extractor is? Okay, maybe I'll just write down a statement here. So, the condition that we would like at the end of this protocol for it to be successful is something like the closeness of two distributions. The two distributions are one, which is the output of the extractor ex. And this is correlated well. This is jointly distributed with the random variables y and z.
00:09:41.724 - 00:10:21.174, Speaker A: So this is actually the joint state, in some sense, of the three parties. So Alice, Bob and Eve. This is held by Eve, that's with Alice and Bob. Now, what we'd like is that the output of the extractor be independent of what eve holds. And not only that, we'd like it to be nearly uniform. So this should be close to, let me call it, u. And if this is over k bits, then this is uniform over k.
00:10:21.174 - 00:11:07.804, Speaker A: And then we'd like this to be, you know, if you're thinking about classical random variables, we'd like it to be independent. So in product distribution with y and z. So ultimately, the security condition goes as this is less than equal to epsilon, this distance. And epsilon is a parameter that you could choose for your extractor. Is there an example that gives me a sense of, it seems sort of still foreign? Is there an example that give me a sense of how they work? Sure. So one example is, let's say you pick. So this is the two wise independent hashing example.
00:11:07.804 - 00:12:07.806, Speaker A: So you pick z uniformly at random from zero, one to the n, and the extractor simply outputs the scalar product x dot z, which is the sum I x I z I mod two. So that's a possible function. And in fact, this turns out to be highly secure. And you could, you know, you could. So this, this produces only one bit, but you could think of picking a sequence of, you know, such Z's and then producing successive random bits in the same manner. By taking in a product, producing more bits degrades the quality of the extractor. And not only that, the seed that you're picking is really long.
00:12:07.806 - 00:13:28.854, Speaker A: And what we'd really like is to balance the two and try to achieve the optimal parameters, meaning try to have the least amount of communication, use the least amount of resources. Thinking of randomness as an expensive resource. Uniform randomness. Yeah. So the goal is to achieve the best security parameters with least use of fresh key. Okay, so as I said, this scheme here, which has been known for, for a while, is, is not really optimal. A few years ago, Amnon Tashma showed that a very well known extractor, one that's been studied in classical computer science for, well, about 15 years now, is also secure against a quantum adversary, provided you plug in the right kind of ingredients into the scheme.
00:13:28.854 - 00:14:53.476, Speaker A: Now, I want to explain how this extractor really works. I mean, that would take me far afield. Instead, what I'd like to focus on is a piece of the analysis that actually focuses on where the action is as far as dealing with quantum adversaries is concerned. So actually, almost all known extractors, and especially this extractor, works on what's known as the reconstruction paradigm, which in, you know, broadly speaking, says something of the following sort. If the distance between, you know, the ideal distribution and what is actually produced is large. So, you know, you start with assuming for contradiction that e is not secure against a quantum adversary, then there is a reconstruction procedure for the initial rockey x. So what is this reconstruction procedure? What does it look like? Well, what it does is it produces a.
00:14:53.476 - 00:15:43.292, Speaker A: Well, it, you know, using, it's not able to do this directly from. I'm not quite sure why this. I see. So it's not able to do this directly from the information that Eve has, it needs a little bit of help. And this, you know, but this help is actually quite minimal. So there's a short string a, which in the case of the quantum extractor, or the analysis for the quantum case, doesn't produce the entire string x, but does something, well, almost as good. And I'll tell you why it's almost as good in a bit.
00:15:43.292 - 00:16:28.414, Speaker A: What it does is that given any index for the string x, so anything ranging from one to n and some small number of copies, q or y. So y is a quantum state. So you just take a q fold product of identical copies of Y. Now given this state, which is now a composite quantum state, the procedure outputs bit xi with high probability. So probability at least some p. Yeah, this reconstruction procedure. So there's an algorithm, not necessarily.
00:16:28.414 - 00:18:16.038, Speaker A: In fact, it's not uniform because of the string a that produces this bit xi. Now question is, why does this give us a contradiction? Well, it so, turns out that in spite of the exponential, or apparently exponential resources afforded by quantum states, that it is not possible to produce short codes that will allow you to do precisely this, in spite of this fact that, um, measuring one bit doesn't necessarily, uh, you know, it well, it necessarily disturbs the quantum state that you have, so that it doesn't necessarily imply that you can successively reconstruct all the bits of x. It turns out that the code has to be at least linear in length. And I've written down here a one line proof that I'll later on fill in after explaining what all the symbols here mean. The upshot is that if the number of quantum bits in the state y is small, then this here, the short advice plus q times b is small, but it couldn't have been, because it's at least linear in length. So it couldn't have been that. That we could distinguish between these two distributions with high advantage.
00:18:16.038 - 00:18:51.176, Speaker A: Yeah. Where do you get the argument that there exists a short string a such that you can reconstruct the whole string? It seems naively that you might be able to reconstruct one bit of x, but why does it get all of them? Okay, so that will take me into. Yeah, that's more classic. That's right. Where does the primary queue come from? Is it part of the security? No. So the reason we have. We need copies of why is because, you know, the classical argument.
00:18:51.176 - 00:19:33.824, Speaker A: I mean, the classical part of this argument gives us a procedure that needs access to the state Y q times. Now, as I said, you know, measuring a quantum state possibly disturbs it irreversibly so that you wouldn't be able to necessarily get the correct information each of those queue times unless you had an independent copy of it. So what should I think of QSB? Is it constant? No, actually, it's not a constant. It's probably polynomial. Thuma probably knows better. It could be up to polynomial. So the polynomial in n.
00:19:33.824 - 00:19:58.064, Speaker A: Yeah, I'm not sure. It's not constant. It's not constant. You'd like it to be as small as possible. Yeah, that's right. It depends on the output. Sorry.
00:19:58.064 - 00:20:08.888, Speaker A: It's probably like polylog n. Is it. I forget. No, no, it's. I think it's some small power. Okay. You don't want it to be large, but in order to ruin the whole thing.
00:20:08.888 - 00:20:50.388, Speaker A: Right. Yeah. Oh, do I need to plug it in? Okay, so let's look at the second application. So this is, you know, this is a topic that's been talked about quite a bit. And, you know, it's probably fresh in your minds from this morning. This is about the local hamiltonian problem. In fact, in one dimension here, we have n particles all arranged on a line.
00:20:50.388 - 00:22:24.124, Speaker A: Each of these particles is a d dimensional system, and they interact with nearest neighbors. They interact through an energy term, an energy penalty term, hi, between, you know, the ith and the I plus one th particles, which is a hermitian operator that is basically d squared by d squared, acts on the space of the two qu d and has norm at most one. A central problem in this area is to understand what are the properties of the ground state of this hamiltonian h h, which is the sum of these local terms. Now, the ground state is basically the eigenvector of h with the minimum eigenvalue h is an exponential sized matrix. So, you know, naively, if you were to try to calculate this ground state, you would take exponential time. In fact, even if you asked a slightly weaker question, well, maybe question that's much weaker than asking for the entire ground state, which is basically asking for an estimate of the ground state energy to within a one over polynomial additive term. It appears to be hard even for quantum computers.
00:22:24.124 - 00:23:52.854, Speaker A: So it's QMA hard to actually estimate this quantity. In fact, it's even hard if you allow these hamiltonian terms to be all the same, for the system to be translationally invariant, although it's not quite as hard as this. On the other hand, there's been a very successful tradition of being able to heuristically solve, and more recently, rigorously solve this problem when the Similtonian is gapped. So what that means is that the difference in the least eigenvalue and the next higher eigenvalue of the summation operator is at least a constant that's independent of the length of the system. Now, you've probably come to see that the reason that this happens is because of what's known as the area law. If you looked at a general quantum state on this n particle system, it could potentially be highly entangled. So if you look at, in fact, an interval of this sort, it could be that each particle inside is maximally entangled with a particle outside.
00:23:52.854 - 00:25:34.034, Speaker A: So that, you know, in this example, if you considered an interval of length l, you might have entropy l log d, log d being the entropy, maximal entropy of any one particle. What the area law says is, in fact, that this is not possible when the Hamiltonian is capped. When it's capped, really, what can happen is that the entanglement only is proportional to the size of the boundary which is basically constant in one d. And this result, although was conjectured and believed very widely in the physics community, was only rigorously proved about seven years ago by Hastings, and then vastly improved in the recent years. In fact, it's this area law that gives rise to the algorithm that Zeph talked about. So here's a key step in Hastings original proof that gives us some insight into why such an area law might hold. Actually, really, I wouldn't be touching what I consider to be the key part of this argument, which is perhaps far too, well, one far too sophisticated for me to wrap my head around, but also because, again, it doesn't fall into the theme of my talk.
00:25:34.034 - 00:26:22.454, Speaker A: But I'll present at least what you can infer from it once you're given this tool. So here's a high level outline of what the proof is. Again, the proof is by contradiction. You start with the assumption that there is some interval, in fact, there is some split partition of this line into two parts, let's say after particle I, such that when you look at the reduced state, let's say, on the left, it doesn't matter where you. Which part you pick. Reduced state of the. Well, the ground state on the left, then its entropy is high.
00:26:22.454 - 00:27:34.608, Speaker A: Now, what that actually implies, as we'll see a bit later, is that the entropy of the reduced states for any cut, so, you know, going from particle I all the way up to sum I plus m is also large. Now, the crucial step in Hastings proof is the following. If you look at an interval within this region, I to I plus m, let's say, of length l, let's split it into two halves, a and b, and look at the entropy of this joint system, ab. So the particles that are in the contiguous, two contiguous intervals, a and b. Let's call that the reduced state, rho above. And then if you focused only on the left half or the right half, you'd get rho a or rho b. Now, if you looked at the entropy of the entire interval, it's.
00:27:34.608 - 00:28:14.934, Speaker A: We'll, you know, we'll see in a bit what entropy means. But, you know, right now, just think of it as some measure of randomness present in the state. It could be as high as the sum of the entropies in its two parts. And morally, what this means is that the state is actually, you know, if indeed it achieves this maximum maximal entropy, then it's a product. It's an independent product of rho a and rho B. So the two halves actually are not entangled with each other or even correlated with each other. Other.
00:28:14.934 - 00:29:39.044, Speaker A: So, as I said, the crucial part of Hastings proof is a consequence of the Lee Robinson bound, which says that this cannot really happen, meaning rho ab is, in fact, quite far from the product state rho a, tensor rho b. In terms of quantum computation, what this means is a quantum information. This means that there's a measurement that distinguishes the joint state rho ab from this synthetic product of our minds, which is the product of the two reduced states with probability. That's exponential in the length of the interval about Lee Robinson bound. And sort of a dynamical picture, that correlation sort of spread at some speed. How does that sort of dynamical picture mesh with this implication that you have here, that it implies the entanglement? That's a good question. If I remember correctly, it basically appears in the form of the decay of correlations, you know, in terms of this length l, the length l of the interval.
00:29:39.044 - 00:30:42.996, Speaker A: Maybe that's as far as I'm willing to risk. I think the dynamic ones are used in the proof of the exponential degree of correlations that you're okay. But it's kind of the harder sentence. Right. All right, so what can we conclude, you know, from this measurement? What we'd like to say is that, in fact, the entropy of this interval is not quite as large as this. In fact, it's much smaller. So, if you look at this difference, which is the maximal entropy, and what actually is it's constant.
00:30:42.996 - 00:31:28.734, Speaker A: It's proportional to l, the length of the interval. And this gives us a recurrence on the maximal entropy of, uh, the state on any interval of. Of length two l, as you know, twice sl minus some linear term in L. And if you solve this recurrence, you get something like l log d minus, um, order l log l. Okay, so. So, really, what this is saying is the. You know, even if these two halves of the interval were.
00:31:28.734 - 00:32:55.024, Speaker A: Had high entropy, the entropy arose as a result of entanglement within, and not through entanglement with particles outside this interval. Okay? And you can see directly here that if this length l is large enough as compared to the dimension or the particles, you get a contradiction. You have a question, arun? Which step depends upon the gap? The. Yeah, it's the implication of the leave Robinson bond here. Okay, let's move to the third application. So, I'm just throwing a whole bunch of concepts at you and a whole bunch of sort of properties with the hope that once I start explaining what they are, you might begin to understand how they are used and what the principles behind them, behind their use are. And in fact, I'll go back to these applications and fill in all the details once we have those definitions.
00:32:55.024 - 00:33:48.864, Speaker A: Okay, so here's another, here's another application in quantum complexity theory. Now, so we're all familiar with NP proofs for three satisfiability given a Boolean formula over n variables. Question is, we know that there is an efficient proof, I mean, a small witness for satisfiability. It's of length n. Basically, the prover would specify what the satisfying assignment is, provided there is one. And there's an efficient way to verify this. Question is, is this the shortest that a witness might be? And it's believed very widely that indeed it is.
00:33:48.864 - 00:35:12.314, Speaker A: And the reason is. Well, is that we basically believe that there are no sub exponential algorithms for satisfiability, that this is the strong exponential time hypothesis. It says that there is no algorithm for three Sat that runs in time exponential n to the gamma for gamma less than one. Surprisingly, if you allowed quantum provers, well, not one, but two quantum provers, you can, you get a quadratic improvement. So you have proofs of length, roughly square root n. A caveat is that these two provers are required to be completely independent from each other, meaning they each send to this quantum verifier a state rho and sigma, respectively, which are in product with each other, in tensor product with each other. The verifier basically takes input the formula, and does a suitable computation on this tensor product, state rho, tensor sigma, and outputs whether the formula is satisfiable or not.
00:35:12.314 - 00:36:22.434, Speaker A: In the case that it's satisfiable, he outputs satisfiable with probability one. So the completeness is one. But when the formula is not satisfiable, he outputs not satisfiable with constant probability, strictly less than one. Again, you might wonder how short these quantum proofs could be. It turns out that for the same reason that the classical proof is optimal, the quantum one is also optimal. Well, not for quite the same reasons, but the same underlying belief that there are no sub exponential time algorithms for three SAt. And this follows from a result by Aram Harrow and Fernando Randall, who gave an algorithm to essentially optimize over, well, something akin to product states over two quantum systems.
00:36:22.434 - 00:37:45.394, Speaker A: Now, such an optimization problem is it's clearly quadratic, because you're trying to optimize over a tensor product of two positive semi definite matrices. And we don't know in general how to solve such beasts efficiently. So what they do instead is to optimize over a set that nearly, well, quite approximates the feasible region quite well. And the underlying observation here is that product states are what's called infinitely extendable, meaning, if you had a state rho tensor sigma over bipartite system, then you could imagine it as being a part of a much larger state, as large as you wish. In fact, with many copies of the system B, so identical systems b one, b two, b three, b four, all prepared in state sigma. So that if you looked at any pair abi, then the reduced state on this subsystem would be exactly the same as on ab. So it would be rho tensor sigma.
00:37:45.394 - 00:39:16.854, Speaker A: That's what it means to be infinitely extendable. Now, the main theorem here is this can't be possible for an arbitrary state rho ab unless it is close to being product in the following sense. In fact, you can make a quantitative statement of this kind when you don't have an infinite extendability, but only k, where you're constructing a state with k copies of b. So let's say tau ab is such a state which can be extended to tau ab, one, b two b three up to bk. So that when you look at the reduced state of Tau Abi, it's, it's exactly tau ab. So if you have such a state, then what we can guarantee is that it's close to the convex hull of these set of product states. And it's close in this sense, according to some norm, the state that's closest in the set is within polynomial and log dimension of a divided by divided by k here.
00:39:16.854 - 00:39:40.614, Speaker A: So provided k is large enough if you pick k to be polynomial in log dimension a. And, you know, in the case of this quantum proof, a goes as exponential in square root log n. So this is really something like square root, log n. So ultimately, k needs to be that order. Could you show the. Definitely extendable again. Okay, let's go back here.
00:39:40.614 - 00:40:25.954, Speaker A: So I showed you. Well, this picture is. Maybe I'll write it down. I think that's better. I only said it in words. Maybe that's so state tau ab is extendable. There is another state tau.
00:40:25.954 - 00:42:15.382, Speaker A: So let's say this is over space h tensor k. Then if there's a state over h with little k copies of this capital k, this is over this trace h tensor k to the tensor k such that the reduced density matrix, or reduced state, the partial trace. So what are h and k? Again, these are the underlying Hilbert spaces l of h tensor k means it's a set of linear operators so quantum states, it's a positive semidefinite operator with trace one and k extendability means there's another such positive semidefinite operator in a larger space. So that when you reduce it. So let me just say this is so if you trace out all b, except for bi from Tau a b, one of two b, sub k, and I'll define these operations in a moment. So this is exactly equal to tau ab. This partial trace operation takes a state in the larger space space and produces a state in the smaller one.
00:42:15.382 - 00:43:27.214, Speaker A: And it's a linear operation that we'll see in a moment. And it imposes the condition that the smallest state that you get is what you had before for each eye. Okay, so perhaps all I'd say about how this is derived, well, for now is that it's a consequence of what's known as, you know, the chain rule for mutual information. And the, what's more useful at this point is probably understanding, you know, why such a result might hold in the first place. If you look at this condition here, what this tells you. Well, let me write this. Let me give this a name, which is Tau Abi.
00:43:27.214 - 00:44:49.804, Speaker A: So you have these k states, Tau Abi, all of which are identical to ab. Now, if tau were entangled across a and b, or a and b one, then it couldn't possibly have the same entanglement with also b. A couldn't have the same entanglement with b two, b three, b four, and so on. Why? This is monogamy, right? This is the monogamy of entanglement. Can you explain that? Why? I'm trying to remember why that was the case. Okay, so I guess basically, you know, in terms of, let's see what a good example is. Okay, maybe I can explain it in, you know, for classical correlations, right? So, classically we know that.
00:44:49.804 - 00:46:34.530, Speaker A: So if you have two random variables, x one, x two, that are correlated, jointly correlated with the third random variable, y, then this mutual information, so the information that y contains about x one, x two, is essentially a sum of the information with x one and x two. So it couldn't happen that all the information, all the correlations that were shared with, you know, between x one, x two, and y came from x one and also additionally from x two, right? So the chain rule here says I of x one. I mean, this exactly is x two, sorry, x two. So this is sort of the monogamy of correlations in the classical world. And a similar property holds in the quantum world for entanglement. Okay, so what is this bias? Well, so what we've achieved is actually a reduction or an approximation of an optimization problem, or at least the feasible region for this quadratic optimization problem, by the feasible region for a semi definite optimization problem. And the function, and the objective function is actually a linear function over the variables.
00:46:34.530 - 00:47:33.658, Speaker A: So ultimately, we reduce this problem to that of semidefinite SDP. And the bottom line is that for suitable choice of k, which is log dimension of the first space, we can do this optimization in time, which is exponential in this log dimension. And so what this means is, if you had quantum proof that was something like, let's see if I have. Okay, so I haven't written it down. So if you had a quantum proof that was n to the half minus gamma long, then you would get an exponential one minus twice gamma algorithm for three sat, which we believe is not possible. Okay, so I probably missed your queue for the half an hour. Oh, I have time.
00:47:33.658 - 00:48:52.234, Speaker A: Okay, good, good. Okay. Oh, that's fantastic. Yeah. Okay, so I'm going to, I'm actually going to switch to the board, and I'll consult my slides here so that we can, I can fill in all the steps and. Okay. All right, so I have about, you know, 1015 pages of notes that I probably just skimmed through.
00:48:52.234 - 00:50:53.514, Speaker A: And at first I thought it would be good to have sort of a rigorous definition of what the basic elements of quantum computation are. But instead, you know, I'm just going to write down a few things on the board with the hope that once, once they're put up there, you'll recognize all the elements. So this is the second part of the talk where I'll be talking about what a quantum state is, what quantum operations are, what measurements are, and the way, the simplest way to think about them to be able to understand to a large extent, you know, the properties of the information, theoretic properties that go into these three applications. So the kind of state that we are most familiar with is basically a pure state, which is a vector, a unit vector in some d dimensional complex euclidean space. But in general, what could happen is you could, you know, you might be studying a composite system, a system that is, that consists of, you know, in the case of the area 1d area law, we looked at a system with n particles with each of dimension d. So in that case, what, you know, what we might be interested in is looking at the state or the description of the state of only a part of the system. And once you do that, the correct viewpoint for a state is not simply a vector in such a space, but something more general, which is basically an ensemble of states, meaning a probability distribution of these states.
00:50:53.514 - 00:52:15.664, Speaker A: And such a thing could arise just more directly where, you know, out of a probabilistic process and in fact, even from a measurement or part of the state. Right? So it's natural from a whole bunch of, you know, for a whole bunch of reasons. So these are what are called mixed states. This is some probability distribution over such pure states. Now, it's highly cumbersome to work with, you know, this description of states, even though it's, you know, you can, you can do it without any, you know, without running into any problems. But there's a much neater, much cleaner, more succinct representation of these mix states in the form of what's known as a density matrix. The density matrix corresponding to this mixed state is a linear operator, and you construct it as a convex combination of these outer products, vi with its dual.
00:52:15.664 - 00:53:26.204, Speaker A: Now this is an operator on this d dimensional space, and it has really very special properties. One of it is probably immediately obvious from this description, this definition. If I call it rho, then rho is positive, semi definitely. Not only that, it has trace one. Actually, these two properties turn out to exactly characterize matrices that arise from mixed states. Why is that? Because if you take any matrix sigma, which satisfies, which is positive, semi definite, and as trace one, and you look at its spectral decomposition. Oops, this is sigma.
00:53:26.204 - 00:54:49.964, Speaker A: Look at the spectral decomposition of sigma. Sorry, let me. All its eigenvalues are non negative because it's positive semi definite, and they sum up to one because it has trace one. So morally, you could think of sigma as having a ryzen out of a mixed state, where you pick eigenvector U sub I with probability lambda, sub I. So that's basically what you, all that you need to understand about quantum states. What about operations? Well, one basic operation is preparing some number of particles in a fixed quantum state. So let's say you have some k qubits prepared in zero in the state zero.
00:54:49.964 - 00:56:09.874, Speaker A: So appending some ancilla, what are called ancillary qubits. So, you know, having, having introduced this notion of a density matrix, what I'll be doing for the rest of the talk is identifying a quantum state with its density matrix. Now you might ask why that's a legitimate thing to do. In fact, it's not quite evident from this description, but there are very simple examples where distinct mixed states give rise to the same density matrix. But very powerful property of this representation is that any two mixed states with the same density matrix actually behave exactly the same under the same evolution. So under quantum operations and under measurements, they have exactly the same behavior. So I'll be identifying a state with and representing, writing it always as a density matrix.
00:56:09.874 - 00:57:16.734, Speaker A: So this operation here basically looks like taking a state rho and mapping it to transit product like so. Now, another familiar quantum operation is what we all know and love about quantum computation. It's unitary evolution of an isolated quantum system. So of unitary matrix u. Under this map, a density matrix rho goes to u rho u adjoint. And this is consistent with our view of how pure quantum states evolve and correspondingly how mixed quantum states evolved. If you simply write down what the density matrix would be for a mixed state like so, after it has evolved under u, it would look like this.
00:57:16.734 - 00:58:48.564, Speaker A: Now, the third operation, well, you might imagine that you have a composite quantum system. So you have a state that is over some d one plus d two qubits, and you let the state evolve. But finally, you're only interested in one part of the state. So the state corresponding to the first d one qubits. Is it possible to express the state of the first d qubits only as a mathematical object over the first space? And the answer is yes. There's this operation called the partial trace operation, and you can write it, it's a linear operation on density matrices. You can write it like so it's basically given by any choice of orthonormal bases, basis for k looks like identity tensor Ei, where EI is an orthonormal set for.
00:58:48.564 - 01:00:04.838, Speaker A: So these are three basic operations. And turns out that any other operation that's physically allowed by quantum mechanics can be derived as a composition of these three. Actually, maybe I should really point out a few things, or at least state them that these three are examples of operations. And I'll just, you know, throw out the words in here and maybe not really explain what they fully, what they mean. So these are all, these are all completely positive phase preserving maps. The second property should be almost self evident. None of these three things, three operations actually perturbs the trace of the matrix.
01:00:04.838 - 01:01:24.134, Speaker A: It maps density matrices to density matrices. So trace is preserved and it's one, in fact, positivity is preserved. So positive semi definite matrices are mapped to positive semi definite matrices. But these three operations actually have a stronger property that they're completely positive, which means that if you, if you're letting a part of a larger quantum system evolve under any of these three operations, then indeed it will be a legitimate quantum operation on the larger system, regardless of how big that system is. Now, something else I'd like to mention about this is that this is a precise mathematical characterization of all possible quantum operations which may be derived by any kind of composition of these stream. In fact, any such operation has a very simple form. At the outset, you might be a bit daunted by just trying to reason about an arbitrary composition of these three.
01:01:24.134 - 01:02:35.354, Speaker A: But it turns out, regardless of what composition you take, you can express it as basically a composition of just three operations, first appending some ancilla, some suitable number of qubits in a fixed state, applying a unitary, and then throwing away part of the system. Okay, no matter what how complicated your original point of operation was, you can view it as a sequence of those three steps. So just maybe. Quick question, how does this relate to the hamiltonian view? I'm sorry, which one? The hamiltonian view. Oh, the hamiltonian view. Okay, well, the hamiltonian view is right here. Basically, the unitary arises, as you know, e to the minus, well, e to the ih for a suitable Hamiltonian evolved for a certain amount of time t.
01:02:35.354 - 01:03:35.294, Speaker A: So u we think of as a discrete operation or basically an evolution of the Hamiltonian for a fixed time interval. So it's integrated or something over time. That's right. Okay. The final thing I would like to introduce, before I go on to something more meaty, is the idea, the notion of a measurement. Often people talk about measurements, as, you know, operations that are distinct from these. But in fact, mathematically, measurements are special cases of quantum operations.
01:03:35.294 - 01:05:37.504, Speaker A: Maybe I'll just recap what the most familiar measurement looks like, and then we'll see how it relates to a general quantum operation. So a von Neumann measurement or a projector measurement is specified by the sequence PI r each PI I being a projective measurement, an orthogonal projection, all. Well, if this is a measurement over space c to the d, so these are all linear operators on the space such that they sum up to identity. And we all know what it means to, or what happens when you measure a pure state v. According to this measurement, we observe one of the possible outcomes, I probabilistically, and we observe I with probability, basically norm squared of the part of the state that's consistent with this projection. Another way to write this is as a trace of PI times the density matrix corresponding to vi. Sorry, v.
01:05:37.504 - 01:07:20.084, Speaker A: The reason why I write it this way is because, you know, for a mixed state row, this is precisely what the concise expression for the probability would be. Now, you might imagine doing a more complicated measurement. So what might that be? You might do some computation, or you might let the state evolve according to one of these operations here, and then finally measure using a von Neumann measurement. Turns out that there is again, a very neat succinct representation of this composite measurement, this general measurement, and it's quite similar to a Ponoyman measurement. It is given by what is also called as a POVM. It is a sequence of operators. Ei, EI are all positive, semi definite, and of course they are on the same space, so that they're operators on c to the d, and they also sum up to identity.
01:07:20.084 - 01:09:39.324, Speaker A: The only difference is that these may have arbitrary eigenvalues anywhere between zero and one, as opposed to projector measurements, which have eigenvalues only zero and one. The rule that governs what the outcome would be or what the distribution over outcomes is, is precisely captured by this trace. So on measuring state rho, we get outcome I with probability phase Ei rho. Okay, so here's a more useful representation of this measurement, this kind of measurement as a quantum operation, and this is how I'll be viewing it. It's a special kind of operation which takes a state rho to basically a distribution over outcomes. So it takes rho to sum over I or states I, each state I occurring with probability given by the corresponding element Ei here. So any such measurement, by the same token as this completely positive trace preserving maps having a succinct representation, also have a simple decomposition as basically appending some ancilla, performing a unitary, and finally doing a von Neumann measurement.
01:09:39.324 - 01:11:24.214, Speaker A: So all of these things somewhat well, should probably explain why we are able to basically precisely capture what the complexity of implementing these steps in our models for quantum computation are. What we try to isolate is basically all the resource requirements, and we try to concentrate them in the unitary evolution rather than in the measurements, so that the final measurements are all very simple. They form neumann measurements in a fixed basis state, which is presumably easier to implement than an arbitrary measurement. I am somewhat troubled by the very last thing, because this one here. Yeah, because when you do a measurement, you stay in the same Hilbert space, right? You are just putting your state into a subspace, but here in the last line, you are creating a whole new Hilbert space, kind of like, it's like a theoretical concept somehow, right? But physically, you could think of implementing it as follows. I mean, what you do is, you know, you use some fresh ancilla qubits, implement your old measurement as a unitary evolution, where you write down the outcome in the fresh ancilla. So in your workspace, and you just discard whatever happened to the original state, and what you're left with is your measurement outcome.
01:11:24.214 - 01:13:49.714, Speaker A: So although that may not be the best way to implement it in the arguments that I'm going to make, this is mathematically an easier thing to work with. Then you are losing some information, but you don't care. Yeah, for what I'm going to do, I won't care about what sometimes you think of as the residual state after a measurement, like this projected state when you do a von Neumann measurement. Okay, so one final thing about states, and I think then after that I'll have to start zipping through something more information theoretic. Okay, so this final concept is to do with how well you can distinguish two given quantum states, given states rho and sigma. Let me define a distance measure on these two states. Find what we'll call the trace norm, or actually better written as with a subscript one, as the trace of the square root of m adjoint m.
01:13:49.714 - 01:15:11.284, Speaker A: So for any matrix m, this is a well defined positive semi definite matrix which has a unique square root. And this trace norm I'm jumping ahead, is basically the trace of this square root positive semi definite matrix. And what is this in the context of distinguishing two states? This is a hermitian operator, which is basically the difference of rho and sigma. This is the trace distance between rho and sigma. Now, how is this related to distinguishability? Well, the way we can access information about states is only through measurements. But it turns out that this trace distance exactly captures how well you can distinguish these two states through measurements. So the maximum over measurements, let's say e, sub I, the l one distance between.
01:15:11.284 - 01:15:59.344, Speaker A: So let's just denote by the operation e, the measurement operation that I've defined here. This gives us two classical distributions. We can look at how different they are in the l one norm. And it turns out that this is precisely the trace norm between rho and sigma. Wait, what do you have on the right hand side? This here is the trace norm of rho minus sigma one. Right. I'm just using the same.
01:15:59.344 - 01:16:47.544, Speaker A: Okay, so let me just explain what this is. Actually, I should have said that before. If you look at what this expression is here, m dagger m is a positive semi definite matrix whose eigenvalues are the squares of the singular values of m. Taking the square root, we get a matrix whose eigenvalues are the singular values of m. And so this trace norm is basically the sum of singular values of m, and it's a one norm because we are taking just the sum of the eigenvalues. So it's the l one norm of the singular values. We could define similarly, the two norm, the p norm of the sequence of singular values, and all of them give us valid norms.
01:16:47.544 - 01:18:44.432, Speaker A: But the one that is most relevant to us in terms of distinguishability is this one here. Okay, so I think that gives us enough tools so that we can zip through some information theory. Okay, in the last ten minutes. So, as I was saying, you know, entropic measures, really, you know, there are no canonical entropic measures per se. It's just that these measures arise naturally when you study different information processing tasks. And one measure that has time and again come very handy is this notion of phonomen entropy. Now, this is for Neumann entropy is not this sort of hammer that you can go and smash any old problem in information theory you have with, in fact, in certain tasks, for example, in privacy amplification, the relevant, in randomness extraction, the relevant entropic measure is in fact, main entropy and not Neumann entropy.
01:18:44.432 - 01:19:51.224, Speaker A: And so you have already seen a few examples where, you know, the canonical entropic measures are quite different from the one I'm defining, but nonetheless, this is one that occurs quite often. And in fact, in the three applications that I mentioned figures very prominently. So here's what it is, the exclusion of others. I'm going to focus on this and quantities that are related to this for any state rho d dimensions. This entropy basically tries to characterize the amount of randomness in the state. So, you know, when I first learned about the notion of, first of all, a mixed state, it was already very confusing because I thought of quantum state already as having some sort of inherent randomness. But fact is that the randomness arises only when you make a measurement.
01:19:51.224 - 01:20:54.924, Speaker A: If you know a priory, that a certain system has been prepared in a pure state v, then in terms of ignorance about the state, there is little ignorance. I mean, you know precisely what the state is. So the notion of von Neumann entropy actually is a more, you know, faithful measure of our ignorance about what the state is. So here's how it's defined. We've seen before that you can, using the spectral decomposition, write it as a convex combination. Oops, that's just I convex combination of orthogonal states. Phonomen entropy is basically the Shannon entropy of this vector of eigenvalues of this matrix.
01:20:54.924 - 01:21:40.794, Speaker A: So if your state was pure, then you would have zero entropy. If your state was uniformly distributed over an orthonormal basis for this, it would be log of d, and it could range anywhere in between. For general quantum states. The way you might interpret it is, you know, very similar to. Well, just from this, let's write this down explicitly. It's a sum over I lambda I, log one over lambda I. You know, roughly speaking, this is the amount of randomness.
01:21:40.794 - 01:22:54.754, Speaker A: It's how random the ith eigenvector is. So I'll just write it as this informally, and this is basically an averaging of this randomness. Okay, this is operationally not a very useful interpretation, but if you take it literally, at least when you study area lawsuit, you can see a correspondence between this view of phonomen entropy. You've already heard about a few interpretations. One is, this is the least entropy of the resulting state. When you do a complete von Neumann measurement of rho, another interpretation is, well, it's the least number of qubits that you need, on average, to compress many copies of rho. Okay, it's very similar to the classical Shannon theorem, noiseless coding theorem.
01:22:54.754 - 01:24:47.342, Speaker A: Okay, so maybe, okay, so I'll, so what I'll do is perhaps just explain this other interpretation of on entropy as a measure of entanglement and entanglement rank, and then gauge, maybe ask the ma what I should do, nearing the end of my time. Okay, so here's, so let's say you have a pure state. And, you know, when I say a pure state, in the back of your mind, you might think of this as a ground state of a Hamiltonian, and you're wondering whether this state can be represented succinctly. In particular, whether, you know, its entanglement triangle across any partition is small. So let's say this state is v, and it's in a tensor product state. I'm just going to look at one partition so that, looking at space h, tensor k. Now, Zeb probably talked about the entanglement rank in the literature.
01:24:47.342 - 01:26:27.314, Speaker A: It's also called Schmitt rank. It's also the tensor rank. And this is defined as the least k, such that you can write v as a sum of tensor products, well, as a sum of k tensor products. There exists vectors AI in h b I in k, such that b is some linear combination, need not be convex. I equals one to k alpha I AI, tensor bi. So in a product state v one tensor v two has tensor rank one. So if you have a state like this, it's clear a state like so, which is what we call the maximally entangled state as rank d.
01:26:27.314 - 01:27:15.292, Speaker A: There's no way to express this as a sum over fewer terms of this sort. Turns out that this notion of entanglement is closely related to phenomenon entropy. And in fact, this is sort of the reason why area laws are studied. And, you know, in terms of either quantity originally, you know, in terms of phenomenon entropy, but for more, for computational reasons, in terms of entanglement rank. So just, it has to be non zero. Oh, you're right. That's right.
01:27:15.292 - 01:27:38.748, Speaker A: So alpha is, but it could be really small, and that still contributes the same as a larger one. But then it shouldn't be related to, you don't need that. I'm sorry. If it's the least k, you don't need to impose that. The non zeroness of alpha. Right, but you're right. I mean, it's not exact.
01:27:38.748 - 01:29:21.784, Speaker A: They don't correspond exactly. They'll be, you'll see the, you know, the slack in one direction. Right. Okay, so the easy direction is the, is the following, you know, for, well, I don't know whether it's easy. There's a direct connection between entanglement rank and, you know, the singular value decomposition of matrices through a. So a simple isometry between bipartite states and matrices. So what this tells us is for any vector v in h tensor k, there are orthonormal sets uj and wj in h and k, respectively, such that, well, and lambda j non negative, such that v can be expressed as summation j going from one to some l lambda j u j tensor wj.
01:29:21.784 - 01:31:03.904, Speaker A: Okay, where l is actually bounded by both dimension of h and the dimension of kick. Why is this remarkable? Well, if you look at the way I've defined entanglement rank, you know, it could be as high as the product of the dimensions of h and k. But what this is telling is that for at least tensor rank of bipartite system, it's always bounded by the dimension of the smaller system. And moreover, it has a very special form where both of these left and right vectors form orthonormal sets which are in bijection with each other. Ok, an immediate corollary from this is that if you look at this, the reduced state of v restricted to either space h or k, then its entropy is bounded. So the entropy of rho, sub h, in fact, it's equal to the entropy of rho, sub k, which is equal to the entropy oh lambda. So the reduced density matrix on either side can be written in terms of these orthonormal sets.
01:31:03.904 - 01:33:00.314, Speaker A: So rho, sub h equals summation I under I. Sorry, row, sub k. Um, the maximal entropy that this could, this could achieve is at most log of l, so u at most log dimension of h or k, whichever has a smaller one. All right, so in the minus, however, minutes. However many minutes I have, maybe I'll just state the other part, which is, you know, which may not be immediately obvious, but is quite straightforward, which is that. Let's just write this here. If the entropy of rows sub h, where this is the reduced density matrix on the h side, is, let's say, kappa, then for all t, let's say, greater than one v is.
01:33:00.314 - 01:33:56.824, Speaker A: Let me just not make this too precise. It's close to a state v tilde of entanglement rank at most due to the t copper. And this closeness is. Well, maybe I can just write it and not say what it is precisely. Something like one minus constant. Okay, all right. Maybe I will just stop for questions and let that guide where I'm going.
01:33:56.824 - 01:34:49.636, Speaker A: We have questions. Sorry, could you repeat the question? Is there an equivalent to a shmiti composition when you have more intense product terms? So if you have three party. So there is a. You know, there's a definition of tensor rank for multipartite states, but no similar, you know, similar structure for tensor rank. I think it's even open, like here. The maximum dimension can be n. I vaguely remember that.
01:34:49.636 - 01:35:13.056, Speaker A: It's even unknown what's the maximum dimension of. I think that in r, it's not even clear what is the maximum dimension. Or am I wrong here? Leonard's the right person, too. Correct. And that's my memory as well. It's somewhere between. Let's say we go to one more party.
01:35:13.056 - 01:35:51.184, Speaker A: So three three party. It's somewhere. The maximum you can have is somewhere between three n squared over four three four of n squared and one third of n squared. No one knows. Maybe this is over the complex field. So maybe the gap is between one half n squared and one three n squared in this case. So what would be the tensor rank for the max? Oh, that's slow.
01:35:51.184 - 01:36:34.284, Speaker A: The maximally entangled, that would be just n. The ii sum of III, that would be just n. So that's a terrible example. So to go to n squared, you really need a tricky construction, I guess. Well, random thing would get you one three n squared. Do you have other questions? Let's thank everyone again. So now we have a 20 ish minute break, and we.
