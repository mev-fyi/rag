00:00:00.080 - 00:00:01.372, Speaker A: Dynamic matching markets.
00:00:01.774 - 00:00:30.410, Speaker B: Thank you very much. Thank you everyone for coming. I think a few of you have seen some sort of some other presentation of this talk, so feel free to leave. This is about dynamic matching market design, a joint work with two wonderful co authors, Shengoo Lee, who is sitting there, and Shayano Visharan, who is at University of Washington. So let's start for this audience. I don't have to actually prove that matching markets are important. They surround us.
00:00:30.410 - 00:01:16.704, Speaker B: Kidney exchange, school choice, labor markets, national residency matching program, of course, dating markets. These are all matching markets in which not only you have to choose, but also you have to be chosen, and prices do not do all the work. I mean, these are important markets. So computer scientists and economists have spent a huge amount of time, starting from Gail Shapley 1962, to analyze these markets. I'm not going to go through all these papers. What I want to emphasize is that if you look at these papers, you can see that the question that is asked in those papers have the following theme. Given a set of agents and their reported preferences, find a matching algorithm with some desirable property, a stability, efficiency, or a strategy proofness.
00:01:16.704 - 00:02:00.852, Speaker B: Take the classic example of this literature, Gail Shapley I have n men and women, and my goal is to find a stable match. And the only question I have to answer in order to find a stable match in this environment is which agents to match. If I match this man to that woman and that man to this woman, the matching is going to be a stable. And the key point about this environment is that once the match is made, all is done. This is a great way to model school choice or national residency matching program. That's exactly what we are now doing in NRMP or New York City. But once you think about these other examples that I have here, you can see that this assumption that the set of agents is given is too much of an assumption for such environments.
00:02:00.852 - 00:03:01.370, Speaker B: In these markets, agents arrive, stay and depart continuously over time, and the composition of options at each point in time is a function of the matching algorithm. If I do not match these two agents today, they might be in the market tomorrow, so I can use them for some other form of matching. And because of this endogenous feature in designing a matching algorithm for these environments, which is the key goal of this paper, we face a dynamic decision problem rather than a static decision problem, in which, in addition to this classic question of which agents to match, we have to answer the second question. And that's the question of when to match agents. That's the question of timing and this is the focus of this paper and this talk. So to see why this is important, I want to use an example from kidney exchange. You can generalize it to some other markets easily or with some tweaks, but for those of you who are not familiar with kidney exchange, a 32nd description of what's going on in kidney exchange.
00:03:01.370 - 00:03:31.314, Speaker B: I have kidney failure. My sister is willing to donate me a kidney. That's great, but we have to be blood type and tissue type compatible. Tissue is a bunch of different protein levels that I can measure and tell you whether the antibodies of my body are going to react to the antigen of my sister. If you pick two random people from the US population, the chance that one of them can donate a kidney to the other one is pretty low. It's of the order of 10%. So it's likely that my sister cannot donate a kidney to me.
00:03:31.314 - 00:03:58.606, Speaker B: That's the bad news. The good news is that there might be some other pair with the exact same problem and the donor of that pair can donate a kidney to me, my sister can donate a kidney to that patient, we can have a kidney exchange. And this is how I'm going to show this phenomenon in a graph. Each node in this graph is a patient donor pair. It's important. So each node is a patient donor pair and each edge in this graph shows, I mean, shows the possibility of a kidney exchange. Now let's see why.
00:03:58.606 - 00:04:31.654, Speaker B: I mean, timing is an important question and why we have to think about timing and information in these environments. Suppose I'm a market maker and I have the following graph. I mean, I have patient donor pairs one, two and three. One can be matched to two, two can be matched to three. In a static environment, which is the focus of literature so far, at least until a couple of years ago. If you ask what is the optimal decision, I mean, match one to two or two to three, you will save two people. Suppose I match one to two, they leave the market.
00:04:31.654 - 00:05:14.664, Speaker B: Now, it might be the case that in a dynamic environment, tomorrow patient donor pair four comes to the market. And because of all those biological complexities, four can only have a kidney exchange with one, in which case the optimal decision is to match one to four and two to three. And I cannot do that if I do not wait in the first period. So information about future trade opportunities is going to be important. Now, there is a second type of information that can help me in such environments, and that's why weighting can be valuable. Suppose I'm in the same scenario, I match one to two, they leave the market. Now, in the next period, I get some signal that this patient donor pair three is an urgent need.
00:05:14.664 - 00:06:29.480, Speaker B: Now, if I do not match one to two in the first period, I can save three by matching three to two. But in this environment that I've matched one to two, two has already left the market. I cannot match them. So information about agents urgency of needs, you can generalize it to information about agents weighting costs is going to be an important information. Now, the questions I'm going to ask in this talk, or at least in the paper, maybe we don't get through all of them in this talk, are maybe these examples that I showed you are just some toy examples. Maybe weighting is not as valuable as we saw. So the first question is, can you quantify this option value of weighting? And then if waiting is costly, what is the optimal waiting cost? What kind of information can help us to make the right timing decision? And if that information is a private information of the agents, can you design a mechanism to extract that information from agents? To see why this question might be important in practice? If you look at different kidney exchange pools all around the world, you can see that the alliance for pair donation, the biggest kidney exchange pool in the United States, they conduct a maximum matching algorithm every day.
00:06:29.480 - 00:07:10.664, Speaker B: So they wake up every morning, they look at their pool, and they find a maximum matching every day. Of course, they have some maximum weighted matching algorithm, but the point is that they do that every day. The United Network for organ sharing does the exact same thing every week. Every Monday, South Korea, every month, Netherlands, every quarter. I'm not saying all of them are right or all of them are wrong. Maybe there is a fundamental difference in all these markets, but the point is that these policymakers are actively thinking about timing in addition to matching. And the goal of this paper is to construct a model by which we can think about the optimal timing decision in a systematic way.
00:07:10.664 - 00:07:42.064, Speaker B: What we are going to find, let me tell you, let me spoil the results. The main result is going to be that this option value of weighting, or the upper bound on the option value of weighting when weighting cost is zero, is going to be huge. So it's not going to be something small. We will see. It's going to be exponentially valuable. And the reason that it's going to be valuable is actually very intuitive. The reason is that by waiting, I'm going to thicken the market.
00:07:42.064 - 00:08:19.254, Speaker B: I'm going to keep options open. I'm going to have a thick market, and then whenever I get this signal, that someone is urgent, I have to match that, that agent with high probability I can save that agent. And we are going to see that this is going to be an exponential effect. Now, the second point is that, as we saw here, this piece of information was crucial in this result. And we are going to see that if you do not observe when agents need is about to expire, when agents are going to depart, then we can do. Then thickness is not going to help us. And that piece of information is going to be extremely important.
00:08:19.254 - 00:09:04.520, Speaker B: And the reason is that if I have that information, I can be patient with respect to others and only serve those people who are going to have very high waiting costs. And in some environments, including kidney exchange, that information might be a private information of the agents or the doctors or the hospitals. Because kidney exchange pools are different entities from doctors and hospitals, we are going to think about some ways to extract that information in a truthful way. So the LEEt review, let me just skip. This is the static literature review. There are some papers on the dynamics of matching markets. We have Nick sitting here and let me describe the model for you.
00:09:04.520 - 00:09:14.694, Speaker B: And then we can come back and think about the differences of different papers with this paper. Please remind me if there is something which is not here in the paper.
00:09:16.274 - 00:09:30.538, Speaker C: So you talked about urgency. Maybe also the different matches have different weights or values. Could that play the same role? That's something I get some match that would be really, really valuable, but not necessarily urgent. Would that play a similar role or.
00:09:30.546 - 00:09:44.454, Speaker B: Would that be different? That would be different in the sense that if I have a very valuable match, but the these two people have very small waiting costs, then I'm not going to match them until the point that one of them becomes urgent.
00:09:44.614 - 00:09:46.278, Speaker C: If you don't know the waiting cost.
00:09:46.366 - 00:10:01.358, Speaker B: Then it might be very valuable. Exactly. But that would be similar. But that would be a different model. We will see. So this is the structure I probably don't get to extension. So I start with the setup of the model.
00:10:01.358 - 00:10:30.298, Speaker B: I will tell you two main results of the paper with Noam helping me in time, and then we will conclude. So let's start with the model. This is everything about the model. So nothing more is about to come. So if you want to take a nap, please do it after two minutes. This slide is the most important slide of the talk. Agents arrive to this market with an exogenous poisson rate mix.
00:10:30.298 - 00:11:00.274, Speaker B: So in expectation m new agents arrive to my market. And then each two agents are going to have an acceptable transaction. There is going to be an edge between these two agents with probability p iid. That's one simplification compared to real world because there are some correlations. But let's assume these edges are independently drawn. And then there is an exponential clock which ticks with rate one. That's just normalization.
00:11:00.274 - 00:11:38.704, Speaker B: Each agent is connected to an exponential clock. And whenever that clock ticks, that agent becomes critical. This is a very stylized way for me to model the fact that we have a very short horizon, but accurate information about agent's departure time. In the paper, we are going to relax this assumption in both directions. What if you know far ahead of time that someone is going to be critical? What if you don't observe agents who are critical? And we are going to see that this information is going to be very important. So whenever an agent becomes critical, that's the last point in time that the planner can match that agent. So an agent leaves in one of the following two ways.
00:11:38.704 - 00:11:48.880, Speaker B: You become critical. Then you leave the market either matched or unmatched, or you get matched and you leave the market. It seems to me as a lot.
00:11:48.912 - 00:12:00.990, Speaker C: Of kidneys, the critical is a little implausible. I can imagine patients gradually go downhill. Maybe there's a sharp drop, but in fact, you really don't want to wait until the high risk.
00:12:01.022 - 00:12:24.924, Speaker B: Yeah, so I can show you a graph, perhaps after this. But in kidney exchange, you usually start by being informed that you are going to have a kidney failure in six months. Then you have basically very low weighting costs up to that point. Then you start dialysis. You have a small waiting cost during dialysis, during the first months of dialysis. Then there is some points that actually. So weighting cost is in some sense convex.
00:12:24.924 - 00:12:43.184, Speaker B: There is some point in which you actually have a very high disutility because of some biological issues that I don't want to get into. But you're right. So I'm just assuming that weighting cost is zero and suddenly it jumps. But basically you have some small decay and then that's true.
00:12:45.264 - 00:12:51.844, Speaker C: And criticality is observed. So you don't need to incentivize an agent to reveal when they're critical.
00:12:53.504 - 00:13:17.530, Speaker B: It depends on how you define the criticality. So, yes, I mean, a kidney exchange pool basically can put some monitoring devices. They don't do it now, so they don't follow the health condition of patients. But that's one way to observe this information, in which case you don't have any incentive issue. But then you have to have some monitoring infrastructure and so on. Rather than asking hospitals, the guy that.
00:13:17.562 - 00:13:20.854, Speaker A: You can be matched to, is that like public information?
00:13:21.994 - 00:13:27.254, Speaker B: No. So the structure of the network is known only to the market maker.
00:13:28.354 - 00:13:30.298, Speaker A: It's known to the market maker, but.
00:13:30.346 - 00:13:32.074, Speaker B: Not to the, not to the agent.
00:13:32.154 - 00:13:33.854, Speaker A: Do they have any private information?
00:13:36.214 - 00:14:00.734, Speaker B: That's a design parameter. So me as a market designer, I can actually design it. But currently, no, they don't observe, because if they observe, then they don't have any incentive to come to the pool. They can just go to a doctor and ask them to match them. That's the reason that they come to the market. So this is how the system works. Just, I mean, you come, you make some random matches, then someone gets critical, you cannot match them.
00:14:00.734 - 00:14:21.658, Speaker B: They leave. I mean, some other people come, this guy gets critical, I may match them. And when I match two agents, both of them are going to leave. All edges connected to them will disappear from this network. This is how the system evolves. So let me just tell you one more parameter and then ask questions about, answer questions about the model. So this model has two parameters, m and p.
00:14:21.658 - 00:14:56.246, Speaker B: I want to use only one bit of your memory, so I'm going to multiply m by p and ask you to remember D. D is m times p. Why? This parameter is important because if the planner is inactive, if I do not match anyone, agents come to the market with rate m. Each agent leaves the market with rate one. So the expected size of the market is going to be m because in that case, arrival and departure are equal. And in a market of size m, if I come to this market, there are m people here, then I have a p chance to be matched to everyone. D is going to be my expected degree.
00:14:56.246 - 00:15:25.652, Speaker B: Of course, I'm in a room full of computer scientists. This is basically the Erdos Schweni parameter. This is how erdosveni defined their random graph. By setting this p equal to d over m, and then d is going to be the expected number of edges you are going to have. The point is that I can measure d. I can go to alliance for per donation, see what is m, the arrival rate, which is around 500 agents per year. See what is p.
00:15:25.652 - 00:15:47.032, Speaker B: And then I can calculate d in different kidney exchange pool. This number d is something between three to ten. So you can think about it as something like that. But the point is that we can actually go and measure it. So if there is any questions about the model capturing, like, blood types.
00:15:47.148 - 00:15:47.616, Speaker A: No.
00:15:47.720 - 00:16:05.444, Speaker B: Yeah, so it's, it's, it's, it's so it's an extremely heterogeneous market because of tissue type compatibility. So the, these protein levels are continuous levels. So in that sense it's not too far. But yeah, in, in principle you have to consider different types.
00:16:07.784 - 00:16:15.624, Speaker A: I mean the probably people were hard to match. I mean, I mean would that be captured by this model? Concentrated around.
00:16:15.964 - 00:16:45.684, Speaker B: So this model is assuming everyone is ex, ante equal. Basically in kidney exchange if you look at tissue type issue, there are two types of people, hard to match and easy to match. So that's the second paper that I'm not going to present today, but I'm writing with two co authors and we actually show that one of the results of this paper will fail. I will tell you which one, but the other one will not. So that the planner observes. Yes, yes.
00:16:47.544 - 00:16:55.784, Speaker D: In the US kidney exchanges, I believe most of the exchanges go via like three way or changes. That's not in your model.
00:16:55.864 - 00:17:12.423, Speaker B: No. Yeah, you are abstracting to just focus on this problem of timing versus matching in this environment. And that's a feature of kidney exchange. We try to make it a more general setting in some sense to capture other markets too. But that's a good point.
00:17:13.083 - 00:17:20.419, Speaker D: Also another thing is that these exchanges have been moving to shorter time scales for the matching. Like they started off maybe months, two months and then.
00:17:20.451 - 00:17:22.203, Speaker B: Yeah, you also started with month. Yeah.
00:17:22.283 - 00:17:28.435, Speaker D: So maybe you're going to tell us that they're making a mistake and they should do something different.
00:17:28.619 - 00:17:56.334, Speaker B: No, I'm going to tell you that they might be making a mistake. But the other point is that there is a platform competition basically in us because there are multiple platforms trying to get more and more easy to match people. So it's not easy to say that they should move into that direction because they won't. But I will tell you that under some conditions you actually want to be greedy. But yeah. So yes and no. So no, wasn't there.
00:17:56.334 - 00:18:24.474, Speaker B: So the planner observes this possibilities graph at each point in time. The planner observes who is critical. And because this is a continuous time model, there is at most one critical agent at each point in time. And the matching algorithm is a function from this network. And who is critical to a matching, a set of disjoint edges. Wow, too short. So, and the goal of the planner is to suppose weighting cost is zero.
00:18:24.474 - 00:18:44.404, Speaker B: For now. I want to find the upper bound on the value of waiting. Then the goal of the planner is to minimize the fraction of agents who leave the market unmatched. I want to minimize the fraction of agents who perish. So this is a well defined Markov decision problem. I know the model. I know the matching algorithm.
00:18:44.404 - 00:19:32.480, Speaker B: I know the objective. But as all of you probably know, each state of this problem is a network, and it's a combinatorially complex state space. I cannot use standard Bellman equations to solve this for the optimum of this problem. And in fact, I don't want to do that. What I want to do is to move from this complex optimum to some simple matching algorithms, because I'm not fully agnostic about the optimum solution of this problem. I know one thing about the optimum solution, which is that I do not match any agents unless, I mean, I do not match any pair of agents unless at least one of them is critical. So if I have two agents where none of them is critical, there is no reason to match them.
00:19:32.480 - 00:20:03.754, Speaker B: Waiting cost is zero by assumption. So I can wait up to the point that someone gets critical. I also know that when someone gets critical, I definitely match that agent, because the best I can do by not matching this critical agent and saving someone else for future is to save someone else in future. So I match, I conduct, I do a matching if and only if someone is critical now, to whom I exactly match a critical agent. That depends on the network structure. So that's the problem of matching. The first one is the problem of timing.
00:20:03.754 - 00:20:47.552, Speaker B: Now, I designed two matching algorithms to detach these two effects. I start by greedy algorithm and mimic of the alliance for paid donation Algorithm. Whenever an agent arrives, if that agent has any acceptable transactions, I choose one of them uniformly at rand. And then I design a second matching algorithm, patient, which has that optimal timing feature of the optimum. Whenever an agent becomes critical, I look at that agent. If that agent has any acceptable transactions, I choose one of them uniformly at random. Now, it's clear that this patient algorithm is choosing the optimal time, but it's pretty naive in choosing the optimum agent.
00:20:47.552 - 00:21:09.932, Speaker B: Suppose I have this network and two is critical. I can match two to one, in which case this would be the remaining network. I can match two to three. That would be the remaining network. Clearly, this is the optimum choice. Patient is naive and chooses one of them uniformly at random. So what I'm going to do next is to detach these two effects in the following way.
00:21:09.932 - 00:21:35.494, Speaker B: So, I know that if this is my loss, the minimum loss is zero. The loss of the greedy is somewhere there. Optimum is here, and patient is somewhere in between. Patient is better than the greedy, but it cannot be better than the optimum. This distance here is my gain from optimum timing. And this distance here is my additional gain from optimizing over the network structure. So this is conceptually what I'm going to quantify.
00:21:35.494 - 00:22:05.430, Speaker B: And remember, d was the. Oh yeah. And that's the point. I want to see whether I'm in this world that most of the gain is achieved by optimal matching, or I'm in this world that most of the gain is achieved by optimum timing. For this talk, I'm going to focus on a steady state, relatively large values of d, m and d greater than two. All of them are cured, carefully studied in the paper. So let's see the main result, which is actually what Yasch said.
00:22:05.430 - 00:22:07.514, Speaker B: I'm going to tell you that timing is important.
00:22:09.254 - 00:22:11.834, Speaker A: Your definition of mapping is with hindsight.
00:22:13.654 - 00:22:53.154, Speaker B: No, opt is the solution of that Markov decision problem. So I look at network, who is critical. I choose the optimum matching. So remember d was this network sparsity parameter, something between two, three to ten in different kidney exchange pools in USD's more than South Korea, because we have more patients. This is the main result that, the main result about timing that the loss of the greedy algorithm is at least one over 2d plus one. And the loss of the patient algorithm is upper bounded by e two minus d over two divided by two. And the last line is just the combination of these two two.
00:22:53.154 - 00:23:58.130, Speaker B: So if these eight, the loss of the patient is at most 17% of the loss of the greedy algorithm, which is suggesting that when weighting cost is zero, then the upper bound on the value of weighting is huge. Now, the second piece of that graph that I showed you, the optimization part, I don't know how the optimum works. I don't know exactly who is going to be matched to whom in optimum, but I can bound the performance of the optimum. And we can see that the loss of the patient which was upper bounded by this number, the loss of the optimum is upper bounded by this number, which is a fractional difference between those two. But this is just telling me that by moving from Pennsylvania to optimum, I gained something, but it's not huge. In fact, if you put some reasonable numbers, we can see that the loss of the greedy is 8.5% and the loss of the patient is going to be 0.9%.
00:23:58.130 - 00:24:31.026, Speaker B: And the additional gain from moving, by moving from patient to optimum is going to be much smaller than the other one. Of course, relatively speaking, it's not much smaller. I'm talking about the, the additive difference. And so, most of the gain in this model, I want to emphasize, is achieved by merely being patient. One of the results which fails if you have multiple types is this second part. So if you have hard to match and easy to match people, then this part is going to be important as well. But this part remains important.
00:24:31.026 - 00:24:54.028, Speaker B: Thank you. And this is just a, this axis is d. That's loss. This blue one is the lower bound. This is the best greedy can do. This is the optimum, and this is the worst patient is going to do. So as d increases, patient actually gets closer and closer to the optimum.
00:24:54.028 - 00:25:08.874, Speaker B: And in these areas of the map, patient has actually a good distance from the optimum. And that's because the probability of observing those structures that I showed you as an example is not actually very low. We see those examples. Patient is naive.
00:25:10.254 - 00:25:11.754, Speaker A: What are these arrows?
00:25:13.094 - 00:25:45.114, Speaker B: So this is the, this is showing that the greed is somewhere here. This is the lower bound on the greedy. And patient is here. And opt is here. So since I only have five minutes, the question is, what should I do? Let me just move from this part to this part and tell you one more theorem which is about the value of information. So this is what we saw. Patient, greedy and zero.
00:25:45.114 - 00:26:27.784, Speaker B: Now I want to relax this informational assumption. What if we know more? What if we know less? So let's forget about what if we know more? Because whatever is that, it's going to be here. You can see in the paper, if you design an omniscient algorithm which knows everything, the loss is going to be e to minus d. So let's forget that for now. The point is that if I do not observe these critical agents, they become critical and they leave the market without informing me. Then the loss of the best possible algorithm you can design the solution of that Markov decision problem is going to be very, very close to the loss of the greedy. So the best you can do without that information is one over 2d plus one.
00:26:27.784 - 00:27:10.210, Speaker B: And I can show that actually the loss of the greedy is upper bounded by log two divided by d. There is a conjecture, conjecture that actually greed is optimum in this setting. But we couldn't prove it because that's, that lower bound is very, very conservative. So this is actually showing that this criticality, information and market thickness are complements. I do not wait to, I do not want to, I don't want to wait to thicken the market. If I do not have that information, and if I have that information and waiting cost is low, then I want to actually wait to thicken the market. And the way we get that lower bound is actually in the following way.
00:27:10.210 - 00:27:46.180, Speaker B: That I don't know how the optimum works. This is my favorite proof of the paper, so let me spend two minutes to present it. But whatever is that optimum? It's going to have some expected pool size, some how many agents I have in the pool, whatever is that optimum. Now, I know that when I do not observe critical agents, the loss is going to be how many agents I have in the pool, times t. Why? Because each agent is becoming critical with rate one. So the perishing rate of the system is exactly equal to the expected pool size. They become critical.
00:27:46.180 - 00:28:33.964, Speaker B: I don't observe them, they leave the market unmatched. And if I run the system from time zero to time cap t, expected pool size, time cap t, tell me how many agents are going to leave the market unmatched. So I know that this is a lower bound on the number of people who are going to leave the market unmatched. On the other hand, I can count the number of agents who come to the market, do not have any acceptable transactions upon arrival, and do not have any acceptable transactions during their sojourn in the market as a function of the expected pool size. And that's going to be a decreasing function. Because if expected pool size is infinity, everyone is going to have at least one acceptable transaction. If expected pool size is zero, no one is going to have any acceptable transactions.
00:28:33.964 - 00:29:15.784, Speaker B: So that's going to be a decreasing function of the expected full size for any algorithm. And I know that my loss is above these two graphs because these are two lower bounds. And if I write down the closed form for this one and intersect it with this one, the best pool size I can have for any algorithm is m over 2d plus one. So there is no way that you can do better than that. If your pool size, expected pool size is less than that, this is going to be, this is going to kill you. And if the pool size is more than that, then the number of people who actually become critical and you don't observe them is going to create some troubles. If this is the pool size, then the loss is going to be one over two plus one.
00:29:15.784 - 00:29:45.534, Speaker B: And I think I'm almost done with the time. So let me do not go through the mechanism design part. So I looked at the timing of the talk. It was 1211, 45 to 1215. I said, oh, I have 45 minutes. That was the, that was a proof that there are three types of economists, those who can count and those who can't. So please ask questions.
00:29:48.154 - 00:29:53.454, Speaker D: Yes, if you know some probability, the question.
00:29:59.474 - 00:30:29.514, Speaker B: So if that's probably. Exactly. So that's. So if that probability is not very, very close to one. So if you observe them with probability 90, 80%, then you still want to be greedy because greedy is doing relatively well. So if you do not observe. So, in fact, let me just finish by this slide, which is there are good reasons to be greedy, and one of them is that if waiting cost is high, so we actually characterize the optimal waiting time as a function of waiting cost in the paper.
00:30:29.514 - 00:30:57.764, Speaker B: And if you don't have this piece of information, or if you have that piece of information with relatively low probability, if it's 90% or less, then you actually do want to be greedy because greedy is performing relatively well. And if p is very, very small, suppose p is zero. So no one is matched to everyone. Then greedy and patient are basically the same. No one can be matched to anyone. If p is very, very high one, then greedy and patient are basically the same. You can match everyone.
00:30:57.764 - 00:31:33.334, Speaker B: And in fact, the value of weighting is an inverse u function of this parameter p. So there are good reasons to be greedy if you are in such environments. So let me just conclude by this last slide that even the optimum algorithm cannot match all agents. As you've seen, it's e two minus d over two. So drink more water to prevent kidney failure. That's the only, I mean, practical policy advice I can have for you.
00:31:35.554 - 00:31:43.330, Speaker A: Why are the markets split? It seems that only increases authority. Where are the different markets in the slide?
00:31:43.442 - 00:32:14.480, Speaker B: In the US? Yeah. So it's a very good question. So there are different people with different objectives. They have their own, I mean, board of advisory and whatever, but I don't know why they don't. That's also a very good question. And that's because one reason is that your doctor introduces you to these different kidney exchange pools. And if I, as the doctor, introduce you to that kidney exchange pool and they find a match, then I'm going to actually do the transplant.
00:32:14.480 - 00:32:29.498, Speaker B: So I don't have incentive to introduce you to everywhere. But I honestly, I ask actually some doctors and al why this is happening that, I mean, 10% of people multiple register and no one knows exactly what's going on.
00:32:29.586 - 00:32:32.770, Speaker A: Is it okay to register, multiply? Or is that like cheap?
00:32:32.922 - 00:32:35.122, Speaker B: No, it's fine. I mean, you can do that.
00:32:35.218 - 00:32:36.254, Speaker C: Steve Jobs.
00:32:37.434 - 00:32:50.262, Speaker B: Yeah. So he did it. That he did that, actually, but he was in multiple cadaver kidney pools, that kidney exchange.
00:32:50.418 - 00:32:55.462, Speaker A: But I understand it's also possible to do a three way swap.
00:32:55.638 - 00:32:56.374, Speaker B: Yeah.
00:32:56.534 - 00:33:03.054, Speaker A: How does this impact lethality? How does this impact the rate of death.
00:33:03.174 - 00:33:32.884, Speaker B: Oh, so there are. So in a static setting, there are papers that show that moving from two to three actually saves a lot of. I mean, it's going to decrease the death rate by a huge factor in, like, dynamic environments. I think there is a paper by Yashan Itai and two other co authors. I don't remember more than two names usually. So they actually show that even in a dynamic environment, moving from two to three is a big jump in saving people.
00:33:34.064 - 00:33:36.504, Speaker A: So let's take the rest offline and thank us.
