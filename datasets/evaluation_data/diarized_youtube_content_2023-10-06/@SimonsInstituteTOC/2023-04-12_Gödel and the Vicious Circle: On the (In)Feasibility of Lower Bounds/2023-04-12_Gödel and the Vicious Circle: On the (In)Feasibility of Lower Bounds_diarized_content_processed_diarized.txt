00:00:00.080 - 00:00:27.086, Speaker A: Today's Richard M. Karp Distinguished lecturer. I'm Sandy Irani. I'm the associate director of the Simons Institute. And for those of you who are unfamiliar with the institute, we are the world's leading venue for collaborative research in theoretical computer science and related fields. We sponsor programs, semester long programs, and have a strong fellowship program supporting the next generation of young scientists. So we established the Richard M.
00:00:27.086 - 00:01:13.924, Speaker A: Karp Distinguished lecture series to celebrate the role of our founding director, Dick Karp, in establishing the field of theoretical computer science, outlining its central problems and a stunning array of results in complexity theory and algorithms. This lecture series features leaders in the field, and it's geared towards a broad scientific audience. Before we get started, I want to go over a little bit of the logistics today. We have folks attending the talk in person, and we have folks attending the talk over Zoom. If you're in the room and want to ask a question, you can just raise your hand. The room mic will pick, pick up your voice. If you're attending by Zoom, I'll be monitoring the q and A, and I can interrupt with questions as they come up.
00:01:13.924 - 00:02:08.176, Speaker A: So we're grateful to the many contributors of the Richard M. Karp Fund who have made today's lecture possible and the series possible. And we're really delighted today to welcome our speaker, Rahul Santanam. You all know he's one of the main organizers of the program that we have this spring, and so we're grateful to his work in doing that. Rahul is a professor of computer science at Oxford and a tutorial fellow at Magdalen College. Received his PhD from the University of Chicago, taught at University of Edinburgh after stints at Vancouver and Toronto before moving to Oxford in 2016. He's worked broadly across computational complexity, structural complexity, circuit complexity, proof complexity, foundations of cryptography and learning theory, and as you might imagine, much of his recent work has focused on the subject of this program, on metacomplexity.
00:02:08.176 - 00:02:15.284, Speaker A: So his talk today is titled Godel and the Vicious circle on the infeasibility of lower bounds. So thank you, Rajol.
00:02:21.544 - 00:03:18.364, Speaker B: Yeah, thanks for that introduction, Sandy, and thank you to the Science institute for this invitation, the opportunity to give this lecture. So I'm going to be talking about the infeasibility of low bounds with the in within parenthesis and in the abstract for the talk. I said I'd be spending the first part of the talk on infeasibility and the second part on feasibility, but that turned out to be quite too ambitious when I was preparing the talk, so most of it is going to be on the infeasibility, and there'll be a little bit about potential approaches to low bounds at the end. So I'm aware that this talk is meant to be for a broad scientific audience, but there's also lots of experts here. So what I'm going to do is I'm going to try to phrase things in a way so that I'm not going to get into definitions too deeply. I'm going to phrase things in a way so that if you're not an expert, you can still kind of get the conceptual point of what I'm saying. And if you are an expert, you know the definition already, and I don't need to explain it to you.
00:03:18.364 - 00:04:04.310, Speaker B: Okay? So feel free to ask questions at any point. Okay. So what I'm gonna be doing is telling you a sort of story about complexity theory and about low bounds and complexity. And there's a human protagonist to the story, which is Godel, as you can imagine, which is sort of maybe not the standard choice. I mean, we often think of Turing because of this impact on the definition of Turing machines and on general intelligence as well, the Turing test, which is especially impactful right now, but in terms of low bounds and sort of like, issues around proving low bounds, somehow, I think Godel is a more sort of appropriate figure. So he's the main human protagonist, and the main mathematical protagonist is the vicious circle. I guess there's more of an antagonist than a protagonist, as we'll see.
00:04:04.310 - 00:04:50.458, Speaker B: And really this talk is going to be about the sort of like, phenomenon of the vicious circle in complexity and improving low bounds. And it's going to be sort of like a speculative story. So I'm going to be kind of using evidence from results that have been proved so far. But there will be sort of gaps in the story, which I'll speculate on. And it might turn out to be the case that some of these end up being filled, but maybe also I'm wrong and it turns out that things are very different. But what I really want to do is to get you interested in the story, maybe to question and challenge it, not so much to buy everything I say. Okay, so let's start off with this famous letter of Godel to von Neumann from 1956, which was sort of anticipated.
00:04:50.458 - 00:05:38.054, Speaker B: The P was his NP problem. It's quite an extraordinary letter. Many of you might have seen it already, but just to kind of look at what it says, he kind of like poses this sort of finitely version of the N. Scheiden's problem. So Einstein's problem of Hilbert is a problem of whether, given a mathematical theorem, you can kind of check effectively whether that's provable or not. And what God asks is that sort of like a finite reversion of this given a formula in first order predicate logic and a number n, can you sort of tell if the machine can check whether there's a proof of length n for f? So he kind of defines his number three event, which is somehow the complexity of deciding whether fest approve of length n. And he looks at the asymptotic complexity and how it grows.
00:05:38.054 - 00:06:20.634, Speaker B: And this anticipates many features of complexity theory. I mean, in terms of, like, worst case complexity, in terms of measuring complexity in terms of the input length, and considering this problem, which is sort of a version of satisfiability, which turns out to be a crucial problem, and the complete problem. So basically, Godel was asking for Neumann about whether p was equal to NP are not long before the question is actually defined. So that's one of sort of God's main contributions, I think, even though it is only recognized later. And so that's one of the reasons he's a protagonist to this talk. And it's quite interesting that he also raises lots of other questions that have been of central interest in complexity. The question of whether numbers are prime.
00:06:20.634 - 00:06:57.848, Speaker B: Can you check that efficiently? And it turns out that he can. There's a famous algorithm of Agrawal, Kayal and Saxena, the question of whether you can avoid exhaustive search when you're trying to solve combinatorial problems. Can you do better than exponential and sort of the size of the solution you're looking for? And that's, of course, a crucial kind of question, complexity theory. And there's a lot of work on fine grain complexity, which aims to answer this question. And Godel sort of like, says here, well, yeah, it's interesting to know whether fear of n is greater than equal to kn or not. But, yeah, the best low bound we can show is linear. But it could be that there are efficient algorithms for this.
00:06:57.848 - 00:07:41.884, Speaker B: And that is interesting, because if you could have efficient algorithms, then you could automate what mathematicians do. And of course, that's something that mathematicians are very a question they're very interested in. Can they be automated or not? It's something that makes us anxious, and so it's something that's important to understand. So, okay, so basically, we'll be dealing with P versus NP and related complexity problems. NP is a class of problems with solutions that are verifiable in polynomial time. So that efficient solution that have short solutions, and you can check that a solution is correct in polynomial time, and P is the class of problems that are solvable in polynomial time. Another way of looking at it is that NP is the set of mathematical statements with short proofs.
00:07:41.884 - 00:08:18.794, Speaker B: And the question of the NP equals P is whether you can find these proofs efficiently. So that's on the way of phrasing this question. So this is, as you all know, a fundamental question which has implications for learning. Though learning algorithms could be doing pretty well without knowing whether NP equals p or not. It's also kind of fundamental in terms of automated theorem proving. Proving theorems efficiently, which learning algorithms still can't do, and to whether cryptography exists, can sort of break crypto systems, which I'm guessing that machine learning algorithms will never be able to do. Um, so there are lots of, um, applications, and not just in computer science, but mathematics and outside for this problem.
00:08:18.794 - 00:09:00.644, Speaker B: But what I want to highlight here is that this is just one of a family of low bound problems, problems where we try to understand, um, like whether problems are efficiently solved, whether certain conditional problems are efficiently solvable or not. And there's many others of this form, and they're all almost indistinguishable to us, for us, given our current state of understanding. So the problem of p space equals p. Can all problems in polynomial space be solved in polynomial time? The problem of NP equals co NP or not? Whether all tautologies have short proofs. All propositional tautologies have short proofs. The question of whether NP has polynomial size circuits, these are all examples of low bound problems. Here we believe the answer is no for all of these, but our state of knowledge is very, very limited.
00:09:00.644 - 00:09:50.212, Speaker B: And then there are these other problems that are more fine grained versions of this problem. You can ask, is the satisfactory problem, which is a canonical NP complete problem, is that problem solver in time 100 times n or not? Where let's say you pick a Turing machine with a fixed number of tapes, with a fixed tape Alphabet, and you ask, are 100 times n steps required to solve Sat? And of course, we believe not. So essentially, Godel's question is about k equals 100. We believe not, but we're still not able to prove it. And then you can ask, like finite reversion. So you can just ask, let's say you take inputs of length 100,000. Does satisfiability on this input length have circuits of size two to the power thousand? And if not, then for all practical purposes, the answer to the NPC question is no, because if you can't even solve inputs of this length, then it doesn't matter whether on inputs that are far, far higher, you can do things in polynomial time.
00:09:50.212 - 00:10:26.978, Speaker B: It isn't going to help us very much in practical settings. We believe that these finite reversions behave like the answer to NP versus P. But of course, we don't know for sure. But I just want to point out that there's a whole class of problems that we're interested in. And so what I'll be saying is sort of relevant to many of them. So, of course there have been like many attempts to attack the mp versus p problem using several branches of mathematics, logic, combinatorics, algebra, geometry, finite model theory and so on. But they've not really given us much information about the problem.
00:10:26.978 - 00:11:02.204, Speaker B: We know as little or less than we knew probably 50 years ago when the problem was defined. Um, so the best known separation we have is that nontermistic linear time is not equal to deterministic linear time. Um, so recall the NPV problem is to show that there's a problem in nonstop polynomial time. There's not in any polynomial time bound deterministically, and we don't even know if nontoxic time n squared is not equal to deterministic time n squared. So, um, yeah, so we have some, some separations, but they're very, very weak. And in terms of circuit complexity, we don't know if Sat or, or indeed any problem. An NP requires circuits of size greater than six times n.
00:11:02.204 - 00:11:40.548, Speaker B: So here, I mean, I'm assuming you've kind of heard of circuits before, but they're sort of circuits are just logical networks which used to solve finite boolean functions. So basically, you take sat on inputs of length n and you ask whether there's these kind of logical networks of size, very small size of linear size, solving Sat, and we still don't know the answer to that. So in practice, sat runs out to be very difficult. But this is additional difficulty from the nature of the problem itself. Right? So, like, I put up like three quotes here. The first is from Scott Aronson, and he has a survey on Sp. Was his NP formally independent? This is from 20 years ago.
00:11:40.548 - 00:12:23.322, Speaker B: And he says again and again in the survey, we'll encounter the bizarre self referencial nature of P in order to NP conjecture that all but asserts a titanic difficulty of finding his own proof. What does he mean here? Well, he means that P nolder NP says that mathematical proofs can't be found efficiently in the worst case. Right? And he's sort of suggesting that maybe this applies to the proof of this statement itself. But that's just a single statement. The P and Aldo NP statement is a fixed statement. And he wonders whether somehow the hardness of P zero NP is related to what the statement is saying. And then there's sort of like a quote by Ketan Mulwale, who developed this geometry complexity theory approach to complexity to lower bounds based on algebra.
00:12:23.322 - 00:12:56.620, Speaker B: Geometry geometry representation theory. He asked, why should the PMP problem not preclude his own proof by strangling the way of any approach? And why should any given approach be feasible, even theoretically? So here he's interested in barriers to his approach, what he calls a complexity barrier to his approach. And he sort of understands the self propensity of the problem as kind of like posing an obstacle to his approach. And then here's a sort of like half serious proof. I mean, it's not quite meant to be a formal proof, but it's kind of a proof of Huey Chen's that p not Np. He says proof by contradiction. Assume p equals Np.
00:12:56.620 - 00:13:22.044, Speaker B: Let y be a proof that p equals Np. Well, then the proof can be verified in polynomial time by competent computer scientists because we can verify proofs efficiently. However, since p equals Np, you can also find the proof, and since this hasn't been found yet, there's a contradiction. So, um. So, so, yeah, so this isn't serious, of course, and I think you'd be well aware of that. But it's interesting to just to see where kind of, this kind of argument breaks down. Right.
00:13:22.044 - 00:14:00.916, Speaker B: So, um. But first of all, we don't know that if people equals NP, there's a proof of it. It could be that people NP, but there's no proof of that fact. Um, so that's, that's, that's per, that's perfectly within the realm of possibility. Um, and of course, how do we generate a proof if you know it? I mean, we might know that people NP, but we might not have an algorithm that we can use to kind of like generate a proof. Um, so, yeah, so, so of course there are holes in this, but, um. But, but it's, it's kind of interesting because somehow it indicates that the longer we go without showing that, um, that p equals Np, the less likely it is that peak was np, because, um, that that will probably be able to come up with a proof sooner or later if, if the two classes are equal.
00:14:00.916 - 00:14:52.378, Speaker B: So that's just like heuristic sort of, um, kind of idea. Okay, so, um. So much for self referrality of positive. Given these two things, given the difficulty and practice of the problem, and given the self referentiality of the problem, we could ask, well, is it independent of, like, formal systems like piano arithmetic, which is where we do most of them, of our reasoning for arithmetical statements, or even a Zamela Frankel set theory, which is, of course, a much, much richer theory, where you have sets in general and very large sets. So this looks very unlikely at first sight, because the question of whether status Bono diagram seems like a very concrete arithmetical statement, and that arises in a natural way. So it seems quite implausible that it would be independent of these large systems. And obviously, if you consider some of the other kind of problems that I stated in the first slide.
00:14:52.378 - 00:15:28.960, Speaker B: So, finite restatements, such as whether Saturn inputs of length 100,000 as circuits of size two to 1000, these are clearly not independent of piano arithmetic, right? Because there's just a finite statement. You can just kind of like check all possibilities and write a finite proof either way. But that's not quite fair. Right? But it's missing the point, because if you're making the problem finite, then you should make proofs sort of like finite as well. You should need to restrict the size of proofs as well. If we are interested in this problem, because we think that two to the thousand is infeasible, then proofs of size two to the thousand are also infeasible. And we need, you need to be asking whether the proofs of size two to 100 or less, for example.
00:15:28.960 - 00:16:08.222, Speaker B: So there are finite tree analogs of the problem, but they don't quite, um, of this independence question, and they still make sense. Um, so, uh, yeah, so that's the question of independence. And, um, of course, the way to attack it is using this other contribution of Godel, which is unprovability. So, Godel showed that in any original formal system, there are unprovable mathematical statements, statements that are true, but unprovable. Um, and I mean, this is, of course, like an extraordinary result, which had lots of repercussions in the foundations of mathematics and philosophy. Not so much in mathematics itself, but certainly in the foundations of mathematics. And it did in logic, but maybe not so much the practice of mathematics.
00:16:08.222 - 00:16:31.624, Speaker B: Um, and, and of course, I mean, it's, it's sort of like fascinating results, because you could sort of compare it with, say, the uncomputability of computational problems. Like, you'd say, look, uncomputable problems exist. That follows just by accounting argument, because there are more computational problems than there are algorithms. But here the space of proofs and the space of theorems has the same cardinality. So of course, you can't use a counting argument. You need to do something more clever. And Godel use this sort of like, diagnosition argument.
00:16:31.624 - 00:17:03.174, Speaker B: So, like, I'm just going to be kind of referencing to this first incompleteness theorem here. So you take any sort of formal system that's rich, rich enough. You don't need very much, actually, to say rich enough. And I'm kind of using the semantic version where you say it's sound, it doesn't prove any false theorems, and it's effectively axiomatized. You can check whether a certain statement is an axiom or not efficiently or just recursively. Then there's a universal statement. The girl sentence such as this can neither be proved nor disproved in the theory.
00:17:03.174 - 00:17:41.586, Speaker B: And the idea is that you can sort of encode the question of whether a statement has a proof using an arithmetical statement, and then you use a diagnosation argument. So he defines the statement fee, which is sort of informally asserting that fee itself is unprovable in a. And he shows you can do that with an arithmetic. And then if Phi were provable, you get that phi is true and he has fee is unprovable. So that can't be the case. So we know that phi is unprovile, and that means phi is true and so not fee can't be provable because of the soundness of a. So somehow this sort of like, self referencial statement sort of like, asserts its own unprovability, and therefore it is indeed unprovable.
00:17:41.586 - 00:18:29.742, Speaker B: So that's kind of like the. The way, very, very intuitively that the argument goes. But I'm not so much interested in the fact that you get an approval statement this way, but rather in the reason why it's unprovable or the proof. Right? So there's sort of like a self referentiality here, this vicious circle that we can't prove the girdle sentence in a sense, because we know it is true. So that's kind of like, there's this kind of like, paradoxical nature of the sentence. And so, okay, that's interesting, but it doesn't seem to say very much for the practice of mathematics, right? Because on the one hand, Goethe's theorem sort of like, really seems to deal a hammer blow to the foundations of mathematics, because there's no way to effectively axiomatize mathematics so that we're able to prove all truth theorems. So that's unfortunate.
00:18:29.742 - 00:19:09.776, Speaker B: But in terms of mathematical practice, in terms of how we actually do mathematics, it doesn't seem to have much of an impact immediately because girdle sentences don't really arise in practice very much. Um, that's not the kind of mathematics we tend to do. But then one might think, well, perhaps these kinds of sentences are indeed relevant for problems such as p versus NP, given the self refreshing nature and similar complexity. Low bound questions. So that's the question that I'm sort of going to explore more in this talk. Like, to what extent is this vicious circle phenomenon present in complexity theory as an obstacle to us proving low bounds? Um, and, and just to kind of like. So right now the evidence might be quite sparse because, well, Algirdle sentences are self referential.
00:19:09.776 - 00:19:51.334, Speaker B: We as NP has sort of like a self reference to nature seems to. That's not very, that's not very compelling by itself, I think. So I'm going to consider this other analogy to this kind of like these results about Kolmogorov complexity. And I need to tell you what Kolmogorov complexity is. So Kolmogorov complexity is sort of like a notion of information, of a string, right? So you kind of consider like a fixed machine, a universal Turing machine, and for any string x over some Alphabet, you consider, you define the complexity of x to be the smallest program that produces this string. Um, so it's somehow the smallest string from which you can generate x, um, or the extent to which you can compress x. And by generate, you mean that a universal machine generates it.
00:19:51.334 - 00:20:39.060, Speaker B: And just a couple of examples. If you take like a string of all zeros, that's going to have more complexity at most log n plus order one, because we can describe a string of all zeros by using log n, this to describe n, and just constant number of bits to describe a program that outputs this log n, this n bit string. Given n, the string of n zero is given n. But even much more complicated strings like the first n bits of PI, you can describe by log n plus order one bits, because there's a program that given n in binary. Given that log n, those log n bits of information is going to produce the first n bits of PI just by approximating PI well enough using standard algorithms. So there are lots of strings. The strings that we tend to use mathematics often have like small complexity and some have even much smaller.
00:20:39.060 - 00:21:10.500, Speaker B: So if you take a string of like two to the n zeros that will also have complexity around log n, so you can compress strings quite significantly using kind of rounded information in them. And Conor complexity has some very kind of basic properties. So for every string it has complexity. It moves the length of the string plus itself, because you can always describe the string by the string itself. And a program that just copies the string onto its output. So you give the string as input, and then it just copies it out to the output. So that's an upper bound complexity of any string.
00:21:10.500 - 00:21:49.736, Speaker B: And there's a simple counting argument that says that for every integer n, there's strings of complexity, at least n. That's just a straightforward counting argument, because for any I, there are at most two to the I strings of complexity I, because at most two to the I, there are two to the I programs, and each program can output at most one string. And so there are at most two to the I strings of complex di. So just summing over all these such possibilities, you have at most two to the n minus one, strings of complexity less than n. And there has to be a string of kernel complexity, at least in. Right? So congratul complexity satisfies these properties that every string has complexity at most the length, its length plus something. And strings require control complexity, at least n.
00:21:49.736 - 00:22:35.472, Speaker B: In fact, a random string requires convoluted complexity, at least n minus one. The same counting argument establishes that. Okay, so there's a sort of like this sort of incompetence theorem, which to me feels like much, much stronger than Godel's theorem and sort of more relevant to mathematical practice. And certainly the complexity theory due to chaitin. And what Chaitin showed is that, again, you take any effectively axiomatic soundproof system, there are only finitely many m numbers m, such that statements will form. The complexity of a string is at least m can be proved in a, right, so this is somehow like these statements, that the complexity of a string is at least seems somewhat more natural than the girdle sentences. So that's one benefit of this theorem.
00:22:35.472 - 00:23:14.406, Speaker B: It seems to tell us more about where incompleteness is actually manifested. And then there's a sort of like strong property that for that, after a certain m, there's not going to be any low bounds you can prove at all. Right, so suppose you're a Kolnogorov complexity theorist, and your job is to prove Kolno complexity low bounds. You'd basically be better off being a priest or mystic because you have no access to whether strings have high congregational complexity or not using any kind of effectively axiomatized proof system. So it's just a very, very strong statement about what kinds of load bounds can be shown. And it says quite a bit about complexity as well, I think. But let me just tell you what the proof of this, because it's actually fairly straightforward.
00:23:14.406 - 00:23:51.354, Speaker B: So, um, suppose, for the sake of contradiction, that they're infinitely many M, for which some statement k of x is at least misprovable in this system. So for some x, you're able to prove that k of X is at least m. For infinitely many M. Well, given any such m, you can design a machine, and what the machine does given M. So M can be described with log M bits. The machine will just sort of like run over proofs to find an x such that k of x greater than equal to M has a proof in the proof system, right? We know that for infinitely many m there exists such proofs. So for infinitely many m, you're going to end up finding such an x, and then this machine is going to output x just given these log n bits of information.
00:23:51.354 - 00:24:23.814, Speaker B: But that implies that the clone complexity of x is mostly log n plus order one, because you've produced a string x which is contained in this. I mean, the string is sort of represented in this theorem. So you've kind of produced a string using just log n plus order one bits of information. But then there's a sound system. So anything that you've proven, it is true. So k of x is also at least n, and for large enough n, this is a contradiction. Right? So it's sort of just from the definition of commonwealth complexity and the basic properties of complexity, it follows that this can't be the case.
00:24:23.814 - 00:25:03.642, Speaker B: Right, so what does it say? Okay, so this is something about Kolmogorov complexity, but what does it say about computational complexity theory? I mean, I claim that intuitively, it's sort of like bad news for us as complexity theorists, because convert complexity is really about compression. It's about saying that strings can't be compressed. And if you think about circuit complexity, that's about compression, too. Like when we take a Boolean function, take a truth table, and you want to know whether it has small circuits or not. That's also about compression. Of course, it's a much more efficient form of compression because you can recover the truth table from the circuit in time, polynomial in the size of the truth table. And the issue here is that you can't recover that.
00:25:03.642 - 00:25:48.170, Speaker B: You can recover X from his description by Turing machine, but you don't have any bound the time it takes. So it might seem that circuit complexity is somehow much simpler form of compression. And that's true. But what if you consider probability in systems that are weaker? Not piano arithmetic in general, or any rich enough theory, but theories where reasoning is bounded, because those are the kinds of theories which actually come up in complexity. When we're doing complexity arguments, we're using theories which have formal time reasoning or proof systems that are efficient. So somehow it makes sense to compare the power of the statement of the power of the proof, right? So here, I mean, the statements are saying something quite powerful. The complexity is large, but, and the proof system is rich, too.
00:25:48.170 - 00:26:23.454, Speaker B: So when you kind of, like now consider statements that are about circuit complexity, it makes sense to consider proof systems that are weaker. And maybe some such phenomenon still exists in that setting. So that's a question. Is there a phenomenon of that sort in that setting? The one thing that's positive about Shaitan's theorem is that it does tell us something. We understand that it's going to be hard to prove these lower bounds. So that's still information. It could be that that was inaccessible to us as well, that we couldn't know anything at all, but at least we're able to argue something about it, so we're able to prove these unproved results, which is already giving us some understanding of the problem.
00:26:23.454 - 00:27:05.190, Speaker B: Okay, so, so much for Chetan's theorem and what it says about complexity. So again, this is like this vicious circle phenomenon. So intuitively, we can't prove that strings have high core complexity, because they do. Right? So that's a vicious circle. And the question in this talk is, is there a vicious circle in complexity theory? So, does the existence of complexity low bounds imply that there are no efficient proofs of low bounds? And of course, I haven't really told you what efficient proofs are yet, so we need to formalize that. But at least intuitively, I hope the statement makes sense. We're trying to take these statements like Godel's theorem or Chaitan's theorem and see whether they're resource bounded versions of them and whether that tells us something about why low bounds are hard to show.
00:27:05.190 - 00:27:51.552, Speaker B: Okay, any questions so far? Okay, so I'm going to start off. So now we're going to get into, like, actual approaches to proving low bounds. There's a circuit complexity approach, which is one of the main approaches in the eighties and nineties to proving low bounds for NP to set, to attacking NP versus p. And the approach was to kind of like, consider these combinatorial models of Boolean circuits. Right? So a Boolean circuit is just like a logical network of and or gates, which you use to solve finite Boolean function. So if you have Boolean function on n input bits, it can be described by a truth table of size two to the n. So that's a very large string.
00:27:51.552 - 00:28:38.200, Speaker B: And Boolean circuit of polynomial size is a much smaller object. So intuitively, like you're asking whether these two to the n length truth tables can be compressed. That's what it means to have a small circuit, right? So it's well known that if a language is in pole time, then the finite portions of it. So if you take l, intersect zero one to the n, like n bit strings in l, then that has Boolean circuits of size polynomial and n. So that's quite a standard fact from the seventies. And therefore, to show NP normal to p, it suffices to show that there's some problem in NP that does not have polynomial size circuits, and this is of course stronger than NP in order to p. But it looks more approachable because we're now trying to analyze polynomial size circuits, which are more combinatorial.
00:28:38.200 - 00:29:24.946, Speaker B: Um, so maybe we can use kind of comptroller arguments and, um, and try to, um, to, to exploit those to get some information. Maybe like if the satisfiability problem with a clique problem, they can try to analyze the structure using comp total arguments. Um, so what the circuit complexity approach to P was NP aims to do is to make progress on this, not by directly attacking this problem, but by showing low bounds in NP for restricted circuit classes. So you take circuits and you consider restricted kinds of circuits, maybe circuits with bounded depth. You have a fixed number of levels of the circuit, and each level feeds into the next level, or a restricted class of gates. So circuits that are sort of weaker than general performance sized circuits, let's try to show the lower bounds for those first and then make progress towards NPV. So this was a program sort of like proposed.
00:29:24.946 - 00:30:02.208, Speaker B: Well, so Mike Sips are had this kind of like sort of idea of trying to use circuit complexity to show lower bounds. And then this program sort of developed naturally, and it was very successful at first. So in the 1980s, there were many, many low bounds that were shown for interesting circuit models using this approach. So for constant depth circuits, which actually are very interesting from a logical point of view too, because they correspond to expressibility in first order logic. There were low bounds shown for the parity function and constant depth circuits for monotone circuits. So you can consider monotone functions where if you switch an input from a zero to a one. The output can't go from one to zero.
00:30:02.208 - 00:30:29.872, Speaker B: That's what it means for a boolean function of a monotone. And you can ask if monotone functions are computed by monotone circuits, circuits where you don't allow. Not gates, just monotone gates, like, and gates, or. Or gates. And there were strong low bounds shown by Rasbro on that model. And then Ross, bronze Malinsky showed low bounds and constant depth circuits with gates that are, that check whether the input has number of ones that's a multiple of a prime or not. So those are Monp gates.
00:30:29.872 - 00:30:48.176, Speaker B: So there. Well, there seem to be sort of like. And there are many different techniques involved in these results. So this is a p that was rich in techniques and seemed to be making progress. But in the nineties, progress ground to a halt. And we still have no idea if NP has pornomic size constant depth circuits, even, say, with mon sex, k. So sex is not a prime.
00:30:48.176 - 00:31:30.220, Speaker B: So these techniques don't apply. But we have no idea whether this is the case or not. So we seem to be stuck. And, um, it was unclear why we were stuffed. So is there a fundamental reason for this? So does the vicious circle phenomenon somehow play a role here? I mean, is that like one of the reasons why we might be stuck on this problem? So there's this kind of really kind of cool notion of natural proofs of Rasburg rubits, which says something about this. So what did ras Brom rubic do? So Rasbron brubitsch kind of tried to think about how low bounds were shown. Typically, the way that lobance was shown at that point was to take some kind of property of boolean functions that implied hardness.
00:31:30.220 - 00:32:03.096, Speaker B: So if you had a circuit class, you want to show that a Boolean function is not in the circuit class. You define a property such that if a Boolean function has a property, then is guaranteed not to be in c. And then you construct some Boolean function with that property. It turns out that the properties you construct often have, um, like these features, that the property can be decided in polynomial time. So the property of a Boolean function, you represent Boolean functions by their truth tables, which are long, which are of size, capital n. And then you ask whether the Boolean function is a property or not. That decision problem is in polynomial time.
00:32:03.096 - 00:32:34.626, Speaker B: So that's often the case with the properties we define. There's this usefulness thing, which is that if f satisfies the property, then f doesn't have circuits in c. And it turns out the properties we define are often dense as well, that a noticeable fraction of Boolean functions satisfy them. So um, it's a case that q kind of like satisfied by, by um, say one or polynomial fraction of Boolean functions. And just for, uh, for a p for a picture. So let's say this is space of Boolean functions. Um, this would be your, your sort of like um, natural property.
00:32:34.626 - 00:33:14.614, Speaker B: Um, um, so these were the functions in C, which are just a very small portion of all Boolean functions, because c is typically some polynomial size bounded class. So there's far fewer functions that have circuits in c than random functions don't have circuits in c. And the property is sort of a subset of the hard functions, and it's quite a dense subset of the hard functions. It occupies quite a noticeable fraction of the space of all functions. And you want this property to be checkable efficiently. So it's not obvious at first why this definition is made, but it turns out that the low bounds that have been shown, that thus far all involve the construction implicitly of such properties. And that's what made this definition interesting.
00:33:14.614 - 00:34:19.524, Speaker B: So what did Rossborough do to show, so they showed first of all that standard circuit lower bounds, so lower bound proofs against restricted circuit classes all give you natural proofs against c. So they could all be, you could all extract from these low bounds, you could natural proofs which worked against this class c. And then they showed that you really couldn't hope to use this to get low bounds against general polynomial size circuits. So under this assumption, which is a standard assumption in cryptography, the assumption that they're hard one way functions, you can think of it as maybe saying that factoring is exponentially hard, or the discrete logarithm problem is exponentially hard under this assumption, which is a natural assumption in cryptography and in complexity, there are no natural proofs against polynomial size, right? So um, given there's a hardness assumption, which is natural, which implies that there are no natural proofs. So um, and, and you'll see later that this also seems like an example of vicious circle phenomenon. But let me first tell you how this proof works. I'll just sketch the proof to you, because it's, it's not hard.
00:34:19.524 - 00:35:06.161, Speaker B: So given this sort of like result of gold, rye, Goldwater and Macaulay, um, that um, like if you have hard one way functions, then there's a few random function family and um, polynomial size. So, so what does this mean? So there's, there's some class of like pseudo random functions, functions that all have small circuit complexity, such that you can't really distinguish them easily from random functions. So, um, like if you're given the truth table of a function, you can't tell whether it's sort of like a random function or not, or one of these two random functions efficiently. So that's what this lemma guarantees, given the assumption of hardness of one way functions. And once you have that, um. Well, it's very easy to see why, um, natural proofs can't exist, because a natural proof can distinguish random functions from random ones. All the c random functions are in this small portion of the space.
00:35:06.161 - 00:35:43.944, Speaker B: They all have polynomial size, while all the functions that satisfy the natural property, um, they're here and there's many of them. So there's a noticeable fraction of the space. So the proper, the, the polynomial time function that verifies q accepts a noticeable fraction of functions, but not any of the c random ones. So it's really distinguishing c random from random. And that's in contradiction to the assumption of existence of one way functions. So somehow this kind of like idea of pseudo randomness allows us to show that natural properties or natural proofs can't exist. So.
00:35:43.944 - 00:36:31.854, Speaker B: Yeah, but let's kind of step back here. I mean, it's not that important to follow the proof, but, but really it's a message that's more important. So somehow there's sort of like a kind of vicious circle phenomenon going on here too, and that we want to give like the proofs we have so far are natural proofs, but we can't give natural proofs of hardness of boolean functions against polynomial size. If, like certain complexity assumptions we believe are true, functions such as factoring are hard. So this is an assumption we believe about hardness of functions. And if this hardness assumption holds, we can't prove hardness. So it's sort of like a similar, kind of like a vicious circle in which the, the truth or the existence of hardness assumption implies that you can't give certain kinds of proofs of hardness, right? I mean, it's not so unlike in like Godel's result of Chaitan's result.
00:36:31.854 - 00:37:22.274, Speaker B: It's not clear how some sort of diagnosation argument is going on here, but it's sort of like a clue to something like a diagnosation argument under the hood of this result that there's a vicious circle phenomenon. So, yeah, so what I want to say about this is that basically this concept of natural proofs is really elegant. I mean, it kind of like, it sort of explains in many ways why we're stuck at the place we are. It provides a dividing line between circular bounds we know and once we don't, and we'd like to show. But for me, it's not like really a fundamental barrier in itself, but it's a clue that there's more fundamental barriers, because, um, it's kind of like not talking about a concept that's naturally defined. It's quite an ad hoc concept, right? It's a concept that captures the practice of complexity theory, but it doesn't feel really fundamental. So natural proofs aren't really proofs in the mathematical sense.
00:37:22.274 - 00:37:48.720, Speaker B: The proofs of mathematical sense are proofs. We write like a formal system and kind of like inferring one line from the other. That's not what natural proofs are like at all. So they don't really tell us much of the probability of lower bounds. It's not clear exactly what they tell us in terms of probability. What we like is something stronger. We like sort of complexity theoretic evidence that for all boolean functions, f, we can't show efficiently.
00:37:48.720 - 00:38:31.968, Speaker B: We can't show with a short proof that f is hard, that f requires large circuits. Right? So, and that's something that natural proofs isn't able to do for us. Any questions? Okay, so given the sort of like this goal that we have set ourselves, I need to maybe be a bit more precise about what short proofs and some reasonable formal system means. So let's say a little bit about that. What? So like, we were quite hand wavy about this. We said we had any rich in our formal system, piano arithmetic, ZFC. We didn't really say much about that, but we need to say a little bit about what efficient proof systems are, or kind of like resource boundary proof systems are.
00:38:31.968 - 00:39:05.194, Speaker B: So, one way to formalize that is using propositional proof complexity, where you study the power of propositional proof systems to prove topologies. So this is of course something that's interest to the SAT reunion program here as well. So, propositional proof system is defined very generally in this paper of Steve Cook, who's represented here, and Robert rechowed. And it's just in general, the polynomial time computer binary relation, such that there is a y. Such an out of phi y holds if only phi isn't taught. And why is this a proof system? Well, if Phi isn't taught, there is a proof. There's a Y which proves it.
00:39:05.194 - 00:39:27.508, Speaker B: So that's completeness. And if there's a proof, then Phi isn't taught. So that's soundness. So if and only if it's capturing the soundness and completeness of a proof system. And you want this proof to be polynomial time verifiable, or in the non informal case, polynomial size verifiable. Right. So this is kind of like a very abstract notion of a proof, and we're interested in whether tautologies have short proofs or not.
00:39:27.508 - 00:39:57.208, Speaker B: So given a topology of size, and we're interested in how large a proof it requires, and given proof system r. So the r proof size of phi is the smallest size of an r proof of phi, and we're interested in sequence of ratios. We're understanding how the proof size grows with the size of the formula. So how large proofs do we need for given formulas? And this is very closely related to the NP versus co NP question. NP is not able to co NP if not only if. For every propositional proof system, there are hard topologies. That's not very hard to see.
00:39:57.208 - 00:40:40.054, Speaker B: Okay, so this is related to the fundamental complex question of whether NP is equal to co NP or not. And we're interested in these propositional proof systems from mathematical point of view, in terms of what they can show about low bounds. And we're interested in particular in some specific these Freger systems, which are very natural proof systems, where the proofs consist of lines, and each line is either an axiom of the system or is derived from previous lines by the application of one or more rules of deduction. So that's a very natural way to do proofs. And the last line is fee, the formula that you prove. So that's sort of like a natural, kind of like class of systems one is interested in. And consider systems where the lines belong to some circuit classes.
00:40:40.054 - 00:41:16.942, Speaker B: So the more expressive a line is, it seems that the stronger the proof system is, but the greater its power is. And it seems that the more we can do. So Freger stands for proof systems where the lines are formulas and external Freger where the lines are circuits. And I know I haven't told you much about what the rules of reduction are. It turns out that these notions are quite robust. And for many different kinds of rules of deduction or kind of formalizations of this, these classes, Frega and eaf, are the same. And a major open problem in proof complexity is to prove low bounds for these proof systems, like how large proofs do we need for tautologies? And Frega and Ef, we don't have any sort of interesting low bounds on these.
00:41:16.942 - 00:42:05.908, Speaker B: So that's like a major low bound problem in proof complexity. And now we can kind of like formalize what it means for a function to be hard. So, given a Boolean function given by its truth stable and a size bound, we can write, if the function is hard, we can write a propositional tautology that says that for all small circuits, c doesn't compute f, right? So this is saying for all c of size s, c doesn't compute f. And it's easier to write this as a DNF of size order two to the n times poly of s, where there's like smaller Dnf's, each of which is saying that, yeah, that c of x is not equal to f of x. And you're just taking the disjunction of all of these. So if the C doesn't compute f, there's got to be an x where C of x is not equal to f of x. And there's just a big or of all of these, of these conditions.
00:42:05.908 - 00:43:06.274, Speaker B: And the propositional variables of this formula encode the circuit C. So that's how we encode that a function is hard and presented to the proof system. So you encode the topology, and then you present it to the proof system. And this is kind of like conjecture of a ralls probe, which says that if there's a function in polynomial time that's hard on average for formulas, then these kind of like truth stable formulas expressing circuit low bound tautologies don't have short proofs. So essentially, this means that Frege can't prove any circuit load bounds that are strong, right? If this complexity assumption holds, then Frege, which is a proof system in which we can formalize much of the reasoning we do, can't prove any circuit low bounds at all for any Boolean function f. And that will be a very, very strong consequence, morally much stronger than what natural proofs tells us, because it says we'll have to come up with radically new proof techniques to prove low bounds. Um, and this conjecture, what it's saying is that there's this implication from circuit complexity lobar to proof complexity lobar for Frega.
00:43:06.274 - 00:43:51.862, Speaker B: Um, and that's kind of like interesting in itself, this attempt to connect circuit complexity to proof complexity. Um, so it's sort of like an exciting conjecture. We don't yet have much progress in it, but I'd like to present it to you to advertise it, because I think it's, um, it's something that really does, uh, um, should have more work on it. Um, I mean, it'll tell us a lot about the hardness of low bounds if we can establish this conjecture. So we don't know really like much progress in this conjecture, but there are recently some other results which are unconditional, which kind of bypass this sort of conjecture. So let me just mention a couple of those to you. So, like, there's this result on this other conjecture called Root's conjecture, which talks about the hardness of a specific problem in NPC.
00:43:51.862 - 00:44:35.868, Speaker B: So rather than talking about whether NP is in size poly or not, it's sort of like talking about the hardness and average of this problem NP, the circuit sized bond for co nonterm stick circuits. So it's asking about a stronger lower bound. And in a kind of a paper with the unpicked, we proved that this is unconditionally not provable efficiently by any non form propositional proof system. So rudicous conjecture itself might seem a bit sort of like arcane to you. It's a well believed conjecture, but it might be hard to access. Why? It's interesting, but the point is that it's a believed conjecture for which you can unconsciously show that in some sense it's not proven efficiently. I haven't told you exactly in what sense, but there is a natural way, and this should be proved efficiently.
00:44:35.868 - 00:45:10.454, Speaker B: And what's interesting about proof idea is that it's really like Godel's argument or Chaitan's argument. It's like saying if the conjecture is true, then it doesn't have short proofs. Girls argument are saying if it's true, it's unprovable. And here if it's true, it doesn't have short proofs. So that's sort of like a formal kind of instantiation of the vicious circle phenomenon, that this conjecture doesn't have efficient propositional proofs. If it is true, this truth precludes that it has efficient proofs. So it's sort of like evidence, I think, that this vicious circle formula is present in complexity theory in like a deeper way.
00:45:10.454 - 00:45:52.304, Speaker B: And there's other evidence of that sort, a different way in which you can formalize proofs using fragments of piano arithmetic, which use polynomial time reasoning. And that example is like Cook, Steve Cook's theory, PV, or Sam Bus, who's here, has theory and hierarchy of theory. That's one too. And much of complexity theory can be formalized within these theories. So propositional proof systems have this property that you have a sequence of topologies and you try to prove them all independently, each one for length, and you prove with a different proof, but boundaries does things closer to practice. Like you have a fixed kind of formula formula which you're trying to prove in the system a proof is a fixed finite object. Just close it how we do, we do it in real life.
00:45:52.304 - 00:46:46.816, Speaker B: And so it's interesting to ask, well, are there also unproved results in boundary arithmetic you can get when it's kind of defined appropriately? I'm not going to say much about efficiently defined concepts, but essentially you try to model polynomial time reasoning within this theory. You're saying that you're only working with efficient concepts and not with concepts that are too rich or too complex. And there's a couple of results recently, including a very recent one of Lee and Olivera, which say that certain kind of results, strong low bounds, can't be proved unconditionally in these systems of pronoun time reasoning. And I mean, the formalization is important here too, but I'm suppressing it because I just think the message, which is that you unconditionally can't prove these results, is very interesting in itself. So I mean, sometimes we wonder, well, with complexity theory, maybe we're just missing a trick. Maybe there's some sort of like simple way to prove low balance. We just haven't found it yet.
00:46:46.816 - 00:47:12.858, Speaker B: But yeah, I think the fact that we can't prove these, these terms in PV and S one, two strongly suggests that that's not the case. It's not our fault. It's the fault of the systems we use, the proof systems we work in. So that's kind of like the message here. And these kind of like results also work through kind of like diagnosis kind of like techniques, which are also instances of vicious circle phenomenon. That strong average case low bounds are approval and boundary if they are true. So it works.
00:47:12.858 - 00:47:47.150, Speaker B: So the main lemma in these results is saying that if these low bounds hold, you can't prove them. Of course, if the low bounds don't hold, you can't prove them either because of the soundness of the system. So if the low bounds are false, they can't be proved. But the main kind of step is show that the low bounds are true, they can't be proved. And that's what gives it the untrue, Bill, unconditionally. So, yeah, so, so far, I mean, I've been like talking about this vicious circle phenomenon and it's sort of like that is a real phenomenal complexity. We need to kind of confront and work around that.
00:47:47.150 - 00:48:38.764, Speaker B: Strong circuit load bounds preclude their own proofs of restricted proof systems. But you could have a more optimistic interpretation as well, which is that what you're doing when you're proving these results is like giving new connections between circuit complexity and proof complexity, sort of saying that maybe, like, if you make progress on circuit complexity, then you make progress in proof complexity. Maybe there also results in the other direction. And if you could show such strong links, I mean, maybe there could be like a synergy between the two areas, and we'd be able to make progress much more quickly than just working on these areas in isolation. So that's a more sort of like, optimistic view. And let me also point out that these results I mentioned so far are all unconditional, and they're essentially low bounds, right? So we are saying circuit low bounds are hard, but we are showing unconditionally that some other low bounds hold. So in a sense, we are making progress on low bounds by showing these results just in a different way than we hope to do.
00:48:38.764 - 00:49:40.366, Speaker B: And perhaps this can be used, exploited to, to make progress on circuit low bounds. And let me just, like, speculate briefly on how to go beyond these techniques. So the current results and hardness of low bounds, I mean, they use diagnosition ideas. They only work for formal systems as far as comparable to the low bounds being shown. Right? If you're doing, if you're trying to prove low bounds in formal, high circuits, we look at systems that formalize formal time reasoning, and we show that they can't show these low bounds. What if you have systems that are much richer? I mean, do you have any hope of showing sort of like hardness for such systems? And let me just put this out there for those of you who might be familiar with the boreal determinacy theorem, which is an example in logic, and kind of like descriptive set theory of a statement that requires proofs that are much, much more complex than the statement. Right? So, um, there's, um, in fact, Friedman showed that the Boulevard theorem required, like, uncomfortably many iterations of the parset operation, even though it's really just a statement about sequences of numbers and sets of sequences of numbers.
00:49:40.366 - 00:50:21.846, Speaker B: And later, Martin showed, gave a proof of it, which is surprisingly not kind of extraordinarily complicated as an inductive proof, uses transfiant induction and establishes it. So this is a case where you needed proofs that are very, very complex. And it turned out that such proofs then existed. And I don't know the history of it, but maybe Martin's final proof was influenced by the unproved result that Friedman showed, the independence result that Friedman showed. And I should point out that this whole sort of, like, area of descriptive kind of set theory and connections to complexity. There have been other ideas before, like Mike SVPSA, when he was trying to prove constant dev lower bounds, was inspired by analogies of boreal hierarchy. Tim Gauss proposed an approach to P.
00:50:21.846 - 00:51:02.442, Speaker B: Was his NP based on border determinacy, which turned out not to work. I mean, Paul Putlock found a counterexample. But then this is a different sort of question. It's about showing independence, and it's about whether you can show stronger kind of unproved results using finite reversions of this result. And this was sort of like an idea suggested me by Odi Khrushchevsky, that this might be something to explore. So I'm just putting it out there in case someone's interested in exploring these directions further. So, yeah, I'm kind of almost out of time, but let me just say like a little bit, maybe about how would you actually get round? So this vicious circle is sort of like present for these very strong low bounds, but it's always for non uniform low bounds.
00:51:02.442 - 00:51:50.454, Speaker B: And could it be that uniform low bounds are just much easier? I mean, after all, to settle np versus p, we just need to show uniform low bounds, not non uniform ones. So could it be that they're much easier? And here's like one way to kind of approach uniform low bounds, and this is an algorithmic approach. So here's like a competition task that I give you like you're given as input a circuit on n variables. The circuit is a polynomial size. It accepts at least a two third fraction of its inputs. And your task is just to output some satisfying input of c, some fixed inputs of y of c. The satisfies c, this probability just slightly over 102 to the n, say n to the power four or two to the n, and you allowed to use like space, say quadratic, or some fixed polynomial space.
00:51:50.454 - 00:52:38.080, Speaker B: It's a randomized algorithm, right? So one kind of like trivial way of kind of trying to do this is just output a random assignment of length n, and that outputs any fixed assignment of the property, only one over two to the n. So it's, and we need something better than that. But that does output some fixed, satisfying assignments with high probability as well. Another way you could try to approach it is using maybe c random generators. If you had a few random generator from say square root n bits to n bits, you could output like a random output of that generator, and that would have like success. Property one over two to the square root n, and that would be good enough, but we don't know cram generators exist so here you're actually given the circuit. Um, you're not given enough space to actually run the circuit, but you can look at the circuit, and you need to output a satisfying assignment with probably just over 102 to the end.
00:52:38.080 - 00:53:23.836, Speaker B: So this is the algorithmic task that you're given, and the theorem is that if this task is solvable, then polynomial space is not good for polynomial time. So you get a uniform low bound, but it's not clear how to get a non uniform low bound from this. And it's kind of like, interesting because it gives like an algorithmic formulation of the p space normal to p problem. The p space normal to p problem is about low bounds. So it's saying that polynomial time algorithms don't exist. But here, by designing kind of an algorithm whose existence we actually believe in, because under some conditions, there's an actual equivalence between this algorithmic task and p space based model to pre, it maybe gives us a way to approach it, since as humans, we seem to be better at designing algorithms than approving low bounds. And if you're interested in this, I'll be giving a talk here tomorrow morning at ten on that in more detail.
00:53:23.836 - 00:54:06.078, Speaker B: But I just thought I present one potential way to maybe get around these kind of vicious circle results. So let me just close with some kind of thoughts about what next. I mean, so this vicious circle phenomenon probably goes much deeper than the results discovered so far. For example, the unconditional results we know are all about much stronger low bounds than NP normal polynomial size, and about NP not in conantic size, or about polynomial hierarchy being separate, and so on. So you know little about this. So it'd be interesting to see whether, for example, you can show that one way functions exist in these polynomial time bounded reasoning systems, or some of the cryptographic primitive, to kind of like, show other results along these lines. I think it hasn't been thought about that much.
00:54:06.078 - 00:55:04.714, Speaker B: I mean, there are these recent results, but I feel that it hasn't really been explored as much as it should have. I mean, as complexity theorists, I guess there's so much beautiful complexity theory, there's elementary, and where we kind of like, prove these elegant results and connections, and we might hope to settle these kind of like, open problems using those ideas, too. But that seems quite unlikely. And I think we need to think more about the metamathematics of these results. The other direction is to find low bound techniques that can be formalized with using polynomial time reasoning and extended Freger, or in bust theory, s one two, because it's quite possible that we will need these sort of like more complex proof, theoretically complex low bound techniques finally to settle these questions. So, like, are there interesting low bound techniques that have the property? And the other thing that I think needs more work is this sort of like understanding this algorithmic method for proving circuit low bounds. Right? So the previous slide, I gave one kind of algorithmic approach to uniform low bounds.
00:55:04.714 - 00:55:56.570, Speaker B: There's an older approach of Williams for proving circuit load bounds for super polynomial time classes, which was quite successful in proving new load balance approved, for example, that nontermistic quasi polynomial time doesn't have ACC zero circuit circuits with mod six gates, polynomial size circuits with mod six gates, which is quite a major step forward. But it's not really understood what the proof complexity of the result is like, what system can be formalized in what barriers are there to showing it. I mean, I'd imagine that if there's an algorithmic formation of a low bound, it's much harder to show independence results. But it's just not a question that's been explored enough, I think. And finally, I think it'll be interesting to find closer connections between circuit complexity and proof complexity for strong proof systems, because there are these kind of like two major endeavors trying to prove circuit complexity, low bounds, trying to prove proof complexity, low bounds. Both seem hard. Maybe you can kind of show that they're essentially the same question.
00:55:56.570 - 00:57:12.082, Speaker B: I mean, Christopher de Mutro has this great quote about how in complexity theory, we can't really increase the number of answers easily, but can reduce the number of questions. So this is really about that. Can we reduce the number of questions and show that these two class are very closely related? So, you know, Exp space does not happen. So when does this break down? Oh, very easily prove exp space doesn't have. So how does this break down? Yeah, so why is that? Not that we can easily prove? Yeah, so there's a crucial distinction between complexity and low bounds, where you sort of like, where your low bound is for a class that is more powerful than the low bound you're trying to show. So exponential space versus polynomial size, and these cryptographic low bounds, where like, you're trying to show like, like NP versus P for a fixed formula NP, you're trying to show any form of time algorithms. And all of these sort of like results are only relevant to this cryptographic regime, which is also the regime which we are most interested in.
00:57:12.082 - 00:57:54.904, Speaker B: Of course, from the point of view, deionization and other kind of applications, we are also interested in these exponential time goes on. But there, as far as I can tell, there are no known obstacles of this kind. It would be interesting to see if there are, but I can't really think about how these techniques can be used in such a setting. Oh, I see. Can we put it in those kind of situations? Yeah. So are you sort of suggesting like a Ross, Ross conjecture were true then for any function including function and X? So where is that? Yeah, coming in. I mean, so, yeah, I mean, so that's a good question.
00:57:54.904 - 00:58:18.788, Speaker B: I mean that's. So in a way an osmosis conjecture will be like a little more damaging in the sense that so far we have no evidence against the hardness of proving, against the easiness of proving that x plus a problem on size circuits. If you prove rasmussenjection, that would sort of like have implications for that problem as well. So that might require a bigger step. It's not clear the Roswell conjecture holes or not. So, yeah, it's not, it's not clear whether it's dividing lines. There hasn't been that much work on it.
00:58:18.788 - 00:59:12.064, Speaker B: It's a great question. Therefore mature factoring is hard. No, I mean like you can even kind of consider like circuits. I mean that's fine too, because the national cruise barrier rules out even the constructive thing, efficient circuits. So yeah, either way. So you need like randomized load bounds or. No, different bounds of course are even better.
00:59:12.064 - 01:00:26.334, Speaker B: Going back to various letter and I guess what he's doing is he's looking a fixed f and then he's making a maximum overf. So he's not like any. Yeah, so you take the max over f. So you're taking a fixed formula and first order and then you're looking at how the sort of like the number of steps grows with n where you're looking for a proof of size n. So I guess in complexity, usually we give f some input as well. But here f is fixed directly able to encode self referential sequences. That's a prerequisite.
01:00:26.334 - 01:01:26.954, Speaker B: No such theory can be simultaneously sound effectively. Is there some criteria one could assign to? So in this case you want to be able to. Yeah, so it is. And yeah, so goddess theorem is very, very broad. While I guess like in the setting you have more restrictions on the kind of reasoning you have and so on. I mean, of course one direction should make those results broader, but it's unclear. I mean, I would guess if you have very, very powerful theories, it might be very difficult to show.
