00:00:00.800 - 00:01:35.814, Speaker A: Okay, there are some homework problems with the exercise shape. They won't be necessary to follow the lecture, but they will definitely be useful to continue following. Let me start with the definition of matrix rigidity. It was defined by valiant. Let's start with the matrix. For simplicity, I'm going to take square matrices and bion matrices, and we want to reduce the rank of a to some value, some target value I so that minimum number of changes is called the rigidity of the matrix a for target rank r, and use this notation for that. It's the minimum number.
00:01:35.814 - 00:03:13.604, Speaker A: So this notation again is the number of non zero increase in a matrix, such matrices c, which we call change matrix, so that when you add the change matrix, the rank becomes at most r, so the least number of changes that you can make to the entries of a so that its rank becomes at most off. Okay, so that's rigidity. What's an easy upper bound on this number? That's essentially a trivial exercise, and it's the first exercise in the problem machine. So for all a and all er can be any matrices. For us, you can show you only need to change, you take some r by r minor and then change everything in the complementary minor, and you can make sure that the rank is at most r. So that's an easy upper bound. You know, what about lower bounds? So valuable.
00:03:13.604 - 00:04:33.673, Speaker A: And this proof is actually, we'll come back to this. If the field is infinite, then a generic x will have as much as minus r squared. Rigidity will actually give you this. And for finite fields, again, this is an exercise. And you can show for most a, it's some constant times, apart from a log factor, this is optimal. So for almost all matrices, we actually know that the rigidity is about n minus r squared. So what's the challenge here? What's the main question about matrix rigidity? So the main question is to find explicit matrices a.
00:04:33.673 - 00:06:30.732, Speaker A: So I mean, when I say explicit matrix, I really mean here an infinite family of matrices, and here I mean a sub n and epsilon and delta are some constants. I will not formally define what explicit is, but the nicest definition would be a should be polynomial time computable in some reasonable model of computation. Again, you know, I will not spend too much time there, but we will see examples where we would consider matrices to be explicit and examples where we don't consider them to be explicit enough for the purposes of complexity. So why do we want to answer a question like this? I mean, in some sense we know the exact answer essentially for almost all matrices so the motivation for posing this question by valiant comes from arithmetic complexity. So let me do that here. So the goal is to prove lower bounds on arithmetic complexity, and I'll define the model in just a second of computing the linear transformation given by the matrix a and the model is very simple. The only operation that we do is compute linear combinations.
00:06:30.732 - 00:07:46.672, Speaker A: So if you say u one and u two are some linear combinations of inputs that you already computed in one step, you can compute, sorry, let's say these are scalars, alpha one, alpha two, you can compute v, which is a linear combination of these two things. This is only operation that we do. So in order to compute this linear transformation given by a, you start with your input vector x one through xn, then take linear combination of combinations of a bunch of those, and take linear combinations of those guys and so on. Okay. And eventually you will get your y. So let's say this is, yeah, okay, so that's a circuit, the input at the bottom, and the computation goes upwards. And the size of the circuit is the number of edges in this graph, or the number of wires.
00:07:46.672 - 00:08:16.208, Speaker A: You think of these as wires. This is a plus key. Scalar multiplications are free. These are scalar multiplications, and we are only counting the linear combinations that are done. And the depth of the circuit is the length of the longest input to output path. So it is the longest chain of dependencies in this computation. That's it.
00:08:16.208 - 00:09:48.986, Speaker A: Okay, so questions. All right, so now it's easy to see that for all n by n a, you can compute this in depth order log n and size order n squared. A big complexity question, is this true to that? The big complexity question is find interesting linear transformations a that require logarithmic depth and superlinear size simultaneously, or rather superlinear size, actually. Sorry, let me rephrase this lesson. That cannot be computed by depth log n. So we don't sacrifice depth, but size. We want to be linear.
00:09:48.986 - 00:10:23.994, Speaker A: So small circuits of logarithmic depth, and we want to show that such circuits cannot compute some explanation. Linear transformation. So that's the lower bound question that we would like to solve. Linear transformations, obviously are extremely important. They come up all the time in computations. One of the interesting linear transformations, of course, is the fast Fourier transform, where you actually don't require size order n squared, but order n log n. But is that optimal? So that's one of the fundamental questions.
00:10:23.994 - 00:11:29.134, Speaker A: So, Fourier transform can be computed using n log n size and logarithmic depth. And you would like to know whether it's conceivable to have a linear time computation for the fourier transform. And if you want to disprove that, if you want to prove that it cannot be done so efficiently. One way to solve that is to address the rigidity problem. So the theorem that valiant proved, and that's the motivation here, is that if you show that the rigidity of a matrix is large enough for target rank constant fraction of n, then this linear transformation cannot be computed in log depth and linear size.
00:11:32.874 - 00:11:33.774, Speaker B: Okay.
00:11:35.634 - 00:11:40.094, Speaker C: So it's kind of a theorem reduction to depth two circuit.
00:11:40.714 - 00:11:41.162, Speaker A: Sorry.
00:11:41.218 - 00:11:47.958, Speaker C: Yeah, sort of reduction to depth two, the theorem in some sense, yes.
00:11:48.006 - 00:13:06.608, Speaker A: I mean, yeah, actually I wasn't going to prove the theorem, but intuitively the proof is some kind of a depth reduction. Now that depth reduction has become such a major thing in complexity, we can think about the proof valiant proof also as a kind of depth reduction. Basically a graph theoretical argument that uses from a previous result is if this graph, this linear circuit, is size order n, then you can identify a small number of vertices which if you pull them apart from the graph, meaning that you take those vertices and all the adjacent edges, then the graph shrinks in the sense that every input or output path that avoids those vertices is actually short. So that's kind of a depth reduction. But the number of nodes that you need to remove to shrink the depth is actually quite small and that gives us the rank bound here. So by removing a small number of vertices, and therefore you sort of route your computation through those vertices and the rest of the computation flows through very short paths. And if it is a short path, you can just expand it into a tree.
00:13:06.608 - 00:13:27.488, Speaker A: And that's the depth two circuit we are talking about. And that depth two circuit, because it's short, meaning the depth is now instead of order log n, it is some delta log n or some small enough delta. And that's how you get into the one plus delta. Okay. A depth two circuit of size into the one plus delta. And that's sort of the rigidity. Those are the changes that you are making.
00:13:27.488 - 00:14:07.924, Speaker A: That depth two circuit is the changes. So it's very fast, I don't want to get into the details, but basically the idea of the proof is to reduce the depth of the circuit by removing a small number of vertices and use those small number of vertices and the residual circuit to get a low rank approximation of the original matrix. So that's an intuition. All right. Okay. So from now onwards let's just focus on this position. Just focus on proving lower bounds on rigidity of matrices.
00:14:07.924 - 00:15:33.344, Speaker A: So what would be a good candidate for proving a lower bound? Now I'll just write here. So that's our central question and like to get as close to answering that as possible. So what would be a good candidate matrix for which you hope to prove that changing its rank to r? Let's consider a generic or general r, although eventually we are interested in this value of r, so that, you know, you will require a lot of changes to bring rank down to the matrix. Almost every matrix. Random one. Sorry, random one. Excellent.
00:15:33.344 - 00:16:48.274, Speaker A: Okay, so there are a bunch of angular matrices depending on how you. So I write this, I don't know, I'm used to writing this with the x size denoting the rows. I know a lot of people are used to writing them as columns. That's why I'm writing this explicitly. All right, now what do we want these Xi's to be? If the matrix wants to be explicit, if you want six matrix to be explicit. Okay, so one interesting choice for x I is Xi being all positive reals. Why am I choosing that? The reason I'm choosing that is if you choose, of course, if Xi are generic, everything I'm going to say is true.
00:16:48.274 - 00:18:01.844, Speaker A: But let's choose Xi to be positive real so that I can actually put numbers here. All right, so if I do that, then the interesting property is that such a random one matrix has all minus non zero, every determinant, every subdeterminant of every size of this matrix is nonzero. So it kind of looks very robust, because if you want to reduce the rank to, say r, you pretty much have to kill every r plus one by r plus one subdeterminant. And you have to do this for every value larger than r. I mean, of course. Anyway, so in some sense, a matrix that has, so these are called totally regular, these are class totally regular matrices. So these are matrices for which all submatrix squares of matrices, obviously all squares of matrices are nulls in.
00:18:01.844 - 00:19:22.794, Speaker A: So this looks like as robust as you can get. All right, so what can we say about matrices that are totally regular? So now I want to prove, it's actually, let me not, it's actually easy to arrive at the theorem by just looking at this problem. So if I have this matrix where every r plus one by matrix a, if I want to hit every r plus one by r plus one sub matrix, I need to choose n of. So the question then is this. And that's how many changes do I need just combinatorial argument. I'm not doing any algebra or linear algebra here, how many changes do we need to hit every r plus one by r plus one submatrix? That's the question. So that clearly gives us a lower bound on rigidity.
00:19:22.794 - 00:20:10.566, Speaker A: Correct. If you keep this diagonal, you don't, I mean, if r is small enough, then many sub matrices, let's say r is n over 100, many little matrices. Yeah, but I mean, there will be many small squares of matrices all over the place. And these are combinatorial sub matrices, right? Yeah, yeah, yeah. I mean, yeah, these are just, I mean, when I say half of plus one sub matrix, I am picking some I, one through I, j, one through j, r and some I, one through.
00:20:10.630 - 00:20:12.314, Speaker C: J, one through j, r plus one.
00:20:12.734 - 00:21:04.044, Speaker A: R plus one, sorry, r plus one. And I'm looking at the intersections of those rows and columns. Let's just submit the six. All right. So the way we're going to answer this is to actually use, it's a combinatorial problem. So we are going to think about a, I mean, the increase of a, or, you know, as a bipartite graph, or rather coordinates of a. So we are going to look at a bipartite graph with vertices one through n, one through nature, and you put an edge between I and j, if that's an unchanged entry.
00:21:04.044 - 00:22:13.694, Speaker A: So if you changed s entries, this graph, let me call this gap g, has n squared as edges. And now we are asking how many edges and what is a square submatrix? A squared submatrix corresponds to a bunch of, I mean, r plus one vertices here and r plus one vertices here. And if it is not changed, this will be a complete bipartite subgraph, no changes in that. Minus r plus one by r plus one submatrix means you have a complete bipartite graph. So let me use a symbol here. Let's call this m. So how large or how many edges do we need? Or rather the other way around.
00:22:13.694 - 00:23:20.702, Speaker A: So what's the maximum number of edges in g to avoid n? R plus one by r plus one complete. Piper. So this is a well studied question. It's called sir e with problem. Well, in general, not just, yeah, I mean, u, v. Complete bipartite graph. And there is an upper bound known on the number of edges that a graph can have while avoiding such complete bipartite graphs.
00:23:20.702 - 00:24:12.636, Speaker A: So the maximum number of edges in a bipartite graph, when you forbid an r by r, r plus one by r plus one compared by concrete, and that's a known quantity. And so this is at most, right, is this number, and that number has a formula. Well, an expression of the bound that we don't have to worry too much about. But I'll write it down. We're talking about an r plus one by r plus one. So it will be something like r to the one over r plus one. This is actually not so critical for the computations, but I'll write it.
00:24:12.636 - 00:24:37.332, Speaker A: N minus r. This is critical times n to the one minus. So, roughly speaking, it's nice. So you have n squared, right? Potential edges. So if you sort of think about this as not too big compared to fraction of n. So this is like something like n to the two. Right.
00:24:37.332 - 00:24:46.144, Speaker A: N to the two minus one over r plus one. That one over r plus one is the loss in the exponent. That's it. And then there is some.
00:24:48.604 - 00:24:50.698, Speaker C: What does that say? R to the. What?
00:24:50.786 - 00:25:22.674, Speaker A: R to the one over r plus one. So it is really a small quantity. So, if you just use this bound bound, you will conclude that s is at least. Well, I mean, you have to put some restrictions on r so that, you know, these calculations make sense. But I will just omit all of those and say something. What? Sorry.
00:25:23.014 - 00:25:26.838, Speaker C: I mean, we're interested when r is equal to epsilon times n, right?
00:25:26.886 - 00:25:47.014, Speaker A: Yeah, yeah, yeah. That would be fine. Yeah. Well, that will be fine in the sense that will not violate these calculations. But it will be uninteresting. Sorry. R some, you know, some large enough r, but not too large.
00:25:47.014 - 00:26:10.594, Speaker A: Essentially, you get omega n squared divided by r and the logarithmic factor. Okay, so. So you go to some combinatorial problems, right?
00:26:11.174 - 00:26:18.634, Speaker B: In the matrix, those entries that are changed correspond to Piper diagram.
00:26:19.934 - 00:26:28.126, Speaker A: Unchanged entries. Yeah, unchanged entries. So you want to this rank condition.
00:26:28.190 - 00:26:30.774, Speaker B: How does it translate complete Piper diagram?
00:26:30.854 - 00:26:41.742, Speaker A: What do you do recently? Oh, you want to. You want to hit every r plus one by r plus one sub matrix? Or in other words, that's already a.
00:26:41.758 - 00:26:44.314, Speaker B: Different question, because hitting is not.
00:26:45.294 - 00:26:53.326, Speaker A: Oh, yeah, yeah. That's not. Okay, so that's not sufficient. That's necessary. Yes. Okay, I see.
00:26:53.430 - 00:27:02.732, Speaker B: So this is like the combinatorial problem that you are now studying. The question of the.
00:27:02.878 - 00:27:12.248, Speaker A: No, no. I mean, it's not. I mean, otherwise we would have gotten about the bond. Yes. It's not, unfortunately. So. Yeah, so.
00:27:12.248 - 00:27:59.792, Speaker A: Exactly. So all I'm doing in this board is try to estimate a lower bound on the number of entries that you need to change in order to have at least one change in every r plus one by r plus one sub matrix. I'm sorry. No, no. Absolutely no probabilities. Not leave, actually, no, not leave. You want to not have any r plus one by, that's forbidden.
00:27:59.792 - 00:28:02.044, Speaker A: R plus one by one is forbidden.
00:28:03.304 - 00:28:34.344, Speaker C: Also, the question relates to this graph. So is the left hand side, is that like the column index, and the other is the growing x related to, is that the relation? You're starting with the complete graph n and complete bipartite graph. And you want to call them stars before, like how is that graph related to this? Plus you want to kill, you want to touch each size r plus one minus. So you want to kill enough edges so that every r plus one minor is touched.
00:28:36.844 - 00:29:30.164, Speaker A: Let's say the left hand side. The left hand side vertices correspond to rows and the right hand side vertices correspond to columns. Right. And now if there is no change in entry ij, you put an edge between vertex I and vertex j. Okay, so every non change corresponds to an edge. And since we don't want to leave any r plus one by r plus one sub matrix untouched, you don't want any r plus one by r plus one complete bipartite graph in this graph. But if you insist on that condition, in a complete bipartite graph, it cannot have too many edges, and this number gives you the maximum number of edges that it can have.
00:29:30.164 - 00:30:13.198, Speaker A: And therefore that number has to be at least the number of non changes you have. N squared over r seems trivial. Yes. So the gain here is like n squared over r for these matrices. If you use a probabilistic argument, yes, for some specific matrices, but for arbitrary, totally regular matrices, n squared over r probably. Okay, you can just divide it. There are columns that shows n minus r over r rows.
00:30:13.198 - 00:30:34.054, Speaker A: Yes. Yeah, yeah, yeah. Actually you're right. I mean, n squared over r. In fact, one of the homework problems is that n squared over r is actually very easy to prove for most matrices. But this is the best known. So I want to, yeah, and in some sense these are the matrices for which you expect almost max model is given.
00:30:34.054 - 00:31:25.934, Speaker A: Is it a random Vandermonde matrix? No. In fact. Yeah, that was going to be the open question that I was going to mention again. Yes. No, it's not. So, okay, what's the conclusion here? So, conclusion is for these matrices, the rigidity is omega n squared over r log n over r, essentially. So you kind of put conditions.
00:31:25.934 - 00:32:05.914, Speaker A: So for a reasonable range of target rank r, you get that bound. So that bound is far from what we want. But unfortunately that's the best we know so far for specific matrices. And there are many such matrices. So here is one example and there are a couple more examples in the homework. For example, the Cauchy matrix. Cauchy matrix.
00:32:08.854 - 00:32:11.834, Speaker C: Why do you require r to be less than n over two?
00:32:12.334 - 00:32:25.622, Speaker A: Oh, I'm just approximating, rather estimating n times n over r. I have something like n times n over r and I'm going to say that's n score over some two epsilon.
00:32:25.798 - 00:32:30.622, Speaker C: No, but r is epsilon times n for any epsilon less than one. Don't you get the same?
00:32:30.718 - 00:33:15.838, Speaker A: Yeah, yeah, sure, sure. I mean, yeah. I mean, I could here one minus epsilon constant. And the most interesting matrices from the point of view of explicitness, and also from the point of view of applications, are matrices or finite fields and derived from error correcting codes. And again, I will not spend time on that. One of the homework problems is that so? Because this is the best result known so far on finite fields. Let me just state that.
00:33:15.838 - 00:34:38.043, Speaker A: So now consider a finite field of q and the generator matrix of an error correcting code is a k by n matrix. In coding theory, they write vectors as row vectors. Stick to that notation. So the code given by g is u transpose g. So the row space of this, so this is clearly a subspace, and the distance, the minimum distance of the code is the minimum weight of a code word. What this means is that in this subspace, if you take any two vectors c one and c two, they differ in at least this many. So let's call this d this many coordinates.
00:34:38.043 - 00:35:47.984, Speaker A: Let's maybe many people know this already. So such a code is called mk. So this is d mkd code. And using algebraic geometry, algebraic geometry codes achieve codes of the kind n n over two is a highly non trivial result. Cos, okay, and now by converting g into standard form, I can write g as identity followed by matrix a. And if such a matrix g has good distance such as this, then you can prove that the matrix a has all, almost the kind of property that we have for totally regular matrices. In other words.
00:35:50.364 - 00:35:52.708, Speaker C: Sorry, what is nk.
00:35:52.756 - 00:37:05.180, Speaker A: And d again, k is this dimension, k is the message length, n is the code word length, b is the minimum distance. So a has the property that every two t by two t sub matrix has rank, at least t for t greater than this d plus one. Okay, so this matrix, I want a square matrix. So error correcting codes, good. Error correcting codes give you good matrices, these sort of totally regular kind of matrices. Not exactly, because the rank is not full, but constant fraction of the full. But that's still okay.
00:37:05.180 - 00:37:44.072, Speaker A: The same arguments will work and you can prove that. And therefore the conclusion is that there are explicit matrices, namely matrices, coming from these kinds of error correcting codes for which we can prove rigidity. Lower bound of this. So in particular, a generic matrix is totally regular. Yes. We don't, it's much worse. It's much worse because there is an.
00:37:44.088 - 00:37:46.604, Speaker C: R in the denominator r equal to epsilon n.
00:37:48.944 - 00:37:53.084, Speaker A: Yeah. If r is epsilon n, this is trivial. I mean, this is linear.
00:37:53.904 - 00:37:55.604, Speaker C: It doesn't say a whole lot.
00:37:57.984 - 00:38:42.164, Speaker A: About the, that's the interesting values of target angle. Sorry, I was wondering about the definition of distance. Is that. Yeah. So formally, if the minimum number of, sorry. The minimum weight of a vector in the row space, nonzero vector in this row space, the weight being the number of nonzero coordinates, but that's also equal because this is a vector space to the distance, meaning the number of coordinates in which they differ between any pair of vectors in this space. And that's why it's called the minimum distance.
00:38:42.164 - 00:38:54.904, Speaker A: If you take any two vectors in this subspace, they differ in at least d many coordinates. All right, so maybe another question.
00:38:57.524 - 00:38:59.332, Speaker B: What assumptions do we need?
00:38:59.508 - 00:39:45.354, Speaker A: Oh, yeah, yeah. I mean, there are, there are some standard assumptions. X I and y j should be distinct and xi plus yj should not be zero. For any other than that, you don't need anything. All right, so for finite fields, that's unfortunately the end of the story. We don't know anything better than that. And that's kind of, those are very nice matrices, but the bound is quite uninteresting.
00:39:45.354 - 00:40:51.014, Speaker A: Okay. All right, so now what do we do? I mean, where are we failing? Right. So one thing that is relatively easy to check and well known is that you cannot improve this number just by combinatorial arguments. So if you want to hit every r plus one by r plus one. Submitted the bound that I wrote down is optimal. But actually there is an even worse conclusion, which is due to valiant himself. So there exists totally regular matrices.
00:40:51.014 - 00:42:46.554, Speaker A: A can be computed in log depth and linear size, which is in some sense bad news if you want to prove lower bounds, but good news when you want to compute linear transformations. In particular, this means that the rigidity of those matrices is small. So total regularity, this condition, which we thought is quite natural and nice to prove, rigidity, lower bonds, actually turns out to be too weak to prove the kinds of lower bounds that we need for valiant square theorem. I will not prove this theorem, but again, it follows from a graph theoretic construction, follows from super concentrators, which are some interesting graphs constructed from expanded graphs. Again, I won't go into that super concentrator. So, linear size and logarithmic depth, or actually, I should say function. Also from here to computing, this is actually resulting to spasm.
00:42:46.554 - 00:43:47.484, Speaker A: Okay, so this condition of a matrix being totally regular is actually not good enough. And I don't know any other property that, that we could sort of use for a whole class of matrices to give explicit matrices like this, like code matrix, or any of these matrices on rigidity. So what I'm going to do now is to consider results where we don't get lower bounds. For explicit matrices, we get stronger bounds, but we sacrifice on explicitness. And we use more algebra than, you know, what we have. We haven't done anything in algebra so far. So then, simple combinatorial arguments.
00:43:47.484 - 00:44:47.196, Speaker A: So it's almost time for the breakdown. So let's take a break. And then when we come back after a short break, we look at some algebraic arguments. Okay, we reconvene in three minutes. Such matrices so xi are now generated. Now switching to complex numbers. Yeah.
00:44:47.196 - 00:45:52.104, Speaker A: I mean, of course you would expect such matrices to have highest possible, or at least almost as high availability as highest possible, but we don't know how to prove. So that's actually one of the big open questions. And let me say generic. So, yeah, unfortunately, even such a basic question, we don't know. We don't know the answer. So what I will do now is prove a much weaker theorem, and you will realize that the proof is actually quite simple. It's not.
00:45:52.104 - 00:46:06.684, Speaker A: Okay, let me just say square over four for small amounts.
00:46:13.464 - 00:46:20.992, Speaker C: Okay, so constant pick once and for all, the epsilon.
00:46:21.168 - 00:46:21.928, Speaker A: Yeah, yeah.
00:46:22.016 - 00:46:25.484, Speaker C: Okay. Just smaller of this.
00:46:25.864 - 00:46:28.484, Speaker A: Yeah, smaller of constant epsilon. Yes.
00:46:31.644 - 00:46:33.664, Speaker C: Small enough something, yeah.
00:46:34.164 - 00:47:40.904, Speaker A: All right. Okay. So at least asymptotically for small enough rank, we know that it has quadratic rigidity. And the way we prove this is to, I will sketch the proof, because I'm probably going to run out of time to do everything I want to do is a notion that SHU have defined in the context of proving circuit load bounds. But that turns out to be useful also to derive rigidity, lower bounds, at least in some limited sense. And this definition, for this definition, we will just think about sequences of numbers, complex numbers, and running out of symbols. So let me just actually, let me slide.
00:47:40.904 - 00:48:52.514, Speaker A: Let me mix this notation and just say a. So you're given a sequence of complex numbers, m complex numbers, and in our case, a will be a matrix, and we just write the entries of the matrix in some order. Okay, so that's why it's okay to do this. And now what we are going to do is consider, take some distinct t elements of the sequence, multiply them, that's a complex number. Think of that as a vector over the rationals, and then look at the Spanish. Okay, so let's put a symbol there. So some d, a t is the dimension for rationals.
00:48:52.514 - 00:50:19.644, Speaker A: A j, one, two, t, I want to distinct. So it's just the number field dimension of the number field generated by this theta. Okay, now why is this interesting? So one little lemma that you can easily prove is if a, let's say, now I'm thinking of it as a complex matrix. It has rank r. Then this quantity, it's very easy to prove, is at most when r, sorry, this is not, this is t. R is the rank square. Well, the proof is actually, it's easy to see.
00:50:19.644 - 00:51:08.712, Speaker A: Let me just, so since a is of rank r, you can write that as an n by r and an r by n matrix. And therefore, essentially these aijs are inner products between rows here and columns here, which are, there are only nr such variables here and nr such variables here. And so if you write these tuples in terms of as linear combinations of tuples from this matrix and this matrix, you will get that most pattern. It's a very simple counting argument. So that's it. So low rank matrices have a small shub smolensky dimension. That's the main point.
00:51:08.712 - 00:51:42.584, Speaker A: And now it's very easy to prove this. So how do we prove this? Suppose c is the change matrix then. Yeah, yeah. So these are just the entries of the matrix written in some order.
00:51:43.314 - 00:51:44.602, Speaker C: M equals n squared.
00:51:44.698 - 00:52:40.434, Speaker A: Yeah, this m is my, yeah. Size of c is. And so we have, we know two things. Well actually we know at least one thing, that uh, the dimension, this schoop dimension of v minus c. So, so rank of v minus c or v plus c is at most r. So this guy we know is at most, now we want to prove a lower bound on this using the genericity of the van DER montage matrix. So now in this matrix we made a bunch of changes.
00:52:40.434 - 00:53:19.622, Speaker A: Actually, let me call this s so that I don't have to write the whole thing. Yes, and so, change matrix that achieves this, and s is the utility. Okay, so there are s changes that I made into this matrix. And therefore, by simple averaging argument, n over two rows have at most two. So the average number of changes per row is s n. S is the total number of changes. The average number is s o n.
00:53:19.622 - 00:54:33.084, Speaker A: So if you take twice the average you have at least. Sorry, no good. Yeah. All right. So now I'll just ignore the elements that have been changed now, I still have n minus two s over n powers. Elements are unchanged in these rows, each of these rows, right. And so if I just take one from each of those n over two rows, one of the powers of xr, they will all be linearly independent over q.
00:54:33.084 - 00:55:58.532, Speaker A: So what we know is, I'll write n over two now is at least that many. You take any one of the unchanged entries in one row and any one of the unchanged entries in the next row and so on, and you can do this up to n over two and all these monomials are going to be linearly independent. That's it. So you just compare. So here you put t equals n over two. So d v minus c n over over two is at most, if I put that n over two, n over two squared. And if you just compare, you get s is at least n minus mc r square two identical.
00:55:58.532 - 00:56:59.898, Speaker A: So this is some constant size on how you approximate this. And you know, anyway, so basically it is n times n minus constant times r squared. And that squared is what's killing us, sort of comes from this. And I don't see how. Anyway, so that's kind of all we know about generic van r one matrices. Actually, we know a little more. But I will come back to some explicit small dimensional, other varieties, which you can prove that it has high current, high rigidity rank.
00:56:59.898 - 00:57:44.014, Speaker A: Yeah, yeah, we, not various, but we do know some special. In fact, I'm going to write one next. So using the same, essentially the same ideas. The problem here is that, the problem here is that you need to, because in each row you have power of the same variable. You cannot take, you cannot mix elements from the same row. So you can generalize this argument, and I will not prove it. Proof idea is very similar to this, but you will do better than this because you have more freedom in choosing these tuples and that.
00:57:44.014 - 00:59:37.920, Speaker A: So let p be this matrix, square root of p I, j, p I, j are distinct primes. Then you can show that something like this. So this definitely meets valiant criterion, a very similar argument. Now, you know, earlier I could not take t to be larger than n because I need to pick one element from each row. But now this matrix is sort of, you know, very random looking and there is no, you're just throwing a lot of quadratic irrational numbers into the matrix and that's really all you're doing. And the same argument allows you now to choose larger t, and if you choose larger t and using similar computations, you can get better lower bound on the rigidity and you actually get a quadratic lower bound for r epsilon nature. Does it apply quadratic complexity or not? It does not.
00:59:37.920 - 01:00:11.494, Speaker A: Quadratic super linear. Yeah, actually it does also imply nearly quadratic. You can actually derive n squared over log and lower bounds for this matrix, but not through rigidity criteria. So there is another path to proving lower bounds on linear circuits, not going through rigidity, but directly applying these kinds of dimensional arguments to the circuits. And then you get n squared over log n. But, yeah, so you do get such a lower bounds, but you also need to take it for that lower bounds. Yes.
01:00:11.494 - 01:00:14.026, Speaker A: What if you don't take square root?
01:00:14.210 - 01:00:15.854, Speaker C: What if you just put the prime?
01:00:17.594 - 01:01:25.590, Speaker A: Well, at least in that case, this proof doesn't work. This proof definitely doesn't work, but I don't know if there is a way to prove this theory. So I mean, you can play with this a little more and get, you know, interesting or, you know, slightly stronger lower bounds. But let me stop here and move to algebra geometry, where you can do even better. But then, you know, every time you do a better lower bound, you get a better lower bound with rigidity. About rigidity, you lose something like here, I mean, you lost the explicitness because these are not rational numbers and these are, you know, you really need them to be rational numbers for the whole group to work. And this is why it is not as satisfactory as you would like.
01:01:25.590 - 01:02:18.744, Speaker A: I mean, as interesting as we would like from a circuit complexity point of view. Yeah, I mean, yeah, so if you can prove that, if you truncate, something continues to hold, I don't know what that something is, and rigidity can be proved, that would be fantastic. But I don't know how to do that. I mean, I don't know how to get some kind of a rational approximation to this proof or the arguments in this proof. Yeah, it will be very interesting. Yeah, I mean, we would, ideally we would like rational approximations up to polynomial precision, but forget polynomial precision. Any finite precision, rational approximations to these matrices can prove that they are rigid.
01:02:18.744 - 01:03:06.604, Speaker A: I mean, we believe they are, but I don't know how to prove it. I don't know a technique, how an approach, how can prove that. All right, so now since going to run out of time, actually, I should have left here anyway. So let me now look at rigidity from an algebraic geometry perspective. And what I'm going to say will be very elementary. And there are better experts in the audience on algebraic geometry than me, and there are actually much stronger results than what I'm going to say that have been proved. So I will just give a brief introduction.
01:03:06.604 - 01:05:21.944, Speaker A: So, all right, so now let's, we know very little, this is more like an open direction than explaining what we know. Okay, first of all, the easier thing to look at is the variety of matrices. Actually, sorry, actually, no, that's a well studied, or depending on who you talk to, studied object. And it's really a variety defined by the ideal inr where I and r minus of an n by n matrix. Okay, so we know about, I mean, how to define low rank matrices in algebraic geometry language. Now let's try to do this for rigidity, and that's not hard to do. So let's use rig nr s to be the matrices a complex matrices such that the rigidity of a for target n r is at most s.
01:05:21.944 - 01:07:26.644, Speaker A: Okay, so when these are matrix in this set, if a can be written, so if you take a, so there exists a change matrix a such that rank of a minus c is at most of. So that is why you put it here. And what is c? C is c has at most s non zero increase. So this also we know, let me write that as l because it's a linear space ln PI to be the set of all matrices c such that non zero into is support non zero increase of c or index my PI, where PI is a subset of coordinates of increase and, well, okay, so that's it. So a is in this set. If you know it, there exists a c. Okay, so it's easy to understand matrices of small rigidity in terms of these two varieties.
01:07:26.644 - 01:08:21.254, Speaker A: And for, so let me, okay, so let me just, for notational convenience, let me, in fact, it's actually useful where I rely on the notation should be obvious. Okay. All right. So by making changes in the pattern PI, you're able to reduce the rank to r. Okay, so now this is easy to understand. What is it? You took the matrix a, and if you think about this entry here, is rather the variety. I'm sorry.
01:08:21.254 - 01:09:05.384, Speaker A: Oh my goodness. Okay. Okay. You took a matrix here and when you substitute a matrix from here, you got into a rank. So if you consider a matrix x r, and if you consider set of extra variables indexed by PI, so the minors r plus one by r plus one minus of x plus t PI are zero.
01:09:06.244 - 01:09:08.264, Speaker C: I think you mixed notation there.
01:09:10.844 - 01:09:11.180, Speaker A: A.
01:09:11.212 - 01:09:14.308, Speaker C: Plus t PI, right. And a plus c. Really?
01:09:14.356 - 01:09:18.404, Speaker A: Okay. Yeah, yeah, yeah.
01:09:18.444 - 01:09:20.708, Speaker C: A plus t PI is equal to.
01:09:20.756 - 01:09:25.433, Speaker A: X study a plus.
01:09:26.853 - 01:09:34.353, Speaker C: You're mixing notation. Yeah, I am, because the minors of r plus one minors of a are zero, not a plus t. PI.
01:09:34.813 - 01:09:37.813, Speaker A: No, not a plus c. Actually, a.
01:09:37.853 - 01:09:41.073, Speaker C: Is not, yeah, a is not in there. Right.
01:09:46.053 - 01:11:09.080, Speaker A: So, okay, so since actually running out of time, I'm trying to compress, let me just say this. So the way to think about the Zariski closure of this guy, okay, so this is what we want to understand, right? Is you take the ideal and then you eliminate, so this idea is on all n squared variables. What I was trying to say is eliminate variables indexed by PI out of this, and then you get an elimination idea. And the variables here are variables that are outside this pattern. And the main claim is that it's actually, yeah, it's not. So rig nr PI closure of that is the variety defined by this elevation I did. Intuitively, this should be very clear.
01:11:09.080 - 01:13:05.534, Speaker A: Okay, so what we are doing is when you take a matrix in the variety defined of the zski closure of this, the difference between that and the rank r matrix is exactly the matrix whose support is PI, okay? And if you eliminate the variables in PI, PI is the one that, I mean the matrix c is the one that's existentially quantified. So in some sense, in order to understand the matrices for which there exists a c satisfying this property, you would like to eliminate the variables corresponding to c and get an algebraic condition on the entries of it. And that's exactly what we are doing. Okay? So for every pattern PI, if you eliminate the corresponding variables, you got a whole bunch of elimination ideals for different patterns. And the varieties corresponding to these different patterns, the elimination of these different patterns are exactly the z closure of these variability varieties, okay? And in fact, they are irreducible components of the negative variant of the ziplogs. So that's kind of the geometric picture. Now what do we do with this? So one thing that we can trivially do is try to understand what kinds of polynomials live in this, because if you remember the previous argument, if you know something about the degree of polynomials on which a matrix should vanish, if it has small rigidity, then you could construct matrices which don't satisfy with satisfy such a polynomial equation.
01:13:05.534 - 01:13:35.342, Speaker A: So one approach would be to try to get degree upper bounds on polynomials in the elimination ideals of these flow rank matrices. Is there anything then about how the pattern corresponds to what dimension, what the dimension of the image is? I mean, as long as the size of the pattern is the same, it would be the same.
01:13:35.398 - 01:13:50.770, Speaker C: No, depends on how PI the PI is there's a generic dimension, but you have, say, more than n minus one changes in a row. If you have n minus one changes in a row, that the dimension drops.
01:13:50.802 - 01:13:53.762, Speaker B: For example, n minus r. N minus.
01:13:53.778 - 01:14:07.854, Speaker C: R. N minus r. Yeah. So, but for quote most, there is a basic dimension that it can drop actually right now.
01:14:09.254 - 01:14:17.230, Speaker A: Right? Yeah, but I don't know if there is a, I mean, if there is a systematic way to explain that drop.
01:14:17.342 - 01:14:34.462, Speaker C: Yeah, there's a theorem called terraccini's lemma, right? That enables you to calculate this via infinitesimally. And you just look at the intersections and they're non generic, then dimensional draw, right?
01:14:34.518 - 01:15:34.790, Speaker A: Okay. Yes. Okay. So one thing that's well known already is some very exponentially large upper bounds. The effective measurement shows that I will not state the theorem. There are several results with various conditions. And there is, so there are several results.
01:15:34.790 - 01:17:21.164, Speaker A: But instead of stating the theorem, let me just say what it implies for us. All it implies is that there exists a polynomial g in the elimination ideal degree of g is at most very bad n to the two n square or four n squared. Let me put a constant, some small constant c. Okay, so once you have such a degree bound, then you can construct matrices which don't satisfy polynomials of that degree. And so if you construct a matrix a which has roots of unity. Let me use something else, jk. So is the primitive, are primes that are all bigger than that number, then this matrix.
01:17:21.164 - 01:18:31.978, Speaker A: Yeah. So you have to use a bunch of parameters here, which I didn't specify. But you know, the main thing that you need to argue here is that the size of PI is, I mean, then the pattern is at most n minus. Rather, actually, let me just write exactly. So, one less than the maximum rigidity. So if you consider the rigidity varieties for one less than maximum rigidity, then the elimination ideals will have polynomials of degree at most that, and then you can construct matrices that don't satisfy such polynomials. And that will give us that, okay, this is, this gives you some sort of a finite bound on the algebraic degree of elements in the matrix, but it's still not interesting for the same reasons.
01:18:31.978 - 01:19:18.942, Speaker A: In fact, even worse than the previous matrix is because of this IDP. So, since I'm almost out of time, actually, I wanted to say something about the more recent paper that uses much more sophisticated algebraic geometry. But all four authors are here. We can get a lot more accurate and a lot more intuitive description. Let me know if I'm spelling your names right. I don't get offended.
01:19:18.998 - 01:19:19.914, Speaker B: It's okay.
01:19:23.604 - 01:19:26.124, Speaker C: Since the authors are here, you don't need to spell their names.
01:19:26.204 - 01:20:05.228, Speaker A: Yeah. Okay. All right. Now these names I know. Okay. So they actually take a deeper look at the algebraic geometry of the rigidity varieties. And like JM was saying, you know, much more refined view of these varieties and several, several properties, general properties of the polynomials or generators of these ideals are given.
01:20:05.228 - 01:20:49.094, Speaker A: And some of the conclusions, interesting, concrete conclusions I will, is that Cauchy matrix and generic wendroman matrix have maximum rigidity. There are many more results. I'm just stating these as concrete lower bounds on rigidity. Rigidity. Maximal rigidity for r equals one. For r equals n minus one, it's trivial. But r equals n minus two it is not.
01:20:49.094 - 01:21:56.340, Speaker A: So for generic 101 matrix, we also know that for n minus two, you have maximal rigidity. Okay. Yeah. Unfortunately, I can't say much more about it, partly because I don't understand it, but partly because I'm running out of time. But let me state an open question. Given all these crazy matrices that we have seen in the last few minutes, which seem to have very high rigidity, beating the bound that variant required for this criterion, but somehow we are saying they are uninteresting. And how do we formalize the uninterestingness of these matrices? And how do we sort of go, you know, somewhere midway between uninteresting and interesting, right? So I already said something about generic.
01:21:56.340 - 01:23:30.922, Speaker A: Maybe that's one of the open questions I was going to say. And another open question is construct matrices of rigidity, say n to the one plus delta for a target rank epsilon n in poly dimensional number fields. So we are not asking for explicit in the sense of exact deterministic or polynomial time computability, but just put the increase of your matrix in a number field of polynomial dimension. So if this is exponential dimension, we know how to do it. If it's exponential in n squared, we know how to do it. So I don't even know how to do it in exponential n. But polydimensional would be very interesting then in some regular sense, for, I don't know, non computational sense, these could be considered interesting matrices because then you have in some sense, you know, when do we, when do we want to think of a matrix being explicit? We want to have a polynomial size representation of that matrix, polynomial size representation on a finite computer.
01:23:30.922 - 01:24:40.528, Speaker A: And if the number field is a polynomial dimension, then you can represent it. The entries can be represented polynomial size. So that's the reason why think this would be interesting. So as a candidate for this, you know, one of the matrices that I would like to think of is this, which is sort of, you know, again, you know, one to one matrix, but not very generic and not very explicit either, where these omega is pth through p I, primitive roots of unity and PI are the first or polynomial primes. Okay, first n primes, let's say. I mean, that should not be too big. So that's a, I mean, I think this is sort of, you know, ideally what we would like to do really is if all these omega.
01:24:40.528 - 01:25:23.008, Speaker A: So the Fourier transform matrix. Fourier transform matrix, as I said, is one of the most interesting matrices for which you should try to prove rigidity lower bound, because that would imply that the Fourier transform cannot be computed in linear time. There you have just one root of unity, primitive root of unity, and the, the other omegas are just powers of it. So, ander, one matrix, but generated by powers of the same primitive rule of unity. So now, I didn't write it, I erased it. But if you throw in large roots of unity all over the matrix, we know that it has large rigidity. And so this is sort of intermediate.
01:25:23.008 - 01:25:29.586, Speaker A: And this also belongs to the class of and one matrices, which we would like to prove rigidity for the degree.
01:25:29.610 - 01:25:32.066, Speaker B: Of the field extension will be something like rupee.
01:25:32.090 - 01:25:54.614, Speaker A: Exponential. It will still be exponential. Yeah. Yeah. So, yeah, so that's why, you know, this is a question. I would comment this is sort of a candidate matrix, but, you know, any matrix which is in exponent less than exponential n squared, normal field, I think, would be progressive. Polynomial dimension, I think, would be really, really interesting.
01:25:54.614 - 01:27:18.184, Speaker A: I don't know if it has any implications to circuit complexity, but it's a clean mathematical question. All right, I will stop there. Oh, good question. So actually, tensor rigidity was defined and studied, but I know only one paper doing that, and I don't quite remember the results. They are, of course, considerably weaker than what we know for rigidity, and they do have consequences to complexity, and that's reason why they were defined. And at least in the context that they were defined, they wanted rigidity over g of two, which is actually the most difficult field you would want to provo when you could think of proving lower down some rigidity. So, yeah, over gf two, we have no idea the finite fields that we were talking about, where we had even the un squared overload and squared over r kind of lower bound, or for finite fields of some small constant, but not maybe 49 or so.
01:27:18.184 - 01:27:20.740, Speaker A: Yes.
01:27:20.812 - 01:27:34.924, Speaker C: So how low would the degree of this g have to be to have consequences for complexity of theory, like if it was brought down to polynomial. And. Nice.
01:27:34.994 - 01:28:03.064, Speaker A: Yeah. So I think if you brought it down to polynomial, this question could be answered, but I don't know if it has any direct consequences, at least not to my knowledge. Circuit complexity, it may be possible. I haven't worked on deriving a consequence, but I have a feeling that it might be possible to derive consequences. Dysregm toxic.
01:28:05.524 - 01:28:42.960, Speaker B: Maybe. It could be mentioned that some, I mean, these matrices you study, they look very similar to this poly. Well, people tried for many, many decades to prove lower bounds for explicit polygon, say just one variable. There was this pioneering paper by 73 where he came up, he had a polynomial where the coefficients were just square root of the primes. Yes, of unity. And I mean, I think it's very similar in terms of technique. Now this is the rigidity problem it's related to.
01:28:43.152 - 01:28:45.016, Speaker A: Yeah, yeah, I mean, I think, but.
01:28:45.040 - 01:28:50.764, Speaker B: I think the techniques are very similar. And so it seems that we get stuck always more or less at the same.
01:28:51.964 - 01:28:55.584, Speaker C: What was the variety in question, the Strawson problem?
01:28:57.004 - 01:29:11.996, Speaker B: I mean, the variety that this was just looked at the set of polynomials in one variable, that multiplicative complexity of most r, it's not. So it doesn't have such a nice description as here.
01:29:12.100 - 01:29:15.996, Speaker C: I mean, here the parametric description of the variety is very easy.
01:29:16.100 - 01:29:16.668, Speaker A: That's right.
01:29:16.756 - 01:29:19.646, Speaker C: Question of finding it, equations, which is not so easy.
01:29:19.710 - 01:29:42.134, Speaker B: Right. In the other problem, variety is much more complicated here. Looks so much simpler still. Okay, so maybe the problem is the elimination ideal. So if you could completely understand the elimination idea.
01:29:42.254 - 01:29:54.494, Speaker C: But see, this is a very general theorem, but this is a very special, the original idea you're starting out with is very, very special, and one should use special techniques to get a sharper result.
01:29:55.474 - 01:30:09.454, Speaker B: The problem here where we lose the information is the emanation. I didn't know. We don't know it exactly. So we just know there is some ge, this and that. But if one would know it better.
01:30:09.934 - 01:31:07.614, Speaker A: It may be possible to understand the polynomials in this, I mean, the ideals of these varieties more directly, instead of trying to refine the process of elimination for these particular varieties. I guess that's kind of what you're saying. I mean, I think the geometric intuition for the rank varieties and these are essentially the projections of the rank rank matrices might actually give you more direct information. And it may be possible. So I don't know. Well, I mean, again, these are determinant ideals, and determinant ideals have been very well studied, but I have, you know, from, you know, from my conversations with a few experts, it's not clear at all whether you can improve the bound on the discrete rebound for determinant ids.
01:31:07.954 - 01:31:27.438, Speaker C: It's also worth mentioning that I don't know if it's co authors, but at least one of the authors, this is v. Rosen, also found very explicit equations in a completely different context for a completely different problem. But it solves this problem also. But again, there's still no consequences here.
01:31:27.546 - 01:31:28.142, Speaker A: Yeah.
01:31:28.278 - 01:31:38.230, Speaker C: Some very interesting polynomials there, but, you know, just like what we did, there's no consequence yet, but he has found, I don't know who the other co authors are.
01:31:38.262 - 01:32:02.978, Speaker A: Franzke. I missed that. Okay. Yeah, I mean, I think, you know, I mean, like they did for or equals. I mean, the polynomials that they have for these two ranks are very nice. I mean, they, and you know, once you see those polynomials, it's fairly easy to see that those polynomials are not going to be satisfied by generic one matrices. And they're fairly clean.
01:32:02.978 - 01:32:44.454, Speaker A: And it would be nice to get such polynomials for larger ranks away from small constant and away from n. I have no idea what to do. Sorry. Yes, but like, if you want to put something about rigidity, you need to prove it for all particles, right? Yeah, so, I mean. Yeah, yeah, well, I mean, in every one of them you have a polynomial like this. And, you know, no such degree polynomial is going to be satisfied by this matrix. Yeah.
01:32:44.454 - 01:33:28.114, Speaker A: So you do, you do have to look at all patterns. So I erased it. But the rigidity variety is the union of these varieties. Overall, even if you have all these polynomials, they are not trivial to prove for some specific polynomials. Yeah. Right now, the only, the only property of the polynomials we are using is their degree bound. So, I mean, since we have no idea how to write those polynomials down like they did for other values of the rank, we only use the degree bound.
01:33:28.114 - 01:33:56.690, Speaker A: So it would be nice. I mean. Yeah, I mean, definitely, if you can get more structure out of these polynomials, then maybe you can construct nicer matrices. If you consider a random matrix with random plus minus one matrix. Yes. Is it known? Yes. So that's one of the homework problems.
01:33:56.690 - 01:34:32.872, Speaker A: That's the last homework problem. I mean, basically leads you to the proof over the real numbers. Yeah. A random zero one matrix plus minus one matrix has rigidity, omega n squared. So that's why constructing even zero one matrices of high rigidity is not hard if you are willing to settle for random matrices. So you really need some effective way to produce an infinite sequence of matrices. And that's why explicitness needs to be properly defined.
01:34:32.872 - 01:35:17.572, Speaker A: Even for zero one matrices. It's like, you know, expanded graphs or error correcting codes, where again, it's easy to produce random objects, nice objects, but much harder to produce explicit things. Any more questions? Sorry, which question? This. Yeah, yeah. If you could produce matrices rather than. Oh, I see. I'm not sure.
01:35:17.572 - 01:35:50.318, Speaker A: I don't know. I mean, basically, I don't know how to interpret this point polynomial dimension of a number field in a computational sense. I mean, I believe there is probably something that one can do, but I have not done that. Yeah, that's a good question, actually. That's. Yeah, I mean, with some small rational. They can't be rational numbers, then the dimension would be not polynomial.
01:35:50.318 - 01:36:26.914, Speaker A: It will be really small. But yeah, even then, I don't know. I mean, even then, I really don't know how to have a computational interpretation of that. But, yeah. So in some sense, this is a weakening of what you were asking, right? I mean, this is a much weaker question. I don't know what. Speaker again.
