00:00:00.120 - 00:00:53.754, Speaker A: Thanks very much, Yun, and thanks very much for the invitation to be here. It's truly fantastic, and I'm looking forward to a couple of weeks talking to people. So I'm going to talk about work that I've been doing, trying to look at new ways of thinking about genetic variation data. But there are really a couple of warnings about this talk before I start. The first is that this is somewhat work in progress, so it's not all totally signed and sealed. The second is that at some level, what I'm going to be talking about is rather boring, because it's not really to do with doing deep inference about underlying processes from genetic variation data. It's really all about the data itself and trying to have good ways of describing the raw information that we use as population geneticists in order to make those inferences.
00:00:53.754 - 00:01:57.994, Speaker A: So just to give you a sense of what I mean by that, I want to give you a feeling for how at least my thinking about genetic variation and genetic variation data has changed over the years. A long way back, I was young and naive, and I used to think that population genetic data looked like this, that you would have essentially a binary incidence matrix that would say you've got a bunch of chromosomes that you've sampled from a population, each row in here, and then there are a bunch of sites at which these chromosomes vary. Here, I've just color coded them as blue and yellow, and you could basically treat this as a matrix of northern ones. So I didn't really think about how the data was generated. I just assumed that when it landed on my lab, I could effectively treat it like this. Now, perhaps one of the great successes or surprises of population genetics is really how far you can get with this incredibly naive and simplistic view of what's going on. It actually works remarkably well.
00:01:57.994 - 00:02:52.774, Speaker A: But there are places where you actually have to start thinking a lot more deeply about the error structures and the way in which data is collected. And I think one of these interesting social changes that has happened in this area over the last ten years or so is how the study of the processes that generate the phenomenon in the population have converged with an understanding of the technologies that actually give us the data in the first place. So that's how I used to think about genetic variation data. And then I had an epiphany, and I realized that actually, there's a lot of uncertainty in how we, we assemble this information. And in certain areas, that uncertainty can be really important. So, for example, this is a pedigree where we've been sequencing. This is a pedigree of chimpanzees.
00:02:52.774 - 00:03:40.212, Speaker A: And what we were interested in doing was learning about the de novo mutation rate. Now, if you take the previous approach and you run the data, this was through high throughput sequencing, you run the data through standard algorithms, and you get something that looks a bit like this. And then you just go along counting up the places where the children look different from the parents. You think the mutation rate is on the order of ten to the minus five or something per base, per generation. Of course it's not. That's just because there are a lot of little errors that are made in how you convert the raw data into an estimate of the genotype. And when you're looking for very, very rare things like de novo mutations, ignoring that uncertainty is very important.
00:03:40.212 - 00:04:27.144, Speaker A: And so what you can do in this case is, say, the data that we collect. The sequence data that we collect is an imperfect representation of the underlying sequence, but with an error structure that is totally measurable. So we can actually put probabilities of seeing the data as a function of possible underlying genotypes. And by doing that, you can then devise pretty sensible statistical methods for saying, what's the probability of seeing the data that I've collected over? They're given different models, for example, where there's just genotyping error. It was a de novo mutation. It could have been a gene conversion, or it's a segregating deletion. And by using these approaches, you can actually get very good filters, if you like, that just allow you to zoom in on the de novo mutations.
00:04:27.144 - 00:05:41.674, Speaker A: So that's, if you like, taking how we think about genetic variation data to one slightly increased level of sophistication. But then I had another epiphany. And over the last few years, actually, what I've been particularly interested in is thinking about the data that we collect, in particular, high throughput sequencing data from essentially the lowest possible level. That is, the reads that come out of the machine and how you can put those reads together using graph structures that try and capture as much of the information you've seen. And then you can walk through these structures in particular ways to try and learn about genetic variation in different ways. And so really, what I'm going to talk about today is how we might think a bit harder about these, the structures that can be used to assemble the primary output of sequencing technologies, and use that for inference about not just the diversity that we see in populations, but of course, the processes that we ultimately care about. I should say that if people want to interrupt and ask questions during this talk.
00:05:41.674 - 00:06:38.834, Speaker A: I'm more than happy to have that. So what does this talk about? What am I going to try and convince you? The first thing I'm going to do is to try and convince you that there are types of variation out there that we probably care about that are not well represented by these sort of binary incidents or the genotype likelihood models. And so if you believe this variation is interesting, I shall try and motivate it with some case studies. Then we need methods, statistical and computational methods that are good at accessing this variation and analyzing it appropriately. And of course, given the title, you won't be surprised to hear that I think there are fairly sensible graph based ways of doing this. And I'm going to really talk about two particular applications. One is trying to build a reference graph for human genetic variation, in particular within the, the h lay region.
00:06:38.834 - 00:07:22.950, Speaker A: And the other is to do with the assembly of hypervariable immune related genes in plasmodium falciparum. So that's what I want to try and do. So let's give you some motivating examples. Why am I actually bothering going back down to this level in terms of the raw sequence? So here's an example. This is the variation or variable genes of plasmodium falciparum. This beast is the primary agent of malaria, and it's the primary cause of malaria, associating mortality. I won't go through the life cycle, but the key thing is that the parasite spends a lot of its life sitting around in red blood cells.
00:07:22.950 - 00:08:19.524, Speaker A: Now, the thing about red blood cells is they don't have a nucleus, which is great if you're a parasite, which means there's, there's no sort of immune system that is going to get you. But they're also bad because red blood cells are actually turned over pretty rapidly by the spleen. So you're not going to last long if you're just sitting in a red blood cell. So what the parasite does is it covers the surface of the red blood cell with a large number of sticky proteins. These are called pf EMP ones, Plasmodium falciparum, erythrocyte membrane protein number one. And what these things do is they stick to receptors on epithelial cells. So the red blood cells get stuck to the sides of capillaries, things like that, and that stops them being flushed away to the spleen and being got rid of.
00:08:19.524 - 00:09:14.670, Speaker A: So that's a great design from the parasite's point of view. But of course, by sticking things on the surface of these red blood cells, it makes itself immediately visible to the immune system, in particular the antibody part of the immune system. So you've got these things floating around, and of course we're very good at mounting antibody responses, and that would allow us to get rid of the parasite very quickly. So what the parasite does to get around that is to actually have a very broad repertoire of these genes, and not just broad, but incredibly diverse. So every parasite you look at has about 40 to 60 or so of these proteins. It only chooses one to put on the surface of a red blood cell. At any point, it switches which one it uses, and there's huge diversity in these proteins.
00:09:14.670 - 00:09:50.440, Speaker A: So let me show you what I mean by that. This is an attempt, and believe me, this is just an attempt at a multiple sequence alignment. You can see there are some bits of it which are relatively conserved, but there's huge amounts which are hugely, hugely diverse. So these sort of sit around the genome of the falciparum, but they're not well behaved genes. It's not like there's a gene here and you go to the next parasite and you'll find an ortholog that looks somewhat like it, actually. They have this system which shuffles this variation hugely. So these genes sort of move location.
00:09:50.440 - 00:10:27.450, Speaker A: They can hop from one telomere to another telomere. They can undergo processes of recombination. So they have lots of internal mechanisms for generating variation. So if you want to look at this variation and try and understand the processes that have led to it, you'll face with this issue of how do you actually look at data that looks something like this? Well, the first thing you might do is to say, well, I've got a multiple sequence alignment. Let's try making a tree. And when you make a tree of these sequences, you find that there's not really much structure there. There are a few that are kind of related to each other, these little clusters up here.
00:10:27.450 - 00:11:30.794, Speaker A: But apart from some kind of interesting optical illusions down here, there's essentially no structure in this state of whatsoever, sort of, kind of mindless. So how do you go from that? How do you actually start looking through these data for patterns? Well, one thing you can do is you can say, well, let's not try and look at the whole sequence at once, but let's sort of break this down into little words. Let's imagine taking windows of nine amino acids, sliding them along, and whenever two sequences share a particular nine letter amino acid word, you can sort of merge them into one. And in regions where they're variable, then you'll see some going one way, some going another way. That's a very informal description of what this is. But as you can see, this is just essentially a graph representing the variation that was in that multiple sequence alignment. And this very clearly shows these regions of strong conservation, and then this, these regions where there's a huge amount of diversity.
00:11:30.794 - 00:12:42.694, Speaker A: But what you see when you try and sort of push into this in more detail is that there is substructure within here. There are some sequences that sort of cluster together locally, but if you find the sequences that cluster together locally up here, they're unlikely to be clustered together locally down there. Rather, there's a process of recombination, essentially, that is shuffling the variation up among these genes. And although I'm not going to go into the detail of how we did this, if you take, what you can do is to say, here's one of these protein sequences. I want to reconstruct that sequence as an imperfect mosaic of other sequences in this data set that allows you to find relationships between sequences and others within this set. And what you find, this is just a Vitovi decoding of the process, is you get this sort of mosaic structure. So this is a sequence from one of the chimpanzee parasites, and you can see that there's a block like mosaic process that relates this to that.
00:12:42.694 - 00:13:32.258, Speaker A: And here's another one, again, a chimpanzee sequence where there's large stretches of homology to sequences that you find in humans, and that wouldn't at all be visible from a straightforward alignment. So here's a situation where sort of standard alignment doesn't work. But when you devise ways of looking at the data in interesting fashions, you can see evidence that there's a. These little bits of sequence are being maintained over long evolutionary periods, but are being shuffled up all the time time. So that's one example, here's another. This is, if you like, is the human counterpart of the VAR story. This is one of our most variable regions of the genome.
00:13:32.258 - 00:14:28.524, Speaker A: This is the class one HLA region. The class one HLA genes are what present bits of protein that are in cells to our immune system. So all of our cells have these MHC proteins on them, and what they do is they take proteins, all the proteins that you find in your cells, they break them up into little bits. Those are loaded onto these receptors, if you like, and then shown to the outside world, and in particular shown to the immune system. If this protein is something that is normally in you, that's fine, your immune system won't respond. But if this happens to be a bit of viral protein that's kicking around your cell, that will elicit a response from your immune system that will come and kill the cell, and you'll get clearance of the virus in that case. So these are sort of the key features of our, one of the key features of our adaptive immune system.
00:14:28.524 - 00:15:22.578, Speaker A: So these genes, the class one genes, there are three of them, a, b and c. They're encoded within the HLA region on chromosome six. Each one of these loci is very diverse. So, for example, the B locus, there are well over 2000 different sequences described at the protein level. So there are over 2000 haplotypes, if you like, of protein sequence within the class one MHC and Hlab. And this is just a plot of showing the frequency of certain key alleles around the world. And any population, geneticists would know that these are very highly differentiated and probably reflect something to do with the different exposure to different pathogens that populations have had over evolutionary time.
00:15:22.578 - 00:16:45.920, Speaker A: So, very diverse. But that notion of very diverse is actually a very simple statement. It doesn't tell us much about what's going on within the system, how is it structured? Are there relationships between what you see in these different loci? And again, one thing that we can do is just to make simple structures, graphical structures, that represent something about the variation that you see in these slow sides, which allow us to find some kind of interesting patterns. They're just descriptions of the data, essentially, but they're trying to describe the data in ways that allow you to pick up interesting biological processes. So, for example, if you take the HLA a loci, you can make a graph that summarize something about the variability as you move along the sequence. So again, we've just taken the protein sequences, and we're moving along in a window here of, actually, this is the DNA sequence of length 31 bases. And we say whenever there are two alleles that share the same 31 letters, then you're going to collapse those into two, those two into one, and you can make a graph structure that can be visualized like this.
00:16:45.920 - 00:17:15.097, Speaker A: It's very simple, doesn't really show very much. So that's HLA A. You take HLA B. Now, HLa B is related to HLA A. They kind of do the same thing, A, B and C. We don't really know why you've got three different loci for this at all. So you can do this for B you can also do it for C, which again kind of does the same thing as a and B, but it's again expressed on all ourselves.
00:17:15.097 - 00:18:13.884, Speaker A: See, there's a bit more structure within c, which is kind of interesting. But then the very interesting thing comes when you put them all together and what you find is a complete mess. But if you look carefully into this complete mess, you find plenty of cases where actually there's a lot of shared sequence between these three loci. So the c is the, sorry, a is the green one, but you can see b and c. Actually there are lots of cases where these are essentially tracking the same. And so what this tells you, and again, I'm not going to go into any more detail on this, is there's actually a lot of traffic between the different loci that's essentially caused by gene conversion. So rather than thinking this about one gene over here, one gene over here and one gene over here, you really need to think about the three as a, as a family, where there's actually a lot of cross talk between the different members of it through recombination like processes.
00:18:13.884 - 00:18:25.704, Speaker A: So I think that's an interesting observation and we'd like to understand what's going on here by taking this on to look at what's going on in grade eight, for example. Yeah.
00:18:25.864 - 00:18:35.826, Speaker B: One bit of confusion make it a lot clearer. Do these graphs require construction of a multi sequence alignment? First, is it just a graphical representation of a.
00:18:36.000 - 00:19:16.764, Speaker A: So they require limited statement about homology, but they don't require a multiple sequence alignment. So you have to. So what they essentially require you to say is if you find this k mer here in this sequence, and you find this k mer in a different sequence, those are homologous. So they require statements about essentially allelesm rather than homology, if that makes sense. So it requires some sense that these are alignable, but it doesn't actually require a multiple sequence alignment. Okay.
00:19:20.944 - 00:19:21.776, Speaker B: I think there are a couple.
00:19:21.800 - 00:19:24.924, Speaker A: Of questions over there. Sorry. Yeah.
00:19:25.344 - 00:19:28.324, Speaker C: The choice of k, why 31?
00:19:29.264 - 00:19:31.564, Speaker A: Absolutely no good reason whatsoever.
00:19:32.184 - 00:19:34.088, Speaker C: There's no biological reason.
00:19:34.176 - 00:19:59.554, Speaker A: Absolutely no biological reason. And later I'll talk about how we relax that. Actually, that's just, it's one of the unpleasant things in making grass. So you want to get rid of that and we can actually get rid of that choice later. And also the vertical heights, that's just some algorithm that's just arbitrary, minimizes crossing over. Exactly. It's exactly that one and up versus down.
00:19:59.554 - 00:20:29.026, Speaker A: What's the just as explaining? It's just a way of representing it so as you minimize the crossing of lines. So it's just a standard. It's one of these off the shelf packages for showing, illustrating networks. There are other ways that you might choose to show this. And I'll give you an example of that later. Yeah, sorry, Lauren. Then going, do you have a way to deal with repetitions and sequences? Yeah.
00:20:29.026 - 00:20:51.198, Speaker A: So that's an important point, this. And it's kind of Annie's point that you. It's to do. If you see the same k mer multiple times, you don't want to try aligning those to each other. So this does require. This essentially ignores duplicate kmas. So you kind of mask those out when you make this.
00:20:51.198 - 00:20:57.434, Speaker A: And then you can sort of say that this is version a and version b of the same.
00:20:59.174 - 00:21:09.234, Speaker C: So if the length of the genes doesn't match, how do you align? I mean, do you stretch from the beginning to the end, and then the scale is different for each gene?
00:21:10.614 - 00:21:16.954, Speaker A: If they don't match, then I don't have a particular problem with that, as long as there are bits of them that are essentially homologous.
00:21:19.354 - 00:21:22.402, Speaker C: So you go from one end of the gene to the other end of the gene?
00:21:22.538 - 00:21:23.254, Speaker A: Yes.
00:21:24.194 - 00:21:26.850, Speaker C: I don't know. Are the lengths of the genes the same?
00:21:27.042 - 00:21:57.046, Speaker A: Pretty much, yes. There is some variation, but that's not really very important, actually, because what you require is that if you find this k mer here in sequence a and the same k mer in sequence b, that those are essentially. You can line those up. You can then have different parts of different lengths as long as they will line up again when they next share a kma. Okay. So you can have bigger loops on one side than another side. That's perfectly legitimate.
00:21:57.046 - 00:22:10.430, Speaker A: But in terms of just a representation, sure. We've then stretched it so that everything appears the same length and they all have to be in the same order. Right. You could have inversions. Yeah. So, yes. Well spotted.
00:22:10.430 - 00:22:17.954, Speaker A: So inversions are the killer for this. And I've made sure there are no inversions here. They're on.
00:22:18.774 - 00:22:32.238, Speaker B: Can I just ask, just for clarification, you pointed to these things and saying homology, but then you mentioned that the functional biology of these things is similar but not well understood often.
00:22:32.366 - 00:22:32.742, Speaker A: Yeah.
00:22:32.798 - 00:22:43.930, Speaker B: And so I'm wondering to what extent one has to be open to the idea that these things are convergent. In other words, I'm just trying to figure out whether I should be thinking.
00:22:44.002 - 00:22:51.282, Speaker A: I don't think you should worry about that. So if you look in, they are.
00:22:51.378 - 00:22:56.034, Speaker B: Capturing the same viruses and presenting them. Right.
00:22:56.074 - 00:22:56.714, Speaker A: That's right.
00:22:56.834 - 00:23:00.270, Speaker B: And they are independent mechanisms for doing that.
00:23:00.382 - 00:23:03.766, Speaker A: They're essentially equivalent. They essentially use the same machinery.
00:23:03.910 - 00:23:19.474, Speaker B: Yeah. So I guess what I'm saying is I don't really understand why if I see a particular part of a c allele and a b allele that look similar, why those couldn't be there by convergent evolution.
00:23:20.614 - 00:23:38.712, Speaker A: I'm not saying they couldn't be there by convergence. So some of this could be convergence factor. Absolutely fine. But you definitely see, you know, if I went up to k of 100, you'd still see lots of sharing. Okay. So by that time you're probably less. The hypothesis of convergence is less.
00:23:38.808 - 00:23:42.044, Speaker B: I wasn't thinking of the really long stretches. Yeah, I'm just thinking.
00:23:42.464 - 00:23:57.220, Speaker A: No, clearly, if you go take the extreme and you go down to a single amino acid, then convergence is quite likely at that level. So there's a sort of a spectrum of credibility. The single amino acid to the hundred amino acids.
00:23:57.412 - 00:24:02.624, Speaker B: Some of the functional domains in the actual HLl molecule are quite small.
00:24:03.124 - 00:24:25.874, Speaker A: Yes, absolutely. But they're 2030 amino acids long. When you see homology over identity over that stretches, I think the hypothesis of convergence confirmed by what? Yes, well, this is, this is actually a DNA sequence.
00:24:27.334 - 00:24:28.834, Speaker B: You're not distinguishing?
00:24:29.734 - 00:25:12.762, Speaker A: No, not in this one. Okay, so that's example two, example three. Hurry up here. So those are cases where we've taken quite diverse and quite difficult, called loci, and we've. So we know that that's going to be somewhat complicated. Here's a different example, which is actually much closer to a lot of what's going on in whole genome sequencing at the moment, which is trying to assemble variation across, again, the HLA region here, not the class one region, but the class two region. So the class one alleles are what are presented by all your cells.
00:25:12.762 - 00:25:43.952, Speaker A: They're going around. That's what used for presenting from everything. There's a separate set of low cycle class two alleles which are used by what are called the professional antigen presenting cells. So these are the serious bits of your immune system. And these are, again, extremely variable. And they're variable not just in the sequence content of the individual loci. There's also a lot of variation in gene number.
00:25:43.952 - 00:26:57.524, Speaker A: So there are these DRB one, for example, some of these class two loci, but there are also dRB four, five, six threes, and they're variable in copy number. So some people might have a three, some people might not. So there's a lot sort of paralogy there, and there's a lot of broader structural variation within this region. And so when you take an individual and you try and map reads back to this class two region using a standard single reference, which is called PGF, that's the one that's in GRC 37, for example, and you look at coverage, you get this incredibly sort of messy coverage, and then a lot of the reads around this region show strange features, like the pairs mapped the wrong chromosome, or the pairs are way too far apart, or they're in the wrong orientation. And when you zoom into the alignment, you see a complete mess like that, which you certainly wouldn't want to try and work with. So from the perspective of trying to assemble variation within this region, then actually the single reference doesn't work very well. It's very hard to access the variation within this region using our standard approaches.
00:26:57.524 - 00:28:24.710, Speaker A: But in this particular case, actually, this is just from one individual we can cheat because we actually know which haplotypes this individual carries, because they were actually one of the individuals used for the construction of the references. And what many of you may not be aware of is, although we think of the human references as just sort of a 3 billion base pair string, actually, there are some bits of it, like the HLA, where people have gone to the trouble of assembling different versions of that sequence. And so we know, actually, that this individual carries one that's called PGF and another that's called man. And if you just do the naive thing of taking the reference genome, standard reference genome, and augmenting it with another contiguous, which is essentially the man haplotype, and remapping the reads to this slightly augmented genome, then what you see is that the coverage looks much neater and you massively reduce these nasty reads. You still get some spikes that are telling you that things are going wrong, but it looks much nicer, and you can quantify that in various ways. For example, this is the total number of reads that map here to the single case. You get a load that are badly mapped and then a bunch that are decent.
00:28:24.710 - 00:29:47.894, Speaker A: And then when you do, when you map to this augmented reference, you get many more than map well and many fewer than map badly. So what this tells you is in this region. And this is going to be the general statement that I'm going to use to motivate the rest of this talk, that we actually know a moderate amount about variation in this region and variation more generally across the genome. But when we're trying to characterize variation within these regions, we don't use it very well. If we could use it if we could map to all these different versions simultaneously, we might end up with a much better understanding of the primary data that goes into our downstream inference. So that's essentially the preamble to this talk speed up. So where am I going with this? What I've said is that there's a lot of diversity that's quite difficult to get at, but given that we know quite a lot about genetic variation already, we should use that information to try and help us assemble the genome in the next individual.
00:29:47.894 - 00:30:53.636, Speaker A: So it's essentially, I think of this as a population genetics insight. One of the more powerful insights that's widely used in statistical inference is that novel sequences tend to look like those we've already seen, although you need to allow for a mutation and recombination. And quite often you actually need rather few references or reference sequences to capture the vast majority of sequence space. Most things that you look at look like some kind of mosaic of a relatively limited number of reference sequences. And so we'd like to use that insight. So the big question then is how you formalize that relationship and how you make that work in the context of assembling the next genome and, of course, interpreting it. So, yeah, David, to what extent are these problems a short read phenomenon that gets overcome with longer reads? And to what extent are they going to be still there with longer reads? I think there are two answers to that.
00:30:53.636 - 00:31:55.844, Speaker A: One is in the short term, long reads can help, but the current generation of long reads is quite error prone, which makes using them out of the box quite hard. So I think in the short term, actually, you'll want a mixture of short, high quality reads and the longer, more error prone reads, and you'll still need to do things like this. So that's a sort of short term, I think, in the longer term, suppose you could give me everyone in this room's genome assembled absolutely perfectly haplotyped end to end. You'd still need a way of describing that variation, and that would allow you to analyze it later. And I think that some of these structures that I'll talk about are actually quite useful in that context. So, for example, the HLA stuff I showed, or the bar stuff, I'm assuming those are perfectly assembled. This isn't about looking at the primary data.
00:31:55.844 - 00:32:57.852, Speaker A: These are thousand or tens of thousands bases long. But in order to see the structure there, you still need to come up with ways of looking at substructure. So I think these are ways essentially of looking at substructure within data which have an application in short read or moderately long read data, but actually are more fundamental. Okay, so how do we represent variation? How might we try and formalize the relationship between multiple references and what you see in the n plus one sequence? So there are various structures, graph structures, that you can use to represent sequence and variation. One that we're all very familiar with is the notion of a multiple sequence alignment, where every column in this essentially has a notion of homology. It's a strong statement, very strong statement about homology. And typically, in something like that, it's clearly not true.
00:32:57.852 - 00:33:58.216, Speaker A: We can't make very strong statements, at least about homology, on the basis of that. On the other side, we have various structures that make no statement about homology. For example, de Bruyne graph, which is just taking raw sequence, breaking it up into words of 31 or whatever you want, and then looking at how those words overlap with each other. You can make structures called de Bruijn graphs out of that. They're simply a representation of the raw sequence, and there's no statement of homology made from that. You can try and make statements about homology by, say, thinking about how you map the adjacencies of bubbles onto, say, a reference or something like that, but actually, that's not present in the initial structure. But there are problems with the De Bruyne graph, which is that you can't, you're fixed at this k, and it's a rather unpleasant thing.
00:33:58.216 - 00:34:47.319, Speaker A: So what you can then do is sort. You can think about, how would you generalize the de Bruyne graph? And here you come up with things which are either called or related to things called string graphs, which are. You can think of the string graph essentially as a variable k de Bruyne graph. In fact, the way we do it is that we use a de Bruijn graph and augment it with various information about paths through the de Bruijn graph. Again, there's no statement about homology, but you're capturing more of the information about the underlying sequences that have gone into your input. And then the sort of final thing is, again, the string graph has no statement of homology. So if you wanted to go back to making statements of homology, you need to sort of align these.
00:34:47.319 - 00:35:50.320, Speaker A: And so here's a structure that you might call an align string graph, where you're going to make statements, as we were discussing earlier, that where two sequences touch in this, where they share essentially a kma that is essentially unique within the genome. That is, you only ever find it once within the genome, where you find those two kmars, you're going to be able to allow yourself to clamp two genomes together so that they. And then paths that come out of that represent alternative. Alternative paths through genome space. So I'm going to just spend a couple of minutes trying to tell you a bit more about that. And here, the very what we try to do to illustrate this, and actually it's more than proof of principle, it's demonstrate how you can use these structures to get a better understanding of what's going. Going on in the genome, is that we try to make a graph like this for the HLA.
00:35:50.320 - 00:36:38.102, Speaker A: And that's not just these classical HLA alleles, that's the entire three to four megabase, what's called the extended HLA region. And the reason that we've done that is that it's a region where we actually know a lot about variation. It's not well serviced by the standard approach of mapping short reads to reference. So how can we combine all this information into one? So here's a sort of a cartoon of what we've got. We've got about eight haplotypes that have been sequenced using Sanger technologies that sit in GRC 37 or 38. These are assembled to different levels of quality. They've got some quite big structural changes between them, but they've all got.
00:36:38.102 - 00:37:31.572, Speaker A: They've also got long stretches of strong identity within this. There are these hypervariable classical HLA alleles, where we obviously know what's going on in these four references. But then we typically have hundreds or tens or thousands of other sequences that we might want to use as alternative paths through the space of genomes. And then in addition, we've got information from the 1000 genomes about a whole bunch of other snp's that there are across this region. And so we want a structure that allows us to put all this information together into a single graph. This is kind of what it does. You take all of these, and as I said, where you find these kmas, which are essentially unique within the genome, you allow yourself to squash them or to merge two genomes.
00:37:31.572 - 00:38:14.200, Speaker A: And that gives you a graphical structure which allows you to describe essentially everything that we've got over there. So the key thing then is there's a notion of alignment, there's a notion of alleleism, in that these are the alternative paths. The alternative paths don't actually have to be homologous. This could be sequenced from Mars, this could be sequenced from Jupiter. And so you don't actually have to make statements about homology of the alternative paths, but you do make statements about homology of the intervening bits. So we call this a population reference graph. It's a compression of the input data.
00:38:14.200 - 00:38:47.700, Speaker A: We can retain the initial information if we want. Most importantly, it's a generative model. So we could use this structure to simulate new genomes by sort of choosing paths through this population reference graph. And because of that structure, there's a suggestion. It sort of suggests an efficient way of doing genome inference. That is, you take. You take data from another individual, and then you try and work out which bits of this graph essentially light up.
00:38:47.700 - 00:39:37.786, Speaker A: And can you reconstruct the pair of paths through this graph that you're going to make up an individual? So our current implementation has three stages. In stage one, you take. Well, the stage, naught, if you like, is to construct this population reference graph in terms of the analysis of the next genome. The first thing we do is we take our reads and we convert them to a clean debris graph. We then take this de Bruijn graph and compare it to our population reference graph. And we want to try and work out the best pair of paths through this graph that are present, that are present in this individual. And we do that essentially by using diagnostic kmars.
00:39:37.786 - 00:40:35.954, Speaker A: So we say within this path, there are various k mers which are unique to that bit of the graph. That is, you don't find them elsewhere, either within this subgraph or across the rest of the genome. That's going to allow us to find bits of this that are present in the individual. We have an error model that says, if you see one of these, don't always go that way. So that allows you to use Viterbi algorithm to find the best pair of paths through this. So what comes out of this is something we call a chromatype, which looks a bit like a string with some bubbles in it. And then we have a final stage, which isn't ideal at the moment, which is where we then, because this is only going to find variation present in the reference set, we then have another stage where we remap the initial reads back to this chromatype to allow us to find novel variation.
00:40:35.954 - 00:40:40.538, Speaker A: This bit's a bit clunky at the moment and doesn't seem to do much good.
00:40:40.586 - 00:40:42.658, Speaker C: Are you supposed to start with a.
00:40:42.666 - 00:40:45.362, Speaker B: Diploid here, or is that why there's two?
00:40:45.458 - 00:40:57.334, Speaker A: Yes, that's right. So, okay, this is human. So we're being deployed. So we mapped the reads back to both, say, path one through here and path two through here.
00:40:59.574 - 00:41:03.654, Speaker B: Moment, that's when you recapture the snips that you might have lost.
00:41:03.814 - 00:41:40.662, Speaker A: That's right, yeah. Needs to be honest, this isn't how we're going to do it in the long term. It's just how we're doing it at the moment. You can do it as a two stage thing and that's where we go to next. I'm conscious of time, so I'm just going to very quickly go over the evaluation. We're going to compare this to, if you like, the best case mapping based approach, which is Hetton Stampy platypus. So that's going to be our best practice, mapping based approach.
00:41:40.662 - 00:42:44.716, Speaker A: And we've evaluated this on a bunch of data, Snip array data, sequence based typing, that's sanger typing of the classical HLA alleles, because these are based on bits of the genome that are relatively easy to find, that's somewhat biased. So another thing we've looked at is simply KMA recovery. So the k mers that we think should be in this individual, how many of them actually do we find in the sequencing data? And then for one of our individuals, we've got some very long reads data called molecular data from Illumina, which is absolutely gorgeous, but costs of loss, unfortunately, which allows us to sort of look at longer range accuracy of the method. And as I said, there are a bunch of samples that we've looked at. I'm just going to rattle through what we find. So, first of all, in terms of snip data, basically, whether you're mapping or using these graph approaches, you basically come up with the same inferences and they're all kind of much of a muchness and very good. So snip array data is easy.
00:42:44.716 - 00:43:11.736, Speaker A: At the classical HLA alleles in the class one ABC, the methods are pretty much the same. Actually. Platypus does extremely well here. Peleton should be pleased. In the class two, which are harder for a mapping based approach. You see, the mapping based approach does a lot worse on the whole, and that the reference graph approach does pretty well. It's not perfect and we haven't yet understood why it's not perfect.
00:43:11.736 - 00:44:06.688, Speaker A: Possible that the sequence based timing isn't quite 100% accurate is my guess. So here, the reference graph approach works quite well if you look at the KMA recovery. So this is how many. So you can, for each of these different approaches, you can just. There's a reference, a platypus approach and then the straightforward Viterbi decoding and then the amended reference graph approach, you can ask how many of the kmas that you think should be in the chromatype do you actually recover? You can kind of see they all do remarkably well across this region. The reference does very poorly. If you ask how many are actually recovered, then the reference graph approach does recover more and does not recover fewer than the mapping based approach.
00:44:06.688 - 00:44:31.766, Speaker A: That's reassuring. So we're doing a little better. There are regions where there's a lot bigger divergence between methods. So again, this is the class two region. If you look at the red line, that's the platypus approach. You find some regions where a lot of the caimas you think should be there, you don't actually recover. If you take the graph based approach, you do much better.
00:44:31.766 - 00:45:18.730, Speaker A: That's the pink, if you like, although there are still some regions where we do very poorly. So I'm just going to zoom in on that region and show you what's going on there. And to zoom in, we've used these molecular long read technologies where what we have to do is to align these long reads to. To actually the chromatype. And so we've had to do a bit of jiggery pokery to make that work. When you align these to the chromatypes, essentially what you find is that the mapping based approach is very good at finding, is very good in terms of describing variation, which is not a big edit distance from the reference. That's what you expect.
00:45:18.730 - 00:46:23.404, Speaker A: But the graph methods are much better at finding the bigger deviations from the reference. So they're not quite as sensitive as the mapping based approach, but they're much more effective at finding the bigger divergences as you'd expect. If we just go back to this region where there's a big divergence between them, then what we think is actually happening is that within our, our set of reference haplotypes, we simply haven't covered the space of variation that you find in the class two region. So actually, we dug out a. Here's a contig from the molecular data that we've dug out from this region. And what you find within here is that there's a massive, great inversion compared to any of these sequences that we have in our reference. And so essentially, this is saying that within this class two regional, our reference set of haplotypes is still fundamentally limited, and we need to do more work to try and assemble a better, a more comprehensive description of what's going on there.
00:46:23.404 - 00:47:06.458, Speaker A: So then, finally, what I hope I've convinced you of so far is that this is a pretty simple minded approach. We are simply saying, here's a bunch of sequence that we know about. Let's try and put that all together in a graph, so that when we take sequencing data from the next sample, we can try and reconstruct that individual's genome as a series of paths through this graph. It's a good in the sense that we have a prototype. It works pretty well. We can use it to assemble variation within the HLA quite effectively. Our current implementation is not optimal in a few regards.
00:47:06.458 - 00:48:04.522, Speaker A: The first is that it uses de Bruijn graph structure, which throws away this longer read data. So by fixing it k 31, and there's no good reason for doing that apart from it works, then you throw away any information that's at scales 32 and above. And our two step chromatype to remapping of the reads is inefficient and doesn't seem to add very much. So that's one ideal. We think that both of these things can be solved by new data structure, which we're calling an annotated de Bruijn graph, which is really a poor man's approximation to a string graph, but is much, much more efficient than a string graph, where you essentially have a de Bruijn graph. But then you thread through it mini paths, which are the information that you see in the reads about how K mers relate to each other. So here are the blocks of the k mers, and in this structure you wouldn't a simple de Bruyne graph, you wouldn't be able to navigate from a to c.
00:48:04.522 - 00:48:38.966, Speaker A: But if you record this red path information, that allows you to recover the information. It's a nice structure in many ways. You can use paired information, end information. You can do error correction of reads very effectively with it. It essentially becomes a k agnostic de Bruyne graph. So if you look at as a function of k mercy, how well do you do at recovering contigs within this is a staph aureus genome, basically above k of about 20. You don't seem to gain very much more by increasing your k.
00:48:38.966 - 00:49:17.830, Speaker A: So essentially in this new structure, we effectively are making a k agnostic de Bruyne graph, or string graph. It's memory efficient, so it's about 50gb for the first human. You add another human, that requires about another one gig. We think that as you add in additional ones, that should go roughly as log of the sample size. So this means we should be able to put hundreds or thousands of humans in at once, which is quite exciting. And it's got a very nice structure and it's very easy to operate. You just have a drop in model based on finding k mers just with a hash table.
00:49:17.830 - 00:50:08.218, Speaker A: So it's extremely neat to operate and it works well. So this is data now going back to malaria where we're trying to reconstruct these VAR sequences. So here are a bunch of samples that we've done from across between two reference genomes three, d seven and hp three, there are a whole bunch of progeny. If you look at the n 50, which is a measure of how long your contigs are without this new structure, you get tiddly widdly contigs which aren't good for anything. But then with this new structure you're getting much longer ones. It's still not perfect, but they're a lot longer. And some of the fun things when finding is if you start looking among these progeny, you start being able to find mosaic structures which are indicative of non allelic homologous recombination between these five genes.
00:50:08.218 - 00:50:42.980, Speaker A: Just that process that I started by describing. But that was at a population level. This is actually the level of a particular cross. And moreover, you don't just find mosaic structures, you find complex mosaic structures whereby what you find in a progeny is a sort of a complex shuffling backwards and forwards of the two parental sequences, which is much more like. So the template switching process you see in HIV than say, a classic crossing over event. Right. So I've reached the end, hopefully.
00:50:42.980 - 00:51:12.894, Speaker A: I've convinced you that graph structures for representing sequence variation can be quite informative. You can use them to see interesting patterns that you wouldn't have seen before. And they can also be quite effective in allowing you to assemble the genome of your next individual. That's quite, quite useful. I think it's a way of saying here's all the knowledge that we've got before. Let's use that to try and assemble what we find in the next individual. And there's a whole bunch of people to thank.
00:51:12.894 - 00:51:25.494, Speaker A: In particular Alex Dilte, who did most of the graph work, Isaac, who's invented the new data structure and Zam, who's a long term collaborator on the whole thing. Thank you.
