00:00:00.640 - 00:00:19.194, Speaker A: String of talks starting with Simon Dew from University of Washington. He's going to talk to us about offline and to player zero sum Markov games. Simon, take it away. I will be the chat for a question.
00:00:19.894 - 00:00:56.404, Speaker B: Yeah, thanks for the introduction. Hello everyone. I'm very happy to be here talking about our recent work on the offline multi agent reinforced learning. This is a joint work with my student Xiwen Choi from UW. So today we're going to talk about two player, zero sum Markov games. So because this is a multi agent reinforced learning workshop, so I don't think I need to motivate this problem. So, you know, basically this problem talk about two players compete against each other and each has a strategy.
00:00:56.404 - 00:01:57.924, Speaker B: And our goal is to find national equilibrium, which is a pair of strategies that no player can do better by unilaterally changing the policy. And for this problem, there are many applications like poker, go chess games, also some investment as well. So this is the standard two player Markov games. Today I'm going to talk about the offline setting of this two player zero sum game. In the online setting, usually we can control an agent which is interacting with the environment, and based on the collected data, this agent can update his policy. Nowadays, often represented by some neural networks, this is an online setting. For the offline reinforcement learning setting, we are given a big data set which is collected from the past interactions using some policies or sometimes even a set of policies.
00:01:57.924 - 00:02:50.414, Speaker B: And then we just use this big dataset to train the policy. And we hope that this learn policy gives good performance when we deploy it. The motivation for offline reinforced learning is that in many settings, real world applications, uh, you know, online interaction with environment can be very costly. Uh, so we already want to use existing data to learn good policy. Um, so this talk will be focusing on, uh, learning and Nash equilibrium in this offline, um, two player summary Markov games. And we will focus on what are the conditions we need to have in order to learn a natural library. So before getting into two players or some setting, uh, let me first review a little bit on what we know about offline single agent reinforcement learning.
00:02:50.414 - 00:03:44.718, Speaker B: So here's a quick review of what is single agent reinforcement learning. Here we are controlling an agent which is interacting with an environment. At each time step, the agent is at some state that's called St, and the agent takes some actions at the agent receives some reward based on some reward function that depends on state and action. And the state or the environment transits the agent to the next state according to some transition probability which may depend on the time step, which I did, known as pt here, and move to the next state as t plus one. And here, let's say we consider the episodic settings. So for each episode you have a planning horizon or the episode length h. So basically, this kind of interaction repeats for h times.
00:03:44.718 - 00:04:31.454, Speaker B: So this is a single agent reinforced learning. And the goal of single agent learning is to learn good policy. Or you can view policy as a mapping from state to action. Basically tells you which action to choose based on the current state you are at. And the goal is to maximize the total expected total reward, or the value function, which is defined this way with expectation or the policy and the transition, say, starting from a fixed state, let's say s one. And in terms of a learning problem, I want to learn a policy that is close to the optimal policy, which is measured by the difference of the value. In terms of this policy and the open policy, this is a standard single engine reinforced learning.
00:04:31.454 - 00:05:34.376, Speaker B: And here, let's say we consider this most basic setting, the tabular setting, where we have finite state, finite actions and say reward scanning is from zero to one. And in this case, the sample complexity will depend polynomially on these factors, sah and mono epsilon. Okay, so this is a single agent reinforcement learning. And for the offline setting, this is the offline single agent reinforcement. Basically we have, let's say, n trajectories, each trajectory have in total hydrogen steps. For each step, we have a data set of this form, say we have current state action, reward, next state. And the key notion in this single agent reinforced learning or offline reinforcement learning, is that we say that this data set is collected from distribution, which we denote as zero, where rho is the data collection or the behavior policy.
00:05:34.376 - 00:06:01.386, Speaker B: So we use this policy to collect data, okay? And you can think of this policy as even as a mixed policy with different fixed policies. And this is like a mixture of policies as well. So basically what we care about is this distribution zero that generates this data set. So this zero, it's a state action distribution that depends on the policy.
00:06:01.530 - 00:06:02.050, Speaker C: Go.
00:06:02.162 - 00:07:07.054, Speaker B: And this MDP transition matrix p. Okay? And again, the goal is the same. Want to learn a policy from this data set without any action based environment such that the policy is near optimal, say, within Epsilon arrow to the optimum policy. Okay? So different from the online setting, the natural question for the offline setting is that what kind of assumption we need to make on this distribution that generates data under which we can learn a new policy. So a simple thought experience is that you can think of if the open policy can basis some state actions, but our data collection policies does not visit those state actions. For that case, you cannot hope to learn a good policy or learn new outcome policy because you don't have the coverage over some state action that are visited by open policy. But here they are, you don't have any information about those state and actions.
00:07:07.054 - 00:08:36.974, Speaker B: So there's a long line of work on single agent offline reinforcement learning. And researchers have identified this assumption. It's called single policy coverage assumption, which is shown to have be both necessary and sufficient. So first of all, it cannot be weakened because if you do have some state actions, that is so here by definition, the definition of single pass assumption is that the behavior policy only covers a single open policy. So this is necessary because if you have some state action that is not visited or not collected by your behavior or data collection policy, then you cannot hope for to learn a neural component policy. On the other hand, this is also sufficient because we do have some algorithm based on pessimism to learn with say polynomial number of samples. So formally, this assumption states that there exists some constant, let's call it a c single, such that the kind of density ratio and this importance rate between the distribution induced by the open policy, let's say PI star, and this distribution induced by your behavior policy, this importance rate is upper bounded by this constant, say Single.
00:08:36.974 - 00:09:48.554, Speaker B: Okay, there's some stronger assumption people have made in previous work showing like you cover all state actions, but this is much weaker because you only need to cover the state actions visited by the outcome policy. And in terms of the scaling, the single can be, in the best case, it can be one, when say rho equals to PI, then say single becomes one, but it can also be infinity if you do not have some state action that you do not cover by this behavior policy. Okay, so sample convexity will eventually depend on this concept. Okay, so in terms of algorithm, there's a ultimate idea called pessimism, which basically penalize uncertain policies. So we will be more conservative when choosing the output policy. And for this setting, there are also optimal bounds where scaling is s h cubed c single divided by Epsilon Square. Okay, but this is what we know about single agent offline reinforcement learning.
00:09:48.554 - 00:10:29.704, Speaker B: So this work, we studied the two player zero sum Markov games. So the sighting is similar. So here we are controlling there are two agents at each time step. They are all at both at some state stage, and they choose a pair of actions at and bt. So a is action for we call Max player, b is action for the mean player, and then they receive some reward based on the state. And this action pair and the environment will transist these two players to the next state according to a transition function. And again this repeat for each time, each planning horizon.
00:10:29.704 - 00:11:20.074, Speaker B: So this is a zero sum game. So the max player with a sequence of actions, a one to h, this player tries to maximize the expected total reward. And we also have a mean player which an action sequence, b one to BH, who tries to minimize the expected total reward. So our key insight actually is just by starting a much simpler version of the two player zero sum games, Markov games. We call it zero sum bandits. Basically it's a special case of zero sum Markov games in the case that h equals one. So basically you don't need to do any planning, there's no transition at all and you have fixed state, so s equals one.
00:11:20.074 - 00:12:04.834, Speaker B: In this case, only the reward function matters. Basically, the max player want to try, try to find a, you know, action such that can maximize reward. And meme player tries to find minimum reward. Okay, so this is the setting. So again we study the most basic tabular setting. We have finite number of states, finite number of actions for both players to rewards bonded by one, and sample complex will scale with these factors. Okay, so in terms of the, we also need to introduce some notions in this setting because it's game.
00:12:04.834 - 00:13:00.382, Speaker B: So we have a policy pair, let's call it new and new. So say Max player is playing this policy new, and mean player plays this policy new. So generally these policies are stochastic policies, because this is a two player game. So the policy is mapping from the state to distribution over the actions. So, similar to single agent setting, we can define the q function, the volume function with q function. Given a set of a pair of policies, mu and new is defined as the starting from state and the pair of actions, the expected reward from this time step h. And then following the policy pair mu and nu, and for the value function, it is the expected reward from this state.
00:13:00.382 - 00:13:50.114, Speaker B: S following this policy, pair mu and Nu. So here we also need to define the best response and for both maximum mean players. So given a policy for the banks player, the best response for this policy, it's a policy for the mean player such that can minimize this value function. So that's why it's called basic response and notation. We denoted as the V Mu, which is the value of the best response for a given policy of the max player. And also we can define the best response for the mean players by symmetry. So for mean player policy new.
00:13:50.114 - 00:14:42.532, Speaker B: The max best response volume is v new, which is value achieved by maximizing or the all policy of the max player for this given mean player's policy new. Okay, so a Nash equilibrium, say Mu star, new star, satisfy that the new star is basically the best response for Mu star, and Mu star is the best response for new star. It's a standard result in game theory. And so we eventually want to output a pair of policies, say new and new. So we want to measure how good it is. The measure we use is called duality gap, which is difference between the best response values of the mean player and the max player. Okay, so this is the archic gap.
00:14:42.532 - 00:15:24.352, Speaker B: And our goal is to find a pair of policies such that this gap is more than epsilon. Okay, so, okay, now let's study what is the offline setting. So, similar to single agent setting, we have batch of data sample from some data collection policy pair. So now we have a pair of policies and the pair we didn't ask row. And this row will induce distribution over the state and action pairs. Also, it depends on some transition p of the environment. And goal is to learn a policy pair set.
00:15:24.352 - 00:16:25.418, Speaker B: This gap is smaller than. Absolutely. So again, before study, under what kind of condition about this distribution induced by the behavior policy under which we can learn a near Nash equilibrium. And the first thing you may think about is to generalize the idea in the single agent setting. In single agent setting, we know that you only need to cover the optimal policy and then it's enough to recover the new optimal policy. Okay, so here, can we do the same? Um, so you, uh, the analog is corresponding assumption is the following, that the density ratio or the importance rate between, say, um, a pair of natural equilibrium, mu star, v star over this data collection, uh, uh, policy data distribution induced by this policy, uh, it's upper bounded by some constant. Okay, so this is, uh, natural generalization of the single policy coverage in a single agent setting.
00:16:25.586 - 00:16:33.082, Speaker A: So, one question. Inside your capital d set, you meant to have a and b in there, like.
00:16:33.178 - 00:16:39.294, Speaker B: Ah, yes, sorry. This should be b reward and next stake.
00:16:39.914 - 00:16:43.602, Speaker A: So both players are observing the state, the two actions chosen.
00:16:43.658 - 00:17:00.724, Speaker B: Right, right. So here we just consider, yes, we are in an offline setting. Basically we can observe everything and the data can be generated according to any policy. And we study what kind of properties of this policy pair, we can learn a good natural.
00:17:01.544 - 00:17:02.444, Speaker A: Thank you.
00:17:02.784 - 00:17:28.044, Speaker B: Yeah, sorry. So this should be here. Okay, so first, let's study whether this condition is enough. Like we only cover one Nash equilibrium, our first result. And there's actually the key takeaway from the talk is that this is not possible. So we give a counter, very simple counter example. Showing this assumption alone is not sufficient.
00:17:28.044 - 00:18:05.914, Speaker B: So here's our counter example. So let me dig into a bit detail. So basically, you have two games, one. In both games, you have two actions for each player, say a one, b one and a one, a two and b one b two. Okay? So the first game, if you play a one b one, you get reward 0.5. A one b two get reward one, a two b one, you get reward zero, and a two b two get reward 0.5. So for this game, the national equilibrium is just a one b one.
00:18:05.914 - 00:18:37.214, Speaker B: We can check that very easily, because at this point, there's no incentive for both player to change this action. The second game, it's this one. The only difference from the first game is that we change this a to b 2.5. Okay? But this change also changes the Nash equilibrium. The Nash equilibrium for game two becomes a two b two. Again, you can check this by. There's no incentive for postpaid to change its action.
00:18:37.214 - 00:19:30.634, Speaker B: Okay, so now let's consider the following setting where our data set covers a one, b one and a two b two with equal probability. Okay? So this data set consists of many a one b one, a two b two, and we observe this reward. In this setting, we, this dataset does cover the Nash equilibrium for most games. Okay. And in this setting, the C single is two, because it use a one, b one, a two, b two with equal probability. Okay? However, the main problem for this example is that with only with this data set, you cannot differentiate whether we are in game one or game two. Right? Because we can only observe that you play a one, b one and get 0.5
00:19:30.634 - 00:19:51.124, Speaker B: reward. You play a two, b two, got 0.5 reward, and basically, now differentiate these two. Okay, so this is basically our counter example. It's very simple. It shows that if you only cover, you tends to only cover Nash equilibrium. It's not enough to learn this Nash equilibrium.
00:19:51.124 - 00:20:32.162, Speaker B: Okay, so basically, the insight is that you do need some information about other action pairs in order to identify which is Nash equilibrium. Okay, so what assumption can we make? So now we need a stronger assumption in order to learn the Nash equilibrium. So this is assumption we make. So this is the second takeaway, like we believe this is a natural assumption. Okay, so let me first introduce this assumption. So the assumption is following. So, for natural equilibria, mu star, new star.
00:20:32.162 - 00:21:34.074, Speaker B: Okay, we need a behavior policy covers mu star, new for all new and mu, new star for all mu. Okay, so this table shows the intuition. So again, this a one b one is a Nash equilibrium. So, we not only need to cover a one b one, we also need to cover all the all a one b one a one b two to a one bb. So the entire row also, we need this entire column, say a one b one a two b one to a b one. So, this is the unilateral coverage assumption we propose. So, more formally, we assume that there is some constant, say unilateral, such that the density ratio between any d mu star nu over zero and d mu star and zero is upper bounded by this car.
00:21:34.074 - 00:22:20.694, Speaker B: And again, our semiconductor will depend on this constant. And this requires for every state action pair and every policy pairs. So in this, I think you can see here there's a difference between the two player zero sum game and single agent. For a single agent setting, the C single can be, in the best case, can be one, but in this case, c unilateral is, in the best case, it can be just a or b, because you need to cover the entire row, one column. So in the uniform distribution case, it would be a or b. And for the others action pairs, you can cover them or not, doesn't matter. But if you cover some, then we will make you a singular side computer.
00:22:20.694 - 00:23:18.304, Speaker B: Okay, so we also show that, I don't think I have time to go over this, but you cannot really weaken this assumption. So again, we can propose some example that if you only slightly violate this assumption, again, you have this unidentifiability issue that you can now differentiate two games with different Nash equilibria. So basically, we show that this assumption cannot be weakened. All right, so now let's go to the algorithm. We show that under this unilateral assumption, you can actually learn a natural equilibrium. So for the bandit case, the algorithm idea is similar, or it's natural extension of the single agent setting. So, we estimate each reward and then we obtain an upper bound and lower bound based on, say, hoeffding bonds, things like that.
00:23:18.304 - 00:24:32.154, Speaker B: And then we compute natural equilibrium for using the lower bound, and we contain a pair of policies, say mu under bar, new under bar, and also we compute natural equilibrium for the upper bound of this reward, say new upper bar, new upper bar. So this is an illustration. So, we obtain an interval for the reward of the interval for the reward, and we use upper bound to compute one Nash equilibrium, slower bound to compute Nash equilibrium. And here our output is for the max player as the policy corresponds to the lower bound of the reward. And our output for the mean player is the policy that corresponds to the upper bound of the reward. So this represents the idea of pessimism because we kind of penalize uncertain action pairs. So for both player, we use this prismatic policy to ask our output.
00:24:32.154 - 00:25:20.242, Speaker B: Okay, so this is our optimum idea. So kind of a natural extension of the single agent setting. And for the results, um, so with this unilateral coverage assumption, uh, we can have this kind of bond like it's ab times c unilateral or epsilon squared. Uh, if you have some stronger assumption about the coverage, that's called uniform coverage, which basically means that you cover entire, all the action pairs and prove to say unif, which is the constant for, uh, this coverage assumption, uh, or absent square. And for term based games, again, we can obtain stronger bond, which is c unilateral or epsilon square. So these results justify that this assumption is a sufficient assumption. And furthermore, as I showed before, this assumption cannot be weakened.
00:25:20.242 - 00:26:07.156, Speaker B: So this is kind of complete picture about this assumption. At least this assignment cannot be weakened and also sufficient. And in terms of lower bound, we just inherited from the single indian setting. With this unilateral assumption, you can obtain a c unilateral over epsilon square thermal complexity. So you can see here there's a gap and it's still open whether you can close this gap. But for the uniform coverage or turn based games, we can obtain the matching upper and lower bounds. Okay, and for the Markov setting or the reinforced margin setting, I don't think I have time to go over all the details, but basically it's similar to the single agent setting.
00:26:07.156 - 00:26:58.082, Speaker B: You do some dynamic programming as some bonus to clockwise uncertainty. And again, you obtain both upper and lower bounds for each state. Action tuples. And then you compute q function, three function, and what you output? Again, it's an asymmetric output for both players. And here, the most technical part is how do we design a bonus? Here we use a reference function and Bernstein bond to obtain a tight bonus, or at least to get a tight dependency on the h factor, the horizon factor. But our dependency on a and b are still not tight for this setting. But again, for uniform coverage and turn based game, we have a matching upper and lower bound.
00:26:58.082 - 00:27:44.604, Speaker B: So again, this also shows that this unilateral assumption is sufficient for this Markov gains. Okay, so in summary, so we gave kind of this first study on when it's possible to learn this natural equilibrium offline setting. And the key I take away is that the single policy coverage is not sufficient. So this is kind of separation between the single agent setting and the two player zero sum setting. And we propose this unilateral covet assumption. It is sufficient and cannot be weakened. The algorithm is found pessimism for both players, and sometimes we can obtain optimal bounds and sometimes still a polynomial gap.
00:27:44.604 - 00:28:24.802, Speaker B: I also mentioned that there's a concurrent work with some authors from Berkeley as well go around. They also study the same setting and also study the linear MDP setting as well. So, some open problems, whether we can improve this dependency on a and b. That's one natural assumption of a natural question. Another question is whether we can generalize these kind of assumptions. And I'll put some ideas to multi agent settings. And we have an upcoming work working on this directions, and we'll release it maybe a couple of weeks later.
00:28:24.802 - 00:28:28.774, Speaker B: And thank you. And I would like to take some questions.
00:28:38.034 - 00:28:43.774, Speaker A: Thank you very much, Simon, for a very interesting talk. Do we have any questions in the room?
00:28:49.364 - 00:29:07.860, Speaker C: Hi, Simon. Thank you for a very nice talk. I'm wondering, for the algorithm, because in the online setting, when we do Markov games, we usually just compute one upper bound and one lower bound, and then we do the, like, nash equilibrium with upper bounds and lower bound together. But it seems like you are doing natural equivalent for the lower bound. Lower bound.
00:29:07.892 - 00:29:27.966, Speaker B: Or basically just, you know, we have. You just run this algorithm twice. You can obtain a lower amount for this reward, and then compute natural equilibrium. And then you compute the upper bound, and you compute natural equilibrium, and your output, it's one from the upper bound and one from the lower bound.
00:29:28.070 - 00:29:43.828, Speaker C: Yeah. What I'm saying is, like, I think in the online setting, what we typically do is, like, we combine our upper bounds and our lower bounds, we compute the natural equilibrium for that general sum game. Yeah, I'm wondering if. I'm wondering if something like that would work or. Or that would not work.
00:29:43.876 - 00:30:06.754, Speaker B: Um, that may work, but, um, here is. I think this our version, like, at least in the offline version of offline setting, is, uh, you can use a simple algorithm, because you only need to compute the, uh, zero sum game. You don't need to compute the general sum game, so that somehow, uh. Okay, offline setting makes it simpler. Yeah.
00:30:06.874 - 00:30:07.974, Speaker C: Okay. Thank you.
00:30:18.074 - 00:30:33.434, Speaker D: Thanks, Simon. So, one question, generally, what is the setting where you imagine you'll have players playing, like, randomly, and you have data offline data of this form? Were you also active? Active experimentation?
00:30:35.214 - 00:30:42.350, Speaker B: So you're asking when, like, how this data is generated, or.
00:30:42.502 - 00:30:53.510, Speaker D: Yeah, like what? Yeah, like, maybe you mentioned the beginning, forgot like the practical scenarios where you have offline data, people not playing NASA equilibrium or some other equilibrium notion.
00:30:53.702 - 00:30:54.474, Speaker B: Yeah.
00:30:54.894 - 00:30:58.794, Speaker D: And not having the ability of active experimentation.
00:31:01.534 - 00:31:06.222, Speaker B: Oh, you mean what practical applications do we have this kind of in the.
00:31:06.238 - 00:31:09.110, Speaker D: Game applications, you can actively experiment. So it's not offline. Right.
00:31:09.142 - 00:31:36.864, Speaker B: So, okay. Yeah, that's right. In some environment setting, I mean, of course it's based on some assumptions. You do not want to really interact with the environment. And in some recommendation system, sometimes you can also model it as a game. And where you interacting online is kind of costly. So you do want to use some offline data.
00:31:36.864 - 00:31:44.664, Speaker B: Yeah, I agree. In games, you do have ability to have a simulator so you can get a lot of data.
00:31:47.084 - 00:31:57.324, Speaker D: For example, would you expect if people do some no regret learning or something, some other form of learning that's natural, that your coverage criterion will be satisfied?
00:31:59.784 - 00:32:03.464, Speaker B: So you mean when this condition will be satisfied?
00:32:03.584 - 00:32:13.044, Speaker D: Yeah. So if you have some form of learning players, when you collect data from learning agents that use some form of learning algorithm, would your coverage be satisfied?
00:32:14.184 - 00:32:38.624, Speaker B: That's a good question. So for, I'm not sure about no regret learning whether you can generate data set that satisfies our assumption. But one simple example is that if you both player play the actions uniformly randomly, then you can have a dataset like size flower assumption.
00:32:43.824 - 00:33:12.084, Speaker A: Well, all right, so I have one question. So if, like, what is, what would be a generalization of this coverage to, you know, like infinite action spaces and stuff like that? Like, okay, either in single RL or like, or you know, just even do just normal form, like single shot games.
00:33:12.804 - 00:33:47.984, Speaker B: For example, in the linear setting, what you need is kind of least eigenvalue of the coherence matrix collected by defined on your feature space. That's what people use for linear setting. For more general, if you have a feature space, for more general setting, I'm not sure. I think there sometimes people still use this kind of density ratio to, or this density ratio to define the coverage.
00:33:50.244 - 00:34:09.644, Speaker A: All right, thank you. So, in the interest of time, let's take further questions offline. Thank you, Simon, for your excellent talk. And next up, we have Sylvana. You want to set up.
