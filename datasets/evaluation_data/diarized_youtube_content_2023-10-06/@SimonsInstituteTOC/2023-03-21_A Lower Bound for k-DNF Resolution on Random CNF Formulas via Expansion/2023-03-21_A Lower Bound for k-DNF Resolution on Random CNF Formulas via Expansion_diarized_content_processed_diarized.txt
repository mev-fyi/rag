00:00:00.120 - 00:00:33.190, Speaker A: My name is Deja Safarella. Thank you for the introduction. Today, I'd like to talk about some lower bounds around the formulas in ADNF resolution group system. And this is based on a joint work with Merisha Kawov. And I think actually, especially the introductory part to this will mirror Dmitri's talk quite a lot. But just for the sake of completeness, like we started at the very beginning. Okay, so, yes, suppose we have some CNFDM formula.
00:00:33.190 - 00:01:18.780, Speaker A: And we would like to know if it's possible enough to prove efficient that this formula is unsatisfiable. And here proof is something that we can check by a polynomial time algorithm. And efficient proof is a polynomial size proof. And here's like an example of the most well known and well studied proof system, its resolution. Here we start with the original clauses of a formula phi. And we can use these two rules, weakening and resolution rule, to drive some new clauses until we arrive at an empty clause. And it is not hard to check that this system is sound complete.
00:01:18.780 - 00:01:49.520, Speaker A: And that the proven resolution can indeed be checked if polynomial time of its size. Now, there are lots of proof systems, right? And here resolution. Here is. And here's res kDNF resolution. So, like, there are some proof systems for which we can prove lower bounds. And there are some proof system for which we don't know how to prove level bounds. And there is a notable shortage of like methods for proving these lab rebounds.
00:01:49.520 - 00:02:32.314, Speaker A: Like say, for resolution. For we have this monoton interpolation thing. And for so's PCR and other things, we had the restriction technique, but we do not have much more apart from that. And yeah, I mentioned random formulas, right? So these are candidates to be hard for all proof systems. And again, in some systems, we do have lower bounds for random cnfs. And in some we don't say we have lower bounds friend and formulas in so's and VCR. There are some known in resC.
00:02:32.314 - 00:03:26.496, Speaker A: But we don't have anything for ac zero frequently. So in bound in depth Frege the lower bounds that we have there, like for more structured formulas, pigeonhole laxating. And there's always, always some kind of switching lemma involved in these lobber bands and random cnfs. We don't even have lover bounds for like depth two frigate. And resk is actually a subsystem of depth two frigate. So that was kind of aimed to motivate you that it's actually reasonable to study random formulas in rescue. Okay, so here in KDNF resolution, we have like these four rules which kind of mimic resolution with the difference that we do the operations with kdnfs instead of clauses.
00:03:26.496 - 00:04:14.904, Speaker A: So we have this weakening rule much like linear resolution. We have these two rules with and which help us to assemble and disassemble terms of bit kick. And we have this cut rule which is very much like resolution rule. Okay, so as I mentioned, it is a subsystem of ac zero three again, and actually non lever bounds. Random formulas in rescue are also based on some kind of switching on small restriction switching lemma. And also here's another fun fact about kdnf resolution. If we would have a nice enough lower bound for k, just slightly larger than logarithm, from that we would get a lower bound for all k, which would be just depth two phrake at this point.
00:04:14.904 - 00:04:53.424, Speaker A: Okay. And yeah, random delta cnfs. In this family of formulas we have n variables, mcloses and the most important parameter for this formula is density. This m over n the density of clauses with respect to variables. So there is this density threshold. And below this threshold these formulas are satisfiable with high probability. And after this threshold they are unsatisfiable with high probability.
00:04:53.424 - 00:06:09.806, Speaker A: And when you continue to increase this density more and more, these formulas become easier to refute with density increasing because we have just more constraints. And at some point when you have too many constraints, you will just have some local contradictions and these formulas would be very easy to refute. As I mentioned, this family is believed to be hard for any proof system just after this density threshold, like it's conjectured that just after this density threshold, this formula should be very hard to prove. And the nice thing about this random formulas is that their underlying graph is a bipartite expander. Now what is an expander here on left side you have m closings, on the right side you have n variables. And the fact that graph is an expander means this, that any small subset of vertices on the left should have a lot of neighbors on the right. So expanders have these like three important parameter r.
00:06:09.806 - 00:06:47.344, Speaker A: The size of the set on the left for which the expansion property works, delta is left degree. And there is this expansion parameter with like this epsilon involved here. And yeah, here's the picture. We also can ask for something stronger than just huge number of neighbors. On the right side we can ask for a huge number of unique neighbors. They are in pink in this picture. And a unique neighbor is a vertex which have just one adjacent vertex in the set eye on the left.
00:06:47.344 - 00:07:41.134, Speaker A: Okay, so yeah, underlying graph of random delta CNF is a bipartite spec. And what do we know about random double cnfs in kgnf resolution? Let's just denote this closed density thing as this d. And they're basically these two lever bounds by Arih Novich and by sigrilen Bassempaliatsy. In the first one, we have constant density and k up to square root of log n over log n. And in the second one, the density is n to the one over six. And here, delta depends on k, and k is constant. So what we were able to do is this.
00:07:41.134 - 00:08:11.424, Speaker A: We were able to improve both of these. In the first setup, we were able to push k up, and in the second setup, we were able to get any polynomial density like this. This should be read as for any polynomial density, there exists some delta such for any constant k. Yeah. And here delta is also independent k.
00:08:15.084 - 00:08:31.344, Speaker B: For any polynomial density or for sum. Because isn't it true that when the. Oh, okay. The polynomial has to depend on k? So it's like density n to the k over two becomes trivial. What?
00:08:33.604 - 00:09:03.414, Speaker A: I'm pretty sure it's for any constant k. But, like, first you should pick polynomial, and then you should pick delta. Yeah, that's, that's right. Way of the quantifiers. Yeah. So this is, this is the formulation result. Basically, what we require is that the dependency graph of the formula is a good enough boundary expander with the unique neighbors, and that k is small enough for this to be true.
00:09:03.414 - 00:09:20.374, Speaker A: And in this case, we have an exponential upper bound for the rescue proof. And now I would like to talk about the technique, which I know is kind of an optimistic for half an hour talk, but let me still try to do that.
00:09:23.634 - 00:09:32.834, Speaker B: Can you repeat, please, the difference between the SBI and electropic result? And what is the improvement?
00:09:32.874 - 00:10:09.274, Speaker A: That I don't. Yeah, sure. That's basically summarizing this table. So, like, in the first setup, m, the number of closes is constant times m, and the k is this square root of log n of login, and we were able to get slightly large. Okay. And then the second setup here, like, there's polynomial density, but for a small polynomial, and also depends on k. And here in red is the incremental.
00:10:14.134 - 00:10:21.710, Speaker B: In the first example. Now, you mean that delta equal three? Is that obsession?
00:10:21.862 - 00:10:50.874, Speaker A: No, it's not. Delta equal three we want. You weren't able to achieve that delta. About 100 should do. Yeah. Okay. So shall I move to the technique? Okay, I mentioned that basically everything that we have for rescue is a restriction.
00:10:50.874 - 00:11:28.622, Speaker A: And how does this work. A general scheme for any kind of restriction technique is something like this. We would like to apply a restriction which preserves the structure of a formula which doesn't refute it right away, but it should also decrease some parameters of the proof in a predictable way. So the reasoning would be okay, we apply the restriction, some parameters of the proof decreased, but the formula is still not refuted. It's to retain some of the complexity. So these parameters are not zero. So in the beginning they will be huge.
00:11:28.622 - 00:12:04.054, Speaker A: The proof was huge, something like this. So we would like to make restrictions to expand your base formulas that leave them hard. And that's actually kind of the oldest trick in the book, the closure thing. The expanders can fix themselves. Then you apply, then you delete some part of the graph. Again here the left part closes the right part of variables. And imagine I want to substitute some variables on the right, the set t.
00:12:04.054 - 00:12:49.346, Speaker A: And after that the graph loses its expanded properties. And it turns out that we can fix that. We can delete some other small part of the graph such that the rest is expanded again. And this trick is widely used to improve complexity, to prove some level balance again in resolution, PCR, so's etcetera, etcetera. But the thing is, you can do this closure trick in different ways. So the first way to do quoting, let's just say that we delete the set that violates expansion. So we again, we substituted some set of variables t.
00:12:49.346 - 00:13:29.050, Speaker A: And then on the left we have some clauses which do not have that many neighbors. So we just substitute all their neighbors to satisfy those clauses. And yeah, that is basically deleting this whole blue part. Now it turns out that this closure part will not be too big. We don't lose much in the parameters of our expander. We lose something in the r parameter, and we lose something in this expansion parameter. And actually we cannot iterate this.
00:13:29.050 - 00:14:11.364, Speaker A: We cannot just apply this trick too many times because we will lose this expansion here. But actually, now let's do the second type of quotient. Let's say that we delete the maximal set that violates expansion. And it turns out that because expanders are so nice to us, this maximum set will still be not too big. We still have this predictable loss in the r parameter and the loss in the expansion parameter. But you can repeat this with the same guarantee for the expansion parameter. So you can delete the set t and its closure, and then you can delete something else and take closure again.
00:14:11.364 - 00:14:54.612, Speaker A: And you will not go below this to epsilon loss, which is pretty nice I think this is my favorite type of closure. But what we do is like this secret third thing. Let's say that we want to delete the maximal sequence of vertices such that each next vertex in the sequence violates expansion. So we deleted the set t. If we see a vertex that lost too many neighbors, let's delete it. If after that we again see a vertex that lost too many neighbors, deleted, and so on. And it turns out that the set that we get in the end, well, it's still not too big.
00:14:54.612 - 00:15:44.340, Speaker A: We still had this loss in the expansion, but now the set is also uniquely defined. And this type of the closure also have other nice properties which we will explore later. Okay, so we know how to make restrictions that viva formula hard, and now we want to learn how these restrictions affect the proof. Here is the important parameter for kdnfs, which is a covering number. So suppose we have a KDNF alliance. In the group are kdnfs, right like this on the slide, this is three DNF, and the covering is just a set of variables that hits every term. Just a hidden set.
00:15:44.340 - 00:16:38.284, Speaker A: Like here, it's x two, x six. And the thing about covering is if you have a minimal size of covering at least q, then you would be able to find q over k non intersecting terms in your DNf just by greedily picking the next one. And if you have a minimal size of carbon less than q, then you can do this thing. Let's just build a decision tree. And in this decision tree, let's just ask every variable from the covering and in the leads put our KDNF. But under this restriction, under the corresponding restriction on the path. Now as we have asked, at least one variable from each term, in the leaves there are k minus one dimensions.
00:16:38.284 - 00:17:33.176, Speaker A: That's like the way to decrease the width of the terms if we have a small covering. Now this leads to this idea. This is actually what both of these lower bounds and random cnfs use, and it's making use of this dichotomy. So for every line of the proof, either we have a big covering number, and then we would be able to find a lot of independent terms which are easily killed by random restriction. Now this is like, this is not obvious how to extract those independent terms, because the fact that the terms do not intersect, that doesn't necessarily mean that they're independent under our restriction. But we will address this later. Just imagine that we were able to extract this independent terms.
00:17:33.176 - 00:18:05.050, Speaker A: Yeah, this is the first picture. Yeah. And otherwise these terms intersect a lot. And we would be able to replace this line in the proof by decision tree plus a small collection of k minus. Then just repeat this k times and what is left is actually a resolution curve. Now that's the picture, that's the scheme. So again, we start from a sequence of kdnfs.
00:18:05.050 - 00:18:42.824, Speaker A: We apply the restriction and some of them are killed, and some of them are transformed into those trees. And then we apply restriction again. And each one of the k minus one DNF's in leaves is either killed or we can continue to build those trees, do these k steps. In the end, you have a sequence of decision trees. And like this just didn't came from nowhere. This came from a k DNF resolution proof. So this decision trees can be transformed into a resolution proof for which we know how to put layer bound.
00:18:42.824 - 00:19:59.024, Speaker A: So that's kind of the general scheme behind those two lava bounds and random cnfs that were known. And yes, we will address the issue how to pick those independent terms now, because we know that the restrictions in expanders have of a very special kind, right? They need to be closed. So how do we pick independent terms? Well, one could ask for something else, except for the expansion property, for example, lower degree of the graph, and that kind of bounds the density of the formula immediately. So, if you just try to pick those independent terms such that their neighborhood in the graph does not intersect, then they are independent. But actually, we do not need low right degree to address this issue, because closure encapsulates very accurately what part of the graph actually depends on the term. It's just closure and its neighbors. And the closure has small size and it's independent of the right degree of prep.
00:19:59.024 - 00:20:44.824, Speaker A: So yeah, let us return to this picture with the pink cucumbers. So we have this dichotomy with the covering number. And as I mentioned, expanders are very nice to us. So let me just try to say magical word closure three times, and say, if we get any overbound. And yeah, that's it. So instead of just covering number, let's just look at closure covering number, and we will look for closure independent terms, and we will reduce not the size of terms, but the size of closure. That's the definitive three times.
00:20:44.824 - 00:21:38.710, Speaker A: Okay, so now the dichotomy is like this. We either have a lot of terms which closures do not intersect. Well, either we have aligning the proof and the closures of terms intersect a lot. So yeah, closure independent terms behave very nicely under closed restrictions. So indeed, if we have a lot of closure independent terms, then this line in the proof will be very easily killed by the type of restriction that we use. Otherwise we will be able to just build a decision tree like before and reduce the sizes of closures. And we know that closure doesn't have much bigger size than the term itself.
00:21:38.710 - 00:22:06.050, Speaker A: So let's just iterate this all k times and we are left with the resolution proof. Again. And this is the modified picture we start from. Again, sequence of kdnaps closure is quite small. We apply restriction. We get the sequence of the trees, and in the leaves of this tree, closures of all terms are smaller. Apply restriction.
00:22:06.050 - 00:23:04.852, Speaker A: Again, we do this Ck steps for some constant c. Again, we end up with a sequence of decision trees and we transform it into resolution proof, which again we know how to pull up the bounds. Now this might sound a bit suspicious because closure depends very heavily on the structure of the graph. Can we really do this? Can we really just gradually decrease the size of closure if we apply all the, all the restrictions? Yeah, yeah we can. Because the third type of closure has this nice property, it's subgraph preserving. So yeah, that's why we had to do the secret third thing. Okay, that's, I think that's pretty much it.
00:23:04.852 - 00:23:59.778, Speaker A: And I also have this slide with a couple of open problems. I mean, it's always nice to push k up in KDNF resolution. And one could also try to prove some lower bounds for weak pigeonhole, at least for k equal to. That's it. Thank you very much for listening. Any questions? Yeah, yeah, I think that's actually a question which kind of requires a very long answer. Why? Why the parameters are like this.
00:23:59.778 - 00:25:02.954, Speaker A: But I think maybe something that's kind of related to your question is the fact that the sequence of appliance restrictions, it's not actually sequence, it's one restriction. So there's like in the original ideas, there's like some kind of switching m out right here and here we reduce some other measure, but it's still, it's not, it's not like a sequence of restrictions. I guess if we had a property like in the second type of closure where we could iterate this, a lot of times I think we would be able to get a better k. My question is what you think the barrier is of this beyond square? I have no idea, actually. Yeah, that's, that's pretty much the answer to your question. Can you tell us why? Why if you could do a little bit better than login, you could get depth too. Yeah, sure.
00:25:02.954 - 00:25:43.794, Speaker A: Suppose you have like a group which has larger k, right? So let's just hit the restriction and all the terms that are greater than seven width, they get killed. And if we kill all the terms with big k, I just left with I'm not sure what range of parameters is here, but I guess we could go from exponential size proof for case like a larger than operation to yeah, any other questions?
00:25:46.294 - 00:25:51.754, Speaker B: So you mentioned the weekly journal principle for is there any connection?
00:25:53.974 - 00:26:16.534, Speaker A: Yes, a little bit like with the restriction thingy. The thing is that it seems like this problem would require some entirely different techniques. We do not know how to prove something that is like entirely restriction based or principles.
00:26:20.034 - 00:26:28.694, Speaker B: This for n squared features for n squared meters for linear equivalent.
00:26:38.214 - 00:26:38.654, Speaker A: Thank you.
