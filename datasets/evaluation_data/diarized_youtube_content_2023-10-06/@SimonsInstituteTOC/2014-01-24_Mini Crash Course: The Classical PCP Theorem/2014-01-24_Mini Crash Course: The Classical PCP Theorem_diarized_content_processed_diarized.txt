00:00:04.600 - 00:01:23.354, Speaker A: So some time ago, Umesh asked me if I could come to this booth camp and talk about the PCP theorem. I said that maybe with four or 5 hours I would give an introduction, but I wouldn't come to Berkeley in the morning. Then when I said, oh perfect, no problem, you start at 930 and you have an hour and a half. So I'll see what I can do here. So first I want to so I described two equivalent statements of the PCP theorem and their equivalents, very simple equivalents, but very useful because some arguments are much more natural from one point of view and some other arguments are much more natural from the other. So a lot of complications can be saved by having in mind this ability to switch from one point of view to the other. And I'll also discuss the parallel repetition theorem, which gives an equivalence of the PCP theorem to a highly non trivially equivalent other formulation.
00:01:23.354 - 00:02:52.654, Speaker A: And then I want to say something about the two known proofs of this theorem, the original proof from 1992 and Iridino's proof from about ten years ago. But in the interest of time, instead of trying to redescribe all the details of those arguments, what I will do is to describe two constructions of error correcting codes. They contain all the so conceptual ideas that appear in these proofs. And then, as some allows, I'll describe how those proofs actually work. And what are the technical difficulties that come up in implementing those ideas that are very simple to apply to construct codes in order to actually prove the PCP theorem. So the original proof sort of corresponds to algebraic construction of error correcting codes using also concatenation. And the norse construction corresponds to a construction of error coroutine codes using expanders.
00:02:52.654 - 00:04:45.688, Speaker A: That doesn't really make a lot of sense as a construction of codes, while those are actually fairly good codes, but it's just a very good model of what goes on in this program. So let me begin from the statement, and feel free to ask any questions. So in the, in the first statement. So we start from some np complete problem, like three coloring or three SAT, let's say three satan. And the statement says that there is a reduction polynomial time computable. So this is a polynomial time algorithm that, given a formula given input to three satisfaction, given a three CNF formula, phi constructs an instance of a problem that I will call three CSP for boolean constraint satisfaction problem with three variables per constraint. Let me see here how it looks like an input to a three constraint satisfaction problem is something where there are boolean variables, just a slightly more general version of three SAT.
00:04:45.688 - 00:07:37.418, Speaker A: There are boolean variables, and then there is a collection of boolean constraints, each with at most three variables. So, for example, an input of three CSp could be something that says the constraint x one, x two and x four, the constraint x two or not x three, and then the constraint x one equal x two x over x three. So, and the goal would be generally to satisfy as many of those constraints as possible, ideally all of them three SAT would be the special case where every constraint is there, or between three variables, possibly with complementation. So the claim of this theorem is that there is a reduction takes some input of an np complete problem constructs a collection of boolean constraints of this form with the property that if the original input was satisfiable, then the constructed three CSP problem is satisfiable. And if the formula is not satisfiable, then every assignment to the variables of the three CSP problem contradicts some constant fraction of the constraints, and here epsilon is some absolute constant. In fact, for this statement, epsilon could be 24% with the using more recent proofs. So in this form, this is a statement about complexity of approximation.
00:07:37.418 - 00:08:37.354, Speaker A: It says, if you are looking at boolean constraint satisfaction problems, even if they are fairly simple constraints with just three variables per constraint, it's NP hard to get approximation beyond some constant factor. Now, the term PCP in the term PCP theorem, PCP stands for probabilistically checkable proofs. Well, here, this is about hardness of approximation, and so checking of proofs. It's the second equivalent point of view about this theorem. So me. So describe how from this one gets to think in terms of proof checking. So suppose that we suppose that we know that something like this is true, such a reduction exists.
00:08:37.354 - 00:10:46.454, Speaker A: And suppose we are given a Boolean formula phi. So a witness or a proof that the formula is satisfiable would just be an assignment to the Boolean variables to make the formula true. But we can think of, given this reduction, about a more complex, also more interesting witness or proof that the formula is satisfied, which could be the you could compute. Let's call this reduction r. So compute R of I and then define a witness that file satisfiable as a satisfying assignment for this constraint satisfaction problem. Now suppose that someone gives us a Boolean assignment with the claim that it satisfies the constraint satisfaction problem coming from this theorem, and hence witnesses that the boolean formula is satisfiable. Well, instead of just looking at all of it, we can construct the constraint satisfaction problem that comes from the deduction.
00:10:46.454 - 00:12:08.624, Speaker A: Pick at random one of the constraints and then read the three variables appearing in that constraint in this assignment, and then see if just that particular constraint is satisfied by this assignment. So then what's, what's going to happen? If, if this were actually a satisfying assignment with really one, it will verify the constraint. But if the Boolean formula was not satisfiable, then no matter what's written here, there will be a constant fraction of those constraints that are not satisfied by this assignment. So even just picking one at random, there is some epsilon probability or 24% probability. Using the best known proof that we will see an inconsistency. And in fact, if we pick at random, large constant number of constraints and 100 over epsilon of them, and we look at whether they are all satisfied by this assignment, if this is satisfying, we will see that we predict one. But if the Boolean formula is not satisfiable, there will be astronomically small probability that no matter what's written here, will satisfy this verification.
00:12:08.624 - 00:13:07.834, Speaker A: You must think of it in the following way. Suppose that like to make it even more assessment about proofs. So imagine that you are interested in proofs of some particular mathematical statement like the Riemann hypothesis, and you're interested in whether there exists proofs in some formal language. Maybe the proof verification system cock up to a certain length, maybe a gigabyte. Then you can certainly write a large, but not tremendously large three CNF that will be satisfied if and only if there is a proof of the Riemann hypothesis of length at most 1gb in Cog. So you could compute this reduction. This will give some particular collection of Boolean constraints over triple sub variables.
00:13:07.834 - 00:14:16.104, Speaker A: So then what's going to happen? Someone says, okay, I found a proof for the Riemann hypothesis. I have encoded it as an assignment for this formula. I also re encoded it as an assignment for this collection of constraints. Now, we pick at random 100 of those constraints and we read the 300 bits in this proof. Well, if it was valid, the whole thing will check. But if this check is verified with probability, even kind of negligibly large, that means that this formula had to be satisfiable, which means that somehow whatever is written here is in fact proof of the Riemann hypothesis, or at least if what we observe at only a probability one two to the 100 of occurring if the Riemann hypothesis has been false. So from a frequentist not bianca point of view, you can have very high confidence that the proof is valid.
00:14:16.104 - 00:17:43.943, Speaker A: Now the, now that's some features of this verification what you're saying is that there is some verification procedure for non traditionally encoded assignments for trizat such that whenever formula is satisfiable, the verification procedure accepts with pre d one when it's unsatisfiable, no matter what is written, even if it doesn't satisfy the definition of the encoding it rejects with some constant probability, it only looks at a constant number of bits, and the amount of randomness needed to generate the locations that are being ready is only logarithmic in the size of the formula and in the length of this proof, because all the randomness that we used was to pick a random, one of those constraints. So let me write down a definition. We'll say that decision problem, like some computational problem with a yes or no answer like Trisat, belongs to what we call pcp one s order of log n q. If there is a verification algorithm v, fancy polynomial time, uses order of logarithm bits of randomness. So this is the, this part of the definition given an input x to the problem and proof y will select few locations in. Yeah, we'll read the proof just in these locations and then accept or reject. And here this q.
00:17:43.943 - 00:19:34.394, Speaker A: This q is this q in this example was three. And then the properties are that if we have an input for which the correct answer is yes, there will also be a proof that v accepts with probability one. And if an input is not in a language, then no matter what proof is being presented, vx adds to pretty at most s. Okay, and this is the, and this is this parameter. And this formulation of the PCP theorem is that three Sat. But in fact any problem in NP belongs to such a class. There exists an absolute right now.
00:19:34.394 - 00:20:16.682, Speaker A: So I just want to verify that not only this statement about hardness of approximation implies this statement about checking proofs by only looking at a constant number of bits. But in fact the two statements are equivalent. Q is three. The definition is sort of valid for every parameter. Here we could state the theorem. Then we'll see that it's equivalent under so different choices of the number of queries. For now we'll state the theorem where the special case in which q is equal to three, this is stronger.
00:20:16.682 - 00:20:49.342, Speaker A: The smaller is the number of queries. Because the verifier can always make more queries and ignore the answer. It's false when the number of queries is two. So three is the minimum for which it works. And increasing the number of queries just makes epsilon larger. It is possible to increase the number of queries and make this probably one minus epsilon smaller or epsilon larger by simply repeating the verification procedure several times. So we'll talk about those equivalencies later.
00:20:49.342 - 00:23:19.074, Speaker A: But if you repeat this k times, the number of queries will be three times k. This probability will be one minus epsilon to the k. So to state those equivalencies in a way a bit more compact, let me give this just one more definition. I'll call the gap constraint satisfaction problem one s to be the problem where we're given an input, a constraint satisfaction problem with three variables per constraint. And we want to distinguish the case where this is satisfiable from the case where every assignment contradicts, every assignment satisfies at most the fraction s of constraints. So this statement of the PCP theorem, the first statement is saying that Trizat reduces in polynomial time to a problem of this form. And the second statement says that Trissat belongs to this probabilistically checkable proof class.
00:23:19.074 - 00:24:28.104, Speaker A: And we've argued that this implies this, that if you have this kind of reduction, you can use it to give a definition of a proof checker that verifies the validity of a three set formula by checking a random one constraint in this reduction. But the other reduction, I mean it's also possible to get a reduction in the opposite direction. So to kind of clarify that those two points of view are completely equivalent. So suppose we have machinery of this form for three sat actually for any problem of our choice. So we give an input to our problem. We are also given some proof. Pick at random three locations in this proof, we read them and then decide whether to accept or reject.
00:24:28.104 - 00:25:57.244, Speaker A: And we make this selection using only a logarithmic amount of randomness. Then how do we get a reduction of this form? So we can do the following. Suppose we have sediment number two. We're given an instance of three saps. So we will simulate what the verifier is doing for all choices of the randomness. Now we'll think of the proof as being kind of parametric. Say the proof will contain some collection of bits, let's call them y one, up to ym, which we don't know what they are.
00:25:57.244 - 00:26:51.764, Speaker A: And then we see what the verifier would do over the proof for all choices of randomness. So you can say well, suppose the randomness is on zeros. Then we will select entries y two, y five and y ten. And then we'll accept or reject depending on what are the eight values of those three entries that it reads. And we have the code for v. So we can also figure out for each of the eight possible outcomes of those queries whether it accepts or reject. So maybe we verify that this accepts, maybe if and only if y two is equal to y five and y ten.
00:26:51.764 - 00:27:49.214, Speaker A: So then we will write the constraint y two equals y five and y ten. Then we move on to the next possible choice of the random string. So maybe this time we, with this choice of randomness, selects y one, y ten, and y eleven, and we can again simulate what it does for each of the possible outcomes. And maybe it only accepts when yn is equal to one. So we'll write and equal to one. So we can do that for each choice of randomness, because there are only polynomial in many, and we construct in this way a polynomial size constraint satisfaction problem. And what are the properties of this constraint satisfaction problem? Well, every assignment to those boolean variables correspond to a possible proof in a proof system.
00:27:49.214 - 00:29:39.984, Speaker A: And the number of constraints that such an assignment satisfies is precisely the number of random strings that would make the verifier accept that particular proof. So for every assignment, the fraction of constraints that it satisfies is the same as the probability that the verifier would have accepted a proof identical to that assignment. Now, if the formula is satisfiable, there is a proof that is accepted probability one, which means that this collection of constraints is satisfiable. And if the formula is not satisfiable, then every proof is accepted at most with probability s, which means that for every boolean assignment to those constraints, at most an s fraction of constraints can be satisfied at the same time. So that gives a reduction of three sat to this kind of constraint satisfaction problem. And notice that it's with exactly the same parameters. The probability of accepting an incorrect proof becomes this fraction of constraints, and the number of queries becomes the size of the constraints.
00:29:39.984 - 00:31:44.844, Speaker A: And in fact, this is true also, if we look at probabilistically checkable proofs where some q number of queries are made, and we look at constraint satisfaction problems that in general, instead of having three variables per constraints, have q variables per constraint for some q. Now, having given the definitions of those problems, so we show a couple of trivial equivalent and then one highly non trivial equivalent formulation. Is there any question about any of this so far? So, one thing, as we mentioned before, it's always possible to decrease this probability of error by increasing the number of queries. And so if you have whatever you can do, just repeat the verification protocol three times independently, the number of queries will increase by factor of k, but the probability of accepting will be so the end of if you have to accept k times independently each time. This happens with probability at most one minus epsilon. So this probability can be made to go to zero. Also, this probability here is called the soundness of the system.
00:31:44.844 - 00:32:52.254, Speaker A: And the reason is that in this view, where we are checking the validity of a proof, the first condition is saying that if a statement of the form x is in l is true, then there is a valid proof of that. So that's a completeness statement, true statements and truths. And here this is saying that if a statement of the form x is null is false, then there is no perfectly valid proof. So it's a soundness statement, because it says there are no perfectly valid proofs of things that are false. By extension, this probability is also called the soundness probability, or just the soundness of the system. So the soundness can be made to go to zero exponentially in the number of queries. And this also means that when you're looking at constraint satisfaction problems, when you look at constraint satisfaction problems with more variables, the hardness of approximation increases.
00:32:52.254 - 00:34:21.134, Speaker A: And in general, when you're looking at constraint satisfaction problems that have some key variables per constraint, the best approximation you can get is exponentially small approximation factor in k, sort of. Conversely, if you have a PCP theorem with some large number of queries, you can always bring them down to three, maybe increasing the soundness. So if you have something like it. So up to the exact value of the soundness. If you prove the theorem, maybe with 25 queries, it's also true for three. And this is an implication that is easier to see in the optimization version of the problem. Let me briefly describe this.
00:34:21.134 - 00:36:12.794, Speaker A: So suppose I have this approximation problem to distinguish between satisfiable and one Manus epsilon satisfiable instances of Boolean constraint satisfaction problems with q variables per constraint. Well, this is a problem that I can reduce to the special case where every constraint is the or of q variables. And I can do this very simply. If I have some constraint involving lots of variables, I can always rewrite it as a CNF. If it involves q variables, I can always rewrite it as a q CNF, and this will be equivalent. But then I can just think of this collection of cnfs as just being a collection of terms. And then if this was satisfiable, this was satisfiable, and this is satisfiable.
00:36:12.794 - 00:37:09.704, Speaker A: But if every assignment contradicts some epsilon fraction of those constraints, it will contradict an epsilon fraction of those cnfs. So that means for an epsilon fraction of those cnfs, at least one term is false. When you write a function of k of q variables as a CNF, you don't need more than two to the q terms. So here there's at least an epsilon fraction of the CNF's in which at least a one over two, the q fraction of terms is false. So that's why the fraction is false, goes down by two de q. And then there is just the standard reduction of k sat to three satisfaction, where you lose an extra q minus two. So this reduction is equivalent to this containment.
00:37:09.704 - 00:38:35.494, Speaker A: So if you have the PCP theorem with any constant, the constant can be made with some loss, can be made to three. And also you can say that the verifier looks at those three bits and then just computes the order of them, doesn't do any other computation. Now, so in terms of how simple can be the procedure that the verifier employs here, we have something very simple. There are, it just reads three entries, every entry is a bit, and if it were reading two entries, the ethereum would be false. But so also minimal verification that the verifier could do would be if the proof is written down using a ternary Alphabet and verify only looks at two entries. So instead of looking at three bits, maybe it looks at two elements from a ternary Alphabet. Or in terms of constraint satisfaction, it's a problem where we have variables that take values over a triple of possibilities, and every constraint is only over two variables.
00:38:35.494 - 00:40:30.994, Speaker A: So to see this equivalence, suppose I have an instance of gap trisa. So this is a collection of constraints where each one is just the or of three variables. So we'll construct a new instance that has actually all the Boolean variables that I had before. But also, if there are m clauses, there are going to be m new variables that will take values in one, two, three. And then, intuition for this new problem is that the axis will be the Boolean assignment that satisfies the formula. And those extra variables for every clause will say, which is, say, the first variable that is true and satisfies the clause. Like maybe in this formula, setting x one to false and x two to false and x three to true and x five to true and x seven to true would satisfy those two clauses, and maybe also all the others in between.
00:40:30.994 - 00:42:51.304, Speaker A: And in the translation here, the axis would take the same value, maybe true is one, false is two, and three is never used. And then, you know, y one would be three, because the first clause is satisfied by the third variable, ym would be two, because it's the second variable that satisfies the clause, and so on. So in this new problem, the actually this is a place where it's better to think in terms of verification than optimization. So you want to define a proof system that makes two queries and the outcome are ternary. So here in this proof system, I have a part which is those axis and a part that is those y's. And then the verifier will pick at random, will read the corresponding picket randomly clause, and I will end, will read the corresponding yi. Then it will pick a random variable in the clause and it will read it here.
00:42:51.304 - 00:44:15.284, Speaker A: Now if this was maybe the first variable in the clause, and what you read here says the second variable satisfies the clause. In those cases you always accept. But if you happen to pick maybe the first variable here, and here it says the first variable satisfy the clause you accept only if that's actually true, only if this variable does satisfy the clause. So now we're only reading two entries in this proof, and they're only ternary. And I want to argue that clearly if this formula is satisfiable, this process accepts with probability one. But I also want to argue that if this formula was one minus epsilon satisfiable, this process has a probability, at least epsilon over three of detecting an error. So what's the reason? Let's look at the number of clauses that are satisfied by this assignment.
00:44:15.284 - 00:45:21.942, Speaker A: It cannot be more than a one minus absolute fraction of the clauses. So when the verifier picks a random clause here with probability at least epsilon, it's one of the clauses not satisfied by this assignment. Now, so, which means it's one clause in which all the three variables in a clause are false. So the, so the value here will point to one variable that does not satisfy the clause. Now, two out of three of the times, we're going to pick one variable that was not the one pointed at here, and so we accept, but one third of the time we're going to pick that one and we will see the inconsistency. So if we take epsilon over three, we see the inconsistency. Now, this type of proof system has the following properties.
00:45:21.942 - 00:47:09.896, Speaker A: It makes only two queries, and the proof itself is split into two parts, and the queries are made one on one side and one on the other. So this type of, so, I'll write what we just did. We sort of show that if some language reduces to gap three sat one, one minus epsilon or equivalently one, then this language belongs to what we'll call two prover one round proof system with parameters one, one manusception over three, logarithmic randomness and Alphabet size three. So here a two prover one round stems from the fact that when the verifier reads those two pieces of the proof, a conceptually equivalent view is that the verifier is actually interacting with two agents that don't communicate with each other and that are trying to convince the verifier that the formula is satisfiable. And the verifier interacts with those two provers in only one round of communication. It makes the query, gets the answer, and then gets to make a decision. And so here we're saying if the formula is satisfiable, accepts root radius one, if it's unsatisfiable, accepts at most with this probability.
00:47:09.896 - 00:49:11.254, Speaker A: And this is the randomness, and this is the range of possible answer it can get. It's the Alphabet in which those proofs are written. All right, so, so we also get this here. So finally, the, the highly non trivial equivalent formulation of the PCP theorem is that in systems of this form, which is very useful for a lot of hardness of approximation applications, intermediate tool, it would be good to have systems where this error probability can be made to go to zero, maybe allowing the range of the possible answers to grow similarly to what we did here, that we allowed the number of possible questions to increase and letting the error go to zero. So here also we could do a sequential repetition that says, well, the verifier will pick independently k choices of what to do, and then we'll make, we'll read k entries in this proof and k entries in this proof, but it's no more a system where you're only looking at two locations. So what has been studied instead is what is called parallel repetition. This is opposed to sequential repetition where you execute the protocol and then you execute again and you execute again.
00:49:11.254 - 00:50:35.604, Speaker A: So in parallel repetition, in the proverbs view, the verifier picks at random first a pair of queries that it would make according to some choice of randomness. Then it picks fresh randomness and picks another set of queries and so on. It picks k of them, and then it makes all the first queries to the first rower, all the second queries to the second floor. So in this example it would say, okay, I have my three set formula, I'm going to pick at random k clauses. And then here I want to see in each of those k clauses, what are the variables satisfied? And then here, in each of those k clauses, what's the value of a randomly chosen variable? So here the proof would consist of each entry of the proof would be k tuples of boolean values. Here would be k tuples of triples of values. Here you get k answers.
00:50:35.604 - 00:52:16.614, Speaker A: So yeah, the impression is that since those are independently chosen executions of the protocol, although it's possible for the answers to correlate to each other, those correlations shouldn't help. It turns out that there are interesting examples where it is possible to exploit correlations. And even if you suffer from a system where the maximal acceptance probability is one minus epsilon, here you can get something where you can do much better than one minus epsilon to decay. But it's still true that the probability of acceptance, even for the best proofs or the best prover strategies, goes down exponentially. Okay, so this shows that. So in particular, if you do this k repetition. So you'll need t times more randomness.
00:52:16.614 - 00:53:24.258, Speaker A: If you do k repetition, k times more randomness, the number of possible answers will increase exponentially. And the theorem is that this becomes something like, I'll blow it up. This becomes something like one minus the cube root of epsilon to the power of some constant times k. So it's still possible to make the error go down exponentially, but in a highly non trivial way. So it can be that, or still quite a bit more than one multiplier to decay. So those are sort of various formulations. The error can be bounded away from one with just three queries.
00:53:24.258 - 00:54:25.344, Speaker A: It can be made exponentially small with a linear number of queries. Three queries always suffice. Two queries also suffice if you allow at least three possible answers, and two queries suffice with an exponentially small error if you allow bigger answers. Now, let me give some intuition for how those proofs work. And as I said, instead of trying to actually describe the proofs, I will describe two constructions of error correcting codes that follow the same conceptual steps as those proofs. But I want to start from the construction and mimics what's going on in the north's proof, so that it's the more interesting one. And if we run out of time, maybe we'll not see this one.
00:54:25.344 - 00:57:19.890, Speaker A: So the goal will be the following, will be to construct some mapping that takes some string of bits that we want to encode, maps it injectively into a bigger string, and we want this encoding to be at most of polynomial length, and we want it to be injected in a very strong way, that for two different inputs the encodings must differ in a constant fractional locations. So the idea is that then they could go through some channel that corrupts the constant fraction of bits and still at least information. Theoretically we have not lost any information about the original message. In fact, normally we want this to be linear, but the constructions that correspond to those proofs will be just polynomial. So we'll define the following transformation that given whatever we have gotten so far, so this will be an iterative construction that keeps improving given whatever we got, we will define a better construction where the new construction is only a constant times longer than the previous one, and the minimum distance is twice as good. Minimum distance, I mean the guarantee that for any two different inputs the outputs differ in at least this many bits. So I will give some transformation that takes some encoding and gets a better encoding.
00:57:19.890 - 00:58:32.964, Speaker A: Better in a sense that if you look at what's the fraction of bits that differ for any two different inputs, it's twice as much. So then we will start from the identity encoding and then apply the transformation log n times. Well then it does what we want, because initially the identity encoding, this parameter dealt in the identity encoding. Any two different inputs, the rock would differ in at least one bit. So this parameter delta is one over nature. Every time we do this improvement, this parameter delta doubles. So it takes at most slogan times to become a constant.
00:58:32.964 - 00:59:43.928, Speaker A: Now each time the length of the encoding increases by a constant, but if we multiply n by constant log n times, it still something polynomial in n. All right, so I need to show the existence of this transformation. We will do this transformation in two steps. One step will actually quadruple the minimum distance, but the encoding will not be binary anymore, it will be over strings of some non boolean Alphabet, and then we will halve the minimum distance but get back to a Boolean encoding. So altogether we go from Boolean encoding to Boolean encoding, and the immune distance doubles, and the length of encoding only grows by a constant factor. So first let me show how you could do that if you would allow the length of the encoding to increase polynomially. That's really not good, because if the length of the encoding increases polynomially, for logarithmic number of steps, it becomes exponential.
00:59:43.928 - 01:00:30.304, Speaker A: But just to see the idea. So suppose I have the encoding of the previous step two to the polylon, even if it increases. No, no. So if you go from n to n squared and then you do it again, it becomes oh, I see, ok, so it's so if you do it k times, it will be n to the two to the k.
01:00:32.164 - 01:00:44.042, Speaker B: So I think quatumesh says, so nothing which is non constant is good for us. Even log log, even if you multiply it with log log, it's not going to be good for us.
01:00:44.138 - 01:01:48.364, Speaker A: Yes, you will not get a polynomial encoding with anything other than constant, although you could get a sub exponential encoding with anything that is slightly super linear. Super polynomial, yes. No, but this would just be for illustration, what? It goes on polynomially. So I could, so I could do the following. Just define c prime to be of length m to the fifth. And the idea is that if I want to compute c prime of x, first I compute C of X. So it'll be this string of n bits, and then c prime, not to scale, will have one entry for each choice of five entries in C of X.
01:01:48.364 - 01:03:08.266, Speaker A: So that's why they're going to be m to the fifth entries. And the content of this entry will be the five bits that are in C in these entries. All right, now I want to, so what is the minimum distance of this new code? If I look at C of X and c of Y, they differ in some delta fraction of coordinates. When I look at C of X and c of Y, what's the fraction of coordinates that will differ in? Well, it's easier to argue about the fractional coordinates in which they're going to be the same. So for, just look at the random coordinate in this code, for the two code words to be the same. What it means is that I picked five random coordinates here, and they all landed in this area of density, one minus delta. So that means that they can only agree in one delta to the five fraction of coordinates.
01:03:08.266 - 01:04:17.914, Speaker A: So they differ in this much. So this is about five delta. And let's say it's bigger than four delta, if delta is sufficiently small. So I can quadruple the mean distance by using a code that is not binary anymore. But then I can also get a binary code which will be of length. Well, there are better ways of doing it, but let's say two to the five times n to the five, just by re encoding each of those non boolean entries with a Boolean error correcting code. So now this, instead of being an entry, becomes its own little encoding.
01:04:17.914 - 01:04:59.994, Speaker A: Using the Adam or code, I can make it be of length two defined. Now everybody, one entry in which those two code words were differing now becomes a whole string of bits in which those two code words will differ in half of the places. So this means that the fraction of coordinates where now these boolean strings differ from each other is going to be half of what it was here. But this was already four times what it was here. So I still double the minimum distance.
01:05:02.214 - 01:05:07.634, Speaker B: That's actually the exponent. It's m to the five times two to the five.
01:05:10.774 - 01:06:00.524, Speaker A: To the five times m to the five, huh? Oh yes. Thanks. So this is a non boolean encoding of m to the five elements of this Alphabet. Then with the re encoding it becomes boolean and the length increases a little bit. So this would work, but it, so the composition would destroy the code. But let's think of sort of why we define the new code in this way. So in this new code, every entry of the new code was indexed by a tuple.
01:06:00.524 - 01:07:27.732, Speaker A: And what we wanted from these tuples was the property that they're going to hit a set of density delta with probability four times delta. Now this is true if the tuples are chosen completely randomly, but there are also many, which means then we need to enumerate all tuples to define the code. But this is also true for pseudo randomly chosen tuples of coordinate. So then when we enumerate pseudo randomly chosen tuples, the code doesn't get as long as here. So one particular way of choosing tuples pseudo randomly in a way that makes this construction work is to think about an expanded graph with m nodes. So as many as the coordinates of our code and constant degree. So in this graph there is a node for every coordinate.
01:07:27.732 - 01:08:39.694, Speaker A: So when in particular we think about the encoding of some x and encoding of some y, there will be some delta fraction of nodes that correspond to the coordinate in which they differ, and one result in which they don't differ. Now something that is true about expanded graphs is that if one constructs a random walk in the graph, so starts from a random vertex, then moves to a random neighbor. This is a probabilistic process, then moves to another random neighbor and so on. And say we do that for ten steps. Well then the sequence of ten nodes generated in this way is something that could be done in only mix times, some constant to the ten different ways. Because a random walk is defined by the first vertex, and then at each step you only have a constant number of choices to make. So there are only constant times m walks this way.
01:08:39.694 - 01:10:01.024, Speaker A: Nonetheless, a random 10th table chooses in this way as many of the statistical properties of just picking ten vertices uniformly triangle in particular forever. I mean, this will depend on the degree and the expansion. But for a set of vertices density delta this process has probability, say more than four times delta of heating that set. But for sufficiently good expanders even five steps would be enough. So given this fact, instead of constructing c prime to have one coordinate for every five tuple of the original code, I'm going to construct c prime to have one coordinate for every, let's say ten step random walk in this graph. So each coordinate will correspond to ten vertices, but not all choices of ten vertices, only ten vertices that could be in a random walk. And then the content of this entry will be whatever the code was containing in those ten coordinates.
01:10:01.024 - 01:11:30.404, Speaker A: So now what is the length of this encoding I have? So this is what each entry is like, and the number of entries is m times the degree of the graph, which is a constant to the power of ten or maybe nine if that's the first vertex. And if the, I mean there will be a much more complicated expression here. But if the expander is sufficiently good and delta is sufficiently small, I can make it so that a tense type random walk hits this set with PD four delta so that any two encodings will differ in a four delta fraction of places. Then I will do the boolean re encoding. Maybe it will be a little bit bigger here. And so this will be my final encoding lambda order of m because there will be m times some constant, many coordinates, each one having two to the ten bits. And now I do have this property that in the new code is linear length and twice as much distance.
01:11:30.404 - 01:13:43.314, Speaker A: And in, so in the north proof she, so she tries to, I mean she tries, she proves the PCP theorem in this form. And so what she, so what corresponds in this construction is that the length of the encoding sort of corresponds to how big is the constraint satisfaction problem that she constructs. And the minimum distance sort of corresponds to what is the soundness in the construction. So she starts by saying that. So Trizat itself is a gap problem where this is just the number of clauses, because it's problem of deciding if it's all satisfiable or if at least one clause is contributed by every assignment. And then she gives a way of saying, well, start from some input x of. I mean there are various ways to set up this induction, but let's set up this one.
01:13:43.314 - 01:15:20.424, Speaker A: And then you apply this reduction a logarithmic number of times and you get the PCP theorem here. This delta needs to be sufficiently small. Now this reduction works in two steps. There is also, it's a combination of two reductions. So one reduction will increase the soundness error by, say factor of four, but then the problem will not be boolean anymore. And then she has another step where the problem gets back to being boolean. But this error gets worse by a factor of two.
01:15:20.424 - 01:16:30.460, Speaker A: However, what you gain in the first step in error is more than what you lose in the second step. And so you have this improvement, and each of the steps increases the size of the input only by a constant. So the step where the problem has becomes thrown with a better soundness error and the non boolean character is sort of similar. It involves random walks in expanders. So one constructs the new problem in a somewhat similar way. And the step where you turn the non boolean problem into a boolean problem, sort of a nature of encoding non boolean information within algebraic code. I'll describe the construction of quotes that are more in the spirit of the original PCP theorem.
01:16:30.460 - 01:18:25.584, Speaker A: And there, there the connection is even looser. But just give at least a couple of ideas of what the argument looks like. So here we're going to do completely algebraic encodings. So we're going to start from the following observation that if f and g are polynomials, univariate polynomials of degree d over some field, then they agree on at most p inputs different polynomials. This gives rise to a good non boolean error correcting code, the Reed Solomon code, where if you have some n bits of information that you want to encode, you can map them to something like this in a following way. You fix fix a field of size n. N is not prime, but of size between n and two n.
01:18:25.584 - 01:20:09.114, Speaker A: Then you observe that there are two to the n polynomials of degree n over log n, because each polynomial is specified by these many field elements. Every field element encodes log n bits, so every such polynomial encodes n bits. So fix some. It does it all easy to compute some bijection to n bit strings. So then n bit string x will be mapped to to string of this form. What I mean is that f sub x is, according to this bijection, some polynomial that corresponds to the n bit string x that we want to encode. And the encoding is the evaluation of this polynomial at two n divided by log n locations.
01:20:09.114 - 01:21:30.166, Speaker A: Now this is a good code, because if I'm looking at a different input, this would be a different polynomial evaluated at the same collection of inputs. But if those are two different polynomials, they can agree on at most as many locations as the degree which is n over log n. So they can only agree on half of the inputs. And this can be instantiated differently, but always gives rise to excellent codes, but those codes are not boolean. In fact, each entry has not just a constant but even n possible values. So this will be coding where every entry is something that contains login bits of information. But then again, if we re encode every entry using a boolean code, although it's wasteful, we could use again the Hadamard code.
01:21:30.166 - 01:22:44.334, Speaker A: So this would become n bits, and the whole thing would be of length, n squared over log n. Then for every two code words before they re encoding the different half of the positions, after the re encoding, in half of the positions, those have become bit strings that differ in half of the bits. So overall, the two boolean code words differ in a quarter of the positions. So if you wanted better efficiency, also shorter length, you could do this in two steps. So first, encode each position with an exponentially smaller code of this form. So now each position would have only log n possibilities, then encode that with the other mark code, and then it only blows up polygorithmically. Now, the original proof of the regional proof of the PCP theorem sort of involved two ways of constructing probabilistically checkable proofs.
01:22:44.334 - 01:23:55.744, Speaker A: One had Boolean proofs, but required the proof to be exponentially bigger than the three sat formula that you wanted to prove satisfiability of. And then there was another system that had polynomial length proofs that you only need to look at into a constant number of places, but each place contained 0.1 million nine bits of information. So then what they did was to combine those two proof systems, much in the way that you would concatenate error correlating codes, so that you saw one Boolean proof system that tells you that a location that the original proof system would look at would make that verifier accept. And now this is a statement about something that is only of logarithmic length. Even if it's encoded exponentially, it's polynomial. So a bit like here, you use a code that has an exponential blow up, but you don't apply to the whole information, you only apply to something that has n possibilities or that contains slogan bits of information.
01:23:55.744 - 01:24:54.430, Speaker A: Okay, so I guess those two toy models are as much as I can tell about the two proofs of the PCP theorem. And maybe it's a good place to start 1 minute early too. It's excellent. I have a question about sort of the history of the nerve proof came quite a bit later. Is that true? Yeah, about twelve years later. Do you have a reason for why that would be? She was quite young. Well, I guess people have been thinking about hardness approximation for a long time.
01:24:54.430 - 01:26:07.134, Speaker A: Before the PCP theorem, it seemed very hard to get reductions that would go from Turing machines or AP completeness and get you hardness of approximation. It doesn't seem you can only gain or lose a little bit of approximation and that you needed something completely different to do this jump. So then when the idea of proof systems came up, then there was this revelation. This is precisely what's needed for hardness of approximation. And the sense was that without this, there would be no other way of doing this job. And the thing is that you could try to sort of, as irid did, to do things in stages, but unless even if the length of in your reduction blows up by log n, you end up with a super polynomial reduction. So it just didn't seem that any kind of gradual approach would work, and I guess was not bound by bunch of wisdom.
01:26:07.134 - 01:26:32.794, Speaker A: I guess Pierreman sips tried to do something like that early on, right? Actually that's true. They had expanders and this notion of improvement. So in fact, construction of codes, that is different, but bigly in the speed of this construction of codes. Let's go back to sipsen spinmind, the failed approach to prove the PCP ethereum this way.
01:26:35.494 - 01:27:38.804, Speaker B: There is another component. It is that Omer Rheingold's proof for the undirected graph connectivity being in logarithmic space. So that proof has much like the structure of of dinur spruce. And it was a year earlier than dinur's proof. And I think it was a very inspiring result, I mean, for many reasons, just for that, but that had this kind of structure, and it's just coincidental. It's an open or. I think it's a very interesting question, like, to actually connect Omer's CRM, which is a completely different CRM, to like, the north's proof somehow.
01:27:38.804 - 01:29:09.914, Speaker B: I also want to maybe mention another thing about these two different proofs, is that so they are very different, actually, in the following sense, that, like in the original proof, like first there is a huge gap, created a huge gap, meaning that we create some kind of constraint satisf. I mean, it's not a constraint satisfaction problem, but some kind of satisfaction problem with a huge gap. And then, but then that's, that's not a constraint satisfaction problem because it does not read constant bits and so forth. So then by various structural changes, like, we make it what it's needed, but then we lose the gap, but then the gap still becomes like a constant gap. So originally, the gap that blows up to, I don't know, one versus one over n or something like that. And then during the transformations, the gap decreases. So in the nour's proof, actually the gap at every step, well, as Luca said, gradually increases because she has this gap increasing transformation with the expanders.
01:29:09.914 - 01:30:31.264, Speaker B: And that's a new, that was a new gap increasing transformation, which actually kept the structure better than other gap increasing transformations. So the original gap increasing transformation, well, that did terrible to the structure, but it increased the gap hugely. And then there was another gap increasing transformation, which was the parallel repetition, which did actually quite well with the structure, but then it increased the instance size hugely. So denur's gap increasing transformation is so modest with changing the structure. So that's why we can just do log n iterations of it like with the other transformations, like doing more than constant number of iterations would be a complete killer. So these are very different proofs in structure. And I'm just saying that because if you want to duplicate like it in the quantum, then we might have to choose which structure we like at all.
01:30:31.264 - 01:30:32.904, Speaker B: It's not clear.
01:30:34.564 - 01:31:52.508, Speaker A: So one could, before that, you could see what's happening in the pyra repetition theorem as being the non de randomized error correct encode construction, where you simply start from a code and then for every k tuple of coordinates. So then you need to de randomize this with expanders if you want the new encoding to still be linear in size compared to the other. And as Mario said, for approaches that keep the number of queries constant and make the gap better. The parallel repetition theorem was the only result of this kind, but there was a result of Fagin Killian that showed that the pyre repetition theorem cannot be de randomized. So that if you, in the setting of the pyre repetition theorem, you make the, you construct k queries for the two provers, but not independently with some pseudonym distribution. No matter what the pseudonym distribution is, there will be some case where the provers to better than you would expect. So what goes on in Ritz proof is not a de randomized pile of repetition, because that cannot work.
01:31:52.508 - 01:33:04.354, Speaker A: But it's something that is so similar that. So there's one more reason that it was so unexpected that it would work. So I guess, again, the conventional wisdom was that the true meaning of the fag and killian impossibility result is that there is nothing that can be done to kind of improve the gap without blowing up the instance size. Yeah, so this is a question from Patrick Hayden, who's watching this live and texted me. Okay, so he's asking, can you say a bit more about how the toy error correction questions relate to proof systems? So there isn't any kind of generic transformation that if you have an error correcting code, can become a proof system. But if you have error correcting codes with certain additional properties, then they can yield proof systems. And for those codes, they do have those additional properties.
01:33:04.354 - 01:33:28.666, Speaker A: And so when you're able to get all the coding threading parameters nicely, it also implies that one gets good proof systems. But what those additional properties are, some of, it's a very long story, so it's hard to summarize if you want to try.
01:33:28.770 - 01:34:44.034, Speaker B: Well, I just wanted to say that. So when we teach the PCB CRM, we always try to teach, or not always, actually, I think it's. I learned a lot of new things, especially how to teach the north proof. So we always try to teach it through the error correction code analogy, because that's the baby version of the PCP theorem, because PCP theorem isn't, because it is a sophisticated error correcting code. And as Luca says, with extra properties. And last time I taught it, I was actually trying to develop a framework in which, which is sort of a generalized framework which generalizes error correcting codes and pcps, which I call knowledge verification systems. So the essence of that was, although I've never written it down, but the essence was that in the case of codes, we verify simple knowledge just simply that, like, this string is the same as that string.
01:34:44.034 - 01:35:39.394, Speaker B: And in the PCP theorem, we verify, like, a more complex knowledge that, like, that. This is a proof that this three set formula is satisfiable. And so there is a system can be defined again, which I call the verification system, that, like, does the simple thing with certain parameters or whatever, and does the more complicated thing. And. But I think it would have been nice to have like a continuous kind of way of going from the simple to the complicated. And I don't think I achieved that. And so we still teach it, like, look at it.
01:35:39.394 - 01:36:07.054, Speaker B: So we just say the simple thing, and then we say that the complicated thing is just the same, just more complicated. So. I'm sorry, that's what I. Questions. Thank you again.
