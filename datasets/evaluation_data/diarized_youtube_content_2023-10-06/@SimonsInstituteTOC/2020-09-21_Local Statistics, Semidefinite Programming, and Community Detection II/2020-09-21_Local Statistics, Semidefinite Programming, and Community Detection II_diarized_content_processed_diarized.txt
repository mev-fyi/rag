00:00:01.880 - 00:00:54.448, Speaker A: Yeah. Thank you for the introduction, Ryan, and thanks to all the organizers for the invitation to speak here. So let me remind you of what I'm going to prove. So a is going to be the adjacency matrix of an additional graph where the edge probabilities are d over n, and you should think of d as being a constant. And we're going to use a bar to denote the centered adjacency matrix. And our goal is going to be to show that if x is any positive semidefinite matrix with one from the diagonals, then with high probability, the value of the LC non backtracking part of the centered adjacency matrix itself with x, is at most the square root d to the l quantity. So speaking, we're going to show its at most one plus epsilon to the l squared d to the l for large enough constant l, where l depends on epsilon.
00:00:54.448 - 00:01:01.504, Speaker A: But from now on, I'm just going to use less than approximately equal to to hide this one plus epsilon to the end.
00:01:03.284 - 00:01:54.924, Speaker B: Can I ask a que? I'm sorry to jump in so early in your talk, but this is actually a question, sort of from the combination of the two talks. So this SDP doesn't have any new constraints on x. It's the. What's different from the kind of good old fashioned STP is that you're taking the inner product with this backtracking length l power instead of the adjacency matrix. So is this. Forgive me, but is this somehow dual to the idea of taking a more complicated SDP with more constraints that Jess was talking about? I guess this is a simple SDP with a different inner product thing, as opposed to an SDP with more constraints. Am I missing something obvious here?
00:01:54.964 - 00:01:57.532, Speaker A: Yeah, I guess I'm not really sure what you mean by dual.
00:01:57.668 - 00:02:04.384, Speaker C: I think. I think that, yeah, I had the same confusion as Chris, but all of these are constraints, right?
00:02:05.204 - 00:02:06.300, Speaker A: Yeah, all these are constraints.
00:02:06.332 - 00:02:14.720, Speaker C: One for each inequality, Al dot x less than square root of the l for each child, right?
00:02:14.792 - 00:02:31.764, Speaker D: Right. So, not that I can jump in, I think I could have set this up better in mind. The point is that this little results are very large results. As it turns out, that synopsis approved is just showing that some particular constraint is violated in the SDP with lots of constraints that I mentioned in the prior talk.
00:02:32.104 - 00:02:59.100, Speaker A: Right, right. So actually, for the planet model, if you're above the case threshold, then this a dot l dot x, where x is a planted solution, is going to be way larger than square root d to the l and here we are showing that in the null model it cannot exceed square root of d to the l, and hence the null model can never hope to satisfy this particular constraint. Okay, yeah, so I guess you should.
00:02:59.132 - 00:03:04.624, Speaker C: State that the SDP is just check the feasibility of all of these constraints for a child.
00:03:05.164 - 00:03:06.984, Speaker A: Yeah, that's exactly right.
00:03:08.044 - 00:03:20.744, Speaker E: May I also chime in? Prasad sorry, Prasad Raghavendra says as a comment, at degree two local statistics reduces to non backtracking locks, since the only local statistics with two labeled vertices are non backtracking locks.
00:03:25.044 - 00:04:29.854, Speaker A: Right. Okay. Yeah, so I guess we wanted to prove this bound on the SVP value square d to the l. And before I move on, I just want to point out that several of these ideas actually do come from are inspired by previous papers, namely once by Bodenov, Lalage Massoudier, a paper by Fan Montanari, and also some joint work with Ron O'Donnell and Pedro Paradesh. But the main technical challenge here is somehow getting the statement to work for l being a large constant, whereas these previous works can only get such a statement to work for l being something like log log squared n. And as a first attempt you could make the observation that actually the elf non backdrop power, the operating of the elf non backlash power, is actually an upper band on the SDP value. And one could hope to prove the band that we want by banning the operator arm.
00:04:29.854 - 00:05:33.264, Speaker A: But actually try to do this directly fails. And it actually fails for a fairly simple reason, because with high probability an additional graph is actually going to have vertices of degree roughly log n, which automatically gives the matrix a super constant eigenvalue. But actually this band that we want to show square d to the l is a constant. But actually the reason for which this fails also gives us a simple fix. So one thing we can notice is that a typical vertex has degree d. Or more generally, if you look in the distance t neighborhood of typical vertex, you're going to see roughly d to the power t neighbors, and there's only going to be like a very small fraction of vertices whose balls exceed this d to the t number by a lot. And what we're going to do is we're going to delete the small number of vertices from the graph.
00:05:33.264 - 00:06:34.964, Speaker A: By the way, notice that this is just something for the analysis. So let me define m to be the elf non backtracking power of the adjacency matrix of the graph, where certain bad verses have been deleted, where bad verses are the ones which participate in short cycles, and also the ones that have large distance three neighborhoods and actually, since we have this particular constraint on the SVP that the diagonal entries are all equal to one, we can actually show that deletion of vertices does not really influence the SVP value by much. And it turns out this new matrix m is indeed spectrally bounded by square root d to the l. So now we're going to jump into the proof of bounding the operating of m, but I'm going to pause for a second to see if there are questions.
00:06:37.284 - 00:07:13.344, Speaker B: Well, since you ask so deleting vertices on short cycles feels a little bit like up to some point using self avoiding random walks rather than just non backtracking or non backtracking. Or if you like non backtracking with a bit more memory, you may not visit any of the past three or four or five places you've been. So I'm just wondering if there's anything to be gained by that kind of power of the adjacency matrix, since non backtracking sort of has memory one.
00:07:14.684 - 00:08:43.774, Speaker A: Yeah, so I guess I'm actually still not sure of how to make the proof go through if we use self awarding walks, for reasons that I will address actually on the next slide. Um, right, so our approach for bounding the op amp is actually to start by bounding its kth moment by some number c. And once we have a bound on the kth moment of c, if k is like a large enough number, then we actually have a bound on the operator norm that holds with high probability of c to the power one over k. And the way we're going to try to bound the kth moment of m of the operating of m is to notice that when we choose k to be an even number, the operating of m is actually bounded by the trace of the kth power of m. And actually the hope is that the expected trace of the case part is at most square root d to the l, whole thing to the power k. So is what we want so we can get the desired opera norm bound on m. And Chris, I'll get your question very soon.
00:08:43.774 - 00:09:44.624, Speaker A: So, um, our first natural goal is actually to try and understand what the, um, a trace of the k th part of m actually is. And if you do a little bit of combinatorics, um, you will see that the trace of m to the k is like a sum over a huge variety of walks. And in the sum of a huge variety of walks, the summon is actually an appropriate weight of a certain walk. And now let's understand what this huge collection of walks is. So Ashley, recall that m was the l non backtracking power of a certain centered adjacency matrix. So trace of m to the case, actually counting. I was actually summing over a closed block which have length k times l.
00:09:44.624 - 00:10:33.584, Speaker A: And these closed box have a very special structure. In particular, they're composed of k non backtracking segments, each of which have length exactly equal to l. So. Okay, now let me quickly address Chris's question. So if we instead use self avoiding works instead of non backtracking works, then we'd be looking over walks where there's case self awarding segments of length l each. But as you pass from one self awarding segment to another, you could still encounter short cycles and this could cause problems in our analysis. It's okay, I'm gonna momentarily pause here too to see if, um, people have any more questions.
00:10:33.584 - 00:11:31.364, Speaker A: Okay, so now let me keep going. Um, so this particular trace of m to the k quantity. It's like this crazy summer where all walks and looks super daunting. Um, so actually, fortunately I'm not gonna make any of you actually think about this. Um, let me just say that there's a lot of song and dance that one needs to go through before you get from this point to something interesting. So I'm going to perform a slight offhand and fast forward to where the next interesting thing happens. And here, by waving my hands a little bit, I'm going to say that this trace of m to the caveat is like a summer walks weight of w is dominated by a very small number of walks with very nice structure.
00:11:31.364 - 00:12:15.004, Speaker A: In particular, it's dominated by walks which are even, that is, every edge is walked on an even number of times. And also these walks have the property that they have almost no cycles. They're very close to tree and all the cycles on them are also very long. And the first interesting combinatorial question in trying to account for these walks is to count walks on a tree. So let me set up a clean combinatorial question. So you have a diary tree. It's rooted at this vertex which has been labeled as v.
00:12:15.004 - 00:13:33.654, Speaker A: And we want to count the total number of closed box that start at v ended v. These closed box need to have length kl and they need to have a special structure that they're composed of k length l, non backtracking segments. And recall that we'd be happy if the bound we could get on the number of such walks is square root of d to the lie. And the way we are going to bound the number of closed box is one encoding argument. So if you give me any walk on the street, and if I can compress the description of this walk to log of some number c bits, then I've actually bounded the total number of walks by c. So in particular, I'll have to give you an encoding of a closed box of the special structure in the tree using kl over two times logd bits. Now the first observation that goes into this particular encoding is that, okay, so first let me define what an up step is and what a down step is.
00:13:33.654 - 00:14:27.584, Speaker A: So notice that every step in this walk either moves away from the root or moves closer to the root. So if a step moves closer to the root you can think of it as climbing the tree and we call it an up step. And if it moves away from the root we can think of it as descending down the tree and we can call that a downstep. And because you have to start at v and end at v, the number of upstairs is actually equal to the number of down steps. In particular, half the steps are up and half the steps are down. Now here's the second observation, and this is where we're going to use like the special structure of the walk of being composed of k non backtracking segments which each have length l. So um, in one of these length l non backtracking segments, I claim the structure is always I move s steps up and l s steps down where s is possibly zero.
00:14:27.584 - 00:15:30.864, Speaker A: And the reason for this kind of structure is if I take a downstep, the only moves I can make in non backtracking fashion are also downsteps. So this actually really constrains what a segment can look like. And this is going to inform the following way to actually encode a walk, you give me a walk w and then what I'm going to do is in each segment I'm going to see what this number s is like. How many up steps are there? It's going to be a number between zero and l. So I'm going to use log l bits to write down how many up steps are taken per segment. So for example, maybe this part of the encoding just looks like zero up, two up, three up, five up and so on. And um, and notice that this takes log l bits per segment, so that accounts for k times log l bits.
00:15:30.864 - 00:16:25.864, Speaker A: So another thing I want to point out is if I tell you that you're taking an upstep, or if you know that you're taking an up step, I don't even need to tell you where you're going because every vertex has a unique parent. But if you're taking a down step, I do need to tell you where you're going. In particular, I need to tell you which of your d children you're going to. So, encoding each downstep takes log d bits. And since half the steps of the kl steps are down, this accounts for log d time kl over two. And actually this log d times k two is like the number of bits we wanted, but we have like a bit of bonus, okay, with the surplus number of bits ends up being something pretty insignificant. That's you're paying only log l over l for offset and that's the source of your savings.
00:16:25.864 - 00:17:26.234, Speaker A: So this establishes how one counts walks in a di tree. So now I'm going to move on to an overview of how one actually accounts blocks that have some small number of cycles in them. But before that I'm again going to pause for a second to see if there are any questions. Okay, so let me keep going. So we actually now care about understanding walks on shapes that may contain some cycles. So in particular you could have, um, some graph that looks like the one on the left and there's some vertex v and we want to count v to v close box, which satisfy the same set of conditions that we discussed earlier. Um, but this is no longer tree.
00:17:26.234 - 00:19:00.494, Speaker A: So actually the strategy, uh, at this point and, and I would say this is like roughly where new ideas start coming in and we depart from pass work, is to identify a spanning tree as well as a set of excess edges, namely the edges which are not in the spanning tree. And now once we have a decomposition of the shape into a spanning tree plus excess edges, we know that any walk breaks down into an alternating fashion where there's a stretch of the walk that's on the tree followed by a stretch that's on the excess edges, followed by a stretch on the tree, and so on. And now the way we're going to encode such a walk is by actually encoding the three segments in an almost identical way to what was described earlier. So note that actually the three segments are no longer closed box because really they go from one endpoint of an excess edge to another endpoint of an excess edge. And these two endpoints need not be the same vertex. But I guess it's a fast say that an almost identical idea works for encoding these segments of the tree block. And actually the way we're going to encode access steps is also rather nice.
00:19:00.494 - 00:20:15.312, Speaker A: For each access step that appears, we're simply going to list we're just going to make this big list of access edges in the order in which they appear in the walk. So I just wrote an example encoding at the bottom. But actually if the number of steps on excess edges is too big, we could run into trouble because this list of excess steps could be really large. So we can't just choose any spanning tree. We need to be clever about which spanning tree we choose. And I guess a rather natural choice of which spanning tree to use would be you take the shape and then you weigh every edge by the number of times it appears in the walk and choose the maximum spanning tree according to these edge weights. So this choice definitely minimizes the, the number of excess steps that one takes.
00:20:15.312 - 00:21:16.470, Speaker A: It minimizes a list of the excess step encoding. However, it's still not actually clear that the list you get is short enough in length. And yeah, I won't have time to get into how one actually proves that this list of exercise is fairly small. But um, it's, it's actually a part of the proof that I really like. And it, it involves like some nice tools from combinatorics such as the moldbound for irregular grass, as well as properties of linear programming, relaxation of finding maximum spanning tree, and this actually ends the technical part of my talk. And now I'm just going to discuss three future directions and then we'll have time for questions. So the most natural next question is the following.
00:21:16.470 - 00:22:05.084, Speaker A: This local statistics thing gives us an algorithm for the distinguishing problem. It's natural as whether the search problem could be solved. So, okay, let me be a bit more specific. When I say search problem, you want to. So the basic version of search problems, you want to recover, say, in the two communities, you want to cover the two communities better than random guessing word. But actually there's a more nuanced question here, which is for a given two community block model with certain settings parameters, there is an optimal, um, possible recovery that you can perform information theoretically. And there's a paper of mosul, Niemann and sly which does this, but not right down to the chaos threshold.
00:22:05.084 - 00:23:05.424, Speaker A: They do it at like some factor, maybe ten above the case threshold. So one question is whether rounding the local statistics SVP, um, could achieve, could, could solve this problem right down to the CaS threshold. And yeah, this is a problem that probably needs like new ideas and could be very interesting. Another direction is. Well, we illustrated how to apply this SCP to certain hypothesis, to the hypothesis question of community direction. But one could ask whether it could be applied to other natural questions such as finding planet spikes and wigner matrices or planet solutions in random constraint satisfaction problems. And here's another direction that would be to try to apply these ideas beyond the realm of bayesian inference.
00:23:05.424 - 00:23:46.174, Speaker A: In particular, let's say you want to determine moments of Gibbs distributions for your favorite problem. Like, let's say. Yeah, let's say you had a deregular graph and you wanted to know what the moments of the uniform distribution on 2d colorings were. And there are algorithms for sampling from gates distributions, in some cases using global dynamics, or more generally, some form of Markov chain Monte Carlo. But the question is, can the local statistics sv be used to give deterministic algorithms in these settings? And I think with that, I'm done.
00:23:49.714 - 00:24:18.954, Speaker E: Okay, let's thank Jess and Siddanth together, see if I can pipe in some applause. Should have faded that out. Okay, great. So now I guess Jess and Siddhant can potentially even team up on answers to questions that you may all have, for which we still have plenty of time.
00:24:21.894 - 00:24:25.714, Speaker A: I just want to check with Chris whether I answered his question about self awarding box.
00:24:27.774 - 00:24:48.584, Speaker B: My question was a little vague, so. Yes, you did. I was just trying to connect it with, I guess, massulier's positive result from some years ago about using the matrix that counted. Self avoiding walks up to a certain length. I wonder if one of the things you've done is find a simpler proof of that somehow. Sort of, kind of.
00:24:52.364 - 00:24:56.624, Speaker E: So some of the point is you have to delete all the vertices that appear in short cycles.
00:24:57.524 - 00:25:43.314, Speaker A: Yeah, so, okay. One thing that I realized we didn't really communicate is that the reason we need to do this whole deletion procedure is because we were also actually shooting for this additional robustness property in our algorithm, which is if an adversary deletes like delta and or deletes or inserts delta and edges, where delta is a sufficiently small constant, we still wanted to solve the distinguishing problem sufficiently close to chaos threshold, and that's why we needed to get the proof to work for constant l. Otherwise, the old machinery does work without any deleting verses and short cycles or deleting high degree vertices. And you just have to make l, like log, log n squared.
00:25:46.494 - 00:25:49.634, Speaker E: Sorry, but does this mean you don't get a polynomial time algorithm?
00:25:50.254 - 00:25:56.384, Speaker A: You do get a polymeadam algorithm, because I guess these non backdrop powers can be computed efficiently.
00:25:59.084 - 00:26:30.974, Speaker E: Also, there's a comment from Prasad Raghavendra in the audience. He says future direction number four show lower bounds for the number of rounds of local statistics STP hierarchy below the chaos threshold. This will be evidence towards proving a computational phase transition where the problem can be solved by degree two local statistics above chaos threshold, and no constant degree local statistics SCP below the chaos threshold. Yeah, I think maybe Jess might have implicitly said that question in her talk, but, yes, this is also a great open direction, I think.
00:26:31.134 - 00:26:34.594, Speaker A: Yeah, thanks for sad. I meant at it, but forgot about it.
00:26:35.694 - 00:27:02.994, Speaker F: Can I ask a question about this robustness guarantee? So, there, I think there was, it was made by Ankar and Alex and Amelia several years ago now, which showed some lower bounds about also having to do with robustness. And I don't remember exactly what the formulation of their result was. Can you say, what is the relationship between these things and. And whether you're.
00:27:05.054 - 00:27:45.414, Speaker A: Yeah, so, actually, in that paper, what they showed us that, um, depending on the. So, so, yeah, depending on what the adversary. Depending on the number of insertions and deletions that the adversary can make, the case threshold actually shifts up. And they gave an algorithm that was robust at twice the KS threshold. But I guess ours is like a more fine, drained robustness theorem, where we're able to say we can be robust to. To delta and edge insertion deletions. Epsilon closes threshold, where epsilon depends on delta.
00:27:45.414 - 00:28:18.074, Speaker A: And. Okay, one thing that I do want to point out is, in their paper, they actually considered a different noise model as well, where the adversary is, Montgomery could add as many addresses they wanted between communities and subtract as many edges as possible within communities, or maybe the opposite, but. Okay, we only allow, like, bounded numbers. And I guess if, like, these ideas can be used to get better settings in. The other noise model is also a cool direction.
00:28:20.534 - 00:28:21.314, Speaker F: Thanks.
00:28:22.574 - 00:28:28.994, Speaker E: I don't know if everybody saw, but in the chat, Alex, once a different form of robustness, we allowed an arbitrary number of monotone changes.
00:28:29.494 - 00:28:32.614, Speaker A: Uh huh. Yeah, that was actually the model I just mentioned. Yeah.
00:28:35.114 - 00:28:41.294, Speaker E: Let's see some more questions. Do you have, like. Go ahead, Chris.
00:28:42.234 - 00:29:20.198, Speaker B: So, I think I'm just saying back what Siddhant just said. I just want to understand. So, it could be that actually counting self avoiding walks up to a certain length could be somewhat more powerful than kind of that level of using non backtracking walks. But on the other hand, doing that for lengths larger than log n is computationally expensive for self avoiding walks, whereas for non backtracking walks, there's a linear recurrence that allows you to do it in polynomial time. So that's kind of. Yeah, that's sort of. Okay.
00:29:20.198 - 00:29:22.314, Speaker B: Anyway, that's interesting.
00:29:25.674 - 00:29:33.054, Speaker E: Yeah, I was gonna ask you prove, like, lower bounds for this local statistics scp for like any kind of like toy problem or easier problem.
00:29:33.914 - 00:29:47.802, Speaker A: Right, right. I guess if the SO's degrees too. We show that if you make your local statistics degree any large enough constant, you can't beat the KS threshold in.
00:29:47.818 - 00:29:50.548, Speaker D: The case of deregular graphs and the.
00:29:50.676 - 00:30:05.612, Speaker A: Regular and also the block model. But I guess, yeah, I guess we don't know any load bands where you're out to up the SO's degree. Right.
00:30:05.668 - 00:30:11.260, Speaker D: So I think analogous to so's where there's kind of this quadratic barrier where lots of nice things happen.
00:30:11.412 - 00:30:34.324, Speaker A: Yeah. Though I think I should mention the, the Barack Hopkins Kothari pura chin Moitra Kelner paper for like songs best low bound for planet Peak also gives the local statistics lower bound, though this like presaged local statistics. So they don't like say it that way.
00:30:35.544 - 00:30:36.432, Speaker E: I see.
00:30:36.608 - 00:31:12.434, Speaker A: I think in general, pseudo calibration based approaches probably should give local statistics lower bounds. That's like the sense I have. Is the same true for the Sherrington Kirkpatrick lower bounds? Yeah. Oh, I guess it's true for, strictly speaking, it's true for the planet boolean vector. Lower bound for the SK lower bounds comes from another layer of reduction. So like you have to, you have to talk about what planet model you're using. If you're asking about local statistics at.
00:31:16.494 - 00:31:28.834, Speaker F: Actually, at some point, wouldn't you expect local statistics to do something interesting in Sharington and Kirkpatrick because of Andreas algorithm?
00:31:31.374 - 00:31:44.332, Speaker A: So I guess we also need to be clear about which computational problem we are talking about. So if you're talking about refutation, it's not clear. It should. But maybe for search, presumably.
00:31:44.388 - 00:31:49.828, Speaker F: Yeah, because Andrea thing applies only is talking about the random problem where you don't have some planted structure. So that's actually.
00:31:49.876 - 00:31:50.404, Speaker A: Okay. So it's.
00:31:50.444 - 00:31:51.704, Speaker F: You need to define something.
00:31:52.284 - 00:32:02.904, Speaker A: Yeah, but presumably something like a solution that matters, moments of the Gibbs distribution, then maybe at some number of levels something interesting could happen.
00:32:07.704 - 00:32:22.884, Speaker E: This is going back a little in time. But Prasad also made a comment addressing Chris's question. He said that the number of short cycles of length less than log n is small and deleting or add them would not change the statistics in this model. So the SDP is not sensitive to it.
00:32:31.724 - 00:32:48.684, Speaker G: I guess this is a much more naive question, but sort of for the community detection type questions, I can understand that these are the sort of statistics that you look at, are related to sort of the local structures in the graphs, and you look at non backtracking walks and so on and so forth.
00:32:48.724 - 00:32:49.092, Speaker A: Right.
00:32:49.188 - 00:33:13.664, Speaker G: But sort of to one of the questions that siddhant had on its slide, leading to, for example, spiked Wigner matrices and stuff, you would look at sort of the same length k non backtracking walks, possibly on the Wigner matrix, but in some sense they are not so local. Right. So I guess I'm wondering sort of in other problems how local you expect things to be and what that would mean, I guess, if you have some understanding of that.
00:33:15.604 - 00:33:52.592, Speaker A: Right. So I guess one thing to keep in mind is that the local statistics SVP is looking at more than just the local structures in the graph, because it's also trying to search for like a planted spike at the same time. Sure. So presumably, even in something like Spike Wegner, if I take two vertices and I look at all length l non backtracking walks between these two vertices, let's say multiply the edge weights, I think I would expect that to be correlated with the product of. Oh, yeah. No, no.
00:33:52.648 - 00:34:00.724, Speaker G: I'm sure they're saying that maybe they are not so local in this sense, but yeah, I mean, I understand. Sure.
00:34:06.024 - 00:34:09.044, Speaker E: Okay, do we have more questions for Sadant and Jess?
00:34:10.624 - 00:34:11.048, Speaker A: Yeah.
00:34:11.096 - 00:34:15.324, Speaker F: How does the running time depend on your proximity to the chaos threshold?
00:34:16.993 - 00:34:46.934, Speaker A: Right. It's actually a reasonably good dependence because the so's degree only needs to be two and the only thing that's increasing is the number of constraints. And the number of constraints just becomes like a large constant. And computing these matrices takes like computing the m matrix takes order, m matrix multiplication equations. So. So it actually stays like NSV solve plus matrix multiplication solve.
00:34:54.754 - 00:35:09.374, Speaker E: Great. Okay, so we have another talk in two minutes, so we'll all take a little teeny break. And if Amit wants to set up, that'd be great. I think you should be able to control the screen share.
