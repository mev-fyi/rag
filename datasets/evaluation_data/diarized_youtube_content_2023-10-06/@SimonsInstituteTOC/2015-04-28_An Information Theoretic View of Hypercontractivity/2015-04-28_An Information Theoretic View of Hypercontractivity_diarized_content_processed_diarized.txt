00:00:01.040 - 00:00:46.850, Speaker A: Okay. Hi. I'd like to thank the organizers for inviting me here, and especially because it helped me stop procrastinating and do something which was on my mathematical to do list for more than a decade. So when I got the invitation from Jaikumar, I said, well, what will I speak about? I have the result. I'll mention here with Wojta Rodel from ten years ago. But then I remembered I always wanted to prove Wechner's inequality, the primal version using entropy. I said, okay, so now I have a year to do it, and I'm glad to report that I succeeded with tons of time to spare.
00:00:46.850 - 00:01:05.894, Speaker A: I finished a week ago. So we're okay? Yeah. I'm glad you didn't invite me a week later. I don't know what would have happened. Do you want to give another talk tomorrow? There is another talk tomorrow, maybe. Okay. About the Riemann hypothesis.
00:01:05.894 - 00:01:24.906, Speaker A: Yeah. Deadlines work. Okay, so this is a. I took this picture. The locals may identify this cinema. This is last time I was here, and it's an advertisement for my talk. Okay, so here's what I'm going to do.
00:01:24.906 - 00:02:06.516, Speaker A: First, I'll tell you a bit about the inequality. I'm not going to assume that you're familiar with it, although I'm sure many of you are. And then I'll mention this older result with Redel that we proved the dual and the new proof of the primal version. And although they're both information theoretic, I see absolutely no connection between the two proofs. They proved dual results, which are equivalent, but the proofs have nothing to do with each other, as far as I can see. Okay, so let's start with telling you why you should be interested in this inequality. So, I was too lazy to write this slide myself.
00:02:06.516 - 00:03:17.704, Speaker A: I cut and pasted from a paper of blaze and tan, which I'll mention later, and they say, let me read aloud what they say. First introduced into theoretical computer science by the celebrated work of KKL, the hyper contractive inequality is seen utility in a surprisingly wide variety of areas, spanning distributed computing, random graphs, k Sat, social choice, inapproxima, ability, learning theory, metric spaces, statistical physics, convex relation hierarchies, whatever that is, etcetera. In almost every one of these results, there are no known alternate proofs that do not require the use of hyperconctivity. Part of the reason I wanted to prove this inequality is to understand it better, because I've used it a lot myself, and I like getting more friendly with inequality before I use it. So. Okay, so here's quick background, we're looking at functions on minus one, one to the n. So most basic function is you give me a vector, I return one of its coordinates, so they're n of those functions, they're monomials of degree one.
00:03:17.704 - 00:04:13.834, Speaker A: And if you take a set of coordinates and multiply them and get a monomial of degree, whatever the size of I is, and you can take any subset of n, you have two to the n, such monomials. And the space of functions on two to n points also has degree as dimension two z. And so these monomials span all functions on all real functions on the space. So they form a basis for the space of functions. And in fact, if you take the uniform, the uniform measure and the standard inner product, they're orthonormal. So they all have expectation zero, expectation squared one. And if you take the product of two different, one, two of two different monomials, you'll get another monomial from this family, which again will have expectation zero.
00:04:13.834 - 00:04:49.384, Speaker A: So if you have any real function f, you can expand it as a polynomial, and the coefficient of the polynomial corresponding to coordinates at I is the Fourier transform at I. So this is the unique expansion. And. Okay, now I want to tell you what this operator that we're interested in, what it is. So, first, you can look at the two point space. You have a function on two point space. Any such function can be written as ax plus b.
00:04:49.384 - 00:05:49.602, Speaker A: And what the operator does to the function, it leaves b, the constant part fixed, and the part that has expectation zero, which is orthogonal to the constant. It shrinks by an epsilon factor. So that's what it does to functions on two point space. And if you take the tensor of the two point space n times, then the functions on the tensors of the functions on the two point space span the functions on the two to the n point space. And you can take the tensor of the operator, and it works on functions on the n dimensional space. Now, I'll tell you how this operator works in, I'll repeat this in a slightly different, from a slightly different angle, but I wanted to say, to point out the tensor definition, because most proofs of the inequality go like this. We prove the inequality on the two point space, and then the inequality tensor rises, whatever that means.
00:05:49.602 - 00:06:27.478, Speaker A: So you have to use something about, to use Minkowski's inequality when you see that you can keep the inequality when you tensorize. But here's a different way of defining the operator. I'll just tell you what its eigenfunctions are. So, for any of the degree one monomials, it just multiplies it by epsilon. And if you have a degree d monomial, then it will multiply it by epsilon to the d. So these are the tooth, then eigenfunctions of this operator. And they have, these are the eigenvalues.
00:06:27.478 - 00:07:12.844, Speaker A: And you see it tends to really suppress or kill high degree monomials. And this is useful, and it's relevant because when you look at the function, the high degree part of the function is what's responsible for most of the influences. And if you want to kill. So it's something that happens a lot, that you want to suppress the high degree part of the function. This is what this operator does. And. Okay, so, and you extend it by linearity to any function, the operator on the function, it hits each Fourier, each monomial by epsilon to the degree of that monomial.
00:07:12.844 - 00:07:55.262, Speaker A: That's one way of defining the operator. Here's a totally unrelated way, but it's, the definition is equivalent. You take a vector x, and then you take a noisy version of x. So y is a noisy version of x for every coordinate you can. With probability epsilon, keep it fixed, and with probability one minus epsilon, you flip a coin and tells you whether you're going to reset it or not. So you just yi independently for each I you set it to be, you choose it such that the expectation of xi y is epsilon. So y is a noisy version of x.
00:07:55.262 - 00:08:46.169, Speaker A: And then let's say we'll call them an epsilon correlated pair. And here's a different definition of the operator. If I want to see what t of f. And now it's indexed by epsilon what t of f is at the point x. Then I average the value of f on all y's, where the probability of each y is the probability that a noisy copy of x will give me y. So in this definition, you see that it takes the function and it replaces its average, its value at the point, by an average of the points around it, where points, which are closer get more weight and points the further away get less weight. So when you see this definition of the operator, it's pretty clear that it should smooth out the function.
00:08:46.169 - 00:09:25.744, Speaker A: If the function has a peak, then after you apply the operator, you're going to get something smoother. And indeed, this is what Beckner's inequality, which was proven by Bonhomie and Gross before Beckner. This is what it states. It states the following that take a function f, and I want to compare f to t of f. I want to say that t is smoother? Well, the two norm is always. Now, epsilon is some number less than one. So one plus epsilon squared is less than two.
00:09:25.744 - 00:10:28.804, Speaker A: I'm saying that if I take t of if and I take a higher norm, which puts more emphasis on peaks, it's not going to be more than what I'd get if I take f with a smaller norm. So this is a way of saying that t of f is smoother than f. The two norm of t of f is equivalent to a smaller norm of f is less than a smaller norm of f. So the operator smoothes out the function, and as I mentioned before, t kills the high degrees. So this sort of brings to mind the fact that perhaps the low degree polynomials are smoother than high degree polynomials. And the dual version of this inequality, which is equivalent, you just need Helder's inequality to go from one of them to the other, says the following. So let's look at the function whose Fourier transform is exactly on level m or up to level m.
00:10:28.804 - 00:11:08.494, Speaker A: So we have a degree m polynomial f. And now I'm going to compare the q norm of f for some q greater than two with the two norm of f. So the q norm of f is larger because it gives more weight to the peaks. But this tells you it's not much larger, it's smaller than something to the m times the two norm. And that's something as square root of q minus one. Okay, so this tells you that low degree polynomials, the q norm, was larger than the two norm, but not much larger. So, tells you that low degree functions are behave nicely.
00:11:08.494 - 00:11:57.610, Speaker A: Okay, so let's, now let me try to say what we proved and how we proved it. So, with Wojta, back in 2001, we wrote a paper about the proof of the dual version, and we could have said in the introduction the following. Bonhomie and others proved a similar result in the seventies. However, our result is more recent and less general. So why bother at all? Because, well, it uses entropy. So it gives a new way to look at this inequality. And the reason I wanted to do this, I think when you compare the two different norms, it gives you some combinatorial understanding of why the four norm is comparable to the two norms.
00:11:57.610 - 00:12:40.010, Speaker A: So what we proved, we compared the four norm to the two norm and it's exponential in the degree. But the base of the exponent we got was fourth root of 28, where, if you remember, we should have gotten square root of three. Okay, so fourth root of 28 is more than square root three. So this is weaker. But what I said to myself then is that I don't care about the constant. Or I pretended that I didn't care about the constant, because a, in all applications, I know, no one ever, no one ever says, okay, now we're going to use KKL. And the constant KKL is 3.5.
00:12:40.010 - 00:13:27.156, Speaker A: Usually people don't care about the constants. And second thing was that it seemed to me that although we were being sort of wasteful, and I could see where we were being wasteful, it seemed to me that this method had its limits and there's no chance of getting the best constant even if we tried. So I didn't try too hard. And then I was quite astonished, oops. A decade later, to encounter this paper by blaze and tan, where they, not only for four, but for every even integer, they used entropy, and they got the precise, the best possible constant. So it was surprising to me that this method is that powerful that you can really get the constant. Exactly.
00:13:27.156 - 00:14:26.120, Speaker A: Okay, so let me say a few words about what I did with Voyta. We have this, we have this function, let's say it's homogeneous of degree m, so it's only a sum of monomial, a linear combination of monomials of degree m. And let's see, what is its two norm. So the two norm is just the expectation of f squared. You take f inner product with itself, then xi and xj are orthogonal. So f hat I and f hat j disappear, and you're only left with the squares of the Fourier coefficients. Now, when you take the fourth norm, what happens when you multiply four of these monomials? Well, if there's some coordinate that appears in one out of the four monomials, or three out of the four monomials, then when you take the expectation, you're going to get the expectation of f xi, or of xi cubed, which is zero.
00:14:26.120 - 00:15:16.534, Speaker A: So when does it not cancel? Whenever I take x I x j x k x l, I count f at if l. Precisely. If every coordinate appears in an even number of these sets, I, j, k, and l, and that's just saying that their symmetric difference is zero. So the four norm is the expectation of f. And now you see why even integers come in. If I were trying to, the three norm is not the expectation of f cubed, it's the expectation of. So? So I have to compare this expression over quadruples, where the sum the z two to the n sum is zero with the sum of all the squares.
00:15:16.534 - 00:15:58.620, Speaker A: So here's what we're going to do. I'm going to fix some partition of my ground set. So I'll call this script I, and my calligraphy is very weak, so I'm just going to use capital for script. This is going to be the I part, the j part and the k part. And then that if I want I Jkl, the symmetric difference be zero. That means that L has to be this part. So if I know I, j, k and l I, j and k, then I know l.
00:15:58.620 - 00:17:01.774, Speaker A: So I'm going to partition my ground set once and for all, or I'll partition it, and then I'll look at these sums, but I'll only look at things which are consistent with my partition. So I'll only count if hat I if the set I sits inside the big I and same with jk and l. And for quadruples, I want the whole thing to be consistent with my partition. So I look at IJKL, I look at their Venn diagram of those four sets and see is it consistent with the partition I took? If it is, then I count it, if not, then I don't. Okay, and now I, I'm going to prove that for a fixed partition, if I look at the what corresponds to the four norm and it's actually smaller than what corresponds to the two norm, and I'm going to be happy. And then, and in order to do that, I'm going to use a fractional version of Shiro's lemma. I'll tell you about Shiro's lemma in a second.
00:17:01.774 - 00:17:43.976, Speaker A: And then. So this is what Nati told me, that in Hungary, they learned this in elementary school. So maybe Gabor can tell if it's true or not, that you prove something for the random case. And then you show that the general case is just like the random. So if this is true, for every partition we take a random partition, it will hold for the random partition. But when you do the random partition, what you get here is going to be intimately related to the four norm, and what you get here is going to be intimately related to the two norm. So you get what you want, but you lose some multiplicative constant.
00:17:43.976 - 00:18:25.648, Speaker A: So that's why I was saying this can't give you the tight constant. What's the difference between the script I and regular I? Script I. Where is there a script I on the board? So this was a partition of the ground set. It starts with a let. Oh, so the, these are a subsets of my ground set. And these. So, okay, the union of script I script j's script k.
00:18:25.648 - 00:18:51.916, Speaker A: This should be script k and script l is all of my ground set. I take my ground set and I partition it into seven parts. And these four parts are script I. These four parts are script j, et cetera, et cetera. Okay, does that answer your question? In the sum where here. So now I'm looking at all quadruples I, j, k l, which are consistent with script I. Script J.
00:18:51.916 - 00:19:16.804, Speaker A: Script K. Script L. What does that mean? It means that the intersection of I and j lie in the intersection of script I and script j, the intersection of in j minus k. So I look at the Venn diagram of ijk and l. It also gives me seven parts. Each part lies within the corresponding part of the partition of the ground set. Okay.
00:19:16.804 - 00:20:34.146, Speaker A: And the reason I'm going to need that is because I want it to be the case that if you tell me what the union of I, j, k and l is, I can reconstruct from that what ij and k and l are. So if I, j, k and l are consistent with my partition of the ground set, and you tell me that their union is this, then I know what I, jk and Al are. This is I, this is j. So that, okay, so. Hmm. Okay, so yeah, yeah, but so I have what, ten minutes? Okay, so, okay, so what's sher's lemma and fractional shear's lemma? And why is it related to this? So here's shearer's lemma. I have some ground set v and I have a family of subsets of v.
00:20:34.146 - 00:21:13.844, Speaker A: Let's call it e. And I have f one up to fr are subsets of v. So let's start with the case where there a partition of v, but they don't necessarily have to be a partition of v. So let's say I have f one, f two and f three. Then for every element little e inside big e, every set in my family, it has, I can look at its projections at its intersection with f one. Let's call this little e one. Intersection with f two is little e two, and intersection with f three is little e three.
00:21:13.844 - 00:22:27.160, Speaker A: And I can ask, what do I see when I look only through a window on f one? How many different sets do I see? How many different intersections do I see? Let's call the set of intersections that I see. E one and e two is what I see when I look through this window. And e three is what I see when I look through this window. And obviously the product of the three sizes here is greater or equal than the size of e, because there's an injection from sets in e to the cartesian products of the I associate every set in e with a triple coming from the first coordinate from here, second from here, and third from here. So obviously the size of e is less than the product of these three. Now what happens, let's you have some positive integer t. What happens if every one of these sets, every vertex, every element in my ground set is covered by at least two of these sets? Okay, so if every element is covered by two of these sets, then this is overshooting.
00:22:27.160 - 00:23:09.888, Speaker A: And I get, if every element is covered by t of these sets, I get to put a t here. Okay, that's shearer's lemma. Shearer's lemma says that if every element is covered by t of these f's, then you get to put a t here. And there's a very nice, well, the shearer's proof isn't difficult, but Gaikumar, when he was a postdoc in Jerusalem, he showed me the book proof of this. It has a very nice proof. And if you know the, the bulk proof, then you can also prove the following. So this is a small variation on Schiro's lemma.
00:23:09.888 - 00:24:01.714, Speaker A: What happens now is the setting is the same as before, but every one of these eis gets a weight. So for each ei associate with it a non negative real weight w I of ei. And the reason I put wi here ei is a set. That set could once play the role of e one and once play the role of e two. It could happen that that set is the intersection of one edge with f one and have a different edge with f two. Remember, f one and f two overlap. So when I look at it as a projection of something from the intersection with fi, I give it a certain weight wi, and then we just get a weighted version of what we had before.
00:24:01.714 - 00:25:10.628, Speaker A: Instead of counting how many sets I have in my family, I go over each set in my family and I give it the product of the, of the weights of the project, of the projections. Okay, so if all the weights were one, then this product would be one and I'd just be counting the number of sets in my family. But here, for every set I count it, take the product of the projections of the weights. And on the right hand side, instead of counting the size of ei, I go over the elements of ei, and for each one I look at the weight raised to the t power. So there is one weight function for each I, there's a weight function wi, but I is ranging from what to what? I is ranging from one to r. It's the. So for every set wi of ei, or is it wj of ei? No, I mean, wi of ei or no, no.
00:25:10.628 - 00:25:32.352, Speaker A: So I look at e and e has projections. Ei is the intersection of e with fi. So I look at an edge in my hypergraph, I look at the set e, then I look at all its intersections. Each one of them is endowed with a weight. Take the product of those weights. Okay. Okay.
00:25:32.352 - 00:26:15.304, Speaker A: So how is this related to, how is this related to the inequality we were trying to prove? So here's the fractional version of shearer. And now this should look to you a little bit like this. So let's see what we have here. The my family of sets. So now I, for every I, j, K and L, which are consistent with my partition, I look at their union. That is e, that's an set in my family. And the weights, when I look at e, I look at the intersection of e with I.
00:26:15.304 - 00:26:41.006, Speaker A: That's, this here, this here is going to be ei. So now I have e I, e, j, e k, e l. Instead of I going from one to Nijk and L. Are you going from one to R? Yeah, instead of I going from one to R. Now I just have I jk. Now. And the weight I give is the Fourier coefficient.
00:26:41.006 - 00:27:20.406, Speaker A: I didn't say before, but we can assume that all the Fourier coefficients are positive because on the side it's supposed to be bigger. They appear with squares. So if they're negative. Yeah. Okay. So the weights of the projections are the Fourier coefficients, and so the product of the weights associated to the he are just the, the product of the four Fourier coefficients. And why is t equal to two? Well, precisely because this is a, this is the symmetric difference of I, j, k and l is zero.
00:27:20.406 - 00:27:58.304, Speaker A: But the union is everything means that every, every element is covered either two or four times, so it's covered at least two times. So I use the lemma with t equals to two, and I get that. And that was what we needed for a fixed partition. And then for the general case due to the random partition. Okay, let me. So that's all I wanted to say about the stuff with Vojta. Let me say just a few words about the new proof.
00:27:58.304 - 00:28:48.334, Speaker A: So now I want to prove the primal version now. And there are two cases, the simple cases when the function that appears is a Boolean function, and the more general cases when it's not necessarily boolean. And also, I want to, instead of looking at one function, I'm looking at a two function version of this inequality. Supposedly, seemingly, it's stronger. I mean, by taking two functions to be equal, you immediately get the inequality for one function. But it's not really stronger, it's equivalent. So let's see, what, what's the Boolean case? I have two sets, x the script x and script Y.
00:28:48.334 - 00:29:32.450, Speaker A: And I take x and y to be epsilon correlated vectors. And on one side of the inequality, I ask, what's the probability that that capital X falls in script X and capital Y falls in script Y? That both these things happen. And on the right hand side, I just look at the measure of script X and the measure of script Y. But I have to give it some advantage. The fact that these are correlated means that I can make the left hand side slightly bigger than what you'd expect. But if I raise this to the power of one over one plus epsilon, then I'm making this side bigger also. And this tells you that this size is bigger, but not too much bigger.
00:29:32.450 - 00:30:37.938, Speaker A: And trust me, this is exactly, it's exactly the Boolean case of Wechner's inequality. And then, of course, you can say, if it's not Boolean, then I want the same thing for functions f and g. And why would this have anything to do with information theory? Well, the thing is that I'm taking x randomly and Y randomly, and what I'm going to do, what I'm not going to do in this talk, but what I do in the paper is I reveal the bits of x and y one by one. And I ask, how much information do I gain from revealing these bits? And I compare it to how much information I'd get if I was just looking at each one of them separately. So if I take the log of both sides on one side, I'm going to get the entropy of something on the other side, I'm going to get the entropy of something else. And then if I use the chain rule, then I just go bit by bit. And when I decided to do this, I said two things could happen.
00:30:37.938 - 00:31:38.978, Speaker A: One thing that could happen is that on average, the bits corresponding to the left side will have less information than the bits on the right side. And if that's just on average, and it depends on what you've seen before, then all is lost, and this proof won't work. But happily, what happens is that no matter what you've seen up till now, every single coordinate that you reveal, irregardless of what you've seen up till now, the bits corresponding to the left hand side carry no more information than what they're supposed to carry. And proof goes through. So instead of doing it by induction on the dimension, you're just saying that for each coordinate you get some contribution. And on every single coordinate, the contribution to the right hand side is greater than the left hand side. And for the non Boolean case, some I thought would be more difficult.
00:31:38.978 - 00:32:22.458, Speaker A: But actually you take the same proof and add two more lines. So you just slightly alter the definition of the random variable whose entropy is the left hand side. You add just one more coordinate, telling you once you know x, you're somewhere between one and f of x, and another coordinate tells you somewhere between one and g of y. And you can assume that these are positive integers. I won't say y right now. And then, just by adding two more lines to the proof, you go from the Boolean case to the non Boolean case. And let me just go through the proof quickly.
00:32:22.458 - 00:33:09.380, Speaker A: So if you see any errors, tell me because. Yeah, and then, okay, you have to take, of course, this case into account and. Yeah, and then you're done. Yes. Can you give us like a minute or two, just the definitions of random variables, without showing us? Sure, yeah, I'll show you. But you really don't need any ingenuity for it. It sort of jumps out at you.
00:33:09.380 - 00:33:40.922, Speaker A: Because this is what we want to prove. You take the log of both sides and trust me, this is just straightforward. You write out this expression, what it means. Each point x gets weighed two to the n, and you weight it by. If x and y agree on a coordinate, you get an upgrade, one plus epsilon. If they disagree, you get a downgrade one minus epsilon. So this is what you have to prove.
00:33:40.922 - 00:34:21.933, Speaker A: And then, as usual in entropy, you want everything to be integers. So you look for two integers, snr, r and s, such that their ratio is one plus epsilon over two and one minus epsilon over two. And then you get this. And now you can tell me what the random variable is. It's going to be very clear. I look, I define for every coordinate I. I define four sets.
00:34:21.933 - 00:35:27.200, Speaker A: These are just auxiliary sets of size r, r, s and s. Well, I'm thinking xi and yi can be 0011. And then what does my. I have a random variable, zxy. And I take it uniform on all, on all vectors such that x belongs to x, y belongs to y, and for every I z I. So if xi is equal to one and y I is equal to zero, then I have to take zi from here, okay? And this just tells you that conditioned on this being xi being one and y being zero, it's going to have entropy log s. It's uniform on a set of size s.
00:35:27.200 - 00:36:23.066, Speaker A: And so that tells you what z should look like. And then, okay, then you have to see what it means bit by bit. And eventually it comes down to a calculus question, a function. It's going to boil down to a calculus question where you have a distribution on four points and you have to show that some function is non negative. So you see, you look at the entropy of x, you look at the entropy of y, you look at the joint entropy, you have some price to pay where every time x is equal to y, and you have to show that this is non negative. And if you just had to show this was non negative, it would be quite difficult. But you know what the minimum, you know what you hope the minimum is? When.
00:36:23.066 - 00:36:44.070, Speaker A: When both x and y are uniform on the whole space, then you get the quality. So that tells you what you expect to be the minimum. And then you just have to show that that's the unique minimum. And use Lagrange multipliers and you find conditions for the minimum. And it turns out that this is the unique minimum. Yes.
00:36:44.222 - 00:36:47.646, Speaker B: One epsilon is a very special norm because the conjugates are equal.
00:36:47.750 - 00:36:48.502, Speaker A: How are they?
00:36:48.598 - 00:36:50.350, Speaker B: One plus epsilon is a very special norm.
00:36:50.382 - 00:36:50.550, Speaker A: Right.
00:36:50.582 - 00:37:08.834, Speaker B: Because the conjugates are equal. How does it carry off other norms? Like two norms? Or like in your statement, you have this one plus epsilon on both the powers. That's because one plus epsilon is very special, because the healthier conjugate, one plus epsilon and the corresponding Q for Ronald Wechner is matching.
00:37:09.414 - 00:37:09.846, Speaker A: Right.
00:37:09.910 - 00:37:15.234, Speaker B: So you're on equivalent form, which is for one plus epsilon. No. Am I.
00:37:16.054 - 00:37:29.864, Speaker A: You're asking about the PQ. I didn't try, but I'm optimistic that you could try to do the proof and it would work. Also, for PQ, it's very simple.
00:37:29.904 - 00:37:34.280, Speaker B: So, I mean, for one, how does one norm proof, coach? The other norm is. I don't know how.
00:37:34.432 - 00:37:47.164, Speaker A: So what you're saying is this could work for a slightly more general statement, and maybe it does. I don't know. I didn't check it. Thanks.
