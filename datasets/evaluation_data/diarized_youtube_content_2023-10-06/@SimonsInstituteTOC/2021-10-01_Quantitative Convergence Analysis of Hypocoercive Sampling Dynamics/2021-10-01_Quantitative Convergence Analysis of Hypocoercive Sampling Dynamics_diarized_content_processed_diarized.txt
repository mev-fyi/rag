00:00:00.440 - 00:00:19.854, Speaker A: All right. All right. Welcome back. So our next speaker is Zhenfeng Liu from Duke. He's actually one of more of our serious remote participants for the semester. And you always see his name in those Zoom meetings. So he's going to talk today about hypocursive sampling dynamics.
00:00:19.854 - 00:00:21.114, Speaker A: Take it away, Jen.
00:00:21.414 - 00:01:07.994, Speaker B: Thanks a lot for the organizers for putting together such a nice workshop and for the kind invitation. Unfortunately, we couldn't be there in person. I plan to come in October, though, so that to meet you guys in person and to discuss. So, what I'm going to talk about today is about some of our recent works on understanding the convergence analysis for hypercohesive sampling dynamics. I don't think this word has appeared yet in the workshop, but it's actually something familiar to many of you. I should mention that this is a series of work, and joined with my former student Yu Chao and Lihan Wong. And Lihan is actually in the workshop, so that if you have any questions, you can ask him during the break.
00:01:07.994 - 00:02:23.010, Speaker B: Okay, so I guess everyone here is, I mean, we're all here to talk about sampling, so that I can just basically skip these slides, but just to give you some ideas of where we come from, so that we want to understand the convergence rate and asymptotic variance and so that, to determine the efficiency of MCMC sampling algorithms. Okay. And now our purpose here, I mean, so for today is to consider sampling problems in high dimensional space with continuous state variables. So it's x and in RD, so it's a full space sampling problem, and we'll be mostly focused on strongly log concave distributions, while some of results actually apply in more general cases. And so for this case, as we have seen during the workshop and also during tutorial, that a common design principle of sampler, MCMC samplers is to first start with a continuous time Markov process and then do a time disclamation. Perhaps the most famous example for that is the overdraft long event dynamics, or, I mean, in statistics machine learning literature known as longevity dynamics. So the is the dynamics just this is like a random noise per turbine gradient flow under the potential U.
00:02:23.010 - 00:03:04.482, Speaker B: And one can show that, I mean, so this is the invariant measure of the dynamics, and one can use this, I mean, as a sampling dynamics for the Gibbs measure. And as always, we are hoping for, like, fast convergence of the sampling dynamics to equilibrium. And for this case, it's actually sort of easy to see. And for that, I mean, so the focus of my talk or the techniques that we'll be discussing in this talk is actually PD technique. So I also choose this topic to connect to two themes of this program. So, one way to analyze this is to write down the backward chromosome equation associated with the SDE. So this describes the evolution of observable.
00:03:04.482 - 00:03:51.376, Speaker B: If you start with some observable h zero, and if you flow the time to infinity and the solution will converge to a constant and converge to expectation of the observable h zero. So which is what we want to sample. And now, so the long term behavior of the sampling dynamics or the overdone Japan dynamics is well understood. And there are many approaches to do that. But one easy approach is through the concurrent inequality. So if the potential, if the distribution satisfy a Punkahir equality in this type, so that you bound the variance of the distribution in terms of the gradient of h squared with a punkahi constant m, and so then we know that it will exponentially converge. Okay.
00:03:51.376 - 00:04:18.462, Speaker B: And just as an example, that, so if your potential is m strongly convex means that Hessian of the potential is bounded from below by m, then it satisfied, I mean, the distribution satisfied the puncture inequality with this constant m here. Okay. And so I guess it's sort of familiar to everyone that under the assumption of puncturing on, I mean, the sampling dynamics converges exponentially with the rate given by the puncture rate constant in l two.
00:04:18.638 - 00:04:19.102, Speaker C: Okay.
00:04:19.158 - 00:04:55.904, Speaker B: And, I mean, this is a simple proof. It's actually a one line proof for this. It's the standard energy method. You multiply the PDE on the left hand side by h and then take it from derivative. Okay, so I hope this is clear so far. Okay, so just as an introduction of what we know for the overdamped case, or it's reversible case. Okay, so the focus of our kind of consideration of work is really trying to understand, I mean, the quantitative convergence rate for the hypercohesive cases, or in other words, in other words, for that is a non reversible sampling dynamics.
00:04:55.904 - 00:05:41.954, Speaker B: And perhaps the most famous example, at least to this audience, is the so called underdamped longer dynamics, or the kinetic longevity dynamics, which, I mean, you can, this is sort of related with the HMC, as we also see later. This part is the hamiltonian dynamics. So the time derivative of position is given by the velocity, and the time derivative velocity is given by the force, which consists of three parts. So one is the gradient part, which is similar to the overdamped case before. And so that this consists of the hamiltonian part of dynamics. And then we have the noise and dissipations where this the only parameter here, and I kind of non dimensionalized. So that only parameter is the gamma, which is the friction parameter, physically speaking.
00:05:42.694 - 00:05:43.422, Speaker D: Okay.
00:05:43.558 - 00:06:50.372, Speaker B: And I mean, the reason that this is called under damp longer dynamics, and I mean, actually in physics is usually just known as long german dynamics, is because that if you send the friction parameter to infinity and the rescale time properly, then it will actually converge to the overdrive dynamics. And one can also show that using either PDE or just check the generator. For example, that invariant measure for this longer band dynamics is given by product measure, where in the variable x is just the Gibbs measure that we want to sample, and in the v is a gaussian pressure. So now we try to understand the convergence rate for this longer band dynamics. I mean, the purpose of that is hopefully, once we understand the convergence of the continuous dynamics, then we can think about time dispositions and getting the estimate for the discrete sampling MCMC samplers. And as before, I mean, kind of parallel to the overdamped case. One natural approach is to think about the PDE analog of the analog of the backhoe microbial equation that we have seen before or for equation.
00:06:50.372 - 00:07:29.574, Speaker B: And in this case for the kinetic longevity dynamics, the corresponding Kolmogorov equation is given by this. I mean, where the generator consists of two parts. So one is the hamiltonian part, which is actually nonreversible because this is kind of a rotation, if you think about what this action is on the phase space. And the other part is the fluctuated dissipation part, which consists of the noise and also dissipation of the velocity. And this part is actually reversible. And I mean this, I mean, if you think about, if you neglect, say for a moment, I neglect this part, and just think about dynamics in V. This is actually can be viewed as over time dynamics for the gaussian potential.
00:07:29.574 - 00:08:26.284, Speaker B: So, and of course, I mean, once you write down a generator, you can check explicitly that the invariant measure is given by what I claim to be so, which is rho infinity. So now the question is that, I mean, so if we think about the convergence rate of overdamped, as we said before, that if you think about convergence rate in chi square or in l two, the rate is given by m, which is the puncture constant of the measure. So what's the case of the underdamped case? So this is actually the question that we started asking ourselves a couple of years ago. And so if there was any, I mean, so by in some sense, you are kind of augmenting the state space by introducing these auxiliary variables, which is the velocity from a physical point of view. And the question is whether that helps. And indeed it helps. And this is the theorem that we proved a couple years ago.
00:08:26.284 - 00:09:34.650, Speaker B: So, under some technical assumptions, what one can show is that in l two, the sampling dynamics actually converges exponentially with a rate by lambda, which is given explicitly by this formula, where I mean, so the gamma is the friction parameter that we have in the dynamics, and m is the perfect constant of the, of the Gibbs measure that you want to sample. I mean, related with this u, for example, if, I mean, if we know what m is and we can choose gamma to optimize this rate. And if you choose gamma, if you choose gamma proportional to the square root of m, then the rate that you will get is the square root of M. Okay, and let me just mention that we have results of the covariance rate analysis for more general cases. But I'm going to focus on this particular case here, which is, you can think of it as strongly a concave case and where this m is really concave parameter. Okay, so, let me make a couple of remarks. So, first of all, I mean, so this is, I mean, if we simplify the situation by setting that average of the observable is equal to zero.
00:09:34.650 - 00:09:58.194, Speaker B: And this is just a summary of the results, so that x exponentially converges in l two in the dual space of the observable, right? I mean, with the rate, which is the square root of m. Okay, and by the way, I mean, so this, we are thinking taking the perspective of observable here. But if you're thinking in terms of the probability measure of the forward equation, this means that the chi square divergence converges with this weight.
00:09:58.654 - 00:09:59.474, Speaker D: Okay.
00:09:59.934 - 00:10:47.770, Speaker B: And first of all, if you compare with the overdrive decay dynamics, remember that overdrive dynamics has the exponential rate, but with, with a worse constant. Okay, I mean, thinking about case, we are always thinking about case where m is small so that this is slower than this. Indeed, you see that the under damped lounge event actually accelerates the convergence. And also, it's not hard to check that this root m dependence of the convergence rate is optimal because you can take u to be just a simple gaussian. And by explicit calculation of what's happening of a gaussian measure on the dynamics, you can check that this is the optimal rate. You cannot get anything better than this. And also, I mean, there are many techniques available to get quality quantitative estimate for the convergence rate for longer.
00:10:47.770 - 00:11:31.914, Speaker B: And as far as we know, that this is only a technique given root m up to date. Okay. And also, I mean, because we have showed the convergence in chi square. So this also implies the convergence in relative entropy and total variation, which are all in root m. Okay, any questions for this before I move on? Just making sure that not I'm still connected. So, one thing that I want to mention, and actually I think bring to attention of this community is that, I mean, so oftentimes we spend a lot of time to analyze Mala or HMC type of algorithm. But actually there are many interesting sampling algorithms out there and recently proposed by statisticians and applied mathematicians.
00:11:31.914 - 00:12:26.284, Speaker B: And I feel they actually worse considerations by the community trying to, especially in the CS community trying to establish non symptomatic bonds. So for that purpose, actually, I'll spend some time trying to introduce you some new dynamics maybe, which as far as I see, haven't been covered in this program. So, I mean, there was a new family of sampling dynamics called piecewise deterministic markup process, or PDMP for short. And so their kind of behavior is a bit like, so you have a random clock, which is, you can think of it as Poisson clock, and I mean, besides between the clock wings. Okay? And so that dynamic just follows some deterministic trajectory. So that's why it's called the deterministic marker process. So piece, but it's piecewise, so that when the random clock happens and then it does something, and to ensure that you're actually sampling the correct measure.
00:12:26.284 - 00:13:18.404, Speaker B: One of the example that is perhaps familiar, uh, because it's very related with HMC. So it's the randomized HMC algorithm. Okay, so, uh, so it's almost the same as the standard HMC, but instead of the standard HMC, where you run the Hamilton dynamics for a fixed duration of time, capital t, and then you redraw the velocity. So here, what you do is that you run the hamiltonian dynamics, but you, at the same time, you have a clock which ring at a rate that is gamma. And when the Poisson clock wins, what you do is that you just redraw the velocity according to a gaussian distribution. So it is the hamiltonian HMC algorithm, but for the time duration t, which follows a exponential random variable with rate, which is given by gamma. I hope this is clear.
00:13:18.404 - 00:13:49.942, Speaker B: Now, this is one of the example of the PDMP dynamics, but then there are more, I mean, many more recent examples. So one of the, I mean, kind of, I will introduce two of the kind of more popular ones that is kind of used these days. So one is so called zigzag sampler. So this dynamics is actually quite interesting to some extent. And what it does is that, I mean, so forget about this part first. So only forget about this part first and only look at this.
00:13:50.118 - 00:13:50.838, Speaker D: Okay.
00:13:50.966 - 00:14:34.664, Speaker B: So if the dynamics, if this is my only part of my generator, then what the dynamics go, is it just a straight line? So what this says in terms of the ode is just that the x dot is equal to v. So with a fixed velocity, the particle just flies in a straight line. So, as you can see from the picture here, maybe not large enough, I'm not sure how it views in person, but you can see that this follows a straight tractor. And then it, I mean, there were several, there were several Poisson clocks. My wing. And when it wins, it does a couple of things. So if this person clock wins this part of the generator, what it does is just that it redraw the velocity as a gaussian random variable.
00:14:34.664 - 00:15:17.434, Speaker B: And if only with this, then there is no reason that this would sample the correct measure that that we want, which is the Gibbs one, which is the Gibbs measure. And then you add on top of this Poisson clock, a bunch of another person clocks, which is the number is given by the dimension of the problem. And for this Poisson clock, what you do is that whenever they ring, you flip the velocity in that direction. So imagine that you are flying this direction previously. I mean, as we said, we always go straight line, okay? And whenever one of the Poisson clock rings. And what you do is just that, you flip the velocity in that direction. So which is this direction that we're talking about.
00:15:17.434 - 00:15:37.274, Speaker B: Okay, you flip the velocity. So that's a reflection. And you go to that. I mean, you follow the trajectory in that new direction. So as you can see also in the picture here that you go here, and then one clock wins. I mean, you flip the velocity direction in x one and you go back there, and then you're going like this.
00:15:37.814 - 00:15:38.542, Speaker D: Okay?
00:15:38.678 - 00:16:45.634, Speaker B: And then you pick the rate of this flipping of the directions according to the partial derivative related with partial derivative of potential multiplied by the velocity. And then you can check that actually, for this generator, I mean, you can check that this l star acting on the Rho infinity before is actually equal to zero, which means that the environment measure of this sampling dynamics agrees with what we want to sample. Okay, so this is known as zigzag sampler, introduced in 2019. And there is a very similar kind of related sampler, which is called bouncy particle, which is slightly introduced slightly earlier in the physics literature and also picked up by in the machine learning literature later on, which is very similar, so that you follow some straight lines, and then when the clock wins, you either withdraw the velocity from a gaussian, or in this case, you flip, but not according to a coordinate plane. You flip according to the gradient of u, so the level set of the potential functions. Okay. And so this is like a cartoon that shows the trajectory of this under the boundary particle process.
00:16:45.634 - 00:17:25.554, Speaker B: Okay. And one advantage that why people like these things is that, I mean, so oftentimes we know that if we have a lot of data, that we need to estimate the gradient, you know, using some stochastic gradient estimations. And now, if you use a long dividend sampler or using HMC, and you're not using the accurate gradient, then that will induce bias. But one nice properties of these executive particle samplers is that even with the stochastic gradient, they could be made unbiased. So I'm not going into details of these things and also applications, but I have been collaborating with some statisticians that duke on these things. And if you're interested, we can talk more afterwards.
00:17:26.534 - 00:17:27.774, Speaker A: Can I ask a quick question?
00:17:27.894 - 00:17:28.582, Speaker B: Yes.
00:17:28.758 - 00:17:31.502, Speaker A: So can you come back to the previous slide for a sec?
00:17:31.638 - 00:17:32.314, Speaker E: Sure.
00:17:32.934 - 00:17:38.510, Speaker A: Okay, I see. So I was looking for where gradu was. So you only need the coordinates of you.
00:17:38.542 - 00:17:49.710, Speaker B: Okay, yeah, exactly. So this is actually a part we'll come back to. So it's a bit like a cognate algorithm that Ching discussed earlier in the workshop, and it's actually for each bounce, you only need partial derivative.
00:17:49.902 - 00:17:59.326, Speaker A: Okay. The other question that I have is, could you remind us what the role of gamma is in the, in the discrete time HMC? Because you only discussed the continuous time HMC. And so gamma.
00:17:59.390 - 00:18:06.034, Speaker B: Yeah, one over gamma is kind of like t. So, which is a duration of the hms at the hamiltonian trajectory on average.
00:18:06.614 - 00:18:08.062, Speaker A: Okay, thanks.
00:18:08.198 - 00:18:16.102, Speaker B: So the smog, in other words, the small gamma, is that you are running the hamiltonian dynamics for longer. Before we draw the sample, we draw the velocity.
00:18:16.278 - 00:18:18.798, Speaker A: Any expectation the gamma plays the same role, right?
00:18:18.846 - 00:18:20.022, Speaker B: For those, yes, exactly.
00:18:20.158 - 00:18:21.074, Speaker A: All right, thanks.
00:18:24.114 - 00:19:26.542, Speaker B: I mean, the purpose of the randomized HMC, by the way, is to actually kill the resonance that could happen if you run the gamma at fixed interval, so that you may run into trouble. So that's. And also for analysis on the continuous level, it's actually easier, I mean, to formulate the problem this way. Okay, so now, I mean, so we have results, I mean, which is actually follows from a similar analysis framework as the longevity dynamics for the quantitative convergence rate for these dynamics as well. So, I mean, we are summarizing the results of these pdmps on this single page. And the take home message, I guess, is that for the randomized HMC, as you can imagine, because it's so connected with longer band dynamics, and actually on the continuous level, the convergence rate is the same. By the way, I should mention that when I write o, so everything I implied is absolute constant, so that there was nothing dimension dependent or anything like that hidden in o variables, so that all these things are dimension independent except the last one.
00:19:26.542 - 00:20:35.378, Speaker B: And now for zigzag, it's deteriorated a little bit by the condition number where the l is. I mean, so for simplicity here, we assume that the gradient u is smooth so that the hashing is bounded from above by l and bounded from below by small m, so that this is the condition number that you can view and which is sort of reasonable because, I mean, as you can see that in a zigzag, because the algorithm design is designed non explicitly coordinated planes, so that if you kind of make the potential much skewed, and because the algorithm does not know that and has, I mean, it's not adaptive to that, so that it will suffer from the condition of. Okay, I mean, you can make it more adaptive, but I mean, we're talking about the original zigzag stack here. And for bps, I mean, we do not know whether it's sharp or not, but I mean, the best we can prove is that it will be deteriorated by root of D in the, in the convenience rate. And I mean, as far as we know, that all these convergence rate are state of art. So as of today. Okay, so, of course, I mean, many of the audience here are interesting algorithms rather than PD analysis.
00:20:35.378 - 00:20:37.170, Speaker B: So that, I mean, it's just.
00:20:37.362 - 00:20:50.654, Speaker A: Sorry to interrupt just one more question. So those previous results, again, this is for, like, exactly, simulation of the hamiltonian dynamics. For example, for RHMC, you're assuming that you have exact hamiltonian dynamics in between two clocks.
00:20:50.774 - 00:20:57.206, Speaker B: Yeah, exactly. So that, I mean, we are talking about PD results here and I'm coming to the algorithm.
00:20:57.350 - 00:20:59.414, Speaker A: Okay. All right, thank you. All right, thank you.
00:20:59.454 - 00:21:32.114, Speaker B: So all these are continuous time results for PDE's or for the SD itself. Okay, so now let's come to the algorithms and the zigzag sampler. I mean, and one advantage, and it's also very similarly for bouncy particle samplers. But I'm going to focus on this exact sampler. And I mean, the reason, one reason is because we can obtain a slightly better rate here. Okay? And one advantage for that is that you actually do not make any discretization error, because the only dynamics that you need to integrate is as simple as this. Right? So it's just a straight line.
00:21:32.114 - 00:21:59.700, Speaker B: And so unlike hamiltonian dynamics, or longer than that, we need to discretize it to make errors. I mean, so in principle, you can simulate this. Exactly. Okay. Of course, there was no free lunch. And whereas the price, the price is actually, you need to pay the price to simulate the Poisson process. And remember that for zigzag process, what do we have? A generator is something like, I mean, we sum over each directions and then there will be something that is vi and partial derivative of u in that direction.
00:21:59.700 - 00:23:01.960, Speaker B: So this is the rate. Okay? And to sample, to sample a Poisson clock with this rate. So if you do it naively, then this will be a nightmare, because what you need to do is that you need to integrate this along the trajectory and then kind of do a transformation of the exponential random variable, which is, I mean, not, which is unpractical, and then you're also making errors. And one way to do that, actually, there is a generator here. One way to do that is that you introduce an upper bound for these things, so that if you can simulate a Poisson process with a rate that is higher, that is guaranteed to be higher than any, I mean, of the rate that you occur in the inhomogeneous Poisson process, then you can simulate the Poisson process with a larger rate and then do an acceptance rejection afterwards. Okay, to get the correct exact, I mean, this is kind of like rejection sampling. Uh, but for the Poisson process and to get exact simulation for, uh, for the inhomogeneous poisson process.
00:23:01.960 - 00:23:52.294, Speaker B: And this is known in the literature called the Poisson theming. And for that, what you need to do is to get a uniform upper bound for this rate. Okay, now, I mean, this, I mean, in general, this is a hard question, right? Because, I mean, this depends on your knowledge of the, what potential look like. But if you assume that we're in the setting of strongly law concave distributions, with which, I mean, with known parameters, of the lipstick constant and strongly convexity parameters, then that is possible. I mean, because, I mean, this is just a gradient, right? So you can bond it by using the Lipschitz constant of the gradient. Okay. And so in that context, okay, if you have a strong log concave, and then you can come up with a upper bound using the, using the potential, and using the l of the elliptic constant of potential.
00:23:52.294 - 00:24:36.284, Speaker B: And then we'll give you a practical algorithm, okay, and the algorithm that we analyzed the complexity, I mean, together with Li Han last year for the zigzag sampler. And what we can prove is the following. So if you assume that potential is strongly convex and with condition number kappa, okay, and we have to assume a warm start assumption that chi square, I mean, the reason that we have chi square is because on the PDE level, all our convergence results are for chi square, okay, so it's l two convergence. So that, I mean, so for the warmness, we also need to assume chi square. And for the chi square of initial distribution with respect to target is bounded by something like exponential d. If it's exponential d, then it's not oneness. But unfortunately, we need to have a log d here.
00:24:36.284 - 00:25:21.776, Speaker B: Okay, so which is, which make it not a feasible start. Okay, and under this assumption, and also if you do not require like a very extremely high precision. And the reason for both of this, and that is because we cannot run this process for extremely long time, I mean, before some disaster may happen, because everything is random. So we need to control the run time, okay, I mean, the total, I mean, total time of the sampling dynamics, which comes into play here. Okay, then with these two assumptions, then with, I mean, but this is really mild because this is exponentially small indeed. Okay, but this is a, this is a restriction to our starting point. Then with high probability, the zigzag sampler would output a random variable that with errors smaller than epsilon.
00:25:21.776 - 00:25:30.164, Speaker B: And the query that we need in terms of partial derivatives is d times kappa squared, and everything else is polylog.
00:25:30.824 - 00:25:31.504, Speaker D: Okay?
00:25:31.624 - 00:26:22.294, Speaker B: And I mean, here all, I mean, all these constant that we didn't mention are absolute constants. Okay, so, I mean, by the way, I should mention that this is the number of partial derivatives, which is related with the question asked before, so that when each bouncing event occur, we need to do a acceptance rejection. And that only use the partial derivative in the six example. So if you think that the evaluation of the partial derivative is one over d cheaper than the evaluation of the gradient, then effectively the cost of the sampler is root d times kappa squared. So of course, I mean, this, you may want to compare this with some of the recent works that we hear in the workshop. And also Yuan Su will talk about their work after me, I think. So, where they get for Bala, I mean, you should compare with Mala, because we're in a high accuracy regime.
00:26:22.294 - 00:26:41.254, Speaker B: So our absolute dependence is polyloc and in Mala, they were able to get d root half times Kappa complexity. But I should comment that their warm start assumption is much stronger than ours because their warm start assumption is constant while we are exponentially divided by log D.
00:26:43.034 - 00:26:57.184, Speaker C: John Feng, I wanted to ask a clarification about that last point, because you still have the log of the chi squared divergence at the start in year bound. In what sense is this warm start assumption like weaker?
00:26:58.124 - 00:27:14.664, Speaker B: Oh, you get a good point. Yes, sorry for that. Take it back. So, I mean, so yeah, I think we should say that, I mean, we can work with this assumption. And then you are right, that dependence here actually depends on G. Sorry for that.
00:27:15.044 - 00:27:16.224, Speaker C: Okay, thank you.
00:27:18.164 - 00:27:37.254, Speaker B: And I mean for that. Actually, I don't exactly remember your bound, so maybe we can compare that afterwards. Okay, so now, I mean, given the remaining time, I think I have like 15 minutes left of. Ten minutes left.
00:27:39.154 - 00:27:41.138, Speaker A: Yeah. 1111.
00:27:41.186 - 00:28:09.886, Speaker B: Okay, ten minutes. So now I will actually talk a little bit about how we actually carry out the analysis for the convergence rate. And the, actually, the framework of the analysis are quite similar. So that I'm going to focus on the laundry bank case. So remember that for longer bank case, what we need to do is to actually understand the convergence of the PDE with initial condition given by that. And we know that this PDE will eventually go to constant.
00:28:10.070 - 00:28:10.846, Speaker D: Okay.
00:28:10.990 - 00:29:18.284, Speaker B: And now the difficulty of this analysis from a PDE point of view is that the operator l we have is not elitic. And because we have something first order here, and this is a rotation, it's not a diffusion. Okay? And this part is elliptic. And so in more technical terms, this is known as the hypo elliptic equation, hypo elliptical operator. And that's also, I mean, why we call it hyper cohesivity. And the reason is basically because you only have like, I mean, first order derivative in x, but not second order derivative in x, we only have second order derivative in v direction. And as a result, if you hope for coercivity, which in a sense is just as simple as following, so that if you take inner product with l, two of your generator with f, and if you do integration of parts in the previous case, so that in the overdamped case, what you will do is that if you take an integration of parts, I mean, for the generator of the overdamped, overdamped dynamics, and you can actually see that this is just a bike, this is actually just the gradient square and the l two norm of the gradient of g.
00:29:18.284 - 00:29:58.954, Speaker B: And then by puncura inequality, we can use this to bound out too. So this is actually comes into the proof of the energy method. So you want to mimic the same thing here for this, for the new kinetic language band equation. And what you can, you can follow the same line of the proof, and then you can do intuition of parts. But because, I mean, there is only actually the second order in v. And what you find is that this part of the generator, because it's for a starter and it's actually canceled out in this intuition of parts. And so after intuition parts, you only get the gradient, which is kind of you like, but that's only in the v direction.
00:29:58.954 - 00:31:03.614, Speaker B: So there was no hope that this gradient in the v direction can bound the l two norm, because, I mean, the function, for example, can depend only on x, so that the gradient vanishes, while the l two norm of cos does not vanish. Okay, so this is the trouble that I mean, preventing us by using the simple cohesivity estimate of the energy method to prove the convergence and to overcome that, the idea, which dates back to pomander, and also we sort of inspired by the work by John Crystal Murat and so Armstrong. And the idea is, actually, we need to augment its state space by also considering what's happening not at instant time, but also within time duration. From zero to t, we consider the augmented state space. So now it's not only x and v, but also we put in time t. And in that augmented time space, it's possible to establish a puncurrent inequality of this kind. So this is a puncurrent inequality that in the augmented time space.
00:31:03.614 - 00:31:38.730, Speaker B: So that lambda here is the back measure, and from the interval, from zero to t. And the mu is the measure on the extended state space. And on the right hand side, we have the gradient of v, which is what you expected from the generator. But if you only use that term, there is no hope to control the left hand side. And in order to control left hand side, we also need the other part of the equation to help. Okay, so that this is the, this is a non reversible part, non reversible part of the dynamics in our equation. And using together these two, that you are able to bound out to.
00:31:38.730 - 00:33:17.698, Speaker B: Okay, and now if we are able to prove this inequality, and the rest of the analysis actually just follows from a slightly modified energy estimate argument. So it's just because that if you write down how the l two norm changes, remember that, I mean, all our analysis is chi squared or the l two. So this was actually, the idea was borrowed from the paper by Armstrong Murat. And you can rewrite that difference as a time integral of the gradient of f, so that this is just far from the equation, okay, now using the equation, we know that this thing can be rewritten, I mean, as part of that, as the, as the other parts of equation, okay, so, because we know that the equation is given by the time derivative times, the hamiltonian part is equal to fluctuation dissipation part, so that this, I mean, we can rewrite this thing, okay, boundary field from below by the other parts of the equation in l two naught. So now you just split this term into two parts. One, is this keeping some part of original, and I mean, and also using the help from this term, and now using the Planck in aquarium, that we can use the sum of these two terms to control the l two norm, so that putting them together, you can, I mean, so this is split into two terms, and putting them together, you can lower bound the l two norm, or, because we have a negative sign that here that we upper bounded from the negative of the l two norm. Okay, so you get the exponential decay and the rate, I mean, so for, to get rate, you need to do some calculations to, to figure out what's exact ratius.
00:33:17.698 - 00:34:11.964, Speaker B: And then by the way, for those of the people who are familiar with HMC, so the t. So the time duration that we need to take here is actually proportional to root one over to get the optimal rate, this is actually proportional to one over root of m. So, which means that for running the HMC algorithm, if you want to get in a corresponding convergence rate, the deterministic hamiltonian flow needs to be as long as that. But then the problem is that as far as I know, if you run the HMC, the hematom dynamics for that time duration, it's very hard to establish a good coupling contraction argument to get the corresponding rate so far, even though that this is suggesting this kind of time duration. But on the level of coupling, we were not able to get the convergence rate that is matching using the coupling method.
00:34:12.624 - 00:34:40.750, Speaker A: Zhenfeng, can I ask a quick question about this inequality? I'm a bit confused. So if I look at the right hand side of your first, so I'm in the big display. If I look at the right hand side of the first row and the left hand side of the last row, it seems that you've proved again what you said you could not prove that the square norm of f is upper bounded by itself its gradient, which again, you know, if f depends only on x this will be zero. And so, yeah, this is, I mean.
00:34:40.782 - 00:34:59.614, Speaker B: So there are two difference here. So, first of all, this is actually over a time interval. So this is integrated over time from zero to t. And also, I mean, this is not for a general f. This is f being a solution of our PDE. Because, I mean, we use, in this inequality, we use the fact that f solves the backward Komogov equation.
00:34:59.734 - 00:35:01.194, Speaker A: Okay. All right, thank you.
00:35:04.264 - 00:35:25.680, Speaker B: And there is no hope if you want to prove this thing without the time augmented, so that that will be simply false. That term blows up. Yeah, exactly. So what I mean, if I hear Lihan correctly is, I mean, what he was saying, that if you send this t to zero, then, I mean, these constants blows up, so that the inequality is not valid.
00:35:25.832 - 00:35:26.924, Speaker A: All right, thank you.
00:35:30.964 - 00:36:00.486, Speaker B: Okay, so, I mean, to prove of the puncture inequality, I mean, I don't think I have time to go over these things, but I mean, if you are interested, you can talk to me afterwards or maybe talk to Li Han. But the idea is actually not that difficult in the sense that, I mean, we just try to do some integration of parts. But the most technical step is to find the best. Right. Test function to do the integration by parts. And for that one is a technical lemma. And so I'm going to skip that proof, I think, to leave some.
00:36:00.486 - 00:36:50.544, Speaker B: So, this is the technical construction of test function that we need to make. And then after you construct the test function, that things just follows from, like usual PD arguments. Okay, so with that, I think, I think it's a good time to stop and maybe leave some time for questions. So, I've discussed some of the quantitative analysis, but mostly on the continuous level, for these sampling dynamics, that is long event. And I think it's interesting to think more about these PDMP sampling dynamics as exact bouncy particle samplers and so on, so forth. I think there are many, many open questions. For example, I mean, what's the optimal rate for the bouncy particle sampler? Does that actually depend on dimension or not? I mean, remember that our rate has a root d, but it's not so clear to us whether that's sharp or not.
00:36:50.544 - 00:37:23.344, Speaker B: And also Twitter, I mean, so we showed analysis of the, under some one side assumptions of the zigzag sampler complexity. But I mean, to some extent, I mean, the assumption of the warmness is not as great. So whether it's possible to remove that. And in terms of rate, I mean, there is a kappa square dependence there, which I don't think is optimal. And I mean, so for example, how to improve that? These I think will be very interesting directions if people want to work on these.
00:37:23.724 - 00:37:24.260, Speaker D: Okay.
00:37:24.332 - 00:37:29.104, Speaker B: With that I want to conclude and if you want to read more, these papers are listed here. Thank you for attention.
00:37:34.604 - 00:37:41.224, Speaker A: I thank you for this great talk. Jenfei, do we have any questions? Let's start with Sinho. I'll get to you.
00:37:43.004 - 00:37:51.694, Speaker C: Hi, could you comment on what are the difficulties of trying to do this like a time augmented version of like a log solve eleven equality for this setting?
00:37:52.354 - 00:38:25.498, Speaker B: Yeah, so that's a very good question. And I think many asked this question and we also talked with a couple many people on this. So one difficulty I guess is even to formulate what should be the inequality. And I mean so that's already a bit unclear. I mean, because I mean here you need terms like this to help. And then of course, I mean if you think about loxabliff, then the most straightforward conjecture will be that to replace this f by root. Sorry, replace this f here by root, root f.
00:38:25.498 - 00:38:46.444, Speaker B: And replace these f's by root f. That doesn't seem to work as far as we can try to make it work. But maybe that's. I mean maybe there are some better ways to come up with the right form of inequality and try to prove it. But I mean. Yeah, so we are not, we do not really have much ideas how to do that.
00:38:46.784 - 00:38:55.004, Speaker C: Just a naive question like what if, what if I keep the f's the way they are and on the left hand side I replace it with like the entropy of f squared.
00:38:55.784 - 00:38:58.672, Speaker B: Yeah, I think that's equivalent way of writing things.
00:38:58.848 - 00:39:00.328, Speaker C: I see. Okay, thank you.
00:39:00.456 - 00:39:14.340, Speaker B: Yeah. One is the modified logos oblivion, one is logos object. And of course here the left hand side needs to be changed. I'm not meaning to control the l two norm.
00:39:14.372 - 00:39:21.780, Speaker D: I'm controlling f log f. Hi Jiang fi Chen. So can I hear me?
00:39:21.812 - 00:39:22.784, Speaker B: I can hear you.
00:39:23.484 - 00:39:32.944, Speaker D: Right, so your analysis very much reminds me of this hypercoercivity analysis for velocity of focal planck equation.
00:39:33.294 - 00:39:34.034, Speaker B: Yes.
00:39:34.414 - 00:39:45.634, Speaker D: So. But they do require the support of the support of axis, basically the entire domain. Do you encounter the same difficulty?
00:39:46.934 - 00:40:04.018, Speaker B: Yeah, I mean, so here in our case actually we need some kind of gross assumptions of the. I mean so if I go back to the main theorem. So I think you are kind of related for the.
00:40:04.066 - 00:40:05.786, Speaker D: You have one slide that talks so.
00:40:05.810 - 00:40:24.294, Speaker B: That you need actually you two roles and then the hashing of you is not too crazy. So that, I mean, so in particular I don't think it would work if it's a compact support. Right. Yeah, it's a similar issue. Yeah. So, I mean, by the way, this assumption actually also appears in Vlan's work of hypercoassemity.
00:40:25.654 - 00:40:34.634, Speaker D: Yeah. The second question I have is the PI minus identity. That operator looks very much like the BGK operator.
00:40:35.054 - 00:40:37.462, Speaker B: Yeah. That's why it's called kinetic longevity equation.
00:40:37.598 - 00:40:46.674, Speaker D: Okay. So there are other collision operators that drives velocity domain back to gaussian. Have you thought about those?
00:40:48.454 - 00:41:36.414, Speaker B: I mean, we haven't tried, but I think, I mean, to some extent, I mean, from our analysis point of view. Let me go back to here. So, from our NASA point of view, if you use this, or if you use the. I mean, if you replace this by what's happening in the Forker Planck equation, the fluctuate dissipation operator, actually, they don't really affect the rate. So I think as long as the collision operator has a uniform gap and then the gamma is. I mean, you multiply it in front with a gamma parameter, I think the behavior would be all the same, except with some absolute constant difference, because we are doing two. I mean, in language of kinetic equation, we are already actually showing the commercial rate for two kind of collision operator.
00:41:36.414 - 00:41:40.474, Speaker B: One is the focal plan collision and another is the BGK collision.
00:41:41.134 - 00:41:42.794, Speaker D: Have you thought about Boltzmann?
00:41:43.854 - 00:41:47.666, Speaker B: Well, that makes things nonlinear, but it's a.
00:41:47.690 - 00:41:51.054, Speaker D: It's a. It's a particle system. So you have the interaction between particles.
00:41:51.554 - 00:41:58.058, Speaker B: Yeah, we haven't thought about it. I mean, so for linearized Boltzmann, I believe one can do something, but in nonlinear one, I don't know.
00:41:58.226 - 00:41:59.054, Speaker D: Okay.
00:42:05.434 - 00:42:06.334, Speaker B: Okay.
00:42:07.594 - 00:42:09.374, Speaker D: Okay. Thank you.
00:42:10.234 - 00:42:12.094, Speaker A: So, we had a question over there.
00:42:18.254 - 00:42:39.754, Speaker E: Hi, Jeff. This is mole. Very nice talk and work as always. Quick question. So you had both very nice results at the continuous level and at the discrete level, and you talked about the proof for the continuous processes. And can you say a few words about how do you bridge to the discrete results?
00:42:41.154 - 00:43:08.614, Speaker B: Yeah, that's a good question. So, I mean, the difficulty is that we're working in chi Square, right? So there are recent results that can analyze the chi square convergence for over time, language discretized. And I mean, so far we are not able to. I mean, so just the difficulties to control the chi square error after. Due to discretization. And I mean, so far, we are not able to get very nice rates for that. These.
00:43:10.554 - 00:43:11.934, Speaker E: Okay, thank you.
00:43:13.354 - 00:43:19.494, Speaker A: All right, so I'm going to stop here and thank Jeff Fink for a great talk that generated a lot of questions.
00:43:21.434 - 00:43:21.874, Speaker B: Thank you.
