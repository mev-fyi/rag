00:00:01.240 - 00:00:50.784, Speaker A: Hello, and thanks for coming to my talk on computing efficiently on encoded data. I'm really excited to talk about this. This is some joint work I did with Mary Looters, who's right there? Okay, so let's start. So I'll first explain the problem setting in distributed storage, and then I'll describe our result there, which pertains to Reed Solomon codes, our main result. And then I'll step back and just explain why we worked on this, why we defined this new problem. What are some similar ideas in the literature, and what was our motivation for working on this? And then I will explain what a linear recovery scheme is and how we used those to get our results. And then I'll conclude with some future work directions.
00:00:50.784 - 00:01:38.974, Speaker A: So let's say that you have b bits here and you want to store them on some computer nodes. And one way you might do that is to just split up your b bits into chunks. So you have a chunk of the bits on each node, and when you want to get your bits back, you can just download b bits. And that's called the bandwidth of recovering all your bits. And this naive approach, very simple, and, but we really care about minimizing bandwidth in this talk. And so if you really just wanted to compute some function f of your bits and you didn't care about recovering all your bits, maybe, you know, sometimes you'd be able to get by with fewer than b bits downloaded. And that's what we're interested in doing.
00:01:38.974 - 00:02:57.334, Speaker A: And, you know, so, but for the functions that we consider in this talk, if you use this naive setup, you can't do better than b bits. Okay, so there's a very serious problem, though, with this setup, which is why it's not going to be used. So if you lose one of your nodes out of your many nodes, if one goes down and your data is lost, then you can't recover that chunk, you can't get it back because there's no error correction, and you can no longer compute functions like f if they depend on that chunk of the bits. So we know how to solve this problem using an error correcting code. And so if you use one of these codes, you encode your message, the bits, and they take up a bit more space, and so you put them on more nodes, perhaps, but you can still just download b bits and you can recover your input bits as long as you've chosen a reasonable code and certainly the codes in this talk. So once you download the b bits, you can compute this function f. And that's like kind of the naive approach again to computing this function f.
00:02:57.334 - 00:03:47.066, Speaker A: And then now that you have this code, if you lose one of your nodes, it's okay, as long as not too many nodes are lost, you can do error correction and you can recover all of your data, which is really good. So we looked at this problem. When you're using a read Solomon code. We said, okay, you're going to view your message bits as comprising k symbols in some finite field of order, big q. And so if it's big q symbols in the field, then each symbol can be encoded as log two of q bits. So in total, that's k log two of q bits. It doesn't have to be two bits per symbol.
00:03:47.066 - 00:04:33.444, Speaker A: That's just what I've shown here. But in general, you'll have some finite field and you'll have k symbols of that field, and b is equal to k log two of q. And you just encode those in this code, called a Reed Solomon code. Okay, great. So this code is nice for a lot of different reasons, and it's used in some systems, like it's built into ceph and hdfs, which are distributed file storage systems. And just like it's a reasonable code, you can download b bits and you can recover your message, and you can compute a function on your bits that way. But we were interested in the case where the function you want to compute happens to be a linear function over this field fq.
00:04:33.444 - 00:05:27.464, Speaker A: And this is just a natural class of functions you might want to compute. So it looks like big f of your message symbols f one through fk is equal to the sum over I of lambda I times f I, where the lambda I are arbitrary elements of this field fq. So in other words, you want to take the dot product of this lambda vector with your message vector f one through fk. And what we showed is that for this class of linear functions, if you pick any one of these functions, you can get by downloading just little o of b bits to compute that function, so little o b bits across all the nodes in total. And that's going to allow you to compute fast. And this is really surprising. It works for any coefficients lambda I.
00:05:27.464 - 00:06:54.396, Speaker A: And of course, the bits that you download may depend on the lambda I, but for all lambda I, you can, you can make the correct queries, and then you can, you can recover this, this linear function. Okay, so this was extremely surprising to me, because I, when I first started working on this project, I just kind of naively assumed that an error correcting code was kind of like a big, bulky suit of armor. And so it offers protection, but it is going to slow you down and, you know, meaning it's going to be more difficult to compute on your data because now it's encoded and you have to think about that and worry about decoding it or something. But it turns out, like, our result shows that at least for these, for the purpose of linear computations, the Reed Solomon codes are more like the Ironman armor. And that's because they don't just protect your data, but they give you this new capability of computing this, you know, any arbitrary linear function with b divided by log b, that's the little o b from before bits downloaded. And so this is only possible because when you encode in the Reed Solomon code, you're doing some non trivial computation on all of your message symbols to produce the codeword symbols, and then you're storing those. And so you have this kind of like, library of these existing precomputations that you've done.
00:06:54.396 - 00:07:44.644, Speaker A: And so by making intelligent queries to the nodes, you can do better than the naive approach, which would require b bits to compute the linear function. And, okay, why do we work on this? So this idea that you have encoded data and you want to compute on it and you don't want it to cost a lot of bandwidth, has shown up before in several different places. So the first place is regenerating codes. And regenerating codes are. So in that context, imagine, like, for example, a single node failed, like I showed before, and all you want to do is replace it. So you need to recover, by making queries to all the other nodes, you need to recover that codes. Sorry, that nodes, code word, symbol.
00:07:44.644 - 00:09:02.710, Speaker A: And that's just a special case of computing a linear function, because for a linear code, the codeword symbols are themselves linear functions of the message symbols. So our work in that sense, it directly generalizes regenerating codes, and it's also related or similar to ideas, encoded computation and gradient coding and homomorphic secret sharing. And so there's a lot of existing work in all of these directions. And, you know, variants of this problem have shown up. And what we do in this work is we define this new problem of just computing unencoded data, kind of like regenerating codes, but generalizing that to arbitrary linear functions. And, you know, that we make the first, we take the first step in computing these arbitrary linear functions. So what our specific result is, I'm just going to state it here for some fixed rate r, and then let's, based on the rate, we'll fix a sufficiently large prime power q, and we'll consider the class of Reed Solomon codes of dimension k, which is r times q to the t, and full length, meaning we use all the evaluation points in our field fq to the t.
00:09:02.710 - 00:09:48.224, Speaker A: So n is q to the t. And okay, really important now we have little q. So big q from before is just going to be little q to the t. And I'm going to keep little q red to remind you that there are two different fields now. So for any set of coefficients, lambda one through lambda k, then we can compute lambda transpose f, where f is your message vector, by making queries to these nodes. And the total bandwidth cost is just proportional to in bits. And if you think of q as a constant, then this is like b over log b bits if the total message size was battery bits.
00:09:48.224 - 00:10:26.584, Speaker A: So this is our result. And it of course is a bit more general than this. It can handle these failed nodes so they don't have to participate. They could just sit out for one of the rounds and they don't have to help us. And we can also handle equivalently fewer than q to the t evaluation points, because if, you know, some nodes have failed, well, maybe instead they just never existed in the first place. And it's the same thing as far as the scheme is concerned. I'll just note though that our bandwidth cost is the lowest.
00:10:26.584 - 00:11:30.880, Speaker A: When you use all of the evaluation points in the field and you are able to contact all of those nodes and you can vary, in our full theorem, you can vary this base field size q, you can vary the rate, you can vary the fraction of the nodes that participate. Of course, there's a maximum limit of one minus r times in failures before you'd actually start losing data. And one, perhaps the limitation is that you do need to work over an extension field like f q to the t. And so we do mention that a hack of sorts for working over prime fields. So if you really had messages in a prime field, you want to, you know, you want to compute linear functions of them. If you add several of these messages, then you could use our scheme to perform batched linear computations on them in parallel with low bandwidth. But the way that you do this is you view those stacks of messages as like representing messages in this extension field Fq to the t.
00:11:30.880 - 00:12:21.272, Speaker A: So there's still this notion of this extension field showing up. So just to review, we have f one through fk base in our field, f to the k, and now we encode it. So how do we encode that in a read Solomon code. Well, you just consider your message symbols f one through f k as the coefficients of some polynomial of degree at most k minus one. And then you fix an evaluation point so just some alpha I for each of the nodes, and each of those points comes from the field Fq. And then you evaluate that polynomial at the evaluation point corresponding to each node and give that, you know, evaluation to that node to store. And so this is a Reed Solomon code.
00:12:21.272 - 00:12:58.404, Speaker A: And now we, let's say later, decide on some function sum over I of lambda I times f I. And we really need to compute this function. So we just have to broadcast that function to all the nodes, and then we can download little of b bits total and use those bits to recover that function. Great. So the schematic for how this will work is we're going to just focus on linear codes over some extension field. And you can imagine like little q is fixed and t is growing. And that's how we grow the field size in the code.
00:12:58.404 - 00:13:54.490, Speaker A: And then we'll treat the code symbols, these evaluations f of alpha j as actually being vectors in the vector space f little q to the t. So the t dimensional vector space over the finite field of order little q. And then we'll make f f little q linear queries to the vectors. In other words, we'll just multiply those vectors by some suitable matrices, and then the resulting vectors will be what gets sent back to us. And then we'll reconstruct, you know, from those queries, we'll reconstruct with the linear combination of those queries, the desired linear function. Okay, so how do we view a field element like this, alpha in f little q to the t as a vector of t dimensions over f little q. Well, this may or may not be surprising, but you can do this.
00:13:54.490 - 00:14:31.582, Speaker A: There's an encoding to go back and forth, and this works really well, f little q to the t, the extension field. It also has a vector space structure over f little q. So as soon as you fix a basis, then, then you can do this mapping. And addition works fine. So, you know, alpha plus beta in the big field is just rep, you know, if you do the, the encoding to some use and v's, and then you just add up those vectors, you get the appropriate vector for the appropriate field element that represents alpha plus beta. Right? So addition works fine. And we can also make multiplication work really nice.
00:14:31.582 - 00:15:13.764, Speaker A: And the way we do that is we can encode. So let's say that we want to compute like lambda times beta, where lambda and beta are in the big field f, little q to the t. And. Okay, so we can do that by finding some big capital lambda that's a t by t matrix with entries in f little q, such that when we, when we multiply that big matrix by the vector that encodes beta, which here's v, then what we get out will be the vector that represents lambda times beta. Okay, so we can make this, we can make this nice mapping, and we can map field elements to vectors to add them. And it all works fine. We can make them into matrices.
00:15:13.764 - 00:16:03.814, Speaker A: Everything has entries over the little field. And I'll use this, this depiction to show what's going on in our construction. So for our construction, just to review, the user comes along and has some linear function they want to compute. And the, let's say this is some node I, and they have the code word symbol f of alpha I. Well, they're just going to choose to view that, you know, assuming that the symbol is in some big extension field f, little q, to the t, they can view that as this t dimensional vector with entries in fq here, that's u. And then they're going to take that vector u and multiply it by some matrix a, and they get some new vector which they send back to the user. And let's say that's an fq to the s.
00:16:03.814 - 00:16:52.628, Speaker A: So s is the number of the base field symbols, the order q field symbols that we get back from this node. So they get back au. And so a has to be s by t in order for the vector that gets sent back to be s symbols. And in our construction, we find a way to keep s proportional to a constant. Right, as long as you don't change the rate or the base field size or the, of course, number of nodes. Sorry, the number of evaluation points in your code as a fraction of the field size fq to the t. Okay, so how do we, what do we do with this? Right, the user got back this a times u.
00:16:52.628 - 00:17:27.042, Speaker A: Well, they're just going to sum up over all the nodes. J rj times ajuj. Right. So they receive a juj is some s dimensional vector and they multiply it by some tall t by s matrix r. I haven't told you yet how you get the r matrix or the a matrix, but if you, if you have the appropriate matrix, then matrices, then this would define some linear recovery scheme for this, this code. So this is what a linear recovery scheme is. It's linear over this base field f, little q.
00:17:27.042 - 00:18:21.234, Speaker A: And this is what it looks like, and this is exactly what we'll, we'll do. So when we, when we zoom out across all the nodes, we get back a one times u one from the first node, and so on, we get all these s dimensional vectors. And so that's just, if q is constant, it's proportional to s bits across all the nodes. And so in total, if we can keep that s small, then like I mentioned, we get little o of b bits, we get o of n bits overall. And then how do we reconstruct, or how do we get the appropriate rj and aj? That's the remaining thing to be explained. So let's just write down f the message symbols as f one through fk. And then of course, the code word is these evaluations f of alpha one through f of alpha n.
00:18:21.234 - 00:19:02.782, Speaker A: We'll call that C. And remember, lambda is the vector of coefficients and we want to compute lambda transpose f. So the first observation is that you can equivalently write this as gamma transpose c, or c is the code word for some appropriately chosen gamma in dimensional vector of some other coefficients. And this is true because this is a linear code. And so encoding and decoding are linear operations. And so you can just do those operations in between lambda and f. And then, you know, you get instead gamma and circle.
00:19:02.782 - 00:19:56.654, Speaker A: So the gamma is not necessarily unique, but there's some gamma. And so pick any gamma and we'll work with it. So what we're going to do next, now that we've transformed it from thinking about lambda transpose f into gamma transpose C, is we'll just expand the gamma symbols into matrices and expand the C code word symbols into these t dimensional vectors. And that way we can write it in this big block notation. And so we have gamma one through gamma n comprising this big vector gamma. But now capital gamma is like these matrices that are t by t and great. So recalling that c, which is now represented as this u one through u in stacked vector, that's a code word of this linear code.
00:19:56.654 - 00:21:32.024, Speaker A: So there's an idea which is to add a row corresponding to a dual code word that when we multiply that row vector by u, will just give us zero, because that row vector came from c perp, the dual code. And this, this holds, even though we've expanded it into these big t dimensional chunks, for each symbol, it still holds that the inner product will be zero. So all we've done when we add this row to, let's say, the last row of the big gamma block matrix is we've just added zero to the right side, so it's still true. And so we do it again to a different row of the gamma matrix, and we just do it with some, you know, v, w and z for all the rows. And now when we look at the jth block, gamma j subblock plus the wj, zj, vj symbols, that, then that resulting matrix, we want it to be low rank, so we want it to have rank s. And once it has rank s, then we can factorize that jth block into rj times aj, where rj is t by s a, j is s by t. And once you do that, then now you see this equation from before that reconstructs lambda transpose f as equal to gamma transpose c, which is equal to the sum over j of rj aj, which is just that, jth block times uj.
00:21:32.024 - 00:22:23.494, Speaker A: And this is exactly where we get the linear recovery schemes. So I haven't shown you how, how to get these dual code words, those colorful rows, right? And our construction only is for read solomon code. And the result that we get is that s is indeed a constant for fixed rate, et cetera. And the total bandwidth is then order in bits, which is b over log b. And if you use the naive approach, which would be to download all of your data, recover all your data, and then compute the function that would be o of b bits for b bits of your data. So this is an improvement. And it turns out that at least the dependence is optimal for the dependence on b if you have a constant rate code.
00:22:23.494 - 00:23:03.394, Speaker A: And because we have a lower bound for any mds code of b over log b as well. But this lower bound assumes a linear repair scheme that works in the way that I've described. So there is a gap for the dependence on rate. So if you vary rate, if you vary also the number of failed or non participating nodes, which. Okay, so here's that gap. So on the left here we have this lower bound of this proportional to in or b over log b bits. And then in the middle we have our, the bandwidth for our scheme, and you can see it's also proportional to n.
00:23:03.394 - 00:23:47.254, Speaker A: And then the trivial scheme would actually be in login bits, order in login bits. Right. So there is this gap for future work to just find the correct dependence on the rate and q. And also, so I haven't mentioned before what gamma is. Little gamma is just the fraction of failed nodes here. So it's, you know, direction for future work to find the correct dependence on those quantities. Another natural direction for future work is to generalize to other code parameter regimes, for example, when t is very large or right, if you want to use fewer evaluation points and not use this entire extension field.
00:23:47.254 - 00:24:27.634, Speaker A: And also, it would be cool to generalize beyond linear functions, maybe to low degree polynomials. We have an idea in our paper for how to compute some, some nonlinear functions using our methods, but that's just some of the nonlinear functions, and there are a lot more. So that's another direction. And another would be so. So Khan and Tomo have this pioneering recent work exploring. So they create regenerating codes over prime fields instead of extension fields. And they do that with a nonlinear repair scheme.
00:24:27.634 - 00:25:14.354, Speaker A: And it would be really cool if you can use a nonlinear repair scheme for computing on encoded data instead of just repairing a codeword symbol and maybe even a nonlinear scheme to compute a nonlinear function. And the advantage of this is it might help you work, work over prime fields. Or one advantage of that is that you might work over prime fields and then another natural direction. Of course, we just talked about reed Solomon codes mostly, but there are lots of other codes. And it would be really neat if you could compute linear functions on data encoded in other codes as well. So these are some directions for future work. We just kind of define this problem of encoding on computing on encoded data.
00:25:14.354 - 00:25:34.144, Speaker A: And there are a lot of similar ideas out there, and hopefully, this is just the beginning of lots more work to come. So here are my references. Thank you very much for watching my talk, and I hope to see you at ITCs. Thanks.
