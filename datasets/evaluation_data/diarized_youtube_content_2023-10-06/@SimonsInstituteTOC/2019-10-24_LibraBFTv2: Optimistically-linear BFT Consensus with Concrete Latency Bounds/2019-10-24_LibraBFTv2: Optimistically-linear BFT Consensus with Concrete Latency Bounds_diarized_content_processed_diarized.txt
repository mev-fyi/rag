00:00:03.080 - 00:00:26.634, Speaker A: Cool. So thanks for having me here. I'm very happy to be able to talk about the Libra BFT protocol, which is the protocol that we have developed for the Libra blockchain. This is a joint work with many people. Obviously, some of them from the research group are in this room. Daria Malki Jordan. Is this Alberto Sonino.
00:00:26.634 - 00:01:15.646, Speaker A: And before I start, I would say I can make a test. Maybe if you were to choose a BFT consensus protocol for a major blockchain, and let's say you're a CTO with a background in science, but not a specialist, what is the one thing you will ask about this protocol? How much are you going to pay the person that. Oh, that's interesting. I should remember that. All right. Okay, so I think maybe one thing you can ask is, like, you'd like to have a proof of safety of the protocol that you actually understand or you think you understand. Okay, so, and so we were lucky to meet the authors of the hot stock protocol early on in our project.
00:01:15.646 - 00:01:48.614, Speaker A: And I think it's fair to say that's the one thing that we really like about this protocol, that we, we had the sense that we kind of understand the proof of safety. It was kind of concise, on top of using a very strong model, eventually synchronous. And then we also felt like, ok, the protocol looks really efficient. We got this nice linearity, responsiveness, which I will talk about later in pipelining. And so we're ok. Well, this is it. We just need to implement it.
00:01:48.614 - 00:02:22.096, Speaker A: In theory, everything is done. But maybe the problem is there's always a bit of difference between theory and practice, although in theory it is known. And so I did. I shut down everything. All right. And so we started working on. And so here, the catch is that the pacemaker, I mean, the nice thing about hot stuff is you can really prove safety and to some extent prove liveness, abstracting away a big chunk of the leader election.
00:02:22.096 - 00:02:53.970, Speaker A: And so that's the part that was missing. And so this is the reason why I'm here today is because in Libra BFT, we made precise what we need to achieve liveness. And in fact, we wanted to be really built in the protocol, because, I mean, like, if you go to an engineering team, you say, hey, thanks for building the protocol. Now you need to build another protocol for the election. They tend to not like it so much. They think, like, when they combine everything, it's going to be hard to debug everything. So we really want to combine everything, which is also kind of standard for many blockchain protocols.
00:02:53.970 - 00:03:45.040, Speaker A: And so in this talk. So I will start by, I'm sure many of you know the RSA protocol, but I'm going to revisit nonetheless using Libref BFT notations. And so you might notice some differences here and there. And then I'm going to go into the main topic of the talk, which is the round synchronization that we developed for Libra BFT. And in fact, I will present the version one and version two, and version two being the most interesting, I think. And if I have time at the end, I will try to conclude with a number of general comments on the model we're using for everybody is using for state machine replication. Okay, so I'm going to start with the core hotstar protocol within Libra BFT.
00:03:45.040 - 00:04:27.454, Speaker A: So how does it work? So as you know, these are leader based protocol. So let's call, so let's say we have a round number and the leader is deduced from the round number. Okay, what we expect from a leader is to, and I'm going to actually use the nice, the diagram. So what you expect from a leader is to gather whatever user transactions you want to execute in the blockchain. Put that into something we call a command, put the command into a block, sign it, broadcast the block, and to everybody in the validator set. And what you hope is that everybody is going to reply. They are going to carry on a number of verifications.
00:04:27.454 - 00:04:47.054, Speaker A: If they like your blog, they're going to reply with a vote. And once you have a quorum of votes, you can aggregate them and create what we call a quorum certificate. And then in Libra BFT, the leader would simply rebroadcast the quorum certificate. And that concludes the round of this leader.
00:04:47.554 - 00:04:48.414, Speaker B: Okay.
00:04:50.394 - 00:05:28.348, Speaker A: So what's interesting with hot stuff, it's also there's an extra data structure in particular. So we expect a vote to contain the hash of a block. We expect a quorum certificates to contain, to be linked to the block that is actually certified. But in Libra BFT, you also expect the blocks to contain a hash with the quorum certificate of a previous block. So this gives a structure of chain that is interesting. And you can see that, for instance, if we agree on the block b two, well, we have to agree on everything that is before. So, and this was well explained in the presentation.
00:05:28.348 - 00:06:10.738, Speaker A: Also earlier, there's the idea of pipelining that this certification here certifies not only b two, but the certification of b one. So you see where I'm going here. And obviously we want the round to be increasing along the chain. That's part of the condition for validity that is checked when you're about to vote for. So you see, I'm going here we now we can in fact very easily state a commit rule, which is the following. If I know a chain that has, that ends with three QC's and the round numbers happens to be contiguous. So let's say one, two, three in this example, then I can look at the first block of my commit rule and say, okay, well, this thing is committed.
00:06:10.738 - 00:06:58.574, Speaker A: But as I mentioned before, what's nice, since we are chaining everything, not only I commit this block, but I commit any other block previously not committed that would appear earlier in the chain and in the right order, as you imagine. Okay, so sorry. Okay, so now why do we care about continuous rounds? So this is going to appear in the proof of safety. But I guess another question is like, why is it the case that sometimes you'd have non continuous rounds? And in fact, this happens when a leader doesn't do his job. And that's kind of the only thing we need to know about view changes in ourselves. There is no view change. This is just a gap in a round number.
00:06:58.574 - 00:07:51.982, Speaker A: So I just explained the commit rule. Now we'd like to know whether this actually works, whether this enforces safety in terms of the absence of fork in the protocol. And so, safety comes at a price. The nodes have to, in addition to the well formed nest of the well formed nest of the blocks, the nodes have to do a few verifications that are called safety rules. So, and that involves tracking two integers, two round numbers. So nodes must remember for any block they voted for, they must remember the round, and they must remember the round of the grandparent of that block. Okay, and so you need to remember the maximum number for those two values.
00:07:51.982 - 00:08:43.744, Speaker A: And every time you're going to vote for a block, you check that the round of the block is greater so that the round of any previously voted block, and you need to verify the round of the parent of the block is greater or equal than the run of any ground parent off voted block in the past. And that's not a tipo, this is really parent and ground parent here, although it may seem mysterious. And so the nice thing with those two rules is that's kind of all you need to know to prove the safety of hot stuff. So I actually made the exercise to gather everything you need in one slide. Probably. I don't have time to go over the proof. This is the main lemma that says if I have a block with a QC at a higher or equal round than the block I just committed, then I know there's a path, there's a chain that goes from the commit to my block.
00:08:43.744 - 00:08:54.010, Speaker A: Okay, and so why does it prove safety? Well, this thing. So, first, this diagram leaves in the union of everything the nodes know.
00:08:54.082 - 00:08:54.450, Speaker B: Okay?
00:08:54.482 - 00:09:42.338, Speaker A: So this could be a commit in one node. This could be a block that is about to be committed somewhere else. And what I'm saying is, oh, wait, there's a chain between this one and this one. And so I just showed that there's a linear sequence between all the commits across the network. Okay, so, and once again, the nice thing here is really that this slide is possible. You can have one slide for the safety proof of hot stuff and era BFT. Now, if we look just a little bit closer, what's happening in terms of storage and communication? So we have those chains of blocks, and it might seem like, oh, we find ourselves having to synchronize a tree of blocks and QC's over the network.
00:09:42.338 - 00:10:14.284, Speaker A: But actually, in practice, you can show, you need only to remember the highest commit rule you ever saw and the chain that precedes the highest QC you ever saw. So that's only two chain, and they are mostly equal. Also, when everything is good, then you have, in fact only one chain, and you can show that the communication, at least the number of messages, is linear in the happy case. And if you're using. So I search in nature, that's strictly linear in terms of network complexity.
00:10:15.104 - 00:10:15.960, Speaker B: Okay.
00:10:16.112 - 00:11:05.222, Speaker A: All right, so that's for the safety core hard stuff. Are there any questions? So far? Everything is standard, maybe. Okay, how about liveness? So, for QC, so we want to create a commit, right? So for QC to be created, so all the honest nodes needs to be to agree on the round number. Otherwise, they will be surprised to see this block being created by somebody they don't expect. The leader must be honest to propose, and all the nodes must vote for the proposed block. Right? So that's this stage we're talking about, the voting. So why is it a concern that all the nodes need to vote? Well, we, as I just explained, for safety, we just added these two constraints.
00:11:05.222 - 00:11:40.340, Speaker A: Okay, so the question is, like, as a leader, how do I propose my block to make sure that people can actually vote for it? Meaning that these two constraints are going to pass for every validate other, every other validator. And I think it's fair to say that one of the main contribution of hot stuff and distinctive point is that to propose a block, you don't need to wait a fixed delay. You just need to count a number of samples, n minus f rounds of qc given to you by people that just entered the round.
00:11:40.492 - 00:11:40.916, Speaker B: Okay?
00:11:40.940 - 00:11:57.528, Speaker A: So once again, you, and let's say in tandem, in, for instance, you have, you would only wait for a fixed delay and then you propose, okay? So in hostf, you wait for n f nodes to join you around and tell you what is their highest QC round.
00:11:57.676 - 00:11:58.600, Speaker B: Okay?
00:11:58.792 - 00:12:40.178, Speaker A: When that happens, you take the highest number, and that's where you need to extend the chain. And if you do that, you are guaranteed that people are going to be able to vote for your proposal, which means that that solves the problem we had for the likeness. So what's left here is the two other points. The honest node needs to agree on the round number and the leader must be honest. The second point means, in particular, if the leader is dishonest, we need to be able to skip that round in a timely manner. Hopefully this whole problem is called round synchronization, or so leader election. Okay, so in a very simplified way.
00:12:40.178 - 00:12:46.994, Speaker A: So we'd like to know what happens between the red dotted line that is actually making notes, switch to the next round.
00:12:47.034 - 00:12:47.734, Speaker B: Okay.
00:12:53.934 - 00:13:44.592, Speaker A: Once again, if we take the point of view of a practitioner. So we starting from a network that is very nice, it seems like you really have the minimal amount of messages in there that is able to guarantee safety. And now we're going to add stuff to it. So we would like to add the minimum amount of overhead. Also, if we add something, we'd like to know that actually it's efficient. So we'd like to know that even if you have byzantine nodes in the system and the network is scheduling the messages in the wrong order, subject to the delta delay after GST, we'd like to know that still the commit latency is fine. So like I said in the hot stuff, round synchronization is delegated to an external protocol.
00:13:44.592 - 00:14:37.194, Speaker A: Actually, we recently we were studying what this external protocol could look like in a separate line of work. In tendermint, the protocol includes round synchronization and is using timeouts and probabilistic gossip. So the natural thing to do that we did in Libra BFTV one was to try to make something similar work. And in Libra BFT two, we thought, ok, it works. But we don't really like probabilistic gossip because it's a nonlinear amount of messages, including in the best cases also, you know, like introduce probability, probably probabilistic behavior in the system. So it's a little more difficult to debug what's going on when you look at the exchanges between the nodes. So, and this is the result of.
00:14:37.194 - 00:15:33.138, Speaker A: And so Libra BFT is the result of this attempt. Okay, so now I'm going to introduce, okay, how the run synchronization works in Libra BFT. So the first idea, and it's still pretty reasonable, I would say we're going to add the maximum duration to every round number. After the maximum duration is reached, every node creates a timeout object and broadcast it. Okay, now when do you move to the next round? You move to the next round when you have seen a quorum of votes, a QC as before, or when you have seen a quorum of timeouts, which we call a timeout certificate. Okay, so here we see that, like leader, the last node was supposed to be the leader. The leader doesn't do anything.
00:15:33.138 - 00:15:53.122, Speaker A: Everybody is starting broadcasting timeouts. And after, and so after receiving their own timeout plus two more, they switch to the next round. And this node apparently was the leader. And ideally, we'd like this node to immediately broadcast a new proposal.
00:15:53.298 - 00:15:54.134, Speaker B: Okay.
00:15:56.794 - 00:16:43.544, Speaker A: So that would be a solution with almost no overhead. A second idea while we at it, since we are already communicating, we're gonna, when we do that. And that's also related to the notion of chain. Since we are about, we want to communicate entire chains with people every time there's an arrow in this diagram. What it means is I'm telling you a summary of everything I have, and then you ask me what you're missing. Okay, so that's the second idea that could help for liveness, and it's still pretty natural. And so the question now is, all right, are we good to go? Is this protocol bft live? The answer is no, not quite.
00:16:43.544 - 00:17:34.329, Speaker A: First, there is something I told you that I haven't really implemented yet. I told you, hey, it's awesome. We are responsive because we can propose after receiving n f message from the people that just joined our round. Okay, but I haven't implemented that. So in fact, what could happen is that everybody that joins my round, if I'm leader number three, everybody needs to talk to me to say, hey, I just joined, and this is my highest QC, by the way. But then we see that we introduced some latency here because now, as the leader of round three, not only so after I join my round, I need to count the number of Red arrows coming in. And so that introduces a little bit of delay.
00:17:34.329 - 00:18:40.844, Speaker A: So a question would be like, okay, can we avoid that? And then a second problem is the what if leader is dishonest? Doesn't really broadcast the QCD correctly. So what could happen is that you have a leader that only broadcasts the QC to a subset of f plus one nodes, gather f one votes, then use f malicious keys to create a QC. And then you take this QC, and you show it only to the people you were talking to in first place, which means you have f notes that haven't heard about your new QC, so they're stuck at the previous round. And then if the next leader is dissonance, you can repeat. So that's not necessarily a problem, because, of course, eventually you're going to have a harness leader. But it's kind of bad for if you are really into analyzing the latency of the system very precisely and also a good question to ask. So, in Libra BFTV one, by the way, this is solved entirely by using probabilistic gossip.
00:18:40.844 - 00:19:19.824, Speaker A: So there's no way for somebody to censor, I mean, to selectively disclose a QC, because everybody's resharing everything. So, in ebab FTV two, we'd like to remove the gossip. Okay, so we gonna. We are going to have some kind of problem like this for some time. So the question we can ask ourselves is, okay, as long as you have these honest leaders, they can forget about a part of the network. But the next leader that is honest, can this leader do a good job nonetheless? Okay, that's if you are really into having a system that goes back very quickly and tracked after a problem. That's a nice property to have.
00:19:19.824 - 00:20:15.194, Speaker A: So, the solution that we developed that I call fast proposal, is a tweak, a very simple tweak on the definition of the protocol of the Vashon one protocol. And so, I'll tell you what we obtain before telling you what is the modification. So, what we obtain is that I can propose a block as soon as I receive one of those green arrows. So if anybody in the system joins my round, they are going to send me a notification. First, they are going to convince me to join this round, if I haven't already. And second, they are going to convince me to give me enough material so that I can propose a block right away. What is interesting with this as a byproduct, is that when I propose a block, I'm also synchronizing the entire chain, that is, before that block, which means that I'm also completing the broadcast, if needed, and bringing everybody to my round.
00:20:15.194 - 00:20:28.646, Speaker A: So the property is that if somebody joins around after GST, if somebody joins the route at time t, by time t plus two delta, everybody else is in that round, is at this round.
00:20:28.710 - 00:20:30.686, Speaker B: Okay? Okay.
00:20:30.710 - 00:20:47.538, Speaker A: And the way you achieve that very quickly is you inject the highest QC round inside the timeout object, and you change the safety rule to prevent somebody from voting if they ever created a timeout for this round.
00:20:47.706 - 00:20:48.458, Speaker B: Okay?
00:20:48.586 - 00:21:01.954, Speaker A: So when you do that, you can show that, in fact, the time, the TC thing counts as n minus f samples of highest QC. So that's exactly what you need to propose. And that was also already the case for QC.
00:21:02.894 - 00:21:03.694, Speaker B: All right?
00:21:03.854 - 00:21:39.634, Speaker A: Okay. Are we done yet? Well, not quite, because the same problem with incomplete broadcast that we just discussed with the qcs, we can have it with tcs. So what could happen is that you have a leader at round k. Let's say it's slow and the next one doesn't want to help, is this. So the attacker waits for half plus one on s node to timeout at the round k. Okay? And then takes the f one and completes that into a quorum again. And that gives the attacker a tc, and they can do whatever they want with the TC.
00:21:39.634 - 00:22:05.808, Speaker A: And so what they're going to do is to give it to people who haven't timed out yet at this round. And since that's going, this is going to bring, in this case, the node number two to the next round. And so it means that in the system, except for the TC that the attacker just created, there's not enough timeouts for the harness nodes to move to the next round yet.
00:22:05.976 - 00:22:06.632, Speaker B: Okay?
00:22:06.728 - 00:22:32.744, Speaker A: So at this point, you have one node that is at one. At round three, everybody else, this node was malicious. Let's say it's not. It's not not responding. So everybody is going to wait at round two, they have already timed out. So for their point of view, their job is done. They are waiting for a TC to happen, and the TC is not happening because this node is already at the next round.
00:22:33.044 - 00:22:33.548, Speaker B: Okay?
00:22:33.596 - 00:23:30.774, Speaker A: So what is going to happen is eventually, this node is going to timeout at the next round, and then it's going to propagate the byzantine TC. But the whole thing took, intuitively, twice as much time as before when we were using gossiping, the whole thing was taking only duration k. Now it takes duration k plus duration k plus one. And so you see that this node will only timeout and run at round three after three periods of timeout. And so the question is, can we do better? We see that removing gasping is degrading, in a way, the worst case latency of our protocol. Can we fix that? So the solution that I, that we, that we developed is the following. So, and we try not to ruin the linearity of the protocol in the happy case.
00:23:30.774 - 00:24:02.914, Speaker A: So what's going to happen is that a node, if after timeout, nothing happens yet, it's going to start pulling data from everybody at a lambda duration k interval. So, concretely, in our case, you see that we have a node that knows the next TC, and the other nodes don't know it. After a while, they have already timed out. After a while, they are going to query everybody, including the node that had the missing information, which is going to bring them to the next round much sooner than before.
00:24:03.354 - 00:24:03.930, Speaker B: Okay?
00:24:04.002 - 00:25:03.682, Speaker A: And so this means that the next round is going to fail also sooner than before. And so we can show that in this case, to summarize, a bad leader can only cause us a delay, one plus lambda duration k, which is, if lambda is small, it's a trade off. We can choose the lambda to be arbitrarily small. It is close enough to duration k, which was the ideal case that we had in the case of gossiping. All right, so how are we done with liveness yet? Almost. So, for a commit to happen, we need three qcs in a row to be created in the reports we discussed the consequence of incomplete broadcasts, or we deal with the consequence of incomplete broadcast for commit rules as well, that introduce one more mechanism that is similar to the one I just described, just based on commits instead of rounds, and that introduced this constant. I.
00:25:03.682 - 00:26:03.574, Speaker A: The reason I'm talking about this is because now I want just to show you a glimpse of the final latency bound for Libra beef tv two. So the theorem that you obtain after this perhaps painful, painfully detailed analysis is that you really know, under BFT assumptions after GST, how much time you need to wait for the next commitment. The way it works is you look at the highest round between honest nodes. You find out what is the lowest end such that three leaders in a row are consecutive and the timeout is large enough. And then this is the formula. You know that this is how much time you need to wait. And you are guaranteed that a commit will happen by this time, no matter what, no matter how the network is rescheduling, reordering the messages, as long as it's still synchronous delta, is the bound on message delays.
00:26:03.574 - 00:27:04.230, Speaker A: Okay, so to conclude on Libra BFTv two. So I introduced some liveness mechanisms that allowed us to remove probability gossip, which means that we are optimistically linear linear network complexity in the case of in the happy path, and I showed that the impact on BFT latency, the formula before is still limited. In the report you have more details about the way reconfigurations are handled and the way we can make leader function and duration function depend on the latest commit. If you want to receive the leader election, for instance with vrfs. I should mention that the whole of this there's a reference implementation, a minimal reference implementation that is now available on GitHub. This is the one that was quoted in the report from the beginning. And stay tuned so that this is not the end of the story.
00:27:04.230 - 00:27:39.326, Speaker A: There will be future versions of the protocol and if I have maybe one more or a couple of more minutes. So just a few generic remarks about security. So we worked in the eventually synchronous model. I think it's a strong model. So good. However, very often in the academia the results are only about eventual liveness. And so for a practical point of view, it doesn't really tell you when you are going to commit.
00:27:39.326 - 00:28:38.534, Speaker A: There are few, perhaps too few results about bound in delay, and so this is one more of them. Perhaps by analogy with the concrete security reduction in cryptography, this is going to become more mainstream. Other slightly I'm going some I kept all my controversial statement for the end of the talk, as you can see. So the, in the paper we intentionally deviated from the usual definition of eventual synchrony. So by allowing messages to be lost during periods of asynchrony, which I think is more realistic, because if you don't want this, you need to have unlimited buffer during periods of asynchrony to make sure you never lose the messages. There's also the question of whether you allow malicious harness nodes to to crash and restart. Do they count as malicious? If they do that during the period of asynchrony, I think it's fine.
00:28:38.534 - 00:29:55.408, Speaker A: Another interesting question that was also discussed this week is do we really believe in this BFT assumption, one third of malicious nodes? If we are in a permission model, and you talk to engineers when liveness is concerned, they think that actually one third of malicious node is way too conservative. So in fact that's kind of, that was one of the motivation to aggressively optimize the network complexity in the happy path for Libra BFT. If node membership is open, then how do you know that f is smaller than one third of the nodes? It's actually kind of reckless. So we try to and this is still an open topic of research, obviously, but so we try to be future proof by already discussing some ways to incentivize nodes in Libra BFT. And finally, this is my last slide. In the BFT model, we say that malicious nodes can deviate arbitrarily from the protocol, but usually it's still at a pretty high level of abstraction. If we are really serious about it, we would think that one of the nodes are trying to do horrible things, including on a network level.
00:29:55.408 - 00:30:33.984, Speaker A: For instance, they could try to inject packets that are going to go to make the honest node go out of memory. And that would be a problem in practice, right? Or can the DNS node force along the same lines? If you inject a lot of amount of data in the network, can you in fact make the messages of the honest node behave asynchronously? If you do that, then you have no period of synchrony anymore and the theorem in fact is not going to apply anymore. So that could be a practical problem that maybe we need more attention. And that concludes my talk. Thank you very much.
00:30:43.194 - 00:30:45.194, Speaker C: Any questions before the break?
00:30:45.314 - 00:31:53.624, Speaker A: Do you have some immediate ideas how you can squeeze in or squeeze out more performance, get synchronization of the GSD? Or do you think you're pretty much doing it already? Okay, so if you really, really care about latency, maybe you would be okay to pay logarithmic n login for communication for the broadcast. And maybe probably stick gossip was okay in first place. Yeah, we will see. I mean there are some, anybody is welcome to look at the proofs AnD see, oh, this is not TiGht here. So to be really, if you look at, you can have some Notion that the proofs are tight because some factors are actually what you need to wait. What you need to wait no matter WHat, if you think that the nodes might be honest. So like you need to give the nodes a chance to perform their thing and so you need to wait at least the amount of time out it takes.
00:31:53.624 - 00:31:56.984, Speaker A: So part of that is already tight.
00:32:01.024 - 00:32:06.488, Speaker C: You could also mention the paper we uploaded to the archive, the vm, right?
00:32:06.536 - 00:32:58.344, Speaker A: Oh, well, so, I mean there's a. Yes. So I took your question as the happy path, but now in the unhappy path, we are still quadratic in the number of messages, which might be unfortunate. If you think that quadratic number of messages is likely to saturate the network and bring asynchrony to you, then you could actually create a kind of asynchronous loop where you time out that breaks the network. So you time out more, and that breaks the network and. Yeah, and so the work with Odin, Naur, Dalia Spiegelman and me. So it attempts to make the run synchronize, strictly sub quadratic in all cases.
00:32:58.344 - 00:33:01.944, Speaker A: Perhaps using BDF also would help at some point.
00:33:07.244 - 00:33:11.164, Speaker C: Okay, so we're only ten minutes behind.
