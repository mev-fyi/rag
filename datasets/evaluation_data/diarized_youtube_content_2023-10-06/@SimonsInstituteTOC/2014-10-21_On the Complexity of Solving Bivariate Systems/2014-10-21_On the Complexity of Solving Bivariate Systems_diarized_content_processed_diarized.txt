00:00:00.160 - 00:00:01.192, Speaker A: Various systems.
00:00:01.310 - 00:00:32.274, Speaker B: Thank you. Sorry. So, I'm going to talk about a problem for which we already have polynomial time algorithms. It's not too hard to devise solutions in the class p. Solving systems of two equations into unknowns. What really matters to me here is to try to find something that's as close to optimal as possible, as we will see, because that was motivated by a couple of practical applications. I mention one of them in a second.
00:00:32.274 - 00:00:50.966, Speaker B: So, systems of two equations and two unknowns have shown up frequently recently, in large part motivated by problems of computing the topology of a curve, which means, given a plane curve here, finding a graph that looks pretty much like the.
00:00:50.990 - 00:00:52.714, Speaker C: Curve, and I'm going to leave it at that.
00:00:53.574 - 00:01:00.322, Speaker B: So, trying to discretize this kind of curve here by finding points of interest and then connecting them in a suitable way.
00:01:00.378 - 00:01:03.534, Speaker C: And I don't want to define more precisely what I mean here.
00:01:03.994 - 00:01:25.546, Speaker B: Concretely, what this boils down to, in part, is, first of all, solving some equations that will give you the points of interest. So there are these guys in green over there, and in that case, if my plane curve is given by f equals zero, f squared, three, these points would be the real solutions of f equals zero, and the partial derivative with.
00:01:25.570 - 00:01:27.658, Speaker C: Respect to y is zero.
00:01:27.826 - 00:01:37.402, Speaker B: And once you have them, you have more work to do. But that's part of the, part of the work. And as you can see, there are tons of references here, and it's not exhaustive.
00:01:37.578 - 00:01:39.614, Speaker C: That's a very popular subject for research.
00:01:40.914 - 00:02:11.972, Speaker B: I had to deal with bivalid systems in another context, which was walking over a fine field, counting the points in the Jacobian of a curve of genus two, so as to describe, to define a hyperlink fast hyper elliptic cryptosystem. So that was work with Godary. And for that we had large binary systems to solve with coefficients in a large prime field. And as well, and I'll get back.
00:02:11.988 - 00:02:12.844, Speaker C: To that in a minute.
00:02:12.964 - 00:02:49.750, Speaker B: And as well, that question of two equations, two unknowns, it also shows up as one of the ingredients in an algorithm that was mentioned a couple of times before our geometric resolution, due to in that, in that precise version, justille cell and salvi. And that's kind of a symbolic homotopy algorithm. So your symbolic homotopy means you are going to lift an actual algebraic curve and then cut it through by the next equation, which leads you to solving systems of two equations in two notes. But to be perfectly honest, that's, that part is not the bottleneck of the whole process.
00:02:49.822 - 00:02:52.518, Speaker C: So it just appears there, but there.
00:02:52.526 - 00:03:08.830, Speaker B: Are more costly steps. So before actually solving systems, I'm going to look at something which is rather close, not quite the same, but if you can do that, you're almost done, which is computing resultants and these simple.
00:03:08.862 - 00:03:13.554, Speaker C: Resultants, the silver cell resultants of two equations in a couple of variables.
00:03:14.754 - 00:03:55.658, Speaker B: So, I'm starting by the simplest case. I'm given f and g with variable y over field k, degree d both. So the input has d terms, more or less two equations of degree d, computing the resultant of them. The resultant has one term, it's a number, and it's known that you can compute that in essentially linear terms. So this ot here means I'm not looking at discarding logarithmic terms up to log terms. That's a linear time algorithm. So it was known in the sixties that you could compute a resultant from a suitable twisted polynomial remainder sequence.
00:03:55.658 - 00:03:59.426, Speaker B: It was known essentially since Schnager in knud that you can do it in.
00:03:59.450 - 00:04:01.934, Speaker C: That amount of time by dividing conquer process.
00:04:02.874 - 00:04:04.946, Speaker B: And so that's up to the logs.
00:04:04.970 - 00:04:06.336, Speaker C: That's the best you can hope for.
00:04:06.490 - 00:04:28.924, Speaker B: The input has d coefficients. You take quasi linear time in D. Fine. The question we are looking at looks more like this one, actually, when instead of having univariate inputs, I have bivariate ones. And yeah, computing results of these kind of guys is tantamount to solving the system.
00:04:28.964 - 00:04:29.664, Speaker C: Indeed.
00:04:30.204 - 00:04:50.824, Speaker B: So in this case, this is the case that really hurts. As we will see, if both guys have degree d, it takes about d squared coefficients to write them the dense representation. Their resultant has degree d squared in the variable x, and it's not known.
00:04:50.864 - 00:04:52.680, Speaker C: How to compute that in an optimal manner.
00:04:52.792 - 00:05:16.870, Speaker B: The best that's known, you can use an algorithm to rideshot that will do that, or you can use chinese remaindering theorem to go back to the universe case, if your field allows, it is d cubed. And why is that? Well, it's about d squared times the amount of work you had there. So that was optimal because you are input bound. No, it's not optimal anymore. And that's really annoying. Going back there.
00:05:16.902 - 00:05:17.302, Speaker C: Whoops.
00:05:17.358 - 00:05:40.868, Speaker B: Going back there. In that particular case, for instance, we had equations of degree about, I don't know, a couple of thousands deep. So about a few million solutions. And, well, that's fine once you have them, we are happy that was okay. But we would really have liked to have something better than this DQ bar to compute this million solutions instead of waiting two weeks. If we could have waited only a couple hours, we would have been glad.
00:05:41.036 - 00:05:42.784, Speaker C: Fortunately, we don't have that.
00:05:44.244 - 00:05:45.596, Speaker B: I would have loved to say something.
00:05:45.660 - 00:05:48.304, Speaker C: About this case, but I don't have anything to say.
00:05:49.924 - 00:06:12.288, Speaker B: So let me move on to something which is going to be suboptimal again, but less suboptimal, first by introducing a t, and then later on it will become integer coefficients. But first let me introduce another variable, t. So now we have equations in three variables. Imagine we are intersecting two surfaces in three space. The resultant is more or less the.
00:06:12.296 - 00:06:15.884, Speaker C: Equation of their projection on the t and x plane.
00:06:17.344 - 00:06:40.268, Speaker B: So the input size, we have an extra variable from d two to d cube. The resultant, we have one extra variable in degree, d squared. So from d squared to d four, the running time, we pay another d squared compared to what we had before, still not optimal, less suboptimal, because five quarters is better than three two.
00:06:40.316 - 00:06:43.424, Speaker C: But still it's not as good as you would want it to be.
00:06:45.044 - 00:07:04.714, Speaker B: So that last column here leads me to the Boolean case. So, relying on the analogy between polynomials in t and integers, I can take everything I had here and look at the case of systems with integer coefficients instead of looking at a t degree.
00:07:05.134 - 00:07:09.110, Speaker C: I can speak of bit size or logarithmic height or.
00:07:09.182 - 00:07:31.420, Speaker B: Yeah, bit size would be fine. And everything I said there translates more or less directly in the boolean world. Over there, all costs were in operations of. Ok, here all costs are in a Boolean model of your choice. The resultant is a large object. It has d to the four bits, and we can compute it in that.
00:07:31.452 - 00:07:32.604, Speaker C: Amount, d to the five.
00:07:32.724 - 00:07:42.596, Speaker B: So here I'll be able to say something so that's not optimal. And the goal is to prove, to show algorithms that get you closer to the optimal, not quite for resultant, but.
00:07:42.620 - 00:07:45.114, Speaker C: For solving systems which are more than the same thing.
00:07:47.574 - 00:07:53.678, Speaker B: So yeah, to define my actual question. So I'm not only wanting to eliminate.
00:07:53.726 - 00:07:56.314, Speaker C: Y as I want to recover it.
00:07:57.374 - 00:08:05.606, Speaker B: So my idea at first would be something like that. Given an f and a g, find a p and an s such that. Well, we have something that was described.
00:08:05.630 - 00:08:06.614, Speaker C: Before in Agnes talk.
00:08:06.654 - 00:08:14.506, Speaker B: For instance, we find the solutions of the equations by means of p of x equals zero and y expressed as.
00:08:14.530 - 00:08:15.814, Speaker C: A function of x.
00:08:17.154 - 00:08:18.386, Speaker B: So, and then there's a little bit.
00:08:18.410 - 00:08:21.146, Speaker C: Of things that needs to be to be taken care of in order for.
00:08:21.170 - 00:08:43.174, Speaker B: This to make sense. If the solutions look like that, you won't be able to express them in this manner because y is a multi to one here, and the usual workaround is to change coordinates. So I will do that instead of projecting parallel to the y axis. I'm going to project parallel to something else and most directions of projection will be fine.
00:08:43.634 - 00:08:52.474, Speaker C: There's going to be d square points here. So there are at most about d to the four bad directions. That's one.
00:08:52.634 - 00:08:59.402, Speaker B: So that here accounts for the fact that here what I'm writing are not really f and g, but f and g up to a change of variable.
00:08:59.538 - 00:09:04.654, Speaker C: That's just to account for possible non generic projections.
00:09:04.814 - 00:09:08.406, Speaker B: And then there's also the fact that here I'm not going to want to.
00:09:08.430 - 00:09:10.434, Speaker C: Compute multiplicities of any kind.
00:09:11.934 - 00:09:16.070, Speaker B: So I'm going to, the equality I want is a square free polynomial here.
00:09:16.142 - 00:09:18.914, Speaker C: Which describes the radical of my input.
00:09:19.854 - 00:09:26.422, Speaker B: So here everywhere the input is as coefficients in l a domain like imagine the integers or k of t and.
00:09:26.438 - 00:09:28.834, Speaker C: The output as coefficients in the fraction field.
00:09:29.364 - 00:09:38.396, Speaker B: And that's what I want to do once I have this. Well, I'm saying that from my point of view the problem is solved. You probably still have something to do with it.
00:09:38.420 - 00:09:45.544, Speaker C: You have, I don't know, isolate routes or say something about the roots. But that's not my problem. My problem is computing these guys.
00:09:46.484 - 00:10:04.352, Speaker B: And everything that we saw for resultant more or less carries over to computing these guys up to a small, small precaution I have to take. So again, I'm going to look specifically at these two cases where there's a.
00:10:04.368 - 00:10:10.564, Speaker C: Notion of length of coefficients or height, as Teresa said two days ago.
00:10:11.304 - 00:10:13.320, Speaker B: So over the integers it's the bit.
00:10:13.352 - 00:10:15.480, Speaker C: Size, so the logarithmic in any basis.
00:10:15.512 - 00:10:26.136, Speaker B: You want, and over the polynomials it's degree. I'm going to assume, just like I did before, that the degree, the degree of the input is d and that the length of their coefficient is the.
00:10:26.160 - 00:10:32.384, Speaker C: Same d. Because that will give me simpler formulas just, yeah, you could do it without that assumption.
00:10:32.424 - 00:10:56.818, Speaker B: But let's go with d and d, it's just fine. They, oops. So I want to compute two guys, an eliminating polynomial and something that gives me y for the eliminating polynomial. It's the same thing as a result. And this is a resultant or factor or resultant. So it's easy to control its size about d square bits per coefficient.
00:10:56.946 - 00:10:59.974, Speaker C: And that's essentially optimal that this kind of bound.
00:11:00.634 - 00:11:19.660, Speaker B: Unfortunately, if you don't take any precaution, the other guy is not as well behaved. And that's something that you see when you compute Lex square, no basis over the integers. Typically the last polynomial will be pretty nice and all the other ones will be pretty not nice. And this is quantifiable by an upper.
00:11:19.692 - 00:11:21.676, Speaker C: Bound on their coefficients here.
00:11:21.740 - 00:11:23.732, Speaker B: That is d four rather than a.
00:11:23.748 - 00:11:26.184, Speaker C: D squared, and that's kind of sharp.
00:11:28.084 - 00:11:33.252, Speaker B: So don't compute that. Compute something else that Agnes mentioned in.
00:11:33.268 - 00:11:34.620, Speaker C: Our talk as well.
00:11:34.812 - 00:11:59.282, Speaker B: Instead of computing s here, compute a twisted version of it where you multiply it by p, prime and reduce mod p. So if you know one, you know the other. And as it turns out, this one works better. Coefficients are smaller, and that's due to the weight shows up naturally from the resultant, for instance. And yet this guy shows up more.
00:11:59.298 - 00:12:06.574, Speaker C: Than 100 years ago in old papers, this kind of parameterization was there already, and this one has a better bit size bound.
00:12:07.314 - 00:12:14.374, Speaker B: In numerical analysis, you have the barycentric Lagrange interpolation, which is said to have.
00:12:14.414 - 00:12:20.714, Speaker C: Better approximation property than the usual Lagrange approximator interpolation. It's the same thing.
00:12:22.574 - 00:12:34.478, Speaker B: So bottom line, once you choose properly what you want to compute, it fits in about d square bits in your computer, per coefficient.
00:12:34.526 - 00:12:34.822, Speaker C: Sorry.
00:12:34.878 - 00:12:36.102, Speaker B: So d to the four in total.
00:12:36.158 - 00:12:38.754, Speaker C: Because you have about d squared coefficient to compute.
00:12:39.314 - 00:12:48.762, Speaker B: The goal is to be to give an algorithm close to that, as close as we can. So sorry, I said bits because I was thinking integer k's.
00:12:48.858 - 00:12:54.374, Speaker C: If you are in the polynomial case, that's the amount of coefficients in k. The bounds here, they work in both cases.
00:12:58.914 - 00:13:19.624, Speaker B: So the bottom line is that in the integer case we can get close to optimal, and in the polynomial case we cannot. So this question is one of these funny questions where we have better bit complexity results than algebraic complexity results. So in the boolean case first. So I'm going to.
00:13:21.084 - 00:13:22.100, Speaker C: Whoops.
00:13:22.292 - 00:13:24.556, Speaker B: To use Monte Carlo algorithm, I would.
00:13:24.580 - 00:13:28.424, Speaker C: Love to give deterministic results with the same cost, but I don't know how to do that.
00:13:28.924 - 00:13:31.100, Speaker B: So I'm going to use Monte Carlo algorithm.
00:13:31.132 - 00:13:35.634, Speaker C: We'll see in the next slide. Already I'm going to start choosing stuff at random.
00:13:38.014 - 00:13:58.142, Speaker B: I can obtain a running time optimal up to d 2d epsilon by using actually techniques that have been around for a while, but not combined in that way yet. So using symbolic Newton iteration, that's an old idea for polymer system solving is.
00:13:58.158 - 00:13:59.974, Speaker C: From the eighties or nineties.
00:14:00.794 - 00:14:04.066, Speaker B: So I'll have to take care of multiple routes by using, if there are.
00:14:04.090 - 00:14:06.154, Speaker C: Multiple routes by using a deflation algorithm.
00:14:06.194 - 00:14:23.914, Speaker B: In that case, the one u to le Cerf. And the slightly new ingredient here is in order to make this work, I also will use an algorithm due to kitla. Yeah, and humans have four years ago now that was introduced that they used for factoring polynomials for instance.
00:14:23.954 - 00:14:25.202, Speaker C: And here, well, it turns out it.
00:14:25.218 - 00:14:48.688, Speaker B: Also can be useful in other contexts. So I said at the very beginning that you can, it's not too hard to find polynomial type deterministic algorithms for all these. That's true. The best deterministic result is here. That's by busidine. So in what lazare, morose, Royer and Puget.
00:14:48.736 - 00:14:53.486, Speaker C: So in not the proper order, d.
00:14:53.510 - 00:14:57.382, Speaker B: To the, in the Las Vegas context, and d to the five.
00:14:57.438 - 00:14:59.234, Speaker C: Monte Carlo is easy to find.
00:14:59.894 - 00:15:05.182, Speaker B: So there's these. The more probabilistic you are, the better.
00:15:05.278 - 00:15:06.878, Speaker C: You can be, at least in theory.
00:15:07.046 - 00:15:23.426, Speaker B: And so, strangely, over the, in an algebraic context, the ide carries forward, but the algorithm do not because that component is missing only for the case of simple roots. There is something better than the d.
00:15:23.450 - 00:15:27.774, Speaker C: To the five, for instance. And that's frustrating, but that's how it is.
00:15:28.394 - 00:15:46.258, Speaker B: So the scheme is something which is extremely classical. So I'm working with the coefficients in a domain, integers or a polynomial, choose a maximal ideal there. So either reduce mod p or specialize at t t zero.
00:15:46.306 - 00:15:49.334, Speaker C: Random p or random t. So I'm probabilistic.
00:15:49.794 - 00:16:10.866, Speaker B: You solve your equations modulo that p or t naught. That gives you your output, let's say mod p. You can lift it modulo p squared p four p eight. As we, well, we heard about Newton iteration quite a bit in analytic context, that's in a mnematic approximation context, but.
00:16:10.890 - 00:16:11.736, Speaker C: It'S the same thing.
00:16:11.850 - 00:16:13.864, Speaker B: And once you have enough precision, you.
00:16:14.204 - 00:16:15.384, Speaker C: That's a mistake.
00:16:16.044 - 00:16:20.784, Speaker B: Once you have enough precision, you go from mod p to the something to.
00:16:21.844 - 00:16:24.504, Speaker C: Rational coefficients by rational reconstruction.
00:16:25.364 - 00:16:31.380, Speaker B: So the beginning and the end rational reconstruction are easy. And what is difficult is the, of.
00:16:31.412 - 00:16:32.624, Speaker C: Course, what's in the middle.
00:16:38.214 - 00:16:53.750, Speaker B: So I want to actually write what Newton iteration looks like, because then we'll be able to see what is going to be costly. So that's a bivarate iteration. So I can write everything first in a context of an approximation over the reels.
00:16:53.782 - 00:16:55.718, Speaker C: For instance, I would write that my.
00:16:55.766 - 00:17:01.022, Speaker B: Iterate xnyn is improved into xn plus one, yn plus one.
00:17:01.038 - 00:17:02.918, Speaker C: By means of this correction term, I.
00:17:02.926 - 00:17:43.148, Speaker B: Have to evaluate f and g, and I have to invert a jacobian matrix. And for the moment, let me just assume that I'm close to a root, to a simple route where that matrix is going to be invertible, so everything works smoothly. So that's the usual version of Newton iteration. And there's something completely similar that exists for calculating in a MT context with formulas that are going to be entirely inspired by those that take your solution, say mod p and that. So your polynomial is modulo prime and that return the same solution you want, but modulo you have the square of.
00:17:43.156 - 00:17:45.824, Speaker C: Your prime, and then from power two to power four, and so on.
00:17:47.604 - 00:17:50.420, Speaker B: It's obviously not the linear algebra that will be costly here.
00:17:50.492 - 00:17:53.264, Speaker C: I have to work with a two by two matrix, so that's fine.
00:17:53.644 - 00:18:00.860, Speaker B: All the work is in the evaluation of the functions. So in a numerical context, it would be evaluating f and g at my.
00:18:00.892 - 00:18:02.472, Speaker C: Root, my approximate root.
00:18:02.668 - 00:18:12.216, Speaker B: In this context here, what this means is for f, for instance, I have to evaluate it modulo this, which means.
00:18:12.320 - 00:18:16.808, Speaker C: Reduce it mod p and y equals.
00:18:16.856 - 00:18:25.832, Speaker B: X, all that with coefficients modulo power of your prime, and that's all that there is. Once you can do that, all the.
00:18:25.848 - 00:18:26.840, Speaker C: Rest is going to be easy.
00:18:26.872 - 00:18:40.878, Speaker B: So if you can do that efficiently, everything works fine. So in most instances where these ids were used in a symbolic context, the idea was that the inputs were given.
00:18:40.926 - 00:18:42.514, Speaker C: By means of a straight line program.
00:18:43.094 - 00:18:53.854, Speaker B: And then, well, you have a straight line for your f. This reduction, it just makes sense to do it step by step modulo p, and do every operation like we saw a repeated squaring.
00:18:53.894 - 00:18:55.870, Speaker C: In the first talk, I think on Monday morning.
00:18:56.022 - 00:19:01.870, Speaker B: All calculations mod P. It's just natural. If you have an SLP, there's not.
00:19:01.902 - 00:19:02.750, Speaker C: Much to say here.
00:19:02.782 - 00:19:13.110, Speaker B: Just follow it Modulo P. In this context of bivarate equations, well, I could assume that they are given by a straight line program, but often they are.
00:19:13.142 - 00:19:22.754, Speaker C: Not bivariate equations, the ones you have to deal with. Usually they are just dense guys. And so I'm going to not assume a straight line program f is given as a dense polynomial of degree.
00:19:24.524 - 00:20:08.060, Speaker B: So that's what makes this not too easy. So here is where the difference between the integer and the algebraic case shows up in the way that this reduction is going to be done. And over the integer keys, we're going to use a modification of very strong results by Kellyanne humors from a couple years ago. I didn't write what was the input there. So in a slightly different context, what they obtained is an algorithm that computes this kind of composition. Given a g, an s, and a big p, all univariate compute g of s mod p. So imagine they all have degree d.
00:20:08.060 - 00:20:28.664, Speaker B: So the input has d coefficients, the output has d coefficients because it's reduced. And you have to assume that you're walking in a boolean context. So here in a z mod nz, then in that context you can do this in essentially optimal time. So the beauty here is in the d to the one plus epsilon, naive.
00:20:28.704 - 00:20:32.244, Speaker C: Algorithms, d squared, this almost linear time.
00:20:32.864 - 00:20:56.872, Speaker B: So up to this epsilon, it's optimal. And it's not known how to extend these ideas to some, to an algebraic algorithm. So at the time, that was extremely impressive, because that gave the first improvement for the complexity of factoring polynomials or fine fields in 15 or 20 years, just by me, just because this operation.
00:20:56.928 - 00:21:02.044, Speaker C: Here, for instance, is at the core of algorithms for factoring polynomials or fine fields.
00:21:03.504 - 00:21:28.798, Speaker B: So what we are going to do is very close to this. It's a slight extension of it to a bivalid situation. What we had to do was more or less computing that, up to the fact that the f was bivariate. This is not a big deal. You can take the Kellarian monsites and make them work in that context. So I'm given my f. What I want to compute is the reduction of that f module of this kind of idea, which is computing f of x.
00:21:28.846 - 00:21:32.534, Speaker C: It's by the x remains itself. And this s that I know.
00:21:32.574 - 00:21:53.594, Speaker B: So this and that looks similar, and the results are the same. So now f has d squared terms. This guy has degree d squared. We are computing modulo n, which has about length d squared. So optimal for all this would be about d four.
00:21:53.714 - 00:21:59.146, Speaker C: And indeed, it can be done in an almost optimal amount of time, up to that epsilon, the same as up there.
00:21:59.330 - 00:22:06.426, Speaker B: So that's the basic idea for the algorithm of the integers, provided there's no.
00:22:06.450 - 00:22:11.084, Speaker C: Issue with multiple routes, because I assume that I could just run Newton iteration.
00:22:12.744 - 00:22:45.948, Speaker B: Let me just say a word about the algebraic case. In the algebraic case, there are solutions for this composition problem. This is the very famous algorithm by Bretton Kung from 78, which gives you at least sub quadratic. Again, for that problem here, the natural running time would be d squared. And you can bring in dots to d two, a little bit less than two, by means of matrix multiplication. This tiny omega here is multiplication matrix multiplication exponent. So that amount there turns out to.
00:22:45.956 - 00:22:50.652, Speaker C: Be less than two, but not by much, very far from the optimal we saw before.
00:22:50.748 - 00:23:03.716, Speaker B: But that's an algebraic algorithm. You can plug that into the scheme we saw before, and you count your beans and you arrive at the running time, which is d to the omega plus seven half, so less than five.
00:23:03.780 - 00:23:07.444, Speaker C: But way less impressive than this one.
00:23:08.184 - 00:23:09.528, Speaker B: And. Oh, pardon.
00:23:09.656 - 00:23:10.200, Speaker A: Yeah.
00:23:10.312 - 00:23:12.136, Speaker C: Oh, yeah. One more slide.
00:23:12.200 - 00:23:12.924, Speaker A: Okay.
00:23:14.224 - 00:23:21.464, Speaker B: And what's making all this work now is matrix multiplication with polynomial entries. So it's a very different idea.
00:23:21.504 - 00:23:23.204, Speaker C: From kedlar ya animals.
00:23:25.984 - 00:23:35.436, Speaker B: Finally, how to recover the general case. So actually, this is all technique is there. This is difficult, but I don't give.
00:23:35.460 - 00:23:36.704, Speaker C: Any details, so it's fine.
00:23:38.044 - 00:23:48.324, Speaker B: So there's the obvious issue that we don't know how well, Newton iteration won't work close to multiple routes. So one easy fix would be to just forget the multiple routes. But if you want to get them.
00:23:48.364 - 00:23:50.444, Speaker C: You use a definition algorithm.
00:23:50.604 - 00:23:52.684, Speaker B: And there's a whole lot of them.
00:23:52.764 - 00:23:58.064, Speaker C: Starting from work in the eighties, biogeica, Watanabe and Mitsui.
00:23:58.944 - 00:24:17.976, Speaker B: The one we relied on is due to the. Because it's easy to control what's happening in that one. You don't add new variables, you don't add new equations. You just work with your two, f and g, and you extract stuff from them. So at least it's feasible to control. In his context, it was easy.
00:24:18.040 - 00:24:20.604, Speaker C: No, not easy. Feasible to control the complexity.
00:24:21.304 - 00:24:30.222, Speaker B: We couldn't use these results off the shelf because in the multivit case, he.
00:24:30.238 - 00:24:35.310, Speaker C: Had to do some simplifications that would have been hurtful for the result in the pyruvate case.
00:24:35.342 - 00:24:46.718, Speaker B: So we had to rework a little bit, everything. But in the end, we get what we want in the boolean context, because the Kelly Moss algorithm is flexible enough.
00:24:46.766 - 00:24:54.604, Speaker C: That even with the deflation stuff, it will still work. It does not work for the moment in the algebraic context. And finish.
00:24:59.344 - 00:25:00.592, Speaker B: Any questions?
00:25:00.768 - 00:25:01.784, Speaker A: Yes, Agnes.
00:25:01.944 - 00:25:12.204, Speaker D: So in this new pen iteration, you also had an inversion. A modular inversion is the complex boolean.
00:25:13.584 - 00:25:15.564, Speaker C: There's no problem on this one.
00:25:16.344 - 00:25:18.056, Speaker D: I mean, it's harder than evaluation.
00:25:18.120 - 00:25:21.760, Speaker B: It is, but. So, for instance, what you said the other day, in a parallel context, it's.
00:25:21.792 - 00:25:29.734, Speaker C: Not easy, but in a serial context, here, no problem. It's another unit iteration. No issue here.
00:25:30.914 - 00:25:42.610, Speaker A: Any other questions? So, I guess the precursor of my algorithm with humans was this paper by humans where he had this kind of pseudo algebraically characteristic p and he uses forbenius. Can you copy that?
00:25:42.802 - 00:25:44.934, Speaker B: If I remember well, his p had to be small.
00:25:45.634 - 00:25:47.042, Speaker A: Yes, the p had to be small.
00:25:47.138 - 00:25:58.344, Speaker B: And here we're not really in mod P. We have Mod P to a to a power. I don't want to say anything. I have no idea.
00:25:58.424 - 00:25:59.084, Speaker C: Yeah.
00:26:02.344 - 00:26:03.304, Speaker A: All right, thank you, Eric.
