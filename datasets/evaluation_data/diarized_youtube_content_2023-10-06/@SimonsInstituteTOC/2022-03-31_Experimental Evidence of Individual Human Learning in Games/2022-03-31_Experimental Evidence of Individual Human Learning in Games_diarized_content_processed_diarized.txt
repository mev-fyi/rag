00:00:00.600 - 00:00:41.084, Speaker A: Hello everybody, and welcome to the second, the second talk of the day. We're extremely happy to have Colin Kamerer from Caltech University giving a talk today. So Colin is a true foundational expert in several areas, including behavioral game theory, intersections between neuroscience and economics, experimental economics, and recently using tools for machine learning in the mix. So he's really here to prime our brains to think about learning in different ways. So thank you so much Colin, for your, for participating in our workshop. And now the floor is yours.
00:00:41.424 - 00:01:46.068, Speaker B: Thank you Georges, and thanks everyone for coming and listening. So I'm going to talk about something that's quite narrow, but within that narrow scope, there's a lot of work and ideas, which is experimental studies falling mostly under the rubric of experimental economics. That means we typically explain things to people like they're fully informed about payoffs and we pay actual money to try to motivate them to think carefully. And usually most of these games are repeated stage games, which means there's a single game that's played over and over with the same group of people, sometimes with mixtures, what's called the stranger protocol. And there was a big boom in this area starting around the mid nineties to maybe 2010. And then it's kind of tapered off a lot because experimental economists, I think, have just often moved on to other interesting topics. I'm going to talk mostly about two things, which is reinforcement learning, coming straight from learning from 100 years ago, which is learning about what other people are likely to do from their past histories.
00:01:46.068 - 00:02:06.904, Speaker B: And then a hybrid approach that kind of combines the two elements called EWA that we worked on. And the goal here is to develop good descriptive models. That's it. We want to explain the data generated in the lab. And when we occasionally. I'll show you two studies actually from field data. And unusually, even among experimental economists who are generally quite adventurous, and behavioral economists.
00:02:06.904 - 00:02:58.830, Speaker B: I really like models which are biologically implementable and therefore consistent or can be tested with evidence from things like eye tracking, fMRI, neural recording of animals. You're going to see a little bit of all those today, not eye tracking. And again, most experimental economics labs, people come in, they see someone on the computer screen, they press a button, all we record is what they picked. Sometimes we ask them questions, sometimes we record response times. And I would like to record what they're looking at on the screen and maybe their all these other things because I think that the theories need to come up frankly, and therefore we need some data to pick out the best theories. Again, this idea that the theories have to be biologically implementable and tested thereby is not the modal view in the field. I think it should be, and that's my view.
00:02:58.830 - 00:03:29.084, Speaker B: Okay. We're going to start with something very far from that, though. That's cool. This is a lottery game that was played in Sweden. So this is field data, even though it has a lot of nice structure. And it was in 2007, about 53,000 people played each day, and each of them chose an integer written on that little card in the upper right that you see from one to 99,999. And the lowest unique number wins a prize of €10,000.
00:03:29.084 - 00:04:07.276, Speaker B: And then there's some other consolation prizes, which we ignore. And so this is a cool game because you have to do two things, one of which is strategic, which is to pick a low number. That's easy, and to pick a unique number when everyone else is trying to pick a unique number, two. So, as you can imagine, we're going to derive a mixed equilibrium and see what happens. Okay, so it turns out, amazingly, some of you will know about Poisson and Ash games due to Roger Myerson, because the number of players is not the same each day. If the number of players is Poisson distributed. Okay, it's actually not quite, but it's an approximation.
00:04:07.276 - 00:04:59.568, Speaker B: Then you can compute the mixed equilibrium, and we cannot compute it because of combinatorial explosion. If the number of players is fixed, if exactly 53,000 people are playing, we can't compute the mixed equilibrium at least. I mean, we can, maybe somebody can. But if the number of players is put on distributed, even though that kind of assumption makes life very difficult for things like auctions, in this case, you can compute a mixed real game, and it has a nice, very elegant simultaneous equations. So if n is the mean of the Poisson distribution and p of k is the frequency of people choosing the number k, all those k values have to solve that system of equations. And so this game is going to illustrate short run initial conditions, like what happens when people first play. They've never played this game before.
00:04:59.568 - 00:05:43.838, Speaker B: It was just invented from thin air by a swedish person who was a student of Martin Dufenberg. And then what happens as they learn? We're going to actually see literally day by day data for the entire group of people playing. Okay? So in week one, there's seven observations here, seven days, which is about 400,000 observations. The Poisson Nash prediction is the dotted line, and on the left is all the numbers up to 100,000. And on the right is the numbers from one to 10,000, because as you can see on the left, the game theory predicts that 95% of the numbers, approximately, will basically never be picked. Numbers above 5000, never. And in fact, that's a pretty bold prediction.
00:05:43.838 - 00:06:25.344, Speaker B: Right? Nobody knew this before Robert Osling and Joseph Wong, who were quite brilliant, solved it. And the people who designed the auction didn't because they wouldn't have had so many numbers available because a lot of the numbers never get picked. However, there are two deviations from the Poisson Nash equilibrium, which has a really beautiful structure. The most likely number to be chosen in equilibrium is one. Slightly less likely is two, then three. And there's a kind of a slippery slope, and then it plunges at around 5513 and the numbers drop very dramatically to essentially epsilon. Okay? And however, the plays of numbers that people picked in Sweden, they pick way too many small numbers.
00:06:25.344 - 00:07:00.664, Speaker B: Those are the gray bars that are above the dotted line right on the lower left. Like a lot of people, a couple hundred people picked one and two and things like that. And then there's a gap from about 2500 to 5000 of that predict distribution where they don't choose those enough. Okay. Model called cognitive hierarchy, which is, as the name implies, is kind of a hierarchy of levels. Like level zeros do something random or something salient, and then level ones think they're playing level zeros, and level twos think they're playing level ones. You can fit these data pretty well with the estimated average levels of thinking of 1.8.
00:07:00.664 - 00:07:30.604, Speaker B: Okay? And if you. This is a rare case where we had field data first, then you can go back and run it in a lab. And in the lab we can see if the lab is like the field rather than the other way around. It's pretty close. And in addition, we can do a few other cool things, like in the swedish data, we don't know who, if Ingemar or Bergman played seven days in a row, all we know is the population of choices. We don't know if one person picked over and over, but in the lab we do. So we can see if they really mix.
00:07:30.604 - 00:07:52.664, Speaker B: Do they have lucky numbers? They get fixated on, how do they learn and adjust? And the lab data is very, very close. The puts on national Nash equilibrium is different because the numbers are only one to 99. There's fewer people. There's 26 people on average. That's the mean of the Poisson distribution. The dotted line is Poisson Nash. It just kind of goes almost straight down to 13.
00:07:52.664 - 00:08:24.994, Speaker B: And the data are really, really close. Really, really close. Right there's a little bit of people picking low numbers a little bit too often. And again, there's sort of a gap to the middle of the distribution where there's not enough people picking 910, 1112. So it's roughly similar in style. And this is an example of where I think, as is often the case, experimental economics lab data fits field data very, very closely, even though it's different people. These are students at UCLA and Caltech.
00:08:24.994 - 00:09:01.423, Speaker B: You know, they're, they're playing for smaller amount of money, et cetera, et cetera. Okay, here is a GIF that Robert Osling made that shows 49 days of learning. So you can see each of the distributions, how the learning works in a very fine grained way. People have made very few of these gifts. I think it's quite neat. And you can see day one they're really way off. People are choosing way too low, but by day 49 they're kind of filling in the gap between blue and red.
00:09:01.423 - 00:09:28.696, Speaker B: And the number of one choices gradually goes down. There's still a lot of people picking one thinking, you know, I'm going to pick one. No one else is going to think of that, right? Nobody wins at numbers like that. The winning numbers are typically, by the way, around 2000, up to 3000. Okay, so learning. So it turns out the best paper on this is not ours. It's by Eric Moline, Ausling and Wang in games economic behavior.
00:09:28.696 - 00:10:04.934, Speaker B: And they basically show that the best model of population learning is essentially an imitation model where it's very much like what John Nash called mass action, where the group is a whole shift in the direction of the last winning number that they. And that you can do that with very few parameters. And that's basically what we think is going on. Again, we don't have individual level data. Okay, so to reiterate, I'm going to talk about fictitious display reinforcement, hybrid model, a number of other people. I think Dan Friedman is here. He worked on replicator dynamics, has some beautiful early work on that.
00:10:04.934 - 00:10:57.126, Speaker B: By the way, I have a book called Behavioral Game Theory 2003 that has a long chapter on learning. And I try to be very thorough on all the different approaches people had used around that time. But that was 20 years ago. Somebody should update it. There's a few field data. I'll end with those at the end. Okay, so one interesting thing is that in at least three different areas I know of, and there may be quite a few others, that there's been a kind of rediscovery of a difference between learning from experience, like picking a number in the lottery and figuring that you lost and that you know the number 20 614 won versus kind of computing a strategy value from beliefs or from comparisons of hypothetical payoffs that you could have had if you picked a different number that goes under three different rubrics.
00:10:57.126 - 00:11:29.324, Speaker B: The simpler type of learning is called model free in decision neuroscience, borrowed from CS Nimbarto. I believe in game theory. In the nineties, it was introduced in the form of simple reinforcement. You pick an action, and then you update the propensity to pick the action based on your payoff. The complex version is called fictitious play, and the complex version and decisionura is called model based. Model based just means you have an idea of what all the possible actions would lead to. Like, you can see the game matrix.
00:11:29.324 - 00:12:03.284, Speaker B: And in 1950, there's this Nash quote here, taken right from the typescript of his thesis. He talks about the mass action interpretation of equilibrium, where participants are supposed to accumulate evidence about the relative advantage of it. And Jorgen Weibo kind of updated that in the nineties and wrote a similar paper, introducing what he calls innovative play. And so, innovative play means playing a number you never picked before based on some model of how well it will do. Okay. All right. Blizzard of notation.
00:12:03.284 - 00:12:33.570, Speaker B: Sorry about that. But it just. This is what it is. So, belief models typically work, like on the left, in which this is something like the simple counting of fictitious play. The idea is that you see what somebody did last time. They picked top, bottom, right in a matrix, and you just count the number of times. You can use a decay factor row to create a sort of recency bias so that more recent observations get more weight, and you compute beliefs in that way.
00:12:33.570 - 00:13:02.654, Speaker B: This is a very special kind of fictitious display, right? There are lots of variants you could introduce. This is just counting past behavior, geometrically waiting, and then equation 2.9 here is that we're going to write Taitien's expected payoff for person a from strategy j as a function of these beliefs. And you can then write, because the beliefs are a function of lag beliefs. Right, the upper left. You can then write the expectation as a function of the lagged expectation. And it has this interesting form.
00:13:02.654 - 00:13:59.412, Speaker B: So, it turns out that in reinforcement learning, mostly al Roth and Yido are working on this. But many other people like Dan may have worked on it, and Rajiv Sarin and others were working on variants of reinforcement around this period of time. And the standard model is that there's a reinforcement level that's a function of the decayed lag reinforcement, plus an identity term which weights the payoff of the strategy, if that's the one you chose, and otherwise, there's no reinforcement. This is just one vanilla version. There's other features in here. Z grid, which has the expected of payoffs from fictitious play, and the reinforcement levels as both as boundary cases. You can just write an equation like this, 2.2.
00:13:59.412 - 00:14:42.554, Speaker B: And we just substitute a for attractions. And the idea is that the attraction of strategy j is multiplied by a weight delta, even if you didn't choose it. And if you did choose it, the weight is delta plus one minus delta, which just adds up to one. So you fully weight receive payoffs from strategies you chose and you weight by delta strategies that you didn't choose, the so called innovative play model based etcetera. Okay, and then we can use delta to kind of tell us how strong is a pure reinforcement effect where strategy is reinforced versus a fictitious display model based type effect. Typically, we get deltas around 0.5, but it varies.
00:14:42.554 - 00:15:17.138, Speaker B: And also the. The identification of parameters is, as you can see, there's a lot of things multiplied together. And then also these attractions are then fed into a softmax function. Okay, so here's some neural evidence without much background. This is a paper by Liu Xiao Zhu, who is a postdoc of Ming Su, who's there at Berkeley. She is my academic grandchild because Ming was my student at Caltech. And what they did was to play a game where you could estimate the reinforcement signal and the belief signal and the signal from the hybrid Ewa model.
00:15:17.138 - 00:16:14.486, Speaker B: So, in each, you know, there's a time series of these numbers. Then you look for areas of the brain that have bold signal, which is blood flow activity, that's correlated with the time series of the behavior. This is called brain behavior, computational event related fMRI. What you see is that in putamen, which is on the left, which is part of what's so called basal ganglia reward circuitry, you get neural betas. That is, the blood flow signal in the brain is associated with the number that we think people are computing for all three models. However, in anterior cingulate cortex marked at this purple line, which is an important region involved in conflict and cognitive control and error monitoring, it's thinking a little harder about something that's more complex. You can see that there's a signal that rather, activity in ACC is correlated with the belief based computation and with the hybrid, but not with reinforcement learning.
00:16:14.486 - 00:16:29.390, Speaker B: So reinforcement learning and belief based learning are both imputaments, but it is absent in anterior cingulate. So if you the answer to the question is there a region which does not compute reinforcement, but computes other things? That's the answer question, Colin.
00:16:29.422 - 00:16:56.714, Speaker A: Yes, sorry, I have a question. So, here, how do you know when the experiment happens that an agent, let's say, uses reinforcement or use belief based? So, which I presume this is what you're matching. You say, look, so somebody looks at the behavioral data coming from the agent somehow, like playing a game again and again, and then you do some sort of, like, machine learning to estimate, like a best fit of. Of those parameters in.
00:16:56.754 - 00:17:04.530, Speaker B: Yeah, yeah. Yes. Except usually this study. This is pretty old, as it. Economics study goes. It's ten years old. Yeah, exactly.
00:17:04.530 - 00:18:06.204, Speaker B: So what happens is you take the data from behavior, from what they did, and then you fit a model, or fit multiple models as if you don't even have the brain data. Then we take the best fitting model for each person, and we say, what areas of the brain have a time series of some number, like what choice you made or the probability that you are using reinforcement learning that's correlated with the behavioral model? Exactly. This process was essentially invented by John O'Doherty, who's my colleague here, about 2005, and has really swept through neuroscience. And sometimes you get really results. But jog in mind is that if you really understand the behavior first with this computational model, which could include machine learning, by the way, that's often used. If you have lots of areas of the brain and you want to pick out just a few, regularization is absolutely the way to go. And that's been very, very popular, sometimes called MVPA.
00:18:06.204 - 00:18:26.468, Speaker B: So our view is, if you really understand the behavior computationally and you really fit the behavior pretty well, then you're going to find interesting things in the brain, and if you don't really understand the behavior computationally, it's going to be a mess. Right. And again, this is. I mean, I'm kind of an evangelist for this viewpoint. Right. But it's been very successful. We think.
00:18:26.468 - 00:18:28.784, Speaker B: We think. Okay. Hi.
00:18:29.124 - 00:18:41.576, Speaker C: Yeah, another question. Could you possibly define, at a high level, what are the key differences between reinforcement learning and belief based learning and what those two are? Because at this point, I'm not totally sure of that.
00:18:41.600 - 00:19:10.960, Speaker B: Okay. Oh, great. I'm really glad you asked. Is that quite central? Thank you for asking. So, the key is in something like the lottery game, right? Reinforcement means I played the number 82 and I got zero, and so I do not reinforce 82, and I don't do anything with the other numbers in belief based learning, I picked the number 82, and I got zero. But I know that the winner was 2500. So I say to myself, gee, if I picked 2500, I would have had €10,000.
00:19:10.960 - 00:19:29.484, Speaker B: So I then update the value of choosing 2500 by Delta Times €10,000. And that's mathematically the same as if I'm forming a belief about what everyone else will do, and then I'm learning to best respond to that belief. Does that help?
00:19:30.624 - 00:19:47.724, Speaker A: Maybe just to give you also, like, a comment, I think in the reinforcement learning setting is, like, closer to our bounded setting. So we only get access to the utility from the action that you see, whereas in the belief space, it's actually access to the full vector.
00:19:48.184 - 00:20:13.856, Speaker B: Right, good. Perfect. Yeah, that's absolutely right. Exactly. It's as if you're doing a bandit task and you get a payoff from the band that you selected, and then you also find out some information, could be partial information, about what the other bandit rewards were. Okay. Okay, I'm just going to skate through this one.
00:20:13.856 - 00:21:00.944, Speaker B: This is actually data from a paradigm in which you implant an electrode in a monkey's brain and record individual neurons firing from different regions. And all you need to see in this picture is that aO, these are essentially firing rates. Ao, which is actual payoffs. When the monkey is looking at the actual payoff they received from playing a game, it's basically a hider seeker game, that the neural firing rate about 0.2, 200 milliseconds after they get the neural firing rate in an area called dorsolateral prefrontal cortex, which is here, and orbital frontal cortex, which is above your eye sockets, hence orbital. The blue and purple lines go up. That means those neurons in that area are firing.
00:21:00.944 - 00:21:29.938, Speaker B: In addition, when the monkey looks at a hypothetical path, it did not receive, like, if I made a different choice, what would I have gotten? That's what a belief based model would do. You're building up a belief about the other player, and you think, oh, that other player did this. If I had chosen differently, I would have done better or worse. And the neurons also fire. That's orange and red. They also fire, but almost only half as much in response to the hypothetical payoffs. So, again, the key idea here is very, very simple.
00:21:29.938 - 00:22:01.364, Speaker B: In off the shelf, straight reinforcement learning, the only thing that's reinforced is what you actually picked, and so there's no need for the animal to look at the hypothetical payoffs. But if you want to learn faster, like, in a belief based way, than the hypothetical payoffs, and again, that's model based. Just play has that implicit assumption and innovative play from weibo okay, so the hypothetical paths are attended to in the monkey brain? Yes.
00:22:01.824 - 00:22:14.384, Speaker A: Just a small point. I'm not sure if you can do anything about this, but sometimes we do have some slight delay from you on your end, like from the Wi Fi. I mean, for the most part, I think it works fine.
00:22:15.164 - 00:22:19.984, Speaker B: I'll just talk slower or say, try to say less.
00:22:22.124 - 00:22:37.464, Speaker A: I'm saying that maybe another approach, that if the problem persists or becomes harder, maybe we'll ask you to maybe turn off your screen and we can still hear you, but maybe not see you, but we'll still be able to see your slides.
00:22:41.244 - 00:23:01.104, Speaker B: Okay, maybe that will help. Okay, here's another game that's going to show us learning and show us some model comparison. I call this price matching with loyalty rewards. It's kind of like a Bertrand competition. There's two players. They simultaneously, without communicating, pick a price integer price from age to 200. Those are pennies.
00:23:01.104 - 00:23:45.174, Speaker B: And then the price they both receive is the minimum, right? So it's like an industrial regime in which you have to match the other guy's price, but the low price firm earns that number, p, which is the minimum plus a reward, r. And the high price firm earns P minus r. So the conflict here is similar to the lowest integer game. The two players together want the price to be high. They want to collude and try to pick 200. But if the other person picks 200 and you pick 199, you get 199 plus the r reward and a typical r value. In their original paper, they play with various values of r, and the typical r value, for example, is 50.
00:23:45.174 - 00:24:17.394, Speaker B: So there's a very strong incentive to undercut the competitor. But if everyone undercuts everyone else, you can imagine the national brian, if players are perfectly selfish, is 80. You should just have this race to the bottom where everyone is constantly undercutting. And even though they end up worse off than if they could somehow collude at 200, that's what happens. So let's see what happens in the data. Here's empirical frequencies, okay? So on the x axis, the front axis is number categories. From 80, 80 is a distinct category up to 200.
00:24:17.394 - 00:24:46.884, Speaker B: And then the periods they play ten times. So the periods go toward the back from one to ten, one to nine to than ten, okay? And then the frequencies are on the vertical axis. So what happens is they start out choosing. A lot of people choose 150 and some choose 200. Those are the most common choices in period one, right? The purple bar and then the light lilac bar. Nobody picks 80, right? But gradually, what happens is the high numbers disappear. Right.
00:24:46.884 - 00:25:48.512, Speaker B: There's a few people, this may only be a couple of people picking 200 later, but basically the system drifts downward after about five periods. The modal choice is orange, which is around 125. And by period 78910, 80% of the people in the last period are choosing 80, the unique Nash equilibrium. So this is also an exemplar of something we often see in experimental economics, which is the data start at far from equilibrium, in this case at 200, which is this kind of like artificial cooperative outcome that is not sustainable, and then it drifts. In this case, in ten periods, you get 80% conformity with the unique Nash equilibrium of 80. Okay. And so we're going to try to understand our approach, and approach of many people at this time was to try to explain every single person's observation, every person, period of data, rather than just moments of the data, although, of course, those can be quite useful, too.
00:25:48.512 - 00:26:13.516, Speaker B: So we're going to try to produce a model which reproduces these data. And by the way, in our work, we almost always used in sample validation and then, or in sample fitting and then a holdout test sample, so that we're, since a lot of these models have a lot of degrees of freedom, we're trying to control for degrees of freedom as well as using bicaic measures. Yes, question, just. Yeah, what, what happens when the players.
00:26:13.700 - 00:26:16.116, Speaker A: Report the same price? I.
00:26:16.180 - 00:26:31.420, Speaker B: Sorry, I missed that part. Um. Oh, you know, I think then they just get p. In other words, I think nobody gets, nobody gets, gets r and nobody loses r. And how is that an equilibrium? Oh, because, again, uh. I see.
00:26:31.492 - 00:26:32.236, Speaker C: Nevermind.
00:26:32.380 - 00:26:57.034, Speaker B: Yeah, yeah. Oh, you mean how do they get stuck at 80? Yeah, because you don't want to go above, right? If the other guy's at 80 and you go above, the price is still 80. You couldn't figure it out. Yeah, it does depend on r. If r is very small, then the results can be different. Like if r is less than one. Right? So.
00:26:57.034 - 00:27:31.968, Speaker B: And actually, that's a feature, not a bug. So by varying r, if you think about the game theory a little bit, as long as r is above one, by varying r, the Nash equilibrium doesn't change at all. It's always 80. But when you make r like 80, which they did in some of the experimental sessions, the movement toward equilibrium is much faster. So a lot of these things, which in equilibrium shouldn't actually make a difference, will make a difference in the pace of learning, as you know. I mean, as you can imagine. That's why we should not just study equilibrium, only other processes like learning.
00:27:31.968 - 00:28:02.818, Speaker B: Okay, so that's the empirical frequency. Again, here's a belief based model. So this is fictitious play, and it does a pretty good job on the basic trends. It's much more orderly, of course, looking than the data because there's no sampling error. The main flaw with the belief based model is the expected payoffs from the belief based only vary in a smallish range. And so in a softmax with a fixed inverse temperature parameter, it's really hard to get really high probabilities. You know, it's like 80% chance of picking 80.
00:28:02.818 - 00:28:10.454, Speaker B: So it doesn't quite generate tall enough bar charts at the bottom, but it does capture the basic trend pretty nicely.
00:28:15.834 - 00:28:34.974, Speaker A: So I have a short question. So, for this belief based model. So your model has quite a few parameters. So. So I could imagine several different instantiation as an example of a belief based model. Is this, for example, like, exactly fictitious play or like.
00:28:36.834 - 00:28:43.570, Speaker B: It'S a geometrically weighted fictitious display? Wooden Burger Levine's book has a little discussion of this. Yeah.
00:28:43.682 - 00:28:50.734, Speaker A: Okay. And I guess the choice of this discounting does not really affect the qualitative results that you're showing us. Right.
00:28:52.164 - 00:29:20.294, Speaker B: Actually, it's pretty important here because you have to. In these data, it looks like people are decaying beliefs a lot, because in the belief based model, you're going to think, oh, this guy's going to pick 200, but after one period, they don't pick 200 anymore. In order for your beliefs to match what people actually do and then generate good choices and then fit the people's choices, you need a pretty decay rate that's much less than one.
00:29:20.594 - 00:29:34.130, Speaker A: Okay, so in this case, you believe model is the best possible, the best matching of your model with, let's say, delta equals to one and the other parameters. Okay, exactly.
00:29:34.162 - 00:29:53.394, Speaker B: So, yeah, essentially it's delta equals one, and then all the other parameters are inherited. Yeah, exactly. So our general approach is to have a pretty. A pretty busy family of models with different parameters. And then you just clamp, like belief base is just clamping delta equal to one. Right. And reinforcement learning is just clamping delta equal to zero.
00:29:53.394 - 00:30:36.642, Speaker B: Yeah. Okay. So there's belief based. We just saw this is choice reinforcement with payoff variability, which means that you kind of scale payoffs by variance, which seems to often be important and seems to correspond to neural processes, too, of what's called efficient coding. And this is terrible because in choice reinforcement, if you never picked 80, it's hard to learn to pick 80 unless you add in something else that's a bit ad hoc, like spreading the reinforcement to neighboring, what's called stimulus generalization in animal learning, spreading reinforcement to neighboring numbers. Right. So you reinforce 150, but you also reinforce 149, 151.
00:30:36.642 - 00:31:11.788, Speaker B: And so what happens is the data moved down away from the high numbers, but way too slowly, and it doesn't build up the choices of 80. It does build up choices of 80 from nothing to something, but it does so very, very slowly. And so by the last period, only 15% of the people play 80, and in the data it's 80%. So reinforcement learning, this is often the case. It moves in the right direction, but it's just too slow. It's just too slow. And again, we're trying to not just say, simulate thousands of reinforcement trials and fit it and look at the data.
00:31:11.788 - 00:31:54.072, Speaker B: We're trying to actually match period by period. Like after ten periods, what do people do? What is reinforcement say they would be doing under optimized parameters? Optimize for a fit. So reinforcement goes in the right direction, but it's just not quantitatively close. And this is a version of EWA that has a few extra twists and that does great. And again, it's sort of exploiting the fact that it uses the engine of belief learning so that you can learn to play low numbers even though you never played them before. Right. Which reinforcement can't learn, but because of the way the scaling is done, that's not in standard fictitious display.
00:31:54.072 - 00:32:40.080, Speaker B: It's. It also can produce really hard, like a hard stick where a lot of people are picking one number after ten periods, which is what happens in the data. The convergence of natural galbarine is very reliable. If you do it session after session, you'll see the same thing with different groups of people. Okay. Okay, next topic, strategic teaching. So one of the things that always haunted us and was really kind of embarrassing was these simple models like reinforcement learning was developed to understand animal behavior, including humans.
00:32:40.080 - 00:33:24.040, Speaker B: And fictitious play was designed to understand convergence to games, to give us a way to kind of think about almost like a mental to originally, but neither of them really take seriously the fact that you're playing another human being who's thinking and learning. Right? So you could apply fictitious play belief formation to learning about states of nature. Right. Like to learning in a bandit task where the bandit rewards are not produced by a human being's behavior. So we really need some theory, I think, in which the fact that humans know they're playing other humans who are learning is part of the theory, which again, somewhat embarrassingly, is not true. In the reinforcement and even the EWA implementations. Right.
00:33:24.040 - 00:33:54.660, Speaker B: Kind of like it was, you know, I don't know. I'm not that embarrassed. But it was a missing ingredient. So Fudenberg and Levine and later Joel Watson and others developed quite a bit of interesting theory on this. And they coined the term strategic teaching, for when an agent's actions create an immediate payoff, as well as influencing the learning of another agent. Right? Like, if you think the other agent is learning from fictitious play, you want to, like, create a belief about what you'll do, which will give you a bigger payoff in the future. Okay.
00:33:54.660 - 00:34:34.672, Speaker B: And the paper by Hampton Beausarts and O'Doherty and PNAs, actually, they call it influence value, which is a nice term. Also, the influence value is this special component of future payoffs from this teaching influence. Right? And they actually have evidence for that in. In the brain. And it's a very beautiful paper. And also, this can help us understand so many things, like bluffing, right? Like, if you're a really good poker player, you want to create almost like, a Persona, you know, where you're playing in a particular way to get somebody to think that you're going to continue to do that in the future. Or think about, like, a con or a sting, right, where you want to.
00:34:34.672 - 00:34:52.728, Speaker B: Um, often in these long. What's called a long con, you actually enable. You get. You get people to make bets where they win some money from you, and they think, oh, we're going to win even more in the future. And then in the future, you create the sting and you take their money and they get nothing. So this idea of strategic teaching is so interesting. It's so interesting.
00:34:52.728 - 00:35:30.674, Speaker B: And it came to game theory quite late, you know, and there hasn't been much research on it. So, sadly, I'm just going to show you one study of ours from quite a while ago, from ten years ago. Okay? So this is a study about what's called a repeated trust gain. Tragically, we did not use the word trust in the title of our paper, which was a colossal mistake in 1988, because later people came along and said the trust gains. But it's a repeated trust gain. You'll see why when I walk through the details. So the idea is that there's a single borrower and then a series of eight lenders, and each lender just plays one, period.
00:35:30.674 - 00:35:58.984, Speaker B: But they know all the previous history. Okay? And the idea is the lender can give you no loan, and then nothing happens. Everyone gets ten. That's the bottom row of this table. Or the lender can lend and if you repay, the lender gets 40 and you get 60, the borrower. But if you default, the lender loses 100 and a so called normal type borrower gets 150. So there's a big motivation to default.
00:35:58.984 - 00:36:39.874, Speaker B: And final twist is there's two types of borrowers. There's what we call an honest borrower and the honest borrower basically gets zero if they default. So this is somebody who has a conscience or feels guilt or something like that. Okay, so if you're, suppose you're a lender and you're in position number six again, you're going to do eight of these in a row. You've seen five times that there were loans and repayments. But you know that the, the 8th is the last period and so there's a fair chance that in the last period, in the 8th, they're going to default. Do you lend or not? And it depends upon what is called reputation, which is basically the perceived probability based on the history that the borrower is an honest type.
00:36:39.874 - 00:37:11.852, Speaker B: Right? Because the honest type, in fact, I think the way we did the experiments, the honest types, they were constructed, you have to always repay. You have to always repay. It's a dominant strategy because you don't get any bonus from defaulting. And so the reputation is going to be the probability that you're honest based on what you did in the past. And we did this in the eighties because there was a real boom and interest in these reputation modeling in repeated games. It was an extremely interesting topic. It never got much traction in decision or science or even too much in experimental economics, although we worked on it.
00:37:11.852 - 00:37:45.816, Speaker B: And then John Cagle wrote a couple of beautiful papers on entry deterrence, which is a very classic problem in industrial organization. And so there were six or eight really beautiful papers on this, very prominently published. This one in 1988 was in Econometrica. And then somehow interested in this class of games just fell off, even though I think they're extremely fascinating. Okay, so this is a sketch of this sequential equilibrium. The idea is that, remember on the x axis is time one to eight. Nine is basically after the period is over.
00:37:45.816 - 00:38:24.254, Speaker B: And the y axis is a probability of honest being an honest type. So the types are drawn, each sequence of eight, like each time down the computer screen, it says, okay, you're a regular x type. Or it says, oh, you're a y type. You have to always repay. Okay? And it's independent each time. And in this case the probability of an honest type, the prior, which is known by everybody, is one third is one third, okay. And so the way it works is in the first three periods, if you're a regular type person who's tempted to default, but if you default early, you'll know won't get any more loans, right? You won't get a flow of these payoffs of 60.
00:38:24.254 - 00:38:56.974, Speaker B: So you should always repay, that's a one, always repay, that's a one, always repay, that's a one. Starting in period four, you should pay back with probability 0.81 and you should default 90% of the time. If you do that. If you do repay, the posterior probability that you're honest goes up, because 90% of those people default. So the probability goes up, which induces more loans. And gradually what happens is as you go along in this later part of the sequence of eight, you should increase the chance of defaulting and decrease the chance of paying back.
00:38:56.974 - 00:39:47.830, Speaker B: And if you get to the 8th period and you get a loan, you should definitely default. And amazingly, if you care to go back and read our paper, the data look quite a bit like this after people have played these sequences maybe ten or 20 or 30 times. So even though the mathematics that generates these predictions is very non obvious, I hope you can get the intuition that your probably default should be going up toward the end because the influence value reduces because you only have a couple of periods left, very much like repeated pristine dilemma. Right? You often see full cooperation until the last three periods or something like that. Okay. And so we developed a different version of strategic teaching that has two extra ingredients. One is alpha is there's a percentage of sophisticated borrowers, borrowers who think that they're teaching the adaptive learners.
00:39:47.830 - 00:40:32.086, Speaker B: They think each of these lenders is learning using EWA. And I'm going to kind of teach them what to expect in the future. And the other term is epsilon, which is the weight you put on future payoffs, which is influence value. So this is really a different model here. The sophisticated borrowers think, what should I do now to change their beliefs about me for the next person so I can make more money in the future? And it sounds a lot like reputation building, but it's essentially the same intuition, just in different mathematics framework. Okay. And this is eight separate experimental sessions with some variation in the probability of an honest type person existing and in the payoffs.
00:40:32.086 - 00:41:03.018, Speaker B: So you would expect some wiggle room. And the results are pretty messy, I must admit. Epsilon, in about half the sessions is very close to one, which means they're really looking ahead at all the entire future place. But in some of the other sessions, it's quite low, and we don't know if we're really not identifying this number really well, or are there differences in groups of people? Probably some of each. And the alpha value, except for 0.7, which is very high, is usually around. It's either very small or 20 or 30.
00:41:03.018 - 00:41:48.614, Speaker B: So in a bunch of sessions, it's around one third. In other sessions, it's around two, three. That means that a bunch of the borrowers are not doing this very sophisticated look ahead. Okay, one more fun thing related to learning, but I'm not going to show you the behind the scenes learning surgery. We also have a beautiful study, thanks to Chris Martin, on humans and chimpanzees. These are humans in a game preserve in Africa, where Chris Martin's advisor, Matsuzawa Sensei, who is a very, very famous primatologist in Japan, he basically oversees chimps in Nagoya in a lab. And they also have access to chimps in the wild in.
00:41:48.614 - 00:42:16.182, Speaker B: I think it's in Benin or gabon or someplace in West Africa. Okay, and so this is a hide and seek game, right? So one person wants to match, they're going to just use left and right screens. You see the chimpanzee right, he's looking at left and right boxes. They just use a touchscreen and they press left or right. The chimps are very smart. We share 98.7% of our genes with them.
00:42:16.182 - 00:42:45.262, Speaker B: They're our closest kin in the animal kingdom, genetically. And not only that, and we're their closest kin. So chimpanzees are more closely related to us genetically than to orangutans. And so they can perform tasks, I think, similar to maybe three or four year old children, except for speaking. They have pretty good working memory and everything, and they're very trainable. Okay, so each of these dots is one person. Each of these triangles, rather, they're triangles.
00:42:45.262 - 00:43:14.978, Speaker B: And what this is going to show us is the probably that the match, the matcher chooses right, the right box on the x axis. The probably the matcher, the mismatcher, chooses right on the y axis. Okay, so in this particular game, the payoffs are asymmetric. So the Nash equilibrium is that the matcher should choose right 50% of the time. The mismatchers should choose right 80% of the time. That's the mixed, unique, mixed equilibrium. Okay, I'm sorry to not show you the game matrix.
00:43:14.978 - 00:43:44.040, Speaker B: And what you see is the chimps, which are yellow and are very close to the Nash equilibrium. Right. They're basically around 50% matching, choosing right 80% mismatchers choosing right. And the humans. We have two samples of humans, which is japanese people in Nagoya, and Africans shown in this picture here. And the Africans, by the way, were paying for money. And the money we paid was, they're not well paid, they're low income.
00:43:44.040 - 00:44:11.406, Speaker B: So the money we paid was similar to, like, two days wages. So they're highly motivated, and the chimps play for food rewards. Food rewards, okay. And what you see is that the chimps are closer to natural librarian than the humans. Humans are basically just around 50 50. They're a little bit in the direction of the Nash equilibrium, but they're close to just 50 50 matching. And if you take just the chimp data, we have three different parameters, versions of the game.
00:44:11.406 - 00:44:48.716, Speaker B: So the naturally, a shift from 50% mismatch or right to 75, and then to 80. The squares of the Nash equilibrium and the triangles are the average chimp data, average across six chimpanzees. And I claim that this is the best adherence to Nash equilibrium in a mixed equilibrium game ever seen among any species. It's really close, don't you think? It's really close. And there's only one deviation out of the six comparisons. Three games, and then two distinct strategic roles. Match and mismatcher.
00:44:48.716 - 00:45:16.288, Speaker B: There's six possible numbers. Only one of them is off by about 0.10. They're playing right a little less often than predicted, and the rest are within 0.0101. If you look at our paper, it turns out the reason is that the chimps learn better. They basically respond to the previous history more than the humans do. And again, we don't think it's just motivation, because the african humans were playing for a lot of money. Somehow the chimps just get into gear.
00:45:16.288 - 00:45:53.934, Speaker B: And our papers is a kind of real science paper, and we talk about. Matsuzawa has this idea, what's called the cognitive trade of hypothesis. Basically, when kids are two or three years old, they start to play with other kids and they learn language, and that sort of, like, crowds out other abilities, whereas the chimps continue to play hide and seek and other games that involve status, including something called sneaky copulation, and you can guess what that is. The chimps continue to play hide and seek type games. They're not talking to each other, and they're not cooperating. What they're doing is playing hide and seek so they have more evolutionary history. That's the idea.
00:45:53.934 - 00:46:18.934, Speaker B: Finally, here's another learning in the field paper. It's basically an agent based model of drug trafficking. And all you need to know, really, is that the data are in these red lines. And then the model is. It's a very complex model, but it does have EWA learning. So the drug traffickers and then law enforcement both learn from drug busts, like where to avoid trafficking your drugs. And this is a map.
00:46:18.934 - 00:46:38.474, Speaker B: I just love this graphic. It's very busy, obviously, but this is Guatemala, Honduras, Nicaragua, Costa Rica and Panama. And here's the cute thing about that. Here's the paper. There's one other author who's not listed here. Think about why that might be. They don't want to die.
00:46:38.474 - 00:46:43.874, Speaker B: Okay, that's it. Thanks very much.
00:46:51.854 - 00:47:02.866, Speaker A: Okay, we're opening up for questions. What do you see as the future.
00:47:02.930 - 00:47:52.784, Speaker B: Directions of this line of research, and what are the kind of goals or applications? Yeah, good. So one thing is, I would love to see more applications to field data. Again, even just a simple distinction between reinforcement of what you actually did, what you personally experienced, versus something you hypothetically kind of you think about could be quite important. For example, in my colleague Lawrence Jin and Nick Barbaras, who are finance professors, have a paper applying this to stock market investment. And it seems like this model free, model based distinction is quite useful there in explaining some things that are puzzling about stock price investment. I think the industrial organization, when you have entry into a new. There actually are a couple of papers on entry into new businesses.
00:47:52.784 - 00:48:29.476, Speaker B: And what businesses succeed and fail, and how does learning work would be very interesting. Probably the biggest, really embarrassing omission is that in these class of games, every game that I showed you, it's the same game. And every period, you make a choice, and the choice generates a payoff. But that choice doesn't change the game configuration at all. Right? A much more interesting general model is when the choice you make, for example, in the cocaine traffickers, suppose there's a big drug bust. What happens with a big drug bust is the traffickers lose. The law enforcement wins because they get bigger budgets.
00:48:29.476 - 00:49:04.406, Speaker B: And in addition, some law is passed that changes all the structure of the gangs. Mercenary Nagel has. And I think Reinhard Zelton have a couple of papers on this. But we'll, we rarely study that in experimental economics, where the action choices everybody makes actually then changes the structure of the game. Like a new entrant occurs, or there's a regulation, there's some tax. And I know that in Q learning and lots of other areas in cs, that's more like the standard model. In other words, you're interested in not just rewards, but state transitions.
00:49:04.406 - 00:50:02.464, Speaker B: So an actual generic state transition. And that's something we just haven't tackled. And the second thing is getting back to reputation. So I think the reputation modeling is, it's very well known in economics, and it worked out a lot of applications for things like entry. There's been a little bit of work in political science. Think about conflict like Putin in Ukraine, right? Is, you know, what type of reputation are people trying to build for future, you know, for future that has future consequences? How does irrationality enter into that? If Putin is really sick, as has been rumored, so that the time horizon is short, that's like the 8th period of the lending game, right? Who knows what might happen? And there hasn't been any decision. Neuroscience, I think we're going to start to work on that this year to try to see, because our view is, if we understand the behavior and the behavior is interesting, we're going to find something interesting in the brain, something kind of clear and something interesting.
00:50:02.464 - 00:50:48.714, Speaker B: And we also have. I know most of you aren't as interested in the brain as I am, but we also have a lot of causal tools. Like, if you can identify a brain area like dorsomedial prefrontal cortex, which computes influence value, that's what John Odoherty found. We can actually up and down regulate that with electricity, with something called tdcs, transcranial direct current, and we could basically create more activity in that region, or less activity. And we might be able to make people more reputation minded or less reputation minded. And that would be just interesting to do. It's sort of like another check that the model's on the right track, not just computationally, in terms of the numbers, but also in accounting for brain activity and causal effects.
00:50:48.714 - 00:51:29.654, Speaker B: Another thing, a number of people interested in individual differences. I'm not that interested in individual differences. You know, are there people who probably IQ is associated with being more model based and being more in the direction of fictitious play? There may be differences in life cycle, you know, like adolescents start to become more sensitive and become more fictitious play, like, you know, after being reinforcement when they're little or something like that. And quite, quite a bit of that going on in neuroscience, using, not using the fictitious display and reinforcement structure, but using model free, model based.
00:51:37.194 - 00:52:03.352, Speaker A: Other questions. I have a question, actually about these parameters. So, yeah, is there, are these parameters persistent in regards to a single individual from one type of game to another, or are they dependent on the actual game that I'm playing? So let's say if I would play rock, paper, scissors, and then, you know, I come back to your lab, and I play, like, coordination game.
00:52:03.408 - 00:52:04.032, Speaker B: Right.
00:52:04.208 - 00:52:11.976, Speaker A: Would you strongly. You know, would you expect me that you have this? I would have the same feat. And have people actually studied this?
00:52:12.160 - 00:52:40.364, Speaker B: Oh, yeah, yeah. No, that's a very good question. Yes. Um, in psychology, that's basically called states and traits, which basically means, are you, you know, are you a certain type of person and you play every game in a certain way? Like, are you a fictitious play type or your reinforcement type? Or it could be that. That different games elicit, you know, behavior, which is more fictitious play like, or more reinforcement like. We've studied it a little bit. We, um.
00:52:40.364 - 00:53:16.874, Speaker B: We haven't studied it very much, but a few people have tackled that. You know, you get a group of people, and they play three or four different games, and do you see general tendencies? Mostly, there's some general tendency, but it's kind of modest. In other words, the correlations, the types across measured types across different games, or parameter correlations, like delta, might be like, 0.3, but that's true of a lot of psychological traits. You know, they're not always extremely. They're not extremely correlated across domains. Thank you.
00:53:17.574 - 00:53:49.674, Speaker C: I'm wondering, in the monkey hide and seek example you showed, it seems like still the humans are playing very similarly. And I guess, is there any way to actually define a solution concept mathematically that would predict how humans would play? And would that be really. Would that, like, solution concept have a notion of stationarity, just like the Nash equilibrium does? Or do you have to, like, consider those modeling those problems through the dynamics?
00:53:56.574 - 00:54:31.072, Speaker B: Sorry, I'm not 100% absorbing your. I have to go to another call. I'm not 100% absorbed here question. Generally, we haven't explored that a lot. There's a lot of theory, like Futurama. Levine have a beautiful book on this, which is now 20 years old. There's a lot of theory answering that, trying to answer that question, which is what learning dynamics generate, what types of equilibration, and I think it's the case that fictitious play will generally converge to certain sets of equilibria.
00:54:31.072 - 00:55:03.948, Speaker B: Again, it may not. May be a broader set than Nash. And I haven't really studied that. I mean, it's not my comparative advantage to study that problem. It's. Other people are much better at figuring that out. I mean, I will say just kind of intuitively, that in almost all the data sets we've used, which ranges from a bunch that you saw today, including chimpanzees and field data, like the lowest integer game in Sweden and then this cocaine trafficking game.
00:55:03.948 - 00:56:05.264, Speaker B: And we have some data from movie disclosure of movie reviews, and we have a lot of types of data. And there's almost no game so simple that people will play equilibrium right from the beginning unless they're very sophisticated and maybe they talk about it, something like that. And there's no game so complicated that you can't get close to equilibration, even with a loopy game, which is quite difficult to solve mathematically. So that it's just a kind of folk theorem, basically, that the forces of equilibration are pretty strong. Again, there may be, like, diabolical examples like Shapley introduced a game with a mixed equilibrium in three strategies in which fictitious play would just cycle around. And that that basically caused game theorists for 20 years to not sort of give up on fictitious play until it got re. The debate got reopened by Dave Krebs and Drew Fudenberg and a lot of others.
00:56:05.264 - 00:56:23.688, Speaker B: I wish I had a more satisfying answer, but it's just. It's just something I don't know. I don't know much about it. Is lunchtime.
00:56:23.816 - 00:56:28.604, Speaker A: Yes, it is lunchtime. So once again, let's thank Colin. Thank you very much.
00:56:29.944 - 00:56:30.424, Speaker B: Thanks, everyone.
