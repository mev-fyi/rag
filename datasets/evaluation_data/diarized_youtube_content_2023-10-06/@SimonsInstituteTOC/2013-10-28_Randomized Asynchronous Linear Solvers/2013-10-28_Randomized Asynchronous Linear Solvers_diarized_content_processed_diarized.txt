00:00:00.640 - 00:00:04.334, Speaker A: And he'll talk about randomized asynchronous biotechs.
00:00:04.494 - 00:00:26.914, Speaker B: Thank you, Ali. And thank you for inviting me to come give a talk here. So I'm going to talk about asynchronous linear solvers for symmetric positive definite matrices. And I want to start with some motivation why we are looking on asynchronous methods. So I'm going to give two motivations. One is more related to applications and big data. And I'm going to ask you a question.
00:00:26.914 - 00:01:01.956, Speaker B: We want to solve ax B. A is symmetric positive definite. It's large, it's sparse, and we want to use an iterative method because the matrix is too big to do a direct method and does structure that is not very good for direct methods. And the question should we use Gauss Seidel or CG? I mean, Gauss Seidel is a very old technique. It was invented based on relaxation or fixed pro iterations. It invented before CG. And usually it's not much use today, because if you look at the theory, you know that CG will converge in square root of the condition number.
00:01:01.956 - 00:01:41.220, Speaker B: There's also dependence on the epsilon, which is log one over epsilon, which I'm kind of ignoring now. But Gauss Seder, if you take a randomized version of it, not the classical one, which I'll give details on the randomized in a few slides. If you look, if you do the convergence, only the time, the amount of iterations is proportional to the condition number. So if the condition number is big, this can be a big difference. So why would you want to use a Gauss Seidel? Well, you then try it on a matrix. For example, we have big data. I'm taking for, I took a matrix from big data, and this is a matrix from doing some regression on social media data.
00:01:41.220 - 00:02:27.864, Speaker B: And in general, the results do match the theory. You really see that CG is progressing much faster. I didn't continue beyond 200 iterations, but it's clear the trend. I mean, if you continue to write, I mean, CG will continue, will bypass the randomized gaussian area, which is stuck. I mean, it's on slightly below ten to the power minus three, it will continue to go to very high accuracies. But what's interesting is that if you look at the star, you tend to see the Gauss Seidel based method, this very cheap small iteration method, they tend to make a lot of progress early on. And what happens is that in many applications, specifically for the application we were looking on here, this is all the accuracy you need.
00:02:27.864 - 00:03:02.034, Speaker B: If you go beyond that, you're not getting anything in terms of the downstream use for how this solution is used. So if you do just ten iterations, take this solution, you get all you, you can opt to get from solving this equation. But if you do ten iterations of CG, you're up here and you lose a lot in terms of the iteration. So you need to continue up to here. And if you continue here, of course, you're not getting anything. So we really want to be around here and to take this measure that does a lot of fast initial progress. But of course, as Sivan mentioned, to have good software, you need to paralyze that.
00:03:02.034 - 00:03:53.270, Speaker B: And there's quite a few ways to paralyze it. We looked at an asynchronous way to paralyze it, which is, I think, quite easy, an easy way to do it, and we can show that it gets you good results. So I'm going to give you a second motivation, and this is more like well known trade. It's not like, not a concrete example of an application is that cpu clock speeds have reached the limit and we have to use more processors. But as we get to really large processor counts, we know that there's a problem with communication between those processors. And specifically there is a problem with scalability because of synchronization points that if you have p processor, you do need to do order of log of P to just to synchronize them. So if P is very large and you are paying a lot for desynchronization points, and because of these trends, asynchronous methods are becoming more and more attractive.
00:03:53.270 - 00:04:42.804, Speaker B: You do want to avoid desynchronization points as you go to higher processor counts. So asynchronous numerical methods are pretty old. They're probably the oldest. Well, the idea is basically, I must say, that is to avoid the idle time, but just letting processor progress without explicitly synchronizing between them, just using maybe information that is a bit old. And the idea was perhaps first suggested in 69 with chaotic relaxation. And since then it had been applied to many, to a lot of applications, but they were never really considered mainstream and not very popular compared to a method like CG or, or other things that are synchronous. And there might be quite, there's a few reasons for that, and I'm going to give you my take on it.
00:04:42.804 - 00:05:23.406, Speaker B: The issue is that the analysis is usually based on fixed perpetration, so you can only prove a convergence in the limit. And that's a little problematic because you don't know how it behaves in practice. And really, there's examples of matrices where Dasyncles method converge very slowly, so we don't understand about them. The concurrency is usually not enough to see any benefit, but they are much slower than a single counterpart, so it's not very attractive. And finally, that's even more severe. Restriction problem is that the traditional method often place very severe restrictions on the matrices. They can prove convergence.
00:05:23.406 - 00:06:11.276, Speaker B: For example, diagil dominance, like Sivandi discussed in the previous slide, is one of those conversations conditions where we know where it was known that various asynchronous methods converge, but more general, like take symmetric positive definite matrices without any structure, then there were no known algorithms for that. But now there's more recent work, and I'm going to give detail a few that are relevant. There's some work on, in a distributed setting, on agents, that's not like the talk yesterday. That's not the setting we are talking about. It's a different setting. We are looking at a shared memory or distributed memory setting. But from that line of work, there is one method that is more related to what we are presenting, and it's on synchronizing clocks based on the Kartma's method, the same method Sivan discussed in previous slide.
00:06:11.276 - 00:07:27.310, Speaker B: And that's some similarity to the method we are going to describe, because we are using Gauss Seidel, which is related to the Kartmas. Much more relevant for this talk is the recent work on Hogwarts, which is basically asynchronous parallelization of stochastic gradient descent. There's a convergence rate analysis, you can say how the asynchronous effects for shared memory model, and our analysis is inspired by this, but it's significantly different, and we have much better convergence for this specific problem of solving linear equation. So I'm going to give details on a nice thing or solver for general schematic positive definite matrices, we can show it converges in a provable rate and it's condition number, it's linear and it's a randomized algorithm. So all arrow bounds are going to be an expectation. So I will start with some background on randomized gal what that means, and then give the algorithm some experiments. So let's start with randomized Gaius ten and I will explain that using some, basically a very simple iteration, an almost obvious one, especially after the first talk today, which is basically doing coordinate descent, maybe doing coordinate on this function.
00:07:27.310 - 00:08:09.688, Speaker B: But let's start with a more generalized setting where we have a and x and we have some direction vectors d, and we always want to do steps in those direction vectors, which I'm not specifying them yet. So we do this kind of equation where we have some step size gamma sub j, and we have this formula where comes this formula, this is the formula that will minimize the error in the a norm of the next step. This is an analytic equation. If you just do that, that's the minimization of this one. So it really makes sense if you are greedy to do these steps. If you are select a direction just to use these gammas. But the question is of course, how to select these direction vectors.
00:08:09.688 - 00:08:58.175, Speaker B: And actually most methods to do solve iterative solving equations can be cast as this kind of iteration, even cg, just as a clever way of selecting these direction vectors. And one simple way to select them is to just do around robinhood coordinates. Just look at the identity vectors, where you just take the first identity vector, the second identity vector through force and cycle. And this, if you kind of look at the equation long enough and try to figure out, you'll see that this is equivalent for any iteration of the classical algorithm of gaiusadella. It's a different way to describe Geisel, but it is an equivalent one. And what's nice about these direction vectors is that the iterations are very small and cheap. You need only one entry of the residual of this guy of the difference between b and a times x.
00:08:58.175 - 00:09:49.976, Speaker B: So you need only one row to look at one row of the matrix to compute this guy. And then you can do this step very efficiently. Okay, so how this works? Well, to analyze it, you can analyze using classical techniques in fixed punctuations, but if you want. But a new, somewhat more attractive way that relates things to a condition armor is to look at a randomized version of it, where you select the direction vectors, not in a cyclic way, you kind of select them with some probability. The probability should be proportional to the, to the diagonal values. And then if you do, if you use this kind of way to select the direction vectors, you can bound the expected a norm of the arrow after m steps, using basically reducing by some factor that is smaller than one. So you have a linear convergence.
00:09:49.976 - 00:10:32.654, Speaker B: But what really governs the convergence is this ratio of the smallest eigenvalue and the trace of the matrix, which is the sum of the eigenvalues. So the running time is going to be proportional to the inverse of this guy, which is the trace divided by the smallest eigenvalues. And since the trace is bounded by n times the largest eigenvalue number of iterations is bounded by n times the condition number. But n iterations is about equivalent to one matrix vector multiplication. So this is equivalent to basically order of the condition number of doing as much operation in CG here, which basically the matrix vector is the most expensive operation. So this is the way to analyze. This is the analysis.
00:10:32.654 - 00:11:10.442, Speaker B: And I'm going to go quickly through the analysis because it's going to be useful later on when we analyze the asynchronous method. It's a very simple analysis. It can be put in one slide, but there's a few more details maybe. But basically this is all the analysis, basically. Now we are going to assume that from now on that a as a unit diagonal, we can always achieve that by rescanning the matrix. And then basically we want to look at the step size, which is basically this expression with equal probabilities. And then you can just do very simple algebraic manipulation so that the a norm after j plus one iteration is related to the a norm after j iteration minus some term that is positive.
00:11:10.442 - 00:11:34.760, Speaker B: So it's positive. That's what we progress. But again, now this is random variable, so we can fix the residual. It's basically the average entry in the residual. But this guy, we can always bound using the eigenvalues like this and then take expectation of both sides. And then we get the bounds on the expectation on how much we progress. And we take the lower bound and plug it in here and just get the bound we described.
00:11:34.760 - 00:12:08.290, Speaker B: And then we also given an upper bound, we can bound how fast we can progress. This is of course not something you usually want to do, but when you are going to analyze that sinkhole method, this is going to be important, actually. Okay, so now we're going to get to the asynchronous version. So before that I'm going to give some pseudocode of the non synchronous method. So this is just a pseudocode basically at each you select a random entry, you read the entries of x that you need to compute the step size. You compute it and then you just do the update. And now I'm going to describe the asynchronous version.
00:12:08.290 - 00:12:51.104, Speaker B: It's a very small, very easy variant of this. Basically I'm just taking this algorithm and I'm executing on all processors, and all processors are going to share the vector x, but all of them are going to do not share anything else except of the same shared x in shared memory that they are going to operate. So this is the algorithm. I said all I process, I execute the same algorithm and I'm going to do two small modifications of the algorithm. First of all, I'm going to require the updates here to be atomic. So you can do that usually if you use an outworld, but it means nothing can interfere in the middle when you are doing these operations. And I'm also going to add a small step size here, beta, which has to be smaller than one to show convergence, but it can actually be quite close to one.
00:12:51.104 - 00:13:38.760, Speaker B: So this is just two small notifications. And now we want to analyze this. So to do that we need to write iteration in some meaningful way. So the writes are atomic, so we can order the updates in some order, some serialization of the updates. We can write some fixtures, iteration x sub j, which is basically all the updates that happened up until after j iterations, basically some linearization. And now what happens in the asynchronous method is that when you are reading the entries here on the line four, some of the updates that happened before that are going to to be read. You're reading the vectors, you're going to see updated values and some are not.
00:13:38.760 - 00:14:32.304, Speaker B: Some maybe updated, you read them before they were updated or something like that. So you can always describe a set of k, of j, of updates that are consistent with the vector you read. If the vector you read must be because the operations are atomic, must be consistent with some set of updates. So you can have some maximum set, and then you can always describe your iteration in terms of this additional kind of non existent iterations that are dependent on set of updates. You can describe X after a specific set of updates as a curl, as basically just taking these updates. And you can describe your iteration as in terms of the residual on that iteration that actually never happened, but you can describe your iteration in terms of it. So you can write some useful expression for how how X J plus one looks as a function of X.
00:14:32.304 - 00:15:20.444, Speaker B: This doesn't work anymore. X J. Okay, now you want to analyze this and of course you have to make some assumption, because if you allow some delay, some updates will never be read. They will have arbitrary slow convergence. So you need to assume there is some bounded asynchronism, which we say we have a parameter tau that really says that all updates that are older than Tau will be read when you will be read at a specific iteration, which is this is just a formal way to say that every iteration that is on enough will be read. And of course you need another assumption is that you're not really completely adversarial. Because if you kind of set the kjs when you know the random choices here, then that can really mess up your convergence.
00:15:20.444 - 00:16:00.574, Speaker B: But it's not, it's not a realistic assumption. So we allow kjs to be arbitrary, but they don't, but they cannot depend on the actual random choices. Okay, so now I'm going to be slightly more technical in the analysis in a few slides. I apologize for that. So we can do the analysis starts by just doing some algebraic manipulation to write there or after j plus one iteration as a function of what happens after j iterations, plus some progress here that is always positive, and then some additional terms that isn't always positive. That's something that really comes from diosyncrasy. It measures the vector that we would have wanted to see and the vector that we actually see.
00:16:00.574 - 00:16:35.354, Speaker B: So we have some progress that is positive, but this can be negative. But the idea is that since these two guys are different by only tau coordinates and matrix a is also sparse. So this guy can be, in many cases will be zero because the entries that are non zero here might not be the entries that are non zero in that specific row, because we are selecting a specific row. So with that probability this guy is going to be zero. With other probability it's going to be actually bad for us. It's going to slow us down. But the idea is to try to bound this negative effect.
00:16:35.354 - 00:17:14.954, Speaker B: So now you of course take expectation and you have this expectation, you have reduction in the expectation and, and some term that now is positive because we take an upper bound. And this guy is, depends on basically on this thing. That is a measure of basically how sparse the matrix is relative to the size because we have one over n here. But this guy is always smaller than n. But if the matrix is sparse, this guy is going to be one over n. So we have a very small, very small component here. Another nice thing is that the progress is proportional to beta, but the damage here is proportional to beta squared.
00:17:14.954 - 00:18:13.150, Speaker B: So we can always set beta to be small enough that this guy will be bigger than this guy. But the final observation is really useful is that the guys that appear here in the expectation are basically guys that appeared in previous iterations because this one can be bigger than this one, but it's always much smaller because of these coefficients than the progress we made at previous iterations. So we kind of need to basically do kind of compensation with the progress made in some iteration by the damage that is causing this iteration. The way you can do that is basically now take a sum, just aggregate, write the error after k iteration as a function of the first iteration and all the progresses that were made. And then we can bound the progress by some lower bound and get this expression on the expected error after amitration. This is still a little difficult to analyze because we have here only upper bounds on this guy. So we cannot really show enough progress.
00:18:13.150 - 00:18:44.364, Speaker B: But the way you need to do this is, again, like I said, you need the upper bound. You need to show there's enough progress because you only have upper bound. You need to show that the progress is not too fast. So, because what happens is that since your progress is proportional to the error after, in the previous situations, if these guys are too small, then you are not showing any progress. So we needed to bound these guys from below. That's came, that comes from the slide before that. Earlier on that I showed some lower bounds on the, some upper bounds on the rate of the progress.
00:18:44.364 - 00:19:26.550, Speaker B: So use this guy and you can show that after you did enough iteration, which is proportional to the number to n, then you have some reduction in the expected error by some constant factor. That depends on the condition. And some consider that really captures what the asynchronous does. So hopefully if row two is small, then this guy is going to be small. So if you do enough tiny steps, then the expected error stops, goes down, decreases by some constant factor, and then we can occasionally synchronize the thread in order to reduce the expected error. But of course we don't always really want to synchronize. This is like we synchronize every Ann iteration, which is not that bad.
00:19:26.550 - 00:20:06.104, Speaker B: We don't need to synchronize every iteration, but we are still going to synchronize about the same amount of time as we are going to do, like CG. So you can walk a little harder and I'm not going to give the details. And you can show, you can show a bound without any additional synchronization. The terms are much more complicated. So this is the results and it appears in the paper. And now we can do additional assumption if you really want to improve. So here you have a dependence on tau squared, which is basically the square of the number of processors, and here you have enough tau cubed.
00:20:06.104 - 00:20:55.620, Speaker B: You can improve this if you kind of infer somehow that you read entries that are always consistent, that the interests that you read are consistent with some iteration, then you can show an improved bound, which depends only on tau. So you are linear in number of processors, and then you basically have a weak scaling regime. But of course enforcing this is, can be a little hard. It might, it might be maybe some hardware that allows you to do that, maybe transaction memory, but it's still not easy to enforce this. But this analysis is not completely useless in the sense that because the matrix is very sparse, usually the entries that you read will be consistent with some iteration. So the real behavior, if you don't do anything, the real behavior will not be this, but it will, but sorry. It will not actually not be this as well.
00:20:55.620 - 00:21:24.372, Speaker B: So it will be something between the two. But in terms of the bound, you have to make this assumption if you want to throw a stronger bound. Sorry. So now some experiments, this is again the same matrix before rising from social media analysis. And we use a single blue gene Q node that has 16 cores. Each one of them can do four way upper threading. So there's total of maximum amount of 64 threads that we can spawn here.
00:21:24.372 - 00:22:01.216, Speaker B: And now we take this matrix and we run it. And again the y axis is the running time and so lower is better. And it's a logarithmic scale and also the number of processors scale. So we see that the asynchronous method, as we expect, because everybody's independent, really has linear speed up and it continues to around 48 times faster at 64 thread. And consider that some of these threads are virtual. I mean we only have 16 actual cores. So this is pretty good speed up.
00:22:01.216 - 00:22:43.118, Speaker B: CG is actually parallels in pretty well as well. But as the nava processor gets up, you kind of see that it deviates from the linear speed up and it, and if you poly, if you add more threads, then you can boil, tends to flat out. And at 64 thread the asymmetry is twice faster. And here I'm doing ten iteration because that's all we need for the application and some. Okay, so are you paying that in terms of the residual for using an asynchronous method? Well, you are paying something, the theory shows that, but you're not paying a lot. So the black line is what happens if you just use randomized geicidell on this matrix. No asynchronousity.
00:22:43.118 - 00:23:17.924, Speaker B: You do asynchronous method, it's 48 times slower, but it gives you slightly better results after ten iterations. It gives you around ten to the power minus three. And the asymmetry is slightly higher, but it's still the same order of magnitude. It's not a big difference. That will really affect how you're going to use the solution. So, conclusion we have a new approach for asynros linear solvers, it uses randomization to gracefully under the non deterministic asynchronous behavior. Now, if you look through the slides, we never made some assumption on the symmetric positive definite matrix.
00:23:17.924 - 00:23:53.190, Speaker B: We only assumed that it is fast enough so that there's not too much collisions when you're doing the asynchronous situation. But we are not imposing any structure or limitation on it. So we don't need something like diagonal. We can apply it to a wide range of matrices. And we just took one, I just took one application here that usually if you use traditional method, they cannot even show convergence for it. And for most matrices, okay, this may be a slightly strong claim, but for many matrices, it will converge. If the matrices are relatively sparse, it's linear convergence in terms of the condition number.
00:23:53.190 - 00:24:23.134, Speaker B: There's some slowdown due to the asynchronousity, but it's not too big. And for least square, you can probably generalize this to the randomized calculus and get similar convergence rate. And Sivan discussed tiny steps about vulparization, but asynchronousity can maybe under that. So that's why some of the idea of using asynchronous intricacy to use these tiny steps, but in parallel, basically. And that's all.
00:24:29.234 - 00:24:30.214, Speaker A: Questions.
00:24:39.634 - 00:25:07.744, Speaker B: On the randomized kakamas or so I, you're talking about something else asynchronous coded. So I saw, he gave some talk about that, but I never saw a paper, so I don't know the details. I met him a few in, in February, and you mentioned that he's working on that. Is this out? Is it on archive or something? Okay, so I look for it.
00:25:11.764 - 00:25:17.332, Speaker A: So, example, matrix, I think it is 1000 nanosils per row.
00:25:17.468 - 00:25:48.310, Speaker B: It is, yeah, something like that. No, no, this is a real application. So, no, no, that metrics, I wanted the metrics that it really made that really doing. I wanted the metrics that it makes sense to do this, weak iterations that really get the benefit. And I had some guy who did this analysis on social media. I cannot say exactly what the analysis is, but this is the metrics he had. And I said, this is the matrix that is good for this method.
00:25:48.310 - 00:25:50.054, Speaker B: And it was turned out to be good for this message.
