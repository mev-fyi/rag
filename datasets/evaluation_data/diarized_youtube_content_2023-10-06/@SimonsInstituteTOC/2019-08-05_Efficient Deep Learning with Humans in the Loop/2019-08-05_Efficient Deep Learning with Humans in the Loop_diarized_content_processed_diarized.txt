00:00:01.640 - 00:00:08.714, Speaker A: I guess part of the theme for today was the universe of deep learning. So I guess, oh my God, you have to put humans in the room.
00:00:09.494 - 00:00:10.246, Speaker B: The failure.
00:00:10.310 - 00:00:17.046, Speaker A: We're not failing enough. Fire powers combined. Yeah.
00:00:17.110 - 00:00:17.326, Speaker B: Cool.
00:00:17.350 - 00:01:21.024, Speaker A: So I guess today I'll talk about the great hope and my great disappointments also and various efforts to try to increase the sort of data efficiency of machine learning and calling upon our human friends to help us in that endeavor. So I'll discuss a few different papers. A lot relate to active learning. Some newer works try to maybe deal in a more creative way with ways that we could inject some kind of human interaction into a learning loop than just sort of like standard label querying mechanisms. And I didn't have enough time and to get thumbnails properly sized for every single collaborator. But I guess a lot of this work was worked on when I was working closely with Anima Nandkumar at Amazon AI and a bunch of great interns that we had. So check them all out.
00:01:21.024 - 00:02:17.988, Speaker A: More accomplished than I am shortly. So, yeah, the high level idea, deep learning, is great stock slide with list of successes. We all like it. There's a kind of narrative that we'd like to believe that algorithms play some role. Our creativity as computer scientists are somehow responsible for the great feats that we accomplish now, predictive modeling. And the reality maybe is that we owe as much or more to the availability of computation and large data sets to sort of explain the capabilities that we have now versus 20 years ago. And in particular, this talk, I'd like to focus specifically on the data side.
00:02:17.988 - 00:03:24.004, Speaker A: I guess the hope is, now that we have these capabilities, can we go back and achieve some of the same feats, but without relying on so heavily on such massive data? So if you look at a lot of modern systems right now, computer vision systems depend on millions, or I guess Facebook is pushing it now to billions of images. Sammy made a face. So it was insider information that Google is also using billions of photographs to train their models. ASR systems use tens of thousands of hours of annotated data. Typical named entity recognition systems use hundreds of thousands of annotated words or, you know, on the order of one to 200,000 annotated sentences. I don't know how many hours of self play alphago used, but it was a lot. And, you know, our goal here is just to say, can we, can we do something by being a little bit smarter that's less taxing on our interaction with the environment? And so there's a lot of different approaches people use to try to cope with data scarcity.
00:03:24.004 - 00:04:53.644, Speaker A: In particular, I'm going to focus on active learning. But just to give a little bit of a high level survey, some of the bigger successes in getting the kind of results that we depend on now include data augmentation. So we exploit sort of known invariances, transfer learning between related tasks, when the tasks are so related that they really have the same inputs and output spaces. We describe this as a domain adaptation problem, and in particular in the NLP world. Chris spoke a lot about various contextual embeddings, and now sort of semi supervised learning is a way that we're able to exploit a situation where maybe input data isn't scarce, but labels for our specific task is right. So the kind of invariant, the kind of, you know, considerations when we have to walk through, when we think about how are we going to reduce our data dependency, are, you know, what precisely is scarce, is it our data or is it just the labels? What invariances are there that we might be able to take advantage of? Do we have access to large labeled data sets for related tasks? Just how related? What is even the right notion of how related? Um, and just how large are those data sets? Um, I think a lot of these are things that are the bread and butter of the actual modern sort of practical application of, uh, deep learning. Although, uh, to a lot of these questions, we don't even know how to ask them properly, let alone answer them.
00:04:53.644 - 00:05:59.860, Speaker A: Like just what does it mean for tasks to be related for in the context of like modern transfer? And then, you know, ultimately what I'm going to try to, after a brief survey, if something's happening in the field, is dig more into this question of can we do anything? That's especially smart when we have access to annotators for the purpose of interactively querying labels. All right, so data augmentation, I think even at the time of the original image net paper was a kind of indispensable technique. Things that people are familiar with, sort of transformations that we know should preserve the label, we just dump them into our training set alongside the original image. Things like crop zooms, small rotations. There have been a number of papers, we had one, a bunch of other people have done similar things in vision. This is in the context of ASR, where we started thinking about maybe for certain types of augmentations. We have reason to believe that they shouldn't just get the same output prediction, but we believe that these are so similar that the model should sort of regard them as almost being the same image.
00:05:59.860 - 00:07:22.204, Speaker A: In this case, in the context of speech recognition, we were looking at noisy speech first, regular speech, where we had, you know, we have access to simple mechanisms for corrupting speech by this, you know, using additive noise, by either incorporating background music, telephony, various kinds of static. And we apply a penalty saying that basically at every single layer in the network, we would like not just the original version and its corrupted counterparts to have the same corresponding predictions, but they should actually have the same representations. In the context of transfer learning, there's a whole lot of work now based on this idea of pre train on some source data for which there's lots and lots of data. Fine tune to your target task, do this even when they're not the same task, just broadly, you know, they both happen to be audio, they both happen to be language, they both happen to be natural images. It's so strangely effective that I think, I mean, I guess now it's getting its due in the context of NLP. But until now, I mean, this was, for me almost like a big mystery in machine learning that I think if we had even the faintest idea that we could feel happy about the kind of success we're getting from this method, it would be like the biggest story in the world. Like, this really is the story of modern deep learning in a way that we are able to.
00:07:22.204 - 00:08:20.280, Speaker A: If you had anything else where you say, give me 100 images and I'll take you from 20% accuracy to 90% accuracy, I think we would have a parade. But there's something just so profoundly unsatisfying is that it all depends upon some mysterious properties of the initialization of these neural networks. And we don't have a faint idea really why it's working. We have some stories that we tell about the features being transferable, um, and these depend upon yet other stories about the features being kind of hierarchical. We're not quite sure how to kind of articulate crisply what we mean by this. Um, I spoke here last time about, um, you know, when we have really, uh, uh, strong assumptions about how the source and target domain are related and they're the same tasks, sometimes we could actually do things that are maybe theoretically principled and get out, uh, you know, models we can maybe be happier about. Um, and we talked a little bit about these representations.
00:08:20.280 - 00:09:25.552, Speaker A: So, so what I'd like to, you know, focus most of the talk on is, uh, now, you know, to, to what extent can we think of, uh, annotation intelligently as giving us some, some route to improve data efficiency of machine learning? Um, in general, uh, we'll be thinking about this kind of loop where we have a sort of human annotator and a model that's learning in a kind of dynamic cycle where we have access to some huge pool of unlabeled data, and we're going to make some kind of choice. We're going to have some selection criteria that's going to decide which examples to annotate next. We'll give it to the human. Human will give us a label. We'll add the newly labeled example to our label dataset, we'll update our model, and then we'll select again. And maybe what's missing from the picture a little bit is that usually our selection criteria here is somehow going to depend upon the intermediate model that we have after training on some subset of the data, there's a number of variations of the active learning setup. We can have this.
00:09:25.552 - 00:10:22.316, Speaker A: What we're going to focus on largely, and what a lot of the literature and sort of the deep learning, active learning setup focuses on is this pool based idea where we have this pool of unlabeled instances that we're selecting from. There's also variants where you think of as data arriving in a streaming fashion, or perhaps most interesting, but least explored, the idea that we would somehow create new examples de novo with the idea of probing the annotator with example that maybe wasn't given to us by God. But, yeah, I mean, I think the reason why the pool based setup is so appealing is because we can fake it. We just take any supervised data set, and we put on a mask and pretend that we don't see the labels. And now we say, I've got an active learning problem. I don't want this to be an evangelical talk, even though I mentioned God. So I'll talk a little bit about some problems with the whole setup, too.
00:10:22.316 - 00:11:04.560, Speaker A: And I think there's a danger of this covering your eyes and pretending not to see the labels on a dataset that you've already seen. Um, among other things we'll get to is oftentimes when that's a very popular data set or a benchmark data set, it's already the one that informed the choice of architecture that you have the learning rate that you have. Uh, so we're in a setting that maybe isn't the true setting that we would have if we were approaching a new problem. And if we weren't approaching a new problem, you know, then why the hell were we soliciting labels? So, um, some of the basic, you know, considerations that we have. So some of them are common to all active learning. Some of them are a little bit specific to the deep setting. One is we need some choice of an acquisition function.
00:11:04.560 - 00:11:59.348, Speaker A: So how are we in each round going to choose which examples to label? We need to choose. This maybe gets a little bit more specific about the deep learning aspect. How many instances from the unlabeled pool are we going to select for query per round? And in this case, the reason why it matters in the context of deep learning is just that retraining the model, when you're talking about giant neural networks, is often very expensive. So we have to actually take into account how many rounds of retraining can we afford to have. So whatever our target dataset size is, this is going to give us some trade off between how much retrain we're willing to do. To that end, we also have to think about things like, do we want to fine tune our models between rounds or do we want to train from scratch? There have been times where it seemed like training from scratch was better in terms of performance. But if we train from scratch every single time, then our experiments would take months to run.
00:11:59.348 - 00:12:45.774, Speaker A: So we have to do something absolutely horrible, like every K rounds we flush and retrain from scratch, or something like this. A big problem that may be a recurring theme about trying to get active learning to work is that it puts us in a setting where we have to get things right the first time. I know there's a lot of theorists in the room, but there's a conversation outdoors earlier that even the theorists are running experiments. So I guess a lot of you guys have tried to train a neural network at least one time in your life, and how many of you got it to work the first time and nothing works the first time. And so there's something extremely dangerous about supervised learning. Is this really kind, forgiving problem, because it. It allows you to.
00:12:45.774 - 00:14:11.932, Speaker A: Once your dataset's constructed and you've gotten your splits right, you could just mess everything up over and over again and get absolute nonsense results until you feel like you've kind of dialed in, you've found the right learning rate, you've said the right prayers. But this isn't the case in active learning if you have a limited budget, if your whole point is that I can only limit, I can only label some fraction of examples, then you put yourself in a tricky situation where what we do in active learning papers is we screw it up and we pretend we didn't. But in reality, if you screw it up and you spent your budget and you collected the wrong examples, it's over. Or if it's not over, it's because you lied and that wasn't your budget in the first place. In which case, why didn't you just go out and label all the data? So there's some kind of, like, there's a large body of beautiful theoretical work that unfortunately I have not in any way contributed to on active learning in kind of restrained settings. When you make strong assumptions about the model class or kind of data dependent assumptions in the sort of grimy empirical world that I spend most of my time in, there's a handful of heuristics that are popular. A lot are under the umbrella we call uncertainty based sampling.
00:14:11.932 - 00:15:14.166, Speaker A: So there's like the least confidence heuristic. I just look and say, among all my instances, which is the one for which the maximum probability prediction out of my predictive distribution is lowest, and I'm going to select that one for query, there's a maximum entropy heuristic. These are the same thing in binary classification, they're not the same thing in multi class classification. More recently, there's an interest in leveraging sort of stochastic models, specifically like what we call bayesian neural networks. I'll postpone a philosophical discussion about what, if anything, is Bayesian about them. But here, the idea is that if I have a stochastic model, I can sample multiple predictions from it and look and say, basically, like for each example, if I looked at the highest vote getter among the classes, how many, what fraction of the votes did it get? And I'm going to rank them according to that. There's a couple of popular techniques that are sort of used widely in practice.
00:15:14.166 - 00:16:15.614, Speaker A: There's this dropout method due to Yarran Gaul. The idea is to say you're going to train the model with dropout. But unlike the normal use of dropout, where you're going to turn it off at test time and then rescale everything, you're actually just going to keep using it at test time, and you're going to just sample a whole bunch of separate dropout masks. You're going to make a forward pass through each of them, and you're going to consider the output for each different dropout mask as a different vote, and assess your notion of confidence based on the amount of agreement using that sort of active learning by disagreement heuristic. Another popular approach is based on this paper called weight uncertainty by blowing and Dell and colleagues. This length should take you there from DeepMind, and they have this paper where it's based on this notion of, instead of having just point estimates on each of the weights, you represent each weight by a distribution. So you have now a very, very simple, simplifying distribution.
00:16:15.614 - 00:17:10.822, Speaker A: You just say each weight is an independent gaussian. And the way things are going to work is the way I'll make a forward pass through this model is I'll sample a value for each of the weights from its corresponding, you know, two parameter distribution, and then I'm going to make a forward pass through that network. Um, so it's, it's hard to say, you know, when you've made such a absolutely wildly restrictive assumption about the variation, variational distribution. You know, what precisely, uh, you were doing and what precisely you're estimating. But these uncertainty estimates have, you know, when you make visualizations to try to compare versus, if you did something else like bootstrapping, you know, what kind of uncertainty do you get off the manifold? In simple toy cases, you get something that looks nice when you apply it in the context of exploration for reinforcement learning or as an acquisition function. It's this kind of thing. It seems practically useful.
00:17:10.822 - 00:18:04.026, Speaker A: So it's become a popular technique among practitioners, whether or not we sort of believe in it as a matter of principle. Um, so, you know, you, you wind up with this sort of familiar situation where you are minimizing a Kl divergence between, uh, the variational distribution and the posterior. And after some simplification, what you end up getting is just your normal likelihood objective plus a fancy regularizer. And, uh, you know, you have a sort of hidden hyper parameter in there, which is, uh, what is, you know, the variance of your prior over the weights. And that's often said not because we have actual prior beliefs, but because we monkeyed around a little bit and found something we liked. So I don't want to go too deep into it. I've used these techniques first in the context of not active learning, but in the context of exploration in reinforcement learning.
00:18:04.026 - 00:18:40.764, Speaker A: But there was a similar objective, and the goal was to say, I want to learn a policy for a dialogue system, and I want to do it with as few interactions with the environment as possible. Our motivation was basically the dialogue community looked at what was happening with DQN in the context of Atari and Alphago and got very excited. The problem is you have this sort of pipeline here where there's some language understanding module. It's a fancy way of saying slot filling. It comes in. We ultimately have to choose what's the next action for the dialogue system to take. And then we have to generate some corresponding utterance to signal to the user.
00:18:40.764 - 00:19:42.292, Speaker A: And this piece here, we used to use a lot of reinforcement learning for the slot failing. And this piece here used to be hand coded. And the hope was, could we replace that with something driven by reinforcement learning? The only problem is if you had millions of interactions with a human in, I don't know, customer support setting in order to get an even slightly passable policy, then probably already you're out of business. So it's not like playing video games in that regard. You can't just, uh, cover your butt in simulation. So, uh, you know, the idea here was, well, let's just do Thompson sampling, um, where, uh, you know, the idea of Thomson sampling as alternative to epsilon greedy exploration is we're gonna choose each action according to the probability that it's the best. Um, so we're gonna use these, uh, weight uncertainty neural networks, um, uh, try to get around this fact that, like, the models that we're used to getting uncertainty estimates out of like gaussian processes and those that sort of give accurate predictions, which neural networks are, you know, the best tools we have, um, are different from each other.
00:19:42.292 - 00:20:45.714, Speaker A: So we need weight uncertainty to try to get uncertainty estimates out of neural networks and use it to estimate the q function. Um, the kind of trick here, like the, the detail that gets allotted is like, what we're doing is actually kind of the wrong thing. What we really, um, I guess in discussions of uncertainty generally, I think it's very, a detail that gets allotted to often is like, what are we uncertain of? And the uncertainty you get out here is uncertainty over the parameters. Um, what you really care about maybe is uncertainty over the q function itself, not uncertainty over the parameters for the q function corresponding to the specific policy that we have right now. So it's like we're sort of doing the wrong thing in general, but we get nice results. Um, and a number of other people sort of have also done, uh, something that is similarly qualitatively wrong and, you know, the same way, and also got nice results. So there seems to be some, some utility, at least in the context of exploration of these methods, even though we're maybe not expressing uncertainty over the right thing.
00:20:45.714 - 00:21:57.494, Speaker A: So coming back to active learning, you know, we were just to see, can we use this kind of technique to do a much better job in the context of, you know, do we really need 200,000 examples, sentences or whatever, 700,000 annotated words in order to get towards like in the ballpark of the state of the art performance of a named entity recognition system. So named entity recognition is just a task where we assign a per token tag, and these tags indicate, is this the beginning of an entity or an internal tag in an entity, or is this a background token? Um, and we can use it to identify spans that correspond to people, to places, to organizations, um, whatever sort of the set of entity types, uh, that are in play for your particular task. I don't know why. Um, when I used to try to go on Google image search to find, uh, a pretty example of Ner, it was something about World War Two that had Hitler in it. And, uh, I went today to try to find something more palatable and I came up with Trump. So something says something about ner. Hopefully it doesn't say anything about my search anyway.
00:21:57.494 - 00:22:50.992, Speaker A: So among the issues that we have here is that what we're going to do is we're going to do is active sampling over and over again where we're going to go out, we're going to train our model, flag some examples to add them to the data set retrain. So that's extremely time consuming. This actually required us to make more efficient models. So we went out and basically figured out every single possible component in an NER pipeline that we could swap out a recurrent model and replace it with a convolutional one to give us a significant speed up several x. So our model consists basically of word embeddings that are going to be formed at the character level concatenated with some kind of lookup embedding, like word two vec. This was pre bert. We'll have a sentence encoder, which will also be a convolutional model.
00:22:50.992 - 00:23:43.704, Speaker A: And then finally, the only recurring connections here would be a tag decoder, which is going to predict the corresponding tags. And it turned out that even though beam searches are common for this kind of thing, greedy decoding does just about as well in the context of Ner. That's not necessarily true for machine translation or ASR, where the beam really matters. And so the active learning heuristics that we'll look at are kind of standard and an adaptation of like a standard uncertainty heuristic. In this case, we're going to look at, basically we're just going to normalize our, for our least confidence heuristic, we're just going to normalize by the length of the sequence. That's because otherwise, you know, if you're signing a probability to a tag sequence, you wind up setting where you're just going to be penalized for select. You're always going to assign low probability to long sequences because you're chaining together.
00:23:43.704 - 00:24:30.630, Speaker A: You have to get more total tokens. So what we do instead is we just look at the sort of average confidence across the sequence. That's our conventional uncertainty heuristic. And then the alternative is to use bayesian active learning by disagreement. And so what we come away with is, uh, you know, this is our first paper. So after many, many experiments and a lot of excitement, we find that with, you know, roughly one quarter of the samples, we could achieve 99% of the performance, uh, that, uh, you know, sort of the best model got. And these two lines here are the performance of, um, the top line is the performance of the best deep model using all the data.
00:24:30.630 - 00:25:46.316, Speaker A: And the, um, the, the lower dotted line was the performance of, uh, of, of a shallow method using all the data. Because part of what we were trying to understand via the study is, um, th this, this point of like if we're thinking about deep learning versus shallow learning in the context of being better, if you have the right amount of data, the question was, well, using active learning, like, when can you cross that line where it's you're better off using deep learning than not using deep learning? And how different is that from, you know, if you're not using active learning? Which would be, I think I'm tethered to something which would be the sort of just standard approach where you were just your data set. You know, the x axis here is what fraction of the data set is annotated? And so that line there would be if you had just selected IId as opposed to doing active learning. So again, the reality check here is that active learning sounds great on paper, but there's this problem, which is that we have a lot of papers coming out on active learning now that are painting a cartoonish sort of picture of annotation. Right? Hindsight's 2020, but not foresight. So what we often do is we're in a situation where we've tried a whole bunch of things. We found the thing that works better than IED sampling.
00:25:46.316 - 00:26:43.700, Speaker A: And then we've said that, you know, this thing works. But if for every single new problem, you have to try ten different active learning heuristics and each one has to be able to sample a different subset of the data. And then you can go back retrospectively and say there exists one that does much better than IAD sampling, then you're kind of in trouble, because the whole premise is that you don't get to do that. Right. And then moreover, there's this kind of weird loop that the very architectures we're using, the hyper parameter choices that we're making, everything is kind of wrapped up in these same data sets that we're using to simulate the active learning. And, you know, things can get even hairier. So what we did in one paper, this is at EMNLP last year, is we tried to say, I had a student who was just really ferociously organized, Aditya Sidant, and was able to keep track of a really insane amount of experiments.
00:26:43.700 - 00:27:57.464, Speaker A: But we just wanted to say is, hey, if we just took, instead of this kind of standard setting where you fix a task, you look at one to two data sets and you have one particular model of interest. If we were to look across a range of settings, and by a range of settings, I mean multiple tasks, several tasks, at least two data seats each, at least three models each, and then multiple runs for every set. If we were to look and try to see, how does there actually emerge a consistent story that says, you know, whether active learning is beneficial for these deep models? And interestingly, you know, the results were probably less pronounced than in this paper, but we did find that across this wide variety of settings, that active learning seemed to give an advantage. Specifically, the approaches based on these so called bayesian neural networks worked best. And there didn't seem to be statistically a big difference between the benefit you got from the dropout method and the Bayes by backprop type approach. How are we doing on time, by the way? But I don't know what the current time is.
00:27:58.364 - 00:27:58.836, Speaker B: Cool.
00:27:58.900 - 00:28:07.544, Speaker A: All right, great. But here's where things get a little bit weird. So then we kind of started thinking a little bit more critically about.
00:28:09.544 - 00:28:09.808, Speaker B: I.
00:28:09.816 - 00:29:42.132, Speaker A: Don'T know, I'm a practical person, and so I'm kind of motivated by, can I actually use this? And the next kind of observation that we sort of had was, what lasts longer, data sets or models? And I think that very often in a lot of businesses, in a lot of practical settings, the dataset lasts longer than the model. I think it's not true of every case. I think there are cases like certain recommender systems where they're refreshing them on a daily or weekly basis and they're probably not changing the model that often. So I don't think this is universal, but I think it's something that we often don't think about in terms of casting the motivation here, what's got the longer shelf life? And I think across the board, we're still using imagenet. Granted, that's not maybe a good example of a practical data set, but I think it's worth looking at just in our own lives. I think a lot of us have data sets that we've stuck with over the course of tens or hundreds of different models coming and going. And so it's this question of like, well, if we're going to do active learning, what happens when the model goes stale? Like when I put another model in, in place of the model that I had before, am I still going to retain the benefits of active learning, even if there are benefits in the first place? Because we have this kind of training loop where the very selection criteria for determining which examples to select from the unlabeled pool are based on a specific model that we were training at the time and a specific heuristic kind of confidence score that we get out from that model.
00:29:42.132 - 00:31:14.766, Speaker A: So we end up coupling the data set that we collect to the specific model or specific architecture that we're training. The question is, well, once we break that and we go out and say, now I'm going to sample, now I'm going to use whatever like Bert is the new thing, Ernie's next. And when it comes out, can I use the same data set? Or if I basically shot myself in the foot by virtue of doing active learning on the sort of, you know, coupling my data to the model that I no longer want to use? And the results here were kind of underwhelming and I think should be a bit of a cautionary tale, maybe for the practical application of this naive active learning, or at least based on the kind of currently fashionable heuristics, which are, you sometimes get better, you more often do worse than when you just have IIV data, when you're transferring from, from one model to another. And then another thing that was a little bit worrying was that, you know, especially since I'm an author of both papers, is, uh, you know, I just want to kind of get to the truth here, is that I'm looking at these results that kind of did not quite agree with here. We say let's do a large scale study, a kind of empirical study on these active learning approaches for a broad range of NLP tasks. And now we're saying let's do the same thing. But we're specifically focusing on this question of the native versus the transfer data set from the same model versus to a different model.
00:31:14.766 - 00:32:17.974, Speaker A: And we found is when we even looked at the, on diagonal entries, the results were not as compelling as in the kind of previous paper. And so we spent a huge amount of time just trying to figure out what is the difference in these implementations. And it turns out that although we looked across many different models, we looked across many different tasks, many different data sets, many different models, a few things were the same. Among them, what were the base representations that we were building on? So apparently something that's like seemingly innocuous or even more sense like seemingly irrelevant just to what we're investigating, like what is our choice of representation? Is it glove or is it worth a vec? Could make the difference between seeming like we get a benefit on active learning across all these data sets and models to getting benefits on, you know, no better than random. And I think that also is the kind of thing that should, should kind of give us a little bit of pause and deciding, you know, like would I actually, I still think it's. I don't mean to discourage research in the area, which I think is a very different question from should I actually build a product based on this? Which I think is the answer is right now. No, I shouldn't.
00:32:17.974 - 00:33:15.668, Speaker A: So there's another of other approaches out there based on pseudo labeling, based on kind of heuristics that have to do with the gradient that I'll kind of leave off for now. So then I like to talk about some other papers where we've tried to maybe think about annotation in a broader way and maybe try to be a little bit more creative with it. So, you know, there's this pattern of active learning papers where we just sort of say we take an example, we send it to the annotator and they give us a label. And if we're doing something like multi class classification like imagenet, we just give them an image and they just come back and say, like, this is a persian cat. That's not how the image net data set was created, though. That's not actually, so to speak, how the sausage gets made. Real labeling isn't like you just say, give me the label out of this ontology of 1000 categories you've never heard of, and they just come back and tell you the precise one.
00:33:15.668 - 00:34:25.266, Speaker A: It often consists of asking a whole bunch of simple, often binary questions. And what we wanted to do is kind of change the view from thinking about annotation in terms of the number of examples that we query to thinking about a realistic view of the annotation procedure and thinking about how many questions we have to ask. So in the case of Imagenet, this is how it was actually created, is that they got, they had a set of categories that was like they were able to create. And if anyone's in the room who was actually involved in creating it, you could correct me if I get anything wrong, but my understanding from reading the paper is they had an ontology that came from the wordnet hierarchy, which gave them this sort of hierarchy of nouns. And then they were able to use the creative step there is they used Google image search to sort of filter a set of candidates of images that were likely to belong to that category. And then the primary use of the crowd workers wasn't to say, here's an image, give me one of a thousand categories. It was to ask them binary questions like, say, here's an image, is it a persian cat or not? And then you're just, you know, in this very familiar kind of more classical crowdsourcing setting where you're asking a binary question, you have votes.
00:34:25.266 - 00:35:34.808, Speaker A: Then, you know, you could, you could do majority vote, you could try to model like individual worker quality jointly with estimating the labels, do something like that. But it becomes a sort of classical sort of crowdsource binary feedback crowdsourcing problem. So what we try to think of is if I do have an image and I want to sort of like feed it to an annotator and I want to get a label and I've got some sort of multi class data set, likely, I'm not able to just ask them what's the label? But I have to ask them, I have to drill through some kind of set of binary questions. This is maybe a more plausible feedback mechanism. So we were working with a subset of imagenet called tinyimagenet. And because it does derive from this word net hierarchy of concepts, we do actually have this tree that we could drill through. And so our idea was to sort of to say, could we do something where we think of active learning not just as a matter of selecting samples, but a matter of jointly selecting, like, which example to feed to the annotator and also what question to ask them.
00:35:34.808 - 00:36:12.616, Speaker A: So a query consists of like an image and a category. And that category could be like a leaf category from the hierarchy, but it could also be something higher up. So you could say, is this a domestic animal? Is this a dog, is this a cat, is this a natural object or it's artificial object? You could go, go all the way, all the way up the hierarchy. And so the idea here is we start off with unlabeled data. We have some hierarchy over classes. We're going to choose a question where we define question here as an example class pair, where the class could be a superclass. Like, it doesn't have to be one of the, one of the leaf nodes and annotators just going to give binary feedback.
00:36:12.616 - 00:36:55.780, Speaker A: And at the end of the day, after we ask one query, what we have is what we call partial label. So we've been able to rule out some, some branch of the tree. And then we have some set of query, we have some set of classes that may apply, some set of classes that definitely don't apply. And now we can continue asking questions based on the same example. We could throw that one back into the pool and pick another example where we're going to choose both, you know, the image and the query. And we sort of a few things that we need to take into consideration. One is when we have these partially labeled examples, how do we update our models based on the partial feedback? And then our goals here are sort of twofold.
00:36:55.780 - 00:39:21.982, Speaker A: One way we can think about it is to say that if I fix any number of any budget, sort of like the classic active learning setup, rather than fixing the budget in terms of the number of examples labeled, if I fix my budget in terms of the number of questions asked, how do I get out the best classifier? Or the second question here is, I could say if my goal is to eventually annotate the entire data set, how do I do that while asking the fewest number of questions? So the kind of intuition here could be something like, if the model, if I feed in an image, some midpoint in training, and the model is already sure that it's either a dog or a cat, there's zero probability assigned to the possibility that it's a bird, that is a monkey wrench, that is automobile, then at that point, I don't need to waste time asking questions sort of above the level of hierarchy where the model sort of already does not believe there's any sort of plausible chance that the category, there's important questions to ask independently or offline about how much stock we want to put in the outputs that come out of neural networks in terms of thinking of them as degrees of belief or probabilities. But I'm going to punt on that. So, you know, we consider a set of sampling strategies. One idea is to look at these sort of expected information gains. So like, based on, based on the predicted probabilities assigned to each class, we think if I were to, if I were to ask this yes or no question about one of the nodes in the hierarchy, and I were to get back an answer according to the probabilities that I've already assigned to that, you know, the right class being within that superclass. They're sort of, you know, what do I, what is my sort of expected reduction in entropy of the predictive distribution upon, you know, getting that answer right? When I get back that answer, I'm going to find out that some subset of classes are just ruled out and some other subset are feasible and I'll just rescale my probabilities accordingly. So I want to know what's the expected sort of information gain or reduction in entropy? And we look at some other heuristics, like one that just tries to decrease across the data set the number of potential classes that could apply to items as quickly as possible and the other one that tries to sort of label as quickly as possible.
00:39:21.982 - 00:40:11.304, Speaker A: So expect the number of remaining classes is going to look for each example and it's going to try to choose the one that's going to get it closest to an exact label. It's basically, it's trying to procure exact labels as quickly as possible so we get these results. I mean, they're in a way encouraging and discouraging. So it turns out asking active questions. So in this case we're looking at top one accuracy versus sort of the number of, sort of bits of information solicited from the annotator. We find the active questions and the active, we call this active learning with partial feedback, which is basically like questions. Selecting both the questions and the examples both do sort of much better for a fixed number of questions than the IID baseline.
00:40:11.304 - 00:40:56.360, Speaker A: Purely just doing active learning without doing the active questions. So in this case that means that you're going to solicit the, you're going to solicit the label in the naive way. You're just going to drill through the hierarchy. You're not going to be intelligent about how you select a question, only about how you select the example. The, you know, traditional active learning heuristics don't do very well here, but being smart about which question you ask actually does. And here, what we look at over time, instead of looking at accuracy is we look at what fraction of the, of the data set has been exactly labeled as a function of the number of queries that we've run. So we find that, you know, when we do the active questions, we do label the data set much more quickly.
00:40:56.360 - 00:40:56.976, Speaker A: Yeah.
00:40:57.080 - 00:40:59.272, Speaker B: Something fundamental here is.
00:40:59.448 - 00:41:01.288, Speaker A: Yeah, sure.
00:41:01.376 - 00:41:16.546, Speaker B: Usually when we do learning, we learn a predictor. Then at test time I use the same predictor over and over again. For every example that comes and the predictor is very fast to run here, it looks, looks like those questions are tailored around the test example. So you have to.
00:41:16.650 - 00:41:18.054, Speaker A: These are training examples.
00:41:19.154 - 00:41:21.162, Speaker B: Oh, so these are for the training examples.
00:41:21.258 - 00:41:47.494, Speaker A: Right. So, like, I start off with a classifier that's random. It's going to assign near uniform probabilities to everything. I'm going to go, I don't really have much way of disambiguating, so I'll probably choose some example. I don't really have much basis, so they're all the same. But I'm going to ask a question here at the top of the hierarchy, and then over time, yeah, it's going to. But when we report accuracy, we're reporting accuracy on held out data that was never seen at any point during this procedure.
00:41:48.314 - 00:41:49.094, Speaker B: Yeah.
00:41:54.314 - 00:42:17.388, Speaker A: Yeah. So I see about five minutes. You said four at this. .4. All right, great. So another thing that we started looking at was about crowdsourcing and in what way we could integrate learning and crowdsourcing. So the classical kind of crowdsourcing setup says, I have a bunch of data points, either some images or something like this.
00:42:17.388 - 00:42:56.436, Speaker A: And I have imagine that it's just a binary classification problem. Then each annotator is going to give me some vote, is it belong to the positive or the negative class? And I'm going to then basically try to jointly learn. I'm going to. The naive thing I could do is majority vote. And then the slightly smarter thing I could do is I could try to jointly learn the noise profiles of each of my workers and infer the real labels. So I learned this worker happens to disagree with their comrades more often than others. So I start lowering the confidence that I have in that worker.
00:42:56.436 - 00:43:30.150, Speaker A: And over time, um, you know, as a result, that's going to change my inference of what's the correct label. And then if I were to go back and re infer the quality of workers. And so they have some kind of em algorithm based on this model by Dawid Skeen. Um, but they actually, these, these models ignore the features themselves. They don't look at the examples. And as a result, uh, basically, if you have singly labeled data, so if you have each example is only assigned, uh, to one annotator, then these models collapse on saying that all the workers are perfect. Like in the absence of disagreement, they can't.
00:43:30.150 - 00:44:29.812, Speaker A: Uh, so I'll skip over the classic model. But basically what our, our approach, uh, to this model, uh, was, was basically to, um, to say, you know, we're addressing a few questions. One is, um, even when the data is singly labeled, can we still both estimate worker quality and estimate the labeling function? And. Yeah, and then more broadly, we're kind of interested in shedding light into this fundamental question of what is this trade off between? So what people normally do is they pick some number of times to have every example labeled. They pick some number k that's chosen. Who knows? Heuristically, maybe I'll have every example labeled five times and they go out and they have some big data set and whatever their budget is, presumably examples themselves, like images are cheap and what they have is whatever their budget is divided by five or divided by seven or divided by three, whatever's their level of redundancy. That's the number of examples they could label.
00:44:29.812 - 00:45:23.518, Speaker A: And we wanted to sort of shed some light into this question of how do you trade off the redundancy with which you label items versus the number of items that you can afford to label. And I guess the spoiler is that, um, in our setting what we tend to find is that actually you're most often better off, uh, having as much noisily labeled data as possible. Like you're better off saying, uh, I'd rather have, uh, um, a million examples labeled one time each than a hundred thou, than 100,000 labeled ten times each. And I'll eat the noise, I'll figure out how to deal with the noise. Um, and maybe intuitively it kind of makes sense because you think if I were to take a single example and keep labeling it over and over and over again, it's obvious at some point that you have diminishing returns. But maybe what's less obvious is what happens when you only have three labels versus four or five. So, um, we have a bunch of synthetic examples.
00:45:23.518 - 00:46:24.904, Speaker A: Um, you know, I'll, I'll, I'll kind of just kind of go through the highlights because we don't have too much time left. Um, the, the key idea of the algorithm is what we're going to do is we're actually going to use disagreements between the workers and alarm the model to estimate their sort of worker confusion matrices. We're going to then take those worker confusion matrices and this is going to, we're going to go back and relearn our model, but giving more or less credence to the labels vis a vis a weighting mechanism based on how noisy we think each worker is to get out yet a new model, and then go back, take the new model, relearn, re infer the worker confusion matrices. There's some caveats related to generally the effective importance weighting that's come up in more recent work that I've done that, you know, makes me want to kind of revisit this and see if we could do something that'll make more sense in the context of deep learning. But in practice it tends to work pretty well. And what you find is this, these plots over here. So this is on Cifar that, you know, this is the degree of redundancy.
00:46:24.904 - 00:46:46.696, Speaker A: And when we have a fixed annotation budget, we find almost always that once you get your best performance at somewhere, either one or two annotations, for example, but very quickly, as you increase the level of redundancy, your model gets worse. Yeah. If you have redundancy of one, how.
00:46:46.720 - 00:46:48.528, Speaker B: Can you compare labels?
00:46:48.696 - 00:47:37.664, Speaker A: So the key idea here is that you're going to insert the model in as a stand in for consensus. So you're going to learn a model on noisy labels, compare the model to the workers. Disagreement with a model sort of constitutes like disagreement with a consensus. And we're going to think, use annotators who disagree with the model more often than other annotators are noisier. We're going to take this into account, then we're going to use this to update our waiting to give less credibility to their examples, relearn our model, then take the new model, go back, compare with the workers again. So it's like you could think of it as doing a classical crowdsourcing, em typing only the, rather than comparing workers to each other based on disagreement on examples they both annotated, is that we're going to use the model itself as like the stand in for the consensus. And so there are some theoretical results under kind of classical settings.
00:47:37.664 - 00:48:58.004, Speaker A: When we have a capacity constrained model in deep learning, we're sort of left in the usual precarious situation we are for having to rely on empirical support. So anyway, I think that's about where we are time wise. Um, kind of what I, what I'm kind of really interested in now is, uh, this question of can we use annotators to do something, uh, yet more wild? Uh, so in the context of, uh, sentiment analysis, I was kind of interested in this question, just what actually is sentiment? And, um, we often speak of it as though there's this disentangled thing. But the question was, can we actually have humans go and rewrite movie reviews to turn the sentiment knob, make this a positive review, make it a negative review, instructed not to make gratuitous changes, and we have some interesting results about what happens when you then learn on the original versus now counterfactual reviews versus learning on both. But among other interesting things, you find that there are certain things that I think NLP community likes to call artifacts, which is anything that is a correlation you don't like. But you find things, if you look at movie reviews, that romantic means very positive and horror means very bad. But if you after you do this, those types of kind of surprising associations disappear from the data set.
00:48:58.004 - 00:49:06.504, Speaker A: Matus is telling me I'm done. And I am done. So anyway, thank you.
00:49:11.524 - 00:49:12.744, Speaker B: One quick question.
00:49:16.514 - 00:50:22.768, Speaker A: I have a quick question. Sure. Are you worried that your message will help generate fake news? Of all the things happening in deep learning that could potentially generate fake news? I don't know that the particular works presented in this talk are the highest risk. I am worried in general about the capacity of this technology, maybe not so much in terms of synthesizing text, which is what the media went crazy over, maybe a little bit more in terms of the ability to create for amateurs to create forged photographs, but I don't know that my work is really in the right zip code. Sure. But in this case, the process for changing the movie review as a human is editing a movie review. So this is actually like the human who's doing the changing.
00:50:22.768 - 00:50:42.474, Speaker A: What we're hoping to do is reduce our dependence on things that really aren't semantically tied to the quality of the review, but are just what I guess we would like to call spuriously associated or something. Okay, I'm sorry. We'll take the other question offline. Let's have lunch. We'll come back at 215.
