00:00:00.640 - 00:01:08.174, Speaker A: To explore deep, unsolved problems about the nature and limits of computation. The institute's core activities revolve around semester long research programs on specific topics in the foundations of computing and related fields. We've been going since 2012, and recently we learned that our funding will be renewed for the second decade, so we're very grateful to the Simons foundation for that. For this semester, although we've been operating online only, we have two very active programs, one on probability, geometry, and computation in high dimensions, and one on the theory of reinforcement learning. The institute has a long standing tradition of appointing a science communicator in residence with the aims of supporting authors and journalists in the areas of computer science and mathematics, of helping them connect with the experts who participate in our programs, of increasing the visibility of theoretical computer science, and of helping to educate our participants about communicating their work to a broader audience. So we're delighted to have as our science communicator in residence this semester. Brian Christian Brian is the award winning author of the most human, Human, and algorithms to live by.
00:01:08.174 - 00:01:35.594, Speaker A: These books have won many awards and commendations, including from the New York Times, Wall Street Journal, the New Yorker, and the MIT Technology Review. Brian's written articles for the Atlantic, the Guardian, the New Yorker, the Paris Review, Wired, in the Wall Street Journal, and he has a third book coming out in early October, and we're very happy to be hearing about the topic of that book today. Brian's talk Today is entitled the alignment Problem, machine learning, and human values.
00:01:35.714 - 00:02:54.540, Speaker B: Welcome, Brian thank you so much. Peter so I'm going to talk about the book that I've been writing for the past four years, which is finally coming out in three weeks. And it feels really fitting to be giving the first real talk about the book here at Berkeley, because Berkeley has really been my home and my primary academic community during that time. So I first want to express my gratitude to Citrus, where I've been a visiting scholar starting in the spring of 2017, and to Chai, where I've been an active participant throughout that time as well, and also to the Simons Institute. Attending some of the seminars and workshops here was incredibly generative, both for familiarizing myself with certain technical concepts, connecting certain interdisciplinary dots, and meeting folks across the community, not just here at Berkeley, but beyond. So I want to give a special thanks to Brandy Noneki and Camille Crittenden at Citrus, to Stuart Russell and Mark Knitsberg at Chai, and to Richard Karp and Kristin Kane here at Simons for inviting me into the fold and making me feel so welcome and so at home here, and so I'm very honored to be starting this semester as the scientific communicator in residence here at Simons. It's a great honor to me.
00:02:54.540 - 00:03:58.244, Speaker B: So thank you. I want to talk about the central theme of the book, which is the connection between machine learning and human values. In some ways, it feels quite ironic to be addressing in large part, the theoretical computer science community, particularly those who work in machine learning and especially in reinforcement learning, because in some sense, I see my role as being something of an ambassador from that community to the public at large. And so it was an interesting challenge to think about how to make an address to that community. The second challenge for me is that the book is very deliberately an opportunity for people who are not myself to speak. I decided very early on that one of my priorities was to create space for people, in large part to tell their own stories. There are probably about between 100 and 150 people who speak at one point or other in the course of the book, and I think that sort of multivocal quality is really one of its distinguishing features.
00:03:58.244 - 00:05:01.474, Speaker B: For better or worse, this morning it's just me, so you won't quite, quite get to appreciate that aspect of the book. Third, and finally, a big part of what the book sets out to do is to actually teach people machine learning. I believe very passionately that there are many careers that are intersecting with machine learning at this point, whether it is public policy, the law, medicine, and so forth. And a lot of people are having to find themselves kind of skilling up in machine learning literacy midway through a career that was ostensibly about something else. And so a big part of my goal is pedagogy. I want people to walk away knowing the difference between supervised, unsupervised and reinforcement learning, and what an objective function is, and the difference between behavior cloning and inverse reinforcement learning, etcetera. And if ever there was a group of people who did not need me to give them a primer on machine learning, it is the Simons Institute research fellows.
00:05:01.474 - 00:06:41.230, Speaker B: So what I would like to do is highlight some of the interdisciplinary connections and the places where machine learning makes contact, sometimes in very surprising ways, with other disciplines. Because I think it is very broadly true that to the degree that AI, machine learning, reinforcement learning, are about discovering fundamental aspects of what it means to think, to have a mind, then the further along that field goes, the more relevant existing fields, like developmental psychology, cognitive science, education, management science, et cetera, become. And so for the next 30 or so minutes, perhaps I can reverse my normal osmosis and instead of being a kind of ambassador for theoretical computer science, I can attempt to be an ambassador to theoretical computer science and highlight this, what I see as an incredibly rich circumference of connections to other fields and opportunities for research and for cross pollination. So on to the central thrust of the book itself before we explore the actual contents together. So there's a story that I assume most everyone here knows, at least to some degree or another, which is the remarkable progress that has been made in machine learning, particularly deep learning, since the beginning of the last decade. There is something very poetic to me about the fact that it was neural networks in particular that are responsible for this incredible breakthrough, because neural networks were essentially one of the very first ideas in computing. They're older than the von Neumann architecture.
00:06:41.230 - 00:07:37.614, Speaker B: They're older than the stored program computer. Von Neumann's 1945 EdVAC report, which is the first written description of a stored program computer, contains in 101 pages only a single citation, which is McCulloch and Pitts, 1943. So I started researching the life of Walter Pitts, going through oral histories and the McCulloch archives at the American Philosophical Society in Philadelphia. And I was astounded at the life stories of some of these early pioneers. I remember reading an oral history from one of their contemporaries, Jerome Letvin, who just casually mentioned, oh yeah, and when Pitt started working with McCulloch, he was 17 years old and homeless. Well, that certainly got my attention. And Warren McCulloch basically came like, became like his foster parent.
00:07:37.614 - 00:08:29.524, Speaker B: The more I read, the more fascinating and poignant that story became. So I thought, okay, I found my, my opening scene. So it's Walter Pitts, age twelve, deciding to run away from home. And of course, we all know the story that, that runs from there through Frank Rosenblatt and Marvin Minsky and Seymour papert, the rebirth in the 1980s. And then what I see as kind of this ultimate final triumph in 2012 with Alex Net. When we first meet Alex Krazevsky in the book, he is in his bedroom at his parents house in Toronto, and his two GTX 580 gpu's have been running nonstop for two weeks, and now it's too hot to sleep. And that was just 2012, not that long ago.
00:08:29.524 - 00:09:09.744, Speaker B: And yet it feels now that we almost live in a different world. I think we've become in some ways desensitized to how discontinuous some of these jumps have felt. And it's important, I think, to remember how much they caught even experts off guard. So one example, sort of at random, is Richard Sutton, who authored of course. Course. The canonical book about reinforcement learning, gave a lecture in 2015 at UT Austin, where he presented a graph. And here's the slide of the strength of computer go programs.
00:09:09.744 - 00:10:02.414, Speaker B: There was this very striking linear trend line, which, if you extrapolated it out, he notes that projecting this trend suggests a computer will be world champion by 2025. So within ten years. And it happened the very next year, led by the team at DeepMind, led by David Silver. And I think this is just a very striking illustration of how abrupt some of those jumps have been. I got very curious, by the way, in the origin of this graph. So I started digging into where rich got it, and the original version had been made for rich a few years earlier by a grad student of his named David Silver. So I think that's quite ironic indeed.
00:10:02.414 - 00:11:11.244, Speaker B: At the same time, all of this is happening in deep learning and deep reinforcement learning. There is a subtler but equally significant movement that's happening within society, which is the penetration of machine learning into greater and greater contact with human and institutional decision making. So, to illustrate this, this is a look at the number of states in the US that are using statistical risk assessment models to assist in parole decisions. So, by 1935, it's just one us state. By 1960, it's two out of 50. By 1980, it's four out of 50, then twelve, and then in the year 2026. And by 2003, the association of Parole Authorities International's Handbook for New Parole members writes, in this day and age, making parole decisions without the benefit of a research based risk assessment instrument clearly falls short of accepted best practice.
00:11:11.244 - 00:12:43.638, Speaker B: And in 2016, Supreme Court Chief Justice John Roberts is visiting Rensselaer, and he is asked by Rensselaer president Shirley Ann Jackson, can you foresee a day when artificial intelligence will assist with courtroom fact finding and even more controversially perhaps, judicial decision making? And Roberts responds, it's a day that's here. And so I think on both of these counts, both the astonishing capacity of these systems, on the one hand, and on the other hand, the increasing surface area on which they touch our lives. People began, as we all know, to get concerned. And these concerns take root across two distinct but fundamentally united groups. There are people worried about the present day, about whether the systems currently being deployed really represent the interests of the people that they're supposed to. And there are people worried about the near term future, as we increase and increase the capability of these systems, that we might be setting ourselves up for a truly catastrophic failure again. Despite their different priorities and distinct but overlapping communities, I began to see the fundamental question underneath each set of concerns as being the same.
00:12:43.638 - 00:13:54.204, Speaker B: So, put most simply, how can we make sure that these systems do what we want? And this problem, of course, has a name, and the name is the alignment problem. And I think this is more or less where the public conversation around machine learning, around ML ethics and technical AI safety kind of ends. But in my view, this is really where things get interesting, and this is really the point where the book begins. So if we look to the period of roughly 2014 to 2016 as a giant pulling of the fire alarm, then, to continue the analogy, what we begin to see from roughly 2016 to the present is the first responders start showing up. This set of concerns moves from being marginal and to some degree taboo, to comprising one of the central concerns, I think, of the field. We have, this explosion of workshops, conferences, research centers, nonprofits, grants, all happening within this short time. And I heard from people again and again that they.
00:13:54.204 - 00:14:52.244, Speaker B: They would go to, for example, neurips in 2016 and tell people that they worked on safety, and the response would be something like safety. And then by 2017, there was an entire neurips workshop on safety. I heard versions of this story again and again, with the years perhaps plus or minus one, and the area of focus being safety, fairness, transparency, et cetera. So there is this incredible zeitgeist, I think, pivoting towards these issues. There is a first generation of PhD students just now graduating, who have matriculated with the explicit purpose of wanting to work on AI ethics and AI safety. And not only is there this incredibly spirited energy around these topics, but there are actual results. There are tangible victories being notched, and there is this agenda now that is well underway.
00:14:52.244 - 00:16:37.152, Speaker B: So, with, ironically, a minimum of actual computer science and a maximum emphasis on the interdisciplinary dimensions of this field, what does that agenda actually look like? So, the book is divided into three parts, which comprise three chapters each. And so I'd like to just take a very brief glance at each in turn and highlight what I see as being some of the frontier of interdisciplinary connections that exist around this core theoretical computer science contribution. So, the first chapter starts with one of the most visceral and sadly iconic examples of machine learning failing in an ethically significant way, which is the famous example of the two black Americans being labeled by Google Photos in 2015 as gorillas. We get to meet Jackie Alcine, who was one of those people and is himself a software engineer with an ML background, and who knew instantly that something had gone wrong in the training data that he, as soon as this experience happened to him, he immediately surmised that there was just a paucity of black faces in the training data. So he knew exactly how this had happened. But the question was, why? Why had that come to be the case, that this training data was so unrepresentative of the population at large? So that's the deeper question. And I think this is a great example of where ethical and long term technical safety concerns intersect.
00:16:37.152 - 00:18:16.644, Speaker B: So you can frame this as a question of inclusion and representation. You can also frame it as a question of robustness to distributional shift. How do models behave when they get outside of their training distribution? And there is a lot of really, I think, exciting and encouraging work being done here. So you have the work of people like Joy Bulimwini and Timnit Gebru and many others, bringing a focus to this question of, where do these training sets actually come from? What do they actually look like? And I think there's an interdisciplinary theme here as well, which is that there's an almost 200 year story of the intersection of racial justice and photography. So, as I started researching this area, I was fascinated to learn, for example, that the single most photographed American of the 19th century, more than Abraham Lincoln, et cetera, was Frederick Douglass, the pioneering abolitionist, who felt that photography was a critical tool for emancipation. Because, of course, before photography, you had engravings, which were done by hand and almost always exaggerated black people's features in stereotyped ways. So there's an irony that we go from the 19th century, in which the single most photographed American is Frederick Douglass, to the 20th century, where commercial film is being developed and calibrated by color accuracy with reference to a model.
00:18:16.644 - 00:19:43.044, Speaker B: The first model that Kodak used was named Shirley. And so these have become known as Shirley cards, which, until the 1970s, were almost exclusively white. And in the book, we get to meet some Kodak executives who describe, amazingly, that they were receiving pressure in the sixties and seventies from the chocolate and furniture industries to make film that better portrayed brown Hughes. But changing the nature of the film, of course, changed the ability of the film to represent people with darker skin. So here we are in the 21st century, in which we have these kind of weird echoes back to the 20th century, and we have this broad movement of questioning, what is the nature of these training sets? And you have people like Anil Jan and Huhan publishing statistics about the composition of labeled faces in the wild and showing that, for example, it contains twice as many images of George W. Bush as it does of all black women combined. And as recently as I believe, the fall of 2019, a warning label now appears on the page where you would go to download the LFW data set saying basically, caution, this data set is not representative of the US or the global population.
00:19:43.044 - 00:21:08.392, Speaker B: The other domain that we explore in this chapter is language models. And looking at the many stereotypes that emerge all the way from simple word embedding models like word two, vec and glove, all the way to the more modern, enormous trend transformer networks like GPT-2 GPT-3 these models are often employed, I think, uncautiously, in recruiting and hiring contexts. And we encounter the story of an Amazon team that looks with horror at the individual terms, that their model is kind of up ranking and downranking. There's a theoretical computer science story here in work on debiasing vector models by collapsing the gender space. And although the story there is not quite so simple, and there's work as recently as two weeks ago from OpenAI on fine tuning these big transformer models from human preferences. And I think that story is also very much being worked out. The interdisciplinary story here is that we've added a new tool to the arsenal of social science that as these language models become in some ways uncanny reflections of human norms and human biases, including the ones that we would rather we didn't have, they become a measure for actually watching society change.
00:21:08.392 - 00:22:29.050, Speaker B: And so there's been some really interesting interdisciplinary work on applying these models to historical corpora, you know, 1930s, forties, fifties, sixties, seventies, and watching, in a quantifiable way, the norms of the society change. And so I think machine learning is not only a tool, that it's not just the case that social scientists are increasingly having input in these models, but that these models are also giving social scientists a brand new set of subject matter and an entirely new lens with which to look at the world. So the second chapter is fairness. And I think most people who are familiar with the machine learning literature on fairness know about the compass tool, which predicts the risk of recidivism and is used in pretrial detention. So in this chapter, I really try to dig into the backstory. So, statistical risk assessment models go back to the 1920s, and there's a time when the conservative head of the parole board in Illinois was thinking about getting rid of the parole system entirely. And he believes it is simply an asset to criminals that you would ever let them out ahead of the the full sentence.
00:22:29.050 - 00:23:50.614, Speaker B: And a sociologist from Chicago named Ernest Burgess ends up collecting enough data to persuade him to change his mind. And that is really the beginning of the use of statistical risk assessments in the criminal justice system and looking through archival newspapers was very enlightening to me. For example, most of the criticism against these models in the thirties was coming from the right, whereas today it is largely speaking the progressives that are the most skeptical. So I think the most visible of the contemporary critiques of these models is from Propublica Julia Anguin, who made this very famous article, machine bias, critiquing the compass model. And I got to meet both Julia Anguin and the creator of compass, Tim Brennan. And I was quite pleased to actually find a bit of common ground where I could convince them to agree with one another. There's a theoretical computer science story here on how do we operationalize fairness? And this goes through the work of people like Cynthia Dwork, Moritz Hart, John Kleinberg, Sam Corbett Davies, Alex Chaldachova, Christian Lum.
00:23:50.614 - 00:25:33.696, Speaker B: Looking at everything from these impossibility proofs of mutually satisfying different metrics of fairness, to things like the long term feedback loops that exist when these systems get put into practice and start generating the very data that they will go on to be trained upon. The interdisciplinary story here includes not only the long term history of risk assessment, but also contemporary political scientists like Columbia's Bernard Harcourt, who argues that the very premise that better predictions lead to a better public safety is itself wrong, which I think is a very interesting argument for people in ML to contend with. And more broadly, we have the question of not just data provenance, where are the data coming from, but the sort of human computer interaction and the user experience aspect of how these risk assessment instruments actually get put into practice. So as part of my research for this chapter, I spent a day going to arraignment hearings in San Francisco right after San Francisco began using the Arnold tool. And it was very illuminating to me to see the degree to which individual judges did or didn't actually comprehend this giant printout that they're being given with the risk assessment information on it. So there's also this deeper question, of course, of what exactly is the ultimate purpose of certain aspects of criminal policy. If we can identify that someone is a risk pretrial? Well, there might be two extremely different risks that we have in mind.
00:25:33.696 - 00:26:24.262, Speaker B: There might be a risk of violent reoffense. There might also be a risk of failure to appear for the court date. And it may be the case that the solution to the former problem might be incarceration, it might not. The solution to the second problem is probably something like a text message. And so I think there's a lot of work to be done not necessarily in how the models themselves are developed, but in whether they're essentially used according to the label on the side of the tin. And I think there's a lot of work to be done there. And many people, of course, argue that fine tuning exactly what the objective function is of these systems, or exactly what this sort of fairness constraints imposed upon them, that while that discussion is fruitful, there are also ways we can sort of cut the Gordian knot entirely.
00:26:24.262 - 00:27:33.574, Speaker B: So we could just decriminalize marijuana, for example, and then not have to worry about how to fairly assign pretrial detention for people who are arrested for that. There are some states, I believe, New York state and Maryland, if I'm not mistaken, that are increasingly moving towards a model where if you're arrested for a nonviolent misdemeanor, then you're simply released full stop. And so then you don't need a model to predict whether or not to detain the person, if you simply never detain the person. In the domain of transparency, the chapter on transparency focuses on the domain of medicine. And we meet Microsoft's rich Karwana, who in the 1990s was developing machine learning models for predicting the severity of pneumonia. And his neural network model wins this kind of bake off against logistic regression, rule based models and so forth. But very significantly, he urges the doctors that were partnering with them on the study not to deploy the neural network precisely because he doesn't know what's in it, he doesn't know what it's learned.
00:27:33.574 - 00:28:47.514, Speaker B: And in particular, the rule based model had learned this rule, that if someone is asthmatic, then we should predict that they are at lower risk for pneumonia, which, if you think about that for a second, doesn't make any sense at all. It turns out that this is actually a real correlation in the data. But it's precisely because asthmatics are given higher priority care, that they, on average, do have better outcomes than regular people. But this is precisely the care, of course, that the model would deny to those patients. So transparency allows us to, of course, catch some of these things before they actually go into deployment and affect people. And there's a really rich computer science here that I think is really exciting from Karwana himself, trying to explore a space of models that are ideally as expressive or capable as neural networks, but as interpretable as something like a ruleist. And so he's pioneering ideas like generative additive models and his own sort of personal extension of that to people like Cynthia Rudin at Duke University, who are exploring the space of certifiably optimal simple models.
00:28:47.514 - 00:29:43.418, Speaker B: So, rather than using our computational horsepower training a big model, we use our computational horsepower exploring the space of simple models and finding the ideal simple model. And on the sort of deep learning side, we have people like OpenAI's Chris Ola working on unpacking and visualizing deep convolutional networks, and people like Google's bean Kim working on concept activation vectors and interpretability measures using high level human concepts. The interdisciplinary story here in my mind is that transparency is fundamentally a human concept. A model is transparent to the degree that people understand what's going on and use it appropriately. There is nothing in the abstract that transparency means outside of that. And so user studies should be totally unavoidable. And not only that, but they are often counterintuitive.
00:29:43.418 - 00:31:00.204, Speaker B: So one of the results here that comes to my mind is the work of Jen Wortman Vaughan and her collaborators, who showed that simple, transparent models with a small number of parameters and clearly visible weights were much more trusted by human users, even when those models were operating outside their training, distribution and outputting garbage. So I think user studies like that are really useful at problematizing the simple story that we might otherwise get about thinking about model transparency. So there's also the legal angle, obviously. So transparency intersects with the law and things like the GDPR. And I also think there's a critically interesting intersection here with a bunch of mid 20th century psychology. So there's a tradition within psychology, going from Ted Sarbin in the 1940s to Paul Mule in the 1950s, to Robin Dawes in the 1970s, looking at comparing expert human judgments to linear models with uniform weights, unit weighted regression. And the TLDR is that unit weighted regression demolishes expert judgment.
00:31:00.204 - 00:31:57.164, Speaker B: Even when you still give the human expert the results of a unit weighted regression, uh, they're still worse than just using the regression on their own. And when you give the machine learning model, the human judges, uh, decisions as input. Uh, the model doesn't even use it. Uh, it's just not helpful. Um, and I think this is really, really provocative. And, and in particular, um, one of the things that Robin Dawes was interested in is this question of how do you build a model when you don't have an objective function? So you want to identify high schoolers that will go on to flourish by getting higher education? Okay, well, first of all, you're going to have to wait 20 years to get the training data for that, and you have to implement your model now. Secondly, what do you mean flourish? How do you operationalize this idea of someone who responds well to going to college.
00:31:57.164 - 00:33:14.126, Speaker B: It might take a really long time to figure out how to operationalize that, if you can, but you have to make the model now, so what do you do? And amazingly, there are still results that you can prove about what a good model might look like, even under those conditions. And I think results like that are relevant for thinking about these farther future questions about AI and what are the objective functions that we really want to give them. And again, this is happening in psychology in the 1970s, but it feels in some ways more relevant now than ever. So that's part one, which looks at supervised and unsupervised learning and present day risks. Part two turns the focus to reinforcement learning specifically. So in chapter four, we get to meet Andy Bardo and Rich Sutton, and we learn the roots of reinforcement learning in the ideas of hair Klopp, who had this idea that neurons were what he called heterostatic maximizers, pushing back on the cybernetics movement of the forties and fifties that thought that purposeful behavior necessarily required negative feedback and a system that wanted to reach equilibrium and stay at rest. Harry Klopp said, no, that is not what life is like.
00:33:14.126 - 00:34:08.564, Speaker B: That is not what organisms are like. We are maximizers. And there's a deeper historical story here, too, though, which goes all the way back to the 1890s and the work of Edward Thorndike on what he called the law of effect, which is that, you know, by default, we take actions randomly. The results of those actions are, in his words, either satisfying or annoying, and that we modify accordingly to do more of the satisfying things and less of the annoying things. And there's this wonderful historical moment here where it turns out that Edward Thorndike and Gertrude Stein were classmates at Harvard in William James's psychology class in 1896. And Gertrude Stein described him as a funny character. And these ideas really carry all the way through to reinforcement learning in the 20th century.
00:34:08.564 - 00:34:50.914, Speaker B: And so in this chapter, we talk about RL concepts like credit assignment, the difference between value learning and policy learning, temporal differences. Knowing my audience, I think it's fair to imagine that you don't need me to say too much more about that, but there are a number of really rich connections here. RL is premised on this idea that rewards are scalar, they're cardinal, they're fungible, anything can be compared to anything. Real life doesn't always feel that way. We agonize. Do I do the thing that's the most lucrative? Do I do the thing that's the most or do I do the thing that's the most fun? Well, RL traditionally doesn't have this problem, right? The rewards are scalar. So you just compare four to five to six, and you do the six.
00:34:50.914 - 00:36:11.604, Speaker B: So there are philosophers like Oxford's Ruth Chang who think that this fundamentally multidimensional character of human rewards what she calls incommensurability, this inability to be collapsed from a vector representation to a scalar, is absolutely central to the human experience. People from the RL community essentially counter argue that you do in the end decide, and so you can kind of infer that there was a scalar attached to that that was greater than the scalar of something else. And of course, this really intersects with economics and revealed preferences and utility. And needless to say, there's an entire literature there, front Pareto to von Neumann, etcetera. You also have contemporary people in the neuroscience community, people like Paul Glimcher and his colleagues at NYU, trying to unpack the actual mechanisms by which the mind attempts to do this dimension reduction in the space of value, and looking into the question of where and how and with what model is the brain doing that. There's a lot that we are starting to know in the last 20 years, but a lot that still out there to be learned. I think the most thrilling collision between RL and neuroscience is the dopamine system.
00:36:11.604 - 00:37:54.734, Speaker B: So some of you may know this story, and I'm compressing a lot here, but in the 1990s, it was shown by Peter Diane, Terry Sinowski, Reed Montague, that temporal difference learning basically explained this open problem in understanding the function of the dopamine system. And for me, that's just this totally climactic moment of the science coming full circle, that these models that had grown out of animal learning in the late 19th, early 20th century finally come into their own. And not only that, but actually solve this outstanding riddle in the way that the human brain, I think that's a really, really encouraging indicator that RL is basically on the right track and that we're discovering fundamental mechanisms of learning, not just engineering practices that work for specific problems, but universal mechanisms for learning that evolution has stumbled into again and again. So from there, we get into shaping. And, you know, anyone who works in reinforcement learning is familiar with the, the delicacy of designing appropriate incentives. And there's a fascinating interdisciplinary story here, too, that starts with BF Skinner during World War Two, teaching pigeons, because he's been assigned this project to put pigeons inside of bombs and have them peck at images of bomb targets to create, like, live homing missiles, basically. And he has this quote that, you know, my colleagues and I knew that in the eyes of the world, we were totally insane.
00:37:54.734 - 00:39:28.662, Speaker B: And along the way, he does, he develops these principles of what he calls shaping that you can start approximating, rewarding approximations to the behavior that you want. And so this idea obviously goes through theoretical RL. And you have the work of Stuart Russell and Andrew Ng in the late nineties, showing that the way to avoid problems of incentive is to create what's called a conservative field, or basically make a situation where if you return to where you started, the net shaping reward is zero. Put differently, we want to reinforce states of the world, not actions of the agent. And this ends up having all these ramifications in the cognitive science community. So, for example, the work of my good friend and collaborator Tom Griffiths at Princeton, his former PhD student from Berkeley, Falk Lieder, using these principles of directly borrowing ideas from the shaping theory to create mechanisms for what they call optimal gamification. So how do you incentivize people not only in a way that doesn't lead to degenerate behavior, but in ideally the optimal way? And so the computer science, or the cognitive science rather, is borrowing that idea very directly from the RL theory.
00:39:28.662 - 00:40:38.504, Speaker B: And I think there's a lot more to be worked out there as well. Of course, we know that not only are we motivated by explicit incentives from outside, but anyone who spent time with kids and animals knows that we're motivated intrinsically as much as extrinsically. And it became obvious in the mid 20th century that rats were willing to walk across an electrified fence just to peak around the corner. Monkeys are as willing to lever press to look out a window as they are for food. And so this started an effort within psychology to try to understand the nature of intrinsic, as opposed to extrinsic motivation. And there's a long and wonderful history here. The computer science story, I think, is people like Google Brains, Mark Bellamar, who who is working on extending count based exploration into non tabular settings, people like Juergen, Schmidt Humer, who's thinking about intrinsic motivation as the ability to compress information.
00:40:38.504 - 00:41:36.674, Speaker B: People at OpenAI, like Yuri Berda and Harry Edwards, and here at Berkeley, people like Deepak Pathick, Polkitt, Agrawala, Trevor Darrell, exploring intrinsic motivation based on the idea that the agent be motivated to take actions essentially, which surprise it. So, as this formal work in RL is getting worked out, there's this, all of these connections to infant psychology. There was a great story that Alison Gopnik Berkeley psychologist, told me about reading about Trevor and some of his students work in the Berkeley newsletter, and they were talking about how interested they were in taking her ideas about infant curiosity and applying them to RL. And she emailed them like, guys, I'm right here. I'm like, across the street. Let's actually collaborate on this. And so it's been really exciting to see those two worlds come together.
00:41:36.674 - 00:43:07.538, Speaker B: And on the one hand, developmental psychologists are using RL as a formal model to explain infant behavior. And at the same time, the people in RL are turning to what we know about infants to think about motivation and intrinsic drive that might be useful just in exploration for RL. So the third part of the book gets most squarely into the question of normativity and aligning deep RL agents with human norms and human values. So one of the things, one of the central themes that anyone knows in RL is what's called imitation learning, or sometimes behavior cloning. And there's a really, really rich story here, not only the computer science story in the book, we meet Dean Pomerleau from CMU, who is crazy enough to drive all the way from Pittsburgh to Lake Erie on the highway for 2 hours, letting a neural network steer his car in 1990 using a system that had one 10th of the processing power of a first generation apple Watch. So the surprisingly long and slightly daredevil history of behavior cloning in self driving cars that continues all the way to this day. And we meet Waymo engineer Stefan Ross, who developed the dagger algorithm for avoiding cascading errors in imitation learning.
00:43:07.538 - 00:44:28.536, Speaker B: There's also this wonderful human story here, too, which is that, um, zoologically, all of our words for imitation, not just in English, but in almost every language we say, you know, to ape something, but the. The real prolific imitator in nature is not apes at all. And there's an entire primatology literature on, just on that topic, but it is, in fact, humans. And furthermore, the, the human capacity for imitation is extremely sophisticated and goes way beyond merely duplicating behavior in ways that are surprising and really, I think, informative for thinking about how this might work with machines. So there's also, I think, a really interesting connection in imitation to not just primatology, but philosophical ethics. And in the interest of time, I won't get too deeply into it, but there's a, there's a classic philosophical tension going back to the 1970s between what are called possiblism and actualism. So do you do the very best thing possible in a situation, even if it requires a very precise follow through, and you know that you'll screw it up, or do you do the.
00:44:28.536 - 00:45:31.004, Speaker B: The lesser action that, you know, you can actually follow follow through? So this debate has now been going on for something like 40 years, and it's absolutely relevant to thinking about things like batch off policy RL. So I'm really intrigued in the way that those literatures are starting to come into contact. In chapter eight, we get into, arguably, the heart of contemporary AI safety research. So ideas around inverse reinforcement learning, cooperative inverse reinforcement learning, deep RL from human preferences and so forth. Um, and Irl itself has this wonderfully colorful history that really goes back to, uh, Stuart Russell walking to Safeway, um, and thinking about his gait as he went down the hill. And this gets him thinking about, you know, what. What is it that animal and human gates optimize? Why is it the case that we still need to hire motion capture people, and we can't reliably produce, like, realistic looking gates? And this gets into the idea of IrL.
00:45:31.004 - 00:46:48.644, Speaker B: So if the human gate is the answer, what's the question? There's an entire interdisciplinary literature here just on the science of gait and all the different theories that people have had over many decades for why horse gates are certain ways, and why there are phase transitions in quadruped gates at certain speeds. And are they optimizing stress on the joints, are they optimizing the calorie load, et cetera. And Irl now directly offers us a way to answer and address questions like that. So there's this purely theoretical story that goes through Peter Abeel's work on getting helicopters to do stunts, people like Chelsea Finn working on things like guided cost learning to Jan Laika and Paul Cristiano, very memorably teaching a mujoko agent to do a backflip just by comparing different clips and picking the one that looks slightly more like a backflip. There are, I think, many interdisciplinary questions here. There are ethical questions about recommendation systems. You can infer what someone wants, but are they acting the way that they want to be acting in the first place? I think that's an interesting question.
00:46:48.644 - 00:48:07.554, Speaker B: More broadly, as robots become more capable and able to work sort of elbow to elbow with humans in manufacturing settings and so forth, there's an entire interdisciplinary literature on human teamwork. And so I'm thinking about people like MIT roboticist Julie Shaw, who has done a lot of work borrowing ideas straight out of the human human teaming literature, and showing that they apply basically wholesale into thinking about human robot collaboration in factory settings. And so I think there's so many cases like this where there is just this windfall of insights ready to be plucked, basically. And as these systems get more and more capable, I think that's only going to continue to be the case. So the book ends with a chapter about uncertainty. So we meet Stanislav Petrov, the soviet officer who single handedly saved 100 million people's lives by not doing anything when his missile system told him that the US was attacking. But the attack seemed weird to him, so he simply didn't do anything, and to some degree, save the world by that.
00:48:07.554 - 00:49:55.504, Speaker B: So this theme of uncertainty, and in particular, what action to take in the face of uncertainty, I think is very interesting in an ML context. So you have people like Oregon's Tom Dietrich, who talks about what he calls the open category problem. It's one thing to classify images as one of a set of categories, but most of the things in the world are actually in none of those categories. So how do you deal with that? There are people like Oxford, Yaron Gall, who are working on the uncertainty estimates you get out of bayesian neural networks and how you can approximate that using dropout. There are people here at Berkeley and Chai Smith, Amilli, Dylan Hadfield, Minnell, Peter Abeel, Anka Dragon, Stuart Russell, working on the idea that if you're interested in obedience in a system, the ability to intervene and stop it, the system must necessarily be uncertain, have some uncertainty over what it thinks your objectives are. Again, here at Berkeley, Gregory Kahn and his colleagues at Bayer have worked on robots that slow down when their collision predictor model becomes uncertain using a mixture of dropout and other techniques. I think there are a number of questions here, purely in the theoretical computer science side, around how do you measure uncertainty? How do you measure the impact that your action might have? And then what do you do in the face of that uncertainty and with that sense of impact? So, on the impact side, there are people like future of Humanity Institute Stuart Armstrong, DeepMind's Victoria Prakovna, Oregon's Alex Turner working on operationalizing this notion of high impact actions in an RL setting.
00:49:55.504 - 00:51:04.174, Speaker B: And there's also an entire medical ethics literature on what do you do in the face of uncertainty. There's also a legal literature on this idea of preliminary injunctions or irreparable harm. And I think all of these things start to become relevant and start to feed into what we are doing in the field of RL. So, to conclude, I think, as we gather here at the end of 2020, in sum, I think we're ready to etch another chapter into this history. So we have seen, first, the promise and secondly, the peril of these systems, and we're now at the beginning of what I see as the third phase. There's a small but brilliant group of people beginning to rally and muster around these issues. The first responders, as I say, are on the scene, and I hope we can grow these numbers, and I hope that the book inspires people into this area, which I see as being not only one of the most fascinating and dynamic areas, but also one of the most important projects in computer science and, frankly, in all of science.
00:51:04.174 - 00:52:01.144, Speaker B: And I think this is a challenge to meet. This is an opportunity to meet that challenge head on and also a chance to learn, I think, something really radical and profound about ourselves. So as a place to end, I was going through some archival papers of Alan Turings, and I came across a conversation that he had on BBC radio in 1952, and we'll skip some of the context, but he's saying, I was doing a lot of work trying to teach this machine to do something very simple, and a very great deal of instruction was needed before I could get any results. The machine learned so slowly that it needed a great deal of teaching. And one of his fellow panelists interrupts him and says, but who is learning, you or the machine? And he says, well, I suppose we both were. So with that, I just want to say thank you, and I welcome any questions that you might have. So thank you very much.
00:52:03.444 - 00:52:47.344, Speaker A: Thanks very much, Brian, that was really great. You are welcome to use the Q and a feature if you'd like to ask any questions while folks are thinking about it. So it struck me in looking at the table of contents of your book that you went through, it was a very logical progression with the titles that you had. It could have been a textbook, but it's really clear that the book's filled with fascinating stories. So I'm curious about. So, for instance, I was surprised to learn about Walter Pitts being a homeless prodigy. Yes, but.
00:52:47.344 - 00:52:55.944, Speaker A: So I guess I'm interested in how the stories have shaped how you chose to present the topics in the book.
00:52:56.524 - 00:53:53.362, Speaker B: Yeah, I think that one of the challenges in writing nonfiction is that you have to juggle these different things. So to your point, it is to some degree a kind of a textbook. It's sort of a sheep and wolf and sheep's clothing, a textbook that is presented as a narrative so that it can reach a broader audience. And so for me, I had to really juggle the question of what I saw as being kind of the central result in an area with what seemed like the best story. And sometimes those things lined up really naturally. So, for example, Rich Karwana, discovering that pneumonia, the pneumonia model, predicts that if you have asthma, that you should go home and, you know, it'll be fine. That is both a wonderful story on its own, and it also happens that rich is today, you know, one of the people who's really active in this area.
00:53:53.362 - 00:54:34.936, Speaker B: So that kind of presented itself to me on a platter. Other cases where, you know, in the uncertainty chapter, it opens with this somewhat canonical story of the soviet officer. And that's not. It's not, on its face, a machine learning story, although he was using a machine learning system that was giving him this assessment that, okay, it looks like there's a us attack. We're rating it at high confidence. And he knows that what he's supposed to do is pick up the phone and tell his superiors, but he knows that they're going to order this missile launch. And so he's effectively.
00:54:34.936 - 00:55:41.384, Speaker B: It's effectively his hand on the button, and he decides not to do anything. And so there was just an anecdote that I found of a machine learning researcher talking about in the past, when if someone had given them a thought experiment where there was a button they could press that would convert the universe to hedonium, you know, computationally optimized matter for producing unpleasant experiences, they would have pressed that button, and now they're not so sure, you know? And when people mention that thought experiment, they say, I don't know, maybe. Maybe we shouldn't press that button. And so that, for me, was both a literary moment where I could connect this historical example to a present day contemporary researcher, but also a chance to have some literary symmetry where the chapter begins with someone not pushing a button and it ends with someone not pushing a button. So each chapter offered me different opportunities for that, but it was very much an exercise to try to balance the curriculum, if you will, and just the pure storytelling.
00:55:41.924 - 00:56:00.154, Speaker A: Yes. Fantastic. We have a few questions, but we're running short on time. Maybe just the first one up there. Anna asks, are there any particular areas that you think should be receiving more attention than they are now or any communities that you think that you think should be talking to each other more?
00:56:00.854 - 00:56:29.930, Speaker B: Oh, it's. It's hard to say, in a way, because I feel like the entire book is that, you know, it's like all of these people should be talking. And for me, one of the. One of the pleasures of the book is that I actually, you know, I got to be more than a fly on the wall. I actually got to have conversations with people where I said, oh, you know, so and so is working on exactly that. And, you know, here's someone in Cambridge who's working on that, and people's ears would perk up. And I get to write letters of introduction.
00:56:29.930 - 00:57:38.334, Speaker B: And so, you know, for me, that's one of the great pleasures of getting to do a sort of broad, interdisciplinary work like this is to actually get to stimulate some of those connections. It's really hard to pick just one, but, I mean, I think a lot of the work on infant cognition is. Is pretty amazing. And that comes up again and again. It comes up in shaping, because infants are really good at thwarting, I mean, and small children, not just infants, really good at thwarting the incentives that you try to design as a parent in ways that are, I think, indicative of problems that we can expect from, you know, RL systems more generally, all the way through intrinsic motivation to imitation, to the ability of children as young as 18 months old to infer from your actions what you're trying to do, even if you're failing to do it, which is this sort of IRL thing. So I think that the connections between the infant mind and AI obviously go all the way back to the very beginning Turing was talking about. Imagine building a program that simulates the child's mind, not the adult's mind.
00:57:38.334 - 00:57:55.294, Speaker B: Then all we have to do is subject it to the appropriate course of education. So that's a very old idea, but in some ways, it feels like it's just bearing fruit now and is far from being exhausted. So I think that's really a really rich area that has a lot to offer.
00:57:56.474 - 00:58:06.922, Speaker A: All right, well, I think we're going to have to cut it off there. There are several more questions, but we're out of time. Thanks again, Brian. That was a really fascinating talk.
00:58:07.018 - 00:58:08.514, Speaker B: It was my pleasure. Absolutely. Thank you.
