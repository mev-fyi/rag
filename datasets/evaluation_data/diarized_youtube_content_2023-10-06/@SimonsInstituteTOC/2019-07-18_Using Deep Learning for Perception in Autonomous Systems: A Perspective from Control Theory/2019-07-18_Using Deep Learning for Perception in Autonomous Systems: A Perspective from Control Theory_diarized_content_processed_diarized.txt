00:00:02.160 - 00:01:32.466, Speaker A: Maybe this talk is a little bit of an anomaly or a shift from some of the other talks that have been in the workshop. But Alex asked me to speak about our experiences using deep nets in a feedback loop for a real control system. And that's really been our interest and the interest of my research group, really, since it's inception, we've been interested in safety and control and how you design mathematical and computational tools to verify safety of a particular control system. And we've been working historically on safety critical control systems, like air traffic control systems, flight control systems. And in this problem, as you'll see, we'll be focusing on navigation using perception in the feedback loop. Okay, so, just to give you a little bit of a perspective, this is one experiment that we've done. So we have a robotics lab.
00:01:32.466 - 00:02:17.556, Speaker A: We're flying quadrotors, and we're back to Quadrotor to basically follow a step trajectory in the lab. And we precomputed, we don't really need the audio.
00:02:17.700 - 00:02:18.100, Speaker B: Okay?
00:02:18.132 - 00:03:01.078, Speaker A: So, one of the tools that we've been developing in my group for a long time is that of reachability and reachable sets. And the idea is really quite intuitive. Suppose you have a system, a dynamic system like a quadrotor flying, and you have a dynamic model of that system. And you also. Let's start by assuming that you know the environment that the system is flying in, then you can use those dynamics and you can use a model of the environment to basically compute under different control actions and with respect to disturbances which may act on the system. What the. Like a preview of what's going to happen with those dynamics by computing a forward reachable set.
00:03:01.078 - 00:03:51.902, Speaker A: Or you could ask, are you going to ever reach unsafe conditions by characterizing those conditions as a set in the state space of the system and computing a backwards reachable set using those that dynamic model. So, we've been doing a lot of work in my group on numerical. So, first of all, mathematical and then numerical methods to compute these using optimal control in differential gains, we solve a Hamilton Jacobi Isaacs equation and we compute level set. So we compute a value function through that, and then we compute level sets of those, and we use those to characterize this reachable set. So, in this experiment here, what we've done is we've precomputed a reachable set for this system. We just want it to follow a fairly simple step trajectory in the lab. We don't want it to crash into the floor or fly into the ceiling.
00:03:51.902 - 00:05:03.504, Speaker A: So it's basically an aerodynamic flight envelope. And we precomputed that under nominal model. But then halfway through the experiment we switched on, so it's just starting again now, a large fan which is introducing a disturbance in the lower part of the room which is going to affect the dynamics of the quadrotor. That disturbance is not modeled and it wasn't taken into account when we pre computed that reachable set. So what we're doing here, this is some of our first work in safe learning, and you kind of see, you know, two experiments that are run sequentially or superimposed simultaneously with the kind of ghosted out quadrotor that's being jutted around. That's what happens if you just keep flying the quad rotor within that precomputed reachable set. The other quad rotor is using a safe learning technique that we developed where when the quadrotor, which is measuring its own behavior, detects that it's being buffeted around, it shrinks the reachable set to the point at which the disturbances that it's measuring comply with the safety certificate that is used from the precomputed reachable set.
00:05:03.504 - 00:06:12.002, Speaker A: So it shrinks the reachable set to a safe region that complies with its current measured values of that disturbance. So it's a technique that, and you can prove it safe under certain assumptions of how the disturbance enters the dynamics. So the wind shift that's acting on that quad rotor is buffeting it around a bit, but it still is bounded, and it's considered a disturbance, which is, it's still can be bounded with a lipschitz constant in the dynamics of the system. So it works for relatively mild disturbances. It doesn't work, for example, in the kinds of problems that we're really thinking about in this workshop or in navigation problems where you're just discovering things in the environment. So for example, you've got a robot that's navigating. It knows where it wants to go, but it has no idea or it's got very little idea of what the environment around is going to be as it navigates towards the goal.
00:06:12.002 - 00:06:20.164, Speaker A: And that's where we really need to introduce and include perception modules in the feedback control loop.
00:06:21.104 - 00:06:24.884, Speaker C: Are you computing the reachable sets, recomputing them on the fly?
00:06:25.464 - 00:07:11.896, Speaker A: In this experiment? We're not recomputing the reachable set. We precomputed a reachable set, and then we're shifting to a different level set of that reachable set. So we're doing that in real time. And what I'm going to show you a little bit later in the talk, we're recomputing the reachable set at each goal. Other work that we've been doing, and this is another reason that we'd really want to use perception in the loop, is unknown environment, but also people are very unknown. So autonomous systems that are navigating around people. And we've been doing a lot of work jointly with Ankar Dragon's group here at Berkeley to develop models which allow us some sensible predictions of how people are moving in different kind of structured scenarios.
00:07:11.896 - 00:07:47.900, Speaker A: So, for example, here we've got Sylvia in my group who's moving around. There's a quadrotor. These are our smaller quadrotors. They're planning trajectories, and they're planning collision free or probabilistically collision free trajectories based on a bayesian update that we're performing on the prediction of how she's moving with some pre stated goals and some understanding of how she's moving between the goals. Okay, so that's just to give a little bit of an overview. But the context here is dynamic systems. You know, we care about safety.
00:07:47.900 - 00:08:29.314, Speaker A: We're interested in learning safely, but now we've got this sort of open arena of problems where the things we have to learn are not characterized very well. So unknown environments, people in the environments. And we want to be able to try to either push our tools or develop new tools which will allow us to use kind of the best in perception, but still within a feedback loop. Loop. And that's really, you know, over the past two years, as we've been working on this. So this is Samuel Bansal, who's here in the group, who's here in the group, and also here in the talk, has been developing this. And I'm going to spend the first part of the talk talking about kind of our lessons learned in doing this.
00:08:29.314 - 00:09:49.644, Speaker A: Okay, so the first part of the talk we call supervising learning using optimal control. So this is navigating in unknown environments as an example. But how might we actually train a perception neural net when we know something about optimal control, and we know how we'd want to control this thing optimally if we knew what the environment was? And then after that, I'm just going to switch in the latter part of the talk to how we integrate safety, the kind of reachability based safety that I talked about already when it comes to the vehicle dynamics. So what I'm not going to talk about today is, I think, and this is why I said it's a little bit of a shift or an anomaly, given the rest of the talks, is the really important problem of proving safety of the neural net itself, that you're actually getting what you expect as you apply test data to that neural net. You're not going to have the kind of classifications that we've been seeing in some of the talks in this workshop already. And another point is that we're part of this work, as I'll say at the end, is funded. I'm running this DARPA contract under the assured autonomy program, and we're also funded by the Navy.
00:09:49.644 - 00:10:29.068, Speaker A: In fact, Bezad is here. And here the problems are really trying to look at real autonomous systems aircraft and have them navigate safely around other aircraft or other obstacles, for example. So we really care if it's classified as an aircraft or a pig. Okay, so let's go to the. Let's sort of start with this first part of the talk and start with a motivational example. So, as I said, this is work that Sameil and Varun have been leading. It's jointly with Jitendra Malik and Sara Gupta in Jitendra's group, who graduated last year from Jitendra's group.
00:10:29.068 - 00:11:04.094, Speaker A: And this is the version of the paper that we have. So this is one of the robots in our lab. It's a turtle bot. It has a number of cameras on board, but for the purpose of these experiments, we're just going to use an RGB camera. So one of the inputs to the system are just pixelized images, so 224 by 224 images, that we're getting over time as this robot is moving through the environment. And the setup is that it knows where it is, it knows where it wants to go in its own body coordinates, but it doesn't know the environment around it.
00:11:05.634 - 00:11:07.922, Speaker C: There is a GPS type of sequence?
00:11:07.978 - 00:12:06.806, Speaker A: Yeah, well, it's not a GPS, but there's an odometer on board. And so we're going to assume for the purposes of this talk that it knows it has good measurements of its own inertials. And that's actually something that's a fairly easily solved problem. So we can make that assumption either indoors or outdoors now. So, in terms of the setup, and I think an interesting question here, and we've gone through different iterations of architectures, but is what kind of architecture that we should be considering here, what kinds of architectures? And as you'll see in our comparisons, there's been a lot of work in the practical, deep learning community on end to end learning. So that basically means going from images all the way down to control actions. So the neural net is learning that pipeline from images to control actions.
00:12:06.806 - 00:13:04.874, Speaker A: And as a control theorist, we have a kind of intuition that part of that we don't really need to learn. We typically know well the dynamics of the system that we're controlling. And also, there's been a huge amount of work done in planning over the last decades, so we can really take advantage of a lot of this planning and control work and just use learning where it's needed. And so the architecture that we're proposing and the one that we've been working on really, for about the past year of a two year project is one in which the neural network is taking in images. It's also, as I said, it's got its own measurements of its own inertials, so it's also taking its own current linear and angular velocities. This is the robot and information about its goal position. We're going to assume that's given for now, and it's going to as an output.
00:13:04.874 - 00:13:47.664, Speaker A: Rather than outputting control actions, it's just going to output a waypoint, a next way point to go to. And we've got this in a loop so that there's a time horizon, which is typically shorter than the time it takes to go to the goal. So we'll basically be iterating this over a number of time horizons until it reaches or doesn't reach the goal. So the perception module, rather than going from image to control action, is going from image to waypoint. And then we're developing the control pipeline or the planning and control pipeline that takes that waypoint and gives, you know, a plan and a set of sensible control actions. Okay, so I have a few things to say about this architecture. Yes.
00:13:47.784 - 00:13:50.328, Speaker C: So it does know its current position, right?
00:13:50.456 - 00:13:51.960, Speaker A: It does know its current position.
00:13:52.072 - 00:13:56.504, Speaker C: And the theta hat over there, is that the angle?
00:13:56.584 - 00:14:57.952, Speaker A: Yeah. So the robot model that we're using here is a very simple model, and we're assuming in this case, and I think this is a point that we, we've been discussing a lot, is what should the representation of that waypoint be? It should comply with the model or the dynamics of the system. So if it's a very complex walking robot, maybe there are more states that you want to include in the waypoint than just the xy position in the current orientation. But it actually makes a big difference for even this robot if you include just XY in the waypoint or XY theta. And so there's a lot of trial and error that we sort of went through to figure out what works well. But I think a very interesting question is when we're proposing an architecture where all of the perception is then collapsed into a waypoint and then the waypoint provides an input to the control architecture. What really should the representation of that waypoint be for different kinds of dynamics or different kinds of tasks that you have? Yeah.
00:14:58.008 - 00:14:59.720, Speaker C: Is the waypoint to be treated as.
00:14:59.752 - 00:15:01.848, Speaker D: The destination to reach for now?
00:15:02.016 - 00:15:29.684, Speaker A: Is the destination to reach for now? That's a good way to describe it. It's something that you're going to start moving towards. And maybe it's also we have this parameter h, which represents the horizon over which you're going to apply the control that you generate to reach that waypoint. But that may be shorter than that may not be. You may not actually ever get to that waypoint depending on how you design the waypoints and how you design h.
00:15:31.304 - 00:15:41.668, Speaker C: I still am a bit confused. So what is the role of the neural network part of this whole system? It's supposed to see the screenshot and what is supposed to give you.
00:15:41.716 - 00:15:44.224, Speaker A: It's supposed to give you a waypoint.
00:15:44.604 - 00:16:00.340, Speaker C: I see. But like. Because, like where do you. Because like I guess there are two, three questions that are curious, like where. Where you are, which you say that is already solved. So this is not a problem. What is in front of me? Can I actually, you know, go forward or not? Yes, actually.
00:16:00.340 - 00:16:06.482, Speaker C: And actually, like what is your regarding getting to the goal post? So where the last one is addressed.
00:16:06.538 - 00:16:39.654, Speaker A: So it's the second problem of your three problems that we're focusing on. So the first problem we've taken away, maybe it's an important problem, but it's basically solved. The third problem is something that control theory has treated for decades. We don't need to solve that problem. And that's really my point about model based learning versus end to end learning. It's the middle problem, which is the hard problem that control theorists have kind of ignored, and I think that's wrong. And I think we should be working together with people doing learning and people doing perception to come up with these architectures.
00:16:40.474 - 00:16:43.854, Speaker E: So this is a pseudo reinforcement learning?
00:16:44.234 - 00:17:15.834, Speaker A: Yeah, yeah. Well, no, we don't use. So we're using supervision, so we're using supervised training. But it's interesting to compare this to reinforcement learning. And so I'm not going to say it's reinforcement learning, but you could say pseudo and I'll describe why. Okay, so I want to say a few more things about this before we go on this architecture. So as I said, there's a perception module, there's a perception module, a planning module and a control module.
00:17:15.834 - 00:18:08.984, Speaker A: The planning module, again, we went through different iterations of what to do, and we tried out many different things. In fact, maybe I can say this end to end learning works really well. And as control theorists, we were a little bit surprised, because we thought immediately when we start introducing model based control instead of end to end learning, like when we say, you don't have to learn this, just put models, put good planning algorithms in and do it, it became very challenging to actually meet the same specs as end to end learning. And it took a long time to be able to understand, you know, and to tease out the differences. So the planning module that we have now that we're now using here, and by no means is this the only thing we could do or not. You know, maybe not even the best thing we could do, is a spline based planner. It's a simple planner, the kind of thing that's been used a lot.
00:18:08.984 - 00:18:44.134, Speaker A: We're using third order splines. Partly that's because the dynamics are fairly simple. In fact, for those who know control, this is a differentially flat system. So if you use a third order spline, then you're basically capturing all the dynamics of that vehicle. So we're using, you know, if you have position and speed and acceleration characterized in your spline, you've basically characterized the whole path of that dynamic system, which is, of course, a simplification of the actual robot. But it's a pretty good model. So that allows you, when you're doing the planning, to come up with a set of linear equations that you can solve online.
00:18:44.134 - 00:19:11.284, Speaker A: The control we use is standard. We're just using an LQR control, and it's linearized. So to get the linear in LQR, we're linearizing these dynamics around the path around the spline based plan that we've computed. So that's very standard in control. And as I said, we tried out a number of different architectures, and this is the one we're presenting now. Okay, so the training. So this is the important part.
00:19:11.284 - 00:19:36.144, Speaker A: The idea is that the really hard thing is looking at an image and recognizing where to go. That's what the neural net is going to do. Just to say a little bit. And I'll come back to this. We're using a convolutional neural net. We tried out different pre trained architectures, which is standard now in computer vision. We're using a Resnet 50 architecture for the neural net.
00:19:36.144 - 00:20:24.784, Speaker A: And we, as I said earlier in the talk, we're supervising the learning using optimal control. So let me tell you a little bit about what that means, and I'll talk about how we train the neural net to sort of describe that. So sort of physically, for this navigation problem, we used to train the neural net, a set of databases that Stanford provides. It's this 3d Stanford 3D indoor spaces. They've, they've built up a number of textured environments of a number of their buildings at Stanford. And these are, it's a database that everybody has access to. So we used two of those indoor spaces to train the neural net, and then we held out one of the spaces as a test set.
00:20:24.784 - 00:20:54.470, Speaker A: In general, when you're running this, the location and shape of the obstacles is unknown. But in training, we're going to assume that. So for training, we assume that we know the location of the obstacles and the shape of the obstacles, and we develop a cost function. So an optimal control. The cost function here is the distance to the obstacles. So we know where we are in terms of the robot. Distance to obstacles, we want to avoid those.
00:20:54.470 - 00:21:12.294, Speaker A: And the distance to the goal, we know where the goal is. So we've got, you know, we're trying to go towards the goal and avoid the obstacles. So we're just using a very standard cost function for the optimal control problem. And then the optimal control problem is to find a waypoint which minimizes this cost.
00:21:12.454 - 00:21:18.514, Speaker C: So you're linear in. The cost is linear in the distance to the obstacle and not kind of increasing.
00:21:19.494 - 00:21:33.444, Speaker A: No, we're using euclidean distance, and likewise, we're avoiding. So we're trying to get to the goal and we're trying to avoid the obstacles, but it's trying to get maximum.
00:21:33.484 - 00:21:35.384, Speaker C: Margins from the obstacles.
00:21:37.404 - 00:22:05.452, Speaker A: Well, it's trying to essentially. Yeah, but you can weight the, you know, you can choose the weighting on the goal versus the obstacles. So. And I'll say a little bit about that. But when you don't weight the obstacles enough, then as optimal control tends to do it, we'll try to, like, you know, skirt corners to be able to get to the goal. That's not such a great thing to do when you're training a neural net, because then it tends, the actual thing tends to bump into corners a lot. So.
00:22:05.452 - 00:22:08.524, Speaker A: Yeah, but essentially, we're using a very simple cost.
00:22:08.604 - 00:22:11.944, Speaker D: Is there some radius beyond which you don't care for the obstacles?
00:22:12.684 - 00:22:19.384, Speaker A: Well, yeah, I mean, just physically, the radius of the robot itself, when you're.
00:22:19.924 - 00:22:20.876, Speaker D: You don't care about.
00:22:20.940 - 00:23:05.572, Speaker A: Yeah. And so we could do something a little bit more complex, and maybe this is what you're asking that we just, you know, we don't, the obstacle isn't going to have any effect on us if we're far enough away, but right now that's, we're using something simpler than that to evaluate this. So we're comparing this with end to end learning, because, as I said, that's kind of our goal, to be able to see if we can do better, because intuitively that's what we expect. We also trained a similar neural net for but to go all the way from images to control actions. So we trained a Resnet 50 for end to end learning. And that was trained using, instead of using waypoints, we use the smooth control commands that are generated from this architecture. So we have two neural nets.
00:23:05.572 - 00:23:40.164, Speaker A: One is the one we're developing and one is for comparison. And then overall, when we use that optimal control, so we use that optimal control for training. When we use it to train the neural net, we're using an MSE loss as an objective function, which is on the waypoint position. So this is what the expert would give for waypoint. This is what the neural net would give for waypoint, and that's our loss function. And then in the case of end to end, it's just the control. What's the difference between the two controls? Okay, so that's what I wanted to say about training.
00:23:40.164 - 00:24:00.994, Speaker A: And so now we take that. So now we don't know where the obstacles are. We're just putting this robot in an environment, knows where it is. Again, it knows we're using the body frame coordinate, so it knows where it is in its body frame, knows where the goal is in its body frame, has its inertials and it takes an image and it has to figure out what to do.
00:24:01.414 - 00:24:13.726, Speaker C: So I'm not sure you got the training. What are the labels in the training examples? Do you get? It seems like you need waypoints as labels.
00:24:13.870 - 00:24:31.236, Speaker A: So in the training examples, you're still generating the waypoints. So you know the environment because you have the database, you know where the obstacles are, and you generate your waypoints and then, so that's what's, that's the, that's the data that's used in training.
00:24:31.300 - 00:24:33.884, Speaker C: Okay, so you have some data that gives you waypoints.
00:24:34.044 - 00:24:44.464, Speaker A: Well, we get waypoints using our methodology, but it's using our methodology with known environments and known locations of obstacles.
00:24:48.704 - 00:24:54.744, Speaker E: So the Stanford data set, they're point cloud, right? So that's a three dimensional.
00:24:54.904 - 00:24:59.764, Speaker A: They do, but this is, they also have textured meshes and we're using textured meshes.
00:25:00.704 - 00:25:06.560, Speaker E: How do you handle more than one ideal path, like a multimodal correct path?
00:25:06.672 - 00:25:54.004, Speaker A: Yeah, that's a good question. So a number of waypoints may, I mean, we're solving this optimal control problem, but we're also constrained by the, you know, the waypoints that we can generate. So we select, I mean, we may come up with several waypoints that are good and we, you know, we select one of those that is conflict free. However, where I have a point to say about at the end, about that, after we talk about reachability, you know, we may actually be able to use safe learning in the context of training this neural net. I'll talk a little bit about that at the end. All right, so this is a comparison with end to end. So ours is the red model based and gray is end to end.
00:25:54.004 - 00:26:34.492, Speaker A: So we're getting about, if you look at the top plot, about a 25% improvement in success of reaching the goal than end to end learning. And then if you take those successful runs, so just the ones that are successful for both end to end and are model based, these are the kinds of comparisons, so less time to reach the goal. Average acceleration along the trajectory, lower. Average jerk along the trajectory, significantly lower. And so this is starting to see what we would expect from control theory. Like, if you're including a model, I mean, that's the, the dynamics. So you're going to get smoother trajectories, you're going to do things that adhere to the dynamics of the system more.
00:26:34.492 - 00:27:23.734, Speaker A: So here are a few plots just for a couple of different tasks where, you know, you've got starting out position and the green is the goal position, and just the comparisons with what the method, our method does model based versus end to end. And so you see this sort of difficulty, for example, in turning sharp corners with end to end or doing things that require a sharp decision once it gets that image of something in front of it. And as you might expect, the control profile for end to end is much smoother. So you see less jerks, less kind of changes, sharp changes in velocity in the red than you do in the gray.
00:27:24.724 - 00:27:32.356, Speaker D: I'm sorry, can you remind me of what exactly is the way you capture the distance to the obstacle and the cost function?
00:27:32.500 - 00:27:44.784, Speaker A: It's just the euclidean distance. So euclidean distance to goal and then a negative weight on euclidean distance to obstacle.
00:27:46.604 - 00:28:42.348, Speaker D: I guess what I wonder about is you have settings where it doesn't truly tell you not to crash or something like you sort of. I guess I'm thinking of something like the scale of your penalty should go up dramatically as you approach the obstacle. I'm thinking of a situation where I would have two obstacles and like the sum of the euclidean distances would be the same. Now it's possible I fundamentally misunderstand something. I just want to understand. What I'm saying is I can imagine I have two obstacles and if I'm looking at the sum of the euclidean distances, then being right next to one of them or right next to the other one is just as good as being in the midpoint. Whereas if, I said the closer you get, the scale is going to go up like super linearly, then that would push me.
00:28:42.356 - 00:29:08.700, Speaker A: So I wonder if that's a good question. And I should say we did. So we've tried out different costs. So assign distance, euclidean distance with coefficients. Yeah. So, but we could mimic that with like the, like the relative weights on those individual cost functions so we can, you know, scale the importance of avoiding obstacles versus going to the goal.
00:29:08.732 - 00:29:24.340, Speaker D: For example, what I'm saying is if you have two obstacles, then the weight can't, whatever the weight is on avoiding obstacles is the same. And like this is, how do I feel about being in the middle versus.
00:29:24.532 - 00:29:48.514, Speaker A: Being right up against one obstacle? It's a good question. And these are questions that come up in planning and control anyway, but you've still typically got your weight that's pulling you towards your goal in that case. So there's typically something to break the asymmetry. But in a perfectly symmetric case, you might run into some of that back and forth.
00:29:49.254 - 00:29:52.474, Speaker D: Nullifies the obstacle avoidance, potentially.
00:29:53.574 - 00:30:08.554, Speaker A: Again, yeah, no, it's true, but it depends on the weights, the relative weights. So these issues can come up and those are issues that exist in, you know, planning and control. Anyway, so yeah, you said you fail.
00:30:08.594 - 00:30:10.178, Speaker C: In 18% of the events.
00:30:10.226 - 00:30:10.794, Speaker E: Do you know why?
00:30:10.834 - 00:30:11.974, Speaker C: And can you do better?
00:30:12.274 - 00:30:44.058, Speaker A: I'll show you some of the failures, but that's a good question. We're typically failing in places, you know, some of them we just don't know. And that's actually, you know, something that I'm interested why I'm attending this workshop. A lot of it is. Here's one example. We're training typically, so we're using a held out data set from Stanford for test, and then I'll show you some test results. So where we just took that same architecture, did no other training, so it's just trained in simulation and we applied it on a real robot here at Berkeley.
00:30:44.058 - 00:31:54.606, Speaker A: So typically you'll see that the failures occur when you see chairs that are a little bit different than the ones that you trained on. So, for example, in the held out test set, they have different shaped chairs with some kind of metal bases that swirl where you don't see those so much in the trained set. So those are the kinds of things, they're perception, largely perception errors, where the thing just gets too close to the obstacle because it's not recognizing that it's an obstacle. I just want to say very quickly that not only did we compare this with an end to end learning scheme, we compared it with the traditional pipeline in vision and control, which is slammed. And that's very important to do. I mean, this is a typical slam based scenario. But here, you know, and I think this is an important point for learning is that and why computer vision has basically adopted deep learning as opposed to these more traditional pipelines, is that the, you know, the sensor failure in slam is one of the biggest reasons that slam fails in a lot of cases.
00:31:54.606 - 00:32:28.440, Speaker A: And this just shows the RGB image and what a typical depth sensor. So we're running a fairly recent slam algorithm here to do the comparisons. And in all of these cases, these are cases where the black indicates where the depth is not known. It's typically around an obstacle. So failures, and it's not just things like this. Smudges on the camera. We work a bit with Skydio, which is that company that's designing basically a flying camera that tracks you as you're doing sports events.
00:32:28.440 - 00:32:58.834, Speaker A: And if you touch the lens, you get a little smudge on the lens. Immediately, slam algorithms start to fail where the learning algorithms don't. And so you can see the kinds of comparisons of the model based scheme versus. Well, this is end to end. But even a traditional slam pipeline, and very much over a memoryless slam pipeline, it should be stated that our method is memoryless. So it seemed like a good comparison to do. Just let me show you some of the simulation results.
00:32:58.994 - 00:32:59.770, Speaker E: One question.
00:32:59.882 - 00:33:00.434, Speaker A: Yeah.
00:33:00.554 - 00:33:09.694, Speaker E: If we do an exhaustive end to end learning, could it reach the same level of performance as things as the model based?
00:33:10.674 - 00:33:57.824, Speaker A: That's a good question, I believe. Yes, because the failures are in cases where you know that they're failures which appear to be at that intersection of perception and control. However, the amount of data that you would need to train, I believe will be much more than a model based scheme. So I think the sample efficiency of one of the reasons to use a model based scheme is that intuitively, you should do less training. And that's what we're seeing here. We're doing an apples to apples comparison in as much as we can in terms of the training data used in. We used exactly the same training data in all of these comparisons.
00:33:59.644 - 00:34:14.972, Speaker E: When I think about your training method that suggests waypoints, I think underlying that might be a dynamics, like there might be an implied dynamics there. And could it be a problem if your test time dynamics are significantly different than your implied training dynamics?
00:34:15.028 - 00:35:06.256, Speaker A: That's a really good question. So if the test time. So we're assuming a model here for this turtlebot, which is pretty good, you know, it's a four dimensional dynamic model. We're trying this now. So Koshel Srinath, who's in our me department and works on walking robots, is really interested in applying this to his walking robot with a camera on board. And we're asking so should we be changing the waypoint or is a position and an orientation kind of a good encapsulation of anything for a navigation problem? I don't know. I think that the waypoint has to be somehow it has to capture something rich enough about the dynamics, but ideally be simple enough so that this algorithm and architecture can be transferred from one robot to another in one task to another.
00:35:06.256 - 00:35:31.540, Speaker A: But obviously if your tasks involve a robot that's going to the kitchen, it's got an arm, it's got to do some taking a knife out of a drawer. The waypoints have to be more rich in terms of the task itself. So for navigation, perhaps this is a good waypoint to use. I don't know. This is something that we're exploring now for more richer tasks. It's obviously not going to be enough. Yeah.
00:35:31.732 - 00:35:47.316, Speaker B: For navigation, I guess something I would curious about is why you're using the full image. Like do you think about modifying the images? Because it doesn't really matter if it's a bike or a chair. Right. So is there not a way to sort of either filter these images or use only depth sensors or something?
00:35:47.420 - 00:36:21.412, Speaker A: That's a really, really good question. It seems. Seems to matter, although, you know, it's a neural net. So again, we don't know, it seems to matter whether it's a bike or a chair. But does it matter? You know, does in terms of it recognizing, because in some way we're just training it to recognize automatically that it's got to avoid these things, but it shouldn't really. And so we've been doing experiments now where we're holding out parts of the image and asking, you know, what is the importance of relative parts of the image? It's very experimental. So it's.
00:36:21.412 - 00:36:23.828, Speaker A: I don't know. I think it's an excellent question.
00:36:23.916 - 00:36:41.060, Speaker B: I guess what I was saying was that if you're having trouble navigating the robot in a different environment with like a different color chair, for example, maybe color isn't a feature that should be factored into actually getting the best way point. Or maybe a depth sensor would actually minimize the difference between the chair shape.
00:36:41.212 - 00:36:43.948, Speaker A: Yeah, I totally agree with you, and it's a really good point.
00:36:44.116 - 00:36:49.514, Speaker E: Sorry. From that, from the picture over there, it seems like you using a stereo camera.
00:36:49.554 - 00:36:50.334, Speaker C: That's the.
00:36:51.274 - 00:37:07.450, Speaker A: Yeah, it is a, it is a stereo. It is an RGBD camera. But we're just using the RGB image here. I mean, we compared it with slam where we did use the d for slam. So we're, you know, in that comparison. But for this architecture, we're just using RGB.
00:37:07.642 - 00:37:10.106, Speaker E: No classical, like object recognition. Object.
00:37:10.170 - 00:37:34.190, Speaker A: No classical object recognition at all. All right, let me just run this. I've got a couple of experiments to show you. So, you know, this is what the robot sees. So these are just for your own benefit. The robot doesn't see these, but you can see. So it's asked to go from this point, which it knows, to a goal, which it knows, and it doesn't know what's in between.
00:37:34.190 - 00:38:17.768, Speaker A: It's got a hallway with some bikes, it's got some chairs that it has to go around to be able to get to the goal. So you can see, I think what's nice, and we noticed immediately, when you compare with end to end, is the smoothness of the control profile as opposed to the kind of jerkiness you see in the end to end learning approaches. Here's one which I like to include because it shows something about a little bit towards semantic understanding. So it has to go out of the lab and into the hallway. And to do that, it has to recognize that it has to go through a door. We never trained it to go through. Well, I mean, it was implicitly trained to go through doors using the architecture that we developed.
00:38:17.768 - 00:39:06.754, Speaker A: When it comes into the hallway, immediately, it's in a very shiny floor with a lot of sunlight. That doesn't, that kills the slam, but that does. Well, here, it doesn't even care about the shininess of the floor. And, you know, the environments are static in this set of experiments, but we tried the same architecture on a somewhat dynamic environment. So here is Varun, one of the lead authors on this, who is just moving the chair around based on where the robot's going to get towards its goal. So we're not saying we're working on dynamic environments, but you can see a little bit of a start towards a generalization, and dynamic environments is actually what we're working on now. Yeah, I mean, it's smooth because it's using control, so it makes sense.
00:39:06.754 - 00:39:42.918, Speaker A: So finally we got some results that seem to make sense with what we would sort of intuit. Let me just talk about some lessons learned. You know, I started a bit late, but I don't want to go too far. So I do have a second part of this talk on control. I'm probably going to skip that, I think, just because I think this is actually the point that I wanted to get across some of these. I've already said so. Data representation is important whether we use an inertial view or whether we use a body view, how we represent its ego coordinates and how we represent the goal.
00:39:42.918 - 00:40:23.348, Speaker A: It appears to be much more efficient to use an egocentric representation. And that's actually quite interesting from a planning point of view. We have a parallel project that I'm not talking about here with Jack Galant in cognitive psychology, who works on how people. He puts FNIR's caps over people in simulation car driving environments, and he asks, what are the features that are important when people are planning? And so this is something that we're now trying to understand. Can we take some of these ideas to use them in autonomous system planning? As I said before, optimal control can be too optimal. Again, this all depends on the cost function we're using, and we talked a little bit about that. But, you know, it tends optimal control.
00:40:23.348 - 00:41:23.024, Speaker A: If it can, it will tend to, or it depends on how you set up the cost function. But, you know, if you set it up so that it skirts corners, just that's typically not a good idea if you're using it in supervised training of such a neural net. The waypoint representation is important, just using position versus position and orientation. It was actually a very big deal in achieving success here to use orientation. And I think a lot of you know this because you're neural net people, but I'm a control theorist, so building on existing neural net architectures was very important. The computer vision community has a whole set of pre trained neural nets that they use and that have been optimized, and there's a lot of tools and methods that have been developed over time that work really well. So working with Jitendra's group was really important here to understand, or maybe not understand, but sort of get the idea about good starting points and what we should do.
00:41:23.024 - 00:41:53.312, Speaker A: This makes sense. It's very important to consider image and perspective distortions during training. So don't just use the actual data, but distort it. And so that's something that we understand from control. And this comes back to the reinforcement learning point. We're not using reinforcement learning here, but in a sense, you could consider this whole architecture doing reinforcement learning. When we thought that once we had this architecture, we could improve on it by just applying reinforcement learning and it got worse.
00:41:53.312 - 00:42:08.496, Speaker A: And this is something that we're now exploring in more detail. It's not clear why. Again, this is empirical, but RL unsupervised learning at this point appears to be worse than just the RL architecture I showed.
00:42:08.600 - 00:42:40.994, Speaker D: When you say improve on it using reinforcement learning. So this question is like sort of, how is the system set up? Is the whole architecture, the entire pipeline set up such that you could have what would be say like a deep reinforcement learning system that is initialized at truly having the exact same, like the sort of case where you would think you could strictly improve would be when your RL system could be initialized to be doing like what you are doing.
00:42:41.334 - 00:42:58.486, Speaker A: That's a good point, but we, I mean, this architecture, I mean, we talked a lot about it, but it's fairly generic and, you know, we're allowing the system to start off in any initial condition, different configurations, and it's a fairly simple problem. Just reach a goal through a number.
00:42:58.510 - 00:44:01.922, Speaker D: Of waypoints, I guess, at some point output, multiple waypoints, and then you have this procedure for choosing one. Yeah, although the RL says to deep reinforce lines, like the only, at least I'm probably not behind the times. But the two ways I know of having it is like you either have a value function and then your policy is given by choosing what you think is going to be the action, that's going to be the highest q value or highest expected q value. Or more common when you start off with some kind of behavior cloning or something, your model set up as a continuous function whose outputs are, you know, can be interpreted as a probability distribution over your possible actions. And so what I wonder is just because it seemed like there were some components of the decision process that weren't like, you know, like you had this in some like natural language stuff or whatever, where someone initializes a model that is trained as a supervised model, so they're doing like some behavior cloning and it's natural to strictly improve on it with RL because you say, okay, the model's already discontinuous mapping, that is.
00:44:01.938 - 00:44:03.546, Speaker A: I see what you're saying giving me.
00:44:03.570 - 00:44:19.842, Speaker D: Probabilities at the end. And so I just applied some of that policy gradient on top of that. I guess what I was just wondering is like, how do you get the starting point for the RL giving that the pipeline which I don't fully understand all the details of. So I'm not pretending that I. Maybe it is a very clear answer. I just curious.
00:44:19.898 - 00:44:55.610, Speaker A: Yeah, I think that maybe, to answer your question. So we're using, when we apply reinforcement learning on top of this, we're using the same kind of structure, the same kind of cost function. But what you're saying is unless you're specifically at that particular point and you're looking at that particular trace and you look at what the values of the cost function are, maybe what you're doing in reinforcement learning is not going to measure it with what you attained already using the supervised learning procedure. And that's true. I don't know. It's something that we've just. We looked at in no deep way yet.
00:44:55.610 - 00:45:04.494, Speaker A: So as I said, I have a secondary point and I think I'm just going to use this question.
00:45:05.514 - 00:45:08.894, Speaker E: Did you try to use some amount of memory?
00:45:09.234 - 00:45:10.014, Speaker A: Yes.
00:45:11.314 - 00:45:15.394, Speaker E: Picture from a second ago inside the same architecture?
00:45:15.474 - 00:45:46.580, Speaker A: Yes. In fact, that's something that, since we completed this stage of the work, we've been spending more time on incorporating visual memory into this. And that's, I would say it's ongoing. I'm not sure how. Well, I'm not sure how much improvement we're going to get using visual. It's clear we'll get some improvement. In fact, one of our co authors, Sara Gupta in Jitendra Malik's group has worked on a visual memory pipeline and that's the algorithm that we're incorporating.
00:45:46.580 - 00:46:25.510, Speaker A: I think it's going to improve it a little bit. It's not clear that it's going to be groundbreaking in terms of improving it, but that's a good question, too. So just to conclude some of the. And it comes back to this question, what are the cases in which it fails? Those 18%? So here's a case where the chair here. So this is, again, we took the results from training and simulation and no real experiments in training and we just applied it in a real environment as you saw in the previous movies. But here's a case of failure. So, you know, these chairs with.
00:46:25.510 - 00:47:06.054, Speaker A: It's got some kind of back, but then the bottoms stick out a bit. Those don't exist at Stanford. So trained at Stanford, trying it at Berkeley, different environments, it fails. So it's kind of what you would expect. I'm going to not go into detail about the later part of the talk, but I'm going to skip to the, skip to the end just to say that what we've been doing now is taking that same architecture that I presented and incorporating what we call kind of a safety filter or a safety verifier. But importantly, it's not on the neural network. That's the hard problem that this workshop is focusing on.
00:47:06.054 - 00:48:07.204, Speaker A: We've basically computed using a depth map and computing an occupancy map. So basically using a traditional slam pipeline to compute reachable sets like I discussed at the very beginning of the talk, and use those for safety verifiers. So if it's going to approach the bottom of that chair and it looks like it's getting close, the reachable set control is going to push it away from that, so it has a better chance of reaching the goal. And so that's part of our current work. So let me conclude by just saying that the goal here and the goal of this line of work is to think about sensible, reasonable, good architectures for incorporating perception into the control loop. We discussed a method for using supervised learning trained using optimal control. So it provides this perception planning and control pipeline where these are model based, and this is trained using optimal control.
00:48:07.204 - 00:49:02.602, Speaker A: So the neural net is just here. We compared it both with Slam as well as to end to end learning, and applied it to this vision based navigation task. And then in later work we've been using real time updates of this safe set, so that computation we're now doing in real time, taking advantage of warm starting and local updates to compute that Hamilton Jacobi filter in real time. And so to conclude, I'd just like to thank the members of my group as well as Jitendra's group, who worked on this. As I said, it's led by Samuel Bansal, who's here today, and funding primarily through DARPA. In fact, Bezad had a grant years ago that we amiri that with MIT. So it was Daniela Rus and me and Jitendra and others where we really had the kind of foundations of this work there and other folks.
00:49:02.602 - 00:49:04.054, Speaker A: So thank you very much.
00:49:09.064 - 00:49:15.944, Speaker B: Claire. So we're kind of over time. So let's just ask the speaker the questions during the coffee break. They'll copy break until eleven.
