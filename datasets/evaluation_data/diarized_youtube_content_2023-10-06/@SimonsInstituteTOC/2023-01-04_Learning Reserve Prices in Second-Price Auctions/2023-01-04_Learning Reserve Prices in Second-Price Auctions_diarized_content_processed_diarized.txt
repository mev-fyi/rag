00:00:06.320 - 00:01:03.314, Speaker A: Thank you for coming today. I want to share my recent work learning reserve prices in second price auctions. This work was done together with Pinyanhu and Cao Xiu. We will consider the most classic model in auction theory, single item auctions. In this model, the seller has an item to sell and he is facing earn potential bidders. Each beta I has a private value for this item and we assume that those values where I are drawn from possibly join the distribution team. In an auction, the builders need to report their values for the item and after that the seller will pick a winner based on the reported values and determines the winner's payment.
00:01:03.314 - 00:02:12.116, Speaker A: To get a better sense, let's take a look at a concrete example of second price auction with anonymous reserve mechanism. Suppose we want to sell artwork by council and suppose the seller sets a reserve price of $250 million on this artwork under suppose there are three potential buyers. Alice has a random value drawn from the uniform distribution between 100 million and 400 million. And suppose her outcome value is 200 million bob. Suppose his value is between also a uniform distribution and the outcome value is 300 million. And suppose Charlie also uniform random value and the outcome value is 400 million. Then let's see how the anonymous reserve for mechanism works.
00:02:12.116 - 00:03:48.388, Speaker A: First you say Charlie as the highest bidder will get it and its payment is the maximum maximum between the value of borrow and the reserve price. Why? Because you say the auctioneer first uses the reserve price to filter the low value banners. That is the Alice will be rejected by the reserve price but also for Bob and Charlie has higher value sensitive reserve and so say two joins runs the second price option and Charlie as the higher bidder gets the item and pays the value of Bob so his payment will be $300 million. Consider, consider another example. Suppose the reserve price now is raised to $350 million and considers the same outcome values over alias of Anna Charlie. Then the artwork again will go to Charlie and his payment now becomes the reserve price 350 million. Why? Because now Charlie is the only person whose value is higher than the reserve.
00:03:48.388 - 00:04:25.014, Speaker A: So he will definitely win and his payment is exactly the reserve price. So as you can see, a very natural and consider concrete example. Suppose the reserve is now $450 million. Then the item will auctioneer will buy in the artwork and no better between Alice, Bob and Charlie will get the item. So a natural question. Was a seller? Yes. Well suppose.
00:04:25.014 - 00:06:03.074, Speaker A: Suppose I know the value of distributions of alias, bob and trolley. Then I would like to optimize my reserve rights to the to maximize my expected revenue. And let's consider it points out that the second price auction with anonymous reserve is not as a revenue optimal auction and settle optimal auction is the well known Madison's auction. Madison's auction is restricted to independent values and the optimal revenues are characterized by virtual values. That is, you can say first the seller uses no value distributions to determine the virtual value functions defined in this way for every bitter. Then using the reported value, the option here will calculate the virtual values of every bit and the winner of this option is exactly the beta with the highest non negative for social credit, and the payment for the winner is his or her threshold value for which keep winning. And it's interesting to observe that in the case of I d value distributions, Madison auction degenerates exactly two second price auction with anonymous research.
00:06:03.074 - 00:07:37.282, Speaker A: But in practice, marathon auction is much more complicated and less used in impractical than second price option with anonymous research. Okay, and you say the mentioned models assume that the seller has the foreknowledge of all bidders value distribution. But in practice those knowledge is learned from historical transactions or other personal data. And it's a more realistic model would be assumed that the seller has just sample oracle access to the underlying distribution team. And a natural question first started by Koh and Roth Garden in 2014 is the sample complexity of options. That is, we suppose we have id samples tools value distribution and our task is to design our algorithm using those samples to obtain an option. And this auction should generate as a golden half for revenue.
00:07:37.282 - 00:09:04.274, Speaker A: That is, suppose in the case of Mary's auction, our outcome option a should achieve Epstein approximate approximation to the optimal revenue that is either multiplicative, uh, guarantee of one minus epsilon to the optimal revenue from the actual value distributions, or l tier for arrow with the band with respect to the optimal values. That is, in the case of merits option, we need to learn almost approximately optimal virtual value functions. And in the case of four second price auction with anonymous reserve, since all the designable part of such auctions is just the reserve price. So learning nearly optimal option exactly means lending a nearly optimal reserve price. So our task again is to get multiplicative one minus the epsilon approximation to the revenue optimal revenue or additive for revenue loss or for epsilon. Okay. And to make this problem interesting, we actually need some assumption on the distributions.
00:09:04.274 - 00:10:05.880, Speaker A: Why? Because without any distributional assumption, this problem will be treated is meaningless. That is, any algorithm has unbounded a sample complexity. Instead, four canonical distributional assumptions have been made in the literature. First is what zero one bunker supports value distributions. And for this setting we often configure additive arrow for revenue maximization. The second setting is that the value distributions are supported between an interval of one between one and very large value of edge. For these two boundary support settings, we can either consider independent value distributions or correlated values.
00:10:05.880 - 00:11:10.704, Speaker A: But notice that only for independent values, marathon option revenue optimal, the third and the fourth settings are about regular distributions and MHR distributions. These are two technical assumptions about single dimensional value distribution. And you say because of this MH. Because regular and MH are restricted to independent single dimensional distributions. For in these two settings, we implicitly assume that the value distributions are actually independent. There are hundreds of works on the sample complexity and other aspects for revenue maximization on this topic. Okay, let me give a summary of previous work about the sample complexity of medicine option.
00:11:10.704 - 00:13:52.494, Speaker A: There is a long line of works exactly starting this problem and the nearly tighter sample complexity was finally settled in the work by Cheng Huo Zhihuang and Xin Zhejiang in 2019. In particular, they prove that in the first bunker support setting, the sample complexity is in every setting, the nearly tight sample complexity has almost nearly linear dependence on the number of beetles and a polynomial dependence on Epstein the approximation arrow and in our work we adopt the technical framework by in this work by Wang and Zhang to set the press auction with anonymous reserve and obtain again obtains nearly tight sample complexity in each of these four settings. It's interesting to notice that this tighter complexity, sample complexity has no dependence on the number of bedders and the dependence on absolute approximation arrow is exactly the same in every setting as marathon option. Another interesting thing is that we notice that in the two bundle support settings, our algorithm and our sample complexity results does not require the value distributions to be independent. That is, even for correlated value distributions, the same sample complexity still holds, but for the regular setting and the MHR setting, because the regularity and the MHR distributional assumption, each itself will assume independent values. So for these two settings, our algorithm just work for independent values. Also, I need to mention that our algorithm in our work we just pull for the matching outbound.
00:13:52.494 - 00:15:21.974, Speaker A: That is, we gave algorithm and we pull both matching up bound the sample complexity as those results. In contrast, matching lower bounds are obtained in these two previous works. Okay, next, let me give you a high level description of our algorithm. First, recall that our algorithm has sample access to the underlying value distribution d and in total we have m and examples and indexed by k from one, two, three until m. Our algorithm runs in three steps. First, using those samples, we just pick the highest value and the second highest value from each sample and we use those first order samples and second order samples to build to construct the empirical first order and the second order cdfs. Next, we use these two empirical cdfs to construct the dominated empirical cdfs.
00:15:21.974 - 00:16:54.744, Speaker A: And the important part, the point of this construction is that you say at the two extremes of CTF, I mean while the CTF is close to zero or one, some way up more careful and we just treat the empirical distribution by a very small number. But at the mid part of every CDF we can be more broad and we can trigger a larger part of the CDF. The intuition behind these dominated empirical CDF CSS will you say the empirical, the original empirical cdfs are unbiased estimation of the actual value distribution. So it's an actual first order CTF and the second order CDF. And by shading them by a small amount with high probability, the dominant empirical ctfs are again it's definitely dominated by some the empirical distributions and with high probability they are also dominated by the underlying value actual value distributions. But because the way just shade each of them by a very small. So the dominated empirical cdfs are very close to the actual cdfs.
00:16:54.744 - 00:18:43.094, Speaker A: The actual value distributions the third step of our algorithm structure tool, we use the dominated empirical cdfs instead of the actual value distribution and we simply return the optimal reserve with respect to the dominated empirical distributions. This algorithmic paradigm is the same as the framework by Guo, Wang and Zhang in 2019. In particular, this poison's work designed such an algorithm for marriage auction and our main contribution is to shoot framework also works for second price auction with anonymous reserve. I mean it works not only for individual distributions, f one, f two until sn as for merit option, but actually also for some sketch for distribution. I mean in our case the first order and the second order cdfs are actually the sketch some sketches of the actual samples. Another interesting thing is that we should framework actually goes beyond independent settings of independent value distributions and we actually in the two bounded support settings. Our sample complexity of our algorithm holds even for correlation silo distributions.
00:18:43.094 - 00:19:46.094, Speaker A: Okay, to sign we needed to get to analyze how many samples are needed for our algorithm to get a good revenue guarantee. So the analysis takes two steps and the first step relies on the revenue monotonicity property. Also, our options and our anonymous reserve option requested a second price auction. With anonymous reserve. The outcome has two possibilities. First, suppose CA is exactly one beta whose value is higher than the reserve price. In this case, the outcome revenue of our auction is exactly the reserve price.
00:19:46.094 - 00:21:23.892, Speaker A: And in the second case there are at least two or even more bidders whose values all are above the reserve price and our auction is exactly the same as second price auction, and the outcome revenue will be the second highest value. So we can rewrite the outcome, the random outcome revenue in this formula. And after rearranging writing it in this form, and by taking the expectations, we can rewrite the expected revenue in this formula in terms of first order, the actual first order value distribution, and the second order value distribution for every possible reserve arm. It's interesting, it's easy to say that you say suppose statistical dominance in terms of value distributions implies revenue monotonicity. I mean, suppose we are given two groups of distributions. The first group is first order and the second order cdfs dominates the second group ctfs, since expected revenue from this first group will be higher than the second group revenue. Also, another interesting observation is that the expected revenue, it has two terms.
00:21:23.892 - 00:22:48.280, Speaker A: The first term trust relies on the first order CTF under the second term trust relies on the second order CTF, but not but under. I mean, this is exactly the linearity of expectation. And this linearity is exactly the reason why our algorithm can work even for correlated value distributions, because the correlation, even if the value distributions are complicatedly correlated and the first and second order cdfs can, can be correlated in a even more complicated way. But when we talk about expected arrhythmias, there is nothing to do about with such a correlation. The second part of our analysis is relies on intermediate object. I mean the step doubly treated CTF is constructed in the following way. I must emphasize that it relies trust is constructed from the actual value of distribution rather than the empirical distribution in the following way.
00:22:48.280 - 00:24:46.794, Speaker A: And another point is that unlike the dominated empirical cdfs, which is shaded in this way, we treat doubly shrink the CTF. We shrink it intuitively twice as much. Why do we need this construction? Because it greatly simplifies our analysis. You say the actual empirical CDF is unbiased estimation of the actual value distribution and with high probability. With the high probability, the dominated empirical distribution by shading one unit of distance is dominated by the actual actual distributions. But since we shred the double shaded cdfs are constructed from the actual distributions by shooting twice as much, whereas high probability is empirical distribution is dominated by the actual distributions, but itself dominates the double shaded cells. And using this statistic dominance together with the monsoon revenue monotonicity, we can obtain the following tree of equations and the first inequality simply says that simply applies this revenue monotonicity.
00:24:46.794 - 00:26:12.056, Speaker A: The second inequality simply uses the optimality of our. You see, our reserve is the optimal reserve with respect to the octopus dominated empirical distribution. So the second inequality simply uses this effect, and the third inequality simply again use revenue dominance. I mean the fact that the shredded empirical distribution dominates the double digit cdfs. And what a way. The only remaining step is to show that even the revenue from double H 3D cdfs is close enough to the actual revenue, to be precise. First you say by introducing this dominated CDF, it incurs no loss of generality because you say we know that in the worst case the empirical distribution can be trust double shaded cdfs.
00:26:12.056 - 00:27:43.320, Speaker A: And you say we only amount of our shading scheme is just twice as much as before. And intuitively you can imagine such increase increases the sample complexity twice and gives symptoms the same sample complexity. But this doubly shaded CDF is much more convenient than the empirical distribution to analyze because it's fixed CDF. But the empirical distributions are random objects rely on that rely on the samples. And so our remaining task is to show that even this, even the optimal revenue from this double a traded CDF is close enough to the optimal revenue from the underlying actual value distribution. And the fourth step, we will use a bunch of tools for extreme value CRMs and tailor bounds for regular and MHR distributions. So previously such tools are developed in the topic called simple versus optimal mechanisms on set mechanisms.
00:27:43.320 - 00:28:44.152, Speaker A: Pass the proof. Approximation ratios of single mechanism with respect to, in terms of revenue with respect to optimal revenues. And in our work we extend such techniques from simple versus optimal mechanisms. I mean, extend such techniques from showing proving approximation ratios to proving sample complexity bounds. Or for learning algorithms, consider MHR distributions, for example. By definition, we know that a single dimensional continuous distribution has satisfies the monotonous rate and all MHR satisfies its property. If and only this is the function to defend based on the value CDF itself is a concave function.
00:28:44.152 - 00:31:33.064, Speaker A: On the support of this distribution and using this concavity, if you are familiar with this topic, you'll notice that, roughly speaking, Mheard properties exactly almost means such a distribution f two goes to increase at least as fast as exponential distribution. I mean, in the case of exponential distribution, this function zhi will exactly linear function. I mean, that is the boundary case concave function. And it's a foreclose that, it's a foreclose that by scaling this MHR distribution, by scaling MHR distribution f three, then such distribution, I mean, as you can see here, data increase is the increase at least at, at least as fast as exponential distribution. And our, our extreme value CRM for such MHR distribution in the sense that not only every individual distribution goes to increase as fast as exponential distribution, but even the first order distribution, I mean, the distribution of the highest value of four unmanned MHR random variables, such first order and second order distributions also goes to increase at the least as far as exponential distribution, the basic proof idea behind such extreme value distribution is better geometrical interpretation of MHR distribution. I mean, with geometrical interpretation of concurrent of such distribution g suite and using such interpretations, we will get our extreme value distribute CRM and for the case of other settings. For the case of other settings, we also develop such techniques, extreme value theorems and other tools, and tailbones for tailbounds for regular and MHR distribution.
00:31:33.064 - 00:31:36.044, Speaker A: That's all. Thank you.
