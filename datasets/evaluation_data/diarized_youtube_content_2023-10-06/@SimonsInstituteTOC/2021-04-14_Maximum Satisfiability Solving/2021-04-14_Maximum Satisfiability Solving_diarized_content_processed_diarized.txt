00:00:00.240 - 00:00:41.236, Speaker A: I think. Let's get started. So today we're in the beyond satisfiability seminar series, and perhaps one of the smallest steps beyond sat is to take a satisfiability, a sat formula plus an objective, for instance, you know, satisfying as much of the formula as possible. This is known as maximum satisfiability solving, or max sat solving. So, a rich paradigm that captures many problems that we know and love. And we are fortunate today to have Jeremia Berg and Matti Yarvisalo from the University of Helsinki who are behind some of the. So I should say Max sat solving.
00:00:41.236 - 00:01:12.724, Speaker A: I think it's fair to say that this is a dynamically developing area. Lots and lots of exciting news. Cool algorithmic techniques in the last few years, decade or so, maybe pretty amazing performance of the best solvers. I'm sure Jeremias and Matti will tell us about it, and they're very well placed to do so, since they are behind some of these cutting edge techniques and the cutting edge solvers. So with that, without further ado, Matti and Jermias, the floor is all yours.
00:01:13.704 - 00:02:21.936, Speaker B: Thanks very much and thanks for the invitation. You can hear me, right? So, yeah, we're splitting this sort of switching between me and Jeremias, and I think there's a place for a break there after Yeremius's first turn, but I'll start with an intro now. So the story behind this is that this is sort of a revised and truncated version of an earlier tutorial on Maxat that we gave at Ekai last year, and that then build on on a tutorial that we gave with Fahim last year with Ruben. Ruben unfortunately couldn't make it today because of his personal schedules, got really, really bad. And. Yeah, and that tutorial was based on an earlier tutorial that we gave with Fahim at AAA some years ago, but hopefully this is slightly updated and even more sort of a by example. And here we're really focusing on solving rather than encoding problems with Maxat due to the time constraints.
00:02:21.936 - 00:03:36.674, Speaker B: But let me continue. Hopefully this will bring something new to some of the participants, but this is rather high level and non technical still. So what we're talking about today is Maxat, or maximum satisfiability, and as Jakob already spelled it out. So this is an exact boolean optimization paradigm that essentially builds on Boolean satisfiability solving and its remarkable success, and really takes SaT solvers and applies them iteratively to solve optimization problems. Due to the great progress made in recent years in the solvers, there's really an expanding range of real world applications where Maxat is the declarative optimization paradigm of choice. Even so, it really offers an alternative, for example, and more to more classical approaches, such as, of course, one has to mention mixed integer programming. And in Maxad, of course, we can provide probably optimal solutions.
00:03:36.674 - 00:04:43.828, Speaker B: We can even, in principle, provide proofs. Yeah, and it's really, since we're modeling problems with logic. So, just like in SAT, this is very much a sort of paradigm, especially in cases where the problems underlying problems that we want to solve yield sort of natural encodings in logic. But this branches on to, just like the success of SAP, to more and more heterogeneous domains, problem domains. So I'll give a brief motivation and basic concepts, and then we'll turn to the core part of the talk, namely Mucsat solving. And we're really focusing on particular types of algorithms that have turned out to be some of the most successful ones when we cover both complete and so called complete and incomplete algorithms. Yeah, so, of course, when we talk about SAt, that's your problem.
00:04:43.828 - 00:05:38.984, Speaker B: So we were given a solutionologic formula, and we asked whether it's satisfiable. And it's in hope. We all know by now, attending the Summers Institute series and all the other developments going on, that SAT is indeed a great success story, not just theoretical tool. And this is due to Sat solvers. And why I'm mentioning this again, is that when it comes to Mucsat and building mugsat solvers, the most important thing is that we have sat solvers that can prove the non existence of solutions, and in particular, provide explanations for the non existence of solutions. So, yeah, and there's, of course, been this great, remarkable improvement in SAT solvers. But I'll show you a similar plot for Muxat.
00:05:38.984 - 00:06:47.956, Speaker B: But the point is, yeah, we're using these core MP search procedures, SATs solvers for optimization. And why do we want. Why is optimization interesting? It's because one could even claim that most often in the real world, there's some sort of an objective that we need to minimize or maximize in terms of. So we want to find the best solutions, whatever best means, and it depends on your domain, we might want to find the least resource consuming schedule. Sure, there's something most resource consuming can mean many things, you know, human resources, money, whatnot. Or we might want to find smallest explanations in debugging configuration, or even explaining, say, machine learning models. So there is indeed high demand for automated approaches to finding good solutions, or ideally, optimal solutions to computationally hard optimization.
00:06:47.956 - 00:07:43.744, Speaker B: Problems. And this is where Maksat today pops in. We're not going to talk much at all about applications. There's a lot of reason, as I already said, drastically increasing number of applications of Mocsat in various types of domains. Here are some high level titles for the types of problems that Mocksat has been successful in recently. I would actually advise people to pick up the now very much up and coming second volume or edition of the handbook on SAT where there's an updated chapter on Magsat where some of these domains are discussed a bit more. And there's also plenty of references to check out.
00:07:44.604 - 00:08:18.640, Speaker A: I have a question regarding this impressive list. It's a slightly vague question, but let's see if I can make it to make sense. When I go to this AI conference. I get the feeling that some applications for, say, sat solving or answers at programming or Maxat or whatever are more like applications in academia that researchers have figured out that in principle you can do this. And then they code it up and they evaluate it. And some applications are more like maybe actually in industry being deployed to solve real problems. Definitely.
00:08:18.640 - 00:08:37.888, Speaker A: Like my understanding is that for SAT soldiers that intel and other companies are actually using these tools to verify their processor designs. For instance, do you have anything to say about what kind of mat set applications are actually deployed and which one are more academic stage, if the question makes sense?
00:08:38.056 - 00:09:27.934, Speaker B: Yeah, it does. It's a tough question, doc, because I work in the academia, but as you mentioned, intel, for example, let me give a short answer and a very vague answer to your vague question. So one example comes from intel, where people at intel are actually developing some of the state of the art tools for Maxat, and surely there is a reason why they are doing that. So I think, just like in SAT, there's problems that really are sort of nicely formulated with logic, and I believe this sort of extends to optimization as well.
00:09:28.554 - 00:09:29.814, Speaker A: Fair enough. Thanks.
00:09:32.154 - 00:10:25.294, Speaker B: Yeah, and all these improvements are really sort of why all these applications are popping up are due to the fact that Muxat solvers are getting more and more more efficient. And that's why we're talking about the algorithms today. And maybe I'll continue with my answer to Jacopo. I should say that when there's a chance of beating the state of the art commercial mixed integer programming tools, then I believe it's in itself clear that these solvers can be used in beyond academia. Yeah. So just to make it more illustrative. So probably you've seen these plots for satsolver performance.
00:10:25.294 - 00:11:17.052, Speaker B: So the higher the better nowadays. You used to be the more, right, the better, but now it's the higher the better. So essentially, what we've seen within the last, say, ten years is a similar progress than we've seen start a bit earlier for solvers. Now it's happening for Max solvers. So essentially, if you take, there's take standard benchmarks and run them on the best solver implementations from different years. In a Maxat evaluation, you will see on the same hardware, you will see that there is indeed evident progress from one year to the next. Okay, so, basic concept, so just to make sure that everyone's on the same page.
00:11:17.052 - 00:12:15.148, Speaker B: So we're talking about MoxAt. So again, in its purest form, our input is just a set of clauses, so, conjunctive normal formula, and we're asked to find a satisfying truth assignment that satisfies as many of the clauses in the formula as possible. Yes, and this is of course in its purest form, oxide. And what's typically how it's presented in complexity books and things like that. But of course, this is not that convenient for modeling practical problems. And that's why we nowadays, when I say maxed, we mean essentially weighted partial maxat. So we can, we can associate weights with clauses, so we can have weighted soft clauses or soft constraints.
00:12:15.148 - 00:12:52.464, Speaker B: And then of course we can associate infinite weights to some clauses, meaning that they are hard. They have to be satisfied by any solution. If you're allowed to deem some clauses as hard, you talk about partial omexide. And if you're allowed to use non unit weights on soft clauses, then talk about weighted mugs. So we're talking about weighted partial max. So here's a neat example that we're going to use to illustrate all of the algorithms that we're going to talk about today. So pay attention.
00:12:52.464 - 00:13:41.134, Speaker B: This is not to say that this is state over. The maxat is a state of the art approach to finding shortest paths in grids, but this is just for illustration purposes, because it's nicer to have something concrete to illustrate algorithms on, rather than just clauses. Hopefully this will work out well to you as well. So, okay, so the point is that we're in a grid. So there's the start cell and a goal cell. So we want to find a path using horizontal and vertical moves going from s to g, and we want to find a shortest path from s to g. And there's some blocked squares that we can't enter go through.
00:13:41.134 - 00:14:36.394, Speaker B: So here, for example, a shortest path would go from s to a c d e, l, r, that's one, and g, that's one shortest path, as I said. So this is really to for illustration, but you'll also now see the only encoding that you'll see in the talk. So how do we encode this in max? So we need to take some boolean wearables. So here's sort of a unary encoding. So we take for each unblocked grid square, a boolean variable and associate the grid's name with the variable. So it's g, ab and so forth. And its intended meaning is each of these variables is that it's true if and only if the passing the square in question is visited by our path that we're constructing.
00:14:36.394 - 00:15:50.806, Speaker B: Now the constraints. So to start off we can see immediately two constraints. So we need to visit s and we need to visit g because we want to find a path from s to g. So we're going to impose unit hard clauses, s and g forbid all solutions where s and g is not in. And now since we want to find the shortest path, we can penalize including each particular cell to the path by these soft unit constraints. So we have soft clauses of weights, one, namely stating that unless you do not, unless you do not visit a, so then you will get a penalty of one, meaning that if you enter a in the path a is on, the path you construct, then you're going to get a penalty of one. And now we need to still enforce the existence of a path between s and g with additional hard clauses.
00:15:50.806 - 00:16:33.984, Speaker B: Right? So one way to think about it is that we're going to leave from s and we're going to end up in g. So we need to have exactly one visited neighbor for both s and g. And then all of the other squares that are visited by our path need to have exactly two visited neighbors, one predecessor and one successor. Okay, so exactly one visited neighbor. This is quite easy. So in logic, so propositional logic. So the neighbors of s are a and b.
00:16:33.984 - 00:17:16.844, Speaker B: So we need to state an exactly one constraint over a and b. So a plus b is one, and it's just the exclusive r over a and b. So it's which is the closest, a or b and not a or not b in conjunctive normal form. Similarly for g, well, it has three neighbors and we should visit exactly one of them. So it gets a bit more hairy because we have more than two neighbors to talk about. So at least one is easy. It's just the clause saying we see k, q or r and at most one.
00:17:16.844 - 00:18:43.644, Speaker B: In CNF, you can, for example, state with pairwise disallowing all pairs over the three grid positions. So not k or not q and so forth. And now to talk about the intermediate squares or grid cells on our path, we need to say that each of them, if they are on the path, then they have to have exactly two visited neighbors. So now it's a conditional or ray file constraint, saying that if, if we take the penalty of including e, for example, on our path, then there's exactly two of d, j, l and f, the horizontal and vertical neighbors of E that are visited by the fast. Okay, now this of course requires encoding the cardinality constraint over here in CNF. But the good thing is that without going any more into detail, there's, this is of course cardinality constraints pop up all over the place in the real world. So the SAT community has developed quite efficient and compact encodings for such things.
00:18:43.644 - 00:19:45.264, Speaker B: So we know that this can be done and these are just hard causes. So it's just a Sat encoding. So the, it's just a Sat encoding with an addition of the soft clauses, unit soft clauses that penalize visiting the individual grid cells or express. And that's why I'm excelling. So, yeah, so now this gives us any solution will give us a, some sort of a path, and then a Moxat solution, or an optimal MoXat solution would be then something that, that violates as few as possible of the units of constraints, for example, the green path. So an optimal solution has cost eight in this particular grid. Okay, so remember this, we're going to use this, you will see this grid later on.
00:19:45.264 - 00:20:30.600, Speaker B: But yeah, just a word on the complexity. So, of course, deciding whether, whether k clauses of a given CNF formula can be satisfied, that's an NP complete problem. It's quite easy to see, because SAT is NP complete. So what we can essentially algorithmically do is iterate over the k and for example, with linear search, and to find an optimal solution to pure max SAT instances. And it's actually sort of FP NP complete. So in general. So we essentially just need a polynomial number of SAT.
00:20:30.600 - 00:20:36.960, Speaker B: Oracle calls in practice, and this is really what's done.
00:20:37.152 - 00:20:54.734, Speaker A: We have our first question, Mati or second, I guess I already asked one, the first one in the chat. So, are the utmost constraints necessary in the encoding for the shortest paths? They're necessary for the SAT solar to produce a valid path, but the Maxat solver will remove dead ends anyway, right?
00:21:06.114 - 00:21:08.534, Speaker B: That is a very good question.
00:21:10.914 - 00:21:32.494, Speaker C: I would say that you're right in that most constraints are not needed, but in practice, I assume that asset solver does better if you include them. They lead to more propagation and give you more information. So the individual calls will be faster. But formally speaking, you would not need them to get a correct encoding, but you'll get a better one.
00:21:33.834 - 00:21:49.854, Speaker B: Yeah, you still can produce cycles, cyclics, top torso. So it should be fine. Yeah, that's, that's, that's right. Yeah. Hmm. Stefan doesn't agree.
00:21:50.194 - 00:21:52.626, Speaker C: That might be true, actually. Good question.
00:21:52.770 - 00:22:13.314, Speaker B: Unrooted circles. Yeah, that's, that's the scale of it. So, yeah, I thought this through once. I had this question before. So, yeah, the issue could be that you could have sort of sub tours that are disconnected.
00:22:16.574 - 00:22:46.544, Speaker A: So just to point out, because I think when this is being sent on YouTube, people can't see the chat that we see. So Stefan Gocht is writing, I think you could have unrooted circles without the utmost constraints, and this could lead to an unknown connected path. I mean, it's true that the fact that this encoding is correct seems to use this parity argument that only the source and the sink have odd parity. Right.
00:22:47.244 - 00:22:48.100, Speaker B: Yeah.
00:22:48.292 - 00:23:15.768, Speaker A: So if I'm allowed to disconnect, then I'll make a small cycle somewhere around the source and another small circle with a fork somewhere around the sink. And now I have a really short path. Path. So it might be that it's actually needed. And sambas is having the same concern that if you. I think Sam is saying that if we remove this, this would not force the end nodes to be connected. I think we're using like the parity argument to connect the.
00:23:15.816 - 00:23:44.544, Speaker B: Yeah, yeah, yeah, yeah, I think so. Yeah, I had to think this through. I had this, I should remember the answer to this question. So I had this question before. Okay, so, but let me continue. So we'll use this for illustration of the algorithms. And so you'll see sort of the algorithm work on the grid rather than CNF.
00:23:44.544 - 00:24:42.784, Speaker B: Okay, so, yeah, just like in SaT solvers, there's, these are maxed solvers are push button solvers, so you don't really need command line parameters, especially for the complete solvers. And there's, there's just like, for such solvers, there's the Dymax CNF format, there's the Dymax WCNF format, which essentially differs only in that where this is a clause over the variable indices one, two and three, and then x one is negated. There's, there's, before the clause, the first thing on each line, there's a weight and there's a so called top weight that allows you to state that particular clause is hard. So sort of representing infinite weight essentially.
00:24:47.324 - 00:24:54.680, Speaker A: Oh, so in the format there are no hard clauses, but you have to make them hard by making ridiculously large weights.
00:24:54.772 - 00:25:41.920, Speaker B: So this is, this is perhaps should the standard, if you ask me personally, the standard should be changed. And one could use the letter h, for example, to start a hard clause because computing the top is also a bit not so nice. And then what you see is people develop, producing maxat instances, the interpretation of which can depend on the solver because of how they're parsed in, and whether, for example, the top is assumed to actually refer to something that's exactly stating what's a hard clause and whatnot. So there's actually some problems with this, and this would be nice to change.
00:25:42.072 - 00:25:52.526, Speaker A: Oh, you mean if I give a top value that is actually too low, then it might be solver dependent, whether this is treated as a hard clause or a soft clause with this particular weight.
00:25:52.680 - 00:26:22.344, Speaker B: Yeah, that sounds a bit scary. And also, so if you sum up the weights for the soft clauses, should the top be higher than that? And then you get representational issues as well. If you don't know what's the optimal solution and the cost of an optimal solution. Right. What the, what should the top be? And if you sum up all the, the weights, that could be a ridiculously large number.
00:26:23.164 - 00:26:34.068, Speaker A: I guess sibily is noting this in the chat also. This seems to be platform dependent. Yeah. Okay, but this is the format we have for today.
00:26:34.156 - 00:26:34.404, Speaker C: Yes.
00:26:34.444 - 00:26:39.224, Speaker A: We're not going to change it before the end of this seminar, unfortunately, no.
00:26:40.884 - 00:27:48.464, Speaker B: Yeah, and so, yeah, and as I showed you the plot dimension, max, at evaluations, there is this annual event called Moxat evaluation that, just like in such competitions, aims to assess the state of the art in Moxat solvers and then produce these benchmarks that can be used in research work and developing further solvers. And this is not as huge as the SAT competition series in terms of the number of participants, but there are various solvers each year being evaluated. It's not a competition per se, but aims to be more of an evaluation with less strict criteria for participating. And nowadays there's tracks for both complete and incomplete solvers that we're going to talk about today. Yeah, and I should mention that. Oh, I do mention it. Okay, so, yeah, so maxed solver is just a practical.
00:27:48.464 - 00:28:49.202, Speaker B: So it's an implementation of an algorithm for finding solutions to given maxat instances, complete solvers are guaranteed to find optimal solutions. So probably optimal solutions to any instance. Given if resources and incomplete solvers are essentially tailored, whereas complete solvers are tend to be allowed to use a lot of resources, incomplete solvers are tailored towards producing relatively good solutions, whatever the interpretation of that means is to even very large instances fairly quickly. So for example, in the evaluations, the complete solvers are given, for instance, time limit around 1 hour, whereas incomplete are run for a minute or, or five minutes per instance. But there needs to, for incomplete, there needs to be no guarantees on how good the solutions are. Really. Yeah.
00:28:49.202 - 00:30:05.374, Speaker B: When it comes to max devaluation, I want to mention that nowadays we have an open resource, open source requirement starting from a few years back and before that this always mostly available in binary. But nowadays we're trying to really make people produce open source muxat solvers, because this is how this part of the reason why SAT is such a success, so that anyone can build on previous solvers and hopefully this will be picked up on more and more. Okay, so let's now get to the algorithms. So, and we'll start with the complete algorithms and in the end we'll get to the incomplete ones, which actually mix sort of ideas from complete solvers. So that's why actually you notice the hyphens in the incomplete solver description a couple of slides ago. So that tried to refer to the fact that some of the incomplete solvers, if you run them for enough, long, long enough, they will actually provide optimal solutions. But it really depends on the algorithmic foundation of solvers.
00:30:05.374 - 00:31:51.634, Speaker B: So when it comes to complete solvers, you can categorize the main sort of mainstream algorithmic approaches developed as follows. So there's a branch, and sort of a specialized branch in bound approach that can be very effective on small but hard and randomly generated instances in particular. But we're not going to focus on this today, but we're going to talk about, perhaps one can say more recent developments that can scale to very, very large maxod instances. Namely these are all using to one extent or another, but to a far extent in any case, SAT solvers. And when it comes to the SAT based Maxat algorithms, there's essentially three types of approaches, so called model improving approaches, core guided approaches, and the implicit hitting set approach. And due to the fact that, well, due to the time constraints and also due to the fact that, well, the core guided approach is implemented by the greatest number of solvers nowadays available and korguided together, the implicit hitting set approach turns out to be quite effective nowadays. So we'll outline these approaches, and then after that we'll turn to incomplete and explain how to combine some of the ideas from these approaches and different ideas as well towards incomplete soil.
00:31:51.634 - 00:33:22.414, Speaker B: Just very shortly, I mentioned model improving. So what does this mean? So it essentially refers to searching for, for the optimal bound on the cost from top down in terms of the cost function range. So we just instantiate the upper bound to be all the number of soft clauses when we talk about unweighted instances. And then we can ask essentially whether we can set it, whether the SAT solver can find a better solution. Always. So, can you answer, can we use leave unset so falsified less than the current bound number of clauses? So can you satisfy more clauses than the currently best solution found? And as long as you find a satisfying truth assignment with a SAT solver to this query, then you update your working formula with a tighter bound. So this is essentially a cardinality constraint that we're adding each time to the sub solver and asking again, and if we do this by linear search from top down, then of course immediately when the set solver says no, there's no solution, we know that the previous solution wasn't optimal.
00:33:22.414 - 00:33:27.174, Speaker B: So this approach can be quite efficient.
00:33:28.474 - 00:33:37.814, Speaker A: Question, does it make sense at all to consider binary search in this setting rather than linear search?
00:33:38.874 - 00:34:10.784, Speaker B: Yeah, this can be done and it has been also proposed, but I believe there's no really, in the current sort of state of the art, there's no solver that does that binary search. I'm right on this. I don't, I'm not aware of one. So there used to be earlier on. Yeah, there used to be earlier on one that combined this model improvement with so called core guided, but it's, it's sort of gone away.
00:34:14.564 - 00:34:15.476, Speaker A: Okay, fair enough.
00:34:15.540 - 00:34:56.403, Speaker B: Yeah. So model improving can be very efficient when the actual optimal cost is quite high. So you can't really satisfy many soft clauses because then you're going to terminate quite quickly. And there are solvers that use variants of this algorithm, so one shouldn't totally disregard this at all. So there are cases where this can be very effective and going beyond complete, as Jeremiah will spell out in more detail later, this approach is indeed applied together with other things in the realm of incomplete solving.
00:34:56.563 - 00:35:16.934, Speaker A: So just adding that, there's a comment from Armin bire in the chat that Fahim explained last time that binary search risks to overshoot the best solution quite a bit, and this produces large formulas. Oh, because you have to re encode every time, I guess, also.
00:35:17.914 - 00:35:20.530, Speaker B: Yeah, well, it's one of these clashes.
00:35:20.562 - 00:35:27.818, Speaker A: Of theory and practice. Right. As a theoretician, of course you should do binary search, and except in practice, you don't want to do it.
00:35:27.986 - 00:35:40.166, Speaker B: Yeah. Yeah. I think because it's really dependent on the instance and its optimal cost.
00:35:40.310 - 00:35:40.718, Speaker C: Yeah.
00:35:40.766 - 00:36:26.924, Speaker B: There's, it feels like there's always instances on which it will not work well because of this, what Armin wrote. So it will overshoot when there's an opportunity to overshoot. Yes. Okay. But we'll turn to now core guided, and as a prequel to core guided, Jeremy will talk about core guided more in a moment. But let me just spell out that just like you can do model improving from top down, you can go bottom up and do, you could do lower bound search, of course. And you would just go ahead and ask if you can satisfy all soft clauses.
00:36:26.924 - 00:37:26.064, Speaker B: And this is, of course, what, what you would do for, as the first step in essentially any muxat algorithm that uses a sat solver, because this is a very cheap first check. Okay. So. And then you can just, you know, go on to ask, okay, what, okay, is there a worse solution if this one doesn't have a solution? And this is quite bad now, because now we don't have even a solution. And when you go on and do this, you have to use these cardinality constraints, just like in model improvement, but you don't really get information from these unsat cases if you don't do something else, if you don't rely on what the solver can really provide us.
00:37:26.564 - 00:37:49.094, Speaker A: Sorry for disturbing, Mattia. I'm just adding one more comment from the chat here so that everybody can hear it. That in constraint programming, it's usually that only a few values on either side of the objective are really hard. But sometimes you get nasty instances where even lot over the objective is really hard. These are particularly nasty for binary search rights. Kieran Mchreesh, sorry for interrupting. Please go on.
00:37:49.994 - 00:38:17.874, Speaker B: Yeah, yeah, approaching the, but approaching an optimal bound can be very hard. Yeah. If you approach it sort of carefully. I think it's also for the model improving, it's a challenge. Okay. Yeah, but the point is that you don't really get information if you don't use some additional information that the software provides us. And this is why this approach in it like this is not really implemented at all.
00:38:17.874 - 00:39:18.084, Speaker B: But now the point is that we can use more information from the solver because we can get unsatisfiable course that give us hints towards what subsets of the soft clauses are conflicting with each other. And producing conflicts. And now we can actually stay, essentially start relaxing these soft clauses using this core information. And in these cores, what they yield are much tighter cardinality constraints towards an optimal solution. And now this is essentially what the core guided solvers do. And this is how you can scale to millions of variables. So, just to state by an example, so if you have a CNF formula that's unsatisfiable, anything that that's unsatisfiable, that subset of the clauses that's unsatisfiable is called an unsatisfiable core.
00:39:18.084 - 00:40:04.264, Speaker B: And what we would hope that this is that the solver provides us a small unsatisfiable cores, because then we can do these, impose these tighter cardinality constraints when, when at the next iteration of the solving. And of course in Maxa, the point is that the course talk about soft clauses. So the hard constraints are sort of a background theory. So you do always these checks, all hard clauses on. And then you have soft clauses, a subset of soft clauses that, together with the hard clauses are unsatisfiable. And. And the course talk about this subset of software.
00:40:04.604 - 00:40:11.944, Speaker A: There's a question. Are these unsat cores computed only for the soft clauses to rule them out? Asks Sibyl.
00:40:13.124 - 00:40:50.484, Speaker B: I hope I just said it. Yes, in not too many words. Yes, exactly. So the hard clauses are always there, but they're not in the don't occur in the course themselves. So now we're getting to this illustration that we're going to use for the algorithm. So, for example, remember our shortest path problem. And now what we have is the hard constraint stating that s has to be visited, and because we visit s, we have to visit exactly one of a and b.
00:40:50.484 - 00:41:42.312, Speaker B: Now we have the soft clauses stating that don't visit a, don't visit b. Well, I would prefer you not to visit a. I would prefer you not to visit b. So now if you add these, you have these clauses at these clauses, and the hard clone states, the sat solver, the solver will say unsat, and one of the cores it can return is exactly these. The. The clause that is the negation of consists of the negations of these unit clauses. And that's the core, because what the core is really stating is that if any path that goes from s to g has to go through a or b, so you cannot satisfy both of these soft clauses.
00:41:42.312 - 00:41:58.704, Speaker B: So that's an unsub coding one website. Okay. All right, so now let's allow rms to pick up on this and give you details on some of the core guided algorithms.
00:42:00.124 - 00:42:30.920, Speaker C: Yes, thank you. You need to stop sharing so I can share instead. So, hello, everyone. Nice to see so many people here. And I'll continue from the exact next slide. I hope it's visible now. So, as Matt already mentioned, core guided algorithms are the one type that is mostly implemented by the solvers that are available today, starting from 2006 with the so called foo Malik algorithm.
00:42:30.920 - 00:43:25.532, Speaker C: There's been new solvers almost every year, not quite, but as recently as last year. And there are various differences between these. They use various different heuristics and clever tricks which are a bit too subtle for me to have time to go into here. So what I want to do instead is to focus on sort of two high level views on how to use the course. The first one I'll call you want an approach where you take all of the cores that you find and then you relax them together. And the second one is where you try to make as much use of the information provided by individual cores as possible by relaxing the course separately. These boldenings of solver names should be taken with a grain of salt.
00:43:25.532 - 00:44:43.304, Speaker C: As I already said, many solvers use also sort of combinations of these techniques, but on a really high level, and a bit roughly, you could separate the core guided algorithms into these two groups. Okay, so first, starting with the approach where you try to relax the course together, which is implemented by others, among others, by the so called MSU three algorithm. Here we start by initializing a set that I'll call r to be the soft clauses that need to be relaxed, or the squares that we can visit in this example. And a lower bound. Initially, we have the lower bound set to zero. Then we ask the SAT solver, can we find a solution that satisfies all of our hard clauses and unsatisfies at most lower bound from the set r, so instantiated here we're asking for a path that visits at most our lower bound amount of nodes from the z r. So initially we're asking, is there a path that goes through no nodes in the z r? And this is, of course, unsatisfiable.
00:44:43.304 - 00:45:42.934, Speaker C: And the SAT solver will give us an explanation for why it is unsatisfiable. One of these cores, so say it gives you this one containing a and b, like Matti said, that they form a core. And now we can then update our set r to include the nodes a and b, and also increase the lower bound. So in the next iteration, we are asking, is there a path that visits at most one node from the set r? That now contains a and b and no others. So, can we go from s to g via only one node from a or b? And this is still unsatisfiable. And we'll get a new core, say, for example, the one containing the node c and g, or the soft clauses corresponding to these nodes. And then we again bump up our lower bone and keep going.
00:45:42.934 - 00:46:21.894, Speaker C: In the fifth iteration, we might have found chords corresponding, containing all of the nodes here in gray. And now we're asking for a. Where is the 6th iteration? I think actually we're asking for a path that goes through these gray nodes, but at most five of them, and we still can't find one. And we get our. But then we get our final core and ask the sat solver for a path that goes through, at most, six of the nodes in this set, and now it finds one, which we then, which we then return as an optimal solution.
00:46:22.354 - 00:46:33.214, Speaker A: And Jeremias, are you going to tell me where these numbers came from? Which numbers this two increased to five to six.
00:46:34.314 - 00:47:33.874, Speaker C: It's just a basic sort of lower bound linear search. We're trying to find a cost a solution goes zero. If we can't, we then have to say, well, how about a solution of cost one? And then if we can't, we bump it up to two, and then we keep going until we find the first bound where we can find the solution. So it's just, we start from the trivial lower bound of zero, and then we keep increasing it until we get satisfiable. So the difference to this naive approach is that we're not considering all of the soft clauses, we're not allowing the SAT solver to falsify all soft clauses, but only soft clauses that have appeared in some of the cores that we find along the way. Okay, so in this particular example, we end up with not having to consider this soft clause corresponding to the o here at all. So we'll just keep on saying that you're not supposed to visit it.
00:47:33.874 - 00:48:27.544, Speaker C: We're only allowing the falsification of the soft losses corresponding to these gray nodes, not all of them, which is the difference between this and the naive lower bounding search that what we talked about, and I noticed in my presentation that I'm actually saying in my examples, we're not going to incur cost for the starting or goal loan. So hence the cost of an optimal solution for me is going to be six. So we visit six nodes along the way. That's minor difference. Sorry about that. So this algorithm can be quite efficient, and it is used in practice in solvers that exists today. And especially if the course that you find are fairly small, then you don't have to relax.
00:48:27.544 - 00:48:53.064, Speaker C: Then the set of soft clauses you end up relaxing is small. And if you can find a solution of a low cost, so remember that here we're starting from zero, and then we're just pumping it up in unweighted case by one each time. So if your optimal solution has a low cost, then you don't have to do that many iterations, and hence you might be able to terminate quicker.
00:48:54.144 - 00:49:08.696, Speaker A: So I have another technical question. So this fact that you're allowed to falsify and so, and so many of the soft clauses, this will now be encoded in CNF in some nifty way that you have for encoding your cardinality constraints.
00:49:08.840 - 00:49:22.782, Speaker C: Yeah, this is exactly. I think what's going to be my next sentence would be that the problem, or the challenge of this, is that you still have to do a cardinality constraint that says you're only allowed so and so many clauses.
00:49:22.958 - 00:49:47.842, Speaker A: And then I'm wondering if we, are we perhaps also using here, I think, which I think is a standard trick in Maxat solving. But I'm not sure if it's obvious to everybody that without loss of generality, you can have like a pre processing step where you rewrite your problem so that all the soft clauses are actually unit clauses. And that's why you just minimize the sum of the blocking variables. Right. Maybe you said this and I missed.
00:49:47.858 - 00:50:35.134, Speaker C: It, but we didn't. And this is a very good point if you want to talk about how to actually implement these solvers. But in practice, all of the implementations of all algorithms that I'm aware of do these tricks. So we can assume that any kind of soft poses that you have are going to be of this form with a unit negative literal. If you have an instance where that's not true, then you just simply take a new literal, add that to your original soft clause, treat that as hard, and then add the new literal as a unit negation as soft. This is a sort of technical, important technical detail that you need when you want to implement your mapsat solver. You can contact me and I'll tell you more about that.
00:50:35.134 - 00:50:40.154, Speaker C: And now the other problem.
00:50:42.574 - 00:50:44.382, Speaker A: We might take you up on that promise, you.
00:50:44.398 - 00:51:55.974, Speaker C: Know, just so in addition to the fact that you still need this cardinality constraint, which can be a bit tricky and can be a bit large, another problem with this relaxing all course at one approach is that you sort of, in a certain sense, you lose information that a course could provide, you're not making full use of this course. So if you go back to the same example and look at what's happening in the third iteration, as I said, we're now asking the SAT solver for a path that visits at most two of these gray nodes and goes from s to G. However, based on the course that we extracted, we actually know something stronger. Matti already alluded to this. We extracted a core containing the clauses corresponding to a and B. And the fact that it is a core says that any path between S and G has to go through either a or b. And similarly for the second core, we also know that any path needs to go through either c or G.
00:51:55.974 - 00:52:47.404, Speaker C: So this constraint here is, in a certain sense, to lose. We know that the solutions to this here, which, for example, would set c to true and g to true, are not going to correspond to actual models. But in this algorithm, we're not telling the SAT solver this, we're not letting it make use of this knowledge that we have just by core abstractions. And Carlos is saying in the chat that you can also use different cardinal constraints for non overlapping course. Yeah, and this goes. And that's absolutely, absolutely true. And for this type of algorithm, this picking your cardinality constraint smartly when you have non overlapping cores is one of the key features that makes it effective.
00:52:47.404 - 00:54:10.594, Speaker C: This example here is a bit easy, because all of your cores are non overlapping, but it can. But there are some clever things you can do for more complex instances. But the way in which I want to take this from here is instead to talk about this second alternative that I mentioned, which is to make use of this information by relaxing each core separately. So if we go back to the same example this time around, we're taking a so called working instance, which I denote by h zero the initial hard clauses and s zero the initial soft clauses. And then we first ask the SAT solver for a model that would satisfy all of the hard clauses and all of the soft clauses. So implicitly, the first call is asking, do we have a solution to this Maxat problem that has cost zero? But more importantly in this case, we can also see this call to ask for a. This will get clearer in a second, but I'll say it for the first time now the call is asking for a path that goes visits at most one node from each of the found cores.
00:54:10.594 - 00:54:59.904, Speaker C: So initially we have no cores. So we're asking for a path of length zero of one that visits at most one node from each of our zero cores. And similarly to before, this is of course unsatisfiable. And we get a core. And now this time around, we relax this core immediately into the instance. So in the sense we're encoding the information provided by this core into the instance itself, we're saying, okay, so a and b soft process, a and b form a core. So in subsequent iterations, you're allowed to falsify at most one of them, because we know that any optimal solution has to falsify at most one of them.
00:54:59.904 - 00:55:57.916, Speaker C: So we go back to the zollver, and now we know that we need to falsify at least one course. We found one course, so we know that we have to falsify at least once, of course. And now we're asking the sad solver, is there a path that visits at most one from this one core that we actually found? And still we cannot get to the goal node. So the next core we might get from the solver might be at something like this. This is saying that you need to visit at least one of these nodes, and now we can again do the same thing. We re relax the instance saying, okay, so in, in subsequent iterations, you're allowed to falsify one node from this core and one node from this core. And then we go back and then we keep going until we found enough of these cores to actually allow us to terminate.
00:55:57.916 - 00:57:08.654, Speaker C: After we found, I think it's five, six cores, we now ask, is there a path between the start and the goal nodes that visits at most one node from each found course? So in other words, goes through a node of a specific color at most one time, and this time around we can of course find one. And the sat solver turns satisfiable and we get our model and we're happy. In comparison to the earlier algorithm, this time the cardinality encodings are in a certain sense tighter and we don't need to explicitly maintain the lower bound and the set of cores. As I said, as soon as we get a new core, we'll encode it, encode the information into the formula itself, and you don't need to maintain it. And, and the fact that the lower bound increases by one sort of follows from the number of cores that we find in this unweighted case. For each core that we find, we increase the lower bound by one. But in practical implementations, you don't need to care about what your lower bound is.
00:57:08.654 - 00:58:35.682, Speaker C: You just keep going until the SAT solver gives you satisfiable, and then you can check what the cost of that model was. So, as I said, the first instantiation of this kind of an idea was by Foo and Malik in 2006, and that was sort of the kickoff into the research of corguided algorithms. And nowadays, most of the existing solvers that use this approach implement the so called oll algorithm, which I believe Fahim also talked about, which was this was first proposed in the context of answer set programming and then extended to Maxat in 2014. There are differences between this, and the differences are mainly in how this relaxation step, what kind of process you add to relax the course. And in practice, it's quite clear that the OLED algorithm is the one you want to use at the moment, at least until someone comes up with a better one. And as I already talked about, in comparison to the other algorithm, this time we only need cardinality constraints, saying at most one of the clauses in each core can be satisfied. And these at most one constraints tend to be more effective for Sat solver to reason over, which is a nice thing to have.
00:58:35.682 - 00:59:27.674, Speaker C: But then on the flip side, now you need many different cardinality constraints, one for each core. And there's also been some work in showing that when you try to extract a core from this new formula, that can, doing so can be exponentially harder than from the original formula, especially, or I would say actually only if you have these overlapping cores. So as long as your cores are non overlapping, that's no problem. But as soon as you have overlapping cores, might end up having a harder time finding your course. And I guess that's it for my first part about this complete algorithms. Next it would be we would move on to the implicit hitting set algorithms, which Matthew would talk about. But would this be a good time to have a break? Maybe.
00:59:28.254 - 00:59:40.056, Speaker A: I was just going to ask you. I think it is. And before we officially take a break, are there any questions at this point from anyone? I mean, we've had a few questions.
00:59:40.080 - 00:59:43.004, Speaker C: In the chat, yeah, thank you.
00:59:43.864 - 01:00:06.676, Speaker A: If anything has been those we've taken care of. But anyone else, based on the material so far, any questions from anyone? Otherwise, while you're thinking this, what will happen now is we'll take a ten minute break, give or take seriously.
01:00:06.780 - 01:00:08.116, Speaker C: Can I answer the question?
01:00:08.300 - 01:00:10.304, Speaker A: Yes, read it out and answer it, please.
01:00:11.124 - 01:01:38.670, Speaker C: Victor is asking, how does one actually find a course? And in practice, you use the assumption interface provided by most of the SAT solvers that exist. So you call your set, you add this, you call your set solvers, while assuming your soft clauses in a way that would satisfy them. If you remember, I said that in practice you use this unit lateral set subclauses, and then both of the SAT solvers today are able. If the result is unsat, the SAT solvers can give you back a subset of your assumptions, which is enough to explain unsatisfiability, and that subset will correspond to the core. So we use sattal version of black box manner to do this. The second question from Armin, are people considering to use cardinality constraints natively or at most one constraints instead of encoding? Yes, yes, for sure people are, and I think, don't quote me on this, but I think the Maxino maxolver actually even uses PB constraints to some extent natively. In our work we had some problems getting that to be as effective as encoding, or more generally speaking, having these separate sort of propagators to be more effective as encoding.
01:01:38.670 - 01:01:41.074, Speaker C: But that's an interesting direction for sure.
01:01:44.594 - 01:01:58.694, Speaker A: I just can't resist saying that there is also a pseudo Boolean solver that does this core guided approach by Emir Demirovich, myself, Stefan Gocht and some others.
01:01:59.034 - 01:02:01.014, Speaker C: Yes, I've heard of it.
01:02:02.474 - 01:02:33.264, Speaker B: Yeah, it's interesting. But I mean, so it's interesting that it seems that it doesn't seem to really pay off to try to do native cardinality constraints, at least insat or Maxat inside. Yeah, in Maxat. So maybe it's about the explanations. Like if you put them as propagators, the learning, just that the learning is actually important.
01:02:33.914 - 01:02:52.034, Speaker A: I think it depends on how you use them in the conflict analysis also, I think that make a big difference. I mean, once you're moving to suitable and constraints, you can do all kinds of things. Like you can extract cores that are actually not just clauses but pseudo Boolean constraints.
01:02:52.154 - 01:02:53.334, Speaker B: Have you done that?
01:02:54.314 - 01:03:28.496, Speaker A: We have this in the paper that I put in the chat. We're actually extracting the cores are not clauses, they're general generic pseudo Boolean construction and then we round them to cardinality constraints in order to update the objective function. But in principle you could. I think Emir Demero suggested doing some kind of. You can extract a generic as like a general sudo boolean core and then try to do some kind of dynamic programming to see how to update the objective function in the best way. There are all kinds of tricks you could try to pull, but also the suitable and conflict analysis might give you better course in the first place, even.
01:03:28.520 - 01:03:49.720, Speaker B: If you stick to course, if we stick to Maxad, I'm. Yeah, I'm. To me, for, to me it's quite unclear whether one can really make the what could make this pay off. But yeah, I agree.
01:03:49.752 - 01:04:03.664, Speaker A: It's not clear that's why. But that's exactly why I said that. The promise Jeremiah has made just a few minutes ago is a risky one, right? I think it's a very interesting question. I'm somewhat optimistic, but I don't know. And I'm obviously a little bit of an outsider, but I'm learning as we speak.
01:04:07.084 - 01:04:08.704, Speaker B: CP, yours is.
01:04:11.004 - 01:04:24.704, Speaker A: Constrained. Is that constraint programming solvers use resolution and it works. Lazy clause generation constraint programming solvers. I don't know. Emit. Do you want to unmute and speak? Maybe. Do I need to unmute to you? I can allow you to speak.
01:04:25.964 - 01:04:30.604, Speaker B: Do you mean to say that the propagators are not needed? No.
01:04:30.644 - 01:04:31.820, Speaker D: So, can you hear me now?
01:04:31.972 - 01:04:33.004, Speaker A: Yes, we hear you.
01:04:33.044 - 01:04:34.244, Speaker B: Yeah, no, I was just saying that.
01:04:34.324 - 01:05:02.194, Speaker D: So in lazy cloud generation constraint programming solvers, one would still use the resolution proof system, and there we would use this kind of native support for kernelity constraints. So just that you said that you don't think that in magsat it would be useful, which could be true, maybe, but still, like in, since constrained programming it seems to be useful, then there's some thing, there's some evidence that it could be also useful in Maxim.
01:05:02.934 - 01:05:22.494, Speaker B: Yeah, I think the point is that what you start off with is this extremely low level format. That's why I said if you stick with Maxat, it's much more unclear to me whether it can pay off compared to if you start with something that's a higher level declaration of your problem.
01:05:22.954 - 01:05:29.066, Speaker C: I don't think it's a matter of the proof system or so, but it's just that like effectiveness of sub solver solvers.
01:05:29.130 - 01:06:13.214, Speaker D: Well, it was more like that in CPU. It wouldn't generate additional, like for example in Macsat you would. So I think the difference is you, you would add additional variables to the encoding, and these additional variables are very useful compared to just using a trivial explanation, let's say. So what I was saying that the additional variables you introduce are very valuable for max set, for example. But CP typically doesn't use these additional variables. But it's still, but it still works, which is remarkable. In a sense, it's a bit surprising, but it's true that indeed there's no real work on combining this kind of constraint programming style with Maxet.
01:06:17.234 - 01:06:33.042, Speaker B: Yeah. It can also be that, you know, the specific approaches that we have right now don't really work with these ideas, and you have to think out of the box more not to try to build on what you already have, right?
01:06:33.178 - 01:06:34.334, Speaker D: Yeah, definitely.
01:06:37.754 - 01:16:36.994, Speaker A: So I don't see any further questions. So maybe let's now take, you know, ten ish minutes a break, enough to grab a cup of coffee, and then we'll resume, I guess, let's say. Yeah. So eleven minutes to the hour. So the break is officially on now. So we are resuming in one or two minutes. That is, assuming that our speakers are still available.
01:16:36.994 - 01:17:25.394, Speaker A: I don't know if there are any questions that have arisen during the break. Thomas has raised his hand, I think. Do I, Thomas? Maybe. Should I. Since we're having the break anyway, could I promote you to Panamas? Maybe trying that again? Since you're during the break anyone wants to be, we'll just go ahead and promote some people. Unless you protest, you're not missing anything.
01:17:27.774 - 01:17:28.994, Speaker B: Thanks, Jakob.
01:17:30.374 - 01:18:17.684, Speaker A: Here we go. I'm in upgrade mode today. I don't know if it's the case, also that you'll have to tell me whether it's the case that attendees cannot send chat messages to everybody. I'm not sure I think it. But when you're sending chat messages, please make sure to send them to both panelists and attendees. Except I'm doing my best now to make sure that we don't have any attendees left and we don't have this issue.
01:18:18.264 - 01:18:27.644, Speaker B: I think both by default. I was in my chat. It sends only two panelists. Maybe you just have to switch it.
01:18:28.384 - 01:18:28.720, Speaker C: Yes.
01:18:28.752 - 01:18:32.324, Speaker E: And attendees can send chats also to everybody.
01:18:32.784 - 01:18:55.734, Speaker A: Yeah, I guess I'm just experimenting. We're all learning this in this new fantastic zoom world. I just took the liberty to upgrade everybody to panelists. I hope you don't mind. I guess you can. No, Antonina says that attendees can only send messages to Q and A. No, I don't think that's true, because.
01:18:56.434 - 01:19:00.094, Speaker E: I also sent messages to the chat when I was only attending.
01:19:00.794 - 01:19:02.814, Speaker A: Yeah, they can. Yeah.
01:19:03.254 - 01:19:08.354, Speaker E: And I think I also had. Could also choose whether I wanted to send to everyone or to panelists only.
01:19:09.134 - 01:19:10.454, Speaker A: So we can try again.
01:19:10.494 - 01:19:11.238, Speaker E: You can downgrade.
01:19:11.286 - 01:19:21.142, Speaker B: Sorry, I might be wrong, because I seem to remember you might not be able to send personal messages when you're not in d, but you might be sent to everybody.
01:19:21.238 - 01:19:34.614, Speaker A: I think personal messages are. But this is also connecting back to what we said at the beginning. I think chat rooms might be slightly different in this regard. You know, there might be customs, and this is a rich new world that we're all learning to live in.
01:19:35.834 - 01:19:40.974, Speaker E: But I think Antonina is right. No personal messages for attendees.
01:19:42.914 - 01:20:10.676, Speaker A: So, anyway, I think we're good to go. So, again, I'm monitoring the chat for questions and sometimes reacting quickly and sometimes not so quickly, but the speakers can also. I mean, good that Jeremiah caught a number of chats that I missed, and everybody should now be able to unmute and just ask questions. So I think let's just continue.
01:20:10.860 - 01:21:56.014, Speaker B: All right. Yeah, so I try to be fairly quick on this implicit hitting set part because I feel that there might be overlap with previous talks on this. And, and it's good to give some time for Jeremiah to talk about the incomplete part because there's quite a bit of new developments there. But, yeah, about this implicit hitting set algorithm for Maxat. So one should first of all point out that this idea, general idea underlying this algorithm is essentially founded in Reiter's early work on diagnosis, and it's been been used in various types of settings, and you can use it for all sorts of hard optimization problems and reasoning problems associated with different types of oracles, not just sat solvers and IP solvers as done in Moxat, but, yeah, so Jessica and Fahim proposed this first in their HS solver for Maxat, and it's, it's been quite influential, and it's really one of the main approaches to Maxat solving these days. So, hitting set, I think everyone knows what's a hitting set, but of course, we're given a collection of sets of elements over some universe. And now hitting set is then something, a set of these set of elements from the universe that include at least one element from each of the sets in our collection.
01:21:56.014 - 01:22:24.448, Speaker B: And then here we're interested in optimal hitting sets. Essentially, if we don't weigh, have weights on the elements, then we're just interested in finding a smallest hitting set. And if you would have weights on the elements, then you would be interested in finding a minimum cost hitting set.
01:22:24.616 - 01:22:38.520, Speaker A: So, question regarding weights. Actually, I guess so far we've been unweighted in this talk, but is there anything that actually has depended on the weight, or would everything you did so far extend easily to the weighted gears?
01:22:38.672 - 01:23:25.482, Speaker B: Yeah, so the core guided. So there's sort of a standard trick that you need to know how to implement, and no one said how to implement it in a paper, but anyway, so you can implement these core guided approaches. They do work for weighted partial max, no problem at all. You essentially, when you extract the core and compile the core into your formula with these cardinality constraints, you sort of consider its residual weight of the clauses that were in the core. So you sort of take out the minimum weight over the soft clauses in the core. And if weight remains, you still have those soft clauses, and that's how it sort of works.
01:23:25.658 - 01:23:30.974, Speaker A: Andre asked if there's a penalty in terms of performance when you get non uniform weights.
01:23:33.154 - 01:24:04.854, Speaker B: Yes. Yeah, it's harder and one of the. But it's very domain specific, which solver is more effective. So it really is the case that it's interesting that when it comes to Muxat, these different approaches really have their different benefits. And this shows up, depending on the problem domain, which one to choose. You know, if you compare to software, you know, CDCL is CDCL.
01:24:05.354 - 01:24:12.258, Speaker A: So then on top of it all, you will have a small machine learner that tells you what to do, or we're not there yet.
01:24:12.426 - 01:25:04.674, Speaker B: If you want to do that, go ahead. You can do that? Yes. Okay. So what does this have to do with Muxat? So, if we have an optimal hitting set of the set of unsatisfiable cores, of your Muxat instance. So now our collection of sets is the set of collection of unsatisfiable cores, of the instance of which there's plenty, of course, exponential number of them. So if you compute any optimal hitting set of the unsat cores, then if you rule out the soft clauses in that optimal heading set, and you know that there's a solution to the rest of the clauses in terms of SAt. So the rest of the clauses are unsatisfiable.
01:25:04.674 - 01:25:34.212, Speaker B: And that will be an optimal solution to your muxat instance, because you computed an optimal minimum cost heating set, ruling out all of the inconsistencies. So all of the cores. So you're sort of relaxing all of the cores away by computing a heading set. There's only one issue here. So we, we don't want to complete the whole set of unsatisfiable cores. Right? Because that's a big one. So that's what I said.
01:25:34.212 - 01:25:54.104, Speaker B: So. So, uh. Already. Sorry. So the key insight is then that you just want to find a hitting set of the onsite cores, but you want to find it somehow, implicitly, in hope. Hope. You just try to hit some cores and hope for the best and put it short distance to work quite well.
01:25:54.104 - 01:26:33.564, Speaker B: And then you just need to call a set solver once you've been lucky to get a solution, and then you're done. So you need to do this, of course, iteratively. So you don't want to compute a huge pile of cores, but you want to compute cores fast. So what iteratively is done, then in the implicit heating approach, you accumulate a collection of onset course. So compute cores and then compute a hitting set over the course, and then rule out those of clauses in that hitting set. You compute it from the rest. Next, sat solver core.
01:26:33.564 - 01:27:25.014, Speaker B: And then you get more cores, and then just compute another hidden set until the set solver says returns a satisfying assignment. That will be an optimal solution. And the heading sets are in practice computed just with an integer programming solver. Now I, now I said that Maxat this is, can be integer programming, and now we're using integer programming inside Max. Isn't this interesting? But the interesting bit here is that we're using it for, to solve a completely different problem. It doesn't know anything about the actual problem except for the cost, the weights of the unit clauses. And it just has this, you know, at least one constraint, the IP of the mean cost hitting set, stating that you need to hit this set.
01:27:25.014 - 01:28:34.262, Speaker B: And it doesn't know that it's, this deals with Max, of course, but in some sense now we're using IP at its best, because it doesn't really know much about the problem, but it can deal with these cost objective functions well in a fairly unconstrained setting. And satsava is of course providing us cores. And now the difference, of course, compared to core guided approaches, where you keep on bloating up the formula that the sat solver is called on with this cardinality constraint. And you're not doing anything like that. You're using assumption, the assumptions interface, to sort of rule out some of the soft causes from consideration at each iteration. And so in hope that actually those instances are easier to solve rather than getting bloated and harder to solve. And what's getting bloated gradually is the IP, the mean cost hitting set IP, yeah, I like to show this illustration, but this is again how it works.
01:28:34.262 - 01:29:31.774, Speaker B: So essentially you give the cost function, so the weights of the soft clauses to the mean cost heading set. And then you start with, you don't have a hitting set, you don't have any cores. And then at the first iteration you essentially just check with the SAT solar if you can satisfy all sorts causes. Typically that tends not to be the case in optimization. So you get a core, you add it to your set of cores, which was still empty, and then you ask for a trivial hitting set of one core. You get something, you hit one soft clause, you remove that soft clause from consideration for the next set solver call, and you keep on doing this cycle in principle like this, until the SAT solver says SAT, and you're at an optimal, probably optimal solution. Things are not like, don't work this directly in practice.
01:29:31.774 - 01:30:13.454, Speaker B: You have to pull a lot of tricks to make this really, really efficient. But still, the underlying framework is like this. And as I said, this can be used for anything, because you have a core extractor of whatever your problem is, and you have a mean cost heading set, which can sort of have an arbitrary objective function. You just need, in principle, you could do quadratic objectives. If you have a QP solver that's effective, and you can have a QBF solver, you can have an SMT solver, you can have, you know, whatever. So ASP solver, anything that provides a scores. But now we're talking about Max, so that's all.
01:30:13.454 - 01:31:19.044, Speaker B: Okay, so, continuing with our shortest first example, this is maybe a bit naive when it comes to implicit hitting sets, but let's see what could happen. So the intuition now is that the hitting sets are sort of guesses of candidate paths, given the core information that we currently have, and as long as we don't have enough core information, then there's no way of the heating set providing sort of paths to us. Meaning, of course, the heating sets are talking about those soft units over the cells that we don't satisfy, which are exactly the grid cells that we're visiting on our path. So the heating sets try to talk about the path, right, the path that we're constructing. And the sats are always trying now verifying the candidate and typically saying, no, no, no, but you have to visit at least this, this cell and this cell or this cell, because this is not a path.
01:31:20.424 - 01:31:47.608, Speaker A: Okay, so there's a question that since you have this hitting set problem, I mean, that is also an optimization problem, and so you sort of can, that's why you don't run the maxat solver on it. So, so Nikola asks, can you explain, can you explain why Maxat solvers are bad at mean hitting set problems? Is it inherent or for Maxat solvers using CDCL or other, other explanations?
01:31:47.776 - 01:32:12.630, Speaker B: Because, Nicola, yours, the underline, my short answer is no, no, I, no, you can try it out, or you can do funky stuff like taking an implicit helix over and solve the helix problem with that. So, but I don't have a proper idea.
01:32:12.822 - 01:32:20.434, Speaker A: But like, feeding the max, the heating set subproblem to a max set solver would, would not be a good idea.
01:32:20.774 - 01:32:22.034, Speaker B: You can do it.
01:32:22.454 - 01:32:47.974, Speaker F: It's pretty bad. The core guided is really, really bad at these heating set problems. I tried so if you have lp, relaxation helps. I mean, for the hitting set domains, there are certain heuristics, but I just wonder if there's something principled to say.
01:32:49.554 - 01:32:51.974, Speaker B: It's actually a good, good question.
01:32:57.394 - 01:32:58.734, Speaker A: Carlos is also.
01:32:59.074 - 01:33:05.934, Speaker B: Yeah, sorry, no, no, I don't have a real answer to this.
01:33:07.114 - 01:33:18.214, Speaker F: So another idea I was toying with was to recursively solve the, the heating set problem with another heating set and then recursive.
01:33:18.514 - 01:34:24.014, Speaker B: Yeah, that's, that's, that's very interesting. You just have this idea of mirrors inside mirrors. You can do it indefinitely. Okay, Carlos, do you send to the mip solver any subset of the hard clauses? There are things that are done, but not really directly. You can sort of get course, identify course in the beginning, but not really much when it comes to the actual search part. No, as far as I know. So Fahim, you know, has some weird looking tricks in Max Hs, which maybe work on some specific instance that are maybe not as, you know, something that you could really publish on.
01:34:24.014 - 01:34:33.554, Speaker B: And algorithmically maybe. Do Yeremy's have anything to add to that?
01:34:36.734 - 01:34:39.708, Speaker C: I mean, I'm thinking of constraint seeding. So we do send.
01:34:39.846 - 01:34:42.048, Speaker B: Yeah, yeah, that's what I referred to.
01:34:42.096 - 01:34:43.124, Speaker C: Yeah, yeah.
01:34:43.704 - 01:34:49.204, Speaker A: But not, say again, you. It was hard to hear.
01:34:49.504 - 01:35:03.324, Speaker C: So if you have a clause containing only blocking variables, you can just convert that into a linear inequality and give that to the MIP solver, for example. And this kind of stuff is done on a more technical level.
01:35:03.944 - 01:35:38.152, Speaker B: So, so you can sort of make. Infer that, okay, I have these assumptions and. Okay, they actually, I can quickly infer that. Ah, okay, so there's, there's an actual, you figure out that there's essentially a clause over just assumptions. That's of course immediately a core in some sense. Right. So you can feed that, and that's done in the beginning, but I think still not really in other contexts we've done.
01:35:38.152 - 01:36:07.424, Speaker B: So if people followed the Asp talk by rosten, I think it was a week ago. So I did with a student of mine, an Asp hitting set optimizer. And there. So depending on your problem domain, you can do things like use different kinds of LP relaxation. Relaxations. And there's LP relaxations when you have the power of a MIP solver as well. But in Maxa, nothing really is done.
01:36:07.424 - 01:37:06.634, Speaker B: Okay, so let me continue so that we'll hear about the incomplete as well. So, yeah, so as I said, implicit heating set heating set solver provides a Scandinavia path and start solver assisted none. All that. Here's a code okay, so our healing set is first empty. So we'll just check with the SAT solver if S is g and that's not the case. So we get a core stating for example, just like in the RMS's example stating that, you know, we need to visit one of these neighbors. So here's the core over QRG, okay? And then the IP solver gets this as a core and there's picks one of these as a heading set, say Q.
01:37:06.634 - 01:37:46.870, Speaker B: And now we rule out Q and ask if there's, there's a path essentially that goes only through Q and that's of course not the case. For example, because of A and B are not visited, then you have to go through a or B. So that's a core. And this is a bit of a trivial example because it turns out that if you do in this problem setting, all of the cores will be disjoint and over unit clauses as well. So, and this continues. So now you get a hitting set that hits one of the cores. Notice that it changed.
01:37:46.870 - 01:38:18.140, Speaker B: It used to be q, now it's k. And this is how what happens. So you don't really have a focused view. So you always compute a healing set over the set, of course, and it's independent of the previous hitting set over the course that you had. Now you hit a and Q and remove that. And then there's always this barrier that you have to go through and it continues. Barrier, barrier, barrier.
01:38:18.140 - 01:38:51.768, Speaker B: Okay. And at some point the IP solver gets this right in terms of hitting something optimally that consists of a path. Now notice that we had all these cores, these barriers, so it can also hit really badly. So it can hit all these cores but in a wrong way and that doesn't constitute the path still. So this is quite a problematic case as well. I think for an implicit hitting set solver it has to be quite lucky. So.
01:38:51.768 - 01:39:48.914, Speaker B: But, yeah, but when it's lucky and it does hit correctly and of course when you're at the correct bound, you've almost solved the problem. Let me, that's a good, nice bridge to stating that you really don't implement the implicit Henness approach like as the rudimentary skeleton, as I showed you. So you do all sorts of tricks. For example, you do a disjoint phase where you don't just extract one core at each iteration with the set soil, but you do a disjoint phase where you extract the core and then tell the solver, disregard those clauses. Is there another core? Disregard those courses as well. Is there another core. So you get this disjoint cores essentially for free, because after the first core the set solvers cores tend to be quite trivial.
01:39:48.914 - 01:40:57.286, Speaker B: And then you can throw all of the cores to the next hitting set IP core. Then you can use tricks from the classical IP literature because there's an LP solver there. So for example, you get reduced costs and this allows you to actually start fixing values of variables. Essentially these reduced costs tell us that given that we have tight enough lower and lower bounds currently, we can actually infer that we can deem a soft clause not to be satisfied. We can just fix the assumption variable to true in that clause, or we can deem that, okay, we can fix this assumption to false. There's an optimal solution that fixes, satisfies the soft clause. And this upper and lower bound information comes from these tricks actually.
01:40:57.286 - 01:41:32.442, Speaker B: So you do this disjoint phase and at some point the SAT solver says start. But that has nothing to do with an optimal solution really. But it gives us a bound because it gives us some solution. So it gives us an upper bound. Right. So you have this bounds information that you can use in collaboration with the IP tricks. And yet, I mean, as last year proposed that sat even got a best paper award for this abstract cores that sort of bring some elements of the core guided approaches to enthusiastics and binds, binds sort of cores together in a more.
01:41:32.442 - 01:41:47.346, Speaker B: Because if you noticed in the hitting set approach, it's sort of the IP solver doesn't really, you know, it's a bit lost, can be a bit lost and it's a good example of how the shortest path example, and these are.
01:41:47.370 - 01:41:52.974, Speaker A: Really anything, what these abstract cores are, or this would be a separate seminar.
01:41:54.674 - 01:41:56.574, Speaker B: Do you have a sentence on that?
01:41:58.154 - 01:42:10.554, Speaker C: You add extension variables to the heating set problem that are more expressive and can capture cores more expressively. That's a very short explanation, but somehow.
01:42:10.594 - 01:42:13.774, Speaker B: They bind the cores together better.
01:42:14.814 - 01:42:16.074, Speaker A: Okay, thanks.
01:42:17.094 - 01:43:23.586, Speaker B: And yeah, so CPlex, yeah, you can use Soplex, it doesn't affect performance much, you might see some degradation. But as Solex is quite good, so these are still not that hard. You know, there's a lot of iterations and these costs are not too hard, so you won't lose much. I don't know why we and everyone else wants to use CPlex. I think there's no reason except that it's the first thing that comes to mind still due to history, how many iterations? Aha. That depends a lot on the instance. And the other thing to point out is that another question that someone typically asks is that, what's the bottleneck? And it's really interesting that it depends on the problem as well, a lot.
01:43:23.586 - 01:44:31.672, Speaker B: So sometimes the SAT solver is the bottleneck, the core extraction is the bottleneck, and in implicit hitting set algorithms, you need to minimize the cores, or to a great extent, find small cores. You sort of minimize them to a threshold. And if you're using too much time for minimization, you should give up. But you really want tight, as tight cores as possible. But, yeah, sometimes the heating set problem gets bad, sometimes the core extraction gets bad, and then often neither get. And that's why it's, it's sort of competitive. And, well, I don't know who wrote this, usually superior to CPlex, but it's interesting, really to notice that if you think about Moxad encodings or problems, then when CPlex is good, implicit approach tends to be good as well.
01:44:31.672 - 01:44:50.604, Speaker B: But. And you can beat CPlex with this. All right, yeah. Then maybe hiramis can continue with the incomplete stuff. Unless there's more questions.
01:44:53.884 - 01:45:42.464, Speaker F: So, just a comment. I have a little toy where I combine hitting sets with core guided, and I found it quite useful for finding near optimal solutions. You have two phases. One where it finds as many cores as it can cheaply, and then does hitting sets over those and went over once it runs out of steam, does core relaxation to make progress. Okay, so, yeah, I just wonder that there must be a lot of things one can draw in from, say, core enumeration techniques to leverage this heating set world.
01:45:42.804 - 01:45:58.834, Speaker B: Yeah, I think that the, if you think about these different current approaches, putting them in the right way together would be the, what you would want to do. The question is only.
01:45:59.214 - 01:46:20.034, Speaker F: Yeah, so I think there are two things I haven't seen. One is to leverage core enumeration, and the other is to refine upper bound. So that maybe that's your next section. But you can really, as you do this core enumeration, you find local solutions that you can also improve.
01:46:20.414 - 01:46:20.798, Speaker B: Right.
01:46:20.846 - 01:46:25.134, Speaker F: Yeah, and to really. And that feeds back into core.
01:46:25.174 - 01:47:30.714, Speaker B: So in practice, though. So some years ago, we actually did a sort of a mock sat based implicit hitting set approach for a specific problem domain. And if you go beyond these sort of evaluations, where you have these heterogeneous benchmark sets and you actually know more about your problem and you know actually how to get really good course in different ways than just using a satsolver. And in that domain that we were working with, it's sort of causal discovery. The Sats over a horrible course, just really huge. And there were very small ones that we sort of figured out as an automatized way to figure out these templates that you can quickly compute the course out of some course out of your input. And now the world opens up, because if you start plugging in this domain specific information to this type of an approach, it really helps.
01:47:33.494 - 01:47:35.234, Speaker F: Cardinality constraints.
01:47:36.014 - 01:47:36.794, Speaker B: Sorry?
01:47:37.134 - 01:47:39.766, Speaker F: Essentially detecting some cardinality constraints.
01:47:39.830 - 01:48:12.284, Speaker B: No, no, no. So this is really a domain specific. So this, in this setting, there's a weird notion of paths and very complicated notion of paths and inconsistencies between paths. And now you figure out small cores that the SAT solver never could find. So we could have cores of length three, and the SAT solver will give us cores of length 40. So without this information, the approach wouldn't work really too well. But if you know this, then it does.
01:48:12.284 - 01:48:31.654, Speaker B: So you have to help the solver to start, and now you start getting better bounce information as well. Okay, so, yeah, I guess you can continue.
01:48:32.954 - 01:48:35.254, Speaker C: Yeah, you need to just stop sharing.
01:48:35.674 - 01:48:36.694, Speaker B: There we go.
01:48:37.514 - 01:49:27.908, Speaker C: All right, so we're running out of time, and as you can see, we got a lot of things to talk about. So I'll briefly go over some things in the incomplete world and shamelessly pick me my own work to talk about some more. So why do we want to do incomplete solving? Well, the obvious answer is scalability. And the second obvious answer is that in many practical applications, we might not care about a formal proof of optimality. If we have a good solution, it might not matter if it's not provably the optimal one. Instead, we just want to find a good solution as fast as possible. And especially in Macsat, the incomplete approaches tend to be these so called anytime algorithms that find intermediate solutions.
01:49:27.908 - 01:51:04.156, Speaker C: This has been hinted upon in this here already. So as essentially all of the different types of algorithms that we talked about here, effective implementations of them do find non optimal solutions during search. And if you terminate them, they can give you one of those. They might not be optim, you might not get guarantees on their quality, but in practice, they often are quite good, and this includes the core guided and the implicit algorithms. So the central question in incomplete solving is actually how do we combine these different approaches and shift their focus from trying to prove optimality as efficiently as possible, to instead trying to find a solution that is as good as possible quickly? So if we look at the solvers from last year's maxed evaluation in the incomplete tract, we see that almost all of them implement more than a single algorithm type. So the deal here is very much to take different kinds of algorithms and try to make as effective use of them as possible, instead of just picking one approach and sticking with it until you find your optimality proof or have to terminate. So here are the three approaches I'm going to talk say something about.
01:51:04.156 - 01:52:15.046, Speaker C: The first is to go back to the model improving search and quickly show talk a bit about why that isn't as effective as a complete approach and what you can do to make it more effective in the incomplete setting. And then the one I'm going to talk a bit more about is what we call core boosted search. This is work I did with Peter Stackey and Emir Demirovic, and it's an algorithm that combines core guided and model improving search in a very much non trivial way. And finally, if I have time, I'll say something about some stochastic local search approaches that have been proposed for Maxat. Some model improving search should in this. This we already talked about. This is just, you get a solution, for example, by invoking a desert solver on your hard clauses, and then you encode this cardinality constraint that says, give me another solution that is better, and then you keep on going until the sat solver tells you, now I can find a solution that is better and why this isn't good, as effective as a complete approach we've sort of already touched upon, and it's because of this encoding of the cardinality constraints.
01:52:15.046 - 01:53:26.404, Speaker C: And in a more general case, the pseudo Boolean constraint, they can get very large. If you have millions of soft poses, you end up just choking the solver on this large constraint. But in the incomplete setting, there's been some clever work on what you can do to sort of make that easier. And one is the so called varying resolution approach, where you just bluntly take a constant and divide all of your weights by that constant and just leave out weights that with weight, leave out clauses whose weight go to zero when you divide. Because you're just looking for a solution, you're not necessarily looking for the best possible one, and you don't need to guarantee optimality. And the second improvement is the core boosted search that I'm going to spend most of the remaining time on. So to see the intuition for core boosted search, we go back to this grid and the core guided approach, as I hope is clear to most of you by now, when we invoke a Maxat solver on this instance, we're asking for the shortest path from SDG.
01:53:26.404 - 01:54:10.904, Speaker C: But if you now remember the approach where you relax each core separately. We can see what happens after a few iterations. So this relaxed formula can be seen as asking, what is the length of the shortest path that goes from the core, one of the variables of the core in the core ab, to one of the squares in the core q, q, k or r. So this is a new mag set instance that has a lower optimal cost. And there's nothing that actually says that we have to keep on doing this core guided approach. We can just stop at any point and change to another approach. And this is essentially what we call core boosted search.
01:54:10.904 - 01:55:07.134, Speaker C: So we start in a core guided phase, and if you happen to find optimal solution, we're happy and we end. If not, we then take the final reformulated instance and give that to a model improving algorithm. The intuition for why we would want to do this comes from these bottlenecks that we've already talked about. So for the core guided phase, you don't have to use big cardinality constraints, but the core extraction steps can get increasingly more difficult. And the model improving case, the initial size of the pseudo boolean or cardinality constraint might be really large. So you might want to try to do anything you can to decrease it. So by combining these two in this, in this way, we're expecting to get something that is never significantly worse than either one of these individual components.
01:55:07.134 - 01:56:07.354, Speaker C: But actually what we found is that this combination can even be better than even any single than either of its components. Here's a plot showing the evolution of the gap that means difference between the upper and the lower bond over time on a specific instance. The blue line up here is the linear search component, which is the model improving algorithm. And we see that the pseudo boolean is the weighted instance. So the pseudo boolean constraint is just too large for the solver to make any progress over the five minutes that we gave it. And now the orange line here is a core guided component, which initially is able to find some cores and improve the bound, but then after some time, it stagnates and gets stuck in this plateau. And then finally this combination, this core boosted surge, starts off in the core guided surge, matching its performance.
01:56:07.354 - 01:57:09.414, Speaker C: And then before the stagnation happens, it switches to the linear phase. And due to the reformulation steps done during the core guided phase, the new to the Boolean constraint is actually small enough for the solver to be able to make further progress and find easily the best solution out of all of these three. And after publishing this work, it's been further improved by putting a local search solver at the start, and this led to state of the performance in the incomplete category on the unweighted instances, and then for the weighted instances. What you want to do is basically a sort of stochastic local search approach. So, as I'm sure many of you can see, the hard part for a basic. For a stochastic local search in SAT is to guarantee, you need to guarantee that your hard clauses are satisfied. So if you just go around flipping literals, it's very hard to make sure of that.
01:57:09.414 - 01:57:53.144, Speaker C: So there are some proposed solutions to this, and one very natural one is to extend the weight function that you have to all clauses. That means the hard clauses as well. You start off by putting some weight, they use one as the weight for hard clauses. And then you keep flipping literals and increasing the weights of clauses that are frequently unsatisfied, with the hope of finding solutions that actually satisfy all of all of your hard clauses as well. But it turns out that a better idea is simply to use a Sat solver to make sure that you find solutions that satisfy the hard clauses. So, very briefly, this is the final time. We'll go back to the grid.
01:57:53.144 - 01:58:20.524, Speaker C: You have a solution. Then you start looking at all of your soft clauses. First, you look at the soft clause, not a. You see that your current solution falsifies it because the path goes through a. So you ask the SAT solver for a new solution that follows all of your all of the soft clauses that you've already assigned, plus the new one. So you're saying, find me a path that does not go through a. Sat solver is happy to give you one, for example, this one.
01:58:20.524 - 01:59:08.924, Speaker C: And then you move on to your next stoppause, which is b. Now, in this solution, the path goes through b. So you're again asking the sad solver for a solution that does not. That satisfies the clause that means the path that does not go through b, but also satisfies the previous clause. So you're asking it for a path that does not go through b nor a, and this time you cannot find one. So you add the negation of yourselves, pause into the fixed, and whenever you find a class that is already satisfied by your best solution, you don't need the sad solver, and you just keep going. And by doing this, you actually end up with a currently state of the art approach and incomplete category for wave distances.
01:59:08.924 - 02:00:09.504, Speaker C: So this is my, this is the very much shortened version of this part of the talk we're trying to find increase the scalability of magnet solvers without sacrificing the solution quality too much. And off of the different approaches that exist, we've seen that they tend to exhibit orthogonal performance on different domains. As Matti also said, it very much depends on the problem domain, and hence, in order to be as good as possible, you want to try to combine solvers, combine solving techniques as well, as well as you can. And this is from the tutorial part. If you want to try your solvers this, you should go to the evaluation, try to try an incomplete solver, you should go to the evaluation webpage to see how well they did. But again, if you have a new domain, it's very much possible that some other solver is going to be the best one for your domain. So it might pay off to try the other ones as well.
02:00:09.504 - 02:00:16.644, Speaker C: So should I want to just wrap this up or do you want to say something?
02:00:17.544 - 02:00:42.054, Speaker A: Yeah, I have a question, actually. I guess we already talked about this a little bit, but again, understanding, like, which algorithmically, which approaches are good for which type of problem. Is there anything we can say? Like, yeah, I mean, I don't know. Core guided is good for problem instances of type such and such, and modeling proving seems to be much better for problems of type whatever or. No.
02:00:43.834 - 02:01:27.550, Speaker C: There'S nothing that I'm happy with. You can say stuff like if you have a low optimal cost, then you might want to do a core guided search, which is essentially a lower binding search. But at the same time, how would you know if your cost is optimal then if you have lots of diverse weights, it might pay off to try the IHS implicitly set approaches, because then you're letting a MIP solver do the weight handling. And if you have a high optimal cost, you want to try and model improving. But beyond experimental like testing and these sort of, in my opinion, a bit, maybe even a bit naive statements, I don't know, it would be interesting to have a better one.
02:01:27.742 - 02:01:35.614, Speaker A: And the cost of different weights. Would that be because like, the encodings get expensive, or is it also like for other reasons?
02:01:36.914 - 02:02:13.474, Speaker C: If you have a lot, lots of different weights, then a core guided approach needs to. The standard trick that Matti alluded to will result in lots of residual weights. So you're going to get a more significant blow up in your formula if you're doing a purely core guided approach and the PB constraint on the model improving side the encodings for the PB constraints that are commonly used, the size of them depend on the more diverse you have your weights, the larger your encoding is going to be. So hence you just blow up the formula more and more.
02:02:14.654 - 02:02:39.468, Speaker A: And then there's one more question, because you mentioned this idea of somehow just dividing the weights and do sort of a, if I understood like a rough search first, and then maybe look at the actual problem when you run. There's also, I think this, what is referred to, if I'm using the correct terminology, as stratification, where you just basically just look at the large weights first. Is that kind of the same or is it slightly different or how do.
02:02:39.476 - 02:03:13.504, Speaker C: They in stratification, you're essentially not changing the weights, you're just saying that I'm going to look at this subset of weights first and try to find my course and do rotations over those in the varying resolution that I mentioned. You're dividing the weights in order to also decrease the size of the PB constraint in your rough search. So very sort of, roughly speaking, the approach I mentioned here is for model improving search, while the stratification is for core guided search.
02:03:15.244 - 02:03:16.504, Speaker A: We see. Thanks.
02:03:19.204 - 02:03:20.982, Speaker C: Do you have a question, Nikolay?
02:03:21.108 - 02:03:30.458, Speaker F: Yeah. Do you know if any of the core guided methods backtrack over which course it uses for relaxation or they always fix with the relaxation?
02:03:30.546 - 02:03:31.574, Speaker A: I think in your.
02:03:32.354 - 02:03:44.082, Speaker C: As far, as far as I know, they don't backtrack. But that would be an interesting question to explore if you should and how you should in that case, because if.
02:03:44.098 - 02:03:47.594, Speaker F: You make a bad decision, you're making basically host.
02:03:48.494 - 02:04:03.470, Speaker C: Yeah, the problem might be that the relaxation, the process you introduce, if you then decide to backtrack, you might have to sort of restart your Sat solver, but that might pay off. I said, I like the idea.
02:04:03.622 - 02:04:09.046, Speaker F: The question is, have anybody done? I mean, is there any, is any.
02:04:09.070 - 02:04:59.154, Speaker C: Not that far known about it, as far as I know, no. And we had a few more slides just summing up what we talked about. There are a number of different approaches that you can use. Developing an intuition for what you want to use or a theoretical understanding would be even better and interesting future work. And then some further reading. Fahim gave a talk on similar topics just a while ago, and I'm actually going to give a shorter talk on Maxat preprocessing in the beginning in May as well. So you're all welcome to come there.
02:04:59.154 - 02:05:11.974, Speaker C: I guess we're sort of out of time. I'm of course happy to answer any questions, but if there are none, I'd like to thank you all for your attention.
02:05:14.474 - 02:05:26.050, Speaker A: Big thanks to both. I think we usually go over time, so I'd say there's you know, people can always exit when they like. And if there are any questions and if the speaker still have energy to answer, then we're happy to take.
02:05:26.082 - 02:05:31.294, Speaker C: Yeah, I'm in no hurry, so go for it.
02:05:36.654 - 02:05:37.874, Speaker E: I have a question.
02:05:39.174 - 02:05:39.742, Speaker C: Yeah.
02:05:39.838 - 02:06:16.974, Speaker E: About this answered course. I asked before for computing this answer course you only considered. I understood that you only considered this soft clauses. But might it make sense to consider also the hard clauses? I mean, for the example you presented, it makes sense without hard clauses. But maybe in another example, consider compute the answer call, maybe, which contains also hard clauses, but then just somehow not consider them, discard them. I don't know whether it was clear.
02:06:18.394 - 02:07:00.328, Speaker C: Well, in a way, sort of. Now, there are people who are better at sat solvers than me, but it feels to me that by using these assumption variables and putting them on a soft clause, I don't know if it would be harder or more complicated to compute the sort of full onset core with the hard clauses as well, compared to only computing the soft pause part for magset, you still have to satisfy all of your hard pauses. So that's sort of why you don't. Why we don't care, so to say. But that's interesting question. You would get more info, because I.
02:07:00.336 - 02:07:13.152, Speaker E: Was thinking, maybe these soft clauses altogether are not answered, but together with the hard clauses, they become. Everything becomes answered. This was my intuition. But I don't know. I'm not a max.
02:07:13.208 - 02:07:44.444, Speaker C: I mean, that is usually the case. I mean, as with the example that we saw here, the soft clauses usually encode some kind of an objective function. So if you just take the soft. If you have the path example, if you just take the soft process, the solver will just say, well, don't visit any nodes. So you need the hard clauses to sort of get the unsatisfiability. So that is usually the case. You need the hard clauses to have unset in many practical benchmarks.
02:07:45.894 - 02:07:47.474, Speaker E: Okay, thank you.
02:07:50.894 - 02:08:39.264, Speaker A: Any other questions? So, I also forgot to mention, by putting in the chat, that workshop, Simons workshop participants, those who are registered for the Simons workshop, should have access to the Gathertown workshop room. And you're welcome to meet there after the seminar. Or I guess already now. So, I should have mentioned. Another thing I could perhaps mention is that next week we're having a talk by Bart Bogart on explainable constraint programming. I think sort of, hopefully the Simons webpage would contain more information. And I think the week after that, on April 27, we're doing QBF solving with Martina Seidel.
02:08:39.264 - 02:09:47.454, Speaker A: Just by you know, program announcements. Anything else? We did have a lot of questions during the seminar proper, so maybe people feel they got to ask what they needed to ask. I don't see anything in the chat at this point. If any of the participants want to ask anything, then please, I guess one thing. Yeah, so like a high level question, you sort of alluded to it a little bit. I guess that this implicit hitting set approach is sort of a hybrid approach in the sense that you're using stuff that MIP solvers are good at, and then you're somehow combining the Boolean search that Magsat provides with the more LP kind of view that MIP solvers would provide. Have you thought about, or do you know if anyone has explored like a tighter, say, Maxat LP MIP integration, or would that make sense?
02:09:53.474 - 02:10:00.664, Speaker C: That would be interesting. And I'm assuming you might be alluding to your own paper on this.
02:10:01.004 - 02:10:08.172, Speaker A: No, I'm not. We have certainly pursued, I don't view this as a, I'm not doing self plugs all the time, so.
02:10:08.268 - 02:10:30.124, Speaker C: No, no, but I mean, I read that and it feels like you could maybe do something similar with nutset, perhaps. So I don't know of any approaches that go beyond this reduced cost that Matti was talking about, but I'm sure there is something to do, to do there.
02:10:37.984 - 02:11:15.404, Speaker A: Yeah, no, but because it's somehow, I mean, it's a very high level. Like, you basically take two black boxes and it's a bit intriguing that you somehow identify this part where, yeah, somehow the mip solvers are very good at this type of problem, and then the max, that's always, like, good at the other, and it's like, well, I mean, you could like, imagine, like, I don't know, having a maxat solver and then once in a while just solving an LP relaxation. Except that somehow, I don't think when you're having a CNF and you're relaxing, I guess those are pretty terrible relaxations. I guess.
02:11:17.184 - 02:11:30.334, Speaker C: This is sort of, this is my thoughts when reading this approaches. I don't know if people aren't better at Delta relaxation, say they are. I'll take your word for it. But that would be interesting to know if you could gain something.
02:11:31.314 - 02:11:38.014, Speaker A: I don't claim to know what I'm talking about at this point, I'm mostly speculating. Maybe Nikolaj wants to say anything.
02:11:39.434 - 02:11:45.934, Speaker F: Maybe it's related. Do you run MIP solvers on the magset competition?
02:11:46.844 - 02:12:02.772, Speaker C: Oh, this is. Yeah, good, good, yes. There was a solomer a few years ago. I can't remember exactly, but there was one which sort of just worked. I think it worked. There might be something else, but. But in broad terms, it worked by encoding it as MIP.
02:12:02.772 - 02:12:04.916, Speaker C: I'm solving it with a Mip solver.
02:12:05.060 - 02:12:10.460, Speaker B: Just, just the classic. Or like, direct encoding would be good.
02:12:10.492 - 02:12:17.626, Speaker F: To just have a baseline to. So it's well known what the, what the comparison points are.
02:12:17.650 - 02:12:20.794, Speaker B: Yeah, yeah, yeah. At some point, is it clear how.
02:12:20.834 - 02:12:24.050, Speaker A: Skip would perform, say, and then the latest competition.
02:12:24.202 - 02:12:32.494, Speaker B: So some years ago, it used to be that if you run, you know, you would just win, but it's not any more the case.
02:12:37.034 - 02:12:41.038, Speaker C: Yeah, Carlos is saying ILP, but I.
02:12:41.046 - 02:12:57.382, Speaker F: Think the current question is, okay, how does it compare to MIP? And then if the competition evaluation basically has a vanilla magset encoding into MIP, which is, I mean, totally trivial.
02:12:57.438 - 02:12:58.038, Speaker A: Right.
02:12:58.206 - 02:13:47.444, Speaker B: Yeah. Can you hear me? Yeah, yeah. I don't know if you heard what I said, but it used to be the case, like ten years ago, some years ago, it was the case that if you just run, you know, Cplex or whatever, you would win or be the best in the evaluation, but it's not anymore the case. Yeah, but it's true that it should be there by default. That's a very good point. One should just have it there always as a comparison, as a base one, because that's clear that it's clear that you would need to win it, but it's of course the same. If you think about, you know, if you want to incentivize people to work on it, it's good to maybe not have it there until you're better.
02:13:53.864 - 02:14:17.174, Speaker A: But I mean, it is a relevant question because, like, I think if you take the. I think if you take the pseudo Boolean competition, which doesn't happen very often, I guess latest version was in 2016, and so. And you run skip on it, even, I don't know, I guess, like Gurobi and CPlex would be even better. But I think skip the MiP solver, skip wins the pseudo boolean competition.
02:14:17.254 - 02:14:18.234, Speaker B: Yeah, yeah.
02:14:18.734 - 02:14:22.594, Speaker A: And that is, of course, I mean, that is depressing. Yeah.
02:14:22.974 - 02:15:04.764, Speaker B: But for Maxa, this. It's not the case, I think, anymore. If you, if you do it, I think you won't win. Be the best. But, yeah, for PV, I think because it's pretty much MIP then already, so it's sort of in the ballpark of MIP solvers. Another thing is, of course, that where are the benchmark, like, the problem domains that are clearly PBo and how do they differentiate from MIP in general, yeah, that's a good, to an extent that, you know, the classical branch and cut doesn't just kill the competition.
02:15:05.384 - 02:15:34.514, Speaker A: So I mean it's not the case in general. I think it's a benchmark selection problem also. For instance, I mean we've seen that. I think if you want to analyze say like binarized neural networks, then I think it seems to be somewhat conventional ish wisdom that MIP solvers are not so good because they somehow, when you relax these like whatever they're called, these relu constraints, activation constraints, they're like not good for LP relaxations. Also I think we've seen that.
02:15:37.534 - 02:15:37.918, Speaker B: Say.
02:15:37.966 - 02:15:54.452, Speaker A: Like hardware circuit verification problems, MIP doesn't seem terribly good, whereas PB pseudoboolean solvers can be really good and also pseudo boolean solvers can be really good for binary neural networks. And I guess Maxat solvers have been used for like this robust or adversarial learning and stuff.
02:15:54.628 - 02:16:05.772, Speaker B: Yeah, yeah, I guess it's a matter of how logical the somehow, how good is it, is it to reason over the actual problem rather, but also Victor.
02:16:05.828 - 02:16:38.662, Speaker A: So there's a very good point I think by Victor Miller made in the chat that is worth pointing out that one reason for the superiority of skip, or at least of think of CPlex and groby is that their pre processing works hard to recognize and exploit substructures. And I think also like really to decide things like, oh, it seems like you're doing an appsac problem. Well I'll pull out my dedicated dynamic programming algorithm to just try to solve this problem. I mean there are massive amounts of domain knowledge in these.
02:16:38.718 - 02:16:53.966, Speaker B: Yeah, it's very interesting because if you run it on instances that, that seem very similar from the same domain, you see that it chooses different lp solvers and stuff like that as well.
02:16:54.150 - 02:16:57.342, Speaker G: Yeah, but. Okay, can you hear me?
02:16:57.478 - 02:16:59.166, Speaker A: Yeah, we can hear you well Carlos.
02:16:59.270 - 02:18:02.298, Speaker G: Yeah, but for that maybe you need to pass more hair colossus to the cPlex solver if you want the cPlex solvers to recognize those structures because otherwise you are just having the jetting function for the heating set what you are doing, but you need the hardest or some part of the hardest. For example, what is very important is detection of clicks. Click detection is very important for Maxat solvers. As you know in the competition for complete Maxat solvers there is a phase of click detection of at most ones constraints and this is a trick that works very well. This is why I was asking what happens if you plug something that is not CPlex or Kudobi that we know there is some black card going on there, and you just take an open source MIP solvers that maybe they are not applying so much preprocessing. This is an interesting question, but I agree with you that it would be very nice to have this comparison with CPlex and Gurobi. Just the direct translation and to see what happens.
02:18:02.298 - 02:18:08.254, Speaker G: I don't think this is bad for the community. I think it should be encouraging for us.
02:18:08.374 - 02:18:48.824, Speaker B: Okay. No, so my point was that it used to be the case that they win, but they don't anymore. I'm just saying that in reality, if you organize the PBO competition, for example, and the reality is that currently CPlEX or Gurobi will just blow everything out of the water. So it's not very, you know. Yeah, but you know, it's good to have it there, but you know, you need to support people, you know. Of course you have to face reality, but you don't get improvement with different kinds of algorithmic approaches if you don't encourage them.
02:18:49.884 - 02:19:42.923, Speaker F: I've got questions to comments on that. So pre processing their papers in maximum that about pre processing as well. So I think the cardinality detection is one of the pre processing steps I added after looking at some flight crew examples. It couldn't do the crew examples without this at most two detection because otherwise you kill the core extraction. And then there are other approaches that look at how you split maxat constraints into disjoint sets if the weights are stratified. So I think there are lots of room one can do in the MACsAT world for just pre processing.
02:19:46.023 - 02:20:23.290, Speaker G: For the glitch detection. There is some cases where the problem is, and this is related to what you were commenting before about tracking to the previous core. So the problem is the order of the course that you retrieve, because if you do it in the correct order just by using the oll algorithm, you are going to get the same as doing the detection of the clicks for some problems. This is the case and this is very important and it's telling you that the order is actually a key piece of the whole story.
02:20:23.472 - 02:21:29.276, Speaker A: Okay, so let's see. I'm trying to manage the chat. So Kieran Macrish is apparently running off for dinner, but mentions that instances from problems involving matching under preferences might be another good source of candidates for maximum and Sudo Boolean solvers. I'm currently grading two masters dissertations on these kind of problems where constraint programming is stomping over MIP, and the CP encoding is very sat friendly, except for the objective function. And I think we've also had some news from Glasgow, I think the Glasgow. Well, I don't know if I dared to say this in Nikolai's presence, but apparently the Glasgow people did try pseudo Boolean solver on these problems also, and it's killing the MIP solvers, so. And Kieran suggests plugging a seminar on this, which I'm doing now, namely on.
02:21:29.276 - 02:22:04.822, Speaker A: This is not a Simon seminar, this is a 02:00 p.m. central European time, so maybe not very west coast friendly, but still a seminar by William Peterson at the University of Glasgow, talking about these kind of problems, where actually mip solvers seem to be fairly soundly beaten by other techniques. I think I was interrupting someone, though, maybe. Or was I? Oh, ah, I didn't. I plugged this in the wrong. Wait, let me, let me see. Did I post this? The wrong one? Oh, maybe.
02:22:04.822 - 02:22:05.714, Speaker A: Yep.
02:22:09.214 - 02:22:10.114, Speaker F: Kidney.
02:22:21.274 - 02:22:31.374, Speaker A: And I guess, Nikola, you're saying. And as SmT solver right there, I'm very happy with advances in Maxset and PB. So you're just an SMT solver, you harness anything you can use. Is that so?
02:22:35.034 - 02:22:36.294, Speaker F: I don't complain.
02:22:40.454 - 02:22:52.154, Speaker A: Yeah, I think this is. Any final questions or comments, which maybe we should let our speakers off the hook? I think this is a great discussion, but I guess they deserve a break at some point.
02:22:53.534 - 02:22:56.754, Speaker C: No, thanks for all the good questions, good ideas.
02:23:02.394 - 02:23:22.814, Speaker A: So I think that, on behalf of everybody, just thank you so much for a wonderful tutorial. This was great. Thanks, and looking forward to see all of you then in a week from now about explainable constraint programming. So thanks so much for today.
02:23:24.794 - 02:23:25.694, Speaker C: Thank you.
02:23:30.174 - 02:23:30.654, Speaker A: Thank you.
