00:00:00.160 - 00:00:06.438, Speaker A: Welcome to the afternoon session. The first talk will be by Paul Cristiano about AI alignment.
00:00:06.606 - 00:01:00.866, Speaker B: Yes. So this talk will cover some slightly unusual topics, so you should feel unusually free to stop for questions or clarifications or heckling. My voice is not doing super well, so if I start whispering, it's just because trying to conserve it, feel free, if I'm unclear, to ask me to speak a little more clearly. So what I mean by AI alignment is the problem of trying to build AI, or the problem of building AI, which is trying to do what we want it to do. So the idea is that even if you build smarter and smarter systems, you'd like them to be applying that intelligence in the service of the tasks which we constructed them to solve. And the reason I think this may be a problem, or sort of a theoretically interesting problem, is that ML works particularly well in the setting where we have a clear objective, where we can say, like, here's your score. In a game, you want to optimize your score, or here's some data that you want to predict.
00:01:00.866 - 00:01:49.106, Speaker B: But we often want to use ML in cases where we don't have some simple short term objective. And maybe this is particularly true as we look forward to speculative applications. So if you imagine, say, an AI system which is trying to advise me on how I should spend my week, or an AI system which is trying to advise us on some policy decision, the actual metric we care about is something that we can't measure easily. The actual metric is something like how good are the consequences over the next ten years or 20 years, where, like, first, we can't wait to see those consequences. And second, even if we could, we don't really have a precise definition of what is good, we don't have a precise definition of what it is we want. And we might be concerned that if we construct some simple proxy that we can optimize with ML, we'll end up sort of optimizing the proxy at the expense of breaking the connection between it and what we care about. So I'm going to be talking about alignment sort of as a reduction.
00:01:49.106 - 00:02:45.224, Speaker B: That is, I'm going to assume that we're handed some powerful ML techniques. That is, we're handed some algorithm which is able to optimize a well specified objective, and we want to use that capability sort of as a black box to perform some hard to specify task. And we'd like to do that reduction in a way which is both efficient, so it doesn't introduce much overhead, it's competitive, so that the policies we get out are in some sense about as smart as the policies which we could have gotten out of the black box optimizer we were given and is scalable. So we don't want to have to make some assumption of the form this works as long as we're not optimizing over too large a space, or we're not optimizing too hard over that space, or so on. I'll be a little bit more precise about what I mean by this in the next few slides. Again, to the extent there's unclarity about the problem, feel free, just chime in now. So first to talk a little bit about what I'm going to assume that I have, what we have to work with, what we're given.
00:02:45.224 - 00:03:19.084, Speaker B: Um, I'm going to work in sort of an online learning setting. So I'm going to imagine there's some fixed class of policies. So you might think of this as being like some space of neural nets with a billion parameters. There's some space of inputs and outputs, maybe to make life simple. These are like sort of questions and answers, which are both just expressed in natural language. The inputs might also have a bunch of other context as part of them, though. And the algorithm expects to see some sequence of decisions where at time step t it receives a context, or x provides an output y and we provide a loss.
00:03:19.084 - 00:04:12.814, Speaker B: We might be either in sort of a bandit setting where we just provide the loss for the particular output that it gave, or it might be in more like a supervised setting where we provide sort of function which it can use that would give it the loss for every value of y. And then this algorithm brings some combination of optimization, a clever choice of model class, clever regularization, clever exploration strategies that allow it to achieve a low total loss. And by low total loss I mean by comparison to the best model in this class, we'll say this learner's good. If the total loss it receives over some arbitrary sequence of decisions is close to the total loss of the best policy in this class. So we might imagine that if the space of neural nets was convex, then something like SGD will give you some guarantee of this form. And if we make the regularization or the exploration or a model class or optimizers better, then we end up with bigger classes, c that we can compete with, and lower bounds on the regret. This is kind of what I'm imagining that we're given in the reduction.
00:04:12.814 - 00:04:55.862, Speaker B: This is where we start. We have some algorithm which has a property of this form, some regret bounds, some model class. Now to say a little bit about what we want, I'm going to try and present it in a way that makes it as structurally similar as possible to what we have to really emphasize the distinctions. Again, I'm going to imagine that there's some class of policies we'd like to compete with. Neural nets with billion parameters, say at each time t, we want to be able to come to the system we've built, make some query. Again, we want to maybe ask it some question, receive some response, and now we have some preferences over the different responses that the system could give. So at time t, based on the actual context in the world, not merely based on the context we provide, we have some utility function of the different answers it could give.
00:04:55.862 - 00:05:32.444, Speaker B: How good would it actually be if the system produced this answer? We don't directly observe that utility function and as before we sort of want to build a system which delivers a lot of utility compared to like the best model in this class. Like best case, some would have come down and said here are the parameters that you actually want to use in your question answering system. System. And we would like to achieve a high utility compared to that benchmark. And when I say that this is competitive, when say the reduction is competitive, I mean that I want it to compete with a similar class c. So maybe we're going to end up competing with a class that's smaller than what our algorithm could have done. I don't want to increase the regret by that much.
00:05:32.444 - 00:05:48.464, Speaker B: And I say it's scalable. I say this reduction should work for arbitrarily large classes. We may have to make some assumptions on the class C, but hopefully they won't be of the form, it's not too big. So I presented these in a way that makes them.
00:05:49.284 - 00:05:53.020, Speaker A: Can you emphasize what's the difference between this slide and the previous slide?
00:05:53.132 - 00:06:14.862, Speaker B: Yeah, so I think there are basically two gaps. So one is that in order to get our bound we have to provide the loss at every step, but we can't provide the utility U. So that's the first gap and the main one that I'm going to focus on in this talk. Um, and there's another large gap where the regret is going to depend on sort of the maximum difference in losses. Um, that's going to give us a vacuous bound if we try applying it to like an actual practical deployment of ML in the world. Right. It's going to give us some bound.
00:06:14.862 - 00:06:34.630, Speaker B: Like you know, there will be at most one day on which every self driving car in the world crashes, which is going to be really the actual boundary utility that results is really weak. We need something stronger, which is where we get into sort of ideas like verification or adversarial training or so on. So yes, ICV's or my research I think about these are the two main gaps between what I'm happy to help myself to and what I want to achieve.
00:06:34.782 - 00:06:42.134, Speaker A: Yeah, so you said that utilities are unobservable, right? So what is, what does the algorithm observe?
00:06:42.214 - 00:07:34.514, Speaker B: Yeah, so we're going to have to, in order to get any traction on this goal, we're going to have to make some kind of normative assumption that relates some observations, um, to the utility function, and we're not going to have any hope of having an algorithm without such an assumption. I guess I think of that as part of the statement of the problem and part of what makes it sort of a fuzzy problem is like what kind of normative assumption do we want to make? I'll talk a little bit about that over the course of this talk. Yeah, oh, sorry. So we need some normative assumption that relates something we see to the utility function. One option would be to say the humans just know what is good. So for example, we could ask humans which is better, this outcome, or this outcome, or this output, or this output, and say that if humans prefer one to the other, then in fact the utility of that is higher. So if you imagine that you then have oracle access to a human, and you make this assumption relating to that human, to the utility, now we're in the regime where we can at least hope to have algorithms.
00:07:34.514 - 00:08:14.174, Speaker B: In some sense, this is the most natural, or like simplest or most natural assumption. They sort of run into two problems. One is that humans are expensive, and so if we make this assumption, the complexity of our algorithm is going to depend, we're going to have to make some large number of calls to this human. And a second is that I don't actually want to take humans as my source of ground truth. That is, I might think that in the long run my systems are making decisions that are more complex than what a human could understand, or we'd like them to in fact help us make better decisions than we could have otherwise made. I think this is a good starting point for the kind of assumption we might make. In the first part of the talk I'm going to discuss a little bit problem one, and then the bulk of the talk I'm going to talk about problem two.
00:08:14.174 - 00:08:49.324, Speaker B: So our first problem was that humans are expensive. I'm relatively optimistic about being able to overcome this problem in practice, although it's certainly a fairly like it's a real problem, it's a serious problem. I'm optimistic about being able to do something. So examples of how we can get traction or why we might hope to be competitive. Part of competitiveness means if the algorithm we were starting with didn't necessarily need to make these calls to humans, we want to not have to make that many calls to humans. We want to not increase the total cost of our algorithm that much. So, reasons why I'm optimistic about being competitive.
00:08:49.324 - 00:09:25.336, Speaker B: One is that what we're trying to do, or sort of the definition of the goal is often simpler than a policy, which in fact behaves well. So we might hope that we can exploit that gap to reduce our dependence on humans, maybe from humans. You have to interact with humans in order to learn what you should be trying to do. You have to interact with the real world in order to learn how to achieve it. If the statement of what you're trying to do is much simpler than how to achieve it, then most of your sample complexity is just going into these interactions with the world, which we're not making any worse by this reliance on humans. The idea is that we can just query the humans. If we have this assumption about humans, we can imagine querying them on points where we can learn a lot about humans.
00:09:25.336 - 00:09:58.594, Speaker B: So we don't just have to be asking about random decisions that come up. We can imagine learning from cheap correlates. So we could say maybe humans have some complex thing they want if they have a system acting in the world on their behalf. But most of the time it can just learn from some simple proxy, like how do I make a lot of money? Or like how do I sort of control the environment in some simple way that's like, useful for practicing how to become intelligent? And we can imagine pre training on other tasks with cheap feedback, for example, like predicting the world or predicting what a human would say. Zachary Tim covered a lot of these in his earlier talk, and that's one reason I'm not going to dwell too much on this challenge. I'm optimistic about being able to find something. I think right now it's a hard problem.
00:09:58.934 - 00:10:03.594, Speaker C: Do you query humans about deterministic outcomes only, or also.
00:10:04.694 - 00:10:36.644, Speaker B: Yeah. So it's worth noting that, like, if I actually start with this assumption, this assumption is not enough to actually get off to the right, because, like, there's going to be trade offs this algorithm needs to make where there's uncertainty. So I actually have to be able to say something and at least some probability, like need to ask them about some probabilistic mixtures. So I could pick some reference, two reference outcomes and ask them about mixtures between those reference outcomes and y and y prime. And so I need to be able to query them about that kind of information to actually have hope. I think there's really a lot, this is a reasonable example, this, like if humans prefer y to y prime, then this utility is more in practice. Like you're not going to want to make an assumption simple.
00:10:36.644 - 00:10:45.700, Speaker B: You're going to want to be a little bit more sophisticated, like talk a little bit more about the sense when humans are approximately optimal or kind of know what's up, go something more mild. Those two problems will still both apply.
00:10:45.812 - 00:10:51.636, Speaker A: I'm a bit confused about the level of formalism. I mean, everything was formal and suddenly there's humans.
00:10:51.780 - 00:10:52.196, Speaker B: That's right.
00:10:52.220 - 00:11:04.820, Speaker A: So can you think of it as you have a whole oracle and you want to minimize the number of queries to the oracle and there's only certain type of queries that the oracle can answer, or do you want to be, and regret with respect to the oracle?
00:11:04.892 - 00:11:48.034, Speaker B: I mean, yeah, so this is unfortunately just a feature of the talk where the level of formality is going to continuously drop starting from a high point, and now it's lower, it's going to become lower still. But you could set up this problem at this point formally by saying you have access to some oracle. H, we're going to make this assumption that like H y prime is one if and only if U of Y is greater than U of Y prime, and then we can talk about query complexity of that algorithm. Um, unfortunately, like, so as we move to this world where we start asking what are the actual assumptions under which say active learning works well or reward learning will work well, then things are going to get dicier to try and formalize. And as we talk about going beyond humans, these normative assumptions are going to get even dicier. Part of the game is trying to make those assumptions to something more precise, and part of the game is going to be coping with formalizing what we can and coping with what we can't. I'm not super happy with the situation, certainly.
00:11:48.074 - 00:11:52.490, Speaker A: So what do you assume about humans? Do you assume that they are transitive in their preference?
00:11:52.682 - 00:12:36.952, Speaker B: Well, so certainly if I make this assumption, so if I make this assumption, then certainly that implies transitivity, which is a good way to just debunk this assumption outright. Like you can tell this assumption isn't true because we're in fact intransitive. So in the long run you're going to want to make something weaker, which is like an example of an assumption you could make, is that the probability with which a human prefers one outcome to another in some sense depends on the utility gap. That's also obviously going to be false, since then you would say that an ensemble of humans, especially large ensemble, is transitive. And again, this gets into where the game is not like it seems quite hard. This is like the problem we care about in some sense, but it seems very hard to get like a real formal statement, which is still both plausibly true and plausibly soluble as a problem. So I expect there'll just be a compromise on the two sides.
00:12:36.952 - 00:12:56.178, Speaker B: You have formal statements which are obviously not true, like assumptions you can make of humans that are obviously too strong, under which you can solve the problem, and then you have the actual thing you care about, which is not formalized. Do you hope that progress on these assumptions that were too strong are getting you towards an algorithm which actually works, works in the world? Like I said, this is just going to get worse once we ask about how you go, why is there outcomes?
00:12:56.226 - 00:12:58.586, Speaker A: Or distribution of the outcome?
00:12:58.690 - 00:13:08.058, Speaker B: So in this setting, why is just actions proposed by the system? So you can imagine relaxing it, which is something we'll talk about in a second to humans observing outcomes of decisions and having preferences over outcomes.
00:13:08.226 - 00:13:11.530, Speaker A: So humans have preference over actions, not over outcomes.
00:13:11.682 - 00:14:08.430, Speaker B: Here I'm saying we have preferences over actions. What? So this object, you, reflects not only a human's preferences over outcomes, it also reflects a human's expectations or subjective beliefs over which actions will lead to which outcomes, which is a sense in which wanting to do better than humans interferes with this. So one way in which you can do better than humans is you could say, instead we get to observe the outcomes of decisions, and now we think humans preferences are correct about outcomes. That game is still going to be hard or still going to be intractable to work with, because the outcomes humans care about are long distance in the future. Sort of, in some sense, can't get around this problem of needing to fold in both human preferences and some like, somehow you have to get these expectations of outcomes that are ten years hence. Because we can't actually, if we just have an assumption that human preferences over outcomes are right, then we're going to run into a wall once we say, well, now we care about like the effects of this policy in ten years, and we don't get to observe that, or we can't have enough observations to plausibly have a regret bound using only that feature. So we like need to learn something more about the structure of the world, or we need to make some further assumptions about the structure of the world in order of traction.
00:14:08.430 - 00:14:11.434, Speaker B: We can't just rely on feedback if the time horizon is so long.
00:14:13.574 - 00:14:36.122, Speaker A: Yeah, so just to make sure I'm thinking in the right spaces. So to take a concrete example, if I were thinking about the self driving example you were using before, then the preference judgment would be in fact over all the various controllers and actuators and all the other things that the. That data is controlling.
00:14:36.218 - 00:15:15.092, Speaker B: Yes, I've alighted some of the complexity. Yeah, I've alighted some of the complexity by just saying you have a context, then you have an action. In reality, you may care about sequences of actions, but setting that aside, your context would be like the video so far of what has happened. An action would be like some set of motor outputs, and then your preferences would be from a human's perspective, what do they think? Is the utility marginalizing over, like, all the places where that video would appear in the whole world? What's the expected utility from taking this action? I was kind of asking for it by beginning with a more formal statement. Continuously getting less formal. Yeah, this is the kind of thing. I don't feel great about it.
00:15:15.092 - 00:15:44.444, Speaker B: It's the kind of thing I work with. I don't know how to. I don't have a better way to go. Sort of imagine working from two ends of gradually formalizing better and better, and then also trying to have algorithms that seem like they can possibly work. If we want to isolate this challenge now we can be in the regime where we can make it completely formal in various ways. I think all the formalizations are probably, they're not going to be quite right. And so as normal, there's going to be some sequence of better and better formalizations to capture more and more.
00:15:44.444 - 00:16:24.174, Speaker B: I'm going to talk a little bit about this first structural feature that might make it possible to be more efficient, where maybe what you want done is simpler than the policy. So back here I just assumed that the human directly had preferences over actions. And then learning these preferences over actions is quite hard. But I could assume instead that humans have preferences over outcomes, with more of the complexity being in the mapping between actions and outcomes. And maybe then I can get further. So we can imagine again removing the sequential setting and just talking about the single step setting. I can imagine there's some transition function where the outcome depends on the situation in which action was taken.
00:16:24.174 - 00:17:13.256, Speaker B: The humans now have preferences over triples, like there's some function r. Or maybe you can query r by talking to a human. And this tells us, like, if you're in situation x, take action y produce outcome z. How happy are you with that? Now if we have to consult a human, if you imagine like a human spending an hour, you may be in the regime where querying r is very, very expensive compared to querying t. At least we want to control the sample complexity in terms of r as well as the complexity complexity in terms of t. So if we can have much lower sample complexity in r than in t, then we are sort of okay if r is a lot more expensive than t and the bound on like how expensive it's okay for r to be, just depends on how low we can make the sample complexity. And again, here we make some similar assumption, like in fact, like saying that u has this form sort of amounts to saying that.
00:17:13.256 - 00:18:11.784, Speaker B: Like saying that has this form amounts to assuming something about the relationship between whatever the definition of r is and you eg, that if a human thinks for an hour, they correctly report their preferences over algorithms. So in this setting there's a very simple approach, although analyzing even this very simple approach in theory is already fairly difficult. Where I just say in parallel, we're going to fit some model r hat using supervised learning and then minimize sort of that the learned reward function. And this is going to work under some assumption about the learnability of what the human is doing, some assumption about how that is the complexity of learning, that is simpler than the complexity of learning. Good policy. But then actually even further assumptions will be needed to get this going in the theory. So that gives us, if we imagine the normal RL setting where we have sort of rollout workers interacting with an optimizer, that makes our policy good.
00:18:11.784 - 00:19:07.524, Speaker B: Our trajectories go from the rollout workers to the optimizer, and updated parameters go to the rollout workers. That picture just gets expanded by we take the resulting trajectories and send them to some human labeler. Again, perhaps at this point want to use sort of some clever active learning scheme to decide which trajectories to query at. Maybe we want to use pre training to be able to more quickly fit this predictor which says, what is a human going to do? How is a human going to label those? If we have a predictor of human behavior, we can hand that to the optimizer instead of directly having the human involved in each trajectory, thanks again, there might be a label back from the reward predictor to the human labeler. If you want to be strategic about how you select, what's a label? This is an example of a very simple strategy for hopefully helping with the sample complexity or the dependence on humans and humans being expensive. It doesn't really deal at all with the seconds challenge of wanting to go beyond humans. And it's also certainly not a whole story for how you would reduce the.
00:19:07.524 - 00:19:10.464, Speaker B: How you can be okay with having a really expensive work function.
00:19:16.884 - 00:19:39.680, Speaker A: Yeah, I just have a question on going beyond humans. When you say that, do you mean going beyond any individual human for a particular problem? There's some intrinsic noise in any individual human, or going beyond the aggregate of an average of a large number of humans?
00:19:39.792 - 00:20:04.304, Speaker B: Yeah, I think there's a lot of assumptions we could make that sort of impose weaker and weaker restrictions on a human. Maybe the weakest one you could imagine is like, if you assemble the largest group you can feasibly assemble, then they have some statistical edge in every case where they're more likely to choose the better outcome. That's sort of maybe not literally the weakest, but one of the weakest things you can imagine. I am sort of not happy even with that. I'm not happy even with that. But even getting to that, you sort of will have some problem where sample complexity there becomes prohibitive.
00:20:04.804 - 00:20:08.580, Speaker A: You want to do better, but philosophically, you want to do better than that ensemble.
00:20:08.732 - 00:20:55.714, Speaker B: That's right. So I think a problem from this perspective is if you're fixing that ensemble, you're not going to be scalable, in the sense that if your model class is substantially better than humans, you'll definitely start doing the wrong thing, substantially better than your group of humans. So part of why I talked a lot about this, despite not in the long run being interested in sort of doing what a human would do, or doing what a human thinks is good, is that I think this building block of coping with very expensive sources of feedback about what is good is an important building block. Even if we want to go beyond humans, and I think it is one of the. In this regime, we can actually do things that are theoretically well specified. And so that's nice. Whenever one can pull off a problem that at least makes sense, if I'm willing to assume that the human is actually telling me which things are better, which outcomes are better, are better, now I can actually start to prove theorems, or at least write down algorithms that we can characterize what they're supposed to do relatively well.
00:20:55.714 - 00:21:18.500, Speaker B: So this is the thing I spent a lot of time working on over the last few years, sort of coping with humans. I still don't think. I think a lot of people have worked on this problem. It's like an important problem in lots of domains. That's part of why I'm relatively optimistic that it's something we can bank on having in the long run. Yes. If anyone wants to ask about this part of the talk, maybe now is a good point to pause.
00:21:18.500 - 00:22:02.004, Speaker B: Otherwise I'm going to jump right into like the desire to outperform humans, or how we can even think about what might work there. So our basic problem here now is that humans are not actually ground truth. That is, I can look over the course of, say ten minutes or some length of feedback, which I can actually get over the course of a training process. But if I want to evaluate how good is the situation after ten minutes, and I'm asking a human, a human isn't actually giving me the ground truth about that. What I actually care about is like, you know, after ten minutes the world has entered some state. I care not just about the intrinsic value of that state, but like, what is the value with respect to sort of. I want the value function for this, like human evaluation.
00:22:02.004 - 00:23:13.540, Speaker B: I want to say, are we in a state where like, things are going to go well going forward? So I'm going to lump all that into this utility function u and say this utility function's unobserved. Even if we assume that humans can tell how intrinsically valuable a state is, like, they can look at everything and be like, is good stuff happening in the world? They don't know, like how much good stuff will be happening five years down the line. And we would like a reward function r such that optimizing that reward function is equivalent to optimizing this unobserved utility function. I'm going to talk about a possible sort of a laxer assumption we can make on the humans than that they know the right answer. Um, and then some of the difficulties involved in that approach. So rather than assuming that a human can give me correct answers to questions of the form, which of these two outcomes is better, I can assume that a human could decompose that hard evaluation task into slightly easier subtasks, and then likewise features as subtasks that the human could decompose evaluation of that subtask into slightly easier still subtasks. So sort of like saying, instead of making an assumption that a human can directly answer hard questions, assume that they know, like consistency checks amongst these questions and then if I have that assumption, I could then try and do something sort of like alpha zero, where I take my hard problem.
00:23:13.540 - 00:23:34.574, Speaker B: I have my system try and perform that hard problem. And then to tell how well it was doing, I ask a human to decompose it into easier subtasks, use the current model to perform those subtasks, and thereby define some really expensive reward function. It's going to be expensive because it involves not only consulting a human, it involves using the current model to perform a whole bunch of subtasks. So it's necessarily going to be at least some large constant factor more expensive than executing the model.
00:23:37.034 - 00:23:49.414, Speaker D: You just get grounded in a concrete example of tasks that breaks down the kinds of subtasks you have in mind. Or maybe that's your next slide.
00:23:49.874 - 00:24:55.614, Speaker B: It is. It's maybe not the most grounded example in the sense that it's a very speculative long term example, but hopefully gives a flavor of the kind of I'm thinking about and I'm happy to talk. And some other examples will come up later that are a little bit closer to current practice. So if I imagine like some programmatic task that a human is not very good at, like design some regulation in this industry, which we might in the long run, you know, as the world gets more complex, we'd like ML to help us solve this task better. We say, like, a human can't just look at some piece of legislation and say, was this good? But we hope that a human could say, if I had assistants who were able to solve a bunch of sub tasks very well, the human can orchestrate that effort in order to give like slightly better answers than one of the systems could have given directly. So I can say, like, in order to evaluate how good this policy is, there's a bunch of sub questions like, what are the effects going to be on various industries? What are other consequences they should be paying attention to for these possible outcomes? How good are these? What will enforcement costs be? Et cetera. I could hope that each of these tasks individually is easier than solving the top level task or evaluating the top level task such that if I have a model with a given level of sophistication and I apply it to these subtasks, I get out a reward signal which is smart enough to guide the model to improve at the high level task.
00:24:55.614 - 00:25:39.474, Speaker B: I can hope that we can continue applying this down the road. So if I, like, take one of these subtasks, like predicting the effects of regulation industry, I can break it down further and say, like, what are activities that are affected what people would be affected? How might they change their behavior? Which of those changes are they most likely to engage in? And I can hopefully just continue drilling down until I get to questions where I'm at least happy just accepting human level behavior on those questions. So if I ask, like, once I get to very concrete queries about the world that a human can just go check now, I say, great, I'm happy just training an ML system to do what a human would do, but cheaper. This is the kind of thing I'm hoping for, like the kind of setting in which this assumption about decomposition might be both plausible and substantially weaker than an assumption about directly being able to say, which piece of legislation do I think is better?
00:25:40.974 - 00:25:48.442, Speaker D: It seems like this requires a couple capabilities that we lack. And I wonder which one you think is like, the harder one.
00:25:48.538 - 00:25:53.690, Speaker B: Like one is a couple seems like an understatement, but yeah, one is this.
00:25:53.842 - 00:26:18.006, Speaker D: Like it's a big bottleneck, like there's this decomposition, but everything you decompose it into is like the kind of counterfactual question that we don't have, like, great tools for answering in general. And is that challenge decomposition or. It's the challenge that these questions are all the nature that is very different from, like, what deep networks have launched data give us.
00:26:18.190 - 00:26:45.494, Speaker B: Yeah, so I think there's one part of my response is, I definitely think the hardest part of building a system which answers questions like this is probably not producing an objective that induces good answers. Like, it's probably like these tasks are all absurdly hard tasks. Like, if you throw, like some big language model these tasks, it's going to give you terrible answers. So that's like, first thing is, that's like the hardest part. Definitely, like the biggest difference between the world now and the world. We could solve these problems. So I'm just using a motivating example in that way.
00:26:45.494 - 00:27:22.194, Speaker B: Then, more directly, your question, like, it is the case that all of these questions are the kinds of things that are not. It's not easy to get a big data set with answers to these questions to train a model. So you could maybe say there's two possible reasons existing models can't solve questions that involve, like, this kind of counterfactual reasoning. One would be the models sort of architecturally, there's no way of setting the weights of the neural network that would give good answers to these questions. That's like one possibility, and a second possibility is like, there is no training signal which actually would induce or allow us to optimize something to discover that. And so I'm sort of setting aside or bracketing the first question and say, looking forward to some world where in fact there's a setting of the parameters which would allow it to answer these questions, which I think is a little bit of an open question. Right now.
00:27:22.194 - 00:28:00.374, Speaker B: We don't really know exactly what neural nets are capable of doing if you have an objective that incentivizes the right behavior. We're getting quite good at this next word prediction task, but that doesn't imply that we're very good at these other tasks. I'm trying to bracket the first issue and then for the second issue of like, can we train realness to these things? Can we have objectives? I'm just hoping to recursively continue applying this approach. So you're sort of just learning how to reason about such counterfactuals by looking at what a human would do or looking at what answers a human would regard as good to these questions. And so you ground out a counterfactual question so simple you can hope to learn them from just a bunch of observations with humans, or like humans saying, you know, is this answer good? Or demonstrating, here's the answer I would have given to that kind of question. Those are the two. That's the answer.
00:28:00.374 - 00:28:36.744, Speaker B: Do you think it's reasonable to give some sort of estimate as to how many? Like, each time you recursively do this, you're going to have more subtasks per test, and so you're growing in the number of possible tests and then. That's right. I think what you're saying is the idea you'll eventually get to a test that's straightforward that you could use an ML model to. But at this point we have like to percolate back out of that recursion. We have so many subtests that we're aggregating together. That's right. So actually running this decomposition would be very impractical if you imagine you have a branching factor of like ten and you think you have to go to a depth of like 20 in order to solve a task that's a very big number of tasks.
00:28:36.744 - 00:29:24.762, Speaker B: So hope is to never actually perform this kind of decomposition more than one level deep. Or like it's sort of imagine analogous to a tree search in Alphazero, where you might say, we could run this tree search. Actually exploring the entire game tree in this way is deeply impractical. But we hope that a model which is able to solve this task at the top would also have a reasonable level of performance on all of these subtasks and all of their subtasks and so on, such that if we train a model not only to solve the top tasks, but also to solve all of the subtasks at the same time, the total capacity that model needs is not much higher than the capacity a model would have needed to solve just the top level task. And then we can hope, in parallel with solving the top task, we're actually going to train all of these subtasks. And during training, we're going to use the model to solve the subtasks that appear rather than actually expanding out the recursion. So next slide is going to be a little bit of a schematic of the training process, which may hopefully help a little bit with that.
00:29:24.762 - 00:29:31.202, Speaker B: But before going on, if there any other questions about the last slide, I'm happy to. Yeah, some of what you're saying here.
00:29:31.258 - 00:29:40.394, Speaker A: I mean, some of these tasks are really, some of these problems are really not quite machine learning problems. So do you imagine sort of machine learning system?
00:29:42.814 - 00:30:23.664, Speaker B: Yeah. So again, I want to make this distinction between, like, first I want to hope that if there's like a setting of parameters in your neural net that could solve the top level task, there's a setting that could also solve the sub tasks to a high enough level of quality to certify that solution to the top level task. And then there's the question of how you train those tasks. But like, to the extent a game theoretic question comes up, I'm just asking, is there a way that a neural net can solve that kind of game theoretic question? And then do we have an objective that incentivizes it? The objective that incentivizes it is not going to be like a traditional game theoretic objective. It's going to be more like ask a human to evaluate that response or to generate a possible answer to that question using the same kind of decomposition. So if you have a game theory question, you're going to ask, ultimately you ask, like, what is the equilibrium of this game? You submit that question to your neural network. It produces an answer.
00:30:23.664 - 00:30:53.610, Speaker B: A human looks at that answer and now wants to say, is that a good answer? And so the human is going to ask some sub questions which might be like, are there better responses for this other player? They would ask that sub question. In that case, you're sort of implicitly doing tree search in the same way you currently would. So in that case, you can have a more game theoretic algorithm, but it's not a special case. I mean, these are sort of the same procedures used to evaluate all these answers, namely ask a human, how would you evaluate whether that answer is good? And one kind of thing a human can do is ask is that in equilibrium, like, what are possible deviations? I don't know if that makes sense, but do you worry?
00:30:53.682 - 00:31:03.294, Speaker A: I mean, in many of these settings, some pairs of strategies are knowledge of temporary because that will create a nuclear war or something like that.
00:31:03.374 - 00:31:41.832, Speaker B: Yeah. So we're not hoping to learn about these things by any of these things by feedback in the world. We're hoping to learn sort of humans have some reasoning process and like part of the game here is how do we do these things when we don't want to wait ten years to see what happens? So we're hoping to learn about those pairs of strategies by asking a human like, is this a better response that makes you skeptical of the original equilibrium? And we're not necessarily getting impact input from the world. And we can talk about that a little bit later. How that comes in. You're learning to reason sort of in the way implicit in what humans currently like. Taken to the extreme, where you're sort of implicitly assembling this very large network of tasks to talk a little bit concretely about what this training process would look like.
00:31:41.832 - 00:32:24.048, Speaker B: We're training some system to take its input like a question x and produce an answer y or whatever, to take as input a video and produce a torque for your robot. The way we're going to produce the reward function. We're going to construct this expensive reward function that we then optimize in the same way we talked about optimizing a human reward function earlier. Now, instead of just going to a human, that pair xy goes to a human who is allowed to interact with many copies of the current model, that human interacts with many copies of the current model to produce a reward function. And then we provide that to the top model as its signals optimize. And you can either imagine sort of constantly doing this using the current model and the sub pieces, or you can imagine doing training a sequence of models which we hope are increasingly competent. The first one is just trained, imitated humans, subsequent immediate human interacting with the first one and so on.
00:32:24.048 - 00:33:20.276, Speaker B: Or not to imitate, but to optimize this reward function. Yes, like I said, this talk started off much more formal. At this point we're to a thing that has pseudo code. But if you want to analyze this and say whether it works, things are getting quite hairy. And that's the thing I've thought about a lot, but I'm not going to get into in as much depth as I would like over the next ten minutes. So I think there are a bunch of big questions if you want to take this kind of strategy. So one big question is, can we actually use this kind of decomposition to train a machine learning model? So is this competitive? Does it introduce a lot of overhead? Does it actually, like, discover the best model in the class at answering these questions? Or like, how much longer does it take than if I just had these targets directly? There are a ton of theoretical questions about whether decomposition is a plausible strategy at all.
00:33:20.276 - 00:34:17.428, Speaker B: It's like, how do we articulate the kinds of normative assumptions we are making? For which tasks should we expect this to be possible at all? How do errors compound? If you imagine a machine that makes errors on some class of statements with some probability, how do those compound as you iterate? Training? When can we stop at saying something like this works in practice, like this strategy makes good predictions in practice, or, or happy? And then there's a question about reasoning, which maybe in some sense could be a theoretical question. But for now it's just an empirical question, which is, does human reasoning in fact have the structure where a human can decompose a hard evaluation task into easier parts and do so recursively? There's some of the big questions about the overall feasibility of the strategy. You may hope in the very long run you end up with some sort of working examples and some theory that explains why they work. Right now, I think I'm more in a state where here are some big problems. Maybe we see ways to attack each problem and it's ways off. You have a system which you have a good argument that it's going to work. Maybe again, a reasonable time to step back and take questions.
00:34:17.428 - 00:34:25.344, Speaker B: Is there anything about what is basically going on? What is the hope here? How would this training strategy work at all? And so on.
00:34:27.684 - 00:34:43.484, Speaker A: Yeah, so, stepping back, humans are hard. Do you have any hope or proxy of some non human thing that can be used to test this methodology?
00:34:45.984 - 00:35:54.964, Speaker B: Yeah, so I guess I would say for me right now, the kinds of tests I'm thinking about divide into a few categories. So some are like empirical tests with ML systems today, and those are mostly going to have to use simpler systems, which maybe range from, like toy systems to a little bit less toy, where you say we have some kind of decomposition and we want to understand, like, what properties of that decomposition need to have in order for this training process to work well. So that's one category of tests, the second category of tests is like, in fact take humans. And we can, even without ML, just probe whether human decompositions have the kind of structure we want. Like, if you replace an RL system with a human who's actually just trying to receive a high score on some game to find in terms of other humans, like, what did the equilibria of that game seem to be? We can sort of have this more adversarial game where we probe, like, are the equilibria actually good? Or can someone sort of show that there's like, bad equilibria or strategies that aren't. Yeah, so those are like two kinds of empirical tests we can do now. And then maybe the third kind of thing is just theory, understanding such decompositions and, like, conceptually, what properties would human reasoning have to have? There's then a question of, like, on what timescale do those come together such that ML systems are actually able to imitate the kinds of human reasoning or answer the kinds of questions that plausibly have this structure, like, to plausibly admit this kind of decomposition.
00:35:54.964 - 00:36:46.326, Speaker B: And I think I just like, I would like to be in the place where you can test that. We can test that as quickly or as frequently as reasonable, so we see whether it's true. And when it's true, I think it's at least a little bit off. I think there are simple domains in which you can maybe test this, even with humans, where there are domains like, if you imagine some simple estimation problem where a human has been asked, like, estimate how many cars would fit, like if you paved all of Chicago or something, then you can ask that human, if they're going to solve that, would like, break it into pieces, break them into smaller pieces, eventually go to Google to look something up, and you can say, can we use that kind of decomposition already currently to train machine learning systems to answer that kind of estimation problem. And that's the kind of thing we can test now. And sort of it's on that spectrum between toy and real application, where the structure of the domain is simple enough. You may not need to use humans in this way to solve it, but you do get to learn some non trivial decomposition from humans.
00:36:46.326 - 00:36:48.274, Speaker B: You get to see more of this system at work.
00:36:52.814 - 00:37:21.018, Speaker C: I'm just wondering, in software engineering and automation, there are a lot of examples of what you're talking about decomposing higher level tasks in the smaller tasks and so on. And people try to build them so they are atomic, they are repeatable, so they have certain objectives you could just look into that stuff and see how use that is input, because there is quite a lot of familiar harvesting out there.
00:37:21.146 - 00:37:50.566, Speaker B: Yeah. So I guess there's two kinds of things in that space that seem interesting to me. One is there are lessons from the architecture of software, which is just largely about this game of how do I break some complex task into pieces which can be composed in some way that is easier to specify than the whole program. I do think that's part of my optimism about the project. There's a lot. A lot of what we know about decomposability comes from people wanting to decompose things for one reason or another. I do think there's a challenge there where in some sense the workability of the proposal depends on.
00:37:50.566 - 00:37:57.474, Speaker B: Can we apply this in domains that haven't been amenable to automation historically? Are there hard domains where this breaks down?
00:37:59.614 - 00:38:08.376, Speaker C: Tasting software is a card because you have to be really creative. Breaking a system that doesn't exist. Yeah, it's an interesting intellectual challenge. So maybe.
00:38:08.520 - 00:38:40.684, Speaker B: Yeah, I definitely think we can find these hard domains where it's interesting, where they're like, close enough that it's plausible you can apply the same ideas, but not obvious that you can just automate it, and then we can ask in those domains. I think there's a lot to learn from the cases in which people in fact work on decomposition. Sort of a similar set of lessons in the context of actual organizations which have to do decomposition. Another place that you could actually hope to get sort of easier versions of this problem that don't involve humans from domains that have this kind of structure already. And software engineering is a potential source of like on that spectrum from toy problems, something we actually care about. Software engineering can be moved a little bit past toy towards more like stuff we actually care about long run. So I think about both those.
00:38:40.684 - 00:39:26.164, Speaker B: Okay, I'm going to talk a little bit for the rest of the talk. I don't know if I should wrap up at three or 45 minutes after starting. Yeah, more or less about five minutes sounds good. I'm going to talk a little bit about sort of these toy domains, asking in some sense. The simplest part of this question was, if we have this kind of decomposition structure, can we use it to train models? Sort of the easiest to investigate empirically. As I mentioned before, this is sort of a similar situation to alpha Zero, where we're just going to invoke the current model a whole lot of times in order to generate training data. In Alphazero we use tree search here we're going to use some different kind of decomposition.
00:39:26.164 - 00:40:44.792, Speaker B: You might wonder, like does that strategy work only because tree search is kind of optimizing the reward or like quality in the game as a monovariant? Or does this just work in general? For similar reasons, I guess I would be reasonably optimistic coming in, and I think Alphazero provides good evidence that it's going likely to work. But it is. I mean, doing RL is a little bit different than other tasks, and it is nice when you have an invariant that's going up over the course of time, which is not the case in this decomposition. So in part, just to get experience implementing this kind of thing and to see what difficulties came up, last year I worked on some simple prototypes, systems trained in this way. So if we take some algorithmic tasks which just very naturally have this decomposition structure, can we train a system to solve them using this kind of approach? So the idea is we fixed some simple domains with natural decompositions, for example, like given a permutation to compute large powers given some series of assignments, evaluate variables given some function, sum it up, overall inputs matching a certain wildcard pattern. And these are all tasks which a neural net could be trained to do if I had input output examples. So if I give you a permutation and a power, and I say collect a dataset of 100 million of those, then a transformer can learn to do any of these tasks.
00:40:44.792 - 00:41:33.564, Speaker B: But I could ask if I don't have input output examples, and instead I'm just handed this decomposition. Maybe I have some oracle that implements this decomposition, but can I use that to train a neural net roughly as efficiently to solve this kind of task. We're not going to assume we have any efficient algorithm for solving these problems. And in some cases iterating this algorithm doesn't lead to a sufficiently efficient algorithm, like iterating the given decomposition and constructing the whole tree doesn't lead to a sufficiently efficient algorithm. For example, in this problem you want to use dynamic programming. If you want to get an efficient algorithm, you don't just want to actually construct the whole tree. Um, can we learn about as well? So if we have this data set again, this is like a really simple proxy for these cases we ultimately care about where we think maybe humans are able to decompose a task into easier parts, but can't solve it directly.
00:41:33.564 - 00:42:13.164, Speaker B: Just gets like one tiny aspect of the difficulty here. So what the system ends up looking like is we have some humans, or in this case some oracle, which we're going to count how many times we call, which implements our decomposition, we hand them some problem that we're interested in solving. Like given this function, what's like the sum over some large space of inputs. The human then interacts with the current model in effort to solve that problem. For example, the human might say, if I want to compute the sum, I would first cut the domain in half and send each half to one of my models, one copy of my model. So we dump questions into the human. We ask them to try and solve the problem by interacting with the current model.
00:42:13.164 - 00:42:42.316, Speaker B: We then take transcripts of those interactions and send them to another model, which is going to try and predict how a human would try and decompose the problem. We then take that predictor of a human. We send it to workers. These workers now get handed questions. They try and copy what the human would do in order to decompose the problem and interact with the current model. In order to produce a big data set of questions and answers, we send that to some optimizer that tries to train the model to imitate. Now we're not doing rl, we're just doing supervised learning.
00:42:42.316 - 00:43:29.404, Speaker B: Train the model to imitate what would have been produced by this kind of decomposition. And then we send that model both back to the humans so that they can interact with the updated model, and to the workers so that they can produce better answers, roughly what the flow of data and parameters looks like, like this model. Again, the tasks we worked on were very toy on these tasks. So I guess I should also say the distribution of questions which the human answers depends on the sub questions that occur in the course of this decomposition, there's not a fixed distribution you need to be able to answer. So, in the same way that in order to play go, you need to know which states are going to occur in this game tree. In order to answer these questions, you need to know which questions are going to occur in these decompositions. So on these admittedly varied toy tasks, you don't have like big ML difficulties introduced by this recursive process.
00:43:29.404 - 00:44:16.244, Speaker B: You train like roughly as fast if you use these decompositions as if you have ground truth waves. It's really just isolating one small part of a way things could have gone wrong. Why did you gain experience with any practical implementation? So the next steps there, I think, in two directions. One is making the toy domains more complex, stressing more of the ways in which this kind of decomposition might fail, especially working with much larger sets of possible questions, so that non stationary of that distribution becomes a real problem. And then another direction is just to actually start exhibiting decompositions in interesting domains. So a lot of we're working on right now is working with language models that can kind of start to produce the kind of reasoning a human would engage in when they decompose a task. And trying to move from these domains that look more toy to domains that look a little bit more like actual interesting question answering.
00:44:16.244 - 00:44:43.944, Speaker B: That's a large engineering project. Cool. So I'm going to stop there. Maybe I'll skip real quick to some overview so we can start at that. Yeah, so we want some normative assumption that relates utilities to observations. I don't want to use humans as a gold standard. Coping with expensive reward functions seems like a powerful building block since it lets us start ignoring the complexity of the reward function we construct and just focusing on what's the positive normal assumption.
00:44:43.944 - 00:45:00.464, Speaker B: And then decomposition is one plausible strategy for relaxing this assumption of human optimality. But there are lots of open questions both about does this kind of thing ever work? Does human reasoning in fact have this kind of structure which we'd need, and is this suitable as the training paradigm for ML systems? Thanks.
00:45:04.724 - 00:45:41.084, Speaker A: Okay, we have time for one or two questions. Yeah, I have question should we not be worried about the immediate, by stitching together the answers of the decomposed esters?
00:45:42.584 - 00:45:54.644, Speaker B: I do think you're going to get non unique answers in general probably, or like maybe there's a single optimum of this reward function, but like there will be something where like stochasticity in the sub answers you get will affect the answers you get at the top level. This sort of relates.
00:45:59.704 - 00:46:14.564, Speaker A: Some sort of complexity. So if you are ending up with a solution which is very different because you are getting ways of combining, then I didn't see it being addressed.
00:46:15.784 - 00:46:52.534, Speaker B: I think my main take on this is that it's an empirical, there's an empirical question of whether humans can in fact. So if you take a complex question, it's much more efficient for a human to solve that question holistically often than to engage in this kind of decomposition. There's an open empirical question of like is that merely a matter of efficiency, or is it actually an essential part of how a human can produce a good answer? Or is there a way that a human can recognize good answers with this kind of decomposition? Mostly I think at this point that's an empirical question about human reasoning on these sort of complex or open ended tasks. And the main thing I'm excited about for making headway on that is just running a bunch of experiments with humans in order to start assessing, to identify kinds of questions for which the decomposition yields poor answers or bad evaluations.
00:46:54.284 - 00:46:58.164, Speaker A: Any other questions? Okay, let's thank the speaker again.
