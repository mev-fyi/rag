00:00:00.520 - 00:00:57.194, Speaker A: Barbara Lisco. She is a professor at MIT, where she is the head of the programming methodology group doing research on BFT and distributed computing. She is the first woman to win a PhD in computer science in the US. And I'm going to go through a list, a very small sampler of distinctions, awards and things that I can say about her because I really want to hand the floor to her. But believe me, the list is very long. So, in terms of awards, in 2004 she won the von Neumann Medal for fundamental contributions to programming languages, programming methodology, and distributed systems. Then in 2008 she won the Turing award for her contributions to practical and theoretical foundations of programming languages and system design, especially related to data abstraction, fault tolerance, and distributed computing.
00:00:57.194 - 00:01:21.886, Speaker A: Normally, after the touring award, people don't bother. Continue. But in her case, in 2012, she was inducted to the National Inventors hall of Fame. And this continues. A list of memberships and honors. She's the member of the National Academy of Engineers, the National Academy of Sciences. She's a fellow of the American Academy of Arts and Sciences and a fellow of the ACM.
00:01:21.886 - 00:01:50.434, Speaker A: And she has seven, several, which I won't count honorary doctorates, mostly from european universities, that I wouldn't be able to pronounce the names anyway. A bit of lineage. She was advised at Stanford, I believe by Professor McCarthy, himself a touring award winner in 71. Her thesis was, anybody knows?
00:01:50.814 - 00:01:53.254, Speaker B: Yes, chess.
00:01:53.334 - 00:02:35.346, Speaker A: Yes, I probably have to play chess. And also in terms of lineage, she has some, you know, one or more advisees in the auditorium here today. Except for those advisees, does anybody know who was advised by her here in this room? Yeah. Yeah. So I think that we could do a very simple mathematical extrapolation from McCarthy, Turing, Barbara, you know, it took, let's see, 27 years, I think, between the Turing awards. So we can extrapolate that. Okay.
00:02:35.346 - 00:02:37.934, Speaker A: And with that, I'm very pleased to have you. Please.
00:02:44.554 - 00:02:47.374, Speaker B: People are complaining that the video is not working.
00:02:48.754 - 00:02:54.854, Speaker A: Can you say that to me? See, I solved the problem already.
00:02:56.794 - 00:03:13.816, Speaker B: Okay. So I want to make sure everybody can hear me. Yes. Okay. So in a way, I'm here under false pretenses because I don't really work on blockchains. And I'm here because of the work I did on practical byzantine fault tolerance. And what I'm going to do today.
00:03:13.816 - 00:03:56.908, Speaker B: Oops. That was not a good idea. First I'm going to talk a little bit about how practical byzantine fault tolerance came about, and then I'll talk about some recent work I've been doing with Luba Shrira and Morris Herlihy on cross chain deals, and I'll end up with some thoughts about replication. Okay, so I need to go back in time to around 1997. And at that point my group was working on a. We were very involved in research on distributed computing, and it was broadly interpreted because it included work on programming languages. Anything in distributed computing was kind of fair game.
00:03:56.908 - 00:04:25.744, Speaker B: The group was focused on a client server object store, some of the stuff that came out of the group at that time, just to mention a couple things. This is distributed information flow control. Andrew Myers. That's had a lot of impact in the systems community. Distributed garbage collection. That was Umesh Mashwari, a very sophisticated, lovely, simple algorithm for managing garbage over a distributed system. I had a very talented group of graduate students, and one of them was Miguel Castro.
00:04:25.744 - 00:05:22.734, Speaker B: And Miguel was looking for a thesis topic in around 1997, I don't remember exactly when. And I suggested to him that he go and look at the DARPA RFPs, the request for proposal. And what had happened was that the Internet was now out in the public and bad behavior was becoming a very important issue. And so DARPA had an RFP asking for ideas about how we could manage some of the problems that were happening on the Internet. And so Miguel saw that RFP and he suggested to me that maybe we should work on byzantine fault tolerant replication. And I put the word practical there in red because, of course, there was theoretical work on byzantine fault tolerant replication already in existence. But we were concerned about how do you come up with a system, a way of doing it, a protocol that you could actually use in practice.
00:05:22.734 - 00:06:09.626, Speaker B: Now, one of the things I was thinking about as I was getting this talk together was what was it that caused Miguel to come up with this topic? I mean, first of all, if you think about graduate students, they don't all come up with their own topics. But Miguel came up with this by himself. And I think the reason he was able to do this was because of the history of the group. Okay, so now I've gone back to the early 1980s when I first started working in distributed computing. And one of the things that was disturbing me at that time, and this is maybe around 19, 84, 85, distributed remote file systems had come into existence by then. And I was concerned about the following problem. When you have the files on your own machine, you know if your machine's up, your files are there.
00:06:09.626 - 00:06:56.614, Speaker B: If your machine's down, you can't get at them, but if it's up, you can get them once you have a remote file system. In those days, there were two reasons why you couldn't get your files. If your local machine was up, you still wouldn't be able to get them if there was a problem with the Internet or that machine where your files were stored was down. And so I started to think about what to do about this, and it was clear that we needed replication, and people were thinking about this. But it's not an easy problem because you have to have a system. It's got to be a replicated state machine, and it has to work properly in the presence of concurrency and even more important, in the presence of failures. And it's critically important that when somebody logs in tomorrow after failure, what they see tomorrow is consistent with what happened today.
00:06:56.614 - 00:07:22.984, Speaker B: And there was work on the time, but it was mostly thinking about having the user control things. It was kind of like a locking solution, and that wasn't working very well. So I started to work on this problem. I was working primarily with one of my students, Brian Okey, and we came up with a strategy how to do this. So those are the issues. And I said this already. Okay, so, onward.
00:07:22.984 - 00:08:00.924, Speaker B: The strategy they came up with was fundamentally what we're all using for replication today in this sort of train of research, we decided one of the replicas would be acting as the primary. It would tell the backups what to do. They would do exactly what it said. But of course they had. And of course the primary would wait long enough to know that the backups knew what was going on before letting the client know what was the answer. But the backups were meanwhile watching the primary because the primary might fail. And in that case, they would carry out a second algorithm, a view change algorithm.
00:08:00.924 - 00:08:29.462, Speaker B: Simple. You know, actually, this stuff seems trivial to me now, but, you know, it was hard to come up with then. And this is basically, by the way, Paxos. It's just that I didn't understand that. Leslie didn't understand that. And I actually saw Leslie give a talk at MIT at about the time we were writing a paper on this, and he was dressed in a toga. And, you know, it was very mysterious, and I had no idea he was talking about the same thing.
00:08:29.462 - 00:08:44.550, Speaker B: Okay. And one point I wanted to make out, this is crash failures. So this is not byzantine failures. So it's a simple failure model. Nodes are up or down. Network either delivers good messages or recognizably bad ones, or it doesn't deliver them. Simple model.
00:08:44.550 - 00:09:16.766, Speaker B: Okay? And the replicas maintain logs, which, of course, are fundamentally immutable ledgers. Okay? So I think the reason Miguel thought one reason why Miguel thought this up was because, unlike almost everybody in the country, my group knew about this kind of replication. It was sort of in our DNA. There's the papers that we published. So, you see, this is late 1988, another paper. The first one was about replication in the presence of a database. The second was about a file system.
00:09:16.766 - 00:09:46.434, Speaker B: These are not great papers, by the way. I mean, that paper is not as readable as it could have been. And the reason is that we didn't fully separate the algorithm from the application that sat behind it. So, you know, partly it's Leslie's fault for not writing anything until much later, and partly it's my fault for not going back and rewriting this stuff at the time. I've subsequently rewritten it. And, of course, many other people have done similar things. Okay, so the point is, my group knew about this work, so Miguel knew about this work.
00:09:46.434 - 00:10:13.824, Speaker B: And so for us, we had this advantage that other groups really didn't have. We had this sitting there. We knew how this system worked. So it was an easier job for us to take that next step. So Miguel and I started to work on this problem. DARPA was happy with the proposal, and we got some money and we started to work on this project. Okay, so this is now around 1997, when Miguel came up with this idea, and we used exactly the same strategy.
00:10:13.824 - 00:10:38.072, Speaker B: So, the primary runs the protocol. It orders the requests. The backups are going to watch the primary, and they'll do a view change if they don't like what they see. Okay. They maintain logs, just what we were doing before. But, of course, there's a huge difference here because once you acknowledge that, you are talking about byzantine failures, where the replicas can lie. The network can be malicious.
00:10:38.072 - 00:11:10.308, Speaker B: You know, everything changes. So just because the primary said do it doesn't mean that you should obey it, because the primary might be bad. So you have to handle all those corner cases. It took us quite a while to figure out how to do it, but the grounding in the earlier system, pre stamp replication, was a big help. Now, you all know this stuff, and this stuff existed before we started this work. But one of the things that happens in the system is that you need more replicas. So for simple failure resilience, you need two f plus one.
00:11:10.308 - 00:11:42.296, Speaker B: Here, you need three f plus one. Because if you look at this picture, what you see is that we're running some operation, and we can never wait for that last replica to reply because, of course, it might be down, or maybe it never got the message. In this case, it didn't get the message. So we have to be able to go forward with one of those replicas not responding. This is the f equal one case. But of course, as is the case here, the replica that didn't respond is actually an honest replica. And one of the replicas that responded is the bad replica.
00:11:42.296 - 00:12:04.820, Speaker B: And now imagine that there's a view change. Okay, so now there's a view change and we lost an honest replica. But because our quorums are big enough, what we know is that we have an honest replica in there somewhere that knows what happened. We have that liar, and then we have this guy that's clueless over here. Now, notice there's another problem here. So we need these bigger quorums. But there's another problem here.
00:12:04.820 - 00:12:51.536, Speaker B: We have the guy who actually knows what went on and we have the liar. How do we distinguish between them? When we go on to the view change, how do we know which one's telling the truth? So everything in here is done with cryptography. Everything has to be done with certificates that prove that this is what the state really was. So I'm not going to bore you with this discussion of how this worked, but basically what happened is you can see PBFT as an extension of the viewstamp replication protocol, where the quorums were more. There was an extra phase in the protocol, what we called the pre prepare. This had to do with the fact that the primary might be lying and so you couldn't act on what the primary said. You had to, first of all, get sort of, the group to agree that that was the right next step.
00:12:51.536 - 00:13:35.576, Speaker B: So there was an extra phase in the protocol, and we used certificates to handle the case that replicas could lie. So how do you know what the truth is and view changes, as I was discussing a moment ago, and we maintained a log and we had blocks because this is expensive. It's not hugely expensive, but it's expensive. And you want to amortize the cost of running the protocol across many client requests. Okay, a couple of other things I just want to mention here, before I move on is that the log we were thinking of was not something that the replicas understood. It was a set of client requests. It knew what clients it was talking to.
00:13:35.576 - 00:14:18.254, Speaker B: So it knew this was the 25th client request from that client. But it did not know the content of the messages and it did not try to execute this stuff. I thought of this log, and I still think this is what's going on in blockchains, really, is that they're just a set of calls on operations of an underlying application. And this is the system sitting in front of the application, in our thinking, that ordered things in advance so the application which was replicated, could be operating correctly. Okay. And by the way, Miguel had a kind of a proof that this worked in his thesis, because it was, it's a complicated algorithm, even though it seems simple compared to stuff that's happened in the future. And there's been other work in the systems community really working on the proof of correctness of this stuff.
00:14:18.254 - 00:15:11.758, Speaker B: Okay, so I'm, there's the paper, and I'm going to go on now and talk about a different topic. Okay, so what I'm going to talk about next is some work I've been doing with Morris and Luba on cross chain deals. So if, in fact, blockchains turn out to be a thing that is really used in practice, we have to imagine that there's going to be a lot of them and there's going to be computations that are going to use more than one of them. And so how do you make those computations run? And where am I? The problem is what I see on my screen. Oh, I'm sorry, I left something out. Before I get there, let me just say something about this. So, yes, I wanted to mention.
00:15:11.758 - 00:15:38.760, Speaker B: So here it is, 2000. Miguel has finished his thesis. We both think about PBFT. We say, gee, will this ever be used? You know, I don't know. Maybe it'll be used for a key distribution center. Mind you, the viewstamp replication stuff was finally starting to be used in 2012, years after it was invented. So, you know, you expect a kind of a lag time like this between research and what happens out in the real world.
00:15:38.760 - 00:16:06.364, Speaker B: But, you know, we didn't think it'd be used very much. We kind of thought, you know, what will f be? Well, you know, maybe it'd be two or three. We weren't sure. And then we both went on and did other stuff. Okay. And then, you know, along in 2009, the paper on bitcoin appeared, and it wanted a persistent, immutable ledger. And this is basically the killer app for PBFT.
00:16:06.364 - 00:16:48.204, Speaker B: So, not that I paid any attention to this whatsoever, I was completely ignorant, but it was just kind of amusing that this happened. Okay, so now we're today, and I'm going to talk about some recent work, and as I said, this is work that I did with Luba and Morris, and I'm going to talk about just a piece of what we talk about in that paper. Luba has other things. I mean, Morris is going to talk about other stuff. Okay, so what's a cross chain deal? Here's an example. Bob wants to sell two tickets for 100 coins. Alice is a broker, and she's happy to buy those tickets for 100 coins and sell them to Carol for 101 coins.
00:16:48.204 - 00:17:22.914, Speaker B: So the idea is Bob sells the tickets, Carol gets the coins. I mean, Bob sells the tickets and gets the coins. Carol gets the tickets, and Alice gets a little fee for being the broker. Okay, so we're living in a world where the blockchains manage assets. So there's a ticket blockchain, a coin blockchain. The parties exchange the assets online, and we want a protocol that's going to make all of this work. Okay? Now, it's kind of like an atomic transaction.
00:17:22.914 - 00:17:46.290, Speaker B: And Morse is going to talk more about this this afternoon. I mean, you have this idea that the deal is either going to go through or it isn't. But it's a weird environment. I mean, in a way, when I was working on PBFT, I felt like I had entered a hall of mirrors. You know, you had to think so strangely about the fact that everybody could be corrupt. Actually, only f. Could be corrupt here.
00:17:46.290 - 00:18:15.870, Speaker B: Everybody can be corrupt, and they may have agendas that you know nothing about. Maybe the agenda is just to have the other by lose his assets and you don't care if you gain anything. It's a very corrupt world and they might collude, and there's no limit on how many of them can be bad. So there could be 20 of them and 19 get together and try to cheat the 20th. So you have to handle all those cases. Now, clearly, there are certain acceptable outcomes. So here's an acceptable outcome.
00:18:15.870 - 00:18:44.462, Speaker B: Carol gets the tickets, Bob gets the coins, Alice gets the fee. Okay. Another acceptable outfit is that Bob keeps the tickets, Carol keeps the coins, Alice gets nothing. Okay, you can define what this means, but that's sort of an acceptable outcome. That's like a commit the first one, like an abort the second one. Okay, here are obviously unacceptable outcomes. Carol gets the ticket, but Bob doesn't get his money.
00:18:44.462 - 00:19:07.446, Speaker B: That clearly doesn't seem acceptable. Another one is the assets are tied up forever. And the current work on what are called cross chain swaps. I'm going to show you what happens with that. So there's been work on this issue. How do I run these deals? It's been limited. It doesn't handle full deals.
00:19:07.446 - 00:19:31.856, Speaker B: It handles what are called swaps. And the idea of a swap is that take Alice out of the picture Bob is offering his tickets. Kara's going to give him the coin. So the two of them are involved in a direct exchange of assets. And the current protocols that exist for this are fully decentralized and they assume synchronous communication. Okay. And here's just what's going on.
00:19:31.856 - 00:19:54.598, Speaker B: I'm not going to describe these protocols in any detail, but the idea is that you start off by putting the assets into escrow. This is like locks in a transaction system. You put the asset into escrow. This means that it's locked up. It's there for a particular party if the thing should go through. Otherwise it's going to revert to the owner if something goes wrong. But it's locked up.
00:19:54.598 - 00:20:22.064, Speaker B: Okay? And then the parties are working on this protocol and they put their assets into escrow, actually in a particular order in these particular protocols. And then when things are kind of okay for somebody, they claim their incoming asset by saying commit. And the commit, since these are malicious, has to have some kind of proof that it's okay to release that asset. So. Yeah.
00:20:23.484 - 00:20:25.252, Speaker A: What is dah in t status?
00:20:25.308 - 00:20:37.424, Speaker B: Oh, sorry about that. So d is the deal. Okay. A is the asset. So that's identifying a particular ticket or a certain amount of coins. H is I'll get to in a minute. And t is also something.
00:20:37.424 - 00:21:18.738, Speaker B: That's a time. Okay, so the proof, the commit DP, what p is, it reveals the secret that h is hiding. So the simple way that one of these deals works is Bob puts his tickets into escrow with a hash of a secret he invented. Alice sees this, and she puts her coins into escrow, which with that same hash, when Bob sees the coins, he can do the commit, revealing the secret. And then when Alice sees that commit, she can take that secret and get her tickets. So that's the way this protocol works. Okay? But remember, these are malicious parties.
00:21:18.738 - 00:21:56.078, Speaker B: And maybe Carol's secret agenda is she's not going to give the coins to Bob, but she just wants him to lose his tickets. Okay? And there isn't any way of saying abort here. Instead, what happens is the commit must get there in time, so that t is a time by which the commit must arrive. If it doesn't arrive by then, the asset returns to whoever provided it. Okay? And this is in a timed model, a synchronous communication model. So the idea is you can select such a time. Now, you're going to be careful about selecting that time.
00:21:56.078 - 00:22:25.610, Speaker B: You know, it's going to be pretty far in the future. Ok? But it's absolutely true that this is a possible outcome. So I put in red the fact that, in fact, Bob might get the money, but Carol doesn't get her tickets. Bob releases his secret, but Carol doesn't get the commit there in time, and so she doesn't get her tickets. Okay. Carol's the one at risk in this particular example. But we solved the problem of assets are turned up, tied up forever.
00:22:25.610 - 00:23:09.788, Speaker B: And, you know, there's nothing surprising going on here. I mean, after all, we know the network is really asynchronous, right? And in an asynchronous world, you can't have both safety and liveness. And what's happened here is we've chosen liveness over safety, okay? And although you can choose a time in the future, and maybe it's pretty likely that everything will be okay, still there could be a denial of service attack and that would cause somebody to not get there in time. So it's a real issue. And to me, coming as I do, out of consistency and transactions and stuff like that, it's disgusting. I really don't like it. Okay, so I was asking the question.
00:23:09.788 - 00:23:38.120, Speaker B: I started to work with Lubin Morris, can we do better? And the answer is yes. But we have to have some shared resource that everybody is willing to trust. Okay? That other protocol didn't have such a shared resource. I mean, it didn't have something that was going to crack at the state of the deal the way. I'll show you in a minute. So we're going to have a shared resource. What in the systems community we talked about for years is a whiteboard.
00:23:38.120 - 00:23:46.912, Speaker B: We had this idea. There was this thing sitting out there. It was an application. You could be writing on it. Everybody could look at it. They could sort of see what the street story was. We're going to actually use.
00:23:46.912 - 00:24:10.244, Speaker B: It doesn't have to be a blockchain. It could be a database. But we're going to use a blockchain here. And this is what we call the certified blockchain. And the idea is that this blockchain is going to be used to record the status of the deal. So the status of the deal is going to be. Doesn't exist yet, but if it exists, it's either active or it's committed or it's aborted.
00:24:10.244 - 00:24:47.462, Speaker B: Okay? So now I'm going to just run through the protocol. The way this thing starts is that some party to the deal? And by the way, this is general protocol that handles more than swaps. It can handle that example I gave you earlier with Alice and another thing I want to say is that in the paper that we wrote about this stuff, we have another protocol that handles general deals in a synchronous model. So we sort of. And that's a kind of. That's a nice protocol, but I'm not going to talk about that. Okay.
00:24:47.462 - 00:25:16.018, Speaker B: So the first thing happens is somebody puts start deal. Starts the deal up on the CBC. And so there I'm showing you that an entry has showed up on the CBC blockchain saying, we're starting deal D. And D is a description of the deal. Okay? The parties are watching this, and then as soon as they see it and it says the right thing, they can all escrow their assets. Okay? So it's the same old escrow. There's the deal description.
00:25:16.018 - 00:25:31.298, Speaker B: A is the asset. There's some other stuff there I'm not going to. I'll come back to in a bit. Okay. After. And this is very different from the previous protocol in the sense that there's no order of things anymore. All parties can escrow their assets immediately.
00:25:31.298 - 00:25:58.918, Speaker B: The reason for this is that this protocol allows you to abort, whereas in the other protocol, the order of things, in some sense, when you put your asset on the chain, you were already sort of committing yourself to the deal and there wasn't much of a way to back out. But here you can put your stuff up on the blockchains with confidence. Okay? And then after stuff goes up on the blockchain. So you're watching. All right. First you want to escrow your assets. Then you want to do whatever transfers are required.
00:25:58.918 - 00:27:00.654, Speaker B: So after the escrow goes up, the transfer goes up. Actually, I believe these probably could happen in parallel, but that's a programming issue. I mean, in other words, you don't really have to wait for an asset to be escrowed before you put the transfer on the blockchain because they could be executed in the reverse order. I mean, we're used to doing that kind of programming. What's showing up on each blockchain is, in the simple cases, first an escrow, then a transfer, maybe another transfer, until each asset is in a state where what the escrow says is, should this transaction commit, this particular asset will go to party, such and such. So in our example, the ticket will be up there, and the escrow will say, it's going to go to Carol, and the coins will be up there, and it will say 100 are going to Bob and one is going to Alice. So you look at these, and so you're watching your incoming assets after you took care of your outgoing assets.
00:27:00.654 - 00:27:59.594, Speaker B: And when you see that one of your income, your incoming asset, is exactly what you want, then you can commit, and the commit goes over on the CBC. And actually, if you think about what I'm doing here, it's like two phase commit turned upside down because rather than having an outside coordinator that's telling everybody what to do, we had this passive blockchain that's being used as a whiteboard where people are writing down what they want to have happen to the deal. So if you're happy with what's coming to you and you like the state of the deal, then you can say commit on the CBC. And each party does this independently. And if they all do it, then the transaction is going to commit. And if somebody aborts before they all commit, then the thing will abort. And if you say commit and then things are taking too long, you can say abort later.
00:27:59.594 - 00:28:08.322, Speaker B: And what matters is the order in which these things show up on the blockchain that determines the status of the deal. Yes.
00:28:08.498 - 00:28:11.682, Speaker C: Why does the start deal need to be written to the blockchain?
00:28:11.858 - 00:28:13.574, Speaker B: Why does it need to be there?
00:28:13.954 - 00:28:17.002, Speaker C: Can't you just omit that first message?
00:28:17.098 - 00:28:38.634, Speaker B: Well, it has a lot of information in it. It says it has a, first of all, it has a nonce in it, so we can be sure there's no replay attacks. It also has a list of participants. It might have other stuff. I don't know. I mean, I never thought about that question, whether you could get rid of the start deal. What's interesting is the start deal comes from somewhere.
00:28:38.634 - 00:29:05.580, Speaker B: There might be many of them because these are malicious parties. So it's the first start deal that matters. And parties, when they see that start deal, if they don't like it, they don't have to go ahead and obey the protocol. One of the things that's nice about this protocol is that everybody acts for themselves. Everybody decides what they like and what they don't like. Even if they are malicious, they can't hurt the others. Okay.
00:29:05.580 - 00:29:41.584, Speaker B: So I don't know. I mean, it's an interesting question. So, in fact, I mean, I just wanted to say it's the order of the votes that determines the outcome. It's flexible in the sense that you could commit an abort later if you don't like what's going on, parties better be careful because when you say commit, you better make sure that what you're agreeing to is what you really wanted. By the way, what you're agreeing to isn't necessarily what you signed up for. So Alice might sign up for different tickets than she thought she was going to get, but she decides. Do I like it? Each person decides for themselves, but they better be careful that they know what they're signing up for.
00:29:41.584 - 00:30:02.958, Speaker B: And this means not just their incoming assets, but their outgoing assets. If you made a mistake in what you put into escrow and you commit, tough luck. Okay. And as I said, parties can commit, vote to a commit, and later abort. Okay, so really? Yes, there's still a little bit of.
00:30:03.006 - 00:30:24.286, Speaker A: Synchrony in the sense that there's a race. If, let's say, there are just two parties, one of them commits and the other one takes a little longer. So Bob and Carol. Bob commits, Carol takes a little longer, but finally she sends her commit. At that point, Bob decides to abort. So now there is a race between the abort and the committee.
00:30:24.350 - 00:30:27.630, Speaker B: There's definitely a race, but it's being resolved at the CBC.
00:30:27.742 - 00:30:42.302, Speaker A: So it's not a problem of consistency. But there is a synchrony assumption who gets there first. The medium has the authority to also essentially determine the outcome if there are relays in the network that are getting both of these requests.
00:30:42.318 - 00:30:53.534, Speaker B: Okay, you're absolutely right. But what doesn't happen? What can't happen? Okay, let me just get to that for a minute and just say.
00:30:55.314 - 00:30:56.282, Speaker A: Did I find it?
00:30:56.338 - 00:31:30.874, Speaker B: Maybe we're too far. Maybe I don't have it on, do I? I seem to have lost a slide. Okay, so I'm just going to go backward and point out, okay, what can't happen is this. Okay, it's true that the parties could get together to block the deal. So somebody could decide, I just want this deal to abort, no matter what this other guy wants. You're absolutely right about that. What you can't do, though, is steal the assets.
00:31:30.874 - 00:31:46.924, Speaker B: So basically, this is a protocol that comes down on the other side. It comes down on the side of safety as opposed to largeness. I mean, that's true, but that's not really my point. My point is you can't cheat people like Bob. Yeah.
00:31:47.744 - 00:32:13.228, Speaker C: This type of two phase commit like protocol, does it not have the disadvantage that if the exchange rate, let's say, of the two assets being exchanged, fluctuates, the second party to complete the trade has a free option so they can choose to either, like, abort or commit? Like if the exchange rate moved against the original price that they had agreed on.
00:32:13.276 - 00:32:32.174, Speaker B: I think that these protocols. Yes, it does. I mean, at the moment you decide to commit at that point, you're committed. Up to that point, you can change your mind, but you can't cheat somebody, you know. And that's the point. I mean, other than that, yes, you can always. Things fluctuate.
00:32:32.174 - 00:32:40.114, Speaker B: You can change your mind. I don't know. Maybe, Morris, I don't know if I picked up on this question properly, but.
00:32:40.814 - 00:32:51.150, Speaker D: I think he means a financial option. Like it's a derivative. So it's a call. You're given the option to buy at a certain price, but you don't have the obligation to buy.
00:32:51.262 - 00:33:16.846, Speaker B: That's right. But these deals are like that, so they don't guarantee that the deal will go through. What they guarantee is that either people get what they thought they wanted or the assets are not lost. Now, you can imagine adding more to this stuff, like, you know, there's a penalty for having been involved and so forth and so on. But I think this is in the nature of the beast. Yeah.
00:33:16.910 - 00:33:22.990, Speaker E: So if I send, commit and later on to abort, that abort is not guaranteed to happen.
00:33:23.182 - 00:33:40.054, Speaker B: No. It only will happen if it gets onto the CBC before all the commits show up. And that's very important because otherwise you would end up with a state where some people would commit and some would abort. Yeah. Do you have a question? Okay. Okay. Let me just go forward.
00:33:40.054 - 00:33:58.370, Speaker B: Okay. So there's just a little problem I haven't told you about yet. Okay. So in a way, this is very nice. It's like two phase commit. The truth is sitting there on the whiteboard. But the problem is somehow you have to get that information to the contracts.
00:33:58.370 - 00:34:23.901, Speaker B: And blockchains aren't allowed to read other blockchains, which is actually a good thing. And so they can't go and look at that blockchain and figure it out. Instead, the parties to the deal have to tell them. Okay. And the parties themselves are malicious. So if they could figure out a way to mislead the contract, then they could do it. So you have to be careful.
00:34:23.901 - 00:35:13.662, Speaker B: And so the way you do it is. I keep getting ahead of myself here, is that the commit has to contain a proof. Okay? And this proof is a clear description of the state of the CBC, just like we had a proof before. But there it was a sort of a simple idea of releasing a secret here. It's got to be the state of the CBC. And that's kind of ugly, right? I mean, it could be that you have to show the blockchain the entire CBC from the start deal, the first start deal on the blockchain. Up to what I call the definitive vote, which is either the last commit or the first abortion.
00:35:13.662 - 00:36:07.560, Speaker B: And that can be a lot of stuff. Okay? And that's not very desirable. But if we're using PBFT, then we can get by with the certificate. So if you think about the CBC being executed by practical byzantine fault tolerance in some form or other, then what you have is you have a bunch of replicas that, you know, their keys, they can produce certificates that can prove the case. And I want to just point out here that in the entire CBC thing, you can't omit pieces. You can't say, here's just a Merkle tree, and I don't have to have that block because there was nothing of interest in there because that would allow a dishonest party to skip over pieces of the deal. So it has to be a very complete piece picture of the deal.
00:36:07.560 - 00:36:45.986, Speaker B: If I go back to the way that PBFT worked, I could just ask them a question. I could say, what's the current status of the deal? And back would come a certificate signed by f plus one, which means that at least one honest one said, this is the answer telling me the status of the deal. And of course, another thing is we get finality, which is kind of nice. And that's another issue that shows up in the first one. It's not just up to the definitive vote. There has to be many more blocks before we can be sure that that definitive vote is really there. Okay, so, yes, you started with a.
00:36:46.010 - 00:37:02.592, Speaker A: World of many blockchains, not just one. It seems like the CBC, you know, the participants, the validators in the CBC, each one of them would need to understand or subscribe to every one of the blockchains that participated in.
00:37:02.608 - 00:37:22.044, Speaker B: No, no, the CBC is totally separate. It's just another blockchain. And you can imagine there are many cbcs. So one of the things that happens when you decide to do a deal, if you decide which CBC you're going to use, it's one of the parameters you can choose. And the CBC doesn't know anything about what's going on in these assets, these blockchains. It's just itself.
00:37:22.404 - 00:37:25.784, Speaker A: The replicas in the CDC decide whether they.
00:37:26.844 - 00:37:52.186, Speaker B: So they understand. They have a proof. They understand this is the state of the CBC. So they understand the state in practical, byzantine fault tolerance. We didn't store just modification commands. We also had queries. And what queries did is they didn't change the state, but they caused an answer to come back and the answer would be signed.
00:37:52.186 - 00:38:05.294, Speaker B: You would accept an answer as a client when you had enough agreeing answers. So you had a certificate. Yeah, but it may not be like that. Maybe you don't want those queries there, but you have another way of asking questions. Yeah.
00:38:06.474 - 00:38:08.794, Speaker A: In this design, the CPC does not.
00:38:08.834 - 00:38:51.114, Speaker B: Need to understand any other blockchain, but all the other blockchain for you. The CPC need to understand the CPC. Well, if they were having to do it in the first technique, then, yes, they would have to know the sort of the structure of that CBC. They'd have to understand. Even if you imagine this is all the CBC does, which might actually be a truth, you know, you might imagine, if the world is organized this way, that these cbcs are very specialized to just managing this kind of thing. Still, that means every blockchain would have to be reading the blocks of the CBC. So we have to understand, on the other hand, if we go for the practical, byzantine fault tolerance, all they have to know is who the signers are and what their public keys are.
00:38:51.114 - 00:39:14.136, Speaker B: And if you imagine you have reconfiguration going on with that CBC, then they'd have to know a little more. They'd have to see a chain, maybe, because they have to know who were the signers for the start deal, so they can get started. And then if there were, they'd have to find a chain if things had changed since the star deal was placed there. One more question.
00:39:14.240 - 00:39:28.968, Speaker F: So this does mean that every chain involved in a deal, in order to add a new CBC, for example, every chain that might want to use that CBC would have to learn about the new CBC and learn how to evaluate the set of appropriate validators.
00:39:29.096 - 00:39:44.130, Speaker B: It doesn't have to know the CBC. It would have to know the format of the CBC. So if we imagine all these cbcs, totally independent, are all running exactly the same set of operations, then you can imagine that different cbcs could be used.
00:39:44.242 - 00:39:48.674, Speaker F: The chain would need to know what set of keys consists of a valve.
00:39:48.714 - 00:40:11.784, Speaker B: What? Oh, you're talking about this down here. Ah, yes. Well, that actually is part of the. It's not part of the deal. I really didn't show it. But in those dot dot dots there on the escrow, what goes in there is the set of public keys for the replicas at that moment in time. So for the block that contains the start deal, we start off with those public keys.
00:40:11.784 - 00:40:54.588, Speaker B: So that's how it works. Of the current replicas that signed the block that contains the start deal. And from there, you can just march along. And one thing I'd like to say about this. So let me just sort of finish this up, right. It's a shared resource, unlike the previous protocols, which don't have any sharing, but it works in a synchronous network, and it falls on the side of safety rather than liveness. If there were an attack on the CBC that caused it forever not to be able to do something, then you'd be stuck.
00:40:54.588 - 00:41:26.038, Speaker B: But of course, if there was such an attack, you're going to be in pretty serious trouble anyway. You know, the big difference between synchronous and asynchronous is that with synchronous, you have to say a time and guarantee that stuff happens by then. With asynchronous, what you say is, it will happen. It's really eventually synchronous, but you don't have to predict exactly by when. And in truth, as long as things are working, there's not that much difference. But I like the fact that the things fail this way rather than the other. I also think, by the way, that this is a nice model for distributed databases.
00:41:26.038 - 00:41:49.810, Speaker B: If you think about transactions that span databases coming from different organizations, it seems nicer to have this passive approach to managing things than the active approach that people have looked at in the past where there's this external coordinator telling them all what to do. Okay, so I'm just going to finish up with just a few questions about fault tolerance. Yeah.
00:41:49.922 - 00:41:56.890, Speaker D: Why don't you say it's not live? I mean, if no one ever boards, if everyone just commits and communication is reliable at some point, it's.
00:41:57.002 - 00:42:06.706, Speaker B: I'm just talking about the problem that for some reason there's been some sort of denial of service on this attack on the CBC. So the CBC is not able to record these things.
00:42:06.850 - 00:42:07.282, Speaker D: Okay.
00:42:07.338 - 00:42:49.916, Speaker B: Okay. It's not lie. Lydness has nothing to do with the members of the deal because they can always time out and decide to abort. Okay, so I just want to talk about replication for a couple of minutes. Practical byzantine fault tolerance kept a log like a block on a blockchain. But it's different from the way blockchains were described in the bitcoin paper, because the idea there was that the replication algorithm didn't know anything about the content of the message, with a few exceptions, like you could ask it to give you some non deterministic value to use in your computation. So there were a few things that knew, but it didn't know much, and in particular didn't have to know the application.
00:42:49.916 - 00:43:18.164, Speaker B: This meant those requests could have been actually encrypted so that there's no privacy issues. And there's another thing that's even more important that I'll get to a bit later. So it's a different model of computation. It doesn't proclaim, include having things running at clients. It was really, though, a client server model, where the idea was the work happened at servers. Clients did queries and got the answer signed by a sufficient number of replicas. So you could believe that it was the truth.
00:43:18.164 - 00:44:06.490, Speaker B: Okay, my second question, really, you know, I'm not working in this area, so. And I've read some of the papers about replication, sort of based on practical business fault tolerance, but trying to go forward. What are the actual limits on how many requests per second can be managed by this replicated system? And I think that's an interesting question. I think it's interesting to ask the question, what applications do we know about and what are their requirements? I think it's also an interesting question to ask. How fast can this system run? And what I really want to point out is that I don't think that replication is the problem here. I mean, remember, we're working with blocks. We're amortizing the cost of the replication across a lot of requests.
00:44:06.490 - 00:44:33.574, Speaker B: Bitcoin is very misleading because that's an application that doesn't do anything. But in reality, applications do things, and when they do things, that's work. And you can't run this stuff faster than the application. It doesn't matter, because how fast the replication works, because the application has to do the work. So it is important to keep in mind that there's a limitation here on what's going on. You can't exceed it's. Can't exceed it.
00:44:33.574 - 00:45:29.252, Speaker B: And there is also an interesting question though. If I think about distributed. Think about databases running on a multi store, which is what I've been multicore, which I've been working on recently. The throughput is incredible on those systems, but they get to find the order in which they do transactions opportunistically. Here, the order is in some sense predefined, and that means it's going to slow you down. So an interesting question is, how far can you push this? Okay, my second question, or next question is, is sharding necessary? And I actually have to say that when I think about sharding, it sounds a lot like working databases, again, where, you know, if there's too many conflicts, you actually can't make progress. If the application really has a lot of work to do, and all this work is interconnected so that you can't sort of separate things out in a really natural way so they don't interact very often.
00:45:29.252 - 00:46:25.248, Speaker B: You can't make much progress. So maybe what we're talking about is just some sort of partitioning, which is the way you solve stuff in database systems. And then finally, I just want to say, what is f really? Okay, this is not really a race to the top. It isn't interesting to ask the question, how big can f be? And will the system still work? What is actually interesting is the question, how small can f be? And, you know, the fact that we're spending so much money on the power or wasting so much power to run bitcoin, I mean, we can't do things that way. We have to think in terms of what is practically realistic. So let me just conclude with a few, just to remind you about some of the stuff that Miguel and I talked about a long time ago. So in those days, we were thinking about a very small f, okay? And we were worried about that small f.
00:46:25.248 - 00:46:44.834, Speaker B: And so we pointed out a whole bunch of things you wanted to think about about who those f replicas were. The first thing was they better be distributed. They had to be in different data centers. They had to be geographically separated. They maybe belonged to different companies. Maybe they were even in different countries. So you really wanted to separate them.
00:46:44.834 - 00:47:29.644, Speaker B: Secondly, they better be dedicated computers. It's ridiculous to try and run these algorithms on the computers your employees are using on their desk because the biggest threat to systems is people falling for phishing attacks and spam attacks and all that other software they're running, you know, has bugs in it and just makes the whole thing collapse. So you really want them to be dedicated computers and you want them to be in a protected environment. Even if they were dedicated computers, if they were just sitting out there in the middle of the room, that means, you know, hundreds of people have access to them. If one of them has been corrupted, okay, this is the insider attack. You know, then you're toast. You want a protected environment where very few employees are able to get in there to actually see the hardware.
00:47:29.644 - 00:48:03.308, Speaker B: And furthermore, you better keep their secret keys hidden. And you probably got to shield that place. You know, you don't want to have snooping equipment that can find out the secret keys. Because if a secret key is ever exposed, then there it is out in the world. And then I think you can go back and start to rewrite history because you can start to have a fork. You know, it's really, you don't have finality those secret keys really matter, even if you're doing reconfiguration. You know, if some guy who was a replica last month knew its secret key, that can really cause damage.
00:48:03.308 - 00:48:41.284, Speaker B: So Miguel and I put that into a secure coprocessor. And of course, there's been a lot of work on how to shield what that computation in the secure coprocessor was doing. But the idea was that software didn't ever see the key. All it could do was ask for signing. And that's really a good structure. And then of course, there's the program correctness problem, because no matter how careful you are, if there's a bug in your code, then there's a possibility that all your replicas will become bad simultaneously. But honestly, program verification has come a long way from where it was in 2000.
00:48:41.284 - 00:49:13.424, Speaker B: And I think that if you do some of the stuff we're talking about and you think about people. TBFT by itself is not that complicated, really. The operating system could be a stripped down operating system because it's not a general purpose computer, it's just for this function. There's an interesting question about the application. Another point about this is that if you separate the ordering of the log from the application, then you're proving the correctness of something much smaller. Of course, that doesn't mean the application doesn't have to work. So you have that problem.
00:49:13.424 - 00:49:58.484, Speaker B: And then finally, I just want to remind you that we talked about something called proactive recovery. And the idea there was, of course, all you require in PBFT for safety is that no more than f failures happen simultaneously. And so we were concerned about replicas are getting compromised, but if we constantly reboot them from a secure co processor that has the sort of the, the pure system state, then we'll make a very small window during which they could all be compromised. So hardware has come quite a way since then. This paper was written almost 20 years ago. But I just wanted to remind you of all these questions, because I think they all are relevant to this question of how can we keep the size of f reasonable.
00:50:13.804 - 00:50:15.384, Speaker A: More than we asked for it.
00:50:17.124 - 00:50:18.904, Speaker B: Ok, this question's back there.
00:50:19.364 - 00:50:21.784, Speaker E: Can we answer to your questions also?
00:50:22.364 - 00:50:23.264, Speaker B: Oh, sure.
00:50:24.284 - 00:51:12.434, Speaker E: The question that you had about the application latency, what the operation execution cost, that's an important one. So certainly if you look at cryptographic protocols that people dream of, where easily you spend a millisecond or two in a signature verification, and sometimes people will dream of that they can still have 10,000 of transactions per second throughput. Yeah, but the partial answer, and the answer I wanted to give is that in Hyperledger fabric, which was involved in the design, we have an answer to this question where we separate the execution of smart contracts from the critical part of the, the ordering, because execution takes place in a speculative way, like in a replicated database, before the ordering is, before the agreement on the order is reached. So that opens a path to scaling.
00:51:12.774 - 00:51:47.388, Speaker B: Yes, I'm aware of work like that, and I think Morris also did work like that. The sort of idea that you speculate first and so, and you can retain information about the order and use it later on and so forth. I mean, I think this is all good stuff, but I do think there's a fundamental issue of how much work you have to do to run the application, and that that's going to be a deciding factor in how much throughput you can get. But of course you're going to try to make it run as fast as possible. And as I said, in databases, it's amazing what the throughput is. And this is not really trivial stuff. You know, these are actual transactions that do work.
00:51:47.388 - 00:51:55.794, Speaker B: So it's not like all I did was move a little money from here to there. So I think it's interesting area. Yeah.
00:51:57.134 - 00:52:04.758, Speaker E: Does the CBC blockchain have to be a separate blockchain or it can also. Or can it also be an asset blockchain?
00:52:04.886 - 00:52:19.470, Speaker B: It could be one of the blockchains they're already using. It just has to be a blockchain they agree on. But the real difference is they have to all agree they're going to use that blockchain and that blockchain has the truth on it about the state of the deal. Sure.
00:52:19.622 - 00:52:21.754, Speaker E: So it was more for.
00:52:26.374 - 00:52:28.214, Speaker B: Okay, yeah.
00:52:28.374 - 00:53:01.082, Speaker F: Regarding the speculative execution idea, I think a lot of times when building systems and databases, there are a lot of techniques for optimization that sort of break down in an adversarial environment where the participants can make sure that there's always an in opportune ordering or set of transactions or set of operations that will. For example, speculative execution can be slower than regular execution if you end up having to undo things a lot. And so I'm wondering if you've thought about that, about which techniques work in an adversarial environment and which ones don't.
00:53:01.138 - 00:53:07.694, Speaker B: I haven't thought about that at all. I think it's an interesting question. I haven't thought about that yet.
00:53:07.994 - 00:53:33.012, Speaker C: I have a question. In the CBC model, how do you solve the deal matching problem? So basically, how does Bob know which ticket is cheapest and which ticket is expensive. So Bob want to buy the cheapest ticket. And actually this relates to another problem is the produce of the front running attack in the steel mansion.
00:53:33.108 - 00:53:42.836, Speaker B: Okay, so I really couldn't hear your question. It was something about Bob and tickets, but I did not pick up on. Did anybody, can somebody, for example, Carlo.
00:53:42.900 - 00:53:53.924, Speaker C: And another guy, David, both sell tickets and one is sell 100 coin and another sell is about 110 coins. So basically Bob wants to buy.
00:53:53.964 - 00:54:15.894, Speaker B: You talking sort of like an auction, right. I rely on my friends here who are more knowledgeable about blockchains than I am. But my understanding is that you have to put a little bit of skin in the game in order to make an auction work. Right. So there has to be some penalty for backing out, but otherwise this can't handle auctions.
00:54:16.634 - 00:54:26.618, Speaker C: I think he's not mentioning backing out, but I'm about to propose a certain price and somebody sees me broadcast and before it's committed to the blockchain. You mentioned front running to get in.
00:54:26.666 - 00:54:47.144, Speaker B: Ah, yes. Well, yeah. Is there anything different here than what I just talk? I mean, what I'm saying, is there any difference between this kind of protocol and the protocols that use, you know, once the stuff is out there, people can see it. It's an advantage for stuff being secret. Yeah, right.
00:54:47.484 - 00:54:56.812, Speaker C: I mean, I guess there's protocols where you have to commit to everything you're doing first to prevent front running, you add an extra stage. Basically you use the hiding commitment so that it's not.
00:54:56.868 - 00:55:01.564, Speaker B: So as I say, I refer to my. They will take that offline.
00:55:03.784 - 00:55:19.688, Speaker D: So this cross chain swaps, in language of the nineties, it's like communicating replicated state machines. And I wonder whether this was a question back then or whether back then it was just a client who talked to replicated state machine or whether there's some old word on this issue.
00:55:19.776 - 00:55:49.058, Speaker B: Well, so I was there then and my recollection is nobody would have dreamed of doing such a thing. You know, we knew the world was asynchronous. We wanted systems that worked in an asynchronous model. And so we never thought about, can I run transactions in asynchronous model? And, you know, tough. It's really. Yeah. What was interesting was going from two phase commit to this was took quite a bit of time because it sort of turns upside down.
00:55:49.058 - 00:55:53.054, Speaker B: And the fact that everybody can lie adds a little sort of dimension to it.
00:55:54.414 - 00:56:04.022, Speaker D: If Kevin and Bob don't agree on the same CBC, could two CBC's communicate to play as if there are one CBC?
00:56:04.198 - 00:56:15.874, Speaker B: I don't know. I haven't thought about that. So. Good question. You know, we just came up with this recently, and.
00:56:18.974 - 00:56:19.954, Speaker F: I don't know.
00:56:20.454 - 00:56:24.714, Speaker B: I mean, clearly there's issues here, like, who do you trust? And so forth.
00:56:24.794 - 00:56:33.098, Speaker D: I trust my CBC. And somebody else has to communicate and play once.
00:56:33.146 - 00:56:35.234, Speaker B: I mean, in Virginia, for her thing.
00:56:35.274 - 00:56:38.746, Speaker A: To work, it's not even clear. I mean, you need to add something to it.
00:56:38.810 - 00:56:40.330, Speaker B: You have to have some way.
00:56:40.522 - 00:56:42.106, Speaker D: But they should be able to.
00:56:42.250 - 00:56:46.314, Speaker B: I have. I suspect not. Morris, I'm looking at you.
00:56:46.394 - 00:56:47.674, Speaker C: I wouldn't do it that way.
00:56:47.834 - 00:57:03.262, Speaker B: I wouldn't do it that way. If Morris says, I suspect not, I think you have to have. We have to agree always on. Not the whole world, just each group that wanted to do the deal would have to agree on a particular CPC.
00:57:03.358 - 00:57:08.194, Speaker D: China, and somebody in the US trying to make a deal. They will have to agree on the same CPC.
00:57:08.494 - 00:57:13.234, Speaker E: It's kind of a fair exchange, right? You need to trust third party to solve it.
00:57:13.614 - 00:57:15.246, Speaker D: Yeah, but I have my office and yours.
00:57:15.270 - 00:57:18.424, Speaker E: Your office, which is the same thing as fairness.
00:57:18.544 - 00:57:20.200, Speaker C: But you can abort. There's no fairness.
00:57:20.272 - 00:57:20.432, Speaker B: No.
00:57:20.448 - 00:57:23.560, Speaker E: If you abort, abort, depending on what you're getting.
00:57:23.592 - 00:57:23.736, Speaker C: Right?
00:57:23.760 - 00:57:25.404, Speaker E: So there's no. You're not forced to.
00:57:25.744 - 00:57:28.624, Speaker C: Even after everything in excess revealed, you.
00:57:28.624 - 00:57:29.744, Speaker E: Can say, no, I'm not going through.
00:57:29.784 - 00:57:31.244, Speaker C: So there's no fairness requirement.
00:57:32.104 - 00:57:45.184, Speaker E: Yeah, but in the fair sense, you have two parties that want to exchange assets, right? And either both get what they want or not, which is not happening here. It is. If one aborts, then of course, whatever. That's the whole requirement.
00:57:46.004 - 00:58:07.208, Speaker B: Okay, so I believe you need a single CPC. We sort of prove that you need some sort of single shared resource in order to have a protocol like this work. And it has to do with the old Fisher lynch paper and how, you know, in a distributed system, you can't control without time. You can't order things in the same way at multiple sites.
00:58:07.356 - 00:58:10.936, Speaker D: Delay, because you combine it together. But you could add to the.
00:58:11.040 - 00:58:17.368, Speaker B: Okay, it's a challenge. I'll take it offline. And there's a question over there.
00:58:17.496 - 00:59:23.314, Speaker C: So, earlier you mentioned how, like, there can be, like, observability, basically, between two ledgers. How if I have a ledger and I want to inspect the state of the other ledger, it's impossible in this situation, due to. From my view, at least, due to the lack of some objective metric to verify that the ledger is at a specific state. And hence why you do. Why you have to do this two phase commit if you're trying to. My question is like, if you're trying to observe the state of a proof of work like chain, so not the PDF chain, couldn't you use the work itself to verify that the transaction, so that the first like transaction in your swap has happened? So I would send them the money to some address and then the other party like has enough proof that the moment I have sent the money to the address they have enough proof to just get the assets that I had locked in an earlier stage. So this allows you to do it like in one less like transaction while also removing the need for the.
00:59:23.734 - 00:59:31.098, Speaker B: Well, the, the tricky part here is that when you remove the money, how do you make sure that the other guy gets what you're giving in exchange?
00:59:31.226 - 00:59:47.346, Speaker C: What you're using here is the work itself. So that's why I'm saying that in a PDF or some mechanism where it's not expensive to create this attack, this wouldn't work. In proof of work change, perhaps this would be.
00:59:47.530 - 00:59:55.076, Speaker E: But you have to know that you're not in the fork. It's the main thing. So how do you communicate that to the other ledger that doesn't know that much.
00:59:55.260 - 01:00:01.396, Speaker C: Right. So you have to be relaying the current difficulty and you can always calculate the current difficulty.
01:00:01.540 - 01:00:08.904, Speaker E: If I'm feeding you a fork all the time, it has left difficulty and cutting your network of the main chain, you're lost. It's proof of work. Asynchronous.
01:00:10.444 - 01:00:11.744, Speaker B: All right. Yes.
01:00:13.724 - 01:00:36.044, Speaker A: On your BFT model, I wonder if in the blockchain age we might want to add two bullet points. One is secure hardware for enforcing, guaranteeing. The other one is economical incentives. Economical rationality. It guarantees that you're incentivized to behave correctly.
01:00:37.944 - 01:00:56.342, Speaker B: I think that something like the CBC, if implemented in reality, you would want. I mean, I was talking about some of this stuff. Secure hardware. Yes. And incentives. Yes. I mean, when you start to think about who would be providing the replicas that make up this, you know, I wouldn't want them all at one company.
01:00:56.342 - 01:01:18.174, Speaker B: If I was doing something that involved multiple companies, I might be happy to have them all in the United States. If I was doing something that was within the United States. It's much harder to know what to do when it's China in the United States. I agree. There's a lot of lack of trust there, but yes, I agree. And I think coming up with a set of requirements would be a good thing. So I.
01:01:23.714 - 01:02:12.824, Speaker A: Any more questions? I have one more. Going back to where you started before. Use temperature application and paxos. You know, it's a fascinating story of similar ideas, similar techniques being developed in parallel, even different formalisms. I guess there were a lot of papers written about the nuanced differences. Second or third order, the community was working towards the same solutions. I wonder if you could finish with some comments about this process of collaborative scientific innovation that advances the state of art.
01:02:13.884 - 01:03:09.710, Speaker B: Well, yes, I was just going to, in the spirit of. Let's just finish with something. What I'll tell you about is that when I invented data abstraction way back in 1973, that idea was floating in the air and other people were thinking of similar things. You look at a paper written by Jim Morris at about that time, you can kind of see data abstraction lurking in there, even though he didn't pick it out. So it's absolutely true that in a community, a research community, people who are thinking about similar issues are sort of coming up with similar ideas, and it's just the way it is. I just happen to be lucky that I came up with that idea and wrote a paper and got that paper out, you know, with that idea in it before anybody else managed to pin it down. But it was not true that, I mean, if I hadn't done it, somebody else would have done it.
01:03:09.710 - 01:03:59.802, Speaker B: And so this is not exactly an answer to your question, but it's sort of the way, you know, things work. What was a little odd about the story with, I mean, what really happened with Ustamp replication and Paxos is, is kind of a, it's a commentary on both me and Leslie. I mean, what Leslie says now is, I came up with the name she actually implemented, you know, but, you know, it's actually, he kept expressing it in terms of proofs, and that made it difficult for people to follow it. I did it in terms of, here's a protocol you can follow. But what I didn't do was I didn't pull that out and separate the protocol from the application that was sitting behind it. And if I had done that, then I think people would have understood a lot better. And actually, when I look back on my career, I see other things I didn't do.
01:03:59.802 - 01:04:44.294, Speaker B: And, you know, you're always making decisions when you're doing research about, do I do this or do I do that? And I had a tendency to say, I did that, I'm going to go on to something else. And that is what you want to do, because you don't want to sit there spending three years just kind of cleaning up little details. But on the other stand. I invented, my group invented parametric polymorphism for clue, and it's there in the reference manual, and it's there in the paper that describes clue. But we should have written a paper about parametric polymorphism and how you implement it and so forth. And that might have been a real help to the programming language community where it took until Haskell. Haskell came out with its.
01:04:44.294 - 01:05:04.484, Speaker B: I forget what they're called. But anyway, the things that they used to describe what we called where clauses, that's 30 years. I mean, that's ridiculous. So there's a fine line here between moving forward and spending a little more time, and I think I sometimes move forward a little too fast. Anyway, thank you very much.
