00:00:00.600 - 00:00:59.776, Speaker A: Yeah, so tensor. So what I'm trying to tell you today is about sort of a crash course on tensor networks, which is something that I guess, let me tell you, start with the goal that I have for people hearing this talk by the end of the talk, and the goal is, one, to have a sense of tensor networks, but two, to have a real genuine interest or excitement in taking this sort of point of view and applying it to whatever problem you are thinking, thinking about. And at least the point of view of sort of thinking about quantum computation in terms of tensor networks has been pretty much the chief influence in terms of how I think about things. And I'm a fan, and I think it's been pretty helpful. And so I'm trying to convert other disciples in this realm. So just to get a sense how many people are solid on what tensor networks are, raise your hand. Good.
00:00:59.776 - 00:01:57.984, Speaker A: All right, so this will be a crash course from beginnings. All right, so just to give you a sense of what it is, it's a geometric way of thinking about operators, tensor networks are, and it's sort of very helpful for seeing a lot of things in sort of the algebraic world of manipulations of operators. And so kind of, let me just give you a sort of outline of where I'm going to go. I'm going to start with really from scratch. So the basics defining tensors and tensor networks, then I'm going to sort of talk about the connection between tensor evaluating tensor networks. And. Well, I guess that it's a BQP complete problem, so that it really captures what quantum computation captures.
00:01:57.984 - 00:02:43.756, Speaker A: Let's see. Then I'm going to sort of talk about simulating them can, and that sort of will touch upon matrix product states and peps and miras, which are objects used for actual simulation. And all these things I'll be sort of talking about from the theoretical point of view. And I think Daniel will be talking a little bit more about how these things are used in practice. And let's see. I think that's about it. Oh, no.
00:02:43.756 - 00:03:46.250, Speaker A: Maybe if I have time, I'll run through the proof, very simple proof of the commuting, sorry, the area law for commuting Hamiltonians in any dimension, sort of. And that's any image. And so we've heard about area laws being, you know, something that we've established in 1D. It's a mystery for higher dimensions, but for commuting ones, it's simple. And this is one way to see it. Quite a sort of ends up being a picture. And that's it okay? So starting from very, very basics.
00:03:46.250 - 00:04:53.974, Speaker A: All right, let me tell you how I think of a tensor. And so this may be different from how other people do, but it's equivalent. So I think of a tensor as a vertex with a bunch of edges coming out of it, some constant number of edges or bounded by a constant. So in this case it's five, and then it's got a, it's, let's call it m. Okay? And we also have an idea of a set of labels for edges. In this case, we're going to just think of the set of labels as being b labels, zero through d minus one. Okay? And so now let me tell you what a tensor is.
00:04:53.974 - 00:05:52.034, Speaker A: A tensor is an object like this, such that if I assign a label to each of the edges, possibly a different one, possibly the same, but every time I assign a label, it spits out a number. Okay? So for instance, if I label this one and this two, and this also may be two and this five and this three, then it spits out some number, let's say 2.7. And if I change the labels, it spits out some other number. So one way to say what this is is this is an array of numbers. How many are there? Well, there's d, possible labels of each of these things. So they're d to the fifth in this case. Any questions so far? Okay, so that's the definition of a tensor.
00:05:52.034 - 00:06:46.764, Speaker A: So now there are interpretations of the tensor. How do I think about them? And so let me just, just for notation sake, let me take h to be a b dimensional Hilbert space. And let's imagine a basis for that Halbert's face as being indexed by these guys. So as we sort of standard do, we can think of, these will be an orthonormal basis for H. So instead of writing c to the d each time, I'm just going to write h now. So that's all. Okay? So I want to interpret this element, and the first way I'm going to interpret it is as a vector.
00:06:46.764 - 00:07:51.124, Speaker A: It's going to be a vector in tensor product of, in this case, five copies of H. And what vector is it going to be? Well, M is going to be sum over all possible labelings. So let's go l one through l, five of the value of the tensor. If I was to label the. So now let's just imagine that I've sort of assigned, this is the first leg, this is the second leg, this is the third leg, this is the fourth leg, this is the fifth leg. And so when I write m l one, l two, l three, l four, l five. What do I mean? I mean, take m, find l one to this one, l two to that one.
00:07:51.124 - 00:09:00.148, Speaker A: L three is five. Okay, so this is a number, right? And this is going to be the coefficient in front of. So this is, so for every picture like this, as long as I've told you what order to think about these edges, or for every order that I think about these edges, this corresponds to a vector, right, where I'm just pulling out, it's just encoding the coefficients of the vector. Okay, so that's one interpretation. So every time I have a picture, yeah, sorry, say that again. Why use the same label in this example? Just to make sure that you didn't think I wasn't allowed to use the same label. So this would be the coefficient in front of, if I was thinking about it this way in front of the thing.
00:09:00.148 - 00:09:52.964, Speaker A: 12235 oftentimes if I just draw one picture and I put all different labels, then people think you have to label them distinctly. They don't. Okay, so this is the first interpretation of the tensor, but there are others, and the others are in terms of linear maps. And it's not just one linear map, it's a bunch of linear maps. And let me give you an example of one of the linear maps. But the important thing is that the one of them, well, let me say that after we define the linear map. So in this case, back up here, and I'm going to single out, I'm going to divide the edges into two groups.
00:09:52.964 - 00:10:42.848, Speaker A: Is that blue and black? Yeah. Are they completely indistinguishable? Yeah. Pretty funny to think of me sort of switching pens to put different blacks up there. So again, this is just a sort of, I'm going to sort of divide the set of edges into two groups. They didn't have to be planar in their division. I could have, well, let me define what I, let me say what I'm going to do first and then I'll say all those extra things. But I'm going to think of this as a map, m a linear map.
00:10:42.848 - 00:11:09.924, Speaker A: From where to where? Let me tell you from where to where. From two copies of h to three copies of h. So h tensor h to h tensor h, answer H. Okay. And let me just sort of identify them, et cetera. And let me tell you how that map works. One way I can define a map is, say, how it acts on a basis.
00:11:09.924 - 00:12:02.014, Speaker A: So here's a basis for h one tensor h two consists of things of the form l one l two. So let me tell you how m acts on that. And it's going to be a sum over all possible labelings of the red guys. So l three, l four, l five. And so with all these sums, right, I'm not going to write it down, but l is cycling over all these possible labels. And what is it going to be? Well, there's going to be some component in the direction l three, l four, l five, which is a basis element for this space. And what's it going to be? It's going to be what you might expect it to be, which is m of what you get if you were to label m by all five of these things.
00:12:02.014 - 00:13:02.872, Speaker A: Any questions? Okay, so let me say a couple of comments about this. Right? So this is not the only such map there is, right? I could pick any two, map it to any three, or I could pick one to four, four to one, etcetera. Right? So, and I don't require them to be, this is what I said a moment ago. I could have taken the red ones to be this one, this one and that one and the two black ones to be those two. There's no geometry, there's no restriction for me to think about them in these different ways. Okay, so far so good. Okay, so this is just a language, right? I've got these objects again.
00:13:02.872 - 00:13:55.314, Speaker A: In summary, these objects are an array of numbers, but I have different ways of thinking about them, all in terms of elements in the Hilbert space. So either as a vector in a Hilbert space or as maps between two Hilbert spaces. Okay, so now let me tell you about two basic operations on tensors. And the first one is a product, a binary operation. So it's a way of taking two, two tensors and producing a third one. Okay, so here's the product. So I have one tensor m, and let's say I have another.
00:13:55.314 - 00:14:19.854, Speaker A: I'm going to define the product to be this picture. I have a tensor m and a tensor n, right? So an n now is, only has a different number of legs coming out of it. That's fine. And I'm going to define this picture to be the product. And I have to. So what does that mean? I'm going to have to tell you, this is going to be a new tensor. Let me tell you what tensor it's going to be.
00:14:19.854 - 00:15:10.954, Speaker A: So this is going to be a new tensor. And I guess I should say, let me sort of say one more thing. Let's say we'll call this a five tensor. And what does that mean? It means it has five free edges on it. Okay, so now I'm going to tell you what this product is, and what this product is going to be is it's going to be a seven tensor, eight tensor. Thank you. Let me actually change this drawing just a little bit.
00:15:10.954 - 00:15:43.454, Speaker A: Problems here. I'm going to move this edge up a little bit. Fun over a little bit. It's going to be an eight tensor. And what eight tensor is it going to be? Well, to define an eight tensor, I have to label these guys and tell you what number comes out. That's what I have to do. So let me label these guys.
00:15:43.454 - 00:16:56.998, Speaker A: L1, l two, l three, l four, l five, eight. And what it's going to be is it's going to be the product of two things. It's going to be the product of M with some of these labels and end with the rest of these labels. So it's going to be equal to M times n. Let me label them accordingly. L1, l two, l three, l six, l seven. Okay, so this is a number, right? It's a tensor with all its labels labeled.
00:16:56.998 - 00:17:33.517, Speaker A: This is a number with tensors. Just multiply those two numbers together. And so one way to sort of, if you're trying to sort of get a picture of what I've done is I basically sort of surrounded both of these guys like that and I've sort of said how many free edges are there coming out? Well, they're eight, right. And then I say for every labeling of the eight of those edges, that produces a number here and a number here and I multiply those two numbers together. So this is a composition. So if I have a five tensor and a seven tensor, I get a twelve tensor out of this. Right.
00:17:33.517 - 00:18:17.896, Speaker A: It's a product. Okay, so that's one operation. There's one more operation that I want to go through and that's called contraction. There's a second one contraction. Okay. And so again I'm going to take M and N, I'm going to draw. Okay, so these were the two inputs.
00:18:17.896 - 00:18:35.380, Speaker A: And now I'm going to draw a picture where one of the lines of M is connected to one of the lines of n. Okay. Alright. So this is a new picture. It takes as input two old pictures. Right. Which we understand.
00:18:35.380 - 00:19:23.106, Speaker A: So now I have to just tell you what this new picture is. Right? So first of all, it's gonna be a tensor on, it's gonna be a, however many free edges, there are 123-4566 free edges. It's going to be a six tensor. Let me just draw them so that you see the correspondence. And so now for this six tensor, let's say it's called p. I have to tell you, if I label everything, l one, l two, l three, l four, l five l six, if I label all those things out, should pop a number. So let me just tell you what number that is.
00:19:23.106 - 00:19:42.130, Speaker A: So I go back to this picture and I label them. L1, l two, l three l four. All the free edges. L five, l six. Ok. Now I have this one, which we'll say is an internal edge because it's connected on both sides. It's not labeled.
00:19:42.130 - 00:20:20.606, Speaker A: Right. Because there's no free edge to label it. It's not labeled. So I imagine that this one I can label different letter k for any choice of labeling of k. Now, both of the tensors are fully labeled, right? So for any choice of k, there will be a number associated to here and a number associated to here. So for each choice of k, I multiply those two numbers together and then I add over all possible choices of k. So let me write down the actual statement.
00:20:20.606 - 00:21:12.212, Speaker A: Right. So the actual value of this is going to be the sum over k of this picture. Of really of this picture. This is labeled by k and this is labeled by k. So it said more comfortably in sort of this notation, maybe it's the sum over k of m. L1, l two, l three, l 4k times n k l five, l six. Okay.
00:21:12.212 - 00:21:23.904, Speaker A: So if you're seeing this stuff for the first time, it's just a bunch of things, right. It doesn't, it's hard to get a little bit of a feel for it. So now we're going to spend a little time getting a feel for this stuff. But are there any questions so far?
00:21:24.494 - 00:21:34.754, Speaker B: Is it useful to think of this as a composition of two operations, you know, n something that takes two indices to one, and then, you know, M is taking one index to four.
00:21:35.054 - 00:22:04.794, Speaker A: Yes. Yes, it is. And we'll see that in a as we. Yes. So you've anticipated. Yes, it is. Any other questions? Okay, do they allow to tensor to the, to do the contraction on tensor with itself? Yes.
00:22:04.794 - 00:22:23.758, Speaker A: Okay. For sure. And in fact, I could have, I could have defined it that way, that a tensor contracting with its sort of joining up two of its own edges. Right. And then I could have constructed this by a combination of taking a product and then joining up two of its own nature. And let's see. And so I should stress.
00:22:23.758 - 00:22:52.304, Speaker A: Right. So though I've drawn this picture in the plane. Right. You know, I could have any two could, contraction can be between any two free edges. Okay, so let's get a feel. So before we get, and this is really sort of, it's kind of crucial to get a feel for these things. So I'm going to maybe start back over on the first things.
00:22:52.304 - 00:23:32.448, Speaker A: So, so this is the, this is the interactive part of things. So to keep you engaged if you know this. Okay, so actually, let me ask the question first. I want an interpretation of this picture. So I guess I should have said, sorry, I guess I should have said one other thing at this point. Ok. Once I understand product and contraction, I can start doing a lot.
00:23:32.448 - 00:24:14.296, Speaker A: I can start doing them in sequence. Right. In any order that I want. Right. So I can end up, I now have, if I draw a picture like this, let's say just drawing this so it doesn't have to be planar. If this is m and this is n and this is p, something like that. I could have built this out of these guys.
00:24:14.296 - 00:24:30.720, Speaker A: Right. Because what could I do? I could have thrown m, p and n down just on their own. M and n down. That would have caused their product. That's a new tensor. Tensor that, sorry, that's a new tensor. Take the product of that with p, that's still another tensor, then start contracting by joining whatever edges that I wanted.
00:24:30.720 - 00:25:28.410, Speaker A: Each time I get a new tensor, I can then contract again, do a new tensor, etcetera. And so this sort of together, this gets, allows me to draw any pictures like this, which are called tensor networks. But we'll come back to that, because without a feel for what these things are, it's sort of, it's hard to absorb this stuff. So, but I'm going to draw two different pictures here. Okay. So this is a picture of two, one tensors that are contracted together, and this is a picture of two, three tensors that have their lines joining them. And so the question is, what's a good interpretation of this? And if you know already, then let me give, the point is for people who don't know to think about this.
00:25:28.410 - 00:25:34.694, Speaker A: So let's give it a minute or two. And if you don't know, you have to think about it. You can't do something else.
00:25:39.274 - 00:25:42.414, Speaker B: The first looks like an inner product of two vectors.
00:26:14.454 - 00:26:42.734, Speaker A: Some sort of signal. If you think you have, you understand this. So that, I'm not waiting for nobody short of the, what's that short of? Really got the answer right. Without revealing any information. I'll give people another. This is going to continue to happen. So you might as well invest in thinking now.
00:26:42.734 - 00:27:42.774, Speaker A: Okay, how about somebody who didn't know the answer to this question before trying it out? I'm thinking of first picture, M and M are both single vectors, and you take the trace of m. N transpose the inner product, and then, but more generally, you can take. So for the second picture, it's also the trace of m prime, n prime transpose. And there was another word that you said just before in a product. So it's not actually the best way to think about this is as an inner product. So let's just see why that is. Right.
00:27:42.774 - 00:28:36.644, Speaker A: So m, let's write down what this thing is. Right. So just trace through the definitions. It's the sum over k of what? Of m with a label of k times n with a label of k. So let's interpret m in the one way that we can interpret it is as a vector. So as a vector, m looked like summation m sub k in the direction of k. Am I writing big enough? And n looked like summation n sub k, meaning the number associated with labeling it by k, direction of k.
00:28:36.644 - 00:29:09.516, Speaker A: And what is this picture? This picture is sum over k. Each time I label it, well, I get an nk times an nk. So that's not quite how I think about pair of products. It's like missing a complex tangent, David, a complex tangent. Okay, so great, you reminded me of what to say. For now, let's just interpret everything. Let's picture that we're talking about real numbers as these things go on.
00:29:09.516 - 00:29:39.166, Speaker A: Okay, so all the arrays are real. For complex, you have to fuss around a little bit with inner products, but not much, sorry. With complex conjugates, but not much. So if these were real vectors, this is the inner product that, ok. Yeah. Ok, so does everybody see that? Right, so the summation of mk nk, that's the inner product of this vector, with that vector, and that's this picture. And now you should be able to see or think through what this is.
00:29:39.166 - 00:30:25.254, Speaker A: So this is also an inner product. When I think of m as sitting on a vector on h tense, or h tense or h and n n prime and n prime. So whenever you see pictures like this, one way to interpret them is as inner products. All right, so that was example one. Here's example two. I'll give you a new picture to look at. These are two pictures.
00:30:25.254 - 00:30:49.414, Speaker A: So just concentrate on this one and then extrapolate to what it might mean to this one. But I'll give you a minute or so to try to understand a meaning for this. This is a new tensor constructed from the old ones. And we want to sort of have an understanding of what it is. Just like this was a new tensor, or this was a number constructed from two tensors, turns out to be the interpretation of the inner product. So let's see if we can figure that out. So I'll give people a minute to think about that.
00:30:49.414 - 00:31:52.854, Speaker A: Anybody who hasn't thought about this before have an idea. Matrix product. Matrix product. Okay. Yeah, tell me more. It's a matrix product. Okay, matrix product.
00:31:52.854 - 00:32:52.434, Speaker A: I'm just summing over one index. Okay, that sounds good. So two things were important. One is that I'm going to interpret these tensors as matrices, as linear maps, and then I'm going to interpret the answer as the matrix product of the two of them. So the first thing to say is that, you know, m has a bunch of interpretations, but one of the interpretations it has is a map from here to there. Right? So let's THInk about it as the map from here to there. And let's write m, I guess, ij to be, as one typically thinks of, you know, matrices, right? So Mij will map from I to j, and then I have an n down here.
00:32:52.434 - 00:33:31.034, Speaker A: I'm also going to think of as a map from here to here, nkl. Okay. And so now I have to interpret this picture, and if I have an I here and an l here. So I'm going to interpret this picture as a map from up here to down here. Let's see what map it is. Right. So to interpret it as a map, I'm going to tell you what the, if I label I and J L, what the number that comes out is.
00:33:31.034 - 00:34:12.826, Speaker A: And what the number that comes out is is it's a sum over all possible indexing here. And what do I put here? Well, I end up with Mik times whatever I would put here, which is NKL. And that should look familiar. Right? That's matrix multiplication. Okay, so stringing things together, and this one is, sorry, just to say this one, right, is the same thing where I interpret this tensor as a map from these guys to those strings, and this tensor as a map from these guys to those strings. And this is composition of those two maps, or matrix multiplication. Okay, great, good.
00:34:12.826 - 00:35:41.234, Speaker A: Well done. Okay, I have one more for you. This thoughts from somebody who hasn't thought about this before. Trace. Trace. Okay, so how am I going to think about this? I'm going to think of m as I did in this case, as a matrix. Right? And so if I have to interpret this picture, this is going to be the sum over all possible labeling all labels of this edge sum over k.
00:35:41.234 - 00:36:13.674, Speaker A: And now you see, when I labeled this k, that actually labels both sides of this thing by k. So it's going to be the sum over k of mkk. Hmm. So that's the sum of the diagonal elements of m trace and m prime. Again, if I see this picture, one way to interpret m prime is a map from these guys to those guys. And once I do that, then this picture becomes the trace of that map. I guess I should point out it's not the trace.
00:36:13.674 - 00:37:24.158, Speaker A: There are a bunch of other ways to think of m prime as a map from one place to another. It's not the trace of most of those is the trace of the ones if you were mapping it the other way. Okay, so, all right, so we sort of encoded a lot of sort of linear algebraic maneuvers that sort of as pictures. So now I want to say, what's the proof of trace of ab equals trace of ba? So, okay, so I don't know. This hearkens back to some years ago for whoever has seen this formula before, which I'm assuming most of you have. Okay, so you have to, I mean, if you actually wanted to write it out, you'd have to write out a bunch of sums and you'd have to reorganize things and do stuff. But let's just draw the picture for the trace of ab.
00:37:24.158 - 00:37:49.266, Speaker A: We just figured out what that was. Right? That's this picture. Right. What's the trace of ba? Well, that's this picture. Okay, now, for some reason, these pictures are the same, right? All right. I don't know. Let's start moving this guy down.
00:37:49.266 - 00:38:09.954, Speaker A: Okay. Move him from here down to here. Kind of looks like that. Now let's move him up to here. I haven't changed the value of this, right. I'm just stretching strings around. So move them over to here like this.
00:38:09.954 - 00:38:41.892, Speaker A: Now let's move them up like that. And finally we'll move them back over to here. So now we've got the trace of ba. We haven't changed the picture, right. There is a slight difference, because there are two different legs, and if you index them correctly, you switch the legs. No, I didn't, actually. That's why I put it upside down.
00:38:41.892 - 00:39:06.890, Speaker A: So the top of the bee always stayed with the top leg and the bottom of the bee always stayed with the bottom leg. So they got switched here, but they got switched again when they came back. I see, okay. Yeah, but that's an astute point, right? You have to keep track. B has an interpretation as a matrix, both going from here to here and going from here to here. We have to make sure we're going the same way. But we are, because you do it twice.
00:39:06.890 - 00:39:48.184, Speaker A: So by the time it was sitting over here, it was reversed, but then it came back around. These kinds of things pop up all the time. You're trying to prove something is equal to something. You write down the two pictures and you look at them and you're like, oh, my gosh, they're the same pictures. And this is the easiest version of it, but it happens a lot. And if you were to start to write down on a more complicated picture what it means to stretch and move things around without actually changing the picture, you end up having to switch between linear maps and various other things. And so the whole idea of this thing is that these pictures reveal structure that is not as easy to see otherwise.
00:39:48.184 - 00:40:29.124, Speaker A: And maybe the trace of ab, trace of ba is not a great example of it, but it happens, and you'll see it happening some more. All right, so, okay, so that's the warm up. So let's get down to sort of for those who are itching to get further. So tensor networks, right, is, I already said what they are, right. You sort of plop a bunch of tensors down and you join some of the edges up. And that's a, that's considered a network because it's sort of considered things. And I'll deal with open networks and close networks.
00:40:29.124 - 00:40:58.934, Speaker A: And open networks means you still have some free edges. It's a new tensor. You still have free edges, and closed means you have no free edges. Every edge is ends at a tensor on both sides. So closed networks represent numbers. Open networks represent still tensors. So in particular, these are numbers.
00:40:58.934 - 00:41:48.866, Speaker A: Oh, and I should have said. Sorry, one more thing that I should have said is related to what Ora was sort of saying is there's a, you know, m has, if I interpret it as a matrix, we can think of it as a map from here to here, or we can think of it as a map from here to here. Those are two different maps. And the relationship between those two maps is, if we're talking about real matrices, is there. Well, actually, in any way, we're talking about is there transpose? So flipping the order, if this is the input, and that's the output, that's the transpose of the other way around. Okay, so here's the deal with networks. You can define tensor networks, you can, you can write down tensor networks to calculate interesting things, maybe it's not so surprising.
00:41:48.866 - 00:43:20.506, Speaker A: But anyway, so let me do an example or an exercise to give you a minute to work. So write down a tensor network closed whose value is, let's say, the number of three colorings. Okay, so let me just make sure we know, actually, let me do, sorry, before I do that one, let me do the simpler one, which is the number of perfect matchings. Okay, so let me just tell you what perfect matchings are, right? So if I give you a graph that may be perfect matchings are you want to pair up, you want to pair up pairs of vertices. Basically what you want to do is completely disconnect this graph into segments. So a particular matching would be to match these two together. To match these two together.
00:43:20.506 - 00:44:19.834, Speaker A: To match, I guess you could match these two together and then you wouldn't do anything with the rest of them. That would be one example of a perfect matching. Okay, so let's try to design a network, that tensor network that counts the number of perfect matchings. So should I give, maybe I'll give you a minute to think about that, how you might do that, I guess perfect matches. So just matchings, right? What is it called? What is it called if it's not, I mean, I guess, I mean matchings, perfect matchings means including everything, right? Yeah, but count the total number of matchings.
00:44:22.094 - 00:44:23.526, Speaker B: Empty set is also one.
00:44:23.630 - 00:45:41.374, Speaker A: Empty set would be one, anything. So that, I'm having trouble concisely saying this, but yeah, each vertex has degree zero or one. Okay, any suggestions? Okay, so the condition that I want as described, right, is that h vertex, I want to be either is degree zero or degree one. So I'm going to stick a tensor out here at each vertex that only accepts labelings. So let me say my label set will be just two elements, zero and one. And I'll let, if I have a zero on an edge, that'll mean the edge is not there. And if I have a one on the edge, that'll mean the edge is there.
00:45:41.374 - 00:46:40.554, Speaker A: And so what do I want out of a tensor? I want it to only accept something that's consistent with the matching. And what would be consistent with the matching. So these are each of my tensors. What I want them to do is to be zero if more than one edge is labeled by one, and I want it to be one if either only one edge or no edges is labeled by one. So this tensor should take on the value zero if I label it by zeros and ones, so that there's more than one and it should be one if there's one or less. So I put that tensor at each of these things. And now if I ask you, this is a closed tensor network, it's a number.
00:46:40.554 - 00:47:09.624, Speaker A: If I ask you what that number is, well, the only time. So it's a sum over all possible. So maybe I should have said, I should have said this a little sooner. If I want to evaluate this tensor network, what am I thinking of doing is I'm summing over all possible labelings of the edges. And every time I have a full labeling of all the edges, I get a product of the numbers that pop out. Every tensor is now labeled. If this was a closed network, let's sort of close it up.
00:47:09.624 - 00:47:44.484, Speaker A: As soon as I've labeled all the edges, every of the tensors locally around them are labeled. So they each correspond to numbers. And I take the product of those numbers, and the value of this tensor network is the sum over all possible labelings of those products. So in this case, if I've done this, I'm going to take the sum over all possible labelings of the edges. Well, all possible labelings of the edges are all possible ways of either putting or removing an edge. And this is only assigning a one. If locally you're consistent with a matching.
00:47:44.484 - 00:48:36.344, Speaker A: And to check whether something's a matching, all you have to do is check locally at every point whether it's a matching. And so when I actually add this up, this is going to give me one for every matching that I have and zero for all the other things. So in the end, it's going to sum up a bunch of ones and it's going to count exactly the number of matchings. I could have done perfect matchings by insisting everything had one exact one edge. Yeah, except I ran into trouble because this didn't have any perfect matching. But, yeah, that's right. I could have done perfect matches matchings now, also, if I wanted to weight the perfect matchings by, if I wanted to do matchings and not perfect matchings and weight them by sort of heavier if you have more elements in them.
00:48:36.344 - 00:49:16.544, Speaker A: I could have, instead of made this a one, made it something like a two. Right. And that way the, the value of any particular labeling would be two to the number of pairs that I've actually included in there. So it would sort of heavier weight. This is just to give you a sense of the, and any problem for which, any problem for which your constraints are local, for satisfying the problem. So like this matching thing or three color coloring, you can design a network for where the individual tensors checks, whatever that local condition is, assigns a one, when that local condition is satisfied as zero otherwise. Right.
00:49:16.544 - 00:49:58.152, Speaker A: Does that make sense? So this is another way to sort of just give you a sense that these things are, they count important things. Okay, so, okay, so now let's go an intuition why there is an algorithm, there is an efficient algorithm for perfect matching. I mean, the fact that you have a tensor network doesn't mean that you have an algorithm. So is this a coincidence or. I don't know, I don't know. Actually. I don't know the answer.
00:49:58.152 - 00:50:45.264, Speaker A: It just says there's a counting problem related to it. You would have such that networks for Sharpie complete problems. Yeah. Ok. In practice, the way you'd evaluate this is just the same way as just checking every possible connections, like either the edge is there or not, and then evaluating. Right? And this is what that's doing automatically, really or not automatically, but it's a way of encoding that process. Okay, so let's go back to quantum related things.
00:50:45.264 - 00:51:40.144, Speaker A: And so let me start with the following. So here I've got a tensor and I'm going to interpret it as a linear map. And so one thing that all linear maps have is a singular value decomposition. So a singular value decomposition says any matrix m can be written as square. It ends up being a unitary times a diagonal times another unitary. There's a change of basis and then there's a diagonal and another change of basis. And on this diagonal sits the singular values.
00:51:40.144 - 00:53:20.042, Speaker A: So let's sort of write down that decomposition here. And it says, well, you can rewrite this picture as a unitary, make an open circle for a diagonal matri a diagonal tensor. And so how do I sort of, what is this picture? Sorry, what is this picture? How do I think about this picture? Right. So for every labeling I of an internal edge here, the diagonal aspect of this forces this Guy to also be I and spits out the diagonal element at I. And this is a unitary operation. So it will take I to some Ui up here, and this will take I to some vi down here. Okay, so this is just the singular value decomposition, but with tensors, right, you can, you can view tensors as linear maps, but there's also another interpretation which is as vectors.
00:53:20.042 - 00:54:17.234, Speaker A: Right? So what is the singular value decomposition, which is sort of decomposing this matrix? Right? What does it say about decomposing vectors? Because I could have just thought of this as a, just put it like that. And I think of it as a vector on H tensor h. Let's take this picture and write it down in terms of a vector. So I just kind of flip this down, and what I end up getting as a vector is there's a sum over I. There's this number di that comes out. And then there's this guy, which is ui. And then there's this guy which is ui.
00:54:17.234 - 00:55:02.894, Speaker A: This copy was sort of corresponding to this h. This copy corresponded to this h. And what is this? Somebody tell me what this is. We know this. If you're dealing with Schmidt, Schmidt decomposition, this is the Schmidt decomposition. So the Schmitt decomposition of a state is just the consequence of the singular value decomposition. It's just the difference of interpreting a state from a to b, a state with two, a bipartite state from an a and b as a linear map from a to b, and doing the singular value decomposition.
00:55:02.894 - 00:55:58.574, Speaker A: And this is what pops out. Okay, so that's sort of helpful because it's kind of demystifies this Schmidt decomposition. And then I guess I should say also, you know, whenever you have a unitary or a partial isometry and we're talking about real at this point, remember that if I take my unitary and then I take the flipped version of the unitary, just to write it down in pictures, u times u transpose, because we're talking about real, right? Is the identity. And what is the identity? The identity looks like this. Why does the identity look like this? Because I sum over all possible labelings of this guy. And so it comes out, I gets mapped to I. This is just a, this is the unitary relation.
00:55:58.574 - 00:56:46.574, Speaker A: And then I should, I guess I should say one more thing. What is, what is the, what does a bell state look like as a picture? So what does summation ii look like? Forget the normalization like this. So this is a tensor, right? It happens to not have a dot here because the dot is implicit. It just doesn't do anything. Right. And this tensor, if I want to interpret it as a vector, I sum over all labelings, right. There's only one place thing to label, which is I.
00:56:46.574 - 00:57:16.190, Speaker A: I sum over all I, and I get I, I. Do you have a standard direction for reading, of reading things? Yeah. No, I mean, that's sort of part of the, I don't. So, but that should lead to confusion at various times. I mean, somehow you have to, you have to say, right, in this case, we're thinking of it as a vector. So, and then you have to decide which copy comes first and things like that. Yeah.
00:57:16.190 - 00:57:30.474, Speaker A: As a map, it's the identity as a vector. It's the bell state. And that often that's actually what ends up happening. I mean, a lot of the stuff that happens with bell states is really just the fact that it's the identity map when thought of as a map.
00:57:30.774 - 00:57:50.974, Speaker B: So probably these are all, I mean, abbreviations for particular kinds of tensors, right? I mean, it doesn't quite fit into your original definition of a tensor or even your diagram of this vertex with three edges and that kind of stuff.
00:57:52.514 - 00:58:57.884, Speaker A: I think it's okay with the idea of a tensor network, right? So in a tensor network, all the free edges to understand the coefficient, if I want to interpret a tensor network as a vector, I label all the free edges and out pops a value. And in this case, if I label this by k, I guess you're right, I label this by l, it's going to be zero, unless k equals l. But I guess somehow, implicitly, everywhere along here is a little guy that says, hey, a string has to be labeled on both ends by the same. So how would you write, say, the singlet state, like zero one plus one six. So I'd have to put a little guy here that is on zero, one and 10 is equal to if it was labeled. Sorry. It wants to send this to one to one and the other two.
00:58:57.884 - 00:59:51.698, Speaker A: Sorry. And the other two to zero. Okay. Okay, so, okay, so it should come as no surprise at this point that I can model a quantum circuit as a tensor network. So let's do that. There's only one small thing to be done. So a quantum circuit, right, as a tensor network.
00:59:51.698 - 01:00:49.864, Speaker A: Well, let's just sort of say what it, a quantum circuit starts with things initiated in, let's say, zeros, and then it starts to apply little unitaries for a while, maybe like this, et cetera. And eventually, at some point, it measures, say, over here, that's what a quantum circuit does, right? And the output of the circuit is zero with some probability. And let's say if it was zero with some probability. Oops, sorry. I seem to have measured a place where there wasn't anything. And the, you know, and so, okay, so how do we think of this as a. I just wanted to go back to the state description real quick.
01:00:49.864 - 01:01:52.310, Speaker A: How do you deal with normalization? So you're pointing out that this actually isn't, this is this state, not really the Bell state, which would be normalized, is that what you're saying? So when you just drew the loop, right? Yeah, that's this state. So it's unnormalized as a Bell state. Yeah. So would that be useful in a tensor network, or would you have to be wary of the fact that you should definitely be aware of that fact, but in terms of what does that do to the whole problem? You can sort of keep that as side information because it just multiplies the whole, whatever outcome you come, it just scales by that factor. So it's a cup or a cap, and every time, which people have. Yeah, that's right. That's right, though, that kind of leads you at certain points to having to deal with things.
01:01:52.310 - 01:02:37.604, Speaker A: But, yeah, that's right. And that's been done, and it works out just fine. Any other questions? Okay, so let's write down the tensor network associated to this thing, right? And it's just about exactly the same thing as the circuit. Right? In terms of, I started with the all zero state, right? So I can write down, I can call something zero if it's sort of, this is, you know, here we're gonna have edges that are just labeled by, if this is qubits, we're just gonna have edges labeled by zero and one. And the zero tensor will be something that says, hey, if I see a zero, I spit out a one. If I see a one, I spit out a zero. So those are those tensors.
01:02:37.604 - 01:03:23.158, Speaker A: That's what happened at the beginning. And now for each unitary, I just, this unitary, I just stick in the tensor corresponding to the unitary, et cetera, until I get down to where I'm going. Right? So that part is fine. And so what do I have at this point? I've got all these crossings with these unitaries. So I've got, at this point, before I've done any measurement, what do I have? I have a big tensor network. How many free edges are there? Just these guys. Let's interpret this big tensor as a vector in a Hilbert space of the tensor product of these copies.
01:03:23.158 - 01:04:20.374, Speaker A: What vector do I have? I have exactly the vector that the circuit has. So now I have to measure. So what does measuring do? Well, measuring says I'm going to look at the first coordinate, and I'm going to measure the amount of the portion of the vector that has a zero in the first coordinate and the norm squared, and the vector that has a zero in the first coordinate and the norm squared that has a second. So first, let me project onto the part of the vector that just has a zero. So how do I do that? I just stick a zero right here, another zero tensor, because a zero tensor says, hey, look, the only guys I'm going to accept are the guys whose label, right here is zero. So now I've got just the part of the state I've got, not the whole state. I now have the projection onto zero.
01:04:20.374 - 01:04:48.584, Speaker A: That's what's encoded here. Let's make it a zero that sort of sends zero on the other side. So now I've got just the part that's sort of zero. And. But what does a computation do? It spits out zero with a probability, the probability being the norm squared of that thing. So I need to encode the norm squared of this vector. I've got a vector.
01:04:48.584 - 01:05:20.048, Speaker A: How do I encode the norm squared? I inner product it with itself. That's, that used to be on this board. That's that first picture. So what do I do? I take this picture, I flip it over, put it upside down, join up all the edges. That's what I do. Just pretend I did that. What is this a picture of? This is a picture of, this is a number.
01:05:20.048 - 01:06:23.586, Speaker A: What's this number? This is the probability of getting a zero. If I change these two to ones, then it's the probability of getting a one. So from circuits, I can write down a tensor network that tells me the probability of getting a zero or a one, and in fact, just getting, they're complementary. So I just would need to know what the probability of getting a zero is. So, okay, so what do I have? I've turned a quantum circuit into a tensor network whose value is the probability of measuring zero out of the quantum circuit. And so you might, so you might ask the opposite question, right? Which is, if I have a tensor network, can I think of it as a quantum circuit? Right? Can I evaluate it as though it was a quantum circuit? So here I took a circuit and I turned it into some sort of tensor network. Now, notice something special about this tensor network.
01:06:23.586 - 01:06:44.524, Speaker A: All the tensors had an interpretation, or most of the tensors as unitary operators, right? If I pick the edges in the appropriate order, whereas tensor networks in general don't have to have that property. Right. I can choose to put anything on a tensor.
01:06:52.604 - 01:06:53.924, Speaker B: You already showed, you can.
01:06:54.004 - 01:07:29.136, Speaker A: Encode sharp decomplete problems using tens of items. Okay, so let me address that first. Okay, so that was a good point. Right? So, however, let me just say, let me backtrack and say what is a quantum circuit? Does, the quantum circuit can't actually tell you what this probability is, right? You can only approximate what this probability is up to one over polynomial, right? You can run it polynomial number of times, see how many times you get a zero. You get an approximation for the probability of this thing coming out. So actually what a quantum circuit can do. So I can turn this into a tensor network.
01:07:29.136 - 01:08:20.184, Speaker A: Right? And the value of the tensor network is what the quantum circuit approximates up to one over polynomial. So there's some level of approximation that is going on here. And so I could ask the, so now I ask the other question, which is sort of, can I design a quantum circuit that approximates tensor network? And we go to Umesh's point, which is, no, you certainly can't, because I can design a tensor network that computes sharp p things, and we can't do that. So I'm going to put a meta, based on time, I'm going to put a meta theorem on the board, and then I'm going to take a five minute break after that.
01:08:20.924 - 01:08:27.584, Speaker B: I mean, you could build a polynomial circuit, it would just not be a polynomial size circuit.
01:08:31.364 - 01:09:51.033, Speaker A: Yeah, but if we want, sort of, if we're thinking about this correspondence. Yeah, no, I guess that's right. Yeah, but that's not the direction I'm going to go in. So here's the meta theorem, which you can look up the details of, right, which is that approximating the value of a tensor network, closed tensor network, to a additive approximation, delta is b QP complete. So it's equivalent to quantum computation. And I have to, all I have to tell you is what this additive approximation level is, and I won't tell you that, but I'll give you a hint at that, and we'll move on to matrix product states. So for any tensor network, I've drawn one.
01:09:51.033 - 01:10:24.034, Speaker A: I want to tell you the level of approximation. So here's a tensor network. This theorem says, hey, a quantum computer can compute the, can estimate the value of this up to some additive level of approximation. I need to tell you what that level of approximation is. And so the first thing that I do is I order the vertices. I assume that the tensor network has an ordering of the vertices. So let's just say it was 1234.
01:10:24.034 - 01:10:28.010, Speaker A: Thanks. And here's what I do.
01:10:28.162 - 01:10:38.826, Speaker B: Sorry, Zeph, shouldn't there be a condition on just the degree, I mean, what you call the, I don't know what you call it, but the number of indices the tensors have in your network.
01:10:38.930 - 01:11:16.162, Speaker A: Yeah, so let's assume they're bounded by some constants. Yeah, and by the way, I should have said, I picked it so that all the labels, the labeling sets are the same size. But you could say some edges can be labeled by five things, some labels by nine things. Fine, generalizes just fine. So let me tell you how I'm going to interpret each of these tensors as linear maps based on this ordering. And so what I do is I picture, this is called a bubbling. What I picture is a bubble that was going from the outside and swallows the vertices in the order that they were given.
01:11:16.162 - 01:11:57.184, Speaker A: So here comes this bubble, and then it comes and it swallows vertex one. That's what it does first. And this tells me in order, this tells me some input edges and some output edges. The input edges are the guys that have already been cut by the bubble beforehand, and the output edges are the ones that then get cut by the bubble afterwards. So in this case, there are no edges before and three edges afterwards. I interpret this, I haven't really sort of covered this, but it's, I can interpret this tensor as a map from one dimensional place to h tensor, h tensor h, just as a vector. It turns out to be equivalent to the norm of that vector, as if I thought of it as a vector.
01:11:57.184 - 01:12:37.024, Speaker A: But let me just continue for a second. If I now bubble the second one by bubbling two, in that case, the input edges were just this one, because just before I bubbled that vertex, this was the only edge around two that had been cut. So I think of it as a map from this guy to these three guys. So each of these. So now once I have a bubbling, I have an interpretation of each tensor as a map, as a linear map with specified input and output edges. And each of those maps has a norm associated to it, an operator norm. If it was unitary, the operator norm would be one, which is what happens in circuits.
01:12:37.024 - 01:13:54.844, Speaker A: But if it's non unitary, then the operator norm could be something else, and my delta ends up being the product of these norms over the bubbleg. And then my value of the tensor network, the approximation I get. So if the value is v and my approximation is v tilde that I get, it's that the size of this is smaller than delta over polynomial. So this is the level of additive approximation that you can get. So for a quantum circuit, if I bubble straight down, everything has norm one, and I get the standard approximation of up to delta that ends up being one, and I get a one over polynomial approximation to the value of the network, which is the tensor network presented. So you think of the tensor network as presented with a bubbling with a ordered vert. So it's a graph with an ordering on the vertices and the values of the tensors.
01:13:54.844 - 01:14:33.304, Speaker A: Sure, achieving this approximation is as hard as BQP. Yeah, it's equivalent. So now you. So I think of a quantum computer as approximating tensor networks to this level of approximation. And so the level depends on the ordering. So take the minimum, if you could find it. So that finding the minimum ordering is a hard problem, but, yeah, so, okay, so if I have a tensor network and an ordering, then I get some level of approximation.
01:14:33.304 - 01:14:41.944, Speaker A: But you're also claiming BQP hardness of this problem. So it's most clear to me what the delta is for the hardness.
01:14:47.644 - 01:14:50.224, Speaker B: The hardness comes from the first reduction.
01:14:52.784 - 01:15:31.754, Speaker A: So if I bubble a circuit this way, that one was exact computation, right? It wasn't asking for approximation. So the quantum circuit gives an approximation to the output of a quantum circuit with the probability of showing up as zero. Is an approximation to this, a one over poly approximation to this network. If I give you this network with this bubbling, meaning sort of ordering the vertices in this order, then this delta comes out to be one. And it's the same thing.
01:15:40.054 - 01:15:43.274, Speaker B: The value of the network, though, is exactly the probability.
01:15:43.914 - 01:16:44.964, Speaker A: The value of the network is exactly the probability. And approximating the value is what the BQP complete problem is up to a level complete. Okay, so I'm about to take five minute break. So at that point, let's talk about it just before the five minute break. Let me just tell you why, what this has sort of led to this interpretation. Well, so there are a lot of problems that you can write down as tensor networks that don't naturally have a quantum circuit associated to them. So, for instance, some topological things like the Jones, computing the Jones value of the Jones polynomial at certain polynomial, and then there are some statistical mechanical models that you can definitely want, model for which the partition function value is a tensor network calculation.
01:16:44.964 - 01:17:20.346, Speaker A: And so in this sort of thing, you get quantum algorithms for approximating these things. And all those, there's sort of a series of results like that. All of them have that flavor to them. And there's a question as to whether this level of approximation is worthwhile for something like the Jones polynomial, where you really, this is an additive approximation. You're looking for something stronger than that. To sort of, topologists would be looking for something stronger than that. But that whole sort of thread, that's what's going on here.
01:17:20.346 - 01:18:00.974, Speaker A: You interpret it as a tensor network. You try to come up with good bubblings. You try to come up with equivalent tensor networks that compute the same value but have better delta approximations. That's sort of the picture. So let's take a five minute break, and then we'll get to matrix product states. Last bit. So I want to move to what can we do classically with these tensor networks? And I guess I'll sort of, I'll tell you.
01:18:00.974 - 01:19:32.008, Speaker A: So here's a theorem, which is that a log depth quantum circuit can be simulated classically in polynomial time. And this is sort of, this will be, okay, so this is an eight. Well, let me just sort of say, let's try to figure out how this would work. Right? So a log depth quantum circuit, I just sort of told you how we can sort of think of it as a tensor network, right? And by the way, I should say quantum circuit, where the gates are operating on, it's not arbitrary pairs, they're neighboring pairs. So our circuit looks like this. And the important thing is that this is a log size picture over here. So how do I do this classically? Well, this is a tensor network.
01:19:32.008 - 01:20:03.110, Speaker A: Remember, tensor networks, you can think about their orders in any way they want. So as a quantum circuit, I think about it as there are n things here, and I think, evaluating, I sort of evolve some state in n copies of h, and I evolve it down, and I finally use my computation. But I'm not going to do that. I'm going to think of it as a tensor network where I'm sweeping across this way. So tensor networks don't care about Unitarians. They don't care about anything. I'm just going to sort of sweep around it this way.
01:20:03.110 - 01:20:48.514, Speaker A: So let's see what it means to sweep around it this way. Right? Take a bubble, and I start to go this way across it, and I encounter, you know, I pick some vertex to swallow first. And after I've swallowed it, I have a sort of, I've split this picture into two pictures. There's sort of one vertex with an edge coming through, and then there's the rest of the picture over there, right? And so I can encode, I can just look at this side of it as a vector, and it's a vector on some one copy of h. Now I sort of swallow another one. Maybe there's another one over here. So I swallow that one, and maybe things are coming out of that like that.
01:20:48.514 - 01:21:39.994, Speaker A: By the time I've swallowed the second one, I've got more edges across the boundary. But I can keep track of whatever that vector is. And as long as every time I go along, as long as this dotted line only hits on the order of log strands, then I'm talking about a vector in a tensor product of h log times. So I'm talking about a vector in a polynomial size subspace. That's something I can do classically. I've got no problems with that. So as long as I can sweep along and at each time I'm intersecting edges for which I only see a log number of guys coming across, I can just do this classically.
01:21:39.994 - 01:22:22.566, Speaker A: So one of the features of a log depth quantum circuit is if I sweep along this way, the geometry is such that any cut is only going to cross on the order of log n lines. So I can sort of sweep through it. By the time I'm done, I've swallowed everything. I've got a number out. That number is the exact value, not the approximate value, the exact value of this circuit. And I've been able to do it because I've only had to go through linear algebra on a polynomial size subspace for a polynomial amount of time, for n order n time. This was sort of surprising when it first was seen this way.
01:22:22.566 - 01:23:12.904, Speaker A: But if from this point of view, it sort of makes sense. And not only that, I should say, look, why don't I just define a notion of a bubbling for which I only see log things at any step along the way and say, hey, any network that has that geometry associated with it, I can do that classically. So that's, there's a theorem that says, and in particular, I could even say, let me just count the number of things that cross as I go along. And let's call that the bubble width. Let's say the b is the maximal, b is the bubble width, which means if I've given you a bubbling, you count how many times things cross. At each step of the bubbling, you find the biggest of those numbers. And this says, if b is the biggest of those numbers, then you can simulate it in two to the b time order, two to the v time, classically.
01:23:12.904 - 01:23:22.904, Speaker A: So as long as the bubble width is log, we can do it in polynomial time. As long as the bubble width is sub linear, you can do it in sub exponential time.
01:23:25.764 - 01:23:33.344, Speaker B: You could generalize this notion, too, right? Why not have more bubbles? And why just expand one at a time?
01:23:34.994 - 01:24:12.822, Speaker A: Turns out it's you're right. So the bubble doesn't have to be a continuous bubble. Okay, that's right. That's right. We can sort of, bubbling is picking an order, and then you sort of sweep those guys in while not sweeping any other guys in. Sometimes to do that, you have to do funny things, or you can think of it as bubbles popping up in different places and expanding out and joining. Okay, so finally, let's talk about the tensor networks.
01:24:12.822 - 01:24:49.970, Speaker A: That people use in practice. And so the first of them is a matrix product state. So what is a matrix product state? It's a particular tensor network. And the tensor network is the very simple, it looks like this. And in this case, how many free edges are there? They're n. And we're going to think of it, we're going to interpret this tensor network as a vector. So where is it? It's a vector in H tensor to the.
01:24:49.970 - 01:25:45.934, Speaker A: Nice. And, you know, these, these, we're going to sort of, they're sort of, now it's going to be helpful to think of two different labeling sets, one for the free edges and one for the other edges. And so the free edges have this labeling set zero up to date. We can talk about these other edges having the label set, possibly larger. And the set of labeling sets that we use here is called the bond dimension. Bond dimension is the number of labels in the labeling set for the guys that are growing up there. So a matrix product state with bond dimension B is this type of tensor network where I haven't said, said anything about what the tensors that go here can be anything.
01:25:45.934 - 01:26:30.284, Speaker A: So this describes some class of vectors. But now let's just see very clearly what it is that we can do with that class of vectors, because this is the important, this is the key thing. If I want to take one of these vectors and inner product with another one of these vectors, what is the picture I end up getting? Is this picture, right? Two combs joined up. Right? Because now we've learned, we've learned how to do inner products, right? And I get this picture. This picture is a tensor network. It's closed. Let's talk about its bubble width.
01:26:30.284 - 01:27:02.808, Speaker A: Well, if it went this way with its bubble, that would be bad. But I can go this way. And if I go this way, the bubble width at any one time is like two or three or something, they're only going to cut a few edges. And if those edges, if those bonds are no more than polynomial in size, let's say so, my bond dimension is no more than polynomial in size. This is something I can do classically. So, okay, so this is a set of states for which I can do all my linear algebra. I can compute inner products.
01:27:02.808 - 01:27:42.090, Speaker A: I can compute norms. I can do all that stuff. So this is at the heart of why classical simulation uses matrix product states, because I can do linear algebra. And it just, luckily for us, it also encodes interesting states. For instance, it encodes, as we saw before, good approximations to ground states of 1d gapped systems. Okay, so it's not a stretch from this position to say, okay, that worked in 1d. What goes wrong in 2d.
01:27:42.090 - 01:28:11.004, Speaker A: Right. So what would I want out of a 2d picture? So, out of a 2d picture, I would sort of say, remember, my free edges were. What were they? They were the actual two dits. Right. And so here I'm going to have trouble. Well, I'm not going to have trouble drawing, but I'll have trouble seeing what I'm trying to draw. This is supposed to be a big lattice, and then I'm going to imagine out of each of these, there's a free edge coming out.
01:28:11.004 - 01:29:03.274, Speaker A: So it's like a hairbrush, assuming you get the picture. And so what is this? Now, this is another tensor network. What does it encode? It encodes a state on how many qudits, right? Well, they're n squared, right? There's a square array of quits. Okay? And this is what people call pets. And it's an open question, for instance, whether the area law essentially says. The area law for 2d would say gapped Hamiltonians. Their ground states are representable in this form.
01:29:03.274 - 01:29:49.124, Speaker A: But remember, let's talk about this question from a simulation point of view. If I want to do linear algebra with these guys, one of the things that I want to be able to do is take inner products. So imagine picking this guy up, having another one, and squeezing them together to take inner products, because we join them at the free edges. So what would you get? You would get sort of a 2d lattice and another 2d lattice on top with edges joining them. So now try to bubble this thing, right. The trouble is, there's no way to bubble it, even though there is a narrow dimension, which is the height right there. These sort of, you can't avoid.
01:29:49.124 - 01:30:13.940, Speaker A: Whereas in the 1d, we could take the height bubble that, and then the length, which was the other dimension, which was the bad one. We just sort of did that in time. We sort of spread that out. Here, there are two other dimensions that we have to go through. And no matter how we do it, if we sweep in the one, we still, right away on our bubbling, we hit the other dimension and we have to sort of do n things. So there's no. The bubble width of something like this is of order n.
01:30:13.940 - 01:31:12.346, Speaker A: And so that's going to take two to the n exponential time to simulate. So we're back to square one. So people sort of looking at this thing said, hey, look, even if I can't work with these guys, right, so what can I do. Well, they said, well, maybe there's a way of doing a little bit more, and this is what I'll finish up with. So I can't. So now my quest is for, like, a two dimensional system, right? And sort of how can I encode states using a tensor network to kind of get at some of the complexity of those that those states could have, but while still keeping it in the realm of something where I can do some sort of computation classically on them? And so let me draw you a picture, and I'll show you. So I know that, in general, you know, this is not going to work with 2d.
01:31:12.346 - 01:31:40.064, Speaker A: So let me. So one of the things you can start to think about is maybe I can specify the type of tensors here in some way so that they have some relations between them. So that allows me to do calculations. So maybe not all tensors are okay, but maybe some of them sort of combine in some good way and so that I'll have it be able to simplify the network. And so let me show you a picture of what this is. Not exactly representative, but it's representative of the class of things that I can do. Let me show you this picture.
01:31:40.064 - 01:33:15.934, Speaker A: Suppose I choose something like this. So let me just describe what the architecture looks like of this thing at these levels, every other level. At these levels, okay. At these levels, things split into two, right? So one leg comes in, two legs come out. At the next level here, neighboring legs get sort of mushed, get sort of joined and interacted with. And then at this level, again, each of these nodes split into two. And then at this level here, the neighboring nodes interact.
01:33:15.934 - 01:34:17.754, Speaker A: And then we'd have another splitting into two here. So it's a tree like structure with some joining in intermediate levels. Okay, so what is the feature of this? So now, suppose I I demand certain structure out of these guys. And the structure I demand is that when I interpret the tensors as going from above to below, let me insist that they are either a unitary, like this case, or a partial isometry. In this case, when they splitting into two, they're partial isometry is when there are two guys coming in and two guys going out, they're unitary. So. So imagine I have a state that's described where all the tensors I've specified satisfy this extra property of being unitary or partial isometry.
01:34:17.754 - 01:35:07.054, Speaker A: And now imagine that I want to take that state, and I want to compute its energy relative to some local term of a Hamiltonian. So a local term of a Hamiltonian would be some sort of evaluation at the bottom of some pair of guys, let's say. So that would be a matrix applied here, special matrix. I'm going to put a square there for it because it's not part of the original encoding, something I want to, and if I wanted to compute the energy, remember the energy is h inner product with the vector h applied to the vector inner product with a vector. So I have to take this thing and turn it upside down. So I take this whole picture and turn it upside down. Okay, now let's sort of see what happens away from this guy.
01:35:07.054 - 01:35:42.344, Speaker A: Okay, away from this guy. Let's see what happens out here. For instance, this was some partial isometry here, right? That's what we insisted that that geometry be, we've insisted that the tensor networks have unitaries or partial isometry. This is some partial isometry here. It's being joined up with the transpose of the partial isometry here. This is the exact, this is the corresponding partial isometry. So this is apply partial isometry, apply its transpose, which is an inverse.
01:35:42.344 - 01:36:20.166, Speaker A: So as long as nothing came in between, which it didn't because the operator was over here, these two strands disappear and I just get the identity with this strand. Does that make sense? I'm going to erase this. I'm going to just put a spaghetti strand up like that. That happens everywhere except right around here. So let's just sort of trace through what happens in the intermediate things. It keeps happening in here so that all these guys are strands, so that I'll eventually get to this guy and that guy and they'll join up. The partial isometry is nuclear app.
01:36:20.166 - 01:37:25.680, Speaker A: The only thing that I can't do that for is what I'll call the shadow of this guy. And the shadow of the sky is any edge that eventually leads, any node that eventually leads to this edge. So it has a shadow that kind of looks like this. So when I'm done doing all the simplifications I can do, I'm going to have this shadow, which I can't do anything with, and I'm going to have all these strands on the outside that just kind of simply just join the outsides in parallel places. If this shadow is small enough, for instance, suppose this shadow has only log number of vertices in it or edges in it, then I can simulate that classically because now everything has bubble width just fine because there are only log things of it altogether. That makes sense. So for the picture that I drew, you might have been saying, and perhaps I should have said this at the beginning, you were saying, look, this already has log depth as a whole thing.
01:37:25.680 - 01:38:00.762, Speaker A: You could have done this just fine. But I can do the same architecture where the bottom row is a two dimensional picture. So the bottom row is a two dimensional picture. I have unitaries that interact locally. On the next step, then I have partial isometries that take this square into a square half, you know, for every four of them, I get one. Okay? So I can sort of, which is a scale, I can change the scale. I sort of get a smaller scale, then I can interact all those guys together, then I can do that again.
01:38:00.762 - 01:38:52.124, Speaker A: And by the time I'm done, I've got a log, sort of just the two dimensional version of this picture. But the same thing will be true, which is that if I want to stickle, if I want to take the inner product of it with itself, with a local hamiltonian or two local measurements, where I could sort of talk about two point correlations or finite point correlations, the shadows of each of those guys are only going to be log in size. And so when I simplify that picture, I end up something that I can compute with classically. So I can't do any computation unlike with the matrix product six. I can take two different guys and stick them together. Here. I had to have taken the same guy because I needed the unitaries to match up exactly, right? So I can do some calculations, like calculations of energy, calculations of multipoint correlations between local measurements, but I can't do everything.
01:38:52.124 - 01:39:42.264, Speaker A: And these are called mirrors. And you can do the most general version of a mirror would be something whose architecture allows the shadows above every point to be only log. Or, I mean, most generally, all you would need was the shadows above every point to be a bubble width log. And this is, you know, I've sort of described one implementation, so I think we should stop. Right, it's about time to stop. So I'm going to stop there. If I had had more time, and if anybody wants to ask me about this, these pictures also give you a simple proof of a d dimensional area law for commuting.
01:39:42.264 - 01:41:14.764, Speaker A: And there's, as part of the warm up exercises, something that I skipped, but that is really kind of quite nice, is what's going on with teleportation as a picture. I wish I had gotten to, but I haven't, and that's it. So when you have a diagram like a matrix product state, is there some way of interpreting the non free edges as entanglement? Or can you sort of see how maybe the entanglement in nature's product space is somehow simple, or is there any. Yeah, so there is very much an interpretation. And the essential interpretation is that the bond dimension is the rank entanglement across a cut. So I guess I should have said that in general is if you have a state that's described either, if it's described as an NP's, you want to try to understand something about it's sort of encoding in some sense. It's sort of telling you straight off the back, if I make a cut somewhere, the entanglement across the cut in terms of the number of terms, you need to describe it in the form of something over here, tensor something over here, the number of terms is exactly the bond dimension of the things that you've cut.
01:41:14.764 - 01:42:07.894, Speaker A: So in 2d, in this picture, if I was to sort of cut it like that, it would be exactly the number of terms I need for this particular matrix. Product state would be the product of assuming these are all, let's say, constant size, bond dimension would be this to the number of guys that I've cut. And so that's, so an area law says something a little less than the fact in a two day RLL, it would say something a little bit less than the fact that peps have as a polynomial size bottom dimension representation. But in spirit, that's what it should be saying. And in fact, maybe that's even the better way to say it in terms of, it'll be in some sense the more useful way to think about it. So if it's true, but it's a little slightly stronger statement.
01:42:14.134 - 01:42:31.086, Speaker B: I had a question about this commuting case. So is it, I mean, is it correct to think that the ground state would be actually a product state? Or are there more interesting?
01:42:31.230 - 01:43:24.386, Speaker A: Yeah, it's not a product state, but it is a, it's a tensor network like this that's formed essentially what it boils down in the commuting case, you can get at the ground state by just multiplying the projections together because they commute. So that becomes a projection onto the ground space. Or actually, if you want to specify the set of violations, you can get a projection onto any of the eigenspaces. And that projection onto any of the eigenspaces is an operator with a very nice simple form. So as an operator 2d, it would look like free edges coming out the top and free edges coming out the bottom with these very small bond dimensions of size and d squared or something like that. And so now if I want to think of a vector sitting inside there, well, I can take an unentangled state. Apply this operator and I get a state that's guaranteed to be in the space that it's projecting onto.
01:43:24.386 - 01:44:20.512, Speaker A: And I can see that it has very simple structure. It's just this, because just sticking zeros on the top of these, you sort of applied it to the all zero state. I should say something else about the complexity of. If I hand you a peps and I specify what all the, what all these guys are telling, whether it's non zero is, is extraordinarily hard problem. So unlike in the matrix product state, if I sort of, you know, I can do any, I can compute its norm, for instance, as one way of telling whether it's non zero in 2d. Just even knowing whether it's non zero, whether you've encoded a state that's non zero, is. How hard could we say it was? I don't know.
01:44:20.512 - 01:44:33.864, Speaker A: It's very hard. So there's sort of, it's kind of tantalizing because all this information is right there. And at the same time you're given, you're handed this thing and you can't verify anything about it.
01:44:42.524 - 01:44:44.324, Speaker B: In that case, let's sign this up again.
