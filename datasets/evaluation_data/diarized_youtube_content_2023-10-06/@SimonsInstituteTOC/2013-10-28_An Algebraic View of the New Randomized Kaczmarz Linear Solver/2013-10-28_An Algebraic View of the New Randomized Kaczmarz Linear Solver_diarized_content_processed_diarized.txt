00:00:02.120 - 00:00:33.444, Speaker A: Okay, thanks, Ali. So, I mean, I prepared the talk, and I realized that the tackle is very cryptic. So really what I'm going to talk about is I'm going to talk about my perspective, an algebraic perspective, linear algebra perspective on a new algorithm for solving linear equations. The new algorithm is not by me, it's my perspective on somebody else's algorithm. And the algorithm is a randomized graph algorithm for solving linear equations. I was really delighted to be invited to this workshop, and I thought. So.
00:00:33.444 - 00:01:13.644, Speaker A: Thanks, Michael. And I thought that beyond the technical talk, I maybe should give something big, something slightly more general. And I've decided to offer an insight that obviously pertains to this particular talk, puts the talk in some sort of context, but hopefully slightly more general. And the core of this insight is that practitioners can take theoretical results, theoretical ideas, theoretical algorithms, and turn them into really fantastic software. So this is just the core, the full insight. This is true, but only in theory. And in practice, this is often.
00:01:13.644 - 00:01:49.860, Speaker A: Sometimes it's possible, but often this is very hard or impossible. I guess for theoreticians, this is just fine. It's not disconcerting at all, because in theory, all is well. But for practitioners like me, there is an issue here. Now, I'm not complaining at all. I like this situation, because really, what it means is it means that I can read theoretical papers, learn the theory, discover really exciting results, results that really excite me. And sometimes the theoretician thinks that there is still a long way to go.
00:01:49.860 - 00:02:42.674, Speaker A: Sometimes the theoretician may think that that's the end of the story and everything is done. But for me, I know that there is still a way to go in terms of doing really interesting research, to bridge the gap between the theoretical result and what's going on in practice. And the talk I'm going to give today is really a talk that where I'm going to walk through with you part of the way, not all the way to fantastic software, but part of the way that chaim and I, with some help from Haim Kaplan, did to bridge this gap between what was really a very exciting theoretical result and the fantastic software that we hope will be able to deliver one day. So the setup is really very, very simple. We want to solve linear systems of equations. Ideally, we would want to solve linear systems of equation, ax equals b without any side constraints. But this is a, a workshop on big data.
00:02:42.674 - 00:03:49.274, Speaker A: And Sergey said correctly on Monday that we really want to try to focus on problems that we can solve in linear time or log n, or close to linear time. And unfortunately, even though these linear systems are really the simplest type of high dimensional problems that you can imagine, we don't know how to solve all of them in anything close to linear time. So what do you do? You pile up more side constraints. So let's assume that the matrix is also symmetric, that it's sparse, that it's semi definite or positive definite, which is a class that still contains a lot of very important practical problems. And it turns out that even with all of those side constraints, we don't know how to solve them in anything close to linear time. So we had one more constraint, which really should nail the coffin, and that's that it's diagonally dominant. So diagonally dominant is really a very strong side constraint, because what it means, it means that the matrix is fundamentally isomorphic to a weighted graph, and that gives enough structure to come up with graph algorithms that really are very, very powerful.
00:03:49.274 - 00:04:22.690, Speaker A: And one subclass of diagonal indomit matrices, which I'm going to focus on because it's easier to explain what's going on, are Laplacians of graphs. So here we have two graphs. Here is the two dimensional mesh. It's an unweighted mesh, I numbered the vertices. And if there is an edge between vertex I and vertex j, one and two, then the matrix has a non zero value. In position one, two or six seven, there'll be a position, a six seven will be non zero. This graph is unweighted, so the absolute value of the off diagonals is all ones.
00:04:22.690 - 00:04:51.650, Speaker A: The diagonal is the degree, the weighted degree. This is an unweighted version. There are weighted versions, and you can generate a Laplacian of any graph. This is a two dimensional mesh, a three dimensional relationship. You can create a Laplacian for unstructured meshes, expanders, any kind of graph. So we want to solve linear systems that are isomorphic to weighted graphs. So how do you solve linear systems? So we'll start with really ancient history.
00:04:51.650 - 00:05:22.048, Speaker A: So the history is really ancient. So it started something like two these algorithms were invented. The first algorithms, and algorithms that we still use every day were invented something like 200 years ago by Gauss and Cholesky. One idea is to factor the coefficient matrix into triangular factors. If the matrix is positive definite or semi definite, you can factor it into a triangular factor and its transpose. Unfortunately, Gauss didn't know about big data, so he came up with an n cubed algorithm. So this is really.
00:05:22.048 - 00:05:50.736, Speaker A: Ok, so no big, no, no. Like John said yesterday, in the fifties, sixties, seventies, people realize that if the matrix is sparse, you can take advantage of that. There are very clever algorithms that factor sparse matrices. And if you specialize to, let's say, two dimensional meshes like the one I showed before, then it's much, much better. The complexity, arithmetic complexity, is much, much better than n cubed. It's n to the one and a half. For 3d meshes, it's quadratic n squared.
00:05:50.736 - 00:06:33.706, Speaker A: So this is much better than gaussian elimination on a full matrix, but still too expensive for big data. Another point, which John said yesterday, and is important in the rest of the talk, is that these exponents, the complexity really depends on the size of balanced vertex separators that you can find in the graph of the matrix. If the matrix has small separators, then you can solve the linear system very efficiently. If it doesn't have small separators, you can solve this efficiently. So in the roughly end of the seventies, early eighties, people got to this point. It's great, but it's not what you really hope for. So, is there anything else? And the answer is yes.
00:06:33.706 - 00:07:21.184, Speaker A: There is another completely different class of algorithms that actually precede this a little bit there from the fifties and sixties, which are exemplified by conjugate gradient. There are a lot of them, but the most famous is conjugate gradient. These are iterative algorithms in which you have an approximate solution vector xt, and you repeatedly iteratively update it, and in such a way that it converges to the solution x. The cost of each iteration is the number of nonzeros in the matrix a, or, if it's isomorphic to a graph, the number of edges which I'll denote by m. So it's easy to understand the cost of each iteration. It's harder to understand how many iterations you need in order for the solution to converge to something close to the exact solution. And it turns out that this depends on the distribution of eigenvalues of this matrix a.
00:07:21.184 - 00:07:59.284, Speaker A: If the eigenvalues are clustered, then convergence will be rapid. If it's not clustered, convergence will be slow. And this really is not a rephrasing of Gauss's ideas in any form or shape. This is really completely different. And the way I'm going to convince you, try to convince you, that this is really completely different is by saying, or by mentioning, that these algorithms are faster on 3d meshes than on 2d meshes. So what was hard for gaussian elimination is easy for this, and this is exactly for the same reason. So if the matrix has large sep doesn't have small separators, then conjugate gradient converges quickly.
00:07:59.284 - 00:08:58.052, Speaker A: If it has small separators, then it's great for sparse gaussian elimination, but not good for conjugate gradients. So this is really completely different. But it turned out to be really way, way too slow on meshes, many, many practical problems. And the next idea was to say, okay, so we have these two completely different classes of algorithms. Can we mesh them together in some way that the overall algorithm will enjoy the benefits of both of them? And this turned out to be possible using a technique from the sixties and seventies called preconditioning. So the idea is, you say, okay, so the coefficient matrix is a, and if it was very, very sparse or had small separators, I would factor it and solve the linear system, but it's too expensive to factor. Can I find an approximation p preconditioner? So, can I find an approximation of a called p? I'll denote it by p so that I can afford to factor p into triangular factors.
00:08:58.052 - 00:09:52.816, Speaker A: And then I'll use this factorization to transform the linear system to an equivalent linear system, but which has a different coefficient matrix, this l inverse a l to the minus transpose, which really fundamentally is a symmetric version of p inverse a. So if p is similar to a, then you expect p inverse to be similar to a inverse, and p inverse a to be close to the identity. And it turns out that if it's close to the identity, it doesn't need to be close to the identity in matrix space. It needs to be close to the identity in the sense that its eigenvalues are clustered. The identity has just one eigenvalue that's repeating many times one. So if the eigenvalues of this coefficient matrix are clustered, this will converge very quickly, and hopefully you can find a p so that factoring it will be much faster than factoring a. When people discovered this scheme, they started looking for ways to construct p.
00:09:52.816 - 00:10:43.536, Speaker A: And for many years, the only ways to construct p were either heuristics in which it was very difficult to say anything concrete about the convergence rates, or it was very problem specific, until in 1991, the theory community came up with a really cool algorithm to construct these matrices p, these preconditioners, in a way that allows for a very satisfying analysis of the convergence rate and the total solution cost. So let me describe. The basic algorithm is very simple. So we have a mesh here, and we notice that the mesh is isomorphic to a graph. So it's not just that the matrix has a graph of its non zeros. It's an isomorphism. So anything you do on the mesh really corresponds to something algebraic that you do on the matrix.
00:10:43.536 - 00:11:22.184, Speaker A: And what's more natural, if you were looking for small separators to say okay, this doesn't, the smallest separator here will be eleven vertices. Let's try to drop some of the edges to make the separator smaller. So you can start with this coefficient matrix and sparsify it. So here we've sparsified it all the way to a tree. If I delete this vertex, for example, I chop it into two roughly equal parts by just removing one vertex. So factoring this thing is going to be very easy. Now what about these eigenvalues of p inverse times a? Well that tree doesn't look that much like a, so probably the eigenvalues are not going to be very clustered.
00:11:22.184 - 00:12:03.634, Speaker A: Then you can prove that they're not going to be very clustered. But you don't have to be that greedy. So you can look for something like this where it's still meshy, it still looks like a mesh, it's still pretty similar, but still the separator is now half the size and here the separator is even smaller. And it turns out that there is a way to analyze rigorously these preconditioners, and they're based on a very important idea which is going to play out throughout the talk. So you have to pay attention to this. And the idea is that you change the representation, you don't do any work. It's not an algorithm, you just change the representation of the problem from what is effectively the adjacency matrix of the graph to the incidence matrix of the graph.
00:12:03.634 - 00:12:53.770, Speaker A: So u is an incidence factor, it's a matrix, it's a short and fit matrix. The rows correspond to vertices in the graph and the columns correspond two edges in the graph. And the column that corresponds to edge e, which is ij, has just two non zeros, the square root or the square root of the absolute value of a and minus the square root of the absolute value of a. And it turns out that if you take the outer product of this column and its transpose, which is what happens here, you reconstruct the original matrix. So we moved from one algebraic representation of the graph to another representation of the graph. And this was key to a discovery in 2003. So the combinatorial preconditions started in 1991 using a fairly complicated type of analysis.
00:12:53.770 - 00:13:47.588, Speaker A: But in 2001, Bowman and Hedrickson used these incidence factors to come up with a very clean neat and useful way to characterize the convergence rate of these algorithms. And what they said is, they said, ok, we have this incidence factor, uh, and we're using a subgraph as a preconditioner. So let's order the columns of U so that the subgraph, the basic variables, the basic edges show up first, and then there are the edges that we've dropped, the non basic edges. So p is really the outer product of the incidence factor ub times Ub transpose. And if it's a spanning subgraph, then there is a matrix w that transforms this incidence factor into the original incidence factor. And it's very easy to see that you can construct these w's from path embedding. So convincing yourself that such a w exists is really pretty easy.
00:13:47.588 - 00:14:32.690, Speaker A: And once you do this, you can construct w any way you want. Once you do this, you can show that the bound on the eigenvalues of this preconditioned matrix lie between one and the two norm of w. So it gives you a bound on the clustering of the eigenvalues of the preconditioned system. Now, the two norm is kind of hard to estimate, but you can bound the two norm by other norms. And these other norms, like the infinity norm, the one norm, the Frobenius norm, are related to combinatorial properties of the embedding from which you construct a w. The congestion dilation stretch other graph embedding properties. So that gives you a tool to really look at the two graphs, do graph reasoning, and come up with an algebraic bound on the spectrum of the preconditioned system.
00:14:32.690 - 00:15:36.916, Speaker A: And with this technique, and also earlier, more complicated techniques, people came up with a whole bunch of algorithms to sparsify graphs in a way that's useful in this context. Maximum spanning trees, all kind of constructions like the one I showed before are due to Anil Joshi. The one I want to point out is these low stretch trees. So, low stretch trees, Bowman and Hendrickson realized at the same time that they discovered these W's. They discovered that low stretch trees have good bounds and the forbidden norm, which translates into a good bound on the two norm, which translates into rapid convergence. Okay, so none of this is really new, but a couple of months ago, Haim said, okay, have you seen this very exciting result from stock 2013 by John Keller, Seidford and John Keller and a couple of his students? So I said, no, I looked at the paper. What's the paper? The paper is really about the new algorithm.
00:15:36.916 - 00:16:00.946, Speaker A: And now you can understand why we got so excited. I mean, this is a problem that people have been solving, inventing algorithms since Gauss's time, and it's not clear that there's anything new to discover here. And they claim that they have a new algorithm. Very exciting for us. The algorithm is iterative. So I said, okay, it must be some rephrasing of these old algorithms from 1991, 2004 and so on. But no, it's a different algorithm.
00:16:00.946 - 00:16:52.934, Speaker A: It doesn't use conjugate gradient, it doesn't use anything like conjugate gradient. It's iterative, but not conjugate gradient. The analysis was completely new, doesn't, didn't build on any of the earlier analysis. It was based on some kind of electrical reasoning, which to me as a person versed in linear algebra, seemed kind of weird. I've seen it before, used as a motivation, but not really as an analytical tool. So is it completely new? It says it's completely new, but it uses these low stress trees. Where did they come up from? I mean, is it somehow related to the old results? Can I use my insights about these old results? Ten or 20 years of research, is there anything useful in these old results? But if the result is cast in a completely new type of analysis, it was very hard for us to understand whether it's related and how it's related to all the results.
00:16:52.934 - 00:17:41.570, Speaker A: And because of this hint, the low street trace, I said, okay, it must be closely related to things that people did before, but it just expressed in a different way. And Haim and I searched and found a way to express this in a way that's more understandable to us, hopefully more useful in implementations, and reveals the structure, the relationship between this result and earlier results. So that's what I'm going to show. So what's the algorithm? The algorithm is really weird. So you start with this matrix, which I said you can express as either a, the adjacency or the outer product of the incidence factors. It's just a different representation, no computation here. And you start, but this matrix is n by n, and this is n by m, so it's larger.
00:17:41.570 - 00:18:21.284, Speaker A: So you take this fit larger matrix and you say it's not large enough, I'm going to make it even larger. So you expand it into a square matrix that's n by n, number of edges, by the number of edges, you make the linear system much bigger and you expand it. So here's the u block, you add an n block to make it square and full rank, and you do this in a very specific way, which sounds very hard. You add the basis for the null space of u. So you take the rows of, you take u, and you find the basis for the null space. So u times n transpose should be zero. Normally, it's hard to find null space basis.
00:18:21.284 - 00:18:46.008, Speaker A: You turn this into a bigger matrix, and then you take that bigger matrix as a coefficient matrix of a new linear system. Kf equals zero b. And I'll refer to this zero b as a right hand side called g. And you solve this linear system, which is bigger than the original. Once you solve f satisfies two block equations. Uf equals b and n. F equals zero.
00:18:46.008 - 00:19:19.492, Speaker A: Uf equals b means that f is orthogonal to the null space of f is orthogonal to the. To the null space n, which is the null space of u, which means that it is in the row space of, uh, or the column space of u transpose, which means that there is an x such that u transpose x equals f. Turns out that because of the incidence structure of u, it's easy to find that x. If you have f, finding x is easy. So, first you construct n. Then you solve this linear system. You find f, then you find x.
00:19:19.492 - 00:20:12.254, Speaker A: That's easy. And now, because you have the equation u, f equals b, that's ufo. Uf is u times u transpose x, which is ax equals b, and you've solved the problem. Obviously, this complicated scheme only makes sense if solving kf equals g is guaranteed to be easy. And that's the way they've constructed this algorithm in a very clever way that guarantees that maybe solving ax equals b is easy or hard doesn't matter. Solving kx equals kf equals g is always easy. Okay, so how do they solve kf equals g? They went back to an algorithm for 1937, which is really a version of a general scheme from the 19th century called constraint relaxation iterations, which really is the easiest thing you can think of.
00:20:12.254 - 00:20:34.376, Speaker A: You take an approximate solution f, which still doesn't satisfy kf equals g. You say it doesn't satisfy the equation. Let's find one equation that it doesn't satisfy. So you say, okay, let's look at the ith equation. Does it satisfy the I equation? No. Let's fix the approximate solution f so that it satisfies this equation. Maybe this ruins satisfaction of other equations, but you don't care about this.
00:20:34.376 - 00:21:16.786, Speaker A: You just do this and repeat. There are several ways to do the update to update the approximate solution f or f t to satisfy equation I. So there's Jacobi relaxation. That's the easiest one. You modified the diagonal element fi. They used an algorithm called Kashmage, which corrects the solution by adding a scaled copy of the row that is not satisfied. So you look at the row times your approximate solution, and if it's not satisfied, you take that sparse row and you add a scalar multiple of it to your approximate solution to make it to satisfy the equation.
00:21:16.786 - 00:21:59.678, Speaker A: This has a geometric, this is a projection. It has a geometric reasoning on why it makes sense to do this, which is not really important in this talk. You're basically moving f as little as possible in euclidean space to satisfy that equation. So they're using this equation. So they're using this method by Kashmas, and they use this method by Kashmir in a very specific way that's important for ensuring that the algorithm runs fast on kf equals zero. What they do is they start from an f zero, which they find using another algorithm, which is really easy, not very complicated. So they start from an initial guess that satisfies u.
00:21:59.678 - 00:22:24.954, Speaker A: F equals b. So it satisfies the top equations, but not the bottom equations. Nf equals something else. I denoted it by z, which is, in general, not, not zero. And it's easy to see that when you run kashmage on this matrix, because this is a basis for the null space. Ft always satisfies this equation. U times ft always equals b z.
00:22:24.954 - 00:22:42.944, Speaker A: The bottom part is not zero, but it converges to zero. And the reason it happens is very easy. If you pick an equation that's in the u block, it's already satisfied. This is the invariant. And if you pick an equation n times one of the rows of n times f should be equal to zero. It's not zero. You fix x, you fix f.
00:22:42.944 - 00:23:14.428, Speaker A: But what you do to f is you add a vector that's in the null space of u, so it doesn't violate the equation uf equals b. So you're always remaining in that invariant subspace. Okay, so that's the algorithm. There is nothing more to the algorithm, but there are three hard questions. One is how to construct n cheaply. In general, it's expensive to construct null space basis. How many iterations will this take? And how to run them very efficiently.
00:23:14.428 - 00:23:57.914, Speaker A: So how did they construct the null space basis? They split. And now this is our explanation of what they do. They would explain this in a completely different way. But what they're really doing is they're doing this splitting of you into the basic and non basic variables, where ub represents a spanning tree, and therefore it has this matrix w that you can construct combinatorially that will transform ub into u. And obviously because ub shows up here and also shows up here, this w has a lot of structure. Its leftmost block is the identity. So you have an identity times the intra and then the interesting part of n, which I don't, which I denoted by wn.
00:23:57.914 - 00:25:03.604, Speaker A: Once you come up with this expression, it's very easy to show these three lines, to show that not this matrix, not w, but some derivation of it, minus wn, transpose I is an outspace basis. And you prove this just by multiplying by u and you get the zero, which means that it's an alt space basis. Now how do you construct wn? I basically said this before, you construct wn using a path embedding. So because every column in UE, whether it's a basic or non basic, has two non zeros and all the columns in u have two non zeros, basically you line up columns so that the non zeros cancel out until you get the two non zeros at the end. It's trivial to derive those coefficients. Basically, a column of view specifies a path in the tree that connects the two endpoints of the edge that you dropped that you're trying to express algebraically. Now, what would make a good n? Something that makes Kashmaj converge quickly.
00:25:03.604 - 00:26:01.650, Speaker A: And the convergence of Kashmaj is determined by or bounded by some condition number of the coefficient matrix k by the Frobenius norm of the matrix times the two norm of the inverse of the matrix in this application, because the reason they started from this special u zero is to make not all of k meter but only the n part, because really you're iterating, you're running kashmage on a rectangular matrix. This n matrix, the u part really doesn't pay any role in the iteration. So what you want is you want an n with a small sum of squares. That's the forbidius norm, and that cannot make vectors small. That's the two norm of the pseudo inverse. And it turns out that this matrix is good, that the pseudo inverse is never too large because of the I block here. And the Frobenius norm is really the Frobenius norm of wn, which is the stretch of the tree.
00:26:01.650 - 00:27:01.024, Speaker A: So that shows you why they chose the low stretch tree. And it shows that the choice of the low stretch tree and the structure of the algorithm is really completely built on that matrix w, which was the matrix that was used to analyze the combinatorial graph preconditioners since 1991 or since 2003, when it was discovered. So that's why they use a low stretch tree, because it makes sure that the null space basis is a null space basis that makes Kashmage converge quickly. Now, the other really very important advance that they made was a way to run kashmir iterations in a way that makes every iteration very, very cheap. And to do that, they looked at the expression. So they say, okay, so we are going to update f. How is f updated? F is updated by a scalar multiple alpha of the row that we are trying to fix.
00:27:01.024 - 00:27:36.546, Speaker A: We are trying to fix row I. How do you choose alpha? You choose alpha using this formula, which ensures that you satisfy equation I. And most of the terms here in terms of the progress of the iterations are constant. You can precompute them ahead of time. Gi this dot product, this sum of squares, they're all fixed. The only thing that's interesting in this part, this is a dot product between constants that lie on the path in the graph. They depend both on the path and on the edges that you traverse times the current solution.
00:27:36.546 - 00:28:19.790, Speaker A: And what they tried to do was to come up with an abstract data structure that supports three abstract operations. One operation computes those dot products. So basically we wanted to create a data structure that's an oracle that you can say, here is a path in the tree. What's the dot product of my current vector along that path? That's one operation. The other operation is say, ok, it should be the dot product, should be one thing, but it's something else. Let's modify the flow, this f along the path so that it would satisfy the equation by adding alpha times ki. And at the end, the f is represented implicitly in this data structure.
00:28:19.790 - 00:29:11.758, Speaker A: At the end of the algorithm you really need f, is a set of numbers, is a vector of n numbers, so you need to be able to output them. But you need to do this only when you, when you decide to stop the iteration. So this, you can do this when you tear down the data structure. You don't need to do this efficiently all the time now, so it looks almost doable, but it was not doable, because here this k in this inner product k, the indices of k are I and J. So it means that the inner product is between the current solution vector and constants that lie on the path in the tree. But the constant depends both on the edge that you're traversing in the path and on the path on the endpoints of the path. And there is no, nobody knows how to do this abstract data structure efficiently, but they discovered that you can do an algebraic transformation.
00:29:11.758 - 00:30:03.404, Speaker A: I won't show it here, too detailed. It's not very complicated, but too detailed in a way that removes the I index from the coefficient here. So you can move to basically a different space in which these dot products involve constants that belong to the edges. So you have a tree, every edge in the tree has a constant and the variable. And what you're doing is you're taking pads in the tree and you're computing the dot product of the constant times the variable along the path. Five minutes, summing this up, getting a constant that you turn into alpha, and then you update the path using a structured way that data structure can do efficiently and their data structure can do this in o of log n. It's a really interesting data structure.
00:30:03.404 - 00:30:29.804, Speaker A: It's not that complicated. They are very adamant in the paper saying that it's not very complicated. There is a reason that they're so insistent, and the reason is that it's a special case of a very general data structure called dynamic trees that was invented by Slater and Tarjan. So I said, okay, I looked at this. It was hard for me to understand. They explained how to do this. It was hard for me to explain.
00:30:29.804 - 00:31:09.094, Speaker A: We have in my department an expert in data structure, Chaim Kaplan, who was a student of Tarzan. So I said, okay, I'll take this to Haim and ask him how it's done. What's the data structure? He said, oh, dynamic trees, nobody uses dynamic trees, they're way too complicated. I said, oh boy. But then we read the paper carefully together, and we realized that John Keller's claim is absolutely correct, that here the tree doesn't change. So this application of the dynamic data structure is really simple, it's implementable, it's not complicated. And we figured this out completely.
00:31:09.094 - 00:31:38.426, Speaker A: So it's based on something very complicated, but it's a simple instantiation. Ok, we've got to my last slide, which has two slides wrapped in one, because Ali is saying that I ran out of time. So the summary is really interesting. It is a very exciting new algorithm. It's not directly related to combinatorial preconditioners that were invented over or developed over 20 years. It is using low stretch trees. And that's not a coincidence.
00:31:38.426 - 00:32:20.038, Speaker A: It's because of fundamentally the same basic reason, and that is that transformation matrix w, which here is used as a null space basis and in combinatorial preconditions is used in another way. You want a null space basis. This w with a small norm that's how Bowman and Hendriksen came up with low stretch trees, and that's how Kenner is. Students came up with low stretch trees. There are two key ideas. One which they express in one way, which is that they have a combinatorial method to transform an ill condition, a hard, small problem, to a larger easy problem. And the way that I'm expressing this, which hopefully will be useful in the future in generalization of this, is that this is a null space method.
00:32:20.038 - 00:32:49.780, Speaker A: You take the problem, you create a null space of the relevant part of the matrix, and that gives you the power to turn a hard problem into a bigger easy problem. The other part that's very important, and as far as I know, completely new in linear algebra, in algorithms. In linear algebra is an implicit way to represent the iteration vector rather than an explicit representation. To make iterations cheap, we've started doing experiments with this. It seems to have large constants. Convergence seems very slow. So it's linear in the number of edges, but linear with the large constant.
00:32:49.780 - 00:33:23.304, Speaker A: And the other thing that I want to mention here, which is also critical to the question of whether we can turn this into fantastic software, is whether this is the way they describe the algorithm. The way I currently understand the algorithm, it's inherently very, very sequential. It's doing a linear number of iterations. The iterations themselves are very cheap. That's not good in terms of parallelism. So unless we can find a parallelized version and hopefully a parallelized version that's provably parallel and provably effective rather than heuristic, this is not going to turn into fantastic software. There is a lot of work to do here.
00:33:23.304 - 00:33:24.404, Speaker A: Thank you very much.
00:33:28.304 - 00:33:49.152, Speaker B: Take one question. So I'll ask a question. The old methods are based on replacing a with u times u transpose, and in this case, you're using those instance matrices. Well, if I already have the factors in some other form, not necessarily incidence.
00:33:49.208 - 00:34:14.446, Speaker A: Matrices, so can you use them here? I don't know. I mean, well, it depends what kind of factors you have. The incidence factors are really not something that's challenging computationally. It's just a different way of representing the matrix. So you can represent it as an array of matrices or as a linked list or. So the U factors are really exactly the same representation. So it's not doing anything yet.
00:34:14.446 - 00:34:19.774, Speaker A: If you have factors that are triangular factors or something useful, that could be useful maybe here, but we don't know.
