00:00:00.240 - 00:00:33.416, Speaker A: Shiva is a professor in the circuit. IAC department awards during the Career award society, skis paper award, multiple teaching excellence awards. And yeah, today we are going to a two part tutorial. Thanks, Palu. Thanks Palu and Shipra, for inviting me to speak at the session. And I also thank the other organizers who are running the semester long program. And Balu told me that since this is after lunch, everybody's sleepy and he asked me not to wake him up in case he falls asleep.
00:00:33.416 - 00:01:04.184, Speaker A: So what I decided to do is I want to make sure you guys don't go to sleep. So I'll keep pausing in between for questions. And feel free to stop anytime because this is supposed to be a tutorial, so I don't have to cover everything. But let's make it interactive. I can give you a quiz. I can make some problems on the fly. Okay, so the title is very long.
00:01:04.184 - 00:01:35.940, Speaker A: I'm not going to parse it. It'll become clear as we go along. But two words I want to highlight, stochastic approximation and reinforcement learning. So the part, one of the tutorial that I'm going to talk about will be completely on stochastic approximation. I'm not going to talk about reinforcement learning at all. And Xiv, in his part, is going to use the results that I present from the first part and present results on reinforcement learning. So this is my part of the talk is basically joint work with.
00:01:35.940 - 00:02:00.648, Speaker A: It's from two different pieces of work. One is joint work with Zaiwe Karthik and Sanjay. Sanjay is also here in the audience. And the other work, the second work is joint work with Zaiwa and Martin, who was my former postdoc, who is currently at University of Minnesota. And Martin might be joining us remotely at some point. I don't know if he's already there. Okay, let's jump into my part of the tutorial.
00:02:00.648 - 00:02:18.256, Speaker A: So what I'm going to talk to you today about is simply this problem. I want to solve a fixed point equation. So x is a D dimensional vector. I only live in RD finite dimensional spaces, and f bar is an operator. This is a weird notation with a bar. You'll see why later. So.
00:02:18.256 - 00:02:52.032, Speaker A: So what's an operator? It takes a vector, splits out a vector. Now all I want to do is find a vector that solves this equation, fixed point of this operator. And obviously, to do this, we need to have some properties on f bar. And it's known classically that if you just keep applying this f bar, start from your favorite x zero and keep applying f bar then this iteration converges to a fixed point. If f bar is a contact shell. This is called Banach fixed point here. So Banach fixed point theorem first guarantees that there is a fixed point, and then it says that if f bar is a contraction, you converge to it geometrically fast.
00:02:52.032 - 00:03:13.736, Speaker A: The geometric convergence is also called linear convergence in optimization literature. And contraction just means that you take your favorite two vectors, x and y, and apply f bar to those vectors. Then the distance decreases by a factor of gamma. Less than one. Gamma is always less than one. Gamma is my contraction factor. So if you have two vectors and you apply this operator, you get two new vectors, sorry, this should be f bar.
00:03:13.736 - 00:03:42.570, Speaker A: You get two new vectors, and the distance has contracted. And Banach fixed point theorem does not care about what the underlying norm is. It could be any normal d. And Banach's theorem is applicable even in infinite dimensional spaces. We'll see that in our case, the role of the norm is going to be very important. So actually, everything that is said in terms of contraction works, even for pseudo contractions. So everything I said in terms of convergence works also for pseudo contraction.
00:03:42.570 - 00:04:03.934, Speaker A: What's a pseudo contraction? This is same equation as before, except that instead of y, I put x x star and f bar of x star is x star. So I replaced X star here. And pictorially, what this is saying is that you have x star, you have your x. If you apply, again, this should be f bar. If you apply f bar, the distance to x star is decreasing the distance to some other arbitrary y. I don't know. So this is a generalization.
00:04:03.934 - 00:04:29.202, Speaker A: When you have pseudo. To define a pseudo contraction, you need a x star, but contraction implies existence of x star. And pretty much everything I'm going to talk about today also applies for pseudo contraction. But I'll just focus on the contraction for this talk. Okay, now it gets interesting. Everything I said so far is classical. Now the problem is I want to run this banach iteration, but I don't have access to fbar.
00:04:29.202 - 00:04:54.776, Speaker A: I have an access to a noisy oracle. So I give it XK. Its job is supposed to give me f bar of XK, but it's going to add some noise in the process. And the noise can actually, it can be iid, or it can depend on XK. We'll talk more about the noise model later. Now the question is, what do I do now? Should I just pretend as if there's no noise and just keep doing this iteration? And almost, yes, turns out you can keep doing that. But it's good.
00:04:54.776 - 00:05:30.352, Speaker A: It's a good idea to introduce a step size. So what is the step size? What this is saying is that earlier Banach just went to f bar of XK. What we are doing now is I'm at XK. Instead of going all the way to f bar of xk plus noise, which the oracle is suggesting me to go to, I'll do a convex combination. And this convex combination weights can change with time. The idea is that if I know I am somehow closer to the optimum, then maybe I don't want to trust this oracle too much because it's noisy, so I want to put less weight on it as I go along. So over time, maybe I want to decrease this alpha case, and the same thing can be rewritten like this.
00:05:30.352 - 00:06:02.190, Speaker A: X k plus one equal to x k plus step size times f bar of xk plus noise minus x k. The main question I'm going to address in this talk is, does this work? I mean, the answer is supposed to be yes, otherwise I'll not be talking to you. How well does this work? I want to characterize the rates of convergence, and I'll tell about what that means in the next slide. So this is the outline of my talk. I'm going to first introduce stochastic approximation. We are currently midway through the introduction. Then I'm going to talk about rates of convergence, finite sample bounds.
00:06:02.190 - 00:06:22.286, Speaker A: I'll do this in the mean square sense. X k is now random iteration. Because of the noise from the oracle, I look at the distance square. The square is not important. You could get rid of the square if you don't like it, but essentially it's expectation of the mean error. And I'll present a sketch of the proof. It's a Liapunov proof.
00:06:22.286 - 00:06:55.838, Speaker A: I'll talk about what the Lyapunov function is, and so on. And after that, I'm going to talk about high probability bounce on this X K minus x star on the error that I want to be exponentially decaying. And then also, if time permits, I'll also present proof sketch of that result as well. Some of you in the audience may have seen this part of my talk, but then I want to tell you that this part is brand new. It's so new, it's just off the oven. It's not even published yet, but the paper is going to be out in a couple of weeks. I mean, it's not even on archive yet.
00:06:55.838 - 00:07:17.022, Speaker A: Publishing, anyway, takes a long time. Okay, so let's continue talking about stochastic approximation. Um, so again, this is the equation I want to solve this is stochastic approximation algorithm that we already saw. I want to motivate it using several examples. Let's start with optimization. Uh, any questions so far? Okay, uh, let's start with optimization. I have a function.
00:07:17.022 - 00:07:33.514, Speaker A: This is not an operator. Now, it's a function. It's a scalar valued function. I want to find an x that minimizes this. And let's say this is convex. Then this is same as saying I want to solve gradient of f equal to zero, and I'm writing that equation in a funny manner. I just added x's on both sides, and I put a eta here.
00:07:33.514 - 00:08:10.592, Speaker A: If I solve this equation, that will be the minimum of f of x. Now, this starts looking like my fixed point equation, where my operator is basically this guy. And it can be shown that if f is smooth, strongly convex, for an appropriate choice of EtA, this operator is indeed a contraction with respect to l, two naught. So indeed, if I prove some result on stochastic approximation, I can immediately apply it on optimization. But that's not required because optimization. Oh, sorry. When I don't have noise, this, when I, when you don't have access to exact gradients and I have noisy gradients, then the stochastic approximation algorithm reduces to SGD.
00:08:10.592 - 00:08:38.542, Speaker A: Basically, I have f bar of x minus x that x cancels with this. So I'll end up with simply gradient of f. The theta is absorbed into my step size. So, by far the most popular form of stochastic approximation that's studied is SGD. And many of the results I talk about today are well known in the case of SGD, but not all the results. I think the first part is well known. The second part, I'm not quite sure if some of the results are known even in the case of SGD.
00:08:38.542 - 00:09:15.414, Speaker A: And so this way of modifying optimization into fixed point might look a little bit silly. Uh, but turns out there are many ways of translating optimization into, uh, finding fixed points of operators. And, uh, this connection can be used to actually formulate many optimization algorithms in terms of, uh, fixed points solutions. So there's a recent book by Wattawyen and Ernest Rue where they talk about many optimization algorithms from operator methods. So, uh, that's one example. Then the second example is on, is from reinforcement learning. This is what Xiva is going to talk about in the second part of the tutorial.
00:09:15.414 - 00:09:59.480, Speaker A: So I'll not dwell much upon this. But here, f bar is going to be my bellman operator. And in mdps or reinforcement learning, I want to solve a Bellman equation, which is a fixed point equation of this form. And if we take the stochastic approximation algorithm, most of the popular reinforcement learning algorithms, like TD learning, q learning, can essentially be thought of as some variance of the stochastic approximation approach. And turns out that the corresponding operators here, the underlying norms, can be very different. For TD, one can show that it's a contraction with respect to weighted Lp norm for various values of p for all p from two to infinity, but whereas for q learning, it's a l infinity norm contraction. So, various norms that show up play an important role in my talk.
00:09:59.480 - 00:10:27.850, Speaker A: I'm going to present results that work for all norms. So more details about reinforcement learning will be in the second part of the talk. I'm not quite sure. Maybe Chaba knows something about how, when TD was discovered, did they think of it as Robin's model, stochastic approximation or. No, it's the fixed point iteration definitely is something.
00:10:27.962 - 00:10:35.674, Speaker B: TD was discovered without any mention of this point whatsoever. Just like thinking.
00:10:37.254 - 00:10:43.358, Speaker A: Yeah. In fact, it was thought of as like some kind of optimization algorithm where you drop some terms and so on, I guess.
00:10:43.486 - 00:11:12.732, Speaker B: No, it was like there was a story underlying. It was like, oh, you need to boost. You use your own predictions. That's the TD error, since that's like in supervised learning, you're trying to predict the value to take the difference to the value at your trust based on the difference. Here, the error has its own prediction because you don't know the future values. Yeah, but you can predict them. Why not like you optimistically, plugins, practical sensitivities in the book.
00:11:12.732 - 00:11:26.604, Speaker B: Cause this optimistic area speaks of this, because there's a prediction that shouldn't, has no reason to be correct. You plug it in silver, it's kind of like pretty neat.
00:11:29.624 - 00:11:37.960, Speaker A: Yeah, that's right. But I think the connection was made at some point. I think it was already well known by nineties. In the neurodynamic programming book, they talk.
00:11:37.992 - 00:11:50.084, Speaker B: About it's early nineties. Connection was connection to BP. And then from the point of view.
00:11:51.114 - 00:12:41.542, Speaker A: Are most convergence groups based on this connection? Yeah, in the neurodivergence programming, the convergence is based on the stochastic. Okay, so the third example I want to give you is about solving linear equations x equal to b. And again, I'm again making some silly manipulations to make it look like this. I multiply both sides by ETA, bring this onto the left side, and then add x on both sides. I end up with this. Now this starts looking like this, and my operator is basically whatever you see on the left hand side. And turns out that if a is Horwitz, Hurwitz just means that the real part of all the eigenvalues is less than zero.
00:12:41.542 - 00:13:11.114, Speaker A: If a were a symmetric matrix, this is same as negative, definitely. But when a is not symmetric, negative definiteness is different for symmetric matrices. When a is not symmetric, I only care about the real part. And it can be shown that as long as Hoorwood's property is satisfied, I can find a eta under which this whole thing will be a contraction. I mean, this is just affine term, so this is almost irrelevant. Uh, but this matrix operator and matrix are the same thing. In the linear case, this matrix, or the operator will be a contraction with respect to sum weighted l, two naught.
00:13:11.114 - 00:13:44.632, Speaker A: And then if I write stochastic approximation in this case, again, this is my f bar operator. There's x here, there's a minus x here. They cancel off, and I essentially end up doing this. And, okay, one thing I want to point out is, what is the source of noise here? I want to solve these equations. Here I am pretending that b is unknown, and I get some samples bk, whose mean is b. Then I have this update. Then I can also ask the question about what if a is unknown? Here, I'm pretending a is known.
00:13:44.632 - 00:14:25.184, Speaker A: And that's what I'll talk about in the next slide. And linear stochastic approximation works in both cases. So, in fact, I was told that the many, many ways of solving, I mean, I know that there are many, many ways of solving linear equations, but I was told that if you have large number of equations, like in machine learning, like td learning, and so on, then, uh, one of the most popular ways today is to basically do this. This is, this can be thought of as some variant of HgD. Uh, okay, good. So, any questions so far in this slide, I'm going to present my formulation more formally. So, I want to solve f bar of x equal to x, and my operator f bar.
00:14:25.184 - 00:14:56.440, Speaker A: Actually, if I dig into it, there's more going on. What I have is a mapping f. It takes two things, x and y, not just x alone. And y for me is a markup chain. And f bar is simply an expectation of this with respect to the stationary distribution of my markup chain. Let's actually make this concrete. So in this case, the corresponding stochastic approximation, it looks same as before, but instead of f, instead of just f bar of x here I'm writing f of x, comma Y.
00:14:56.440 - 00:15:22.242, Speaker A: So earlier I had w as my noise now I'm introducing a second source of noise. I could have combined everything and then called it, modeled it as a single source. But there's a reason why I'm splitting it into two forms. And if we think of it as, think of the special case of linear stochastic approximation. My f of x comma Y can be thought of as ak times x. So instead of y, I have Ak. Ak is also noisy here.
00:15:22.242 - 00:15:47.360, Speaker A: And this should be bk, my wk additive noise. Wk can be thought of as my b. So Yk is, I call it multiplicative noise, inspired by the special case and w as my additive noise. Here it is bk. So the way they affect the iterator different. And that's the reason I want to make this distinction. And when I actually start modeling them, I also allow different behavior on both of them.
00:15:47.360 - 00:16:19.784, Speaker A: And some of these assumptions are inspired by what shows up in reinforcement learning. So I assume YK is a finite state ergodic Markov chain whose stationary distribution is mutation. A special case of that would be YK is IiD with distribution mu. And one consequence of finite state ergodic is that Yk is geometrically mixing. So the distribution of YK approaches its stationary distribution geometrically fast. If you don't like Markov chain, just pretend it to be iid. For the purpose of my talk, XIV would worry about the Markov chain in his part of the talk and the additive noise WK.
00:16:19.784 - 00:16:46.100, Speaker A: If you want to keep it simple, think of it as iidi. More generally, it can be martingale difference. Martingale difference just means that it has conditional mean zero. It can depend on the history in complex ways, and Wk can also depend on x k. And it's bounded in this manner. So if x, k grows, wk can grow, but the support Wk is random. Think of it as its support is bounded in this manner.
00:16:46.100 - 00:17:03.444, Speaker A: So this bond is almost sure. And finally, f bar is contraction. And I want to highlight that it's a contraction with respect to an arbitrary norm. Any questions about the formulation? Yes.
00:17:07.784 - 00:17:09.084, Speaker B: Why not faster?
00:17:09.744 - 00:17:39.094, Speaker A: Why not faster? So this linear assumption is almost like here. If a k is some bounded random variable, then the noise that's generated by this is like some b times x k. So I'm allowing something like that here. But if it is faster, if it's quadratic, then the noise can kind of shadow out the main entrance. So I want to. So usually in hg one, people will pretend this is a constant. I'm relaxing it to be linear.
00:17:39.094 - 00:18:07.248, Speaker A: And okay, but the honest reason why I know people do this is because this is how it shows up in just works. And this norm has to be the same. Very good question. So, in finite dimensions, all norms are equivalent, which means you can go from one norm to another. You'll introduce just a constant, but the contractions under different norms are not equivalent. If you change the norm here, you should introduce a constant, and here and here. And those constants together may go above one, and you may not have a contraction.
00:18:07.248 - 00:18:39.672, Speaker A: So when you define a contraction, the norm is very, very important. But here, since there's some big constant here, you can work in whichever norm you want and absorb the other things in this constant. But still, your question is valid, because if I start jumping around between the norms, the constants I get have dimensionality dependence here. I think for the purpose of the talk, I'm just assuming they're the same constants. Okay, good. So now I'm moving into the results in the first part of the talk on mean square bounds. So I'll just jump right into the results.
00:18:39.672 - 00:19:14.326, Speaker A: So, at first, I focus on the case where the step size is a constant, and this is all same. So there is some condition, I mean, my results, you should all think of these as like simplified forms of the theorems. I'm hiding so many things under the carpet. All the constants are not really constants. They depend on certain problem parameters and the dependencies explicitly presented in the paper. But if the step size is small enough, essentially what we have is that the mean square error converges to zero. Forget about this log term for the time being.
00:19:14.326 - 00:19:30.014, Speaker A: So what you have is something less than one power k. So it's going to zero geometrically fast. And there's a second part. There's a constant. Again, forget about the log term for the time being. And this constant essentially depends on my step size. So the picture is basically that I start somewhere far away, very quickly, geometrically fast.
00:19:30.014 - 00:19:53.512, Speaker A: I'm approaching x star after some point. I'm not making any progress. I'm just wandering, wandering around. And this is inevitable whenever there's a constant step size, because this noise is not letting me converge anywhere. And the log alpha terms show up mainly because I'm assuming markovian noise. If I had iod noise, these logs would not be there. Because I have geometric mixing, I have these log terms.
00:19:53.512 - 00:20:15.038, Speaker A: And by the way, geometric mixing can be relaxed a little bit. You need to have fast enough mixing. It can be polynomial of large enough power. I forget if it is two or three, and then these would change accordingly. So only in this theorem, I'll highlight some dependencies I'm hiding. The initial condition typically shows up here. And usually people call this as an optimization error, at least in optimization world.
00:20:15.038 - 00:20:43.696, Speaker A: And this is statistical error. I like to call it bias and variance. So, this term shows that if you did not have noise at all, this would be your rate, which is geometric, which we knew from Banach's theorem. And because of noise, you are getting this rate. Essentially, this theorem is saying you don't even have convergence because of this additional alpha terminal. And I also want to highlight that our result is true for any norm. And certain factors that depend on the norm are hiding in my c one, c two, c three.
00:20:43.696 - 00:21:07.244, Speaker A: And I want to show what happens in the special case of l infinity norm contractions. Because among all lp norms, these are the hardest to analyze. Once you understand the behavior for l infinity norms, generalizing it to all the norms was pretty straightforward, at least in our approach. So, suppose you had l infinite norm contraction. Uh, then this would be one minus gamma over two. Gamma is my contraction factor. And most importantly, this is going to be log d.
00:21:07.244 - 00:21:34.494, Speaker A: If you had two norm contraction, there wouldn't be any dimensionality dependence here. But if you have infinity norm contraction, there's a logarithmic dependence in dimensionality. And we believe this is not an artifact of our proof. I think it's, uh, fundamental, because infinity norm is like Max, and these are all random. Max of d Gaussians is like log D. So we should expect a log d somewhere. Yes, yes.
00:21:34.494 - 00:22:03.106, Speaker A: Mixing. Yeah, I mean, you can go from one norm to the other using a constant. So we have geometric machine for all the cases. Yeah, for all norms, we get this result. But isn't like in the RL, at least like you don't need mixing, you don't need to have mixed properties. If the gamma less than. What do you mean? If you have a Markov chain, you need some handle on that.
00:22:03.106 - 00:22:36.034, Speaker A: There are some papers where people pretend the markup noise is id noise when you are ignoring it, because gamma is less than one. So it's. Oh, that's what you mean. I see. Okay, so here, I'm not even thinking of that here. Basically, I'm ensuring that my f is close to f bar. So I don't even have the discounted setup here that comes.
00:22:36.034 - 00:22:46.474, Speaker A: No, no, I mean later we're going to use it for discounted reward, but we use the geometric mixing to essentially argue that this f bar is close to f. Yeah.
00:22:48.474 - 00:23:14.334, Speaker B: Maybe what you're thinking about is that people have collect the examples in such a way that the mixing doesn't even apply. But if you just have passive data collection and you just have this data stream, I know it comes from the Markov chain. I don't see how you can get away without some sort of mixing.
00:23:15.894 - 00:23:31.014, Speaker A: If you have discounted, I understand that you don't care about steady state, you only care about one or more horizon. But even within that horizon, the samples that you get needs to be somehow close to the stationary distribution or whatever distribution you want to be. And mixing is useful. But isn't this option to just have.
00:23:31.054 - 00:23:31.834, Speaker B: Infinite.
00:23:33.574 - 00:23:42.114, Speaker A: Convergence converts to stationary? This may have impact.
00:23:44.854 - 00:23:57.194, Speaker B: I mean, if you only care about the asymptote, then you're not going to see mixing. I mean like the mixing constants allow them to appear in the acid.
00:24:02.534 - 00:24:43.504, Speaker A: In the infinite infinity contraction case, this is the only dependency. And the c one's dependence on the initial condition, pretty much because it's square here. But like I still want, like maybe we can discuss this offline. But I'm not sure if you need like convergence overlap between. So I'm actually finite states. So all these are essentially the same finite state markup chains. If you have existence of stationary distribution, pretty much you'll get jumped in the same.
00:24:43.504 - 00:25:02.220, Speaker A: Okay, maybe we should. So one can use even this to get sample complexity. So if I say that my mean square error. Oh yeah. So if you have a sort of.
00:25:02.252 - 00:25:12.264, Speaker B: Non isy terror in the object, and you know, if the error is somehow going to zero as this k goes to infinity, do we get a similar result?
00:25:14.004 - 00:25:15.772, Speaker A: I guess my question is how fast.
00:25:15.908 - 00:25:18.116, Speaker B: You know, does that need to go to zero?
00:25:18.300 - 00:25:38.170, Speaker A: I see. I think we can, we haven't thought about it, but essentially this marker reality is going to give some terms like that. Here we are assuming geometric, but geometric is not required. If you have like, I don't know the exact dependence, but one over k, square of cube or four, something like that, then I think it should not matter. That's my gut reaction is the classic.
00:25:38.202 - 00:25:39.218, Speaker B: Error that goes with.
00:25:39.266 - 00:25:43.170, Speaker A: Yes, that's my gut reaction, but I'm not 100% sure. We have to look at the details.
00:25:43.242 - 00:25:43.610, Speaker B: Thank you.
00:25:43.642 - 00:26:08.064, Speaker A: Thank you. So I'm saying that suppose someone tells me that I want to give them a guarantee that this error is less than epsilon. Then what I would do is I would put my step size small enough so that this is like epsilon over two. And I would wait till this becomes epsilon over two. And that gives me my sample complexity. And if you do the calculation, you'll get a sample complexity of o of one over epsilon squared. Tilde just means that I'm hiding log factors.
00:26:08.064 - 00:26:46.248, Speaker A: Um, so what this says is that by picking steps small enough, I can make error as small as I want. But if I run it for this many iterations, I'll get error epsilon. But if I run it longer, I'm not going to make any improvement because in constant step size I can't make an improvement. So then if I want asymptotic convergence, if I, if I want a good rate, but also eventually go to zero, I have to use diminishing step sizes. Or I could decrease step sizes in some adaptive manner. Looking at my iterates, I'll just focus on the diminishing step sizes case. And here I'll focus on the case when step sizes of the form some constant divided by k power and exponent.
00:26:46.248 - 00:27:18.756, Speaker A: And this exponent is supposed to be between zero and one. If it's zero, we get constant step size case from the previous slide. And if it is more than one, turns out you may not even go to x star, because if it is more than one, the total radius, total distance you can travel starting from x zero will be bounded. If your x star is outside, you can never reach it. So we'll only consider the case when this is between zero and one. And I again have both these optimization stochastic terms in the rate earlier, remember the worst term, the second term was a constant. It was not even going to zero.
00:27:18.756 - 00:27:42.220, Speaker A: Now this is going to zero. And I'm only showing the second term, I'm showing the dominant term here. I'm just absorbing everything else into c four. But the main thing is that this log again comes because of geometry, the Markov mixing. If it was IED, this would be a one. But the main thing is that I cannot allow site to be equal to one in this case. But if I want the best rate, I want site to be as large as possible.
00:27:42.220 - 00:28:04.776, Speaker A: So the natural site to work with is one. So it turns out when psi equal to one, that is one over case step sizes step size case, you should be very careful with the constant alpha. In this case, this alpha is almost irrelevant. This alpha is sitting somewhere in the constant. The rate is not affected by alpha. But in the one over case step size case, alpha can affect the rate. Alpha should be large enough, in which case you get what you expect.
00:28:04.776 - 00:28:28.616, Speaker A: You expect. Putting psi equal to one here will be the right rate. That's what you get. But if alpha is not large enough, then the rate can be sacrificed and this phenomena is already well understood in SGd world. So pretty much qualitatively, everything I'm talking about here is known. In the case of SGD, our main contribution is to do this for general infinity norm contactions. Again, this my initial error hiding there.
00:28:28.616 - 00:29:12.194, Speaker A: And in the case of infinity norm contraction, this is my one minus gamma over two, and the c six has this log d here. And one minus gamma dependence looks like this. Um, so this one over k, again, if I ignore the logs, this one over k corresponds to a sample complexity of o tilde of one over epsilon square on the mean error without the square. And the advantage of this, compared to the previous slide, the sample complexity is the same. But the advantage of this is, in the previous slide, I had to tell you epsilon ahead of time, and then you pick a constant substance depending on that. Here I don't have to tell you anything, you just pick a step size that's, that satisfies this condition, one over case steps that satisfies this condition. You keep going after so many iterations, your error is epsilon.
00:29:12.194 - 00:29:55.000, Speaker A: If you run longer, your error guarantee will get better and better. So that's the advantage of using diminishing step sizes, the algorithm. By algorithm, I mean the update rule, which has step size in it, does not depend on, does not depend on your target error episode. Any questions? C two is, it's known, it's actually showing. Yeah, in optimization, people worry about not knowing it. In reinforcement learning, it will just be the risk of factors for. Okay, I'm running out of time, so I think I'll skip this slide, I'll jump right into proof sketch.
00:29:55.000 - 00:30:11.444, Speaker A: Any other questions before that? Yes, what is offers adaptively? Yeah, but we don't explore that. We only do one over here, like Adam Adaga, even SGD world assumptions, it's very messy.
00:30:12.104 - 00:30:13.804, Speaker B: We can chat offline like.
00:30:16.504 - 00:30:44.910, Speaker A: But what I can tell you is that our main contribution is to come up with the reapprop function. So if you figure out how to do a GD for anagram, you can use a reapprop function. You'll get a similar result for the. Maybe there are things that might break down, but this would be my initial reaction. Sanjay is not happy. Okay, good. So Shipra was talking about asymptotic convergence.
00:30:44.910 - 00:31:07.234, Speaker A: So asymptotic convergence of these algorithms was done back in the nineties. And one approach is to use the OdE method. And one can learn about this in the book by boorker. We do not use ODe method, but I use ODe method for inspiration. I basically look at what OD method is doing, and I copy the same step without following OD method. That's my proof technique. And let's first talk about OD mEthod.
00:31:07.234 - 00:31:23.650, Speaker A: So the idea is, you take the stochastic approximation, send this onto the other side, send alpha k onto the other side. You get this. Now squint your eyes, wave your hands. This looks like a derivative. And since we are squinting and waving, there's no noise. So I just get this, and I get this ode. I am squinting and waving hands.
00:31:23.650 - 00:31:53.026, Speaker A: But burka does not do that. He makes this connection very, very formal, and he has a big theorem. What is his theorem? He says that if you can prove that this ode converges to its fixed point, then under appropriate choice of step sizes, this guy also converges to the fixed point. But that's only asymptotic result. So if you want to use his hammer, you have to show this Guy converges to its fixed point. And he did that as well. And usually when you have OD's of this form, dynamical systems, basically to show their convergence, one would use a Liapnow argument.
00:31:53.026 - 00:32:22.372, Speaker A: So the idea is one comes up with a function, and in this case, the right Liapnow function to study the ode is infinity norm square. I'm focusing on the infinity norm case for the proof, just to be specific. And one can show that the time derivative of this. After using chain rule and using the ode update, one can show it has this negative drift form, and then this converges to its fixed point geometrically. It converts to zero geometrically fast, exponentially fast. And worker would say that that implies convergence of this. Done.
00:32:22.372 - 00:32:26.724, Speaker A: But we want to get rate of convergence so we don't follow this approach. Shabbat has a question.
00:32:26.844 - 00:32:32.172, Speaker B: You're sweeping under the radar. M is not refreshable. Yes, I am.
00:32:32.268 - 00:32:47.144, Speaker A: Okay. Yeah, I am, because I'm not going to use this in a proof. This is just for intuition. So I'm sweeping a lot of things under the. Actually, the way worker shows this is also very interesting. He looks at p norms and let's be good. So he does not face the issue.
00:32:47.144 - 00:33:08.820, Speaker A: So what I want is finite and bounds, but I don't use od method, but I want to take inspiration from it. And the main challenge relative to the ode is that ode is nice and clean. I have my update that looks like this. This is same equation as here. I added and subtracted f bar. This term is a good term. This is same as the ode term.
00:33:08.820 - 00:33:29.252, Speaker A: I kind of know how to handle that. Inspired by this and, but here I don't have derivative, I have discretization. I have to handle that error here because of Markov. This is my markavian error. This is where I'm going to exploit geometric mixing. And if I had IId, this error would, oh, it would still be there, but it will at least be mean zero. Now, it's not mean zero, it has a bias.
00:33:29.252 - 00:33:56.408, Speaker A: And of course I have my additive noise error. I'm worried about all the errors, but this is my, this term, I'm happy. And among all these errors that are scary, my good friend is alpha k, which is going to help us by decreasing alpha k over time. I'm going to get handle on these errors. Okay, so let's actually further think about this. What is the ode? Seeing the ode, the Lyapunov function is essentially saying that this ode is kind of trying to minimize this Liapano function. That's one interpretation of Liapnow functions.
00:33:56.408 - 00:34:39.792, Speaker A: So if you have this Liapano function and you follow this ode, you will essentially go to the minimum very nicely. Now, the stochastic approximation wants to do that, but it is jumping dancing around because of discretization and noise. Now the question is, in spite of it, is it going to be ok? And the answer is, it's going to be ok if your m is smooth. This is the same definition of smoothness in optimization. Essentially, this is saying I can truncate the Taylor series at second order with a constant here without the Hessian, and this l is the smoothest factor. If I have something like this, then the errors that are being introduced here can be captured here, and I can handle them, and life will be good. But the only problem is that this infinity norm square is not smooth.
00:34:39.792 - 00:35:00.176, Speaker A: Two norm square is smooth, p norm square is smooth for p between two and infinity. But infinity norm square is smooth. That's the reason I keep saying that infinity norm is the hardest to analyze. And so what I want is, okay, fine, worker came up with a nice liap function. I can't use it. So I'll come up with my only apano function. And whatever I come up with, I want it to be smooth.
00:35:00.176 - 00:35:28.760, Speaker A: And since I know that the correctly Arpano function is this guy, whatever I come up with, I want it to be close to that guy. This is the approximation property. So this is my wish list. If I find a function that satisfies these two, hopefully it might work. I mean, I have to do this approximation in the right way, otherwise it might not work. Suppose I satisfy these two, then essentially I can write a recursion like this, which is reminiscent of our od. This is like my difference of the Lyapunov function, and this is the negative drift.
00:35:28.760 - 00:36:02.954, Speaker A: And there'll be some terms corresponding to this alpha k. And then once I solve this recursion for various values of alpha k, we'll get the theorems that you talked about. Okay, so now the question is, what is this Liapnow function? And it's basically, take my correctly apano function for the ode, and take my favorite smooth function and smooth it out. So this box operator, it will smooth the whole thing. If you have two convex functions and you apply box between them, the whole thing you get will be sorry. If you have two convex functions and one of them is smooth. After you do this, you'll get a convex function that's smooth.
00:36:02.954 - 00:36:31.134, Speaker A: So I can take my favorite smooth functions and then do this. And then my favorite functions are basically p norm squares for p between two and infinity, excluding infinity. And when p, when this function is actually two norm square, this is called moro envelope. More generally, we call it generalized moro envelope. And now, what is this box operator? It's called infamous convolution. It looks like convolution, except that in convolution, you have an integration here, and you have a multiplication here. You replace integration with min multiplication with plus, you get infinite convolution.
00:36:31.134 - 00:36:57.522, Speaker A: Um, so there's this nice, uh, once you think of convolution, you should think of Fourier transforms, this nice duality between functions, their fourier going back and forth between these two spaces. Similarly, there is nice venture duality between functions and the duals. Um, and duals of smooth functions are strongly convex and so on. So there's a very beautiful theory behind it. But all I need is that I apply the smoothing operator, and then I get a nice smooth function. Everything works. I get bounds.
00:36:57.522 - 00:37:30.322, Speaker A: Then I'm going to, I said I use p norm square. I'm going to optimize over p and mu to get the best possible bounds. In fact, optimizing over p gives us the log d dependence for the infinite norm square. Any questions? Okay, I'm missing a little bit of connection, how you use this thing to get the sample complexity. So, what we do is we basically look at the dist of m. We do something like this. Think of proof of optimization of smooth, strongly convex functions.
00:37:30.322 - 00:38:03.808, Speaker A: You look at quadratic x k minus x star square as Lyapuno function. So instead of square, I use this m as a Liapano function. Then I have to use properties of m, I have to use, have to use the smoothness property and the expression for gradient of m. And then I can get something like this. Once I get this, I just solve the question pretty much. That's the reason I was saying that if you have some proof of gradient descent, at least in principle, I believe we should be able to do, we should be able to generalize it to this case once you know what the Liapnow function. This is the problem in understanding infinity norm stochastic approximation.
00:38:03.808 - 00:38:35.156, Speaker A: So in the neurodynamic programming book, the authors basically say that infinity norm, there's only, we don't know the Liapnow function. So basically we find the Liapnow function. So one last word about how to handle the noise. So essentially smoothness is going to be our friend in handling the noise. And whenever I have these errors, this discretization and additive noise errors. So this additive noise error, I'm bounding by xk and then any error of the form XK, I'll use smoothness to get a handle on it. I'll use smoothness to get a handle on this too.
00:38:35.156 - 00:38:57.834, Speaker A: For this, I need something more. If this was IId, its expectation will be zero. And that would have been, that would have made me happy. But this is not IAD, this is markavian. So the expectation of this is not zero. But what I'm going to do is here, I'm going to use Markov chain mixing. So when you actually work within the proof, you'll get a cross term of this form.
00:38:57.834 - 00:39:14.770, Speaker A: If this was IId, you can just simply take the expectation inside. This will become f bar. The whole thing is zero. But when this is not IId, what we do is the problem is the expectation of this is not this. And not only that, this x, k is correlated to YK. So you can't simply pull the expectation inside. So what, what we do is we condition tau time slots away.
00:39:14.770 - 00:39:44.356, Speaker A: Tau is the mixing time. So condition on this, this markup chain has mixed. So these are almost independent. Now you can pull the expectations inside, make this almost zero, and then work with it. And this trick, this is how I'm exploiting mixing. And this trick is, it goes back all the way to Bertzika Sansiculus in their book, where they use this for asymptotic convergence. But more recently it was used by Srikantan ying for finite ending much going way slower than I expected.
00:39:44.356 - 00:40:05.918, Speaker A: But it's okay. I'll try my best to stop at three. I will not have to cover everything. Though. Yeah. No, actually, in general, is everything to infinite states. As long as they're compact and you have fast enough mixing, you don't even need junk.
00:40:05.918 - 00:40:07.586, Speaker A: In fact, we do that in one of our papers.
00:40:07.610 - 00:40:09.814, Speaker B: Not in this way, by the way.
00:40:12.074 - 00:40:52.654, Speaker A: So we need. Actually, when I showed the assumptions, I hid couple assumptions, technical assumptions, on the, under the carpet. I need some lipstick properties on this f and so on. Because I have this f of x, comma y. I want to make sure that the super of this does not go to infinity and things like that over all values of. Yeah, maybe one can relax it, but at least we assume compactness. Do you have anything to say about that? In terms of x for all y, same lipstick constant for all y.
00:40:52.654 - 00:41:11.334, Speaker A: The simplest thing is to take super over y. And then. All right, so I have only 15 minutes. I'll at least try to state my tailbounds. If I don't talk about the proof. The proof is quite interesting. I'm happy to talk to anybody offline.
00:41:11.334 - 00:41:27.286, Speaker A: Okay, so now the question is. So this is my stochastic approximation. We have mean square bounds of this form. And if I want. Now I want tail bounds on this guy. Not happy with mean square bounds. I want to guarantee that my error is less than something.
00:41:27.286 - 00:41:43.150, Speaker A: If I want that tail bounce, I can always happily use Markov inequality. And if I'm interested in probability like this, I can use Markov inequality. This cancels with this. That's why the same thing. And then I get one over z here. But I'm not happy with this. I want exponential things.
00:41:43.150 - 00:42:10.354, Speaker A: So, essentially, what I'm saying is the mean square is bounded by this guy. Now, if I want to give high probability, like 99% probability boundaries, I have to blow it up by something as a function of my blow up factor. How does the tail decay? This is the decay of the tail. This is one over z in this. In this case. But I want exponential tails because those are, like, very fast decaying tails. And this is the question I want to explore in this part of my talk.
00:42:10.354 - 00:42:30.458, Speaker A: Shabba seems not happy. Good. And the same thing can be written slightly differently. Instead of I can call this guy Delta, then this z becomes log one over delta. And that is this. This is the form we'll be working with going forward. Exponential.
00:42:30.458 - 00:42:34.114, Speaker A: Whenever this exponential, if you invert it, you'll get log. That's all there is to it. Yes.
00:42:34.194 - 00:42:45.902, Speaker B: I was just wondering what the bias there, the expectation of the deviation spread itself. It is okay, somewhat like. But I guess, like you fall it.
00:42:45.918 - 00:42:46.474, Speaker A: Into.
00:42:48.574 - 00:42:58.182, Speaker B: Like, normally you study the vision from the mean. The mean itself is okay. After size of one over case, three things into the same.
00:42:58.238 - 00:43:07.438, Speaker A: Yeah, yeah, yeah. I combine the two. Normally, people look at the behavior of the mean and the deviation from the mean. I'm not doing that. I'm just combining everything together. In fact, that simplifies.
00:43:07.566 - 00:43:09.758, Speaker B: Yeah, it's a little bit true.
00:43:09.886 - 00:43:43.658, Speaker A: Yes, yes, yes, yes, yes, yes. What shabbat is saying is that in high dimensional probability, people ask two questions. Like if you look at Van Handel's book in the beginning, he says there are two questions. One is, how does the mean of this guy behave? And the second question is, this guy, without the mean, how far does it deviate from a mean? What I'm saying is, I'm not even making the distinction. I'm just looking at this guy combining them together, like the variance of that. Okay, pictorially same thing. One was, detail looks bad compared to exponential tail.
00:43:43.658 - 00:43:59.626, Speaker A: Nothing much here. The answer is yes. Otherwise I wouldn't be giving this talk. And by the way, this means that if I prove a result like this, this means the sample complexity of this form. I have to parse this. Now, my sample complexity is in terms of two things, epsilon and delta. This is like, this is like pack bounds.
00:43:59.626 - 00:44:35.194, Speaker A: So I want my error to be less than epsilon with probability one minus delta. And then the question is, how many samples do I need for that? If I prove a bound like this, with this particular dependence, I'll get one over Epsilon Square and log one over delta. And that's in some sense the best you can get in this setup. Okay, so I have some slides on related work. I think I'm going to skip this, but one thing I want to say is that if you use constant step size, bad things can happen. It was shown that the stationary distribution is heavy tailed. What this means is that there are large enough moments that depend on the constant.
00:44:35.194 - 00:45:06.354, Speaker A: For a given constant step size, maybe there is like, let's say 100 hundredth moment. It keeps blowing up the ten. And as n goes to infinity, it goes to infinity. So you'll get heavy tail stationary distributions, which means you cannot get exponential, exponential tailbones. I'll skip all of this, and I want to convey to you that this problem is much, much more challenging than the previous one. And I want to show that in the case of approximation. So this is our linear stochastic approximation.
00:45:06.354 - 00:45:29.054, Speaker A: Let's keep it simple. Let's say b zero. Then this is simply x times one plus alpha a. So essentially, all I'm doing is I have a vector. I'm repeatedly multiplying it by a matrix, which is this one plus alpha a. Now what do we know about this matrix? When expectation of a is, that's all we know. One can show that for the right choice of alpha, when that guy is Hurwitz, expectation of this guy is a contraction.
00:45:29.054 - 00:45:53.576, Speaker A: But this guy himself is not a contraction. So what's happening is I have a vector. If I keep applying a contraction, nice things will happen. But I'm applying something, a random matrix, which is in expectation, contraction. But in reality, sometimes it can even be expensive. So, but since in expectation it's contraction overall, you expect good things to happen. But when I say good things to happen, I want a very fine bound on that in terms of exponential deal.
00:45:53.576 - 00:46:15.000, Speaker A: And that turns out to be hard. Showing mean square bounds in the setting is not that hard. We already did it, that's why it's not hard. Um, uh, but tails are heavy. So in constant sepsis case, you can't show exponential tails. And, but there is a recent work that does tail bounce under constant sep size. Uh, they get exponential tails, but they assume that all of these guys are contractions.
00:46:15.000 - 00:46:51.964, Speaker A: So that's a very strong assumption. Oh, for constant substance, you'll have constant gap. But what I mean is that by making the steps small enough, you can make it as small as then. Yeah, so I was saying that about this recent paper, they assume that all of these operators are contractious. By the way, the story I'm telling you about multiplying by contractive versus expensive, that's true. More generally, I think in the linear case, it's very difficult to see then if they don't have that assumption, the real thing, when this expectation is in expectation, it's contractual and otherwise it's not. They only get polynomial things, and that's inevitable.
00:46:51.964 - 00:47:10.916, Speaker A: You cannot get exponential tail because it's heavy. So we get exponential, but we do that with dimension substance. You can't do it with constant substances. And we do it not for linear. This, this work is only for linear. We do it for general. Okay, so this slide is same as before, the assumptions, the setup, I'm going to modify it.
00:47:10.916 - 00:47:28.512, Speaker A: I'm actually going to simplify it a little bit because it's harder problem. I want to simplify it. The first thing is, I'm not going to look at all diminishing step sizes. I only do one over kick. Some of the results in our paper that's yet to come will be for one over k power z also. But we'll just stick with one k. So now I'm throwing away markavian thing.
00:47:28.512 - 00:48:02.252, Speaker A: So Markavian is very important if you want to use this in RL. But that's going to be future work for this work, the paper that's going to come out, we just work with IID process. So with state with stationary distribution mu, and I'm also assuming bonded support. In reality, I have a assumption more general than this. Think of it as some appropriate generalization of iid to martingale. But I'll not get into it right now. But the boundedness, I want to justify it by saying that if you do not have boundedness, if you go back to linear stochastic approximation and think of one dimension, so scalar, then in three iterations, MgF does not exist.
00:48:02.252 - 00:48:17.346, Speaker A: In three iterations it becomes heavy tail. So the bondedness is in some sense essential to get, to get any tails. HD does not even talk about multiplicative loss in general, they only have additive noise. This is.
00:48:17.410 - 00:48:18.374, Speaker B: Yeah, okay.
00:48:20.234 - 00:48:36.250, Speaker A: My own, all my difficulty is with respect to multiplicative noise. Additive noise is lot better to analyze. The multiplicative noise is what makes a challenge. Good. And the contraction property and the noise assumptions are the same. Now let me present the result. Same setup.
00:48:36.250 - 00:49:09.530, Speaker A: I just put one over k here. So what I'm seeing now is that if alpha is large enough, the numerator thingy is large enough. Then I basically give you what I promised, that with probability one minus delta, I have the mean, the square error, no mean square error is less than or equal to one over k. Log one over delta with, with probability one. And this gives the sample complexity that I promised. But there is a catch, you know, there's a catch because I have this bracket, the cases. So it turns out we can show this only for k large enough above this log one over delta.
00:49:09.530 - 00:49:56.780, Speaker A: And below that we only have a terrible bound that is growing in k up to this log one over delta. The question is, why is this bond going up at the beginning? Why do we need, why is our nice one over k bound only after some time? So if I show it to you pictorially, this is what we have in terms of log one over delta. Up to this time, our bound is growing. After that, it's what we expect. And so the intuition for this is that we believe this is inevitable. And the reason is that, remember, I'm applying by this matrix, random matrix that is expansive sometimes, but in expectation it's contracted, which means you need enough samples for averaging to kick in to make the overall thing contraction. And how many samples you need depends on how accurately you want a contraction.
00:49:56.780 - 00:50:28.250, Speaker A: And that depends on log this one over data. So. So because of this, we believe that this initial increase, maybe this bond can be improved, especially it can be, there is a jump here, it can possibly be improved so that there's no jump, but the increase we believe is inevitable. And Martin did a simulation run of 5000 runs of scalar case contraction. But like the random thing can be expensive. And then we see that initially it goes up and later it goes down. Yes.
00:50:28.402 - 00:50:31.454, Speaker B: What's the exact f operator in this particular.
00:50:32.194 - 00:50:55.566, Speaker A: What's the exact f operator? Yeah, it can be anything. It just has to satisfy. In all my talk, my f is an arbitrary operator that satisfies this. So you can think of the three examples I gave you, either linear or HGD or the simulation. On the simulation we just did, the scalar. So the linear thing, linear stochastic approximation, where Ak is now a scalar. So he took that Ak to be mean 0.8.
00:50:55.566 - 00:50:58.326, Speaker A: But the random thing can be either less than one of it.
00:50:58.350 - 00:51:05.474, Speaker B: Is it like 0.5 plus gaussian noise times x? That's something like that.
00:51:05.854 - 00:51:22.872, Speaker A: I actually have the exact thing right here. So it's a 0.8, b zero. And I don't know, I don't know the randomness though. But he ensured it goes up. It goes up a one. Yeah.
00:51:22.928 - 00:51:24.804, Speaker B: It has to be bounded noise by black.
00:51:26.104 - 00:51:39.856, Speaker A: Okay, maybe he did burn only where the mean is 0.8, but the above value is by Burnley. I mean, it takes two values, one less than one, other greater than one values, right? Yes.
00:51:39.960 - 00:51:47.160, Speaker B: Does the o law or no. Is that the sort of like one over one minus eigenvalue of the matrix.
00:51:47.192 - 00:51:52.232, Speaker A: If it's a linear case or something like that. I think all of that is being hidden in the scene.
00:51:52.328 - 00:51:52.872, Speaker B: Okay?
00:51:52.968 - 00:52:20.568, Speaker A: Yes, that's being hidden in the sequence. Sure. All of those need to come out. So like I said, this paper is, we have the result, we have the theorem proof, but we are still writing the paper, figuring out the story. I think we want to try and make those dependencies explicit. In the case of units of, Xiv did this in the case of Bellman equation, and then he got one over one minus gamma power five in some context for q learning. And the five is optimal there.
00:52:20.568 - 00:52:23.244, Speaker A: So I don't know what it is in the linear case.
00:52:23.904 - 00:52:28.004, Speaker B: Is there some intuition as to what's happening in the first phase and the second phase?
00:52:30.184 - 00:52:54.250, Speaker A: Yeah, I think it becomes slightly clearer in the proof, which I may not have time for. But essentially what I'm saying is that the increased part, this bound that we get is actually worst case part. So it is possible to improve this, but the increase in is inevitable because think of it this way. So let's say you are multiplying by this Bernoulli A. By Bernoulli, I don't mean Bernoulli. It's not zero one. Let's say it takes 0.5
00:52:54.250 - 00:53:25.864, Speaker A: and two with probability so that the mean is 0.8. Okay, so I'm multiplying by this a. But if you run it for ten iterations, then there is non trivial probability that you just keep going, expand, you just keep getting the two and then you are far away. Right. And here we are interested in the exponential tail. So this tail, the one over delta tail, maybe you have just been, you're just being increasing, but the longer you run it, averaging kicks in, and then you'll start seeing that it goes down.
00:53:27.044 - 00:53:31.344, Speaker B: Yes, yes, yes.
00:53:32.084 - 00:53:35.220, Speaker A: Otherwise it's constant size. Then it's known that there's some moments.
00:53:35.252 - 00:54:00.068, Speaker B: Like you would have some stationary curves. Yeah. Things you shouldn't have. This two phase behavior. The two phase should be kicking in because the step size becomes small enough so that you have enough averaging before you move and know you are mostly moving in the right direction. Beforehand, the step size was slightly large. Okay, I want to point out our.
00:54:00.116 - 00:54:07.904, Speaker A: Algorithm does not depend on Delta X. It's just alpha or k step size. But the bound, the increased thing comes up to log one over delta.
00:54:08.684 - 00:54:38.040, Speaker B: There are some constants related, I don't know, like I'm just trying to understand, like very simple algorithm, this multiplicative update. And like for a while it doesn't even go to zero. Fine. But eventually it does go to zero. What changed between the two things? Like, it's not because know the values are bigger. No, they need to decrease because they eventually go to zero. Right.
00:54:38.040 - 00:54:44.920, Speaker B: So it's only the step size that changed between the time that we started size.
00:54:44.952 - 00:55:14.722, Speaker A: It's more than that because we're interested in the tail probability. So if you look at what is happening when you're multiplying in the Bernoulli case, the bad events, initially the probability of them happening is not, is significant. If I just do two coin, two coin tosses, it's quite likely that both of the times I'm expensive, in which case I'm going up in the first two time steps with quite high probability. I'm interested in the scale probability, the far end of the table.
00:55:14.898 - 00:55:51.664, Speaker B: So yeah, let me raise a question. I said that the only memory the algorithm has like two times that memory, which time step we are at, because the steps are. And that, and the other is like what value do we have? Single real number, there is no other memory, right? And so if you're initializing at some value and we are at the some time step zero, or the only difference is the steps, because we are going to forget everything.
00:55:55.784 - 00:56:32.644, Speaker A: I see what you're saying. So if we think of it as one over case, step size, in the one over case, essentially what we're doing is just running averages. So if you think of the setting of law of large numbers, if you do a running average, Chernobyl bound does not have this behavior, it has just strictly going down. But the difference is here we have the multiplicative form, there it is, additive form. So even in shutdown boundaries, if I just have two samples, I can't, like cheddar bond is going to be pretty bad, right? Once I have enough samples, then it kicks in. And then I know that on average I have gone down. But here, because of the multiplicative thing, in the beginning, when it is not good, it actually goes.
00:56:36.384 - 00:56:38.724, Speaker B: Only because the step size is small enough.
00:56:39.184 - 00:56:40.524, Speaker A: I see, I see.
00:56:41.064 - 00:56:43.936, Speaker B: It starts to go down. Okay, good.
00:56:44.000 - 00:56:58.386, Speaker A: I think. Let me just show you one more thing and then I will stop. So the result we have is actually slightly stronger than what I showed you. This is same theorem as before. I haven't changed anything. Now I'm going to change it. It's the same theorem.
00:56:58.386 - 00:57:23.766, Speaker A: I write it differently. Remember, there's a probability one minus delta, and then I have this inequality. I'm just writing it as probability of this inequality, like greater than or equal to one minus delta. Same thing. But the stronger theorem we have is that this result, and this is for a given kick, anytime. The stronger result is that let's fix a capital k for all time. After capital k, we have this inequality, as long as I add a blow up factor of log K.
00:57:23.766 - 00:57:54.984, Speaker A: Ok, this is called anytime concentration. So let's think of this pictorially. Earlier I said that at any time the error with high probability is less than the dotted curve. Now what I'm saying is let's blow it up by a log factor, log K factor. Then if you start, then starting from this point, the trajectory that I see is with probability one minus delta inside this blue region. Or I could start the blue thing not there, I could start it here. Then the capital k also shows up.
00:57:54.984 - 00:58:06.644, Speaker A: Then I'm in this region by just doing the small block factor. I'll be within this. I think I'll have to stop here. I'll skip the proof sketch, and I'll be happy to answer any other questions.
00:58:14.414 - 00:58:19.194, Speaker B: Yes, for constant step size, you don't get exponential concentration.
00:58:22.574 - 00:58:40.064, Speaker A: You do get polynomial concentration. In fact, I mentioned srikantaning, where they show that these large moments don't exist. They actually obtain the moments. Till some point, you can take Markov inequality. With that, you'll get polynomial bounds. There's also the dermal set, all paper where they get polynomial bounds. But I think it's in similar spirit.
00:58:40.064 - 00:59:03.984, Speaker A: This is for IID, actually, so there's no mixing time. For the second part, I assumed it's IID. So if you go back to mean square and then think of the IID special case, tau is basically zero. So all the log tau terms call it.
00:59:05.004 - 00:59:06.732, Speaker B: Sorry, it's a different reason that.
00:59:06.748 - 00:59:08.304, Speaker A: You have different reason? Yes.
00:59:12.364 - 00:59:16.864, Speaker B: Throughout this analysis, did you assume at any point that there is a unique fixed point for f bar?
00:59:17.284 - 00:59:29.754, Speaker A: Yes. So in the two ways you can think of our results, we are assuming it's a contraction. If it's a contraction, one promises you this, a fixed point. Unique point. Or we assume there's a unique fixed point and we assume it's a pseudo contraction. Either way, we need.
00:59:31.254 - 00:59:35.834, Speaker B: Yes, it's have a valley. And then you could converge to a valley.
00:59:37.574 - 00:59:56.934, Speaker A: Oh, yeah, I'm glad you asked, because we do have a result saying that for the first part on mean square, we generalize it for seminars. So as long as you have a seminar, then the fixed point is actually not a point, it's a subspace. And then we show convergence to that subspace. And we actually use this to study average reward reinforcement learning, because there, the bellman operator is spansome in our context.
01:00:00.634 - 01:00:10.706, Speaker B: So in this two phase, in that setting, when you don't have a unique fixed point, you could be potentially going.
01:00:10.730 - 01:00:11.894, Speaker A: To a different position.
01:00:12.794 - 01:00:13.226, Speaker B: Okay.
01:00:13.250 - 01:00:30.090, Speaker A: And we don't have, in the seminar case, the error that we look at is not x minus x star norm square. We just look at the seminar, which means. So basically one whole subspace. Think of a line. The line is six points. We only look at the distance to the line, so behavior is same within the line. It will be going after infinity.
01:00:30.090 - 01:00:38.314, Speaker A: We don't care. Okay, thank you.
