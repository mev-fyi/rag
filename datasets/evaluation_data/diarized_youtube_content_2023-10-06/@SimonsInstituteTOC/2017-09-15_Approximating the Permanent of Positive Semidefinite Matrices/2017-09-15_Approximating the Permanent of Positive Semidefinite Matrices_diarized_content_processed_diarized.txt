00:00:00.120 - 00:00:29.068, Speaker A: Manari is talking about the approximate permanent of PST. Hi everyone. So, this is joint work with Leonid Gorwitz, Shayanowisan and Amin. So, let me start by defining what the permanent is. I assume most of you know what it is, but let's just go over it. It's just the unsigned determinant. So, in the determinant, you take every diagonal of a matrix, every permuted diagonal, and then you sum these up according to certain signs.
00:00:29.068 - 00:00:58.824, Speaker A: In the permanent, you just don't use the signs. So here's a two by two example. For this two by two example, the determinant is just ad minus Bc. The permanent is just the unsigned version, ad plus Bc. Despite this superficial similarity, the permanent is very hard to compute. So it was shown that by valiant that it's sharply hard to compute the permanent event for zero one matrices. They correspond to the number of matchings in a bipartite graph.
00:00:58.824 - 00:02:00.884, Speaker A: It was shown by Scott Aronson that it's even hard to compute the sign of the permanent when you're dealing with general matrices that can have positive or negative entries. And then it was also shown by a simple argument by Greer and Schaffer that it's sharply hard to compute the permanent for positive semi definite matrices, which is the subject of this talk. And the reason is simply because the permanent is a low degree polynomial. By low degree, I mean degree n. And if you are able to compute it over somewhat open set, then you can use polynomial interpolation to basically evaluate it at every point in the space. Okay, so the permanent shop you have to compute. But what about approximations? Well, there is this general result of Gorwitz that you can approximate the permanent of any matrix within an additive error of epsilon times the spectral norm of m to the n.
00:02:00.884 - 00:02:43.764, Speaker A: This is not that. So you should note that this thing without the epsilon is an upper bound, is always an upper bound on the permanent. So this might not give you anything, basically in terms of multiplicative approximations in most cases. But we are interested in regimes where we can do multiplicative approximation. Okay? So, because finding the sign of the permanent is sharply hard, we should restrict ourselves to classes where possibly the permanent is positive. Okay, so the trivial class is the class of matrices with non negative entries. Um, it's very trivial to see that the permanent is positive.
00:02:43.764 - 00:03:47.770, Speaker A: In that case, uh, we already have a very, uh, very great work by Jerome Sinclair and Vigoda that, uh, basically gives an FP ras for approximating the permanent in this case. So, one plus epsilon approximation, and the running time is just polymerized nn one over epsilon. Uh, there is also a deterministic approximation, and the best we know is, uh, a constant, slightly better than two, but roughly to do the n approximation due to Gorwitz and Samartnytsky. But there is this other class which hasn't been studied as much, which is the class of positive semi definite matrices. So these matrices aren't, it's not trivial to see that the permanent is positive in this case anymore, but this is still true that for a PST matrix, the permanent is positive. I'm going to give you one and a half proofs of this fact. So prior to our work, the best results were basically a deterministic and factorial approximation to the PST permanent.
00:03:47.770 - 00:04:36.140, Speaker A: And that's a very simple approximation. You just take the main diagonal and multiply all of the entries, and that gives you an n factorial approximate approximation. You can slightly improve this by considering by partitioning your matrix into blocks of size k and then multiplying the permanence of the blocks on the main diagonal. But still, the approximation you would get wouldn't be simply exponential, it would be n or omega or some poly of n to the some poly of nice. So the main result we have is that we can actually do a simply exponential approximation of the permanent in this case. So the constant is e to the gamma plus one. Gamma is the Euler's constant.
00:04:36.140 - 00:05:20.366, Speaker A: It's roughly this constant. The constant is not that important, it's just that it's simply exponential. There is no dependency on n in the base of the exponential. Okay, so here is the first half proof of why the PSD permanent is positive. So the proof goes via these things called complex Gaussians. So if you've never worked with them, they're very similar to normal Gaussians, except that they are complex valued. So a single complex valued random variable, I call it the standard complex caution, if it's real and imaginary parts are independent cautions, each with variance one half.
00:05:20.366 - 00:06:13.274, Speaker A: Okay, so the total variance is one, but each part has variance one half, and this is the probability distribution of it. Now, if you have a number of these that are independent of each other, I call that a multivariate standard question. Okay, so very similar to the real questions that everybody knows and loves. Um, so more complicated cautions, you can get them as linear transformations of these. Okay, so if you apply a complex matrix c to these standard complex cautions, you would get a complex caution, uh, which is, uh, uh, which, which is sometimes called a circularly symmetric, uh, complex. Caution. I'm not going to go into the details of what that means, but, but these are the Gaussians that we are going to be dealing with.
00:06:13.274 - 00:07:24.254, Speaker A: So just linear transformations of the standard complex Gaussian. So when you're dealing with complex valued random variables, when you're defining the covariance matrix, you can't basically take the expectation of the square anymore. So if g is distributed according to a standard normal sigma, this doesn't mean that sigma is the expectation of g squared or gg transpose. What this means is that sigma is the expectation of gg dagger. So you take the complex conjugate and then do the transpose. Just want to point that out. So just to make sure that everybody is with me, what is, so if you have a single univariate complex, caution, what is the expectation of this thing? So g is a single univariate, caution.
00:07:24.254 - 00:08:00.254, Speaker A: Nope, it's not one. It's zero. Okay. The reason is that if you take the square of a complex, caution, it's still circularly symmetric with respect to the origin. So its expectation is zero. The thing that's not zero is the expectation of the norm. All right, now here's the formula that relates the complex questions to the permanence of PST matrices.
00:08:00.254 - 00:09:35.754, Speaker A: You just take the permanent of the covariance matrix and that's going to give you the expectation of this thing. So you multiply all of the norms of the individual coordinates squared, and that's going to be the expectation you have. Okay? Now why do we even care about this? Well, if you, well, it's a simple observation that if you take higher powers of these things, higher powers of these norms, you can reduce it basically to this simple case, okay? So if, if you want to compute, for example, the expectation of g one to the power two to two k, you can treat, uh, you can treat the different copies of g one as separate components of a multivariate gaussian, right? And write this as the expectation of g one one squared up to g one k squared, where the different copies of g one are jointly gaussian, but they have a rank one covariance matrix. Basically they are always equal to each other. So this expectation captures basically all monomials of the squares of the norms. So you can do these expectations with these things. Okay, so that was a half proof, because I didn't prove this to you.
00:09:35.754 - 00:10:03.444, Speaker A: But note that the quantity inside the expectation is always non negative. Therefore the permanent is always non negative. So that's the half proof. But here's the full proof. It's a different method, and I'm going to use the components of this proof. So I have to give it here. So given a matrix M, you can define this thing called the sure power matrix of M.
00:10:03.444 - 00:10:59.396, Speaker A: So if m is n by n, the sure power is going to be an n factorial by n factorial matrix. So the rows and columns are indexed by permutations. Okay? So the row and column indexed by permutations sigma and ta is just, you take the permutation resulting from, you take the main, you take a permuted diagonal of m and multiply the entries corresponding to those permutations. Definition here. Now the thing is that this matrix that I described is a minor of some high power tensor of m. So if you don't restrict sigma and ta to be permutations, if you just let them be any function from n to n, then you get the m tensor with itself n times. So this is just a minor of that matrix.
00:10:59.396 - 00:11:38.814, Speaker A: Now if m is psd, this thing is pst, and therefore the sure power is also pst. Now here's the second observation. The permanent is an eigenvalue of this sure power matrix. If you sum each row or each column of this matrix, you're going to get the permanent. So if you multiply the true power matrix by the all ones vector, you're going to get the permanent times the all ones vector. So therefore this shows that if the matrix m is positive semidefinite, then the permanent is non negative. Okay, so that's a foolproof.
00:11:38.814 - 00:12:33.900, Speaker A: All right, but this actually proves something slightly more. It also shows some monotonicity. So what that means is that if you have two matrices m one and m two, such that m one is bigger than m two in the loner order, then the permanent of m one must be bigger than the permanent of m two. Again, the proof is exactly similar. Now that we have this monotonicity property, the thing we can do to approximate the permanent is to sandwich our matrix between two other matrices whose permanents are easy to compute. Okay, so formally we are going to find an upper bound matrix d and a lower bound matrix vv dagger, uh, that, that uh, sandwich our matrix m. And then, uh, either, and then, uh, we can use either of their, uh, permanents as an approximation to, to the permanent of our matrix.
00:12:33.900 - 00:13:52.984, Speaker A: Okay? So what we formally show is that you can take this upper bound to be a diagonal matrix and you can take the lower bound to be a rank one matrix. And uh, you can make sure that the ratio between the permanent of the upper bound, and the lower bound is always bounded by the factor I mentioned before, the simply exponential factor. Now, it's clear that if you have a diagonal matrix, you can always compute its permanent because we just multiply the diagonals. But it's also not that hard to see that if you have a rank one matrix, you can still compute its permanent. If your rank one matrix was, if v was just the all ones vector, then the permanent of vv dagger would just be n factorial, right? Because you would get the all ones matrix. Right. Now if you multiply any row or column of v by some number, any entry of v or we, which corresponds to a row and a column of vv dagger by some number c, then what happens to the permanent is that the permanent gets multiplied by c squared.
00:13:52.984 - 00:15:06.508, Speaker A: So this means that the permanent of vv dagger for a general v is just the product of the entries of v squared times n factorial. So you can still compute this very easily. Okay, all right. Um, now if I promise you that there is such a sandwich, then how do you, how do you find one? Okay, if I promise you that there is always a d and v v dagger such that their perimeter are bounded from, are upper bounded from each other, how do you, how do you find the number to output as the per, as an approximation to the permanent event of m? Can you find the best D or the best vv dagger? Okay, so I don't know whether we can find the best vv dagger, but we can find the best D. Okay, so you ideally want to find the best upper bound on the permanent using diagonal matrices. So you want to solve this program, the infinite permanent of D such that D is bigger than M. And there is a not very hard transformation that makes, that transforms this into a convex program.
00:15:06.508 - 00:16:03.164, Speaker A: Okay, you have to work with the inverse of the matrix t, and then your objective becomes a convex function and you can minimize it once you take the log. As I said before, I don't know of any such convex program that gives you the best rank one matrix, and that seems to be connected to polynomial optimization over the sphere. Okay, so next I'm going to give you a sketch of the, basically the proof that such two matrices exist. So again, using the same renormalizing trick, you can always multiply a row or a column by a constant. And you know exactly what happens to the permanent, and you exactly know what happens to the relaxation. You can always make sure that the optimum solution is just the identity matrix. Okay, it's just a normalization.
00:16:03.164 - 00:17:01.080, Speaker A: So if you're if you normalize in this way, then duality of the convex program that I mentioned. I don't expect you to see this immediately, but trust me that what the duality gives you is a PST matrix b. This is the dual variable to the constraint that these loner bigger than your matrix such that the diagonal of b is one. Okay, this is the diagonal. This is because the gradient of the log permanent is basically the identity at our optimal point. And we have this complementary slackness condition that, that this lack in our inequality I minus m or d minus m is orthogonal to the, to this matrix p. So their product is zero.
00:17:01.080 - 00:17:34.234, Speaker A: These are both PST matrices. Every entry on the main diagonal is one. So the product of these two matrices is exactly zero, and they're both PST matrices, so their images are orthogonal to each other. You can write this as the b being n times b. Okay? Now any b. So these two properties on b have a name. A PST matrix whose diagonals are all one, is just called the correlation matrix.
00:17:34.234 - 00:18:20.784, Speaker A: Now if you take the orthogonal projector onto the image of b, okay, that is going to be a lower bound for our matrix m. Why is that? The reason is this equality. If you take anything in the image of b, then our matrix m acts as identity on it. So b equals mb just means that anything in the image of b is preserved under the operation of it m. Right. And outside of the image of b, the project, the orthogonal projector is just acting as zero. Therefore our matrix m is bigger than the orthogonal projector p.
00:18:20.784 - 00:19:07.904, Speaker A: So that's basically where the lower bond is coming from. So what we have to prove is basically an analog of the van der Waden conjecture for PST matrices. This analog, I'm going to read it to you. If you start with a correlation matrix b and you take the orthogonal projector onto the image of it p, then all I want to show is that the permanent of p is bigger than some exponentially small number. For those of you, yeah. So for those of you who know van der Wahard and conjecture, this is basically the equivalent of w stochastic matrices. No, no, no, I mean, it's just an analog.
00:19:07.904 - 00:20:08.196, Speaker A: Okay, so how are we going to prove that the permanent of this complicated projector is big? Well, we're going to, again, as I mentioned before, lower bound it by a rank one matrix. Okay, so we are just going to take some vector in the image of v and we make sure that, that it's a unit vector. Then vv dagger is a lower bound for the orthogonal projector. So it's a, this vv dagger, this operator is just projection onto the direction of v. It's definitely smaller than the projection onto all of the image of b. And I'm going to show that there is, you can always choose a unit vector v such that this permanent is large. Now how are we going to generate this vector v? As I mentioned before, we can't hope to find the best v.
00:20:08.196 - 00:20:58.088, Speaker A: We are going to choose it randomly. So b being a correlation matrix, simply means that it's the gram matrix of a bunch of unit vectors. Okay, so you have unit vectors u one through u n, and the entry ij of v is just the dot product of ui with uj. Okay, these being unit vectors is simply equivalent to b having the, having ones on the diagonal. Okay, now one way to generate something in the image of b is to take some vector g and project every one of the uis onto g and treat that as a, as the coordinates of a vector. Okay? So that's going to be something in the image of b. I have to normalize it, because as I said before, I have to make sure that this vector that I'm picking is a unit vector.
00:20:58.088 - 00:21:41.376, Speaker A: Otherwise I don't get my inequalities. So I'm going to pick a direction g and project everything onto g and then normalize the resulting vector. I have to show that I can choose this g so that the sphere experiment is large. And surprise, surprise, we are. Oh no. Well, we are going to pick that g to be a complex caution, but that comes a bit later. So if you translate what the permanent of the resulting vector is, and what's showing that permanent is at least exponentially small number means, uh, it becomes a probabilistic statement.
00:21:41.376 - 00:22:51.062, Speaker A: Okay, so uh, so take u to be a random vector. Okay, so it's, uh, you can think of it as a random sample from our u one through u n. Okay, now for, for this random vector, I'm going to define this geometric mean arithmetic mean ratio, where the numerator is just the geometric mean of the norm squared, the denominator is just the arithmetic mean of the norm squared. Okay, so if u is a random unit vector, this ratio is always one, right? Because the norm squared is a constant function and the geometric mean is equal to the arithmetic mean. But if u wasn't a random vector, this would always be smaller than one. Okay, now my, my direction g has to have this property that when I project all of my vectors onto g, the gma m ratio doesn't become too small. And this is a statement that's independent of the dimension, the number of vectors, everything.
00:22:51.062 - 00:23:47.434, Speaker A: Okay, so here is a self contained statement of it. So you have a unit vector u, which is a random unit vector. You can think of it as a discrete distribution on the unit sphere, but doesn't have to be discrete. Okay, you want to find direction g such that when you project your u onto g, the gma m ratio is lower, bounded by some constant, and that constant is going to be e to the minus gamma. Now, once you have this statement, everything becomes very clear. What you do is you pick g to be a random complex complex Gaussian. Okay? So if you pick your random complex Gaussian, then there must be some g such that the gmam ratio after the gmam ratio is bigger than the expectation of the numerator divided by the expectation of the denominator, because these are positive numbers.
00:23:47.434 - 00:24:28.212, Speaker A: And then you can use Jensen's inequality to take the expectation here to the exponent. Right? Okay, now we have expression in terms of the norms of the orthogonal projections of our random vector u onto our random direction g. Yeah, sorry, I have a very, like, super basic question. So you're picking g to be a complex Gaussian. So g is chosen. Then you're saying there exists a g. What does that mean? So, okay, so the probability of this thing being bigger than this thing is bigger than zero.
00:24:28.212 - 00:25:08.136, Speaker A: That's what it means, the notation. Yeah, yeah. So these expectations here are over g, and the expectations inside are over u. But I can move all of the expectations inside and take over g and u both together. Now, the thing is, if you look at the projection of g onto the projection of u onto a random gaussian vector g, that's just a single univariate random Gaussian. So g dagger u is just a single univariate random Gaussian. I can compute the expectation of it.
00:25:08.136 - 00:25:38.836, Speaker A: So for every fixed u, this is a single univariate quotient. I can compute the expectation of the log and the expectation of the denominator, and I get minus gamma and one, and that's it. I did this. So let me conclude here. So what I showed you is a simply exponential approximation for the permanent of PST matrices. One thing that I didn't show you is that our analysis is tight. There is no way to.
00:25:38.836 - 00:26:28.514, Speaker A: This algorithm doesn't give you any better approximation. So the one way to get better approximations might be to improve the set of matrices with which you are sandwiching your input matrix. So we were dealing with diagonal matrices and rank one matrices perhaps you can do block diagonal and then higher rank matrix. Okay? And I'm hoping that maybe some techniques from hierarchies might be useful here, because it's not even clear how you can find the best diagonal matrix or the best high rank matrix here. So this is a vague idea, but let me, do I have time? No, no. Okay. All right.
00:26:28.514 - 00:27:21.872, Speaker A: There are other ways of getting better approx. Potentially getting better approximations, but the ultimate goal is to get a one plus epsilon approximation. So for all of these problems, for approximating the permanent of PST matrices or positive matrices or any of those things, the kind of approximations that you are hoping for are either exponential, or if you go slightly below exponential, then it automatically gives you a one plus epsilon approximation. What I mean by that is, let's say if you have a two to the n to the 0.99 approximation, that automatically gives you a one plus epsilon approximation. Okay? So as soon as you break this barrier, you're in the one plus epsilon approximation regime and perhaps one natural area. Where we could use ideas to get one plus epsilon approximation is the design of Markov chain Monte Carlos.
00:27:21.872 - 00:28:40.254, Speaker A: But I currently don't know how to do this. Is there a special case of PSD matrices where permanent has a natural meaning, like a complicated meaning? Natural meaning. So the permanence of PSD matrices, I didn't mention this, but they also appear as probabilities of certain events in quantum optics. I myself don't know much about that, just know that they are related. What breaks down when you go to, sorry, what breaks down when you go to compute the so complex quaternions? Yeah, I don't know because I haven't thought about this carefully, but I mean, you don't have commutativity, right? So even if you're taking the norms of the squares, it matters, right? Right. Yeah.
