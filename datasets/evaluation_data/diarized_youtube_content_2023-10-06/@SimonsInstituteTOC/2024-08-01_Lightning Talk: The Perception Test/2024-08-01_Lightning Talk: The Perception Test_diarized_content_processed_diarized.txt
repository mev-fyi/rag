00:00:00.240 - 00:00:43.784, Speaker A: So this is a brief overview of our perception test project, where we aim to measure the perception capabilities of our AI models. So in our team in DeepMind, we are interested in designing perception models. So by perception here, I mean the process is involved in receiving and interpreting data from sensors in order to understand the environment. So if you take this image, I hope people can see it. Initially you'd say this is an image of a garden with some people cycling in the garden. However, then your priors kick in about the physical world and you realize there's something off about the picture because you see that one person is upside down. And then we actually realize nobody's cycling.
00:00:43.784 - 00:01:35.998, Speaker A: Here there are two people just laying on the grass. However, when we ask our models, they feel completely on this task. So our goal is really to design models that achieve human level scene understanding. And the prerequisite for achieving such a goal is to have benchmarks, one or more benchmarks, to measure progress and guide research. And we think that this is an easy thing to get, because when we check, for example, in the community, there are lots and lots of data sets, and there are lots of benchmarks out there. And this is just a brief checking online on paper suite code, which is an open platform in the community that indexes datasets. So there are close to 5000 data sets on perception related modalities.
00:01:35.998 - 00:02:17.134, Speaker A: So we can see 2700 data sets, image related, 800 something video and so on. And the number keeps increasing. Like 500 new data sets have been added in the past six months. However, although these datasets have driven a lot of progress in the deep learning community, they also have significant shortcomings when it comes to actually evaluating perception models. So models that can do more than just image classification or object detection. So we have this very high fragmentation across tasks and modalities. And this is because most of the existing benchmarks have been designed for these specialized models.
00:02:17.134 - 00:03:42.300, Speaker A: So, you know, object classification, object detection, image segmentation and so on. And we could think that this is fine, we can just pull together all the benchmarks and that way we can evaluate actually a full perception model. However, that would result in a very expensive evaluation because, yeah, put together 5000 data sets, that is very expensive, expensive to evaluate on, but we would also end up with a very redundant coverage of some of the areas. And most of the existing benchmarks actually focus on semantics. So recognizing objects, recognizing actions, whereas other very important areas are completely missed, like memory related capabilities or understanding physics, abstract patterns, there's nothing on that counterfactual reasoning, kind of zero datasets there are some attempts, though, using synthetic data, for example, for cleverer cater. However, people developing models, they always say, oh, there's too big this shift between the training data and the test data, so we cannot really interpret the results from that. So we said, okay, let's design a benchmark that uses real world videos where we can test this kind of capabilities that we care about.
00:03:42.300 - 00:05:41.180, Speaker A: So we looked around, took inspiration from the psychology community, perception tests that are used for humans. And then also the synthetic data sets, the ones that I mentioned that already were looking into these directions, like testing for counterfactuals, and then other real world data sets that also had some interesting features, like ssv two, where they have adversarial examples. For example, a person tries to pour water in a cup, but instead of doing the task correctly, they spill on the table. We don't see this often in YouTube videos, so that's why we really need to have this kind of data if we want to test properly our models. So in designing our benchmark, we started by listing the capabilities of an ideal perception model, and then we started thinking in what kind of videos and tasks can such capabilities be tested, and then what kind of annotations do we need to collect to test automatically for these capabilities and to do analysis? But kind of the first big question here is how do we probe for actual understanding of visual and audio signals and not language bias? Because most of the current models we know, they have been trained on language first and not vision in the way that humans learn. So our models first learn about, I don't know, turing machine before learning to recognize an apple in an image. And our answer to that was to invent tasks and simple games, design adversarial actions and adversarial configurations of objects to break the language biases, and to make sure that we end up with a balanced data set, where, for example, we have the same question that is applied to very many different videos, to have different answers in the different videos, so that we go past the, I don't know, positivity bias, where every question, all the questions in the data set had yes answer, or something like that.
00:05:41.180 - 00:06:37.168, Speaker A: So, enlisting the capabilities, we thought of, what are the important capabilities for acting in the real world? So either if it's a digital assistant or an actual embodied robot, we think that they need to have good memory, abstraction capabilities, understanding of physics and of semantics. And then we also want to have different types of reasoning. So these were actually introduced in the cleverer data set by Joshua Tenenbaum's team. So we test for descriptive, explanatory, predictive, counterfactual. So in each of these areas, we identified different capabilities and happy to hear about the object permanence. So this was one important capability that we also considered in our benchmark. And then you can see in the other areas the kind of things that we looked at.
00:06:37.168 - 00:07:29.286, Speaker A: And we also included semantics, even though semantics is already covered a lot in existing benchmarks, but we focus on aspects that do not exist in those benchmarks. So, distractors, adversarial actions, and I'll show you some examples. So, of course, there are no videos out there that would allow us to probe for all these capabilities. So then we decided to design the videos ourselves and film them with crowdsourced participants. And by doing so, we also managed to get a diverse and balanced data set in terms of geography, gender, skin tone, and we also have very diverse camera motion. And of course, by doing this process, we also have now ethically sourced data with consenting participants, no privacy concerns. So this data will be available for perpetuity, for research.
00:07:29.286 - 00:08:04.810, Speaker A: So this is open source and anyone can use it. This is to have an image. So these are the people that were involved in our study. So we had more than 100 people involved in filming our videos. And as I mentioned, we designed these video scripts, so very simple situations or games that can be performed by a non professional actor. We shipped some small props, so letters, shapes, and we designed multiple variations for script I. And yeah, we encourage the participants to be creative when filming the videos.
00:08:04.810 - 00:08:34.426, Speaker A: And as mentioned, for privacy reasons, there are no faces, no speech in the videos. We do have audio, so from the object interactions, but no speech. And this is an example of the Cups game variation. But I'll show you the videos. So this is how we test for object permanence. We have the cups game scenario. Basically, you have cups, and then you hide a small object under one of them, and then you shuffle the cups.
00:08:34.426 - 00:09:07.726, Speaker A: So this. So we vary the number of objects, the materials, the camera can be static or moving, the objects can be identical or not, or they can even be transparent. So all these variables, of course, will affect significantly the difficulty of the end task. So we filmed this like, these are the full videos that are filmed. So here they are played in the loop. However, the way we showed this, this is an example of how we actually show this to the model. So this is the prompt.
00:09:07.726 - 00:10:05.920, Speaker A: The person uses objects to play an occlusion game. Where is the hidden object at the end of the game from the person's point of view. So we want to make the question as clear as possible. To reduce ambiguity of viewpoint or other things. Let me play the video with the test. Where's the cup? Where is the hidden object? So we have these three options. Anyone going for a, b? I'll tell you the c.
00:10:05.920 - 00:10:33.190, Speaker A: The correct answer is c. The cap. The hidden object is under the first object from the right. So it's not a trivial task. So actually that's, this is the, oh, sorry. Am I seeing this in the correct. I'm just trying to see if this is mirrored in some way.
00:10:33.190 - 00:11:18.066, Speaker A: Am I seeing this? Okay, I'll figure this out. It doesn't show the same way as on my. Oh, no, sorry. Oh, you want to play? Okay, let's see. It's from the person's point of view. Okay, here, it's here, right. So it's from the person point of view.
00:11:18.066 - 00:11:49.626, Speaker A: Okay. So that's, okay, that's correct. All good. So yeah, this is actually very interesting task is a simple game, but it's very interesting because we can actually probe for object permanence, understanding motion, like understanding, being able to track objects. And of course here is very difficult because you have identical objects. So it's not just simple tracking, it's tracking with identical objects plus solidity. Like you have to understand that the small object that was hidden, when the occluder object gets moved, the hidden object gets moved as well.
00:11:49.626 - 00:12:16.748, Speaker A: So this is a very, very challenging task. Sorry. So example of another task where we have two variations. For example, here we assess understanding of stability is stable configuration. So here we collect both positive and negative example. So you have a person just stacking objects on top of each other. In some case they are, you know, the final configuration is stable.
00:12:16.748 - 00:13:25.210, Speaker A: In some cases it's not. And the way we showed this to the model, we chop the video at the moment where the person is about to place the last on the last object and we ask is the configuration likely to be stable after placing the last object? So also I mentioned that we have adversarial configurations of objects. So you can all see this kind of things popping in the videos, like shoes on the tables and bottles put in weird, weird positions. Here, an example to show you how we test for multimodality to make sure that the model is actually understanding audio in this case. So we have this kind of situation, a person interacts with different objects and with an electric device point an electric device that normally would make sound when, when it's on, like a hair dryer or an electric shaver. However, if you just look at the video, you think that they are on because you see cables there. But actually, when you do get the video with audio.
00:13:25.210 - 00:14:11.460, Speaker A: Okay, so this one does work. This one, it doesn't work, right? So we instructed participants, you know, to place, to, to have adversarial configuration. Like, you see, the cable is there. You'd think that it is connected, but you actually have to listen to the video, to me, to, to be sure that it is, it is actually connected. And talking about adversarial configurations. This is a simple example that I just run with, with Gemini just a few days ago, and just asking the model to describe the actions done by the person in the video. And I've just put in red the things that it gets wrong.
00:14:11.460 - 00:15:04.482, Speaker A: So it's kind of most of it. So basically, you think, okay, this is a very simple thing, right? You just stack three objects, but it really gets wrong, all the spatial relations between the objects. It's possible because the camera angle is a bit unusual, but it really sees objects in the wrong order. Or here, talking about adversarial actions. This is an example, a more semantic example. So it's not an invented situation, an actual just making tea. So now, again, I asked, describing detail, the action done, but it fails completely to notice that the teabag was not put in the cup, that it was next to the cup.
00:15:04.482 - 00:15:38.528, Speaker A: And of course, you think it's a small detail, but actually, in terms of task completion, this is a completely failed task. This tea is not, this is not a t. Even if I ask it explicitly, an action is not executed correctly. Can you spot it? It still fails to give the right answer. And I've tried many, many times, and it just doesn't get it. So I think this does show a strong, very strong bias coming from language that these models have. So we've collected this video.
00:15:38.528 - 00:16:21.038, Speaker A: So we have 11,600 videos in total, up to video with audio and up to 35 seconds long. And then we collected lots and lots of annotations, different layers or different types of annotations to enable analysis and also to enable evaluations that do not depend on language. So we have visual question answering, but we also have object tracks, point tracks, action segments, sound segments, and grounded VKA. So, okay, this is the benchmark in terms of evaluation. So when we released the benchmark, we initially tested the model that we had back then. So, Flamingo and Sevilla, they were pretty close to each other. So random.
00:16:21.038 - 00:16:59.090, Speaker A: Here is 33% on this task. So the multiple choice video QA has 130 unique questions, and then each question is applied to many many videos. So we have about 20k video question pairs. So, yeah, first, Flamingo, Sevilla were tested, were better than random, but still not great. Then GPT, four V and Gemini came around and we saw good improvement. So they are at about 55%. However, when we put them in context, this is human performance, the humans do more than 90%.
00:16:59.090 - 00:17:56.060, Speaker A: So we see that there's still a significant gap there. And because oral questions are tagged with different capabilities and types of reasoning, we can now look in more detail where the models fail. And what we actually notice is that, for example, models like Flamingo and Sevilla tested in zero shot or even eight shot, they're worse than random at counterfactual reasoning. And that's quite surprising. But the reason, we think the reason for this is that when we ask the counterfactual questions, among the different, among the negative options, we also include the visible option in the video. So let's say if the question is like, we have a person writing a word, and we ask if the person had written the letters in reverse order, what would be the second letter? And we put also as a negative option the actual second letter from the, from. From the video.
00:17:56.060 - 00:18:28.158, Speaker A: And the model always chooses that. So the model really fails to do this counterfactual reasoning there. And, yeah, we can go even in more detail. So, yeah, in summary, this is our. This is the benchmark that we proposed with newly filmed videos to also reduce data contamination. So we don't want to have this. You know, we hope that this data is not included in training sets, and it includes both low level and high level tasks with language and non language evaluations.
00:18:28.158 - 00:19:20.646, Speaker A: So maybe, interestingly, or I don't know if it's a feature or a bug, we decided to not attach difficulty levels to tasks and questions because, as mentioned, these models are trained in a very different way. So what is hard for humans might not be hard for models and the other way around. So we decided to not attach, like, to not wait to the final score by some weights. And, yeah, all the videos and data are available. And we actually have a public challenge coming soon, coming at ECCB 2024 in Milan. And we'll have Josh and Abhinav there as speakers and, yeah, of course, limitations. So, because we don't have any faces or there's no social component to this, also, this is a motor free, so there's no active component.
00:19:20.646 - 00:19:40.610, Speaker A: And I think, as Lucy also mentioned, there's no guarantee that if a model succeeds on this test, it has human level perception. Right. However, if it does fail considerably on this test, then it does mean that they need to. There are things that need to be improved. Okay. So I think this is it. This is our team.
00:19:40.610 - 00:19:49.550, Speaker A: Az, who was here, Andrew. All who's also in the room. Yeah, this is it. Thank you.
00:19:55.650 - 00:20:03.910, Speaker B: Thank you. Any questions? Okay, I guess I'll give you. Here we go. I'll give you our next question.
00:20:06.450 - 00:20:24.980, Speaker C: Hi. I just have kind of an overview question. So you mentioned that it clearly struggles with understanding kind of unusual scenes with incorrect actions. What, in your opinion, is missing in these models? Is it a data problem or a structure problem or what?
00:20:25.360 - 00:21:05.730, Speaker A: Both, probably. So definitely for counterfactual, like the adversarial actions that I showed, I think is a data problem. However, when it comes to counterfactual reasoning, for example, I think that there might be something missing in the models because counterfactual does require some additional capability. You need to first understand the scene. So this is just pure perception part, but then you need this ability to imagine this extra thing or this alternate reality. So I think that does require a generative component so that our models do not have now. So I think, yeah, that we do need extra modules in the models.
00:21:05.730 - 00:21:12.490, Speaker A: Thank you.
00:21:16.830 - 00:21:19.550, Speaker C: We'll be back at 130. It's a little bit.
