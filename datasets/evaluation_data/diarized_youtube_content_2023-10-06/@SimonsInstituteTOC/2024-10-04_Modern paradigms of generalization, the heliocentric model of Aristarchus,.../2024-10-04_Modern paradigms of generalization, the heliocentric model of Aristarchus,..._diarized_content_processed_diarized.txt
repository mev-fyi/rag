00:00:00.160 - 00:00:00.876, Speaker A: Hi, Ry.
00:00:03.145 - 00:01:04.765, Speaker B: Welcome everyone, to the Simons Institute and to the boot camp on modern paradigms of generalization. My name is Sampath Kannan. I'm the associate director here at the Simons, and I will see a lot of you if you're here for the program over the entire course of the semester. For those of you who are new to the Simons, a brief word about it. Founded in 2012 by a generous grant from the Simons Foundation, Simons Institute brings together many groups of people for sustained collaborations on a particular topic over a semester, generally in theoretical computer science. Although we have been extending our limits of theoretical computer science to include interdisciplinary things that theory can provide a useful perspective on. And so we've had programs, a lot of programs in quantum physics, quantum computation, computational biology, theoretical computer science and the social sciences, things like that.
00:01:04.765 - 00:02:12.025, Speaker B: Machine learning and theory used to be really closely connected at the origins of these fields back in the 80s and before. And we think that it's profitable for both sides to be reconnected more strongly. And so the Simons has a sustained effort over the next few years in trying to have programs that connect theory and machine learning. So what can we say, rigorously and with guarantees in the field of machine learning, in large language models, things like that? This is one of the first, but we will have three or four programs in the next few years on these top section. Let me start by thanking the organizers of both the program and the bootcamp, who happen to be identical. In this case, chiefly Mattis, who's been the main person who's been communicating with us, but also behind the scenes, I think Po Ling, Andre, Daniel, Peter, who's our local spy on this program, and Tony and Rich as well. We look forward to a great program.
00:02:12.025 - 00:02:50.051, Speaker B: And then let me give you a few ground rules. One important thing, which you may be surprised to hear, is that we do not allow any food or drink in the auditorium. If you brought food and drink by mistake, make this it's okay, but don't do that again. And we also ask that you leave the auditorium at the end of the day immediately and continue your discussions outside to allow the staff to close up and leave not too late. So, because the staff are constantly monitoring all these workshops. So at 5pm or whenever the program ends, please step outside. And if you have to talk, keep talking back.
00:02:50.051 - 00:02:53.055, Speaker B: Have a great program and let me hand off to Max.
00:03:00.755 - 00:03:25.309, Speaker A: Is the volume fine? It's not too loud? Feels a little loud. Okay. Yeah. Thanks everybody for coming to the program and the boot camp and especially with the early morning and the non informative title. So I'll just cut to the chase. The title is actually the outline of what I'm going to tell you about. So I have four goals today.
00:03:25.309 - 00:04:02.345, Speaker A: The first one is I'm going to motivate the program. And I've actually asked them, even though it's an introduction to the program, to include this in the YouTube video because it's sort of call to arms to more than us. And one thing I'll include in that the concrete content. There will be some open problems. And then it might have sounded a little bit too artistic, but I will mention something called the helocentric model of Aristarchus. The heliocentric model, as everybody knows, is about 2,300 years old. And if you trace the origin of science, you can also trace the origin of machine learning.
00:04:02.345 - 00:04:45.755, Speaker A: And then I will have some selfish content which I will discuss a little bit about gradient descent and I will talk about, interestingly, how there's a parallel between gradient descent and actually the scientific method. And then I have some other stories and you'll just have to wait to see what those are. So. Oh, and I've instructed some subset of the audience to heckle if you have not been instructed and you'd like to heckle, that's also optimal. So yeah, so I'd like to acknowledge a few people and point out some unusual things. It's on and off. Okay, so Sampath mentioned the organizers and I'd also like to acknowledge the workshop chairs.
00:04:45.755 - 00:05:15.525, Speaker A: So as everybody knows, the way these things work is the assignments program always have a set of workshops which are pretty exciting. It's also exciting between the workshops, but just to acknowledge all of the chairs, one of which has a visa problem. But still, I will acknowledge all of them. So one is Samori. Hey, there's Samori Andre. We've already called out some third person. And then Fanny, of course, Fanny Yang has a visa problem, so she is not here.
00:05:15.525 - 00:05:44.913, Speaker A: But hello over YouTube. And I'd just like to say a few comments about the Simons Institute because many people are confused about the Simons Institute, especially when they hear about like the Flatiron Institute and other places. So if you. So Jim Simons, many of you know, passed away and he took sort of a. He's a mathematician, but he took sort of an investment fund approach to supporting the sciences. So all of the different Simons things are very different in style. The Flatiron Institute is different than the Simons Institute and things like that.
00:05:44.913 - 00:06:26.225, Speaker A: And yeah, many I have an Anecdote for every person on this list. And if you want to make a guess, one of the people in this list is photographed later in the in the talk. So if you can guess which one appears in a photo in the talk, I will give you, let's say, yeah, some amount of money, let's say 20 bucks. If you can guess correctly who it is, the first person that tells me who it. Okay, so feel free to email me if you have guess. Okay, so yeah, getting to the actual content. So generalization, making it as vague and not falsifiable as possible is just the ability of machine learning methods to perform well outside the data that they saw.
00:06:26.225 - 00:06:57.711, Speaker A: And many of us are familiar with a variety of standard setups. I'm not claiming the one I'm writing here is the standard, but it is one of the ones a lot of us learn in our classes at the beginning. It's called the Model of Statistical Learning Theory. This one posits that the train and test data are drawn iid from the same underlying distribution. And just to compare some pros and cons. It's always easy to beat up on the old idea, but it had a lot of extremely valuable consequences. Of course, one is there were a lot of intuitive concepts that were formalized via this model.
00:06:57.711 - 00:07:28.149, Speaker A: One is of course the notion of capacity control, that if you have a large function class and you have a limited amount of data, you're going to have problems. And this is very nicely formalized even if you don't believe the assumptions. And also many other methods were sort of verified by. A lot of methods were verified or derived via this concept. And I just want to say that no one ever believed this was a valid model. I mean no one believed this. So there's no real point or there's no pride in beating up on the assumptions.
00:07:28.149 - 00:07:48.209, Speaker A: So it was always just a nice way to verify things. So if you open most textbooks, they use spam as the justification of the model. Which of course the first thing a spammer does is see what spam gets through and adjusts their distribution. Right. So it was always known that it's sort of an interesting simplification of what happens in reality. I'll say that now it's sort of. I was maybe too aggressive.
00:07:48.209 - 00:08:13.687, Speaker A: I said blatantly far fetched. I'll just give three examples. Two kind of trivial ones are self driving cars. Your self driving car is trained partially via some simulator and some real roads. And then you drive into the mountains and it blows up or something. That's a distribution Shift generative models are an interesting one. People fairly explicitly feed text into their generative models, which are explicitly outside the training set.
00:08:13.687 - 00:09:03.909, Speaker A: They're actually looking for interesting funny corner cases in the prompts. And for LLMs, I'll list a few open problems, but yeah, I'm just trying to get through this setup pretty fast. Okay, Any questions about this? I didn't offend anybody. Okay, so just to highlight and highlight some of the differences between this standard model, one of the standard models, and let's say an LLM, and maybe point out some explicit open problems and just give credit, one of these open problems was actually provided to me by Peter Bartlett a few years back. So, you know, the basic perspective on an LLM is that it takes this input, just a bunch of unstructured text. That's one of the reasons why it's so successful. It has essentially infinite training data, all human knowledge, and the output is actually formally quite different.
00:09:03.909 - 00:09:06.745, Speaker A: It actually outputs question answer pairs.
00:09:07.165 - 00:09:07.845, Speaker C: Okay.
00:09:07.965 - 00:09:33.255, Speaker A: And I think it's important to think about this as very different. If you speak to. For instance, I've spoken to people at Meta and they've told me that it really does not give good question answer pairs. If you just do the unsupervised training, you really have to do one of these instruction tunings or something. Okay, so there are two reasons this doesn't fit the statistical learning theory model. One is the distributions mismatch, so it's no longer a training and test of the same distribution. And the other one is just the task has a mismatch.
00:09:33.255 - 00:10:12.127, Speaker A: Okay, And I'd just like to point out two very wide open problems here. One is why is this task useful for every subsequent problem? Why is it that for pretty much anything people come up with language, we can just do next token prediction and it's so good for that problem. I mean, you can make your intuition, and if you're really confident in your intuition, then please provide me the theorem. The other one is the difference between GPT and bert. So GPT, just be clear, does next token prediction. And BERT does this thing where it does two forms, causal masking and then binary prediction on sentences. And it's very interesting.
00:10:12.127 - 00:10:40.369, Speaker A: If you look at the BERT paper, it actually attacks the GPT model very explicitly. It uses the word harmful to refer to what GPT does. I mean, that's a quote. And so but you know, you can make your intuition on why GPT. I've heard all sorts of words about GPT, why it's better scaling laws, but for a theorem, I Think it'd be very interesting to say formally why GPT is such a powerful model. Okay, so just two open problems. I actually won't revisit these two open problems, so.
00:10:40.369 - 00:11:21.251, Speaker A: Oh, and I should say you should always ask me for my sources. Sometimes I was unacademically negligent. A lot of the sources for this slide and the next are just from the llama 3 paper. So for those of you who have not checked the Llama papers, Llama is open source, is from Meta Facebook and they are quite thorough so it's a good way to check best practices. You can also read their code. So okay, for instance, I can tell you that Swiglu is used in modern machine learning because you can just read the code to llama3 and you can just see it right there. So okay, so let's try to isolate a subproblem within LLM training which is maybe closer to the Cisco Learning 3 model.
00:11:21.251 - 00:12:13.727, Speaker A: And so now I'm just looking at a sub problem which is I take as input a bunch of unstructured text and my goal is to do this mapping which is context a bunch of words or tokens as input and then make produce a distribution over tokens. This is really just the left hand side is really just GPT. Okay, this is the GPT problem and this still doesn't fit statistical learning theory. Now this is a bit of a trick question in the sense that there are a bunch of X's you could have drawn on the right, but I'm just going to highlight one with distribution mismatch and ok, so I think a lot of you are aware of this. This is open problem number three. So a lot of you I think are aware that for LLM training there's a significant difference in downstream performance based on how you balance your training data. Is everybody aware of this? Like if you crank up Wikipedia you perform better later on non Wikipedia tasks.
00:12:13.727 - 00:13:00.337, Speaker A: Is everybody aware of this magic and in Llama 3 they really doubled down on this one. The difference between Llama 3 and Llama 2 is exactly how they choose that mixture. Is anybody aware of what they did to produce the mixture? Pretty wild. You freeze the validation set and you optimize over the training set. So it's like explicit distribution mismatch. You're optimizing it away. You freeze the validation set and if you want the buzzwords in the paper, what it says is you freeze the validation set and you optimize over the training set balancing so that you maintain a scaling law.
00:13:00.337 - 00:13:40.863, Speaker A: Okay, so feel free to solve this open problem, the other one I'll mention this is just a personal thing because I like optimization. Of course the data is highly non iid, right? You take a document, you cut up into a bunch of prefixes and you predict the next token. So the two adjacent prefixes, they have significant distributional overlap. This is not IID training. So even in the convex case, we replace transformer with the linear predictor. We could not apply a standard sgd. But if you open your textbooks, you can find that there's a trivial way to make SGD work on Markov chains.
00:13:40.863 - 00:14:20.657, Speaker A: The problem is the mixing time. So my claim is that if you plug in the mixing time for any one of these analyses, not only do you get a vacuous bound qualitatively, not quantitatively, but actually I claim that the fact that it's not mixing is fundamental to how LLMs work. So that's my fourth open problem is even in simple cases, produce a optimization analysis for this which does not use mixing time and uses some different concept with a different final theorem. Okay, so does anybody have any questions about these four open problems I just dumped on you? Yeah, which one?
00:14:20.761 - 00:14:22.473, Speaker C: I'm just confused about what your.
00:14:22.609 - 00:14:23.565, Speaker A: The goal is.
00:14:25.585 - 00:14:30.097, Speaker C: With the open problem that you're referring to is all four.
00:14:30.121 - 00:14:35.405, Speaker A: Of these are significantly open. I don't know of anything in any math paper or book that can help scratch these.
00:14:35.785 - 00:14:43.065, Speaker C: What kind of answer? I mean there's, I mean there's lots of work on going through and transfer from auxiliary tests. I mean, is this like an open theoretical.
00:14:43.105 - 00:14:52.005, Speaker A: Oh, do you mean question number. Do you mean question number three? Yeah, theoretical question. What is the theoretical question for number three? For number three or number four?
00:14:53.155 - 00:14:54.015, Speaker C: I didn't.
00:14:54.955 - 00:14:58.947, Speaker A: Okay. Nobody understood any of my open problems. Okay, so that's a good question. Yeah.
00:14:58.971 - 00:15:00.979, Speaker C: What's a concrete open theoretical question?
00:15:01.027 - 00:15:09.495, Speaker A: So I have a good joke here. The number of hands raised right now is the number of people that understood the four questions. So I don't even have to ask awkwardly to raise hands.
00:15:09.915 - 00:15:14.915, Speaker D: I think the problem with number four is, for instance, it's like asking when does non convex optimization work?
00:15:14.955 - 00:15:23.391, Speaker A: It's like you took something that makes the problem track them. No, no, I really mean we can stick to the convex case. But what I mean is this. If you. No, no, I mean like if you.
00:15:23.423 - 00:15:25.911, Speaker D: Say without mixing them, anything is without mixing them.
00:15:25.943 - 00:15:26.391, Speaker A: You know what I mean?
00:15:26.423 - 00:15:30.527, Speaker D: Like if you just sort of state it in the, in the negative, basically anything that doesn't have.
00:15:30.591 - 00:16:18.899, Speaker A: Yes. So let me give a version of number four which is fairly crisp. And I'm like being manipulative because it'll appeal to you. But for instance, in Markov chains there are certain types of Markov chains, like for instance, if you run a Markov chain on a mixture of Gaussians where within, if you're near one of the clusters, you're fast, but moving the transit time is exponentially slow between the clusters. So one thing I think happens in these LLMs is this metastability phenomenon where you actually stay within local to a context and then you have exponential transit times to other regions of the generation space. And so I would be interested in seeing a nice SGD analysis of this flavor for number four where you could remove the transformer or make any assumptions you want to simplify the model. And then by any theorem of this type, I would be pretty happy about that.
00:16:19.027 - 00:16:22.267, Speaker D: You're probably going to prove negative results if all you want is some kind of.
00:16:22.371 - 00:16:24.763, Speaker A: Yeah, I don't care. Math is cool, do whatever you want.
00:16:24.939 - 00:16:27.975, Speaker C: So I was actually asking for three.
00:16:29.595 - 00:16:44.335, Speaker A: Yeah. So OK, I can come up with like 50 if you like. But one of them is that a lot of people like writing papers about scaling laws. So one of them is under what circumstances is it possible to optimize over a training set such that you get a scaling law?
00:16:53.415 - 00:17:05.327, Speaker D: I think maybe like the higher level question, like some kind of question around how do you choose proportions for data mixes or something? Like if you have a couple of fixed data sets and the thing that you have control over is the relative proportions or something.
00:17:05.471 - 00:17:52.459, Speaker A: There are many deadlines coming up, so feel free to answer these however you like. You don't need to credit me, but if you want money for the answer, then you might need to satisfy me. By the way, I believe what you were saying, but even though everyone seems unsatisfied, I actually believe those questions serve their purpose. So back to what I'm trying to do in this or we're trying to do in this program. What I'm trying to do in this talk and what we're trying to do in this program is. So the goal of generalization is to analyze why machine learning works beyond its training set and sort of my opinion on what could happen in this program is to of course take old ideas and sort of figure out what the span of them is. Figure out how to use creative.
00:17:52.459 - 00:18:37.243, Speaker A: Creatively use old ideas. For instance, a lot of people, and just to ping Nati who already asked some questions, there are a lot of times people say the uniform convergence can't prove Something to just use uniform convergence as a tool within a proof you can actually use to prove many things that don't seem to be uniform convergence concepts. So to extend all the ideas and also then work in the orthogonal complement, kind of see where we need new ideas. And that's why, if you've been wondering why the bootcamp looks like it has such a diversity of topics, it's because there are so many ways that we could, we could proceed. And I'll just mention some of the topics here. There's a couple of us talking about classical stuff. There's a lot of stuff on distribution shift and robustness, some stuff on double descent and interpolation related ideas, distribution learning.
00:18:37.243 - 00:19:16.529, Speaker A: And then we have a grab bag of some other topics. So if you're wondering why there's such a diversity, it's basically because there are many directions we could go and it'd be nice to have as many ideas shared as possible. Okay, so we are almost done with section one. Okay, so just to summarize this portion, kind of the main thing I said was that statistical learning theory is this train in test and it was a good idea and it'll continue to be a good idea. It has a lot of nice tools. It verified a lot of intuitions, provided some methods. It's okay that it's impractical.
00:19:16.529 - 00:19:56.155, Speaker A: One point thing to point out is that it ignores many beautiful phenomena that are sort of things we're studying here. And then I also highlighted some open problems in LLMs. Interpret them as you will. And what I'll do next is this science portion where I was trying to understand sort of the assumptions for machine learning that came before statistical learning theory. So any questions so far? I'll tell you one of the other stories. There's actually, there's only one story. I just want to point something out.
00:19:56.155 - 00:20:19.725, Speaker A: So that's the definition of gradient descent. And the loss there is the cross entropy loss. I would really like to know why. In terms of energy usage, 99.9% of all machine learning uses only this loss. I wish I knew why that was true. So people are literally talking about building nuclear reactors just to run this.
00:20:19.725 - 00:20:55.065, Speaker A: I mean, there are serious discussions about this. So somebody could tell me why. That'd be pretty nice. But yeah, this is just other story, part one. Okay, so I was curious what came before statistical learning theory. And I ended up going quite far back. So for anybody who's interested in historical aspects of machine learning and statistics, there's a professor at, I think it was University of Chicago who Has a number of interesting papers on this topic.
00:20:55.065 - 00:21:23.465, Speaker A: He has one about Maxim, he has a book and two papers that are notable I'll mention today. One is about maximum likelihood. If you read what the quote says, he says that even before people could formalize it. It's just sort of a natural idea of hunters and gatherers. It sort of makes sense that you go where you expect things to happen. And I liked that he didn't specify humans in his abstract. I showed ants falling pheromone trails, which also uses the principle of maximum likelihood.
00:21:23.465 - 00:22:17.485, Speaker A: And when I was digging through the literature for this and let's say, the origin of least squares and stuff like this, I kept finding one problem coming back over and over again, which was the study of orbits. So a lot of machine learning and a lot of statistics and a lot of science was actually based on the study of orbits, which. Just a personal comment. It's quite nice because it's just people staring at the sky and either making stories to themselves about deities and things or thinking about why it's true and what's actually going on up there. So I find this quite amazing that people just looked up and then figured all this stuff out. So I'll tell you about four anecdotes and then I'll try to extrapolate from them some concepts about the scientific method and how it relates to statistics. So the first one is Cauchy is typically credited with gradient descent.
00:22:17.485 - 00:22:52.787, Speaker A: In terms of references, it seems he's first Gauss. It seems the record is fairly clear at this point that Gauss is not the first to have come up with least squares. But I will mention his book, which is about 400 pages of orbit calculations using least squares. Then there's Kepler and elliptical orbits and then one of the titles, which is this guy Aristarchus from Greece. So I'll tell you these four anecdotes. So I won't say much about gradient descent. I'll just say why Cauchy did it.
00:22:52.787 - 00:23:25.115, Speaker A: So, two comments here. So the definition of gradient descent I gave before is this one you're all familiar with, or how you write the Pytorch code or whatever you like. But then another very nice way, sort of the mirror descent flavor of writing. Gradient descent is just this trade off between an approximation of the energy you're minimizing and some distance criterion. So fairly explicitly from the definition, gradient descent is not. Gradient descent's job was never to minimize an objective. Its job was always to trade off and balance a norm and an objective.
00:23:25.115 - 00:23:56.591, Speaker A: I'm Just highlighting that gradient sense job was never to minimize a function. It was always to trade off between these two. I did try to look at Cauchy's paper, and I didn't see him explicitly highlight this trade off. But just to point out Cauchy had an equation in six variables that he wanted to find a local minimum for, and he really didn't feel like grinding it. So he came up with gradient descent and he ran a couple iterations by hand. And there's a nice paper by Claude Le Marchal, if you're familiar with the name. He's a convex analyst.
00:23:56.591 - 00:24:27.615, Speaker A: He has a nice paper where he includes footnotes that are the original French. Okay, that's anecdote number one. Anecdote number two is studying noisy data to make astronomical estimates. So on the left, by the way, is Tigo de Brahe's observations of the angle to Mars. Now, I don't know about all of you, but if I look at the left, I don't think elliptical orbit. Okay? I don't know if anyone here can just look at that and guess elliptical orbit. If you can.
00:24:27.615 - 00:24:57.275, Speaker A: So pretty good. Yeah, it's pretty amazing. And look at how many years he did this, right? And this is pretty serious dedication. And I should say that for some of these slides, I'm using material from this talk by Terence Tao called the Cosmic Distance Ladder. I would just say that if you're not familiar with this talk, I highly recommend checking it out if you're on just for posterity, people watching this on YouTube. Pause this video. Watch that one.
00:24:57.275 - 00:25:38.183, Speaker A: So, yeah, so Gauss. So there's a paper I mentioned at the bottom by the same historian who tries to determine if Gauss actually had least squares in 1790s as he claimed. But this is from Gauss 1809 book. And obviously, if you look at the data on the left, it's very noisy. And that's actually very interesting because. So what are you doing? When you're a scientist, you make predictions, you make models, and then you want a systematic way in order to rule out whether the model is correct or not. Okay? So I'll call this the Y given X problem.
00:25:38.183 - 00:26:12.669, Speaker A: So you make a model of Y given X, and he just tried to be able to formally say whether it's close or not. Okay. And Gauss was faced with a ton of orbital calculations, and he really wanted to know if his estimates based on noisy data were accurate. So he came up with least squares and all sorts of ways to estimate the error for it. I should say there's another reference I found interesting when I was prepping this. There's a paper by Kolmogorov where he analyzes like a bunch of terms and assumptions in. So maybe I'll put that here as well.
00:26:12.669 - 00:26:36.227, Speaker A: If you want to look up Kolmogorov, you can. There's a Kolmogorov paper on. The title is Gauss and Least Squares. I forget the rest of it. Okay, okay. That's the second anecdote I wanted to share. Now the third one I found pretty remarkable.
00:26:36.227 - 00:27:28.295, Speaker A: So, as I mentioned, this data was taken over the span of almost 20 years by Tycho de Brahe, maybe. Looks like basically 20. And this is the information that was used by Kepler to determine that there are elliptical orbits. And what I find amazing is that many aspects of the next two anecdotes reminded me of what happens in those LLM Open problems, I said. So LLM Open problem three was this one about the data proportions and how you rebalance data proportions and include extra data. If you want to solve a problem that's not going to do Wikipedia, you throw on Wikipedia data, or not even something about factual, you still throw in Wikipedia data or analytic data or symbolic data or something like that. So in order to determine the orbits of Mars, Kepler included a bunch of other information.
00:27:28.295 - 00:28:02.495, Speaker A: He just pumped information into the calculations he was doing. You know, and for me, this sounds like the same distribution shift I was talking about before, where you want to make some comment about Mars and you throw in stuff about all the planets and all sorts of different things. So, yeah, that was how he did this. And another aspect which is interesting is it's also time series data. And he brute forced getting rid of the time series data, which is that Mars returns its orbital length. It was 687 days. And so he just samples this every 687 days.
00:28:02.495 - 00:29:05.303, Speaker A: Which also means that Teoda Brahe really needed to accumulate lots of data for this to be viable. Okay, and now for the last anecdote, which is one of the ones I slipped into the title. So Aristarchus of Greece, he made a lot of these old celestial calculations, things like distances to celestial bodies. And he was the original one to come up with a heliocentric concept of the solar system. And I just drew this picture like this because his original intuition on why this is the case is because it makes no sense for something much larger to be ordering, orbiting around something much smaller. And I should say that even though it's always credited to Copernicus or it's frequently in popular literature credited Copernicus the heliocentric model. If you control F on Copernicus document, you find Aristarchus cited explicitly.
00:29:05.303 - 00:29:52.495, Speaker A: So it was known. And I should also say that the usual reason that it was claimed that the Greeks rejected the heliocentric model was a philosophical or a religious one. But actually there's a data accuracy and an out of distribution story behind it. So what people were frustrated by or they used to attack the model was they claimed that the model made certain incorrect predictions for out of distribution data. So the claim was that if you look at the stars as the Earth's orbit is at opposite points. So you look at, if you assume that we're orbiting around the sun and you look at the two opposite points and then you check the location of the stars, the claim was that they haven't moved, so we can't be in a heliocentric setup. So everybody follow what I'm saying.
00:29:52.495 - 00:30:26.085, Speaker A: So Aristarchus made up a model for the orbits of the planets and then they used out of distribution data, the stars, to argue that his hypothesis was incorrect. And the reason that they couldn't measure, they couldn't verify with parallax. It's called parallax when you do this. On the opposite side, the reason they couldn't verify that the stars were moving was because they didn't have good enough telescopes. Their data was too messy. So it's an added distribution story and a messy data story. And that took 2,100 years to resolve.
00:30:26.085 - 00:30:56.549, Speaker A: That is. Yeah. Thanks, Misha. That is an exceptionally perceptive point. In fact, what they said back then was that it can't be the case that they didn't move or within the tolerance of the because the stars cannot possibly be so far away. So the estimates implied for the distance the stars they claimed was just, they couldn't fathom such distances. Yeah, good.
00:30:56.549 - 00:31:25.355, Speaker A: You're 100% correct. I didn't make that up, by the way. That is the answer. Ok, so that's the end of the anecdotes. Now I'll try to extrapolate. Does anybody have any questions or comments on on these four anecdotes before I move on? Yeah, go on.
00:31:26.055 - 00:32:03.357, Speaker E: So when you're talking about that open problem three, when you're asking that why valuation data being different from the training data might help. It's almost like trying to ask when I'm trying to learn the task. It's like the algorithm itself I'm trying to, not the data that I have. Yeah, so like, it's basically the difference between classical like models Training on the given data and trying to learn a particular function. Instead of that, the task now is to learn to be able to get in a data set and get out of function.
00:32:03.461 - 00:32:03.749, Speaker A: Yeah.
00:32:03.797 - 00:32:08.905, Speaker E: So could that be the formalism or the theoretical.
00:32:09.705 - 00:32:37.975, Speaker A: Okay. I realize people are frustrated because I was vague and people are not enjoying that I was intentionally vague. There are many setups you could follow for this one. Yeah. One is the version you said. Let me phrase it in another way, which is like a DRO version, because John Ducci is going to talk about. Wait, maybe can you say your version again?
00:32:38.635 - 00:32:58.827, Speaker E: My version is. My goal is to learn. Given a particular data set, my data set could change. My goal is, given any data set, I can find out the pattern that I. It's not specific to the data that I have. It's not like I have a given data regression function to that data.
00:32:58.971 - 00:33:28.975, Speaker A: Yes. That is. That is a version of it. I'll also say that there are related questions that are in the literature. For instance, because they're taking a supremum over the reweightings, you could view it as a distribution robust optimization task. But the interesting thing is that it's focusing on doing well on the specific target, which is what you're mentioning. So, yeah, I'm not familiar with the version of this formalism in the literature.
00:33:28.975 - 00:33:41.215, Speaker A: Anyone else want to ask about open prompts? I mean, I'm ahead of schedule. So.
00:33:43.995 - 00:33:46.499, Speaker F: Can you go back to the Khao Shiyam gradient descent?
00:33:46.587 - 00:33:50.695, Speaker A: Yeah, yeah. What's up?
00:33:58.405 - 00:34:02.985, Speaker F: I just wanted to look at the gradient cent formulation one more time.
00:34:03.405 - 00:34:26.095, Speaker A: Yeah, I believe I did not. For those of you that are wondering, you make them equivalent by differentiating the second and setting it to zero. And I wrote it in this way with, you know, some of the terms are extraneous. Right. When you differentiate with respect to W. I didn't need to write that. I wrote it to sort of capture what Cauchy was actually doing.
00:34:26.435 - 00:34:31.443, Speaker F: So I'm a little confused about the connection. I mean, this is the way we can think of gradient descent in general.
00:34:31.539 - 00:34:31.867, Speaker A: Right.
00:34:31.931 - 00:34:46.744, Speaker F: We can always think about it forming a quadratic and minimizing it. So in that sense, like, it's always connected to regularization. If the regularization is the quadratic. Yeah, but somehow you're saying for Cauchy, this was more tied to the application.
00:34:47.364 - 00:35:06.300, Speaker A: Yeah. So I was hoping that in Cauchy's, I could find. Well, he didn't write the second version, he wrote the first. And I was hoping I could find a statement about staying close because he did discuss the issue of local optima, but he didn't say anything about which local optima selects like the close by ones. So. Yeah, it's just. Maybe I was.
00:35:06.300 - 00:35:20.241, Speaker A: I guess part of what's bothering you is I was overemphasizing an obvious fact which is. Is trading off the two. Yes, I'm in agreement with you. Yeah.
00:35:20.273 - 00:35:25.345, Speaker C: Peter, since we've got some time, I'm curious.
00:35:25.465 - 00:35:55.335, Speaker A: In the set of anecdotes that you told us, especially in relation to the program, was it intentional that you didn't mention Newton who came up with a beautiful theory for the electrical orbits? Yeah, why did I not mention that? I mean, I should have. Yes, okay, good point. Yeah, I was just motivated by. Yeah, okay, I should have included that. Good omission to point out.
00:35:56.315 - 00:36:03.717, Speaker C: So I want to push back a bit. Several things you said don't fit with statistical learning theory or not statistical learning theory.
00:36:03.741 - 00:36:04.381, Speaker A: Excellent.
00:36:04.533 - 00:36:08.825, Speaker C: And maybe especially since you put me down in the classic column there.
00:36:09.245 - 00:36:10.745, Speaker A: Oh, excellent. Okay, good.
00:36:11.765 - 00:36:47.567, Speaker C: I mean, I think we have to be careful that. Because I'm not sure exactly what you mean, but it doesn't fit with statistical learning theory. It maybe doesn't fit with, you know, this, as you mentioned at the beginning, this basic supervised learning model where you're learning from IID data from the same source distribution you're predicting to a predictor. But as you mentioned in the beginning, that's just like a simple case to start with that doesn't capture everything. And there has been work on many of these things. In particular, management, you talked about using this task of next token prediction. And the idea of using auxiliary.
00:36:47.567 - 00:37:08.745, Speaker C: That is not what you care about. The idea of using auxiliary tasks to. In order to have some tasks that you're interested in like question answering and then you invent all kinds of auxiliary tasks and use transfer for them in the multitask setup is well established, you know, for over 20 years with both theoretical and empirical work.
00:37:08.825 - 00:37:09.425, Speaker A: That's true.
00:37:09.505 - 00:37:13.657, Speaker C: And so I think it's important here to maybe zoom in on exactly what.
00:37:13.721 - 00:37:16.585, Speaker A: Is it that's distinct now.
00:37:16.665 - 00:37:20.165, Speaker C: Yeah, what we do understand and what we don't.
00:37:21.445 - 00:37:22.345, Speaker A: That's fair.
00:37:23.285 - 00:37:41.053, Speaker C: In particular, I think being a bit more specific there, I think this idea of transfer from auxiliary tasks is definitely, I think if you want one specific reference, probably and Zhang were probably the first to really put it on the table. On the table.
00:37:41.109 - 00:37:44.265, Speaker A: I'm actually not aware of that reference. What are the two. What are the.
00:37:44.605 - 00:37:45.717, Speaker C: Tong Zhang.
00:37:45.821 - 00:37:47.225, Speaker A: Oh, Tong Zhang, sorry.
00:37:56.765 - 00:38:44.465, Speaker C: And I think that what is new Here maybe is that in. And that's just maybe one reference. But in this work in using auxiliary tests there were different predictors for each, there's a different predictor for each test. So you're using transfer between tasks but you're not actually learning the same predictor for each task. You're just tying them together in some way. And Andrew and Zhang's work is through some low rank structure which really you can think of it as learning a two layer network. And I think what's maybe different and maybe less explored if you do want like an open problem here is that your order diagnose using transfer from auxiliary tasks.
00:38:44.465 - 00:39:20.059, Speaker C: But we're training the same probabilistic model. So rather than training a different predictor for each task, we're training a single probabilistic generative model for multiple different sources. So I have one source I care about and I'm using like a different source and fitting, you know, training for like the mixture of these two sources in the same probabilistic model. And that maybe is a bit, I mean I'm just trying to find an edge here to like what is exactly near. But maybe the main message is we have to be a bit careful from these looking blanket statements. If it doesn't fit, it's completely different.
00:39:20.227 - 00:39:21.575, Speaker A: Ok, sure, that's fair.
00:39:22.555 - 00:39:24.091, Speaker D: I think substance loss is even older.
00:39:24.123 - 00:39:24.459, Speaker A: Right.
00:39:24.547 - 00:39:32.879, Speaker D: Domain generalization is basically that like you get to see a whole bunch of environments and then you don't get to change your predictor, you get to be like, you know, evaluated in a new environment.
00:39:33.047 - 00:39:45.847, Speaker C: Right, but in domain, but this is in domain generalization, I mean this is more is a problem. It's not that you're you. I mean if you actually had data from the environment you care about, you just would train in depth.
00:39:45.991 - 00:39:46.319, Speaker F: Right.
00:39:46.367 - 00:39:47.439, Speaker D: Then it would be domain adaptation.
00:39:47.487 - 00:39:47.647, Speaker A: Right.
00:39:47.671 - 00:39:51.599, Speaker D: Like if you do get to see some amount of data then you get to the setting itself.
00:39:51.687 - 00:40:10.835, Speaker C: But here's a bit different because it's not that anyway you do have data from the domain they care about. But I still also get, you know, use data from other domains in order to help me on that domain. Not just to generalize to different domains. But yeah, but you're right. I mean that's also that again is working in the early 2000s annotation.
00:40:12.295 - 00:40:24.117, Speaker A: Okay, good. Yeah, I'll concede that point. I made a mistake of maybe being a little too provocative and definitely not referencing a lot of important work. So yeah, those are two mistakes.
00:40:24.221 - 00:40:25.573, Speaker C: But maybe if I can say something.
00:40:25.629 - 00:40:29.165, Speaker A: In defense of Course of me. Wait, oh, I don't need to be defended.
00:40:29.245 - 00:40:49.281, Speaker C: I think if we look at every individual problem, it's never going to be new because there is so much work on like every single task and yet there is obviously something very new there. Right. Goes way back. But nothing works like this systems work. So that means that we don't understand that.
00:40:49.313 - 00:40:52.561, Speaker A: But I wasn't crisp enough defining it. So I take the criticism. It's fine.
00:40:52.593 - 00:40:54.845, Speaker C: It's important to understand what exactly is different.
00:40:57.465 - 00:41:25.531, Speaker A: Okay, good. We have a semester to do it. So. All right. So based on those anecdotes, I just tried to extrapolate a little bit about the scientific method, essentially. So in all of those examples I gave, there was sort of this feedback of acquiring, I was saying carefully balanced, high quality data mixtures. I made it sound a little bit like that LLM setup and then finding succinct.
00:41:25.531 - 00:42:13.045, Speaker A: I called it succinct Interpretable Models. I mean, I found it pretty interesting that people went straight from circular orbits to elliptical orbits. I mean, they didn't have to, you know, they could have picked more complicated curves, but they didn't. So I guess everyone takes it as a given they jumped that. But yeah, I found that quite interesting. And yeah, so a couple conclusions that I had from this was one, in all of this, it's always about modeling P of Y given X, but the modeling of P of X which we need for these statistical learning setups is not just not present. And then another thing I highlight was I felt that a lot of these open problems or the aspects of the setting that are different from learning are similar to those LM setups I mentioned.
00:42:13.045 - 00:43:20.635, Speaker A: And lastly was this issue of balancing the model fit with the size of the model or the succinctness. So then I come back to what I was calling the other stories and what you'll see at this point is the pattern, as I'm always pointing out, the log sumax and all of these. So just to give a little bit of detail, I think most of you are familiar, but those of you that are not familiar, one of the sort of workhorses in Cisco learning theory is what's called a uniform convergence result. You look at the difference between the training error and the test error and you take this gap over some large class of functions. I guess I should have written, yeah, some large class of functions. And if you just stare at this, you could, and you know, some statistics you could guess maybe using the moments or something to control the right hand side. And of course you can do that, you can use moments of different types, and if you have all the moments bounded, you can get a good bound that way too.
00:43:20.635 - 00:44:09.295, Speaker A: But what's interesting to me is that the standard way people control this is by once again invoking a log some exp. So what I wrote here is the definite. For those of you familiar the definition of subgaussian, I've just sort of rearranged the two sides. I took a log of both sides and divided by the parameter of the definition. So this is yet another place log some X shows up fundamentally in the literature. Okay, so now for the last part, I wanted to tell you a little bit about gradient descent. As I mentioned, this is the biased part of the talk because this is about things I work on and things I like personally.
00:44:09.295 - 00:44:55.521, Speaker A: So here's the definition from before. I'm again being a little bit artistic and not just calling the norm constraint a succinctness constraint. And so my first question is, can we formalize this trade off or make it more crisp in a theorem? And the answer is yes, I will tell you about maybe. So just personal bias is maybe my favorite property of gradient descent or gradient flow. So here I'm using sort of infinitesimal gradient descent where you define it via an ode. And I've left off the assumptions under which the ODE has a unique solution, but just take that as a given. So instead of gradient descent, this is gradient flow infinitesimal step size.
00:44:55.521 - 00:45:26.031, Speaker A: You just flow in the negative gradient direction of your objective F. And I should have said that F is convex here. Sorry for that. Ok, so here's the theorem. And again, this theorem is meant to capture the fact that, oh, I'm missing a term. This theorem is meant to capture the fact that gradient descent trades off between norm and loss. So it's saying for all reference points W bar and for all time I have the distance traveled and the objective at time T is less than that.
00:45:26.031 - 00:46:03.157, Speaker A: Same quantity. Oh, this should be wt W bar, same quantity but for this reference point. Okay, so now many of you have probably seen this where what happens is people say this term is non negative and so you just drop it and you only look at the right hand side and it's telling you that you're going down at a rate of 1 over T. But I'm highlighting the fact that the left hand side term that is often dropped is very fundamental to how machine learning works. It says that you can never have the property that you move too far from a good reference solution. So with a little bit of algebra and just to be clear, this is still part of the theorem. You can derive the following statement.
00:46:03.157 - 00:46:23.035, Speaker A: Either you've passed below the reference point in terms of objective value, or you're at most twice it in terms of norm. Again, the difference between what you might have seen in a convex optimization textbook is that I have selected not to drop this term.
00:46:32.255 - 00:46:32.791, Speaker C: Okay?
00:46:32.863 - 00:47:23.285, Speaker A: And because this is my favorite thing ever, I want to show you how easy it is. So the way a lot of these proofs work is they start with this potential function, look at the norm squared between the iterate at the final time and at the initial time. And I have to say that when I was learning this literature, I always found it interesting that this was a potential function because I was taught that gradient descent minimizes an objective, but then the potential function did not have the objective. Ok, and the other nice thing about this proof is I essentially only have. It's like a set of forcing moves in a chess game. I only have one possible step I can do every time. So the first one is the fundamental theorem of calculus.
00:47:23.285 - 00:48:01.667, Speaker A: Let me clean up my handwriting a little bit. The first one is the fundamental theorem of calculus. And then we just differentiate. So I have that time derivative and this. Now I use the definition or sorry, not the definition. I use the fact that W S is a solution to the ode, so I get the objective. So there's a minus sign.
00:48:01.667 - 00:48:37.793, Speaker A: That's why I flipped this. And I use convexity. So at every step I only have one thing I can possibly do. Then I use convexity. And then just because this proof is so clean that I would like to include every single detail, I don't want to sweep anything, I don't want to hide anything in this proof. So I can lower bound this iterate at time S by the iterate at time T. And this now is.
00:48:37.793 - 00:49:29.665, Speaker A: So now we're done. So now we can rebalance the size and get the first statement and just be clear. I will explain this missing step for you, but we've established this after you just divide. So just to be clear, the reason that it's doing descent does not even rely on convexity. If I look at dds of F of Ws by chain rule, I get this, which is this. So it says that we're strictly doing descent. So this really much going on here, but it gives a powerful result.
00:49:29.665 - 00:50:23.395, Speaker A: And I did not yet give this factor two part, the fact that you never go two times away. There are many ways to check that step. I'll just do it. One of the ways so If I look at this quantity and I add and subtract. Oh, sorry, I did not need that. Yeah, I look at this, I add and subtract or Sorry, yeah, you add and subtract on the inside. You get this, then you apply what we just did and you get.
00:50:23.395 - 00:51:24.345, Speaker A: So now if I just look at this expression, if I just look at the left and the right hand side, if it's the case that we are above an objective value, then it must be the case that this one is less than two times this one. There's no other way to satisfy the inequality quality. So this claim says. Ok, so that's the end of the proof as we've both given this sort of self regularizing inequality and we've given the consequence that unless we fall below an objective value we can't be more than twice in terms of norm. Any questions about this?
00:51:27.045 - 00:51:31.101, Speaker C: Generalize this, Tom?
00:51:31.133 - 00:51:53.455, Speaker A: Yeah, of course, absolutely. Yeah. So actually this is. There are many reasons why. Yeah, there are many reasons why I like this proof a lot. So Nadi was just asking if I can generalize it to other geometries and sort of the. The most trivial one of course is to look at mirror descent.
00:51:53.455 - 00:52:48.875, Speaker A: It's not as clean interpretable in the sense that here in order to get the Factor 2 inequality I had to use this expanding square on the squared norm. And so I'm going to write out the proof. I'm not going to get the same statement that says that we're like twice in terms of the Bregman divergence. I actually don't know how to prove such a statement. But just to be clear how clean the proof is, it basically is exactly the same sequence of steps with no added inequalities. So you still just differentiate the Breguend divergence and use the definition of the mirror flow and the steps go through use. Yeah, you write out the definition and then you just push it through and use the fact that the mirror flow now the mirror map appears in the definition of the flow.
00:52:48.875 - 00:53:02.975, Speaker A: So in the place where we just used the ODE definition, we still use an ODE definition is exactly the same proof. Yes. This works in other geometries. Yeah.
00:53:06.795 - 00:53:09.795, Speaker F: So you once showed me something like this in discrete time.
00:53:09.955 - 00:53:15.963, Speaker A: That's correct, yeah. Is it? Am I correct that there was a factor of like four or something?
00:53:16.019 - 00:53:18.775, Speaker F: It was actually not the same constants for that?
00:53:19.435 - 00:54:10.175, Speaker A: I think so. Some I always get in these struggles with myself which way I like to write this theorem. I think it was unless I made an algebraic error when you saw me do it. It should have been this four just for everybody that's listening in. The difference is that when I wrote the statement, it was not the squared norm, it was the norm itself. So if Fwt is greater than Fw bar, then I get the inequality less than W0 minus W bar quantity squared and you square root both sides and move the two to the right hand side and then you get the factor two. So I believe it was this one with the four, but I actually remember that when we were at the board and Civ was there, and I remember I got flustered at one point, so I think I might have accidentally messed something up and put in like an 8 or something.
00:54:11.955 - 00:54:17.819, Speaker F: So I also want to check that the only place you use convexity is in the first order equations.
00:54:17.987 - 00:54:20.667, Speaker A: Yes. In fact you do have two inequalities.
00:54:20.731 - 00:54:23.531, Speaker F: The last line is actually an inequality even though you wrote an equality.
00:54:23.643 - 00:54:57.213, Speaker A: Yes, so. So this inequality where introduced the factor of two by expanding the square. And just for everybody that isn't as comfortable with this, because this proof is so clean and so basic, I really don't want anyone to feel like there's some disgusting set of steps that I've skipped. But one way for instance, to do that is triangle inequality and. And then extend. So yes, so this step is actually horrifically loose. This step is a nightmare.
00:54:57.213 - 00:55:43.025, Speaker A: All the other steps can be tight. In terms of the use of convexity, the only place it was used was here. The interesting thing of course is you can still choose not to apply convexity and have the same statement. Sorry, not the same statement. You can't collapse the last steps, but you still get a theorem in terms of this inequality with just the gradient in there. So for instance, I've used this to analyze this TD method, temporal difference method, and you actually, even though it's not a convex setting, you can still use. Any other questions about this? Oh, sorry, do you mind repeating the upshot like the either or statement consequence again? Yeah.
00:55:43.025 - 00:56:20.327, Speaker A: So one way to actually look at it is just to ignore the first case. So run gradient flow so you have some reference point, somebody handed you a solution, maybe they say it's the optimum or something. And then you run your gradient flow until you match it in terms of objective value. Another way to think about it is that, oh, I guess it's going to switch the. So I'll draw over here. Let me add myself a page. Another way to look at it is gradient descent starts here and somebody hands you a W bar and they just hand you this.
00:56:20.327 - 00:56:46.141, Speaker A: They say it's pretty good. You don't know how good it is. They don't know if it's optimal. And let's say you look at the level sets of the function, you look at the level set rooted at W bar. What this is telling you is that if you run gradient flow from here, the point where you hit that level set, the set of points such that the objective function is equal this point, the norm from W0 is at most 2 times W bar. Yeah, I should also. Nati was on my case earlier about references.
00:56:46.141 - 00:57:33.665, Speaker A: I should say that this factor of two, I can't tell if it's optimal in the setting. Also, it's very difficult to prove any improvements to it. So one of the only ones I'm familiar with is Ryan has a result for least squares, specifically where he's able to bring that factor of two down in a very general setup. And yeah, with a clean proof, but this one is fairly general. Shrine, is your paper flow or is it descent? Both or. Did I answer your question? I think the easiest way is not actually to treat as an either or. You just pick the time such that you hit the point.
00:57:33.665 - 00:57:54.109, Speaker A: There are many ways to rewrite the inequality to get a different intuition from there. And yeah, I didn't finish the steps for Nati's question, but if you appropriately redefine the ode for mirror flow, then it lets you just push through these steps and you wouldn't get effective.
00:57:54.197 - 00:57:55.705, Speaker C: I mean, you wouldn't get the same.
00:57:56.645 - 00:58:07.439, Speaker A: Yeah, you get something kind of, to be honest, kind of uninterpretable. Or I've used it, but it's fairly clunky. So for KL divergence, I have like a statement, I get like a factor 8 or something.
00:58:07.527 - 00:58:12.071, Speaker C: But you cannot say in general that you're a divergence to.
00:58:12.183 - 00:58:44.805, Speaker A: Oh, I see. Yeah. Yes, I believe I understand Nati's comment. Yeah. So if I whatever like, oh, are we arguing about what is the question? What's that? But yeah, so the question is if we changed the geometry. And yeah, just to be clear, for mirror flow, it looks something like. I don't think I'm messing this up.
00:58:44.805 - 00:59:15.441, Speaker A: The theorem looks like this. You're saying that in this modified geometry where we put the gradient of the mirror map in side of there. So this is how you define it. And if you use this definition for the flow, then you can still push through the proof. However, you will not get a statement this clean. Where this right here is for the Euclidean geometry, this is the square root of two times the divergence. So you will not get this statement with the general divergence, you'll get some weird uninterpretable thing which is like for the KL with a lot of work I could get.
00:59:15.441 - 00:59:34.989, Speaker A: But good question. Yeah, so open problem. I get other geometries with factor T. Ok, good. Oh, so some. Oh. Any other questions about this? This is my favorite thing ever.
00:59:34.989 - 01:00:07.785, Speaker A: So many people here have seen me do versions of this. So just some remarks about it. So one is that in order to get an effective theorem from here, in my experience does require a lot of sensitivity, both the norm and the sublevel set. Because if you just think about it, factor two doesn't actually say that much. And just a proof remark. The proof really is identical to the gradient descent proof. So in particular for the gradient descent proof, you look at the difference of the potential still and then you don't use fundamental calculus, use a telescoping sum.
01:00:07.785 - 01:00:33.395, Speaker A: It's the same proof if you expand the proof of the fundamental theorem of calculus. The fundamental theorem of calculus for Riemann sums boils down to a bunch of telescoping sums. It's the same proof for gradient flow and gradient descent. The difference is that the regularity conditions for the gradient flow can be relaxed. So that's why there's some difference. But this proof goes through for lots of settings. I didn't mention mirror descent, like not to mention, but it goes through for cascading stuff.
01:00:33.395 - 01:01:38.021, Speaker A: I will, I think skip this just so I finish on time and I think people are maybe tired of me talking about it so much. I'll just say that to get an effective theorem for linearly separable settings, there is kind of a natural choice you can make for this reference point and kind of you get a very strong statement in the sense that you plug in the structure of the loss function and this particular choice of the objective or the particular choice, the reference point and you actually get a statement not that you're factor of two, but you're very close to. You kind of can shrink, can shrink that distance. Just to give some references since some of the people are here or almost everybody's here. So this, the first time this for linear prediction appeared was 2018, but also Jing Feng Wu, who's right there? Hey, what's up? And then this is. This B is Peter. So I won't, I won't belabor this actually.
01:01:38.021 - 01:02:15.785, Speaker A: So I want to share this photo. So I'm not trying to imply that most of the speakers were working on their slides at like middle of the night last Night upstairs, one floor. I'd also like to point out that for people that were trying to identify the staff member who was in the second slide and is in a photograph. This is Gary, the cleaning staff who remembered me from my last visit and asked me how I'm doing and where I'm remember where I was like living and all these things. So it was pretty cool. But yes, we were not working on our slides in the middle of the night last night. So I just wanted to mention some.
01:02:15.785 - 01:02:41.141, Speaker A: If you find this type of material interesting, I wanted to mention some other results. The literature usually uses the term implicit bias. Actually this term has been forgotten. Last time I saw implicit bias, it was actually about like social biases. Last time I saw implicit bias in a paper title. And also for those of you that know the history of the Laura papers, the Laura papers cite essentially a bunch of empirical implicit bias papers. But they didn't know about the implicit biased literature, maybe because the name clash.
01:02:41.141 - 01:03:28.445, Speaker A: So yeah, this idea comes back a bunch of times, but just to mention some results by people in the room, or almost all of them are in the room. So Nati was the first to do the gradient descent version of this for linear prediction. Kaifung, who's. Hey, what's up? Yeah, Kaifeng is here. Yes, I'm dropping co authors. Actually I don't even know the full set of your co authors since you added some like with the general version. Was he on this one too? See, now I have to add myself to the slide and I don't want to do that.
01:03:28.445 - 01:04:02.057, Speaker A: So Kai Fung made a generalization where for multilayer relu and that's just what I wrote to be interpretable. But basically for multilayer relu I say infinitely often you go to locally optimal solutions. I say infinitely often because what he proved was that the cluster points are local maxes with interesting proof technique. Actually this paper by Kai Fungi rediscovered like a lot of the technology from the previous decades without having ever seen it. It was pretty amazing. Then this the only. So this was.
01:04:02.057 - 01:04:41.055, Speaker A: I will give the credit here to Zhu AG because he did all the hard parts. The difference between these two lines is that the infinitely often is removed. So one is about cluster points and the other is about limit points. And if you think this distinction is trivial, you can talk to Kaifeng about it. And then I also want to highlight Margalit's result. Where's Margalit? Hey, how's it going? So yeah, all of these people are here all semester, so um, you'll notice the last Two, they had a local max margin. And I should say that this is not how Margalit phrased her theorem, but I'll put it in context.
01:04:41.055 - 01:05:10.563, Speaker A: So she studied two layer ReLU optimization with logistic loss gradient descent both layers, same same learning rate on both layers. So just to emphasize same learning rate gradient descent, not freezing layers gradient descent. Okay. On this two x or data. And you can brute force what the maximum solution is for this problem. And it's this really beautiful sparse solution with four values. And her proof explicitly tracks the progress of the weights and shows that it collapses on these four directions.
01:05:10.563 - 01:06:14.995, Speaker A: Is that a fair characterization? You didn't look fully satisfied, but okay, but the difference is yeah, this local and global. And then all I'll say for the last part of the other stories is there's actually a very clean and easy way to show these max margin results which is if you look at this logistic or exponential loss because once you go follow it for a while it becomes the exponential loss. The sublevel sets are invariant to taking applying a monotone function like the log. So optimization on the logistic or the exponential is like optimization on log some export. So now if you just write the definition of the margin and just approximate it with the log sum X and now do functional theorem of calculus and you literally just do some brute forcing. And in the linear separable case you immediately get this max margin rate. This is a fairly proof with all the details is only a few steps longer than the one I showed you before.
01:06:14.995 - 01:06:55.625, Speaker A: And this quantity for instance, you can find all throughout Kaifeng's paper. That's why I referenced him here. Okay, so that's the end just to summarize. So I tried to kick off the program with some open problems. People were not happy with my open problems. So we do have a task of identifying what is the threshold of where the literature is. And yeah, I made a mistake there I was, yeah, I did not do that properly.
01:06:55.625 - 01:07:29.095, Speaker A: Then I connected to some scientific stuff and I'll say that Peter mentioned that I missed also some prior work there some prior work by somebody. And then we ended up spending a lot of our time here on gradient descent. And what I was trying to highlight in the other stories, this didn't happen on purpose. But as I was preparing the talk, I just realized logs Max showed up everywhere. And everything I was saying. So okay, so I'll stop there. We're basically at time maybe take one question.
01:07:29.095 - 01:08:13.305, Speaker A: But I don't want to stop late. Do you want to ask a long one? Can you Explain this statement you made about how Meta trains Llama 3 by freezing the validation set. And I don't understand that. Okay, I'll give a quick answer, not because I don't like the question, but just so that we end on time. So, yeah, I do not. My. Okay, so first of all, for llama 3 you can find the paper online that's been updated for llama 3.1,
01:08:13.305 - 01:08:33.735, Speaker A: which is the 400 billion parameter model. My recollection was it doesn't fully specify what the procedure was. Here's my recollection. They pick a data mixture in a small model. They. I can show you the plot in the paper. What it shows is they pick a small model and they play with the data proportion to get a certain.
01:08:33.735 - 01:08:54.897, Speaker A: Yeah, they made it like a two dimensional plot, obviously, while you're playing with data proportions. It's a multi dimensional thing. It's a grid search. Right, It's a grid search to search for these different things. They try a bunch of points, but they drew it in a one dimension and then they picked the lowest point, froze that, and then used that to train a larger model. And they did this like bootstrapping sampling thing. That's my understanding.
01:08:54.897 - 01:09:27.945, Speaker A: And they built larger and larger models and their goal was on a log, log plot to get a line like that. And that for them was they've done well and then they finally got some larger data proportion which they claim was fitting a scaling law. And. Yeah, did I give you enough information? So it wasn't clear how much grid searching there is and how many data sources. They're fairly transparent, so perhaps the details are there, I just didn't see them. But yeah, I highly recommend people that are curious about details of LLMs to check the GitHub repo for it. And also the paper, it's been my personal favorite resource.
01:09:27.945 - 01:09:32.314, Speaker A: I guess we'll stop there so we have enough time for coffee. Thank you.
