00:00:02.320 - 00:00:32.334, Speaker A: I see all our speakers here. So it's my pleasure to begin our session today with talk by Sierra Peckberg, and she's going to present a work author with Kirge and Sierra. It has been my pleasure to co author a paper with Sierra, and it's very exciting to see all the recent work that she's been doing on episodic reinforcement learning, and I'm really looking forward to hearing about this new paper.
00:00:34.234 - 00:01:12.734, Speaker B: Thanks. Yeah, I'm Kira Pipeburg, and I'm gonna be speaking about our paper, a unifying view of optimism and episodic reinforcement learning, which is joint work with Gerge Noi. So we consider the episodic RL problem. So here we start off with some initial state x one. We take an action, we transition to a new state x two, and we also receive some reward as a result of doing that from state x two, we then take another action and transition to another state and receive another reward. And we keep doing that up to h times. So, a bit more formally, we have t episodes, and each one consists of eight stages.
00:01:12.734 - 00:01:48.384, Speaker B: In each stage of each episode, we observe a state, we select an action, we receive some reward, and then we transition to a new state. Importantly, in this work, we're going to assume that the transition function is unknown, but the reward is known. And that's what we will call the episodic reinforcement learning problem. Oh, I meant to say, if I talk really fast, please just wave at me and I can slow down. So, in each episode t we want to select a policy pie. And a policy is going to be a sequence of actions. So that says from this state, we're going to take this particular action.
00:01:48.384 - 00:02:35.268, Speaker B: And in particular, we want to select a policy that has high value. So here we define the value as the total reward from some initial state acting according to our policy. When the transition function is given by this p here, and using this, we can define an optimal policy, which is the one that maximizes the value. So if we have a MDP where we know the transition, there are a couple of ways that we can go about finding this optimal policy. So, first one we're going to think of is using occupancy measures. So we define an occupancy measure q, as the probability that we're in state x, and we take action a. And so for every occupancy measure, it will define a policy PI.
00:02:35.268 - 00:03:23.984, Speaker B: And we can just use this ratio here to define that. So, using this notion to find an optimal policy PI, we can find an optimal occupancy measure, and this occupancy measure can be found by solving this LP here. So this, I don't know if you can see my mouse, but this, this term here is just the value. So it's the amount of time we spend in state x taking action a and the reward from doing so, summing over all states and actions. Then the constraints in this queue x one, will just guarantee that our occupancy measures are valid, so they're positive and that they start from some initial state x one. And this constraint here is what we call a flow constraint. So this just makes sure that the amount of mass going into state x is the same as the amount of mass leaving state x.
00:03:23.984 - 00:04:22.154, Speaker B: And it's well known that the dual of this LP has a very nice form. In particular, we can interpret these optimal variables here, these as the value functions, and then we can solve this via backwards recursion starting from big h. And what's nice is that they exhibit strong duality because they're linear programs. And the optimal solutions to both is going to be the optimal value function from an initial state x one. So then, if we don't know p, what we want to do is we want to try and minimize our regret. So we select a sequence of policies, PI t, and we want to minimize the difference between the value of that policy compared to the value of the optimal policy. In this setting, we have a lower bound for regret, which is h square sat and common to a lot of problems where we consider regret minimization.
00:04:22.154 - 00:04:59.176, Speaker B: A common technique is to apply the optimistic principle. And the optimistic principle basically says we want to act optimally in the best statistically plausible world. So just sort of put that into context. We're going to consider a really simple problem with a bandit problem, where we basically have one state and h is also equal to one. So we just have a sequence of actions, each one has some unknown mean. And what we do is we construct confidence bounds around our empirical s of the mean and each within these confidence bounds. These are what we call the feasible worlds world.
00:04:59.176 - 00:05:50.084, Speaker B: We're going to be looking at the highest upper confidence bound here. And so this leads to the well known UCB algorithm, but just plays the highest upper confidence band of each arm. However, in RL, there are two ways that we can use optimism. So the first is what we'll call the model optimistic framework. So here we have our empirical density measure, which is estimated from data, and we're going to construct a confidence set around it, such that the true transition function will be in that confidence set with high probability. We then jointly optimize over this confidence set and the policy space to find an optimal policy, an optimistic policy even, and some examples of algorithms that do this include UCRL and Klucrl. Alternatively, we can consider value optimistic algorithms.
00:05:50.084 - 00:06:36.174, Speaker B: So here we directly build on the UCB framework to add a confidence spam to a value function. This means that we can compute an optimal policy via dynamic programming by just using the empirical transition function and adding some uncertainty bonus here. And examples of this algorithm include UCBVI and UBEV. So, model optimistic algorithms, because they construct this set of transition functions, it's quite easy to show that the true transition function is in it. So this facilitates a simple probabilistic analysis. However, value optimistic algorithms, in this case showing optimism, can be a bit more complex, but we often have to use some complex recursive arguments. So we're going to say that the probabilistic analysis is a little bit more complex.
00:06:36.174 - 00:07:34.694, Speaker B: However, when it comes to implementation, this joint optimization problem here is pretty tricky. So it's quite a computationally inefficient to implement model optimistic algorithms, whereas on the other hand, value optimistic algorithms can be implemented simply by dynamic programming. So this is very easy to implement when it comes to the existing algorithms. The best known regret we have for model optimistic algorithms are loose by a factor of square s, whereas value optimistic algorithms are known to match a lower bound up to logarithmic factors. So on paper, these two methods obvious two classes of methods, the value optimistic and model optimistic algorithms, seem quite distinct. However, what we're going to show in this work is that there is in fact quite a deep connection between them. And in particular, we show this by demonstrating that the two optimization problems solved by the two classes of algorithms exhibit strong duality.
00:07:34.694 - 00:08:23.258, Speaker B: So our main result is to show that every model optimistic algorithm has an equivalent representation as a value optimistic algorithm. So this basically means we can define confidence sets using the empirical transition function and then implement it by a dynamic programming. So I'm going to quickly go through the key ideas of proof of this result. We first start writing the model optimistic optimization problem using these occupancy measures. So this is very similar to the initial lp we saw for when the transition function was known. But we replace the known transition function with this variable p tilde and then constrain p tilde to sit in our confidence set. This optimization problem is unfortunately bilinear.
00:08:23.258 - 00:09:23.854, Speaker B: So we have this variable p multiplying this variable q, and that means it's not super easy to show any duality or to do anything particularly, particularly normal. Instead, we can re parameterize the problem. So, if we introduce these variables j, we see that our first flow constraint becomes linear. And due to the assumption we make about our distance metric, we can write our, write the confidence set in a nice form as well. So, I should have mentioned that we assume that our distance measure d here that defines these confidence sets is convex in both arguments and positive homogeneous, which means that we can multiply it by a constant and just take the constant out. So once we have this reparameterized form, it's a lot nicer and we can go to the next step, which is writing Lagrangian down. This looks a bit scary, but basically what we do is we write the Lagrangian without the constraint that appears in the confidence set.
00:09:23.854 - 00:10:06.174, Speaker B: Then using the fact that these q's are positive variables, we can pull that constraint inside. And then we see that this term here is essentially just going to be what we'll call the conjugate. So this is this d function defined here. And from that we can just write down the dual from the Lagrangian. And we call this conjugate distance here. We're going to use this as our confidence term, which is some measure of the uncertainty that we have about v in this particular instance. And so, what this result has shown is that we can start from model optimistic framework and we can end up with a value optimistic implementation.
00:10:06.174 - 00:10:40.264, Speaker B: So, there are a couple of other things we need to show before we're done. So, we can show the optimal model optimistic policy satisfies value optimistic Bellman equations. So that basically tells us that the policies we get from these two optimization problems are almost equivalent. In fact, they are equivalent if there's a unique optimal policy. We also need to show the value functions are going to be optimistic. So, this is done very easily in the primal model optimistic framework. So, here we know that our objective function is equal to the value by strong duality.
00:10:40.264 - 00:11:26.374, Speaker B: And because our occupancy measures are bounded, and because our rewards are bounded, we know that this is going to be bounded by h minus h. So we don't need to clip anymore. Additionally, it's going to be optimistic because we know the true transition function is in the confidence set of a primal problem with high probability. And lastly, we can show that if we don't want to use this confidence bound, perhaps it's a little bit messy to calculate. If we have enough bound on it, then we can also use this and we'll still get an optimistic value function. And this is particularly useful. For example, if our distance is some normed distance between p and p hat, and then we can upper bound d by the dual norm.
00:11:26.374 - 00:12:42.634, Speaker B: So, with these results, we can provide a general framework for optimism in episodic reinforcement learning. The first thing we do is we define a model optimistic confidence set, such that p is in the confidence set with high probability. We then compute this conjugate d, and then we can put that into the value optimistic equations that we had, and compute the optimal policy via backwards recursion. And this means that we can get the best of both worlds. These algorithms will benefit from a simple probabilistic analysis, which can be done in the model optimistic framework, and also a computationally efficient implementation in the value optimistic perspective. And we can also easily ban the regret of these algorithms. So, if this Cb is a confidence bound defined in terms of the conjugate distance, what we do is we can fix an episode t in a stage h, and we can just use the expression from a value optimistic algorithm to substitute here, and we use the Bellman equations to, to get another expression for v PI.
00:12:42.634 - 00:13:48.426, Speaker B: Then, expanding this a little bit, we have a martingale, and we also have this term here. So this because p is going to be in the confidence set with high probability, in the primal confidence set with high probability, we can actually bound this by the conjugate distance again, because if we go back to where it was defined, there, it's just the maximum over p, which is a density function, and so we can recurse on h here. And so we see that the optimistic v one minus the true value of v one is going to be bounded by the sum of this martingale difference sequence plus the sum of the uncertainty terms. We then use optimism in the primal space to show that our optimistic value function is indeed optimistic. So we can upbound the true optimal value function by our optimistic value function. With this, we can just sum up.
00:13:48.530 - 00:13:50.174, Speaker A: Kira, can I ask a quick.
00:13:50.474 - 00:13:51.294, Speaker B: Yep.
00:13:53.954 - 00:13:55.370, Speaker A: Sorry, finish. Finish your time.
00:13:55.402 - 00:13:57.634, Speaker B: No, go ahead. Go ahead. I think I'd pretty much finish.
00:13:57.754 - 00:14:13.100, Speaker A: Yeah, I just wanted to ask, in the theorem, when you define this cb using phat in there, p hat hd. So, are you going to use the CBHD in the algorithm? And if so, then do you have to always compute it using p hat?
00:14:13.212 - 00:14:14.220, Speaker B: Yeah, you always need to.
00:14:14.252 - 00:14:16.364, Speaker A: Are you maintaining the model as well?
00:14:16.524 - 00:14:18.668, Speaker B: So these are all model based, but.
00:14:18.676 - 00:14:20.344, Speaker A: Then isn't it like a model.
00:14:22.084 - 00:14:22.372, Speaker B: Or.
00:14:22.388 - 00:14:29.932, Speaker A: This is the model based. But I thought you were going to give a way to construct a model like the value optimistic algorithm using.
00:14:29.988 - 00:14:35.374, Speaker B: Yeah, so we're considering value optimistic algorithms where we use this empirical density here.
00:14:37.274 - 00:14:48.106, Speaker A: Okay. Okay, so you'll maintain a p hat, but the, but you will not maintain it a, like a set of p. Yeah, exactly.
00:14:48.170 - 00:14:57.482, Speaker B: So we don't have this confidence set anymore, which is what's often complicating the implementation without optimizing over that confidence. All right, thanks.
00:14:57.538 - 00:15:00.554, Speaker C: Did you wash your hands? Have you folded?
00:15:03.854 - 00:15:40.852, Speaker B: Okay, so once with this result here, we can add a fourth step, if we like, to our framework, where we just need to bound this sum of these confidence terms. So this is often. It's okay, it's not normally too hard to do. And so we'll illustrate this with some examples. So, if we consider Ucrl two for, for example, here, we just define our primal distance using the l one norm. And it's known that with this distance metric, we can set epsilon. So like the bound on how far we can go to be of order square root s over n.
00:15:40.852 - 00:16:30.566, Speaker B: And this will contain the true transition probability with transition density with high probability, then our conjugate d is going to be epsilon. So the square root s over n times the span of v. So when it comes to summing up our confidence bounds, what we can just bound the span of v by h because we're in the episodic setting. And then we can sum over these, which terms which are basically going to be of order one over square root n for every state action pair that we visit. And so that will sum up quite nicely to give us regret of order square root tip. And that's the same as the regret bound that was shown in the original UCRL two paper. And similarly, we can incorporate several other algorithms that already exist into this framework.
00:16:30.566 - 00:17:00.614, Speaker B: So we can use Bernstein confidence intervals for UCRL two b. We can use the KL divergence to define them, or we can use a chi square distance to define our confidence intervals. However, in the tabular setting, all of these approaches have suboptimal dependence on s, which we haven't been able to remove so far. Are there any questions so far?
00:17:03.874 - 00:17:11.586, Speaker A: So, do you think the dependence on s. So this, so this dependence, extra dependence on s is compared to the.
00:17:11.610 - 00:18:02.514, Speaker B: Bounds for the model based, the purely value optimistic ones. So, like UcBVI managed to get rid of. So get rid of the square root s. But we haven't managed to find a valid distance metric in the primal that corresponds to UCBVI. So we can also extend this to factored linear mdps. Here we make an assumption that our transition matrix can be factorized by a feature matrix and some unknown matrix m. So this is going to be the key quantity determining the transitions, whereas this vector phi corresponds to known features.
00:18:02.514 - 00:19:01.694, Speaker B: And this means that for every policy PI, there exists a theta such that the q function can be expressed as an inner product of the feature vector and this theta. And again, we can, in the case where there's no uncertainty so we don't have to estimate the transition density. There is again, duality results, which we've shown in our paper, I think perhaps existed before, but we're not sure. And, yeah, so these basically show that we can again define occupancy measures and we need to do a little bit of projection here. But what we get is we get that in the value optimistic algorithms, we can define a theta hat and then use that to define a value. So then the natural question is how we incorporate uncertainty. And what we're going to do is we're going to estimate this matrix m.
00:19:01.694 - 00:19:33.284, Speaker B: And it can be shown that for any vector v m minus m hat is a vector valued martingale. So we can apply known concentration results to this and get a confidence set around m. The question is then how we use this to build algorithms that fit into our framework. So there are two ways that we can do it. In the first, we construct local confidence sets. So this is similar to what we were doing in the tabular setting. We're defining confidence sets for each xa pair.
00:19:33.284 - 00:20:27.744, Speaker B: And then in the dual space, we see that the confidence bound added to the value function is the weighted inverse norm here. And the algorithm that this presents in the value optimistic framework is equivalent to LSvBI, UCB by Qijian and others, I think. And this, we get the same regret bound as them. However, our model optimistic perspective allows us to find a new analysis framework for it. Alternatively, we can use global confidence sets where we directly constrain these matrices. And this again will give us some confidence terminal, which we can show is almost equivalent to the Eleanor algorithm by Zanetta et al. And again, we get the same regret as them.
00:20:27.744 - 00:20:34.504, Speaker B: And again, benefiting from a new probabilistic perspective by using the model optimistic framework.
00:20:36.524 - 00:20:39.116, Speaker C: Can I ask a nice question?
00:20:39.260 - 00:20:40.024, Speaker B: Yep.
00:20:41.764 - 00:20:45.628, Speaker C: Maybe you can go back to the previous slide where there was the confidence set.
00:20:45.676 - 00:20:45.860, Speaker B: So.
00:20:45.892 - 00:20:55.704, Speaker C: So the number of parameters that is in m is too large, right. But somehow the number of parameters not going to show up.
00:20:57.404 - 00:21:02.716, Speaker B: So you mean when we do these global confidence sets here, I mean like global.
00:21:02.860 - 00:21:12.710, Speaker C: Yeah, like, yeah, like here. Like on the previous slide, it seems that. Yeah, I didn't have time to.
00:21:12.822 - 00:21:13.646, Speaker B: Sorry. Yeah.
00:21:13.750 - 00:21:36.994, Speaker C: Understand what's going on here. So you're boning not the difference between m and m hat. M has, I don't know, like s times d parameters or something like that. And s is big. Right. And you don't want s to show. So basically you are projecting it through for some fixed vector.
00:21:37.474 - 00:21:38.210, Speaker B: Yeah.
00:21:38.362 - 00:22:10.784, Speaker C: And for any fixed vector, this is what's happening. But this is an s dimensional vector. So then, like, how do you use this in the proof? So it seems that the confidence set is not really for, you know, the difference between m and m hat in some norm, but the projection of m and m hat through some vector v, which is an estimation of vector high dimensional vector. So that is something that you have to do with this to be able to use this. So how does this fit to the story that you were presenting before?
00:22:11.244 - 00:22:26.852, Speaker B: So we, when we define the confidence sets using m, using like, some distance in terms of m, we consider the supremum over all of some feasible class of value functions.
00:22:26.988 - 00:22:27.564, Speaker C: Okay.
00:22:27.644 - 00:22:33.624, Speaker B: So we consider taking van, it's a covering argument. Yeah.
00:22:34.744 - 00:22:45.844, Speaker C: Shouldn't that bring out s? Basically, like in a, like, if you take a soup over all possible, then, like, an s is going to appear on the right hand side.
00:22:46.664 - 00:22:51.352, Speaker A: Square root s, I think. Yeah. An extra square root. Yeah.
00:22:51.448 - 00:22:52.124, Speaker C: Yes.
00:22:54.104 - 00:22:56.848, Speaker A: I mean, compared to, compared to the value function.
00:22:57.016 - 00:23:00.284, Speaker B: Yeah. It's the same as the covering argument that you use.
00:23:00.644 - 00:23:31.196, Speaker C: Yeah. But like, in the red bound, veritas doesn't appear. And so I'm really wondering about what, what, what's going on. Yeah. You only need to cover the set of linear value functions, pretty much. It's not all value functions, but are somewhat smooth. So when you, you need to redefine this duality in terms of, like, what are the value functions that may actually appear here on this linear model? And then once you do that, then you're done.
00:23:31.340 - 00:23:31.676, Speaker B: Okay.
00:23:31.700 - 00:23:37.180, Speaker C: So that's a little tweak that you need to add to the. Or. Like, maybe it was part of it.
00:23:37.212 - 00:23:41.344, Speaker B: But I just, no, I don't think I explained that well enough. So thanks for clarifying.
00:23:42.324 - 00:23:55.772, Speaker A: But do you think, like, just following up on that, like, do you think that might be the source of not matching the dependence on s? Exactly. Compared to value function based approach earlier.
00:23:55.908 - 00:24:17.624, Speaker B: So earlier in the previous. Yeah, we think that. So here, when we're using these global confidence sets, we think that we're doing better. And the reason is because we're using global confidence sets. And so an idea that we had about how to improve the dependence on s would be to try global confidence sets in the tabular setting as well.
00:24:23.544 - 00:24:28.216, Speaker C: So, by the way, by global, you mean that it's not stage wise or like.
00:24:28.360 - 00:24:32.248, Speaker B: No, it is still stage wise. It doesn't depend on x. Yeah.
00:24:32.336 - 00:24:34.056, Speaker C: The statewise component.
00:24:34.080 - 00:24:35.684, Speaker B: Not statewise, exactly.
00:24:39.264 - 00:24:42.048, Speaker A: You have like three minutes or so. Sorry for asking.
00:24:42.096 - 00:25:09.394, Speaker B: I think this is basically the last slide. So it's all good. Um, yeah. So, just to summarize what we've talked about. So we've shown that there's a relationship between model optimistic and value optimistic algorithms, and presented a framework for designing, analyzing and implementing such algorithms in tabular setting. We've also been able to extend this to factored linear mdps. However, as we've been discussing, there's still a gap in the tabular setting.
00:25:09.394 - 00:25:49.414, Speaker B: So a future direction for work would be to try and close that gap, which could be defining global confidence sets, the tabular setting, or just thinking a bit more carefully about a distance function. There's also been some work on related work on robustness, so it would be nice to combine those ideas together. For the average reward setting, we think most of the structural results will carry over, and then it's just how we use this to be able to get an efficient algorithm. And lastly, we can always consider removing the linear assumption and considering more general function approximation. So here are some references. Thanks.
00:25:53.274 - 00:26:03.454, Speaker A: Thank you. Kira. Yeah, we have time for 1 minute for like some questions, and I'll ask the next speaker to set up. Emily?
00:26:06.974 - 00:26:10.234, Speaker B: Yes? Should I try to share my screen?
00:26:12.174 - 00:26:24.022, Speaker A: I see one question for Kara. Could you please repeat the last explanation? Sorry, I missed the part it asked you. We still need to maintain hat p, but not a whole set of p. Yes.
00:26:24.078 - 00:26:44.034, Speaker B: So we need to, if we consider model based value optimistic algorithms, they still use an estimated piece p hat to construct the backwards recursion step. But we don't need to care about like the confidence set around p hat. We just add the exploration bonus to the value function.
00:26:44.614 - 00:27:12.708, Speaker C: So, related to that just very quickly. For this linear fact that MDP is though, I think you just need to keep all the data just, and you don't need to have these big matrices around because you need these inner products only this matrices appear somewhere and then you can simplify things so you get exactly back. LSBI UCB yeah, sometimes you can do the simplification maybe.
00:27:12.876 - 00:27:13.744, Speaker B: Okay.
00:27:15.164 - 00:27:34.134, Speaker A: Yeah, I do feel like the worst case v might be the, in the conjugate, might be the issue of issue with the tabular setting dependence on s like that's what we always felt whenever we tried to improve the bound that this because the p hat is really very correlated with the value function and.
00:27:34.514 - 00:27:35.610, Speaker B: Yeah. Yeah.
00:27:35.682 - 00:27:36.378, Speaker A: I don't know.
00:27:36.466 - 00:27:43.574, Speaker B: Yeah, we did look a little bit at trying to sort of. Sorry, I'll leave this till later. I don't want to eat into Emily's time.
00:27:46.434 - 00:27:49.954, Speaker A: Right. Yeah. I'm excited to invite our next speaker and.
