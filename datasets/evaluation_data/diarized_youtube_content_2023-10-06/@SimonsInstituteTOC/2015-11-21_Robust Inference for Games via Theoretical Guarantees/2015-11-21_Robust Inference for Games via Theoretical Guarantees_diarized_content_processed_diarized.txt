00:00:00.280 - 00:00:51.662, Speaker A: Well, first of all, thank you so much to the organizers, who hopefully are going to come for this wonderful conference so much. It was amazing. And so the talk that I'm going to give today is going to be joint work with Daryl Hoy, who is now a postdoc in Maryland, and Vasila Circanis, who is on the market and who is a postdoc in Microsoft research. And so this project really started with me kind of not knowing a lot about AGT a few years back. And I heard about this wonderful concept of the price of anarchy. And it seems really magical to an economist that it seems to be solving really, really complicated problems. And so my first thought was, well, what can we do with it in order to aid inference? And essentially, this project was all started from that very simple point.
00:00:51.662 - 00:02:01.424, Speaker A: And I think that it kind of led to, from my perspective, deeper understanding for us of what the price of anarchy could be. So, first of all, let me just kind of give you a little bit of a background in terms of how a structural economist is thinking about empirical modeling. And I think Susan Athey, on the first day of the conference, kind of gave this hint of the idea of how this approach is going, but just to kind of put you in the same framework. So our goal is really going to be the goal of designing a better system, having the observations from the current system. And so we want to use the data, and we want to use the data in order to make informed decision of how to design a new system. And the system is going to involve some strategically competing agents. And so, first of all, in order to make any predictions in that environment, we're going to have to make a stand that while the agents are responding and we actually can model how they're responding to the system and to each other.
00:02:01.424 - 00:02:57.354, Speaker A: So we're going to have a model, and in most cases, this is going to be a model of the game. And so when we want to make a prediction in such an environment, then our only kind of, our only framework that we can use in this case is going to be the framework of building the model and trying to use that model to generate those predictions other than, rather than maybe trying to run a regression and trying to extrapolate somewhere. And so the reason why the standard regression approach is not going to work is because the newly designed system has never been observed. We don't have that state of the world. So prediction is only going to be possible when we go after the primitives of the model. So the primitives of the model could be the preferences of the agents, they could be some kind of equilibrium selection mechanism. So those are going to be the primitive components of the game.
00:02:57.354 - 00:04:13.954, Speaker A: And so today I'm going to focus on the auction design, and most of the applications I'm going to be talking about are going to be the applications to the sponsored search auctions on Microsoft's Bing platform. So basically, just to sum up, the mindset is we want to make counterfactual predictions. We want to predict the system that we never observed. The only way for us to do that using the current data is to have a model of how the agents behave and to recover the parameters of that model, the primitives of the model, the preferences of the players participating in the game, in order to simulate what's going to happen in your system, aka the new equilibrium. So the question of counterfactual prediction are usually the question of looking at some concrete mechanism to which we're going to compare the current mechanism and say something about that mechanism. So we want to say what are going to be in the current circumstances, what are going to be the parameters of the optimal mechanism. Another, also very popular concept is going to be trying to figure out what's going to be the direction of improvement of the current mechanism.
00:04:13.954 - 00:05:30.184, Speaker A: So what are going to be the changes of parameters such as say the reserve price in the auction, whether we're going to raise it or lower it, in order to improve the properties of the mechanism. And so, of course, in large industrial systems, we want the solution to this problem to actually satisfy the standard condition so that we can actually deploy it in the industrial scale. So we want the solution to become computationally efficient, we want it to be scalable and preferably, we also want to make it robust to modeling assumptions. So we don't, as much as we like, we, meaning economists, as much as we like taking a stand on a particular model and trying to squeeze as much out of that model. We want to be somewhat more flexible in terms of, well, if there are some deviations from that model, we don't want our predictions to become completely obsolete and become completely non robust to that. So let me just walk you through one of such structural exercises. And this is going to be a very simplified example, but I think it's going to really provide the point that why we need to do a project like we did in this particular case.
00:05:30.184 - 00:06:27.772, Speaker A: So basically, we're going to observe the data that are going to be coming from the auction system, and the standard assumption is going to be, well, the bidders are going to be happy with their bids. So they've done their optimization and the system is going to be assumed to be in equilibrium, meaning that the bidders are going to be best responding to the uncertainty of the bids of their opponents. And so the key observation here is that if the system is truly in equilibrium, then we actually are observing that equilibrium distribution of uncertainty. So now in order to model what the agent is doing is we just need to take that empirical distribution from the data. And in that empirical distribution, we're going to compute what's going to be the best responding bit to the distribution. And that's going to give us a clear idea of what the preferences of the agents are if we have the model of the preferences. So basically, given the bid distribution, we're going to solve for the optimal bid strategy.
00:06:27.772 - 00:07:23.450, Speaker A: We're going to then invert the bid strategy in order to find the parameters, the preference parameters, such that make the currently observed bid of this bidder optimal. So that's the kind of very simplified version of what's done in the structural inference. So this approach is very, very dominant in the structural inference. And so the key features that the economists are trying to accomplish while estimating that model is first of all, we need to make sure that the model is identified, meaning that if we have a distribution of the data, there's going to be a unique set of primitives that we're going to be able to recover from the data. So that's very important because if there are multiple sets of primitives, most standard models are actually going to be in trouble. I'm going to show you moving forward how we can relax that. But in principle, we are really, really worried about edification.
00:07:23.450 - 00:08:10.782, Speaker A: So then once we made sure that the model that we've specified is going to be identified, we're going to use the data, infer those primitives and then use those primitives in order to make those counterfactual predictions. So we're going to compute the new equilibrium in the new system using the parameters of preferences that we've recovered. And so right off the bat, we're going to actually face quite a few challenges. So first of all, and I think Susan again, she's been talking about it, so most economists take a really strong stand on what the actual model is going to be. So we're trying to squeeze as much as we can from the data. So most of the models are going to be exactly identified. So there's going to be no parameters that are going to allow us to test the models in a meaningful way.
00:08:10.782 - 00:09:18.246, Speaker A: So it's really hard for us to get variation that's going to say that, well, we actually truly believe that these are the agents preferences. So there's going to be some inversion from the bids to the values and then we're not going to know whether this inversion is robust to, say, the assumption of utility. This is actually going to also leading to the problems with inference, meaning that, well, in this case we're not sure whether this is the correct model. Local deviations from that model can actually generate pretty large deviations in the estimates of the primitives. And then if now we're thinking about using those primitives to compute new counterfactual equilibria in the new system, well, we're going to, that error is going to propagate and that's going to generate even further problems in those predictions for the new system. So let me just again proceed with a very simple example of an auction. And I'm going to consider the first price auction with an exogenous allocation rule.
00:09:18.246 - 00:09:43.366, Speaker A: So this is going to be as simple as it gets. So we're going to look at the utility of the bidder in this auction. The allocation rule is going to be x. So each quantile in the bid distribution is going to be allocated with probability x. The bidder is going to collect the surplus, which is going to be value minus the bidder, this bidder. So this is the first price auction. So we have the allocation rule, which is exogenous.
00:09:43.366 - 00:10:07.360, Speaker A: It's allocating quantiles of the bid distribution. We're going to assume that everything is going to be nice. So this is just, again, for simplicity. So there's going to be a mapping from the quantile space to the space of bits that's going to generate the quantile function of the bit distribution. We're going to denote it b of Q and under some additional. Yeah.
00:10:07.552 - 00:10:14.284, Speaker B: So is there, this is like, are you assuming that the bitters are somehow symmetric and they all are using the same.
00:10:16.624 - 00:10:26.644, Speaker A: Yes, yes. So everything is symmetric. As simple as it gets. I just, I want to abstract away from any problems or complications. So this is just plain vanilla.
00:10:27.464 - 00:10:29.564, Speaker C: So Xq is related to Bq.
00:10:30.024 - 00:10:46.426, Speaker A: Xq, it's exogenous. Let's just say that it's exogenous. It's not even related to BQ. I'm just going to give you what the allocation is. So again, this is done just to make it as kind of transparent as possible. So in real auctions, it's going to be a function of BQ and it's going to be even more complicated, then.
00:10:46.610 - 00:10:50.442, Speaker C: If Xq is exogenous, you don't need to make symmetries about the bitterness. Right?
00:10:50.498 - 00:10:55.682, Speaker A: Sure, sure. But I'm just trying to, again, like this is, you do, because there is.
00:10:55.698 - 00:10:58.214, Speaker B: A b. I mean, x, q is a.
00:10:59.194 - 00:11:05.804, Speaker C: What I say is, if we're talking about all the builders together, xq is a function of the q. But if xq is exogenous, but I.
00:11:05.804 - 00:11:35.626, Speaker A: Think he's using it to illustrate, this is just an illustration. This is not going to be the core of the stock. This is just an illustration. Don't yell at me just yet. So let me just then write down the first order condition for the bidder in this case. And the first order condition is going to lead to a fairly simple, again, exogenous allocation rule. There is no subscript I.
00:11:35.626 - 00:12:17.198, Speaker A: Everything is super symmetric. So we're going to have the first order condition, that's going to be relating the quantile function of the value distribution and the quantile function of the bit distribution. And the unfortunate thing here is that there's going to be the derivative of the quantile function of the bit distribution. So that's going to be really problematic in the sense that we are going to have to infer that object. So on the inference side, though, it seems, well, it seems pretty good, because now we have this equation on the left hand side, this is the object that we don't know and we don't observe, and we want to infer from the data. On the right hand side, it seems that we have a combination of objects that we can potentially infer from the data. So the bit distribution is observed in the data.
00:12:17.198 - 00:12:57.370, Speaker A: The derivative of the bit distribution can be observed in the data. So what economists do is they put hats everywhere once they have an equation like that. So hats, meaning that we're going to replace each object that we have here with the object that we observe from the data. So this is the empirical analog of that object that gives us the estimator for the primitives of the model, which in this case is going to be the value distribution. So now once we have the value distribution, we seem to be completely kind of opening our path to simulate what's happening in any kind of actual auction. So once we have the value distribution in this auction, we can simulate what's happening in all pay auction, whatever auction we prefer. So now if we think about the inference.
00:12:57.370 - 00:13:32.724, Speaker A: So the two objects that we need to infer here, given that the allocation rule is exogenous, is going to be the quantile function of the bit distribution. And its derivative. So the quantile function of the bit distribution is inferred in a standard way. That's just the order of statistics or CDF, whatever you prefer, the derivative is going to be significantly more problematic. So the inference of derivatives is in general pretty bad. And so let's just say that the bit distribution is exponential. So in this case, the derivative of the quantile function of the bit distribution is essentially inversely proportional to the density.
00:13:32.724 - 00:14:24.874, Speaker A: So if we're now going to compute the variance of this object when the distribution of bits is exponential, then we're computing something that's going to be an integral of one over the density. So this integral is going to be clearly diverging. So this means that in fairly generic settings, we actually got an object that doesn't have the finite second moment. So now if we're trying to do summations or some kind of factuals, we're basically dealing with the objects that are going to be. Well, first of all, this means that this object is going to be really sensitive to the tail behavior of the bid distribution. And second, so if we're going to make some local deviations, and by local deviations, I'm going to consider just small change in the tail of the distribution. The properties, the statistical properties of the object that we're inferring, which is the value distribution, are going to be drastically changing.
00:14:24.874 - 00:15:13.334, Speaker A: So let me just give you something more formal. So it turns out that we actually have a theorem which is going to say that if we're going to try to measure the performance of a counterfactual, for instance, this is going to be the revenues of the counterfactual mechanism. We're going to find two distributions of bits which are going to be close in the infinity norm. And here, as you can see, what's happening is that what's going to matter the most is going to be the tail of the distribution. So if the distribution, say, has unbounded support, the difference in the infinity norm can be very tiny. But if it's way in the tail, that's going to matter the most from the perspective of this object. So I can actually generate two distributions that are going to be closed, but they're going to be generating drastically different outcomes.
00:15:13.334 - 00:15:57.864, Speaker A: So this actually cannot be fixed even if we have a bounded support. So this is a non generic problem. If we have bounded support and the density can be kind of dropping off very quickly. Towards the end of the support, we're going to have exactly the same problem. So how do we solve this problem? Well, it turns out that this problem has been thought about both in economics and in computer science and statistics, specifically image processing. And the idea is going to be to basically try to lump all of the distributions that are going to be closed. So now, instead of working with a single prediction for the counterfactual revenue, we're going to work with a set of predictions that are going to be generated by all of the densities that we consider to be close.
00:15:57.864 - 00:16:22.504, Speaker A: And so, well, the next question is going to be, can we somehow, as you can see, this is a very simple situation where we don't even need to compute anything to do that inversion. If we need to compute something, this is going to be really difficult, because now we're going to have to compute this object for any instance of potential bid distribution within some range that we're considering close. And that's going to be a very computationally expensive problem.
00:16:23.004 - 00:16:23.748, Speaker C: Can I ask something?
00:16:23.796 - 00:16:24.268, Speaker A: Yeah.
00:16:24.396 - 00:16:36.324, Speaker C: Are you assuming implicitly, because you said that estimating the bit distribution is easy, the function b of q is easy raised. That makes a monotonic assumption raised.
00:16:36.784 - 00:16:37.240, Speaker A: Yes.
00:16:37.312 - 00:16:40.152, Speaker C: Why can I make that? I mean, so why do you make.
00:16:40.208 - 00:16:56.524, Speaker A: So it's. So what I should have said, estimation of b of q is easy error. So we're still going to have issues with it, but it's just easier. So B of Q is going to be estimated like you can estimate B of Q as in order statistic at Q.
00:16:59.904 - 00:17:02.320, Speaker C: There are contents in values and contents in feeds.
00:17:02.392 - 00:17:03.056, Speaker A: Right.
00:17:03.240 - 00:17:06.336, Speaker C: It's not clear to me that feeds should be ordered the same as.
00:17:06.520 - 00:17:11.480, Speaker D: I think his point is different. I think the point is that even if you make all those assumptions, you still have this problem.
00:17:11.632 - 00:17:30.116, Speaker A: Precisely. Precisely, yes. So in your case, things are going to be even more difficult. So the point is that it's clear that even in the simplest case, we need to work with the sets. And so the next question is what to do with this? So, working with sets. And so this is. Yeah, sorry.
00:17:30.140 - 00:17:36.064, Speaker D: The question is, can you accomplish something similar to what you're describing by, like, Windsor icing, by like sort of truncating the.
00:17:36.724 - 00:18:04.674, Speaker A: No. And that's. So that's another theorem. So this is a kind of completely different branch of research that I'm doing. So there's another theorem that we can prove, is that if everything is determined by the tail behavior, I can always kind of adversarially screw you. So if you're trying to windsor eyes it, then I can always choose the tails that are going to be misbehaving beyond. So this is actually a very general class of problems that have been considered in economics and now very popular.
00:18:04.674 - 00:18:48.244, Speaker A: And this is what's called the partial asset identification literature. This literature was started by Chuck Mansky at northwestern, and the basic idea of set identification is actually very simple. So this is kind of a very plain vanilla introduction, and feel free to exit the room if you feel that this is too simple and offensive. So basically, we're thinking about a survey non response, and what we want to estimate ultimately is going to be some expectation of a statistic. So suppose that it's income. And the problem that we have here is we're going to send this surveys to individuals, and some individuals are going to respond to the survey and they're going to report their income. And let's just assume that there is no strategic problems there, etcetera, etcetera.
00:18:48.244 - 00:19:35.602, Speaker A: Let's just say that everybody just reports are not reports. So the issue that we're going to have there is that not everybody is going to send us back the surveys. And what's also going to be even more complicated is that the survey response is probably going to be correlated with the value that we're trying to measure. So let's assume that the variable that we're trying to compute the expectation of has bounded support and we know the upper and lower bounds of the support. Then Mansky's suggestion to this problem was the following. Well, given that we don't have any extra information regarding how this survey response is occurring, unless we have a model about it, then the only thing we know is going to be the probability of a particular person responding and a particular person not responding. Well, maybe we have some other variables we can condition on.
00:19:35.602 - 00:20:40.992, Speaker A: But then the simplest setup, that's the only thing we have. And so then what we need to do is we need to embrace that the best that we can do for the part of the distribution that we never observe. So this is the conditional expectation of the outcome variable for people who did not respond the survey. The only thing we know for this thing is, well, that it's bounded between the upper and lower bound. So as a result, our prediction for the expectation is going to be an interval where the lower part of the interval is going to be replacing that conditional expectation with the lower bound of the support of y, and the upper bound is going to be replacing that variable with the upper bound of the support for y. So this is what's going to be called the identified set for the expectation of y. So the literature on identified sets has proliferated far into game theory and people have been trying to use it in order to infer sets of preferences, especially in the games that are so called incomplete in the sense that they can have multiple equilibria or there is some issues with rationalities of flares.
00:20:40.992 - 00:21:26.784, Speaker A: The general framework for partial asset identification in game theoretical structural models is going to include the inference for a vector of parameters, which could be functions in principle, so those don't have to be finite dimensional parameters. And what we're going to look at is going to be the distribution of the data. We're going to assume that we have infinite access to the data generating process. So in principle we can just observe this entire distribution of the data. There's also going to be an unobservable component that's going to be characterizing something like unobservable component of the preference. So this is going to be some random noise. So then the model is going to be formulated as a set of equalities and inequalities for the so called conditional moment.
00:21:26.784 - 00:22:15.360, Speaker A: So this could be, for instance, the deviation inequalities in games. So then the goal here is going to be looking at the distribution of the data which is going to be representing the distribution of this variable y. We're going to try to find the set of parameters that are going to be solving the set of parameters theta, that are going to be solving this population system of equations and inequalities. So there's going to be some set which is going to be called theta I and it's going to be called the identified set that's going to be containing the parameters that could have generated the data. So there's going to be the whole set of parameters that are going to be consistent or compatible with the model that generates the equalities and inequalities, and the data which is going to be this distribution of y. I, sorry, makes.
00:22:15.392 - 00:22:18.152, Speaker D: Sense like a robust method of moments.
00:22:18.208 - 00:22:52.674, Speaker A: Basically this is. Yes, yes, well you can, you can, but yes, you can call it that way. But so there is no inference here so far. So this is all about just what is the structure of this theta. So if we think about the properties of the identified set. So the identified set is going to be now producing the set of primitives of the game. So now if we think about, say an auction game, this is going to be the set of values, a set of distributions of values that are compatible with the observable data distribution and say the equilibrium conditions on the utilities of players.
00:22:52.674 - 00:23:44.542, Speaker A: And in some cases this data is going to be the object of interest. And if you look at most structural papers in economics. This is essentially what people are focusing on is what is going to be the set of parameters that are going to be consistent with the data. However, in most practical applications, at least, the applications I've encountered with the ultimate goal is really going to be the goal of counterfactual inference. So this is going to be something like the welfare or revenues of the counterfactual mechanism. And the values themselves, well, maybe they're in some cases they are important, but they're not that important. And so if we now think about kind of going through the path of inferring the identified set for the primitives of the model and then projecting that on some very small subspace, it seems that that there's a lot of waste at work there.
00:23:44.542 - 00:24:34.038, Speaker A: So we kind of didn't need to compute that set originally. So this is something that I mentioned before. So in game theoretical settings, the object of interest is going to be something like welfare or revenue of some optimal mechanism. And so the kind of the approach that we're going to pursue if we're going to recover this identified set projected is going to be difficult. And especially this is going to be difficult when we try to embed it into counterfactuals, because now imagine that we have a set of possible values for all of the players. So each point of the set will need to correspond to some kind of equilibrium. So we'll need to compute an equilibrium for each point of the super multi dimensional set that we've generated for all of the players.
00:24:34.038 - 00:25:43.552, Speaker A: And then that's going to be even harder if there are going to be issues with using some nice best responses. So if there's going to be deviation from the optimal best response, things are going to be very difficult. So what we want to try to do is we want to try to bypass somehow this entire process of inferring this theta I and then projecting it and replacing it with something nicer, even though we're going to generate potentially something larger than what we're going to get if we get the status set here. And so the idea here is going to be to maybe try to reinterpret the idea of the price of the anarchy first introduced by the Kotzupis and Demetrio. So that probably should be familiar to all of this audience. So basically we're looking at the worst case ratio of in our case, we're looking at the worst case ratio of the welfare of the optimal auction and the auction that we're currently running on the data. And there exists really nice, very theoretically attractive methods of evaluating the price of anarchy that are based on just unilateral deviations of players.
00:25:43.552 - 00:26:19.074, Speaker A: So these are kind of very computationally straightforward exercises. And so this is found theoretically useful. And the price of anarchy has very nice properties for a lot of familiar games. And in simulations, at least in my experience, we're actually finding something a lot better when we're looking with concrete distributions and concrete mechanisms. So if we think about. And so this is my own interpretation. So if somebody disagrees with that, please say something.
00:26:19.074 - 00:27:16.268, Speaker A: So my own interpretation is that basically, if we're looking at the kind of classical price of anarchy, essentially the classical price of anarchy is a property of the mechanism. So we're looking at all possible values, and then we're trying to see what are the properties of this mechanism over all possible values as opposed to, say, the optimal mechanism. Now, if we think about kind of trying to somehow blend it with reality, if we're trying to combine it with the knowledge that we have, well, not all of the value realizations are going to be equally. Probably they're going to be realized with equal probability. And so now what we want to try to do is we want to try to somehow infer the idea of what values are going to be most realistic by observing how the bidders play. So if the bidders play low, maybe their values are low. If they bid high, maybe their values are high.
00:27:16.268 - 00:28:08.862, Speaker A: That's informative. So now what we want to do is we want to somehow restrict the price of anarchy, making it the worst case scenario under the constraint that the distribution that we observe is compatible with the observable place of the players that we observe in the data. So this is what we call the empirical price of anarchy. And in this case, this is going to be now a combined property of the mechanism and the distribution of values. So basically, if we are able to solve this problem, we're going to also say something about the primitives, about the primitives of the game, which are going to be the values of the players. And this is also going to be very closely related to the notion of identified set. And if we're looking at the welfare, this is going to be, say, the identified set for the optimal counterfactual welfare, because we never run the optimal mechanism here.
00:28:08.862 - 00:28:12.622, Speaker A: We only run the mechanism that we do run. Yeah.
00:28:12.798 - 00:28:16.974, Speaker E: So do you mean that this is off of the one distribution, or do.
00:28:16.974 - 00:28:17.614, Speaker A: You think that the.
00:28:17.694 - 00:28:22.846, Speaker E: Are you saying that this is the worst case over all of the possible distributions that are still consistent with the data?
00:28:22.910 - 00:28:28.718, Speaker A: Yes. Yes, the second one. So this is all possible distribution that are consistent with the data.
00:28:28.846 - 00:28:43.402, Speaker F: So just to clarify that again. Yeah, I would get the same answer if I did this identified set stuff you were talking about about, and then looked over all those distributions and found the worst case price, like the worst case ratio in that set.
00:28:43.578 - 00:28:55.454, Speaker A: Well, it's not going to be quite so we don't have universal tightness results. So it's going to be contained. And I'll show you the picture. Yeah, I'll show you the picture. How much time do I have?
00:28:56.394 - 00:28:57.562, Speaker D: Just a little over ten minutes.
00:28:57.618 - 00:28:58.734, Speaker A: Okay, excellent.
00:28:59.434 - 00:29:10.494, Speaker E: Okay. Do you have like a concrete, like, set of distribution? It can be one of these and it cannot be something else. Or do you have some sort of distribution that. Okay, it could be something else, too, but with low probability.
00:29:10.654 - 00:29:37.632, Speaker A: So in this work, we're not going to give you the. So we're working exactly on that particular question. So in particular examples, we can give you those distributions. What I'm going to kind of give here is going to be more like, what is the recipe for computing the optimum given the data? So I'm not going to tell you what exactly is violated, but that affects.
00:29:37.648 - 00:29:45.524, Speaker E: The definition, your definition of price of anarchy, because if you have those tails, those don't go away when you take the worst case.
00:29:46.544 - 00:29:51.044, Speaker A: So by the tails, you mean the.
00:29:51.464 - 00:29:53.964, Speaker E: Distributions that could be but probably aren't?
00:29:58.284 - 00:30:04.904, Speaker A: I don't think that they're going to. So I don't think that they're going away. But. So how's the problematic.
00:30:06.164 - 00:30:14.788, Speaker E: So if you have a set of distributions, you know, it would be one of those and it can't be anything else. You take the worst case over that. I understand how this would be well defined.
00:30:14.836 - 00:30:15.212, Speaker A: Right.
00:30:15.308 - 00:30:22.760, Speaker E: If you don't have that because of data, you know, you could have, you could have basically any distribution with some probability, then I don't understand.
00:30:22.872 - 00:30:36.480, Speaker A: No, no, no. So that's not what I said. So I said that those distributions probably exist. I just, I'm not going to be able to. The distributions that are definitely not consistent with the data. So those distributions definitely exist and they definitely exist in.
00:30:36.512 - 00:30:40.416, Speaker C: What does consistent mean? I mean, you computed some expectations of some quantities.
00:30:40.520 - 00:30:41.136, Speaker A: Yes.
00:30:41.280 - 00:31:02.184, Speaker C: Okay. I mean, you know, with some time, the probability could be some crazy distribution that generated those expectations. I think what Thomas is asking is it should be a weighted sum or something like that. Like, you know, you are, you know, you know, every possible distribution explains the data with some power, and you should be doing some weighting.
00:31:02.964 - 00:31:16.284, Speaker A: So you're doing that weighting already, because what you're observing is going to be the distribution of actions. So your distribution of actions is already a mixture distribution, and that mixture contains all possible things that could have been generating the data.
00:31:18.624 - 00:31:36.104, Speaker G: So the confusion everyone is having is everything is, at least in principle, consistent with the data, but maybe with a vanishingly small probability. It seems like, unless you set a confidence level. Yes, you haven't ruled out anything, and hence your price of anarchy should be the same as the theoretical bad.
00:31:36.184 - 00:31:53.374, Speaker A: Sure. What I'm talking about here is a population object. Up to this point, there is no data. So let's assume that we just know the distribution of the data. We just know that. So in that case, it's just a single distribution. There is no noise there.
00:31:53.374 - 00:32:01.014, Speaker A: Yes. Right. So there is no data noise here whatsoever.
00:32:02.474 - 00:32:09.270, Speaker H: You have a huge amount of data. So everything that could be inferred with some probability is inferred for sure.
00:32:09.342 - 00:32:09.662, Speaker A: Yes.
00:32:09.718 - 00:32:13.246, Speaker H: And then there's something that's unknowable, no matter how many they have.
00:32:13.270 - 00:32:37.922, Speaker A: And that's where you take the word exactly. Yes, exactly. Exactly. And so, and then, I mean, at the end of the day, we're going to work with an empirical distribution, and we know how quickly the empirical distributions are going to converge to the true one. So that's going to determine our error bounds. So when population model, yes. So in the end, we're going to plug those empirical distributions in and just produce what's going to be the price of the anarchy.
00:32:37.922 - 00:33:08.656, Speaker A: So, given that I'm running out of time, I'm going to skip the pictures. And so the bulk of the stock is going to be about position auctions, specifically generalized second price auctions that bing runs. And so these are the click. Weighted generalized score. Weighted generalized second price auction. They run in continuous time, meaning that there's going to be a lot of uncertainty that's going to be introduced by the user traffic. Each query is going to be score weighted.
00:33:08.656 - 00:33:49.806, Speaker A: So the scores are going to be generated for all of the bidders. And given that the bidders are not able to respond to the queries in real time, they're going to be responding to expectations of queries. And so in each query, the scores are going to be regenerated, generating this very smooth option. So at the end of the day, things are going to be smooth out. So there's not going to be step functions like in standard generalized second price auction. So this is going to be an expectation of the generalized second price auction, where the expectation is going to be taken with respect to the realizations of the score uncertainty. So now if we think about the decision of the player, in this case, the decision of the player is going to be to maximize the utility from clicks.
00:33:49.806 - 00:34:39.683, Speaker A: So there's going to be a value per click which is going to be multiplied by the expected quantity of clicks that this bidder is going to receive in a given query, which is going to be now a smooth function of bids minus the expected cost. And that expected cost is again going to be some smooth function of bid because we're taking expectation over high uncertainty distributions. So then if we think about implementation of full information mesh equilibrium in this case, now, given the competing bids, we're going to be able to compute those smooth functions. We're going to be just able to simulate them very quickly. And then, as Hal Varian was telling yesterday, what we need to do then is we need to take the derivative of the cost and we're going to get the value equals marginal cost. Very simple, seems very straightforward. The problems are going to arise immediately as you go to the data.
00:34:39.683 - 00:35:07.136, Speaker A: Well, first of all, in order for this model to work, we need the marginal cost to be monotone. And in a lot of the queries it's clearly not. So that approach of equating value to marginal cost is going to break down in 3 seconds in most queries, even in Google, I'm sure. The second thing is that there is the click. Yeah, not even monotone. In some queries, you increase your bid and you cost. Sometimes it happens.
00:35:07.136 - 00:35:43.584, Speaker A: Yes, yes, it happens. Yeah. So the second thing that's going to happen is that the click function is going to have flat spots. And especially this is going to be the case for the top bidders, because when we have Amazon, which dominates everywhere, we're never going to see anybody above Amazon, the click function is going to be, well, essentially flat. We're dividing by something that's equal to zero. We're going to get infinite value. So there's not going to be any value in the marginal cost inference.
00:35:43.584 - 00:36:34.098, Speaker A: And then there's going to be a bunch of other things such as drifting distribution of uncertainty, deviation from the optimal best response. A lot of the bidders are actually not changing their bid that frequently. So we get really large deviation from the standard equilibrium model. And so in this context, partial identification seems to be a natural next step. But then there's going to be a problem, because now we're going to have to recover those large sets of values that we're going to have to project on the set of bits and the approach that we're going to take is going to be this idea of using the distribution of the data in order to provide the bounds by passing the computation of those alternative outcomes. So here's the definition that we're going to use. So this is going to be the definition of the empirical price of anarchy.
00:36:34.098 - 00:37:41.052, Speaker A: So we're going to have the distribution of uncertainty, which we're going to, which we're going to subscript by Theta, and we're going to compare the welfare of the optimal auction with the welfare of the auction that we actually are running. And we're going to take the soup over all possible distribution, over all possible realizations of values in the strategies within some given class, such that the distribution of the bids that we're actually observing. So again, this is the true distribution that we're observing is going to be exactly the same fB. So this is exactly what we know. So what turns out to be a useful approach is going to be the approach of so called revenue covering. So it turns out that this price of anarchy object is going to be able to, we're going to be able to recover it independently without computing this constraint optimization problem, where instead we're going to be able to provide an upper bound for this object using this approach. So let's define the thresholds, and the thresholds are going to be defined by the so called the inverse price per click function.
00:37:41.052 - 00:38:26.804, Speaker A: So this is going to be the price per click that I'm going to have to pay in order to receive exactly z clicks. So everything is an expectation here. And then I'm going to compute the average price per click as essentially an integral. So if I'm paying the average price per click for every click batch that I'm getting, how much in total I'm going to have to pay. So the next step is going to be to define the revenue covering. So we're going to say that the strategy profile of the auction is mu revenue covered. When the revenue of the auction is going to be multiplied by this factor, Mu is going to be greater for any allocations that we're going to impose of the sum of those thresholds across the bidders.
00:38:26.804 - 00:39:25.850, Speaker A: And so then the auction is going to be, in general, mu revenue covered for any strategy profile this inequality is going to hold. So why is this object of interest? Well, it's of interest, first of all, because we can establish this inequality. So this is the sum of utility, and the weighted threshold is going to be bounded from above by some weighted welfare that is going to be prescribed to this bidder. So this is the quantity of clicks times the value which is the total welfare that's generated for this bidder. It turns out that the price of anarchy is going to be bounded from above by this function of the mu, the corresponding revenue covering parameter. So why is this exciting? So the revenue covering parameter is the parameter that's essentially determined by the revenue that we directly observe in the data and by this thresholds that we can compute from the data. So these are the functions that are characterizing the price per click.
00:39:25.850 - 00:40:16.996, Speaker A: So our strategy to then bound the price of anarchy from above is going to be to use a version of this formula. We're going to compute the Mus. So mu's are now functions of simple low dimensional objects that are directly observable in the data. Having the mU, we're going to be able to bound the price of anarchy, and as a result we're going to be able to bound the welfare ratio for the observable distributions of bits. So here is the relationship that I wanted to show you. So basically, now we have kind of multiple levels of things that are happening in this environment. First of all, price of anarchy is not going to be imposing any restriction on the distributions of values because it's just a supremum over all possible distributions of values.
00:40:16.996 - 00:41:15.548, Speaker A: Now we have the identified set that corresponds to the bound on the welfare that we're going to generate over all possible values that are consistent with the data, and they're consistent with a particular concept of equilibrium that we're working with in the data. Then we're going to have the empirical price of anarchy, which is going to. So in some cases it's going to be tight, it's going to coincide with the identified set. But in general it's going to be larger because it may be not ordering some of those inequalities that are going to be now binding for multiple bidders at the same time because the price of anarchy is working with unilateral deviations. And then we have the revenue covering bound that I just showed you that revenue covering bound is going to be larger slightly than the price of anarchy is going to be bounding the price of anarchy from above, but it's definitely going to be containing the identified set. So given that I'm completely running out of time, I'm just going to show you some nice pictures. So this is.
00:41:15.548 - 00:41:42.996, Speaker A: So we worked with eleven keywords on bing. These are the actual relationships between the allocations and the price per clicks. Using this price per click curves, we can compute the threshold. So this is the actual thresholds that we compute and then using our approach. And so there's a lot of numbers, the numbers to look at. And this is the inverse price of. So this is the price of anarchy that we compute using our formula.
00:41:42.996 - 00:42:35.324, Speaker A: And you can see that in principle, for most of the keywords we're looking at, the price of anarchy is actually pretty good. There's a couple of keywords that are clearly problematic, and these are the keywords that are really monopolized. We actually have a few refinements of the formula that we have in principle, we can use some other structures within the game in order to tighten the bound even further. And that gives us slightly better values for the price of anarchy. You can see that we're getting much closer to one in this case. So, given that I'm completely out of time, I try to provide a link between the literature on partial identification and econometrics, and the results from the algorithm game theory. And it turns out that the concept of the price of energy applied to data is providing useful bounds.
00:42:35.324 - 00:43:07.344, Speaker A: And in application to real data, it gives a way of diagnostic of a particular auction mechanism. So essentially, this gives us a very cheap way of seeing whether the current mechanism that we're running is deviating from the optimum for the given set of realizations of value distributions that, that are plausible using our data. And the application seems to be possible to a lot of other markets with the same ease.
00:43:14.444 - 00:43:15.796, Speaker D: Tell us a couple of questions.
00:43:15.860 - 00:43:19.708, Speaker H: Yeah, I thought you mentioned applying this to designing alternative mechanisms.
00:43:19.756 - 00:43:20.344, Speaker E: Right.
00:43:20.884 - 00:44:07.484, Speaker A: You showed just for the current mechanism and the optimum. So I think it is possible. So in the current framework, I'm going to offer you a very unpleasant solution of just using the triangle inequality, which is not going to be very nice and attractive. It seems that it's possible to narrow down the notion of the price of anarchy to some kind of like an arbitrary welfare ratio. I'm not sure that it's always applying. So it applies to very simple games, but, so I think that there is a great promise there. So if we're able to accomplish that, then we solve the problem of factual inference.
00:44:07.484 - 00:44:11.884, Speaker A: Yeah.
00:44:12.464 - 00:44:19.792, Speaker C: So you showed us some EPOA. What were the poi bounds for the setting you were looking at?
00:44:19.888 - 00:44:37.912, Speaker A: Oh, so this is the generalized second price auction. So the revenue covering doesn't even, they're not revenue covered. So that's the beauty, is that, so in our case, we don't need to worry about whether they're revenue covered or not. I'm sorry, I kind of skept over that. We actually just compute the actual empirical revenue covering parameter.
00:44:38.088 - 00:44:39.284, Speaker C: And for welfare.
00:44:41.284 - 00:44:54.212, Speaker A: As far as I know, I am not sure, actually. I don't think so, but. So it's score. Way to generalize. Second price auction. So I'm not sure somebody computed. Yeah.
00:44:54.268 - 00:45:00.260, Speaker E: Can you go to the previous slide? So this first column of numbers, that's for efficiency, right?
00:45:00.412 - 00:45:09.144, Speaker A: Yes. So this one, they're all for efficiency. So. Well, this is the ratio of the threshold. So this is mu, essentially.
00:45:09.714 - 00:45:25.546, Speaker E: So on the phrase four. So that means that our price of anarchy is like four experiment. Empirical price. And you said that that happens when there's a keyword is monopolized.
00:45:25.650 - 00:45:26.146, Speaker A: Yeah.
00:45:26.250 - 00:45:49.794, Speaker E: So help me understand that, because it seems to me that if the keyword is monopolized and somebody's the best guy, it's easy to get efficiency. That guy. Just get it. So why is volume, why is high in those cases? It seems that it should be low. Somehow it's harder to identify and you get more noise in the identification piece.
00:45:50.094 - 00:46:07.460, Speaker A: Basically what's happening is that if you have a dominant bidder, then this dominant bidder is going to significantly shape their bid. I mean, it could be. It could be, yes. So it's not. What's that?
00:46:07.532 - 00:46:08.996, Speaker B: How can it not be efficient?
00:46:09.100 - 00:46:14.212, Speaker G: He shaves a lot, and the other guys shave almost none because they. If you think about a single item.
00:46:14.268 - 00:46:16.220, Speaker F: Auction, then it's not monopolized.
00:46:16.332 - 00:46:20.004, Speaker D: Then Thomas is right. It must be the identification issue. It's that he shaved it enough that.
00:46:20.084 - 00:46:22.956, Speaker G: Well, it's a first price auction, remember? Don't think second price auction.
00:46:22.980 - 00:46:30.844, Speaker H: But that's national equilibrium, right? So it's full information equilibrium. So I'm not going to shade my bit so that I actually lose. I will shade it, but I'll still win.
00:46:31.344 - 00:46:34.712, Speaker A: So I. Okay, that's my point.
00:46:34.888 - 00:46:40.496, Speaker E: Even if you're shading, it's like your definition of monopolize means that you're a winner. But if you're a winner, you're a fisher.
00:46:40.560 - 00:46:58.484, Speaker A: So I wasn't giving you the causal interpretation of these numbers. I just gave you a simple statement of the fact. So this is, so we are observing this, and I think that this, this search phrases are problematic just because of the domination of the few players. But I'm not sure.
00:46:58.944 - 00:47:10.124, Speaker B: Is it problematic for social welfare or is it problematic for your methodology, Miss Thomason? It appears to be problematic for your methodology.
00:47:10.904 - 00:47:29.868, Speaker A: So I don't think so. That. I don't think so. Because. So we can just compute the welfare directly, right? So we can use the, we can use the value equals the marginal cost approach. And I mean, in that case, let.
00:47:29.876 - 00:47:33.356, Speaker D: Me suggest, actually, let me suggest that.
00:47:33.380 - 00:47:34.084, Speaker H: We think that this.
