00:00:02.080 - 00:00:41.960, Speaker A: Hi. Thank you for having me. My name is Tom, and I will tell you about very recent work with Alessandro Chiesa, Michael Forbes, and Nicholas Spooner. And this is the titular spatial isolation implies zero knowledge, even in the quantum world. And what I will actually show you is how to construct zero knowledge map style systems. So, zero knowledge proofs that are sound against entangled provers. Okay, so let me start by telling you a little bit about the problem and why I find it exciting.
00:00:41.960 - 00:01:13.134, Speaker A: So, let me. One thing I want to say. Thomas told me that this should have as little as possible preliminaries in complexity theory or cryptography. So, in fact, you can say it suffices to have zero knowledge about complexity. And crypto couldn't resist the pun. Okay, so what is the problem? So, yeah, here are this nice smartwatch. I will settle for a laptop.
00:01:13.134 - 00:02:31.374, Speaker A: We have a weak computational device and say that this device gets two graphs, and we want to know whether these two graphs are isomorphic or not. Now, this is typically generally a hard problem, and so this poor device might want to use the help of some, either someone more knowledgeable who might have generated the graphs, or someone which is just more powerful, like a cloud server provider, for example. So, typically, a good proof would be just to give the typical NP proof in which we give the bijection between these two graphs, and everything is nice and well. However, this reveals a lot of information about these two graphs, and in particular, it does not preserve the privacy of the poovil. So what zero knowledge proofs give us actually is a way of proving a statement. For example, these two graphs are isomorphic without revealing anything about these two graphs at all. So, in particular, we cannot give this coloring here that we see, okay.
00:02:31.374 - 00:03:42.894, Speaker A: And zero knowledge. So this is their knowledge. It was introduced by Goldbasser, Michali and Rakov. And this is a very influential and powerful notion both to cryptography, to complexity theory, and it is actually even being used in practice, for example, even in deployed systems such as for cryptocurrencies such as zcash and for blockchain technology. So it has the privilege of being both interesting theoretically, and also relevant to practice this notion of zero sorts. Okay, so what do we know about zero knowledge proofs? So, we know from a result of Goldreich, Mikali and Wigdazon that we can have zero knowledge proofs for all of NP, but under cryptographic assumptions. And now we might want to ask, can we get this information theoretically, can we get zero knowledge without any, any assumption, any cryptographic assumption? So, in the setting of, like, interactive proofs, which is something I described here.
00:03:42.894 - 00:04:45.434, Speaker A: We know this is essentially impossible, and we really have to make this cryptographic assumption. However, not all is lost. And we do have another model which is called multiprovic interactive proofs, MIP, which I'll just talk about. And as you mentioned, the original motivation for this model was we want to be able to get their knowledge without making any sort of assumptions whatsoever. Okay, so what is this model? Essentially, we ask ourselves, what happens if we have two provers, for example, rather than one? Now, if this provers can communicate, and then we can actually view them as just one unit. And so we gained nothing. So what we do is we spatially isolate them such that we can be convinced that the strategies are completely independent.
00:04:45.434 - 00:05:43.824, Speaker A: And under this scenario, we know that this spatial isolation, at least in the classical work, it means that these two provers are acting completely independent, like they're strategies are completely uncorrelated. And under these uncorrelated strategies, we can prove zero knowledge, not only for any language in NP, but for the much larger complexity class of next. So this is what we know for maps. However, if we think about the quantum setting, then this is no longer true, of course. For example, it could be the case that these two provers, or clouds here, share an entangled state. And then if they measure it, their strategies can actually be correlated. When you write uncorrelated strategies on the left, you still allow shared arrangements.
00:05:43.824 - 00:06:12.576, Speaker A: When you write uncorrelated strategies on the left, you still allow shared randomness. Yes. Yes, I do allow here it could be correlated just by a common random thing that we can. Okay, so what does it mean to take a shell in entangled state? So this is captured by the MIP model. And another talk about it. But let me just recap it. Essentially, we're saying that we have these two provers.
00:06:12.576 - 00:07:12.464, Speaker A: They're again spatially isolated, but they do share some entangled state between them, which can function in the most naive way as common randomness, but it can also do more. And now we have the question of what is the power of this model? So, notice that what we did here has two effects when we added this additional shared entanglement state. So, first of all, the provers are now more powerful. And on the second hand, so they could use this extra power to convince the prover of. Convince the verifier of statement that perhaps they couldn't convince otherwise. But on the other hand, this also gives them more room to cheat. So it could very well be the case that we actually made the model even weaker.
00:07:12.464 - 00:08:03.306, Speaker A: So what do we know? So, in terms of increasing the power, we actually know very little. We have no upper bound, no reasonable upper bound on map. And it could be that it contains even undecidable languages and in terms of flower bounds for which languages we can get map protocols. So, for a long while, this was a very hard problem. And since this model was introduced at zero four, it took quite a lot of time until a strong result was obtained. And indeed, in the bricklay work of Ito and wittic, it was shown that the class next, which is like NP but much better, is contained in an AP style. So for every language in next, we have an MIP protocol.
00:08:03.306 - 00:09:02.616, Speaker A: Okay, so now we have some sense of what MIP is all about. But let's go back to the original motivation in which maps were introduced. To begin with. We introduced maps, or the community introduced maps to be able to obtain their knowledge unconditionally. So, does this thing still hold in the MIP style? Can we get with two provers or more zero knowledge proof for a large language? And so this is the problem that we try to answer here. And our answer is yes. Or a bit more accurately, we prove that for every language in XP, there exists an MIP style protocol which is perfect zero knowledge.
00:09:02.616 - 00:09:46.184, Speaker A: And all of this is unlike in Yale's talk. This is information theoretical soundness holds against unbounded provers. So this is what I'm going to show you today, or tell you a little bit about. Okay, so, first of all, an immediate question. Why isn't this obvious? So, on one hand, we know that we have map in the classical setting, map equals next. So we have, even from the BFL, from the nineties work, we know that we can get map protocols for next. And on the other hand, we know that we can get zero knowledge maps.
00:09:46.184 - 00:10:57.180, Speaker A: So we have the old knowledge for next. And on the other hand, we know that we can get any language in next, we have an map style. Why not just mix the two together? And the point is that the techniques that are being used to get each of these results are fundamentally different. So in particular, perhaps the reason why the ItO vedic paver managed to get the success of constructing map styles for such a lag class is because it looked, it opened it, white boxed the BFL construction, and it said, let's look at the particular structure there. And heavily used algebraic structure that is being used in this protocol. In particular, it strongly relies on low degree testing and low degree polynomials, whereas the way that we actually know how to obtain zero knowledge for large classes of computation typically use combinatorial techniques like commitment schemes. And these type of techniques do not play nicely with the techniques that are being used to obtain map style protocol.
00:10:57.180 - 00:11:59.604, Speaker A: To get resistance to entanglement and doing naive things like taking the combinatorial schemes and just encoding them by polynomials or somehow trying to endow them with these algebraic techniques ends up eliminating the zero knowledge properties to begin with. Okay, so yes, just a question about just a theorem statement. Can you hope for something better? Do you hope like being able to go beyond NX like some, you know, quantum silver? So that's a fantastic question. First of all, we have no upper bound for map style without the zero knowledge assumption. It could be that it is much, much larger and there are some evidence towards showing that perhaps it is louder. However, at this point, we don't know of showing something which is much more than this without the ornament we do. Yes, I know, I know.
00:11:59.604 - 00:12:32.104, Speaker A: Not yes, agreed. But it depends on what you mean by getting louder computation. We can do slight and perhaps we can actually match that now with your knowledge. Yes. Okay, so, okay, so let me tell you just a bit. Okay, not yet, again, how we do it. So we do it in two steps essentially.
00:12:32.104 - 00:13:22.086, Speaker A: So the first step is a way of taking classical protocols and lifting them to be resistant to entangled strategies. So this is a way which takes a wide family of protocols. I will tell you about in soon protocols with an inherent algebraic and we can completely black box way just transform them into an map style. And most crucially, this is being done while preserving zero knowledge. Okay, so this is the first part. So we know how to get classical protocols, make them quantum while preserving zero knowledge. All we need to have now are these algebraic type of protocols which are also zero knowledge.
00:13:22.086 - 00:13:37.594, Speaker A: And this is the second part. So essentially what we're doing, we're designing these proof systems which have certain algebraic properties and zero knowledge, and then we just lift them via a completely generic paradigm.
00:13:38.574 - 00:13:42.670, Speaker B: What do you mean by generic? So the generic is only if the bottom part is algebraic.
00:13:42.742 - 00:13:43.342, Speaker A: Yes.
00:13:43.478 - 00:13:46.134, Speaker B: So is that, can you formalize it?
00:13:46.174 - 00:13:47.434, Speaker A: Yes, I will.
00:13:47.744 - 00:13:53.416, Speaker B: If you start with an mip that has some algebraic structure, then you can.
00:13:53.440 - 00:14:55.612, Speaker A: Move them exactly right. And the next slide will exactly characterize like give a description of what is this magical class of protocols that we can actually lift. Let me just tell you that the first part, the lifting lemma, this is like mostly conceptual contribution in the term that it mostly about finding how to put the pieces together in a way that is delicate enough to preserve their own knowledge. But technically very simple. And getting this algebraic type of zero knowledge, this is where we have to really work hard and we have to for example introduce their own algebraic ways of commitment schemes and zero knowledge sound check protocols. And this is where things more interesting. Okay, so this is the strategy and what I plan to do now is tell you in a very high level about this part and then present something for you.
00:14:55.612 - 00:15:56.410, Speaker A: Okay, will you explain what these two stick figures at the bottom? What does it, this is zero knowledge. It's taken from all that gold rider big poster that he has in his office. I will tell you about it later. Ok, so now these are the magical algebraic protocols that we see. We will take interactive pcps, or rather an algebraic type of uninterrupted pcps and transform them into MIT stars. Ok, so what would this lemma tell us? If we have any low degree interactive PCP, then we can take it in a completely black box way and transform it into an map star with very similar parameters. So let me go and gradually explain what are the low degree interactive pcB's? Okay, so first of all pcps, this is a rather standard model.
00:15:56.410 - 00:16:45.122, Speaker A: We have prover and a verifier. The prover sends a long proof and the verifier only want to make a small number of queries to this. It only probes it in say three different locations to the end. So these are a way of encoding NP witnesses that allows for very local verification just by probing a few bits. Okay, this is nice and well. However, there is another notion introduced by Kalai and Raz in zero eight, which are called interactive pcps. And what are interactive pcps? These are PCPs, oracles as before, only now instead of just letting the verifier make small number of queries to verify the PCP oracle, it can now also have an interaction with a prover.
00:16:45.122 - 00:17:39.244, Speaker A: So a typical protocol would look like the prover would send this pcp it's proof, and then the verifier improver would engage in an IP, an interactive proof, hopefully communicating as little as possible bits of information. And this could make us potentially, and it turns out that you can get much better construction. In particular, we could have not gotten the result that we wanted if we could only use pcps, but we rather need to use the extra power that can that interactive pcps give us. And by the way, even though this is a completely, looks like a completely theoretical notions, it turns out that you can actually take this type of interactive pcps and compile them into snugs and snarks. So this is also this kind of things actually being compiled in practice to deploy systems.
00:17:39.704 - 00:17:42.240, Speaker B: Of course it commits to some proof.
00:17:42.312 - 00:17:57.510, Speaker A: Not. Yes, it sends a proof. It is now written. It can no longer change it in any way. Now just making queries and talking about the proof. But yes. Okay, and now what is the additional part? So low degree.
00:17:57.510 - 00:18:39.116, Speaker A: So this is all about the thing that jumped here. So we require that this PCP oracle is simply a low degree polynomial. So this will be global. I will view a low degree polynomial using the low degree extension view, which means that here we have some arbitrary information, and this would be a low degree polynomial that encodes this. So this is a readmiler code rule. Okay? And the point is as follows. But if we have these low degree interactive pcps, then we can take them and in a completely black box way, transform them into map style protocols while preserving their knowledge.
00:18:39.270 - 00:18:43.264, Speaker B: So once you understand the proof, why, why is it just the first arrow?
00:18:43.304 - 00:18:58.392, Speaker A: I mean the proof. So let's play this game. I'm the prover. I'm sending, I'm writing down a low degree polynomial. This is the PCP. It is now fixed. After that, the verifier and the prover, they talk arbitrarily.
00:18:58.392 - 00:19:19.748, Speaker A: And this is like being read completely. This is just a standard communication. And the verifier can also evaluate this low degree polynomial in several points. And the point is that instead of sending an arbitrary BCP or an arbitrary oracle to be queried, you have to only send low degree polynomials.
00:19:19.796 - 00:19:33.838, Speaker B: Yes, correctly. The only reason you're generalizing your model should interact with PCP setting is because in the interactive pcps setting it seems probably it's easier to construct zero knowledge than just in the low degree piece.
00:19:33.886 - 00:20:13.972, Speaker A: Absolutely. Yes, yes, yes. More than that, it is the way that our lemma works. We actually need several things that form the interactive PCP. For example, we need to have, the actual verifier would make just one query, which is uniformly descriptive distributed. So this we don't know how to do, and it seems impossible with non interactive PCB's. So we really do have to use this interaction not only to construct their knowledge, but also so it would fit to the framework.
00:20:14.148 - 00:20:16.544, Speaker B: There's a piece that's missing, which is a single query.
00:20:16.924 - 00:20:40.284, Speaker A: Yes, and I will get to it soon. Just saying that if we don't kill about preserving zero knowledge, then we can omit the low degree requirement. So we can take any IPCP and just transform it into an MIP style. But this will not give us anything in terms of this would completely ruin zero knowledge. Okay, so.
00:20:41.984 - 00:20:42.944, Speaker B: You still need the only.
00:20:42.984 - 00:21:24.416, Speaker A: One query to do this transformation we can get. I will go actually, right now. Okay, let me just stress that this is a completely classical protocol. All parties here are completely classical, and we are now going to make it an MIP style protocol, which is resisting and entangled. Okay. And the point is that all the pieces are already there. All we have to do is use them correctly and make sure that we are constructing it in a sufficiently simple way that zero knowledge would actually be preserved and not get ruined by any of the modifications that we're doing.
00:21:24.416 - 00:22:01.864, Speaker A: So in a very, very high level. Let me just tell you about how this lifting protocol works. So we're starting with this interactive PCP. This is the same as before, only smaller, but we're doing some pre processing to it. How do we do this preprocessing? We're using an idea by Kalai and Raz where we perform query reduction. So we use the interaction to get the query complexity to one. And we also make sure that this is being done, that this particular query is uniformly distributed.
00:22:01.864 - 00:22:57.124, Speaker A: Okay, so how would this protocol, how does it work? So, essentially, we start this protocol. So we have this IPCP, and now we're constructing an map style. What does this map style protocol do? First of all, we toss a coin to decide which game we will play. With probability half, we would just invoke the MIP point versus plane low degree test of Natalie and Vedic. Actually, we need a slight variant of this, which Toma was kind enough to provide us with. But this test, we can view it as follows an map style protocol. And we think that each one of the proverbs here has in mind a certain low degree polynomial, at least in the honest case.
00:22:57.124 - 00:24:38.834, Speaker A: And whatever it would ask one of the provers for random point and the other proverb for a random plane. So this is the quantum analog of the point versus plane low degree test. And the beauty of the analysis of this theorem is that we know that if the provers manage to pass this test consistency of this point and plane, then in a sense, this restricts the set of strategies that the prover can use to essentially something which is approximately just taking the quantum state and using it as random coin to decide on a low degree polynomial according to which they answer. So once we do this with probability half, then we know that with high probability, now we can assume that they are roughly behaving like a lookup for a low degree polynomial, in which case, what we do is we emulate the interactive PCP here. So one proverb as before would play the role of being lookup for this polynomial here in the IPCP. And the other one would take on the role of this proverb. Now of course there are, so this is the high level of whiteboards.
00:24:38.834 - 00:25:30.842, Speaker A: Of course there are some complications. And for example, for this low degree test to work, we need the protocol to be symmetric and if we use standard ways of symmetrizing it, then we can lose zero knowledge. So we have to be careful. But hopefully this gives a high level of how this works. Okay, and now to the main part, which is the algebraic zero knowledge. Okay, so this goes as follows. We want to prove, so we know that we can take any low degree IPCP with zero knowledge and turn it into an map style which is zero knowledge.
00:25:30.842 - 00:26:30.498, Speaker A: So what's left to do is actually construct these interactive pcps which are zero knowledge. Right? So the previous techniques, we can't just take any existing construction of pcps and because it was not algebraic to begin with, and we can endow it with algebraic structure without destroying the zero knowledge. So we had to use quite a few components. Well the notable one, this is where I show impressive stack of results. No? Well the important parts are showing new structural results about extensions of the readmiler code, showing an algebraic commitment scheme and a strong theorem, strong zero knowledge subject protocol. Now of course I won't have time to cover all of them. So we will focus on one protocol, which is getting algebraic code commitment scheme.
00:26:30.498 - 00:27:33.858, Speaker A: And the reason I chose it is because I think it's also, it's both reasonable to do in terms of time and it is also, I think interesting on its own outside of the general. So yes, so just a question. So without the low degree condition, was it known before how to construct zero knowledge interactive pcps for next without the load? So yes, but nothing that can play, even remotely play with really known slow degree. Yeah, in the sense that if you, and you can't just so you can have like zero knowledge PCB, and then you say ok, let's try to play with it, reduce the complexity. But the point is that it has to be at the end of the day just one query to a low degree polynomial. And the only way I can see of getting that this is like what our lifting paradigm requires. And the only way that we know of doing it is like take the pcP, your original pcP, and just encode it by a low degree polynomial.
00:27:33.858 - 00:28:13.494, Speaker A: But doing this destroys the whole knowledge. So this is why this cannot be done. Okay, so let me tell you how this debike schemes, commitment scheme looks like. And so again, we have a prover and a verifier. And what's going on now is we want the prover to be able to commit to some element, say just a field element, but it could be any message or whatever you want. And we want it to satisfy two conditions. We want that, first of all, whatever the prover sends will not reveal.
00:28:13.494 - 00:29:07.030, Speaker A: It would hide the message. This is because we want to get this type of zero knowledge. So we want not to reveal our message, but on the other hand, we want it to be committing. So if the prover then chooses to convince us that this is actually that he want to decommit to this element, he can do it in an efficient way. How we do it, the idea is quite simple. What we do is the prover wants to commit to this beta. So it uses a very large think about exponentially polynomials, evaluate on exponentially many points, and it sends a random polynomial that if you take a sum of a systematic particle, think about this as just the evaluations of a logical polynomial.
00:29:07.030 - 00:29:39.588, Speaker A: And this is like the systematic part. So the sum over all of the terms here are the element to which we want to commit to. So why is this potentially good? So the idea is that since summing all over these terms, this is exponentially, exponentially many terms, the. Nope. Efficient verifier can just sum over all of them. And we'd hope. So we hope that this would suffice to hide.
00:29:39.588 - 00:30:00.064, Speaker A: And on the other hand, we can use interaction and use some sort of variant of zero knowledge thumb check in order to decommit to it. So after it sends this polynomial, there is an efficient interactive proof that can convince us of the, of this sum without revealing.
00:30:01.444 - 00:30:06.612, Speaker B: So you take h. Yeah. What's the parameters of h and m? H to the m is going to be.
00:30:06.668 - 00:30:10.744, Speaker A: So you can think about it as like two to the n exponentially. This is a huge.
00:30:12.404 - 00:30:15.156, Speaker B: And then the prover will be commit via sum check or something.
00:30:15.300 - 00:30:16.524, Speaker A: Yeah, some sort of.
00:30:16.604 - 00:30:19.436, Speaker B: But then he needs to work really hard. No.
00:30:19.620 - 00:30:37.336, Speaker A: So we are not. This is our. These are. Okay, so it can be done. This requires to do it carefully. I'm not saying anything about this protocol because we won't have time to cover it, but I can tell you kind.
00:30:37.360 - 00:30:38.648, Speaker B: Of efficient at the end.
00:30:38.816 - 00:31:15.424, Speaker A: Yes. This is part of what you want to do. Yes. Okay. So the idea is that we sent the commitment just by providing the PCP, the logic polynomial, and we perform decommission by doing an interactive protocol. Okay, so now how do we do it? So this might look, this is just, so how does the, so I want to look at our proof polynomial. This is just, I'm thinking about this many terms here in our polynomial.
00:31:15.424 - 00:31:40.936, Speaker A: And essentially what we're sending is an evaluation of the whole thing. Okay, so we're sending a load of a polynomial. It's sum over hm is the element to which we want to commit. And. Yes. Now let's see what happens. Is there a question? Yes.
00:31:40.936 - 00:32:36.684, Speaker A: How is p specified to the verifier p the polynomial? So this is like a PCP ordered using send. And this is like, we think about it as a read mueller code word, just the evaluations, all evaluations. And we can make queries, which means that we can get evaluations of this polynomial. Okay, so let's for a second zoom in on the systematic part on HDMI, and recall that, like our load degree polynomial here, the degree is larger than h. So there are no algebraic structure whatsoever. These are just numbers that are being stacked in this grid or tension in this case, we know that summing over all of the terms here, this is something that is sharply hard to compute. So if this was the case, and if we just sent this, like, many, many numbers, exponentially many numbers, and we'd say this is the sum, everything would have been pg.
00:32:36.684 - 00:33:38.554, Speaker A: However, then we wouldn't have any way of performing decommission using an interactive protocol. So let's see, what is the problem that we're facing? So instead of just sending this blue cube here, we're sending the entire polynomial, all evaluations. Now, the verifier, he wants to compute the sum of all of these things. But you can also make queries here and here and here. Now you can ask yourself, well, is this problem still hard? Perhaps there is some information in the loading extension, some point that you can query that would help us, or perhaps not. And this actually has a strong connection to the arms and wicks on algorithm framework. Because if you think about it, what we really want to do here is we want to prove query complexity lower bound, where you're allowed to make queries not only to the input, but also to its low degree extension.
00:33:38.554 - 00:34:28.614, Speaker A: Okay, now, so there's this question, and when I first heard of it, I said, well, of course it's still hard to compute. What will all this junk that we have in the Lauder extension help us? But I was wrong. And apparently, for example, if we just consider the multilinear extension, then there is a single point in the tensor. It's just the inverse of, if we're in zero one, just the inverse of two. So we can evaluate one point and actually get the sum of the entire cube. So this is depressing because what we will do is our commitment scheme. However, fortunately, it is the case that this is a phenomenon that happens because of multilinearity.
00:34:28.614 - 00:35:16.676, Speaker A: And you can view, you can actually prove query complexity lower bound, that if you're using at least something of quadratic length, of quadratic degree, then such kind of thing cannot happen. And if you want to sum over the entire cube, you have to essentially read as many points as exist in the cube. Can you also just pick a larger field, but that has characteristic two to avoid that problem? So there are. Let's discuss it later. This is quite an interesting one, actually. So, okay, so how much I do? Ten minutes. Ten minutes? Yeah.
00:35:16.676 - 00:36:06.204, Speaker A: Okay. So I don't think I have time to show. So I had in mind to show you a nice proof which actually follows the Alison Widger's own paradigm of using communication complexity reductions. So where I wanted to show you Yvel, Alice and Bob. So there is a nice reduction that you can do, which proves this theorem rather nicely. Let's just get to it. Okay, so just let's conclude that we know that if we have at least degree two and we want to get only the sum over the cube, then we're completely happy.
00:36:06.204 - 00:36:38.892, Speaker A: And this is like a good hiding scheme that can be for which we have a protocol. Now, however, what really happens is that the way we do the decommission is using some sort of sound check protocol. And doing this protocol, the prover also sends us additional information. So it's not that the prover just provides this low degree polynomial. At the beginning, he also tells us other things. What other things? So that's in the commitment phase.
00:36:38.948 - 00:36:41.260, Speaker B: You're not trying to hide that anymore.
00:36:41.292 - 00:37:08.912, Speaker A: At that point, we don't want. So, for example, it can send us partial sums. And I want to know only the value. Only? Yes. This is only for zero knowledge. I want to be able to not, not to reveal any information under the value that the prover wants to decommit to. And on the way to that, he provides me with lots of information about this polynomial, which is problematic.
00:37:08.912 - 00:37:23.786, Speaker A: So, for example, it also sends these partial sums. So for example, you can view this as just sending the sum over a line and sending the sum over some plane. Yes.
00:37:23.850 - 00:37:27.810, Speaker B: Can you say the protocol roughly what the structure of the subject is?
00:37:27.842 - 00:37:40.178, Speaker A: It is a zero knowledge type of subject. It's a subject in which you mask the polynomial by taking a linear combination with random polynomial and sends a polynomial.
00:37:40.226 - 00:37:41.374, Speaker B: You get some answers.
00:37:42.114 - 00:38:17.114, Speaker A: Yes. A univariate polynomial. Yes. And this will correspond to this polynomial. So it gets all the evaluations. So this is summing over all a's and giving you just the first variable and so forth. So at the end of the day, now, what happens is not only we have to know that, given the proverb that making, the ability to make queries here will not reveal something about this sum, but rather we want that if we also know the value of our planes and hyperspaces, then this would still not reveal any information.
00:38:17.114 - 00:38:21.238, Speaker A: And also we can take the linear combination of all the points and the lines.
00:38:21.406 - 00:38:29.270, Speaker B: What are you trying to hide? So you're doing a commitment scheme. When you give the commitment scheme, you said, okay, there's some data that you're trying to hide.
00:38:29.342 - 00:38:29.646, Speaker A: Yes.
00:38:29.710 - 00:38:32.494, Speaker B: This commitment scheme does not hide it. So it's information directly.
00:38:32.534 - 00:38:33.022, Speaker A: Yes.
00:38:33.158 - 00:38:35.518, Speaker B: But then at some point, you're saying, I want to reveal data.
00:38:35.606 - 00:38:36.118, Speaker A: Right.
00:38:36.246 - 00:38:37.350, Speaker B: What are you trying to hide?
00:38:37.502 - 00:38:48.864, Speaker A: So this is me trying to hide. So in the real protocol, okay, now, in the real protocol, we're not only committing to one element, but rather we are committing to a polynomial.
00:38:49.884 - 00:38:51.060, Speaker B: You're opening one and you don't.
00:38:51.092 - 00:39:48.164, Speaker A: And I want to open just one evaluation of this polynomial. However, like just explaining of how to do it for polynomials rather than single elements, this would introduce lots of complications. And this is why I try to hide it under the rug. But, yes, this is a perfectly good question. So I don't want to reveal anything other than this particular point because I committed to a whole polynomial. Okay. And so at the end of the day, what we have to do is extend the arms and diggersome type of framework in which you want to prove, query lower bounds for algorithms that query an input, and slow degree extension to algorithms that get access to what we call the Sigma readmiler code word, which is essentially taking the load degree extension together with all the planes and lines of the type that is being introduced in the Samcheck protocol.
00:39:48.164 - 00:41:16.786, Speaker A: And what we actually prove in this setting is that if you are given access to this thing, which essentially all information that you can get during the protocol, then you cannot learn any summation of x term by making less than, by receiving less than x term from the prover or by querying them yourself. And unfortunately for this, we cannot use the elegant communication complexity reduction, and we have to use purely algebraic techniques. Okay, now I want to conclude by just raising a couple of open questions that I find interesting. So one of them. So we've got next AmIP style and we do it using polynomially many grounds. Now ground complexity is a very fundamental thing that as Yale told us about, getting as close as possible to non interactive is crucial. So can we do this with just ounce? Another question is this lifting paradigm where we can take any interactive pcp and lift it to an map protocol? So can we do it for a wider class of protocols other than just interactive pcps or for example, if we want to preserve their knowledge just.
00:41:16.786 - 00:41:18.578, Speaker A: Yes, I'm sorry, I didn't want to.
00:41:18.586 - 00:41:19.854, Speaker B: Understand the first one.
00:41:20.674 - 00:41:22.066, Speaker A: What is this?
00:41:22.210 - 00:41:26.490, Speaker B: Right, so can't you just have two words? What would.
00:41:26.682 - 00:41:29.866, Speaker A: Can I do regularization for example.
00:41:30.050 - 00:41:30.842, Speaker B: Can you do what?
00:41:30.898 - 00:41:40.974, Speaker A: Oracularization. So there are techniques in this setting in which you can reduce the query complexity, reduce the run complexity. However, they do not preserve their own knowledge.
00:41:41.554 - 00:41:48.104, Speaker B: So in other words, why if the next round can't you just, instead of asking it in next round just ask, ask another prover.
00:41:48.684 - 00:42:19.684, Speaker A: So even doing this type, like being done in KLR, when you replace each round by approver and this type of approach, I can tell you about it later, but they turn out to fail for the type of statements that we need. So this is, we still don't know how to get that we believe it should be possible using. But it some sort of oracularization, some sort of round squashing. But it is not exactly entirely clear how to do it.
00:42:20.264 - 00:42:24.192, Speaker B: Usually mips, there's no rounds, right. There's just a bunch of queries and answers. That's it.
00:42:24.248 - 00:42:39.444, Speaker A: Yes, but, but when you are in this. Yes. When you're in this setting, then suddenly you can't just use. And this is you can just do your typical way of reducing the rounds. Yes, yes.
00:42:39.484 - 00:42:43.624, Speaker B: Is there a scaled down version of this conjecture?
00:42:47.684 - 00:43:50.370, Speaker A: Yeah, so. Not that I know of. In particular, we have tried to get it even for sharp p, which is now setting the immediate stepping stone. And it requires stronger types of algebraic complexity, lower bounds than we currently know how to prove. Okay, so back to this problem, which I also find very interesting, is can we take a larger class than just low degree interactive pcps and lift them? And one way to do it, one strategy that was successful in the classical literature is by trying to abstract. So all of these results are using low degree testing. And the way that we can only get such map style, the only way I know of getting map style protocols for large languages includes some sort of algebraic machinery or testing machinery.
00:43:50.370 - 00:45:07.050, Speaker A: So what we can do is, for example, maybe we consider, instead of just the role of algebra, maybe you can abstract it and replace it with codes that have potential local testability or local decodability. And one place which I find interesting to start is can we use tensor codes and perhaps rely and take the benzos on Sudan type of local tests for them and generalize it to that framework and get entanglement resistant tensor code testing which would have. Okay, so thank you very much. Any more questions? Yeah, so you were saying last week about the intuition for a complete problem for zero knowledge mip strand? Yes. That is, it will take, I think, longer than. So what one can hopeful is that. So we don't know of upper bounds on map style, but now with zero knowledge, then we actually have a simulator that runs in polynomial time.
00:45:07.050 - 00:45:54.514, Speaker A: So we have additional structure so one can hope to prove upper bounds on zero knowledge. And who knows? Perhaps this could somehow lead to some insight on the non zero knowledge model. Yes. So is it an alternative to bidding or it actually strictly contains this verb. So it's more of reframing and abstracting. We are using components that exist in ethic and the novelty is in how to maintain their own knowledge while using them. But you can view it as a way of abstracting what is being done.
00:45:54.514 - 00:46:43.104, Speaker A: By the way, I was wondering, what is high degree PCP? Like the lower type of PCP. So our PCB is actually an interactive pCP. So it is more of the type of bfls. It's algebraic by nature, or so if you only talking about pcps, not interactive pcps, then you not only. So we need a couple of things for like in our setting, we can start by using the interaction to reduce the query complexity to one. You cannot get that for, for example, for general pcps. This is why you can't just take a PCP of your choice.
00:46:43.104 - 00:46:50.084, Speaker A: Thank Tom again.
