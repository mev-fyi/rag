00:00:04.920 - 00:01:13.210, Speaker A: As Dick mentioned, it was here at Berkeley that I first started working on the theory of randomness and pseudo randomness. And I am really happy to see that in the audience are some of the teachers who taught me the subject and some of the collaborators with whom I've. I've worked on the subject. And I'm chagrined to see that other people in the audience weren't even born at the time. Anyway, the other kind of general comment I want to make is that if you like this talk, then you'll love Avi Wiggson's talk that has pretty much a better version of this talk, and it's coming up very soon, next week. Avi, what's the exact date? It's not here. It's downtown, the 22nd.
00:01:13.210 - 00:01:53.564, Speaker A: Okay. Something like the 22nd. Okay. And the last general comment I want to make is I'm sort of, like, turning conventional wisdom on its head. I am mainly citing people at length who are not here in the city, in the audience, and ignoring the relevant citations. For people in the audience. And mainly the people I'm going to cite, it isn't to give them credit is to do honor to our field by claiming them as ancestors.
00:01:53.564 - 00:03:06.710, Speaker A: Okay, so, okay, so I'm going to talk on does randomness solve problems? And every time people see the title of this talk, they've been asking, well, does randomness solve problems? So I'm just going to give you the answer first. You can skip the rest of the talk if you want, because all of it is explaining this answer. Yes, randomness solves problems, but you probably don't need randomness to be random in order to solve the problems. You can have deterministic randomness and it'll solve the problems anyway. And probably most random ways you come up with of generating deterministic randomness. A lot of them work, but not all of them. And then it's certainly true, though, even that randomness solves, like, random instances.
00:03:06.710 - 00:03:42.684, Speaker A: Randomness doesn't need to be random in order to solve random instances of problems. Okay, so that's the whole talk. If you got the bit that you want, you can leave now. Okay, so first I want to say, when I think about problems, I'm not talking about all problems out there in the world. There are actually lots of areas where randomness can provably solve problems. Like in cryptography, you provably need randomness. In game theory, you provably need randomness.
00:03:42.684 - 00:04:04.758, Speaker A: In distributed systems, you provably need randomness. So I'm just going to talk about a particular kind of problem. I mean, computational problem. And solving means having a faster algorithm. Now, you can also discuss the same similar problems with other kind of resources, such as memory. It's interesting. I didn't have time to even start it.
00:04:04.758 - 00:05:00.744, Speaker A: Okay, so, um, so, by a problem, we mean a computational problem. If a computational problem, it doesn't solve the problem of displaying things on the slide. So, computational problem has an input, and we view the input as a discrete, can be represented discretely and finitely as a sequence of bits. It has some kind of output that also can be represented discretely and finitely as a sequence of bits. And it has some kind of relationship that should hold between the output and the input. Okay, so that's what I mean by a problem. And note that a problem always has, like infinitely many instances, each input is a different instance of the problem.
00:05:00.744 - 00:06:59.044, Speaker A: And now, an algorithm is a general procedure that goes through a sequence of fixed computational steps, finitistic steps, and produces from the input an output that hopefully actually has the relationship that that satisfies correctness. And we're interested in the number of steps. So, when I ask, does randomness solve problems? I mean, if we've got a, if we add to our instruction set besides the standard like addition, multiplication, boolean operations, and so on, the ability to, in one step, flip a totally random coin. So, pick a value b at random, equally likely to be zero and one, does that change the power of the model? Does that allow significantly faster algorithms for problems? For computational problems and a computational problem in the sense that I just defined when we talk about the randomness of the algorithm, if the algorithm is going through a number of adaptive steps to flip random coins, you can front load that process. If you know a bound on how many times the algorithm is going to perform that operation, you can flip all the coins in advance and view that as the random input to your algorithm, the results of those t coin flips as the random input to your algorithm. And then you view the randomized algorithm as a deterministic algorithm on the real input and the randomized input. So, when we're talking about whether the randomness of the algorithm needs to be random, we're talking about whether this really needs to be uniformly distributed, this variable here.
00:06:59.044 - 00:08:31.974, Speaker A: Okay, so, now, when would we say that randomness really solves a problem? We would say that randomness is like essential to solve a problem. In the worst case, if there's a problem and a randomized algorithm that, and we don't want a randomized algorithm, just has like an infinitesimally better than half chance of guessing the right answer. We want an algorithm that reliably gets the right answer, so it's reliably correct, correct with high probability, and it's much faster on large inputs than any deterministic algorithm for the same problem. So we formalized the class of problems that have efficient randomized algorithms as the class BPP of problem solvable in randomized polynomial time. And even though we sort of like always use polynomial as our canonical time, you can just think of this as fast. Okay, so polynomial has nice closure properties, but really, everything we're talking about can be totally made general if you allow in our arbitrary time amount of time. And then we're comparing that to p, the class of problems that can be solved deterministically in polynomial time.
00:08:31.974 - 00:10:07.894, Speaker A: So we're asking, randomness helps in the worst case, if BPP is a strict superset of p, and it doesn't help if BPP equals pull doesn't help on every instance of the problem. But we could also ask, and we're going to be looking at algorithms that are probabilistic algorithms, but they're not guaranteed to have high success on every instance of the problem, but they might have a lot of, you know, might have high success on many interesting instances of the problem. So we call an algorithm that solves the many instances, or the problem on a subset of instances, a promised BPP problem, if it's a polynomial time probabilistic algorithm, but it solves it on some class of instances. And so we could similarly ask whether promise BPP is a strict superset of promise P. So does to say that is saying that there are always, there are some instances of some problems that, where randomness is essential, as opposed to the other. There are some problems where even in the worst case, randomness is essential. So if promise BPP equals promise p, then p equals BPP, but it's not known to hold in the other direction.
00:10:07.894 - 00:11:31.708, Speaker A: Okay, so we'll be talking about both of these, at least implicitly. So why would anyone think that flipping random bits would actually help speed computation? Like doing something random? Seems like you should do something that's better than random. You know, if you do flip a zero, you're doing one thing, you flip with one, you're doing the other thing. Well, why don't you just do the better of those two things? So, the first pro, at least the first recorded person who had this idea of using randomness to solve problems was stanislavullum. And this is really remarkable because this idea of using randomness to solve, to speed up algorithms came up sort of simultaneously with the idea of using computers at all. So this is the person, the thing he's holding in his hand is like a model of one of the first. This was actually an analog computer that they used in the Manhattan project.
00:11:31.708 - 00:12:21.384, Speaker A: And Stanislau ulam was a researcher in the Manhattan project. So they hadn't really quite developed the computer, but they were still looking at faster computer algorithms. And this idea of randomness came up, and here's how it seems to have come up. Now, my historical research is all, like, looking things up on Wikipedia, so take it with whatever grains of salt you want to give. But it seems the story is that Senesl Ulam had to have a surgery. And after the surgery, he was in the hospital for a few weeks with not a lot to do. So he did what a lot of people do when they don't have a lot to do.
00:12:21.384 - 00:13:01.530, Speaker A: He played solitaire to pass the time, and he played a kind of complicated solitaire called Canfield. And this sort of has personal meaning to me because there was a. When I was a postdoc in Toronto, there was a computerized Canfield program, and I wasted a lot of time playing Canfield. So, you know, I have something in common with Stezla Ulam, which makes me proud. Okay. Anyway, so it's a complicated solitaire game. It's not so easy to know what the best strategy is.
00:13:01.530 - 00:13:58.344, Speaker A: And so he was, like, trying to compute what the odds of winning under different strategies for Canfield were. And he first tried to do it mathematically. I think this was particularly difficult because he was, like, lying in bed, not able to move very much. So he probably didn't even have a pencil and paper, but he was failing to mathematically analyze this game. And he said, well, if I really wanted to know the answer, we have these computational devices in the Manhattan project. I could use one of those computational devices to simulate thousands of runs of Canfield and actually compute the probability by random sampling. This is sort of like the kind of thought you have when you're under sedation.
00:13:58.344 - 00:15:17.724, Speaker A: But then he went back, I should say he went back to the Manhattan project, and they needed to simulate the behavior of atomic particles to perform integrals that involve countless paths of collisions, of events involving collisions of different kinds of atomic particles, and sum them all up. And they needed a good approximation for the sum. And he had said, well, here's the idea I had about simulating Canfield. We could do the same thing probabilistically simulate, you know, not every kind of combination of events, but simulate what happens probabilistically and see, and thus get a good estimate of the probability. And so they actually did this. This was part of how we won World War Two, okay? Through solitaire games. Okay? So now his colleague Nicholas metropolis, I think, was trying to make fun of him and dubbed this the Monte Carlo method because Metropolis had an uncle who always went to Monte Carlo and lost his money gambling.
00:15:17.724 - 00:16:43.902, Speaker A: And so he felt like, I don't know whether this was like a slur or not, but the name Monte Carlo method stuck. So one way of viewing the Monte Carlo method in generality is you've got some kind of complicated function, and you want to know the average value of this function if you've got a, you know, people do numerical integration for smooth, nice functions by sampling at uniform points, but a lot of functions that come up in actual life, and this is like Internet access behavior taken from some random website. But you can see that this behavior in real life is often very bursty. And so if you do this regular sampling, you may miss peak values and get a very, very bad estimate. But if you sample at random places, it doesn't matter what the function looks like. You don't have to understand the function. You'll get a statistically valid with high probability estimate with relatively few samples, no matter what the function looks like.
00:16:43.902 - 00:18:11.674, Speaker A: So you don't have to know, like, bounds on the derivative, you don't have to know any kind of continuity. So there's a discrete version of this problem where you're trying to evaluate the average of a function, and it can be arbitrarily messy, and that's the circuit approximately probability problem. So the circuit, a boolean circuit is a boolean operation. It just inputs 20 one bits and outputs another bit, and there's just a finite number of them, like and or not. And you can build up others from those. Okay? And so a boolean circuit is a step by step procedure where every step is just one of these boolean operations on two previously computed elements. So when we do computation on any kind of computational device, it eventually gets translated into this kind of Boolean algebra, might go from the cpu, you know, eventually all the actual computation steps happen on the cpu.
00:18:11.674 - 00:19:07.074, Speaker A: The cpu is just a combination of these logic gates. And so every step of any complicated algorithm can be broken down into these boolean circuit operations, into these boolean operations. And so the circuit model can code any kind of elaborate computation. The only kind of constraint on it is that we usually think of algorithms as uniform have to apply to inputs of arbitrary lengths. The circuit model is non uniform in that every input size, it only applies, each circuit only applies to a fixed number of boolean inputs. And so you have to look at what happens when you have circuits for different sizes of input. I'm sort of stressing these points, not because they're that relevant to this particular example, but because it's going to come back later.
00:19:07.074 - 00:20:13.234, Speaker A: Okay, so, um, so the circuit approximate probabil probability problem is to, you're given a boolean circuit and you want to know if you pick the inputs to the boolean circuit at random, you want to estimate the probability that the circuit output's one. Okay, so clearly you can do this with a probabilistic algorithm. You pick random inputs and you see what the probability is that the circuit output's one. And you keep on doing this until you have a very good estimate with high probability. Here, good estimate means small additive error to within 0.1. Additively, say the exact number doesn't really change the problem. But if we could solve this problem deterministically, this is what's called a complete problem for the class promised BPP that we mentioned earlier.
00:20:13.234 - 00:21:31.512, Speaker A: And so solving this problem, being able to deterministically simulate this kind of Monte Carlo estimation, would in fact show that randomness never helps, even on some instances, for solving problems significantly faster. Yes, you might be able to, you can derive one by like, so in some sense there's an analytic solution to this problem. You could write it down as this big integral. You can write down boolean opera, interpolate boolean operations by arithmetic oper by arithmetic operations, and then you can actually like integrate over a big high dimensional cube. And that's in some sense an analytic solution. But if you translate that to an algorithm, you actually get an exponential time algorithm. What it's really saying is it's hard to do really complicated integrals in high dimensional spaces.
00:21:31.512 - 00:22:18.974, Speaker A: Not that it's easy to solve this problem. Does that answer your question? Yeah. So if you just have like a finite number, a fixed number, you can do that. But once you start taking an and, and feeding it into an or, and it rapidly becomes much, much less clear what's going on. Yeah. Oh, right. So maybe the example I gave was bad in that.
00:22:18.974 - 00:23:21.014, Speaker A: Well, I think I at least reused a variable, but yes. So the tricky part is when you, the read ones case is not the hard case. So another probabilistic algorithm that's almost as well five or six years younger, is also named after this metropolis, who named Monte Carlo algorithms. The story is that. So there's a five author paper, and Metropolis is the first author. What I've heard the four other. It's interesting, the four other authors were two married couples, which is pretty rare still to have a paper with not just one married couple, but two.
00:23:21.014 - 00:24:11.994, Speaker A: And I'm told that those four actually did all the interesting algorithm design, and that what Metropolis did was give them permission to run their algorithm on his computer. But I'm not sure whether that's completely fair. But it's also not completely fair. Ever since it's been called the metropolis algorithm, sometimes metropolis hasty for another reason. So now the metropolis algorithm solves a different kind of problem. It's not trying. Okay, so it's used in a number of different situations.
00:24:11.994 - 00:25:17.264, Speaker A: So first, I should say you can use it for sampling. And there is kind of fairly well understood for sampling, and its original kind of motivation was for bias sampling. But it and its variants are most frequently used now as search and optimization heuristics, where you are trying to find the typical NP complete problem. You have a huge number of possible solutions, and you're trying to find the best one according to some mathematical function. So metropolis and its variants, like simulated annealing, go with the winners. Genetic algorithms, ant crawling algorithms, taboo search, and on and on and on. All are sort of like, start with this metropolis idea and put in refinements.
00:25:17.264 - 00:27:16.954, Speaker A: And they're used to come up with probabilistic algorithms, randomized algorithms that aren't guaranteed to work on every case, but seem to work on many instances of the problem. So this is definitely going to be like in the promised BPP world rather than the BPP world. And the reason why you want some randomness is if you just sort of do deterministic behavior in optimization, and you're just trying to find making small tweaks to the current solution, then you tend to get stuck in what are known as local optima. These are solutions where there's no way small change to the solution that makes them better, but they're not global optima. If you make a big change to the solution, you can do much better. So what the metropolis algorithm does is balance greedy descent with randomness that allows you to, even if you're currently at a local minima, you have some probability of going to states that are going uphill rather than downhill, even when you're trying to minimize. Okay, so I'm not going to give the details of the algorithm, but a lot of these different optimization heuristics use this idea of using randomness to balance greedy behavior with random behavior in order to avoid getting stuck at these local minima and eventually finding the global minima, I should say even the basic metropolis algorithm, when you use it in this context, it is not well understood.
00:27:16.954 - 00:28:04.844, Speaker A: So it was like one of the first randomized algorithms suggested. And we still are very, we're still in the infancy of even understanding. And it's a relatively simple algorithm, and we still are barely at the beginning of understanding. And it works really well on many instances, and we still don't know why. Okay. Okay. So then both of the algorithms, randomized types of algorithms that I've talked about so far, were discovered in the forties and fifties.
00:28:04.844 - 00:29:36.324, Speaker A: Much later in the seventies, a kind of new, interesting use of randomized algorithms was discovered in basically number theory and algebraic problems. The biggest success story at the time was primality testing. It's a really basic mathematical problem. Given an integer, can you factor it into two as a product of two to smaller integers, or is it a prime number? And the interesting thing is that primality testing, the obvious approach is to try to factor the number. But factoring numbers seems to be much, much more difficult than deciding whether the number has a factor. Okay? And one reason is that there's this kind of equation that I got wrong is that if you take any number mod any integer, you raise it to a prime minus one, and take the remainder mod p, you get one. If I put the p here, if I didn't put the minus one, then the rest would be incorrect.
00:29:36.324 - 00:30:20.444, Speaker A: So maybe I should just blank this minus one. That actually makes it simpler. So, this holds for any prime. So if you find an x where it doesn't hold, then you know that p is not prime. And so, a lot of the algorithms are based on finding an x where Fermat's last theorem, little theorem, doesn't hold, and thus concluding that the number is composite without actually finding any of the factors. Now, there are strange numbers called Carmichael numbers that also satisfy this equation but are not prime, and that creates difficulty. But let's ignore that.
00:30:20.444 - 00:31:08.818, Speaker A: Okay, so the way the story should have gone is that a number of people discovered that if you plug in random values of x and p is not prime, then you find, quickly find a counterexample. And then Gary Miller discover that if you assume the extended Riemann hypothesis, those random numbers don't have to be random. You can just pick them in a small interval and is still going to find one. That's a counter example. Relatively quickly. If it's not prime, and it's not a Carmichael number. So that's how the story should have gone.
00:31:08.818 - 00:32:22.494, Speaker A: But like history in general, things worked, don't work like they should have. So, actually, Gary Miller's result that if the extended Riemine hypothesis is true, then primality testing can be done deterministically, came first. And then Michael Rebin and Salovey and Strawson independently observed that if you drop the extended Riemann hypothesis, you could still develop a randomized algorithm along the same lines. So, by the way, I love this picture of Bob Solovey when I was a student. This is imagining him to looking like he did when this algorithm was discovered. When I was a student here, he didn't quite look like this, but I took many classes with Bob Solovey while I was a student here. And when I say he took many classes with him, I took some classes from him, but he also sat in many of the classes.
00:32:22.494 - 00:33:34.850, Speaker A: So he was a fellow student in many of the theory of computer science classes. And the thing about him, when he was a student in the classroom, he would answer every question. So it created a lot of competition. Okay, so, okay, so flash forward more than 20 years, 25 years. Agrawal, Kyle and Succina showed how to de randomize these probabilistic randomized primality testing. So, that had also been done before the randomized algorithms existed by Gary Miller, as I said, but assuming some big open problems in mathematics. So these three gave a different randomized algorithm and then used techniques similar to what Gary was talking about to replace the randomness with determinism.
00:33:34.850 - 00:34:52.403, Speaker A: Except instead of assuming big open problems in mathematics, they had to actually use the fact that some, they had to actually prove it themselves, but also use some advances in in number theory. So it wasn't the same algorithm, but they managed to de randomize a randomized algorithm and come up with a deterministic polynomial time algorithm. So this kind of phenomenon, if you come up with a randomized algorithm, and then you later come up with some version of the algorithm, where you replace the randomness with deterministic bits and come up with a deterministic algorithm, happens quite frequently. And there's kind of this life cycle of randomized algorithms that are introduced as randomized, and then they're made deterministic. Sometimes that life cycle takes 25 years. Other times, it happens in the paper that introduces the randomized algorithm. But it sort of makes you suspicious that these randomized algorithms are just kind of a way of thinking about the problem as a stepping stone to a deterministic algorithm.
00:34:52.403 - 00:35:45.036, Speaker A: But here's another randomized algorithm that was introduced around the same time again by a lot of people, in parallel, Schwarzel and DeMille and Lipton. And this one has sort of survived the test of time even better than the primality algorithm we have. Still, even though we've derandomized some special cases, we still have no idea how to derandomize it in general. And I'm going to talk about maybe why a little bit later. I don't know how much later I have. Ok, so you're given an equation where both sides are polynomials in a certain number of integer variables. Um, and we think of this equation as beginning by an algebraic version of the, the circuit model that I talked about earlier.
00:35:45.036 - 00:36:51.324, Speaker A: Um, and then, um, you want to decide whether that equation is true. The randomized algorithm is just to plug in random values for the variables in a certain range. But then there's one other little tricky part where you don't compute the, the value explicitly, you compute the model a relatively small number to keep them from blowing up too big. And I said this problem is still wide open as far as de randomization goes. It's still like a classical randomized algorithm that 30 years later hasn't been de randomized. So another kind of general way randomness is used in algorithms I call random self reducibility, although this may not be totally appropriate. But a lot of geometric problems, they're a lot easier if the problems are far in general position and not close to, and sort of substantially far from anything that's not in general position.
00:36:51.324 - 00:37:54.884, Speaker A: And so a lot of the use of randomness is to say, well, if you've got a worst case problem where things are very close to being, say, in a line, but they're not in a line, we want to randomly tweak it so that no three points are close to being in a line. So we're mapping, we're using randomness to take the kind of atypical worst case instance and map it into a more typical average case instance, preserving the relevant property. So, um, so these are like like four classes of randomized algorithms, and depending on which class you think are the most important, this, this sort of like gives your a priori intuition about whether randomness really helps or not. I think so. Um, so you, okay, so here are like different intuitions you might have. Okay, you might think randomness always helps every problem. I don't think that's a reasonable view, but it's a possible view.
00:37:54.884 - 00:38:41.424, Speaker A: I'll call that the zealots view. You're really into that randomness stuff. Or if you use this last approach where you're randomly tweaking worst case inputs to get typical instances using randomness, you might think that randomness will help in the worst case, but a typical instance, it probably doesn't require randomness. So I'll call that the averaging view. Well, but if you use it as a heuristic, you might think it helps in the average case, but not in the worst case. So I'll call this the heuristic view. Or if you're like a number theory person, you think, well, randomness is helping when there's a lot of algebraic structure to the problem.
00:38:41.424 - 00:39:22.888, Speaker A: And so it's going to help for this kind of class of problems a lot, both in the average and the worst case, but there will be problems where it doesn't help a lot. So I'll call that the structural view, structured view. Or you could just view think. Randomness probably doesn't really help. It's probably just a heuristic for thinking about deterministic algorithms. So I'll call that the skeptical view. So a lot of all these views are, most of these views are, I think, are reasonable intuitions, but we've actually ruled out strong versions of some of them.
00:39:22.888 - 00:40:37.738, Speaker A: And it's not the most extreme that we've ruled out. It's the middle that we've ruled out. And I'm convinced skeptic, because I know some surprising things that would happen if the skeptical view is not true. So let me try to explain why. So I kept on arguing that randomized algorithms have been important from the beginning, but the sad truth is that randomized algorithms have hardly ever been run, been used as advertised. These randomized algorithms that were done in the Manhattan project were actually done with these very crude pseudo random generators based on squaring numbers and taking the middle bits, because it was kind of hard to produce good random bits. And there are other disadvantages to actually using random bits.
00:40:37.738 - 00:41:37.954, Speaker A: So people always used randomized algorithms, not with random randomness, but with pseudo randomness. And you can, it's actually created lots of trouble. And in fact, Luca Treveson just sent me an article where flawed pseudorandomness is creating trouble. Of course, it's creating trouble for people I don't really care about. They're the operators of casinos who are using bad pseudorandom generators in their slot machines. And because of that, they were starting to lose money from people who mathematically analyzed the pseudorandomness in these slot machines and started finding holes in it. So I'm not sure who to root for here in this story, but you can see that this is a danger of using bad pseudorandomness.
00:41:37.954 - 00:43:11.544, Speaker A: But what is good pseudorandomness? Now, I think the first convincing definition of good pseudo randomness was by Andy Yao, building on earlier work of shamir, Blum and Macaulay. So, this is like a taste test, kind of analog, the taste test. You know, if you can't tell the difference between butter and margarine, if you can fool mother nature, then there's no reason to use one or the one or the other not to use the margarine, because margarine is cheaper. So, if you can't tell the difference between the outputs of a pseudorandom generator, maybe using a very small seed and producing a long pseudorandom string and a truly random string, if that's a computationally hard problem, then there's no harm in using the cheaper pseudo random string as opposed to the expensive random string. And if you can make that strong enough, then you can do exhaustive search over the seeds and actually simulate the algorithm on the pseudo random randomness deterministically. Okay, so, how do we get our hands on that? On good. On pseudo.
00:43:11.544 - 00:44:01.114, Speaker A: So, now to say, this is an interesting thing, because now we're saying, in order to design an algorithm, what we need is a computationally hard problem, a problem that has no good algorithm. It's kind of counter intuitive. From a hardness from a lower bound, we're getting an algorithm, and we're going to see, actually, there's some, if I don't run out of time, we're going to see that there's actually some results in the other direction as well. Well, so what does it mean? Like, to have a. What kind of hardness do we need? So, we're going to use. So, hardness versus the class of circuits that we talked about earlier having high circuit complexity. Now, we don't know.
00:44:01.114 - 00:44:39.314, Speaker A: We know as an existence proof, using a probabilistic argument, that there exists, most functions have very high circuit complexity, require almost exponential size circuits. I'm going to go really fast here, but we don't have any explicit functions in low complexity classes that we know require high circuits. So, like, up until last year, the best lower bound was just three n. And that was the same lower bound since 1984. Last year, there was a breakthrough. It's now 3.01 times n.
00:44:39.314 - 00:46:09.174, Speaker A: And if we keep on going this, in a million years, we'll have four n. So, it's a challenge to come up with explicit problems that require large circuits and this challenge in the challenge of coming up with ways of de randomizing algorithms turn out to be pretty much two sides of the same coin. And I want to explain that a little bit in the almost zero time I have left. So, the starting point was this breakthrough paper of the soda Wiggerson that showed how to convert suitably hard four circuits functions into pseudorandom generators. And with some follow up work. The punchline is that if you've got a function that you can compute in exponential time that requires exponential size, circuit size, then you can use it to construct a pseudo random generator that's so strong that you can use it to completely de randomized algorithms, not just in the worst case, but on every case, every instance, instance by instance. So, this says that if we've got such a lower bound, then the skeptic viewpoint is completely right.
00:46:09.174 - 00:46:56.746, Speaker A: Randomness doesn't need to be random. In order to be helpful, we can simulate randomness deterministically. And so if the skeptic viewpoint is wrong, then this kind of hardness doesn't exist. But if this kind of hardness doesn't exist, what that's saying is, for every hard problem, every exponential time problem, I can save time by working offline and develop an algorithm that's tuned to a particular input size that's super polynomially less than the time it takes to solve the problem. I think that's really counterintuitive. So that's why I'm a sort of strong skeptic. Now that we know these results, here's another.
00:46:56.746 - 00:47:48.680, Speaker A: Here's another example of a result that puts the, that helps the skeptic viewpoint. Avi and I showed that if, unless exponential time equals BVP. So this is the extreme zealot position, that randomness helps not just in particular problems, but every problem everywhere, every hard problem, you can solve with this magic bullet of randomness. So, that's a very unlikely event. Or the other conclusion is that in subexponential time, ideally it would be polynomial time. But we don't have that. We can deterministically simulate any randomized algorithm on the typical case.
00:47:48.680 - 00:48:47.950, Speaker A: By typical case, it means that you actually can't find. It's a computationally hard problem to find instances where that simulation doesn't work. So there might be worse case instances on which the simulation doesn't work, but you'll never find them, so they don't really matter. So it says, in some sense, either we have the extreme zealot position or some mild skeptic position, and there are lots of caveats that I'm not mentioning here. So this kind of rules out some of the very possible intuitions, I think, like the heuristic position where you say, well, does randomness help in the typical case, but not in the worst case? That's actually just not true. If randomness helps in the typical case, it helps in every case. And it's just not true that randomness helps for some problems, but not all problems.
00:48:47.950 - 00:49:35.118, Speaker A: Either it helps for all problems, or it can be eliminated, on average, for all problems. So things that seemed like really the most plausible before you started thinking about it, before you started doing this research, are actually the positions that have been sort of ruled out. In the extreme positions, randomness is useless. Randomness is all powerful. Those are the ones that are left. Okay, so in the minute or two remains, I want to say that there are actually some results in the other direction. I said this question of de randomization is linked to circuit complexity.
00:49:35.118 - 00:50:59.524, Speaker A: We've talked about using circuit lower bounds to de randomized algorithms. There are actually results that go the other direction. With Valentin and Avi, we showed that if you could derandomize that circuit approximation problem, then you get a lower bound, not in x like what we needed to get the de randomization, but in just one class bigger non deterministic exponential time, where we also don't have circuit lower bounds yet. Okay, so any, even like mild de randomization of the circuit approximation problem requires proving some kind of new circuit lower bound. And with valentine cabinets, we showed that. Remember that polynomial identity testing algorithm that hasn't been de randomized over 30 years? Well, if you could derandomize that algorithm, even special cases of that algorithm, then you get algebraic circuit lower bounds, circuit lower bounds of the same type that the algorithm considers. And that's also one of the major getting arithmetic circuit lower bounds is one of the major problems in complexity.
00:50:59.524 - 00:51:41.096, Speaker A: So the conclusion is, well, while we don't absolutely know whether randomized algorithms can be de randomized, there are strange consequences if they can't. So I think they really can. It's just a matter of proving that they really that they can be de randomized. But the way we have to prove it is by proving circuit lower bounds. Or maybe the way we have to prove circuit lower bounds is by de randomizing algorithms. I haven't quite decided which one we'll do first, but if you do one, you do the other. If you do the other, you do the one.
00:51:41.096 - 00:52:28.494, Speaker A: So let's just do them both. So, it's a challenge, because in order to solve one major problem. We have to solve both major problems, but it's also an opportunity because we know these interconnections and possibly that can leverage progress on one into progress on the other. And then that can be a little bit improved and leveraged into progress on the one. And you can, like, seesaw until you understand both. So that's my hope. This was the last slide.
00:52:28.494 - 00:53:25.524, Speaker A: You know, I could go back and, like, do some of the others slower, but I got through everything I meant. Yes, well, I'm talking about the metropolis algorithm in itself. Okay, so let me just explain what I mean. So there's been a lot of work on understanding. So Metropolis is an example of Monte Carlo Markov chain algorithm. There's a lot of not. We have a lot of information about when Monte Carlo Markov chain algorithms converge to their stationary distributions, and hence are like sampling according to the way they were designed to sample.
00:53:25.524 - 00:55:20.654, Speaker A: But if you look at how they're being used, they're almost never being used in context where they could possibly converge to their stationary distributions. And when the algorithm. And so it's really, it turns out that these Monte Carlo markup chain methods are really useful even in situations where we know they're not being useful for the reasons we understand. And so I think it's a really challenging problem and really important problem to try to understand why they're useful when they're being run for short amount of time and being run maybe too optimistically and still are being used to find solutions. So that's a good, that raises a whole bunch of good questions that would be like topics for a series of lectures. So the first is, where can we get random bits in nature? Okay, so your quantum processes are the ones that we sort of know are probabilistic. But if you're going to look at a model where we model quantum processes, then we actually go beyond probabilistic computation and into quantum computation, where probabilities are not just numbers between zero and one, but are actually complex numbers interacting in all sorts of ways.
00:55:20.654 - 00:56:29.334, Speaker A: And you can possibly, with quantum machines, do things that you can't do even with this probabilistic model. So the other question is, how do we actually get, from natural sources, truly random bits? That's the main subject of work on extractors, which is a very vibrant area. And it's really closely linked with the constructions of the pseudo random generators that we've been talking about. But how it's linked is also kind of complicated. Did I answer your question? Yeah. So the skeptical viewpoint is we don't ever really need a physical source of randomness. Analyzing algorithms, assuming we had randomness, is just going to be a stepping stone towards replacing that randomness with carefully chosen deterministic steps.
00:56:29.334 - 00:57:28.464, Speaker A: And so then you don't have to worry about physics. If you believe that you don't have to worry about physics or how we're going to get our source of randomness, we're going to, it's just a, it's just a part is just a stepping stone towards analyzing, inventing and analyzing deterministic algorithms. So it's just a conceptual tool. We imagine randomness for temporarily and then get rid of it. But if you are not a skeptic, then you want to actually get your hand on randomness. Actually, I should say, even if you are a skeptic about randomized polynomial time versus deterministic polynomial time, a randomized algorithm could be linear time or sublinear time. And you can maybe prove even that if you have a randomized algorithm, sublinear time, your deterministic algorithm has to be at least linear time.
00:57:28.464 - 00:58:43.554, Speaker A: So you may still want to use the randomized algorithm. And if you look at like the deterministic primality testing versus the randomized primality testing, it's still, the deterministic primality testing is not at all practical compared to the randomized primality testing. So you really do want to also figure out ways of getting your hands on random bits. Um, so if you can, um, if the function is sort of defined ahead of time, and it's computed with high probability, so it's not, it's a function rather than a relation, then you can like look at it bit by bit and just talk about BPP. If it's a relational problem that's more complicated, and that's some of the motivation of Shafi Goldbasser's recent work on pseudo deterministic algorithms.
