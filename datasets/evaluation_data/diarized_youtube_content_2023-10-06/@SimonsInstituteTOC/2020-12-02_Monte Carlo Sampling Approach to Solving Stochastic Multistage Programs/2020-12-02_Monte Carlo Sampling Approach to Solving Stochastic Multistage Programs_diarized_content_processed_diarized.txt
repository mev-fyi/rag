00:00:00.160 - 00:00:32.946, Speaker A: It's a pleasure to introduce Alex Shapiro, who's well known to all of you. I mean, he's the Russell Chandler, the third chair and professor in industrial and systems engineering at Georgia Tech. Too many awards to list, but recently had a Dantzig Prize for mathematical optimization society in Siam, and this year was elected to the National Academy of Engineering. So my pleasure to introduce Alex. And I don't have the title in front of me, so I'll let you state it for us.
00:00:33.130 - 00:00:36.178, Speaker B: Thank you very much. How much time do I have when I show?
00:00:36.186 - 00:00:37.562, Speaker A: You've got 30 minutes.
00:00:37.618 - 00:00:39.346, Speaker B: Yeah, 30 minutes, yes.
00:00:39.530 - 00:00:40.962, Speaker A: Leave a couple minutes of questions if.
00:00:40.978 - 00:01:11.678, Speaker B: You can, but after such introduction, I'm intimidated. Let me open this. Let me do this. Share screen, right? Share screen. It's okay. Can you see my screen?
00:01:11.846 - 00:01:13.118, Speaker A: Yes, it looks good.
00:01:13.286 - 00:01:46.794, Speaker B: Oh, okay. It's good. Okay. So I can start? Yeah. Okay, so let me say a first few, just general words. I listened to many talks here, and what I will talk about, probably different point of view of that, of the stochastic optimization. It's going backwards, long way backwards, probably origins of linear programming with Dantzig and all that.
00:01:46.794 - 00:02:59.516, Speaker B: And in my opinion, this recent years was a very considerable progress in that. And I would say even different point of view, mainly from computational point of view, what can be solved, what cannot be solved and how to do that. So let me first state a general setting, which is reasonably simple, right? We suppose we have a random process you can think about, that is the sequence of random vectors. This is your data, and this is just notation for this, for the history of the process, and then we make decisions. In that sense, it's very similar to what everybody talking about there. But next is this formulation, what I say, and it's somewhat different here. So again, we have objective functions, and we suppose we have certain period of time on the horizon, and we want to say minimize.
00:02:59.516 - 00:03:39.534, Speaker B: It's on average, we minimize the expected value. This is a. So what is important here, and it's probably different from what I heard so far. The first simplifying assumption, simplifying is that the process, that distribution of the process, it's supposed to be a random process, but the distribution is not influenced by our decision. This is very important assumption. Why it's like that, I will explain in a moment. So in other words, the distribution of this data process doesn't depend on our decisions.
00:03:39.534 - 00:04:23.742, Speaker B: And I will mainly talk about linear case. So this, what is here, it's basically typically balanced equations. They are also local constraints. In this formulation, there's no clear separation between state and state variables, and control variables, it's not that important anymore, right? So if we take this point of view. So our policy is just a function of the information that we have at the time c. Now the question is, can this be numerically solved at this point? This is what I will talk about. This is where the all the progress is.
00:04:23.742 - 00:05:33.714, Speaker B: Now, from this, what is written here is if we take at this point of view, we have to solve this infinite dimensional problem, because typically from modeling point of view, the sequence of random vectors there, it can be even more complex than that, are continuous. Now, because of that, our decisions at every point of time is a function of the history. And if we take continuity point of view of continuous distributions, it's infinite dimensional problem. So in order to somehow to solve that, even to put it in a computer, I need to discretize that and the huge progress that was in recent years, and all this by using randomization methods. Okay, now, traditionally, if we're going back to stochastic programming, this was all formulated in terms of scenarios. So if you look all these old books on stochastic programming, you see this construction of scenarios three. This is what assumed to be happening in the future.
00:05:33.714 - 00:06:46.458, Speaker B: So if I go back to that is, can it be really solved this problem, what it means even to solve this problem, the complexity of that is growing fast, both with respect to dimensions. We'll say the state, usually one need to keep track of state variables. I'll talk about that with dimension of that of decision variables, and with a number of stages. And today we have reasonably good understanding of complexity of that sample complexity, when, when randomized that. And the conclusion of that is very pessimistic. And from optimization, from even a historical point of view, it all started with two stages, even two stage. Solving two stages exactly is probably out of question, because one has to compute the expectation value, which is integral.
00:06:46.458 - 00:07:39.210, Speaker B: And as dimension of random growth, one cannot compute it accurately. So the huge progress was made by some randomization methods. I will not talk about that. But even linear case, when we have this constraint, and you see this typically balanced type of constraints, the smart coding type of constraints, because we only look at one period before that, and even if our functions are linear, it's linear problem, it seems that generically it's computationally intractable. There are very strong results about that saying that. And that's understandable. Why so? Nevertheless, one can dismiss it that easily.
00:07:39.210 - 00:09:03.704, Speaker B: So what can be solved and why? I'm talking about net, because if the functions here are convicts with respect to decision variables, and we have this linear constraints, it's a convex function. So basically, if we have finite number of scenarios, scenarios are possible realizations of our process. If it is all linear, it's possible. This constraints here, usually it's simple type of box, type of constraints. One can formulate it as one large linear programming problem. It's called a deterministic equivalent of that. But the number of variables in that grows exponentially, both with dimension of this variable, variables, vectors and number of stages, and very quickly become astronomically large, practically impossible to do anything about that, right? So the other point here that probably, I should say now a few words, because it's now how we know the distribution of that even formulation here know, there are many questions about that, why we compute a, why we compute it on average, take an expected value, even if mathematically I want to formulate that, I need to know the distribution of my random process.
00:09:03.704 - 00:09:54.218, Speaker B: So another program in other direction became very popular now. So we don't know really that it's of that. So we take certain family of distribution and we look at min max problem. So in a static case, static case, when we have the objective function, say two stage, when we, from computational point of view, that for a given x and realization of random data, I can compute the value and probably say subgradient or something like that. Convexity in optimization, classical optimization is very, very basic. It's more important than differentiability. So in a static case, classical formulation is okay, I will take the most possible case with respect to certain family of distribution, how to choose this family.
00:09:54.218 - 00:10:30.904, Speaker B: There's a long, long discussion. I don't have time about that now. How we can even formulate that in multi stage case is not obvious at all. When we have this static case, when we have just object function, there is no dynamics here. No, it's reasonably clear. Of course, we take the expectation with respect to consider this distribution, and we take maximum of that. If everything is okay, it's reasonably clear what is there.
00:10:30.904 - 00:11:15.794, Speaker B: Now, when we go to multistage setting, it's not clear at all what it means, and there are quite non trivial technical questions about that. I don't have time to talk about that. One of the possible approaches about that is to assume that there is a reference measure and the set of distributions that we consider. Now talk immediately, we'll talk about dynamics. Now is absolutely continuous with respect to certain reference measure. Usually it's coming from the modeling. When they say, okay, our modeling is not exact, so there's a certain set of distributions, we can model that somehow.
00:11:15.794 - 00:12:29.648, Speaker B: And the moment I'm talking about that, I can consider the corresponding functional of that, which is defined by the worst possible case. If I going back to that, I can look at that, I can think about it as a function of that. And this functional has all types of properties. So from this point of view, it's very natural to talk about law environments of that. In other words, it's only depend on the distribution of our data of that with respect to the reference measure. And in all these constructions, it's important, okay, again, if we, because we talked about the sampling here, if I assume that it's slow invariant, it's only dependent on the distribution of that, I can probably construct the sampling approach of that. I sample, I assume that I can sample from the reference distribution, or in other words, when I observe a random data, it's coming from the true distribution, I can probably sample that, or I can build a model of that and I can sample it again from there.
00:12:29.648 - 00:13:39.040, Speaker B: And then when I look at its functional, I can talk about discretization of the randomization, and then there's a lot of theory about that, okay, NORAD said it's possible to show this low invariance. You can look at the alloy environs from the point of view of the functional, which is very natural, or from the point of view of the set of densities that correspond to chosen set, and it turns to be this equivalent between these two, you can draw from one to another. And this is just, okay, now one of the classical examples that I can talk is the so called, this is, it has many names. It's called average gradual risk, conditional value at risk. It has also long story, very long history. It was used in economics maybe about 100 years ago in different forms. And it's one of the examples of the very natural examples of risk functional that one can construct.
00:13:39.040 - 00:14:22.008, Speaker B: Somehow this give penalization on our high levels of that, if it go considerably above the average given realization. So it's very natural to talk about this type of risk measures. And again, there's a long history about that. This is example of this so called lower environment for height risk measure. Again, there's long, long history. Now let me talk about economics, I don't have much time. So one immediately question that we have how to extend this type of robust distribution.
00:14:22.008 - 00:15:25.284, Speaker B: It became recently known under the name robust distribution, to multistage setting, to dynamics. So it looks very natural. One has this objective function, no, there are constraints, feasibility constraints, we optimize it over policies, which is a certain set of distributions of our random process, and we take this type of mean marks, it turns out that this is not correct formulation. Why it's not correct formulation, for several reasons. It doesn't take this formulation, if we take it looks natural, doesn't take into account the dynamics of our process and the original dynamics is conditional. Every time we make the decision, it's a conditional. What we observed so far, and this, when we take expected value, it has very natural property of this.
00:15:25.284 - 00:16:33.184, Speaker B: It has various names that conditionally, if I take conditional expectation and expectation of that, it becomes expectation average of averages is total average. When we go to more complex cases like that, when we have distribution robust or risk averse, it doesn't happen anymore. So it's very natural because of that, because of how we start to write now dynamical equations, this is how one can solve this type of problem. One need to consider this certain type of nested formulations of that I try to write it nested formulation. So if this is single term, right, I'll have simply, in other words, I will consider only one probability distribution. It will be usually because of this tau property of expectation, will be usual expectation. But when I consider the maximum of that, I need to take, and I take maximum with respect to that, inside this nested formulation, which I need to formulate for dynamics of that, it's not the same anymore.
00:16:33.184 - 00:17:36.013, Speaker B: I only even wrote it what is called essential supreme, because it's not clear what is really supreme of that. One cannot take point by supreme, because it has many problems, measurability and some other problem. So this is not a trigger, I don't have time to go this really no trivial technical details about that. But what is interesting, if you, for example, if it is finite, all of that, this is a reasonably simple thing. Or say, if you have this discrete distribution, then one can take point wise maximum of that. What is interesting of that, there is one measure, risk measure, one set of measures such that I can write this functional map form, but it's very complicated. It's coming from Lynestone, and it will be not the same as this one, even in the simple case that I will say now, when we will have this so called, this rectangular property of that, even then it's not the same.
00:17:36.013 - 00:18:31.220, Speaker B: So if I do it like that, this nested formulation, I can write at one form in the previous state was just expected value. But here it's something else, which is not observed directly. But what we can do, we can write dynamical equations and the dynamical equation will look like that. So if I make no assumptions about the process, I have to keep track of all history of the process. It's useless. Because of that, I cannot use it for computations, because then I have kept track of all these rates will be impossible. So, one huge simplification of that, if I assume that my set of probability measures has a rectangular structure, it has different names, but it looks very natural.
00:18:31.220 - 00:19:16.622, Speaker B: So I consider my set of my probabilities has this some type direct probe extraction. So if this singleton, if it is just one, it means that this marginal distribution of my data process is stochastically independent of each other. So here I need only to model marginal distributions of each random vector. I don't have to keep them together. This is huge simplification. And of course the question is that how it's in a practical application, is it okay or not? No, it's a long talk. I don't have time to talk about that.
00:19:16.622 - 00:20:21.694, Speaker B: Sometimes, usually it's markovian structure, sometimes one can reduce to that, but I don't have time to talk about it. So, in rectangular case, my dynamical equation simplify enormously, right? So in other words here I have to deal with only with the marginal distribution of my each random vector and it's value function. If I go to this value function, if I take the correspond respect value function, only depend on state variables. I didn't say what state variables so far, but in applications usually, it's still enormous simplification of that now. Yet, there are two basic questions. If I want to solve that numerically, and this is all about at the end of the day, if it to be useful, I somehow had to solve that. One question is, if this has continuous distribution, I had to discretize that.
00:20:21.694 - 00:21:29.604, Speaker B: And we have quite good idea of what it means, because I have to deal only with the marginal distributions. I can sample from the marginal distribution, I have t stages, and I sample from each one of them, say hundred, I have random vector, and by using Montegallo technique, from each one I have sample hundred. The total number of scenarios, even by that, if I do it like that, grow exponentially, because I have to multiply that. So if I have only even have, say three, four stages, it's already become astronomical large, impossible to deal with that. But nevertheless, I can look at this value functions or cost to go functions. The next questions, if I discretize them, I have to only deal with the state variables. How can I deal with them? One way, of course, I can also discretize my state space.
00:21:29.604 - 00:21:58.884, Speaker B: This is out of questions. If dimension of that is say three or even three. And here we deal with some types of dimension ten, 2100, nobody solve cause of the mutuality problem. But sometimes in some cases something can be solved. This is a point. So the main point here that the value functions are convex. If I for example consider linear case, and this is extremely important.
00:21:58.884 - 00:23:14.096, Speaker B: So this natural idea now is try to approximate this value function by cutting planes. This is going long the way back to indiscrete deterministic so called Kelly algorithms at 60 years ago. And after that was huge progress. And that indeterministic case, probably this cutting plane had methods based this level method of Nestor Minorovsky. But anyway, the idea is the following of that, and it became quite successful for solving some classes of problems. So the idea is to approximate the value function by cutting plane going backwards and forward. In other words, we start from this last period of time and then we generate certain, what is called trial points when we add additional cut to our value function.
00:23:14.096 - 00:24:10.586, Speaker B: And we know how to do that. So at every iteration we have lower approximation of our value function by piecewise affine functions, which gives support for them. And we're going backward, adding, adding more and more cards in this classical list of ordering in the forward, after I generate certain approximation of the value function, I can go forward generating random sample of that. And that gives me point estimate of the value of the current, this approximation of policy. And also when I will make next the next approximation. This has become known as the DP algorithm, and it can be extended to risk averse case quite naturally. And sometimes it works.
00:24:10.586 - 00:24:12.094, Speaker B: How much time you have?
00:24:15.394 - 00:24:19.986, Speaker A: I. Seven minutes.
00:24:20.130 - 00:25:49.064, Speaker B: Seven minutes, okay, I still have some time. So if we look, so if we look at this, say expected value, what is called risk, we optimize it. On average, after we generate random sample, we sample from each marginal distribution, we have the value current value. It's possible to show that on average it gives us low bound for the true value of that. So because of that, we can construct lower bound and upper bound for the true value of our problem and see how it converge to each other. Okay, maybe this very important point here. The moment I construct this approximation, I generate a feasible policy for my problem, assuming that has what is called relatively complicated course, infeasibility doesn't happen, because I can go forward, generate a random sample, random sample path, it's called scenario, and every time I go forward, I solve the corresponding equation, this new programming and it generates one possible solution for this generate sample path of my problem.
00:25:49.064 - 00:26:24.524, Speaker B: So it can be done. I can do it for original problem, what is called true problem with continuous distributions by randomly sampling from continuous distribution. Or I can do it for generated discretization. Bottom and that give me idea how well I solve the problem. Because of that, I have an upper bound, which is statistical. I simply have many ide, many, many sample paths. For each one of that I get point estimate and average.
00:26:24.524 - 00:27:45.418, Speaker B: It works sometimes reasonably well, and I have the lower bound. Okay, this is what it's trying to say. Now, as I said, the complexity, computational complexity of that grows fast, generally with the number of stages and the number of this dimension of the state variables, how many state variables I have. Why this method sometimes works at all, we don't have very clear understanding of that. Empirical experiments that we have sometimes work reasonably well. We sometimes resolve the problem with very large number of stages, 120 stages, something like that, with reasonable variability of the data, with the state variable, say 1020, something like that. It seems to be the reason that from complexity point of view, one cannot really approximate high dimensional function, even convex, uniformly everywhere.
00:27:45.418 - 00:28:35.850, Speaker B: No number of cutting planes will start to go exponentially if you want to approximate it uniformly. But we probably, when we deal with real applications, we probably don't need to approximate the value functions everywhere. Sometimes it's low dimension, probably this all goes much lower dimension dimension of the state variable, sometimes probably in several places. We don't have clear understanding of that. This is one of the big questions of that. Another problem is that why we need so many stages, which people, some in application people try to do. I saw the hot talk where the people talk about 10,000 stages, what you can do with 10,000 stages.
00:28:35.850 - 00:29:59.620, Speaker B: Moreover, how you can predict your future that far in application, it's almost always people use a first stage solution, maybe second for some reasons, and then if recalculate it as the new information is coming in. So the question is one of the big questions, how far in the future I should look to have a reasonable first stage solution? In many cases, at least what I know, the probability problem has periodical behavior, nor some problem to solve. We solve it on monthly basis, something it's year has twelve months, and that basically forecast is repeated. So more or less, because otherwise how one can predict find the future. So it turns out that one has this certain periodical behavior of that it's possible to drastically reduce number of stages by using this periodical behavior. And the ideas here are similar to ideas what people use, of course, in optimal control and MDP. But there's essential difference here, right? First of all, we still even if we have say period twelve on monthly basis how to solve that? And what I wrote is somewhat standard.
00:29:59.620 - 00:30:59.830, Speaker B: We have discount factor and we have certain type of period. And then if it repeats itself, we can write the corresponding notice we need to make certain assumptions, the corresponding type of dynamical equations. It's possible to do in risk averse and risk natural case and sometimes it allows to drastically reduce the number of the stages, keeping the essential themes of the problem. One of the reasons why the people use the many stages is end of horizon effect. Because if one uses very few stages the algorithm will at the end say use all your resources because it doesn't care what happens afterwards. Because of the deal with that, people use many more stages of the effect. This take at the automatic level.
00:30:59.830 - 00:31:04.542, Speaker B: Take care automatically. I suppose I have to finish in a moment, right?
00:31:04.598 - 00:31:05.234, Speaker A: Yes.
00:31:05.574 - 00:31:43.626, Speaker B: Okay, so it's possible to do that. It's. I don't have time to talk about this. This is example of this, how we solve huge problems sometimes using a low and upper bound. Upper bound is computed by a certain dual problem. Solving certain dual problems and one of the big, big problems in all that when discount factor gets very close to one, it's well known in that case it becomes more and more difficult. So even the duplications that we deal with the discount factor is very close to one.
00:31:43.626 - 00:31:51.654, Speaker B: It's 0.99, something like that. And yet it was possible to solve with reasonable equities. Okay, questions.
00:31:52.654 - 00:32:13.894, Speaker A: Oh great. Thank you very much. I think that we'll have to, we're going to have a break right after the next speaker, so I think we'll have to defer questions till then, otherwise we're going to lose our break. But I'm sure there'll be lots of questions and I especially. I'd like to see another talk on the examples, but again, thank you very much.
