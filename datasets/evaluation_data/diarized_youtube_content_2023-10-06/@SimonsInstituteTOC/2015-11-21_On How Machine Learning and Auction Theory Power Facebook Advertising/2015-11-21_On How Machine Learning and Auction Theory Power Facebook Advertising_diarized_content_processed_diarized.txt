00:00:02.160 - 00:00:14.014, Speaker A: Thanks. Okay. Yeah. So thanks, everybody. And so apologies in advance. This is so Chin Mei and Joaquin both really wanted to be here. Chin Mei was sick and his son was sick, and Joaquin, just hours ago, had some emergency.
00:00:14.014 - 00:00:39.588, Speaker A: So I'm very much flying by the seat of my pants here. So apologies in advance, but I'll do the best I can. So this is basically, Joaquin was going to talk about the machine learning side of Facebook, and Shin Mei was going to talk about the auction side. And so my background. So Joaquin's in machine learning group. Shin Mei is in this ads delivery product group. I'm in this economic research group.
00:00:39.588 - 00:01:49.120, Speaker A: So I'm definitely not as intimately familiar with either of these areas, but at least on the auction side, I'm working with Chinmai, so I can at least, even if the presentation is a little bit rough, hopefully, please ask questions if things are unclear, and hopefully I'll be able to answer them as best I can. Okay, so as I said, the talk is basically broken down into this machine learning side. How do we predict various things that we care about on Facebook? Probabilities that users are going to be doing different things. And then given those predictions as input, how do we actually decide what content to show a given user? Okay, so at a high level, we can sort of think about Facebook as this personalized newspaper. And so essentially, all this really means is we want some individual content for each person, and each person cares about different stories in different ways. So if I come to Facebook, maybe there's two possible worlds that I could be in. One is where I see these stories on the left.
00:01:49.120 - 00:02:48.526, Speaker A: One is where I see these stories on the right. And so our question is, which of these should we actually show a user when they come to Facebook? Right? And so some of the, you know, maybe this person on the left is, you know, maybe the story itself isn't that interesting, but maybe it's a close tie or something like that. And so it's really a challenging problem here. But what we want to show people is the stories that they're actually caring about and not showing them the things, all the other noise, essentially. And so for a long time, actually, this wasn't really an issue because we just showed all users everything just in reverse chronological order, as I think Twitter had done as well. And eventually it just got to be too much. So there are people that have complained when we switch to this sort of ranking version, because you're not seeing everything.
00:02:48.526 - 00:03:36.630, Speaker A: And there is this risk that maybe we show you something that, or maybe we don't show you something that you would have wanted to have seen. But the reality is just a given person has on average 1500 stories that they could potentially see in a given day. And so we've run experiments on this without ranking people, click, comment, and like things significantly less often than they do. When we have this ranking, people miss important stories or stories that we deem to be important. People see content that we think is irrelevant. Maybe you don't care that someone that you went to junior high with had a carrot cake for lunch or whatever it may be. And so this is why ranking is important.
00:03:36.630 - 00:04:15.064, Speaker A: Essentially, we have to figure out what stories to actually be showing the user. And as I said, there are different, so different users have different preferences. So one person may only like updates that are coming from his or her close personal friends. Someone else may really have an interest in news stories. Someone else may, they actually have value for these stories that are coming from weak ties and they want to get the diversity in stories. Someone else maybe really likes funny cat videos. Somebody else, I guess, likes clicking cows.
00:04:15.064 - 00:05:01.754, Speaker A: I didn't make these slides also as a preface, but I think what this is trying to get at is some very specific preferences that people have and these interests that this would be the ideal as knowing these sorts of things about individual users. But we don't have a way to actually get at this specific of preferences. We don't ask an individual user what sort of things they care about, although there are some ways for users to express their preferences. So we can't make these sorts of decisions for everybody. We just don't have that. You know, we can't measure that. So, but what we can measure are the things that we observe happening on Facebook, right? So these different, what are called feed, feed metrics.
00:05:01.754 - 00:06:16.020, Speaker A: So just different things happening in feed, right. We see when somebody clicks on a given story through to a news article or clicks on a link to a video, when somebody likes a given story, we see that when someone comments on a story, when somebody shares a story, how long someone is viewing a story, we see if there's a video, how long they've watched it. And so these are sort of the observables that we have. And we could think about users having, this is some indication of the value that the user is having by they're giving some signal that they like something by liking it on Facebook. So basically the machine learning part of this then is predicting the probability that these different events occur, right? So it's just simple CTR prediction is one example of this, right? And it's just, we have other events aside from clicks that we also want to be predicting. Okay. So if we have these predictions of some probability of all these different events occurring, we want to say what the value that a given user has for seeing this story.
00:06:16.020 - 00:07:10.834, Speaker A: And so Facebook is very much about incremental improvement, just getting something out there and revising it as we go. So this is sort of what started out was somebody with some good intuition, hopefully, of the system decided that, well, a like is five times the value of a click. Uh, a comment is four times the value of a like. Um, if you are hiding a story, then that's very bad. Uh, and so these, this is basically some linear utility function that we're thinking about where we've sort of decided, uh, you know, some, somebody on uh, this team has decided upon these coefficients, essentially. Uh, so there's sort of an interesting thing here where, so I'm mostly going to be talking about this left hand column for the first part of the talk anyway, right. So we have these relatively sophisticated models for coming up with these probabilities.
00:07:10.834 - 00:08:13.344, Speaker A: The question is, like when you're multiplying them by these sort of hand picked coefficients, you know, maybe there's room for some improvement there. So I can talk online if you want, about what, you know, there has been some improvements on this where now there's sort of a, I guess, revealed preferences approach of trying to back out some more meaningful values for these underlying events. But I'll be talking a little bit at a high level about these event predictions. Okay, so the example that Joaquin gave here is actually, I guess, timely given Star wars is, I think, coming out soon. But so basically we have, this is a possible story on Facebook. For those of you that are unfamiliar, this is one thing that you can do is associate some relationship that you have with another user on Facebook. So here Luke has added this other user, Darth as his father on Facebook.
00:08:13.344 - 00:08:58.984, Speaker A: And there may be some other user, say Yoda, that can potentially see this story, as well as a number of other stories of other people that Yoda is friends with. So the question is, should we show Yoda this story? And I've already said that the way that we're going to think about this is this linear utility function. So probability of a like times the value of a like. And I said for now we're just sort of, we've chosen this value for a like. And so we're really just trying to figure out what the probability of a like is here. And so how do we actually do this? So basically there's a few different types of features at a high level. They're mainly looking at historical data.
00:08:58.984 - 00:09:41.598, Speaker A: So the first thing is the relationship between Yoda and Luke. So there have been previous stories that Yoda has seen from Luke, say these three stories here, and maybe Yoda has liked two of those three stories. So that's our first signal. Second is the relationship between Yoda and this type of relationship story. Right. So maybe if we look at the data on Yoda, he's liked 50% of these relationship stories that have come through, as opposed to when he sees someone posting a photo, maybe he only likes 10% of those stories. Birthday posts, he sees a lot of those.
00:09:41.598 - 00:09:58.760, Speaker A: He doesn't care. He likes .1% of those. So this is sort of. The second piece is the relative amount that Yoda is liking this type of story. Sort of. The third high level feature has nothing to do with Yoda, but it has to do with the story.
00:09:58.760 - 00:10:46.212, Speaker A: Right. What historically have other people done when they see this story? So maybe these five people around the border here had also previously seen this story about Luke adding Darth as his father, and two of them liked it and three of them didn't. So this is at a very high level. These are sort of the types of at least base features that we're thinking about when we're trying to predict these things. So that's sort of at a very high level. You know, we want to show these stories you care about, nothing else. And the main way we're getting at these predictions, so we have this linear utility function and sort of handpicking the coefficients as I described it here, using some of these features to predict the probabilities of these different things.
00:10:46.212 - 00:10:52.220, Speaker A: And so the metrics here are these observables that we actually have, trying to get at the value.
00:10:52.412 - 00:10:55.388, Speaker B: The X's here mean, disliked or did.
00:10:55.436 - 00:11:11.662, Speaker A: I'm sorry, x's here just mean that did not like. Yeah, so sorry, that's ambiguous. So it is not the case that they liked it. They just saw it and did nothing say. Yep, but there could be. It's this linear combination. Right? So there's.
00:11:11.662 - 00:11:40.274, Speaker A: It's not just likes. I didn't show this other prediction of the probability that somebody hides the story, but it's a similar sort of thing where, you know, we'll try to estimate what the probability is that this user hides the story based on how he or she has hit things in the past and how others have hit that particular story. All right, any questions about that so far?
00:11:40.614 - 00:11:46.954, Speaker C: Yes, I guess in an ideal world. Would every user have some sort of interaction with everything on his feed or not?
00:11:47.374 - 00:11:55.942, Speaker A: Your ideal, what it is, would every user. So would we like it to be the case that users are interacting with everything?
00:11:56.038 - 00:12:02.600, Speaker C: Yeah, for example, if I go down my feed, should I like one comment, the next, have some sort of interaction with everything on my feed.
00:12:02.702 - 00:12:29.220, Speaker A: So if you actually interact with stories that if you hide things you don't like and you like things you do like, if everything's working, it should be the case that you see better content because of that. So, yeah, if you're, you're supplying more data, essentially to the predictions. So, yeah. So hide things you don't like basically will improve the stories you see, hopefully, yep.
00:12:29.372 - 00:12:31.676, Speaker C: So, I mean, wouldn't this create some.
00:12:31.700 - 00:13:20.094, Speaker A: Kind of a selection problem that we're going to only see stories that we like? Should we throw in a story that somebody doesn't like every once in a while in order to. Yeah, that's a good question. So, okay, so I guess you're thinking like, maybe in this extreme case, I have these very homogeneous preferences, and now it turns out that I am seeing nothing, essentially, but stories from Darth Vader or something like that. Or from Luke. Yeah, that's a good point. So I am not, unfortunately, intimately familiar enough with how the models are trained, but I imagine there's some exploration that's being done. That's a good point.
00:13:22.774 - 00:13:38.754, Speaker D: The feature selection is sort of like arbitrary meaning, sort of like the features you showed us, are the features used or kind of like maybe, you know, you can look up all sorts of features, like maybe. I tend to like things that, you know, this other specific person likes.
00:13:41.174 - 00:13:41.486, Speaker A: And.
00:13:41.510 - 00:13:44.674, Speaker D: So on, so forth. So there's like a rich set of possible features.
00:13:45.454 - 00:14:24.904, Speaker A: Yeah. So, so, so this is sort of at a very high level, some of the types of features that are used. So actually, one advantage of this system is that it's very easy to experiment with new features. And that was sort of one of Joaquin's main takeaways, basically, was that the system that has been built has, allows for very rapid experimentation, where if you have some idea for a feature, you can basically get it in the system as fast as you can think of it. And so if you have some additional features like that, you can throw it in and test it and it's relatively easy to do. Good question.
00:14:26.324 - 00:14:27.184, Speaker C: Okay.
00:14:27.844 - 00:15:26.804, Speaker A: All right. So far I've been talking about these organic stories. So what is the, my value for seeing some user generated content? And we can ask the same thing about ads, though, right? And so I think Joaquin was originally giving some example here of sort of the ideal case of an ad that he saw. So Joaquin at least used to have a hearing aid, and it just so happened that he saw an ad for this discrete hearing aid that's affordable, that he essentially had no prior knowledge of this and it's not something that he would have gone to search engine and searched for. And so this is just something that he found to be something where you don't necessarily have intent. But there is still potentially a lot of value of seeing ads on Facebook. And this is sort of the ideal.
00:15:26.804 - 00:15:50.676, Speaker A: We would like all ads to be like this. Of course, that doesn't always happen. Okay. Another thing to note about the ads is just the scale of what we're dealing with here. So there's over 2 million advertisers in the system. Advertisers have multiple ads. So at any given point, when a user comes, there's tens of millions of ads that we have to choose from.
00:15:50.676 - 00:16:42.054, Speaker A: We have a billion people that are coming to the site daily. So this results in every day having trillions of ads that we have to come up with predictions for. So the point is, whatever we do, it really has to scale and it has to be fast and furthermore, it has to be accurate. Right. Because especially when we're talking about the, on the ad side of things. So this is sort of a pointer forward to the second half of this. But if you're familiar with sponsored search as well, if we're predicting incorrectly some probability that this ad is going to get clicked, we're sort of incorrectly boosting it and potentially showing it over some other ad that the user would have preferred seeing.
00:16:42.054 - 00:17:21.716, Speaker A: So this is essentially, let's see. Ok, so this is sort of part of what the advertiser, this is part of the interface that the advertiser sees. So the first point here is to note that advertisers can bid for different events, different objectives. So here, this particular advertiser is maybe bidding for users liking its page. That's what it has value for. And so we're essentially converting this into. So there's some value that the advertiser has for a like.
00:17:21.716 - 00:17:57.890, Speaker A: There's some probability that we think there's going to be a like given an impression. Um, so this is basically the same sort of utility function that we were thinking about before, except now instead of those coefficients that we were using, the advertiser's just expressing what its value is for these different events. Okay. Uh, and the other point is just that ads are created all the time. And so we have to be able to learn very quickly what the probabilities of these different events are, which are.
00:17:57.922 - 00:17:59.854, Speaker D: Specific to every advertiser.
00:18:01.394 - 00:18:40.214, Speaker A: The probability of. Yes. Yep. So basically the question is, yeah, what is the probability that this advertiser will have this event occur in this particular context, say, here on the page? So this is, I'll say this is the piece that I'm least familiar with. But essentially there are a few different models that have been tried. This is just starting with a very simple logistic regression. So the main point is things have to be fast, they have to scale.
00:18:40.214 - 00:19:39.234, Speaker A: So let's try the simplest things that we really, you know, that we can and see how they work. So really, all the high level point is here is, you know, we've sort of incremented on a number of different models. First is this logistic regression, then, so this had weights for user ids and weights for ad ids. So individual user and individual ad. This is just a iteration on that, where actually there's just a weight on the click through rate. So there's a lookup table, essentially, that says, what is the historical click through rate for a given user id? What's the historical click through rate for a given ad id? Puts those in and then puts it into this logistic regression. So these are very simple models, very compact, and we've tried some additional things with, like boosted decision trees.
00:19:39.234 - 00:20:50.906, Speaker A: The problem with this, while it leads to more accuracy, is that it is cannot train online as quickly as some of these other models do. So eventually, what? So Joaquin has a paper on this that I can refer anyone to who's interested in it further, but I'll focus more on the econ side of this. But essentially the takeaway from Joaquin was they really didn't have any theoretical basis for coming up with this model of boosted decision trees and then running a regression on top of that logistic regression. It was just something they tried because it was so easy to experiment with these different models. And so the main takeaway, again, was just the ability to try these things so quickly has led to it being very easy to explore these different types of models. So this is more the practical elements of prediction rather than having a good theoretical basis or knowing what it was in advance. Okay.
00:20:50.906 - 00:21:42.738, Speaker A: And one last practical aspect is about the. So I said there's a lot of ads that we have to make predictions for in a given day, really more than we actually can make predictions for in practice. So we don't, when a given user shows up, we don't predict the probability of a click for every single ad that we could potentially be showing to that user. There's some sort of filtering process that's done. And this is essentially just, this is another practical aspect that was used where. So on the x axis, what we're showing here is the course of a week, and this orange line is the number of requests that are happening throughout that time period. So you see the cyclic effect is just the time of day.
00:21:42.738 - 00:22:34.828, Speaker A: So at night in the US, probably we're just getting fewer requests than we otherwise, otherwise are. And so just the system, it's so easy to add these different layers. What was done here is just having some controller that is dynamically controlling the number of ads that are ranked based on the number of requests that are happening at that given time. So if there's fewer requests happening in the middle of the night, we can actually consider making predictions for a larger number of ads for a given request. So this actually ended up being leading. So basically what you're potentially losing by not doing this is some of these things that you didn't bother predicting a click through rate for. Actually, if you did, you found out that it was a good thing to show.
00:22:34.828 - 00:23:36.262, Speaker A: And so by having this sort of dynamic controller, we're able to capture some of this lost value. Okay, so really, Joaquin's main takeaway here, I think, was, I think I've said it enough, but just experimenting with things and having the frameworks in place so that you can very easily try new things and see if they work in practice. And we have found oftentimes that things that we don't always have good theoretical basis for the things that we try, but it's just so easy to try them. Uh, that sometimes that has actually worked out pretty well. Okay, any, so that's, that's sort of the, the machine learning side in a nutshell. Is there any, any questions on that? Good. Okay, so next, what I'm going to be talking about.
00:23:36.262 - 00:24:14.392, Speaker A: So we have these predictions, probabilities of clicks, probabilities of likes. Now the question is, what do we actually do with them to decide not just what organic stories to show, but really how to fill out this entire page when a user comes to Facebook. And so one thing to note is there's actually three different types of stories on Facebook. So the first group is these organic stories generated by users. So this is your friend posting a photo or sharing a video. The second are ads submitted by advertisers. The third are these what we call long term value content.
00:24:14.392 - 00:24:49.044, Speaker A: And so this is content generated by Facebook that we think has some positive value on the ecosystem by showing this to the user. So it's kind of small, but some examples of this would be. It's a lot of recommendations. Right? So recommendations for friends, which we think will give you and this potential friend additional content in your feedback. Recommendations for groups that you might be interested in joining. If you are an advertiser as well as a user, you might get recommendations about things that you can do to your. Improve your ad campaign.
00:24:49.044 - 00:25:28.776, Speaker A: Maybe adding a conversion pixel or something like that. So really, the problem that we have is we are going to take all of this as input. A user shows up, and we have all of these candidate ads, these candidate long term value stories, and these organic stories that have been ranked already in the process I described previously. We want to come up with some allocation and payment, and when we're talking about an allocation here. So now we're in Shinmei's section. So Chinmai is calling this a configuration, I think, because it's. I think he's calling it this because it's not just ads.
00:25:28.776 - 00:26:11.324, Speaker A: It's really sort of allocating all of these organic stories and this other content as well. One thing to note here is that we are constraining the space that we are thinking about these possible allocations or configurations. The organic stories are already ranked as input, so we do not consider on the ad side, flipping organic stories around. We're only reasoning about inserting organic stories in between. Sorry. Inserting ads or inserting these long term value stories in between various organic stories.
00:26:13.144 - 00:26:15.992, Speaker B: What do you mean the organic stories are ranked already?
00:26:16.088 - 00:26:53.400, Speaker A: Do you mean chronologically or not? Chronologically? Good question. Basically ranked according to. You could think of that the utility, this linear utility function, the probability of a click times what we think the user's value of this click is. So we are. So there's basically some process upstream that is doing upstream from this ads allocation problem that is deciding on the ordering of these organic stories, which is based on this sort of utility function. Now, it wouldn't have to be like this, right? We could throw all of the. Decide all of these things at once.
00:26:53.400 - 00:27:10.792, Speaker A: But this is purely simplification for computational reasons and also just organizational reasons within, like, there's employees working on these things and, you know, it's nice to, in practice, really, to have sort of these module modular areas that people can work on independently.
00:27:10.928 - 00:27:16.762, Speaker B: So those utility functions were the same basically, for every user. They were independent of the user.
00:27:16.898 - 00:27:21.506, Speaker A: The coefficients. Yes. Yep. Yep. Good question.
00:27:21.690 - 00:27:35.254, Speaker D: But if you were to optimize them together. I mean, it's unclear what I mean, clearly, how good, how engaging the content you show me is, the more likely it is. I'm gonna scroll down, maybe.
00:27:37.474 - 00:28:22.974, Speaker A: Yes. You're saying like this, probably we're not losing too much by making the simplification I think you're saying. Agreed. Yeah, it would have to be, it can actually happen, but it would have to be a weird situation where we would think that actually showing something in a different order than it's originally in would be better. And so actually, maybe this is a good point to bring up a difference between sort of the classical model. Yeah. So some of the things I like to sort of convey here is maybe differences between what we're doing here and sort of what the theory, you know, is normally the assumptions that are normally being made.
00:28:22.974 - 00:29:04.910, Speaker A: And so one thing that is different here is when you think about a probability of a click, right. In most models, there's this separability of this advertiser effect. So there's some probability that this ad will be clicked on. And then there's this position effect, which is discounting that, this multiplicative effect. So we do have that in our model. But a difference which actually complicates things is we found that different events have different discount rates. So for example, the click discount rate is different from the hiding a story discount rate.
00:29:04.910 - 00:29:53.414, Speaker A: And so this sort of flies in the face of this cascade model. If you think about like, there's some probability that you will get to this story and that, you know, it will have your attention, and then given that it has your attention, there's some probability that you will click or some probability that you'll do whatever event. So what we found is that actually we do see that there's different sort of decay rates for these different events. And so really what this breaks, fortunately, we don't have. There's a number of other reasons where it doesn't matter, where we can't just greedily rank things anyway. But that's what it would break, essentially. It's not necessarily the case that we can sort of greedily say, this is the best story, this is the second best story, this is the third best story, because it depends.
00:29:53.414 - 00:30:45.480, Speaker A: Some story may prefer one slot and some other story may prefer some other slot if one of them has a high probability of getting clicked and the other has a high probability of some other event that decays in a different way. So I guess that's maybe takeaway number one is that this is sort of one assumption that is typically made that we are not making. And it adds some additional complications to what we're doing. Okay, any other questions for the time being? Okay, so, okay, so this is our problem now is coming up with this configuration. So how do we do this? We actually run a VCG auction. And so Chidme says VCG auction is a necessity here. I think really what he's saying is that GSP will not handle this problem.
00:30:45.480 - 00:31:18.424, Speaker A: Right. If you look at these different stories, for one, they're of different sizes, and for two, there's various additional constraints on the, essentially the ordering of things that we can show. For instance, we'll never show an ad and then a friend recommendation and then an ad. So if there's going to be a friend recommendation, we'll put it on. We'll sort of make these things distinct. Right. Friend recommendations and then ads.
00:31:18.424 - 00:31:55.384, Speaker A: So we don't have this model even if we wanted to. It doesn't really fit into the GSP slot model. So this is basically, for those of you unfamiliar, there's sort of two places that we're putting these stories. So one is on the right hand side here and this is sort of some fixed length, but individual ads can have different sizes. Second is the feed. This is where we're spending most of our time thinking about. It's a much larger portion of our revenue.
00:31:55.384 - 00:32:46.584, Speaker A: So the properties here again, we have these different stories that are of different sizes. And this is essentially some infinite, essentially infinitely long page. And what we're thinking about when a user loads the page, we are sort of plopping. If we think about those inputs, we're sort of plopping these organic stories on the top. And now we're thinking about how to insert these sponsored stories. But considering how that is actually decreasing the value of displacing value of everything else that's potentially on the page. And so to make things even more complicated, so we also have horizontally scrollable slots essentially.
00:32:46.584 - 00:33:48.826, Speaker A: So it's not just that we're putting ads within this feed top to bottom, but there's also an individual story can go left to right and also have potential ads go in there. Okay, so why VCG? I think this audience will be okay with this reasoning. So value maximizing and incentive compatible. Now we put an asterisk by incentive compatible because really we need, it's incentive compatible with some assumptions holding that may not actually hold in practice. And so this is sort of a question we're actively thinking. Thinking about is in reality how incentive compatible are these auctions? But then from the engineering perspective, it's also useful to have this because it is general. We introduce some horizontal scrolling ads and there's not any question really about what the allocation rule is going to be.
00:33:48.826 - 00:34:57.014, Speaker A: We already know what it is. So really we're just pushing all of the hard work into this, coming up with some optimization that will find this optimal allocation. Okay, so very quickly, really this is just describing VCG, right? So we, I guess in our context a little bit. So we have some set of agents and we have some set of configurations. So these are just possible allocations that we could have. And so there's some value that a, so this right hand side is the value that a given agent has for some configuration and there's some total value for the configuration which is just the sum of all of the individual values, right? And so of course, you know, VCG, we're just trying to choose this value maximizing configuration and an individual agent, we're just going to charge them their externalities. So let's remove them, let's rerun this algorithm and let's see how everybody else's value has differed.
00:34:57.014 - 00:36:00.234, Speaker A: Now there's question. So the first question is how do we actually come up with this value that an ad has for a given configuration? And I've touched on it a little bit with this utility function, value of click, probability of click. Um, but there's other rules as well, uh, some of which are just saying that we don't want a given user to see too many ads. And so there's basically some business logic that goes in, um, potentially uh, changing the values of various configurations. Um, in addition, uh, this, this is sort of thinking about a single auction in isolation. But of course there's these cross auction dependencies such as budget constraints that are also going to play into what we think about as an individual bid. And then, yeah, so given these values then of course we have to solve this optimization problem.
00:36:00.234 - 00:36:17.144, Speaker A: And so really over here we said incentive compatible had an asterisk, but we could put an asterisk next to value maximizing as well because we're trying to find the optimal allocation, but we're taking many shortcuts that are not actually going to find it in practice. But we're doing our best to find that.
00:36:19.364 - 00:36:23.436, Speaker D: So this is for every user going to his news feed?
00:36:23.540 - 00:36:43.844, Speaker A: Yes. Yep. User comes, loads the feed. If there's some new stories, there's this upstream thing that's deciding on this ranking of organic stories. And then we're going to run VCG to decide where to put these sponsored stories and these long term value stories both in feed and on the right.
00:36:43.884 - 00:36:47.944, Speaker D: Hand side, you're not doing VCG over 10,000 users.
00:36:48.484 - 00:36:53.916, Speaker A: Oh, that's right. So it's for, on an individual query level basis. What about the set of ads?
00:36:53.940 - 00:37:03.928, Speaker C: I remember you said that you restrict some set of ads. So it's possible that if I didn't participate then there will be some other ad shows into this restricted set. You don't account.
00:37:04.056 - 00:37:31.004, Speaker A: Yeah, that's a good point. No. So we don't do anything like that. So there's sort of this process of, because we can't consider all these ads at once. There's this candidate set and then, yeah, once we remove that ad, we don't try to get some new ad from the candidate set. Yeah, I don't know how much. Hopefully it wouldn't make that big of a difference because it would just, the only time it would make a difference is if this one ad was on the margin where it actually would show up.
00:37:31.004 - 00:37:36.134, Speaker A: But I don't think anyone's tried it to say for sure if that would make a difference or not.
00:37:39.274 - 00:37:40.094, Speaker C: All right.
00:37:45.434 - 00:38:29.404, Speaker A: Okay. And so, right, so we've said that advertisers can express values for these different objectives. Some advertiser might care about page clicks, someone else might care about conversions to their website. So the first thing that we do is just convert all of these values for these different events into some common unit, essentially. So some underlying value for a given, say, impression. So there's essentially some bid for an event, some probability of a given event. And so the value function again is just this dot product of the, so the vector of per event bids times these probabilities.
00:38:29.404 - 00:39:24.994, Speaker A: Okay. Okay. So one other twist here from sort of the classic sponsored story or, sorry, sponsored search model is we do have these organic stories that we're inserting these ads amongst. And so we can say how the value has changed for this particular advertiser that has specified this value. But this utility for these organic stories is not in terms of dollars. Right. And so one thing that we have to do is actually come up with some monetary value for this utility that we've come up with for a given story.
00:39:24.994 - 00:40:15.354, Speaker A: And similarly another difference is that a given story, a sponsored story, there's some probability that the user may say hide that sponsored story. So even sponsored stories. So there's some advertiser value for that story being shown in this position. But there's also the user value for seeing this ad in this position. So if we think there's some probability that the user is going to hide this particular story. That's essentially a negative bid that is going into this sponsored stories bid. Okay, so that's what I just described here.
00:40:15.354 - 00:40:49.444, Speaker A: So, yeah, so this is essentially what we're doing is it's not just about the value that the advertisers have for these different stories appearing in different places. We're sort of placing bids on behalf of the user or the ecosystem for these different stories appearing in different places. Okay, so any questions on that? So that's sort of looking at this from a single auction perspective. And now we'll address some of these aspects where, you know, maybe we can't look at a single auction in isolation.
00:40:50.144 - 00:40:54.764, Speaker C: You have to say what value you place on your preferences. What bit?
00:40:55.704 - 00:41:20.758, Speaker A: Yeah, that's a good question. So I can't tell you a number, but I can give you sort of qualitatively how we do that. And that will come up in, I think, three slides. Good question. All right, so one of these cross auction dependencies, of course, which is common in sponsored search as well, is budget optimization. Right. So this is the interface that an advertiser has.
00:41:20.758 - 00:42:07.232, Speaker A: And down on the bottom here, the advertiser is specifying some bid for clicks up top. They're specifying some budget, some amount they're wanting to spend. So this advertiser wanting to spend $700 starting June 9 for lasting about a week. So this is really, I think this actually is not unique to Facebook. This is just sponsored search also has this problem. But we'll say what we're doing here to handle this. So if we think about this particular bid that that advertiser has expressed, if we enter that bid in all of these auctions, it may turn out that the advertiser spends its budget halfway through its campaign.
00:42:07.232 - 00:42:36.058, Speaker A: So this is time on the x axis. You can think of each of these dots as just being an auction that was happening at that time. And even think about these as like single slot auctions or something like that, where this is just the price that the advertiser will pay if he's bidding more than that. Um, right. So it could be that this advertiser is exhausting its budget halfway through the day. Um, this is bad for a couple reasons. One is just potentially expectations.
00:42:36.058 - 00:43:34.344, Speaker A: This advertiser may be intending to have its budget being spent throughout the entire period. Uh, but also it could be that if, if the advertiser actually has the same value for all these different clicks happening, you know, it would prefer to not be paying that high amount for a click up there when it could have been lower and received a click here. So sort of two ways that this is handled. So one is this probab notion of probabilistic pacing. So we'll choose some multiplier and basically probabilistically have this add go into the auction trying to find this multiplier, essentially that exhaust the budget over, you know, so that it spends it smoothly. Over the course of the period, we actually do something different, which is this multiplicative version. And so instead of probabilistic, we just scale the bid down so that the budget is spent in this time period.
00:43:34.344 - 00:44:25.874, Speaker A: Now there's arguments for both of these. It's not, I don't think that this is just strictly dominating the other. It really depends on what assumptions you're making about the value that advertisers are having here. Right? So if it's actually not the case that let's say that the advertiser is bidding for clicks, but it actually cares about conversions, right? And it may be the case that for one reason or another, maybe demographics are different in, you know, for these higher bids it's more females versus males, or maybe there's a common value element, but it could be that actually the probabilistic version would be better. So basically this is an optimal thing to do under some assumptions about the underlying values that advertisers are having for clicks.
00:44:27.934 - 00:44:28.342, Speaker C: Ok?
00:44:28.398 - 00:45:39.024, Speaker A: And to get to the point that was made about what is the actual value that we're thinking about for the organic side. So this is sort of another cross auction dependency that I think the analog really works well with the. The analog is the budget pacing, essentially, where we have this multiplier that's scaling the bid up and down to try to get the budget spent smoothly over this period. Similarly, the intuition is that a user is going to be, you know, have a more negative reaction to ads, the more and more ads that it's seeing. So basically we have a controller on the organic bid, so we're scaling it up and down based on how many ads it has seen previously. So this is sort of a control both for the quality that the advertiser is getting and it's also sort of subsuming reserve prices in a sense as well. Because if we, you know, scaling this up higher is causing some advertiser to have more displaced, it's displacing more value when the ad actually is shown.
00:45:44.724 - 00:45:51.704, Speaker D: So this virtual bidder, does it bid on the bottom right?
00:45:53.324 - 00:45:55.296, Speaker A: On the bottom right, like what you.
00:45:55.320 - 00:45:56.004, Speaker D: Showed.
00:45:57.744 - 00:46:26.224, Speaker A: The right hand side. Yes. So it's basically, you could think of it as any given story, whether it's an ad or otherwise, has this organic component, regardless of where it shows up. And. Yeah, we'll scale that up and down based on how many ads the user's seeing.
00:46:26.884 - 00:46:38.384, Speaker C: Is the control loop roughly keeping the number of ads roughly constant across users, or is there some extra loop that decides how many ads each user should see?
00:46:40.284 - 00:47:08.056, Speaker A: Yeah, good question. So this is. We're not choosing to have some. Yeah, so it's the former. So basically, we do not want, just because some user is more valuable to advertisers than others, we don't want that particular user to just be swamped with ads. So it is sort of a, you know, there's some amount of ads, essentially, that a given user should be able to see. And so we'll, we'll change this accordingly.
00:47:08.200 - 00:47:18.966, Speaker B: Is like organic or ad a binary feature, or is there a scale? Because, like, with the ad you showed for the hearing aid, it said like, one friend has liked this or whatever. Is that like a.
00:47:19.150 - 00:47:49.914, Speaker A: So the one friend had like, oh, I see what you're saying. Yeah. So that's sort of like in this murky middle ground. Like, is it an ad or is it a story? So that is an ad, but this is with some, I don't remember what we call it internally, but it's some additional context that basically we may think that you're more likely to like this thing or click on this thing because we're giving you some additional signal that organic. Yes. Yep. Yep.
00:47:51.054 - 00:47:57.674, Speaker D: Since bit lowering and throttling are so different, shouldn't the advertiser actually decide?
00:47:58.614 - 00:48:47.244, Speaker A: That's. Yeah, that's a good question. And so I think if you look at, I think that's a feature that perhaps we could implement. So really, I think the whole perspective that we're taking really is nothing to do with trying to extract revenue. We're really trying to have this system that is providing as much value to advertisers and users as we can. And so this seems like sort of the areas for improvement is like figuring out how we can better allow advertisers to either easily express their preferences. So either they can express their preferences right now, but it's just challenging to do so, or maybe the language doesn't even allow them to express their underlying value.
00:48:47.244 - 00:49:57.148, Speaker A: So we do have now some incremental version towards that where you do not have to be paste. So you don't need this multiplicative pacing to go down, as far as I know. I don't think we offer probabilistic pacing, but it at least allows advertisers to sort of do this on their own. Whereas before, if they wanted to do this, set an extremely high budget so that you essentially are not going to have your bid discounted, and sort of you're sort of jumping through hoops to making advertisers jump through hoops to do what they want to do. So as much as we can remove that from the system and making, making things easier for advertisers, the better off I think we'll be. So that's sort of this summary of what we're doing on the machine learning side of predicting these different events, and then how we're actually using that to decide what we're going to show to what these allocations and payments are going to be. I'm happy to chat with anyone about this.
00:49:57.148 - 00:50:23.264, Speaker A: I think there's a lot of interesting problems even just understanding that last point about what is this system currently not capturing that advertisers care about and how can we identify this from the data? And second, is just actually implementing some of these features and understanding what is not being captured right now. So thank you very much.
00:50:29.324 - 00:50:40.824, Speaker B: So maybe this is what you meant about being able to express your preferences, or maybe I missed it, but when you said that the BCG is not incentive compatible and you're looking into what it is incentive compatible, what did you need to be doing?
00:50:42.524 - 00:51:49.604, Speaker A: Yeah, so I did not mean that we are considering some different mechanism, but rather it's more an empirical question. Right. So there's a number of assumptions that would need to hold for incentive compatibility actually to hold. For instance, we are actually predicting these different click through rates and things like that correctly would be one of them. And so, yeah, so it's more about understanding in the current system, even if you said that the underlying values are expressible, like there is some single value for a click and some value for a like, and we can have these things separable. Is it actually, you know, even in that case, can an advertiser, do they have incentives to be, you know, changing their bids? So it's more an empirical question rather than coming up with some new mechanism. Because really the solution, I think, is just providing the expressiveness that would allow the advertiser to express their values.
00:51:49.604 - 00:51:55.984, Speaker A: And once you have that, then you can just run VCG. Right. So that's really, I think, what the challenge is.
00:51:56.644 - 00:52:18.004, Speaker C: So when you talk about probabilistic smoothing, I believe that's for one end, what happens when you perform that multiple ends for one auctions, would they have some kind of weird interactions where you just one time remove Os because you think, end of the day?
00:52:19.224 - 00:52:45.314, Speaker A: Yeah, so I suppose that is an issue. So I should say that we're not doing probabilistic pacing, just to be clear. We're doing the multiplicative version, but, yeah, so I suppose that. So maybe, I don't know if anyone could who's doing probabilistic pacing wants to comment on this, but I'm guessing that's just an issue that comes up if you're treating these as just independence probabilities, that maybe sometimes you get that draw.
00:52:45.974 - 00:52:57.674, Speaker C: So the previous speaker spoke about something like this, the equilibrium in this throttling. What was the question?
00:52:59.634 - 00:53:31.436, Speaker A: So the question was, so we're doing this multiplicative pacing, but the question was, with probabilistic pacing, the concern was that sometimes maybe you get, say there's two ads that have a probability less than one, and sometimes it just so happens that neither of these ads ends up being eligible just by the unfortunate luck of the draw. And so I think maybe you were asking if it is actually these independent draws, or if maybe there's something joint that is done that's trying to avoid that. Yeah.
00:53:31.460 - 00:53:38.624, Speaker C: So I just wonder if you guys have any technology that you looked at the equilibriums that such actions might.
00:53:40.284 - 00:53:50.524, Speaker A: Gotcha. Yeah. So the answer to that is just, we're not doing probabilistic pacing, so maybe we'll have to think about that if we, you know, because advertisers potentially do value this, I think.
00:53:50.564 - 00:53:53.304, Speaker C: But even in bid lowering, you'll have this equilibrium thing.
00:53:54.624 - 00:54:15.744, Speaker A: But at least you're not having this situation where nobody is participating. Right. It's just people are lowering their bids. All right, thank you very much.
