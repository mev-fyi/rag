00:00:00.400 - 00:00:42.824, Speaker A: All right, so, yeah, so I guess I'll give the second iteration of this. And unfortunately, it's not nearly as simple as the last algorithm. So that'll be like an open problem to try and make this simpler. So this talk is going to be about solving the closest vector problem exactly in two to the n time. And so Noah already did lattice definitions. So, you know, here's our lattice. I'll just use this notation to denote the lattice generated by basis B, and this notation for the length of a basis, which is just the length of the largest vector in the basis.
00:00:42.824 - 00:01:39.674, Speaker A: So the problem that we're interested in solving is the closest vector problem in this talk, where the difference is we're going to have a target somewhere in space, and our goal will be to compute the closest lattice vector to the target. And this problem, along with SVP, certainly has quite a few applications. And in terms of hardness, the closest vector problem there is approximation preserving reduction from alpha SVP to alpha CVP. So CVP is sort of harder. And in fact, in some sense, it's like the hardest lattice problem among many of the classical lattice problems. So they more or less all reduce to CVP. And you can see that how the hardness transitions as the approximation factor grows here.
00:01:39.674 - 00:02:59.084, Speaker A: And essentially the only thing we know how to do in polynomial time is slightly better than exponential approximations. So what is our main result? We're essentially going to be able to match at least the most significant term of the last algorithm for solving the exact closest vector problem. And there's been also, I mean, the history of SVP and CVP are really joined at the hip. So all of these methods were applied essentially to both problems, though for the randomized sieving methods, I think so somehow the results aren't quite nearly as good as they are for SVP, because you can't get exact solutions, and the dependence is pretty bad on the approximation factor. Okay, so let me tell you the overview. So we're going to talk about how approximate CVP is going to relate to sampling from the discrete gaussian, but over shifts of the lattice. So no, was talking about the discrete Gaussian over sort of the central shift of the lattice in the previous talk.
00:02:59.084 - 00:04:09.364, Speaker A: And we'll discuss briefly what you can expect in terms of what you need to sample. At what parameter do you need to sample to sort of get a corresponding good approximation factor. Then we'll talk about the shifted version of our DGS sampler, which in the mechanics will be essentially identical to the previous sampler. But the analysis of the loss factor is much, much more technical, or, well, it's more technical. And then this, which is really a completely new component, is going to be related to the fact that we're not going to be able to solve the problem exactly by just sampling and waiting to hit a closest vector. So in the previous talk, if you had the right parameter, you would just magically hit a shortest lattice vector with some, you know, non, well, reasonable enough probability. And here we're not going to be able to do that, and we're going to have to essentially slowly learn the coordinates of the closest vector and cluster the approximate closest vectors and use recursion.
00:04:09.364 - 00:05:02.214, Speaker A: And yeah, it gets a little bit messy. So the shifted discrete Gaussian is exactly the same as the non shifted discrete Gaussian in terms of the definitions, except that we have the extra shift, so it's supported on a shift of the lattice, as Noah already showed in previous pictures. As the parameter that you use goes down, we get a more concentrated, um, gaussian. So hopefully that's useful from the perspective of getting approximations. Um, and, you know, just to notice that if you want vectors that are, are close to t or closest to t, that just corresponds to the shortest vectors in the shift. Okay. So it makes sense to try and, and sample short vectors from, from this shifted lattice.
00:05:02.214 - 00:06:32.738, Speaker A: Um, and the main question that, you know, we want to ask is, um, can we hit, uh, closest vectors if we, uh, sample from the discrete Gaussian for small enough parameters? And so here you can already see that maybe there's kind of a small issue. Uh, and, uh, the small issue is the, is the following, which is that, uh, essentially, unlike SVP, where you have the, the kissing number bounds that sort of tell you, uh, how many closest vectors you can have, shortest vectors you can have, but also how many sort of approximately shortest vectors you can have. That doesn't happen here. And the reason is because sort of, you can have a target that's in space, whose distance is sort of proportional to the kind of sparse parts of the lattice. So the distance is really controlled by this large gap. But all of these vectors of which, if I make this x axis smaller, I can have arbitrarily many of them will all be very good approximate closest vectors. So the issue that we're going to have to deal with is that unless we sampled from really, really tiny parameters, we don't really expect to be able to exactly hit this guy or this guy, but we'll be somewhere in here.
00:06:32.738 - 00:07:35.520, Speaker A: Okay. On the other hand, okay, so what can we expect so this is something that we prove using some standard tail bounds for the discrete Gaussian, which is when we look at this shifted discrete Gaussian, and we have it at the following parameter, how big will its squared norm be? And what we can show is that essentially it will be as small as it could possibly be. So this is the squared distance to the target plus some sort of variance term, which is kind of intrinsic to the Gaussian having essentially this type of squared norm at that parameter. So the continuous Gaussian would. Okay, this is off by a factor n would roughly have this as the expected squared norm. So you'll get something here. It's kind of interesting.
00:07:35.520 - 00:08:48.024, Speaker A: It's some sort of squared additive approximation, but it's quite good, and it more or less only depends on the parameter s and dimension. And this can be turned into like a real approximation factor. So if d is the distance, you'll get sort of this approximation ratio, which basically means that as long as s is smaller than something like epsilon times d, you'll get whatever kind of, I mean, you'll get a good enough approximation, but we won't really be like focused too much on the details of this because we're going to be able to solve the problem. Exactly. Anyway, um, regardless, now we know that, you know, if we can sample the discrete Gaussian from low enough parameters, then we can get pretty good approximations. And, you know, that'll just depend on how s relates to the distance to the target. So now let me define a short basis, or a kind of a canonical type of short basis that is used in lattice algorithms all over the place.
00:08:48.024 - 00:09:50.094, Speaker A: And we'll need this for initialization purposes as well as for a recursive algorithm. So for basis B, we're going to define the kind of gram Schmidt projections. So the ith projection will project orthogonal to the first I minus one vectors. And then from there you can define the gram Schmidt orthogonalization, which just takes every vector and projects it orthogonal to the previous ones. And from here, you can now define what's known as a hermicorchen zolotorov, or HKz basis, which is essentially a very greedy type of basis where you're trying to build a basis which has the shortest vectors possible. And how do you build it? You essentially first pick the shortest vector of the lattice. Then you look at the lattice projected orthogonal to the first vector, and you pick the shortest vector in that and put that as part of your basis.
00:09:50.094 - 00:10:59.002, Speaker A: You have to lift it. But it doesn't really matter for us how you do that and you continue successively. And this is computable with n calls to a shortest vector oracle, which we know how to do from the previous talk. So this will be one of the things that we'll use. And we start with, um, so what are the guarantees for our sampler? Um, so here things will be slightly more tricky, um, than in the previous talk, because we won't actually be able to sample from arbitrary parameters, um, or at least not with the basic algorithm. Um, but what will we be able to do? So if we have a parameter that's uh, bigger than this slightly sub exponential factor times the length of the largest gram Schmidt vector in your basis, then we can generate samples from the discrete Gaussian, the shifted one that we want. The number of samples will be slightly more complicated.
00:10:59.002 - 00:12:14.782, Speaker A: So before we just had sort of two to the n over two, roughly that we could guarantee at each stage. If you ignore sort of extra kind of factors based on us trying to get small statistical distance, and this number is the inverse probability of hitting the heaviest bucket of the shifted lattice mod two times itself, the same buckets that we had in the previous talk. And all we'll be able to guarantee is that we essentially can produce enough samples to hit the heaviest bucket. And if you run this a few times, so even, you know, roughly, we will be running it sub exponentially many times, you'll be able to hit all the nearly heaviest buckets. And this is where you sort of expect the closest vector to live, or in one of the heavy buckets, because the closest vector to t will have the maximum kind of energy in some sense. I mean, the gaussian mass of the closest vector is going to be higher than any other single vector. But obviously in a bucket there are many, many vectors.
00:12:14.782 - 00:13:05.004, Speaker A: So it's not exactly like this. And we can do this in two to the n time. Okay, so first thing to note is that if the distance to the target is sort of a little bit bigger than the basis divided by some polynomial, the length of the basis divided by some polynomial, then just one sample from this distribution actually gives you a ridiculously good approximation. And this is not that hard to achieve. You need to do some pre processing, restrict to the appropriate sub lattice to be able to achieve this. But you know, this obviously looks amazingly good. But even with this, it'll be quite a fair amount of work to turn this into an exact algorithm.
00:13:05.004 - 00:14:32.774, Speaker A: And more importantly, I mean, not worrying about the details of how you get the approximation. This sampler on its own will suffice to get the exact closest vector. Okay, so now let me talk about, you know, some of the technical Details related to our combiner in the same way as we had in the, in the previous talk. So here, basically what Noah showed in the previous talk holds in full generality, which is that if you take two Gaussians on different shifts of the, of the lattice, and you look at sort of conditioning the average to land inside also the lattice, but now with the appropriate shift, which is the average of these two shifts, then you get exactly the distribution that you want. That is, this probability is proportional to the gaussian mass, or the gaussian weight with the parameter divided by square root two in exactly the same way as we wanted and had before. And the only extra thing that I wanted to say is, okay, we still have this similar identity. This is the proof that Noah showed you by rotating this two n dimensional lattice.
00:14:32.774 - 00:15:25.812, Speaker A: And this identity is actually going to allow us to prove some really amazing inequalities, which none of which we really understand. But essentially there were many times where we got stuck. And then Duvesh, who's one of the main authors on this paper, figured out how to use these identities to get some other magical inequality to come out, which made everything work out. So this thing is quite powerful. Ok, how do we do the combining? So, it's very similar as we had before. So we have our initialization. So we need to kind of get enough samples to start with and here to do the initialization in the same way as Noah described before.
00:15:25.812 - 00:16:19.774, Speaker A: But here we'll start with a very strong initialization. We'll start with our HKZ basis. And then we'll use standard methods such as those due to Genshin, Piker and Vakum tenath, and to sample large parameters. But large is already going to be sort of the maximum parameter, you know, this size of the gram Schmidt's of the largest gram Schmidt of the basis. Okay? And then the meta procedure as before, is going to be sampling from this kind of squared collision probability. And then, you know, picking unused samples that lie inside that bucket, averaging them and taking them out of the list and repeating. So this is essentially the same procedure that Noah described.
00:16:19.774 - 00:17:47.381, Speaker A: And now the only difference is going to be at least the techniques that we need to use to bound the loss factor. So the loss factor is going to be, or, you know, is going to essentially determine how many samples we can get out of this in the next step. And as before, you know, the loss factor should essentially be the proportionality factor, which, you know, another way to see, it compared to what Noah wrote, is that this l should be chosen so that you don't exhaust the sort of supply of samples you have, at least on expectation. And again, this is going to be controlled by the heaviest coset. So if you look at the inequality that's going to be the hardest to satisfy, it's going to be the one that has the largest gaussian mass. And you get from that, once you plug in the heaviest weight, Cosette, the following funky looking loss factor and the magical thing that we can prove, so this will be quite different than the previous analysis, is that the product of the loss factors in this case is going to be this, two to the minus n. So before we had two to the minus n over two.
00:17:47.381 - 00:18:55.474, Speaker A: Here I'm adding a factor based on the fact that we, at least for every sample, we kill two samples in the previous stage because we're averaging. So, I mean, no, I didn't write that, but this is just to let you know that that's one of the fundamental reasons that we can't do this, like an infinite number of times. And we get exactly this inverse weight of the heaviest bucket. So, to be able to go k steps, given this loss factor boundaries, we need to start with at least two to the k vectors, plus enough just to guarantee that we can average k times. Okay, and I'm not going to prove this, but it turns out that, so this telescoping product that we had in the previous setting is not what we have here. And mainly the reason is because the heaviest bucket at the case stage changes. So at every stage, we get different heaviest buckets, potentially.
00:18:55.474 - 00:19:32.264, Speaker A: And so you need a way to relate the weight of the heaviest bucket in a previous stage to the weight of the heaviest bucket in the next stage. And this is sort of what this inequality does. And it's quite simple to prove, but I won't go through the proof. It does use our magical identity. And more or less, that's, that's it, with one very simple inequality. So that if you have this inequality, you can prove the bound on the loss factor if you fiddle around with it. But let's move now to how we actually try and solve the exact closest vector problem.
00:19:32.264 - 00:20:25.698, Speaker A: So, from the sampler that works at really small parameters, we can get very good approximate solutions, but we can't get exact solutions. But still, however, there's hope for how we can figure out where the closest vector must be. Because you can already see in this example that all of the vectors that we're going to get all of the approximate closest vectors that we're going to get lie on two subspaces. So somehow you do know that the closest vector is either here or here. You don't quite know where, but it's here or it's either there or there. A very powerful tool to kind of start to understand how this is happening. This kind of clustering is again by looking at the parities of vectors, but now in a completely different context.
00:20:25.698 - 00:21:56.044, Speaker A: So we're going to look at two sort of approximate closest vectors, so two vectors x and y that are at this distance from t, so a little bit more than the exact distance. And then we're going to assume that they have the same parity. And then it turns out, and this is more or less has a, well, it doesn't have a proof by picture, but at least the intuition is clear that these two vectors x and y, if they're very, very close to t and they have the same parity, then it means that the average, since the average is in the lattice, has to have a pretty large length, because this length of the average is going to be at least, the length of this is going to be at least this distance. So if you have two vectors on the sphere and you average them and the norm doesn't decrease by a lot, then they have to be very, very close to each other on the sphere. Um, and perhaps the somewhat surprising thing, though, this is really simple algebra, is that you can prove that the distance between them depends only on this kind of extra r squared factor. Okay? So the fact that this is independent from the distance to the target is, is quite, is quite important and enables a lot of what we can show. Uh, so I won't go, I mean, the proof of this is just algebra, but I won't go through it.
00:21:56.044 - 00:22:52.464, Speaker A: And now already directly from this, I mean, this you could see perhaps, I mean, it's the same picture, it's even easier. The parity is quite important. So using parity, a parity argument, which I'll do here, you can see that for any lattice and any target, you can only have most two to the n closest vectors to any target. And it's trivially tight. I mean, just take the all one halves vector in the integer lattice, and all the corners of the cube are going to be closest vectors. But by the previous lemma, the easy way to see this is that if you take any two closest vectors and if you assume that they have the same parity, the previous lemma will tell you that they have distance zero from each other, and so they are the same. And since there are at most two to the n distinct parities, you have a two to the n bound.
00:22:52.464 - 00:24:05.318, Speaker A: Now, slightly less trivial, but also quite simple, is we can use this to do dimension reduction. So what we can show is the following. So let's start with our HKz basis that we computed at the beginning. Let's assume that we're in the same setting, so we have x and y their approximate closest vectors, and they have the same parity. Then if you can show that so the distance is extra r squared distance that they are from the minimal distance to t is smaller than the length of one of your basis vectors in your HKz basis, then if you express x and y, if you express their coordinates with respect to the basis, this is chosen so that their last k coordinates with respect to the basis are identical. So they have to have the same last k coordinates. So this sort of means that at some point, if you look at approximate closest vectors of the same parity, the last k coordinates with respect to an HKz basis is frozen.
00:24:05.318 - 00:25:09.362, Speaker A: So all the interesting sort of action that happens afterwards occurs on the other coordinates, which is how we can start to think about doing recursion. And this also has a very simple proof. So here I'll go through with it. So basically, I just need to show that the last k coordinates with respect to the basis are equal, which is equivalent to showing that if I sort of project onto the last k basis vectors that x minus y is zero. And if this isn't the case, then we know that because they have the same parity, this is a lattice vector. So it's in particular, after I project it in the projection of the lattice with respect to this projection, and it's non zero, because I'm assuming it's non zero, and this is the whole proof. So the projection norm is smaller than the norm of the vector.
00:25:09.362 - 00:26:03.180, Speaker A: Uh, this is smaller than r by the previous lemma. Um, and by our assumption, r is smaller than this, which is the length of the shortest vector in this lattice. So you have a, a vector whose length is small, a non zero vector in a lattice whose length is smaller than the length of the shortest non zero vector. So that's a contradiction. Um, so that, that basically shows that you, you have this kind of freezing phenomenon. And so that won't actually be enough to do recursion in a useful way, but it'll be close to what we need, really what we need will essentially be the following. So we're going to show, or we show in the paper that if you start with an hkz basis, then just staring at the lengths of the basis vectors.
00:26:03.180 - 00:27:49.590, Speaker A: So I'll be looking at the geometry, in essence of the basis vectors, that I can choose a k which will correspond to the last k vectors in the basis, so that no matter what the target is, I'll be able to ensure that with our sampler that we had in the middle of the talk, that I'll be able to learn the last k coordinates of an exact closest vector. Um, and this, so, so this is something that'll be a property just of the basis that I won't have to know the target. I can just look at the basis, look at how the lengths of the, the length of the basis vectors changes, and I'll be able to pinpoint a spot where I can say okay, after this spot, I'll be able to learn all of the coefficients of the closest vector. Um, so more precisely, the way this is really going to happen is that we'll be able to show that if we look at any close enough vector to t for the k that we've chosen the actual exact coordinates, or the, yeah, the exact coordinates with respect to the last k vectors will be essentially completely determined by the parity of the coordinates. So since the, if these are determined by their parity, then I can only have at most sort of two to the k different options of what I see here. Now this won't be exactly true, but it'll be true enough to make the algorithm work. So the idea from this is basically that we'll be able to group the approximate closest vectors by what their last k coefficients are with respect to your HKZ basis.
00:27:49.590 - 00:28:55.024, Speaker A: These will index, if what I said previously is correct, two to the k shifts of the sub lattice span by the first n minus k vectors. And that's where in one of those shifts you have to have the exact closest vector. So you get two to the k shifts, which gives you two to the k sub problems, which are of dimension n minus k. So that should hopefully work. And this is essentially how that works in the picture that I've been showing before, that if you look at the approximate closest vectors, their coefficient, their parity of, the parity of their coordinate with respect to the last basis vector essentially determines where the vector is. So here the parity is even and we see that it has to be anything that's approximate closest vector. And even on the second basis vector has to be on this line and the odds have to be on this line.
00:28:55.024 - 00:29:47.600, Speaker A: Okay, so the final like high level algorithm is, you know, we compute our HKZ basis, we sample, we figure out how many coordinates we expect to learn of the exact closest vector. We sample a whole bunch of approximate closest vectors using the sampling algorithm. From the middle of the talk, we group them according to their last k coordinates and recurse. This is essentially the whole algorithm. So stated this way, I think it looks simple enough. Obviously the details are painful, and in terms of the complexity that this will like, the way we more or less analyze this is the following. So the initialization phase where we compute the basis that's two to the end time, each level.
00:29:47.600 - 00:30:36.176, Speaker A: So if we're at a level where the lattice is n dimensional, we take two to the n time, and for the recursion, we recurse on roughly two to the k sub problems of dimension n minus k. So when you work it all out, um, you get exactly two to the n time. And, uh, the key challenges that some of them I alluded to and some of them I didn't. Uh, so most of them here I more or less, uh, alluded to. So, um, you know how you get the dgs samples efficiently, uh, that we more or less talked about. Um, how we show that the last k coefficients are determined by their parity. What I showed you is that if you look at the parity of all n coefficients, then you know what the last k coefficients are if you have an approximate closest vector.
00:30:36.176 - 00:32:00.490, Speaker A: But here we need that the parity of the last k coefficients gives you exactly what the last k coefficients are. So that's a different, a slightly different story, we don't always get exactly two to the k sub problems. If we did always get exactly two to the k subproblems of dimension n minus k, it would be trivial. But essentially we show that we do get exactly two to the k sub problems, as long as k is small and with k is large, we get a little deviation off of this, but somehow we make so much progress on decreasing the dimension that it ends up not mattering. And the last thing I didn't show you, and this is intuitively not so bad, but the proof, I mean, we have no intuition for why the proof works, is that we actually have to show that we eventually hit the last k coefficients of an exact closest vector. So all of what I've been showing you up until this point is basically convincing you that our algorithm should probably terminate in two to the n time, but not that it will actually find the exact closest vector. And the reason it does is because we'll be able to show that for the right choice of parameter, the discrete Gaussian will actually hit the bucket containing the exact closest vector.
00:32:00.490 - 00:32:45.334, Speaker A: So it'll hit a bucket that has the same parity as the exact closest vector, and that will allow us to find the last k coefficients. And the intuition as to why that should be true is somehow because the closest vector should make the bucket very heavy. But the proof doesn't show you that intuition, unfortunately. So the conclusions, we have the fastest algorithm for CDP. So this two to the end time. What's interesting is that we actually kind of explicitly and sometimes implicitly use ideas from all the previous algorithms. So, I mean, we really use the properties of HKZ bases as in basis reduction algorithms.
00:32:45.334 - 00:33:30.954, Speaker A: We make a sort of massive improvement on the approximation guarantees that sieving algorithms were able to get. And that was very crucial. And then in terms of, with respect to the voronoi based methods, I mean, the high level comparison, that is very perhaps misleading. But I'm curious to understand what it means is that we rely very crucially on all this parity information. And that's exactly how you bound the number of vectors in the Voronoi cell. So I'm not quite sure if there's any real relationship there other than superficial. And the final thing is that the discrete Gaussian has clearly shown itself to be a very powerful tool.
00:33:30.954 - 00:34:29.916, Speaker A: We've seen it used first in pure math to prove all sorts of inequalities about lattices, then in crypto for its statistical properties, and finally as an actual algorithmic tool. So, you know, I expect more, you know, can come from analyzing it. And in terms of the open problems, I mean, is the algorithm in essence optimal? Right. So I somehow feel like it'll be unlikely for us to ever find an algorithm that goes faster than two to the n, because the number of closest vectors can be two to the n. So at least it's more or less output optimal. Um, but can you, can you find some assumption that might justify this? Um, and here I would really like to know if there's a deterministic, or at least, you know, Las Vegas algorithm that doesn't need, uh, randomness. Uh, and this is a picture that's showing essentially the, the Voronoi cell and its neighbors.
00:34:29.916 - 00:35:05.534, Speaker A: Um, and actually I showed, improving some of the work of Macruncio and vulgaris, that once you know, the vectors of the Voronoi cell, you can actually solve CVP and two to the n time, but they're two to the n of these vectors, and it takes two to the n time. To find each of them. So that's four to the n. But I was very surprised that, like, we didn't end up saying anything with respect to the Voronoi cell in this paper. And third, I mean, I think the algorithm is too complicated. And, yeah, I'd really like to see a simpler and cleaner algorithm, so. Thank you.
00:35:05.534 - 00:36:22.854, Speaker A: Yeah. So do you think this kind of approach may also give a single exponential reason for exact CPP in l one or an increasing normal? Well, the issue with that is we really, this clustering property based on parity is, you know, I mean, we're really. Well, but even with that notwithstanding, the problem is, you know, all of our dimension reduction tricks are based on really saying that if two vectors have the same parity and their approximate closest vectors, then they must be very close to each other. And this doesn't hold, you know, when you don't have strict convexity, and, I mean, what we're using is much stronger than strict convexity. I mean, we have this algebraic relation which exactly tells us what the distance between the vectors should be. So, yeah, I mean, maybe if the norm is strictly convex, but even then, it's unclear. Okay.
