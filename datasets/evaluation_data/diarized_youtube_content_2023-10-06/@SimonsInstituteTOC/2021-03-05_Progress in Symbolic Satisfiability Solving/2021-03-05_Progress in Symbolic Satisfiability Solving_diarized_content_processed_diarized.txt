00:00:00.120 - 00:00:30.098, Speaker A: Cardi Moshe is a professor at computational engineering at Rice University, and his interests span almost any area of computer science. He's out of more than 500, co author of more than 500 papers. And for that he received many awards like the IBM Outstanding Innovation Awards, the Godel Prize, the Carnellakis Award, the Code award, and many others. Today he'll be telling us about symbolic satisfiability solving. So please, Moshe.
00:00:30.226 - 00:01:15.332, Speaker B: Thank you, Albert. So it's a bit strange to see symbolic satisfiability solving because you think everything we're doing with SAS solving, of course we're working with symbols, but I'll explain later. Where does it particular terminology, which maybe not be the best one, where does it come from? Well, okay. I need to be able to. Okay, so I want to start with just a reflection of this. To me, an amazing technical development, what I call the SAT revolution, that we are in a field that now we have crossed over 60 years, really you can think of roughly, we started around 1960, maybe the first sad paper was 19, 50, 58 or something like that. And very quickly people realized that this is a difficult problem.
00:01:15.332 - 00:02:05.316, Speaker B: In fact, we can even go to the 19th century and see people realize in 19th century that this is a difficult problem. And the culmination of that intuition of difficulty of Sat was that Cook Levin theorem. Sat is NP complete, which led to the view for many years when I was a graduate student. Sat is intractable, hopeless, don't bother wasting your time. And then in the mid to late nineties, we had suddenly explosion of heuristics started with Karem Sakala and discover of conflict, conflict, conflict analysis and closed learning. And I remember when I first heard Karem talk the first time, he talked about the dilemma rule, which I never had this terminology. And I said, who is this guy? He doesn't even know the basic terminology.
00:02:05.316 - 00:02:35.484, Speaker B: But it really started the revolution. And by already more than ten years ago, we have reached the milestone of solving industrial problems with more than a million variables. And today we are actually thinking of NP easy. Now, I'm one of the people who go around with sad solving. What can we do with sad solver? Because suddenly NP is easy. So this is an amazing, I think, technical accomplishment that I don't think is enough appreciated in computer science. Partly I think we didn't do a good job marketing it.
00:02:35.484 - 00:03:04.682, Speaker B: But I want to be a little bit also of the party pooper and criticize, kind of look at ourselves and criticize it. And my observation is that I think we basically have one algorithm that really scaled to large industrial problems. And this is CDCL. Of course, CDCL is not a single algorithm, it's a family. And there is huge investigation of different approaches. But at the heart of it, there is a single algorithm and is based on what we know now is the weakest proof system. And I'll come back to proof system in a few minutes.
00:03:04.682 - 00:03:38.854, Speaker B: Resolution. But wolfs and debt is, because of this, is the risk of success, the cost of success? I mean, you hear talk sometimes from business manager. They tell you that the downfall of any company is being too successful, because they just become so attached to the success model that they stop innovating. And what we've created in the field is a bit of a monoculture. We are ever, because CDCL is the winning card. Right now, everybody is working on CDCL, practically. Not everybody, but the vast majority.
00:03:38.854 - 00:04:41.946, Speaker B: To me, this is a risky practice, not diversifying your portfolio, so to speak. And I want to talk today about another approach to SaaS solving and discuss its success and failure and potential, further future potential. So let's step back and take a look at the proof complexity aspect of it. So we have these fundamental questions in complexity theory, P versus NP and NP versus co NP. Of course, P versus NP gets most of the attention, but NP versus Co NP is important in our context because of the connection to very, very classical logic. So one of the, I think, greatest accomplishment of the 20th century between logic and computer science is realizing that what is a proof? A proof is an object can be checked algorithmically. And it led the notion of a polynomial bounded proof system, a proof that is polynomially bounded in length and can be checked in polynomial time.
00:04:41.946 - 00:05:44.822, Speaker B: And this is NP, if we have such a polynomial bounded propositional proof system for propositional logic. And that led to an area has been going on since then of studying the quantitative properties of positional proof system, and two important results, one negative one, one positive, and the negative one, that the pigeonhole principle require exponentially long resolution proofs. And I have to tell you a little funny story about the pigeonhole principle. Some years ago, there was a conference in Israel, and we had an excursion, and it took us to a place where there are underground caves. And one of the cave is called the pigeonhole cave, because the people dug in, the rock was soft, and people dug holes, and they raised pigeons there. And so as soon as we walk into the pigeonhole cave, people start giggling, pigeons or pigeonhole. And the tour guide said, what's so funny? And we said, you know, we have a very important principle in computer science, pigeonhole principle he said, oh, please educate me.
00:05:44.822 - 00:06:29.624, Speaker B: And immediately I realized that I cannot say suppose you have n pigeons, because this statement that makes perfect sense to us, makes no sense to a normal person. So I said, I said, suppose you take ten pigeons and you put them in nine holes, then one hole will have at least two pigeons. And he looked at us genuinely patented, and says, and this is a deep principle for you. And you can see that he was thinking, and I have to give tour guides to these idiots, and they get paid for being professor, and I'm here giving them tours. It was a very funny moment. So hack and show that PHP require exponentially long resolution proofs. But if you go to extended resolution, then we can do it polynomially long proof.
00:06:29.624 - 00:07:38.274, Speaker B: So there is a big difference in the power of proof systems. And in fact, one central resolution is the weakest proof system. And distilled, this is the basis of all of our SAT solvers. And the connection to such solvers goes back to, to a paper by Zvi Galil in 1977. And he made the observation that if you have a DPL refutation, a DPL referred proof that fails, then you can just go back and bottom up and transform it into a tree resolution of the same size of the search tree. And I think this is very important result, because it ties search to resolution, to proofs on one hand. And it kind of gave us later, we kind of understood the power of CDCL, when Bim, couch and Sabral showed that if you look at CDCL refutation with closed learning and restar, these are the two only elements that you need CDCL today, we think of it as a family, as an algorithm with more components, but just these two components, cloud learning restarts.
00:07:38.274 - 00:08:19.794, Speaker B: Now you transform into a resolution refutation of the same size, and vice versa, without not a three resolution. Now we know there is exponential gap between three resolution and resolution. And therefore we understand today that CDCL is exponentially more powerful than DPL. And this explained, we can think of it as one explanation of the power of CDCL. And it tells us also that the study of pove complexity is of not just theoretical interest, but of practical interest, and it's relevant to such solving. And I don't know that we've done enough to make this connection to how to harness the power of proof system to start solving. And this partly it's one underlying theme of this talk.
00:08:19.794 - 00:09:08.614, Speaker B: So now I want to kind of jump to 50,000ft and talk about a more general formulation of, rather than just look at sat, I want to look at the broader class of problems. And this is consent satisfaction problem, CSP. And so, CSP is just a very, very, it's a paradigmatic problem that you find all over computer science. You have a set of variables and you have a set of values, and you have a set of constraints. And what is a constraint? A constraint is a tuple of variables and a relation over the, over the domain of the same rit as the tuple. And the intuition of the constraint is that we're looking for, what are we looking for? We're looking for. So a solution is an assignment of values to variables, evaluation.
00:09:08.614 - 00:10:13.420, Speaker B: And evaluation has to be such that you look at each constraint, the value assigned to the tuple x should be in the set of tuples R. So instead of if, instead of looking at arbitrary assignment, we are for each, for each, for portions of the full tuples of variables. Okay, we are looking for restrictions on the, on this allowed assignments. And the decision problem here is, does this combination of values, domain and constraint, does it have a solution? Is there global assignment? This is global consistency that satisfy all the local consistency requirement. And sat is as an example of CSP. But even clean, to me, the most cleanest example of CSP is three colorability. So, we're given a graph, nodes and edges, undirected graph, and we want to know, is it colorable with three nodes? And so here the variables are the nodes, and the values are just red, green and blue rgb.
00:10:13.420 - 00:10:44.314, Speaker B: And what are the constraints for each edge? We form a constraint for each edge uv. We say that the pair of color assigned to u and v must be in the relation that contains six pairs of all distinct colors, rg, rb and so on and so forth. Okay, so this is, we have variables. These are the nodes, values are the colors and the constraints. Each edge is a constraint. This is non monochromatic. So this is the CSP.
00:10:44.314 - 00:11:40.906, Speaker B: Now, in 2004, Albert Fouque and I introduced the concept of CSP refutation. And what is this or CSP proof? I will kind of not distinguish much between them, because it's a refutation, it's a proof by refutation, and it's a proof of unsatisfiability. And what is a proof? Remember a proof, you have axioms, and here the axioms are the constraints that are already given to you in a set of constraints. So each new element in the refutation will be a constraint, and you start from the constraints in C. And there are three inference rules. One is that if you have two constraints, xr and y s, here, r and s relations, then you can take the union of x and y and get a bigger tuple, and you take the relational join. Now you're thinking of these as relations.
00:11:40.906 - 00:12:20.844, Speaker B: The variables are the attributes. You have attributes and values. So this is really relation like in the relational model. And you take the join of the two relation. And it's very easy exercise on the graduate level exercise to show that indeed, if the two constraints x are and y hold, then the join of these constraints must also hold. And the second inference rule again is a relational database operation, which is projection, that if you have a consent xr, you can decide to project out one of the variable. So you delete the variables from the set of Tuples, and you delete the column from the relation.
00:12:20.844 - 00:13:16.034, Speaker B: So this relational projection, I denoted by the bracket notation, you take the relation and you delete one column. So this is a weaker constraint. So now we're kind of weakening the constraints. And another way to weaken the constraint is just add more tuple to the constraints. This is weakening you just take the weakening and weakening we need, mostly for technical reason that I want, I won't get into here if by a sequence of such operations you end up with an empty relation. In fact, the join is itself is enough, because clearly if you take all the constraints and you compute the join of all the constraints, you'll get the tuple of all variables and you'll get all the allowed assignments. And if this is empty, then there are no possible in line and say this is, there is a tight connection between computing joins and multi relation joints, or multi joints and solving CSP.
00:13:16.034 - 00:14:00.534, Speaker B: This connection is established many, many years ago. So in particular we follow that VDC has a solution if and only if it has a CSP refutation. So this is a very easy completeness proof of this notion of this notion of deductive reasoning about constraints. Now this is also connected to a practice, something which is in the practice of consent solving, one practice in consent solving consent consent propagation. For example, in sad solving, we are very familiar with unit propagation, which is a very specific form of constraint propagation. Okay. More generally, both operation out of the three inference rule that we saw, we saw three inference rule, joint projection and weakening.
00:14:00.534 - 00:14:57.230, Speaker B: It's easy to see that part of the thing that people have been considering in the CSP literature are the joints. They are talking really about how to combine constraints and projection. Usually it is done not as Aberdeen projection, but take two joint, two relation, join them and then project them back on one of the set of variables. So you take x union, y you join rns and then you project back on x to get a tighter relation. This is essentially relational terminology, semi join. And if we want to understand also the connection to to start solving, then resolution is precisely. If you have two clauses, then resolution means you take the two relations, the two each clause defined relation, which is a set of satisfying assignment or partial assignments.
00:14:57.230 - 00:15:37.818, Speaker B: If you take the join and you project out the variable on which you're resolving, this is exactly resolution. So if you look at the content of resolution, we do it symbolically, not by set of assignment, but semantically resolution is essentially variable elimination. You take two constraints, you join them, you project away the variable you want to eliminate. Now another notion I want to introduce here is that of tree widths. So thread the composition, and I'm assuming that people are familiar. So this will be a quick, I won't even put the picture. Threaded composition is trying to massage a general graph or general structure into a tree.
00:15:37.818 - 00:17:01.832, Speaker B: So you have a tree and you label every node by a set of, you label every node of the tree by set of elements of the structure. And it has to be such two conditions that every tuple in the structure has to live in one of the nodes of the tree, and the second one that if you look at all the three nodes that contain one element, they have to be connected, so have to form a subtree. And the tree which looks at the maximum label size over all three, decomposition and then the minus one, have been driven whole generation of computer scientists into craziness. So why is three widths important? Because it's connected to the width of, what is the width of refutation? If you think of representing constraints explicitly, you can see that if we have lots of variables, then the size of the constraints may grow exponentially in the number of variables. So it's in our interest to limit the width, the number of variables of each derived constraints. So the three widths, first of all, we can look at the CSP instance, and if we just look at all the tuples and we think of them as a structure, we have different relation names and we can think of it as a structure. So we can think we can associate a tree width with each CSP instance.
00:17:01.832 - 00:18:09.430, Speaker B: So here we are looking not at the size of the constraints, but at a collection of tuples on the left side of each constraint. And that would give us, we can talk about the width and the width of the refutation is the maximality of the consent in the refutation. And the result that falls out from a couple of papers that Albert Fouque and Victor Dalmar wrote in the early 2000 was that if we have a CSP instance of bounded tree with the tree within k, then we can consider, it's enough to consider refutations of a wheat scale. So the tree small tree, which is a good of any of a CSP instance, is a good feature. It helps us to solve it more efficiently. We know, in fact, we know, not just looking at refutation, we know that if we have, if we are also given the treated composition of bounded widths, then we can solve the problem, the CSP in polynomial time, polynomial in the griqui. Now let's go back to Boolean constraint, which is what we want to focus here.
00:18:09.430 - 00:18:39.894, Speaker B: Start solving. So now we have to ask ourselves, how do we represent constraints so we can do it explicitly by set of tuples. Well, as we know, this is going to grow exponentially in the weeds. So that's maybe an expensive way to do this. In typical solving, we use clauses, just, we use clauses to represent constraints, and we'll talk about the plus and minuses of that. We can use linear inequality. If you do cutting planes, you can use linear inequality to do it.
00:18:39.894 - 00:19:23.394, Speaker B: Ideally, what we would like to have is that if we go back to the notion of CSP proofs, we want to have polynomial closure under joint projection and weakening, and for example clause are not generally closed under join. I can take the conjunction of two clauses and represent them as a clause. I can only do it if I combine it with projection. So clauses are only clause under the combination of joint plus projection. But if I am not ready to do immediately the projection, then clauses are not a good formulation. And that's why we came up with the idea of using bdds. Reduce all the binary decision diagram.
00:19:23.394 - 00:19:51.826, Speaker B: So again, I'm assuming familiarity with BDD. So I will just go very quickly. So, a binary decision diagram, and I will not always say ro Bdd, just BDD. You start with a decision tree for a boolean function, and then you minimize it, and you turn into decision diagram. And the idea is that at the leaves you have always zeros and one. And so you start merging. First of all, you merge all the zeros into one, and you merge all the ones into one.
00:19:51.826 - 00:20:38.066, Speaker B: And then you go kind of bottom up, and whenever you have two nodes with the same subtree, you merge them. There's no reason to do that. And in fact, if you have a variable and both directions lead to the same node, you skip it as well. So this is your minimizing the tree, and this goes back to long. This goes back to this to the seventies, but it was popularized by Randy Brand in the eighties. And the key properties that were shown is, one, is that if you fix variable orderings and you get canonicity, and in particular, because you get canonicity, then equivalents mean identity, and all the Boolean operation can be done polynomially, in fact, quadratically. And this says, this tells us builder refutation constitute a provisional proof system.
00:20:38.066 - 00:21:29.874, Speaker B: In the notion of Kukreko in particular, if you're giving me such a refutation, I can check its correctness in polynomial time. That's the important feature here. I want to be able to check it in polynomial time. If you allow, of course, more powerful checking, then I can just say proof system, all pathologies are axioms and we are done, but we want to be able to check proof efficiently. And so BDD refutation give us a propositional proof system. How do they compare to resolution? So, the first observation by Albert Collaitis and myself was that you polynomial simulate resolution, because resolution, as already pointed out, is joint plus projection, and we can do them both in polynomial time here and BDD support it. And to go from closer to BDD is very easy.
00:21:29.874 - 00:22:07.954, Speaker B: A clause, essentially eliminate one assignment. So you get, in fact, a BDD of linear size, the BDD that basically the forbidden assignment leads to zero. Every, all other assignment leads to one. The more interesting observation is that pigeonhole principle has polynomial aside BDD refutations, because the counting, what you need to do, essentially, there is a counting argument at the heart of the pigeonhole principle. And clauses are not good at doing counting. They cannot express counting, but bdds are very good at doing counting. Okay, so conclusion from this is that BDD refutation are exponentially more powerful than resolution.
00:22:07.954 - 00:23:11.392, Speaker B: Okay, now we already know, we know that extended Frege, for example, have polynomial size reputation for pigeonhole principle. But there is a connection in some sense, between BDD refutation and extend and Frege, because bdds have these internal nodes, and you can think of the internal nodes as the additional variables that you can add in extended Fragger proof. But it's a fragment of extended Frege where the new variables, you don't have to be so creative about what variables to add. They fall out, out of the operations of the BDD operations. But bdds are not only more powerful in resolution, they also express other things, like the gaussian calculus, if you're dealing with zero one, for example, with xors, and you want to deal with constraints, and this is actually, there are many applications. I've written many papers where we look at a CNF plus XOR, and there are lots of applications for that. And again, bdds, they, you know, Xor consents, no problem whatsoever.
00:23:11.392 - 00:23:34.626, Speaker B: Gaussian elimination. We can do it with bdds. So, BDD simulates the gaussian calculus, cutting planes. If you have linear inequalities, as long as the coefficients are in unary. Otherwise, we will have. The BDD can grow exponentially. Again, we can, all the operations of the cutting planes, additions, column multiplication, integer division, can be done using bdDs.
00:23:34.626 - 00:24:23.572, Speaker B: So BDDs polynomial simulate cutting planes also. So it's a powerful proof system. So what have we seen so far? We've seen, first of all, a general framework for CSP proof, bounded weeds and BDD proofs. Of course, the question is, is it any good for sad solving? At the end of the day, we want to solve sat here. Okay, can we use it for SAT solving? So, one possibility, which we started, we know it's a complete proof system, so just do exhaustively joins and projection, and see if you can derive the empty constraint will tell you it's unsatisfiable. But then the question is, you know, if we do all possible join and projection, we're going to die. Just like we know that if you do all possible resolutions, you know, we're going to, it's going to explode.
00:24:23.572 - 00:24:56.764, Speaker B: So how do we. We need some discipline in which order to do that? If I had the three decomposition, then it would be easy. I would just follow the three decomposition. I will just follow the three composition bottom up, and I will just join every node of the tree of 3d composition. You join as you go up, you project away variables. In fact, we know if we have three composition, we can do it in polynomial time if it's bounded polynomially where the width is the exponent, but finding an optimal traded composition is NP. Hulk.
00:24:56.764 - 00:25:46.334, Speaker B: So this was my work with my PhD students in the two thousands, Jaekwon pan and Gochan pan. And we were inspired by what was happening in model checking, where people start using BDD based model checking, and that was called symbolic model checking. And they discovered they have to do that. They have to look at quantifier elimination there, when you can do the image operation, and if you need to build the full BDD, it blows up. And so the idea came up of doing what's called early quantification, which is essentially a form of efficient variable elimination. So in general, suppose I have this, I have a formula with n clauses and n variables. And I want now to eliminate, let's say v sub n.
00:25:46.334 - 00:26:26.230, Speaker B: But suppose that v sub n does not appear in the later part of the conjunction. Then I can first compute do the conjunction of c one to ck, then project out vn and then go on. So this is early quantification. And in summary, join is an expensive operation because it grows constraints. So join lazily and project eagerly. This is in some the philosophy of this approach. Now we are facing the other question is, well, but how to order your clauses, the order in which they are given to you, maybe some arbitrary order, maybe may not be the optimal order.
00:26:26.230 - 00:27:02.854, Speaker B: So ideally you'll have an reordering to maximize early quantification. Well, it turns out, of course, finding optimal close ordering is also np hard. It's related to finding optimal three decomposition. So in 2004 and five, pan and I developed a tool called BDDSaT, and we basically heuristic, heuristic form three decomposition heuristic we found in the CSP literature. Heuristic we found in symbolic model checking literature. And this all heuristic about finding. Also you need good variable ordering and good close ordering.
00:27:02.854 - 00:27:46.654, Speaker B: And we build the tool and we compare it to what was then a good, a good solver, which was z shaft. And we started with random formulas. And you see that a load density, a density of one and a half shaft took no time. And bddsat was, this is a log scale. So bddsat was exponential in this, in this case. So that was, there was no good. We said, okay, how about high density formulas? Now, z shaft, we know that the resolution is hard for random formulas, but Schaff was still exponentially faster than BDD sat.
00:27:46.654 - 00:28:12.104, Speaker B: Then we try random miconditional that are also known to be hard for resolution. Again, z shaft was exponential, but it was exponentially faster than then BDDSat. But you know, we looked at a variety of formulas, and one example, we find other positive results. But one example is a classical problem. It's a beautiful problem. It's called the mutilated checkerboard. It's a problem.
00:28:12.104 - 00:29:02.324, Speaker B: You have a checkerboard and you remove the two tiles from the two opposing corners. And now you're asking, can you tile it using dominoes? And there is a beautiful argument, high level argument that says why you can do it has to do, of course, with the colors. But then you can try to use it to solve it using sat solvers. And now symbolic was much, much faster than Zshar. And so this situation that are in some sense incomparable was kind of the summary. So when pan graduated in 2006, and the conclusion was that symbolic scales better on some problems, but in general incomparable. But overall, I would say, you know, we could not really compete well with tools that had the benefit of, at that time, even 40 years of engineering.
00:29:02.324 - 00:29:45.204, Speaker B: So all we can conclude is that, you know, the symbolic approach has some merit and it deserves further study. So what happens? Well, what happens when a tree falls in the forest and nobody hears? Nobody hears it? Was there a noise? That's how I felt about this paper. There was not much, there was not much follow up, some follow up. But overall, I would say is this work has not had. I was hoping that it will trigger a lot of interest in symbolic solving. I can say that it happened to part of my life disappointments. So in the last couple of years, I decided to go back to see what is the potential of symbolic self solving.
00:29:45.204 - 00:30:19.634, Speaker B: Now my interest is in model counting, which comes from applications in AI, specifically in probabilistic inference. Sharpsat is counting the number of satisfying assignments. And this is a harder problem. We know it, it's a Sharpie complete. But it is also very important problem in practice and in particular, especially if you're coming from holistic reasoning. It's not just about just number of assignments, but the sum of the weights of assignments. So the assignments are weighted.
00:30:19.634 - 00:31:09.302, Speaker B: In particular, one popular model is called literal weighting, in which each literal has a weight, positive and negative literal has a weight, and the weight of an assignment is the product of all the literal weights. And then you take the sum of all the weight of the satisfying assignments. Now, because we're trying to reason numerically, we can just use a binary decision diagram, but we use algebraic decision diagram. So the same way that BDS represent boolean functions, adds represent pseudo boolean functions. And again, it's a, it's the same idea of a folded decision tree where the internal load correspond to your branch on decision diagram, on boolean variables. But instead of having just zero one as leaves, you can add arbitrary many leaves and they are labeled with real numbers, I mean floating point. Or in fact there are some, some packages let use infinite precisions numbers.
00:31:09.302 - 00:31:44.376, Speaker B: And again, they have nice properties and you people have used them. For example, in probabilistic model checking, when you want to compute probabilities, people have used adds. They've been successful, very successful for symbolic probabilistic model checking. So we can do weighted model counting with adds. How do we do it? We start with the, with the boolean formula. We compute the bdds we add the weights and we turn this BDD into an add that gives you the weight of each assignment. And now you need to do variable elimination.
00:31:44.376 - 00:32:27.644, Speaker B: All the variables, but you do the quantitative analog of what's called Shannon expansion. Shannon expansion said that to eliminate the variable, you take the disjunction of the assignment zero and assignment one. And here you take the sum, because this is now the sums we're computing sum of weighted assignments. This is now the sum of a of p goes to zero and ap goes to one. Again, the problem is that it's going to be, it's going to blow up. If you try to compute even just the BDD, it will blow up. So my students, Dudek and fun and myself, we went again, we looked at what do we do? And we were inspired by bddsat.
00:32:27.644 - 00:33:01.564, Speaker B: And again, we want to do early quantification. And so adDMC was a add based model counter. That was, we start from the bdds and you had early quantification. What do you see here? The best solver is it was a solver called Defor based on what's called knowledge compilation. I won't get into detail. Turns out that it's very hard to find the best counter. So what became popular in this community is looking what's called vbs, virtual best solver.
00:33:01.564 - 00:33:36.292, Speaker B: So you have a portfolio of solvers, and for each instance, you look what is the best outcome that you get. And one other thing we want to see here is what happened. We have a set of solvers. We're throwing one more solver. How much does it improve the vbs? So if you see here, VBS zero is the green line and VBs one is a blue line where we've added adDMC. And you can see that we have solved about, in this case, about 150 additional formulas. So the portfolio improve.
00:33:36.292 - 00:34:00.516, Speaker B: When you take a portfolio of solver and you add ADMC, the portfolio improved by about 10% of the formula. You can solve before you can solve now. So it's a good addition to the portfolio. And of course, everybody would love to have the best, the winning algorithm. And that's very hard to find these days. But we're looking at the portfolio now. I want to go back, we're just about to wrap up.
00:34:00.516 - 00:34:38.870, Speaker B: I want to go back to 3d compositions. Remember in 2005, we said 3d composition computing is very hard indeed. Optimal 3d composition is hard, but there were huge progress since then in practical 3d composition solving. And the tools are getting better and better, and they're not optimal, but they have another advantage to become any time solver. As you let it run more and more, you get better and better result. So in the past year, again, Jeff Tudek and Wu fan and myself look at what we call now DPMC. And so again, the idea use early quantification.
00:34:38.870 - 00:35:16.394, Speaker B: But now instead of using CSP heuristic, use three decompositions. And it turned out that going from CSP auristic three decomposition gives us a huge advantage. And DMPC actually is a typo. DPMC dynamic programming model counting. It is much faster than ad DMC and it tied for first place in the most recent competition of model counting competition in the weighted track. But again, what you see is not that it is the best model counter. It improved the virtual best solver significantly.
00:35:16.394 - 00:35:45.948, Speaker B: And you can see we are now getting closer. We have a larger benchmark. And using this portfolio of a whole bunch of solver, you add the PMC, and now again you're solving about 10% more formulas. So let me try to wrap it up. I think symbolic technique or BDD decision diagram. When I say symbolic early mean decision diagram techniques, they give us viable approach to Boolean regioning. They are add important addition to the portfolio.
00:35:45.948 - 00:36:30.024, Speaker B: And unlike what happens in academia when you go to industrial tools, they are almost always portfolios, because unlike the people industry care about solving problems, not about publishing a paper. And so they throw the kitchen sink at the problem for the future. Reduction in is a powerful tool, but at the end, start solving or reasoning. Such reasoning will be using algorithmic portfolios. And one question that I think we need to ask ourselves, are the competition somehow discouraging? You know, they're pushing you towards this reductionist approach. In fact, when Satzilla came about, people said, oh, no, no, no, we can't allow that. So let's, let's make sure we're not using portfolios.
00:36:30.024 - 00:37:12.974, Speaker B: Maybe we need to do more research about portfolios, because that's the way it's going to be in industry. The bigger questions are search is very powerful technique, and in fact, CDCL is an extremely powerful combination of search and reasoning. And unless we find a way, I think to combine search and symbolic techniques, this to me is what the next step is. Another question is, again, one of the success of SAT was it was very easy to generalize it to SMTP. Can we take symbolic reasoning? How do we combine bdds with SMT? So I'm trying again to get the community interested in broadening our research agenda. Thank you very much.
00:37:14.874 - 00:37:43.912, Speaker A: Thanks a lot, Moshe, for this very nice talk. So we have a lot of time for questions. So I suggest that you write the questions in the chat or raise your hand or even unmute yourself if you're not allowed to. So let me see, there is a question from Victor Miller says, were you able to even vaguely characterize the problems in which adDMC or DPMC did best?
00:37:44.048 - 00:38:42.538, Speaker B: Yes, yes, actually we have not vaguely, even, even more clearly, it has to do with the, with the three widths. So in fact we pretty much know that if we have a, a small three widths and small, I don't remember exactly the number. I mean it will be maybe, let's say 50, but don't grab me on that. If we have, but I mean, it's still large in a sense, two to 50 is a very large number. But in practice, if we have widths of less than 50, then these three decomposition based tools do actually quite well. And in fact a good portfolio heuristic would be to use this anytime solver, give them a time budget. See, can you get to a trigger composition of this bound, of the magical bound? And if not, go use d, use a monolithic approach.
00:38:42.538 - 00:39:03.114, Speaker B: You use knowledge compilation, but rather than dynamic programming. And I think in fact this tool is, if we combine DPMC with default, with this very simple kind of portfolio heuristic, very simplistic portfolioistic, this would immediately give us the best tool so far available so far.
00:39:06.894 - 00:39:19.984, Speaker A: Okay, there's another question from Kuldeep. Since it looks symbolic reasoning did well for counting and searching and search is really good for NP. What do you think about problems in between, like two QBf or three QBf.
00:39:20.524 - 00:40:18.984, Speaker B: So way, way back in the past, again, going back to my student Gotchang Pan, he also tried these tool for QBf and that has not worked very well at the time. And he ended up developing a tool that actually tried to combine BDD reasoning with resolution. So there is, there is a variant of BDD called ZDD zero separate decision diagram, and you can use it to represent clause sets of clauses compactly. So remember, part of the problem is, and this would be interesting, I think even to, I think people have may have explored it in the context of SAT, maybe current probably knows about this. And so if you have a very large set of clauses and it does become a problem. And so what ZDD enabled to do, able to reason about clauses, but symbolically. And we built then a solver QM res, which was did symbolic resolution.
00:40:18.984 - 00:40:58.924, Speaker B: And for deep alternation, actually not for shallow alternation. For shallow alternation, still search based tool were better. But if you went to deep alternation and for example, you take benchmarks, that translates a benchmark from modal logic to QBF, and you end up deep alternation. And for deep alternation at the time, in fact, QM res was the tool of choice for deep alternation 15 years ago. But as happens in academia, students graduate and they move on. And now we even have a problem running the tool because we did not do a good job of creating a packet with no dependencies. So even running the tool is a challenge.
00:40:58.924 - 00:41:17.674, Speaker B: But yes, I think we should explore symbolic. But even there symbolic, when I say symbolic, we, it does just mean BDD based. We need to think more broadly about what symbolic mean. I think in some context ZDD actually would give us the right combination of thinking, but combining clauses with symbolic representation.
00:41:22.534 - 00:41:44.654, Speaker A: More questions is asking. Portfolios are explicitly scoped out in the set competition. And using a portfolio in the model counting competition was so much unexpected that the competition had to be rerun. But winning this competition is one of the surefire way to have high number of citations. What should we do?
00:41:46.794 - 00:42:28.952, Speaker B: I mean, the question is, again, is how to do, how to, you know, not to throw the baby with the water in a bathtub. Right. So I think what we need to do, I would say, is create separate tracks in summer. One way to try to do it is to have a reductionist tract and a portfolio track. So I mean, just as in very often, if you, if I were go, if I were to compete in, for example, basketball, I could never play zero's basketball because my height is 170. I think they should create, if they've created a basketball track for people between 165, 170, I might have become a professional basketball player. But as it is, I never had any chance.
00:42:28.952 - 00:42:46.824, Speaker B: So. But in other area, in wrestling, we have tracks, right? We create different tracks for different weight classes. So I think we could, it's worth it to put portfolio as a subject deserve to be studied, but we need to separate it from the, from the singular track. Singular solver track.
00:42:48.684 - 00:42:53.244, Speaker C: Yeah, I have a comment here. Oh, sorry, I get an.
00:42:53.324 - 00:42:54.304, Speaker A: Go ahead, please.
00:42:55.404 - 00:43:22.934, Speaker C: So there's like a no limit track, which exactly for this purpose. So there was in 2013 was kind of the clash of this cultures portfolio, but it's not. I think Karen was actually judge back then. This was a really very debated time. And since then, like the, the main track did not allow portfolios, but there was always a separate track which was called no limit. And you could like submit anything, but people stopped.
00:43:23.394 - 00:43:23.778, Speaker B: So.
00:43:23.826 - 00:43:48.562, Speaker C: So what happened in the last like, I don't know, four or five years was that there was no portfolio solver anymore in this track. And so it's not like the organizers of the competition, it's like the submitters. And I have a second comment. So there's a very nice paper by Randy Prieand and Marien Hoyle coming up, which is about symbolic PDD based start solving attackers, actually. So you might check that out.
00:43:48.658 - 00:44:01.674, Speaker B: Yeah, actually I've heard he gave a talk for my seminar. Yeah, I think this is the one follow up. I haven't seen it. I've seen the preliminary version hasn't published yet. But is it accepted somewhere? Do you know what's the status in the. Okay. Okay.
00:44:01.674 - 00:44:28.162, Speaker B: Okay. Yeah, very good. Yeah. So, no, and of course Randy was able to take some cases where we were struggling and he, I would expect Randy Bryan to add some to improve it and led to much better performance. So it'd be nice to see more people looking for non, non CDCL. There was some, I remember I had some communication with the competition at some point, and in fact they tried to create non CDCL track. And I think even that did not for.
00:44:28.162 - 00:45:04.002, Speaker B: You're right, I'm not trying to blame, when I say competition, I'm definitely not trying to blame, to blame the competition organizers. I'm talking to us. We are all in it. I mean, the competition organizer and the competition submitted are all one part of one community. I think we as a community needs to figure out what are the, how do we create more proliferation of approaches and different things. Competition are very powerful tool, so we need to think how to leverage it, how to make it even more successful in particular to neglected areas. Okay.
00:45:04.018 - 00:45:16.534, Speaker A: There was one question from sambas is asking about combining search and symbolic methods. So have there been any attempts to bring techniques from CDCL solvers into BDD solvers like backtracking or close learning.
00:45:17.074 - 00:46:24.958, Speaker B: So what I have, we are now, I have now a project of students that are working on essentially trying to use somewhat of a search, but not pure search, but can we use in some sense parallel search to parallelize this approach by essentially starting with searching on a set of variables and then splitting the problem and sending two different cores. And we will know very soon how successful this is. But I think one of the nice thing about CDCL, I would say it's a very smart combination of search and proof, so to speak. And it's smart in the sense that if doing kind of what we do is we know from the problem that this problem emerged early on. If you just do all resolutions, you die because there are too many resolvent. And the idea here is to use the search to guide resolution. And I think this is really a brilliant idea that I don't think we fully understand the power of this idea.
00:46:24.958 - 00:47:02.530, Speaker B: And the question is, can we use this idea and broaden it beyond resolution. Right now, the way it works at this, as I said, as Galil observed in the pure splitting, if you just combine, it really gives its resolution. But I think this is a very powerful idea that Karim discovered in the nineties, that you can let search guide the resolution. And the question is, can we let search guide more than just resolution? Can we let search guide cs like the joins and maybe in hyper, in extended frequency and search guide introduction of variables and so on and so forth. I think they deserve further study, if I may.
00:47:02.682 - 00:47:38.566, Speaker D: I think you're giving me too much credit for the idea, Moshe. The idea of conflict driven learning and learning from search to guide resolution was started in the early seventies with the truth maintenance systems, you know, for, for CTS, you know, MIT medical diagnosis, they were calling them no goods. I think what made it work so well for cs, for SAT, is the fact that the learning is of the same form as the input. Yes, the constraints are uniform in that sense. So that's what made it work so well.
00:47:38.630 - 00:47:38.966, Speaker B: Yeah.
00:47:39.030 - 00:47:45.974, Speaker D: I don't know if you can find. I think the nature of the, of the constraint is important here. You know, different kinds of formats.
00:47:46.094 - 00:48:07.446, Speaker B: Yeah, but let me give you an example. So there are many applications in which we need, we get a set of constraints, but the constraints are borders and just closes. We have xors, we have cardinality constraints. Think about it. BDD is give them xor, no problem. Cardinality consents, no problem. So we cannot do it's, you know, resolution does not work there.
00:48:07.446 - 00:48:37.574, Speaker B: By joining, projection would work equally well. And so we have the potential, I think, if we can think beyond, if we can find a way to combine in such a way, we have the option of generalizing these ideas. And to me this is one of the challenges. Can we take the key ideas that have led to so much success in CDCL and are they just specific to CDCL, or can we somehow find the essence of these ideas and carry them to even larger success in dealing with wider class of constraints?
00:48:39.954 - 00:49:01.444, Speaker A: So now, going back to this idea of symbolic solving, if I may ask a question myself. So I know that another area that's very much deep in your heart is automata theory. And in symbolic model checking, that's used a lot, and bdds in some sense, they can be thought as automata or length of the input is just fixed, right?
00:49:01.484 - 00:49:07.204, Speaker B: Yeah, yeah. Deterministic DFA. Bdds are dfas, essentially, yeah.
00:49:07.244 - 00:49:24.504, Speaker A: So the question, the question is, of course, an automata, we have these concepts of non deterministic automata, even alternating automata, which plays a very key role in symbolic model checking. So what about using them here? Is there any attempt to using these methods?
00:49:25.484 - 00:50:25.148, Speaker B: So we have used, of course, we use bdds a lot for reasoning. For example, when also pan also did work on using model solving. So the model solving is really, if you look at the satisfiability for modelogic, it's about using three automata and, you know, again, triotomatized, essentially the end of the game, the way. So 300 automatize is a two player game and you solve, you solve the winning states and you can do it symbolically. So again, he built a, built a solver for monologic k for the basic model logic, again, using bdds. The interesting thing is about when we do all this sophisticated automata, third degrees on, we always reducing to deterministic automata because that's what we can manipulate efficiently. Okay, so the exception is satisfiability.
00:50:25.148 - 00:51:11.864, Speaker B: But when we do, for example, temporal synthesis, it's about deterministic automata. And even you look at the reduction to parity automata, when you look at the heart of what's happening, it is reasoning with deterministic automata. People, I have looked together, some italian colleague at symbolic parity game solving. Also building on bdTs, when you have more powerful automata, it becomes very directly, for example, they are not closed under various operation. So the way we get the closure is we reduce them to the Munisik automata. And the municipal tomatoes are very, very close to bdds. Bdds are essentially municipal tomato with skipping irrelevant letters.
00:51:12.924 - 00:51:18.990, Speaker A: So it's a bit like bit blasting in some sense, right, where you are reducing to a less expressive marvel and.
00:51:19.062 - 00:51:20.598, Speaker B: Yeah, yeah, yeah, yeah.
00:51:20.646 - 00:51:23.094, Speaker A: And you pay the explosion.
00:51:23.134 - 00:51:40.554, Speaker B: But yeah, yeah. And I have to tell you, we, we have tried hard to, to say, well, this is stupid, we should be able to be smarter than that. And we had lots of failures along the way of trying to be smarter than bit blasting in on doing, on working with automata. Yeah.
00:51:41.814 - 00:52:04.136, Speaker A: Okay, so thank you very much. We're very much on time. So let's thank Moshe again for this fantastic talk for everyone. And now we're going to take a break before Karem will give us his talk. So we'll resume having this five minute break. So we'll resume at 930 Pacific time. Thank you very much.
00:52:04.160 - 00:52:06.344, Speaker B: Moshe. Again. Yep. My pleasure.
