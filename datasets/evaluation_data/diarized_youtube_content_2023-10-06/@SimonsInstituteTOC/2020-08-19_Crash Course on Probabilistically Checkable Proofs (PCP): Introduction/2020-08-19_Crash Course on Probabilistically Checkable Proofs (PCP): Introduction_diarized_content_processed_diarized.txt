00:00:00.240 - 00:00:14.914, Speaker A: Okay, so welcome, everyone, to the first talk. For our first speaker, we have irid Dinur from the Weizmann Institute, who is an expert on computational complexity in general and PCP theorems in particular. And she will be delivering the introductory lecture of our PCP mini course.
00:00:22.414 - 00:01:54.484, Speaker B: Okay, so, hi, everyone. I'll be shutting off my camera. I just wanted to say hi, at least so you can see me, but I have some Wi Fi issues, so I'm gonna leave the meeting and from my Mac and just remain on my iPad. Okay? All right, so hopefully you can still hear me. So we have a sequence of four lectures, crash course on pcps. And I'll start with the first lecture, which will be very introductory, kind of setting up what pcps are and the context hardness of approximation and going into parallel repetition, the way we designed the four lectures with Dana Moshkovitz. The first two lectures are more introductory, and the second two are more kind of designed to be to describe areas that touch as much as we see on topics of high dimensional geometry that might be interesting for the specific program that we are taking part of.
00:01:54.484 - 00:02:47.344, Speaker B: So there are many geometrical questions. The lectures on Friday and Monday will be touching more on these kinds of aspects. Today will be more introductory. Okay, so let me start by describing three coloring, which is an NP problem. Now, I'm not assuming people have a strong complexity background, and I'm not interested in giving a complexity definition. So the best way I've found to describe the PCP theorem is to give a more concrete example. For example, three coloring.
00:02:47.344 - 00:03:50.980, Speaker B: So what is three coloring? We have a graph, vertices and edges, so the graph can look something like this. Maybe these are the edges. This is a graph, and it's a three coloring, is just a coloring of the vertices in three colors. So a three coloring is some string c in zero, one, two. These are the colors raised to the power of v. So every vertex receives a color, and the coloring is considered to be a legal coloring if c is valid. If for every edge uv c of u is different from c of v, so every edge, these two distinct colors.
00:03:50.980 - 00:04:38.510, Speaker B: Okay, so let's see if this graph is three colorable. So I can color this vertex red, and then I can color this one blue, then I can color maybe this one orange, and I can keep going like this. So this one can be red, maybe this one will be blue, this one red. And then I reach this last vertex and I see that it has three neighbors. One is red, one is orange, and one is blue. So I cannot, there is no way I can color this vertex properly. If I, if I choose to color it red, for example, then this edge will be, okay, wait, how do I do this? This edge will be bichromatic.
00:04:38.510 - 00:05:27.044, Speaker B: This edge will be seeing two different colors, but this edge will be red and red. Okay, so this graph is actually not. So this graph is not three colorable. But there is something interesting about this graph. Notice that the number of I had one vertex in the middle and there were an odd number of vertices around that were all connected by a cycle. So really this graph is supposed to hint at a graph that is a cycle of odd length, of growing length, and all the vertices in the cycle are connected to a central vertex. Such a graph is almost three colorable.
00:05:27.044 - 00:06:25.206, Speaker B: It's not three colorable for the same reason that this example is not. But there is a three coloring which I just drew, which violates only one single edge. So in some sense you can say, oh, it's almost recolorable because there is a coloring that satisfies almost all of the validity constraints. Every edge is happy except for this one unhappy edge. Okay, and this is kind of demonstrating some important aspect of constraint satisfaction problems which we will now discuss. Okay, but before we discuss them, I just want to say that what we are seeing here is a combinatorial problem, three coloring. And I described it as, you know, every edge needs to see two different colors.
00:06:25.206 - 00:07:24.414, Speaker B: But of course you can view it as a special case of what we call a CSP constraint satisfaction problem, where we are looking at assignments to some variables. Here the variables are on the vertices and the assignment is supposed to give the variables colors. And we have a bunch of constraints here, every edge poses a constraint and the constraint says this color should be different from that color. So these are inequality constraints. But in general, CSP's can have more. There's a huge variety of problems that fall into this general notion of constraint satisfaction problem. Okay? For example, the constraints can be DNF clauses, boolean clauses like in three SAT.
00:07:24.414 - 00:08:24.664, Speaker B: They can be CNF clauses, they can be linear equations over some finite field. So there are many, many different examples of constraints, disfruction problems, and for all of them, whatever I'm going to say is relevant. So let me tell you a couple of examples of CSP's. One is three sat, so three SAT, we're all very familiar, right? You have some variables like x one or not x two or x seven. This is one constraint. And then we have many more like this. Okay, so this is a constraint satisfaction problem and maybe another kind of constraint satisfaction problem is thrilling, where the constraints are of the form x one plus x three plus x four equals one mod two.
00:08:24.664 - 00:09:38.414, Speaker B: So these are linear equations, and maybe you have another one, and this is another kind of constraint satisfaction problem. So these are all problems that are problems in NP, in the sense that if you give me a solution to one of these problems, it's easy for me to check them. So let me say that I forgot to say it about three coloring. So three coloring, and similarly, all CSP's is in NP, which means that if you give me a coloring to the graph, you tell me every vertex what color it is. It's very easy for me to go edge by edge and check that this coloring is proper, is valid. So there's a polynomial time algorithm for checking that the coloring is okay. But if you ask me to find the coloring, you give me a graph, and you ask me can you please give me a coloring, a three coloring of this graph.
00:09:38.414 - 00:10:45.296, Speaker B: Then we don't know of a way to do it, and it's believed to be impossible. If you manage to find a way to do it, this would prove that P equals NP. This problem is not only in NP, it's actually NP hard. And the meaning of this is that any other problem in NP can be encoded or recast into this problem, meaning that if someone found an algorithm for finding a three coloring of a graph, of an arbitrary graph, then immediately you can use that to solve any other problem in n two. So that's the meaning of being NP hard here. And so we saw this example of this graph. If you look at this graph, it's not so hard to convince yourself that it's not three colorable and to prove it to yourself.
00:10:45.296 - 00:11:52.304, Speaker B: But when we say that three coloring is NPR, it means that generally when we get a graph of a large number of vertices, there is no efficient way for us to decide if it is or it isn't three colorable. So it's like the graph is out there in the open. But the answer, whether it's three colorable or not, is totally hidden. And if you think about a given graph, for example, look at the graph that I drew here, the set of colorings. You can think of it as some geometric set, zero, one, two to the power v. So it's a set of string, okay? And every string you can attach to it the number of edges that it violates. So you have some kind of a landscape where for every point in the space there is some energy value which says how many constraints are being violated.
00:11:52.304 - 00:12:24.530, Speaker B: And if a graph is three colorable or not. It's just whether or not there's a point that has zero violations somewhere. And the way a graph defines this set of valid strings in an implicit way by a set of equations. So NP sets are always this kind of sets. They are always defined implicitly by a set of equations. And we are trying to see, for example, the question of a graph being.
00:12:24.562 - 00:12:38.910, Speaker A: Three color graph doesn't appear to be moving, at least for me. It's frozen on the, at the very beginning with the graph of the picture of the three coloring graph. Okay.
00:12:38.982 - 00:12:55.848, Speaker B: Yeah, yeah, yeah. I think this is how I put it, because I was just trying to. Thanks for asking. At least I know someone is out there, because I'm not seeing you guys. So please do interrupt me. And I'm just talking about. I was putting this on the screen because I wanted to have a graph in mind.
00:12:55.966 - 00:12:56.580, Speaker A: Okay, got it.
00:12:56.612 - 00:12:56.756, Speaker B: Sorry.
00:12:56.780 - 00:12:57.076, Speaker A: Dangerous.
00:12:57.100 - 00:13:31.274, Speaker B: Maybe I should do this. So you know I'm here. Yeah, yeah. Okay. Okay. So, to finish just this short introduction to constraint satisfaction problems, I'm sure a lot of you are familiar with it, but as a bootcamp, I found it necessary to define this. When I think of the set zero, one two raised to the power of v, it's a set of strings, and I can think of two different measures on it.
00:13:31.274 - 00:14:14.366, Speaker B: One measure is just, if you look at two strings, I can measure their distance and how many vertices do they agree? So maybe I start with some coloring, and I flip one color, I get to another coloring. So they're very close to each other. They're only one vertex apart. That's one kind of measure on colorings, and another one is the measure of how many edges are being violated. Okay? And sometimes you get a graph and you find a coloring that's almost a three coloring. For example, the coloring we have here satisfies almost all of the edges except for one. So you might kind of naively expect that.
00:14:14.366 - 00:14:47.454, Speaker B: Oh, okay. If I can already almost satisfy all the constraints, then I can probably just play a little bit and maybe flip one of the colors and change it a little bit, and I will get a perfect coloring. But in this example, you see how brittle it is. You can satisfy almost all of the edges, but there is no way to satisfy all of the edges. So a lot of graphs and a lot of CSP instances have this property that they're very brittle. In this way, satisfiability. You seem as if you're approaching it, but you're really not.
00:14:47.454 - 00:15:43.200, Speaker B: However, in the kind of instances that the PCP theorem generates. This is not the case. The PCP theorem tells you that actually not only is three coloring NP hard, but actually there is a mapping that takes, for example, three coloring, but it can take any other NP language into a set of instances that have this robustness property. So let me tell you what I mean here. This is still probably quite vague, and I will soon become more formal. The PCP theorem is an algorithm. We call it a reduction, okay, it's a reduction algorithm that maps a graph g to another graph.
00:15:43.200 - 00:17:01.374, Speaker B: Let's call it PCP of g, or I, denoted by g tilde, okay? And it's a polynomial time algorithm. It takes the initial graph and outputs another graph, and it has this special property that if g was three colorable, then also the, the new graph will be three colorable. And if g is not, then the new graph not only it's not three colorable like in our example, but actually it's very far from three colorable, okay, it's far in what, in what distance? In the sense that no matter how you three color it, many, like 1% fraction of the edges will be unhappy, will see the same color on both endpoints. Okay? So it's far from forgettable. Namely, let me call it that. The value of g tilde is less than 0.99. Where, what is the value of a graph? It's the maximal fraction of valid edges.
00:17:01.374 - 00:18:16.202, Speaker B: When I, instead of valid, let me write by chromatic, by chromatic. So when I say maximal fraction, I mean overall over all assignments, no assignments, overall colorings. Okay? So if you give me a graph, I can ask if it's three colorable or not, but I can ask even something more refined, which is okay, over all the possible ways I can three color this graph, every three coloring violates some fraction of edges, and I want the one that violates the fewest, or in other words, satisfies the most, the edges. And this is the value of g tilde. So this just tells me, oh, you know, it could be that initially if I look at all possible graphs, some of them are, even though they're not recolorable, they're very close to this. They, they can be very misleading and they can be like in my example, they can have a coloring that's almost, it satisfies all the edges except for one. So these are like these brittle graphs that they will confuse me.
00:18:16.202 - 00:19:24.602, Speaker B: The PCB theorem tells me, oh, but there's a polynomial time algorithm that takes your graph and changes it in a way that now the output is guaranteed to have this property that if initially you were in a non three colorable situation, then now you're in a very strongly non three colorable situation. Okay, so that's all good. And well, now why would I care about this kind of transformation? So this leads me back to kind of the way the PCP theorem was conceived and formulated and proven initially. This was a line of proof that started in the late eighties and came from crypto and let me call it the dramatized, dramatized. I don't know if this is a good name or not. A perspective on pcps. Okay, so far, the perspective I gave Yurit, there's a question in the q and a that might be good live.
00:19:24.602 - 00:19:42.718, Speaker B: The question asked, does 0.99 depend on the graph g? Oh, okay, great. So thanks for the question. So there was a 0.99 here. 99 does not depend on the graph g. Thanks.
00:19:42.718 - 00:20:32.144, Speaker B: So the way this theorem should read is an algorithm, and with the algorithm comes the number 0.99. It's a guarantee of the algorithm so that for every graph you g, you get G Tilde and you have this property. So that's actually the strength of the theorem that g can grow, the number of vertices can go to infinity, but there is still 1% of the edges that will always cry out. You know, I'm seeing the same color on both endpoints where, whereas without the PCP theorem, there's always going to be one edge that cries out, but as a percentage, it will be a negligible percentage of the edges when the graph goes to infinity. Does this answer the question?
00:20:44.004 - 00:20:49.204, Speaker A: Well, the person who asked the question isn't able to speak, but I think so. Definitely.
00:20:49.284 - 00:21:03.378, Speaker B: Yeah. I mean, I just interjected because I thought it might be a good question other people might have too. Thanks Arit. Certainly, I can't see the questions, unfortunately. So thanks for, yeah, calling them out.
00:21:03.506 - 00:21:19.466, Speaker A: So we have some more questions in the chat, some of which Dana has already answered on the chat, but you can answer them also if you like. So someone asked how much bigger may g tilt be than g, and whether 0.99 can be repeated.
00:21:19.490 - 00:21:51.392, Speaker B: So how much bigger may g tail? Right. So these are excellent questions and I'm going to get back to them soon. Okay, but first we're going to do interlude to the dramatized perspective on pcps. Okay. And we'll get to these questions. So I won't forget. Okay, so so far we talked about graphs or maybe system of constraints and their solutions, and how many constraints are being violated or not violated.
00:21:51.392 - 00:22:28.874, Speaker B: So this is kind of a mathematical or static view. Now, there is a different way to view these things, which is more personified or dramatized perspective, which is a more natural. Sometimes this perspective is very useful. People use it a lot in different areas. And in tcs, it's been very useful, specifically in crypto. So you have entities like, you have maybe a prover, and you think of this as, like, a person, right? And you have a verifier. And when people talk about them, they say, you know, the prover thinks this and that, but the verifier doesn't want the prover to cheat.
00:22:28.874 - 00:23:23.744, Speaker B: So it's all very dramatic, and they interact. And so these guys, they interact with each other. Sometimes there's more than one prover. And also, maybe the verifier has some randomness. And those of you who haven't heard a lot of crypto courses maybe don't know, but this was like, a hugely new thing in the eighties where these kinds of things were explored more rigorously and formulated. And this led to so many beautiful things, actually. I'm sure that everyone has heard about this, but nevertheless, one of the things that turned out is that, indeed, when the verifier has randomness, then the kind of the interaction between the verifier and the prover can become much more interesting and surprising in the power that the verifier has.
00:23:23.744 - 00:23:46.510, Speaker B: Okay, so let's return and see what NP looks like in a dramatized perspective. So the prover is pretty static. The prover just writes the proof and goes away. So instead of a prover, really what you have is a proof. So we start with a verifier. It didn't work. Okay, so here is my verifier.
00:23:46.510 - 00:24:16.054, Speaker B: And the verifier is always like an algorithm. It's computationally, it's supposed to be efficient, like an algorithm, and the verifier receives an input, for example, a graph. You know, that it needs to decide if it's three colorable or not. Okay, and the verifier also has access to a proof. Okay, so here is the proof. No, it's disappearing. Here is the proof.
00:24:16.054 - 00:24:58.604, Speaker B: Okay, so the prover wrote down the proof and left the room, and the verifier can read the proof. So, in an NP. In an NP problem, in NP language or problem, this picture is supposed to describe the following situation. First thing, the verifier reads the input. Verifier reads input. And think of the input as some graph g. So it's a graph g that the verifier needs to decide if it's three colorable or not, then the verifier reads the proof.
00:24:58.604 - 00:26:00.144, Speaker B: So in our case, if it's a three colorable, if it's three coloring, then the proof can be, for example, a string in zero, one, two to the v. So it's just a coloring of the vertices. So think of this as a string where every symbol here is just a color. This is one, this is two, this is zero, this is zero, and these are just the colors of all the vertices. This can be approved. Okay, and then the verifier decides, decides to accept or reject, so computes, and it says yes or no, output yes or no. So this is the output yes or no, okay, and in an NP, in an NP language, the property is that if g is three colorable, then there exists some proof PI such that maybe I don't have enough space here, so let me actually do it.
00:26:00.144 - 00:26:52.974, Speaker B: Do it like this, make everything smaller. Okay, so there exists a proof PI. Maybe I write here the word proof such that the verifier says yes, okay, if of course there is right, the proof would just be a proper three coloring of the vertices and the verifier. How does the verifier decide if to output yes or no? The verifier just goes edge by edge and checks that the endpoints of each edge are different colors. If yes, it will say yes. And if G is not recolorable, then for every proof. So no matter how you give me a coloring of the vertices.
00:26:52.974 - 00:27:40.734, Speaker B: So it's for every proof the verifier says no because there will always be some edge where it sees that the two endpoints are the same color. This is just in this example of recoloring. But as I told you, this problem is np complete. So it encodes any other NP language. So I'm not losing any generality by just talking about this seemingly very specific problem. Now what is the PCP version of this interaction? So we said that there is randomness, right? So we add to this picture, this we denote, we sometimes draw it by a random coin. So people like to put a dollar for the randomness and you add some randomness to the verifier.
00:27:40.734 - 00:28:09.380, Speaker B: So the verifier doesn't just do everything deterministically, but rather it has some randomness. Okay, so the verifier reads the input. Then here it's step 1.5. It tosses some random bits. The number of random bits is actually, this is important. It should be logarithmic in the size of the input. So in our case, if we have a graph of size, something n, let me denote it by n.
00:28:09.380 - 00:28:47.844, Speaker B: So logarithmic in n, where n is the input size. Okay? So it tosses some random bits, and then it doesn't have to read the proof, all of the proof anymore. This is what initially was so amazing. Actually, the verifier reads only part of the proof. Part I'm writing too small now. Part of the proof. Okay, so it can decide to maybe only read this part of the proof.
00:28:47.844 - 00:29:42.454, Speaker B: Or maybe if the random string said something else, it would read this part of the proof. So really, what's the verifier is doing? You should think of it as a distribution over many, many different verifiers depending on the random bits, right? So it's really a distribution. It's a, this randomized verifier is a distribution over different kind of different ways. The coin tosses fell. Okay? And for each different coin toss, the verifier ends up deciding yes or no. So in the end, I need to think about the probability that the verifier said yes and the probability that it said no. So now if I go to these conditions, previously we said if G is recolorable, there exists a proof.
00:29:42.454 - 00:30:17.622, Speaker B: There exists a proof PI such that the verifier says yes. Now I cannot anymore say something like verifier says yes, because the verifier is probabilistic. So instead, I write the probability that the verifier says yes is one. Okay, so that's in the yes case I want. Now I'm kind of making some kind of definition. So I want a proof system where if G was three colorable, there will be a proof causing the verifier to always say yes, no matter what the coin does. Were.
00:30:17.622 - 00:30:49.364, Speaker B: And if G is not recolorable, I want that for every proof. Again, now, I cannot talk about what the verifier is doing, except with probability. I wanted, the fraction of coin tosses that caused the verifier to say no is non negligible. Previously it was zero. Now it should be non negligible. Or really, the way we, we like to, to write it is we always talk about the probability of saying yes. Now it should be less than 0.99.
00:30:49.364 - 00:31:02.524, Speaker B: Okay? And hint is it's the same 0.99 as before. So let's, let's see why. Okay, so let me just say, so what is the. So, so far, I described this kind of.
00:31:03.224 - 00:31:16.034, Speaker C: Sorry, I think, I think there's a question that it might be worth answering live, just to clarify for everyone. So, anonymous attendee asks, is the proof a general algorithm or a string relative to a specific g?
00:31:19.134 - 00:31:46.874, Speaker B: Is the proof. So the proof has an existential quantifier, and then for all quantifier. So the proof, you don't need to think about it as an algorithm. It's the prover is almighty, and the proof can be anything like in NP. The. The guy that generates the covering for you doesn't have to work in any kind of bounded computational time. I'm not sure if I understood the question.
00:31:47.494 - 00:31:59.954, Speaker C: I think maybe it's good to say that there's a protocol that both the prover and the verifier are going to follow, but they can handle that. Protocol applies to any input, but the prover's protocol doesn't have to be efficient.
00:32:01.274 - 00:32:02.734, Speaker B: Right. Sure.
00:32:06.874 - 00:32:08.594, Speaker C: Okay. Sorry. Hopefully this is.
00:32:08.634 - 00:32:13.214, Speaker B: Maybe it'll be clear in a minute. Sorry.
00:32:14.634 - 00:32:16.534, Speaker C: Oh, go ahead. Sorry.
00:32:18.834 - 00:32:36.978, Speaker B: It's okay. I know this medium is tricky, and I'm sorry that I'm not able to answer more immediately. But anyway, thanks for. Thanks for asking. And please, please do ask any clarification. I'll try to understand the question better. Let me just say this right now.
00:32:36.978 - 00:33:04.308, Speaker B: I was just describing like a story. There's a prover, there's a proof. The guy is reading the input. Then there's these toxin coins. So the quantifiers are a bit hard to digest in the first go. So let me now repeat it and try to be a bit more careful and a bit more formal. Okay, so far, in this square, I wrote two statements, but it's not clear who is requiring what from whom.
00:33:04.308 - 00:33:25.232, Speaker B: So let me now state the theorem. Okay. PCP theorem. Okay. And the theorem says that there is a verifier. Okay, so there is a verifier algorithm for every NPC problem, in particular for graph recoloring. Okay, so I can state the theorem for graph recoloring.
00:33:25.232 - 00:34:03.330, Speaker B: Graph recoloring has PCP verifier. So that means there is some algorithm for the verifier that follows steps, follows steps one, 1.52 and three. And in the end, it satisfies these two requirements. These two requirements. Okay, so the PCP theorem says this every NP language. And I don't think the term NP language is very familiar to people outside of complexity.
00:34:03.330 - 00:35:10.306, Speaker B: So let me just say, think of this instead of just read this, as I'm saying, for the problem of three coloring. Okay, so you can just think of this as three recoloring or any other NP problem you like, like satisfiability? Okay, every NP language has a verifier. So this is, just think of this as just an algorithm, but this algorithm has the kind of interface that is drawn here in the picture. So the algorithm, the input comes from here. The algorithm has access to some random point random coins, and the algorithm is reading a proof from some proof oracle. So there is a verifier such that, okay, so I can just copy it again. So I just copy such that on input, such that for everybody on input x for the language.
00:35:10.306 - 00:35:53.064, Speaker B: But we can think of it as a graph g, the verifier. Okay, how do I do this? I tried to be very sophisticated. Yeah, the verifier reads the input. So in our case it's maybe the graph g. Then it tosses some random bits, log n, if n is the size of g, and then it reads a very small part of the proof. So now I can actually even specify it reads o of one proof locations. So that's kind of shockingly reads a very tiny part of the proof.
00:35:53.064 - 00:36:34.714, Speaker B: Right. Usually you expect to have to read the entire three coloring. No, this verifier is only going to read actually very few bits. And then it can, it outputs yes or no. And so, so far, I'm just saying every three, every NP language, or in particular three coloring has this kind of algorithm. And what does the algorithm satisfy? It satisfies this, these two conditions, these two conditions, which is that if the input, so the verifier doesn't know, I mean, the verifier gets the graph g. As we said before, this is an NP complete problem.
00:36:34.714 - 00:37:00.964, Speaker B: It can be something or it can be three coloring. So it gets the graph g. The verifier doesn't know and has no way of knowing if it's a three colorable graph or not. Nevertheless, with the help of the proof, it can read only a tiny part of the proof and decided to say yes or no. And it has this very different behavior in both cases. So now it's important. The questions that I was asked.
00:37:00.964 - 00:37:15.492, Speaker B: The 0.99 here is fixed for this. For this theorem, it doesn't have to do with the size of the graph g. This is for every graph g, and 0.99 is fixed once and for all. Maybe I should have a 0.999, I don't know.
00:37:15.492 - 00:37:55.758, Speaker B: Doesn't matter. Okay, so now what would this verifier be? It is simply the kind of very simple algorithm which does the following. You read the graph g, you toss some random coins and based on them you decide to choose an edge at random. So use the random coins to just select which edge to read. Okay. And then you select an edge, say u one, u two. You look at the proof locations corresponding to u one and u two, and you read the colors there.
00:37:55.758 - 00:39:03.964, Speaker B: If the colors are different, then it looks like a good proof. So you say yes, and if the colors are the same, you found that someone is trying to cheat you and you say no. So this is the PCP verifier, except that there is a problem with this PCP verifier. So I would say that this is a naive PCP verifier. I choose a random edge, except if endpoints. Maybe I should write read colors from proof and accept if distinct if the colors are different. So this is an IFPP verifier, and it would work on graphs that are not of the type that the example I gave in the beginning.
00:39:03.964 - 00:39:59.720, Speaker B: Graphs that are not brittle graphs where if they are three colorable, fine, there is a three coloring, there's no problem. But if they are not recolorable, I don't want them to have these colorings that are very misleading, that look like almost all the edges are happy except for one edge that is unhappy. These graphs are actually problematic because for such a graph, the probability for such a graph, if the prover was devious and gave me this kind of coloring, then the probability that the verifier will see a problem is only one out of the e edges. So the probability here will not be 0.999. It would be as bad as one minus one over e. That's kind of the worst possible. That the proof is very misleading.
00:39:59.720 - 00:41:00.514, Speaker B: There's only one edge that the verifier will catch, and every, and then every, on every other edge, the verifier will say yes, only say no on one edge out of all of the edges. So one minus one over e. When e is the size of the graph, this goes almost to one. Whereas in my, in the theorem I'm saying that the probability is bounded by some constant, the fraction. Okay, so how do I fix this? Instead of running the naive verifier on the initial graph, we first back to where we were in the beginning and we first run the PCP mapping on our graph to get a new graph g tilde, that we know is not Britain. Remember that we had this transformation. If I scroll up, we had this transformation and this algorithm here I am over here that takes a graph g into a new graph g tilde.
00:41:00.514 - 00:42:13.650, Speaker B: And in the new graph. I'm guaranteed that if g was not recolorable, then G tilde doesn't have any problematic colorings. All the colorings always violate at least 1% of the edges. So what a naive verifier should do is first run the pcp transformation on G, and then expect this proof, not a three coloring for g, but rather a three coloring for g tilde. Okay, so going back, going back. Okay, so the PACP theorem was proven in the beginning of the nineties, I guess 90 or 91, and it immediately kind of, well, had many different consequences. But one thing was, it was very shocking to think about this, that a proof for three colorability or for any other NP problem can be verified by reading only a constant number of bits.
00:42:13.650 - 00:42:39.154, Speaker B: Kind of very counterintuitive. So this was one aspect, it was quite incredible. And another aspect is that this is very much related to hardness of approximation, which is what I want to describe now. So maybe, before I move on, let me just pause for a minute to see if there are any more questions.
00:42:47.054 - 00:42:52.354, Speaker A: Can you see the questions that were asked? Because there were a lot of them, but many of them have already been, or all of them have already been answered in the chat.
00:42:54.134 - 00:43:02.256, Speaker B: Oh, okay. I cannot see anything, unfortunately. I had some Wi Fi problems, and I cannot. I'm only seeing my iPad screen. Yeah.
00:43:02.400 - 00:43:10.124, Speaker A: Okay. So I guess the protocol that we're taking with questions is if one of the panelists wants to answer it live, then they will interrupt you and otherwise we'll answer them.
00:43:11.704 - 00:43:36.904, Speaker B: Cool. All right, cool. So let me. So, risking repeating answers that were given in the chat, let me just mention a couple of things here. There are some parameters that are natural to consider for the verifier. So let me talk about them before I move to hardness of approximation. And that will be probably the end of today's lecture.
00:43:36.904 - 00:44:30.872, Speaker B: Parameters of the verifier. So, one thing is the number of queries to the proof. So, we said that in the PCP theorem, the verifier reads a constant number of symbols from the proof. But what is this constant? Okay, so that's one parameter. Another parameter is the Alphabet of the proof. So Alphabet, in the sense of coding theory, maybe the most basic is to think of the proof as written in bits. But for example, in three coloring, it's natural to write the proof in Alphabet of zero, one, two, and in other cases, it's natural to go to an even larger Alphabet because it allows other parameters to become better.
00:44:30.872 - 00:45:12.994, Speaker B: And we'll see this now, a third parameter. Okay, we'll say this soon, but first let me talk about the number of random coins. The number of random is used by the verifier. So we said o of log n, but actually, as long as it's o of log n, this corresponds to a polynomial size transformation. But of course, if it's one times log n, this will be a linear transformation. That's one thread of research is trying to get this to be as efficient as possible. Okay, I won't say any more about this today.
00:45:12.994 - 00:45:42.384, Speaker B: Final parameters are the completeness and soundness. So, completeness, these are the two probabilities. The completeness and soundness are these two probabilities. Over here. Completeness is the probability of saying yes when you're supposed to say yes. Okay, so let's just only treat the case right now that it's one. Often it's close to one, like one minus epsilon.
00:45:42.384 - 00:46:19.356, Speaker B: Sometimes it's other values. And the soundness is the probability to say yes when you're supposed to say no. If the graph is not recolorable, really what you want the verifier is to always say no. But okay, this cannot be done. So you measure the soundness, what is the probability of saying yes falsely? And here we wrote the soundness to be 0.99. So, okay, that's one PCP verifier, and it has soundness of 0.999. You might ask, okay, but I want to know what is the best possible soundness that I can guarantee? Can I get 0.998?
00:46:19.356 - 00:47:09.424, Speaker B: And so on. So that's the completeness and soundness. And we often care about trying to understand the trade offs between these various parameters. Okay, so now let me move to talk about hardness of approximation. Okay, so in general, when we think about approximation, we should first talk about optimization problems. In optimization problem, you're trying to find the best solution. The solution, say, for recoloring, I should think about Max three call.
00:47:09.424 - 00:47:43.712, Speaker B: I prefer to give an example and talk about it than to give a generic definition. So I'm sticking to this example. Max recall in this case will be the problem of, given a graph, find the coloring of the vertices that maximizes the fraction of edges that are bichromatic. And that's a maximization problem. You can think of others. For example, given a system of equations, find an assignment to the variables that maximizes the fraction of satisfied equations. All of these are very similar.
00:47:43.712 - 00:48:43.492, Speaker B: And again, like I said in the beginning, there are examples of constraint satisfaction problems. Okay, so we already know that this optimization problem is NP hard, because we know that it's hard, NP hard to decide if the value, if you can satisfy 100% of the edge constraints, or less than 100%. So we know it's NP hard for this case. Okay, you can think about Max three Linux. So Max Toriline is the problem of you have a bunch of linear equations over the field of two elements, zero and one, and maybe each equation only has three variables, so they're very simple equations. And still your task is to find an assignment that satisfies the largest fraction of equations. In this case, unlike the three coloring case, to decide if the whole system is satisfiable or not, it's very easy.
00:48:43.492 - 00:49:32.130, Speaker B: It's, you just need to invert the matrix. But the optimization problem, to find the maximal solution maximizing the fraction of good equations, actually, this is NP hard. And the reason is that the equations can be over determined. It can be the case that there is no solution to all the equations. And then these algorithms, like gaussian elimination, they just fail. It turns out that this is NP hard to maximize, and almost every CSP you think about is NP hard to optimize over. And so it's natural to think about approximation algorithms, which is an algorithm that is supposed to give you a weaker guarantee.
00:49:32.130 - 00:50:56.498, Speaker B: Okay, it won't find the maximal solution, but it will find a solution that it may be, is guaranteed to be within 98% of the maximum. Okay, so that's an approximation algorithm. And the PCP theorem says that even approximation is NP hard for these kinds of constraint satisfaction problems. So the PCP theorem implies that even, sorry about this. So here I'm just going to give, as an example, max three call. And the reason is, as we saw in the PCP theorem, it's going to be NP hard to decide if a graph is perfectly recolorable or if the maximal fraction is no more than 0.99. We already know it's NP hard to decide between the value of g is one, or the value of g is less than 0.99.
00:50:56.498 - 00:51:59.514, Speaker B: And here the value is, this is the maximal fraction of satisfied edges. When I say satisfied, I mean bichromatic. So this was one formulation of the TCP theorem that these two things, it's NP hard to tell between them. And so if I had an approximation algorithm that approximates to within 0.999, I would give this algorithm the graph g, and it would give me some value that's within a small margin of error, which would allow me to distinguish between these two cases. So such a statement actually proves to me hardness of, or the non existence of approximation algorithms, assuming p is different from NPC. So these are always, all these hardness statements are always assuming that, of course, that p is different from NP.
00:51:59.514 - 00:52:27.564, Speaker B: Okay, and now we get to this value of 0.99. Okay. And. Okay, actually, so I'm going to end in ten minutes, and I want to leave a few minutes at the end to give a roadmap to the talks that will come. So. Okay. And actually, I'm doing pretty good on time, so I will finish this in a few minutes, and then I will talk about what will be in the next few talks.
00:52:27.564 - 00:52:51.622, Speaker B: Okay, so now I'm getting to the parameter 0.99 and how it relates. It doesn't relate to the specific g, but it does relate to the specific. Oops. Problem at hand, max three color. Okay, so if instead of max three coloring, I would have here a different problem. For example, maximizing linear equations, you could expect that this number might be different.
00:52:51.622 - 00:53:40.546, Speaker B: And it is. And here is a very celebrated theorem of Johann Hastad, which says, actually, for some specific problems, like three sat and three lin, I know the exact constant that needs to be here. So. Not I know, but Hasta knows. So for thrill. And similarly for three Sat has no non trivial approximation algorithm. Okay, so I won't have time too much to go into it.
00:53:40.546 - 00:54:20.066, Speaker B: We will talk about it probably in the last lecture. But, for example, for three lean, you have a bunch of linear equations over boolean variables. It's very easy to see that you can always satisfy half the equations. If some assignment satisfied less than half, then just flipping all the bits will satisfy more than half, something like that. What I said is not completely correct, but it's actually very easy to see. Maybe you can do a probabilistic argument to say that in expectation, each equation is satisfied with probability half. So there must be an assignment that satisfies more than half of the equations.
00:54:20.066 - 00:55:12.098, Speaker B: So it's always very easy to satisfy half of the equations. And what hastad proved is that, okay, let me actually just erase three set for now. And what has to prove is that actually it is np hard to decide if the value of the equations, the given system of equations, is above one minus epsilon, or if the value is at most half plus epsilon. And this is for every epsilon. Okay, so you're given a system of boolean equations with each equation with three variables, and it's hard to tell between almost 100% and almost 50%. So, in that sense, there's no non trivial approximation algorithm. And similarly, for three sat.
00:55:12.098 - 00:56:02.994, Speaker B: And today we know that the picture is even more subtle. That it's not really just the random assignment, really. The value that distinguishes between these two cases is the SDP value. And when I say, know this, I mean that the SDP value is the correct soundness, soundness parameter here. And when I say, I know this. So, Prasad Ragavendra proved this, assuming the unique games conjecture. So we still don't know this for a fact, but it's conjectured that we know the correct value up to this, up to this open question.
00:56:02.994 - 00:56:50.474, Speaker B: Okay, so let me, let me just summarize what I said about the hardness of approximation. It just said that the PCP theorem, in its standard form, is equivalent to some statement about the hardness of approximation, for example, of three coloring. But you can run it also on other problems, like on three lean, or on three SAT, many other NP problems, you can interpret the PCP theorem as a statement about hardness of approximation. You will get some parameter like, here we got this, 0.99. I kind of just made it up from the theorem. You will get some worse constant, like 0.999,999. And then I said that hast had proved a much stronger PCP theorem.
00:56:50.474 - 00:57:25.348, Speaker B: We call. It's a tight PCP theorem. It's tight because the parameter that he got is the best possible. There's an algorithm showing you who cannot do more than you cannot do better than that. And so we know for some problems that what is the exact tight parameter? And there's a huge amount of work on this. And when you try to look at tight parameters, you encounter many geometrical questions. And this will be described in the next.
00:57:25.348 - 00:58:05.224, Speaker B: Maybe this will be described, actually, in the last lecture on Monday. So, now let me finish by giving you a little bit of a roadmap of where we're going with this sequence of talks. So, the way I wanted to say it is to ask, how are the theorems, the PCP theorem in Hastad's theorem proven? Okay. And even further theorems. Right. So, tomorrow on lecture two, Dana will talk. She will give a proof of a basic PCP theorem.
00:58:05.224 - 00:58:59.826, Speaker B: It's even, it's weaker than the PCP theorem that I mentioned here. This is what we call the basic PCP theorem. Dana will give a proof of a weak, a weak PCP theorem. I mean, there's so much you can expect her to prove in 1 hour. So this would already be quite an accomplishment. Then, in lecture three, I will talk about kind of how to prove, how to prove a stronger PCP theorem using kind of a tensor tensor operation, which we call parallel repetition. So we'll talk about this operation, which takes one PCP theorem and maybe strengthens it.
00:58:59.826 - 01:00:19.564, Speaker B: And this is important both because there are some very interesting geometrical questions, high dimensional geometry questions here that we thought might be interesting to tell you about, but also because this is a kind of initial step towards proving Hastad's results, because he uses this kind of theorem combined with something called the long code gadget, which will be a key player in the last lecture, where Dana will talk about unique games, conjecture and the long code and all kinds of isoparametric questions that arise when you try to prove tight hardness of approximation. So when you try to prove tight hardness of approximation, like in Hastert's theorem, but in a more general and more applicable to more problems, then you reach all of these interesting questions, and that will be the last lecture. So stay tuned.
01:00:27.304 - 01:00:45.564, Speaker A: Okay, so, thanks, irid, for the great lecture. And so Jesse has just put a link in the chat to the Discord channel, so we can all go on to discord now, and maybe after a break for stretching and or coffee or tea, we can discuss what we'd like, and then we'll reconvene later for Ramon's talk.
