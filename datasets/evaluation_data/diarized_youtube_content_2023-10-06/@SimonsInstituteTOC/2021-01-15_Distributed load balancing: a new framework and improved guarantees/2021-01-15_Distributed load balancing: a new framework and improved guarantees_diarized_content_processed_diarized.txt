00:00:14.360 - 00:00:40.612, Speaker A: Hi, I'm Alan. I'm from MIT. And today I'll be talking about distributed load balancing, a new framework and improved guarantees. This is joint work with Sarah N. Morteza from Google Research and Binghui from Columbia. So, let's begin with the problem setup. So, in the problem, there is a set of sources, s and a set of workers, w, and they are connected via a bipartite graph.
00:00:40.612 - 00:01:28.490, Speaker A: So here the sources are s one s two, s three, and the workers are w one, w two and w three. Now, each source has one unit of load, and we must distribute the load on each source among the neighboring workers. So for each source, we can split its one unit of load arbitrarily among its neighbors. Now, once all of the load has been allocated, we can compute the total load on each worker as the sum of the loads over all the incident edges. So, for instance, in the graph shown, the total loads are one half two and one two respectively. Now, the objective in our problem is to balance the total loads on the workers as much as possible. And the way we capture this balance, this objective, is via a convex objective function.
00:01:28.490 - 00:02:17.554, Speaker A: So there are a few technical details, such as requiring the objective function is symmetric and somewhat well behaved. However, we will not go into those here and almost any natural objective function will fall under our framework. So, to give a few examples, we might consider, for instance, the maximum load objective where we are trying to minimize the maximum load on any workers. Or we may consider an l two objective where we are trying to minimize the sum of the squared loads. And finally, we may consider an LP objective. Now, the final constraint is we want a distributed algorithm that runs in a small number of rounds. And we're working in the standard congest model, where in each round the nodes may communicate a poly logarithmic number of bits of information with each of their neighbors.
00:02:17.554 - 00:03:06.972, Speaker A: Now, the problem is primarily motivated by web server and search engine applications. So in these applications, the sources correspond to query sources, such as, say, users querying a web page. And the workers correspond to data centers. So there may be millions of sources and millions of data centers, which is why we're primarily interested in distributed algorithms that converge in a small number of ram. And one of the main innovations in our work is we're able to deal with general convex objective functions. And this is a much more diverse class and is much more realistic than, say, just the maximum load objective or only other linear objectives. So now talk about a few lines of related work.
00:03:06.972 - 00:03:57.156, Speaker A: The first line of related work is in diffusion type load balancing. So this is in a model that is closely related to ours, except where the workers may communicate directly with each other and split tasks among split load amongst themselves. However, existing algorithms for diffusive type load balancing require a polynomial number of rams, and these algorithms can easily be translated to r setting. However, even in our setting, they also require a polynomial number of rands. So here, polynomial means polynomial in the size of the graph. Another line of related work is in distributed matching algorithms, or more generally, distributed mixed packing and covering Lp solvers. So, matching or.
00:03:57.156 - 00:04:45.014, Speaker A: Yeah, so the problem of matching corresponds to a linear objective function. And so for linear objective functions, there are algorithms that work in a poly logarithmic number of rounds. However, these algorithms cannot be directly applied to our setting, as convex objectives are actually quite fundamentally different from linear objectives. To maybe give some intuition for why this is the case. For a linear objective, the optimum always occurs at a vertex of the feasibility polytope. However, for a convex objective, the optimal may occur in the interior of the feasibility polytope. So there's actually a lot of work to do to get a poly logarithmic round convergence algorithm for general convex objectives.
00:04:45.014 - 00:05:33.514, Speaker A: So now let's talk about our results. So, as alluded to, we give an algorithm that converges in a polylogarithmic number of rounds. So, our algorithm computes a one plus epsilon approximation to the optimal solution and log n log squared d over epsilon cubed rounds, where n is the number of nodes in d is the maximum degree. So this is for general convex objective functions. Now, for the maximum load objective, we have a improved algorithm that converges in log n, log d over epsilon squared rounds. And we'll note that this is actually faster than a direct application of a parallel mixed potential positive lp solver by a log n over epsilon factor. So next, I'll talk a bit about the key technical ideas behind our result.
00:05:33.514 - 00:06:15.344, Speaker A: So, the first main thing to understand is why previous load balancing algorithms get stuck with this polynomial dependence on the size of the graph. So, previous load balancing algorithms all rely on a paradigm, that is, they linearly shift traffic from higher loaded workers to lower loaded workers. So what that looks like in our setting is. So let's consider, for instance, we have some sources and two workers with loads, l one and l two. Now, the source, s two, is connected to both of these workers. And so let's say l one is much bigger than l two. So then in one round, what these algorithms do is the shift delta times l one minus l two load from edge a to h b.
00:06:15.344 - 00:07:14.034, Speaker A: And so the parameter delta here is basically some sort of step size. Now, the reason why these algorithms get stuck with this polynomial dependence on the sides of the graph is that the step size delta must be set, must be set to be sufficiently small, as otherwise we cannot guarantee convergence and the algorithm may oscillate. So delta has to be as small as, say, one over the maximum degree, and thus the convergence rate will be something that is polynomial in the maximum degree. To understand this a little bit better, let's even go through a concrete example. So, consider a graph that is a matching, but there's one additional worker connected to all the sources. So for instance, in the following graph, there's s one, s two, s four, s five, and then l one, l two, l four, l five, and there is a matching. And then there's an additional worker, all three, that is connected to all of the sources.
00:07:14.034 - 00:08:26.934, Speaker A: So initially, let's assume that all of the load is distributed according to the matching. So the red edges contain one unit of load and the black edges contain no zero units of load. So of course this allocation is suboptimal, and in order to make it optimal, you must send one fifth unit of load from each of the sources, s one, s two, s four, s five, onto the middle worker, l three. So what this really means is that the step size has to be as small as one over the degree of the middle node in order for us to get to the optimum solution. So of course, in this trivial example, in this example, we can have some trivial algorithms that beat this simple example. However, if we try a little bit harder, we can also construct examples where it is necessary to shift mode much more aggressively than one over the degree. And so, uh, so, given that this is a very natural thing to try, we try to get around these hard examples with this sort of linear traffic shifting algorithm, and it does appear to be diff, difficult to deal with all these different possible hard cases.
00:08:26.934 - 00:09:14.594, Speaker A: So what we do for our algorithm is we actually adopt a completely different approach. So the first insight is the following. So if we actually know the target loads in the optimal solution, then computing a way to distribute the loads that satisfies those targets is just a matching problem. And as mentioned before, matching can be solved in a polylogarithmic number of rands. So this naturally leads us to an algorithm of the following form. So let's assume first that we're working with the maximum load objective function. So we only care about minimizing the maximum load so we can maintain target capacities for each worker and initialize these targets to be very large.
00:09:14.594 - 00:10:05.974, Speaker A: Now we iteratively test feasibility, and then if it's feasible, we decrease the target capacities until we reach a configuration that is barely feasible. And now this will actually work for the maximum load objective function. However, for general convex objective functions, we will require a much more refined approach because we cannot decrease all of the target capacity simultaneously. So what is our approach for general convex objectives? Our approach for general convex objectives is inspired by the previous approach with some additional twists. So we'll initialize target capacities for each worker. So say we'll initialize them to all be equal to the maximum degree. So of course this will ensure that the initial targets are feasible.
00:10:05.974 - 00:11:13.974, Speaker A: Now we'll test feasibility using a proportional allocation algorithm based on the work of Agrawal et al. So we'll use this algorithm in a very non black box manner. And I'll explain and the rest of this slide and the next few slides will really focus on understanding this proportional allocation algorithm. So what this proportional allocation algorithm will do is when it attempts to test feasibility, it will give us a solution that is close to feasible. Now once we see the output of this proportional allocation algorithm, we'll freeze the target capacities of some workers that we'll call tight. So what these tight workers are is workers whose capacities cannot be improved upon in any feasible solution. So due to the properties of this algorithm, we'll actually be able to construct a certificate that shows that in any feasible solution we cannot significantly decrease the loads on we cannot significantly improve on these tight workers.
00:11:13.974 - 00:11:47.942, Speaker A: So once we find the tight workers, we can decrease the target capacities for the workers that are not tight. So we'll decrease the target capacities for the unfrozen workers. And then we'll iterate steps two and three. So, testing feasibility and then decreasing target capacities for a poly logarithmic number of rounds until we reach conversions. So now the key cog in our algorithm is this proportional allocation. So let's try to understand this better now. So let's label the workers one through w, and we'll initialize.
00:11:47.942 - 00:12:27.334, Speaker A: Let's, let's say we have some target capacity, c one through cw. Now, proportional allocation works as follows. So we initialize weights for some of the workers, say a one through aw. And now we allocate all this loads on the sources proportional to the weights on the workers. So for instance, the following graph, we have a source s and then workers one, two and three with weights one, two and three respectively. Now we'll allocate the load on s according proportional to the weights. So we'll have one six load going to worker one, one third load going to worker two, and one half load going to worker three.
00:12:27.334 - 00:13:28.156, Speaker A: Now we'll proportionally allocate the load on all of the sources. Now, once all of the load is allocated for each worker, we can compute its total load. If the total load is below the target capacity, then we increase its, then we increase its weight by a one plus epsilon factor, and otherwise we decrease its weight by a one plus epsilon factor. And then we'll simply repeat steps two and three for a poly logarithmic number of rams. And so really the reason we can get away with poly logarithmic here is because we're now using a multiplicative weights type of approach. Now let's to give a brief summary of the analysis of this proportional allocation algorithm. So again, I've included the, I've included a summary of the algorithm as a reminder.
00:13:28.156 - 00:14:15.664, Speaker A: So now here are a few key observations and the analysis of proportional allocation. So the first is that if the load is within a one plus epsilon factor of the target capacity, then it will always remain that way. And so the reason for this is the following. So, because of the way we're updating the weights, we can ensure that the loads are always moving towards the target capacities. So if the load is too high, then it's moving down. If the load is too low, then it's moving up. And so now once we're within a one plus epsilon factor, since we're always moving in the right direction and we're never stepping like all the steps are of size, at most the factor of one plus epsilon, we'll always remain close to the target capacity.
00:14:15.664 - 00:15:27.574, Speaker A: Now also, since we're always moving in the right direction, if we know that at some point the load is much bigger than the target capacity, then this means that the weight has been decreased at every previous step. And so similarly, if the load is much less than the target capacity, then the weight has been increased at every previous step. So what this means is that there is a poly in n. So polynomial in n and polynomial in n in epsilon, multiplicative gap between the weights of significantly underloaded and significantly overloaded workers. So this is really the key observation about the behavior of the algorithm. So again, I've included this key observation for as a reminder. And now what does this really mean? So if we have a source s that's connected to two workers where, let's say, w one is significantly underloaded and w two is significantly overloaded, then what happens? So actually what this means is the amount of load being sent to w two is negligible because there's a poly and n multiplicative gap between the two weights.
00:15:27.574 - 00:16:44.210, Speaker A: So basically what this means is that we can cut the edge between s and the second worker. So again, there's a few more details to work through, but what this essentially allows us to do is we can actually construct a cut, certifying that if we have some significantly overloaded workers that we cannot significantly improve on them. So now, uh, again, I'll give a brief summary of how we put everything together in our full algorithm. So again, I'll omit a lot of details. So our full algorithm and our full algorithm will run proportional allocation with a given set of target capacities. And now if after running proportional allocation, the load is more than one plus ten epsilon times the target capacity, then we'll freeze the target capacity and then we'll decrease the target capacities on unfrozen workers by a factor of one plus epsilon. So what the one plus ten epsilon really is for is that if the load is more than one plus ten epsilon times the target capacity, then the, basically it's significantly overloaded.
00:16:44.210 - 00:17:38.370, Speaker A: And this will let us construct a certificate from the proportional allocation algorithm. So really a quick summary of how the analysis works. So, since we start from a feasible allocation where we can ensure that no workers ever more than one plus eleven epsilon overloaded. And then to certify optimality, what we do is we construct a hierarchy of certificates. So we show that basically each time we run proportional allocation and then freeze some set of workers, that our solution is essentially optimal when restricted to the set of frozen workers at the end of each round. So this will give us a hierarchy of certificates. So actually this, what I wrote on the slide is not quite, it doesn't quite work.
00:17:38.370 - 00:18:23.078, Speaker A: And what we actually do in our paper is something slightly different. So. Or is a slight modification. So we'll actually, our certificates will be for a super set of the set of frozen workers at the end of each round. But anyways, so here's a quick summary of how our full algorithm works. And now finally, once we have this hierarchy of certificates, the way we bound the objective value is we use some standard majorization inequalities. So, to summarize in this paper, we give a distributed algorithm for load balancing that computes a nearly optimal solution in a poly logarithmic number of rands.
00:18:23.078 - 00:19:04.644, Speaker A: Whereas previous algorithms for load balancing required a polynomial number of rounds. And our algorithm works for general convex objective functions. And we introduce a completely different approach that gets around some barriers for previous load balancing algorithms. So our new approach. In our new approach, we iteratively use matching algorithms which do conversion of polylor rhythmic number of rounds. We use these matching algorithms to test feasibility, then update target capacities according to the output of our matching algorithms. And then iterating this, we converge to an optimal solution.
00:19:04.644 - 00:19:08.224, Speaker A: So that's all for the talk, and thanks for listening.
