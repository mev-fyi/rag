00:00:00.120 - 00:00:00.304, Speaker A: Started.
00:00:00.304 - 00:00:23.518, Speaker B: Thanks, everyone, for coming to the last day of the last workshop. So just to bring everybody back to where we are. So in this workshop, we've been talking about a lot of things that we didn't really cover in this program or in the other workshops and are kind of things that need a lot of work. And today is a little bit of a combo. So we have two talks on interpretability. So, Julius.
00:00:23.566 - 00:00:24.634, Speaker C: And then beam.
00:00:24.974 - 00:00:34.646, Speaker B: Beam we heard from earlier this summer, and then we'll have one robustness talk and one RL talk today as well. So the first talk is Julius.
00:00:34.830 - 00:00:35.646, Speaker D: Thank you.
00:00:35.790 - 00:00:59.392, Speaker C: So thanks to the organizers for inviting me. And this talk will be about interpretability. And I should clarify that you will not be seeing any deep theorems, so you can, or even theorems so you can relax. And so, so we'll be talking about explanations. And almost every talk about interpretability or.
00:00:59.408 - 00:01:03.392, Speaker D: Explanations, there's usually a question about, can.
00:01:03.408 - 00:01:05.648, Speaker C: You define what you mean by interpretability?
00:01:05.736 - 00:01:11.512, Speaker D: So I thought I would actually try and prevent that by saying that you.
00:01:11.528 - 00:01:38.110, Speaker C: Will not be, I won't be giving you some insightful new definition of interpretability. And this talk actually is trying to see, sort of make it more, not necessarily formal, but rigorous in some sense, apparently, to see how we can evaluate explanation methods. And so if you. Why do we tend to want explanations? Explanations can be interesting from the sense.
00:01:38.142 - 00:01:39.782, Speaker D: Of trying to understand what a model.
00:01:39.838 - 00:01:54.462, Speaker C: You don't necessarily have a handle on how it's behaving. Another sense is that a lot of people tend to explanations to, to be causal in some sense. So to give you an understanding of, if I change this variable, how will.
00:01:54.478 - 00:01:56.326, Speaker D: It affect the output in some sense?
00:01:56.470 - 00:02:16.354, Speaker C: And then, so in terms of explanations for this talk, you can think of it as some artifacts that we'll be able to get from some model, predictive models in this case, and so some artifact that will try and provide insights that you will learn about. And if you're not, if you're not satisfied with this sort of level definition.
00:02:16.434 - 00:02:22.818, Speaker D: Being shortly and Quizzer on some of those properties.
00:02:22.986 - 00:02:52.394, Speaker C: And so I wanted to, I was in preparing for this talk, I was trying to get a sense for number of papers in this area. This is definitely not an authoritative figure. It's inspired by the same figure that. And it's interesting that you can see like a jump in 2017, 2018. But there's definitely a lot of work in this area, and I will definitely not cover most of it. Actually, this would be a very tiny.
00:02:52.434 - 00:02:54.450, Speaker D: Slice of some of the work in this area.
00:02:54.562 - 00:02:57.650, Speaker C: And so you should keep that in mind as you sort of hear about.
00:02:57.682 - 00:03:00.074, Speaker D: Some of the results.
00:03:00.234 - 00:03:26.530, Speaker C: And so why would we care about explanations? So there's a paper, challenges and interpretability, that is for transparency by 2017. That's actually very interesting and touches on a lot of the points you might be interested in. One could be model debugging. And that's the one that would inspire a lot of the work, a lot of the stuff. And you can think about it as, let's say you have some new architecture.
00:03:26.562 - 00:03:28.626, Speaker D: That you developed, also new model, and.
00:03:28.650 - 00:04:02.662, Speaker C: You'Ve decided you've trained this model, and you want to get a sense for when is this model going to make mistakes? And you might be very interested in trying to figure that out. Another one could be safety concerns, ethical concerns. So yesterday there was a session on ethics and fairness. You could potentially imagine, hypothetically, that you could have explanations that you derive from a model that give you insight into this thing. Another one that is also very interesting is, as humans, we tend to be very interested in, especially for systems that we interact with on a daily basis.
00:04:02.798 - 00:04:04.430, Speaker D: More regularly, we tend to be interested.
00:04:04.462 - 00:04:47.794, Speaker C: In some form of artifacts that would sort of satiate our trust or try and say, this is why I'm getting this recommendation on Amazon, or this is why I'm getting this recommendation for this movie on Netflix. And so these are some general high level motivation. There are definitely others that you can think of. And you could definitely also say that maybe explanations might not necessarily even be the right framework to try and attack those problems. But in general, this tends to be the motivation for developing. So to focus on the model debugging framework. If you are familiar with this literature, you might have seen this example before.
00:04:47.794 - 00:05:04.962, Speaker C: So in terms of debugging. So debugging can be a very vague term. Here's an example that's trying to sort of illustrate that. So this is from a paper in 2016 by Russia, and there they had this very interesting example that sort of.
00:05:04.978 - 00:05:06.562, Speaker D: Captured a lot of people's imagination, which.
00:05:06.578 - 00:05:58.434, Speaker C: Is that you have this image that's classified as a wolf, and you can then ask for some kind of explanation. We'll get into what an explanation means shortly. And when you ask for that explanation, so there's a gray, it has grayed a bunch of things out, but sort of highlighting the snow. So this white is the snow in the background. And if you look at that image, you can say, oh, perhaps my model is not learning the interesting thing that I wanted to learn about this object. And so it's focusing on the snow which you might say it shouldn't necessarily be reflective of whether something is a wolf or not. And so this could be some kind of artifact you want to reveal as part of your training process, that you want an explanation to identify another.
00:05:58.434 - 00:05:59.634, Speaker C: Yes.
00:06:00.694 - 00:06:11.910, Speaker E: Do you believe any part of that story? Like, do you believe that it's focusing on anything? Do you believe that anything on this slide tells you even, can I defer.
00:06:11.942 - 00:06:38.548, Speaker C: That question to, let's say, three fourths of the way into the presentation? And we'll try and sort of, that's part of the. So that's one of the things we actually want to learn in this presentation, which is how true is it that we can learn those artifacts from visualizing heat map cycles? But we'll get to that point. So another interesting interest you might have is, let's say, so this is a data set that was released earlier this.
00:06:38.596 - 00:06:40.444, Speaker D: Year by Narales et al.
00:06:40.564 - 00:07:20.848, Speaker C: And it's a bunch of faces. So it's called diversity in phases dataset, and there's a bunch of different faces. So let's say you live in, I guess maybe Cambridge, Massachusetts, and you, when you were collecting this data, you sort of mislabeled a particular subset of this data points. And then you train a model on this. You might necessarily want a model. You might want an interpretability method or an explanation method that would say, this is a subset of my training points that my model misbehaves on. And so this is another potential interest you might have to say, I want to understand what I'm always doing.
00:07:20.848 - 00:08:01.306, Speaker C: And so that's another motivation for why you might want to look at explanations. And so to sort of make it more, I guess it's still high level, but concrete. So you train a model, it tells you this thing is a husky, even though previous slide said it was a wolf. And then it's looking at, you say it's looking at the snow part of the image. And then hypothetically, you could say, oh, this is the kind of thing I want to fix about my models. So this loop of, I've trained a model, and the model has learned some sporious correlation in my data set, and now I want to fix the sporus correlation. This is one big interest you might have.
00:08:01.306 - 00:08:05.506, Speaker C: And I'm sure you've seen all of the examples from the medical literature where.
00:08:05.530 - 00:08:08.418, Speaker D: A model, a CNN trained on a.
00:08:08.426 - 00:08:11.010, Speaker C: Set of images happens to learn something.
00:08:11.122 - 00:08:13.642, Speaker D: Let's say the fact that there was.
00:08:13.658 - 00:08:39.278, Speaker C: A particular type of scanner that was being taken to radiograph. And so this kind of spurious correlation is something you might want to prevent and especially identify before you even deploy a model. And so this sort of model debugging promise that a lot of explanations have is something that I think is very interesting. And that's sort of the big motivation for a lot of the work we'll be talking about.
00:08:39.366 - 00:08:55.793, Speaker E: Dude, is there any model that does the right thing? So is there any model that you would say confidently, any CNN trained for any image task that you would say confidently is learning something that couldn't reasonably describe as a spurious correlation?
00:08:55.913 - 00:08:57.433, Speaker C: You could say it's learning the wrong.
00:08:57.473 - 00:09:01.073, Speaker E: Thing, right, but it's not learning the wrong thing.
00:09:01.233 - 00:09:02.529, Speaker C: Is there anything that is not learning.
00:09:02.561 - 00:09:04.681, Speaker E: The wrong thing, adopting the wrong framework in the first place?
00:09:04.737 - 00:09:24.514, Speaker C: And then I think that's bandits. I'm not sure I can give an answer to that because it's a very broad, it's a very broad question that it's not necessarily, and I'm not sure I can give a definite answer to that. Maybe afterward we can talk, but I need to think more about that. But that's a very good question and.
00:09:24.594 - 00:09:27.730, Speaker D: Something that should probably be incorporated in a lot of this work.
00:09:27.842 - 00:10:30.064, Speaker C: And so before going on, I wanted to sort of give a difference between what you might call interpretability versus explanations. And so interpretability tends to sort of refer to when you constrain the model class you're learning from when you constrain in the sense that, let's say I say I want an interpretable model, and this is definitely a vague term in the sentence that I'm using. But you could say if I learn a bunch of if then rules, and the if then rules are not necessarily too long, then perhaps somebody can look at this if then rules and understand them. But interpretability tends to refer to when you constrain a model. So, for example, you could say this is the difference between learning a lasso model versus, and you're looking at the weights of a lasso model versus you saying you're flexible in the way you learn your model, and then I'm going to come in with a post talk method to interpret it.
00:10:33.884 - 00:10:45.820, Speaker F: I mean, I've seen interpretability used in both explaining a model and designing a model. And I wonder how common is this distinction partition into these two terminology.
00:10:45.892 - 00:11:10.284, Speaker C: So, I mean, this is not my terminal, this is emergent. And maybe the answer comes from here, which is that there's a lot of work recently that's been focused on post hoc interpretations. And then another spectrum of this could be what I mentioned. I might want to sort of say, use only linear models, use a decision tree of one type of thing.
00:11:12.184 - 00:11:18.560, Speaker F: Construction. It's a very clear distinction. The use of interpretability for one, explaining for the other.
00:11:18.672 - 00:11:37.004, Speaker C: Yeah, that's a good point. So I'm taking it from, from sort of stuff language, and I think it tends to, because I actually think it's a very big difference between the two approaches and you need some sort of terminology to differentiate when you're referring.
00:11:38.744 - 00:11:39.484, Speaker A: To.
00:11:40.504 - 00:11:43.488, Speaker B: I think you're right, it's used interchangeably.
00:11:43.576 - 00:11:45.576, Speaker E: And it's not clearly defined or a.
00:11:45.600 - 00:11:48.372, Speaker A: Consensus in the community, but under the.
00:11:48.388 - 00:11:49.932, Speaker E: Sweeper, that's kind of, I think what.
00:11:49.948 - 00:12:00.104, Speaker A: You use in my talk, it would be different. Maybe there's like need for better terminology because these talk is so confusing that for an outsider to the field.
00:12:02.004 - 00:12:19.976, Speaker C: Valid point, very valid point. I would not have expected. That's why I'm using it here. And I'm trying to say, and the point of me doing that is trying to say a lot of the conclusions you will take away from this topic talk don't apply to vastly across the field, they just apply to very narrow.
00:12:20.000 - 00:12:21.688, Speaker D: Slides of the things I want to talk about.
00:12:21.816 - 00:12:25.512, Speaker C: And so that difference and the terminology is helpful in this presentation.
00:12:25.568 - 00:12:26.684, Speaker D: That's why I leave that.
00:12:27.144 - 00:12:40.862, Speaker C: And so I also think the perspective that she offers. So Cynthia Rudin is a pioneering skill and the perspective she offers in her paper is actually a very interesting one. And if you have some time, you.
00:12:40.878 - 00:12:42.074, Speaker D: Should check out the paper.
00:12:43.054 - 00:12:51.814, Speaker C: But in this talk, we'll actually focus on post hoc local explanations. And so I will try and give some.
00:12:51.894 - 00:12:59.354, Speaker B: Yes, so it's a local versus a global view. Kind of another way to separate these.
00:13:00.094 - 00:13:02.194, Speaker C: Yeah, I will get to that point.
00:13:04.294 - 00:13:09.924, Speaker F: Global tells you what is the policy and local tells you why did they make this decision on you?
00:13:12.504 - 00:13:37.464, Speaker C: We'll get to that. We'll get to that. I'm still sort of setting things up here until, as everyone here knows, you can train your favorite CNN on images. And here, because we'll be looking at a lot of gradients. That's why I'm sort of specifying this. So you can say it's going from RD to RC. C is the number of classes you would have and then D is the size of your input.
00:13:37.464 - 00:14:18.512, Speaker C: And so one kind of explanation you can imagine happen would be a sensitivity map. And so what that is here is so you have that input and you've trained your favorite model on this input. So now you can look at the gradient of a class score with respect to the input. And so what this is saying is there's a class corn there, and then you're now looking at the gradient of that scalar with respect to the input, which is a vector the size of your input, and then you can visualize this. And I should also point out, the choice of the visualization is also a.
00:14:18.528 - 00:14:21.792, Speaker D: Very contentious thing in this field as.
00:14:21.808 - 00:14:26.080, Speaker C: Well, because the visualization is actually what the end user would consume.
00:14:26.112 - 00:14:28.884, Speaker D: And depending on how you visualizes things.
00:14:28.924 - 00:15:05.622, Speaker C: You can potentially say different. We'll get to that point as well. And so this sort of gradient explanation would be sort of the fundamental sort of kernel, a lot of the methods we'll be looking at. And here, the reason why this might be interesting is if you think of a single linear model, if you do this gradient. So let's say that a single output, if you do the gradient, that gives you the weights of that model, and then in some sense that's sort of what you would expect as sort of.
00:15:05.638 - 00:15:07.214, Speaker D: The interpretation in that case.
00:15:07.334 - 00:15:20.684, Speaker C: And that's why this is also interesting. And it also conveys this notion that a lot of people are comfortable with, which is the fact that if I change some pixel by a little amount, how much does that change my output? And so you can get a sense.
00:15:20.724 - 00:15:24.316, Speaker D: Of that from the gradient, of course.
00:15:24.340 - 00:15:51.532, Speaker C: So this was sort of introduced, I mean, the notion of using the gradient as interpretation goes back a long time, but in this context was introduced in this paper in 13. There are other references as well. Another notion of inter explanations that we'll look at in this work is examples or sort of prototypes that you can represent what the network is doing. So in this case, let's say you train the model on cats and dog.
00:15:51.708 - 00:15:53.028, Speaker D: Like a cat and a dog data.
00:15:53.076 - 00:16:16.724, Speaker C: Set, and then you have a new test point. So the image on the far left is a new test point you have. You can then ask the question, can you rank all of the training points based on their influence on the test loss? And so this is, you might have seen work from ISML 2017 on this, using influence functions to rank the influence.
00:16:16.764 - 00:16:18.908, Speaker D: Of the training points on the test loss.
00:16:18.956 - 00:16:39.556, Speaker C: In that example, you can also do this in other ways as well. And that's what the second paper is referring to. And so here the interpretation, the explanation you get is you have a test point and then you return the top k training points. In this case, it's three training points that have the most effect on the.
00:16:39.580 - 00:16:41.300, Speaker D: Test loss for this example.
00:16:41.412 - 00:16:52.880, Speaker C: So that's another kind of explanation you can have. And so those will be the sort of two kinds we'll look at in this work, mostly the first, but this would also come into play when we.
00:16:52.992 - 00:16:54.804, Speaker D: Get into that part of the presentation.
00:16:55.184 - 00:17:11.444, Speaker A: So what's the meaning? Does it mean that if you had removed these from the training set, then the classification of that test point would change the loss? Yeah, the loss would change the most significant.
00:17:14.024 - 00:18:01.862, Speaker C: Okay, so local explanations. So there's another dimension you can use to group the kinds of explanations work that is emerging in this area, which is this local versus global view. And so why might this be an attractive thing? So if you're in a linear setting, as, I mean, this would be obvious to almost everyone. If you're in a linear setting, then you just have this weight vector, and that weight vector tells you what you're doing with any point that you observe. If you're in a non linear setting, of course it's nonlinear. Where you are in the input space, things change, and it could change dramatically. And this is the motivation for a lot of the work that has focused on local explanations.
00:18:01.862 - 00:18:19.686, Speaker C: And one kind that's very popular would be, I will, if I want to give an explanation for a particular input, I will fit some local model. It could be linear around that particular input, and then I'll interpret this model as a way to sort of get.
00:18:19.710 - 00:18:23.078, Speaker D: A sense of what the model is doing.
00:18:23.206 - 00:18:25.726, Speaker C: And so that's why the local explanations.
00:18:25.790 - 00:18:30.698, Speaker D: Point is sort of very pervasive recently.
00:18:30.786 - 00:18:45.938, Speaker C: Because it gives you this nice interpretation that you can sort of approximate your model around single inputs. And we'll actually, this is one of the things we might challenge in this talk, whether this is desirable and whether.
00:18:46.026 - 00:18:48.002, Speaker D: You should sort of, if there are.
00:18:48.018 - 00:18:49.322, Speaker C: Ways to get around some of the.
00:18:49.338 - 00:18:51.914, Speaker D: Problems you might have from doing this.
00:18:52.074 - 00:19:04.492, Speaker C: And so this is the, this is one of the inspiration for why you want. And so before sort of going into the actual meeting of the presentation, the key takeaways. And so if you are awake for.
00:19:04.508 - 00:19:07.424, Speaker D: Just one slide, please talk. This is the one you're awake for.
00:19:08.004 - 00:20:18.270, Speaker C: The first one is it's actually very difficult to assess a lot of these local explanation methods. And the reason why you'd want to assess them is that it seems like we're moving to a place where explanations, or the desire for explanations, are the things we'll rely on before we deploy a model. And so if you want to rely on explanations, then the explanations themselves better reflect what the model is doing. And so you might want a sense for how well is the explanation really reflecting what the model is doing. The second part is so the first part would be some work from last year. The second part would be ongoing, still preliminary work, which is that from a privacy perspective, let's say you're a startup and you built a model to sort of diagnose some kind of, some kind of, let's say you're predicting cancer from a bunch of different images, and you've spent a lot of money and you've hired a lot of developers to develop this model, and now you're licensing your model to, let's say, a hospital. And so the hospital can say, here are a bunch of test images.
00:20:18.270 - 00:20:32.382, Speaker C: Can you give us your predictions on those test images or explanations on those test images as well? And if you do this, it's actually very easy to recover the model that you spent a lot of money and.
00:20:32.518 - 00:20:34.014, Speaker D: Time developed as well.
00:20:34.134 - 00:21:24.656, Speaker C: And so there seems to be this privacy, and we'll define what privacy will be. It'll be sort of differential privacy kind of notion. So there seems to be this privacy versus explanations tension, which is you might want to explain your model. So you might want to say, here's, in a sense, some insights you can learn from this model that I've built. But then as you start to explain the model, you run into trouble because it's very easy to recover the model and for somebody to sort of replicate your model. And so these will be the big trade offs that we'll talk about in this talk. And the point would be that perhaps local explanations are maybe we should sort of try and rethink how we use local explanations in this set.
00:21:24.656 - 00:21:50.012, Speaker C: For example, the method that ranks the training points, if, as we all know, your training data is, is a really key part of your pipeline. If you show people your ranking of your training points, if you're very careful with the way you choose the queries you ask of a model, you can very easily recover a significant portion of.
00:21:50.028 - 00:21:53.620, Speaker D: The data set that was used to build a model.
00:21:53.732 - 00:21:54.772, Speaker C: And so these are some of the.
00:21:54.788 - 00:21:58.024, Speaker D: Trade offs that we'll focus on, as I saw.
00:21:58.664 - 00:22:19.568, Speaker C: And so let's talk about the first one. And so you've already seen this. So there's an input, and then you give a prediction. You can look at the gradient, which would be the gradient of some class core, which corn in this case, with respect to the input, you get something like this. Why is this? This is also very, one of the reasons why this is attractive is because.
00:22:19.736 - 00:22:22.184, Speaker D: A lot of other differentiation packages.
00:22:22.264 - 00:22:51.092, Speaker C: This is one line. And so it's very easy to just use this. This is another reason why this is popular. And so there's another method. There are other methods. And so why would you want other methods? One potential reason why you want other methods is when you train a lot of these networks, like most of you know, the gradient tends to saturate. And so it's actually not clear when you do this derivative.
00:22:51.092 - 00:23:00.092, Speaker C: Sometimes it's not clear what the visual representation is. And from a visual perspective, it tends to look noisy.
00:23:00.148 - 00:23:01.500, Speaker D: That's sort of a lot of.
00:23:01.692 - 00:23:03.108, Speaker C: That's a phrase you see in a.
00:23:03.116 - 00:23:06.516, Speaker D: Lot of papers to describe what you.
00:23:06.540 - 00:23:38.648, Speaker C: Get from looking at this gradient. And so this motivation inspired a lot of future work. One of them is smooth grant, and what smooth grad does is you perturb your input with gaussian noise and then you take this gradient and average that. And I actually cannot see this very well. But when you visually look at smooth red, it might seem to you like it's starting to capture a lot of oil. That's kind of corn.
00:23:38.776 - 00:23:40.964, Speaker D: It seems like even though you can.
00:23:41.424 - 00:23:43.656, Speaker C: So imagine, you can translate the smooth.
00:23:43.680 - 00:23:45.932, Speaker D: Grat picture onto the original image, it.
00:23:45.948 - 00:23:53.868, Speaker C: Seems like it's highlighting where the corners. There's another method called integrated gradients. And this actually also has a lot.
00:23:53.876 - 00:23:57.584, Speaker D: Of nice properties, which is you.
00:23:57.964 - 00:24:10.060, Speaker C: So what this does is you have a baseline input, and your baseline is supposed to capture, in some sense, zero. So it's supposed to capture when your model is not really doing anything with.
00:24:10.092 - 00:24:11.220, Speaker D: Respect to the input.
00:24:11.372 - 00:24:20.234, Speaker C: And then this method sort computes a path integral from the baseline all the way to the original input. And then you sum.
00:24:20.394 - 00:24:22.802, Speaker D: So the integral is essentially a sum.
00:24:22.858 - 00:24:36.586, Speaker C: But so you sum that. And the one nice property this has is the attribution you have to each dimension sums up to the final output. And so this is a completeness property.
00:24:36.650 - 00:24:41.258, Speaker D: That is in the paper, and it's actually kind of very desirable, some settings.
00:24:41.386 - 00:25:16.886, Speaker C: And so this is another kind of gradient based explanation technique. We can probably spend all of this workshop going through different kinds of techniques. So these are a few different ones. You don't necessarily have to try and figure out what each one is. So I've told you about gradients, smooth grad and integrated gradients, but there are several different versions. And this actually doesn't necessarily. These are some of the popular ones that people tend to use in the literature, but there's definitely several others.
00:25:16.886 - 00:25:21.686, Speaker C: And I've added the edge detector here. You can ignore it for now, but.
00:25:21.710 - 00:25:24.514, Speaker D: We'Ll get to exactly what that means.
00:25:25.174 - 00:25:48.654, Speaker C: And so there are broad classes of these explanation methods. Some of them, when you compute the gradient, they modify the rules as you're computing the gradient. So they might say something like, as you're doing this matrix multiplication all the way back to input, you should zero out negative activations, or you should zero out negative gradients. Or there are different rules about how.
00:25:48.694 - 00:25:52.022, Speaker D: You should propagate back the gradient.
00:25:52.198 - 00:26:13.080, Speaker C: In another case, lime is a technique that will fit. It tends to be a local linear model around a single input. And then the weights of that model is something you would use as your explanation. And so this is sort of a survey of the landscape. And there are other techniques as well.
00:26:13.112 - 00:26:15.040, Speaker D: That can be more sophisticated.
00:26:15.112 - 00:26:47.376, Speaker C: So this slide is supposed to show you that there are different ways you can visualize these things, and then there are also different ways you can learn them. So, for example, you could say, when I'm trying to fit a local model around my input, I want to sort of satisfy some properties. So you could say, I want to remove a continuous patch from this image, and I want that patch to be small, but I also want so that when I remove that patch, the probability of this image being corn decreases.
00:26:47.560 - 00:26:50.256, Speaker D: And so that's another way in which.
00:26:50.280 - 00:26:51.992, Speaker C: You could learn a model, and you.
00:26:52.008 - 00:26:54.004, Speaker D: Can visualize it this way.
00:26:54.714 - 00:27:33.620, Speaker C: Sort of broad survey of the kinds of explanations you could have in this literature, and you could actually use them to do interesting things. So this is a paper from pnas this year where they've used integrated gradients to analyze. So here they're different molecules. They have a data set with different molecules, and these molecules bind to a specific protein. And depending on the bonds, I'm not a chemistry major, but depending on the bonds you have, that would sort of affect the affinity of a molecule to.
00:27:33.652 - 00:27:35.148, Speaker D: Bind into a specific protein.
00:27:35.276 - 00:28:06.234, Speaker C: And so in this work, they built a model to predict the binding, and then you can attribute the bonds. And so red and blue here are telling you positive or negative, and so you can. And so the point of this is to say a lot of these techniques have been used in very different settings. And so the first challenge we have is if you. So let's say you're very interested in this area and you've trained your favorite.
00:28:06.934 - 00:28:10.510, Speaker D: Let'S say, inception, e three, or some.
00:28:10.702 - 00:29:06.840, Speaker C: Favorite model that you are interested in. And now you say, okay, I want to explain my model. The first challenge you have is how do you pick among all of these different methods, which one to use and which one of them seems like it's because they all seem visually different in some sense. And so which one of them captures what the model is doing? And so that's the first challenge. And so the first part of this talk would be trying to assess those different methods. And so I should talk about how this assessment was done previously. And so one way you might want to use to assess those methods is a lot of these methods give you a ranking, and then you would say, okay, I will take the top 10% of the dimensions and then remove those and then see how much my prediction changes.
00:29:06.840 - 00:29:26.592, Speaker C: And you can do this for all of your features, and then draw what looks like an RC curve, and then take the area under this curve, and you can take that as a measure of how well a particular heat map type is doing. Another popular way would be to, you.
00:29:26.608 - 00:29:28.960, Speaker D: Can train one of those localization.
00:29:29.112 - 00:29:33.000, Speaker C: You can train on one of those localization tasks. So if you are familiar with Ms.
00:29:33.032 - 00:29:37.496, Speaker D: Coco or any data set where you have to draw bound in boxes or objects.
00:29:37.600 - 00:29:48.384, Speaker C: So you train on a task like this, and then you say, how much does my heat map localize inside of this box? So these are some of the.
00:29:48.464 - 00:29:49.336, Speaker D: You have a question?
00:29:49.480 - 00:30:07.524, Speaker F: Yeah, I mean, I have a proposal of a very fast way of evaluating. Yeah, this is the way of saying it's good. And if we don't define what is the purpose of evaluation, I don't see the point of discussing different types of evaluations.
00:30:09.104 - 00:30:36.052, Speaker C: I will get to that point at the end of this, I can come back to you and then you can offer your perspective. And so these are two current ways that people interpret sort of do ranking for this method. And so this is where our first word comes in. So we wanted to, and remember, we're coming from this model, sort of debugging perspective. We wanted to tell, can we use a lot of these heat maps to.
00:30:36.068 - 00:30:38.580, Speaker D: Try and understand what kinds of mistakes.
00:30:38.652 - 00:30:59.944, Speaker C: And possible mistakes a model is making? So this is joint work. So Beanie's here. This is joint work with those people. So Justin, Michael, Ian Moritz and D. And so the inspiration for this, you might have seen some of this before. The inspiration for this was we're going to have two checks. One of them we'll call a model randomization test.
00:30:59.944 - 00:31:24.896, Speaker C: The other one we'll call data randomization test. And so, one possible desirable property of an explanation method would be if I mess up my model. So let's say I randomize the weights of my model, how well is this? What is the change in the explanation map that I get? So presumably, if an explanation map captures something about what the concept means.
00:31:24.960 - 00:31:27.160, Speaker D: So if an explanation map captures.
00:31:27.232 - 00:31:44.792, Speaker C: So this map captures what it means for that image, to be a corn. If I change the, if I randomize the model so that it's not predicting corn anymore, how much does my heat map change? That's the kind of property that this.
00:31:44.888 - 00:31:46.244, Speaker D: Check is trying to.
00:31:47.514 - 00:32:25.680, Speaker C: And so we can do this for a lot of different methods. And the option of this is there are some methods that regardless of the. So in this case, what we're actually doing is reinitializing the parameters of the network. So here, this is inception V three. And on the left, far left, you're starting from the top layer and going all the way to the bottom layer. So you re initialize all of the weights, starting from the top layer, all of the batch number parameters, all of the biases, all the way to the first layer. And for some methods, what we notice in particular guided backpropagation.
00:32:25.680 - 00:32:37.904, Speaker C: For some methods, what we notice is, regardless of change to the network in parameters, we don't observe. And we can quantify this with a.
00:32:37.944 - 00:32:39.824, Speaker D: Quantitative ranking as well. That I wish.
00:32:39.984 - 00:33:06.360, Speaker C: And so that's what you should take away from this slide. I told you that you can visualize these things in different ways. So here, this visualization is essentially showing you what parts of the image were most important, regardless of sign. So if you compute a gradient, you have some negative and some positive gradient. So here, this absolute value. So we're taking the absolute values of those gradients.
00:33:06.492 - 00:33:08.804, Speaker D: That's why you're looking at this in grayscale.
00:33:09.384 - 00:33:14.456, Speaker C: You can also look at if I actually visualize the. Yes.
00:33:14.600 - 00:33:18.272, Speaker A: So the column on the right is just completely random, this one?
00:33:18.368 - 00:33:19.832, Speaker C: Yeah, completely random.
00:33:20.008 - 00:33:21.424, Speaker D: Okay. Yeah.
00:33:21.464 - 00:33:55.736, Speaker C: So this is, if you visualize the positive and negative gradients, it can be a little bit difficult to see what's going on. So we, in the paper, we had some quantitative metrics that we looked at. We looked at. So this is two of them. So you can compute rank correlation. So a lot of this gradient explanation maps that will actually have a score for each dimension. And then you can compute the rank correlation between the explanation you get for the original model and the one of the randomized model and you computer, rank correlation without absolute values.
00:33:55.736 - 00:34:13.052, Speaker C: With absolute values. So the thing to take away from here is there are some methods that even in both cases don't change. And another interesting thing is when you reinitialize the signs, definitely they do change.
00:34:13.108 - 00:34:16.384, Speaker D: Which is what this flatness is capturing.
00:34:17.484 - 00:35:00.599, Speaker C: So the change in signs definitely, definitely happens. But this can be very difficult. Yes, this can be very difficult to captured. So I have ten minutes left, which means I will now rush through a lot of the remaining slides, but so you could sort of say, perhaps this is some arbitrary measure, but think of a case. So this task is a task where you have radiographs and the radiographs are here, you're trying to determine the age from a radiograph. And so you train, you train. In this case, we train an inception v four resonant version on this radiographs.
00:35:00.599 - 00:35:40.986, Speaker C: And the method that I told you about before, the guided back propagation, that doesn't change very much. You can sort of visualize guided back propagation with original model, and then you can visualize again with all of the weights, all of the biases, everything completely reinitialized, and the visualizations look almost indistinguishable. And so the point from this work. So the point from this work is we have another test called the data randomization test that was also getting, that was also getting at this. And the point here is visual assessment of justice. Heat maps can actually be very deceiving because the methods that might look the.
00:35:41.010 - 00:35:46.858, Speaker D: Most visually appealing might not necessarily capture what the network is doing.
00:35:46.946 - 00:36:09.014, Speaker C: And there's this really nice work from Nietzsche at ICML, the last ICML, where they theoretically analyze three of these methods. So they theoretically analyze gradient guided backpropagation and a third one. And they actually prove that the gutted backpropagation method essentially reconstructs the inputs.
00:36:09.314 - 00:36:13.824, Speaker D: So they do this on one layer.
00:36:14.724 - 00:36:58.916, Speaker C: On a network with one ending layers. And so I will now rush through a lot of the presentations. So recent work has also shown that there's a potential fix for some of this sanity checks that we proposed. Essentially, the fix makes it so that each dimension now essentially is attributed to only one output, one output class. And so one thing you might be able to take, so one thing you can take away from this is, are the sanity checks actually useful in some sense? The sanity checks essentially capture whether you can rule out a method. They don't actually tell you whether a method is good. So there's still more work to be done in that area.
00:36:59.060 - 00:37:04.624, Speaker D: And I've included here some of the interesting recent work in this area.
00:37:05.404 - 00:37:42.320, Speaker C: And so there's also some work that you can attack this methods. I will actually skip some of this. The upshot of this is that you can make it so that you can make it so that it seems like the prediction of a model doesn't change, but the explanation changes. I don't know if you can see this, but top is an explanation for a normal network, and then you can essentially do analogs of the adversarial attacks. So the fast gradient sign method, you can have the fast gradient, that method for explanations. And in that case, you can potentially.
00:37:42.352 - 00:37:45.484, Speaker D: Fool some of this, some of these models.
00:37:46.424 - 00:37:58.632, Speaker C: So the key takeaway from that part is that we're just getting to the place where we're able to actually assess some of these methods. And there's a lot of recent work.
00:37:58.688 - 00:38:02.696, Speaker D: That is getting at the CSV. Yeah.
00:38:02.720 - 00:38:32.080, Speaker E: I mean, isn't it sort of taking a little bit of the character of, like, arguing with the religious. So you have something that you already know is insane and that they don't have a. There's not a clear. For any of these methods. There's no clear definition of. There's no clear problem statement for which they are the solution, and there's not even a pretense of one. And I think it takes only a few minutes to sort of see how there can't even be, like.
00:38:32.080 - 00:39:16.884, Speaker E: You make no assumptions about what the model surface looks like. You give me any model, I'll give you any explanation you want for any point. If by explanation you define it as just the way the gradient points. So in this world where they're completely agnostic about the model, we already know that it says that the saliency map tells you nothing. And so on one hand, I think it's a really nice paper that shows, like, kind of makes this point, but are we also at risk kind of being in a world where we're trying to combat insanity with sanity checks? And there's only so far you go, they just will come up with, you know, like, if you don't force someone to actually make a statement about what the model is doing, not just so a negative argument of, like, what it's not doing.
00:39:19.004 - 00:39:42.500, Speaker C: I see your point. I think it's. If you're working in this area and let's say you're coming up with a new postdoc method, I don't necessarily think that person will agree with your take that it's impossible to be able to explain these methods. I think it's a very attractive view to say you can train any model you want, and I will come in.
00:39:42.532 - 00:39:45.922, Speaker D: With a method to try and try.
00:39:45.938 - 00:39:54.746, Speaker C: And explain it, even though you're making it seem like it's a sort of a crazy or. I think your word was arguing with that.
00:39:54.770 - 00:39:55.826, Speaker E: I think it's religious.
00:39:56.010 - 00:39:57.410, Speaker C: Yeah, exactly.
00:39:57.562 - 00:40:13.954, Speaker E: A statement of what it's doing. As long as they never say anything specific enough for it to be a claim that you can refute, as long as it's never something that you can invalidate. There's no falsifiability, then they're just sort of immune to logic.
00:40:14.614 - 00:41:05.692, Speaker C: Yeah, I mean, I think coming to the defense of some of this work, I will say there is a lot of work trying to. So trying to go towards that path. So there's a lot of work. And being asked one of those papers where they're defining metrics for which we can use to try and assess those methods, I will say right now, it seems like there's not like a, like a metric you can pin down to say, this is what the gradient should tell you about this model, or this is what this explanation map should tell you about this model. So we're not necessarily, we might not necessarily be there yet, but there might be a path towards that. And trying to, especially because a lot of those heat maps are very attractive from a visual perspective. And so even if there's no metric, they can still get wide adoption.
00:41:05.788 - 00:41:06.380, Speaker D: Right.
00:41:06.532 - 00:41:10.344, Speaker C: And you might want to be assessing them and sort of saying, here is.
00:41:11.004 - 00:41:17.464, Speaker D: Potentially some methods that fall short. I think it's still an interesting path.
00:41:18.044 - 00:41:19.344, Speaker C: Well, you're saying just.
00:41:19.764 - 00:41:37.320, Speaker E: Well, I think that's sort of their danger, right? I don't think it's their attribute. It's not a virtue, but a bias that it's like there. But they're supported by Zykanti. And, you know, we can make many pretty pictures that have no clear stated significance. But if people find them visually attractive.
00:41:37.432 - 00:41:38.504, Speaker B: We'Re getting a little low on time.
00:41:38.544 - 00:41:39.312, Speaker A: So maybe we should.
00:41:39.368 - 00:41:40.944, Speaker E: Yeah, maybe we should chat offline.
00:41:41.064 - 00:41:47.912, Speaker C: Sounds good. So this part I will actually rush through. So five minutes?
00:41:48.048 - 00:41:48.504, Speaker A: Five minutes.
00:41:48.544 - 00:42:31.106, Speaker C: Okay, cool. So the motivation. So the second part of the talk is about privacy and why you might, and why local explanations might be very tricky to deal with from a privacy perspective. And so let's say you're like I mentioned earlier, you're like a company, and you've developed this model, proprietary model that you think is very interesting and you want to sort of sell it to other people. And it takes inputs and you give outputs to some kind of API, and then you can also give explanations as well. This is, is one of the motivations for this case. And what is actually interesting here is given the predictions and the explanations is very problematic from this standpoint.
00:42:31.106 - 00:43:49.114, Speaker C: So there's this quote that I really like from Dork and Roth. There's a book on differential privacy, and it says overly accurate answers to too many questions will destroy privacy in a spectacular way. There's a work, there's a lot of work now that's shown that when you have access to those predictions through an API or even explanations, it's very easy to recover the model. And with this local explanations, actually it's easier. You can do this model recovery with fewer queries. And so if you're coming from the perspective of the company, you might want to prevent something like this. And I mentioned this earlier, there are other techniques that look at can I rank the training points so that I can, by their effects on a particular test loss? And so if you think about this from the perspective of privacy, and this was mentioned also at the show, create all paper with a few different diverse examples and with enough training points that I've shown, you essentially can capture most of the information in the potentially proprietary data set that was used to develop this proprietary model.
00:43:49.114 - 00:44:32.892, Speaker C: And so these are potentially not desirable from a privacy standpoint. And the privacy standpoint what you're looking at here is differential privacy. So some of you might have heard about differential privacy. And so it captures the notion that if I have, let's say I have two neighboring databases and they differ in one. So you can think of it as one row, the different one row. I don't want the output of my algorithm, my randomized algorithm, to change too much. And so, because if it doesn't change too much, then if you try and do an hypothesis test for whether this person is in a database or not, the power of that test is not necessarily, does not reveal, does not reveal that.
00:44:32.892 - 00:45:31.870, Speaker C: And so this is a very nice definition of privacy. And one of the things that you can actually see that a lot of these local explanations violate in this case, is you're offering an explanation for particular data points that's potentially even in your training, in your training data. And so this can be problematic from a privacy standpoint. And so can you potentially use differential privacy in some way to sort of stem this issue? And so there's this paper from 2016 that sort of made SGD differentially private version of the SGD. And in short, what you do is you clip the gradients, the per example gradient of each input, and then you add noise. And so if you do this, you can train a differentially private CNN on MNS. And the epsilon for differential privacy captures how much privacy you have.
00:45:31.870 - 00:46:05.778, Speaker C: So smaller epsilon is more private model. And what I'm showing here is. So this is a model on MNIst, and each column are different values of epsilon. And the option you should take from this is if you're so this is still ongoing work. So there's some more issues to be handled here, but if your model is sufficiently private, the local explanations you're releasing look like noise, and they might not necessarily even capture the property that you want to, which is you want to provide information to a particular end user.
00:46:05.866 - 00:46:07.814, Speaker D: And they might not necessarily capture that.
00:46:08.354 - 00:46:32.526, Speaker C: So to conclude, there are sort of two potential trade offs that I've sort of tried to communicate to you. One is that it can be very difficult to assess some of these explanation methods, and we've seen some recent work, even beyond the sanity checks that are trying to capture this. And another one is explanations are very tricky. Especially local explanations are very tricky to.
00:46:32.550 - 00:46:36.158, Speaker D: Deal with because in a privacy context.
00:46:36.206 - 00:46:39.470, Speaker C: Because they reveal the model, and this.
00:46:39.502 - 00:46:46.834, Speaker D: Might be undesirable from the perspective of a land user. And I think that's the end of the presentation.
00:46:51.544 - 00:46:53.804, Speaker B: One quick question while beam sets up.
00:46:55.824 - 00:47:26.724, Speaker A: So there seems to be a lot of external observers, a lot of overlap between some of these explainability methods and adversarial robustness and so on. So it seems like these two things should be thought of. They seem very related issues.
00:47:26.804 - 00:47:37.876, Speaker C: Right? Yeah, there's actually. So I did not talk about any of that, but there's a lot of work in that line as well. One strand is if I train. You might have seen this from some.
00:47:37.900 - 00:47:41.068, Speaker D: Of the Mavi presentations, if I train.
00:47:41.116 - 00:48:09.910, Speaker C: More robust model models. Robust in a sense that more adversarially robust. If you look at some of those gradient maps, they look more interpretable if you use interpretable as usually similar to inputs. And so there's actually a lot of work in that line. And there's some. That said, if I give you like a gradient map or some kind of map, I can produce some adversary examples from that. So there's definitely a lot of work in there.
00:48:10.052 - 00:48:11.414, Speaker D: I just didn't talk about.
00:48:13.434 - 00:48:13.994, Speaker A: Speaker again.
