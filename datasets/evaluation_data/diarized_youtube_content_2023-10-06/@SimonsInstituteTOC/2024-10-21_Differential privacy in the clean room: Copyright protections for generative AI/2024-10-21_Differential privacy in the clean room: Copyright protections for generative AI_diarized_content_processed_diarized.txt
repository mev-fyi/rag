00:00:01.280 - 00:00:16.565, Speaker A: Oh, there's one for the video, one for this room.
00:00:18.105 - 00:00:26.285, Speaker B: All right, so for the last talk in the morning, we've got Aloni Cohen will be talking about differential privacy and copyright protection. So please take it away.
00:00:27.205 - 00:00:56.355, Speaker A: Thank you for having me. I want to say that this is a bit of an odd duck on the alignment day. Copyright day is tomorrow is my mouse in the window. So I will not sort of say everything that needs to be said about copyright, but you should come tomorrow if you care more about copyright. But let me start with this. This is a page that's for. From the New York Times lawsuit against OpenAI.
00:00:56.355 - 00:01:20.695, Speaker A: And they sort of. This is their evidence. If you prompt the model with this first phrase from an article of theirs, this article, the output from ChatGPT4 is this thing, or GPT4 is this. And it's word for word, the same as this New York Times. So this is like the sort of thing I'm talking about. And we don't want this. Let's start with that.
00:01:20.695 - 00:01:57.185, Speaker A: So let's say that you're using ChatGPT and you want to write a blog post, or you're a student, you're writing an essay, and you want to use ChatGPT to help you write something. So write a news story about John Fosse winning the 2023 Nobel Prize in literature. And so this is what it spits out yesterday. This is what it spit out for me. And now here's the question. How confident are you that this is not copying this New York Times article? Right? This is. This is the New York Times article from this or this.
00:01:57.185 - 00:02:26.355, Speaker A: I don't know what this is. The Guardian or something, right? These are a couple specific articles. You could check ChatGPT's output against these couple articles. But there's more than just a couple. There's 5,000. And so there's 5,000 articles that exist in the web on Google about this event, right? And you're writing your own, and you want to be pretty sure that you're not getting into hot water. That's the thing I want to think about.
00:02:26.355 - 00:03:06.105, Speaker A: So how much would you bet on it? How confident would you be? And I would say that Maybe you trust OpenAI and how they built the system and that they're doing a good thing, but we have sort of no reason to bet highly on it. Okay, so that's an example. Any questions about this example? Here's a different example. Instead of John Fossey winning, oh, this should have said Geoff Hinton. Something went wrong. What if I'm writing a news story about Geoff hinton Winning the 2024 Nobel Prize in Physics. There's 200,000 results.
00:03:06.105 - 00:03:36.215, Speaker A: How confident would you be that the output of write a story about Geoff Hinton winning the Nobel Prize is not copying any of these 200,000 results? Very confident. Why? Who said that? It's not in the training data. Right. These are the same prompt but very different situations. Right. So I would be quite confident that this, that these 200,000 things are not in the training data. The training cutoff date was to 2023.
00:03:36.215 - 00:03:54.225, Speaker A: And so these. None of these articles are in the training data. None of them. We know that for sure. And so it can be quite confident that there's no copying. Yeah, well, that's a different system. Like it's an entirely different setting.
00:03:54.225 - 00:04:18.015, Speaker A: Yeah, that's right. You could also, you could be copying something else. Right. Like there could be a paragraph in the output that copies some profile of Geoff Hinton from a year ago. Right. There's plenty of profiles about Geoff Hinton before he won the Nobel Prize, but you could be pretty confident that you're not copying these. So here's the question.
00:04:18.015 - 00:04:41.485, Speaker A: Can we make this query as safe as this query? This is what I want to do and I come from a theory background. Thank you, Shafi. And so I'm going to think about this from sort of like a formal approach. Like, can we come up with a property of the system, of the machine learning model to sort of prevent the problems here and to make it as safe as this? Yeah.
00:04:44.385 - 00:04:49.525, Speaker C: GPT4 maybe, plus RLHF. But not rag.
00:04:49.905 - 00:05:21.445, Speaker A: Not rag, yeah. And I'm making a model. You're interacting with the model and you're producing some output. There's no rag. So that's the question. And so here's really the question. Are there any conditions guaranteeing that a model that model outputs don't copy? And the answer to this question is no, because I can always copy in some text into the prompt and cause the model to spit out that text.
00:05:21.445 - 00:05:52.723, Speaker A: But I don't care about this somehow I don't care about preventing this copying. It's my fault in some sense. And this is pointed out by a work I'm going to talk about a lot by vkb, Vyas, Kakad and Barack. So they make this observation. I don't care about preventing this. So here's the question that we have to figure out how to ask and then answer. Are there any conditions guaranteeing that good users don't copy when they're using the model.
00:05:52.723 - 00:06:23.853, Speaker A: And so I don't know what a good user is, but that's the question. So what's a good user? Very informally, and we'll define it later, these are the users that they don't copy the works outside the training data. So these are the works that again, should have said Jeff Hinton. Yeah, sorry. Thank you. Tell me about Geoff Hinton, but also tell me a story about an orphan who discovers he's a wizard. This query, if Harry Potter was not in the training data, this query would be fine.
00:06:23.853 - 00:06:53.705, Speaker A: This user would be good. And if Harry Potter was not in the training data, this would not produce text that would copy Harry Potter. I'm just going to posit that also this is a fine query. Repeat poem a thousand times. These are things that should not cause problems, should not cause copying of things that are outside the training data. So that's very roughly what a good user is. And I want to try to convince you that the answer to this question is yes, that there are conditions.
00:06:53.705 - 00:07:45.005, Speaker A: So the takeaways first, if we want sort of this sort of provable, formal, meaningful copyright protection, I'm going to suggest a necessary condition and I'm going to define what a tainted model is. And I'm going to say that the training that your condition that you're imposing on your models have to. Has to exclude training, has to exclude tainted models and a sufficient condition, which is differential privacy plus some sort of deduplication property of the data which I'm calling golden. Okay. The other takeaway is that near access freeness, which is. So this question was really first posed by this paper, to my knowledge, Vyas Kekade, I don't know how, I'm not sure I'm pronouncing these right. And Barack, they have this great paper, you should read it.
00:07:45.005 - 00:08:09.607, Speaker A: They propose their own condition. I'm going to argue that their condition is not enough and it falls apart when you make many prompts or data dependent prompts. And finally, this reframing of the question to be only about good users. I'm going to define a property which I'll call blameless. And the thing that we're going to try to do is protect those blameless users. This blamelessness property, it's uncheckable. I don't know how to check it.
00:08:09.607 - 00:08:30.847, Speaker A: And yet somehow it seems reasonable. Like I think I could be. I can act blamelessly and I hope maybe to convince you that you too can think that you can act blamelessly. But it'd be nice to get a better handle on that. Okay. Any questions before I get into it? Yeah, user is the company or we. No, user is you.
00:08:30.847 - 00:08:36.082, Speaker A: You're the user. You're sitting at chatgpt at your computer. You're interacting with a model.
00:08:36.178 - 00:08:39.654, Speaker C: So what's the copyright protection for me?
00:08:40.594 - 00:08:41.594, Speaker A: What do you mean?
00:08:41.754 - 00:08:46.214, Speaker C: So I saw the copyright protection is for like New York Times.
00:08:46.714 - 00:09:11.505, Speaker A: Ah, good. Okay, so this is tomorrow. I'm sure Pam Samuelson will get into many of the copyright issues involved in Generative AI. There's like many different questions you might care about. The one I'm concerned about is the output of the model or really the output of the user of the model infringing on some work in the world. Right. So you're the user of the model.
00:09:11.505 - 00:09:22.425, Speaker A: You're trying to write a story about Geoff Hinton winning the Nobel Prize. And what you're worried about is that the thing that you produce is too similar to the New York Times article and that you're going to get sued.
00:09:22.585 - 00:09:23.137, Speaker C: I see.
00:09:23.201 - 00:09:30.425, Speaker A: Yeah. So that's the setting one and two. Yeah, I'm going to talk about that in a second.
00:09:31.325 - 00:09:45.325, Speaker D: So just make sure you're saying high level. If ChatGPT puts a lot of copyrighted data there. I'm not copyrighted data kind of verbatim from New York Times is a training data. And don't train the model. Well, it just spits it out.
00:09:45.365 - 00:09:46.397, Speaker A: It's not my fault.
00:09:46.501 - 00:09:48.485, Speaker D: I'm like free, you know, to use.
00:09:48.525 - 00:09:54.021, Speaker B: All the things the system to be differentially private and then that works.
00:09:54.053 - 00:09:54.397, Speaker D: Oh, I see.
00:09:54.421 - 00:10:03.277, Speaker A: So I want to make it so that if you're acting in like in a kosher way, you're the user that the output won't copy the New York.
00:10:03.301 - 00:10:05.357, Speaker D: Times without cooperation of ChatGPT or with.
00:10:05.381 - 00:10:11.413, Speaker A: Cooperation with cooperation of Chat. So I'm going to impose a condition on the model.
00:10:11.589 - 00:10:12.997, Speaker B: Let's impose a condition and on the.
00:10:13.021 - 00:10:37.545, Speaker A: Data and on the user. Right, because. And we have to impose a condition on the user because the user can always induce a model to produce copy copyrighted outputs. So this is different than the lawsuit. This is different than the lawsuit. But yeah, yeah, it's different than the lawsuit. And I'll let Pam get into the lawsuits.
00:10:37.545 - 00:10:56.745, Speaker A: I don't have any power. I don't have the right to let her. But she will get into the lawsuits. Okay, so this is the takeaways. Yeah, I'll tell you, I'll tell you eventually. Yeah, all the words in quotes. I'm gonna define.
00:10:56.745 - 00:11:25.175, Speaker A: I will try. Okay, so the law. I'm not gonna tell you everything about copyright law. But I think this is your question again. Tomorrow. I'm gonna tell you two very simple things about copyright law. One is that to copy, to show that somebody copies, you have to show that that person, the defendant, has access to the copied work and that the thing they produced is substantially similar to the copied work.
00:11:25.175 - 00:11:56.415, Speaker A: Right? So if I produce something and J.K. rowling comes after me and says, you copied Harry Potter? Just to show that I had access to Harry Potter. Everybody in the world has access to Harry Potter and that the thing, the story I produced was substantially similar. I'm not going to define this. I'm not going to get into what substantially similar is. But let's just, you know, at the end of the day, there's some formal. There's some, like, legalistic tests that judges apply, and also 12 people in a jury block supply, like, this is a human judgment subject to some doctrine.
00:11:56.415 - 00:12:28.733, Speaker A: Can we talk about that later? I mean, the answer. The answer is yes, but it's a preponderance of the evidence and the show. Like, the evidence needed to show that access is not actual access. It's like, it's very circumstantial evidence. The evidentiary standards don't actually require an actual showing. It's circumstantial access. Like, was it in the Billboard top 100 songs? Like, that might be enough.
00:12:28.733 - 00:13:01.525, Speaker A: If you're in the right radio markets, actual access would be great. But usually there's not evidence of actual access. Okay, the second thing I want to tell you about copyright law is that ideas are not protected, right? So I'm allowed to write a story about an orphan who discovers that he's a wizard and gets whisked off to wizarding school. This is totally allowed. I could read Harry Potter and then write that book. There's no problem because the ideas are not protected. The particular expression of those ideas are protected.
00:13:01.525 - 00:13:32.233, Speaker A: Okay, and for more tomorrow. Okay, so an example that sort of distills both these points is this idea of a clean room. So a clean room is something, I imagine I'm taking some data, whatever I take. And this is a painting by Marc Chagall. And I take all this stuff that doesn't depend on the painting, and I stick it in a room with a user who has never seen this painting. And I say, I tell them to produce something. Produce your own painting.
00:13:32.233 - 00:14:07.005, Speaker A: This user constructively doesn't have access. And so whatever they produce, even if it sort of looks similar and it's about a fiddler and a shtetl is not Infringing. This is true. Even if we give the user some of the ideas of the work, like the color scheme and it's a fiddler in the shtetl, and do it in the style of Marc Chagall, as long as the particular expression never makes it into the clean room, this is fine. But constructively great. And this is a driving intuition for us. And this is a driving intuition for vkb.
00:14:07.005 - 00:14:10.985, Speaker A: Right. So they have this idea. Yeah, go ahead.
00:14:11.965 - 00:14:22.071, Speaker B: You're like distilling this function. Yeah, it phrase it as like. Okay, even the prompt is construct something in Chagall style, Right?
00:14:22.103 - 00:14:22.599, Speaker A: Yeah.
00:14:22.727 - 00:14:29.151, Speaker B: So what if that box contained other Chagall paintings? Would this not satisfy the definition of copyright infringement?
00:14:29.183 - 00:14:36.675, Speaker A: No, this would. That would not be infringement. Style is not protective. That's exactly right.
00:14:37.575 - 00:14:43.479, Speaker B: But then it seems to me like you're making a distinction between one versus two works. And I am.
00:14:43.527 - 00:14:54.625, Speaker A: And I'm not going to be satisfied with that, like, person. Like, I'm not going to solve that problem. But I agree with you. But the law doesn't protect style.
00:14:58.885 - 00:15:02.985, Speaker B: There are like some famous examples of, like, paintings that actually come in segments.
00:15:03.525 - 00:15:10.305, Speaker A: Sure. But then you might argue that these are like a part of parts of a whole. You know, you could try to make some argument. Right? Yeah.
00:15:12.495 - 00:15:14.095, Speaker D: How far should ideas function?
00:15:14.175 - 00:15:31.975, Speaker A: No, this function. I'm really not. I'm going to use things that are obviously like this. This is clearly the ideas and not the expression. That's all I need. I'm not really going to compute this thing. Something that is, we can all agree is the ideas and not the expression.
00:15:31.975 - 00:16:06.535, Speaker A: That's all I'm going to need from that function. Okay. So this idea of VKB is that if I take this data that excludes this painting and I run a training algorithm on it, then the model that results is safe for this painting. That this model somehow constructively never had access to Chagall's the Fiddler, and so it can't infringe on Chagall's the Fiddler. And so a user interacting with this model should be safe. That's the intuition of vkb. So I'm going to define.
00:16:06.535 - 00:16:29.887, Speaker A: We're going to have a definition before that, some notation. So I have some data. D. A particular copyrighted work is C, the set of all copyrighted works. This is the logo of the copyright office is this big C. I train a model P, user interacts with the model. The model produces an output.
00:16:29.887 - 00:16:56.725, Speaker A: And we'll use this notation that with the model P, the probability of some event conditioned on the prompt. Right. So the probability of this, the output starting with once upon a time on the prompt Tell me if tell me a fairy tale is maybe 25%. That's the notation. And the thing that we care. The thing that VKB cares about is whether the generation, the output of the model is sort of similar to something in here. Okay.
00:16:56.725 - 00:17:21.267, Speaker A: And I'll point out that the data set can be much smaller set than. Right. Might not contain all copyrighted. Okay. So near access freeness is this definition put forward by vkb. And so they look at the model that's trained on the data and this safe model that's trained. I'm going to sort of wave my hands trained on a data set where I've excluded.
00:17:21.267 - 00:17:49.829, Speaker A: I've excluded this work. It's sort of like a data set that you could put in a clean room if you were trying to protect against this work. Okay, I'll denote that D minus C. And they say if these two things are close, then the output of the model is unlikely to be similar to the work I'm trying to protect. That's the intuition. This is the sentence. The defendant's work is close to a work which was produced without access to the plaintiff's work.
00:17:49.829 - 00:18:28.271, Speaker A: So this is what you could produce without access. And if you use this, you'll be close to using this. I'm going to define, say the definition a second. But here's what the definition gives you. This is the important part, that if the model is NAF with some parameter K, then for all copyrighted works and all events, the probability of that event on prompt X happening with the blue model is not much more than the probability of that event happening with the green model, the safe model. How much? Not much more to this sort of multiplicative factor. 2 to the k where K can depend on the prompt.
00:18:28.271 - 00:18:31.235, Speaker A: Different prompts can have these different blocks. Yeah.
00:18:32.655 - 00:18:49.995, Speaker C: I don't know how the law deals with counterfactuals. And do you have any sense whether. Because this intuitively makes sense to me. But it sounds like it comes down to whether there was access or wasn't access. So like, even though I know probabilistically both of these.
00:18:50.415 - 00:19:08.251, Speaker A: So, so you're. I'm going to not comment on that more than the following. The argument in VKB is an argument from access. I think it's the wrong argument. I think the argument that they're really making is one, from substantial similarity based on what I'm about to say. Let me just. Let me give.
00:19:08.251 - 00:19:48.037, Speaker A: Let me finish. This and then I'll take your question. Yeah, so I think this argument from axis is the wrong argument, but we can talk about that later if you want. So with the example is, if I plug in for this event E, all the works that are substantially similar to C, anything the model could have output that would substantially similar C. Take that event. The probability of the output being similar to C here is not much more than it is here. And what you would expect in some sort of intuitive way is that for reasonable users, this is small.
00:19:48.037 - 00:20:12.189, Speaker A: This is exponentially small in the length of the work. If there's entropy in creative works, then this thing should be very small, and therefore this thing should be very small. This is their argument. This is not true for arbitrary X. Right. If I, again, if I prompt X in the, in the worst possible way, I can get it to do whatever I want. But sort of for reasonable users and reasonable prompts, this should be small.
00:20:12.189 - 00:20:14.301, Speaker A: That's the intuition Ankur question.
00:20:14.373 - 00:20:17.597, Speaker B: Yeah, actually, I just want to try to understand the lemma statement.
00:20:17.741 - 00:20:19.197, Speaker A: Yes, Getting water.
00:20:19.381 - 00:20:28.071, Speaker B: I mean, there's the subtlety to like, you know, little C versus big C. Little C being like the individual thing that someone is suing me over.
00:20:28.183 - 00:20:28.527, Speaker A: Yes.
00:20:28.591 - 00:20:30.279, Speaker B: And then big C being all of them.
00:20:30.367 - 00:20:30.751, Speaker A: Yeah.
00:20:30.823 - 00:20:43.215, Speaker B: So just to make sure I understand, like, okay, if the model, like outputted half of like one painting and half of another painting, this would still be covered by this definition.
00:20:43.295 - 00:21:08.055, Speaker A: Yeah, it depends on this E definition because of this substantial. Yeah, that would be substantially similar to both. Yeah. Okay, good. And so what is their definition? They say that the model is kxnaf. Again, this depends on the prompt with respect to this whole set C and some function safe. This is like some way of scrubbing the data, some way of removing this and then training this.
00:21:08.055 - 00:21:48.915, Speaker A: It's NAF with respect to this set of copyrighted works. If the max divergence between the blue and the green model is at most K X on prompt X. So when I prompt the model with X, the most, the probability changes is by 2 to the K. Right. I mean, this is exactly the definition that gives you this. It's the max overall outputs of the log of the ratio. It's exactly the thing that you need to prove this bound.
00:21:48.915 - 00:22:47.667, Speaker A: Rearrange this and you get max and you can, if you have some uniform upper bound for all prompts, then you just call this K. So this is naf. And the great thing about NAF is that you can do it. You can transform any training algorithm into an NAF training algorithm in a Black box way, which is something you can't do with differential privacy. So this is really nice, I don't want to get into the details, but you shard the data, you train two models with whatever your underlying training algorithm is. And what you do at generation time is you return something, you sample from the minimum of these two probability distributions or you sample proportional to the minimum. And the theorem is that this thing is NAF and the parameter depends on the total variation distance between these two distributions.
00:22:47.667 - 00:23:19.585, Speaker A: So like the sharded, the sharded model trained on the Shard D1 and the model trained on the Shard D2. So this is their theorem, this is the main algorithm in their picture. Take half the data and half the data and as long as the data is sort of nice and there's not copy of Harry Potter and both you should be ok. And this is great because it's black box. It's really nice. Okay, so now the question that we're starting with. Does NAF guarantee that good users don't copy? And I claim the answer is no.
00:23:19.585 - 00:24:00.225, Speaker A: We have two counterexamples. One is I can construct an NAF model where the user does the following queries. The prompt 1 and then the prompt 2 and then the prompt 3, all the numbers keeps prompting the things eventually and just averages the output. So this thing is going to return something very close to noise every time. Average all the outputs together. Bada bing, bada boom. Reconstruct any piece of the training data and another example where the user can query the title of the work and get back the work if the work was in the training data.
00:24:00.225 - 00:24:39.105, Speaker A: And both of these are consistent with naf. And so I hope you share my intuition that these users, like in a clean room, they would be okay. They're not importing the copyrighted work with them, so they're good under my definition. And so what goes wrong in the definition is that in this one, this KX is infinite, so the bound is trivial. In this one there's no post processing guarantee and there's no sort of multiple queries guarantee. So composition breaks this. That's what's going wrong technically at the level of definition.
00:24:39.105 - 00:25:04.619, Speaker A: So now to Scott's question. I wanted to try to tell you a necessary condition. What's a tainted model? And I'm going to say we have to exclude these and NAF is not going to exclude these. That's where we're going. Average the outputs, take the generations, take their average. Okay, let me, let's. It's okay, it's not important.
00:25:04.619 - 00:25:09.667, Speaker A: Like some very bland, some very bland post processing of the bit strings.
00:25:09.811 - 00:25:12.579, Speaker B: I mean, literally you just take noise on the picture and then.
00:25:12.627 - 00:25:36.277, Speaker A: Exactly. A picture, text, whatever you want. Like the training day, this is just going to return like a random string, very slightly biased in the direction of the training data. So take a bunch of queries, average the noise away. Okay, so tainted models. So I want to try to define what's tainted. And so this is the unnecessary condition.
00:25:36.277 - 00:26:12.297, Speaker A: The NAF is going to fail. And now we have to get to what's a good user or what's not a good user. So I want to claim that, let's say this user makes this query, like recreate this picture. So I suggest that this user is good or bad, depending on whether this picture exists in the world, is a copyrighted image. Right? So it's bad if this is copyrighted and it's good if this is not copyrighted, if it's independent of all the copyrighted works in the world. Right? If this user was Marc Chagall, if this user created this image, this would be a fine query. So it depends on which world we're in.
00:26:12.297 - 00:27:01.595, Speaker A: By world I mean the set of copyrighted works. And this is a, this is a claim, this is like a belief, a postulate, that any fixed user, user is an algorithm that interacts with the model. Any fixed user is good in some, with respect to some set of copyrighted data. Like they can't, the user can't be to blame for importing all possible copyrighted works in all possible worlds. Right? That's, that's my belief. And what that tells me is that here's a bad thing is if there's a fixed user that copies in all worlds, like for all C, this fixed user manages to reproduce something in C that would be bad, that would be a failure. And so this is what tainted is.
00:27:01.595 - 00:27:31.457, Speaker A: But let me now define the user's output distribution. So the user's interacting with this model. It outputs something called Z. I'm going to use tau to denote the user's output distribution. So the probability is over training the model, querying the model and producing an output. The user can be randomized. Tau E is the probability that at the end of the day the user produces something in this event E good.
00:27:31.457 - 00:27:58.765, Speaker A: So a tainted model, really a tainted training algorithm, is one for which there's a user that copies in all possible ones, which is to say all possible sets of copyrighted training data. Of copyrighted data. So train is tainted. If there exists a user such that for all Data sets for all data sets and all items in that data set. Oh, sorry. This should be a. There exists.
00:27:58.765 - 00:28:39.305, Speaker A: There exists a work in that data set such that the user produces that work verbatim with 99% probability. This would be really bad. There's a fixed user such that whatever data you trained your model with, the user is able to pick out a work from that training data. This is the bad thing. What's a stupid example of this? Take your training out, take whatever training algorithm you want and build a model and then augment it with some sort of key value store that just maps ideas of the training data. Or if you want like the title of the data to the data itself. Right.
00:28:39.305 - 00:29:18.765, Speaker A: And so at inference time, a generation time, you take a prompt, you look up this prompt in the table. If it's like the name of a work that you trained on, just spit out the work. Otherwise sample from the underlying model. This thing is clearly tainted. What does the user do? Inquiries the name of a work in the training data and is able to reproduce it. Good. So I claim that if I want to satisfy this some condition guaranteeing that good users don't copy, then a necessary condition is to exclude these tainted model tainted models that come out of tainted training algorithms.
00:29:18.765 - 00:29:53.903, Speaker A: And NAF allows tainted training models. Sorry, tainted model with exactly these pictures. So for time I'm not going to get into the into. I'm not going to do the details carefully. This is the one I said before. Every query is allowed to leak k bits of information and so the user can do an averaging over the queries to reconstruct the whole training data set. Yeah, one fixed query.
00:29:53.903 - 00:30:30.001, Speaker A: One fixed query. It doesn't talk about many. It doesn't talk about many. The second example is that the NAF algorithm I showed you before, this is the main algorithm from the paper. You shard the data, you train, you do the proportional to the main thing. If I plug in the training algorithm, I just showed you the tainted training algorithm that augments with this key value store. If I just plug that in directly to their, to their results, the results sort of, you can look up any entry in the training data.
00:30:30.001 - 00:31:13.535, Speaker A: It'll just spit it out just as well as this will. And this was sort of the observation that led to this. This example is from Thomas. Okay, any questions before I leave this aside? Because I'm going to leave this aside right now. The key value should only be in one of the charts. Yeah, so it's exactly because I'm taking the mean of two things and then I'M normalizing, right? So on the prompt Chagalls the Fiddler, one of these distributions is going to be zero everywhere except for on the fiddler. And once every normalize all the probability masses on that one output.
00:31:13.535 - 00:31:27.011, Speaker A: Because I'm like zeroing out this distribution everywhere. When I take them in on the shard that has the data, I'm zeroing out the probability everywhere except for one work, one output. And then when I renormalize, that's the only thing that's ever.
00:31:27.163 - 00:31:28.891, Speaker C: So when this be solvable by this.
00:31:28.963 - 00:31:55.317, Speaker A: Slightly more capable, sure, but I'm. I'm interested in like a definition that, like what's the definition that I, that I should be targeting when I'm being slightly more carefully normalizing my models. Good. So we're still at this question. This is the question we want to answer. Okay, and now, now I'm going to try to answer it. I'm going to try to say the answer is yes, this is the thing we care about.
00:31:55.317 - 00:32:30.795, Speaker A: We have some data, it has some copyrighted works in it. User is interacting with this model. And the only thing we know is that had we removed all of anything that depended on this particular copyrighted work, right? This is like the data set I could have put in the clean room for Chagall's the Fiddler. And the user interacted with that model. If the user was good, the user wouldn't have copied this work. That's the one Intuition. So now I'm introducing this notation, scrub dc.
00:32:30.795 - 00:33:06.055, Speaker A: I'm removing anything in the data set D that depends in a copyright relevant way on C, like it's a derivative work, for example. And my intuition is that a good user is one that won't copy C in this world. And so here's the idea. If these two models are close and the user is good, then this user also won't copy. It's very close to what the NAF's intuition was. And so what I need is I need to figure out what conditions I need here, like how to ensure these things are close and what does it mean for the user to be good. And that's what I'm doing right now.
00:33:07.515 - 00:33:17.965, Speaker D: Imagine my database D contains like chagall painting and 3 out of 3 secret sharing of Chagall painting. This perfect randomness. Can you design this function, Shrub? I don't know.
00:33:18.085 - 00:33:52.835, Speaker A: No. So this, I think scrub is hard to deal with in like an arbitrary setting. But you take it outside, I'm taking it outside, but I'll come back to it. I agree in practice this is the hard part, deduplicating, even if you don't care about any of this, deduplication is hard for real data. Okay, so here's the claim. If the dataset is golden, which I'll define, the training algorithm is differentially private and the user is blameless, I'll define all these words. Then you don't copy, or at least you do with very low probability.
00:33:52.835 - 00:34:36.865, Speaker A: So informally, the condition on the data is that there's not two items that derive from the same copyrighted work differentially private for people don't know. It means that the model doesn't depend on any one item of the training data too much in some precisely defined sense. And the user we've already said. So now I'm going to define each of these three things differentially private training for all neighboring data sets, meaning they differ on one entry. The models are close. I'm not going to tell you what this notation means. If this is not familiar to you, just understand internalize that they're close.
00:34:36.865 - 00:35:30.265, Speaker A: But the condition I need is that the data sets differ by one entry. Okay, so this golden data, I require that the data is golden with respect to some set of copyrighted works. If for all copyrighted works in the data set, scrubbing that work from the data removes at most one entry from the data. And this is exactly the same as saying that scrub DC is neighboring to D in the sense that I need for differential privacy. And the blameless user, again, it's like it's the guy for whom this is safe. So we have this, the user's output distribution tau. I've been talking about it in the real world, in the clean room where I've scrubbed this of some particular work, C.
00:35:30.265 - 00:35:47.037, Speaker A: I'll call the user's output distribution tau minus C. Okay, so it's exactly the same distribution. Train the model, give the user the model. The user produces some output. This is tau minus C. It's a distribution over user output. The user is alpha, blameless.
00:35:47.037 - 00:36:51.255, Speaker A: Alpha is now a parameter with respect to all the things the data set, the set of copyrighted works, whatever auxiliary information he gets. Two things if two things. One is that in the real world up here, the probability that the user produces something substantially similar to a copyrighted work that is not in the data set. A copyrighted work that's independent of the training data is less than alpha, right? So this is like the user produces something. Copy is a news article written in 2024 should be less than alpha. And also the stronger condition is that for any copyrighted Work. If I scrub the data of that work, then the probability, sort of the same thing happens, the probability that you copy any work not in the training data or the work I scrubbed, which is to say now all the works that are not in the scrubbed training data, all the works that are not in here is still alpha.
00:36:51.255 - 00:37:11.735, Speaker A: So I need this condition, this is this blamelessness. It's not checkable. I can't like you give me the code of a user. I don't know how to check this because I don't really know how to enumerate this set. I don't know how to do all these things. But again, I said this at the beginning. I have the intuition that I could behave in this way.
00:37:11.735 - 00:38:18.675, Speaker A: And maybe I don't know if you also have this intuition, but at least it is sort of a well defined property that captures this idea. The user doesn't copy anything that wasn't given to him in the clean room. I think we don't copy every day of our lives, we don't copy things. So I think I can do it when I'm this guy too. Okay, so the theorem statement is if the data is golden and the model is trained with epsilon differential privacy and the user is alpha blameless, then the probability of producing something substantially similar to any copyrighted work, even ones in the data and not in the data, is at most you have this blow up over the alpha. So either epsilon times the number of copyrighted works in the training data or number of copyrighted works that affect the training data times alpha. And if you've seen differential privacy now this is like how to prove this thing is obvious once you have the definitions, goldenness tells you that these things are neighboring.
00:38:18.675 - 00:38:50.925, Speaker A: Differential privacy tells you that these models are close post processing or blamelessness tells you that this. The probability of copying here is small and so the probability of copying here is small. Proof is trivial once you've defined it. Okay, and now sort of to Evgeny's question, how do I get this golden data? And you can buy gold at Costco, but I don't know if I could buy golden data at Costco. So one idea, and so the first answer is I don't really know. This is. I think it's a hard problem.
00:38:50.925 - 00:40:04.185, Speaker A: One idea is that if you're not trying to train ChatGPT, but you're trying to fine tune a model for some bespoke application, and you can create data that you know is independent of copyrighted work, like I can commission new work or I can, you know, get a team of lawyers and engineers to handcraft a training data set of 50,000 items. Like, I can imagine putting a team on this and being confident enough to please my insurer that my data set is, you know, golden or very, very close to. So I think in the context of like fine tuning, this makes a lot more sense. Every copyrighted work in the world, even though it's not in the training data set, only in a copyright relevant way, affect one training data. So there's not two works in the training data that derive from the same copyrighted work. There's not two parodies of Chagall. It's even harder to achieve because I'm quantifying over a set that I don't know, that I can even enumerate.
00:40:04.185 - 00:40:10.065, Speaker A: It's all the works in the Library of Congress and everything on the Internet is like the set of all copyrighted works.
00:40:11.765 - 00:40:22.215, Speaker C: In this use case, if I'm the person 50,000, what I get is that if I got sued for copyright, I could just formally blame OpenAI.
00:40:22.295 - 00:40:52.815, Speaker A: So this is what I want to say next. Like, even if the data set is not golden, we get something. And I think that this is actually like, if we're trying to evaluate a technical measure for protective for preventing users from copyright infringement. One of the questions that we should ask in order to evaluate this is like, what happens when the user does copy? Because the user will copy, we can't prevent the user from copying. Bad users copy. What happened when the users copy? We want to make sure that bad users get nailed and good users don't. That'd be nice.
00:40:52.815 - 00:41:42.265, Speaker A: That username, in some sense is someone who wants to copy, let's say at least that person, he should be in trouble. So I want to protect the user even when the data set's not golden. And so how do I want to do it? When we're at trial time, there's a particular work in question, right? Somebody else, the plaintiff, has done the trouble of discovering the work that I've purportedly copied, that the user has purportedly copied. And now I can take this work and look at the data set and check if with respect to that work, the infringed upon work, there are multiple copies, right? I can do this. And this is something that legal process is. I'm going to finish before that even got to go. This is something that legal process is well suited for, right? Like somebody has sued, somebody has proved that the user is infringed.
00:41:42.265 - 00:42:17.293, Speaker A: Like a jury has already found infringement and now the user turns around and goes to the model provider and says, hey, look, I think you screwed up, not me. And so we check for the goldenness of the data set with respect to this work, how the defendant's lawyers get involved. I hire some experts, you give me some access. Who knows? We need some procedures, we need some contracts for this. And there's sort of like two policy options. I understand that this is like a very simplistic approach to this policy problem. One is that the model provider can cover the damages.
00:42:17.293 - 00:42:49.101, Speaker A: If we're in this case, I can just make you the promise. I the fine tuner, I'll make you the promise. You're the user. If you ever get taken to court and found liable and assess damages for copying, I promise to give you whatever access you need to convince yourself that the data is golden or not golden. If it wasn't golden, I'll cover you. That's a valuable promise to a user. Option two is if the model trainer is getting the data from some sort of, like, body that is representing many copyright holders.
00:42:49.101 - 00:43:31.875, Speaker A: For example, Columbia Records, right? Like Columbia Records may be enforcing the copyrights of its artists or the New York Times. There's many, many works for which they're the copyright holder or the enforcer. I'm going to turn around and say to Columbia Records, like, I want to pay you for your training data. Please make sure that your data is golden. If your data is not golden, you promise to waive any fines for users of my model. This is again, like a very simplistic approach to this complicated policy problem. The point is that these approaches are only possible because I have a guarantee when the data set really is golden.
00:43:31.875 - 00:44:04.745, Speaker A: So even if I can't enforce the data set is golden, it sort of unlocks maybe some contractual policy options that are not otherwise available to me. Okay, takeaways. Again, I claim a necessary condition, if I want a formal meaningful guarantee is to exclude tainted models. And a sufficient condition is these two things. NAF isn't enough. And so I'm not convinced that, like, that something in between is not enough. I don't know.
00:44:04.745 - 00:44:24.945, Speaker A: I don't think of totally triangulated. We've triangulated this yet, and we only protect these blameless users. It's not a checkable property. I don't know how to be sure that I'm blameless. I would like to be sure that I'm blameless. And so this. I find this quite unsatisfying.
00:44:24.945 - 00:44:53.785, Speaker A: A little bit unsatisfying. And I don't again, like the way I'm thinking about copyright law. And how to assign liability at the end of all of this is very simplistic. So getting into making this less simplistic, trying to figure out, like, the policy or the legal question. If you want a copy of the paper, it's not yet on the archive, but it's on my website. If you don't remember this URL, just go to ohlone.net, you'll find it.
00:44:53.785 - 00:45:08.677, Speaker A: That's it. That's everything I wanted. Going to the airport. I'm going to the airport. I'm going to call the Uber now. I have a.
00:45:08.821 - 00:45:10.381, Speaker C: Is there going to be questions on core?
00:45:10.453 - 00:45:15.625, Speaker A: Oh, yeah, yeah. You can. Okay. You can take. No, no, this is not a question.
00:45:16.205 - 00:45:20.791, Speaker C: So. And if you're a student, can you. So I see how many students there.
00:45:20.823 - 00:45:22.935, Speaker A: Are students like this. Okay, cool.
00:45:22.975 - 00:45:27.879, Speaker C: So we'll have. We're going to have a, like a happy hour social thing at the graduate.
00:45:27.967 - 00:45:33.015, Speaker A: Graduate. No, no.
00:45:33.055 - 00:45:38.055, Speaker C: So I was going to say that's why I did this gauge. So I think everybody.
