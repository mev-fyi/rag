00:00:00.120 - 00:00:00.900, Speaker A: Robots?
00:00:01.774 - 00:00:37.336, Speaker B: Well, a bit more than that. I mean, yeah, I know. That's the basic idea. Anyway, so I've been talking about this subject for some time, and some of you probably have seen portions of this talk, but there is going to be new material in it. And it's also kind of an overview of this work that I've been doing for the last couple of years. So, I mean, let's start with a pair of semi definite programs. The primer looks like this linear matrix inequality with linear objective function.
00:00:37.336 - 00:01:11.292, Speaker B: And this is the dual, the dual, I have a PSD matrix Y with linear equality constraints. And just the usual notation is that this curly inequality means that the B minus A is positive semi definite, and the dot product of two matrices is just the strand them out as vectors. And then you take the inner product. So now is the interesting. Okay, so. And positive semi definiteness, of course, is all principle subdeterminants are non negative. I think I'm going to skip this slide.
00:01:11.292 - 00:01:51.188, Speaker B: I mean, why SDP is important. Obviously, linear programming is a subset of SDP and SDP is a convex program. Now, I come to the point that I would like to talk about, which is the duality theory of semi definite programming. So, we have this primal dual pair of sdps. And some things that happen in linear programming, they are inherited in semi definite programming. So, for example, a weak duality comes for free. So if I have an x which is feasible here and the y which is feasible here, then we have this usual weak duality inequality.
00:01:51.188 - 00:02:23.724, Speaker B: So so far, that is okay. And ideally, we would like to have a pair so that the objective values are equal. Right? So I would like to have an x bar and a Y bar where I have equal objective values. Now, unfortunately, this is where things go wrong. So not everything is inherited from linear programming because we have some pathologies. For example, the supremum really could be a supremum, not a maximum. The infimum really could be infimum and not a minimum.
00:02:23.724 - 00:02:47.644, Speaker B: And there could even be a positive duality gap. So the optimal values could be different. And of course that's a bad thing, because then we cannot really certify optimality. So that's the point. And of course, if we have a pathological SDP, that could defeat an SDP solver. I mean, that happens once in a while. Now, I'm going to go through two very basic examples.
00:02:47.644 - 00:03:35.482, Speaker B: The first one is primal, is just has one variable, this linear matrix inequality. So if I put everything on one side, that means that this matrix has to be positive semidefinite. So then x one obviously has to be just zero, right? Because otherwise that determinant would be negative. So the supreme is just zero and it's attained. Now, if I take the dual, the dual is going to be also pretty simple. The dual variable matrix is y, right? And if I work things out, then it turns out that the dual is just this problem. And in the dual I'm trying to infimize dividends and the rest.
00:03:35.482 - 00:04:30.308, Speaker B: And the off diagonal elements are fixed to one and the y 22 is free. So then a little thought shows that the y eleven could be any positive, tiny little epsilon, because I can make the y 22 to be large, but the y eleven can never be equal to zero, right? So then this is a pathological SDP because the dual is not obtained, right? So that's the very basic example, which probably is seen in every textbook. So the other one, the other kind of pathology is positive duality gap. So I can take an SDP with two variables. I try to supremize the x two. Similar argument shows that the x two always has to be zero, because I can put stuff on the other side. And if the x two was non zero, then I would have a negative subdeterminant.
00:04:30.308 - 00:04:53.188, Speaker B: So the x two always has to be zero. And in a dual I have an optimal value, which is one. Right? So there's a positive duality gap. So these are the two basic examples. Now, I would like to understand what is going on with these two examples and with others as well. So I'm going to introduce some terminologies. So I would like to be.
00:04:53.188 - 00:05:51.698, Speaker B: I would like to know when this system, when this positive, semi definite system is good or bad. Right? So I'm focusing not just on a particular SDP, but on the system. So I'm going to say that it's badly behaved if there is a c objective function, so that the supremum of this SDP is finite, but the dual doesn't have a solution with the same value, and that breaks down into several subcases. So first subcase is that dual doesn't attain or there's a positive gap, or maybe even both, right? So that's the definition of a badly behaved semi definite system. And otherwise I call it where I'll be hit. And I'm also going to call a slack, which is the difference between the right hand side and the left hand side, which is a positive semi definite matrix. So that's kind of the setup.
00:05:51.698 - 00:06:48.894, Speaker B: Okay, I should stop. And are there any questions? Okay, so that's so far the setup. And then the motivation of this is that these two systems, they look kind of very similar. So if you stare at them for a while, then there is some strange similarity. And, and what is that? I mean, we can see that, for instance, in the first example, there is this kind of anti diagonal structure, right? So there is this matrix here and there is this anti diagonal structure, right? And in the second one I see the exact same thing or something very similar, right? So there is this right hand side matrix and then there is this anti diagonal structure which shows up. And then the theorem explains why. So this is a semi definite system.
00:06:48.894 - 00:07:48.344, Speaker B: And I'm going to assume that the maximum rank slack matrix in this system is in this form, right? The small identity and lots of zeros. And I can always assume that from a theoretical viewpoint because I can always just apply a similarity transformation to all the matrices, then the result is that this system is badly behaved. If and only if there is a certificate which proves that it's badly behaved, which is there is a v linear combination of the AI's which partitions in this form. And here the v eleven is r by r. So that matches the size of this identity matrix. The v 22 is positive, semi definite. And then the last condition is that the range space of this one is not a subset of the range space of the v 22.
00:07:48.344 - 00:08:29.938, Speaker B: So that's an if and only if characterization of bad behavior. And I mean, let's go back to the first example to make sure that it checks out. I mean, in this system x, one always has to be zero, right? So that's a useless variable. And then, so therefore this is the z matrix. That's a maximum rank slack matrix, because that's the only slack that makes sense. And then the v is just that, right? And then we can immediately see that of course the rain space of one is not a subset of the rain space. So this is if and only if characterization.
00:08:29.938 - 00:08:41.494, Speaker B: And this takes care of telling when the system is badly behaved. I should stop if there are any questions.
00:08:44.274 - 00:08:52.174, Speaker C: Can you say a little bit more about the geometry? So in the space of a's and B's, what kind of set is it? What co dimension does it have?
00:08:52.734 - 00:09:37.272, Speaker B: That's a great question for algebraic geometers. So maybe if somebody gets interested in it, I mean, I'm happy to talk about this. I mean, it's a zero measure set. So we have to be, I mean, if I just randomly generate an SDP, then it's going to be well behaved on the other hand, in polynomial optimization problems that we get from, let's say, lesser relaxation, we do get this. So, you know, that's a great question, and I'm happy to talk about it. So then, okay, so what is missing from this? And this is where elementary operations come into the picture. And actually, that might answer some of Bern's question.
00:09:37.272 - 00:10:32.012, Speaker B: So, if I want to convince someone that the system is badly behaved, then I can show that, okay, here is the z and the v, right? So then, please believe me that my system is badly behaved. On the other hand, I mean, the z and v, they say that, okay, well, therefore, there exists an objective function for which the primal is finite, but the dual doesn't have a matching solution. So it says that there exists a c, but it doesn't give me the c. Right? And on the other hand, it's not a polynomial time or easy to verify proof. Which is, suppose I tell you that, okay, here is the z and the v. And believe me that this system is badly behaved. But why should you believe me? Because there is this theorem, which is five pages long, and then if you read the proof of the theorem, then you can believe me that the z and v really prove the bad behavior.
00:10:32.012 - 00:11:10.544, Speaker B: Right? But I want something more natural than that. So, the idea that came up is that suppose how do I prove just a basic linear system of equations is infeasible? Well, I do elementary row operations, and I create a row echelon form which says that zero x is equal to one. And after that, I don't have to say that my system implies zero x equal to one. Therefore, there is this complicated theorem. And believe me that it's infeasible. I mean, you know that zero is not equal to one. So, I would like to borrow ideas from the row echelon form, and that's where reformulations come in.
00:11:10.544 - 00:12:17.288, Speaker B: So, this is the system, and I'm going to do some of the following operations. So, I can apply a rotation to all matrices where v is an invertible matrix. So I replace the AI by v transpose a I v, and the b by v transpose bv, where I can translate the b by a linear combination of the AI's. Or I can change an AI to a linear combination of the ajs, whereas can switch AI and Aj, right? These are very natural operations, and of course, they don't change the behavior of the system, right? I mean, it's just kind of, I mean, like, first one is just translating the spectrahedron, right? So that doesn't do anything. And these are invertible operations, so they don't affect the behavior of the system. And if I choose the v carefully, then they also keep the maximum rank slack in the same shape in that identity, and lots of zeros. Okay, so, and then here's the result about reformulating that system.
00:12:17.288 - 00:13:16.798, Speaker B: So it's badly behaved if and only if it has a reformulation of this form where z is the maximum slack. So I just normalize the system so that that became the right hand side. And these matrices are linearly independent, meaning that if I take a linear combination and I get the zero, then the coefficients must be zero, and the Hn has to be positive, semi definite. So I claim that if the system is in this normal form, it's kind of like a row echelon form, then it's very easy to convince somebody that this is badly behaved, and I can prove it in like, in three lines. So I will do it. So how do I prove that a system in this particular shape is badly behaved? And this might actually give some hints about how to go about co dimension and things like that. So suppose that x is a feasible solution, right? And suppose that the corresponding slack is s.
00:13:16.798 - 00:14:19.738, Speaker B: So the s is the z minus the summation xi AI. Then I know that the last n minus r columns of s must be zero, right? Because if I have a PSD matrix which is non zero somewhere here, right? Then I take the convex combination of that with this z and I get a bigger rank matrix. So that shouldn't be. So these columns have to be zero. So therefore, by the linear independence condition, the x k plus one through xm, they also have to be all zero, right? So basically the xk plus one through xm, they are like useless variables which are always zero. So therefore, if I take this trivial objective function minus xm, and I supermise it is going to be zero, for obvious reasons, because the xm is always zero, right? So I have an objective function which is a finite optimal value over this system, and at the same time, there is no dual solution with the same value. That's another two lines of proof which I'm not going to present here.
00:14:19.738 - 00:15:14.164, Speaker B: So essentially we brought this into a normal form, which proves easily, without referring to any kind of complicated theorem, that the system is badly behaved. So it's kind of like a wachalon form of our system. Okay? So in an example, so before reformulation, it looks like this. Can we tell that this is well behaved or badly behaved and probably not, right? It would be a challenge, so to speak, after reformulate it looks like this, which is a lot sparser, of course, but sparsity is not so much the point. But the point is that in this system it's easy to see that the x three and x four are always zero. So that gives me a trivial objective function with a finite and actually zero optimal value. And there's no dual solution.
00:15:14.164 - 00:15:49.836, Speaker B: So in some cases it does so. Okay, the corollary is we can do a similar reformulation for well behaved systems. And if I ask this question, is this semi definite system well behaved or badly behaved? It's an NP intersect quantp. Right. And what's the certificate for that? The certificate is I can give you this normal form. Right. And then from the normal form you can see very easily that it's badly behaved or well behaved.
00:15:49.836 - 00:16:25.974, Speaker B: And then to accompany the normal form, I have to give you the elementary row operations which produced it, and the similarity transformation. So it's kind of a very simple proof. And. Yeah, so that's the certificate. And there's a corollary that if it's well behaved, then for every c, which is a finite objective value, I can produce a dual solution which is block diagonal, which is a useful thing to have. So that's it about badly bf stuff. Are there any questions I should.
00:16:27.064 - 00:16:34.616, Speaker D: Yes, just to be sure. When we do the low reduction in linear inequality, it's clear what you do. So it's the same for a semidefinite.
00:16:34.680 - 00:16:37.232, Speaker B: Program, the low reduction for what it.
00:16:37.248 - 00:16:41.280, Speaker D: Is clear algorithmically, how to get to the last form for the SDP.
00:16:41.352 - 00:17:20.834, Speaker B: Yes, yes, it's very clear. So the catch there is that unfortunately we don't know how to compute the z and the v. Right. I mean, I'm going to talk about that a bit more, but once I do have the Z and the V, it's completely trivial to produce the row operations. It's almost the same thing as block gaussian elimination just to begin with matrices. Right? Okay, so how about infeasibility? That's a joint work with mink we Liu, who was my PhD student a couple of years ago. So now this is a semidefinite system in equality constraint form AI dot X equal to bi X.
00:17:20.834 - 00:17:55.154, Speaker B: This used to be y, but now it's x just to work. Simplicity. And the AI of symmetric matrices, there's a dot product. Now what I want to do, I want to prove infeasibility, right. How do I prove that the system is infeasible? There's a certain kind of Farkash lemma for SDP. Which says that there is this alternative system. So if there is a linear combination of the AI's, which is PSD, and the summation Yibi is negative one, it's called the alternative system, then I know that the.
00:17:55.154 - 00:18:37.610, Speaker B: So, okay, so I have these two alternatives. So if this is feasible, then this original system is infeasible, right? So one implies two. And usually I ask this question that you know, and the proof of this is very simple. It's like one line. It's the same thing as in very similar to the proof of Forklamma's usefulness in linear programming. Now the other direction has a very long proof, because that's not even true. So in other words, this alternative system is not an exact certificate of infeasibility.
00:18:37.610 - 00:19:20.068, Speaker B: So there exists semi definite systems which are infeasible. But alternative system is also infeasible. So sometimes it cannot certify the infeasibility. Now there are exact certificates on infeasible, so they exist. So the one that was presented by Ramana in 1995. So that was the first such a certificate of infeasibility. And then there was one by Klepp and Schweitfer, which the algebraic geometry people would probably know best here.
00:19:20.068 - 00:20:19.956, Speaker B: And then there are works on facial reduction, which was originally invented by Borwein and Wolkowitz in 1981. And then a certificate of infeasibility was produced by Waki and Muramatsu. And also this work by Ramana, Leventuncial and Wolkowicz connected Ramana's dual to facial reduction. Right? So there. So exact certificates of invisibility exist. However, I mean, they're not all that simple, right? So, for instance, if you read Ramana's dual, I don't know how many of you are familiar with that, but you really have to absorb the fact that he writes down essentially n copies of the original system and he connects them with these linear matrix inequalities where the Uis and the Wis are extra variables. And then, I mean, when I first saw it, it was kind of difficult to wrap my head around that.
00:20:19.956 - 00:21:06.704, Speaker B: What are these Uis and wis doing there? Okay, so they exist, but we would like to produce something simpler than that. So I would like to produce something which is an exact certificate of infeasibility, which is almost as simple as far cross lambda. So let's see how to do that. So the example that we played with is this very basic infeasible SDP. Now, how do I prove that this is infeasible? Suppose x is feasible, then the first equation says that x one is equal to zero. Right? Now by positive semi definiteness, the first row and column of the x also has to be zero. Right? After that, the second equation said that x two is equal to negative one.
00:21:06.704 - 00:21:51.944, Speaker B: And right away I have a contradiction, right? So this system is totally trivially infeasible right? Now. Okay, so is this very special? The answer is not very special. So the main idea is that we are going to find such a structure in every infeasible SDP. But of course that structure is not going to be there just for free. We have to do our elementary row operations. And so the reformulations come back again. So reformulation is I do elementary operations on the equations and I can also do similarity transformations and then that's all we do.
00:21:51.944 - 00:23:15.334, Speaker B: And of course one comes from gaussian elimination and of course reformations preserve feasibility or infeasibility. Now some basically in your algebra, so if I see an equation like that, when x is positive semidefinite question is, what does it say about the x? Right? So then identity and lots of zeros, and dot x is equal to zero and x is positive semi definite, what does it say about the x? Well of course what this says about the x is that the sum of these diagonal elements must be zero, right? See therefore algorithm of them has to be zero. And by positive semi definiteness, those rows and columns have to be zero, right? So therefore that means that the x actually looks like that. So that's the basic linear algebra. And then, so here's the theorem. The theorem says the following. So the p is infeasible if a null, if it has a reformulation in a certain shape, and the certain shape is like this, I have k plus one equations which are special, right? And there is some other stuff there.
00:23:15.334 - 00:24:39.618, Speaker B: The dot, dot, dot means that there are some other equations which are actually totally irrelevant because this first k plus one are already going to prove infeasibility. And these special equations look like the e one prime looks like there's a small identity and lots of zeros, right? So the a two prime looks like there is r1 rows and columns. So the first one rows and columns can have arbitrary stuff there. So the crosses mean that you can have anything there. And after that there's a small identity matrix and then some more zeros. And then in general, the AI prime looks like in the first r one plus plus plus ri minus one rows and columns, I can have arbitrary stuff, right? That doesn't count. And then I have a little identity there and then I have zeros here, right? So that's the statement that p is infeasible if and only if I can put this into, if I can put it into this particular shape where the e I primes have that particular form, right? So now the question is, why is this as simple as the elementary, I mean, the row echelon form? It's not much more difficult because I can prove that this particular system is infeasible.
00:24:39.618 - 00:26:11.564, Speaker B: And I can prove that in like two lines, right? So how do I do that? Suppose that x is feasible, right? The first equation says that e one prime dot x is equal to zero, right? So now e one prime x is equal to zero. That proves that the first one rows and columns of x are zero, right? So if that's the case, then I can go to the second equation which says that a two prime dot x is equal to zero. And that's going to wipe out the next or two reals rows and columns of x, right? And I keep going like that, right? So I keep going, and then I prove that the first r1 plus plus plus rk rows and columns of x are zero. So I'm just saying that the first so many rows are zero. But of course the same thing is true of the columns. And then I come to the k plus first equation, right? And since the AK plus one prime also has that special form that I get, that AK plus one prime, dot x is non negative, right? Yeah, but on the other hand, the equation says that it's equal to negative one, so that's obviously impossible, right? So, so that's the, I mean, it's not as simple as the proof that the row echelon form proves invisibility, but it's not much more complicated. So it's kind of as simple as we can make it.
00:26:11.564 - 00:26:39.600, Speaker B: So I should stop here and see if there's any questions. No, I mean, I have more stuff to say. So the talk is not over. It's just kind of an intermediate stuff. Okay, so, okay, so back to the example. So the example is already in that form. I don't even have to do anything, right, so here the a one prime is this.
00:26:39.600 - 00:27:13.912, Speaker B: So then there is this small identity matrix and lots of zeros. The second matrix, I have some arbitrary stuff in the first one row and one column, and there's a small identity here. And then there's the minus one. So this is the example, the proof outline. I mean, how difficult is this proof? It's about a page and a half. So it's really not complicated and it's very elementary. And the alternative is that there's a traditional facial reduction algorithm which goes back to Bormann and Bolkovich, the one by Joaquin Mural.
00:27:13.912 - 00:28:16.226, Speaker B: So that's the easiest that we can fiddle with, and then we can actually get this result, or we can do it independently and completely in a self contained fashion. Okay, so then the corollary is that now we get to this point that the semi definite feasibility is in NP intercept co NP in the real number model of computing, right? So that's the result that was stated and proved first by Ramana, but now we have a simpler proof. Now, if I want to show that something is feasible, the system is feasible. I just show you an x, right? You can plug in and you can check that it's actually feasible. Now, if I want to prove that the system is not feasible, that means that feasibility is in co np. Then I can show you the reformulated system in that nice semi definite echelon form, and it's trivially infeasible. And I have to convince you that I really got this system from the original system.
00:28:16.226 - 00:29:13.514, Speaker B: And how do I convince you? Well, I show you the simulated transformations, which I can just package into one matrix, and I can show you the elementary row operations, which I also can package into one matrix. Right? So if I give you these two matrices accompanied by the semi definite echelon form system, then that's completely convincing that this system is infeasible. So it's basically two matrices, which we need to verify infeasible. Okay, so then I keep going. So then, question is that I get asked a lot. So is this just theory? Right? Okay, so you can prove these nice reformulations and you're saying that this is something like the row echelon form. But then of course, for row echelon form, we can compute that, right, by Matlab or whatever.
00:29:13.514 - 00:31:01.060, Speaker B: On the other hand, I cannot really say that we can compute these reformations so easily, because in order to do that, in general, you would have to solve sdps in exact arithmetic, right? So that part is not as nice as in basic linear algebra. However, the, this is the mathematicians topology, right? So this is actually useful for something. So one application is that suppose I want to generate, want to write down an infeasible system of an infeasible SDP, right? So how do I do that? Now, my theorem says that every infeasible SDP can be put into this special form, right? Now, if I want to write down an infeasible SDP, then how can I do that? Well, I can generate any infeasible SDP because I can write down an SDP in this trivial form, in this echelon form, and after that I can just mess it up. I can do some elementary row operations and some rotations. And this way I'm going to get every infeasible, semi definite system. So the analogy that I actually came up with, at some point, suppose I want to create an infeasible linear system of equations, right? So how can I do that? I can say that, well, I write down a system which is ax equal to b zero x equal to one, right? And I can just mess it up doing some elementary operations. And this way, actually, I get every infeasible linear system of equations.
00:31:01.060 - 00:31:49.924, Speaker B: So it's a good way to actually, like, create an exam question in linear algebra, right? I mean, just write down a system like that and then mess it up, and then the students can figure it out, right? So now a very similar result applies to sdps. So we can actually generate some nice problem libraries. So what we did is with minkwili. So we generated a problem library of infeasible and weakly infeasible sdps. Now, the weakly infeasible ones are the particularly nasty ones, because the distance of that defined subspace to the semi definite cone is zero, even though they don't intersect. So they are numerically really nasty. And then that actually generated some interest.
00:31:49.924 - 00:32:57.124, Speaker B: And there is some follow up work by and colleagues, and then by others who made some progress in solving these problems. So that was the first application. Now, the second application is. So you might wonder, okay, so we can put any infeasible SDP into this particular echelon form, right? But what if I have an SDP which is infeasible, and actually, I don't have to do anything at all because it's already in that special form, right? So could that happen? And the surprising fact is that it does happen sometimes, right? I mean, not every time, but a lot of the times it happens. So sometimes we have SDPs which are already in that special form, and we have this preprocessor, which we call sieveSDP for various reasons, because the structure that it creates looks like a sieve. So I show you an example. So this is an SDP which comes from a polynomial optimization problem.
00:32:57.124 - 00:33:26.252, Speaker B: So what we are picturing here is we are picturing the AI's. So the AI matrices we stretch out as vectors. So then I get something like this. The blue and the red is non zero and the white is zero. So before we pre process, it looks like this. And after we pre process, it looks like this. Right? So sometimes this very silly method to detect infeasibility or to reduce an SDP, it works amazingly.
00:33:26.252 - 00:33:58.388, Speaker B: Right? And not every time, of course. I mean, this is not, you know, this doesn't happen every time, but it does happen quite a bit. Right. Got any questions? And so I have pretty fast. So it looks like there will be time for questions. Okay. So then the question is, can we apply this methodology to understand other pathologies in semi definite programs? And the answer is yes.
00:33:58.388 - 00:34:52.704, Speaker B: So there is some promising starts, for instance, to understand positive duality gaps. And the theorem is this. So, positive duality gaps. When the, let's say the primal has an optimal value of, let's say one, the dual has a value of two. Those are particularly nasty, and they are not all that well understood either. So there's a recent result which says that suppose that m is equal to two, right? We have two variables, and that doesn't look like it's a very impressive result, because, I mean, why not m equal to three or something like that, right? But the fact is that when m is equal to one, then one can prove very easily that there is no duality gap. That's really like an exercise, right? But m equal to two, this hasn't been known until very recently.
00:34:52.704 - 00:35:26.764, Speaker B: What is the characterization for m equal to two? So when m equal to two, what I can do is I can say that there's a positive gap if and only if I can reformulate my SDP into this form. Have two variables. And what we have is we have lots of zeros here, right? Empty spaces stand for zeros. We have lambda, which is a positive definite matrix. We have an m, which is a moon zero matrix. And this is an identity. And zero is on the.
00:35:26.764 - 00:36:03.846, Speaker B: And there is a negative identity here, which might be an empty matrix. So the s is greater or equal than zero. So we have some understanding of positive duality gaps as well. I mean, it's a complete one, obviously, but. So there are some papers here. And one interesting point of this is that the mathematics actually gets very, very simple towards the end, right? So at the beginning, it started with some relatively heavy convex analysis, and then going down the list, it really became more like undergraduate level. I mean, I'm seriously.
00:36:03.846 - 00:36:51.224, Speaker B: So by the end, this paper is really kind of at an advanced undergraduate level, and it's really not that difficult to understand. So the conclusion is that there are all these pathologies, we can write down this combinatorial type characterizations and then reformulations come into the picture. The elementary row operations and simulated transformations, and then simple certificate of infeasibility. We can generate all infeasible sdps. And there's some practical uses that we can generate problem libraries and we can do pre processing. So, thank you.
00:36:57.964 - 00:36:59.064, Speaker A: Questions?
00:37:00.604 - 00:37:12.676, Speaker D: So, in the work of Liu and Ying, so they apply Tao class for the speaking. So what is that idea for proving invisibility is similar to yours?
00:37:12.700 - 00:37:54.474, Speaker B: Or it's some, I mean, I actually don't know, it's like homotopy or something like that. I mean, I'm not totally familiar with what kind of algorithm they are using. Honestly, I don't know I'm the right answer. I don't think I'm the right answer to the right person to answer that question. So yeah, I'm just curious. Yes, I mean, we generated this problem library and they were able to recognize not just infeasibility, but weak infeasibility of those instances. And of course, I mean, I don't think that cannot work every time if you do floating point computation, but for some instances it did work.
00:38:03.494 - 00:38:23.510, Speaker E: I have a question. So suppose that you have your semi definite programming, depending on a print, might think of the parameters being performance in some engineering problem. And so then if we insist on high performance, it's infeasible, you insist on less performance, it becomes feasible. So if you take your method, you have a sieve, I guess.
00:38:23.662 - 00:38:24.430, Speaker B: I have what? Sorry?
00:38:24.502 - 00:38:27.126, Speaker E: You have a sieve, I guess, for determining the infeasibility.
00:38:27.230 - 00:38:31.274, Speaker B: Or can you have a normal form which proves infeasibility?
00:38:32.134 - 00:38:38.576, Speaker E: Does it tolerate a parameter in some magical way? Or would you one just have to step the parameter down?
00:38:38.720 - 00:39:05.904, Speaker B: I guess it depends how the parameter shows up. I would say the parameter is very simple. Then let's say one of the AI's looks like an AI bar plus lambda times AI double bar. Then it might be possible to do this symbolically. And I guess it depends how the parameter shows up. That's my guess. Yeah.
00:39:10.724 - 00:39:12.144, Speaker E: That'S an important problem.
00:39:13.244 - 00:39:23.104, Speaker B: Yeah. I mean, how does the parameter show up? That's the question. I'm happy to look into it. If.
00:39:29.904 - 00:39:39.964, Speaker D: So, you're saying that the algorithm for this one is not poly time, but SDP solver is actually poly time, right?
00:39:40.664 - 00:40:17.430, Speaker B: Well, I mean, that comes with a lot of qualifications. I mean, as the piece of polynomial time, that means that if you have a primal strictly feasible point and the dual strictly feasible point, then in polynomial time, we can reduce the duality gap by half. Right. But if those, the x and the z, x and the y, they're not given, then we don't know. I mean, even SDP feasibility is not known to be p. Or is it.
00:40:17.462 - 00:40:28.254, Speaker D: Possible that I run some poly time algorithm and then check if the time exceeded and then declared something like infeasible or some bad behavior happens?
00:40:30.634 - 00:40:54.014, Speaker B: No, because I mean, what is, what is. I mean, I don't think so. I mean, I mean, poly time is already. It depends on lots of, lots of parameters. And then, and there are the constants there to start with. Right. And then the speed of convergence and all those things.
00:40:54.014 - 00:41:05.034, Speaker B: So I think the answer is no. I mean, the absolute expert on this would be Jim renegade, obviously, but my answer is no. So it just doesn't work like that.
00:41:06.574 - 00:41:33.184, Speaker A: I mean, it's just not known because you don't have a visa, you can't have a certificate being a point because of points you get, they can go double exponentially in size. So they're not, they're not bounded in bit length, for example. So the terrain machine is out of possibility in the real number model. Yeah. So you're allowed infinite precision. So then, yeah, you do have a feasibility check. So there it is.
00:41:33.184 - 00:41:34.724, Speaker A: Polynomial time for.
00:41:37.744 - 00:41:52.960, Speaker C: Yeah, let's just going to say what Jim said. That's why I raised my hand. But you know, when you said we don't know whether STP visibility is in P, we don't even know if it's an NP. The corollary of what Jim.
00:41:52.992 - 00:41:53.208, Speaker A: Oh, yeah.
00:41:53.216 - 00:41:56.720, Speaker B: Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah.
00:41:56.832 - 00:42:18.574, Speaker C: So if there is a feasible solution, there must be an algebraic number, each coordinate must be. The degree might be large, is that. But we don't know. Well, that's one of the problems. We don't know. A compact way of representing in the Turing machine a certificate and then a polynomial time algorithm that will verify that.
00:42:21.194 - 00:42:22.162, Speaker B: Certificates.
00:42:22.258 - 00:42:44.794, Speaker C: If you come up with, in the original domain of the domain of the data, as you mentioned, there are examples where the size of the solution is doubly exponential in the unput size. And of course, there are also irrational unique solutions case which sometimes you can certify with some point.
00:42:46.774 - 00:43:20.514, Speaker A: Yeah, but on what your point. So yeah, you could hope that you could write down a system. You could represent the coordinates of the solution as roots of a polynomial. Then you'd be able to apply some decision method for polynomials to decide whether or not it's feasible. But then you won, showed that the degrees of the polynomials you would need to do that make it out of reach? That approach doesn't go through.
00:43:24.114 - 00:43:39.334, Speaker B: It's an interesting point, because if I give you an SDP with integer entries, right, then of course the answer is going to be yes or no. Is it feasible or not? But to get there, you have to go through an enormous amount of mess.
00:43:41.434 - 00:43:42.218, Speaker A: Not a mess.
00:43:42.266 - 00:43:43.490, Speaker C: It's a beautiful.
00:43:43.682 - 00:43:52.134, Speaker B: Yeah, I mean, a challenge. I mean, as we would say it. Right.
00:43:59.514 - 00:44:00.434, Speaker A: Thank you very much.
