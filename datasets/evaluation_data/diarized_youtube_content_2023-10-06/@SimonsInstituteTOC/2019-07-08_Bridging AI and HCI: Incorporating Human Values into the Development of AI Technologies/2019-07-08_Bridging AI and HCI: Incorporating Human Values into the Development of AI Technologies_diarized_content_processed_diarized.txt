00:00:01.040 - 00:00:11.994, Speaker A: Okay, so we'll begin the first session after lunch. The first speaker is AI Joo from University of Minnesota for the moment, and she's moving to CM with soon to talk about bridging AI and ACI.
00:00:16.094 - 00:01:04.854, Speaker B: Thank you for the introduction and hi everyone. My name is Ha Yi Zhu. As I mentioned, I'm currently an assistant professor at the University of Minnesota, but I'm joining the Human Computer Interaction Institute at Carnegie Mellon University this fall. Today I'm going to talk about bridging AI and HCI, incorporating human values into the development of AI technologies. So, as we all know, today's theme, the whole workshop theme, is a recent development in research on fairness. So a lot of machine learning based systems are increasingly used in a lot of everyday services, and particularly in some very high stakes decision making contexts, such as criminal justice and child protection. So there is increasingly, like a growing attention to the issue of fairness in such systems.
00:01:04.854 - 00:02:25.654, Speaker B: So, speaking of the recent development. So, in a recent paper actually published in 2018 and three researchers from University College London and the University of Oxford and interviewed 27 public sector machine learning practitioners across five different countries regarding the challenges of understanding and incorporating like a fairness into their everyday work. Everyday work in terms of like, building actual machine learning systems to assist those like high stakes decision making. And this is what they found in their paper. So they, this is what they wrote, that the interview results suggest a disconnect between the organizational and the institutional reality, constraints and needs, and those addressed by current research into usable, transparent and discrimination aware machine learning absence is likely to undermine practical initiatives unless addressed. So they provide a lot more details. And in their paper, basically what they are saying, that the methods, all these fairness aware machine learning methods are built in isolation, both from the specific stakeholders and their needs, actual values and the context where the system is embedded.
00:02:25.654 - 00:03:33.664, Speaker B: So my background, about my background, I am a human computer interaction researcher. So we, as a group of HCI researchers, what we do is to study the setting in which the computing technology is embedded and the needs and values of people in these settings, and in order to design like a better computing technologies. So we have the tools to potentially can maybe address and close the gap. So this is also like my primary interest and my research interest is to study, design and evaluate the intelligence systems in the real world. And the goal is to create more fair, accountable, better intelligence systems. So based on my recent study, and this is my provocation, which is fairness is not the only human value that we need to consider in the design of AI systems. So fairness is still extremely important.
00:03:33.664 - 00:04:28.467, Speaker B: But we realize that in order for this AI system to actually work, and we need to consider a wide range of human values, including fairness and the challenges, is how we can incorporate these multiple stakeholder values in the design and potentially balance these different stakeholder values, especially when they are conflicting with each other. So my talk today will be organized this way. And I will first talk about one of our recent projects on creating, deploying and evaluating an intelligent recruitment system in Wikipedia. And specifically I will talk about how we address the challenge of incorporating and balancing multiple stakeholders values. And next I will talk about some of our ongoing work, like the redesigning the AI based quality control systems in Wikipedia. Yeah.
00:04:28.555 - 00:04:31.251, Speaker A: What does recruitment system in Wikipedia mean?
00:04:31.347 - 00:05:05.384, Speaker B: I will talk about it next slide, or maybe next few slides. Okay. And the second work is actually collaboration work with Stephen Wu is organizer of this workshop. And finally, I will talk about the big picture. So first, to answer the question, so first, Wikipedia. I guess everyone here knows what Wikipedia is, and Wikipedia is the largest encyclopedia in human history. But what interests me most is it's not just an encyclopedia, it is the largest collaborative project in human history.
00:05:05.384 - 00:06:12.494, Speaker B: So one thing I have been exploring since my PhD is how Wikipedia community actually manages this. Hundreds of millions of volunteer contributors from all over the world with very different backgrounds, expertise and interests, and to achieve a common goal which is right, coherent and consistent encyclopedia. And what we found in my PhD work is like there is an important, like nested organizational structure inside Wikipedia community, which are called Wikipedia projects. And this Wikipedia project play an important role in managing the activities in the whole community. So just like America has like 50 states, and the Wikipedia community actually have hundreds of active subgroups called Wikiprojects, which help govern the activities of millions of volunteer workers. And we found that these wiki projects are important like socialization and management hubs in Wikipedia communities. And these editors, they can seek help and find collaborators.
00:06:12.494 - 00:07:13.434, Speaker B: And these new members learn from the experienced members and they actually can guide and organize these projects, actually have these mechanisms to guide and organize editors work. And they also Wikiproject offer the protection against unwarranted reverts and editor wars. But we also found from our data and from our work is that. So there is like a recent decline of these wiki projects, and a lot of wiki projects sort of are struggling because of the shortage of newcomers. So this is something that concerns the Wikimedia foundation, which is organization that maintains the site. And we have been discussing about how to address and identify these new members for the wiki projects. However, identifying like new people who are suitable for these wiki projects is not a trivial task.
00:07:13.434 - 00:07:54.994, Speaker B: So as of July 2017, the english Wikipedia alone has about 500 active wiki projects covering about 3.6 million articles. And there are about 2.9 million editors who had more than ten edits in their editing history. Wikipedia editing history. And there are about 38,000 new editors registering the site on a monthly basis. So the next thing we can think about is maybe we can actually design an intelligent recruitment system to help these wiki projects to automatically identify, recruit and retain these suitable new members.
00:07:54.994 - 00:08:56.816, Speaker B: So this is basically the goal of this project. However, creating a recruitment system is inherently quite challenging job, quite challenging task. And we also heard about like Amazon has actually abandoned its like recruiting tools, which like biased against the female applicants. So one of the inherent challenges that there is no clear ground truth. So it's not clear like who is actually suitable for this job or for these groups or not. And it is particularly challenging to build these intelligence systems in the context of Wikipedia due to the community acceptance to the AI technologies. Actually, our collaborator Aaron Hafaker wrote this paper, talks about these early AI technologies in early days in the Wikipedia community, and he concludes that AI tools fail.
00:08:56.816 - 00:10:13.224, Speaker B: A lot of AI tools built for Wikipedia community failed because they are insensitive to the contributors motivations and the community values. And what makes designing intelligent tools for the Wikipedia community even more challenging is like the recent tension between the Wikipedia community and the research community. So research that performs like an offline analysis of article content and editor actions is typically not controversial. For example, if you want to create a word two vec based on the Wikipedia corpus, it is completely not controversial at all. However, the community tend to react strongly to studies that will involve the cord interventions. So studies that according to their own word, will affect Wikipedia's natural state and will often encounter resistance from the communities. So actually, an experimental experiment study conducted by researchers from MIT and the University of Pittsburgh in 2017 actually received strong pushback from the community, which led to the blocking of the research accounts, months of heated discussion in the Wikipedia community, and creation of new policy in Wikipedia called Wikipedia is not a laboratory.
00:10:13.224 - 00:10:47.498, Speaker B: And this Wikipedia editor actually precisely described what the Wikipedia community wants. So this is actually a comment on another controversial research, like a proposal which was eventually withdraw due to the strong pushback from the community. So this editor said, if this proposal was an earnest effort to improve Wikipedia, the researchers would have worked with the community from the start to design a study that were consistent with our needs and values. So this is what the Wikipedia community wants. Go ahead.
00:10:47.626 - 00:10:54.466, Speaker C: Sorry. So when you say intervention, does that mean for instance, automated editing or experimental editing of the.
00:10:54.530 - 00:11:36.048, Speaker B: Yes, yes. So anything that is not like external to the community. So the things that you just described is like editing, like making additional edits. Like for example the studies MIT people and universities will do is like they will have some people to edit half of the articles, but like leaving the other half of article as control. So they want to see what kind of like edits will like have effect. So and anything like what we do is also intervention. So we are going to create this new like recruitment systems to put it deployed in Wikipedia and to, although we are arguing that or we are doing this for your own good.
00:11:36.048 - 00:11:44.880, Speaker B: But what they want is that to do something, that anything that conduct in Wikipedia has to be really consistent with their needs and values.
00:11:44.952 - 00:11:47.324, Speaker C: So they don't mind sort of read only research.
00:11:47.704 - 00:12:22.360, Speaker B: So I will talk about that a little bit. So actually there is also discussions like inside the Wikipedia community. Some people strongly say that we do not need any research at all. But then there are other people saying that, oh, there are still group of researchers who are doing something that really will benefit our communities. We should not ban and like everything and which I'm going to talk about at the end. So it's like our study actually have some positive effects on the relationship between the research community and the Wikipedia community.
00:12:22.512 - 00:12:26.296, Speaker D: What about things like translation? Automated translation?
00:12:26.440 - 00:13:10.622, Speaker B: Automated translation, yes. So they are actually also using these tools for translations. So using tools as translations, it's in general fine because they will, but they still want human to like edit the actual translation before you actually put on their, like you put on the Wikipedia. And they also wonder if you do, but they do, like if you are doing this, it's fine if you do it like alone. So it's like every time I translate, I first like use the machine like learning transcription systems and then edit my edit before put on. But if you are going to do it at large scale, then you need to get approval from Wikipedia community.
00:13:10.678 - 00:13:20.270, Speaker D: Some Wikipedias, maybe swedish, I don't remember which one actually has automated ways of generating pages and generating maybe translations.
00:13:20.382 - 00:13:59.584, Speaker B: Yeah, so that's another good point. So it's like right now Wikipedia has about like 50 different language like editions. So every individual like Wikipedia, like language edition is its own community. So now I'm focusing on like English Wikipedia. So English Wikipedia has this policy. So Wikipedia is not laboratory and they really, if you are going to do something, you need to do something that consistent with their values and needs. So however, in order to like, when we think through like satisfying these community needs and values, we also identify additional challenges.
00:13:59.584 - 00:14:49.830, Speaker B: So a recruitment system that will operate in Wikipedia actually involves multiple stakeholders. So there are like new peoples to newcomers to Wikiproject, and there are like current project members and there are like Wikipedia as a whole and they want different things. So these like newcomers to Wikipedia project. And according to the interviews and surveys, and they told us that they value mentorships, they want find the mentors in the project. They value mutual interest, they want to join a project that align with their own editing interest. But for the current project members, they value productivity. So they want to recruit people who eventually come and then actually be productive.
00:14:49.830 - 00:15:57.564, Speaker B: And the current Wikipedia project members, they also mentioned it's important to maintain the control of their Wikipedia project. And we also talked to like Wikipedia, like administrator in Wikipedia, they value like long term growth of the Wikipedia community. And one thing that particular mention is new member protection. So I will spend a little bit more time talking about new member protection because this is closely most, I guess it's more closely related to the fairness. So historically, like new member groups, new editor groups are like a disadvantaged group in Wikipedia because they tend to be mistreated by the algorithmic tools. So actually, for example, a quality control like system that has been deployed in Wikipedia community for a couple of years, and eventually it turned out that it harmed the motivations of a lot of new contributors who are still learning how to participate. So to put it in a technical term, it's just the false negative rate is extremely high for these new editors.
00:15:57.564 - 00:16:57.554, Speaker B: As a result, a lot of newcomers just left the communities. And when their edits were rudely reverted by the quality control systems, which eventually harms the overall growth of the Wikipedia community. So they really want us like when we create these tools, we want to protect these like new editors. So the question we're trying to ask is how can we design the intelligent recruitment system that can align with these multiple stakeholders needs and values? So when we talking about like intelligent system, like algorithm is essential for a component for any intelligent systems. But the traditional approach is to design an algorithm. It basically precisely defines the inputs, outputs and the design property of the system. And abstracting away all these complicated multi stakeholder contacts, these complex human needs and values.
00:16:57.554 - 00:18:05.256, Speaker B: And so we argue that like this like approach, trying to extracting a lot of this like messy real world, is probably not capable of really capturing this wide range of stakeholder values. So what we propose is that to really trying to integrate this like complicated multi stakeholder context in the design, or we just design the algorithm in this complicated real world. So the key of this issue solution is to shift away from a solution oriented approach to a process oriented one, which I'm going to talk more here. So we published this paper at CSCW 2018. So in this paper we propose approach to actually consider these different stakeholder value in the design of the algorithm. So the first step is we call it like we need to talk to the stakeholders, we need to understand their values. So instead of creating any algorithms, close the door like in the lab.
00:18:05.256 - 00:19:02.656, Speaker B: We need to first talk to the people and understand what their values and needs are. And the second step is to like, based on these understanding, want to use this understanding to inform the design in our initial algorithm. And then the third step is to try to again go out to the real world and to pilot testing and deploy these algorithms with the stakeholders. And we want to evaluate and iteratively refine the algorithms based on their feedback. So I'm going to walk through these steps on how we use this like four steps to create the recruitment system in Wikipedia. So first to understand the stakeholder values. So we, as I mentioned actually earlier, so we identify these three different stakeholders in the province, and then we define values like broadly as like what a person or group of people consider important in life.
00:19:02.656 - 00:20:28.406, Speaker B: And we sometimes use the terms like motivations, perspective, needs and interests interchangeably. And then we want to use these stakeholders values to guide the creation of algorithms. And we think that when we create an algorithm, so we should not only consider like algorithmic approaches, specifically in our case, like how to determine the fit between the like a new member and the project. So the reason is that the algorithm cannot be isolated from the data that fuse AI algorithm and the way how the algorithm is presented. So in our particular case, so we think like data preparation, like specifically like who to include in the data preparation is really important because it determines who our recruitment system will target. And also it's important to consider how to present the results to the stakeholders, that is, to whom we should communicate the algorithm output and the user interface for presenting this algorithm's outcome. So we conducted a literature review and a survey to identify the stakeholder values and how the major choices in data preparation, algorithmic approaches and output presentation, how they like, benefit or harm the different stakeholder values.
00:20:28.406 - 00:20:40.654, Speaker B: So there is, I know there are a lot of information on this slide. I'm not going to expect you to read all of them. I will give you some examples of how we use this table to come up with creation of the algorithm. Go ahead.
00:20:40.734 - 00:20:47.326, Speaker A: Just the list of stakeholders. You don't include Wikipedia users at all. So is that not an important reader?
00:20:47.390 - 00:21:53.854, Speaker B: So you mean readers? Yes, yes. Good point. So yes, we didn't like talk to readers in our video case. So one thing is like Wikipedia like as a whole would call it like all these like represented by the administrators, they do consider a lot about like their readers. And so we, and the second thing that like we also trying to like find some readers, but since they are not familiar with the whole editing community and the ecosystem, so they will not be able to provide very helpful suggestions on how these things work. But yeah, I think this is indeed, first, we think readers interest are sort of implied or implicitly there when we interview with these like Wikipedia administrators to understand the goal of Wikipedia as a whole. But second is some readers, they just, at least a few of them we talked to, they just could not be very helpful.
00:21:53.854 - 00:21:57.934, Speaker B: Any question, go ahead.
00:21:58.474 - 00:22:06.494, Speaker C: I guess you're going to tell us this, but what pool of people are you recruiting from the world or the people who've ever entered?
00:22:06.594 - 00:22:32.578, Speaker B: So they are already editors in Wikipedia, but they haven't joined any wiki project. Yeah, but then, yeah, I'm going to talk more detail about even among these people. So great question. I will talk about the answer. Okay, so Wikipedia project. So that's another thing. So like organically.
00:22:32.578 - 00:23:43.492, Speaker B: So any people, they can go to Wikipedia projects and put their names in a project member list and state that, oh, I self identify that I'm a member. But our recruitment system, which I'm going to talk more details is we will send these recommendations to the project organizers and these organizers will invite these new people to join their wiki projects. But that's actually a decision like after we're considering like different stakeholders and they will make that decision. So based on these understanding of the different stakeholder values, we create the initial algorithm. So I will first talk about how we balance these values regarding the data preparation, which is who to include. So basically we will include and who to basically recruit from. So we will recruit these editors from all these new people, all the editors who haven't joined any Wikipedia projects.
00:23:43.492 - 00:24:28.944, Speaker B: But these people, they still vary a lot in terms of their overall experience in Wikipedia community. Some of them might already be there for a while or very experienced, but some of them are like brand new editors. So what we found is that the stakeholders really disagree on whether the algorithm should target brand new editors. So what we found that a lot of current project members, they just don't like these brand new newbies. They prefer people with experience. They think that when you recruit these new people, they won't be productive after they join the project. However, Wikipedia as a whole and they told us it's extremely important to retain these brand new editors.
00:24:28.944 - 00:25:45.258, Speaker B: So actually the low retention of new editor has resulted in an overall decline in the active editor base. So this is one of the primary goal for Wikimedia foundation is trying to retain these brand new editors. So to address this tension between the different stakeholders, so we implemented called a multiple design to handle the varying preferences among stakeholders. Specifically, we included both the brand new editors and experienced editors in the data preparations. But we also ranked these two types of editors separately so that these new peoples do not like won't be overshadowed by the experience. And then second, consideration is about the algorithm approach, how to determine the fit so we conducted data analysis on hundreds of thousands of Wikipedia editors looking at their prior histories and looking at their participation in the wiki projects. And we identified two major ways to determine the fit between newcomer and the project, which is interest based, and the other one is relationship based.
00:25:45.258 - 00:26:57.614, Speaker B: So the interest based one is focused on the interest alignment between the newcomer and the project, and the relationship focus on the existing social connections or personal connections between newcomers and the current members of Wikiprojects. And however, when we talk to these stakeholders again, they disagree. And now this time they disagree about whether we should use a relationship based match or not. So a lot of these current project members, they are saying that oh, we are not very sure if the people who we recruit from based on this relationship based approach will actually be interested and be productive like when they join the project. However, those newcomers, they tend to like, they told us they will feel much more comfortable if they can join a project that they already have some relationship between the current member of the project. And as a result, we decide to actually balance these different stakeholders like values and interests. And we implement both interest based and relationship based algorithm.
00:26:57.614 - 00:27:40.618, Speaker B: So we created four different algorithms, two interest based and two relationship based. And the first interest based algorithm is a rule based algorithm. We simply computed the number of edits these editors made to the articles within the scope of wiki project. So it's a very simple and straightforward like metric. And the second interest based algorithm is called a category based algorithm. So we computed similarities goal between the editors editing histories and the topic of Wikipedia project. We also created two relationship based algorithms, so one we call the bound based algorithm.
00:27:40.618 - 00:28:55.614, Speaker B: So we calculate the social connections between the newcomers and the current member of projects. Specifically, we look at the number of messages they send, like on the wiki project top pages, Wikipedia talk pages, like how they so between the new people and the existing people of Wikipedia projects, we also create a co edit based algorithm which is computed like similarity of the edit histories of the newcomers to the edit histories of all the current members of a wiki project. And the last thing, the final thing we consider about is how to communicate the algorithm's output. So there are two major options we can go with. One is to directly invite the newcomers and the other is to communicate with the current members first. And based on what we talk with stakeholders and these current project members, they feel actually very strongly about directly inviting newcomers and they don't want us to directly invite these new people. And the reason they told us that they want to be in the loop and they want to maintain control of the inviting process.
00:28:55.614 - 00:29:46.476, Speaker B: So therefore, instead of directly inviting these potential new members, we create a user interface for presenting the algorithm outputs to the current project members. And we present the top recommendations for all the four algorithms separating experience editors and the brand new editors. And this is how the interface looks like. This is interface was implemented as interactive table on a Wikipedia page and it will include the basic information about each candidate editors. We also add like simple explanations on why we recommend this new editor to your group. And then we have this invite link so they can directly click it and then invite the new people to join their wiki projects. Also have this short surveys to get their ratings to the quality of the recommendations.
00:29:46.476 - 00:30:42.390, Speaker B: And they can also provide some, any kind of feedback. So now with this algorithm and interface for presenting the algorithm output, and we deploy it in the real world, in Wikipedia to see how it actually works. So we work with Wikimedia foundation and with Wikiproject organizers to deploy our recruitment systems. And over a six month period we evaluate over 16,000 editors and delivered four distinct batches of 305 recommendations to 18 wiki projects. And now we can evaluate these algorithms. So when we talk about evaluation of algorithms, we will first think about the algorithm accuracy. So usually this evaluation of algorithm primarily focus on the accuracy.
00:30:42.390 - 00:31:34.888, Speaker B: However, we think that the accuracy is just one step towards the path to where we want to go. It's not the end goals. So we think that to fully understand the effectiveness of recruitment system, we should also evaluate like stakeholders acceptance, especially in the context of Wikipedia, that people tend to be really like feel strongly about the interventions and especially intervention from the research community. So we want to know whether these stakeholders actually accept our recruitment system or not. And also we did a sort of evaluations on like the impacts of these recruitment systems on the stakeholders. So I will first talk about the evaluation on the algorithm accuracy. So we compare, we look at two accuracy measures.
00:31:34.888 - 00:32:06.194, Speaker B: One is the average rating. So remember in the interface we have this short survey and they can rate how good they think the recommendation are on a five point liker scale. And we also look at the invitation rate. So it's like the proportion of the people of the recommendations. Like these recommended editors were actually invited by the wiki project members. So the second one, invitation rates, is analogous to their click through rate. Yeah.
00:32:06.274 - 00:32:10.386, Speaker C: Did you track going forward the people who were invited?
00:32:10.490 - 00:33:15.128, Speaker B: Yeah, yeah, yeah. We also tracked like going forward. So which I will talk a little bit like at the end. So what we found is that after the inviting these current project members, after inviting these new members, they will actually continue to follow on like follow up with these new editors and answer questions and actually provide some additional help to these newcomers after they inviting these new people. So what we can see that actually the rule based algorithm, which is the simplest and most straightforward, straightforward algorithms perform the best, and the algorithm was the recommendation were rated higher and resulting in more invitations compared to the other three types of algorithms. And we also actually specifically looking at experienced editors versus brand new editors. And we found that the average rating and the invitation rate were not that different, were not different between the experienced editor and the brand new editor.
00:33:15.128 - 00:33:56.086, Speaker B: So this suggests that our algorithmic recruitment tool is not disadvantaging these brand new members, which something like a lot of administrators really care. Yeah, great question. Yeah, yeah. So the threshold we define brand new is like they made less than 100 edits, like in Wikipedia, the whole. So if you made like from one to 100, no matter how long they have joined. So that's another great question. So we believe that in Wikipedia the experience is accumulated by actual editing.
00:33:56.086 - 00:34:40.593, Speaker B: So if you like join for ten years, but you only made like ten edits, it still make you like a newbie because you still do not know how exactly to navigate the edit editing systems and how to interact with other editors. So we set up the threshold 100. This also is something we like talk with the Wikipedia communities. That's actually, they told us that's like the community consensus about the brand new editor definition of the new editors. So no question. And so this is our like evaluation accuracy evaluation. So the next thing we evaluate is the stakeholder acceptance.
00:34:40.593 - 00:35:33.364, Speaker B: So we interviewed both the current project members and the newcomers who were invited to the project. The feedback from the interview participants were very positive. They actually gave us confidence that our recruitment system is really acceptable to the communities. So this is my favorite quote from, from a current project member and said that this puts some science behind recommendations and will be a great supplement to the current processes. So this quote indicates that our tool has the potential to be well embedded into the current recruitment process, like seamlessly embedded in the current process for a lot of wiki projects. And this is a quote from new members who are invited to join the project. And this new message says, thank you for reaching out to me, me and thank you for informing me about this Wikiproject Africa.
00:35:33.364 - 00:36:35.996, Speaker B: I appreciate it. And in order to receive some feedback from the general Wikipedia community, and we also created a signpost, which is Wikipedia's internal blog, to describe our project. And this is a screenshot of some of the comments, like under our post, I'm not going to talk all the details, but in general the comments are very positive. Like very interesting, amazing creation. I'm really excited by the potential for routing editors toward active subject focused working groups. So our collaborator Aaron Hafaker, who is like a principal scientist at Wikimedia foundation, and he told us that this is really seen like such unanimously positive responses in the Wikipedia community, because the Wikipedia community tend to disagree on everything. And we also found some evidence that our project actually have some good, like some positive effects on the relationship between the like research community and the Wikipedia community.
00:36:35.996 - 00:36:55.820, Speaker B: So actually this is a screenshot of some recent discussion around the Wikipedia policy on Wikipedia is not a laboratory. So this first editor said that like editors who are using Wikipedia in any way for experimentation may be banned. So it's like feel very strongly that you should not use these.
00:36:56.012 - 00:36:57.224, Speaker D: It's not a policy.
00:36:57.764 - 00:37:54.026, Speaker B: Yeah, so this is not a policy, this is a discussion like opinion. So this is some people, it represents some people inside communities, like really feel strongly that like Wikipedia is just a way to summarize the existing knowledge in the human knowledge. It's not a way to create a new knowledge. But then there is another editor who responded saying that there are a number of experiments done in good faith to increase the quality of the project or the experience of editors, editor retention, et cetera. An example of a recent one is Bobo Zero three, which is my PhD student, the Wikiproject editor recommendations. So yeah, I'm happy to see that our projects seem to become good examples of doing something like a research community, actually doing something really good for the community. We also conducted like a mixed methods evaluations on the impacts of algorithmic tools on stakeholders.
00:37:54.026 - 00:39:01.542, Speaker B: So in the interest of time, I'm not going to talk about all the details. So basically what we found is that our algorithm can identify newcomers who are more suitable and the current members did additional investigations on our recommendations and selected the most promising newcomers. This also means that our algorithm tool might have like room for improvement. So to reduce these editors like efforts on the additional investigations and some editors project organizers do complain with us that I spent too much time looking at all the recommendations you send me, which distract me, prevent me from doing other more fun things in Wikipedia. And there are also current members provide additional help and mentorship to the invited newcomers, which led to more contributions from these new editors. So this is our evaluation of the three aspects of the algorithmic tool, and in sum we made these contributions. In this case study, we generated this knowledge of the stakeholder values regarding intelligent recruitment system.
00:39:01.542 - 00:39:52.770, Speaker B: In the context of Wikipedia, we created four different algorithms for identifying suitable new members, and we design and deploy the working systems in the field. And the evaluation shows that this system is well accepted by the community stakeholders. We also demonstrate the potential of a general design process for incorporating these stakeholders needs and values into the creation of algorithms. So next I will spend a little bit time to talk about some of our ongoing work on the AI based management or quality control system. I'll be very quick. So basically, Wikipedia nowadays are like starting to adopting more and more AI to manage the activities of editors they use. The recruitment system is something we created and they have the evaluation system, which I'm going to talk about.
00:39:52.770 - 00:40:49.374, Speaker B: And they have the regulation system and the task automated task routing and assignment systems. And we found this AI based management system, not just on Wikipedia. We see that this AI system manipulates what posts you see on Facebook. It's basically automatically identify and censor these unwanted content like hate speech, racist posts. And these algorithmic systems also manage people's activity on these sharing common sites like Uber, Lyft and Tasrabbit. And a lot of people envision that in future, a lot of our human activities will eventually be managed by these algorithms, intelligent systems and bots. So it's extremely important to see how we can create this AI based management system in the right way to incorporate, really reflect and respect people's value.
00:40:49.374 - 00:41:58.924, Speaker B: And this is one of the ongoing projects we do, which is to redesign the machine learning based working evaluation system on Wikipedia. So every day there are about 160,000 new edits coming to english Wikipedia alone. And these edits just immediately go live and currently, Wikipedia have this machine learning based evaluation systems, which is the core of the system is ours, which is a prediction model to predict the edit quality and the outcome. The outputs of R's will be used in a lot of semi automatic editing interfaces, as well as some like automated agents, like anti vandalism bots. And the whole system will evaluate the quality of the edits and then take corresponding actions like reverts, warning or rewards. So the challenge of this machine learning based evaluation system is that first, again, it's actually operating in diverse work contexts. It has been used in anti vandalism, using new page protection, also used for like a Wikipedia education program to evaluate these students contribution.
00:41:58.924 - 00:42:36.884, Speaker B: And in each case we see different stakeholders, and that there is tensions between different stakeholders values. For example, some people, like some stakeholders value quality control. They want to catch all the possible potentially like damaging edits. But there is other stakeholders more value, like member protections. They don't want to falsely accused any well intentioned editors. And again, there are like fairness and accuracy, like trade off. So Aorus has been like already like received reports that their system, so their evaluation systems were biased against anonymous editors.
00:42:36.884 - 00:43:20.782, Speaker B: But we know that once we improve fitness, the accuracy would drop. So there are a lot of tensions between different goals. So that's one of our current projects with Stephen Wu here. And we want to handle great ways to handle these inherent trade offs between different stakeholders goals in this machine learning based evaluation systems. And we recently received some good news about, like, our grants are recommended for funding. Yeah. And the whole idea is like we're trying to see, first, again, we talk to the stakeholders, understand their goals, and then work with them to formulate and then to define their goals.
00:43:20.782 - 00:44:29.214, Speaker B: And I'm not going to go into details. And then we want to capture the trade offs. So we're going to work together to create a family of models with a spectrum of trade offs. And then we can present all of these models to the stakeholders. But we use the way novel technologies to visualize the trade offs, to allow people to navigate all these hundreds of different models and identify this tradeoff, and eventually help them to negotiate and find the solutions to balance with different stakeholders tradeoffs. So that's, again, we think that we want to create empirical knowledge, novel technologies, and novel socio technical solutions to balance the trade off between different stakeholder goals. So this is some, like, we actually talk about how the applications on the algorithmic content curation, actually a lot of social media sites are trying to use these algorithmic system to automatically identify sensor like hate speech, adult content, misinformation, racist sexist ads and so forth.
00:44:29.214 - 00:45:37.724, Speaker B: This is just an example of a recent algorithmic system like Tumblr and how it actually falsely flag a lot of non sexual random posts as adult contents. But this is an interesting question, because there are a lot of inherent trade offs on how you define these hate speeches and adult content. There is tensions between different stakeholders values and different goals, which potentially is something we trying to address in the Wikipedia context and we can apply it here. So, yeah, so I guess the big picture thing is. Yeah, I guess the whole idea is this is the final slides. So we want to integrate a wide range of human needs and values, including fairness, in the design of these AI technologies with broader goals to improve people's lives and accomplishing these collective goals. And I think it's important to have these interdisciplinary efforts.
00:45:37.724 - 00:46:18.064, Speaker B: I think there is a lot of opportunities for the collaborations between the fat star like ML community and HCI. And we are the kind of people who actually go to talk to work with the real people in real system, and then we can like actually work with machine learning researchers to identify, like, problems. And then I think it's important to have the partnership industry and there is also opportunity to create this general methods that can achieve these goals. Okay, so thank you for listening to my talk and I'm happy to answer questions. I don't know if we have time for questions.
00:46:30.774 - 00:46:38.062, Speaker C: Is there a quick explanation why the Rubens algorithm actually is the best in terms?
00:46:38.118 - 00:47:02.774, Speaker B: Yeah, that's a great question. So, actually, one thing I think is really, they can understand the rule based algorithm. So the fact that they can understand this, like they seem to, like, make them like, trust the results more. Yeah. So there is a trade off of understandability and maybe the accuracy and like, the algorithm.
00:47:12.704 - 00:47:19.064, Speaker A: In algorithmic fairness, transparency, control and participation. Perspectives by Min Kyung Lee from Carnegie Mellon.
