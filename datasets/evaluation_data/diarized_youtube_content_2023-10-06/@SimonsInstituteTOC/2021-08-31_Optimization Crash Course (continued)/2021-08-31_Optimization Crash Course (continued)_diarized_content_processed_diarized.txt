00:00:00.120 - 00:00:54.740, Speaker A: Everyone never given a talk in a mask before. So hopefully this is going well. It's hard to communicate all the information, but. Okay, so let's just recap where we left off. So, part one left off with the following picture of first order methods. So, if we have a function that we are trying to optimize and it is strongly convex and has Lipschitz gradients, it's smooth, then we have this upper bound given by gradient descent, this somewhat intuitive method where we just simply go orthogonal to the contours of our level sets. And we also have this lower bound, and there's a gap here on the order of the square root of the condition number, the condition number being l over mu.
00:00:54.740 - 00:02:05.152, Speaker A: And we see that for poorly conditioned problems, this can make a huge difference. So the question when this lower bound was presented in 1983 was, can we close this gap? Which algorithm would do that? So with that prompting the core concepts we're going to cover for this part of the tutorial will be acceleration. Be clear what I mean by that, randomized methods and mirror descent. So, acceleration pretty much starts off with this idea proposed by Polyak, which is called this heavy ball method, this algorithm, where instead of just going in the direction of the gradient or the negative gradient, I add this scaled previous history of my iterates. And there was very much a nice dynamical systems perspective to this. So we can think about. And Polyak introduced this algorithm as an effort to discretize the following ode.
00:02:05.152 - 00:03:17.574, Speaker A: So we have some sort of dynamical system that's second order in time. I take two time derivatives. So by double dot here, or by x dot, I mean the time derivative of my position at time t. So he introduced this algorithm as a discretization and argued that something like this should be able to achieve the lower boundaries. And the idea is just, if we observe the motion of a system traveling along this trajectory, there's this, it mitigates the zigzagging that can occur. And he actually produced some bounds that are settings for which I think if you're strongly convex and you start off near the minimizer, you can show like such an algorithm will achieve this really nice property of getting the square root of the condition number scaling. The trajectory of the picture looks a little bit like this.
00:03:17.574 - 00:04:17.644, Speaker A: As we discussed, gradient descent bounces off these contours, but momentum, you take, what is this gradient direction as well as you consider the vector of your previous history, and you take some sort of combination of them. So you're able to mitigate this zigzagging motion, this of course, though, was hard to make rigorous. And so that same year, such a notion was made rigorous by Yuri Nesterov. So Yuri Nesterov proposed a method that was claimed to accelerate gradient descent. What does that mean? The basic idea is to wrap the gradient descent sequence in something which will improve its performance. So we have gradient descent that has this nice invariance to the step size. So in both settings, I just need a scaling of one over l.
00:04:17.644 - 00:05:22.464, Speaker A: And if instead of just moving in that direction, I take again this combination of my previous history, I can get that one over the square root of the condition dependence on my convergence. He did this in both settings, actually. So for the smooth case, you have this beta k given by this interesting choice, k minus one over k plus two. And in the strongly convex setting, we have a nice ratio of this condition number here. After introducing it, the question was, why this choice? Why specifically this algorithm? What is the intuition behind that? And whenever you have mathematicians seeking intuition, you have a flood of people offering their perspectives from whatever community they come from. And so that indeed is what happened and has been happening. So there's just a series of perspectives as to why such a scaling should work, and what is the scope of this, extending it in various other settings.
00:05:22.464 - 00:06:02.034, Speaker A: And I couldn't pretend to go through all of these, so I'm just going to mention them here very briefly. So there is again, this dynamical persistence perspective, and this was made a little bit more concrete by suboid and Candace. I'll loop back to that one in particular. There's the perspective of a Chebyshev polynomial. So if you have a quadratic, then there's some nice intuition based on how the eigenvalues are scaling that you can actually show concretely why excel or give intuition as to why this would work. There's an SDP formulation formalized by Jori in Chabool. There's this linear coupling framework by Alan Zhu.
00:06:02.034 - 00:06:52.302, Speaker A: There's geometric shrinkage, there's a lot of perspectives, geometric shrinkage being you have these bounding ellipsoids that you try and shrink in an intuitive way. And of course, there are a lot of frameworks to accelerate algorithms writ large. So it is not just gradient descent which can be wrapped in a sequence and accelerated. And so there are tons of frameworks. I'm going to focus on a framework that I think will be helpful when we think about how to extend this concept in more abstract spaces. So hopefully this will match align well with what subit will be discussing for geodesic spaces. And this is also a framework that has been useful for, for accelerating other algorithms.
00:06:52.302 - 00:07:44.636, Speaker A: So basically a template for acceleration, or one template rather, is that we couple some kind of dual step. This will be a mirrored descent step, and I'll discuss what that algorithm is in the next segment or the last segment, and some baseline algorithms, a primal step. And there have been many, many baseline algorithms that have been accelerated. So we have gradient descent. We have this algorithm that I'll discuss at length, mirror descent, proximal gradient descent. So we saw that in the last session, cubic regularization of Newton's method that was also introduced abstractly in the last session. And there are other techniques where you can use higher order derivatives of your function and create some sort of model, some sort of surrogate for it and just simply minimize that.
00:07:44.636 - 00:08:32.708, Speaker A: And all of these come with a specific rate of convergence. And just by wrapping it in a dual kind of coupling it with a dual algorithm, a lot of these can, their convergence rates can be systematically improved in certain settings. So there's a whole industry, and this is non exhaustive, this is just a small snippet of what's been going on. Um, so I'm going to, you know, whenever you talk about acceleration, it becomes hard to not get in the weeds. I think I'm going to give you an illustration that I hope will be helpful again in the geodesic setting. So, you know, we have to get a little bit more in the weeds. So there's this format for acceleration as three sequences.
00:08:32.708 - 00:09:44.376, Speaker A: So here we have a coupling step which consists of performing some kind of convex combination between a sequence Zk and x k. We have a gradient step from YK, and then we have what I'm going to just broadly refer to as a dual algorithm taking some combination of the gradient and the previous direction. That is another way of writing down that two sequence accelerated gradient descent. So we just introduced an extra one. This can be collapsed, of course, if you stare at a lot of the proofs, which are very lengthy, very very lengthy, and you try and look for the quantity that is dissipating along the trajectory of the algorithm. It has this basic format, it's some scaling of the function or the optimality gap, plus some scaling of this distance. And basically what most proofs do when they work from this perspective of let's just write down something that decreases, is you start with just arbitrary sequences, ak and bk.
00:09:44.376 - 00:10:55.930, Speaker A: You write down this difference, and then you use properties of the algorithm as well as the function values to try and get this dissipation property satisfied. So if you do that, you will get some sort of relationship between all these coefficients, these abstract coefficients, and the coefficients for the energy function. So this construction that I'm demonstrating was introduced by Nesterov in zero four and improved very recently by on and Shra to try and get this phenomenon working in the geodesic setting here. What's going on is if you try and bound this energy, you can do so if you have the following update. So it looks a little bit complicated, but we can break this down into two steps. And the first step, you're solving some sort of relationship here with our gamma k sequence based on what Bk and AK is at the state k. And then you use this gamma k to update ak plus one and bk plus one.
00:10:55.930 - 00:11:32.532, Speaker A: And then you use those two things to update the parameters. That's just basically it in abstract terms. So it's highly algebraic. I don't know what more intuition I can offer outside of that, but it's useful to see, because as I said, this will be discussed again. Okay, so let's see why this gives us these rates of convergence. So let's look at how ak plus one and bk plus one are changing. So I'm telling you that this sequence ensures that we get our dissipation property.
00:11:32.532 - 00:12:10.894, Speaker A: So we get Ek plus one is less than or equal to Ek. So we get this decreasing sequence. So if I just simply write that out, I get something that looks like Ak. Here, let me write down. So we get Ak over Ak plus one. And now if you look at how it's being updated, this is just simply one minus gamma k. Very similarly, we can write down with Bk plus one over Ak plus one is.
00:12:10.894 - 00:13:01.814, Speaker A: So I took this lyapunov kind of or decreasing identity and just divided it through by the coefficient, scaling the function value to get this bound, this upper bound. And I'm just trying to understand what the convergence rate is, I can conclude from what these values are telling me in terms of gamma k. So that's why I'm going through this. So if I look at Bk plus one and how it's also changing, I'll get one minus gamma k times gamma k squared times l. I have some sort of sequence that has one minus gamma k. I can pull out in front there's an extra gamma k squared times l scaling that distance function. But if that's well behaved, I can conclude a one minus gamma k rate of convergence a or at least the product of them.
00:13:01.814 - 00:14:14.994, Speaker A: And so what? So what Anand Schra showed is that for any arbitrary value in this starting value, this gamma k, if you solve this equation, is actually converging at a geometric rate to one over the condition number, if the condition number is infinite. So if you're not strongly convex anymore, then we can look what happens to this sequence if you have this term becomes zero. And then you look at how the Bk is being updated. Bk plus one is just simply going to be equal to Bk. The first equation is telling me that gamma k squared over one minus gamma k is equal to bk over lak. And the second one is telling me that bk plus one is equal to gamma k squared over one minus gamma k times lak. So if I take this over here, this is equal to Bk.
00:14:14.994 - 00:14:45.814, Speaker A: So the gamma k's remain constant at the initial value I start with. And also from this, you can infer that the gamma K's are evolving at a one over k rate, and Ak is evolving at a one over k squared rate. So this gives you kind of a. It covers both settings. I could just write the accelerated algorithm in this one format, where all I have to do is solve this recursive equation. So hopefully this will be revisited soon when we talk about geodesic spaces. But I think this construction is nice to see.
00:14:45.814 - 00:15:09.570, Speaker A: And in the end, what I can infer from this then, is that I get this rate that scales as the product of these one minus gamma is. And of course, gamma is converging to one over the condition number. So I could start with any gamma and it will converge. Okay, so that was a little bit technical, and it doesn't offer you much intuition. Yes.
00:15:09.682 - 00:15:17.792, Speaker B: So was this a new feature of this work to actually get both of the cases at once? Like, this was usually studied separately.
00:15:17.898 - 00:15:45.244, Speaker A: So actually, no Nesterov showed. Or in his 2004 book, even you can get both cases, or even in his recent book, got both cases at one. The new feature is this initial starting point for gamma. There was a bound he placed on it, and they got rid of the bound. You can sell it from any arbitrary gamma. Yes. Potentially smaller than one over squared.
00:15:45.244 - 00:15:58.032, Speaker A: Smaller than one over squared. Yes, yes, yes, yes, yes, yes. It should be smaller. Yes. Because it converges upwards. So, yeah, sorry about that. This should go in this direction.
00:15:58.032 - 00:16:46.144, Speaker A: All of these. Are there any other questions about this construction? So this, you know, typically when people offer coverage of multiple settings in one algorithm, it can be called universal or adaptive to whatever the setting is. So this was the adaptively set accelerated gradient descent. So let's get back to intuition. I kind of like thinking about this idea. You can tell my favorite interpretation is this one offered by suboid and Candez as modeling a second order ode. This is Polyac's early intuition just made a little bit tighter by introducing a specific ode and trying to understand its properties.
00:16:46.144 - 00:17:40.214, Speaker A: So, this is a very beautiful paper. I encourage you to read it if you're at all interested in the accelerated gradient descent phenomenon. But essentially what they did is they took the continuous time limit of the algorithm and showed that it can be said to model this second order ode. So this is just one continuous time limit. There are multiple ways to go from a discrete sequences to continuous time, but this ode has nice properties, so they use that to motivate the acceleration phenomenon. So, as I stated, it's like taking gradient descent and adding this acceleration term and this dissipating quantity, which scales as three over t. And the mysterious thing was, why three over t? Why this damping term? They showed that if you have two over t, this actually no longer converges.
00:17:40.214 - 00:18:22.334, Speaker A: It's what's called overdamped. And then if you are larger than you are under damped. Hopefully I got that right. So you get this kind of nice scaling of the damping term. But I think one of the nicer things about this ode is they certified that it itself is in continuous time, minimizing the objective function at a one over t squared rate. And they do this by using a Lyapunov function, which they introduce. And so there's this nice kind of technology of looking at these dissipating quantities in continuous time and showing their links to discrete time.
00:18:22.334 - 00:18:56.888, Speaker A: I myself may have been involved in some of this. So I think clearly this perspective is being offered here in this tutorial. Um, but I do think it does come up and has been utilized in more abstract settings. So that's pretty much what I wanted to cover. Acceleration will kind of come back when we discuss randomized methods, which is our next topic. So I'm going to pause here and take, or just see if there are any kind of questions. Okay, fantastic.
00:18:56.888 - 00:19:55.414, Speaker A: I will move on. And please stop me if you have any. Okay, so these deterministic methods are really nice, but generally speaking, in machine learning settings, we are in this big data regime, where we're often tasked with minimizing a function that has the form of this sum. And when n is large, it will become hard to compute full gradients because we have to sum over all these f I's. There are many ideas for how do you actually perform minimization, but a lot of them are basically of the form. Take a sample from your sample space, and then just simply compute the an estimate of the gradient direction using that sample. So, if you use the full sample, what you get is full batch SGD, or aka gradient descent.
00:19:55.414 - 00:20:41.818, Speaker A: You can compute an element of the sub gradient. So this is subgradient descent. And then the big idea by Robinson Monroe in the fifties is just SGD, which is just compute the gradients on that sample. That's SGD. And the trajectory, you can see it can be quite noisy, but each computation is significantly less expensive. It will scale, it'll be independent of n, whereas GD scales linearly in n. That's the basic construction of SGD, perhaps the most popular method in machine learning.
00:20:41.818 - 00:21:45.642, Speaker A: So, let's get to the theoretical side of this. I can show you many plots of SGD, and there'll be a plot or two, but what can we say about its performance in the settings we've discussed so far? So, one thing, one classical result, or set of results about SgD is the following. So, if you have each fi has is smooth, so it has Lipschitz gradients. And if f, the objective function is strongly convex, then what you'll get is that the gap between the function evaluated at what's called this polyak Rupert average iterate and x star will be in expectation decaying as one over root k in the convex setting, and the condition number over k in the strongly convex setting. And notice that these step sizes are different now. So there's not this invariance to both settings that we can establish via the step size. So we look at these two rates.
00:21:45.642 - 00:22:19.038, Speaker A: They're significantly worse than what we were getting before. So, like, one over root k is a very, very, very slow rate of convergence. Often in practice, we. We see, you know, not that, but nevertheless, this is the upper bound, and there are matching lower bounds. So we cannot do better than this. But there is a big difference between SGD and full GD. So we have this one over k difference, and then one is decaying linearly, and one is just one over k.
00:22:19.038 - 00:22:58.896, Speaker A: So, I mean, this is. This is quite large. Okay, so, in rush, the researchers, how do we make STD better? What can we do? So, the first strategy is just to revisit a very classical idea. I can accelerate all sorts of algorithms. Why not just accelerate SGD? And so there have been several authors who have looked at that and have attempted to do that. And by acceleration, of course, I mean the classical add this coupled sequence and add this dual step. But, you know, SGD saturates this lower bound.
00:22:58.896 - 00:23:48.790, Speaker A: So what you're able to do is improve the initial convergence, but at the end of the day, you get the same sublinear rate for this dominating noise term, so you cannot do much better using momentum. That said, people do it in practice. So if you look at, if you train a neural network, the heavy ball algorithm is often employed, and it often, or sometimes does have nice practical performance. So just because theoretically it's not doing much, doesn't mean that it's not, you know, not worth using. The second strategy, which I'll discuss a little bit more at length, is this idea of variance reduction. So there are many methods for variance reduction. I've just added three, I'll cite some more.
00:23:48.790 - 00:24:36.548, Speaker A: And it will also be non exhaustive, because this has just been a booming industry since I think, like 2012. So hard to cover at all. But the basic idea in a nutshell is we can perform lazy gradient updates, or at least enlarge our storage requirements, store all these previous gradients, and utilize that to get a systematically better rate. What you'll classically see is scalings like this. Each problem will depend on the dimension, because we're computing vectors at scale SD. We'll have this, you know, we have this linear rate and this polynomial rate. But the big difference here is how the dependence on n for each algorithm.
00:24:36.548 - 00:25:29.048, Speaker A: So, as I said, SDD is independent of n, because it's just sampling one thing at a time. This is not batch SGD gradient descent will have this condition number scaled by nice. And these variance reduced methods very classically have this dependence that scales, as in plus the condition number. So we kind of get this, what they say is the best of both worlds kind of phenomenon, which is what happens when you do this Lasik gradient update. As I said, there's this very big and growing library of methods. This is a non exhaustive list. There are way more, but there's SVRG, sag, mesophony, sara, saga, CCA.
00:25:29.048 - 00:26:18.890, Speaker A: I mean, I couldn't cover them all. And I think people, when they're talking about more abstract spaces, have made attempts to bring these algorithms to bear in these settings. The two algorithms I will discuss, because I want to give you some intuition as to what's really going on here, is SVRG and saga. So, SVRG, this is one format of it. There's a double looped format of it. So I'm just going to present this one. So what's the idea here? So, so, the estimate I of the gradient that I'm going to use is going to be the stochastic gradient at my sample, computed at the current iterate.
00:26:18.890 - 00:26:54.126, Speaker A: So this would just be SGD, but I'm also going to be adding the stochastic estimate computed at another point, Wk. And then I'm going to also be subtracting the gradient at Wk. So we can see that this is an unbiased estimate for the gradient at x k. I take the expectation, these two terms go away, and I'm left with the expectation of this. Because this in expectation is equivalent to the gradient. There should be no ik here. So in expectation, I'm doing the right thing.
00:26:54.126 - 00:27:39.958, Speaker A: This is an unbiased estimate. But the idea is this will hopefully have less variance than if I just had SGD. So how does wk evolve? How is wk selected? Wk will be equal to x k with probability one over it. So with very small probability. If wk is x k, I'm computing a full gradient. So an expectation, hopefully I'll be computing on the order of in gradients with the remaining probability. So one minus one over n, it'll just remain the same.
00:27:39.958 - 00:28:35.788, Speaker A: So I'm doing these lazy updates for this estimate that, in a nutshell, is SVRG. And there are nice complexity guarantees that pretty much in expectation match what you would get for the deterministic setting across various function classes. The proofs are a little bit more involved, so I'm not going to go through them at length today. There are other things I would like to highlight, but this is the basic intuition. Saga is a little bit different. What saga is doing is it's going to be storing all the previous gradients I had, I've accumulated, and then just simply updating certain parts of it as I go along. So, one has a very large memory requirement whereby I'm storing all my previous gradients.
00:28:35.788 - 00:29:20.626, Speaker A: The other one has a much nicer memory requirement. But I am computing full gradients every now and again. And so that might be untenable if I don't ever want to compute a gradient. Okay, so those are the two kind of templates for how these variance reduction techniques work that I wanted to review today. And let's see how they perform in practice. You know, it's hard to show plots, because, you know, I can, I can make a lot of plots that look like a lot of things. So hopefully this, this plot does illustrate, though, that these methods do tend to work well for problems in certain regimes.
00:29:20.626 - 00:30:09.938, Speaker A: Typically convex problems where in is n is a reasonable, has a reasonable scaling. But the difference here could be dramatic. So these two upper lines are SGD and gradient descent and saga is doing significantly better. So if we just upped our storage requirements, we can do much, much better. That's where this best of both worlds phenomena or characterization kind of comes in. Of course, these methods can also be, again, accelerated. Most things can be, a lot of things can be accelerated.
00:30:09.938 - 00:31:10.964, Speaker A: So there are a lot of frameworks for doing this. Catalyst is a really nice framework that was introduced by Lynn Mariel Hartui. Hopefully I pronounced that right, and they used it to essentially give accelerated versions of all these methods. So what does that mean? What does it mean to accelerate in this context? It means that I can go from this dependence here, which was the condition number dependence, to a square root n times the condition number dependence. So in certain regimes this will behave much, much nicer. And people have also introduced frameworks coming from the intuition that they brought to bear in the deterministic settings, their estimate sequence frameworks, Lyapunov frameworks, and what have you. And so again, these constructions, these proofs, a lot of them look very, very similar to the deterministic settings where, where things are just k in expectation.
00:31:10.964 - 00:31:14.208, Speaker A: Yes.
00:31:14.336 - 00:31:35.724, Speaker B: So it seems that even though they're all using the same oracle, they all have different constraints. But if we just take maybe the intersection of all those setups, right, we just. Okay, they all like, you know, somehow if I prove a low bound for this is going to apply to all of them. Is there a low bound that's established that says, for example, that the root end should be there in front of the root?
00:31:36.544 - 00:32:17.644, Speaker A: Yes, there is indeed a lower bound for the root in conditioning. And there's also this version of things requires actually each fi to be convex if it's non convex. This was called the sum of non convex problem. That dependence of root in it scales us into the, I think, three over four. So there's, you have to be precise about which regime you are in. Nevertheless, there are lower bounds for each of these problem settings. And these accelerated versions do meet the lower bounds, so cannot necessarily be improved.
00:32:17.644 - 00:32:54.870, Speaker A: I'm going to show another plot of these accelerated versions. It comes from this Driggs paper, this, this last paper I'm citing. So this is kind of what you'll often see. Resolution is not great. The plot is essentially saying that. So there's saga in the red here, and its acceleration version is in the dotted. And essentially we all have, the thick lines are going to be the non accelerated version, and the dotted are the accelerated version.
00:32:54.870 - 00:33:31.294, Speaker A: And of course, across all these examples, the dotted line is below the thick line. So you'll see often plots like this, which is meant to indicate that even in practice, adding these kinds of momentum based frameworks to your descent estimate is going to be helpful. If you, of course know some of the problem settings, it could be also disastrous. Have some misspecifications there. I don't think it's necessarily worth parsing which ones beat which ones in which setting. They're all pretty good.
00:33:33.354 - 00:33:40.442, Speaker C: Could you say a little, since this is new to me, could you say a little bit more about an example of a misspecification that could make this type of acceleration?
00:33:40.578 - 00:34:18.248, Speaker A: Yeah. Yes, yes, absolutely. So, a lot of these methods will depend on knowing the condition number to determine the coefficients in front of the algorithm. If you do not know that condition number specifically, if you underestimate it, you can diverge and you get also wild oscillatory behavior. They're restarting schemes to adapt to this. But you do have to know specifically those kinds of things in the Lipschitz constant as well, to make sure your plots behave like that. But of course, you had no, the ellipse is constant for gradient descent, so.
00:34:18.248 - 00:35:13.854, Speaker A: But it does. There are papers which indicate that gradient descent is way more robust to noise than accelerated methods, momentum methods way more stable to noise. So SGD is still widely used and very, very popular in the stochastic setting. Okay, so the last topic I wanted to cover is hopefully going to be a really good merging into what Nicolas is going to talk about, which is romanian optimization. And that's because mirror descent is a very special, structured case of that. And so, and it also has brought much benefit to the optimization landscape. So, I'm going to introduce some core concepts of manifolds.
00:35:13.854 - 00:36:05.744, Speaker A: But Nicholas is going to do a much, much better job, way more detailed, at a much slower pace, probably, than I will. So, I do want to preface that here, that if this wasn't the introduction you would like, you're about to get another one. So hopefully by the second round, it'll have absorbed in your psyches here. Okay, so, let's start by introducing the concept of a manifold, very informally. So, we say that a manifold is a space which locally looks like r to the d. So maybe the picture to have in mind is a sphere where locally I'm standing here, it looks flat, but globally it does not. And so some examples to have in the back of our minds, because they'll come up in the context of mirror descent.
00:36:05.744 - 00:36:50.790, Speaker A: Is this positive orthod. So, this is the set of vectors in RD whose components are positive, the simplex. So this is the set of vectors whose components sum to one. This is widely used in the context of probability and the space of positive semi definite matrices. So these are symmetric matrices that are pse. At every point, x has a tangent space, which is the set of all tangent vectors at that point. And I guess my rendering of x looks a little bit different than I had originally.
00:36:50.790 - 00:37:47.804, Speaker A: But yes, the tangent space is just a set of directions. I can move in a small amount and still remain in that space, very informally speaking. So the tangent space of the positive orthant is just all of RD for the simplex. It's going to be the set of vectors that sum to one. So now why is that? What's the intuition for the tangent space? So, informally, I want to understand the set of directions which remain in X for small t. So if I look at this, look at the sum I equals one to d of this. For it to remain on the simplex, this has to sum to one.
00:37:47.804 - 00:38:38.574, Speaker A: I already know that this piece here sums to one because x is on the simplex. So what this is telling me is that t times the sum over these directions, one to D has to be zero for small t. And so what this means is that the sum of these directions, these directions, have to be zero. So that's that condition here. And so you kind of can informally derive why these conditions or why this characterizes these tangent spaces. And so for this PSD matrices, the tangent space is simply the set of symmetric matrices. So every tangent space has a romanian metric.
00:38:38.574 - 00:39:42.654, Speaker A: This is a symmetric and positive definite form, and it gives us a way to measure angles on this tangent space between two tangent vectors. And the reason why it's important to think about this metrics in the context of mirror descent is in the special case where x is a subset of our d, we can actually represent the metric by a positive definite matrix. So I can say that. Here we go. So this rendering of gx becomes now I'm going to say the norm of b at x, which is going to be the inner product of g of x n. So I have this, this now matrix which is defining this norm on the tangent space. And the special case that we're going to talk about in the context of mirror descent is what's called a hessian manifold.
00:39:42.654 - 00:40:31.256, Speaker A: This is a manifold where the metric is given by the Hessian of a strictly convex function. So this g here is the hessian of a strictly convex function. Okay, so why? Well, so let's go through some examples, and here I'm going to ask for your input. So we have again, this positive orphaned where we have this log determinate barrier metric. So it's going to be written as the diagonal of one over the component squared. So what is this the Hessian of? Yes, so it's the Hessian of this log function. What about this Fisher metric? This comes up all the time on statistical manifolds.
00:40:31.256 - 00:40:59.334, Speaker A: This is very popular. This is probably the example to have in the back of your head. So what is this, the Hessian of? Yes, entropy. That's great. Thank you. And just by definition, this is just, the log determinant metric is the Hessian of the log debt. So for PSD matrices, this is a very nice metric to produce on your space.
00:40:59.334 - 00:41:44.064, Speaker A: Okay, so I just need two more objects and then I'll have gotten, I'll get to the conclusion of what is mirrored ascent and what is some intuition we have for mirror descent. So the first object is the notion of a differential. This is a cotangent vector. It's a linear functional mapping from the tangent space to r, and it gives the directional derivative along any direction b. So I can kind of write that out explicitly as the inner product of the gradient nb. So this is the definition of the differential. And if I have this definition in mind, I can then define what is the gradient.
00:41:44.064 - 00:42:51.974, Speaker A: Finally, on my hessian manifold, which is the object I am after. So the gradient of f at x is going to be the tangent vector corresponding to this differential. So it's going to be the tangent vector, which makes these two things equal. So on the left hand side, I think I have space here on the left hand side, actually, let me, I have df, the differential of x and v, which I've already told you is the inner product of the gradient at x and v. And on the right hand side, I have this metric between the gradient of f and v, which I can write as a matrix, because I am on a hessian manifold. So this gx of grad x and v is going to be just being a product of grad x times this gx. This is just transpose here.
00:42:51.974 - 00:43:46.568, Speaker A: Okay? So I just need these terms to match. I need this to match this for all d. And so from this I can actually conclude what the gradient is on manifolds that are substance of our d. In order for this to match, it must be the case that this part is equal to the gradient or nabla f, the set of partial derivatives, which means that my gradient is given by the metric inverse times the gradient of f. Okay, fantastic. I've explicitly written this out here hopefully that intuition was enough for deriving why this is the case. Why do I scale the gradient by the metric inverse? And so the gradient flow is just the vector field chosen to be the negative gradient direction.
00:43:46.568 - 00:44:21.694, Speaker A: So this is just going to be this equations dynamic system here, where my time derivative of xt is given by the negative gradient direction. So the metric inverse times nabla f. That's the gradient flow. Okay, fantastic. So now we have what the gradient flow is on a hessian manifold, on a hessian manifold. The gradient flow is now given by the metric inverse, sorry, the inverse of the metric. So this is the Hessian of h times the gradient.
00:44:21.694 - 00:45:33.254, Speaker A: And so, when discretized, this is actually an algorithm that does get used quite often in practice, called natural gradient descent. This was proposed by Mari. And I mean, actually, this is pretty well known. He's probably not the first person to have proposed this, but it's been very, very well studied and is the motivation of a lot of other algorithms that try and approximate this gradient flow. Okay, so we have natural gradient descent, but what is mirror descent? So, mirror descent is a special case of something that happens when we map our space to another space via a bijective smooth map. So let's say I have a space x, and I want to map it to another space, v, via a mapping phi. Called it, call it phi.
00:45:33.254 - 00:46:31.344, Speaker A: Um, and it's, it's bijective and smooth. It's got an inverse, I'm going to call that psi. So I have a gradient flow over here, some kind of curve that's not really, it's not squiggly, but it's fine. We can pretend that's the gradient flow there, and I can track how this gradient flow changes, the push forward under this mapping, so I can track how the function transforms, how the gradient changes. This is just going to be given by the jacobian of this inverse mapping times, the function applied to that inverse evaluated at z, as well as how the metric changes. So I have a metric on this space g. I want to look at the push forward metric under this mapping, mapping x to z via this phi map.
00:46:31.344 - 00:47:14.484, Speaker A: So I have these three objects, and with that I can write down what the gradient flow is on, on z. So the gradient flow is going to be given by this equation. And the key thing here is we have this jacobian inverse multiplying our metric. It becomes a little bit more complicated if the jacobian isn't nice in some way. But something really nice happens when I don't have this abstract mapping, but I have a mapping that is given by the gradient of a strictly convex function. This is what's called a mirror map. And hopefully that'll motivate this notion of mirror descent.
00:47:14.484 - 00:47:53.504, Speaker A: So I have this strictly convex function. I can define its legendre dual via this optimization problem here. And this, this formulation, this legendre dual function has really nice properties. So even if h is not convex, its dual function is, is convex, its double dual is also convex. So I can take the dual of the dual and I get a closed convex function. So this gives rise to differential young inequality. But also I can write down now what this metric is.
00:47:53.504 - 00:48:47.550, Speaker A: So this metric is going to be the inverse of the Hessian of this dual function. The Jacobian is just going to be this partial derivative applied to this mirror mapping. When we write that out, that's going to be the Hessian again of this dual function. Now, when I write up what the gradient flow is, this Jacobian and metric are going to cancel each other. And what I get is this really nice dynamical system in the dual space. So I got just z dot is equal to the negative grad f at x t. So that's the motivation for some of the terminology behind neurodescent.
00:48:47.550 - 00:49:16.254, Speaker A: It's a descent flow in this dual space. That's what that abstractly means. And so, well, I haven't told you what mirrored ascent is, but now I hopefully will. So we have this dynamical system. It's given by this gradient flow in the stool space under this mirror map. And I can discretize this, I can discretize it in many ways. But the most basic thing I can do is what's called an Euler discretization.
00:49:16.254 - 00:50:07.912, Speaker A: So I just simply evaluate the vector field and there's limited space. So I just simply approximate this vector field via over ETA, let's say for small EtA. So as Eta goes to zero, this is hopefully a faithful approximation. And then I evaluate the vector field, which is going to be the gradient vector field at the current point. And so if I do this discretization, I get the algorithm called mirror descent. And of course, when things are nice, so when h is Euclidean, then that primal space is RD. With this Euclidean metric, the dual space is the same space with the Euclidean metric.
00:50:07.912 - 00:50:21.084, Speaker A: And this whole thing just becomes gradient descent. So in some sense, mirror descent, well, not in some sense, it is a generalization of gradient descent, although I suppose there are many generalizations. So, yes, this is. Yes.
00:50:21.904 - 00:50:23.752, Speaker B: So I just got lost for a second. I just want to.
00:50:23.808 - 00:50:26.364, Speaker A: Yes, yes, please slow me down. Thank you.
00:50:27.434 - 00:50:43.054, Speaker B: Mirror descent is natural gradient flow on the image under this map. And the function that you're taking, the hesitation of is this h star.
00:50:43.794 - 00:51:15.742, Speaker A: Yeah. So, okay, one thing to keep in mind. So it's, I have two spaces, I have x, I have a dual, and I'm taking this mapping grad h. But the inverse of this mapping to map. Sorry, I called these both x. This is z. To map an element z back into x, that's given by the gradient of this central dual function.
00:51:15.742 - 00:52:08.354, Speaker A: So these two things are acting as inverses of each other. So the statement is that the gradient flow, that natural gradient flow on x, when mapped under this mapping, becomes mirror flow. This dynamical system over here, I can discretize this in the primal space, and I get natural gradient descent. I can discretize this over here, and I get what's called mirror descent. So that's hopefully more in words, what's going on, that picture that's being painted. But of course, this distinction kind of collapses when h is the Klingon norm, because this becomes the identity map here. So x is being mapped to x.
00:52:08.354 - 00:52:16.124, Speaker A: The gradient of that is just x. Okay, so hopefully things compute wonderful.
00:52:16.784 - 00:52:29.392, Speaker B: So apart from the obvious advantage that you don't have to inverse a hessian when you do mirror descent as opposed to natural variant, if I were to not pay for any of this, would they have the same guarantees? Or there's an example where there's a substitution.
00:52:29.528 - 00:53:10.624, Speaker A: Yeah, so the, the picture isn't as clear. Typically, we do have a lot of guarantees for mirror descent. In fact, merit ascent behaves almost exactly the same as gradient descent. Natural gradient descent is a little bit different. And the reason for that is the following. Okay, so let's think about gradient descent or gradient flow. So we have this dynamical system, and we have, let's say, some sort of function, this optimality gap we're trying to optimize.
00:53:10.624 - 00:54:14.804, Speaker A: If I look at the time derivative of this, that's the gradient of xt times the xt dot. So that's going to be negative gradient f of x t. So a descent flow, natural gradient descent, has this feature where if this vector field is given by natural gradient descent, what I'll get, sorry, this should be squared. What I get is this with the hessian metric. But mirror descent doesn't have that. Very similarly, if we look at the distances as the distance, as this kind of descending factor, it behaves nicely. So the distance we'll be working with is a Bregman distance, which I haven't really quite gotten to, but a lot of proofs of mirror descent feature this Bregman divergence as something that's contracting, that's dissipating.
00:54:14.804 - 00:54:38.854, Speaker A: And this matches well with mirror descent and not as well with natural gradient descent. So these two things actually are a little bit different. And it's exposed by the fact that this is a dual algorithm. Something is going on having to do with this distance function. And this is the primal algorithms making the function descend.
00:54:39.594 - 00:54:44.570, Speaker B: So the difference is not just in discretization, it's also in the continuous, but.
00:54:44.602 - 00:55:47.314, Speaker A: You know, in the continuous one, I can like, they're equivalent, right? So I mean, morally speaking, if I have this ode, I can also write this as the mirror descent ode ddt of grad h t x t is equal to the negative gradient direction. So these two formulations, they're the same ode. So the continuous time distinction, because I can write it both ways, doesn't really show up. I can plug in both values, but when I discretize, then all of a sudden there is a difference there. And what's required of h to bound that error term, it might be slightly different between the two. So, but you know, I don't pretend to have the full story in discrete time. Someone who works on this, you know, Sebastian Bubach, somebody can please hopefully inform the crowd or leave a comment if this is posted somewhere about the difference in discrete time.
00:55:48.414 - 00:55:59.290, Speaker B: Yes, you already mentioned fragment distances, so this very sense looks a bit like a generalized version of fragment iterations, of fragment regulated algorithms. Can you comment a little bit on the relations there?
00:55:59.402 - 00:56:40.124, Speaker A: Yeah, in fact I think that'll be two slides. And if it doesn't come up in two slides, please ping me. But I think what you're referring to is what's coming up in two slides. So I think hopefully this is what you meant by this. There are the discretizations, as I mentioned, that you can take of these odes. I had mentioned what's called the forward Euler or the explicit Euler method, where I evaluate the vector field x k. But if I evaluated discretize this by evaluating the vector field at xk plus one, what I get is called the proximal Bregman method.
00:56:40.124 - 00:57:07.404, Speaker A: And that proximal Begman method can be, can be written in the following form. It's just establishing that x k plus one is the argument of this potential. So this, the function value plus this Bregman distance. And so this is actually a generalization of the proximal mapping I had introduced in part one of the talk. Is this what you were referring to when you mentioned Bregman.
00:57:09.504 - 00:57:11.204, Speaker B: Discretization that you had before.
00:57:12.664 - 00:57:46.704, Speaker A: Exactly. Yes, absolutely, absolutely. So what he's saying is mirror descent can be written as if I, instead of minimizing the function, minimize the inner product between the gradient at x k and x, then this is just mirrored ascent. But I just decided to write it in this dual format. So it could be written in many ways. Both of them can be written using Bregman distances. So these Bregman distances show up a lot when we're talking about these dual discretizations for both, for both of these.
00:57:46.704 - 00:58:43.964, Speaker A: So I just wanted to have one word about Bregman projections, or this proximal Bregman method and Bregman projections in general. So it's possible to do all the things we've talked about involving construction constraints a little bit more generally using dragon distances. That's kind of the punchline here. And then I just want to give you a somewhat illustration of why this all matters. So sometimes the correct or oftentimes if you, if you do something in the right geometry, it can make a significant difference for convergence. So the first thing I do want to mention though is that this Bergman projection, we can define a generalized version of the projection operator via the following minimization problem. So instead of minimizing this euclidean norm, I can minimize this Bregman divergence.
00:58:43.964 - 00:59:49.564, Speaker A: And again, there's this nice relationship between this proximal Bregman mapping and the Bregman projection, namely that if my function is the indicator of the set, then this proximal thing is equivalent to the Bregman projection as this non expansive aspect. This kind of triangle inequality we had it for the euclidean distance, also holds for the Bregman distance. So in some sense all these things are, there's just more general descriptions because we can move slightly to more abstract spaces. So the question is how far can we push these concepts? A lot of things follow through or hold when we move into hessian manifolds. Can we move into further abstract spaces? And hopefully the next two speakers will shed light on these kinds of questions. But a lot of things do map nicely for hessian manifolds. And finally, the algorithm I just wanted to feature is this projective mirror descent algorithm.
00:59:49.564 - 01:00:44.310, Speaker A: It's written in, you know, a certain format, but I could write it via a Bregman distance again, and it's just simply the update where I have this, what would have been a mirror descent step. But I just simply project that using the Bregman projection operator onto my set X prime. And that's my new iterate. So all of these things commute with euclidean spaces, just define it a little bit more generally here. And yeah, small remark about dissipating quantities. Typically all, you know, we were talking about dissipating quantities being a nice organizing principle for how we analyze a lot of iterative methods. In optimization, those same dissipating quantities hold for these more general spaces.
01:00:44.310 - 01:01:27.628, Speaker A: Typically we just replace euclidean distances, sorry. With Bregman divergences, hard to drink water, the COVID time. So that's unfortunate. Sorry about that. A simple example that maybe you can check for yourself if you're interested in checking your intuition, is this distance, this quantity here, which would appear in gradient descent, does dissipate for smooth functions when h is one strongly come back. So this would normally be a euclidean distance. I replace it with a Bregman divergence, and this thing dissipates from mirror descent.
01:01:27.628 - 01:02:19.234, Speaker A: So mirror descent has this really nice technology to it. There are tons of energy functions of dissipating quantities we can associate with its algorithms. So for the last slide, I wanted to give an example that I actually took from a recent work by Boyd, John Ducci, or Stephen Boyd, John Ducci and Merck Blanchi out at Stanford. And they showed that for some certain classes of problems, and I mean many people have shown things like this, but for certain classes of problems, the geometry can make a huge difference. So here's this robust regression problem. It's an LP, we have this constraint set, which is the simplex. I have this LP defined over the simplex, and here they've just constructed an instance of this by just certain approximations.
01:02:19.234 - 01:03:07.554, Speaker A: And then they plotted the performance of the sub gradient method where you use the euclidean norm versus that subgradient method, where you use the Bregman projection, and you can see there's a significant difference. So, using the geometry, kind of paying attention to the geometry of the space you're optimizing over, can make a big difference when it comes to optimizing your function. So it is really nice that we're having this workshop, so hopefully we'll learn a lot more about this and we'll all start thriving methods that will solve our problems. Okay, so with that, I'll pause and I'll take questions. But that's pretty much what I wanted to say, and I really thank you for your attention.
01:03:13.734 - 01:03:50.104, Speaker C: Yes, so I had two follow up questions after your nice explanation about sometimes downside, potential downside of acceleration. One is, could you say anything about the methods used in Practice to estimate the condition number and second for the last example you were talking about for the mirror gradient descent, it is. I'm just trying to understand, you know, how one would be choosing the age. Is one intentionally trying to choose the age to get a better condition number? Or I guess, yeah. Could you say a little bit about.
01:03:50.444 - 01:04:43.924, Speaker A: I'll take it into reverse order. So in some Sense, oftentimes the best h to use the function itself, that would be Newton's method, right? Or I mean, not mirror descent, but the primal form of it, natural gradient descent with where h is f is just NeWton's method. That is really nice because it does, we have affine invariant that gives you really, really nice properties. So there is something to be said for choosing the right age to do optimization on and for statistical manifolds, entropy turns out to be a really, really nice thing to use. And often if you do are working on simplexes or anything like that, that would be the natural thing. So, but I don't know if I have or I'm aware of a formal optimality statement with regard to how to choose h. If anybody here knows of one, that would be lovely to hear.
01:04:43.924 - 01:05:45.442, Speaker A: I would learn quite a bit, but I'm not quite sure there's anybody who knows. So the question is, if you have space, and I'm going to do some sort of romanian descent or the hessian manifold, how do I choose h? What is the optimal way of choosing h such that I converge quickly? Yeah, that could be, I mean, hopefully we'll have four discussions about that. Um, furthermore, for the, for the question about the conditioning of the problem, uh, there are a lot of, there is some work I'm aware of that targets choosing l. And oftentimes, you know, one over two l is the nice, or one over l is the nice thing. But we do line searches when we can. Maybe not for stochastic methods, but even for stochastic methods, people have recently tried to propose some form, some kinds of line searches. So unclear that you need to know l.
01:05:45.442 - 01:06:30.464, Speaker A: Exactly. I am going to be omitting someone's work in this. So hopefully you can again leave a comment and answer this for me with regard to how to optimally or how to estimate l, some of the nice ways of doing that. So the strong convexity constant is actually a bit harder to estimate. So in the context of acceleration, what people do is this thing called adaptive restarting. So people observe that when you hit a period where you're not choosing the appropriate damping term in a strongly convex setting, you tend to oscillate really quickly and what they do is they just restart the algorithm. That's one way of handling oscillation, perhaps to not knowing the problem parameters.
01:06:30.464 - 01:07:10.020, Speaker A: I'm not quite sure. And yeah, gradient descent, you don't need to know. You, you just need to know in the stochastic setting you do, you might need to know, be a bit, just one over l is the setting the step size? That's kind of my knowledge here. But again, I would really appreciate learning more about how people do solve this problem. If anyone has work or knows of work, please, please discuss. Of course. Right.
01:07:10.052 - 01:07:18.284, Speaker B: So I guess now is the time for a break until 02:00 p.m. And we're going to resume with those two talks. And I think I said again, great.
