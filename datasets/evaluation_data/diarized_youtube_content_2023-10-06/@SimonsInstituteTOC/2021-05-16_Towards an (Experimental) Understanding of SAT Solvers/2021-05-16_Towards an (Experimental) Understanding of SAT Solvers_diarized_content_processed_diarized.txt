00:00:00.280 - 00:00:29.838, Speaker A: Our next speaker is. I apologize in advance for mispronouncing most words on the slide. Laurent Simon from Bordeaux. And people probably know you most as a person who wrote glucose, which been one of the more popular solvers over the last years. And let me not take more of your time. And please have a floor. Very happy to have you here.
00:00:30.006 - 00:00:36.314, Speaker B: Okay. Thank you for the invitation. So, Vijay said I had bad results.
00:00:36.774 - 00:00:38.714, Speaker C: No, not bad. Negative.
00:00:39.934 - 00:01:47.480, Speaker B: Well, in fact, no, if I have only negative results, it's about the last performance of glucose. So I think we are making progress in the experimental studio of SAt solvers. And I would try to convince you that it's something very important where we should put a lot of effort in the next year. So I will start by addressing the problem, what is making progress in Sat. And then I will try to explain why it's so difficult to explain the behavior of Sat solver, why they are so hard to understand. I will give you some positive results about some particular parameters, like the centrality of the variables in the benchmarks or the modality of the problems. And also, before concluding, I will talk about some more recent work we have conducted in trying to understand what kind of CDCL, what kind of proof DCL server are working on.
00:01:47.480 - 00:02:30.770, Speaker B: So, are we making progress? Each time I make a token site, I give an example of the cactus plot saying, okay, you see, the performance of the solvers are increasing. We are better and better. So it's not the most recent cactus plot we have. And so, let's say now I have a new problem and I want to. To use a sasat solver. Do we have to use the winner of the last year competition? Let's say Kisat, for instance. So, in fact, let me just see here it is the cactus plot on the planning track competition last year.
00:02:30.770 - 00:02:52.924, Speaker B: So could you guess who are these guys? So, one guy is Kisat, one guy is. Is the winner of the. Of the track. And one guy is old glucose that is two or three years old now. And in fact, there is no suspense. Glucose was the best on all the benchmarks from the planning track. It was very surprising for us.
00:02:52.924 - 00:03:14.054, Speaker B: And because, in fact, Keset showed so much performance on the. On the main track. And if you look at the scatter plot of glucose against QSAT, you see that, in fact, all the new strategies in Kisat are not paying for planning problems.
00:03:14.434 - 00:03:14.866, Speaker C: Okay.
00:03:14.890 - 00:03:54.976, Speaker B: So, in fact, are we making progress? It's not so clear. In fact, I think the new competition should be understanding self servers. This is a new competition. It's a classical sentence in machine learning also where they want to have more and more accuracy. Are we comparing the solver like we would compare the 8th of letters to Richmond? So. And also our experimental sections are often designed to convince reviewer. So do we have it all wrong? And is it recent? In fact, it's not a recent problem.
00:03:54.976 - 00:04:23.684, Speaker B: In Sat and in constraints. There was a paper like almost 30 years ago by John saying exactly that. Testing heuristic, we have it all wrong. And he said exactly what is happening right now. The competitive nature of most algorithmic experimentation is a source of problems that are all too familiar to the research community. Competitive testing tells us which algorithm is faster, but not why. And we have confused research and development.
00:04:23.684 - 00:04:46.794, Speaker B: Competitive testing is suited only for the latter. So I'm not fully. I don't fully agree on this last sentence, but. And there is also an alternative to competitive testing, like empirical science, at least since the days of Francis Bacon. So this is where we could go in the community and it's.
00:04:47.334 - 00:04:50.874, Speaker A: Jakob has a comment in the chat if you like to.
00:04:53.694 - 00:04:54.590, Speaker C: Okay.
00:04:54.742 - 00:04:56.114, Speaker B: No, it's not working.
00:04:56.454 - 00:05:03.328, Speaker A: Okay, so I could just read it or Jakub, you could say it.
00:05:03.496 - 00:05:24.724, Speaker B: Yes, yes. Niklas says that also. I mean, he's staying on some problems. That very particular problem where minisat is still working very well. Even the LBD stuff is not working on some problems. Yes, you're right. Okay, so you see.
00:05:24.724 - 00:05:26.340, Speaker B: I don't know what you see.
00:05:26.532 - 00:05:33.924, Speaker C: Laurent, can you. Yes, make it full screen? Something is blocking your screen.
00:05:34.044 - 00:05:34.652, Speaker B: Yeah, okay.
00:05:34.668 - 00:05:36.212, Speaker A: Yeah, now, good.
00:05:36.308 - 00:05:37.804, Speaker B: When I open the chat.
00:05:37.884 - 00:05:38.624, Speaker C: Okay.
00:05:41.684 - 00:06:40.974, Speaker B: So also, if we really understand what CDCL server are doing, maybe we can go towards ten times improvements in performance. We have made the following observation. I mean, if you look at unsat problems at the end, only 50% of the learn clause are useful to derive the contradiction. And also only 20% of NEET propagations that you perform during your search are used during conflict analysis. If you take these two points, you can observe that if you have an oracle that say do this resolution and this resolution and this resolution, you will be ten times faster. So in fact, are the SAS solver very efficient. 90% of their time is just some kind of heuristics that just probing a lot of unit propagation everywhere just to gather some interesting conflict, interesting clause.
00:06:40.974 - 00:07:16.120, Speaker B: So it's not so impressive if you look at that. And why is it hard to understand what we design? In fact, we are not anymore in a look ahead world. I mean, in the look ahead world, it was almost easy. No, sorry. It was almost easy. I mean, if you have a new id, you just put your heuristic, compute some score, and if you are better, then your heuristic work is good. Now, the state of the solver is based on all the past of the solver.
00:07:16.120 - 00:07:52.224, Speaker B: And it's very difficult to understand if this is something that you added or is it some kind of noise or some kind of new stuff that you should. So, the CDCL solver are changing our understanding. And the way they are solving problems are nothing in common with a human strategy. I mean, it's not like when we want students to learn to code. We say, okay, just solve the problem on a paper and then put in a computer what you have done in a paper. Servers are not doing that. I mean, they are doing something that we don't understand.
00:07:52.224 - 00:08:29.484, Speaker B: It's not the same. They are very fast, unpredictable. They work well on real world instance, but nobody knows what is a real world instance. What are the characteristics of benchmarks that comes from the real world? All components are also tightly connected. It's very difficult to analyze one component without having any side effects elsewhere. And there is no, probably no one simple reason explaining their performance. So, there was some study on trying to understand the CDC servers.
00:08:29.484 - 00:09:39.674, Speaker B: Probably the first studio was from Kathebi Sakala and Marques Silva, where they show that here you have a cactus plot here. This is a minisat with CDCL. And if you remove the learning mechanism from minisat and you keep everything else the same, the heuristic and so on, then you have this solver with this performance. But if you keep the learning, the learning engine, and then you remove the seeds and you have a much more simple heuristic, you have this performance. And if you remove the two watch, you have this and restart this. This suggests that once again, the learning mechanism is the most central piece of solver. There was also more recent work in this spirit, where, I mean, with a few other authors, we systematically test many different configuration of such servers to see if we can observe something on a set of problems that were well known.
00:09:39.674 - 00:10:13.334, Speaker B: The idea of these problems were that, I mean, you could scale the size of this problem, and then you could observe how well strategy could perform on more and more larger problems. What we observe is more or less the same. I mean, you do have to learn clues. You don't have to forget too much. If you forget too much, it gets hurt a lot on some very particular problems. And also there was some unsuspected factor, the key factor of disease. That was very important for some problems.
00:10:13.334 - 00:10:54.968, Speaker B: So if we look back at asset solver. Why they are so complicated? In fact, at first they had two components, conflict analysis and branching. Of course, these two components are tightly connected heuristics. But then after that we added cruise database cleaning strategies and restarting strategies, and then pre processing and processing. And then a lot of strategy inside of the strategy. So now if you want to win the competition, you have to add variable elimination, block and close elimination. Maybe get detection to get rid of some particular problems.
00:10:54.968 - 00:11:01.484, Speaker B: Vivification, xor reasoning, equivalency, class simplification, failures.
00:11:01.784 - 00:11:02.048, Speaker C: You.
00:11:02.056 - 00:11:46.790, Speaker B: You have also a very large ecosystem of branching strategy where it's very important on the very recent problems to have this local search activated. So it's very complicated. And sat solver are more and more complex as the years goes. So we are not progressing in our understanding. We are just making our black box more and more complex to study. Also, when we explain the conflict analysis, we have this very classical figure where you have decisions, here you have propagation, you propagate literals according to clause. I will not go into too details, but this is a classical conflict analysis that we show on paper.
00:11:46.790 - 00:12:25.384, Speaker B: But in real life, is it like that? No, this is a conflict analysis done in glucose. On a real problem it's much more larger. You have like 100 resolution. Here in red it's a conflict, the conflicting clause. You have a lot of decision here that are not related. But then here you have a chain of propagation, so that there is a structure, but also it's much more complicated than what we have on paper. Also it's very difficult to have something because all the benchmarks are very different, but very very different.
00:12:25.384 - 00:13:06.988, Speaker B: There are outliers everywhere. Here we counter the number of decisions that we have made after 10,000 conflicts. On a number of interesting problems. As we see here, there are some problems like 5% of the problems where you need 1000 decisions before reaching conflict. On some other problem, on 10% other problem, you have like 20 decisions before reaching Kung fu. Clearly, I mean, the explanation of why CDCLR are good on these problems should not be the same as why they are good on this problem. And you have outliers everywhere in number of decisions.
00:13:06.988 - 00:13:38.698, Speaker B: If you count the number of decisions per conflict. So you reach a conflict, you back jump, and then you have to make a few decisions before reaching the next conflict. Here you see that on some problem you have to make like 20 or 100 decisions before reaching the next conflict. On some problem you just backtrack and then make a decision and reach a conflict. Or even like you have a successive conflict without decisions. And the same for propagation. On some problem you have an incredible number of propagation.
00:13:38.698 - 00:14:11.848, Speaker B: You spend all your time in propagating. On some problem you don't propagate at all. So it's very complicated to have a uniform way of explaining all this good performance. So let's look at VC's. V seeds is probably the most studied component of SAT solvers. So you all know VC's. You just bump variables that you saw in the conflict analysis and you exponentially increase the score of the bump.
00:14:11.848 - 00:14:58.038, Speaker B: And in minisat, after each conflict, you multiply the bump factor of the bump ratio by 1.05. And what is striking here is that you forget a lot like after in minisat, if you do that after 100 conflicts. So it's like one 10th a second. I mean, it's nothing in time already. If you bump a variable, it will be bumped by 100 times larger than before. If you count the number of conflicts that you have to do before reaching this value. So the bump will be this value larger than before that.
00:14:58.038 - 00:15:34.804, Speaker B: You see that on minisat after 13,000 conflicts, you have score that are this large. So you have to rescale everybody. If you change the bumping factor, it's much more often. Okay, so what, what I want to say is that in fact the idea of e seed saying you want to branch on the most recent is something good. But here you see that it's very fast, it's so dynamic, it's incredible. I mean, okay, so it's very hard to understand. And also it's, it's much more tricky than that.
00:15:34.804 - 00:16:20.804, Speaker B: What is a good variable for CDCL? For DPLL we know, I mean, it's something that splits. But for CDCL it's not clear because you branch on the variable, but if you branch on it, you will have a few chance to see it in the next conflict. So it was very important and then you propagate. It's in the top of the search tree, but at the end you are doing resolution on other variables. And so the score of this one after 10,000 conflict will be like zero. And it was yet the most important variable in the formula at some point. So for VC's we have some good results.
00:16:20.804 - 00:17:01.524, Speaker B: In fact, we observed since a long time that there are some structure. There is a structure in industrial problem on random formula, there is no structure. It's just a cloud of points. So this is a graphical representation of the formula where each point is a clause and there is a link between two clause. If they share these variables and you have a link if they occur in the same clause. So here on boundary model checking, you have this kind of structure, you see a lot of structure here. So this gives the intuition of the modularity.
00:17:01.524 - 00:17:50.438, Speaker B: And also we observe, we tried to work on another measure, the centrality of the variables in the formula. So let's look at the graphical representation that we used for this experimental study. So in fact we had this b parted graph which was directed, and if a close here, if vaporables occur negatively in a close, then you have an outgoing edge from the close to the literal. And if literal, if variables occur positively, so one, then you have an incoming.
00:17:50.486 - 00:17:52.342, Speaker C: Edge from the close.
00:17:52.398 - 00:18:26.970, Speaker B: Okay, so you have nodes, you have not per liter. So it's just a graphical representation. The idea was to, to have a representation of what would be a propagation in the graph. So a propagation in a graph would be like a path in this graph. And so this is a graphical representation of very simple formula. And what we used is the well known page rank algorithm to have an estimation of the centrality of the variable. So this is a very simple formula.
00:18:26.970 - 00:19:22.988, Speaker B: And if we compute the centrality, we see that there is a central variable here and the central clause here. So what we have done is that we took a number of problems from the, from an old competition. So it's, it's already like almost ten years old experimental study. And, and we compute on this original formula, the centrality of all the clothes. And then we keep this centrality and we watch the, the way the solver was working on this benchmark. What we observe is that the variables taken by vcids were so automatically in CDCL. I mean the most picked variables by vcids were the most central variables in the original formula.
00:19:22.988 - 00:20:09.576, Speaker B: Okay, so this is average totality of all pickled variables against all variables, okay, so they are more central. Also, what we observe is that the propagated variables are also most central than the average centrality of the variables in the formula. Which means that the CDCL is branching on central variables but also propagating more on other sound. What we observe also is that so each time you branch, you propagate and at the end you will have a conflict clause. This is the empty clause. What we show here is we take the centrality of the conflicting clause against the centrality of the learned clause.
00:20:09.680 - 00:20:10.296, Speaker C: Okay?
00:20:10.400 - 00:20:53.252, Speaker B: So here what we observe is that in fact the learn clause is much more central than ten times more central than the conflicting clause. So what could we do with that? What can we do with that? In fact, we can try to represent the way the sat server is working. Here you have all the variables in your formula. They are arranged in communities. We see that in more detail after that. And then, so here the communities are connected by the frontier. And the frontier, the variables that are on the frontier are the most central variables in the formula.
00:20:53.252 - 00:21:27.804, Speaker B: Because if you want to draw a path from this cluster, from this community to this one, you have to go through this bridge here. So the more central variables are at the frontier of the communities. So here we branch on the central variable here. So this is a decision, there is a propagation, there is another day decision. We propagate here and here, and at the end here we propagate again. We have a conflict clause. So the conflict clause is inside the cluster, because the centrality of the conflict clause is smaller than the centrality of the land clause.
00:21:27.804 - 00:22:21.476, Speaker B: And what could be said is that in fact the learn clause is learnt at the frontier, because the centrality of the land clause is much more central. Okay, so this is some kind of experimental explanation of what is going on inside soldiers, okay? And then we backtrack and we propagate again. So there was also some work on community structure by Carlos and Setoguy and Jesus Girardez Cru. So the notion of community is that you have more links inside the communities and outside the communities. And, okay, so there are sparse connection between groups. And what they observed is that thanks to a parameter, the modularity of the instance. So it's an array of results.
00:22:21.476 - 00:23:05.084, Speaker B: But don't be afraid. What does that mean? That means that on all these kind of, on all these problems, the average modularity is pretty high. So that means that for most of the problems there are some kind of structure. So all the problems are modular if you just look at them. Okay, and also here, let me explain just one line. So here the idea. So there are two graphical representation variable based on just variables that are linked together, or a bipartite graph with closed and variable incidence graph.
00:23:05.084 - 00:23:43.534, Speaker B: Here you have the modularity of the original instance after the preprocessing. So the processing is not destroying the modularity. And then after learning this amount of clause. So that means that the learning mechanism is a little bit destroying the modularity. Here the numbers are smaller, but not so much. Okay, so the learning mechanism is made according to the modularity of the instance. We also conducted a few experiments on the modularity of instance and the performance of Sat solver.
00:23:43.534 - 00:24:38.464, Speaker B: So we looked at the formula, we partitioned the formula in, in modules, in communities, we marked each other to which community it belongs. And then at runtime, when glucose was running, we compared the, for each clause, we compare the lbd of the learned clause with a number of communities built on top of this clause. And what we observe is that the LBD was pretty related to the number of communities. Here you have the number, here you have the lbd of each land close. So this is a heat map. And here you have the number of communities of the loan clause. And then there is a very clear correlation here.
00:24:38.464 - 00:24:52.958, Speaker B: So, of course, we have some problems that are not so clear here. These problems, okay. Or we also have problems where it's nothing happening and the committee doesn't give a clue of what the SAT solver is working on.
00:24:53.086 - 00:24:53.874, Speaker C: Okay.
00:24:54.254 - 00:25:16.978, Speaker B: Okay. So more recently, what, what we studied is the CDCL proof, okay? So let's simplify the, let's simplify the CDCL. The CDCL is just loop where you learn a clause and until there is an empty clause, and sometimes you will delete some clause because you have to make room for that.
00:25:17.026 - 00:25:17.522, Speaker C: Okay?
00:25:17.618 - 00:26:14.124, Speaker B: So it's a pretty big simplification. And what we studied is what we call the resolution graph of the solver. So each time you have a conflict analysis, you take a number of clause, like c one, c two, c three, and by conflict analysis, you learn this new clothes here. So we just keep track of the ancestors of each clause. And we wanted to represent this graph. So we wanted to see, and at first we had no clue of the shape of the graph. Was it close to a forest, to a tree, to a dag? And so we took a very simple problem like solving less than three or 4 seconds, and we tried to represent this graph and we were very surprised to obtain this kind of very dense structure.
00:26:14.124 - 00:27:01.294, Speaker B: So here, intuitively, it's input clause that are at the end. And here in red is the most recent clause. So here we observe also the history of the solver, where it starts the search here and then includes more and more close from the input to go there. And then here there is the close. So, one question, when we saw that is why the curve was, why the figure was incurred like that, and it was in curve, because here, you see there, there is a lot of very dense, that there is a very dense kernel in the graph. So we wanted to see that. We wanted to assess this density.
00:27:01.294 - 00:27:42.662, Speaker B: And for this we use a notion of graph named Cacor. So the notion of kecor is to see if you have very dense kernel. Insect cytograph. The idea is that you remove, for instance, if you have a graph of cornice three. That means that if you remove all the nodes that have degree less than three and you iterate until there are just the remaining nodes have a degree of three, then you have a k core of three. And so the remaining clothes are inside the k core. And the cornice of the graph is three.
00:27:42.758 - 00:27:43.514, Speaker C: Okay.
00:27:44.054 - 00:28:10.788, Speaker B: And we were surprised to see that, in fact, on real problems, the cornice was very big, I mean, the year. So each point in dark is a, is a selected problem. So it's not a very difficult problem. But we selected 60 problems from the last competition and we shuffled them. So each lighter point is a shuffled version of this problem.
00:28:10.956 - 00:28:11.596, Speaker C: Okay.
00:28:11.700 - 00:29:12.224, Speaker B: And we observed that, in fact the, the average corners of the, of the proof was 1000. That means that in fact the graph is incredibly dense. And it was very surprising. So this is also the cacor size. So you see that you have a reasonable number of clothes in the cacor, like 5000 or 6000 clothes in the cacor. And, okay, well, how much time? So we observe that, surprisingly, even if the cacor is peri dense, there is just, in the cacor, there is just like 75% of the clothes that are useful at the end to derive the contradiction. So you have in the proof clause that are highly connected to other clothes, but that are useless for deriving the contradiction at the end.
00:29:12.224 - 00:29:24.612, Speaker B: And in the, in the k core, you have like 68% of the clues that are learned and you have like 30% of the clothes that are original clothes.
00:29:24.708 - 00:29:25.388, Speaker C: Okay.
00:29:25.516 - 00:29:27.664, Speaker B: I'm not detailing everything, but.
00:29:28.044 - 00:29:28.668, Speaker C: Okay.
00:29:28.756 - 00:30:09.562, Speaker B: And maybe more intriguing. So we wanted to see if we can use the keco for something, because it looks like something very important. That was very surprising. And we use the cacor to try to predict which variables will occur at the very end of the proof. Okay, so we made these experiments. We run glucose on the formula, we measure the number of conflicts needed by glucose, and then we start again, stop at half of the computation, compute the cal core at half of the computation. So, and then from this kecore, we selected a few variables.
00:30:09.562 - 00:31:00.842, Speaker B: Just take the most frequent variable in the kecore. And we wanted to see if these variables would be present at the very end of the computation. And so here we selected so at half of the computation, just five variables. And here it says that over the 60 problem, in 27 of the problem, one of the five variables occurred, at least one is the 20 last loan clause of the problem. So at the very end of the computation, we were able to predict that the clause, that the variable will occur at the inner clause. And we see that we have some very nice results. Like if we allow ourselves to predict ten variables, we can detect maybe half.
00:31:00.842 - 00:31:50.524, Speaker B: We have 50% chance that one of the 10th variable will be in one of the 50 last clause at the very end. So it was like something very encouraging. So as a conclusion. So it's complicated to understand sub solvers, and it's also very frustrating because we know how to design a very efficient solver. But I think that everybody has his own explanation of why they are working. We don't have a proper explanation of why this seed is working, and they are very fast, they are unpredictable. And what we need is a real experimental study of CCDCL server.
00:31:50.524 - 00:31:56.878, Speaker B: I mean, instead of running for the competition, we should run for understanding.
00:31:56.966 - 00:31:58.954, Speaker C: Okay, thank you.
00:32:00.294 - 00:32:07.434, Speaker A: Thank you so much. This was a really, really thought provoking talk. Questions? Comments?
00:32:10.814 - 00:32:55.484, Speaker D: So it seems to me this is amazing kind of experimental view, trying to understand it. But I think there is one element that I've been complaining for years that's missing, and this is looking at scalability, because somehow the way we eyeball these cactus plots, to me, I don't know quite what to make of it. And I'll give an analogy to another area of SAT, which was the phase transition story. So people started with the story how phase transition is going to explain hardness. And it was by eyeballing the easy, hard, easy. It looked like all the hard problems are the phase transitions. But when we start looking at scalability, we discover the hard problems actually show up before the phase transition.
00:32:55.484 - 00:33:16.706, Speaker D: And eventually it led to. Actually, this was empirical observation, but it led to the theory of algorithmic barrier. And it looks at the shattering. I mean, a real understanding was developed because we looked at the problem in a richer way. Scalability, to me, gives you a richer way to look at how things vary. So we look even. Even when we look at what happened.
00:33:16.706 - 00:33:45.484, Speaker D: When you take CDCL and you start dropping key heuristics, well, we see that they differ, but how much do they differ? And scalability will give us more quantitative. Is it an additive time? Is it multiplicative? Is it exponential? And I don't think this experiment should be too difficult to run. There are lots of benchmarks. For example, BMC are naturally scalable, planning naturally scalable, and we should be able to get, I think, a richer view of what's going on by running scalability experiments.
00:33:46.224 - 00:34:31.654, Speaker B: Yeah, I agree. But this is also what we try to do with Jakob on theoretical problems, where you have a number, where you are working on pebbling problems up to 1012 and so on. And you observe something. So, yeah, there are a lot of stuff to study, but it's also not so easy, because on BMC, if you scale the problem, maybe it will become easier. I mean, the contradiction will be much more easier to derive. It's like random. I mean, there is a easy, very hard, hard transition, very hard also.
00:34:31.654 - 00:34:43.084, Speaker B: But after a while, I mean, it's very hard for resolution. But if you do XOR reasoning, you will find a lot of zorclose after a long time, after the threshold.
00:34:43.464 - 00:34:46.484, Speaker C: So, yeah, I agree.
00:34:49.944 - 00:35:05.914, Speaker A: There is a question in the chat. I don't know. Yes. Can you speak?
00:35:06.614 - 00:35:07.998, Speaker E: Yeah. Can you hear me?
00:35:08.126 - 00:35:08.886, Speaker B: Yes.
00:35:09.070 - 00:35:28.046, Speaker E: Okay, so I was wondering how independent the variables from the K core are actually from the solver run, because it was not clear to me that maybe the k core is just a measure of the current state, and therefore you have this correlation. But it's not just. Just a correlation. There is.
00:35:28.150 - 00:35:28.914, Speaker A: I mean.
00:35:30.554 - 00:35:59.212, Speaker B: Oh, if you run. If you run it twice, you may find, I mean, different run will lead to different Kakor. But I think you are right. I mean, it's a picture of the search. The intuition is that, in fact, satsava are greedily searching. I mean, they are just. Just searching in the same direction.
00:35:59.212 - 00:36:25.184, Speaker B: And the cacor is like the. Could be like the. Like the top of a DPL tree, like something that, if you branch, the Kako will force you to go at some place. So it's a very strong force to guide the search, and it's. It's a witness of the status of the search at some point.
00:36:27.764 - 00:36:28.944, Speaker E: Okay, thank you.
00:36:29.284 - 00:36:30.064, Speaker C: Yeah.
00:36:30.364 - 00:37:11.364, Speaker F: Okay, I have a question. I think many of this concept that you presented for capturing the structure, like modularity or K core, I think these concepts are kind of independent of the order of variables or clauses. So if you take two isomorphic formulas, they will have the same structure. But isn't it also, like a folklore wisdom that when you take a sat instance and you permute the variables, then you might end up with a very different solving performance?
00:37:12.784 - 00:37:29.482, Speaker B: For onsite? No, for Ansat. If you suffer, it's reasonable. I mean, you can win, but if you just shuffle. I've made the experiment. It's. I mean, it's. I don't know if it's a normal distribution, but it's.
00:37:29.482 - 00:37:44.054, Speaker B: It's not. It's not something crazy. It's not, like, insat, where you can. You can be lucky. For Ansat, it looks like, to fix the search strategy, you just shuffle. I think the. The.
00:37:44.054 - 00:37:53.530, Speaker B: The wind could be, like, 10% of the CPU time or 20% of the CPU time, but not, not so much. Okay.
00:37:53.562 - 00:37:57.614, Speaker F: But for us satisfactory instances, it makes it for Sat.
00:37:58.314 - 00:38:04.614, Speaker B: I don't know what is the proof for SAT. I agree, but it's complicated.
00:38:05.594 - 00:38:12.074, Speaker C: It could have to, it could have, it may have to do with how vsits initializes.
00:38:12.534 - 00:38:41.094, Speaker B: Yes. I mean, if you. I'm not sure you can be lucky. No. What could be interesting, for instance, is to, to have a benchmark where the edge of sat and unsat and to compare the performance if you remove a clothes or not. I mean, do you have the same kind of search, do you have the same kind of distribution of cpu times? I don't know. I mean, there are so many experiments, but for SAT.
00:38:43.034 - 00:38:44.014, Speaker C: I don't know.
00:38:46.434 - 00:39:43.280, Speaker B: I'm reading the question. Sorry. Yeah, I mean, no, the, I mean, so there is a, a question about varying one parameter. So varying one parameter of, from a set. So there is one parameter for the benchmark or for the SAT solver? For the SAT solver, I think we can do that. But we have to rewrite the SAT server. So it should be, so the Set server should be a very simple version of the sad server like this one, where in fact here the learned clause, the parameter of the loan clause, is a static strategy that say you learn this clause following this rule, you optimize something and the optimization will be very precise.
00:39:43.280 - 00:40:13.574, Speaker B: Like you select the conflict that has the minimal number of decision over all, the formula that contains, I don't know, contains the most recent variables and then we can vary this parameter. But yes, as Karim said, I mean, the paper was, there was a paper of parametric benchmark.
00:40:16.834 - 00:40:29.894, Speaker A: If you don't mind. One last question. What is the difference between those parameters that you talk about on crypto instances versus, say, planning instances and other easy ones?
00:40:34.314 - 00:41:03.814, Speaker B: I mean, the modularity, in the crypto problems, you also have modularities. But in fact, for the modularity, I don't know exactly, because it depends on the, it can be a wrong parameter because it depends on the encoding. And in fact, a module is just the way you wrote the CNF. I mean, if you have a gate, all the variables will be encoded in the gate.
00:41:04.354 - 00:41:06.570, Speaker A: And for the k cores and for.
00:41:06.602 - 00:41:24.494, Speaker B: Centrality for the k core, I don't know, for the planning and we don't have a semantic view of the cake or we are working on it, but it's not difficult. But the cake or was very surprising. I mean, I didn't expect it to have so dense a graph.
00:41:25.234 - 00:41:26.454, Speaker C: It was crazy.
00:41:27.034 - 00:41:27.994, Speaker A: Thank you so much.
