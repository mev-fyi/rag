00:00:03.120 - 00:00:23.654, Speaker A: Hello. Welcome back to the afternoon session. So we have a problem because I don't, you know, I come from a proof of stake blockchain. We don't do mining, so we have to do a vote here. So I propose that the next speaker is Marikor Sepski, founder of Celor, and who agrees that he gives the next talk, as you say. Yes.
00:00:23.694 - 00:00:24.274, Speaker B: Now.
00:00:27.254 - 00:00:28.354, Speaker C: I'll stay here.
00:00:29.494 - 00:00:33.020, Speaker A: Let's go. Now I have already a question.
00:00:33.132 - 00:00:36.764, Speaker C: Yeah, first question, why do you stop.
00:00:36.804 - 00:00:38.824, Speaker A: With millions and not go to billions?
00:00:39.204 - 00:01:21.508, Speaker C: Because millions is already ambitious. Cool. Welcome back, guys. My name is Marik Koshevsky, and today we're going to be talking about trying to scale BFT to millions of nodes using a project we're calling BFT tree. And so just a bit of motivation, why are we doing this? So at Celo, we're using a proof of stake protocol for our platform that makes financial tools accessible for anyone with a mobile phone. And we really like BFT because it's really well understood. It's been around for a while.
00:01:21.508 - 00:02:05.482, Speaker C: There are good proofs for, or its kind of liveness and safety guarantees. And so we really liked a lot of these features. But as I'm sure many people here know, it scales quite poorly. This means that for our proof of stake protocol, we have to do what is common with many other protocols. We have to basically delegate stake to elect, say, hundreds of validators who then perform the PBFT consent. And this really limits the scalability of the whole overall system. And so we wanted to look to see if we could do something that could really scale to the same numbers that we're seeing in proof of work networks and bitcoin.
00:02:05.482 - 00:02:36.510, Speaker C: And so that was our challenge. And so BF three is kind of the result of that effort. And for some really, really quick background, I'll go through really quickly. Why does BFE not scale? Here's an example of some communication showing nodes achieving consensus. On the y axis, we have the nodes themselves. On the x axis we have time, and these are all the messages. And this is kind of the o of n squared messages.
00:02:36.510 - 00:03:33.324, Speaker C: And I'm sure everyone's familiar with that, really limit the ability for this to scale. And hot stuff released something that's really quite elegant. It basically pipelined each of these different phases throughout the BFT algorithm, so that at each block you only needed to do o of n messages. And the trade off here is that you don't get one block finality. You have finality only after three or maybe four blocks. But the end result is for each actual block, the communication is much smaller. So you can run with more validators, but can you run with millions of validators? I would argue no, mostly because at the end of the day, the block producer has to receive all of these different signatures that you're seeing over here.
00:03:33.324 - 00:04:29.194, Speaker C: This is an o of n operation that will be hard both from a bandwidth perspective. And then also if you're trying to aggregate these signatures, then it just becomes quite expensive for one machine to be doing all of that work. And so here's just a quick summary of the different approaches. What we really wanted to look at is this best case critical path. How can we lower that so that we could do better than hot stuff? So in hot stuff case, again, it's o of n. And if you weaken the adversary kind of model a little bit, then hopefully I'll convince you that you can actually do this in log. And if you can do it in log, then you can actually start scaling, scaling this to quite a lot of nodes.
00:04:29.194 - 00:05:26.084, Speaker C: And then the trade off too is you do use more messages, n log, n versus n, and that's part of the other trade off. Cool. So just to quickly show what this algorithm looks like, to try to convey it, I'm going to show a simplified, illustrative version of it to begin with. And the basic approach is to arrange all of the validators as leaf nodes in a tree. So say if you have all of your nodes that maybe were elected through some stake kind of election, or maybe like in ETH 2.0, they all put in the same amount of collateral and they all become validators. So they're all arranged down here.
00:05:26.084 - 00:06:33.202, Speaker C: And then this is kind of more of a topology, like a virtual tree. The inner nodes are actually, and looks like my pointer stopped working. The inner nodes are actually virtual nodes composed of the, it's the same validator that was the left node in one of these kind of pairs plays the role of this parent of these two nodes. So this validator actually also plays a role of this node, this node, that node and that node. Likewise, this validator plays a role of that node. And when you're looking to all agree on a particular state transition, all the nodes, all the validators, will basically aggregate their signatures up the tree. So we use BLS signatures so that we can aggregate them into a single constant signature, and they're basically going up this tree and achieving unanimous consensus.
00:06:33.202 - 00:07:21.994, Speaker C: So only when both parties at each level agree do we move up the tree. And in order to track also which validators have agreed, you're also including the range of nodes that belong to that signature. And we do that with maintaining, basically, we're aggregating the following Tuple, the block info, this aggregate BLS signature, and then the first index and the last index, so that at any point, you know, okay, this signature here is for these four nodes, or four validators. And you keep aggregating up until you hit a malicious node where you can't agree with your sibling.
00:07:22.894 - 00:07:42.660, Speaker A: One question there. You keep aggregating up until something happens upwards. But if I am, let's say, a node that is waiting on two signatures and I hear from one what comes out, I guess, but I don't hear from the other one, what do I do? Do I wait, do I just proceed? Do I?
00:07:42.732 - 00:07:48.540, Speaker C: Yeah, if you don't hear, then that is considered not being able to achieve consensus.
00:07:48.732 - 00:08:03.892, Speaker A: Okay, but sure, but I mean, I come up with my signature if I'm the leftmost node that is letting some engage. Yeah, well, clearly I need to wait for at least a little bit of time to give the other poor person to my right.
00:08:04.028 - 00:08:38.964, Speaker C: Is that. Yes. So there is an assumption that you're waiting in rounds for each level. And so as you're going up when your neighbors disagree, you basically stop aggregating. And so in this example, I'm showing exactly where we've stopped aggregating. So in this case, these two nodes did not agree, and so we stopped here. These two did agree, but this one can agree with this guy because we stopped already here.
00:08:38.964 - 00:09:50.494, Speaker C: So we're going to stop here, similarly over there. And because half of the tree here also stopped, the root cannot aggregate either. And so when this happens, you basically find all of the roots of all of the sub trees that did agree, and you perform kind of a weighted BFT algorithm. And so the nice thing about doing this is that you, the algorithm basically degenerates to BFT, which means that it's easy to reason about its kind of safety and liveness properties. Now, this alone is not enough because it's quite easy to construct entries in the street where you have a lot of disagreement. And so one thing that we do that is pretty key to the performance is we identify all of the nodes that agreed with the overall two thirds consensus that we got at the end. And for the next block, or this is actually after a window of blocks, we move the nodes that disagreed to the right and the nodes that agreed to the left.
00:09:50.494 - 00:10:08.964, Speaker C: And so this, in effect, creates a reputation system where the further you are to the left, the more reliable you've shown to be over time. Okay, I'll get to you in a sec. And whenever you're unreliable or have acted maliciously, you get pushed to the right. Yeah.
00:10:09.744 - 00:10:14.840, Speaker B: What is in the tree that you have the root and say its first?
00:10:15.032 - 00:10:15.588, Speaker C: Children.
00:10:15.696 - 00:10:28.212, Speaker B: They are the ones who are actually malicious. All your leaves are actually the good. And say, let's even because you need one for malicious people. And let's say they're lying at the top. Then what happens? How do you detect them? How do you remove them? In that case?
00:10:28.388 - 00:11:05.760, Speaker C: Yeah, so we'll get to that in kind of the full example. But I think the key here is that by moving nodes to the left and by having this design where the left node of a pair becomes also this virtual node up here, you're basically meaning that the more reliable nodes end up being for the left and also higher up the tree, which means that it's harder for them to. It's not harder, but it's less likely that they will act maliciously and not propagate the signatures from below up.
00:11:05.952 - 00:11:07.004, Speaker A: One more question.
00:11:09.324 - 00:11:10.064, Speaker C: Yes.
00:11:11.884 - 00:11:17.588, Speaker A: Do they send some message to the network that I disagree, or how do you know? Or is it a time off?
00:11:17.756 - 00:11:25.308, Speaker C: Exactly. So we'll get to what happens in those cases in a sec, but they have to act afterwards. Yeah.
00:11:25.436 - 00:11:38.574, Speaker A: So I missed what disagreeing is. Does it mean that they will do something different? Is there a signed statement that you can used to prove they disagree or is timing out or saying nothing or doing fast enough?
00:11:39.634 - 00:11:59.766, Speaker C: Yeah. So if they agree, they both create a signature that signs basically this commitment to the block. And if they don't say anything, then that's considered a disagreement.
00:11:59.890 - 00:12:08.190, Speaker A: Yeah, but how do you know that? Like, if it might have sent a message agreement message to the parent, but.
00:12:08.222 - 00:12:21.638, Speaker C: The parent, the parent could be malicious or there could be a network fault. And that's considered, just like with BFP, traditionally, that's considered. Yeah. Not lack of consensus.
00:12:21.726 - 00:12:30.054, Speaker A: The top has the power to, to move all the honest notes in below it to the left, to the right.
00:12:31.274 - 00:13:25.434, Speaker C: We'll get to that. Cool. We talked about this. We move these nodes there and then now basically we've moved the reliable nodes with high repetition to the left, effectively two thirds of the tree, which means that in the happy case, the two thirds can relatively quickly, just by doing log n operations up the tree, aggregate their signatures and achieve consensus. So that turns out that there's some problems with this, as you guys have pointed out. So I'm just going to talk through some changes to actually make this work. And also be more performant.
00:13:25.434 - 00:14:19.938, Speaker C: And so the first thing is, we're not just using an arbitrary BFT algorithm. We're actually using hot stuff here, which has two benefits. Number one, I think you can have more disagreements and still be able to achieve consensus on a block relatively quickly. But, and I think this gets to maybe some of the questions with hot stuff. Basically, nodes send their signatures to the block producers. So say that this validator is the current block producer. Out of all of the validators that are leaf nodes here, the roots of all of the green trees or subtrees, basically have to send their aggregate signatures and aggregate ranges to that node.
00:14:19.938 - 00:15:09.084, Speaker C: Now, if this node is malicious, it may choose not to send that signature. It could also. Yeah, it could choose not to send it. It could, yeah, I think that's basically most of what it can do. And so that presents a problem, because now we have all of these nodes that are not actually getting their signatures to the block producer. And so that could cause problems. And so the way we resolve this is we actually do log n stages, where the block producer, at each one of those stages stages, will broadcast the signatures, the aggregate signatures that it's received so far.
00:15:09.084 - 00:16:04.934, Speaker C: And if you notice that you're not included, then you have an opportunity to correct that. And I think a strawman version of this would be having all of the validators that weren't included send something directly to the validator. But then you run into the same issue. That would create a lot of aggregating work for that validator. So instead, we do this login times, and we let the children of the node that fail to do its task make the correction. So now these two nodes can send their aggregates, and if one of them is malicious, say this one, then on the next round, these two folks would have an opportunity to do that and so on going down the tree. So in the worst case, it would be log n rounds.
00:16:04.934 - 00:16:26.116, Speaker C: In the best case, two thirds, majority of all the nodes would be off to the left and would be able to aggregate this. So there's another challenge, which is verifying. Oh, question if.
00:16:26.140 - 00:16:37.740, Speaker B: I understand. Well, the point of the broadcast is going up the tree. And then, I mean, your path is only the tree. Do you assume other nodes have path to other. There are other ways or trees the only way of network communication, right?
00:16:37.772 - 00:16:38.852, Speaker C: The tree is the only way. Yeah.
00:16:38.908 - 00:16:58.284, Speaker B: So if the root or the children, I mean, the ones which are circled, those are the malicious people, then they may not even forward the message. So how will somebody else, those are lying in the other part of the tree, how will they aggregate them? They don't even receive, because if the top, most nodes are the malicious, how will the data propagate to the other side?
00:16:58.584 - 00:17:28.398, Speaker C: This is exactly the example I just gave. So in this case, the top node is malicious. It received these two aggregated signatures, but it chose not to forward it to the block producer. And so that's a problem. And so, to reconcile that problem, we let these two children send their signatures if they've detected that their signatures were not included in the last broadcast of the latest signatures.
00:17:28.486 - 00:17:30.510, Speaker B: But even if they are malicious, then.
00:17:30.702 - 00:18:32.954, Speaker C: We go down the tree. So that's exactly what happened here. And so in the worst case is login of these broadcasts, and more and more of these signatures have to go from these nodes to the block producer. So I think if we had a malicious adversary that could basically affect nodes uniformly throughout this whole tree, then it could cause some of this kind of stress on the system. So we'll get to what type of adversaries we're kind of considering here. The other interesting challenge here is actually verifying those signatures. So if you receive one of these BLS aggregated signatures, you have to actually aggregate the equivalent public key of all of those private keys that were used to construct the signature.
00:18:32.954 - 00:19:31.444, Speaker C: And that can take some time. For example, if we wanted to, for a million nodes, get this aggregate public key, we did some benchmarks, and it takes around three minutes on a I nine processor, so you probably wouldn't be able to do that on every block. And so instead, how many keys? 1 million keys. So instead, each node or each validator actually maintains a kind of public key cache. Basically, they have a tree like this, where they store all the intermediate nodes. So this means that if they want to check a signature for this range that they've received from someone, they already have the key that's ready to go. And paying three minutes when you start it up is actually not that expensive.
00:19:31.444 - 00:20:59.314, Speaker C: The challenge becomes when you're actually moving these nodes left and right, once they actually act maliciously, you have to update this tree as well. And so for every node that you're moving, it's going to take log n operations, basically moving up the tree, updating the nodes. And so if you move, say, r nodes over, then you would have to pay r log n. And so the interesting thing here now is you're basically trading off the work that you might have to do to verify these signatures with the flexibility of this tree in booting kind of these malicious nodes off to the right, where they're not going to be able to interfere with the non malicious nodes. And so the way to do that is to basically pick an r that is fairly big. It's basically big enough that you can update this public key tree relatively quickly so that you can do it within the block period, but so that you can still move quite a lot of nodes over to the right after they start acting maliciously. Any questions? So we simulated some performance results, and these are some kind of interesting outcomes of this.
00:20:59.314 - 00:22:38.494, Speaker C: And so the first question we wanted to ask ourselves is, okay, in the best case, what kind of performance could we get? In the case where you only have to do one broadcast, two thirds of the tree are acting sufficiently non maliciously such that we can aggregate those signatures quickly and for up to a million nodes. And if you assume latencies, real world latencies, and a network where you have a million nodes uniformly distributed around the world, where there's no locality between your siblings in this tree, you could be all the way across the other country, across the other side of the world, then you get kind of numbers that roughly look like this. So, not surprisingly, the taller the tree, the more time it takes to kind of aggregate. And so the way to get to numbers that become pretty compelling for an actual blockchain is to increase the branching factor of that tree. And so if you increase the branching factor to 32, then your tree height decreases dramatically, and you can actually get to a case where in roughly 2 seconds, you're able to, in that best case scenario, aggregate all of these signatures. So this is achieving consensus across a million nodes without kind of the subsampling that something like dfinity or algorand uses. And so to do that in 2 seconds is pretty cool.
00:22:38.494 - 00:24:11.110, Speaker C: And then the next question that we were looking at was around adversity models. So, using the typical adaptive adversary, this kind of reputation system doesn't actually buy you anything, because that adversary can basically target left most third all the time, or maybe uniformly distribute their malicious nodes within the tree and really cause havoc to the tree. But if you assume slightly weaker assumptions, and we're still working to formalize these, then you actually get better performance. And so here we were looking at the number of messages based on the percentage of malicious nodes that were also malicious in the previous block. So that's what these different lines are up here for. The red line, if you assume that 0% of the previous blocks validators that of the nodes that are malicious right now, 0% were malicious before, and it was a different one third set that was malicious before. Before then, basically your adversary is able to kind of attack huge swaths of your network with just one block notice.
00:24:11.110 - 00:25:09.322, Speaker C: So as soon as you do that, the whole benefit of this tree really breaks down and you get a lot of messages in the case where even 90% of the network is reliable and only 10% is malicious. But if you start assuming that your adversary can't actually just attack a different section of the network, maybe you're worried about Google Cloud going down for 12 hours, or a state actor turning off the Internet for a whole country, then there's really a lot of correlation between the validators that went offline because of that act and then stayed offline afterwards. It's hard for a malicious adversary that shuts off the Internet for a whole country to then pick basically another entire set of validators for the next block.
00:25:09.458 - 00:25:41.470, Speaker A: But I have a question about this. Right. My worry, given everything that you have described from the last slide, an adaptive adversary. It is actually an adversary that is super earnestly controlling a network. This is static VFT assumption. The adversary commits to those, end of story, but then strategically becomes byzantine when they are in a special position. Because clearly, if you need 99.9
00:25:41.470 - 00:26:07.814, Speaker A: reliability, it means that there are a few key nodes that if you hack effectively, the whole performance goes down. So can the adversary just control a third, but strategically position themselves through the VFT reconfiguration, the tree reconfiguration in that position that then they basically activate their business behavior one by one by one by one to always place you in this miserable position.
00:26:08.694 - 00:26:46.114, Speaker C: Yeah. So this is something we thought about a bunch. And I think, number one, it's not that easy to kind of position yourself exactly where you want to be. Definitely to be the most reliable node all the way on the left, you're competing against all the other reliable nodes. And so it might be quite hard and might take a very, very long time to go all the way into these desirable positions. And secondly, if you're going to have a third of the stake of the network, that becomes quite expensive to do that. So in order to be elected as a validator, you need to put up stake.
00:26:46.114 - 00:26:56.358, Speaker C: And so this is definitely quite challenging. But you're right. I think this is something that we're looking at and we're interested in kind of exploring further.
00:26:56.446 - 00:27:16.810, Speaker A: But I love what you just said. Right. Because effectively, what you're saying is if you assume that the network is adversarial, performance becomes difficult to achieve. And this is actually true in other bmp protocols. You're not special. You're the only one who actually looked at it pretty much. I mean, other people have also looked at similar attacks somewhere else.
00:27:16.810 - 00:27:37.030, Speaker A: And the second thing you're saying is that really, performance under actual third bad nodes is miserable. Again. Anyway, there are, again, lots of papers looking at that. But that opens up the question of, well, if we're going to relax these assumptions, do we continue having all the other machinery for PFP stuff, or can we just do something different as well? Right. Besides that component.
00:27:37.102 - 00:29:06.924, Speaker C: Right. Can we actually make the critical path shorter? Yeah, it's an interesting question. And so, just to summarize on this slide, once you get down to an anniversary that can only change, say, 10% of the malicious nodes from one block to another, which is still pretty big, then your number of messages that you have to send really drops dramatically and starts to look a lot better than what you'd be getting with hot stuff. The interesting kind of thing to evaluate, which we haven't done yet, is to just look at with a million nodes, if you had kind of this one third malicious nodes uniformly distributed through the tree, just how bad is it in that case? I mean, it would degenerate to something that's still better than hot stuff. And so our assumption was always that it probably will result in maybe a five or a ten minute block, and that adversary will only be able to do that once. Afterwards, those nodes will be booted out or a few times in some window. And so that's a very expensive kind of proposition for someone.
00:29:06.924 - 00:29:38.328, Speaker C: They have to invest a lot of money, they have to get their nodes into the exact position, and then they can only really slow down the network just for maybe an hour or maybe hopefully less. And so one thing that we're interested in looking at is actually trying to figure out exactly what is that worst case performance. Just how bad is it? And I think based on some of the other work we've done, my guess is that it will be actually quite minimal. What you can, can do. Sorry.
00:29:38.496 - 00:29:39.776, Speaker A: Yeah, 1 minute.
00:29:39.960 - 00:30:31.534, Speaker C: Okay, that's it. And if you like this talk, one quick shout out. So if you haven't heard of sello, we're a proof of stake protocol with an interesting new lite client that uses snarks to prove the headers are part of the chain so you can sync as a lite client without downloading all your headers. We also have a decentralized PKI that maps hashes of phone numbers to public keys, which means you can find each other easily by phone number and transact easily and securely. We also have a stablecoin platform that allows for the creation of multiple stable tokens. And critically, you can pay for gas with tokens, so you can actually transfer stable value and pay for that with those tokens. So if any of this sounds interesting, next week we're going to announce our incentivized testnet.
00:30:31.534 - 00:30:51.734, Speaker C: Everything at Celo is food themed and so we're launching our incentivized testnet called the great Celos Takeoff. It's named after the great British Bake off. And so if you like tinkering with chains, then definitely keep an eye out for it, and that's it. Thank you.
