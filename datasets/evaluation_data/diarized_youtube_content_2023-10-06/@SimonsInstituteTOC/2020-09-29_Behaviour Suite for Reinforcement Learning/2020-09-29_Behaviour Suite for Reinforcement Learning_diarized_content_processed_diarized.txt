00:00:00.920 - 00:00:22.554, Speaker A: Okay. Hello, everyone. Welcome to day two of the deep reinforcement learning workshop. The theme today is exploration, one of the core challenges of reinforcement learning. Like yesterday, we will have four talks each of 30 minutes followed by a discussion session. Without further ado, let's welcome today's first speaker, Ian Osman from deep mind.
00:00:24.374 - 00:00:48.094, Speaker B: Okay, hi, thanks a lot, Lee Hong. And hi, everyone. One of my first times doing this sort of e conference. So fingers crossed, everything goes okay. But I'm here to talk to you about a paper called the behavior suite for reinforcement learning. And I guess it's work with a bunch of really great co authors, as you see there. And a really relevant link is GitHub.com
00:00:48.094 - 00:00:53.434, Speaker B: DeepMind besuite. Is everything going okay? Technically, by the way.
00:00:55.354 - 00:00:56.730, Speaker C: No, we cannot see a screen.
00:00:56.802 - 00:01:15.298, Speaker B: At least I cannot. Okay, that's good. My screen. Okay. It says it's sharing working better. Okay, cool. Well, that's important thing.
00:01:15.298 - 00:01:36.272, Speaker B: A bunch of amazing co authors and a GitHub link. Okay. And so I guess this talk, it's not just about exploration, it's about all the things that we want in reinforcement learning. So we want to understand our agent behavior and develop better algorithms. I'll post it. Yeah, for sure. But if you search b suite, you'll be able to find it pretty easily.
00:01:36.272 - 00:02:32.020, Speaker B: So that means performance, safety, exploration, obviously understanding everything you want, everything we want to know about our agents, what we want to be able to make them better. And I guess in the field, particularly in deep reinforcement learning, I guess we sometimes are one field with maybe two camps. And I guess on the one side, particularly in exploration, there are the people, and you're worried about polynomial time, upper confidence bound, efficient learning. And then I guess on the other side is the sort of more of the deep learning, deep RL side, population based training, distributed architectures, learning rate tuning. And the outputs have been historically, you know, obviously they're linked, but they can be kind of different. And even if you're the, you know, one person such as myself, you might, you know, put on a different hat when you're in different modes. And so on the one hand is sort of an output with, you know, a stylized version of a problem.
00:02:32.020 - 00:03:08.820, Speaker B: You say you want to understand exploration really well, and so you look at a simplified setting, something like tabular mdps, and you understand that problem really well. And although it's not exactly the problem you wanted to solve, but you get some insight into the fundamental scaling of the problems that you hope will carry over, but it's a simplified setting. Now, the flip side of this is maybe something like Alpha Star. You're trying to take on the world champion at StarCraft. Well, that's clearly not a tabular. You're going to have to pull in all the tips and tricks from deep learning that aren't necessarily amenable to analysis. But you maybe have a much more impressive.
00:03:08.820 - 00:04:15.760, Speaker B: And path to AGI seems clearer. But on the flip side, maybe you either beat the world champion or you don't. You don't get the same insight and hey, well, how much better would I have? How much longer would have had to run the computer if the world champion had been twice as good? Or if the maps had been three times as large? Would I have needed nine times as much compute? You don't really get that sort of insight. But on the flip side, maybe you're developing new interesting tips and tricks and new algorithms that are really going to push you to AGI. I guess B suite, the behavior suite, which is what I'm going to talk to today, is meant to provide some kind of bridge between these two camps. By that I mean simple diagnostic experiments that are designed to provide some amount of understanding, you know, diagnostic tests into the scalability of your algorithm, for example, your abilities and exploration, but are also suitable for large scale, you know, deep RL agents. And so I'll go through this, but that's what the talk is about today.
00:04:15.760 - 00:05:07.140, Speaker B: So B suite is the behavior suite for reinforcement learning, referring both to a paper, you know, we had at ICLR this year and also the GitHub. Okay? And those things go together. The paper is really a position paper about why something like this is useful. And the GitHub is maybe the most useful output that you'll be using in your work. We've had pretty good uptake so far at one stage. We've even trending on GitHub, but hopefully after today's talk, there'll be at least 20 more people or something like that who are desperate to try it. What's the talk? Well, the most important thing I want to explain is what is B suite? What type of experiments go into B suite? Why are they useful and how can you use it going forward? So what is B suite? B suite is a set of targeted experiments.
00:05:07.140 - 00:05:48.542, Speaker B: So the idea is that we're going to collect the best experiments for testing specific aspects of agent behavior. So for exploration, we might build up some specific chain grid worlds of size, you know, small, getting larger, and that the agent may pass the small ones, you know, does, well, solves it, solves it and then fails. And then we keep and it always fails. And with that we say, oh, well, it solved the two smallest grids, so it gets a score 0.2. And similarly, we might test its memory. We might say, oh look, you're going to do a team a's, you're going to start with a really small problem and we'll grow it, and grow it and grow it. And this solved all of them then, okay, that gets a score of one.
00:05:48.542 - 00:06:40.172, Speaker B: So this is the kind of way you use it. Now it should be kind of obvious that, well, what are in these chain grid worlds? What are in these t mazes? You know, that's the value. Well, and also this score might be slightly arbitrary, but the point is it's going to be standardized and you're going to be easily able to compare your notions of exploration and memory against others and run different agents in the same way. So what does that mean? I think you'll see a lot of test suites which are just the environment. Well, b suite is really not just, okay, I'll run through specific things, but the point is that with one flag you're going to specify the experiment and that's going to be all of the environments that you're going to run. So you have to make some choices about what size are the grids and the teammates, and I say you, I mean the designers of B suite. So let's say that's me.
00:06:40.172 - 00:07:16.524, Speaker B: And we decided, hey, you're going to run on the chain from size one to 100, and that is the B suite sweep. Now you might come in another time and say, oh, well, we should change that and we can do it. But the point is there's a defined, only one way to run it. And once you do that, the environment is going to handle all of the logging, everything else is handled. And basically you'll get automatic analysis from that. You'll get a kind of, we're aiming for insight into the scalability of our algorithm. Now I will go through a specific example later that will make this make more sense.
00:07:16.524 - 00:08:18.934, Speaker B: What's the idea? The idea is that if we can bring together all of the best, the highest quality diagnostic experiments, that can be simple, so that they are going to strip away confounding factors. We know that exploration is important for doing better in Starcraft, but we don't want to run on Starcraft because there's so many other complicated things in that. So we want something that ideally is just testing exploration, but it can also be challenging. We really want to make it push the agent beyond its normal range because we want to get insight into the scalability of the algorithm, so we don't just want to say, hey, hey, hey, I've got this great exploration algorithm. It can solve chain grid world size 100. Well, we want to get some insight of the kind that you might traditionally try and get with a theorem, but we're going to try and get it experimentally by measuring the different sizes. And how long did it take to learn? We'll get an insight into the sample complexity of the algorithm and hopefully we'll make this really fast so that we can iterate this with not too much compute, get the results really fast.
00:08:18.934 - 00:08:44.990, Speaker B: So in terms of software, it's really easy to use. You can integrate it really easily with your code. The analysis is pre packaged. So GitHub.com dmin bsweep there are some tutorials there that are really great and you can run through it without even installing anything. I'll skip through this, that's better in the tutorials. But the point is that you can use it with literally ten lines of code.
00:08:44.990 - 00:09:18.816, Speaker B: And what is B suite? It's not just the environment, it's the environment. It's all the logging, it's the analysis and all the long scripts. So it's all packaged together. So it should be really easy for anyone to run their agent on V suite. So already at the halfway through point in the talk, I just want to highlight at the moment, we maybe have hundreds of experiments of V suite, but I'll just run through two. An experiment is the environment that you're going to interact with. So that's the question that Jake asked was which sizes? Well, it defines all of that.
00:09:18.816 - 00:09:54.004, Speaker B: It tells you the interaction. So how many frames, how many episodes you're going to run that and the analysis, the way that you score it, there's no roll your own stuff. It's all going to happen automatically. An example of this is, ok, we want to test memory. So you might ask, ok, well, how long can an agent memorize? We have a teammate, you wait n steps and you get the answer right or no. And it's just very concrete. And the scoring function here is the percentage of which you get less regret than 75% of random.
00:09:54.004 - 00:10:35.662, Speaker B: The idea is memory is a complex. We could have an epistemic discussion of what the meaning of memory is, but rather the b suite approach is that we're going to instantiate a simple problem that somehow represents one of the key challenges of memory, and we'll just be able to measure the agent performance really dispassionately, really objectively on this task. So here are examples of running an actor critic RnN, Bootstrap DQN and DQN on it. And these are the plots that would come out automatically from running it. And you can see, okay, in terms of score, the RNN one did kind of well and the other ones did terribly. Right. But score between zero and one.
00:10:35.662 - 00:10:57.536, Speaker B: And. Yeah, that's cool. I guess that's what you'd expect the RNN to have some notion of memory that DQN or bootstrap DQN do not. But where this stuff becomes more interesting is maybe looking at the scaling plots. Okay. And so this is looking at the percentage of correct, actually, it's the percentage of incorrect episodes after 1000 episodes. So this is mislabeled.
00:10:57.536 - 00:11:22.506, Speaker B: It should be the incorrect. Are they not visible? I hope they're visible. Okay. And so you're looking at mem at length one to 100. And we can see here that basically the actor critic Rnn does really well on everything up to size 30 and then completely fails. Okay. Whereas DQN and bootstrap DQn just completely fail any stage after one.
00:11:22.506 - 00:11:53.294, Speaker B: And so what this is doing, it's hopefully giving you some insight into the scaling of this algorithm, because I haven't told you anything about this actor critic RNN, but actually this was run with a backprop through time window of 30 steps. But you could kind of guess that from the way that this works. It works perfectly. Perfectly. Perfectly and then abruptly stops working. So we probably wouldn't have expected it to keep working if we ran it twice as long on size 31 or size 40. It doesn't have that sort of characteristic.
00:11:53.294 - 00:12:33.930, Speaker B: So you get some insight potentially into the scaling even beyond what, you know, a priori. And I guess similarly, this is a, we're in a session about exploration, so maybe some of you will have encountered the deep sea environment, but it gets, it's a stylized kind of tabular MDP that's meant to be like a needle in a haystack. And so you're in an end by end grid. You always start at the top left where the boat is, and you're going to fall one row per time step. You take action. Zero or one to go left or right, left or right, left or right. You have an actual small cost when you go right, but a large reward if you make it to the bottom right where the treasure is.
00:12:33.930 - 00:13:28.504, Speaker B: So basically, almost all the policies you ever take will have either zero or negative. But if you make to this one policy, which is right, right, right. Which is an arbitrary string of zeros and ones, then you get a big reward. This is a needle in a haystack problem, and if you don't do efficient exploration, deep exploration, you'll take two to the n episodes to find this treasure. But if you do do deep exploration, it's possible to learn much faster. For this b suite, you run on sizes ten to 50 for 10,000 episodes, and your score is those where you do better than this two to ten baseline. This is the idea that, hey, it's difficult to put a handle to say exactly what expiration means when some senses, you could have a definition in terms of polynomial regret or faster learning.
00:13:28.504 - 00:14:13.980, Speaker B: But if we can instantiate those type of polynomial regret bands might be hard to come by for something like DQN or bootstrap DQN or a pseudo count method. But at least you can always run those deep rr methods on deep sea, and you can just see how they do. You can empirically investigate the scaling, even if you can't prove it. This is running actor critic Rnn bootstrap DQn and DQN. Bootstrap DQN, I guess, is a variant which is claiming that it does better exploration than DQN. And at least on this particular notion of exploration, it seems like it does better. But again, the interesting thing is not in the scoring, but it's in the scaling plots.
00:14:13.980 - 00:14:53.794, Speaker B: And this is looking at the number of episodes it takes until you have some average regret, less than 0.9 per time step per episode. Okay? And you can see that on this dashed line is two to the n. You know, all of this is explained if you go in through the GitHub or through the analysis material. But basically, here we see, look, this is the two to the n we'd expect of a, you know, an inefficient scheme. And you can see none of these DQN active critic RNN, they don't do better than that. And bootstrap DQN, it's clearly doing much better than, than two to the n, right? And also, even though we've run this plot, we've run these experiments from size ten to size 50.
00:14:53.794 - 00:16:19.124, Speaker B: If I had to ask you, hey, I got to run bootstrap DQn on deep sea of size 60, let's say. Well, I guess the hope is that by doing an experiment like this, you might feel more confident in saying, hey, I actually think it probably would solve it and maybe it would take around 3000 episodes, something like that, because this, unlike the one we had before, this looks like a smooth curve, like a smooth polynomial scaling. And so, although this is far from a proof, but we hope that by collecting these indicative examples you can get insight into maybe complex algorithms that are difficult to analyze, which can feed both the theory and the practice by cross pollinating ideas. So I guess I've already started on this, so it's really, why are these experiments useful? I guess the idea is that if we really could only stick to algorithms for which we had provable guarantees, well, for one thing, I don't think we would have been using deep learning for a long time, and we may not even be using it now. So practical algorithms often are ahead of the theory. I guess this can be something like an mnist. We can, for reinforcement learning, we can try to collate simple informative experiments that can help with algorithm design.
00:16:19.124 - 00:17:14.594, Speaker B: I guess I don't have time to go through all the code, but it's open source, allows for reproducible research, easy comparisons across code bases. We have examples running it with OpenAI baselines, baselines which we open source dopamine. You know, this is kind of platform agnostic. I guess another big thing is that through this effort of collecting B suite experiments, we can try and instantiate the key issues. You know, a lot of people, for example, we all kind of know we feel like hierarchical RL is important, but at the moment I don't think we have a clear and simple experiment which would be like, oh, if you did really good hierarchical RL, then you should be expected to do well on this task. And so maybe even just encouraging people to look for those kind of examples will help us to find a new clarity in our research. And I guess they can be unit tests for core RL concepts.
00:17:14.594 - 00:17:50.612, Speaker B: But I won't go through this. This is, I guess, how you load the results. It's pretty, I guess this is now technical stuff, we don't have time to go through it, but I guess if you click through that link, bitly p suite colab, you can see that it kind of handles this all automatically for you. No time for the interactive session either, but you know, it's just a lot of example code and output, which you can all find through the GitHub. Yeah, I'll talk about, I guess, you know, Mnist, you know, I guess to count. I'll just talk. Yeah, it's a good question actually.
00:17:50.612 - 00:18:41.320, Speaker B: So Alec is sort of saying mnist is at least representative of real world problem. And I guess we don't know, you know, we don't know if these are representative of real world problems, but I guess the idea or the effort is to. And part of it, you know, I guess you have to argue it through the paper and kind of, you know, from first principles, is to kind of claim that the is to first, you know, I guess I would claim, let's say, let's take the memory, the team A's. Well, then I'll say, okay, the team ace is in some sense representative of some notion of memory. Now, sure, it's not exhaustive, but I think most people would agree. Hey, I can cite various, I can cite various works in psychology or other RL, and same with deep sea, I can kind of make a case for it. Now, if you say to me, oh, tmaze is completely.
00:18:41.320 - 00:19:07.494, Speaker B: It has nothing to do with memory, that's complete rubbishy, I don't want it in there. Well, I think the approach that we're taking is that we'll allow more environments. So maybe, I think you'd have a hard time convincing me, hey, we shouldn't have teammates in there, but if you say, hey, look, I've got a much better experiment for testing memory, well, then with compute that we have, or the deep RL approach, we'll add that into the suite and I guess we can be data based about that.
00:19:08.434 - 00:19:09.214, Speaker A: Ian.
00:19:09.554 - 00:19:10.294, Speaker B: Yeah.
00:19:10.634 - 00:19:13.074, Speaker A: So we have about two to three minutes to run.
00:19:13.194 - 00:19:35.946, Speaker B: Okay, I'm out of time. So the other last thing is, if you run it, you also get an automated latex appendix, so you can add it to your. I've done this. If you have an agent, you might as well run it through B suite, get these kind of output for free. We want better experiments, I guess. I already started with that with Alec. We have some people on a committee who are going to help curate this, but, yeah, let's talk.
00:19:35.946 - 00:19:42.984, Speaker B: Hopefully this is interesting to you, maybe some of you had a look at it before and looking forward to questions. Thank you.
00:19:46.164 - 00:19:52.744, Speaker A: Yeah, thanks, Eric. Ian, this is really interesting. Now we have about ten minutes for questions from the audience.
00:19:55.244 - 00:20:26.584, Speaker C: Hi, Ian. So I think I've actually might have brought this issue up whenever V suite first came out, and we had a little Twitter discussion about it. I'd just like to follow up on it a bit. So, first of all, I think B suite is great. We've used it in my group, some to analyze some of the work we've been doing. But I want to get your thoughts on the trade off between that you mentioned at the beginning, again, of simplicity and understandability versus these more complex things. A lot of these tasks, they really don't seem to be representative of real tasks.
00:20:26.584 - 00:21:07.784, Speaker C: I just want to get your feeling on how important that is. Do you know, do we think these simple domains actually, unfortunately, function fundamentally differently than complex ones? Because, for example, of how important representation discovery is in a lot of these problems, like exploration, et cetera, and without much structure to bite onto, some of these problems just kind of reduce to something combinatorially hard that we wouldn't expect any algorithm to do well on, unless it was designed with, like, an inductive bias to sort of do well on that type of problem, which then doesn't really tell us anything interesting in the end.
00:21:08.684 - 00:21:59.090, Speaker B: Yeah, thanks a lot. So I actually think that's a really good question. And a piece of work which kind of we had in the works, but kind of got thrown off from COVID was an analysis of this, I guess an analysis of, oh, well, what correlates between B suite and other, let's say, problems you care about? Now, if I'm going to throw this back to you, I say, well, what are the problems that you care about? And the answer is probably not Atari. Right. And it's probably not mujoku. Right. So I think that it's kind of that you can ask the same question about those things, like, well, do I think Atari corresponds to the problems that I really care about? And if you're pointing out that what we have in B suite at the moment has huge amounts of missing pieces on the task, AGI.
00:21:59.090 - 00:23:05.530, Speaker B: Definitely 100% it does. But I actually think that maybe, even if that is the conclusion, even if that turns out to be true, you might still find this useful as a debugging or testing thing, because you may just feel like, okay, well, look, I know that some of the problems in there are to solve mnist as a bandit, or to solve mountain car, we should obviously be able to solve that. And then I think that if we can, if it turns out that some of the problems of RL and representation learning are just irreducible, because it becomes really hard when you have to do exploration with representation, with generalization, with memory, which I do agree that's obviously harder. But if it's irreducible, cool, it's irreducible. Like there was nothing you could have done. But maybe in the effort of trying to distill these things into smaller pieces, maybe you'll find smaller bits which we can make better progress on, or we can understand better what exactly it is that makes those problems so difficult. An example being HRL.
00:23:05.530 - 00:23:21.314, Speaker B: Let me tell you this way. I think, like HRL, it's hard to tell whether, you know, how could you claim to make a significant breakthrough in HRL. I think it's difficult to say when you've done that versus just RL. It's not even clear at the moment.
00:23:23.214 - 00:23:46.384, Speaker C: Yeah, yeah, that's a good point. I mean, do you think, do you think there are ways like, or do you see a path toward creating domains like these, but just with like that little extra special thing added so that there is enough structure in the problem that you can do interesting representation learning, for example, and see how it interacts with all these, these other pieces that you're trying to study.
00:23:48.604 - 00:24:18.520, Speaker B: Yeah, I'll say yes. But on the other hand, I guess it's obviously not easy because I don't think we have a good example of that. And if I'd have been able to come up with one, I would have added it. But the approach we want to say is like, oh, we want this to be a place where we can, you know, it's obviously difficult to do it, but we want to be able to consolidate. And if you come up with a great example, well, we want to include it. You know, I think the only trade off we have is some of the computing time stuff. And so we want to keep it.
00:24:18.520 - 00:24:24.084, Speaker B: You know, we have consciously decided to target maybe just this small lower compute regime.
00:24:25.064 - 00:24:38.928, Speaker D: But Ian, one thing you could certainly do is to like, if you think about something like deep sea, instead of just keeping it tabular, it's pretty easy to add some kind of underlying emission process for more.
00:24:39.016 - 00:24:58.176, Speaker B: Totally. So actually I even have coded up a version of deep sea where your row and your column was mnist digits. Right. So if someone wants that, we can do it. I just thought it was, you know, rather, you know, it could become a bit confusing. And also I'd rather. I don't want this to become like the Em, you know, it already is a little bit.
00:24:58.176 - 00:25:10.814, Speaker B: I guess I'm presenting it, you know, but I'd rather, if you are interested in that. Cool, we'll add it in, but I'd prefer it actually to come from you, Alec. And because they'd be like, oh yeah, you'll definitely run on it.
00:25:13.554 - 00:25:21.298, Speaker D: Well I mean we are definitely playing around with such experiments anyways, so.
00:25:21.426 - 00:26:11.434, Speaker B: Yeah, so that's another value proposition is I guess if you're already playing around with the thing, you know, if you feel like you can really put that final layer of polish of like, oh look, you know, in particular in looking at scalability or stuff, well then it's great for you to add your experiment, you know, and I'll help or whatever, you add your experiment to be sweet because you'll automatically get all the other ones kind of for free, you know, plumbed in. And then anyone else who comes and writes a paper, you won't have to tell them, hey, why didn't you run it on this thing? Right? Because they'll run it more easily on yours and they'll also, you know, how frustrating is it if someone, they try to reimplement it? It's difficult, right? And you make a small mistake and suddenly it's not the same thing as what you wanted it to be. You can be like, oh look, no, run from this, it'll be the right thing.
00:26:13.134 - 00:26:14.354, Speaker E: Can I ask a question?
00:26:14.894 - 00:26:17.474, Speaker B: Yeah, I don't know if I pick.
00:26:19.494 - 00:26:42.144, Speaker E: I mean, having code available for different environments and thinking about how can we share code, I think is really important. And we sort of have these frameworks from multiple groups at different times and sometimes the frameworks just don't stick. So I kind of want to know what's different about B suite. Now here you say we want better experiments. What would be like the ideal outcome, like if B suite would make us do experiments that are like the following.
00:26:43.084 - 00:27:31.114, Speaker B: Okay, so I, okay yeah, that's really great. So I think the, there are a few things that are different. So the first is because we made it behavior suite. So we only look at things that the agent does that we've created, we've made it easier to integrate it with other, let's say frameworks, because basically all you do is you swap in the environment. Now that at the moment we support that as either DMM or OpenAI gym environment. And then if you just run your agent on this environment, that's it, that's all you need to do. Because basically the environment is automatically doing all of its logging, it's counting all of your frames and stuff like that because it's behind the curtain of the environment.
00:27:31.114 - 00:28:21.332, Speaker B: So then you didn't need to count and add up this and add up that and whatever. So we have examples of running it with, let's say dopamine, OpenAI baselines, et cetera, et cetera, and a few other things internally in Google. So we kind of hope that, you know, okay, it's not simple, but I want it to be like, oh, you can just put this environment in your framework and then it's just going to work. Well, that's the idea. What do I think? I think that it makes, the other question was about how does this make your experiments better? I think it makes them better just because normally you have to write this loop of maybe handling the data, adding up the rewards, subtracting the noise, I don't know, like processing it. And this is kind of coming, batteries included. So I think that that makes these things a bit easier.
00:28:21.332 - 00:29:07.168, Speaker B: And I guess what's my ideal outcome for this? My ideal outcome is that we'll collect the best experiments for these, simple. For this, you know, you're going to need other experiments for sure. But of this genre of simple diagnostic, you know, insight driven experiments, we'll collect all the best ones and probably we'll collect a few extra as well that weren't as good as the others, but they seemed like a good idea. But we'll collect the best ones, you'll collect yours. And then whenever you run, whenever you write a paper, whenever you write an agent, you'll always just be like, oh, I'll run it on B suite. Now to give you an example, when I say it doesn't cost a lot to run it, we also included GCP instructions, Google cloud compute, but I think with Amazon or whatever, it's the same. Pretty easy.
00:29:07.168 - 00:29:40.564, Speaker B: And the last time I looked, the cost of running a full sweep was about four or $5. Okay, now, so it's not a lot. It takes like four or $5 takes about half an hour. So compared to some of the things, I think it's reasonable for you. You know, maybe if you're reviewing, you're like, oh my God, you know, you might, you, you might say, hey, why didn't they run on Windy world or some type of world that you like? But rather than saying that, just say, add windy world to b suite or whatever problem you want, and you say, hey, why don't you run it on b suite? And then at least you can always get this comparison. That'd be great.
00:29:42.104 - 00:30:01.694, Speaker A: Okay, so maybe we can reserve the last question for the attendees. There's a question. Could metrics be combined for trade offs similar to precision and recall combined for f one to account for complexity problems? Maybe we can find experiments that are in between exploration and generalization, for example.
00:30:04.674 - 00:30:31.606, Speaker B: Yeah, yeah. Well, I think that. I think that, you know, I think that that is part of the, you know, what we have. Could it be done? I think probably yes. Has it been done? Absolutely. Definitely not. I think that that kind of thinking, though, will help us to have more effective discussions on all of these topics in RL because I think that there are probably things spokes on this wheel that we don't even know yet that are a problem.
00:30:31.606 - 00:31:15.084, Speaker B: Or this turns out that scale is not such a big problem. Credit assignment is the same as generalization. But at least if you encourage this, encourage to measure the things we can measure, and then otherwise start trying to find ways to measure things that we don't understand that well. I think it'll help anchor conversations on some of these theoretical and probably in the future, probably we'll move beyond some of these diagnostic experiments because you can have a true theory of exploration in deep RL. But maybe these kind of like psychology experiments, you might view it, are a path that brings us to that level of understanding in the future.
00:31:17.024 - 00:31:21.384, Speaker A: Okay, yeah. Thanks a lot. Thanks, Ian. So now we can, let's.
