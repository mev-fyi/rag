00:00:04.920 - 00:00:59.280, Speaker A: Thank you. Okay, so I'm not even going to talk about all this. This part is not necessary. I'm just going to talk very briefly about if you had a data set and you had to estimate some kind of estimated mechanism that guarantees a specific kind of leakage. How do you do that? Okay, so, and I'm going to give you in the vein of, you know, large deviation, sample complexity bounds. So this joint work, again with some amazing people. Mario Diaz was a joint postdoc of Flavio and me.
00:00:59.280 - 00:01:29.774, Speaker A: He's now back at CMAT. Awesome, awesome mathematician. I learned a lot from him. How is Flavio Stewart? And of course there's Flavio. Right. So just to get you a sense of what the problem setting is, we're sort of going to look at a data set where you have a very clear idea of what the sensitive data is and the rest of the data needs to be published. Okay? And you, and you get these parts in this data, you get this data, you get n samples, and you're going to figure out how to design a mechanism.
00:01:29.774 - 00:01:53.636, Speaker A: So guarantees from finite samples, okay, so here's our formulation. Today it's x. Yesterday it was s, yesterday it was u. So s is our sensitive variable, and there's x, which is correlated with s. And nature determines this, a joint distribution. Right. And what we want to find is a mechanism, w, that x can be mapped to.
00:01:53.636 - 00:02:10.969, Speaker A: Okay, so w is our privacy mechanism. And, okay, sometimes we want these things. All right? So by chosen, by design, what we can choose is a measure of leakage. Okay? So for the stock, and also you can choose utility. I'm not going to talk about utility, I'm just going to talk about leakage measures. Right. So ignore this question.
00:02:10.969 - 00:02:52.014, Speaker A: Terrible question, not at all useful. Here's the question of interest. You give me a data set, the first simple thing I can do, which is really hard to do, but I can estimate a distribution on the data set. Given that distribution, I can pick some privacy mechanism, pick it at random for this distribution, which means I know the cardinality of this set, I know the cardinality of this set. I'm going to find a mapping, let's assume everything is discrete. Now, the question is, if I knew P and I solved an optimization problem like Aaron wrote on the board, I can determine the actual mechanism, which is a transition matrix. Right.
00:02:52.014 - 00:03:06.394, Speaker A: How would the guarantees differ if I estimated p versus if I knew P? It's not so hard to see where I'm going here. Right. This sort of leads to how can you find these things in a data driven.
00:03:06.514 - 00:03:11.626, Speaker B: Okay, are you restricting your attention to the plugin estimator of P?
00:03:11.690 - 00:03:48.844, Speaker A: Yes, I'll tell you that in a moment. Okay, so there's really, I want to walk towards maximal alpha leakage and therefore maximum leakage, but I'm going to go little steps. So I start with alpha leakage, which I did not put on the board. So you may not remember, but it was this ratio between what the adversary knew and the denominator versus for a given s. Right, and then what the adversary can maximally learn given x hat in the numerator log of that and scaling by factor. All right, so here's my result. Basically, the key idea is as follows.
00:03:48.844 - 00:04:22.196, Speaker A: All these measures are lip sheets in the distribution of the data excess. Okay, that's the first thing I prove. And then, so for example, for alpha leakage, there's this Lipschitz result, local ellipsets. I have to be very careful locally Lipschitz. So, well, these are measures that have log and stuff in them. And so you can't really have Lipschitz everywhere locally. Elipsetz means in a bounded interval.
00:04:22.196 - 00:05:00.734, Speaker A: You're lipschits. And the boundary interval is basically assuming that we have enough samples that whatever we estimate is close enough. I'll make that clear in a moment. So the second result we use is a result on what's the bounds and the best plug in estimator relative to the actual distribution. So the bounds depend on, I mean, these are all large deviations with probability one minus delta, right? At least one minus beta. So x and s is really your data set. So it depends on the cardinalities of both of them.
00:05:00.734 - 00:06:01.798, Speaker A: And then one over square root of n is basically how it's going with the number of samples. So you put this together with this to get the following theorem. So the key ideas here are always the same. Whatever measure I use, it's a local lipsianity, and a large deviation is inequality. So what am I saying here? If you give me a data set with n samples, and I estimate the empirical distribution as p hat n from Niid samples drawn from this p, then with probability at least one minus beta, you give me any privacy mechanism w the leakage measure that I can for this w the leakage that I can give you from this estimate is bounded away from the actual leakage. If I knew p by this. Yeah, by this term.
00:06:01.798 - 00:06:24.074, Speaker A: So this term obviously includes alpha. It includes explicitly this s because s is what we want to keep secret. And then the cardinality of the entire input, which is x and s and then one over square root. All right.
00:06:24.374 - 00:06:26.274, Speaker B: This is a negative result, right?
00:06:28.094 - 00:06:37.984, Speaker A: Why is it a negative result? I mean, it's saying. Yeah, it's saying exactly what the cost is going to be for you. I mean. Yes.
00:06:40.084 - 00:06:45.224, Speaker B: Is it minus? No, but there's the Alphabet, there's s. Cardinality of s times cardinality of x.
00:06:45.724 - 00:06:47.812, Speaker A: Yes, it is minus log beta.
00:06:47.988 - 00:06:53.224, Speaker C: How is the size of like s times x compared to log beta? I mean, those seem to be different units. Right.
00:06:57.364 - 00:07:25.164, Speaker A: Okay, so this is a great question. Okay. So if you wanted, really, if you wanted beta to be ten to the minus seven, maybe even better. Why are you shaking your head? Aaron, that means one minus ten to the minus seven. Probabilistic guarantees. That's pretty high. But if you had an image and you still went into a very high feature space, this could potentially be much larger than seven.
00:07:25.164 - 00:07:30.136, Speaker A: So I really think this is the dominant term, huh?
00:07:30.320 - 00:07:36.204, Speaker D: Yeah. That is like the cardinality of the Alphabet is a feature of using the plugin and estimator.
00:07:36.284 - 00:07:36.860, Speaker A: Yes.
00:07:37.012 - 00:07:52.764, Speaker D: Interesting result. Also, I mean, it's worth highlighting the lemma on the top, because that's kind of the basis, the dynamics here. Like, you see how these metrics vary on their small perturbations. And that approximation guarantee in terms of total variation. You could try to drive that using other estimators.
00:07:52.884 - 00:07:57.180, Speaker A: Yes. You'd only do better. You only do better.
00:07:57.372 - 00:08:06.064, Speaker C: Are we looking again? Sorry. This is just, when we're looking at w here is like the.
00:08:06.644 - 00:08:07.620, Speaker A: I'm not gonna.
00:08:07.732 - 00:08:11.524, Speaker C: The margin. Yeah. Is the randomized process, right?
00:08:11.604 - 00:08:12.004, Speaker A: Correct.
00:08:12.084 - 00:08:20.236, Speaker C: That's saying fixed. So when alpha goes to infinity, that's.
00:08:20.260 - 00:08:35.206, Speaker A: A special case of this. This is just, this alpha just is coming from choosing any. Okay, this is. I should have put up what l alpha is. It's a measure. Pardon?
00:08:35.350 - 00:08:36.914, Speaker B: I didn't even hear his question.
00:08:37.774 - 00:08:41.886, Speaker C: When alpha goes to infinity, the Lipschitz constant should be zero.
00:08:42.070 - 00:08:42.794, Speaker A: Right.
00:08:43.334 - 00:08:46.154, Speaker C: It's not supposed to depend on the input distribution.
00:08:47.414 - 00:08:48.490, Speaker E: That's for maximal.
00:08:48.542 - 00:08:58.294, Speaker A: That's only maximum suprema. This is just when alpha goes to infinity. This is just the ratio of the probability of error. Given something divided by the original probability of error.
00:09:01.354 - 00:09:02.602, Speaker E: She has the s dependence here.
00:09:02.618 - 00:09:09.774, Speaker A: As for computer s, that will also show up later, but, yeah. Okay.
00:09:11.514 - 00:09:12.874, Speaker C: All right. Sorry.
00:09:12.954 - 00:09:27.434, Speaker A: This is all I'm, all I'm saying is it's sort of addresses your question. If maybe you didn't even want to estimate maximum leakage, you wanted to estimate infinity leakage. But for this particular secret, s, the key you wanted to hide. This is the cost of order infinity.
00:09:27.514 - 00:09:28.794, Speaker C: Is not max leakage.
00:09:28.914 - 00:09:33.018, Speaker A: Correct. Got it? Yeah. It's just the mutual information of order.
00:09:33.066 - 00:09:36.154, Speaker B: Infinity, which is probability of guessing.
00:09:36.314 - 00:09:40.290, Speaker C: It's like the change in the multiplicative increase in the probability of guessing.
00:09:40.402 - 00:09:42.518, Speaker A: Correct log of that.
00:09:42.686 - 00:09:46.434, Speaker C: Right. Got it. Okay, thanks.
00:09:47.494 - 00:10:09.978, Speaker A: Okay, so now I can do this for lsipsin, and then I can do it for maximal alpha leakage. Okay, now it's very interesting. These are really, you know, this is a remote mutual information. This is substance mutual information. This is the maximum of arrimoto and sipsin. Something very strange happens because these measures are slightly different. Right.
00:10:09.978 - 00:10:38.662, Speaker A: So this is what you get for Sibson. For Sibson, this is the lipsianity behavior. So you estimate Sipsin for two different input distributions. It really depends on the probability of the least likely symbol of the secret. This is the sensitive variable. Okay. And then when you do this, in the maximal case, it's similar.
00:10:38.662 - 00:10:56.954, Speaker A: But now the Alphabet size of s also comes into play because of the way you're taking this maximum. When you work out the details, you have no choice. But, so if s was binary, this is not a big cost, but if s becomes large, this can become problematic.
00:10:59.734 - 00:11:06.042, Speaker E: So, yes, so l, max here is not quite maximum, I guess, because you can change the Alphabet of s, alpha.
00:11:06.058 - 00:11:07.786, Speaker A: Equals infinity is maximum leakage.
00:11:07.890 - 00:11:13.054, Speaker E: Okay, so take alpha equals to infinity. But you have a constraint on the Alphabet size of s. Yeah.
00:11:14.594 - 00:11:30.694, Speaker A: Do I have a constraint on the Alphabet size of s? Yeah, I mean, they're all discrete at this point. But if alpha equals infinite, well, if alpha equal to infinity, this becomes four times absolute value. Cardinality of s divided by m s. Right.
00:11:31.734 - 00:11:32.630, Speaker C: Definition of mechanical leaders.
00:11:32.662 - 00:11:36.302, Speaker E: There's no s in either of the problems.
00:11:36.358 - 00:11:36.954, Speaker D: So.
00:11:38.734 - 00:12:01.832, Speaker A: Actually, no, no, no. So this. Great, great question. Great question. What we're looking at here is ls to x hat. You're hiding all possible functions of the secret, not lx to x hat. X L, as I call it, Q.
00:12:01.848 - 00:12:07.040, Speaker C: Is a joint distribution on secret. And X, what is Q?
00:12:07.072 - 00:12:29.586, Speaker A: A distribution on Q is. Yes, Q is a. So you can say that X and S are jointly distributed as Q. Yes, correct. So if I had to design a mechanism, I still will use my ground truth to design the mechanism. So you assume you're doing this on a training data set, so you have x and s. The thing I was.
00:12:29.610 - 00:12:47.130, Speaker C: Trying to see if, I remember that Max leakage, as Aaron defined it, is independent of Q of the distribution on x. And I was wondering if that sort of shows up, but I guess it doesn't just because of the particular formulation, it's fine.
00:12:47.322 - 00:13:23.146, Speaker A: So because it's from s to x hat, what's coming in here is when I'm estimating. So I wish I had my, I should have put those slides up. What Sibson does is the following. Sibson is sort of a centroid of the conditional distributions. So how well you estimate the least likely determines how it's very crucial to estimating this measure width. And that is why that appears. It doesn't appear in Arimoto, and it's very weird, because Arimoto sort of averages that out.
00:13:23.170 - 00:13:37.802, Speaker C: Now I understand what my question is. Okay, the question is, is this tight? Like, is it, is this in the worst case, over q one and q two? Condition on, the size of s? Not condition on. But for a given size of s, is this, like, are these bounds actually tight?
00:13:37.898 - 00:14:07.034, Speaker A: I think you should be careful about that. I think these are actually tight in the sense, because if I'm doing local ellipses, pretty much q one and q two are assumed to be close enough to each other that n is that I can hopefully estimate. So that another way to put it is we assume that the data set is such that the least likely symbol can be estimated. I have enough.
00:14:07.534 - 00:14:09.114, Speaker C: I'll save my question for later.
00:14:09.534 - 00:14:12.234, Speaker B: So are you thinking in, like, minimax estimation?
00:14:12.574 - 00:14:15.910, Speaker A: This is not right. This is not minimax.
00:14:16.022 - 00:14:16.902, Speaker C: Is there a lower bound?
00:14:16.958 - 00:14:18.354, Speaker D: What would the lower bound look like?
00:14:18.774 - 00:14:21.574, Speaker A: It's a large deviations bound. There is no lower bound on this.
00:14:21.654 - 00:14:38.974, Speaker D: So, just to be clear, for this lemma, for this lipschitz, in simple binary cases, we cover some of the constants. But is it tight in general, like, especially in terms of the dependence, the size of the Alphabet? I'm not sure we know that for sure.
00:14:39.634 - 00:14:46.362, Speaker A: At least with this result of Wiseman, this is what we can get. I think there are much better estimators you can put in.
00:14:46.538 - 00:15:09.874, Speaker D: Yeah, but this result is, even before using the large deviations, it just tells you about small perturbations of the underlying things. And so if the question is, can we get a better constant or something like that, in general, I don't. We can come up with simple examples where this bound matches what you find in that example, like with some binary channels and so on. I think we have that in the paper.
00:15:11.334 - 00:15:12.398, Speaker A: So this.
00:15:12.566 - 00:15:15.714, Speaker C: Yeah, yeah, yeah. Thanks.
00:15:18.854 - 00:15:56.914, Speaker A: So this is the ultimate. The idea behind this is, it is really local elliptic. So how well we capture those constants depends on what range we look at, et cetera, et cetera. All right, so just to be very clear, this is alpha strictly greater than one? I should have said that right up front. So you can't do this for mutual information. You'd have a log n in the numerator and so on. So the basic proof ideas are certainly, this is where we start with the Arrimoto.
00:15:56.914 - 00:16:15.654, Speaker A: You're using these generalized mean inequalities because these are norms, right? These measures, and then you use all these cutlery inequalities to do the bound. All right, so I should have put one more theorem which went to the result on this, but that's not there. But you plug that in and you get the same result.
00:16:19.174 - 00:16:51.744, Speaker B: In John Ducci's 2012 paper, he has something very close to this one on minimax estimate. Before you do the minimax whole problem, something like this. Let's say you take the Kl distance between two distributions and you're trying to relate it under, let's say, local differential privacy to the total variation distance between them. And it seems to me that it's very, very related to this. Did you by any chance.
00:16:53.564 - 00:16:54.824, Speaker D: That'S a good pointer.
00:16:55.364 - 00:16:57.268, Speaker A: Yeah, I'm sorry.
00:16:57.316 - 00:17:00.748, Speaker B: It uses exactly the same machinery. I mean, once you get total variation.
00:17:00.796 - 00:17:04.068, Speaker D: Between distributions, you bound the world.
00:17:04.116 - 00:17:04.644, Speaker C: Except.
00:17:04.724 - 00:17:12.306, Speaker A: Yeah, but I mean, he's looking, he uses Kailh for doing a hypothesis test, right? Am I getting this wrong? And then he does total variation.
00:17:12.370 - 00:17:13.578, Speaker C: You do lots of things with that.
00:17:13.626 - 00:17:38.014, Speaker A: Yeah, but this. So, okay, just, the reason I clarify is there are only two elements to this bound, right? I mean, we're saying we're in a regime where you know that these two are living in some bounded space. They're not so far away from each other. Right. Then how exactly these constants tell me how the function of that behaves. And then I'm just. You can take your favorite best plug and play estimator.
00:17:38.014 - 00:18:24.964, Speaker A: Did he do something more precise? Did he use a better plug in estimator or did he do something? We can discuss otherwise, because KL works differently from some of these measures. So you have to be more careful about that. Any other questions? Thank you. So we're mostly done. Thanks for coming. And we're open to all kinds of comments, feedback, constructive criticism and so on.
00:18:27.024 - 00:18:30.104, Speaker D: By the way, for Alfie Coles, one, which information this doesn't hold.
