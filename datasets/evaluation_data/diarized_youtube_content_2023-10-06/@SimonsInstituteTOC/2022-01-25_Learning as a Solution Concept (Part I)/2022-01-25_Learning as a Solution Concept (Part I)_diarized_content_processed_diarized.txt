00:00:01.440 - 00:00:49.334, Speaker A: Hello everyone, for the afternoon session. So we have Eva agree to present a tutorial on learning as a solution concept. And Eva will sort of pick up a bit from where Kostas left off on Nori grid. Learning in general some games is going to be talking about quality of outcomes when people use learning in games, trying to see what are the pros and cons of using learning theoretic solution concepts in general games. And hopefully that will be. That will set some foundations for the second 3rd workshop that we're going to have on learning in the presence of strategic behavior. Thank you, Eva, and you can take it.
00:00:54.694 - 00:01:27.994, Speaker B: Hi everyone. It's so nice to see you guys. Or at least, you know, at least in part from a distance. I'm really sorry I'm not actually there physically, unfortunately. My plan for the semester was to travel to Berkeley a couple of times, or maybe multiple times. Hopefully sooner or later that's going to be still possible. I'm sorry that with the current COVID level, I felt uncomfortable doing this right now.
00:01:27.994 - 00:02:36.836, Speaker B: I guess I'm doing this online and, well, I sort of see a little picture of all of you on my screen. It's not that big. So I would suggest that actually, just speaking up, if you have a question that would be really the simplest way to go. If someone over there wants to warn me that one of you raised their hand and I'm not noticing it, please do. And as promised, again, Vasily selected the title for what I should talk about. So here it is. I added this in repeated games as a subtitle, and I took the this is a bootcamp very seriously, as in trying to really do a background of what is it that would be useful to know for the workshop or generally for participation in a lot of different activities.
00:02:36.836 - 00:03:56.594, Speaker B: So many of you might know a lot of this, and I apologize if this is a repeat for many participants. Here is an outline of the kind of things I want to talk about during the next 2 hours. So the general question is indeed, as Vasily promised, examples of games and the question we want to ask. And I guess I want to ask two questions overall. One, what do we mean by learning? And what can we say about learning outcomes mostly, which is maybe not quite the continuation of what Costas was talking about. I want to view or want to think about, or like thinking about learning as a behavioral assumption and not. And as such, it's not focusing or I try not to focus on particular learning algorithms that maybe do better, but rather properties that learners might achieve, such as the no regret property and then what can we do? Prove if everyone achieves those kind of properties.
00:03:56.594 - 00:05:20.036, Speaker B: I'm not, definitely not vetted to the norigrade property as the only possible assumption here, and I think a definitely very interesting question if we can make more realistic or better assumptions and use those to prove stronger things. But I guess what I'm most interested in, and what I more want to talk about is assumptions of this form that you can make and that helps you prove stuff rather than concrete designing or naming concrete algorithms that do well in particular setup. Again, it seems more realistic to assume that learners will satisfy some of these assumptions or some assumptions, rather than assuming that they particularly use our favorite learning algorithm or whatever the learning algorithm that we can use in the proof. So quality of no regret outcome learning outcomes is the sort of generalization of price of energy. But I also want to talk about limitations. I guess this next bullet point does indeed overlap with what Costas was talking about, so I might cut that a little bit short. And I guess the last bullet here that nor yet sometimes makes us do the wrong thing or is the wrong assumption.
00:05:20.036 - 00:06:22.100, Speaker B: That's more of an open question than actual results. And in these directions I can show you some extensions of price of energy results using different versions of learning, but some again, many of this is more what I hope some of you or some of us will try working on during the semester. Again, please do interrupt me with any comments and questions. So again, what I'm going to focus on is repeated games. And my two favorite repeated game is traffic rating, where the thing that you're repeating is cars or packets or little things. Choosing paths in a network which is congestion sensitive and more traffic causes delays. And an alternate one which I guess is maybe more or many of you or many people work on is auctions, advertising.
00:06:22.100 - 00:07:56.562, Speaker B: Auction is maybe the killer application of these repeated games that definitely an important application. Advertising space is usually not very expensive for item, but the interest or the value is the insane number of repetitions. Here a model repeated game as a, you know, there's a time horizon. Every step, every player takes some sort of action and then something happens and they repeat this time over and over again. I'm going to throughout make the assumption that players value or costs are additive over period. That is, they aiming to minimize the average cost or maximize the average utility or something of this simple form. And again, they're excellent questions here with interesting other objective functions that they might want to use and again want to assume that they somehow learn doing this learning from past data and the questions you want to ask is what can we say about the outcome of this and what does it mean to learn from data? I guess an older beginning of all this research is actually not using learning just what is usually called equilibrium analysis, that is looking at Nash equilibria of game.
00:07:56.562 - 00:09:47.420, Speaker B: And I guess it's sort of nice to start any price of energy style or quality value analysis with just reminding you that over the last 20 years, or actually more between 2000 or 99, and say early about ten years ago, there was a lot of different results about price of energy on starting with Nash equilibria. So I guess here is some version with Tim Roughgarden with relatively early about traffic routing, which maybe many of you know, is that the cost of a Nash equilibrium in a traffic routing example is no worse than the cost of a socially designed optimum traffic pattern that has to carry twice as much traffic. Again, I'll come back to this, but Nash equilibrium is a solution where no player has incentive to deviate. And technically, the price of energy was defined in this cost of financial equilibrium divided by cost of an optimally designed solution. The results we proved here have these sort of small constant factors, like the factor of two in there, which hopefully you want to view as a positive. This is as opposed to the tragedy of the commons, where the answer would be that the socially designed optimal solution has high value. But then suffice users put Cas on a shared resource, then, unlike the picture suggests, there will be so many casts that the value will go to zero.
00:09:47.420 - 00:11:23.654, Speaker B: So there is a not a small factor of two, but then large factor factor. Again, there are lots of results here, and I'm not going to either go through them or maybe show you one example, but not focus on the price of energy analysis, because that is not the general focus of this, of this special semester, or the topic I'm going to mostly talk about. But there are lots of results. Some of them are bandwidth sharing or traffic routing, and also lots of results on auctions, again, all involving some small constant, like you lose a factor of two or one and a half or something like that. Again, the main question is, what happens if people are learning, and I guess starting with really early history, which I guess I should not, you don't want to spend a ton of time on, but it's nice to start with, it was started in, I guess, early fifties, which has the ancient age to it, that it's before I was born by Brown and Robinson, who were thinking about something called fictitious play. That is, people best responding to the part of the history. And in fact, one focus, which is maybe part of what Costas was also talking about, or maybe not, was thinking of learning as a preplay.
00:11:23.654 - 00:12:17.228, Speaker B: That is a hope that learning converges to an equilibrium solution, a Nash equilibrium. I guess this is what many of the cost us results were about. That is that if you, every participant does learning, or results that cost us reviewed whereabouts. If every participant does learning, then the system converges to equilibrium. So this is really useful if you use learning as a training, and I guess there are many applications where this training is what learning is for. You do a training phase, and hopefully, I guess, as his example shows, you learn to recognize faces or whatever the task is. But in general games, this isn't working out so well.
00:12:17.228 - 00:12:56.992, Speaker B: That is, learning does not actually converge to a Nash equilibrium. And I guess there are very few results. And I guess one class that Kostas did talk about is 20 sum games. But mostly that's sort of the big class where this works, and generally it doesn't. So. Okay, I just noticed that someone posted on the chat window. Are the slides posted? Unfortunately, I didn't post them ahead of time, but more than happy to share them after the talk.
00:12:56.992 - 00:13:49.992, Speaker B: I'm sorry. Okay, so, you know, if this isn't working as a pre play, or at least not in many games and many interactions, it isn't a learning phase that learning is about. It's learning as part of the activity itself. Like as you're running an auction adduction, the participants constantly learn. It's not like they learn and then they done, but they non stop learn. Then we want a different notion, and that's what this presentation will be about. So, I guess I'm going to think of when learning converges, we sort of feel that it looks like this.
00:13:49.992 - 00:14:43.764, Speaker B: They initially do something, and then either they literally land on a Nash equilibrium, at least get super close to it. A Nash equilibrium is a set of actions a, with the property that, you know, no one wants to deviate, which I guess is the no regret property. Again, what this notation says that the cost of. Cost of what a player I observes in this action vector a is less or less than or equal than any other cost he can single handedly generate by switching his part of the action to x. Like everyone else stays with this a action, and he switches to x, and then apparently that costs even more, or at least no less. That's no regret property, and that's an important property. That's what it means to be at Nash equilibrium.
00:14:43.764 - 00:15:51.152, Speaker B: Showing you for a second data from at this point, more than six year old paper of Dennis Nakipalov and Wasservi. It's like there are lots of things that don't stabilize. And I guess advertisement Search on on bing, that is Microsoft system is one of the things. And we have a bunch of data in that paper that we analyze that, hey, look, it's not actually converging to Nash equilibrium and we even offer that actually, we know why. What these guys do is use some sort of learning algorithm. And there are these companies that help, or at the time, these are companies from those years that help the advertisers sort of place the ads well. So, okay, the main message here, and that's where I really want to sort of my starting point of what I want to talk about is this change of focus.
00:15:51.152 - 00:16:50.778, Speaker B: Instead of thinking of learning as a preplay, where the goal is that it converges to something I want to think about as an activity itself. Like people constantly learn and constantly do stuff, and they learning while they're playing. And if you want, you can even think of it as a infinitely repeated game where a participant will show up at some point. At that point, he or she or his company doesn't know quite what to do, but experiments a bit, and then after they played for long enough, they know what they're doing and they satisfy. Some say they use a learning algorithm or satisfy some sort of property. The easiest property to use, as promised by the abstract or the intro, is the no regret property. So remember, no regret for the Nash equilibrium was this.
00:16:50.778 - 00:18:25.050, Speaker B: So the cost of what a player I place is no worse than the cost of any other action that this player could do, given that other people are not changing their strategies. And then the nordic grad property, without assuming the convergence would mean this, that the cost of what a player gets over time, summing over all the periods, is no worse than, and that's a sort of simple, particularly simple version of the property that will turn out very achievable, or, you know, is very achievable, is a single strategy. With hindsight, that is, had he consistently done this strategy x, then that would have been even more expensive because there is an upfront learning phase and you can't expect that the player is knowledgeable. Right in the beginning, I guess we should allow a little bit of regret error. And as Costas already talked about, what are the achievable regret errors in a general worst case setting, what usually is the root t after t periods, assuming the costs are in the zero one range, the overall error is something like root tip is what's achievable in the worst case for simple learning algorithms. There are tons of those. I guess multiplicative weights is something that Costas talked about.
00:18:25.050 - 00:19:31.014, Speaker B: Follow the perturbed leader has some advantages, but again, very simple, and I'm not going to elaborate what these algorithms are, just use the fact that this is a reasonable property. I can offer you two comments, one of which sort of clearly connects to what Costas was talking about, and the other one is maybe goes in two different directions. What Costas offered one sense, I want to be not so sensitive to the error. And I'm willing to say that as long as it doesn't grow linearly with t, that is growing little of t less than linearly. I guess I call that as an acceptable error. So again, root t is something that ample achievable by the simple algorithms. But again, I might not want to be so sensitive, but maybe in some places, I do actually want to be sensitive.
00:19:31.014 - 00:21:07.998, Speaker B: And then from the price of price of energy perspective, I actually prefer a different version of these bands. So if you're willing to give me a little bit of a multiplicative leeway, that is the classical no regret guarantee band, you're comparing to the cost, and there is no multiplier in front of the cost. So it's a pure additive band. If I'm willing to go a little bit multiplicative, say, instead of purely just an additive error, I give you an one plus epsilon multiplier on the cost, and then an additional little bit of an additive error. Then it turns out that with epsilon, with a one plus epsilon multiplicative error, then the epsilon, the effect of the epsilon that the additive error is log, the dimension or the number of strategies divided by epsilon. If you have seen sort of classical analysis of multiplicative aid, or follow the product of leader, they get the root t band by first getting my kind of band, the one I just gave you is the epsilon, and then choosing the best epsilon given these parameters, in a worst case sense, and that works out to be this band, epsilon should be log d over root of log d over t. But I actually would propose, and I'll come back to this in a few slides, that at least for the price of energy style analysis, I might actually not want this.
00:21:07.998 - 00:22:35.126, Speaker B: And I might want to just keep an explicit epsilon and error tolerance into in my bands, because, uh, this makes the error term significantly smaller. So, in fact, we'll make the convergence or the number of times needed, times the number of steps needed for learning to occur much, much faster. And a little bit of multiplicative error is not going to harm at least the price of energy bands significantly. So, again, I'll come back to this in a moment. It's just reminder that, or maybe a fact that hopefully many of you know and seen, that, as you do the classical analysis for either of these algorithms, that root t is coming from a trade off between choosing the best epsilon, optimizing these two terms. And maybe I want to not do that and just leave epsilon as a constant, a small constant, for now. Okay, so, as Costas actually mentioned, if everyone does no regret learning, and again, I won't actually care about what form of no regret learning, then what happens is that the play will converge to, not to Nash equilibrium, but to a form of correlated equilibrium in the limit.
00:22:35.126 - 00:23:57.560, Speaker B: As number of times goes to infinity, the error term vanishes, and the distribution. What happens is the expected payoff is, by definition, is greater than expected payoff for any fixed strategy. With hindsight, I guess, in two person, zero sum games, this means that it turns out, as was proven by Robin, Rob, Freund and Shapira, and sort of similar to the Robinson proof in a different, somewhat different definitions, that the game converges to the value of the Nash equilibrium. That is, the players get the right value. But I want to point out, the game will, the player will correlate. That is, it's not the case that they converge to a distribution of the Nash equilibrium. And I guess the nice example to see this is rock, paper, scissor, where even if you start off with a Nash equilibrium, if accidentally, I play rock a little bit more heavily or more often than I should, then the learning algorithm of my opponent, we'll know that, oh, she's playing rock all the time.
00:23:57.560 - 00:24:42.244, Speaker B: Paper beats rock. So I should more emphasize paper and then, and so on. Then, once I, the opponent emphasizes paper, then I realized that, okay, scissor is a good idea, and we end up with this six cycle that is very, that's much more heavily played than the diagonal entries, which in particular means that the value is right. But our play is not the Nash equilibrium play. It just happens to have the right value. So, of course, if accidentally we play independently, then what we're doing is the Nash. But what happened is that we're not actually playing independently, we're correlating.
00:24:42.244 - 00:25:47.494, Speaker B: So, this is not only for two person, for zero sum games, though, the Nash equilibrium connection is. But instead, in any game, if everyone does not regret learning, then in the limit, they're going to converge to something that, again, literally by definition satisfies this property that the cost for player I on average is greater than. Oops, this inequality is backwards. I'm really sorry. We'll fix it before releasing the slides, but the cost is no greater than any fixed credit gb hindsight. That's like the Nash equilibrium, except that it correlates. And again, the idea is that because people looking at the very same shared history, then that correlate that they get them correlated the same way as how they got correlated in the two person zero sum games.
00:25:47.494 - 00:27:10.744, Speaker B: My play depends on the opponent's play, and we're both sort of reacting to our chef history. So maybe I going to spend a minute here just sort of summarizing what I want to think of as the sort of main message. I want to think of no regret as a satisfying the no regret as a as a behavioral property, that is. Again, I just repeated what I meant by no regret. I prefer this version with the epsilon. It's I want to think of this as I don't care what algorithms they use, as long as whatever they did satisfies this property, then they satisfy no regret as a behavioral model this idea of thinking of no regret as a behavioral model was first suggested by a paper of blom, hagiati, delegate and Ross in the context of traffic granting, and then a little bit tiny bit later, but still the same year, Kistodul, Kovac and Shapiro in the context of auctions. In both cases, they're proving nice properties of what happens when all the players satisfy this as a property.
00:27:10.744 - 00:28:08.904, Speaker B: I like it for many reasons, in particular because it somehow feels okay as a behavioral assumption. If there is a consistently good strategy, please notice it. So if there is this strategy x that better than your strategy, that is, if you fail this, then please wake up to this fact. That is something a decent learning algorithm should observe. I put in a couple of pros and cons in the next slide of what's good about it, and I guess the process may be something you guys all know, because there have been a lot of work in this area. The cons are something that I think we might want to think about as the semester proceeds of what we might want to do about it. So I guess the cons, there are many simple rules that achieve it.
00:28:08.904 - 00:29:16.836, Speaker B: As I already mentioned, it is nice to notice that unlike other games theoretic models, there is no need for common prior. The players don't have to know who they're playing against, or what the game exactly is, or what might be the right strategy, they just learn from data, and it's simple, doable, and I guess maybe the top one that I started with that I didn't say yet, but I definitely love it. It's a behavioral model with a nice mathematical description. It's very usable in doing theory. But unfortunately, there are some negatives also, and here are a few negatives. I'll get back to this, but it's not always doable. Like, I give you nice examples or algorithms that achieve this, but as I'll come back to this in a bit, in some settings, especially multi parameter problems, the running times of this algorithm or the regret bands are prohibitively high.
00:29:16.836 - 00:30:27.434, Speaker B: And I guess the parameter that I didn't emphasize over there was, and maybe worse, paging back to it, so takes a while of paging. Sorry, is that that letter d there? D is the number of different bits achievable, possible, that is, the number of the strategy space. And some games have very big strategy spaces, or even possibly infinite strategy basis. So we have to be a little bit careful with what's achievable. It is reasonable to ask, and I don't have a good answer to that. If I lift the game to, you know, you and I playing against each other, we both entering a learning algorithm, then we actually have a different game on which our strategies are learning algorithms, or any kind of algorithms, and you can ask, is playing no regret learning a Nash equilibrium of this learning game? And the answer is probably not. So that's definitely an interesting, and lots of interesting questions there.
00:30:27.434 - 00:32:07.696, Speaker B: If we played this as a I play a learning game against other people who play learning games, what is the Nash equilibrium of this game? I don't even know the answer to this question. If I limit everyone to play multiplicative weight, and the only thing I allow you to change is the parameters of your multiplicative weight algorithm, what's the equilibrium of that game? And then, of course, maybe the biggest issue is the last bullet on this point is that. And now that's what I'm going to end with in the next hour, is there are many reasons that no regret is literally seems like the wrong thing to do. And, you know, even giving advice to people of what might be the good thing to play or what might be a good model of learning behavior, something that we believe that people or learners or learning algorithms tend to achieve. But it's a different property than no regret, I think, is an important open direction, which I hope this semester will give some progress on. There are some papers that try to think about or look at data, or is it a decent behavior model? I guess one of which is the paper Dennis and Rossini and I had that I already showed you some data from. Some of them are human subject experiments.
00:32:07.696 - 00:32:56.194, Speaker B: I'm not going to maybe go through all the data. I guess maybe worth pointing out that it seems like advertisement data. It's reasonably consistent. I guess about 30% of the bidders have literally no regret, or maybe even negative regret. That is, they did better than the benchmark, and a large part of it is not much regret. So a little bit of error. The context of this paper was, which is another direction that would be interesting to do more during the semester is econometrics, that is, trying to infer people's values from their behavior.
00:32:56.194 - 00:34:56.924, Speaker B: Classical econometrics likes using the Nash equilibrium as an assumption, that is, assuming that the play is at Nash equilibrium, which in particular would mean that if the bits are changing, that must be because your values keep changing all the time. And I personally much prefer the assumption that maybe the environment's a little bit changing and you're slowly adopting it, real learning, but it's not like your values are jumping up and down like crazy. So I, you know, this is sort of the intro part on talking about what I mean by learning, and I guess it came with maybe one important message or a pair of important messages, and the difference between what Costas was talking about, I'm not going to talk about what is the best ways to learn to play in a game. Instead, I want to focus on under very simple assumptions, such as the no regret assumption. What can we say about the game outcomes if all the players satisfy this assumption? So I prefer to think of it not as an algorithm, but as a behavioral assumption. I don't know if there are any questions about this intro part, then this is a good moment to ask. Could I elaborate on the parameter d? Can I? So, the easiest games I was thinking about is diva is the number of strategies that the players have, I guess, in the traffic routing, you can say that, well, it's a graph on some number of notes and edges.
00:34:56.924 - 00:35:47.456, Speaker B: The number of strategies is all possible paths from the source to the destination. So that's exponential in the parameters of, of the problem. That is the graph size. Luckily, the regret band only has log d in it. So that means that the regret is proportional to the graph size, which might be unfortunate on big graphs, that might not be ideal. Life is much, much worse in multidimensional actions. So, step number one, and we'll come back to this in a second, you have to discretize the space because I guess I can bid.
00:35:47.456 - 00:36:43.254, Speaker B: If I can bid any real number, then the space is infinite. But with a little bit of error tolerance, one can discretize the space of bits and say, you can only bid, or I will only bid multiples of some epsilon or $0.01. But if I can bid on multiple items, I have to make this decision on the large range of things, all the possible items, and that's actually a very large configuration space. So again, a huge amount of error comes in because there's so many things. And I guess just remember, just vaguely remember how these algorithms actually work. They all based on the explore exploit strategy. So I guess you have to try all your options and, you know, beat all kinds of crazy things and see how well they work.
00:36:43.254 - 00:37:52.484, Speaker B: This explore part will take an instantly long time because you have to keep trying stuff. So there is something problematic in how this will work. So with that, I'm going to go to the sort of second part of the main thing I want to present is the price of energy. So, quality of learning outcomes. Looking at this price of energy started with a 99 paper of Papo Dimitri Kotzopius and Papo Dimitri, who defined it as I already had it on the earlier slide, the cost of, the cost of a Nash equilibrium, or the worst possible Nash equilibrium compared to some central design optimum. The main message that I'm going to start with and we'll cover is that if everyone is a no regret learner, then we can express this in a. You can think of an extension which I will also call the price of energy.
00:37:52.484 - 00:39:14.924, Speaker B: Though originally that that's some variant on the name to this, where you look at the very same thing on average cost over time. I guess in this case the limit as time goes to infinity, the average cost, overall cost that all the players incurred divide by the optimum, which is, I guess, over t periods, is t times the individual optimum. So this under the assumption that all the players are no regret learners. Again, I said that the first paper that suggested to this was Blum Ligette and Ross and Tim Rothgarden did a beautiful, elegant, very general reduction, which basically saying that almost always, or at least the classical proofs of price of energy automatically extend to this case. And maybe I'm going to do a short summary of why that was. And then there will be some further extensions that is still possible. I guess one also at this point, more than five years old result is Todoris Vasilian and me on even true as dynamic population games, that is in game says enter and leave the playing area.
00:39:14.924 - 00:40:24.414, Speaker B: Still, if they're learners, then they can the price of energy band remains the same or is not so much worse. So here is teams framework, which is something that gets used a lot. And if you haven't seen or maybe forgot, then this is a useful reminder to think about how this works. And I'm going to try to do this both at the high level in a way that sort of easiest to make it very believable that, oh yeah, something like this better be true or easily true. And also maybe give you one example just to get a sense of how this is useful. So I guess the only assumption that we will use in the proof is for one special strategy. For example, whatever the optimum might want you to do that, if you're in Nash equilibrium and you're player eye, you do not prefer to switch to that strategy.
00:40:24.414 - 00:41:46.866, Speaker B: Let's call it AI, is your action that you would do at the centrally designed optimum. And you actually, as player, I prefer what you're doing right now, that's a very special version of the Nash condition. And then these proofs work at a funny looking inequality, which Tim called London use moose, which says that the cost of summing over these funny quantities, that is, mixed quantities that player eyes cost, if he alone would switch to the century design optimum, while everyone else plays at a, should always be satisfied with inequality, should be less than some lambda times the optimum plus mu times the current cost. It looks like a weird inequality, but it's certainly both. Easy to convince you that this is useful, and maybe also that there is a sort of natural meaning behind it. And the way I want to think about why is it useful, is if you currently have a solution that's very, very expensive. So if at the solution a, you're at that's insanely expensive, then what this inequality is trying to say is that, hey, one of you will want to deviate.
00:41:46.866 - 00:42:24.590, Speaker B: So if the overall cost is very high, then some players cost is high. And this possible deviation, thinking of one deviation for everyone is, is summing to be a low number. So one of you must have a lower alternate cost. So why is this low lambda times optimum lambda is hopefully a small constant. So the optimum is much, much smaller. So even lambda times optimum is smaller. And then the assumption up here is that this parameter mu is below one.
00:42:24.590 - 00:43:29.424, Speaker B: So hopefully this would mean if you're a very high cost, that what's in the right hand side is less than the cost of the current solution. So one of you must want to deviate, or, putting it differently, if you're a Nash equilibrium, then I guess by the Nash condition, the current cost is less. And I can simply use this by rearranging inequalities, and then set at Nash equilibrium, the cost, which is the green left hand side, just rearranged a little bit, is less than lambda times one minus mu times the optimum. This is simply rearranging inequalities. So I guess in English, what it says that if the cost is much, much higher than the optimal, then this inequality forces that someone will prefer to do his optimal move. And obviously, that means that the Nash equilibrium must have low cost. So it's a sort of natural condition.
00:43:29.424 - 00:44:38.156, Speaker B: What Tim's two sort of key points of his observation is, was that even though we have a lot of different proofs, they all actually had this framework, maybe not observing that they're using this framework, and also that this is very naturally wants to extend to learning outcomes. So, remember learning outcome, and I guess I use this epsilon version. Learning outcome says that the total cost the player observes is no worse than the optimum. Cost, times this little bit of error, plus some sort of error parameter like regret. Approximate learning regret. And then if the game is randomly smooth, then I can use this inequality instead of the national equality. Say that summing over all the players, this inequality, we get, the sum of the sum of the costs over all the players, is less than the alternate cost.
00:44:38.156 - 00:45:31.692, Speaker B: Using the lambda mu, smoothness is less than optimum plus the cost of these games. And again, just like before, we can rearrange it and get an inequality about the optimal, comparing the overall cost to the optimum. What I want to point out here is two facts. One is that, compared to going back to the previous slide where I had the arab or approximation banda lambda divided by one minus mu, if I allow this epsilon in the learning parameter, I get a little bit of deterioration. Both parameters get a little bit worse by this epsilon, but not much worse. So remember this price of energy bands were constants, like two or 1.5 or something like that.
00:45:31.692 - 00:46:40.704, Speaker B: So if you think that a factor of two bands is interesting, then I'm thinking factor of 2.1 is not that much worse. Allying an Epsilon band here is really very tolerable. And going back to that data that I showed you about learning in Bing online advertising auctions, assuming that people learned with an epsilon here, something like five or 10% is a much more realistic assumption than that they learned with a perfect error. About 30% of the players had no error, and another 20, 30% where the error was below 10%. So I think the small epsilon version of this is probably a better behavioral assumption that there's more people satisfied, and it barely hurts the price of energy band just a tiny bit. It's also worth pointing out what the error band is.
00:46:40.704 - 00:47:38.842, Speaker B: And there I actually went back and have a slide that sort of overlaps with kind of sinks that Costas was talking about. So the speed of how fast things converge, of course, controlled by this error parameter, at the point where the error is bigger than the optimum. Of course, you haven't converged, and you want this error to be smaller than the optimum, to say that you're approximately there. The first kind of results in this direction were, I guess, as Costas told us, an early eighties results by Popov. I observed that this is happening by early paper of Costas with Decobam and Kim in soda eleven. And I guess my colleague Karthik Shitharan had some versions. These guys are all using specialized methods.
00:47:38.842 - 00:49:13.298, Speaker B: There is this really beautiful paper by Wassily and company, which extend this to general games showing convergence results in is learning in general games, all of them using very ancient, or at least, at least the later ones, using specialized method, and in particular, optimistic, so not graduate, sorry, gradual descent algorithms. This is something Costas talked about and was, again, in the framework of trying to think of algorithms that speed convergence. I like our follow up paper from the next year, which says that, wait, wait, these results are true without any assumption, and in fact, even maybe better error bounds if we all, we have to assume is a behavioral model that they're using. No regret algorithms. They don't have to use any specialized algorithm, as long as I give them a little bit of epsilon tolerance in what they're supposed to achieve. And I guess, remember, the error band for the epsilon version had this log b over epsilon. So we end up with a term that's something like that, and it says, essentially, a one over t band in it, which is the idea that Costas was shooting for at the price tag, that I, I'm more aiming for the price of energy band and not regret bands.
00:49:13.298 - 00:50:18.904, Speaker B: And second, uh, that I gave you an extra epsilon in the error, like allow you an extra epsilon in the error. Um, okay, so, um, I guess I have a maybe ten more minutes before the break was scheduled, if I understand the scheduling, right. And I guess I want to use those ten more minutes to maybe go back to somehow basics. And actually, I've been using this or claiming that this price of energy framework is, or smoothness framework is something that can be used and it's useful in general. But I guess I want some example to tell you, you know, how this might work. And my example will be single item first price auction, because that's a super simple thing to do. So what's a single item first price auction? In the simple version, there are these players here and they want to bid on this ugly looking camera, sorry.
00:50:18.904 - 00:51:16.074, Speaker B: And they all put in a price. And I guess because it's a first price action, the highest bidder wins. If that's the first price version, I guess analyzing this as a full information game, that assuming everyone knows everything about other people's values, highest value bidder could have won at the bidding just a little bit above the next highest price. This is not a very useful analysis. So this is a case where equilibrium analysis is not very useful because of this doesn't happen in learning. There's no reason anyone would know the answer to this. So that's not a good analysis.
00:51:16.074 - 00:52:35.194, Speaker B: And I guess one version that's sort of nicer to think about is with some sort of bayesian uncertainty, that is, you know, people's values are coming from distributions. There is this based on this, they have to decide on some bits, and then there is a random process, and in this case it's not possible to bid right above the other person because you don't know it. And, and so this is a better model. Or similarly, there could be a learning version where they trying to learn. So generally, if I want to formulate an auction game. Sorry, 1 second, I need to check something on my slide, I apologize, what exactly happened here? Oh, good point. Nothing, nothing wrong, sorry, you can go back.
00:52:35.194 - 00:53:07.074, Speaker B: Okay, so as a general definition, of course it's not just one item. The finite set of players, players have can bid on some sets, bidding on some items, resulting in some sort of strategy or action vector of what they're bidding on each item, the players get utilities.
00:53:08.414 - 00:53:10.918, Speaker A: We cannot view your slides, we can only view your.
00:53:10.966 - 00:53:28.838, Speaker B: Oh, I forgot to share my slides. That was a big problem. Thank you so much for warning me. Let's go back. One more step back. Yes, thank you. So generally, it's like not have to be a single item.
00:53:28.838 - 00:54:31.244, Speaker B: There are many items, there are many players, they're bidding on items. The bits can be anything. The assumption here is that use the same notation as before, except now it's utility and not cost. And I guess one important fact that I'm going to assume again, to keep life simple, is that there's quasi linear utility. That is, if you get a set of items a, you have some value for this, and minus the price. And what we're going to evaluate is social welfare, which is sum of all the payers values, which you can think of as the utilities, plus the auctioneer's revenue, which counts towards welfare. So, that's the general framework in case of a single item auction, which is where I was coming from, is this just means that the, the social welfare is just the value of the person who owned the item.
00:54:31.244 - 00:55:22.214, Speaker B: So, here is a bid as a strategy function. What makes it Nash equilibrium is this inequality. And maybe it's worth spending a minute to make sure we get the notation here. So, a set of bids is an esch equilibrium. If for any bidder I conditioned on his value, the value or the utility, he would get out of this auction, randomness taken over the bids, and randomness over the other people's values, which he doesn't know is greater than had he bid something else. So he kept everyone else behaved the same, but he is bidding something else, and this is true for all possible other bits. That's what Nash equilibrium means.
00:55:22.214 - 00:56:59.204, Speaker B: And I guess I want to end the current version by giving you a slightly different version of this, which is coming out of a paper of Todoris, Vasili and myself is which is maybe better suited for action games, where instead of the classical optimum plus something plus cost, we want to explicitly talk about the revenue that is the money as part of the smoothness version. That is, we want to say that something is smooth if there be special strategies, like before, so that the utilities of, for any strategy s switching to the special utilities is greater than some lambda times the optimum minus the revenue. And I guess this is just as good, if not better, for social welfare. For the analysis, again, just rearranging this inequality, the people's utilities plus the revenue is what causes social welfare. So it gives you a price of energy of one over lambda. Now, I'm kind of running out of time, maybe a couple of slides before where I wanted to take the break, but I guess maybe what we should do is take a break now and I'll come back to the slide and I guess I'm going to stop sharing.
00:56:59.324 - 00:57:04.796, Speaker A: You have five more minutes, because we started three minutes late, if you want five more minutes to finish something.
00:57:04.860 - 00:57:31.116, Speaker B: But if you want to, I think it's. I'm perfectly fine with breaking here, because that's the, when the break was scheduled. And maybe the food is available for you guys. And then, you know, it doesn't that much matter whether I go where I plan to or just go from here. You have any questions, it's fine with me. Happy to take some questions and then, you know, restart in 30 minutes.
00:57:31.260 - 00:58:02.398, Speaker A: Thank you very much, Evan. Hello. So I had just a very general question about the stability of bad players. Do we know anything about. Usually the stuff that you talked about are like players following some rules. And let's say that if there is one player that is not following the rules, the whole system is going to collapse.
00:58:02.446 - 00:59:22.738, Speaker B: Or so again, I don't want to think of it as flowing rules, as in they don't have to play my favorite algorithm. They should just satisfy this no regret property. There are some papers or results which is actually not in my summary right now or not in the plan of the rest of the talk. But there are a few places where it is reasonably easy to prove results of the form that as long as a large number of players are no regret learners, that is, they do satisfy the no regret property, then we get some welfare, like, I guess, in auction games, that is, buying stuff. You know, if the optimal welfare involves that, I should, you know, Costas should wake up the fact that he values this item infinitely high and he should bid high. But Costas fell asleep and forgot to bid. There's not much I can do about this, but statements of the form that the maximum social welfare achievable by the people who are no regret learners will be achieved or close to that will be achieved by the system.
00:59:22.738 - 01:00:10.554, Speaker B: So again, if somehow maximum welfare is very heavily dependent on one person. And I picked Costos, but maybe in a more classical version I should say, you know, Bill Gates, but Bill gates forgot to be. There is not much I can do about that. But the statement that can be true and is true in many games, is that in utility games like auction games, is that the system will achieve a social welfare within this price of energy band, so close to what would be achievable by the people who follow the norigrat property. I don't know what's up with the other guys. So these are mostly true in utility games, which is nice. In games with costs like as in traffic routing.
01:00:10.554 - 01:01:00.564, Speaker B: There is a funny kind of problem if that people don't behave as nor regret learners. The sole goal of those guys to create congestion, that maybe they can create a fair amount of congestion. So it's harder to come up with what are reasonable models of what's achievable. And in that case, because the people are not, nor yet learners, they could actually be malicious. And malicious people can do all kinds of crazy stuff, as in, you know, just driving circles and creating congestion. Again, there is minimal amount of work by a couple of papers that try to make some headway on this, but it is a little hard to make sense of what might be doable here.
01:01:02.784 - 01:01:08.126, Speaker A: We'll take a break here and thank you. And we'll have a 30 minutes break and we'll be back here.
01:01:08.320 - 01:01:13.674, Speaker B: Sounds perfect. Thank you. Thanks everyone for tolerating me online.
