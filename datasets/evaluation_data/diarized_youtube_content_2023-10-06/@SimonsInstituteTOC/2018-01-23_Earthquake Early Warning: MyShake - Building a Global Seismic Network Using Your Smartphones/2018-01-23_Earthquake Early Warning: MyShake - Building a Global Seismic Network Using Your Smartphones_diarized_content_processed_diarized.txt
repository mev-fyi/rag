00:00:07.040 - 00:00:38.820, Speaker A: Hello, everyone. Yeah, I will continue the session after Richard's introduction of the Shake alert and the Myshake. So, I'm ching Kai. I'm a graduate student working at Berkeley seismology lab. So I'm trained by. I'm actually trained as a seismologist, but also I'm interested in data science, machine learning. So that's the way, actually, we try to adopt a lot of the great algorithms, tools in data science and machine learning back to seismology.
00:00:38.820 - 00:01:26.404, Speaker A: So this project actually is a very nice test field for us to adopt these new methods and new algorithms. And this project, Myshake, is a joint effort between Berkeley's seismology lab and also t mobile, dutch telecom, Silicon Valley Innovation center. So it's a collaboration between industry and, and also academia. And also, last year, more foundations started to fund us as well. As Richard mentioned that this project is still in the research mode. We are not ready to issue earthquake warning yet, but we are working really hard to issue earthquake warning. Just a little bit recap of what Richard just mentioned.
00:01:26.404 - 00:02:23.734, Speaker A: The goal is that we built a seismic network using the smartphones in your pocket. The sensors in your phone actually can be used to detect earthquakes. So the ideal cases is that everyone downloads our myshake app and turn your smartphone into a seismometer, and then you can see that everyone actually working with their, working with their smartphone sitting there. And if the earthquake occurred, a good portion of the smartphones detect the earthquake shaking. And then this information sent to the central brain or the central server, we can actually aggregate the results from a large crowd of phones, and then we can detect the earthquakes. So this is the goal of myshake in the real time applications. So we try to detect the earthquakes as fast as we can, as long as we have the smartphones.
00:02:23.734 - 00:03:34.434, Speaker A: But the technology actually, or the core part of Myshake, as Richard mentioned, is that everyone is working with your smartphone running, maybe doing other activities, how we actually recognize earthquakes from all the human activities. So this is the core part of my shake. So today I will try to make this as a tutorial, like where I try to show you the journey. Actually, we address this problem, what kind of problems we actually encountered and how we address them, and also what the lessons we learned. And hopefully this will be useful to you as well, even though maybe your application is different in other domains. But I think a lot of the problems may share the same characteristic. Like I mentioned, for our problem, the number one challenge is that everyone is moving with the phones, and we have people working with the phone running, sitting there, driving the car and so on.
00:03:34.434 - 00:04:38.444, Speaker A: And if we record some signal like this on your smartphone using the accelerometer, how do we know whether this is an earthquake or this is just some human activities? In order to answer this question, the first thing we need to do is we need to collecting data, and then we need to see how we actually dealing with this data to recognize whether the movement is human activity or the movement is earthquakes. So this is the first step. We started this project. So you can see that we started to collecting data. So we built a prototype system that we actually have the app running on some testing functions, and then we start to collecting human activity data. So this is one human activity data in three components. Since these phones, if you put the phone on the desk, it can measuring acceleration in three dimensions, two horizontal, one vertical.
00:04:38.444 - 00:05:46.830, Speaker A: And we started the prototype system actually here at Berkeley, as Richard mentioned, that we need to choose one platform to work with either Android or iOS. So we actually chose Android at that moment because we think the Android have a lot of penetration around the globe. But the funny thing is that when we started to do the test here at Berkeley, and we found out that out of 300 students class only less than ten people have the Android phones, we have to leave with the situation and try to recruit more volunteers for us. So all together, we actually recruit about 100 students and also friends and families to help us to carry the phone for six months to collecting the data. So all the human activity data looks like this. And then the other thing is that we need to collecting the earthquake data. And then we try to see what's the difference, how we actually recognize which one is earthquake, which one is not.
00:05:46.830 - 00:06:21.596, Speaker A: So this is much harder problem because earthquake not occur very frequent in the last few years. Actually, in Bay area, there's only a couple of large earthquakes, like the Napa earthquake, nine to six in 2014. And also a lot of them are much smaller earthquakes. So we try to, at that moment, we try to increase our earthquake data recorded on the phone. So we actually did two things. The first thing is that we generated some synthetic data. So this is the simulated data we generated.
00:06:21.596 - 00:07:21.290, Speaker A: So how we did that is that we have the traditional seismometers, and then these are running for several decades. There's a lot of recorded earthquakes on this type of sensors. We first get these recordings from the sensors, traditional sensors. And then we downgrade the quantity of the sensor that looks similar to the quality that's recorded on the phone. And also the phone sensor actually is a much poor quality that have a lot of noise. So we actually grab the noise from the phone recordings and we superimpose on the earthquake data recorded on the traditional sensors and use this as a simulated data. So you can see, I'm showing here is one earthquake example from southern California, but you can see the horizontal axis is the distance from the epicenter.
00:07:21.290 - 00:08:15.182, Speaker A: And also the time is the, from zero, that's the earthquake starting time. So the right one is the raw data from the sensor of the traditional seismometer, and the blue one is the one we actually downgrade. And adding the noise to simulate this is our earthquake data. So if you have a look very close to the epicenter, it seems like they match quite well. But when you go further and further away, you starting to see these red traces actually is buried into the smartphone noise. Because the smartphone noise is so high that a lot of the times you can't see the signals very well, especially at further distances. So these sensors are really good to detecting large motion, but not the smaller motion which can be detected on the traditional sensors.
00:08:15.182 - 00:08:52.760, Speaker A: So this is our initial start. We think that we actually can only detect larger earthquakes during the. If we release the app to the smartphone sensors. But also, in addition to this simulated dataset, we also did shake table test. The shake table test is that we have a table probably like one third the size of the one third of this room. Like, you can see this is the sick table, like fenced by this blue bloom here. And this is some human for scale.
00:08:52.760 - 00:09:55.492, Speaker A: So this table actually can simulate earthquakes in three dimensions. And then we put all our smartphones, which actually is here and on the table. And then during the shaking of the table, these smartphones will pick up the shaking movement and record it on the acceleration. And also, by the way, this is the shaking test they are doing at that moment. So this is a column from a bridge. They try to see what kind of shaking actually will, will make cracks or what's the response of this column on the bridge, response to the earthquakes. But if you zoom into this small area where our phones are, so you can see all our smartphones, we put on the desk, on the shaking table, and in various orientations, and also we try to see what's the effect if we don't coupling the phone like the traditional seismometers.
00:09:55.492 - 00:10:28.114, Speaker A: Usually you dig a hole in the ground and also you bolt the sensor on the concrete pier. But a lot of the times, none of the users will let you do that. So that's why we need to test if we just put the phone on the desk. What kind of signals actually we can record to give you a sense how this works. So this is a movie actually showing the shake table in action. And you can see now it's moving. So this table actually simulates a large earthquake.
00:10:28.114 - 00:11:27.688, Speaker A: And also due to the safety issue, we actually proposed that some students carry the phone standing on the table, but they actually not allow us to stand in there just in case that some students get injured and so on. But this is the way actually we got more realistic earthquake data set. So this is another way we actually generate our data set for the earthquakes. And then to give you a sense of what it looks like, you can see this is the shake table data recorded on the smartphone and recorded on a reference sensor on the shake table. So you can see the black one are recorded on one phone and the red one is actually recorded on the sensor installed on the shake table. So you can see if I plot over each other. Basically, this is the raw waveform.
00:11:27.688 - 00:12:02.496, Speaker A: This is the time of the shaking. This is the amplitude. The larger the wiggle is, it means that the larger the shaking is. And also you can see this is the filtered data. We just filter the data into the frequency band. We are interested, you can see it's actually matching quite well. And we think at that moment is that, okay, this data proof that these phones actually can be used to detect the earthquakes, even though it's not bolted on the ground, just put on the freely on the desk.
00:12:02.496 - 00:12:56.618, Speaker A: So this is the way either we prove our concept, but also we are collecting more data. So this is actually the ways we collecting all the data we have for the earthquake and non earthquakes. Actually, during the course of collecting this data, we also collected some real earthquake data, which is very similar to this during the six months. And also another thing to mention is that these sensors, if you think the traditional sensors richard just mentioned, to detect earthquakes, it can detect the earthquakes. Very small earthquakes, even the magnitude is like a 0.1 or 0.2. But for the sensors of these smartphones, at about 10 km, you can only detect about nine to four or five, depending on what type of phone you are using.
00:12:56.618 - 00:13:34.454, Speaker A: High end phones usually have better sensors, but the low end phones have the low quality sensors that you actually missed a lot of the even larger earthquakes. So this is the inconsistent hardware in the phones is another considering issue for us to consider at that moment. But this is the data. Basically, we collected earthquake data and non earthquake data. So we collected a lot of these type of data. And then we needed to think of why we actually can distinguish earthquakes from non earthquakes. So we actually start to use machine learning.
00:13:34.454 - 00:14:19.544, Speaker A: Basically, this is just an overview. I know a lot of people here probably already have an idea of what's machine learning. But basically we have the data, we put the data into a tunable model. We use like optimization algorithm to tune the model parameters to get the results we want. And then after the training model, after we train the model with some of our training data, some magic things can happen when we release the app into the public. So this is the basic idea at that moment, after recollecting all this data. So for Myshake, actually, the procedure is very simple.
00:14:19.544 - 00:15:08.852, Speaker A: So we found out that if you run a classifier to continuously classify whether the motion is earthquake or not, it's actually not efficient. With a very simple, steady algorithm, you can check whether the phone is get steady or not for a very short amount of time. You filtered out a lot of the human activities already. So we actually, in our work flow, we have a steady algorithm, basically checking whether the phone is steady or not. And then we have a pre filter to filter the data into the frequency range. We are interested. And then if someone pick up the phone or someone move to the desk, and this phone will trigger algorithm we call STa LTA.
00:15:08.852 - 00:15:53.256, Speaker A: It's like a short term average over the long term average. Think about that. If I have a signal running through time, and then if I use like two averages as an indicator of whether there's a movement, the short term, using a very short window, it will be very sensitive to some, like a sudden motion. But if I have a long term window, if I take the average, the background noise actually will average out the spiky signals. So this is actually a very sensitive motion detector to see whether my phone is moved or not. But we won't talk more about all these three steps. Today I want to talk more about the artificial neural network.
00:15:53.256 - 00:16:39.234, Speaker A: Once the phone moved by someone, how we recognize whether this is earthquake or not. So this is the focus part of this presentation. So basically, we use the artificial neural network to recognize which motion is earthquake, which motion is not. So, yeah, just some crash course on the artificial neural network. I think. I don't know the audience, probably most of them know some level of artificial neural network. But I always think in a simple high level picture of artificial neural network, that when we were kids, we tried to recognize which one is apple, which one is orange.
00:16:39.234 - 00:17:14.470, Speaker A: Your parents give you these two objects, and then you started to sensing the world like a color, texture, shape, taste. And then these usually called features. And you memorize these features into your mind. And next time when they give you an object, for example, Apple. And then you started to compare the features in your mind and then to see whether it's more likely as an apple or more likely as an orange. But sometimes you make mistakes. For example, someone gives you an orange, like someone gives you an apple, but it's a green color.
00:17:14.470 - 00:18:03.092, Speaker A: Maybe you never see that before. You make a wrong detection or you make a wrong classification. You say, this is maybe orange, but then your parents, in a supervised way, tells you that this is actually apple. Then you update the features that actually saved into your mind, and then you actually use this in the future to guide you to make the decision whether this is earthquake or whether this is apple or this is orange. So this is a very high level way of artificial neural network. So basically, it's a feedback loop. If you think that I have the input of these features, for example, we described in the previous example, and we have some processing components here to process these features.
00:18:03.092 - 00:18:34.554, Speaker A: And then we have the output, whether it's apple or orange. And then we. We can measuring whether we make the wrong decision or make the correct decision. And then we can tell these. We can tell this is like a correctness of our estimation back to the input layer, basically saying that if we make some arrows next time, we try to eliminate the errors. So this is the artificial neural network in simple. In simple way.
00:18:34.554 - 00:19:21.394, Speaker A: And I try to give you an example. Hopefully this is more clear. I always like to use this example. I hope everyone knows this is President Obama and also doctor Shorten Cooper. In the big Bang theory, basically, if we have a group of pictures, we try to classify which one is Obama, which one is Sheldon Cooper. And we basically try to train artificial neural networks to do this, how we do that. So we basically put all our training pictures, Obama and Sheldon Cooper, and we extracted some features, for example, the color of the skin, eye shape of the head, nose, and so on.
00:19:21.394 - 00:20:32.628, Speaker A: And then we use these features to give that into the artificial neural network. And these connections between these features to the artificial neural network, basically, these are like the. The weights or the indicators, which features are more important, which features are not? And then we can generate the output. For example, if we say these pictures are Sheldon Cooper, we actually made a wrong detection. And then we actually can, since we know these pictures are Obama in a supervised sense, we basically can calculate the error and then go put input this arrow back to the network and then update the weight next time for, like, adjust the learning parts of the mouse nose and eyes, which one are more important for me to make the correct decision. So basically, this is the idea behind the artificial neural network. And then we actually, next time we make the correct detection, we make the correct estimation of this is Obama.
00:20:32.628 - 00:21:10.900, Speaker A: So exactly the same thing on the earthquake signals. If we have an earthquake signal and a non earthquake signal, we know which one is earthquake, which one is not. And then we try to extract the features from these time series. So we are actually using a 2 seconds window. Within this 2 seconds window, we extract the features from the two type of different movements. One is earthquake, one is human activity, which can be used to characterize the difference between the two types of movement. And then we use the sliding window with 1 second overlap.
00:21:10.900 - 00:22:39.250, Speaker A: Basically, you can think this window is moving every second. We calculate some features within these 2 seconds window and then use these features to tell me which ones more looks like an earthquake, which one is more looks like non earthquakes. So the features we extracted actually at that moment, about like three or four years ago, we tested about 60 different features, more than 50 different features. So these features include some features from statistical sense, like statistical summary of the 2 seconds window of the time series, and some of them are time domain signals like the amplitude, what's the shape looks like, what's the percentile of the amplitude, and so on, and then some frequency domain. For example, the FFT, the Fourier, faster Fourier transform, and also the wavelet transform, and so on. And also we used some of the features in seismology. So we tried to select the features to give us a clear sense of what's the physical meaning behind, so that we know what type of movement have these kind of features or patterns, that we can actually use that to distinguish which one is earthquake, which one is not.
00:22:39.250 - 00:23:24.914, Speaker A: So we actually tested all these more than 50 features. And then we select, first we think that we don't need a lot of, we don't need all these features. And also we think that implementing on the phone, we don't want to have a lot of features running on the phone. So that's why we actually select a subset of them to be running on the phone, so that we can use that to detect earthquake and non earthquakes. So we use a feature selection schema. So which, basically a process that you select the most important features or subset of the features that running in these all 50 different features. So we test the different ones.
00:23:24.914 - 00:24:30.014, Speaker A: At last, we basically selected the forward gradient deselection. Means that we start with non features, and then every time when we do the iteration we add in one new features into our training and then we monitoring what's the improvement. Basically every time after I add a new variable or new feature into it and we select the feature that give us the maximum improvement on the model. So using this way, actually we selected the best three features. You can see when we use one or two features, this is like the training performance going up is going higher, higher performance. And this is like a number of features we are using with more and more features. We can see at the beginning we are actually increasing the performance of the detection, but then later due to either overfitting or other issues like maybe the correlations between different features, we starting to see a slightly drop of the performance.
00:24:30.014 - 00:25:35.010, Speaker A: But we selected the three features that we think is the best three features gives us both, gives us a nice performance and at the same time gives us a nice interpretation of what's the meaning behind the different movements. So we use these three features actually to implement on the smartphones to detect earthquakes. So these are the three features, the zero crossing, which is basically simple indicator to indicate that your frequency change on the movement. For example, if I change my hand like this, so it will be relatively higher zero crossing. So you can see every time my signal cross the baseline, I count one. So basically it's an indicator of what the dominant frequency is. And also if I change, I move my, my phone slowly, it's relatively lower number of this zero crossing.
00:25:35.010 - 00:26:46.782, Speaker A: And also we use interquartile range. Basically this gives us within this 2 seconds window of detection, what's the average amplitude or what's the shaking intensity, most of the shaking intensity within this 2 seconds window. So these features tells us how fast your phone moves, this feature tells you how large your phone moves. And then this is a feature we used in seismology, we call it the cumulative absolute velocity, which gives you the average energy within this 2 seconds window, tells you within this 2 seconds window how much I actually moved and so on. So these three features actually we found out that they actually tells us a lot about different type of motions, earthquakes and non earthquakes. And also by the way, there's some features actually works better. For example, this is just a simple indicator of the frequency domain signal, what's the dominant frequency is.
00:26:46.782 - 00:27:29.020, Speaker A: And we found out that actually use some of, use the coefficients of the wavelet transform. Actually we have a better performance, but it also use much more resources on the phone if you implement on the phone. So selecting these features is kind of like a trade off between the performance and also how easy or how much resources. It's used on your phone. So we actually came up with these three features running on the phone, and we found out that it can separate most of the human activities from the earthquakes. So this is the place. Actually, we started to implement that on the phone.
00:27:29.020 - 00:28:23.128, Speaker A: I will stop here just to see if there's any questions. Feel free to interrupt and ask me more questions if something is not clear. But anyway, so after we selected these three features, we train an artificial neural network. So basically we found out that we can train a very simple artificial neural network standards like input layer, hidden layer and output layer, and then to calculate whether my motion on the phone is from earthquake or non earthquakes. So this is the architecture, actually, from our implementation on the phone. It's just like a five neurons in one hidden layer with three features, and then we train with this trimmed model. So I actually have these figures here.
00:28:23.128 - 00:29:44.044, Speaker A: I think it's very interesting to show. One trend is that, like, if you think that what you are doing as a machine learning engineer or data scientist, a lot of people think you are doing very complicated things, but these days, actually, you have a lot of the packages that you don't need to spend a lot of time to implement this algorithm. So you basically, maybe with just one line of import, you solved all your problem. So this is exactly what I learned, is that I spent almost one semester to basically implement the artificial neural network from scratch. And then after that, after maybe one or two years later, I found out that there's existing packages that you can actually implement that in one afternoon, which actually is taking the trend, basically saying that these days you can do a lot of more efficient work with the existing community. But anyway, at that moment, I, as a PhD student, spent the whole semester to implement the artificial neural network. One good thing is that you actually have a better understanding of the artificial neural network, but the other one is that you lose the half of you lose one semester basically learning all these things.
00:29:44.044 - 00:30:45.912, Speaker A: But anyway, I will tell you a lot more details of the training, because I think some of the training parts actually is very interesting because these problems actually have a lot of typical problem. If you think that it's a typical machine learning type of project, one thing is that. So, yeah, we know that a lot of models can fit the data very well. Like if I have the blue dots and the red dots here, you can have a lot of, you can have a lot of models. Which one is good? Which one is not? Like the red one, the blue, basically the green curve here actually gives me 100% I can separate the data. But you can see this purple line here also gives me like a separate the most majority of the data, but a lot of them are making mistakes. This black line seems gives me the best results in terms of separating this.
00:30:45.912 - 00:31:54.014, Speaker A: So this is basically enervating and just the right and over fitting your data. So during the training of the model, so we basically separate our training data set into test and training. So with the earthquake data and non earthquake data, we collect it. So we know that a lot of them actually are non earthquake data. So we actually first separate the training and testing and then training the model only on the 80% of the training data and then test the model on the 20% of the data that we never present to the training process. So basically you can see, I think a lot of people know this is like a learning curve, which is like you can train your model as long as you torture your data really well, you continuously decrease your error rate. But if you test on a new dataset, you always see like an elbow curve, which indicates that this is the place you should stop your training.
00:31:54.014 - 00:33:10.246, Speaker A: Basically, this is the time you use this model as your implementation of the real world model on the phone. So we use the tenfold cross validation. So basically this is the idea is that every time you use, you cut the data into different chunks and use one chunk as the test data, the rest of that as the training data, and then you basically use these to test your performance and also select what the architecture of the artificial neural network is. And also one interesting aspect is that this data set is also a very skewed data set. It's a highly imbalanced data set. For example, we have a lot of human activity data, but we have only a small portion of the earthquake data. For this example, if I try to classify Apple and orange, if I have 9000 data points, all from the Apple, only 1000 data from the orange, if I make a classification algorithm to estimate all our data is Apple actually achieved 90%.
00:33:10.246 - 00:33:42.406, Speaker A: So this is the effect of imbalanced data. So this is actually a huge problem also for our data set because the earthquake data is very limited. So it's several orders of difference between earthquake data and non earthquake data. So we tried different ones to address the imbalanced data. We tried to collect more earthquake data data, but it's very hard to do the shake table test and also to record the real earthquake data. And also we tried to resample our data set. We oversampled the earthquake data set.
00:33:42.406 - 00:34:45.112, Speaker A: Like if we only have 1000 earthquake data, we try to sample, we oversample it to 10,000 data, or we downsample the human activity data to like from 10,000 to 1000. And also we tried to generate more synthetic data. And also we tried the different algorithms, which insensitive to the imbalanced data. For example, the tree based data, tree based decision tree based like algorithms or models with penalized model, which we penalize more if you make wrong decisions on the earthquake data. Or we also tried, like another perspective, instead of classification, we also tried the anomaly detection. So in the last, we actually selected the best performance, which is we down sample the data from the human activity data. And during our test with the volunteers students here, we found that actually works pretty well.
00:34:45.112 - 00:35:24.180, Speaker A: So to give you a sense, this is the training data we have. So this is the zero crossing, which basically going here, going this way is more higher frequency, and this is the amplitude of the shaking. Each dodge is one 2 seconds window. So we actually have several million of human activity data here. They overlap with each other. So you can see this is the human activity data. But if we plotting the earthquake data from the sheet table test and also from the simulated data, we can already see the actually separate.
00:35:24.180 - 00:36:16.674, Speaker A: Most parts of the data actually separate from each other. So we actually can train a model. Basically, if we draw a decision boundary across these data sets, we can separate most of them, but not all of them. And also, in terms of downsampling the data, we tried to actually, for the human data, like I mentioned, it's a highly imbalanced data set. We try to randomly sample the human activity data, but we found out it actually works really poor because a lot of the human data actually is within here. Like maybe 90% of the human data is within here. This is because a lot of the time, the human data is sitting on a desk, have very low noise, low amplitude, and a broader frequency content.
00:36:16.674 - 00:37:21.588, Speaker A: So this data is people working, running with relatively larger amplitude, but relatively smaller, or like a low frequency movement. So if we just randomly sample the data, we actually found out a lot of the data falls here. So that's why we use another different way to sample. We actually use a clustering algorithm, the k means clustering algorithm, to cluster these human data into the centroids of each of the cluster group. Use that as a representative of the human activity data. You can see with this clustering algorithm, actually, we get more of the sampling of the much less data from the, from the human activity movements. So with this sampling, you can see we actually reduced several million human activity data to about like the same number of the earthquake data we use.
00:37:21.588 - 00:38:05.318, Speaker A: So this is the final data showing you where is the location, where is the feature location on these two feature map. So you can see this is the final earthquake data. You can still see there's a lot of these ones over, basically are like overlap with each other. So this is the way, if you just draw a decision boundary here, you're still making mistakes when they are overlapping with each other. So, but anyway, this is the final class classification schema we are using. Basically, we use three features for easy visualization. I'm plotting two features.
00:38:05.318 - 00:39:00.034, Speaker A: One is the amplitude, one is the frequency, and the earthquake data is the red. And non earthquake data is blue. And use this to classify which one is earthquake, which one is not. And if you look at the third dimension, the cumulative absolute velocity, we found that a lot of the overlap data points actually got stretched into the third dimension, that actually, when you train the model to build the decision boundary, it helps you to make the decision better for a lot of these overlapped data. So this is the three features we feel that reasonably good to implement on the phone. And at the same time, it's also reasonably easy to calculate, not use a lot of resources on the phone to drain the battery, to compete in other apps to use the resources.
00:39:01.014 - 00:39:04.310, Speaker B: So the earthquake data is generated from the shake table test?
00:39:04.382 - 00:39:07.622, Speaker A: Yes, and also the simulation data.
00:39:07.798 - 00:39:18.962, Speaker B: And so by generating signatures with certain statistics, you can, can basically achieve whatever success rate in the classification you like, right?
00:39:19.058 - 00:39:19.898, Speaker A: Yes, that's right.
00:39:19.946 - 00:39:25.394, Speaker B: So how do you decide how many red points, essentially, to put there?
00:39:25.514 - 00:40:16.420, Speaker A: Oh, yes. So in the very nice, in a very simple way, the more data you have, the better, because it captures different variability of the earthquakes. We try to actually grab all the data we can find, like all the global earthquake data we can find that recorded on the traditional seismic network. And also we try to keep a balance of, like, I'm doing the whole year of shaking table tests at Richmond fuel station here. So basically try to grab as much as data we can have to train the artificial neural network. Yeah, but the other thing is that for the human activity data we collected for the whole six months, using that as the training data, did I answer your question? Yes.
00:40:16.452 - 00:40:19.580, Speaker B: So the 93%, what exactly does it correspond to?
00:40:19.652 - 00:41:06.428, Speaker A: So I'm going to tell you more, a little bit more. So the 93 is corresponding to five months of the human data. With all the earthquake data, we are actually worked out to train the model, and then we use that to. We also have, like, a 20% of the test data. So this is basically the test arrow I'm reporting here. So basically this is saying 90% of the time human triggers are correctly classified as like human triggers, but 7% of the time it's wrongly detected as earthquakes. Yeah, but also let's bring up these performance evaluation.
00:41:06.428 - 00:42:01.206, Speaker A: So just to quickly refresh, because sometimes I feel this is confusing. Type one arrow and type two arrow. You can see for the type one arrow, it's very simple that if you don't have earthquakes but you make earthquake, you make the decision this is earthquakes, but for the type two arrow, which you have, but you actually completely missed it. So these two types of arrow is the one we actually can make during our implementation of the artificial neural network. So this is a quick evaluation. So all this data is not present in the training, but you can see for different earthquakes within. So the actually the type two arrow is different depending on the distances, because close to the distances you have more energy from the earthquake, you are more likely to detect it.
00:42:01.206 - 00:42:44.614, Speaker A: You can see this is two number, the second number showing you how many smartphones within 10 this earthquake. And then this is the number actually how many phones actually detected the earthquake. You can see within 10 km. Only one phone didn't detect these park field magnitude six earthquake. But then when you go to 30 km for this earthquake, you actually have more phones that didn't detect the earthquake. And also for smaller magnitude earthquake, because the energy release is much smaller, you actually see going further and further away, you have less detection rate. So this is.
00:42:44.614 - 00:43:34.666, Speaker A: But we think that within 10 km, for the large earthquakes larger than magnitude five, most of the earthquake signal can be detected by our trained artificial neural network and also for the human activities. So you can see that we use one month data actually as our training test data set. So we actually have about 7% of them are detected as earthquakes. These are more causing problems like when they detected as earthquakes, you may issuing the wrong earthquake alerts. But remember, this is just on one phone. But think about 24 hours. If I have my phone reporting 7% of 24 hours as earthquakes, it's actually still a lot of times.
00:43:34.666 - 00:44:27.224, Speaker A: So that's why not only we need to have this earthquake detection algorithm running on the phone, but also we need way to aggregate the results from multiple phones. Because it's simple to have one phone detecting it's an earthquake and you don't know whether it's an earthquake or not. But it's not easy if a whole bunch of phones within one region actually triggered as earthquakes. For example, within 10 km, all my phone seats thinks it's an earthquake. So the probability of it's randomly triggered by human activity actually is really low. So this is actually the idea of the second level of the detection algorithm on top of the artificial neural network running on your phone. And then that's actually basically running on the server to aggregate the results from multiple smartphones.
00:44:27.224 - 00:45:30.932, Speaker A: This is just a simple cartoon showing you if in one region I have these smartphones, this is all the active smartphones running myshake. And then our detection algorithm, the second level, actually we tried different clustering algorithm. And also basically you can think if I want to aggregate results from multiple phones. So it's clustering both in time and space. So we think that we actually, if some in one region, for example in Bay Area, all the phones starting to report earthquakes, the probability of a force alert is really low. For example, I have four phones starting to think I'm experiencing an earthquake. And then we can say withdrawal a circle of these four phones, like the furthest distance between the centroids of these four phones.
00:45:30.932 - 00:46:36.786, Speaker A: And then we can actually ask the question, how many phones within this circle and what's the percentage of these phones starting to detect in earthquakes? So currently our threshold is setting at 60% within these 10 km radius. And if 60% of the phones are detecting an earthquake, we think this is likely an earthquake. So this is where Richard showing you before, like this is the starting of the earthquake. And then you can see a bunch of phones starting to detect the earthquake becomes red. And then when it's actually above 60% within the region starting to detect earthquakes, we actually detected this earthquake, which is shown as this, like a green star here. And then we can say this is a real earthquake and then this is actually the way we clustering them based in space and also in time. And we actually run this on our data set within the last two years.
00:46:36.786 - 00:47:58.884, Speaker A: Actually it works reasonably well for the, for a lot of the regions that we didn't generate any of the false alerts because this 60% rule, it's very hard to actually exceed this rule, because within a region all the phones are starting to shaking due to some other effect, is very low. So we actually also try to use the students on campus to ask all the students carry my shake phone and go to a football game and then see whether they, when they all. Maybe there's a score or something, whether they can trigger the system or not. So actually that's not triggered at all. But we think this algorithm actually works reasonably good for our first detection of the earthquakes. But anyway, there's another issue you need to consider in the real time detection is that I detected the earthquake, but then you have more information starting from other phones. But these phones, you need to try to associate these phones with these events you already detected, because if you don't associate them correctly, they will say, I have experiencing earthquake here, I detected another earthquake here, I detect another earthquake.
00:47:58.884 - 00:48:32.428, Speaker A: So your system will start to, like. Your system will start to, like, report earthquakes all over the world. So that's why you need to try to associate these triggers for the event. The way we trick. We associate it, actually, we try to take the physics into the consideration or the problem. So you can see this is the distance from the absence center, and this is the time from the earthquake. So the seismic wave actually travels at a certain speed.
00:48:32.428 - 00:49:36.866, Speaker A: So this green one curve, showing you what the expected seismic wave travels for the p wave, which is the fast traveling wave, and which is the. The red one, is the slowing movement or the ice wave. So you can see they actually have a pattern here that if you have any dots before the p wave, it's likely not an earthquake trigger. And if you have a dots, like, far away from this line here, it's actually also not earthquake. So we actually incorporate this physics, like the how fast the p and s wave can travel, take into our problem and implement into our system, and reject the triggers. If I have a trigger far away from the ice wave arrival time, it's likely it's not earthquake detection. And also, if I have triggers before the p wave, it's also probably either the clock is wrong or it's triggered by something else.
00:49:36.866 - 00:50:41.122, Speaker A: So this actually taking the physics into our model actually helps us a lot to associate the triggers for the more and more phones triggered by the earthquakes. So this is like the way we implement it on our phone. And then we actually spend a lot of time to verify whether the algorithm is work, and then we realize that the other one is. I think Richard didn't mention, is that when you build these large scale projects, like these whole systems, and it works well, very well, and then when you release to the public, then you have all sorts of questions. So, we actually spent last year, a lot of times we are actually addressing the technical issue on the implementation side. So, for example, we released the app Friday afternoon, and we soon learned the lesson that you shouldn't release the app in the Friday afternoon. And then you will see that all your engineers disappear for the weekend.
00:50:41.122 - 00:51:34.626, Speaker A: And then if you have any problems, then you will actually in trouble. So our system released, and we all have a bet how many people will download our app. Our crazy guess is about 10,000 phones. Actually within the first few days we actually got 80,000 phones download our apps, which actually crashed our server, crushed our implementation several times. But anyway, this is just some lessons we learned during the course of implementation. They implement this and also got other kinds of requests or requirements from the users as well. But anyway, as Richard showed that right now we almost have about 300,000 people download the app all over the world and help us to detect earthquakes.
00:51:34.626 - 00:52:51.906, Speaker A: So for the real time detection algorithm, like Richard mentioned, that we actually, a lot of the times we didn't detect it because we don't have enough density, because a lot of these phones have low quantity that you need really large earthquake and the really large density within the app center that you can actually detect it. But if you run offline. So this is basically these about 800 earthquakes we detected from this smartphone data is of the fact that we already collecting the data, we already know there's something happens and then we actually can see we recorded like about 800 or. But this is not in real time, the real time one right now we only detected like four large earthquakes, three in California, one in New Zealand and so on. So. But anyway, this is also brings another issue is that like we have these wrong decisions about like the earthquake non earthquakes in real time, because we only use 2 seconds window. Currently we are also explaining, try to understand or try to detect the earthquakes using another approach on the server.
00:52:51.906 - 00:53:42.406, Speaker A: Because if we want to detect them in real time, we only have 2 seconds information because we want to detect them as fast as we can. We can't use for example 10 seconds or 1 minute, that will use the up a lot of time for our detection. But on the server part after the earthquake, we actually collected a lot of waveforms data, five minutes data. And you can see this is earthquake data, this is human activity data. But if you see them at a longer time scale, you actually see very easy or relatively easy to detect them. So due to the time constraint, I won't go to the details about. But right now we are using the convolutional neural network, which is methods really good at detecting the images.
00:53:42.406 - 00:54:32.230, Speaker A: So basically we treat the image, you can think that we treat the images as a real full pixel numbers and then we actually try to. Yeah, I won't go to the details about the convolutional neural network, but we try to form the image of for earthquakes and non earthquakes from the whole waveforms we collected on the backend server because we have much longer time scale. And then you can see this image reformed using the data from earthquakes. And this image actually reformed using data from non earthquakes. So you can see a lot of times, even by eye, you can see some sort of pattern already from the earthquake and non earthquake. And then we train the art. We train actually the convolutional neural network.
00:54:32.230 - 00:55:33.988, Speaker A: So this is our model, basically, we form image of the earthquake from the waveform we collected. And then we use the convolutional neural network got actually really nice successful rate based on our test. But right now the problem is still that we have so many of the human activity data and the earthquake data is still very friction like. Even we have about 800 earthquakes data and still compared with the human activity data, which is in the order of millions of data. So it is still a small portion. So we are thinking ways like transfer, learning to bring in other domain like knowledge, whether we actually learned a lot of similar effect, for example, in music, when they try to detect a certain type of things, maybe some features actually can be used by our network and so on. But this is all the things we are exploring.
00:55:33.988 - 00:55:52.564, Speaker A: And I think I will stop here. And if you have any questions or any suggestions, let me know. I think the time is running out, which is perfect. Okay, thanks. Any questions?
00:55:53.984 - 00:56:04.168, Speaker B: Can you. I mean, when you have this positive trigger from a phone, do you send these three statistics which the trigger was based on?
00:56:04.296 - 00:56:37.068, Speaker A: No, we are actually not sending this three statistic. This is all calculated on the phone. And the phone will based on the three features. So it will tell me whether this motion is more like an earthquake or not. If it's more like an earthquake, then send a message which contains the location of the phone, the time of the trigger, and also what's the amplitude of the shaking back to our server. And then the server will aggregate these very short messages of the location and time.
00:56:37.166 - 00:56:46.896, Speaker B: You think, like, the one of them is basically frequency or number of zero crossings? Yes, those would be very common between different phones. Detecting an earthquake?
00:56:46.960 - 00:57:52.586, Speaker A: Yes. Yeah, we think it actually works well, is because the earthquake frequency, the dominant shaking actually is within certain frequency range. And during that dominant frequency range, the amplitude also have some pattern. If you combine these two information, a lot of the human activities, you see that when you walk with the phone, when you pick up the phone, it's relatively low frequency movement compared with the earthquakes, but the amplitude actually is larger than the earthquakes. But if you put the phone on the desk or a table, so the frequency can be relatively broader, but the amplitude is much smaller compared with the earthquakes. So that's why we think it actually works in our case to detect most of the earthquakes. But also we found out a lot of, sometimes when we do the test, we found out a lot of the transition between different motion actually have some similar, similar features as the earthquakes.
00:57:52.586 - 00:58:04.148, Speaker A: That's when the detection algorithm starting to confusing and make the wrong decision. So. Yeah, but then that's taken care by the, by the network detection algorithm.
00:58:04.316 - 00:58:41.844, Speaker C: Yeah, yeah, so, yeah, sorry, I had a related question actually, which is. Yeah, what, so why have you tried like, what have you tried saying what you get if you do take like the actual frequency data from two different phones and compare them? Because you'd expect as long as two people are at least say 100ft away from each other, that all of the noise should be almost completely independent. And so you'd expect it to be very noticeable, whereas any earthquake system would be like highly correlated rather than just like, sit. Because I guess you're just aggregating basically one bit from each person.
00:58:42.184 - 00:59:28.994, Speaker A: Yeah, actually we do test that we try to see in our two years data how many people within one area actually can trigger almost at the same time during our running of Myshake, even in Bay Area, where we have the dentist smartphones, at a certain time, within like 20 seconds window, for example, if I'm looking at a 22nd window, it's rarely to have more than four phones actually triggered at the same time because a lot of these activities, especially the human activities, filtered out by the artificial neural network, but for the earthquakes within 20 seconds window, you actually see a lot of these triggers by the detection algorithms.
00:59:29.494 - 01:00:05.654, Speaker C: So, yeah, that wasn't quite. So what I was asking though is if you just look at the actual sensor data and compare, say, the sensor data from two different people without, like, without, you know, running this like local neural network on it, can you, by comparing those two, can you get a better result for whether that. For whether a strike is due to noise or due to an earthquake than you would if you just looked at the frequency data on one sensor?
01:00:06.314 - 01:00:18.524, Speaker A: Oh, I see. So basically you are saying from maybe phones at certain distances and I just try to correlating these human activities or earthquakes data.
01:00:18.604 - 01:00:22.756, Speaker C: Yeah, because any human activity you'd expect to be independent between them.
01:00:22.900 - 01:00:48.774, Speaker A: Yeah, we haven't tried that, but, yeah, that might be way, but yeah, we haven't tried that. We think that the human activities, a lot of the times, unless many people starting to run at the same time, at the same location, but a lot of these data currently filtered out by our data, like the artificial neural network approach. But we can try that. Yes.
01:00:51.074 - 01:01:26.658, Speaker D: It seems as though the features, again, operating on a single phone that you're looking at are kind of isotropic. They don't distinguish between compressive nature and shear motion, for instance. Is that the case? And is there something to be gained from, can these detectors sense, well, there's a compass, you have some kind of maybe notion of orient, of reliable more or less orientation. Is there something to be gained from looking at the anisotropic aspects of the motion?
01:01:26.786 - 01:01:32.706, Speaker A: Yeah, right now we are not. Yeah, I know. That's probably better when you consider you're.
01:01:32.730 - 01:01:35.442, Speaker D: Calling it a p wave. Yeah, calling it an s wave.
01:01:35.618 - 01:02:01.042, Speaker A: That's right. So for a lot of these, currently the sensors on the phone is highly depending on like what the sensor quality is. So for the very nice sensors, actually you do see the p wave and s wave for some earthquakes. And you can tell the difference between the compressional wave and the shear wave on these sensors. But a lot of the phones.
01:02:01.138 - 01:02:02.370, Speaker D: Based on gravitational.
01:02:02.562 - 01:02:30.986, Speaker A: Yeah, based on the acceleration. Yes, that's right. But on a lot of these phones, especially the low end phones, during the shaking, for some type of earthquakes, you only see the largest shaking, which is the shear wave shaking. On these phones, most of the, like the relatively moderate magnitude earthquakes, you only see the largest portion on that. But you can't see the compression waves on the phones.
01:02:31.090 - 01:02:34.098, Speaker D: So you lose the time advantage.
01:02:34.186 - 01:03:03.004, Speaker A: Yes, you lose some, but we think for the larger earthquakes, especially the magnitude six and seven. So the compressional waves actually triggers a lot of the functions. We can actually take advantage of that. But for smaller earthquakes, you need the phone be really close to the app center to detect the P wave or the compressional waves. So yeah, that's depending either on the sensor and on the location where you are and also magnitude.
01:03:04.384 - 01:03:09.688, Speaker E: So it seems like in your system you're able to tell which phones are switched off. You have the red dots, the yellow dots.
01:03:09.736 - 01:03:11.044, Speaker A: Yes, that's right.
01:03:11.724 - 01:03:19.564, Speaker E: Reporting or on the system, can you use the silence of a lot of phones. So the dog that didn't bark.
01:03:19.604 - 01:03:19.756, Speaker A: Right.
01:03:19.780 - 01:03:28.212, Speaker E: So you have a bunch of phones being triggered in a certain area, but a whole other bunch of phones not being triggered. Does that tell you, I mean, is it possible to use that information to.
01:03:28.268 - 01:04:03.110, Speaker A: Yeah, that information actually tells you about the magnitude of the earthquake. We are thinking so we are actually thinking now because for the larger earthquake you actually see a large area. A lot of the phones actually detected the earthquake. But for smaller earthquake, you started to see a lot of the phones didn't detect the earthquake. If you see the magnitude 4.4 earthquake from the Berkeley event, even though you can see we have 59, about 60 phones detected the earthquake, but a lot of them are, didn't detect the earthquake during this earthquake. The reason is that.
01:04:03.110 - 01:04:45.934, Speaker A: So when we train the artificial neural network, we kind of also know our lower capability, what type of earthquake we can detect. So our lower capability actually is at about 4.6. So it's actually at the border. So that's why you're starting to see a lot of the phones didn't trigger. But this actually tells you something about the magnitude of the earthquake. You can use this information as an indicator of large earthquake or smaller earthquakes, but also we are thinking whether this information also can tell you something about the detection as well. But right now, we just started that process.
01:04:49.354 - 01:05:03.190, Speaker F: In a major earthquake, a lot of the base stations are going to fail. You're going to get your alert out before the base station stations fail. The base stations fail enough seconds after the initiation.
01:05:03.222 - 01:05:24.250, Speaker A: So that's always debate whether we use the high quality sensors or the low quality sensors. So the high quality sensors, definitely, if we have the efforts of time, money, and then we can deploy them around the globe, that would be great. It can detect a lot of these earthquakes, but at a lot of places we actually don't.
01:05:24.282 - 01:05:25.594, Speaker F: I'm asking you a different question.
01:05:25.714 - 01:05:26.306, Speaker A: Okay.
01:05:26.410 - 01:05:28.258, Speaker F: You're depending on the cell phone network.
01:05:28.386 - 01:05:29.214, Speaker A: Okay.
01:05:29.634 - 01:05:39.266, Speaker F: Sometime between 2 seconds after the shaking starts and 30 seconds after the shaking starts, the cell phone, base station, the.
01:05:39.290 - 01:05:41.730, Speaker A: Telephone tower, you mean. Okay, right.
01:05:41.762 - 01:06:02.518, Speaker F: This does nothing by itself. It depends on base stations. I see owned by my, provided by my cell phone carrier somewhere around, I would imagine, two to 30 seconds into a major earthquake at and T's base stations are going to shut down. T Mobile's base stations are going to shut down and so on.
01:06:02.606 - 01:06:03.238, Speaker A: Yeah.
01:06:03.406 - 01:06:19.554, Speaker F: Is this. Get, this affects your ability to measure and it affects your ability to successfully to alert people, right?
01:06:19.634 - 01:06:38.754, Speaker A: Yes, that's actually a very good question. Yeah. So, no, it's actually not. Otherwise you will hear the earthquake sound. Earthquake. But that's actually a really good question for like, the cell phone tower, whether during the earthquake, whether it will be disabled or.
01:06:38.874 - 01:06:44.774, Speaker F: We know, we know from history that 20% to 80% of them are going to be disabled.
01:06:45.794 - 01:07:21.044, Speaker A: But for the earthquake detection, for the smartphones. So if we actually have the smartphones close to the app center. So it's usually you can see the example usually it's within like a two or 5 seconds. Then you starting to have these phones to detect the earthquake. And then sending this out is also like in the, in the order of like one or 2 seconds. So. But we think that later, maybe for later distribution of the users, if the cell phone tower is down, so we probably will lose all the connections.
01:07:21.044 - 01:08:02.134, Speaker A: But also I think there's a project they are monitoring these type of cell phone towers and they can also tell something about whether this is an incoming like outrage or incoming earthquake and so on. But yeah, we are not in that type of study, but we think that these type of earthquakes actually this type of system can activate especially for the larger earthquake. These phones will starting to detect at the p wave, which is relatively small motion. So then we send out the message and then even before the largest shaking starts, a lot of the places already got the information. Yeah.
01:08:06.034 - 01:08:11.774, Speaker D: Just thinking again, sort of in detail in terms of the sort of the daily rhythm of cell phone use.
01:08:14.914 - 01:08:15.226, Speaker A: You'Re.
01:08:15.250 - 01:08:34.126, Speaker D: Blotting out when it's not steady, you have the steady basically binary switch on and off. So in that other realm of operation, I presume during the daytime when phones are in purses and pockets around through that time zone. Yes, those are out of the cohorts.
01:08:34.230 - 01:08:35.086, Speaker A: That's true.
01:08:35.270 - 01:08:39.206, Speaker D: Do you have a feel for what kind of a falloff do you get?
01:08:39.350 - 01:09:35.884, Speaker A: Yes, so we actually, based on our system in the last two years, during the day, a lot of the times we have about like a ten to 15% of the active phones actually is monitoring the earthquake. But during nights, these percentage actually goes up to 95% because a lot of the fumes are actually steady. But if you are moving during the day, so we think that even ten to 15% if we have enough density, so it's actually enough to help you to detect the earthquakes already. But it's true, this network, not like the traditional seismic network, you have a fixed configuration. This network is basically always changing during the time, especially during the day. Yeah. So earthquakes occur at night is the best time for us to detect the earthquakes.
01:09:38.544 - 01:09:40.112, Speaker E: Let's thank the speaker again.
01:09:40.208 - 01:09:44.744, Speaker A: Okay, thanks. Let me know if you have more questions.
