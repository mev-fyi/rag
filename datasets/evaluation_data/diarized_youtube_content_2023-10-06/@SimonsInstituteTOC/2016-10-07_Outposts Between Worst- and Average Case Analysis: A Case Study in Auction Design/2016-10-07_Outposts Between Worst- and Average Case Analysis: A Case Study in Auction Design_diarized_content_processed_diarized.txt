00:00:00.240 - 00:00:00.960, Speaker A: Welcome, everyone.
00:00:01.078 - 00:00:09.454, Speaker B: It is my pleasure to introduce Tim RAF Carton. As you notice, we had a small change in schedule. So Tim is still talking now, and.
00:00:09.494 - 00:00:10.954, Speaker A: He'S going to talk on.
00:00:13.134 - 00:00:15.434, Speaker B: Well, let's talk about the worst case.
00:00:17.854 - 00:00:45.822, Speaker A: Thanks very much. Thanks for having me. Happy to be here. So, just for the context of the talk. So, the other program which is happening this semester at the institute, algorithms and uncertainty. A major theme in that program is a circle of ideas that I like to called beyond worst case analysis. So, for problems, computational problems, where worst case analysis essentially gives you bad advice about how to solve it, it's a quest for alternatives to analyze algorithms which give you more accurate advice about what algorithm you should use.
00:00:45.822 - 00:01:24.084, Speaker A: So, this talk is sort of squarely in that theme, sort of trying to understand alternatives to worst case analysis, especially ones that draw at least somewhat on distributional assumptions. So, let me just begin. I'll tell you a little bit what I mean by worst case analysis, why you might want alternatives, what the alternatives might be. And then for most of the talk, I'll use a simple problem in auction design to illustrate a bunch of interpolations between worst and average case analysis that have been sort of useful for proving nice results. And obviously jump in with questions at any time. Actually, I picked up the wrong one.
00:01:25.064 - 00:01:26.044, Speaker C: There we go.
00:01:26.384 - 00:01:27.792, Speaker B: Exactly. The first case.
00:01:27.928 - 00:02:06.666, Speaker A: Exactly. But mid course correction. So, worst case analysis, what do I mean? So, imagine we've settled on a computational problem, and we've settled on some performance measure for algorithms like the running time, or like the quality of the solution produced, or like whether or not it's correct. What worst case analysis means is just you take the entire performance profile of an algorithm ranging over all inputs. And as a summary statistic, you look at the one number of what's the worst that the algorithm ever does on some input? Maybe you say, parameterized by the input size. And we all know why worst case analysis is so popular. So, like, what do I tell my undergrads in the undergrad algorithms course? Well, I say, you know what's great about worst case analysis is whenever you can prove a positive result, it's sort of great.
00:02:06.770 - 00:02:07.090, Speaker C: Okay.
00:02:07.122 - 00:02:31.450, Speaker A: It really means you don't even have to have any model of data. You don't have to worry about what input shows up. You just run your algorithm, it's going to do great. Everybody goes home happy. And moreover, perhaps surprisingly, there's actually a lot of problems where worst case analysis gives you great advice. It naturally leads you to exactly the problems, the algorithms you would want to use to solve those problems in practice. Undergraduate algorithms is chock full of those kinds of killer applications.
00:02:31.450 - 00:03:22.894, Speaker A: But as you all know, there's also perils to worst case analysis. So principally it can be overly pessimistic. And so one extreme example of this would be, say, the running time of the simplex method for linear programming, which is famously, theoretically exponential in the worst case, even though you never ever see running times anywhere close to exponential on real linear programs. And then also, I sort of mentioned not having a data model as being a good thing of worst case analysis, but it cuts both ways. So the fact that you don't have a data model, actually, you sort of have a data model, which I would call the Murphy's law data model. So Murphy's law says whatever can go wrong will go wrong. So worst case analysis is basically saying, I assume that the input which is relevant for my application depends on which algorithm I pick, which is a sort of kind of paranoid and slightly incoherent way to think about the world, basically.
00:03:22.894 - 00:04:13.272, Speaker A: So what would be an alternative? Well, there's another way of analyzing algorithms. It shows up in undergrad classes, average case analysis. So how would this work? Well, you posit some distribution over the inputs, and you just want to optimize the expected performance of the algorithm, the expectation with respect to that input distribution. And this is also, you know, super useful and actually more and more these days as the amount of available data gets getting bigger and bigger. If you have a problem, you have a really good understanding of the input distribution, and that understanding is not too volatile, and you don't need a general purpose solution by all means, you know, use the algorithm which is best on average. On the other hand, the concern is that the algorithm you wind up deriving is an artifact of the specifics of your input distribution. So the worry is that you come up with a solution, which is fine for one distribution, but it's not robust.
00:04:13.272 - 00:04:59.006, Speaker A: If you're a little bit wrong on the distribution or the distribution changes on you, maybe the algorithm won't be even near optimal anymore. So those are some of the standard concerns with average case analysis. So, a major theme in the other semester, a big part of beyond worst case analysis is what I like to call hybrid models. So these are trying to take the best of both worlds from the worst case world and from the average case world. And so what's been emerging, what's been clear is that for lots of problems, there's a sweet spot for analyzing algorithms of this sort of a hybrid model. So if in the worst case you look at the worst input in the average case, you average over inputs. In a hybrid model, what you do is rather than committing to just one input distribution, you say, well, let me just make some very coarse assumptions about what the input distribution looks like.
00:04:59.006 - 00:05:39.018, Speaker A: I'm not going to assume it's uniform or exponential or normal or anything like that. I'll just assume maybe independence or sort of some control over the tails, this kind of thing. So I have in mind a whole big set of input distributions, and I have no idea which one is the real one. And then what you'd like is you'd like an algorithm that no matter what the input distribution is in the set, you do well on average with respect to that distribution. So you take a worst case over what the input distribution might be, and then you take average case with respect to any particular distribution. Thinking of it differently, you want an algorithm which is sort of simultaneously near optimal for lots of distributions all at once. So there's a lot of stuff you can say about these hybrid models.
00:05:39.018 - 00:06:23.002, Speaker A: The whole second half of my bootcamp tutorial was all about examples of these. So why do I like them? Well, so when it works out, you inherit the robustness from the worst case world. So because the algorithm has to be good for many, many different input distributions, it can't be overly tailored to any one of them. And then also, actually, when you solve for the best algorithm of this type, the theories tends to naturally lead you to kind of very practical algorithms, things you could actually imagine implementing. And in worst case analysis, that's true sometimes and not true sometimes. And there's famous examples of these hybrid models, which you've heard of, which I'm not going to talk about. That smooth analysis by Spielman and Tang can be thought of as a hybrid model, where basically the distribution just has sort of sufficient entropy in it.
00:06:23.002 - 00:06:41.626, Speaker A: And semi random models also, and then random order models like, say, secretary problems, if you're familiar with those, those all sort of fall under the umbrella of hybrid models. Okay, those aren't the ones I'm going to talk about today, though. But there's many examples. So what I want to talk about today is I want to talk about a simple problem in auction design.
00:06:41.730 - 00:06:42.282, Speaker C: Okay?
00:06:42.378 - 00:07:01.712, Speaker A: So I'm going to use auctions as sort of a scaffolding to talk about lots of different ways of doing performance analysis, okay? Because it's convenient. But all of the ideas we'll talk about are relevant in a broad algorithmic context, not just about auctions. Okay, so what sort of performance measure of auctions might you want to optimize? Okay, well, if you're selling stuff, you might want to maximize revenue.
00:07:01.808 - 00:07:02.568, Speaker C: Okay.
00:07:02.736 - 00:07:34.394, Speaker A: And, you know, even if you just sell something on eBay, there's sort of a decision you have to make which affects the revenue that you make. Okay, so here's what an eBay auction looks like. So you advertise an item, people bid on it, and you don't have many things you can choose. In an eBay auction, almost everything is determined for you. But one knob that you do have is the opening bid. So the opening bid says, don't even bother bidding unless it's at least this number. And you can set it to be zero, totally allowed, or you could set it to be really large, 8000.
00:07:34.394 - 00:08:11.538, Speaker A: And so the question is, how should you set a reserve price when you put something up on eBay? As far as how the rest of eBay works, if you haven't seen it before, it's equivalent to what's called the second price auction. So basically, when you bid on eBay, eBay will bid on your behalf just high enough to beat out all the competition if you're actually the highest bidder. So one thing to realize is, why is it a second price auction? Whatever you bid in eBay in general, if you win, you don't have to pay the full amount that you bid. You only have to pay the minimum amount needed to bid everybody else. That's why it's a second price auction. You only pay the second highest bid overall. And these opening bids are often called reserve prices in auction theory.
00:08:11.706 - 00:08:13.614, Speaker B: Is this a true sketch?
00:08:14.514 - 00:08:27.654, Speaker A: So this is not Photoshop. I'm trying to remember the Google search that led me to this. I think I googled ridiculous eBay auctions and this is what I got. I mean, to add to the credibility of it, I will point you to.
00:08:30.394 - 00:08:32.934, Speaker B: This could be the Twinkie from the Twinkie defense.
00:08:34.794 - 00:08:58.922, Speaker A: I could imagine it could be. Maybe someone's really hungry. Yeah. So how should you set this opening bid? Well, this actually, it's already a super simple problem. It's not that easy to reason about. If there's a sucker willing to pay $8,000, this looks like a great reserve price. If there isn't, this looks like a really stupid reserve price.
00:08:58.922 - 00:09:29.386, Speaker A: But of course, you don't know what people are willing to pay. That's why you're running the auction in the first place. So how should you think about this? And for people who think about algorithms, this is not a new problem. So, like any two algorithms, one's going to be better on some inputs, the other on other inputs. And we still have lots of ways of kind of comparing algorithms and reasoning about algorithms. And again, the first one that you try in the computer science world is worst case analysis. So let me just point out how for this ProB, this just simple, how do you set an opening bid problem? Worst case analysis is a total non starter.
00:09:29.490 - 00:09:29.954, Speaker C: Okay?
00:09:30.034 - 00:09:49.696, Speaker A: Nothing interesting is possible using sort of straightforward worst case analysis on this, on this very simple problem. So to do this, I need a model. So single item auctions. So this is what we'll be thinking about. All talk is this simple setup. So it's really just sort of the eBay setup. You have one seller, one item, some number n of bidders and valuation.
00:09:49.696 - 00:09:52.232, Speaker A: That just means the maximum willingness to pay.
00:09:52.328 - 00:09:52.640, Speaker C: Okay?
00:09:52.672 - 00:10:01.768, Speaker A: So bidderi is willing to pay up to Vi dollars at most for this item. And importantly, only the bidders know these vi's. As a seller, you don't know what these are.
00:10:01.816 - 00:10:01.992, Speaker C: Okay?
00:10:02.008 - 00:10:03.080, Speaker A: That's why you're running the auction.
00:10:03.152 - 00:10:03.888, Speaker C: Okay.
00:10:04.056 - 00:10:35.940, Speaker A: And again, what you'd like to do is make as much money as possible. All right, so what would worst case analysis say? It would say, well, you know, we have this set of feasible solutions. All the different opening bids we could set and input by input. That is, no matter what bidders valuations are, we would like to make close to the revenue we could get with the best of the feasible solutions, with the best opening bid. So, to see the problem, suppose only one bidder shows up. That's not necessary for the bad example, but it makes it simple. Suppose one bidder shows up and they have some willingness to pay v.
00:10:35.940 - 00:11:10.968, Speaker A: Okay, and maybe it's something like $7,999, maybe that's their valuation. Now, there is an opening bid, if you chose it, that would give you $7,999, right? If you set that as the reserve, as the opening bid, you would make this full v dollars of money. Maybe you set the reserve to be v minus epsilon. But if you sort of telepathically knew what their willingness to pay was, you could extract all of that as revenue by just setting the opening bid to the valuation, of course. But if you set 8000 instead, you're going to make nothing.
00:11:11.136 - 00:11:25.120, Speaker D: Okay, is this also taking into consideration the second fact that you get paid the second amount too, because you might price it so high that the second person has much lower valuation, then you only get that much money.
00:11:25.192 - 00:11:46.274, Speaker A: So. Right. So the way ebay works is you pay the minimum, the maximum of the reserve or the second highest bid. So it's kind of only good for the seller if the reserve is in sort of the good case for the seller with the reserve is you split the first and second highest bidders. That's where it gives you a payoff. Yeah, but of course, the risk is if you overshoot everybody, you lose everything. Okay, so again, nothing deep here.
00:11:46.274 - 00:12:17.032, Speaker A: I'm just pointing out that if you thought about what would be the most straightforward way of applying worst case analysis to the problem of choosing a suitable opening bid, it tells you nothing. It tells you every algorithm has no guarantee at all. There's really nothing to prove in a formal sense. So clearly we do need some alternative if we want to sort of reason about different auctions. So, you know, auctions have been around for a long time. I mean, probably like economists have been thinking about auctions forever, right? Yeah, they have. So what do they do? They've made progress on this problem.
00:12:17.032 - 00:12:38.524, Speaker A: Well, the dominant paradigm in economics is what we would call average case analysis. So they would call it bayesian analysis. So we're going to make a distributional assumption. We're going to assume that bidderi's willingness to pay is drawn from a distribution f, sub I. I'll always be thinking of the valuations as being independent. We'll go back and forth between, whether they're identical or not. So both cases are interesting.
00:12:38.524 - 00:13:14.814, Speaker A: So as the seller, the assumption is, you know, or you have in mind these distributions f one through FN. So you know how the valuations are distributed. You do not still know the realizations, you do not know the vis, that would trivialize the problem, but you do know the FIS. Okay, so now we have a distribution over inputs, right? So just bids drawn from these distributions. And now we have a well defined optimization problem. We can save all the auctions in the world, find the one that makes the highest amount of expected revenue, where the expectation is with respect to this distribution. That's a well composed mathematical problem.
00:13:14.814 - 00:13:54.314, Speaker A: There's going to be some auction which is better than any other with respect to some fixed distribution, and it's natural to ask what it is. Okay, so, all right, so I talked about all the auctions in the world, and we're not going to, you know, I'm going to deliberately just stick with very simple auctions throughout this talk, because I know some of you have sort of probably never thought about auctions. So go ahead and think about eBay as a canonical example. Second price auction with a reserve. But in general, the optimal auction can be pretty crazy. It just has to have some way of, given the bids, figuring out who wins and figuring out what the winner, if there is one, is going to pay. So the auction design space is very rich, that we're going to focus our attention on simple points in that design space.
00:13:54.314 - 00:14:38.624, Speaker A: This is the standard economic setup. And indeed, in economics, this problem was totally solved, completely solved by Myerson about 35 years ago. What do I mean solved? Myerson gives a closed form solution for what the optimal auction is as a function of the distributions. To be clear, notice that what the optimal auction is, is going to depend on what the distribution is. So, for example, if you're using a reserve price, clearly, if the distributions are sort of just have small valuations, you'll use a small reserve price. But if the distributions just have lots of high valuations, you'd expect to use a high reserve price. So there is going to be some dependence on what the optimal auction is, on what distributions you assume.
00:14:38.624 - 00:14:52.184, Speaker A: But Myerson solves for the entire mapping. He just says, tell me what your distributions are. I'll tell you what the optimal auction is. So it's a great result. This is one of the results for which Myerson got the Nobel Prize, and I think it was zero seven.
00:14:53.224 - 00:14:53.848, Speaker C: Okay.
00:14:53.936 - 00:15:35.498, Speaker A: And in the symmetric case, when I say symmetric, I mean IId. So imagine all bidders valuations are distributed identically. Then actually the optimal auction is just eBay, which is kind of amazing, because again, the auction design space is really rich, has all these very strange creatures in it. We optimize over this whole big space, and we get back a super practical, indeed already widely used auction, second price auction with a reserve. So there is still going to be some dependence of this auction on the distributional assumption, but it will only be through this reserve price. So depending on what the valuation distribution is, you'll use different reserve prices. But actually, even the reserve price itself has a super simple intuition.
00:15:35.498 - 00:16:04.254, Speaker A: It's what's called the monopoly price of a distribution. That just means what, take it or leave it offer. Would you give somebody drawn from a distribution to maximize your expected revenue? So the distribution is given by f, you just say, okay, well, which price would maximize my expected revenue? So this is the revenue of a sale. This is the probability of a sale, whichever p makes this as big as possible. That's called the monopoly price in particular. And people often find this counterintuitive. The best reserve price to use does not depend on the number of bidders.
00:16:04.254 - 00:16:44.252, Speaker A: It's independent of n. You always just want to use the monopoly price for the distribution. So this is just a magical confluence of theory and practice, right? We solve, over this huge space, the optimal point of something totally can just put to use right away the general case, while Meyerson did solve it. So we sort of know what the answer is. You don't get anything resembling something this simple or really, you don't get anything resembling any auctions which are actually deployed in the real world. So in general, if the bidders are not IID, the optimal auction is complex, and it depends in a detailed way on the distribution. So here, the dependence on the distribution was just to compute this one number.
00:16:44.252 - 00:17:01.624, Speaker A: Here, it's sort of a complicated mapping of the entire distributions. So this is the probability that the person is willing to pay at least p. Yeah. So that's the probability of sale at price p. That's the revenue of a sale at price p. So that's the expected revenue.
00:17:03.684 - 00:17:05.592, Speaker C: Okay. Yeah.
00:17:05.748 - 00:17:08.232, Speaker E: Are there examples of the general case.
00:17:08.408 - 00:17:13.124, Speaker D: In which the distributions come from a family but have different parameters in which the option simplifies?
00:17:14.464 - 00:17:22.800, Speaker A: Yes. Yeah. So, like, if they were, if they were, say, exponential distributions with different rates, you could say something sharper about what it looks like.
00:17:22.912 - 00:17:27.404, Speaker C: Yeah. Other questions?
00:17:29.144 - 00:18:08.756, Speaker A: Okay, so what did we just do? We just did average case analysis. So we're finding. So we have this problem, what single item auction maximizes revenue? Worst case analysis, total non starter. Average case analysis, totally solved. But we sort of, you know, we're tempted to reject the, at least in the asymmetric case, reject what optimal auction theory advocates on the basis of not resembling anything that anybody uses. So this is an example where the perils of average case analysis is, if you carry it through to its logical conclusions, you can wind up advocating things which you can't actually implement and where you also might be worried about robustness. And actually, I mean, this is not, you know, economists know this, too.
00:18:08.756 - 00:18:58.204, Speaker A: It's not like we're the only ones who know the perils of average case analysis. There's even something kind of cheekily referred to as Wilson's doctrine, after Bob Wilson, who's a famous auction theorist. And so he really was in basically a position paper talking about how auction design should aspire towards solutions toward auctions, which do not depend in a detailed way on knowledge assumptions like the details of these valuation distributions. So this is sort of an old critique in economics, but it turns out the computer science toolbox has a lot of tools to make sort of new, interesting progress in this direction. So alternatives, sort of more robust versions of average case analysis from which you actually get simpler and more usable solutions. So, any questions at that point? That's kind of the introduction context, yeah.
00:18:59.224 - 00:19:16.644, Speaker D: The hybrid model definition reminds me of the definition of minimax estimators. And then there's a duality that says minimax are bayesian with respect to particularly chosen least informative priors. And I'm wondering if there's a connection there.
00:19:18.304 - 00:19:27.074, Speaker A: Let's see. I mean, I see the general, I mean, as far as the order quantifiers, I definitely see the analogy with the minimax estimators.
00:19:27.374 - 00:19:37.326, Speaker D: I guess, phenomenologically, when you find out these hybrid model solutions, is it the case that they are also the Bayes optimal solutions for some particular distribution?
00:19:37.430 - 00:19:52.856, Speaker A: That's a good question. Generally not. So that's why the duality part. I wasn't so sure. It didn't sound like that matched up well. So if there's time at the end, we'll actually sort of insist that we only use solutions which at least would be optimal with respect to some distribution. So that'll be in the very final part of the talk, if there's time.
00:19:52.856 - 00:20:15.256, Speaker A: So I think the first part, there is a connection. Second part doesn't seem like it. Other questions? All right, so here's the plan. So we're going to keep talking about this. You know, how should, you know, what single item option should we use to maximize revenue? Worst case analysis fails. Average case analysis kind of succeeds theoretically, but advocates complicated solutions. So now we're going to look at outposts in between.
00:20:15.256 - 00:20:26.012, Speaker A: Okay, we're going to move from left to right. So we're going to start from average case analysis and successively weaken the knowledge assumptions. Okay, I'm going to just highlight a couple results at each of these outposts of what you can do.
00:20:26.108 - 00:20:26.864, Speaker C: Okay.
00:20:28.004 - 00:21:00.140, Speaker A: All right, so the first idea is, all right, let's just, we're still going to have distributional assumptions, but they're going to be more modest in some sense. Okay, so we're going to assume that we only have partial knowledge about what the input distribution actually is. And there's two notions of partial knowledge. I'm going to show you both I think will be natural for this audience. One is suppose I just told you some simple statistics of the distribution, like I told you, it's mean or it's median, something like that. What would you do? And you know nothing else. So otherwise it's sort of a worst case distribution, subject to conforming to the statistics that you know.
00:21:00.140 - 00:21:53.244, Speaker A: And we'll look at a sample model, okay, where your partial know where knowledge is parameterized by how many samples you have from that distribution, from an unknown distribution. Okay, so there's a couple. So the math remains the same, but there's sort of two different interpretations of the math. So the first one, you know, which is sort of the way I phrased it so far, is you just don't, you really just don't know, or you're not confident about the full distribution knowledge, where maybe you are pretty confident about the mean of a distribution. There's actually a second interpretation, which is even if you were fully confident in these input distributions, and so, in principle, you could use the theoretically optimal auction. Again, you might reject this auction, you know, is theoretically optimal because it cannot be deployed. So a second thing, the byproduct of this approach is to, in fact, you're insisting on simple auctions, which are robust with respect to many distributions and therefore not as complex as the optimal ones we talked about before.
00:21:53.244 - 00:22:31.938, Speaker A: Okay, so I started thinking about this with Jason Hartline in 2009 or so, and we were actually motivated initially by the second interpretation. We said, you know what? Maybe, you know the distributions, that's fine. But, you know, what do people use in the real world? People use reserve prices. So what if I go ahead and tell you the full distributional knowledge, but I force you to use a simple auction in the form of an auction that just uses reserve prices. What can you do? So once you're talking about reserve prices, you naturally gravitate toward monopoly prices. So another interpretation of these results is maybe the only thing you know about a distribution is the monopoly price. That's another interpretation of these theorems.
00:22:31.938 - 00:23:05.406, Speaker A: Now, remember, in the symmetric case, in the IID case, what was the optimal auction? It was just ebay. It was just second price auction with a suitable reserve. So if I tell you the monopoly price in a symmetric setting, that is already enough to know what the optimal auction is, you know the monopoly price, you know the optimal auction. Now, the non IED case, each bidder has their own reserve price. You might hope that if you knew everybody's monopoly reserve, that would be enough to run the optimal auction in the non IED case. But that is not true. Okay? Optimal auctions, again, need much more information than just the monopoly price.
00:23:05.406 - 00:24:03.488, Speaker A: So if we're thinking about asymmetric settings and we only know monopoly prices, we will lose something compared to the theoretically optimal auction, we will not do as well because we're using a more restricted auction class. So the goal then is to quantify how much do you lose? How far from this benchmark do you get by focusing on reserve price based auctions. Ideally, you'd like to be as close to optimal as possible. So again, I want to mention different distributions will have the same monopoly prices and different optimal options. So, in effect, you can think of this as a hybrid model, where the set of distributions is all of the distributions that have the specified monopoly prices, and worst case, over distributions that would lead to these monopoly prices. That's how you evaluate your auction. All right, so what can you prove? So suppose I said, wouldn't it be nice if you could just use a monopoly price for each bidder for their respective distributions? Wouldn't it be nice if that was optimal? And we said, no, that's not optimal.
00:24:03.488 - 00:24:45.930, Speaker A: Optimal auctions are more complex, but you can prove approximation guarantees for exactly that auction. So again, to be clear, what are we doing? We now have a different reserve price per bidder. I'll revisit this in a second if this bothers you. Let's see, you have one reserve price for each bidder, the monopoly prices of their respective distributions, and then among people who clear their reserve price, you're doing a second price auction. And again, the price that you pay is going to be the maximum of your reserve price or the highest other bid. And so that's going to get a constant fraction of the optimal auction, even for worst case distributions that conform to those monopoly prices. And in fact, you really have to lose the 50% in the worst case.
00:24:45.930 - 00:25:21.872, Speaker A: So whatever reserve price auction you decide to use, I can show you distributions where the optimal auction for those distributions is twice as good as yours. So you can make stronger assumptions if you want to make this 50% a bigger number, but without further assumptions. This is the right answer. 50% is the right answer. So, I mean, this is not a kind of talk. I'm going to have time to really discuss proofs at all. But so let me just say just sort of one thing, which is in this theory, we sort of rejected optimal option theory, these theoretically complex options, as far as something you might actually design.
00:25:21.872 - 00:25:36.924, Speaker A: But I want to emphasize the traditional economic theory is still crucial in actually doing the analysis of these simpler auctions. So as far as what does the proof looks like? We use exactly the same kind of analytical tools which were developed by economists to solve the optimal auction theory problem.
00:25:38.464 - 00:25:46.004, Speaker D: Yes, I was going to ask, just so I understand the model these individuals are choosing from their distributions independently.
00:25:47.624 - 00:25:50.488, Speaker A: I mean, one thinks of it is they're given their valuation from the sky.
00:25:50.576 - 00:26:01.024, Speaker D: Independently, and they're not reacting, because on eBay, I guess I could react and say, oh, okay, other people are bidding for it and I'm going to even increase my bid now because I. Yeah, so I'm.
00:26:01.144 - 00:26:06.720, Speaker B: You said the one close envelope bid, they call it everybody bids and that's it, right?
00:26:06.872 - 00:26:19.734, Speaker A: Yeah. So it's one shot. Yeah, yeah. And in fact, you know, the auctions I'm talking about, there's no need to strategize. So on eBay, modular timing effects, there's no need to strategize. So if you're being charged the second highest bid, it doesn't help you to shade your bid.
00:26:21.794 - 00:26:25.254, Speaker B: That's essentially the generalization of profit inequality.
00:26:26.834 - 00:26:44.166, Speaker A: You can derive it from a profit inequality. I don't know if I'd call it a generalization, but it's more like a consequence of the profit inequality combined with techniques from optimal auction theory. Yeah, yeah, that's a good point. So if the bids were say like.
00:26:44.190 - 00:27:06.654, Speaker E: Integers in some range, like one to b or something like that, if you're willing to get to do worse to get like a one over log B factor, then you could just pick like a random price according to powers of two or something, and then that would kind of work in general, like no matter what the heck's going on or something like that. Is that right?
00:27:06.994 - 00:27:07.306, Speaker C: Yeah.
00:27:07.330 - 00:27:18.218, Speaker A: So if there's not much of a, like, if you know upfront that the valuations are pretty tightly concentrated, then, I mean, the auction design problem isn't that hard, I guess, is a way. Is that a fair summary or.
00:27:18.386 - 00:27:22.930, Speaker E: This is the worst thing. It's like one over log of the range, which is a lot worse than 50%.
00:27:23.002 - 00:27:25.050, Speaker C: Yeah, right.
00:27:25.202 - 00:27:30.622, Speaker E: Because you kind of have a one over log chance of getting within a constant factor of whatever, right.
00:27:30.678 - 00:27:44.614, Speaker A: I guess at a higher level, I think if you have a pretty good sense of where people's valuations lie, it's not actually clear you'd run an auction. You could just say buy it now at sort of the midpoint of that interval or something like that range is. Yeah, reasonably high.
00:27:44.694 - 00:27:44.910, Speaker C: Right.
00:27:44.942 - 00:28:02.954, Speaker A: So, like, you know, so. And you know, on eBay, you know, still to this day, you know, kind of what it's really used most heavily for is kind of like collectible markets. So, like, I'm a record collector, so sometimes I go on there and buy records and stuff. And there you really do see big spreads in what it goes for. Like, people sell one for $100 and the next week one goes for $20, kind of thing.
00:28:03.494 - 00:28:05.478, Speaker B: Where does the computer science get here?
00:28:05.646 - 00:28:20.630, Speaker A: So, good question. So where's the computer science? So what's what? So the only, you know, throughout everything I'll say today, it's really the sort of culture and the toolbox of computer science which is being applied. So it's not about computation. I don't think I'll say anything today which has anything to do with computation.
00:28:20.702 - 00:28:22.382, Speaker B: 50% of optimal. That's kind of.
00:28:22.438 - 00:28:45.474, Speaker A: Exactly. Exactly. And so here's. It's true, or just a constant fraction. So just some relaxation. So being okay with being, you know, less than 100% of optimal, especially when optimal is something which isn't an option on the table anyways, as is the case here. So for whatever reason, I mean, this tradition of approximation is close to unique for theoretical computer science, and I'm not sure why.
00:28:45.474 - 00:29:01.510, Speaker A: But in any case, so in economics, it has not been part of the mainstream at all. Instead, what you do is you look at kind of, you know, the dominant paradigm is more look at sufficiently strong assumptions that you can get exactly what you want, as opposed to keep weak assumptions and get a sort of relaxed conclusion. So that's right. So somehow, because they have a big.
00:29:01.582 - 00:29:08.262, Speaker B: History now of satisfying, this is not a new concept in something of not trying to shoot for the optimum.
00:29:08.358 - 00:29:17.654, Speaker A: Yeah. So, right. So from the individual's perspective, it's a good point. Yeah. They're kind of used to that relaxation. It's from somehow, like the design perspective. You know, you want to.
00:29:17.654 - 00:29:42.870, Speaker A: And it's not always, it's not always an optimization problem. Sometimes you specify properties and you want to design which meets all these properties, but somehow, you know, reasonable approximation as some kind of property has barely been studied a little bit, but it's really not part of the mainstream. So that's. I mean, and that fact has allowed computer science to make contributions here. So to make progress on this Wilson doctrine, which wasn't being done previously. Yeah, that's a good question. And that'll really be true for everything else I say.
00:29:42.870 - 00:29:52.250, Speaker A: Everything else I say will be borrowing ways of thinking from computer science. But at no point will I mention the word polynomial time or np hard or anything like that.
00:29:52.422 - 00:29:56.066, Speaker D: How much you lose over if you don't have better specific reserves.
00:29:56.090 - 00:29:57.786, Speaker A: Well, that's on the next slide. Yeah. Yeah.
00:29:57.930 - 00:30:02.442, Speaker B: Just to understand. So the mechanism here is exactly the.
00:30:02.458 - 00:30:26.038, Speaker A: Same as before, except everybody gets their own reserve price. So this is Dan's question, which is. Good question. So, and I understand if you're uncomfortable with this. So you look at this auction where, because, remember, different people have different distributions, so different people have different monopoly prices. So when I say use monopoly reserve prices, I mean use a different one for each bidder. EBay, for example, does not allow you to do that.
00:30:26.126 - 00:30:26.382, Speaker C: Okay?
00:30:26.398 - 00:30:55.262, Speaker A: You get one reserve price for everybody. Now, you do see bidder specific reserves in practice, although they're usually sort of disguised because to sort of hide the discriminatory aspect. But like in sponsored shirts, you can use so called quality scores to implement. I mean, they do use quality scores to implement bidder specific reserves. So it's not an impractical concept. But you might ask, you know, what if I force you to use eBay and I force you to prove some reserve, to use some reserve price, and there, the constant gets worse, unfortunately. But still, you can do something.
00:30:55.262 - 00:31:25.076, Speaker A: So there is an anonymous reserve price, which, no matter what the distributions, are non identical, you get a constant fraction. I mean, again, so, like, with all approximation results, you need to be clear eyed about what it is you're comparing to. And when you're comparing to something which is, like, hypothetical and not an option, you shouldn't be sad, right? So, like, if you told me my baseball team only won 65% of the games out of the maximum possible, I would be very happy.
00:31:25.180 - 00:31:25.944, Speaker C: Okay.
00:31:27.564 - 00:31:58.652, Speaker A: All right, so there's no reason, I mean, you can do this exercise with other statistics, not just monopoly prices. So Azar and Macaulay, and then later with Daskalakis and Weinberg, they had some nice results of this form. This is just for single item auctions. There's actually a rich literature which looks at problems more complex than single item auctions and does the same kind of theory. So the other model of partial knowledge I want to mention is samples. So here it's just very directly inspired by PAc style models in learning theory. So here the idea is you don't really.
00:31:58.652 - 00:32:37.868, Speaker A: So what do you know about the distribution? You know a bunch of IED samples from the distribution. Okay, you have to decide on what auction to run using only those samples and nothing else. So you do not directly know the distribution except through samples about it. And so then you can ask sample complexity type questions, like how many samples, that is, how much data is necessary and sufficient, before you can get a reasonable simulation of what you could accomplish with full distributional knowledge. So you'd like, ideally, your expected revenue to be at least one minus epsilon times what you could achieve with the optimal option with full knowledge. Okay, so that's sort of the strong benchmark to use, and you might want to think about Epsilon as, you know, 0.1 or 0.01.
00:32:37.868 - 00:33:02.548, Speaker A: So, unlike the 50% we just saw here. We really want to get quite close. And the question is, how much data do we need to actually do that? And I should say, I mean, you know, this paradigm is actually used in practice. The idea where you look at past bids, you think of them as samples from some distribution, reverse engineer the distribution, think about what optimal auction theory would tell you for those fitted distributions, and then use that to inform, for example, how you set reserve prices on sponsored search engines.
00:33:02.596 - 00:33:02.772, Speaker C: Okay?
00:33:02.788 - 00:33:06.864, Speaker A: So it actually maps pretty closely to how the search engines actually do this.
00:33:08.244 - 00:33:09.104, Speaker C: Okay.
00:33:10.904 - 00:33:42.498, Speaker A: So this is just saying what I said again in cartoons. So let me just mention some results instead. So again, the question we're asking, there's an unknown distribution. You're getting s samples. Based on the s samples, you pick an auction. You were hoping to pick an auction whose expected revenue is almost as good as the optimal one for that distribution. So the question is, as a function of all of the relevant parameters, how many samples are necessary and sufficient to achieve this goal? To get a one minus epsilon approximation, we expect it to be blowing up with one over epsilon, the better you want to approximate optimal, the more samples you presumably need.
00:33:42.498 - 00:34:11.450, Speaker A: It may or may not be blowing up in the number of bidders n, the other parameter we have. So in the symmetric case, it turns out the dependence is there's no dependence on n. There's a dependence only on one over epsilon. And you can actually pin down exactly what the exponent of one over epsilon is for different classes of distributions. This shouldn't surprise you. If you remember way back at the beginning, we said the optimal auction in the symmetric case actually is just a second price auction with a monopoly reserve. And the monopoly reserve is independent of the number n of bidders.
00:34:11.450 - 00:34:35.868, Speaker A: So that's in effect. Why? Morally, you're hoping that you're not going to have dependence on n in the id case. In the general case, you do have dependence both on n and on one over epsilon. It is polynomial on both. However, and one thing I just want to mention about the lower bound. So this is both a lower and an upper bound. The lower bound says, really, if you don't use n samples where n is the number of bidders, then you can't be near optimal.
00:34:35.868 - 00:35:17.914, Speaker A: What that proof actually shows, it kind of shows the limitations of how far the Wilson doctrine could go, right? So we talked about optimal auctions. We said they can be really complex. We said, well, you know, if you're willing to lose, if you're willing to have a coarse approximation, if you're willing to lose a factor two, then you actually don't need distributional knowledge. You just need to know the reserve prices. But what this proof really implies is that if you want a one minus epsilon approximation for sufficiently small epsilon, you have no choice but to use very detailed distributional knowledge. So really? So this is proving that using significantly simpler options than the theoretically optimal ones requires a significant degradation in the approximation factor. There's really nothing you can do about it.
00:35:17.914 - 00:35:49.644, Speaker A: So, questions about that section. Let me talk a little bit about unknown distributions. And so, to show you what I mean, let me show you a sort of famous result from auction theory known as the Buello Klemperer theorem. And so this is proven by a couple economists, but I actually think it has a very computer science y flavor. Okay, so here's what you do. So you think about a single atom oxygen, like we've been thinking about. Think about the symmetric case bitters iid from this distribution.
00:35:49.644 - 00:36:11.524, Speaker A: So the Bielo Klemper and n is the number of bidders. So, Buello Klemper, they compare two things. They compare the revenue of the optimal auction for this distribution, f. So second price auction with a monopoly reserve versus the revenue expected revenue of the second price auction. So Vickrey auction just means second price auction with no reserve. So the reserve price here is zero. The reserve price here is going to be the monopoly price.
00:36:11.524 - 00:36:56.360, Speaker A: So Buelleclamper states an inequality between these two quantities, which seems kind of silly, right? So this is an auction. That's an optimal auction. So I guess there should be inequality going this direction, right? Less, more. The Beel Klemper actually flipped the inequality. Okay, what's the catch? They give the second price auction with no reserve a slight advantage in the form of one additional bidder, again with valuation drawn iid from the same distribution, f. Okay, so if you have to make a decision. So if you have to choose between spending resources to recruit one new bidder to your auction, or learning everything about the preferences of your bidders, the first option is always better.
00:36:56.360 - 00:37:23.460, Speaker A: So that's usual interpretation. Increasing the competition at an auction is really the best way to improve your revenue, which I think is actually pretty good advice in a lot of cases. So that's the bulocklemper theorem. So now, on a technical level, here's what I want to call your attention to. On the left hand side, we have a second price auction with no reserve price. So the left hand side is independent of the distribution of the valuation, distribution, f. Whatever f is.
00:37:23.460 - 00:38:08.764, Speaker A: This is just second price auction with no reserve. But as I vary f this right hand side, I'm using different auctions. So I have one auction on the left satisfying this guarantee for every possible option, on the right, for every choice of the distribution f. So then in this sense, we have one auction which is simultaneously optimal or near optimal with respect to a wide range of distributions, which is really kind of the canonical version of the hybrid model like I was talking about. So there's this notion of prior independent auctions, which is basically just trying to get guarantees of this form. Okay, so you really want one auction who, no matter what the distribution is, the expected revenue of your auction is almost as good as the optimal for those distributions. So one auction simultaneously near optimal for lots of distributions.
00:38:10.704 - 00:38:17.056, Speaker D: Is the idea of that result that additional persons somewhat sets the reserve and exactly at the monopoly price?
00:38:17.120 - 00:38:28.244, Speaker A: Yeah, so not exactly the monopoly price, but basically what you get out of this is an extra person setting. A random reserve is within a factor two of the optimal reserve, which is kind of interesting.
00:38:29.624 - 00:38:37.130, Speaker B: You drop the n plus one to two to nice. Clearly, then you don't have the inequality. But how close do you get to the optimum then?
00:38:37.162 - 00:38:41.054, Speaker A: Good question. So it implies an n minus one over n approximation.
00:38:41.674 - 00:38:42.106, Speaker C: Yeah.
00:38:42.170 - 00:38:45.654, Speaker A: So even with the same number of bidders, you can prove something quite strong about how close they are.
00:38:46.714 - 00:38:49.894, Speaker C: Good comment. Okay.
00:38:52.554 - 00:39:28.408, Speaker A: So that's a prior independent auction. Okay, so you want a close approximation no matter what the distribution is. And I think in the interest of time, I will skip. So the point is, there is an auction fits on three lines, and you can do this, including for very general problems, actually. So you can get, again, you're losing a kind of factor two, which is related to the factor two that we were talking about before as well. But you don't really lose more than that from an unknown distribution. So again, for comparison, remember what I told you? I said, suppose I told you the distributions and I forced you to use reserve prices, you're already off by a factor two.
00:39:28.408 - 00:39:55.694, Speaker A: Now I'm saying you don't even get to see the distributions at all. And so you lose the factor two as before, but then only sort of a factor which is close to one. The idea of the mechanism is very simple, and it's actually completely related to what Dan just said. You basically take one bidder and they're a sacrificial lamb. You pick a bidder at random, there's no chance they're going to win. You use their bid to reserve price everybody else. So that's what makes this work.
00:39:55.694 - 00:40:03.934, Speaker A: So again, what's nice is you get this robust guarantee, but you can achieve it already with a pretty reasonable solution. Three line solution.
00:40:05.794 - 00:40:06.774, Speaker C: Questions.
00:40:10.674 - 00:40:42.306, Speaker A: So just briefly on this last topic. So again, we've been moving from the average case world to the worst case world, but so far we've always had distributions. Ultimately, we've been evaluating our auctions with respect to some distribution. The question was just how much do we know about the distribution? Now we're actually going to recover a form of worst case analysis. Okay, so here we'll use distributions kind of only in our minds as a thought experiment, and we'll actually get worst case guarantees. So these are called prior free auctions. So that last part was called prior independent, where you assume a distribution.
00:40:42.306 - 00:41:23.964, Speaker A: Here we're not going to assume a distribution on inputs. We really want an input by input guarantee. So we want to say no matter what bidders want, we want our auction's revenue to be within a constant factor of the maximum possible, hopefully for a small constant alpha. But if you remember, you should be thinking, isn't this exactly like the non starter example that we had at the beginning of the talk to say why we need alternatives? And it is. You'd be right. So depending on how you define the benchmark, how you define Optav, this may or may not be possible to prove theorems of this form. And if you define this just to be the highest, the highest willingness to pay of any bidder, as we saw, there's no way to reason about the problem, there's no interesting guarantees.
00:41:23.964 - 00:41:59.044, Speaker A: So. And, you know, this is actually one of the earliest contributions by computer science to revenue maximizing auctions, was the idea of, okay, well, I guess what we need to do is be smarter about what we're comparing to. So we need a smarter revenue benchmark. And so this, for those of you that know about online learning, that's sort of a good analogy to have in mind, right? In online learning, the first thing you think you want to do is have an online algorithm that competes with the best action sequence. In hindsight, there's a trivial example that shows that's completely impossible. So you instead say, okay, let's change the benchmark. Let's say that an online algorithm is good if it merely competes with the best fixed action in hindsight.
00:41:59.044 - 00:42:15.340, Speaker A: So that's how changing the benchmark can change an algorithm design problem from uninteresting to interesting, just by getting the benchmarks sort of the right scale. So Goldberg, Heartland and Wright really thought hard about this problem, and they proposed revenue benchmarks and proved some really nice results of this form.
00:42:15.452 - 00:42:15.780, Speaker C: Okay?
00:42:15.812 - 00:43:08.630, Speaker A: So they really showed that prior free auctions can exist for the appropriate benchmark. And for many years it was kind of a dark art of like where these benchmarks came from. Like it wasn't really, you know, you kind of just try to find one which worked, worked in the sense you could prove positive results and also prove kind of nearly matching negative results and that you got reasonable constants. And so, you know, together with Jason Hartline, we tried to think about, you know, is there a systematic way of generating meaningful revenue benchmarks? Like where do these come from, these revenue benchmarks? And there's a very tight connection between the prior independent auctions that I was just mentioning in the last segment, okay? This idea of being simultaneously near optimal for a range of distributions. So here's what you do. We're going to merely as a thought experiment, pass to the average case world, okay? We're not going to actually assume the input comes from a distribution. We will not assume that, but just temporarily.
00:43:08.630 - 00:43:36.978, Speaker A: Let's think in our minds, what auction would we run if input was drawn from a distribution, let's say in the IID case. Well, we've gone over this. We just run a second price auction with a suitable reserve, and the reserve depends on what the distribution is. So that's the result of the thought experiment. If we were Bayesian IID, we'd do a second price auction with a suitable reserve. So here's the template for defining the benchmark. So now fix an arbitrary valuation profile.
00:43:36.978 - 00:44:13.704, Speaker A: Who's willing to pay out. Now restrict yourself to auctions that you would use if there was a distribution. So this is what came up earlier. Now in this case it's just second price auctions with all possible reserves. So the definition of the benchmark on a given valuation profile is how much money could any second price auction with a reserve price extract in this particular input? And that's the definition of the benchmark. So you think about the distributional thought experiment, think of everything that might arise, and then on a given input you take the best of any of those solutions to the distributional problem. And this is what you use as your benchmark.
00:44:13.704 - 00:44:58.748, Speaker A: And this can be thought of as sort of, you know, you want to have, you want to talk about an IED setting, but without a distribution. And this benchmark gives you a deterministic proxy for doing exactly that. And again, you can think about no regret benchmarks as being similar or, you know, in data structures. I think of static optimality as kind of a deterministic stand in for saying, imagine queries were happening in an IID fashion, because then the optimal data structure would, in fact be static. All right, so what should I say about this? So this template regenerates the kind of revenue benchmarks that were proposed earlier, and you can also use it to tackle more general problems. I said I wanted the benchmarks to not only be sort of generated automatically, but also to automatically be meaningful.
00:44:58.886 - 00:44:59.424, Speaker C: Okay.
00:44:59.504 - 00:45:38.476, Speaker A: And this is always the problem with a benchmark. You're like, okay, your algorithm gets close to this benchmark. Is that good? Is that bad? Does it say your algorithm is good? Or does it say the benchmark's bad? Like, how should I think about it? And so here's the sense in which benchmarks that you generate in this way are automatically meaningful if you compete, that is, you approximate input by input, this benchmark. Then as an automatic consequence, you get the simultaneous distributional guarantee that we were talking about in the previous segment. In other words, a prior free auction is always a prior independent auction. It is only harder to design prior free auctions than prior independent auctions. And this is pretty much immediate from the definitions.
00:45:38.476 - 00:46:21.808, Speaker A: So basically, if there was a distribution, you'd be using some optimal auction on every input, and input by input, you found an auction which competes with every single possible optimal auction. And so averaging, you get the same guarantee. So that's the high level takeaway. Why are the benchmarks meaningful? Well, at the very least, it implies the prior independence guarantee, but it's even stronger than that. All right, so just a couple concluding remarks. So I used sort of this auction design problem as sort of a excuse to sort of scaffolding to show you all these different interpolations of average and worst case analysis. But I again want to emphasize, I think all of the ideas we talked about are really not about auction design per se, or about, like, algorithmic game theory per se.
00:46:21.808 - 00:47:09.832, Speaker A: All of these ideas of, you know, like different ways of comparing the performance of different solutions. This is broadly relevant for algorithms. So the reason I talk mostly about auctions is just because there's been a larger concentration of results of this type in algorithmic game theory. There's plenty of analogs and mainstream algorithms, but just the density is the intensity of them is bigger on the algorithmic game theory side. And you may wonder, why would that be? Why these kinds of results specifically for auction and game theory problems? And if I had to speculate, I'd say for the first ten years of algorithmic game theory, it really felt like the Wild west. So for those of us that were working in the field, we knew we didn't know what the right models were. And so we had this kind of communal experimental phase where we were trying out lots of different models to see which ones led to the most illuminating results.
00:47:09.832 - 00:47:32.094, Speaker A: So everything was under question, including worst case versus average case. We were sort of inspired by the economists, how much they had done with average case analysis. But again, so I think that's why it's happened. It's been so fruitful in the algorithmic game theory side. But I really hope that in the years to come, a lot of these ideas will find even deeper roots in more mainstream algorithms. So, thanks very much.
00:47:39.914 - 00:47:48.394, Speaker B: So, you know, one of the things that many of us have been thinking, I mean, you can go back even to simplex. Right, which has been enormously successful.
