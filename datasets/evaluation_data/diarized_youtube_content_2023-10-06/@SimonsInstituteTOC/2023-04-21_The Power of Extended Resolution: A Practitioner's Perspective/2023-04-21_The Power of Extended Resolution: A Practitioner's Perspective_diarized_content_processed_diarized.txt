00:00:01.360 - 00:00:07.550, Speaker A: Hey, good morning, everyone. Let's get started with day four.
00:00:07.702 - 00:00:13.022, Speaker B: And today we are very pleased to have Randy Bryant talking about the power of external resolution.
00:00:13.118 - 00:00:44.312, Speaker A: Randy? Thank you. So it's been an interesting series of talks. I've learned a lot, and. But I've come at this really as an engineer who wants to build tools. And the one tool I've discovered recently and I'll talk about is extended resolution. I'll describe more about what I mean by that. But my career, a large part of my career has been spent doing what I call formal verification.
00:00:44.312 - 00:01:40.104, Speaker A: And this is just a subset of the titles of some of my papers. And the word formal verification shows up a lot. But the honest truth is, and so you can think of the general principle of what this is, is you have some mathematical description of a system and you make some statements about its behavior, and you want to be able to formally verify. That's correct. But the truth is that the tools that my students and I have developed over the years always have this weakness that what if there's a bug in the tool? And in fact, you know, there's a bug in the tool because it's a big program and it's messy. And so you say, I formally verified X, Y or Z, but there's sort of that risk of somebody saying, well, you know, I just found a bug in your tool. Oh, shoot, I'll fix it and rerun everything I did and make sure it's not broken again.
00:01:40.104 - 00:02:35.506, Speaker A: So that's not a totally satisfactory feeling. And so, of course, there's been some really interesting work on trying to formally verify tools, build tools that were built on some foundation of a theorem prover or something like that, that you could argue was correct the entire tool. But the truth is, that's really hard to do, and it's hard to get performance out of tools of that category. So an alternative, which I find extremely attractive, is to have a checker that says, I don't verify the entire program, I just verify individual executions of the program. And the cool thing about that is once you have that proof that you've generated and it's checkable, then even if you find a bug in your original solver, you don't have to rerun it because you have a proof that the instance you ran before was ok. And that's a very powerful statement. And of course, there's always the thing.
00:02:35.506 - 00:03:50.598, Speaker A: Aren't you just kicking the can down the road? What if there's a bug in the checker? Well, there's two answers. That one is that the checker can be a much simpler program and based on a much simpler set of rules. And the second is, it's actually possible to have a formally verified checker because it's a small enough, concise enough program with very clear specification, that formally verify it's within the realm of possibility. So, of course, this is a well established idea in SAT solving that, of course, we can always check if it's a solution, if it's satisfiable, and nowadays, if it's unsatisfiable, we require the SAT solver to produce a certificate of unsatisfiability. And this has become sort of very well accepted. My understanding of the history is, I remember when Sharad first put it into z chaff, and the reason he did, I remember him telling me, was they'd found some lurking bug in the SAT solver they'd been running and many people had been using for several years. And it really shook him up that his tool was basically giving the wrong answer, but nobody had uncovered it.
00:03:50.598 - 00:04:51.514, Speaker A: And so he put in proof generation, and about the same time, others put it in other solvers. But that was really hard work to do because they didn't have very good proof frameworks for. And so Marine and his colleagues developed the drat framework and efficient checkers for that, that have now made it much easier to take a CDCL solver, and just with minor changes to it, have it generate proof clauses. And since 2016, that's been a mandatory for all the entrants in the main track of the SAT solving competition. The impact of it is, it's really helped the solvers a lot, and even the solver developers. So now these modern set solvers are extremely reliable, and when they enter these competitions and they're given totally new benchmarks, it's very rare for them to actually fail. They'll produce the proofs and they're quite reliable.
00:04:51.514 - 00:06:23.516, Speaker A: And that's partly that the tool developers have found this proof checking extremely useful, put some new features in their thing, they try and generate a proof from that modified feature and they find they can't do it, or they come up with a proof failure of some sort, and it gives them a very clear identification of what's going wrong. So that's a sort of theme of it is, can we add proof checking to other types of tools? And one, of course, is other versions of Sat solvers, but also beyond Sat solvers. Where can this fit in as well? And so I'll talk some about extended resolution and why that's significant here. And of course, given who I am, it's got to involve bdds at some level. And that's sort of where I got excited about this, was realizing that bdds are actually very powerful engines for generating proofs in extended resolution. And more recent work that we haven't published yet on what's called knowledge compilation, which is a very interesting topic. So we've seen resolution over and over again, but the idea of it is, I think it's interesting to always remember that it's really just encoding of the idea of logical implication that you're combining two causes because one implies the other and getting a implication out of it.
00:06:23.516 - 00:07:29.472, Speaker A: And so a proof of unsatisfiability and a causal proof is then just a sequence of clauses that you append to the input formula, where each one has to preserve the satisfiability of the formula procedure. In other words, you can't add a new cause that turns an unsatisfiable formula unsatisfiable, and then the final cause is the empty clause, indicating that, well, the original formula wasn't satisfiable in the first place. And of course, resolution is a, gives you a stronger guarantee, it's implication redundancy. It says that actually I'm not even, my new clauses aren't even decreasing the set of satisfying assignments to the formula. So when you get to the end, you've made a clear statement that there were no satisfying assignments to the formula. So of course the way a causal proof can look like then, is that you give the clauses of the input formula and then just start listing proof clauses until you derive the empty clause. And each in this sort of pure resolution style.
00:07:29.472 - 00:08:08.854, Speaker A: Each clause has two antecedents, the two clauses that were combined by resolution to derive this clause. And we keep doing this systematically till we get an empty clause. So Gregory Satan introduced this idea of extended resolution in 1967. And I liked all those pictures and biographies we were getting yesterday. So I did some research on Satan. Actually, I did it in 2020 and discovered that he was in Leningrad at what's now the St. Petersburg University of Technology or something like that.
00:08:08.854 - 00:09:33.726, Speaker A: But he emigrated to the US in 1999 and worked for IBM. But unfortunately he died last year. So did you ever meet him? Oh yeah, I met him in the seventies. Yeah, because he was hanging out at Stanford too, right? So anyways, but he actually worked mostly in programming languages. But his contribution that we know most for is this really simple idea that I can just introduce a new variable called e, an extension variable, into my proof, and then list a set of clauses that encode the formula e if and only if f, where f is an arbitrary boolean formula over the preceding, the input variables and the preceding extension variables, his version of it, actually only you could only build man gates essentially, but this becomes now a shorthand that you can use to sort of abstract what otherwise might be a whole set of clauses. And through repeated applications this can make an exponential difference. So in particular, the idea would be, imagine we wanted to prove that this formula is unsatisfiable, and our strategy was we want to encode u and v as a sort of concrete object.
00:09:33.726 - 00:10:43.512, Speaker A: So we introduce an extension variable that encodes that. And now our proof becomes a listing of input clauses, definition clauses, and proof causes. But using the same pattern as before, and without going into the details of this, what you'll see is that we're using that extension variable to rewrite all instances of U and V into e, and then proving with respect to e. We've seen the variations on this picture already several times. That resolution then, is the sort of underlying logic of CDCl, and it's better than DPL, which is just tree resolution. But there's this whole bunch of other proof systems that are either incomparable or stronger than resolution until you get to the top is extended Frege. But that's equivalent to extended resolution, because as soon as you have the power to build basically logic gates, you can build circuits.
00:10:43.512 - 00:11:47.544, Speaker A: And out of circuits you can build things that computer, or serve as various lemmas and other abstractions that you can make use of. And so that takes you all the way top of the propositional proof hierarchy in terms of what you can do. So as an engineer, I see that as an opportunity. It says that, well, our current solvers are kind of down in the basement here, and yet we have this little thing that's built in and it's supported by the existing proof checkers. So why don't we make better use of it? And that's sort of the idea of the talk. So, as I said, the fact that CDCL is sort of is down there with resolution. What I mean by that is, they will occasionally introduce, modern solvers will introduce some extension variables during the pre processing or in processing step known as bounded variables, variable addition, where they rewrite some of the causes by inserting intermediate variables into it.
00:11:47.544 - 00:12:57.044, Speaker A: And that's a useful thing, but they don't do it in at the level that you need to, to take an exponential thing and, and get it polynomial. It's only a very minor tweak in the overall thing. And there's been a few people have tried to just, well, let's kind of look for interesting stuff and throw in extension variables as we're chugging along in our CDCL solver, but those have really not been successful. So there's this technique that's out there that everyone has understood, but nobody's really had, or a few people have had very good ideas on how to use it, because in some ways, it's such a simple idea and such a powerful idea, there's no obvious way to turn that into essentially working solver. So just as an example, this is, I find, a really interesting benchmark. It just basically says a set of variables has both even and odd parity at the same time. We'll make it a little bit less trivial by using a random permutation on one of those, so that it's not a perfect matching between the two.
00:12:57.044 - 00:14:41.760, Speaker A: But anyways, it's not hard to see this is not satisfiable, but if I feed this to a very good set solver, Keysot, sort of the best there is in CDCL, you'll see that it really struggles. I show my complexity here in terms of proof clauses, but the same picture shows if I show time, that, in fact, I limited the time here to 600 seconds, and I couldn't get beyond n equals 42. And of course, I tried a few random permutations to make it not to give some sense of sampling, but it's really not there. And so one way to think about this is if you imagine in a sort of not clear what your metrics are, but if you think about some problems are easy in the mathematical sense, and some are hard, and CDCL solvers can nail some problems, but really struggle on others in this kind of vague, fuzzy sense of things. What's been interesting about modern Sat solvers is that if you're right, at the limits of their performance, you can solve some interesting problems, some problems that are interesting to some people, mainly practitioners, who use these tools to verify circuits and other things, and some amount of mathematical proofs that people haven't been able to do otherwise. Parity equations, on the other hand, are down in this end, right? These are ones that are super hard for CDCL solvers, but conceptually, from a mathematical point of view, it's as trivial as you could ask. Similarly, pigeonhole mutilated chessboard.
00:14:41.760 - 00:15:42.914, Speaker A: I mean, these are problems you could describe to a child, and the child would go, well, you can't solve that problem, but yet the CDCL solvers solve it. So it's really, in fact, they're so hard that they hardly even have benchmarks for these in the SAT solver competitions anymore, because they serve no purpose in distinguishing solver X from solver Y, since neither of them can solve these problems. So anyways, we've sort of glossed over the fact that CDCL has these glaring weaknesses. And I thought it was interesting the way Karam described that pigeonhole really shows up in real life, like when you're trying to route wires through a channel, that's exactly where you encounter. So these aren't just artificial problems that were made up to prove theorems about. So bring in bdds. The idea of using bdds as a proof generating solver actually was started with work by Carsten zins and armin beer and Tony Usula back in 2006.
00:15:42.914 - 00:16:43.214, Speaker A: And the idea of it is really powerful. And I kind of picked up on this early in the pandemic and started working on this topic and got pretty excited by it. And that's the area I've carried along since then working with mine. But the idea of it is very simple, that we'll introduce a BDD extension variable for every node in a BDD and encode as the defining causes what that node means, and then generate proof steps. But the proof steps really encode the underlying BDD algorithm, the recursion that goes on. And so the idea of this then, once you get that, then the rest is just working out the details that you say. The sort of standard BdD operation is to take two bdds, u and v, and some boolean connective say or, and generate a BDD representing that new function.
00:16:43.214 - 00:18:17.944, Speaker A: And the idea of it is this recursion that starts at every given a pair of nodes, u and v will then recursively descend and generate subgraphs for the two case where x is false and the case where x is true, and then join those together potentially with a new node labeled by x. And of course there's a lot of special cases, but that's the main one. So the idea with this is we'll introduce here an extension variable and then four clauses that basically say u is equivalent to the if then else of x u one and u zero, where u one and u zero know the extension variables for the children. So that's four clauses. And then each proof step, in addition to generating a new node, each recursive call, in addition to generating a new node, will also generate a sequence of proof steps leading to the implication that u and v implies w and you can see that if you imagine so, that's our goal. But you can see, if we recursively assume that the two recursive calls generated proofs of their outcomes, then we can just do a resolution of the those two recursive results with the defining causes for the relevant nodes and get the proof for our node here. And we just keep doing that.
00:18:17.944 - 00:19:04.304, Speaker A: That information just gets carried along as the bdds tug away. We're just spitting out proof steps that are basically at a very, very low level, describing or justifying the underlying logic of the BDD algorithm. We do this, and all of a sudden now this benchmark looks a lot simpler. We can push out to close to 10,000 with reasonable proof sizes using a very automated and simple technique known as bucket elimination, which is just a way of systematically forming conjunctions of bdds and then quantifying out variables. Yeah.
00:19:07.684 - 00:19:13.988, Speaker B: So these extension variables are basically the outputs of the multiplexers that represent that BDD node, right?
00:19:14.036 - 00:19:14.788, Speaker A: Yep, exactly.
00:19:14.836 - 00:19:19.756, Speaker B: So it's like we're just naming the signals in a circuit representation of BDD, is that it?
00:19:19.820 - 00:20:09.182, Speaker A: That's exactly it. You're building circuits and then you're proving properties of those circuits. And that's a good way to think about it, but it's interesting to compare. Now, if we compare a resolution proof from CDCL versus a proof generated with BDD's, they're really different in a qualitative sense that the CDCL proof, what it's generating is conflict clauses saying this part of the search space is not reachable. So just block that off. And over time you're forming this conjunction of those until you come up with the empty set. A BDD proof is more, I've done this recursive call and it's consistent with what I told you it would do at a low level.
00:20:09.182 - 00:20:09.834, Speaker A: Done.
00:20:13.334 - 00:20:55.224, Speaker C: I want to slow you down too much, but I want to make sure I get this. So you've got these clauses and you start out on one hand, you form a BDD that for x plus x x one plus x two. And you also, and then you add that into, then you got also x one, um, X PI one, uh, Xor X PI two. And, and you're, and you're sort of building these two bdds.
00:20:55.764 - 00:20:56.148, Speaker A: Yeah.
00:20:56.196 - 00:21:02.756, Speaker C: And at the end, but not, but at what point do, do you assert the parity of it?
00:21:02.900 - 00:22:46.758, Speaker A: Oh, okay. So the CNF I'm working with has already encoded these xors using Satan variables, right. It's already, it's so it's just, I'm going right from CNF here, okay? And so encoded, these parity formulas are encoded using the sort of standard encodings of xors in there. And so now, but the question you could have asked but you didn't is well, how do you go from CNF to bdds? Okay? And the answer is, for each cause you can, you can actually generate a BDD representation of that cause very easily, as it's very simple linear structure. And you can generate a proof that the root of that is implied by that particular input clause by just one sequence of resolution steps. And so now I have a BDD representation of each clause and I start forming conjunctions and quantifying out variables, and I keep doing that over and over again until in the end I end up with the leaf node zero, and that's the empty clause. Any other questions? So this chart actually shows an interesting phenomena related to that, that even though I'm generating proofs that are totally compatible with the standard checker draft proofs, they're really different.
00:22:46.758 - 00:23:47.606, Speaker A: That one is really more at the problem level describing what you're doing with the search space, whereas the other is more at the algorithm level justifying an algorithm. But it's kind of cool to me that these can all be handled within the same proof framework. I don't have to have a special proof checker just for me to do. And so you'll notice is one consequence is that you don't generate proof steps as often in CDCL as you do in, because you do all that other stuff until you hit a conflict. And here we're generating up to six proof steps for every recursive call. But it's not that huge a difference in size factor. Another for you hackers, you'll notice that the time per proof step for smaller benchmarks is like just below 1.
00:23:47.606 - 00:24:21.352, Speaker A: For larger ones it's about two. And the reason for that increase is just the overhead of garbage collection and all the other stuff that a BDD package has to do. Yeah, so you say that CDCL indicates reduction in search space. So CDCL indicates reduction in search space. I understand that very well, but I don't understand what you mean by BDD. Proof steps justify algorithmic steps. Well, in other words, we don't have this intuition.
00:24:21.352 - 00:25:04.288, Speaker A: The apply algorithm is just this recursive algorithm that builds a new BDD based on some existing ones. And what we're saying is an invariant property of how that algorithm works in terms of what each recursive call returns. And we're building a proof that's basically just stringing together this long sequence of algorithmic steps into a proof that will end up being a proof of unsatisfiability. Yeah, a tiny question. The y axis here is solver runtime, not checker runtime. Oh, yeah. I have a different one on checker runtime.
00:25:04.288 - 00:25:30.868, Speaker A: Oh, exciting. But I don't have it with me. But the checker is a little faster because, because I generate Lrat. If you guys know the file formats. Lrat includes hints for each clause. All the other causes that it depends on. Whereas D rat doesn't have hints, and you have to run a d rat proof through d rat trim, which is incredibly fast, but it takes time.
00:25:30.868 - 00:25:38.004, Speaker A: So it's almost as if these two flip and the outcome is about the same. Give me a sec.
00:25:39.264 - 00:25:43.280, Speaker B: So which are the variables you're quantifying here?
00:25:43.472 - 00:26:09.930, Speaker A: All of them. All the input variables. Right. Because I'm reducing it to the. I could, by the way, in principle, just, it's just an, and it's just this giant conjunction. I could in principle just build up a BDD representation of the whole thing and not use quantification at all. But the intermediate bdds would explode.
00:26:09.930 - 00:26:35.056, Speaker A: So what you want to do is as aggressively as possible, quantify away, existentially quantify away variables. And you do that by having some ordering to the variables and always forming a conjunction that for which the next variable, if you do early quantification. If you.
00:26:35.080 - 00:26:35.832, Speaker B: Early quantification.
00:26:35.888 - 00:26:45.000, Speaker A: Right. Okay. But it's all totally automated. It doesn't require any, in fact, it can be done using random variable orderings.
00:26:45.072 - 00:26:47.200, Speaker B: So when you say no quantification, that.
00:26:47.272 - 00:27:18.394, Speaker A: Oh, this is from other benchmarks. Yeah. The point was, this is from another paper where I tried a whole bunch of different things and got vastly different proof sizes for the same problem. And all this was showing was, regardless of what kind of style of generation I was doing with the bdds, that it's a totally fixed rate at which it generates proof clauses because it's based on the individual steps of the algorithm.
00:27:19.534 - 00:27:28.422, Speaker B: So, Randy, the original DP algorithm was essentially quantifying the variables. What you're doing is just adding extension variables in that process. Is that what's going on?
00:27:28.478 - 00:27:31.434, Speaker A: Yeah. So I'm not doing.
00:27:36.014 - 00:27:39.654, Speaker B: Because that also explodes in the middle. Right. As you do the exemption.
00:27:39.774 - 00:27:43.190, Speaker A: I'm not really doing DP. Yeah. I mean, at some level you're doing.
00:27:43.222 - 00:28:01.934, Speaker B: DP, but I'm just wondering if you added the extension variable introduction to these things, as you do with the existence, the exists on the input variables, would DP then become viable? You're solving the problem. You're trying to show that the anti clause is entailed by the.
00:28:06.554 - 00:28:22.916, Speaker A: So moving on. That was an interesting exercise, but oh, another question. Yes. So why would you first transform edge source into CNF instead of just using the direct PDD representation of adsorbed? I did translate. These are all CNF to CNF. Everything CNF.
00:28:22.980 - 00:28:25.084, Speaker B: Okay. I thought the problem was about parity.
00:28:25.204 - 00:28:56.286, Speaker A: Yeah, but use your favorite way of encoding a parity problem in CNF. I see, thanks. Sorry I didn't go back through that, but everything here I'm working. I'm starting with CNF. So now let's move on that. The truth is though, bdds are great at some problems, but really lousy at others. And if you try to just like start solving sat benchmark competition problems, you pretty much get nowhere.
00:28:56.286 - 00:29:47.624, Speaker A: It just doesn't compete. But there's other interesting classes of formulas that now that I have this engine, this essentially proof engine, I could use it independent of how I actually do the derivation. And by that I mean I've become very intrigued by this idea of pseudo boolean formulas, which are linear equations or inequalities, where the coefficients are integers, but the variables are just zero one value. And as I show here, they can be equations constraints who are actually modular. And the case of radix two, of course they're parity constraints. So those are very interesting in many applications. And one thing, bdds love pseudobuy and formulas, meaning this is, well, not love is too strong a term.
00:29:47.624 - 00:30:34.134, Speaker A: If the coefficients are small, then the BDDs are small. So you end up with this rule here that the largest BDD will be the absolute value, largest coefficient times n squared, and that's independent variable order. You don't have to be clever about this at all. For the case where the coefficients are reasonably small, this is a good number. Of course the coefficients can grow to be exponential in size given a bit level encoding of them. And so potentially they could explode on you. So this is actually what you'd call a pseudo polynomial bound on a pseudo Boolean function, even though those have nothing to do with each other, the two pseudoscience.
00:30:34.134 - 00:31:31.958, Speaker A: And it's much better for modular equations that you have this very simple representation. That's where the width of it is most, the radix r. So imagine you've got some terribly clever pseudo Boolean solver, and you have some way that to get from however you've encoded it. Let's assume CNF and you can extract out of that, or somebody tells you where these pseudo Boolean formulas and how they're encoded in the CNF. And so we could, in principle, given a formula that's suitably a suitable formula, we could do an infeasibility check using the pseudo Boolean solver. And there's a lot of very interesting ways to do that. The problem with that picture is where's the proof? Right? People don't really trust.
00:31:31.958 - 00:32:33.954, Speaker A: Now, one thing about Sat solvers is if you can't prove a formula is untrue, then it's no longer, people don't believe you anymore. So here's the idea that I did with, along with Armin Bier last year, and marine of course too, which is kind of hang a box on the side, a BDD engine. And the BDD engine is not going to be used for the core reasoning that's being done by the pseudo buoyant solver. It's just being, using it to spit out a sequence of proof steps that will justify the pseudo Boolean solvers operations. So in particular, I have to do two things. I have to prove that the pseudo boolean constraints that are extracted out of that are truly implied by the input clauses. And the other is for each low level step of this solver, I have to be able to justify that it preserves its implication, preserving basically.
00:32:33.954 - 00:33:35.304, Speaker A: And so out of that n, then I'll end up just deriving a refutation proof on it. So the point being that the bdds here are just an assistant here, they're not actually doing the reasoning itself. And the key idea of that is you have to break down the solver into some very primitive steps. For example, it might be given two constraints, add them together. So in general, you'll have some operation on two constraints that generates a third constraint. And what you have to do is just prove that their respective bdds, the end of the two argument BDD, implies the newly generated BDD, and the complexity of that is sort of the product of the three BDD sizes. So as long as the coefficients say small, this will be a nice polynomial operation.
00:33:35.304 - 00:34:21.964, Speaker A: And so if we do that for our parity problems, parity is the best because it's these bounded width bvts, and so they're extremely fast. And so using gaussian elimination on those parity constraints, we can just fly through this and it breaks when n gets so large, when this number of variables gets so large that it overflows the field in the BDD package that encodes variables. So I could recompile it to use more. But the point being that this is now clearly a trivial problem for this approach to solution. Right. We've totally blown it away. Yes, yes.
00:34:21.964 - 00:34:53.328, Speaker A: You encode the entire, the entire diagram. Well, the variable for the root describes the entire diagram. And like Karen said, think of it as a circuit representation of that boolean function. Corresponds with boolean function. Yeah, exactly. So this BDD based proof generation strategy.
00:34:53.376 - 00:34:56.524, Speaker B: Seems to work for any rewrite rule.
00:34:58.964 - 00:35:04.724, Speaker A: If you can break it down to a primitive step like this, where this.
00:35:04.884 - 00:35:08.060, Speaker B: Rewriting some right hand side to an equivalent left hand side.
00:35:08.092 - 00:35:57.454, Speaker A: Yes, exactly. That's a good way to think about it. Okay, thank you. So I think that's, again, it relies on having a problem domain for which these bdds stay reasonably compact. Otherwise, of course, it doesn't work at all. But at least in the pseudo boolean case, that seems to be true, at least for some class of problems. So the way I describe this then, is what we've shown that is sort of bdds can help you clean up these really glaring, but it's not showing you that you can solve any problems that one would consider mathematically interesting.
00:35:57.454 - 00:36:39.744, Speaker A: Yes. So if the former is satisfied and you have eliminated variables from the border, you'd like to get witnesses for those eliminated variables. Oh, that's easy to do. Sort of a back substitution on it in reverse order. Yeah. In fact, you can, it's fairly cool that you can generate multiple witnesses by just essentially, you know, generating a solution by this back substitution, blocking that one and doing it again. And you can just keep enumerating them over and over again.
00:36:39.744 - 00:37:17.784, Speaker A: And this, by the way, can be used also, not just for unsat, but for. Well, I guess if it's satisfiable, then all this doesn't really matter, that having the proof generation doesn't really matter. I was thinking of something else. Sorry. But you know, someday what we'd really like to do is get here. We'd like to do interesting problems that can't be done alone with CDCL, but some other tool possibly would do that. And I honestly don't have the answer to that.
00:37:17.784 - 00:38:36.884, Speaker A: But it seems like one approach would be what I'll call CDCL of T, which is not an SMT solver. It's saying let's have multiple causal reasoning engines for Boolean problems and be able to run them on different parts of the problem and coordinate their activities by having them exchange unit propagations and conflicts, just like an SMT solver works, and the one instance of this that I know of is crypto mini set, of course, which has built into it both a causal part and a Gauss Jordan elimination part. And so it can at least take two different of these engines and operate them together. It's harder than you know. It's easy to draw the picture and hard to implement the code, especially deciding how to coordinate between them is really tricky. And of course, they're very different engines in terms of CDCL runs really, really fast in its inner loop, but doesn't get you very far on each step, whereas Gauss Jordan is doing these big elimination steps. So there's sort of a lot of coordination issues.
00:38:36.884 - 00:39:38.230, Speaker A: But one thing mate and I did last year was show that we could sort of tack a BDD package onto his parity solving code and use it to generate proofs. Because before that, crypto minisec, when given an unsat problem, couldn't generate proofs if it was using its Gauss Jordan engine. So it was an interesting experience that I think showed that it can be done, but it's hard to do it that easy to do. It'd be interesting to build in other solvers into this. So I think this is if part of this talk is, here's some interesting research direction you might think of. This is definitely high on my list for you. So the latest work we've done, and this hasn't been published yet, but it's in the topic of what's called knowledge compilation, which I don't know why it's called knowledge compilation.
00:39:38.230 - 00:40:32.844, Speaker A: To me, they're just buoyant formulas, but it's the idea of it is CNF. It's really hard to say much about it. In particular, it's very hard to count models or say anything about it. And so what we want to do is transform that CDC NF into a representation for which things like model counting become very simple. And of course, there's going to be a possible exponential blow up because it's a sharp peak set issue. But what's remarkable to me is that there are programs out there, and particularly we've been using one called d four, which is incredibly successful. And it's based on a sort of CDCL style, but it builds up essentially a circuit representation of this formula for which counting becomes easy.
00:40:32.844 - 00:41:54.874, Speaker A: So just to go into it a little more, but the problem you have with that is just like a sat solver, it returns this box that you can query and answer questions about. But how are you sure that that box you created is logically equivalent to the input formula, and then I make these queries. And even if I trust that representation, how do I make sure that it computed things like the number of models correctly? So that's the general context here I like a formulation of paper I read by Kim again, her co authors, that describe this as algebraic model counting. And what they say is imagine a set of total assignment will just view it as a set of n literals assigning a value to each input variable. And so a model, as you know, is the set of all possible satisfying assignments for the formula. But now let's map this into a commutative ring where what I want to do is compute. I'll say that the there's a weight associated with each variable, and the weight of its negation is one minus that, and that the weight of a assignment.
00:41:54.874 - 00:42:47.160, Speaker A: Then it's the product of the weights of its literals, and the value of the overall formula is the sum of the weights of its satisfying assignments. So in particular with this definition I can do model counting in the following way. I assign one, two to each variable. And what I'll get out of that when I run this ring evaluation is I'll get the density, the fraction of assignment of possible assignments that satisfy this formula. So if I just scale that by two to the n, I'll get the count of the number of models. Or if you imagine the weight being a probability between zero and one, then the computed value of this will be the probability that the formula is true. So there's a lot of other stuff you can do with this model.
00:42:47.160 - 00:44:19.444, Speaker A: It's really a pretty handy thing. And the main thing, and this is as far as I can tell, all model counters rely on this principle that if I can represent a boolean formula in this falling very restricted form, I say that I'll only perform conjunctions over cases where the two subformulas are over disjoint sets of variables, and only do the disjunction over two formulas where they have disjoint models. Then basically, then I can argue that the set of satisfying assignments to a conjunction will be the cross product of the and, and so the ring evaluation will be the product, and the set of satisfying assignments to a disjunction will be their disjoint union. And so again I get a sum over this ring, and negation comes for free. You just do one minus to get that. So these model counters knowledge compilers come with various idiosyncratic file formats, but at the core what they're doing is just representing a formula that obeys these restrictions for all its conjunctions and disjunctions, and potentially as negations on the edges in arbitrary places. So this is an example of, of what they might produce, in my own way of describing it.
00:44:19.444 - 00:45:25.364, Speaker A: So what we've developed is a tool chain that will take the output of a knowledge compiler and convert it to this form, which I call a pog, which of course is an acronym in search of a meaning, but a nice easy one to pronounce. So, partitioned Operation Graph and what it will produce is a file that both encodes that graph and provides a proof justifying that the pog is logically equivalent to the input formula. And I'll talk about what that means and how we do it. And then checking becomes twofold. One is we want to make sure that that equivalence holds, and the other is we want to be able to safely do this ring evaluation over this representation. But we've shifted that again like before. We've shifted the trusted base to be on the checking side, away from a very complex knowledge compiler and a fairly complex proof generator.
00:45:25.364 - 00:46:45.838, Speaker A: And if you don't trust our trusted code, well, I've got even something better for you, which is a really trusted version of it that the graduate student who worked with us on this was able to write a lean four, which is a proof system, a version of checkers for this and ring evaluation that is guaranteed is formally verified. And actually this was for those who've ever worked on it. I found this a very interesting exercise. First of all made me appreciate how hard it is to prove things that we kind of just say, well, it's a ring, fine, it's no problem, right? But also it exposed some weaknesses. We had to add a few more conditions and rules, nothing major, but some very subtle points that we'd overlooked, that only uncovered when he couldn't get the proofs to go through. So if you're ever devising your own proof format or checker, this is a useful exercise. So what does it mean to prove equivalence? Well, let's say that the formula is a set of clauses, phi sub I, and how I'll define the pog logically is using extended resolution.
00:46:45.838 - 00:47:50.694, Speaker A: I'll say that there's an extension variable for every node in this graph or every operator in this graph, and it will have its defining causes according whether it's an Android nor operation. And I can introduce negation by just allowing the inputs to an argument to be literals, not just variables. And I'll, so that there'll be a set of unit clauses that basically defines the circuit. And then I'll say, and there is a unit cause for the root node and that combination of these defining causes and that unit cause. What they say is any assignment that satisfies the input formula will satisfy this circuit, you know, produce a one on this circuit and vice versa. We have to prove equivalence and not just one direction here. And so the proof of that actually is two different proofs.
00:47:50.694 - 00:48:57.154, Speaker A: One is a forward implication proof that every assignment to the formula will generate a one on this circuit, and similarly that conversely, basically anything that would falsify one of the causes will also produce a zero on that circuit is what the reverse implication proof looks like. And so the forward one looks kind of standard resolution steps based on the circuit structure. You can imagine what it looks like. The reverse one's interesting in that the way you prove satisfaction in a causal framework is you delete the clauses and you have to justify that each deletion doesn't somehow enable some new assignments to come through and satisfy what's left of the conjunction. And so what you do is you build up this proof, and then you start deleting closets. You delete away all the ones that were used in the implications that you built up. You don't delete the unit clause at the top, and then you start deleting input clauses.
00:48:57.154 - 00:49:06.634, Speaker A: And each time you delete it, you're saying anything that would falsify that input clause will also falsify my output. Done.
00:49:08.974 - 00:49:12.054, Speaker C: When you say clause, you mean CNF?
00:49:12.174 - 00:49:13.798, Speaker A: Yeah, my whole world is.
00:49:13.926 - 00:49:26.896, Speaker C: So where does the restriction about, you know, taking only the and over disjoint variables and or over disjoint, where does that occur in this?
00:49:27.000 - 00:50:13.732, Speaker A: It doesn't occur in the proof. That's not a necessary part of the proof. It's a necessary part of the representation to be able to do ring evaluation. So there's separate proof obligations on the, for the disjoint that the, the other part of this file format is for every time I do an or, I have to give a set of essentially a very small resolution proof that says these two are mutually exclusive and those are checked. And then the, the disjointness of the conjunctions you can do by just keeping track of the dependency sets for all the intermediate nodes in the graph. Say that.
00:50:13.828 - 00:50:15.404, Speaker C: I thought that was going to be the hard part.
00:50:15.484 - 00:50:17.676, Speaker A: No, it's right. You can do that.
00:50:17.860 - 00:50:26.584, Speaker C: I can easily keep track of what variables I've used. So the conjunction was the hard part, but the disjunction, why is that?
00:50:26.884 - 00:50:31.060, Speaker A: Because I give a resolution proof that we'll show those are mutual, that show.
00:50:31.092 - 00:50:32.324, Speaker C: That there's nothing in common.
00:50:32.404 - 00:50:55.234, Speaker A: Right. Okay. And in this case, the values produced by these model counters, it's true, trivial, because the disjunction occurs when they branch on a variable and they explore the case of the variable true and the variable false, and then just. Or those together. So there's a single variable that's guaranteeing, I believe that those are disjuncting. Okay. But that's part of the, one of the proof obligations.
00:50:55.234 - 00:52:20.804, Speaker A: But the equivalence proof doesn't rely on either of those properties. But the point being that we're so used to talking about refutation proofs in resolution, but they're also equally capable of doing satisfaction proofs by this, if you can justify in a proof framework where you have to justify each clause deletion. So we ran this tool we developed on benchmarks from last year's SaT, I'm sorry, model counting competition. And of the 180 unique benchmark files that we started with, D four could complete 124 of them using a 4002nd time limit. And then we allowed the proof generator, excuse me, up to 10,000 seconds, and then let the checker run as long as it needed to run. So what you see is these are, these are some pretty serious benchmarks, even though this was a prototype tool of sorts. And you'll see that the time run by the proof generator would be typically factors of ten more than the D four runtime.
00:52:20.804 - 00:53:05.336, Speaker A: There were a few cases where it actually ran faster than D four, which is interesting, but we were able to do total two sided proofs for 108 of them. And we could only do the reverse implication side. We could do that for nine more of them. The reverse implication is a simpler proof generation. And then there were just seven of them left that we couldn't get anywhere on. We either timed out or ran out of space. But just to give a sense of sizes, we're not small that the number of defining causes generated by D four went up to 2.7
00:53:05.336 - 00:54:15.020, Speaker A: billion, in fact, overflowed the 32 bit number. I had to track cause ids. We weren't able to generate proofs for any of the inputs that went over a billion defining clauses, but we were able to do some that were in the hundreds of millions of them. And you'll see that the proof size is sort of highly varied, but there's a big blob of them that are roughly three times larger the proof than the defining clauses. And then there's some that are basically very hard to satisfy formulas, so very small number of models, and the proof had to encode sort of all the unsat conditions in that result. So that just shows an example of what I like about this work, too, is that you think model counting, oh, you've got a reason about numbers and addition and blah, blah, blah. But this idea of going through knowledge compilation first, which most model counters do either explicitly or implicitly, then that's a pure boolean problem.
00:54:15.020 - 00:55:09.414, Speaker A: And that's the sort of harder lift in terms of the program, because once you have it in that representation, then the counting part of it becomes simple. And for my colleagues who did the lean proofs of it, that was the easy part of the proof. The buoyant part was the hard part of it. So the point being that even causal proofs are useful for more than just pure buoyancy problems, problems that, on the face might seem more than pure brilliant. So just to recap then, I think this idea of checkable proofs is extremely powerful. And it has this effect, by the way, that's very habit forming in the sense that once you seen this capability in one tool, you want it everywhere else. I think a lot of people in this room have felt that way.
00:55:09.414 - 00:56:05.712, Speaker A: On the other hand, since at the core, most of us are researchers having more problems to solve. As John Hooker says, this part of guaranteed employment, even for those of us who are retired. But also, I think one thing is I've learned, too, is that the causal frameworks that were established and well known are extremely powerful. And it's useful if you can work within that framework. For example, our generator makes calls to catacol, makes calls to drat trim. It's making use of these causal reasoning tools that are available to us and can all work together. And there's other places that I didn't have a chance to go into, but there's clever places where we insert lemmas into the proof as a way of handling the case of nodes in the graph that have multiple ancestors.
00:56:05.712 - 00:56:35.340, Speaker A: So we don't have to expand that graph into a tree and potentially have an exponential blow up there. So there's all kinds of cool stuff once you get the hang of extended resolution. You can sort of build little boxes to do this and that. It's a lot of fun. Okay, so that's so.
00:56:35.372 - 00:56:50.564, Speaker B: I didn't know Satan had to do with programming languages. And I've been trying to remember where I saw this. They have this thing called ghost variables in programming languages. I can't remember the paper. I can't I don't remember that he's mentioned there.
00:56:50.644 - 00:56:54.588, Speaker A: No, because he, I don't think that's what he's known for. So I.
00:56:54.676 - 00:56:57.264, Speaker B: But they talk about ghost variables which seem like extension.
00:56:57.684 - 00:57:10.636, Speaker A: Satan left behind all interest in logic at some point in his life and became a programming language person and never looked back. But Don would know better than I did. And by the way, if you want to look around, you have to look at these various different romanizations of his name.
00:57:10.700 - 00:57:12.596, Speaker B: Yes, yes. With the y and all the, with.
00:57:12.620 - 00:57:17.500, Speaker A: The y and stuff. So, and so you have to look at different romanizations to pick up everything.
00:57:17.612 - 00:57:29.666, Speaker B: But the notion of extension variables appears in other fields like ghost variables, definitions. Yes, things like this, you know, where you just label some complex formula and then use it.
00:57:29.690 - 00:57:40.986, Speaker A: Yeah. Ghost variable is a very interesting idea. There's one remarkable thing about the way.
00:57:41.010 - 00:57:48.210, Speaker B: You approach this knowledge compilation, namely that you're not certifying what d four is.
00:57:48.242 - 00:57:50.780, Speaker A: Doing actually, but you're taking its outputs.
00:57:50.812 - 00:57:52.740, Speaker B: And creating a proof when it's finished.
00:57:52.812 - 00:57:53.424, Speaker A: Right.
00:57:54.524 - 00:57:58.652, Speaker B: Have you considered doing it directly in there? And are there well advanced?
00:57:58.668 - 00:58:36.184, Speaker A: Have you ever looked at the code for D four? No, I'm joking. But you're right, that's an interesting property, that we're just taking it like it rolled off the truck and then we're just starting to look at it. We are relying little that we know that there's some aspects of it. In particular, we know that every time there's an or there's going to be a variable that it's splitting over, and also it's in a negation normal form, so the only complements are on the leaves and we're making use of that stuff, but we're not making use of any of the internal structure.
00:58:36.484 - 00:58:40.372, Speaker B: And does this mean that the tool doesn't work for general ddnmfs, but only.
00:58:40.468 - 00:59:28.384, Speaker A: Those that have this specific property of branching on the or? First of all, I don't know if you know, D four has its own idiosyncratic file format and I haven't implemented the one for the other more common format. But I did do a little bit of evaluation of, there's a program called D sharp which is sharp SAP modified to produce ddnfs, and it works fine for those. So it's not restrict, in other words, all knowledge compilers that I know of, all top down knowledge compilers that I know of effectively obey those same set of rules for what kind of DDnf they produce. They're always branching on a variable and they're always starting with. Well, the NNF stands for negation, normal form there.
00:59:31.804 - 00:59:49.354, Speaker B: So there's this DD with a small d and a capital D. And there, I think it's a semantic condition saying that the two branches of the or cannot be simultaneously satisfied even if they shared a common variable.
00:59:49.894 - 01:00:21.034, Speaker A: Right. There's actually two. If you look at Darwish's papers, not only does he have a lot of acronyms, but there's double meanings for some of the acronyms. So the first D can mean either decision or deterministic. Decision means there's a single variable, but undeterministic just simply means that they're mutually exclusive. So, so the, so if I were given, by the way, a more general one, I'd just call a sat solver there to prove that it's their, their, uh. Uh, the mutual exclusion.
01:00:28.174 - 01:00:29.892, Speaker B: So I had the, I found your.
01:00:29.908 - 01:00:56.334, Speaker A: Idea really interesting when you're talking about like, how to prove parity formulas. And you, essentially what I understood is you have, you take your clauses, you turn them into bdds, and then you merge them. Yes, but, and then the order in which you merge them is very important for, for your algorithm. Right. Well, it's sort of like if you know how spouse sparse, you know, gaussian elimination solvers works with pivot selection. I use strategies like that. Okay.
01:00:58.634 - 01:01:01.014, Speaker D: Did you find any bugs? Now I'm curious.
01:01:01.394 - 01:01:13.614, Speaker A: Oh, no, that's an interesting thing with d four. I haven't found a single bug in it. It's really amazing because I never would have expected it. It's a pretty fancy piece of code and I haven't found any bugs in it.
01:01:18.474 - 01:01:26.704, Speaker B: You could maybe also try to use Sharpsat CD because you can use it as an algebraic model counter.
01:01:27.644 - 01:01:35.596, Speaker A: Right. You can plug in similar sharpset doesn't produce representation, but sharp set CD has.
01:01:35.620 - 01:01:40.996, Speaker B: These templates you can plug in, you know, change the semi rings it works on.
01:01:41.060 - 01:01:41.492, Speaker A: Yes.
01:01:41.588 - 01:01:43.064, Speaker B: So you could get the.
01:01:43.524 - 01:01:45.444, Speaker A: I can do a comparison. Yeah, yeah.
01:01:45.484 - 01:01:50.624, Speaker B: You could get a, get an s, you know, DDNF.
01:01:53.094 - 01:02:21.074, Speaker A: Yeah. Well, yes, I'll look at other. So, you know, as you know, when you're trying to get something done, meet a paper deadline, you have to decide what you're going to do and what you're not going to do. So g four, by my small experiments I ran, g four was just killing everyone else. So. And it, it was in the model counting competition of all the knowledge compilation based ones. I think the topic.
01:02:22.694 - 01:02:30.318, Speaker B: No, you can get the traces. Right? Like from Sharpsat and Sharpshoot TD as well, right?
01:02:30.406 - 01:02:34.274, Speaker A: These are all doable things, and I'm not saying I won't do them.
01:02:35.454 - 01:02:36.566, Speaker D: I think you're.
01:02:36.750 - 01:02:42.274, Speaker A: Yes. Did you try sightings?
01:02:44.454 - 01:02:47.696, Speaker C: Try teams problem on parity graphs?
01:02:47.720 - 01:03:43.828, Speaker A: Oh, yes. So I didn't show it, but all the Satan formulas over graphs in particular, the standard benchmark was devised by Urkerhardt based on expander graphs to show a lower bound resolution proof of an extremely small formula. And so back in the first Sat competition, Lee, I forget his first name, from France, submitted a formula of the Satan form, the orca Hart formula with the parameters called m equals three, which is the smallest possible instance of that formula. And back then, none of the solvers could deal with it. It was included in the anniversary track last year. And guess what? None of the solvers could do it. The smallest instance, extremely small formula, like 800 clauses or something like that, and it just absolutely kills them.
01:03:43.828 - 01:04:05.884, Speaker A: So we can do workerheart up to m equals 200 and something, and the formulas grow quadratically with them. They're pretty easy if you think about anything you can do, any kind of parity reasoning you can do, and go really fast.
01:04:08.904 - 01:04:38.024, Speaker D: Yeah, I mean, it's just a remark on the before kills it, which I think is probably right in case you just run it on CNF. I think what's kind of counterintuitive or kind of painful is that current set of saTs, like mobile counters, sometimes you need to run a pre processing tool, either B, C, or Arjun or something. Otherwise it gets stuck, unfortunately. So that's, I mean, it's very counterintuitive and like, it's a bunch of shell scripts that everybody's running. So that needs to be solved, to be honest.
01:04:38.064 - 01:05:07.884, Speaker A: And, well, that would be very interesting because as was pointing out, I'm just solving, looking at the artifact that's produced. So if you've run it through all kinds of crud, you know, I can still feed it into my tool and presume we get some results. So that would be some very interesting experiments to do. Okay, on that note, let's conclude. Let's thank Randy again. We had 21 minutes. We meet at eleven.
