00:00:05.080 - 00:00:36.578, Speaker A: Thank you, Stephen. Okay, so thanks, everybody, and thanks for the invitation. It's been a pleasure being here so far. So today I'm going to talk to you a little bit about a project that we've been working on for about a year and a half now in creating a market space for data and providing an algorithmic approach. This is work with our students, Anish Agarwal, Tawhin Sarkar, and Devracha. This is going to be a slice of the work that we're doing. It's just hard to talk about everything we're doing in half an hour.
00:00:36.578 - 00:01:24.104, Speaker A: So let me just sort of motivate the work and maybe give you some idea about some of the challenges and how we resolve some of the challenges for creating such a marketplace. You know, so the data has become a currency in the digital market. Almost every corporation right now is thinking about utilizing either their own data or buying data in order to predict certain things or improve their value proposition or their business model and so forth. In fact, some companies have completely changed their business model into a data driven business model. And so this thing become really, and by the day, very important. And in fact, this quote is kind of interesting. It says that personal data is the new oil of the Internet and the new currency of the digital world.
00:01:24.104 - 00:02:31.530, Speaker A: From our perspective, if it's a currency, then how do you value it? Okay? And it seems like there's a lot of interesting challenges that face not just individuals like us when we are asked to sort of provide our data for a value. Also, even big companies like Bloomberg, Reuters, Nielsen, these companies engage sort of long term contracts with their customers. They're often, it's not the case that they would actually figure out how much value the data that they sold their customer is. And in fact, they're having a very hard time convincing other people to buy their data because it's not clear how the data is going to be valuable to them. And everyone is going around talking about so how much value this data set is. And if you talk to Reuters, I mean, they have these pricing methods of data sets over certain periods of time, and it doesn't seem to make a lot of sense, in fact, actually, even from an individual point of view. And there's another issue there is that if you, for example, give your data to 23 andme, you pay $100 to give your data, and in return you get a service.
00:02:31.530 - 00:03:05.804, Speaker A: And the service is that they tell you, okay, where you came from and some extraction of the projection of your genome on different parts of the world. However, what you don't know is that right now they own your data, and potentially this data. We don't know what's happening. This data could be sold to insurance companies. So maybe we discover something wrong with the genomic that you have, and then you're insurance would get jacked up. So now, all of a sudden, the service you got is much, much more expensive than $100 that you paid. And I think this is happening in credit reports.
00:03:05.804 - 00:03:35.776, Speaker A: It's happening to us everywhere. A privacy issue, obviously, is the crux of all of this. But we know that we have no privacy. And in fact, I think it's becoming even more complicated in looking at breaches like the equifax situation. 150 million people lost their data, or their data became public financial data, social data, and so forth. And it's not clear that we know exactly how to value that breach. I mean, it's certainly kind of overwhelming to know that so much data has been sold.
00:03:35.776 - 00:04:23.034, Speaker A: But how much value? How much does it matter? If my data is available to some random person out there, how much can they do with it? I'm not really sure myself how to value that. Certainly the whole thing that happened with Cambridge Analytica, which is not a breach, that was a sales that Facebook has done to Cambridge Analytica. Everybody is sort of appalled, actually. Did even Facebook get the right value from Cambridge Analytica on the data that they sold? So there's all these questions are really interesting. And so it kind of calls for creating a market for data to understand how to value it and how to exchange it. It's a real time market because things are changing very, very rapidly, and almost the history matters very little in these kind of situations. But there are challenges to creating a market for data.
00:04:23.034 - 00:04:54.750, Speaker A: Okay. And some of these challenges I listed over here, one is their application at zero marginal cost. If you have a data point, you can sell that one data point, but you can also sell multiple copies of that data point. And it doesn't cost you anything to sell multiple copies. Its value may increase or decrease depending on the vendors that are buying the data from you. We don't know that upfront, how many copies of the data you ought to be selling. The second thing is that the value of the data to a firm is inherently combinatorial.
00:04:54.750 - 00:05:36.694, Speaker A: That is, if I have multiple features or data sets, and I am interested in a particular prediction task, maybe different combinations of these data sets will contribute better to my prediction. And so you have to try out all of these different combinations in any whatever market that you create. Of course, the prediction tasks that companies have vary widely, different companies have, and I'll give you some examples in a second. One aspect is that the authenticity and the value of data cannot be determined a priori. You almost have to do take the data, try it out, see if it adds value to you, and then price it. Okay. Because in general, you don't know ahead of time what the value of the data is.
00:05:36.694 - 00:06:20.276, Speaker A: This is not the stock market where you have some prediction of what's going on. And finally, I would say that also there's a value, there's an issue of externality, because if a data point goes to my competitor, then it actually matters to me. Okay? And so it's not just whether I get the data, but also if the data goes to the competitor, it actually changes my, my cost function. Okay. And so, okay, so just quickly here, I don't want to spend too much time comparing this. You can actually go through the nitty details of comparing, creating a data market, a data market to auction markets or stock markets or even prediction markets. And you'll see that some of these challenges are not present in these existing markets.
00:06:20.276 - 00:06:44.740, Speaker A: Some are and some are not. But for example, in the ad and stock markets, you simply cannot replicate the ad. There's a physical space that you sell. There's a physical stock. If you start replicating, you devalue. And there is also an issue about the history of being relevant to the pricing of a particular product. And finally, I think the way you allocate these things depend simply on the bid.
00:06:44.740 - 00:07:16.466, Speaker A: If you bid higher than the price, you get the allocation. If you pay more than the price or the price of the stock, you buy the stock. And prediction markets have similar sort of simplicities to them. Okay? So I want to get into talking about creating a robust matching market for data and describe it to you in pieces and tell you how we sort of address some of the challenges. Okay, so what is the market? What is this market? It has sellers that come in. The sellers have data sets that they would like to sell. Okay? Think of it as features.
00:07:16.466 - 00:07:49.284, Speaker A: Think of it as data sets. Fixed RT vector. Okay? Buyers come to the market, and the buyers have both a prediction task. They're interested in predicting something as well as they have a way to value the accuracy on this prediction. So a good example to think about is that the buyers are retailers. They're trying to predict a demand on a particular product at a particular time. Okay? The accuracy in this prediction translates directly to the cost.
00:07:49.284 - 00:08:27.304, Speaker A: And they know that if you give me better accuracy on this particular cost, this is the amount of dollars they actually save. So this valuation is something that they know upfront. Okay. And they're interested in this demand question. Right. And so maybe they're buying data from, from individual people or from Twitter sentiments or from kind of finance, trying to figure out how the economy, inflation rates are they buying data from everywhere, trying to predict this particular task. Now, the market's task is to take the data that's available and assign it or allocate it to the buyers.
00:08:27.304 - 00:09:14.894, Speaker A: Of course, after they do that, they have to extract a certain payment from the buyers and take that payment and distribute it to the sellers. And that distribution has to be also fair because different sellers, their data was more valuable than others. Okay. One thing that is important in this particular market to think about, and the example I gave you, maybe in retail, is a good sort of running example for this, is that the market has to agree on how we evaluate the accuracy of prediction. So that's something that is agreed upon. In addition, actually, we have to agree on how we do the prediction. So what is the inference algorithm, machine learning algorithm, whatever we use that is already fixed and agreed upon by everybody of how to evaluate this so that we, we take that out of the formula right now.
00:09:14.894 - 00:09:37.988, Speaker A: Okay. And the two of these two. Okay. So one other point I want to make is that, of course, there's many different levels of this that you can make this problem complicated. You can have sellers coming in in real time, buyers coming in real time and matching happening all the time. Right. Now what I'm going to do is I'm going to fix one side of this and I'm going to say that the sellers are fixed.
00:09:37.988 - 00:10:01.176, Speaker A: I will have m, of these sellers. Buyers are coming one at a time, in real time, and this allocation is happening. So a buyer is buying the set and then they leave and then they buy another set and so forth. Right. You can have the flip side of this problem and also you can have the general problem that will take a lot more time to describe and we don't have all the results for all of these complications. Okay. All right.
00:10:01.176 - 00:10:46.036, Speaker A: So the application of the two sided market also has different applications. I talked a little bit about the retail application. Of course, this could be in transportation where somebody, so for example, municipality is interested in estimating the utilization of infrastructure, congestion of some sort, right? And so they would like to collect data from Uber, from Lyft, from buses, from taxis, whatever, right. And they want to run this real time prediction. So how do they do that? How do they, what's the value of the data and how they price it. So that's one part. Another example which actually got us really interested in this is trying to understand how all of this affects the real time demand response in the power in electricity market.
00:10:46.036 - 00:11:33.226, Speaker A: And that's another place where you're thinking about the data of people in their utilization, for example, of their vehicles or electric vehicles or in potentially having their own renewables. You know, how is that data helpful to some sort of an aggregator or aggregators that then will be able to mitigate some uncertainty with the grid? So how do you do that particular pricing market? Okay, so let me describe the problem. So the sellers, as I said, there would be m sellers fixed in the problem, each have data set x sub j vector from zero to t. Buyers come in the system yn at every time, n for some time. And as I said, the buyers have a prediction task. I'll describe that as a vector. Yn could be the demand.
00:11:33.226 - 00:11:59.866, Speaker A: For example, in the retail example, mun is their valuation of a unit gain of accuracy. So this is how much money they're willing to pay for getting a certain unit gain of accuracy. And bn is their bid. Okay. Which I haven't described yet what that is. All right. So what happens in this market and quickly run through this real time market, how it's going to work, and then tell you what the challenges are.
00:11:59.866 - 00:12:26.982, Speaker A: And then if I have time, I'll tell you how to resolve them. Okay. So the first thing is in real time, time n or time t, the market has priced the individual data sets. So it has a p vector that prices these data sets. The buyers come in and they bid. And based on the bid and the price, they allocate data sets to the individual people. Okay.
00:12:26.982 - 00:12:36.954, Speaker A: And then they evaluate the gain in accuracy and then they determine the price, okay. The payment that these people have to pay. And then they allocate.
00:12:37.914 - 00:12:48.098, Speaker B: One of the challenges you talked about in the early slide is that if you also get the data or you get the data, that somehow I have an externality for that.
00:12:48.226 - 00:12:50.494, Speaker A: So when you say that's missing here.
00:12:51.754 - 00:12:55.586, Speaker B: That means no one else will get that data or multiple people. No.
00:12:55.610 - 00:13:19.576, Speaker A: So in this model that I said that the buyers are coming one at a time. So it doesn't have that particular problem. Right? Because I'm allowing every buyer right now to decide without, so I don't have that problem in this model. That problem exists in the big picture, but it doesn't exist in this model. Yeah. Okay. So, okay, so the price was fixed.
00:13:19.576 - 00:13:36.392, Speaker A: People bid, they got allocated data sets. That determines the gain, which determines the price they pay. That price is taken and delivered to the sellers. Okay. That's the. And then the market updates the price and this process continues. Okay.
00:13:36.392 - 00:14:02.674, Speaker A: That's the real time market that we're proposing. What are the issues that come up? All the issues that you have in any type of market, the first thing is you want the individual buyers to bid truthfully. Okay. That is, you want them to bid the value the way they value the data. And that means that somehow this has to be structured, forced somehow, by the way the marketplace is defined. Yeah.
00:14:04.894 - 00:14:06.914, Speaker C: Everybody makes this assumption. But I.
00:14:07.294 - 00:14:18.334, Speaker A: That's a philosophical debate. That's interesting. Right. Because then you're gaming the system, and gaming the system means that we actually don't know how exactly to value the data properly. Right. But, you know, but that's. Okay.
00:14:18.334 - 00:14:26.770, Speaker A: Okay, let's take. It's an assumption. Yeah. No, I mean, it's good. It's a philosophical conversation, but it hasn't stopped. Yeah. Little bearing.
00:14:26.770 - 00:15:10.794, Speaker A: What I'm going to say. Second thing is, how do you allocate, what is your mechanism for allocation, and how do you define the payment function? Okay, it's interesting. You have a lot of choices. Can you define the allocation and the payment function in such a way that you can guarantee the faithfulness? Okay. And then finally, and also, how do you divide the division over here once you take the payment and you want to divide it among the data sets, how you do this in a fair way? And I want to describe what fair really means. And finally, how do you update the price vector to maximize revenue at every step? All of these have to be defined properly and also be computational in real time for this market to actually make sense.
00:15:11.854 - 00:15:16.182, Speaker C: Is there a concept of efficiency, since the data can be replicated freely here?
00:15:16.238 - 00:15:31.854, Speaker A: Okay, so I'm getting to that. Yeah. So I'm getting to that. Right. Okay, so, okay, so the truthfulness quantity is. Yeah, the truthfulness is that when you actually. Well, the buyer, basically, the way they will bid, they will try to maximize their utility.
00:15:31.854 - 00:15:52.350, Speaker A: What is the utility of the buyer is what they get in terms of accuracy minus what they pay that the market defines. I call that you. I'm not getting into the details of that. I want that maximization to equal to their valuation. That's what truthfulness means. Okay, well, okay. It's not obvious how to do that.
00:15:52.350 - 00:16:19.494, Speaker A: It depends on a lot of different factors. The market, essentially what it does. The market defines the allocation and the payment, and then we want to define that in such a way that you're not incentivized to game the system. So here is the first sort of observation result. Well, the assignment of product of data to people is fairly trivial. You come in with a bid. If your bid is higher than the price of the good, you get the good.
00:16:19.494 - 00:17:09.784, Speaker A: And so this is the allocation you get. If your bid is greater than PN of M, which is the current price that the market has for feature m or data set m, you get that particular data set. Now that you got a combination of data sets, what is your payment function? And here we propose to use Myerson's payment to say that the payment is the incremental gain you have for every feature you get. Okay? That's the, it's proposed before. And so what you're having is this is what you get. So g is the accuracy function that you, how you measure, and m is some algorithm that maps whatever allocated set that you got. So you have the machine learning algorithm takes the allocated set of data, measures how well you've done the prediction, and then this is what you pay.
00:17:09.784 - 00:17:27.996, Speaker A: You what you get minus all the. So what you get is the incremental value. So if you look at that in terms of a graph like this, what you get in the Morrison's rule is the area above in this graph over here. Okay, now, the interesting thing, it turns out that I must have missed something.
00:17:28.060 - 00:17:30.180, Speaker B: What's a bid? A bid is what.
00:17:30.332 - 00:17:40.164, Speaker A: So the bid is one number, one number that you provide, and then if the. And so that, sorry.
00:17:41.544 - 00:17:45.444, Speaker B: There should be an actual. What's the meaning of that number that you.
00:17:45.784 - 00:18:17.304, Speaker A: So that number is what you're willing to pay for one unit of accuracy in your prediction. You bid that amount, and what I want you to bid is the truthful value. There's a question. So can you remind us what features are data sets? The data sets are the features that you're buying. X, sub j. Is it an allocation of a subset of x, sub j is one data set. And you allocate any one of these combinations of these, of these data sets right now is only one buyer.
00:18:18.004 - 00:18:22.164, Speaker C: One buyer. But can you sell the same data to somebody else?
00:18:22.244 - 00:18:49.094, Speaker A: No, not in this model. Yeah, not in this model. I mean, we're not going to complicate this indefinitely. I mean, there's already enough complications in this simple model. Okay, let me finish and then I'll answer some questions. Okay? What's that? The price for the price of the data is public, so why would an entity comes in? Why should they bid higher than, well, we just proved that they bid their. So that's the point.
00:18:49.094 - 00:19:03.786, Speaker A: If this payment function is the correct payment function, then everybody is incentivized to bid exactly their truth value. It's what they perceive as the value of the data to their function, and they will be the exact value, but.
00:19:03.810 - 00:19:09.794, Speaker B: They know they're not going to get it unless it exceeds the published, the.
00:19:09.834 - 00:19:27.134, Speaker A: Maximum utility, the maximum utility for them of the bid equals to their valuation. So whether they know or not, this is the maximum utility, this is how much you get the maximum value out of there. Let me, let me keep going.
00:19:27.174 - 00:19:35.886, Speaker B: And your picture appears to about some, I don't know, properties of combined payments.
00:19:35.910 - 00:20:27.332, Speaker A: So I'm coming to some of the properties that I need for computation. I'm coming to that. Let me get there. Okay, so, okay, so the myosin's payment is the incremental value. The first statement is that if you use this payment function, then in fact the allocation is equal to the valuation, which means that you're not incentivized to game the system. So that's the first observation about this particular question. Now the question is, now that payment has been collected, you bid your valuation, the payment has been collected, how is it divided fairly among the different players? And this is coming back to the idea of how much value does each data set add, add to the precision of the prediction task.
00:20:27.332 - 00:21:28.674, Speaker A: So you get into this sort of question of the shapely allocation, almost like an allocation game. You have to see, if you allocated nk elements to this particular buyer, what is the value if you take each one at a time and each subset at a time, and you get this exponential growth and checking all possible combinations. Okay, so that's, the second thing is, okay, now what does fairness mean? First of all, before I get to the description of the algorithm, the first thing is that we want to make sure that whatever you collected is given to the sellers, right? So that there is no percentage of that is taken to the market. Now that one can argue that maybe the market should be paid in a certain way, and we haven't put that in the model. Second thing is, if two independent people provide similar data sets with similar contribution, they need to be paid the same amount more. So if you have a data set that doesn't add value, you should get zero allocation and so forth. This is the notions of fairness.
00:21:28.674 - 00:21:56.348, Speaker A: And what happens is that the shapely allocation actually guarantees the fairness. It's not a problem. This is well known. Shapely allocation will guarantee the fairness. Now the observation we made over here is that actually the shape, the allocation, as it's written, it's written in terms of an expectation over a uniform distribution. And so, in general, you don't have to compute this allocation in an exponentially. You don't have to look at all possibilities.
00:21:56.348 - 00:22:31.124, Speaker A: You can just sample linearly on the set of permutations, and you will get arbitrarily close to the allocation that you want. And you can actually write down the concentration so you can get bounds of how well you do if you take, say, a uniform distribution over k steps. Okay, so that's another observation. I think it has to be, although we didn't find it anywhere, I would imagine this probably already exists in the literature. So the shape reallocation works. It's fair, but it has one problem, and the problem is back to the replication issue. Okay, so here is the situation that you have.
00:22:31.124 - 00:23:12.104, Speaker A: If two sellers, a and b, provide the same value for the prediction, they actually will get allocated equal amount of money because they provided the same value according to the shape allocation. So then one of the firms will say, look, okay, now I understand. Maybe I will come in as a fake company with the same data set, okay? And now I will get better allocation because now the three of us are allocated same value. So I get one third, one third. And so then, and then that means that I can get more profit. And that means that the gaming is happening by the sellers, not by the buyers. That's maybe the question you asked.
00:23:12.104 - 00:24:03.012, Speaker A: Okay? And that's a hard problem. And the response to that problem that we have, and I think, you know, potentially reasonable, is that you have to now define some sort of a distance between data sets, okay? And penalize, actually penalize data sets that are close to each other, okay? So you have some information distance between them. And then what you do for the allocation, you actually penalize by the sum of these distances. Okay? That turns out to be actually fairly efficient. So what you do, you take the allocation that comes from shapely, and then you scale it down by the distance between different data sets. And then that lowers the allocation, but it prevents the problem from happening so quickly and pictorially. What happens when a and b are identical and you have the scaling? You get something like one over two e.
00:24:03.012 - 00:24:46.346, Speaker A: But then if you replicate now, you get one third e squared, and that is actually, in sum, less what you would have got earlier, and you're not incentivized to replicate. Now there's a problem with this though, and the problem is it doesn't have the balance. The sum of these allocations does not equal to one, because you scaled everything. So now either the market has to pocket the difference, which can happen in a lot of markets. Okay? Or that we have to figure out how to come up with a new scheme for which you can also have balance. And this particular furnace. Well, it turns out we proved the following result, that this is impossible, that you cannot actually do the robustness and balance simultaneously.
00:24:46.346 - 00:25:28.182, Speaker A: Then, in fact, one has to give in, and then there's a trade off between the two. What's the right trade off and how can you get the maximum? I would say robustness, minimizing the balance of the offset. Still an open question. We don't know how to answer that question, but we definitely know that there is a trade off because both of them cannot be satisfied at the same time. Another sort of interesting downside to these things is that now, even though a and b were not a replication of a, the fact that they're similar, both are discounted. So similar data sets get discounted. Different data sets don't get discounted as much.
00:25:28.182 - 00:26:05.900, Speaker A: Okay. And so that means that the market naturally is rewarding people that come in with different data. Okay. That could be a problem in the sense that, okay, people are sort of feeling that, okay, it wasn't my fault that I'm similar to his data set or her data set, but at the same time, the market naturally wants to see newer ideas or newer data sets that can actually imply, or imply better accuracy in depth. So this particular scheme actually kind of discounts people that are similar to each other. Okay, last piece is the revenue maximization. So now we've done all of this, right.
00:26:05.900 - 00:26:46.564, Speaker A: The market has computed the accuracy from the accuracy. It computed the price that the buyers have to pay, took the price, divided it by the sellers, and now the new buyer is going to come in. When they come in, we have to update the price vector. Update of the price vector is solving a maximization or problem of revenue, a revenue defined by the Morrison's function. Those functions are complicated, and the price vector is a vector of prices. So there's a combinatorial effect of trying to figure out how to increase each one of these components. So you can either use multiplication algorithms, multiplicative algorithms, or bandit problems, which we heard about before.
00:26:46.564 - 00:27:36.426, Speaker A: All of them have pros and cons, so to speak. Right? Because the problems are really complicated. They're exponential and complexity. And what you start looking for is additional structure that can simplify this problem. Okay? It turns out that it's actually, interestingly enough, in the Meissen's payment function, which is sort of what I described earlier. If you actually assume that the gain function g, little g, not big g, sorry, is submodular, then actually you get major simplification. So the reason this is actually, if g is submodular and the payment function has this form, it looks very much like lavage extension theorem, which tells you that if you can write a function in terms of the difference of sub modular functions, then that function is convex.
00:27:36.426 - 00:28:29.962, Speaker A: I'm not giving you all the details, but that extension theorem is actually very powerful. So if in fact g is submodular, then the resulting optimization problem can be made convex. Now, this is complicated to guarantee that the myosin's payment is a submodular function, because it depends not only in g, g is fairly trivial. It's like Rm's or very simple, but it's the allocation and the algorithm that computes the improvement that dictates whether you have submodularity or not. You can come up with interesting examples and counter examples of reasonable machine learning algorithms that are sub modular, not submodular. So we're spending quite a bit of time trying to understand how to model this piece for some of the examples that I mentioned before, namely the retail example. But if that is the case, then actually the problem becomes an online convex optimization problem, which simplifies the algorithm tremendously so quickly.
00:28:29.962 - 00:29:11.762, Speaker A: And I know I'm, I'm out of time. I have a minute. I wanted to say that what we are doing right now is, of course, we're trying to extend to the flip side, which I think brings up different kind of EVA left. I mean, sort of the flip side of it, which is when the buyers are there altogether, and then there's externality to the buyers and competition between the buyers. So that kind of brings in other complexities to this particular market, and then how to piece the two together. So we're doing that, but at the same time, we're building sort of prototypes for these applications, in particular, one for the retail and the other one for the energy market. These are markets that will sit under the actual market.
00:29:11.762 - 00:29:57.654, Speaker A: I mean, there are markets on top of them, and these will sit under those markets, providing the correct prediction and the correct supply chain necessary to run the top market. So, trying to model the different pieces of what would you take as a cost function, what would you take as a machine learning algorithm? What are the prediction tasks that people will come up with? Now, as you do this, you regulate the way data is being traded and value of prediction that people are getting, you begin to get some structure for understanding. For example, are there certain ways in which these bids behave or the valuations behave? And we have some ideas about how to do that as well. Anyway, I spoke too much. I stopped here. Thank you.
00:30:02.434 - 00:30:04.626, Speaker C: We have one question about the second speaker.
00:30:04.730 - 00:30:05.414, Speaker A: Yeah.
00:30:07.914 - 00:30:11.530, Speaker B: Reasonable thing about movies, online movies as data.
00:30:11.682 - 00:30:40.074, Speaker A: But then the recommendation systems are saying, oh, it's like the movie you saw, not, they don't exaggerate the difference. Yes, kind of. Yeah. Yeah. What is different about the movies a little bit is that it's not clear by buying a movie what prediction or what sort of value I'm getting other than entertainment. It gets very complicated. Trying to even understand my valuation seller tries to appeal.
00:30:40.074 - 00:31:02.044, Speaker A: Yeah. So the modeling piece of my interest in the movie, how much I'm willing to pay for, is going to be complex, but it is interesting to think about it that way. Why not? I mean, it's the same thing. Pricing the movie should also depend on these kinds of markets. In a way. Anytime there's a digital good, there should be a market that decides how you pay the price for. It's just that the modeling becomes harder.
00:31:02.044 - 00:31:05.524, Speaker A: It's a good question. Yeah. Yeah.
00:31:08.144 - 00:31:20.048, Speaker C: Since you're designing a market, is there any hope to come up with some metrics of market power in data that would similar to other markets?
00:31:20.136 - 00:31:39.644, Speaker A: Yeah, I think so. I think so. I don't have the answer. I think so. It does look like you could probably have market power. Data companies probably can play that role. It's good to understand how much you can actually impact the market by certain types of data that you have or access to data that you have.
00:31:39.644 - 00:31:43.564, Speaker A: Yeah. Speaker.
