00:00:00.160 - 00:00:10.634, Speaker A: If it is about qualities, I add to it the next sentence. But every single value I've learned from Peter.
00:00:11.094 - 00:00:12.714, Speaker B: Are we supposed to read that?
00:00:14.014 - 00:00:17.394, Speaker A: You know the rest? Yes.
00:00:19.494 - 00:00:20.394, Speaker C: Okay.
00:00:21.214 - 00:00:46.344, Speaker A: Few pictures, I guess. Three circles of families. Starting with very young Peter. Somewhat older, still very young Peter. But in the second circle. And then the third circle, which is academic. So David Blackwell and Eric Lemon.
00:00:46.344 - 00:01:22.804, Speaker A: And the younger generation, which I just pick random pictures that I had in one home. So it was simpler to put anyhow. So this is the background. A few things about this talk. Which is too small probably to read. And this is a problem. And the subject of the talk would be statistical testing.
00:01:22.804 - 00:01:53.876, Speaker A: It is a historical account, so it's not a research paper. Historical account of a work done during 60 years. From his PhD to unpublished recent paper. The common theme is non parametric assumptions. At least under the alternatives. Saying history about Peter is slightly understatement. Because Peter is making history.
00:01:53.876 - 00:02:27.356, Speaker A: So we have to take this in mind. I was surprised to the level of uniformity that I found in these papers. The sequence would be chronological, but there is no. Almost no order. I want to make a point that this is not a history of testing. Nor it is intellectual history of Peter. But a small intersection of the two.
00:02:27.356 - 00:03:09.794, Speaker A: That is Peter and testing, which is not a main topic for Peter. And the main. In a nutshell, what I see is this basic tension that we have. And I guess Peter has that statistic. Testing is many fields. For example, in the field that brought Peter to statistics, ecology, testing is much more important than history. Strictly more important, for example, because what is now is called transfer learning.
00:03:09.794 - 00:03:30.898, Speaker A: So if you do experimental psychology, your issue is transfer learning. And estimation doesn't transfer easily. Testing much more easier. And therefore the focus should be on testing. Because testing can be generalized. Estimation values are not generalized. They are function of the first year students.
00:03:30.898 - 00:04:05.150, Speaker A: Which used to be the research sample in the history or the Turks of Amazon. Currently they are not the general population. So making estimation is not something that makes any sense. Moreover, everything is in the context of experiment. And nobody cares about this particular manipulation. We want to generalize, and this is testing. However, we are used to thinking about optimalities.
00:04:05.150 - 00:04:48.533, Speaker A: That is what we are trying to do. Optimality, minimax efficiency. And we don't have a notion of the concept of optimality. In the testing final, the talk would be, as I see it, in a c minor, relatively undertoned. I leave, I guess, the d major finale with Crescendo and Tutti. And all this to the last talk. Which probably will be delivered okay, but I'm supposed to celebrate.
00:04:48.533 - 00:04:52.214, Speaker A: So there would be some music.
00:04:53.594 - 00:04:54.334, Speaker C: And.
00:05:00.554 - 00:06:13.906, Speaker A: So I try to look on the work of Peter, on the 60 years of work. This is not psyched picture. This is the most satisfaction outside statistics. I didn't too much. So I look by you. This is the most artistic over you. This is what you hear in the background is a concerto for something.
00:06:13.906 - 00:07:21.854, Speaker A: And what is a something the young generation doesn't. Okay, there would be more. So let's start with a little known paper of Peter from 2000 called non in semiprometric statistics, compare and contrasted.
00:07:25.374 - 00:07:25.854, Speaker C: Which.
00:07:25.934 - 00:09:03.444, Speaker A: Tries to understand or to make a point, what is nonparametric and what is not? And in particular, that the word nonparametric is used in two orthogonal ways in statistics. So there are two layers of two types of nonparametric statistics. One of them is the older version, which is dealing with things, what is usually called now nonparametric statistics. I think the one is a sense of non parametric statistics that would be used in at least one of the afternoon talks, is something that is connected with density estimation regression, non parametric, so called non parametric regression. The main point that this procedure was developed not so far with the notions of, of syntactics, distribution, inference, et cetera. But there's something like consistency, no confidence, real confidence bounds and p values. And the central point is a sentence in blue that says for VJ.
00:09:03.444 - 00:09:56.424, Speaker A: However, practically speaking, the techniques are mainly used as exploratory tools and compared with their visual effects, visual results. This were not Peter's words. It means that he sees for the first time this today. I'm not sure he saw them before. What you wrote, the red sentence in the red sentence, say asymptotic approximation, were developed, but the criteria of optimality, such as integrated mean square arrow, were and are not driving force. Rather, the development of interesting, pretty pictures for exploratory analysis is per month. And the editor didn't agree to publish this sentence.
00:09:57.764 - 00:09:58.544, Speaker C: Yeah.
00:10:01.204 - 00:10:06.216, Speaker A: So this sentence was supposed to be changed and it was changed to the blue.
00:10:06.400 - 00:10:07.844, Speaker B: What journal was that?
00:10:10.264 - 00:10:12.484, Speaker A: Statistical. What is this?
00:10:17.584 - 00:10:21.524, Speaker B: Inference and planning sp?
00:10:24.024 - 00:11:47.904, Speaker A: Yeah, the editor didn't agree officially because they referred. I don't believe that there was a referee in the middle, but the compromise was a blue one. But Peter had nothing to do with this compromise. The older sense of non parametric statistics is things like rank test, things where you want to guarantee p values for non parametric failure. And the driving force of all this work was classical inference, p value testing. Here you have the notion of a robustness, which is part of this package, part of this trend of non parametric statistics, in the inference sense of the word? No pretty pictures. Semi parametric is sort of trying to look on sort of non parametric problems, but with inf, strongly with emphasis on inference.
00:11:47.904 - 00:12:23.024, Speaker A: Square root of n rate optimality, bootstrap would appear, which is the main topic of Peter, is a combination of this concept of non parametric. It started with non parametric maximum likelihood estimator, with the idea of being able to plug in. And it is strongly related to a permutation test, of course. And the whole idea is to do inference.
00:12:25.204 - 00:12:26.104, Speaker C: Okay.
00:12:41.984 - 00:12:44.564, Speaker A: So we have to continue with music.
00:12:58.024 - 00:13:01.324, Speaker B: So again, trying to figure out what are.
00:13:58.554 - 00:13:59.294, Speaker C: It.
00:14:48.614 - 00:15:35.220, Speaker A: Thanks to the University of Michigan, Washington. Okay, the early stage local alternative. This is the PhD thesis, or the paper, a paper based on the thesis, some asymptotically nonparametric competitors of the hotel. Length t square. So I'll be very far side. Many years ago there was, I think it was a talk by Prokhorov in Berkeley, the International Congress of Mathematicians, but it was one of the Russians. And the talk was given in Russian with the interpreter.
00:15:35.220 - 00:16:16.094, Speaker A: And then Peter described is that the second slide was long with a lot of details. The speaker talk about it ten minutes. And then the translator said, assume that it is Prokhorov. Mister Prokhorov presented something that we all know so we can move on. And that was the last slide I could follow. So the paper starts with basically a translation model, multivariate translation model. You have an estimator of theta.
00:16:16.094 - 00:17:07.438, Speaker A: X is vector, you know, P. Theta is location vector. It is based on basically vectorizing parameter, real estimators, invariant estimators. So you have basically, typically the structure is something like. Something like h of the cancellation, which is mu n, let's say zero, but not necessarily, and which is the expectation. And you try to equate it. Syntotic is based.
00:17:07.438 - 00:18:12.854, Speaker A: Everything is under local alternative is gaussian, the shift which is proportional to the alternative. So we are looking on local alternatives, which is the basic notion of all this work, which is theta n. Now, its vector is one over square root of n a. And we have both of the estimator, two types of synthesis, which are strongly related, one of them the distribution of age under the alternative, this estimating equation. So this is estimating equation, its behavior or the estimator. Both of them are normal with shift. And we have two types of tests which are strongly related.
00:18:12.854 - 00:19:09.988, Speaker A: One of them is based on the estimator and one of them on the estimating equation. And the tests are as can be expected. So everything is derived. There is example, we are speaking about the 60, so the mean, the Wilkopson based on work, ipson test, the median. And then we come to efficiency, because we are dealing with mathematical statistics. So there is a notion of Pitman efficiency, which was per month in the work of testing, which says that you compare the sample size needed to get to achieve the same asymptotics. And we get a sort of results like that we have.
00:19:09.988 - 00:19:25.014, Speaker A: And the point is that the left hand side and the right hand side are typically different. And so everything is analyzed.
00:19:26.514 - 00:19:27.254, Speaker C: And.
00:19:29.914 - 00:20:19.924, Speaker A: The conclusion is the following. And there are two conclusions. First is technical workshop and the medium based test are more robust than something that is based on the mean. But to my talk, more important is undeveloped statement, say undeveloped, because that's all is written about it. Section four also points up to the need of satisfactory criterion. What is meant by better, independent of delta. What is meant by better? What is the better statistics? The problem is that better statistics depends on the alternatives.
00:20:19.924 - 00:21:10.694, Speaker A: And there are a lot of them. And that's the main problem of testing, that the best statistics is depending what you think about the alternatives. And this is wild forest. And therefore, we don't have a satisfactory notion of statistics. There is another sentence in the middle that says there always exists a direction in which one does worse than in one dimension, whatever estimate one chooses. And necessarily because we compare two things, one in which one does better. So directions are the best statistics depend on the direction, and you don't know the direction.
00:21:10.694 - 00:21:54.654, Speaker A: Not much later than that, we have an implication. And this is sort of typical. It is work done with a shell test for monotone failure rate based on normalized spacing. So the model is that we have a distribution with failure rate and age, not, say, f is exponential. So it's very, very much parametric. And age, one is the failure rate is one.
00:21:56.754 - 00:21:57.474, Speaker C: How?
00:21:57.634 - 00:22:10.254, Speaker A: I don't know. And that's again, the main problem. The alternative is non parametric. Completely nonpar. Almost completely non parametric. Yeah, it's non parametric. We will not semi parametric.
00:22:10.254 - 00:22:59.714, Speaker A: Well, they started with a set of tests based on other statistics, which are just x's. Look on the different, normalize them properly. And if it's exponential, this should be IAD. So we look on the rank of them, the rank of the spacings. And under h zero, the spacings are exponential. And under h one, there are stochastic monotone decreasing. And therefore it makes sense to look on tests which are in some sense monotone in the spacing.
00:23:01.454 - 00:23:02.194, Speaker C: And.
00:23:04.694 - 00:23:19.704, Speaker A: The result of the paper says that all monotone tests are ranked tests. If we want to be pure monotone, that we have to do rank test. All monotone tests are unbiased that expected.
00:23:21.684 - 00:23:22.424, Speaker C: And.
00:23:27.044 - 00:24:51.346, Speaker A: One type of a monotone test is one that look on the rankings. Some function implies some function of it, weight it. And in particular, the motivation of the paper seems to be a test suggested by so it's some of the I and they say two things about it. The first thing is that is not efficient. And here we can say because it's dominated by another test, which is always better. And here is a. Again, I'm not going to enter into all the detail to leave enough time for music and the main that then they try to analyze and what they claim that any test of this sort, if x is exponential with mean one or with non lambda, it doesn't matter because we can always say normalize, then the efficiency.
00:24:51.346 - 00:26:10.358, Speaker A: All tests of this form are non efficient. So that's the fact that it is sort of non interesting results. So it's very strong results that they were able to prove, because if you look on alternatives for any alternative, any direction, you always can improve it. But if you drop the assumption that you know the scale, then suddenly all the standard type of tests, in particular the one with optimal c and J, are efficient relative to other tests. But still, if you know, you think about every test would be efficient relative to a specific alternative. So this is really in the paper, second part of the paper now authors by Peter alone say the classes are all the statistics of this form. When you have to normalize because you don't know the scale, all linear function of the spacing locally.
00:26:10.358 - 00:27:16.904, Speaker A: Most powerful tests, based on the ranks, or in particular way on the ranks, all of them are asymptotically equivalent to the most powerful test for a given direction. Problem is that you have to know the given direction. Similar problem is in the following shortly thereafter. No, it's more than one year, it's ten years. It's looking on taking the walk and suggest a test for how well a model fits, fits the assumption, which is uniform variance and nonlinearity. I'll talk only on the variance part of the story.
00:27:18.604 - 00:27:19.344, Speaker C: And.
00:27:24.324 - 00:28:32.804, Speaker A: So it is a basic linear model. The model that we consider is that the variance depends on the expected value conditional expectation. So tau is the conditional expectation of y given x, the linear part of it. And we consider the local alternatives, that is, h zero is that sigma is one. But it may departure proportional to theta in a direction a. For example, sigma can be e to the power of theta tau, which is the deviation idea. In this model, there was an estimator, there was a test, and that was proven under condition somewhat difficult to make precise.
00:28:32.804 - 00:28:39.404, Speaker A: If, which is very polite way, I would probably put it in a stronger statement.
00:28:40.104 - 00:28:40.844, Speaker C: And.
00:28:43.344 - 00:29:40.574, Speaker A: So a test was suggested, and we get our test statistics. So, the first part of the paper is taking this test suggested test and making the condition precise and the proof proofs real. And then we move to generalize this guy. But now we need complicated conditions and we need long proofs. And the resulting tests are asymptotically unbiased. The power is independent of the method of estimating the parameters, which is, again, something that would appear again. And again, there is explicit theorem for the efficiency or the relative efficiency of the test.
00:29:40.574 - 00:30:10.274, Speaker A: We can choose b to make things robust. But if we, the solution is known as expected, it be the function that appear here is explicit. And now we are left what is a different a's would give us different.
00:30:10.654 - 00:30:11.394, Speaker C: Test.
00:30:13.714 - 00:30:31.334, Speaker A: Which would be given, and we don't know a. That's the whole idea of our problem with testing, that we don't know the alternatives. We have nonparametric alternatives, and a different alternative would give us a different test.
00:30:45.934 - 00:30:46.794, Speaker C: Okay.
00:30:48.974 - 00:31:34.776, Speaker A: One thing about the graphics, all the graphic except one was done by me. I use Google scholar, except for one tiny point, which is the name of Tobias Ryden, which was chaotic completely, and it covered all the screen. I didn't touch anything. Everything is automatically. And it was. I know a few points that I would correct if I want, but I insisted of just taking the automatic results not to enter into biases. Okay, so, one thing that knowing PTO think was very important to him was coursers.
00:31:34.776 - 00:31:46.004, Speaker A: So, this is a list of the courses. People walk with it. And do you imagine how long it is? How many courses? Give me a number.
00:31:54.424 - 00:31:56.124, Speaker B: How many numbers would it take?
00:35:06.783 - 00:35:52.284, Speaker A: Cincinnati pops with japanese 105 millimeter ray guns. Military guns, just not the ones that you have here after a touchdown. A only directional test. Okay, so, I mean, looking now for 20. Paper by Alice Salia, Peter and Thomas Tocquer. Goodness of feed. Test for kernel regression, multiplication to option implied volatilities.
00:35:52.284 - 00:36:46.784, Speaker A: They care about the options part of it, but the rest, yes. So, the issue is completely non parametric with w, nrp, vrq y real. And we look on the nonparametric expectation of y given w and v. And the edge note is that expectation of y doesn't depend on v. So the idea of the paper is in theory very simple. Let's look on the nonparametric kernel estimator of the two expectations. The one expectation of y given w and v, and the one and the other.
00:36:46.784 - 00:37:05.784, Speaker A: The other one is given only w with age node. Let's say that the two are equal, h one that they are not equal. So we look on kernel stimulus.
00:37:08.424 - 00:37:08.712, Speaker C: We.
00:37:08.728 - 00:37:40.472, Speaker A: Are looking in high dimension. Canal estimators in high dimension is catastrophic. Here it is small because we know we have to compare test. It's really nasty job. The assumptions are nasty. The kernel should have r, which is larger than three p plus q, which are the dimensions. We are talking about three quarters of the sum of the dimension zero moments.
00:37:40.472 - 00:38:13.624, Speaker A: So it's non standard estimator. We have to keep the bias. That's the main problem. Everything we try to look on the numbers, it's okay. It's not parametric in the worst case scenario, but under. The result is that under age nodes that the statistic is asymptotically normal with the given too complemented for me mean invariance. But it appears in the paper.
00:38:13.624 - 00:39:08.444, Speaker A: So the next stage of the paper that I saved myself the trouble of looking is complicated estimator of the too complicated mean invariants which will return to it. But now we are looking again our story, looking under alternatives. That's the main idea. And the test has non trivial power. If epsilon is very large, because you have age here, you think you probably are larger than one, and then therefore it's very slow. So that's what I said. You want to be omnidirectional.
00:39:08.444 - 00:39:37.784, Speaker A: The price for being omnidirectional is not. You are not omnipotent. You are omnipotent. You don't have power in any direction. That is the main conclusion. We'll return to this model in the last slide, I guess, of the talk. How much do we have? Okay, so if we are talking about.
00:39:37.784 - 00:39:50.232, Speaker A: So now another look on the crosses.
00:39:50.288 - 00:39:52.328, Speaker B: Because crosses are what really important.
00:39:52.416 - 00:39:53.204, Speaker C: I guess.
00:40:49.144 - 00:40:50.324, Speaker B: I have.
00:41:00.944 - 00:41:01.764, Speaker C: Breath.
00:41:04.244 - 00:42:09.168, Speaker A: Okay, and we are coming to the bootstrap and I will not explain the title. Okay, as we said, the bootstrap is the plugin estimator. You want to estimate parameter of the distribution of a statistics tn under the null under the true distribution. And you consider the distribution of tn under the empirical distribution or any other estimator. But the standard bootstrap. What people usually understand by the term bootstrap is under the empirical. It works when the law of t given the two distribution is continuous in its sum neighborhood of nn, which is consistent with empirical.
00:42:09.168 - 00:42:37.744, Speaker A: That means the empirical is close to the two enough to have this continuity of the distribution. And therefore the distribution of t under f can be estimated by plugin, plugging in the empirical. The m out of n bootstrap was the short basic already in the first paper of people, Bickel and Friedman, which is almost, I guess, the first real paper about.
00:42:39.484 - 00:42:40.544, Speaker C: Bootstrap.
00:42:41.244 - 00:43:47.404, Speaker A: So we know the one that suggested the bootstrap, but the ones that analyze the bootstrap. It was mentioned then, because problem with situation where fm doesn't belong to this neighborhood, it's not close enough to the truth for having this continuity, but only we need to have much more precise estimate. Also I played it between the two. So usually you look at it in the opposite direction. Fn belong to the relevant neighborhood under m sample of size. Nice. The example there was a typical example, is the maximum of the uniform distribution.
00:43:47.404 - 00:44:46.854, Speaker A: The problem there is that there are anomalies because of ties, for example. So we know that the syntactic distribution of the maximum is after normalization, exponential continuous. But if we do bootstrap, it will be very discrete. Okay, this is when we usually talk about sort of confidence interval. The story in testing is completely different, because in any or almost non trivial case, almost all non trivial cases, fn is not, doesn't belong to h node. We want to know the estimate is the behavior of the estimator under h node. And the empirical doesn't belong to them.
00:44:46.854 - 00:45:03.214, Speaker A: So I could take many papers that are dealing with this. This is a huge topic in PthLC portfolio. I picked one of them.
00:45:04.234 - 00:45:04.974, Speaker C: And.
00:45:08.234 - 00:46:20.074, Speaker A: The main idea is that we have, we have some statistics that they can tell us. We have to look on the distribution under m smaller than n. And if we have the standard asymptotic, the square root of n asymptotics, and we center properly, everything will work. I not enter into the old details. As the translator said, we all know it, we all know them, so we cannot, we don't have to mention it. And under a few particular, trivial, very general structure, that there is really jack of all trades, the m out of n test work with true power function and continuous alternatives. So it gives you what we want.
00:46:20.074 - 00:47:51.454, Speaker A: It is thought something is missing in this description. Because if we assume that everything is standard square root of any asymptotics, why do we need a bootstrap at all? Where, where is the problem? And there is another paper that I want to mention by two advisors. Now, Peter and Yossiyahov and this is under the situation where what we try to estimate, we assume that there is a sort of expansion, and now we can look on the aim out of any bootstrap, but under different aims, and we can find the extrapolate to get the distribution under n. The main focus of the paper is different, is doing it in optimal way because it saves the computation. I didn't mention just because we are speaking about statistics in the big era. In his first paper, he writes that one. The problem with the Hodges and Lehman estimator is that the computation is the level of n squared log n, which is almost impossible.
00:47:51.454 - 00:47:57.294, Speaker A: That's written. You wrote it.
00:48:00.714 - 00:48:01.494, Speaker B: Yeah.
00:48:09.774 - 00:48:29.954, Speaker A: So that's the basic idea. And we are coming to the final chapter or not the final chapter. The current chapter. Sorry, sorry, sorry, sorry. The current chapter. And this is archive paper. Correlation with stellar extremal properties.
00:48:29.954 - 00:49:38.306, Speaker A: So it started by a work of. So they try to find non parametric test for b variete distribution with xy. And we want to basically test whether x and y are independent. So the test is based on take the X's, order them, rank the y's and rank them. Give Ri to the ranks of the y to be the rank of the yi. If the x and y are independent, of course they are independent. If they are not, then if Ri is large, we expect Ri plus one, which is the next close to it would be close to it, the next y.
00:49:38.306 - 00:50:33.704, Speaker A: The y of the following X would be almost as large. And therefore we expect this to be be a test. So Cn is converting to Cxi, which was suggested by date, in fact. And C has a property that it is between zero and one c zero only. If x is orthogonal to y, independent of y, it is one. If y is the precise function of a of a x. C is asymmetric in y in x, so cxy is not cy of x.
00:50:33.704 - 00:51:23.504, Speaker A: Let's here copy and paste the g should be. Let's sort of ignore this line. It is strongly related to the classical symmetric maximal correlation of rna, which is symmetric, which look on the supernum over r and age of the correlation between function of x and function of y. R square is a maximizing value of the self adjoint operator. Conditional expectation given y, condition expectation given x, or vice versa.
00:51:28.164 - 00:51:28.972, Speaker C: It's symmetric.
00:51:29.028 - 00:51:50.204, Speaker A: It's nice. It has some anomalies. That is, you can make r go to one, while c go to zero, because just we pick a point and then. So it's not much better. It's more. There are reason to prefer one on the other. But that's theoretical.
00:51:50.204 - 00:52:45.784, Speaker A: Took me time to read what I wrote. The problem is that it is Hebrew to fasta, mirrobelu to fasta. And if you have size a lot, you have not sized at all. I guess the efficiency of this model would be zero under any contiguous alternative. There's the same problem as we saw in the econometrics paper. When you try to be omnidirectional, you get zero efficiency for any direction. So what can be done in.
00:52:48.364 - 00:52:49.104, Speaker C: Here?
00:52:50.044 - 00:53:57.154, Speaker A: The paper returned to idea in 2000. Paper, older paper from 2006, tailor made tests for goodness to fit in semiperative hypothesis, which dealt with basically with a same problem, with same, almost same conclusions. And look on, basically look on the model we have here. We look on a parametric model. So we look on all class of parametric model, like that, which under zenal, theta equals zero would give us independence. We create some correlation between the two, but we permit. Basically we look on a but, so we don't know the transformation of the x and the transformation of y.
00:53:57.154 - 00:54:52.722, Speaker A: We can create the tangent spaces for theta one, theta two, etcetera do the projection, and eventually we get. The projection is all function of x and y, which has mean zero conditional expectation under conditional expectation under x zero conditional expectation under y equals zero. So we just take any function and we have to normalize it. It's co parallel, so it's under the uniform, it's relatively simple. And we get, for any particular test, we get the log likelihood standard test. So we get this expression, so we now can combine them. And this is the old, this is zero in expectation, only under the null.
00:54:52.722 - 00:55:21.634, Speaker A: All of them would be zero, only under the null. So each one of whatever is the true alternative. It would have some power. One of them would have some power. Now we sum them, because otherwise, because we have infinite of them, so we have to temper them. That's the title, if you remember the title, the well tempered check of all trades. So that's the tempering.
00:55:21.634 - 00:56:07.274, Speaker A: And we get a test. We can do this like that. We can do it like Smirnov, look on the maximum, whatever. I don't say that this is, and we don't say this is the only way to combine them together. But the basic idea was, and this is in the last, as I said, the last topic of Peter, or at least the last paper on the topic of Peter that I know about, is the idea of how you can combine, test, and get something which is, which has power against all alternatives, and more power against the more interesting alternatives. And you have to decide what is more interesting. Okay.
00:56:07.274 - 00:56:33.974, Speaker A: And finish the celebration. Okay. So I took two approaches. So the list of all. Also, I now want to what I think should be the right approach.
00:56:34.554 - 00:57:08.474, Speaker C: My point of view. Should all the queen dance be forgotten and never brought to mind. Should old acquaintance be forgotten. And days of old lang syne poor old lang syne, my dear poor old lang syne. We'll take a cup of kindness yet. Four days of all language.
00:57:10.854 - 00:57:18.114, Speaker A: One big forest with Russell. All individuals. Okay, thank you.
00:57:37.064 - 00:57:44.168, Speaker B: So I have a question. What is the role of testing in.
00:57:44.176 - 00:57:48.844, Speaker A: The big data era when you have millions of observations and thousands of everywhere?
00:57:56.094 - 00:57:56.994, Speaker C: Okay.
00:57:57.494 - 00:58:03.434, Speaker B: First of all, my little experience.
00:58:07.174 - 00:58:07.774, Speaker C: Is.
00:58:07.894 - 00:58:09.314, Speaker A: That you never have.
00:58:11.854 - 00:58:34.050, Speaker B: You never have anything. You always have very little. Because if you have a lot big, large number of data points, your questions are becoming very precise. You don't collect millions of data points to ask questions which are you can.
00:58:34.082 - 00:58:35.734, Speaker A: Answer by 100 points.
00:58:36.914 - 00:58:44.550, Speaker B: So my personal experience to give you a new field, I have all called.
00:58:44.622 - 00:58:47.030, Speaker A: To a call center of middle sized.
00:58:47.102 - 00:58:52.634, Speaker B: American bank for half a year. And I tried to compute some.
00:58:55.774 - 00:58:56.994, Speaker A: So I did simulation.
00:58:58.934 - 00:59:00.206, Speaker B: The simulation worked fine.
00:59:00.230 - 00:59:04.382, Speaker A: So the theory was nice. The problem is, with all data for.
00:59:04.438 - 00:59:29.806, Speaker B: All calls for middle sized northeastern bank, Melton bank, weren't enough to answer the precise question, which is what happened at 10:00 in the morning for the typical type of testing. So the same questions exist. Now, my main point, as I said.
00:59:29.910 - 00:59:33.182, Speaker A: Is why testing is important, because place.
00:59:33.238 - 00:59:35.994, Speaker B: Is saying testing is what we can do.
00:59:37.334 - 01:00:09.148, Speaker A: And if I'm looking on this surrounding where we collect experiments, experiments are. Experiments are not really working. We want to move. And this is, again, the same problem with medical. There's all this issue of transfer learning. Your sample population is not the general population. And it's very meaningless to ask about estimating parameters because the parameters are not going to generalize the question whether something works or not.
01:00:09.148 - 01:00:14.304, Speaker A: You may hope generalizes, yeah.
01:00:20.484 - 01:00:52.424, Speaker B: Some underlying themes which actually I have to myth or almost all ones which I forgot. But what is the common theme? Continuous alternatives. And the problem of how to deal with. I don't know how to deal with the continuous. I have to try to be optimal, but I will be optimal only in few directions. So either I can say.
01:00:54.404 - 01:01:09.878, Speaker A: 64 in which directions I'm good to combine them, to combine them, etcetera, different directions.
01:01:09.926 - 01:01:13.286, Speaker B: But the thing is, we have nonparametric world out there.
01:01:13.350 - 01:01:15.634, Speaker A: We want to be optimal in all directions.
01:01:19.934 - 01:01:48.308, Speaker B: One topic that doesn't. Doesn't fall into this, which is a bootstrap and that's the reason I use the aparachic title. Because as much as I could say you don't care about the test. You don't care about the test. The test is given for you. You try to see how you can handle it. You don't come with the test.
01:01:48.356 - 01:01:51.944, Speaker A: You don't suggest test. So there's a procedure to find the.
01:02:04.034 - 01:02:09.954, Speaker C: I think this is a discussion we can continue outside. Let's take a 15 minutes break and thank you for his great talk.
