00:00:00.240 - 00:00:49.514, Speaker A: Great. So it is my great pleasure to introduce Jennifer Chase. Jennifer is now associate provost for the dean of data science and information and dean of the School of Information at UC Berkeley. Before joining Berkeley, Jennifer has been, I should say, the legendary co founder and leader of multiple interdisciplinary labs of Microsoft research, including a tiny lab in Israel. Jennifer is very decorated with many awards for both leadership and scientific contributions. So I should say Jennifer is a scientist and a leader and has a lot of vision. So maybe I'll stick to the convention of this workshop and just skip all the awards.
00:00:49.514 - 00:01:27.104, Speaker A: So when I asked Jennifer to give a keynote talk in this event, she replied with what I should have guessed. She would say, let's talk. And then the following day, we are having a Zoom conversation, which starts with my very modest request to Jennifer to talk about some of her work on algorithmic furnace. And within minutes, it ends with a topic that goes way beyond algorithmic furnace and features an amazingly multidisciplinary panel of leading figures in social sciences, computer science, social welfare, sociology, public health, and medicine. So, Jennifer, the floor is yours. Thank you.
00:01:28.524 - 00:02:47.822, Speaker B: Thank you so much. I'm going to talk for a few minutes, and then I'm going to introduce my panel to you, and we'll get started with the panel. So, as Michal said, at first, when I was invited to this a while ago and it was supposed to be a full week, I thought, okay, I'll talk about work in algorithmic fairness, bias and bios, things like that. Many of the computer scientists here have visited my labs that I had at Microsoft for many years, where we had just an incredible group in fairness, accountability, transparency and ethics who worked closely with, with many of the scholars here. But, you know, when I decided to come, why did I decide to come to Berkeley, okay, and leave those wonderful labs? I decided to come to Berkeley because I wanted to go beyond the scholarly pursuits. It sounds strange to say Microsoft has scholarly pursuits, you know, but. But really, a lot of what we were doing were scholarly pursuits.
00:02:47.822 - 00:04:14.494, Speaker B: It was one because we have reality coming towards us all the time in terms of the online world. But the opportunities here are so great. And there are amazing scholars in not only computer science, but also law and how it interacts with technology, like Deirdre Mulligan and Pam Samuelson and others who were here. But when I got here, I realized there was even more than that that we should be thinking about. And I started talking before COVID times. So before COVID and black lives matters with Linda Burton, who's one of our panelists. And, you know, and I wondered, what can we do to get from a lot of the questions we're thinking about in algorithmic fairness to really having the social impact? And that also meant working with the practitioners, the people on the ground, who are dealing with the lack of a just world with the inequities.
00:04:14.494 - 00:05:36.166, Speaker B: So, you know, Berkeley is just so rich in this area. We have a graduate school of education that, you know, is trained over 1000k through twelve teachers who deal with the reality of that system. Linda is the dean of the School of Social Welfare. We train lots of the social workers in this state who deal with, you know, everything that is missing in our social welfare system and the fallout of the fallout of the inequities. We also have a school of public health here which trains our public health workers. And, you know, there had been last fall a paper that Ziad Obermeier, who's one of our panelists, had done with Sandil Mullenthian, and, oh my, I've forgotten his name now, but one of John Kleinberg's students on racial disparities in healthcare. And that came out sometime in the fall.
00:05:36.166 - 00:06:46.494, Speaker B: It was featured in the New York Times, and then we all saw those inequities play out in the way Covid disproportionately affected certain racial groups and economic groups. And so what I really want to do, I feel like we are bringing together the people from these fields and the people who are training the folks in these fields who can help us to understand better what the real problems are. So not have, as I had said to ready yesterday, these drive by collaborations, but, you know, really, really immerse ourselves. But this is very much a work in progress. We are just starting this. And so I would love to have many of you join us in this. Now I will tell you about the members of my panel.
00:06:46.494 - 00:07:41.698, Speaker B: The first is Red Idababy, who many of you heard this morning. Her talk so beautifully set this up. I thought she was going to talk about something more specific in her fairness work, and she really set up the reason why we are trying to do this. So she's a computer scientist in AI and algorithms and social justice and machine learning for, for social good. She's a junior fellow at Harvard, and I am thrilled that a year from now she will be joining us as a faculty member in EECS and as one of the leaders in helping to build this new area here. Okay, next. So ready? Just say hi so that people always see your face if you're not on.
00:07:41.746 - 00:07:42.494, Speaker C: Hi again.
00:07:43.554 - 00:08:11.126, Speaker B: Okay. The next is Linda Burton. Linda is the dean of the School of Social Welfare. She just joined Berkeley a year ago from Duke. She is a professor of sociology and she specializes in family structure and poverty and inequality and in child development. Next. So say hi, Linda, so that you're.
00:08:11.190 - 00:08:12.554, Speaker D: Good morning, everyone.
00:08:13.714 - 00:09:02.383, Speaker B: Okay. Next is John Eason. John is joining us from University of Wisconsin in Madison, where he is an associate professor of sociology and the director of the University of Wisconsin justice lab. Before John was a scholar, he was a community organizer. He organized for state senator Barack Obama way back when. So John has gone from being a practitioner to being a scholar. His work focuses on criminal justice more broadly, on community health and race and punishment and the rural urban divide.
00:09:02.383 - 00:09:04.883, Speaker B: So, John, why don't you say hi?
00:09:07.984 - 00:09:12.360, Speaker E: The space bar is supposed to let you temporarily unmute. It didn't work for me.
00:09:12.432 - 00:10:01.004, Speaker B: Hello. Hi. Next is Marzia Rasami. Marzia is jointly, she's a junior faculty member, an assistant professor jointly in computer science and medicine at University of Toronto. She does machine learning and computational medicine. She holds both a Canada research chair and a CIFAR AI chair. And in addition to doing, you know, healthcare NLP for medical records, she also looks at inequality in healthcare.
00:10:01.004 - 00:10:29.904, Speaker B: And she's actually really pushing me. We are. Oh, yes. One other thing we're doing, which is very exciting, is we are building a joint school with UCSF on machine learning and AI for clinical care. And Marzia is pushing me, I am on a lot of UC wide committees. I'm on the committee, the steering committee for. For governance over healthcare data.
00:10:29.904 - 00:11:31.038, Speaker B: And Marzia is pushing me to try to open up that data so that we can discover what is missing in our healthcare. And then finally we have Ziad Obermeyer. Ziad is an associate professor in the Berkeley School of Public Health. He is actually a medical doctor, an emergency room doctor, although for the last several years, he, on the side, trained himself in machine learning. And he has been. He's one of the leaders in machine learning for healthcare. And I had mentioned his article from last fall, which in a sense predicted what would happen in a pandemic.
00:11:31.038 - 00:12:29.764, Speaker B: I mean, that wasn't the intent of it, but it just played out as a perfect prediction of what he had found. So those are my panelists. What I would like to do is I'd like to get to know each of them a little better. So I would like each of them to spend, you know, a few minutes, five minutes, telling us a little bit about themselves, their background and their work, and why they think it is important to build a community of scholars and practitioners together as we consider fairness and social justice. So in alphabetical order, let's start with. Ready it. Although we heard a lot this morning from her, but maybe just a few more minutes, and then we'll move on to Linda.
00:12:30.944 - 00:13:17.904, Speaker C: Sounds great. Thank you, Jennifer. And thank you, everyone, for sticking around for this panel. I think we started a lot of interesting discussions earlier this morning, and I'm happy that we'll have an opportunity to continue some of those threads. So you saw a bit about my research this morning, so I won't repeat it, but I'll say that I work on a mix of theoretical applications and also on the AI side as well, applications of machine learning. But more than that, what I'm really focused on is helping in issues related to poverty and inequality. Really, I try to work on problems where I start with the problem, and then I see what techniques that it might need.
00:13:17.904 - 00:13:47.610, Speaker C: And that's led me to do sometimes very theoretical stuff and sometimes very applied stuff, and I sort of just work my way backwards. And I think that that's actually something that we as a community could do more of. I think a lot of times we have our set of techniques or tools that we really like, and we sort of develop them, and we maybe use language that makes it seem like the work could be applicable. But then in reality, when we go out there, it's actually like it's not quite the right fit.
00:13:47.642 - 00:13:47.810, Speaker B: Right.
00:13:47.842 - 00:14:29.604, Speaker C: So it's a sort of like, you know, you have your hammer and you're looking for a nail sort of analogy. And I think we really need to flip that on its head and think about what it would look like if we were to start with a domain, really try to understand that domain, know the people in that domain, build relationships, build trust. And I appreciate what Martha said this morning about actually coming in with humility, about, you know, whether you're actually needed at all. It might be the case that. That you're not, and then having that guide, the kind of research that we want to do. So I think that is something that's incredibly important that I hope we'll get to talk more about. So with that, I'll stop, because I got a chance to speak a lot this morning, and I'm excited to see how this conversation evolves.
00:14:30.824 - 00:14:56.270, Speaker B: Okay, so next, my partner in crime, Linda. We are building the theme of human welfare and social justice at Berkeley in our new division, which, by the way, wanted to tell you it's no longer called data science and information. It is computing data science and society. That is the official name of the new division. Okay, Linda.
00:14:56.302 - 00:15:57.032, Speaker D: Well, good morning, everyone. And as I begin my comments, I want to send out a special thank you to Jennifer for her collegial spirit, her just willingness and mission to mentor others and to bring others along, and most importantly, for your friendship. Jennifer, you are amazing, and I'm really happy to know you and to be able to work with you. So it's Jennifer. Well, we're gonna do, we are truly partners in crime, and we are going to do amazing things. I just know that. So, as Jennifer indicated to you, I'm the new dean of the School of Social Welfare at Berkeley, and I come here as the new dean as I'm a sociologist by training, which is often an unusual discipline in terms of leading a school of social welfare.
00:15:57.032 - 00:17:09.800, Speaker D: So what that means is I have the bird's eye view on in my position as dean and as a sociologist on practice. So I get a chance to see what's going on on the ground with individuals in the communities that we serve, as well as the training of our students to be able to do that. But in addition to that, I bring the scholarship along with it as well, and scholarship about social relationships. I'm very theoretically oriented, and I love all kinds of methodological perspectives. In fact, I'm what you might call a big scientist type of sociologist. And what that means is I really thrive on opportunities to do collaborative research, to do interdisciplinary research. I been involved in some of the largest studies, for example, of welfare reform and its impact on families using mixed methods approaches, one of the largest studies in the country on that.
00:17:09.800 - 00:18:07.298, Speaker D: So that's important to keep in mind as we move forward in our initiatives here. But methodologically, I would have to describe myself as an ethnographer. And what that means is I study context, and I look at change over time in individuals and social relationships in institutions. I look at those things over time because I'm not studying families that say, I go to visit them one day and that's it. The longest time I've ever been engaged in studying families on particular studies that I've worked in is seven years. So I get to learn seven years intently of those individuals lives. And in doing so, I pay considerable attention to what people say and how that might look different from what they actually do.
00:18:07.298 - 00:20:30.274, Speaker D: So I'm very curious to see what comes out in our collaborations in terms of how we're able to capture that, the consistency between what people say and what they actually do, because we all know that oftentimes the depth is truly in the details when we look at these particular issues, and that nuances are extremely, extremely important as we think about understanding not only human behavior, but the behavior of institutions as well. What are those nuances that we might not necessarily be able to capture with surveys or certain types of data, but may have really critical explanatory value in terms of us understanding outcomes. So I know one of the first questions dinner proposed to us, and I'll say, deal with this very, very quickly. I wanted to say that in building collaborations, we all have to understand that we bring very sometimes very different currencies to the table, but that all of our currencies have value. And as I look at the currency that we as individuals in social welfare and sociology bring to the table, we currently right now bring a currency that is extremely important in the world order and how we move through that. And that is the social consciousness that we bring on the ground about issues of social justice and fairness, and specifically right now, as we deal with those issues around race. In addition to that, I'll just end with saying, as we move forward in collaboration, some other points that I think are important for us to consider, that you probably have already covered in the conference our language and how we understand the language of our disciplines, and how we relay that to each other and explain it to each other, so that we're able to communicate effectively around the issues that are most important to us.
00:20:30.274 - 00:22:20.684, Speaker D: The framing of the work that we do, what is the theoretical approaches that we use in the work that we do, and how do we use the inductive and deductive will? How do wheel, how do we go back and forth asking questions, addressing questions both inductively and deductively, to come at new perspectives and addressing the issues. It's also important that we don't see each other as add ons to any project, that we're all in and in real time and very dedicated to our projects right at the beginning, that none of us are add ons, that we are also are inclusive, including the communities that we work in, members of the community, in the work that is done, so that they only not help or actually develop some of the questions, research questions that are actually addressed, but that their feedback on whatever answers come from those questions are also incorporated in the interpretation. And finally, that we also look at multiple levels simultaneously. I have always been intrigued by the work that allows us to look at individuals, social relationships, context, institutions and public policy simultaneously to give us a better sense of how those factors might interact with respect to shaping outcomes that are important for the betterment of mankind, so I'll stop right there. And thank you so much.
00:22:26.464 - 00:23:10.500, Speaker E: I'm here, Jennifer. Can you guys hear me? Okay? I want to first thank Jennifer for the. For being invited here. I also want to acknowledge Linda Burton, who was my postdoc mentor at Duke, who gave my research trajectory a whole different bend. From the moment I met her forward, I was able to figure out a very difficult discipline to navigate. So I want to acknowledge and thank both Jennifer and Linda. Linda actually covered several of the topics I was going to hit on.
00:23:10.500 - 00:24:12.652, Speaker E: So I'm going to give a little more personal account of who I am and why my experience is important for understanding algorithms. We all know that six in ten Americans either know someone or has a family member incarcerated or someone who's been incarcerated. For African Americans, it's nine out of ten, long before these reports were produced. I grew up black in America, so I understood this. And this is actually what drove my research agenda and drove me to be a community organizer. Actually started organizing on the south side of Chicago at the age of 22, hoping to reform housing and the criminal justice system. What I found was that at that period, it was difficult to go beyond what was just on the ground.
00:24:12.652 - 00:24:55.542, Speaker E: So I began asking questions in terms of the small research actions we were doing that led me into grad school about why we were producing so many people, partly why we were sending so many people to prison. And the answer was simple. Most people said, oh, it's an economic answer. It's about jobs. And in fact, I start my book, big House on the prairie. Rise of the rural ghetto and prison proliferation with a community action and a reflection on some of the work we were doing at that time. Where we were closing drug houses on the south side of Chicago.
00:24:55.542 - 00:26:10.468, Speaker E: And at the end of one of these marches where we had a police commander agree to shut down one of the drug houses. This is about our fifth. At the time, an older gentleman stated that we were simply sending our black children from our neighborhoods downstate in Illinois for white workers to get jobs. This is what inspired me to go back to school to figure this puzzle out. Was this in fact true? And I began collecting data on every prison ever constructed in the US. I now have the largest and most complete data set through the prison proliferation project, where I know where every prison is and when it opened, all 1663 facilities. I got so interested in this that I took my wife and two small children to rural Arkansas to study the place called Forest City and reconstruct their social history as to why they built a prison.
00:26:10.468 - 00:27:14.614, Speaker E: Now, what does any of this have to do with big data, you'd ask? Well, I'm a multi method researcher, and the, what some people may call me search, that's researching yourself in a way, or using qualitative methods which seem less rigorous and more slippery and less accurate. I actually argue it allows you to be more reflective, reflexive, I should say it allows you to be more reflexive and actually interrogate data in a way that you might not, using other methods. I, in fact, use reflexivity and quantitative analyses. I think there's a lot that we can all learn from each other. There's a debate right now in the criminal justice field. There's a recall on algorithms themselves as part of the call to defund the police. So I'm going to save my comments on that for when we get into those questions.
00:27:14.614 - 00:27:16.214, Speaker E: Thank you.
00:27:19.614 - 00:27:23.754, Speaker B: Thank you so much, John Marzia.
00:27:24.894 - 00:28:32.080, Speaker F: Hi, everyone. That's a hard act. Follow. So, the reason that I got interested in health as a machine learning person is actually because I was talking to my doctor friends in Boston, and they were telling me stories about being in clinical situations and not knowing what to do. And I thought, well, don't you have evidence, don't you have guidelines that you can leverage in these situations? And they gave me a lot of clinical literature that I was not aware of at the time, being a computer scientist, demonstrating that there often is not very much evidence on what makes sense for all patients in given situations. Randomized controlled trials are very spotty, and then even worse, as I discovered later on in my PhD, often randomized controlled trials are very biased. And so the evidence that we have about how to treat people is based on populations that are convenient to recruit or that the recruiter feels are most representative of humanity.
00:28:32.080 - 00:29:10.908, Speaker F: So young, white, college educated men. So this is problematic, because when we talk about the machine learning revolution in healthcare, there's sort of this assumption that medicine is a science. This is an objective field of study. We have ways that we treat people. There are known effects and causes that we can leverage. And maybe it's like computer vision, maybe if we have enough pictures and we show enough dogs, we could have people label these dogs, and then we could have an algorithm learned to detect dogs. But that's not the situation that we're in.
00:29:10.908 - 00:30:22.642, Speaker F: The setting is very different. Here we have images, case reports, examples, EhR data, all of these things very heterogeneous, that sometimes have bias embedded from the get go. Everything from what questions are funded to be studied in clinical medicine. There's very little done on endometriosis it affects one portion of the population. Going further to once you're studying a problem, what kind of graduate students are hired to study that problem? What kind of papers get published? What kind of people can participate in your clinical research studies? So, whether we're talking about retrospective data analysis for healthcare or prospective data collection in clinical trials, bias is baked in because doctors, researchers, scientists are humans, and humans are biased. There's so much bias in this system. I do think that there's a huge opportunity for people in machine learning to actively engage in the domain, understand how deeply rooted bias is in medicine, and how specifically in North America it has been deployed against indigenous communities and people of color.
00:30:22.642 - 00:31:19.428, Speaker F: Historically, there are horrible examples that are in every textbook and many that are not recorded in textbooks, books. I think that we can potentially not make things worse. But one of the things that I believe very strongly is if people who do machine learning don't engage very deeply in the domain, in healthcare, we will make things much worse. So it's not a given that all of this automation, optimization, prediction, counterfactual estimation is not going to make things worse for very specific portions of the population. So the last thing I want to say is, this was really driven home to me by a student who still comes to every talk I give and asks the same question. She comes to every single talk here in Toronto. And at the end she says, can you give me one example of when it is to my advantage to report my gender and my race? She's a black woman.
00:31:19.428 - 00:31:53.894, Speaker F: To an algorithm where I will get something better. Give me an example. In finance and education, in healthcare, I have yet to come up with an example of when her providing her race and her gender to an algorithm would benefit her. And until we can answer those questions, I think there's a lot of conversation and soul searching and really deep embedding in the domain that has to happen with people who work in machine learning, algorithmic fairness, and in these domains.
00:31:56.634 - 00:32:17.526, Speaker B: Thank you, Marcia. So, finally, we turn to Ziad, who, as I said, is actually a doctor and a machine learning researcher. And in this stunning article, I feel predicted what we saw with COVID Thanks, Jennifer.
00:32:17.550 - 00:33:35.454, Speaker G: I think that's far too kind to the work, but I will say I wanted to just share one tidbit with you guys from this article, because I think it illustrates the kind of research that I do and that I think is really important to do. The basic point of the article is that we were studying an algorithm that was used to allocate an extra help program to people who needed it. So the algorithm was trying to find people who needed help with their health, and to find people who had big health needs, they did something that actually sounds very reasonable, which is they found people who had large health costs. And the reason that that sounds reasonable is because, in general, when you're sick, you go to the hospital and you generate costs. And in general, the more costs you generate, the sicker you are. The problem is that when you're black in America or when you're poor in America or anywhere, you don't get the same level of access to the healthcare system when you need it. And so even though, on average, sick people cost money, on average, sick black people cost less money than sick white people.
00:33:35.454 - 00:34:38.623, Speaker G: And that has its roots in a number of structural factors about our society and our healthcare system. It has its roots in the fact that doctors treat people differently because of the color of their skin. But whatever it is, it translates into this fact that, conditional on health, black patients have lower costs. And an algorithm that simply predicts costs is going to build in that structural disadvantage that black patients face. Why do I bring up that fact? Well, I think it illustrates a certain kind of problem that's really, really important for us to solve, that we see in lots of different fields, from criminal justice to hiring and labor economics to health. And so the problem that we were studying was, on one hand, a very technical problem about measurement error, like the difference between a proxy variable that we have and a latent variable that we care about. It's about covariance structures between health and cost and race.
00:34:38.623 - 00:35:40.744, Speaker G: So it has many technical features. But on the other hand, it's about a deep question, which is, how do we measure health? What is health? How do race and poverty condition our access to care? And so how we formulate a problem. And I think I saw salon was here somewhere, although I don't see him on my big gallery of Zoom people. He's described it as how we formulate a problem, and that is fundamentally a multidisciplinary, rich thing that requires both of those skill sets. And so I have a lot of collaborations with people that I've learned a lot from. And I think when people think about interdisciplinary collaborations, like, if one of you wanted to collaborate with me, I think the dominant way we think about these is like, okay, well, I'm the doctor on the project, and I contribute some, you know, substantive knowledge, and then someone else does the math. And I actually think that that's fundamentally the wrong way to think about these problems.
00:35:40.744 - 00:36:22.466, Speaker G: And the proof of that is that you know, we stumbled on this large scale bias in an algorithm that was being used to make decisions for, like, tens or possibly hundreds of millions of people in the US alone. The data scientists that developed that algorithm did not catch it. The doctors and public health, public policy people who bought the algorithm didn't catch the problem. The doctors who applied the algorithm to their patients didn't catch it. The patients didn't catch it themselves. So you can't just rely on, like, engaging people to solve these kinds of problems. This requires enormous investment on both sides.
00:36:22.466 - 00:37:43.564, Speaker G: So that means that when you look at, like, Marzia's work, which is, you know, some work that I know well, Marzia has made enormous investments in learning about health. I have made enormous investments learning about math. And as most of you know, doctors are generally enumerate. And so, like, that's despite everything about medical education that kind of tries to beat the math out of you. But it's a huge investment on both sides. And so I wanted to bring that up because I think that that's, I think fundamentally, what is wonderful about the work that is going on at Berkeley, that the collaborations that I've been involved in outside of Berkeley as well, and I think what it means is that when we think about how we diagnose and fix problems and algorithms, our work suggests that the difference between a good algorithm that is fundamentally progressive and reallocates resources to people who need them, and a bad algorithm that is regressive and gives more to the people who have more already is a series of very small technical choices that really, really matter. And you can't see those, and you can't make those choices correctly unless you really, really know both the content area and also some of these technical skills along with it.
00:37:50.324 - 00:39:19.662, Speaker B: Thank you, Theod. This causes me to mention one thing that is very exciting about our new division of computing, data Science and society, which is something called the data Science Commons, which is going to be a multidisciplinary home for scholars. So everybody you see here, it has a certain amount of risk tolerance. The five people who spoke here have some risk tolerance in that they decided to make choices in their careers that were not the fastest route to success. They're successful anyway because they're awesome. But you tend to follow the straight and narrow, and especially pre tenure people, really often, you know, they're, they're advised to follow the straight and narrow because, you know, you want to get tenure in one department, you have to talk to the people in that field. You have to publish in the appropriate venues for that field, and then when we go to hire people, also, there are a lot of people with a background in computing who, unless they are advancing the state of the art in computing, will not necessarily get a job at a top three department in computing.
00:39:19.662 - 00:41:01.364, Speaker B: And I'm sure the same is true in other fields. I mean, if you're a sociologist and you're spending all your time doing machine learning, they probably wouldn't like that either. And so what we are doing is we are creating groups within this data science commons. We are trying to define what it means to be a deep, multidisciplinary scholar and to use that definition and to de risk so that if someone hasn't published exclusively in a particular field, we are recognizing them for having impact. Right now, we have an educational system which discourages impactful work of this sort, at least among junior scholars. And so I think it's really important that we give people like Ziad time to learn the math and the machine learning, and people like Marzia time to learn the medicine, and people like John time to, you know, to really learn these, these different disciplines, both the kind of anthropological and the more quantitative and ready it to venture out into more what might be viewed as philosophical questions, rather than, you know, I mean, thank goodness we're starting to get a few computer science conferences that. That allow this, but I think that it's very, very important.
00:41:02.264 - 00:41:02.648, Speaker C: Now.
00:41:02.696 - 00:41:48.874, Speaker B: I don't know. I had a few other questions I was going to ask. I get the impression that John was going to answer a question about algorithms, so maybe we let him do that now about what we should be thinking about as we. As we create algorithms and what we as, you know, people who write algorithms should be thinking about. And then maybe we'll just open it up, because I think we have some amazing panelists here. So, John, answer the question. Give us the answer you were postponing, and then we'll open it up.
00:41:48.994 - 00:42:37.794, Speaker E: The answer I was postponing was simply that people live in your error term, there are people that actually live there, and you need to consider them in the context in which they live. So we talk about a criminal justice system, for example, as if it's one thing, and it's actually not. Just the 50 states each have a system. There are 3100 counties, more than 3100 counties, and more than 26,000 us census places. So that goes to the multi level nature of any analysis you do, and that's on a national scale. Right. And so I consider context the same way Linda does, but I do it at different units.
00:42:37.794 - 00:43:19.084, Speaker E: Of analysis. That's why I didn't want to repeat a lot of what she was saying. But there are people in your error term and you have to consider them. And I think being reflexive, which means not announcing your bias to the world, but working through your own sets of biases and then learning to mitigate them. For example, if I can go on for one more minute, I thought that most, like many of you probably still do, I thought that most prisons were private. They were built in poor, rural white communities. And actually, most prisons are public.
00:43:19.084 - 00:44:01.854, Speaker E: Most prisons are run by states. 82% of them are run by states. And as you increase the percent black and Latinx in a rural community, the probability of getting a prison increases. If I wasn't open to admitting these biases, I had, I wouldn't have been able to find the counterfactuals, which sort of set my research agenda down a whole different path. And if we aren't willing to be reflexive and say, hey, this is what I actually think, and then say, hey, I was wrong, and learn from that, I don't think we're going to be able to push forward much.
00:44:06.254 - 00:44:42.430, Speaker B: Thank you, John. So I was going to ask more questions, but, you know, we only have about 15 more minutes. I wonder if there are questions or comments. I mean, someone can just write comment or question in the chat, and then I'll call on you if you want to do it that way. If not, we have more questions ourselves. So no questions or comments.
00:44:42.502 - 00:44:44.414, Speaker C: I can also just jump in. I wanted to ask.
00:44:44.454 - 00:44:45.862, Speaker B: Yes, please do.
00:44:45.998 - 00:45:02.586, Speaker C: Let me just jump in. Yeah. So I think I appreciate everything that's been said on this panel. There's so much, actually, that I want to respond to, but I'll just have to follow up with you one on one. One thing that I really think we have to talk about this is the issues of representation.
00:45:02.650 - 00:45:02.874, Speaker B: Right.
00:45:02.914 - 00:45:59.696, Speaker C: So going back to what you said, Siad, about how you needed to be trained both as a sort of as a doctor and as a machine learning researcher to see these issues. You know, sometimes I see those types of papers and I'm like, yeah, of course that happened. You know, like, so when we saw, like, the Amazon resume screening, that was like, oh, we removed people's gender and so we removed their name, so it's totally fine. And I was just like, no, it's not fine. Clearly you did not have a woman in that room or a woman who was empowered enough to speak, because we would be able to see immediately all the other ways in which our gender would show through in our resumes. And I think what you're saying, Ziad, about your work, which I think is incredibly important, and you were saying, you know, you had to have had this, like, medical training and machine learning training to see that issue. I think it would have also sufficed if you had machine learning training and you were a black person.
00:45:59.696 - 00:46:34.432, Speaker C: Right. If I was dealing with one of these algorithms and I was told, this is how we're going to predict stuff, I would just be like, are you aware of structural racism? Like, this is clearly going to be an issue. Right. And so I think there's this tendency where, like, we're starting to value multidisciplinary collaborations. We have a long way to go, but we're starting to value them. I think that's really wonderful, but I think we don't see people's lived experiences as a kind of expertise. And we try to find, like, these, like, weird workarounds to just sort of say, we don't exactly need those communities here.
00:46:34.432 - 00:46:56.104, Speaker C: If we just had the right combination of the different disciplines, then maybe that would sort of COVID it. And I don't think that kind of proxy representation is something that we should stand for anymore. I think we should all be working very hard to make sure that those that are being directly affected are actually leading the conversations, not just being checked in on every so often, but actually leading the conversations.
00:47:04.364 - 00:47:42.744, Speaker B: Comments from other panelists on this? I mean, I think it's very important as I look over the Berkeley faculty, for example. Well, just, you know, all faculty. Right. It's not just Berkeley by any means. Ready? It showed us the statistics this morning in computer science. What was it, like? 23, right. Meanwhile, 13 graduates, black graduates in 2000, in the year you got your degree.
00:47:42.744 - 00:48:03.132, Speaker B: Right. And 23 full professors. And. Yeah. Going up from, like, 2.1 to 2.9% on black professors of computer science as you go, you know, from full down to associate down down to assistant professor.
00:48:03.132 - 00:48:27.748, Speaker B: Yes. And it's just. It's. Yes, and I think that we do not. I mean, there's something else that I believe about, you know, not about being a minority of any sort. I hear sometimes that I'll just personalize it to women, since that's my experience. Okay.
00:48:27.748 - 00:49:07.154, Speaker B: In the four years I was at Princeton, no other woman got her PhD in math or physics. The few who were ahead of me all dropped out. Yes, it was. And, you know, and people say, oh, and then when they get tenure, you know, everyone just throws all these things at them. They get all these grants, they get and it's like, no, you know, the people who got to the tenure stage have been subject to a much stronger process of natural selection or unnatural selection. Okay. And so if you were looking at.
00:49:07.154 - 00:49:58.850, Speaker B: You know, if. If you were comparing what the top woman in a field like particle physics got to the top five to 10% of the men, which was the appropriate ratio, you'd say, yeah, that's comparable. So. So I think that there is just not enough appreciation of what someone who is the only or nearly the only has to go through to get there. And we don't sufficiently take that into account as we choose which scholars we are going to bring into our communities. I mean, I'm sorry. I know I'm preaching to the choir here, probably, but anyway, that's my.
00:49:58.850 - 00:50:00.334, Speaker B: That's my sense.
00:50:01.234 - 00:50:03.330, Speaker A: Kira Goldner has a question.
00:50:03.442 - 00:50:05.534, Speaker B: Great, Kira.
00:50:10.034 - 00:50:11.530, Speaker A: And then in Bal.
00:50:11.682 - 00:50:12.814, Speaker C: Can you hear me?
00:50:14.354 - 00:50:14.898, Speaker H: Can you hear me?
00:50:14.906 - 00:50:16.094, Speaker A: Yeah, we can hear you.
00:50:16.674 - 00:51:20.244, Speaker H: Okay, so my question was about three Ziad or anyone else, about, like, the level of collaboration of doing each other, you know, learning each other's disciplines that you think is necessary. For example, I had a recent collaboration of, you know, using theoretical computer science to model and understand health insurance markets, where we collaborated with a domain expert in health insurance markets. But he didn't do any math, and I do not know everything about health insurance markets. We had some meetings, and he talked back and forth and learned how to talk to each other, and I learned something about health insurance markets, and we explained to him our model and helped make sure that he thought it was a good model. But again, he didn't do any math, and I am not an expert on health. And so, I guess where in between completely separating the process with little discussion versus, you know, the doctor doing the math, do you think is the right balance in order to really get at the problem and understand the issues?
00:51:24.184 - 00:52:45.852, Speaker G: I'll just go first, but if anyone else has thoughts on this, I don't think it's an easy question to answer, because I do think it'll just be so context dependent. I'll maybe, like, give you a concrete example from one field that I've gotten more exposure to, which is, if you think about the example of behavioral economics, the thing that made that field work was that a bunch of economists started taking the psychology literature very seriously, and a bunch of psychologists also started learning how to write down the kind of concepts that they had been used to expressing in kind of words, in kind of things that look more like greek letters. And so I think that field proved to be an amazingly successful example, but I think there are other ones. I think applied math is another really great example of people taking applied problems seriously and then learning about their own theoretical problems from that. So I think that there's a nice feedback loop that can develop. But I do think it's. I would say that the reason it's very costly to do multidisciplinary work is because the level of investment is huge.
00:52:45.852 - 00:53:18.344, Speaker G: It's like, for me at least, it wasn't just working on a paper. It was spending years doing problem sets in textbooks by myself. It was reading a bunch of papers in the areas that I was interested in. So it's very costly, and I think we wouldn't want to minimize that. We have to be realistic about that. And as Jennifer was saying, we have to create the structures that are friendly to those kinds of big investments. Because realistically, I think.
00:53:18.344 - 00:53:40.384, Speaker G: I'm not saying you can't write a great paper in a kind of standard collaboration model, but I think if you really want to push forward something that's fundamentally new and different, this market is pretty efficient. If it were easy to do, that paper would already have been written. And so the fact that that paper is not written is indicative of how much investment you need to make to get there.
00:53:46.844 - 00:53:48.864, Speaker B: Inbal has a question.
00:53:52.604 - 00:53:53.084, Speaker D: Right.
00:53:53.164 - 00:54:02.914, Speaker A: I wanted to ask, what would you advise a pre tenure faculty who are not at Berkeley and don't enjoy this open mindedness, that want to do multidisciplinary work?
00:54:06.494 - 00:54:08.354, Speaker C: You should just move to Berkeley.
00:54:11.094 - 00:54:12.406, Speaker F: I knew that was coming.
00:54:12.510 - 00:54:13.914, Speaker H: I knew that was it.
00:54:14.934 - 00:54:24.282, Speaker C: I've just been doing it one person at a time. You know, I'll do all 70 people, 61 people on. Yeah.
00:54:24.458 - 00:55:06.058, Speaker B: Look, I think, honestly, I think it's personal risk tolerance. I think we have to build the structures that de risk it. We really, really do. And different people have different comfort levels with risk. I dropped out of high school and lived on the streets of New York City. So somehow, you know, taking a risk and maybe not getting a job in mathematical physics at Princeton wasn't going to kill me. You know, I think that, you know, it is hard.
00:55:06.058 - 00:55:47.006, Speaker B: I think you should look. If you're looking for jobs, you should look for environments where there are people who are going to support you. I think that it's very helpful to have a senior scholar or two out there, either at your institution or someplace else, who can support you in this. And I think you should realize it might take a little longer. I mean, Ziad said he had to go and do all these problem sets, and I just worked 24/7 everybody told me I wouldn't get a job. And three and a half years out of grad school, I got seven tenured offers because I had worked that hard, because, you know, but I probably killed my health. I was.
00:55:47.006 - 00:57:19.096, Speaker B: I was working like 22 hours a day, you know, but it's just, I think it's a matter of how, how risk tolerant you are. And there are people who become wonderful multidisciplinary scholars after they get tenure, and it's a little safer, and that's okay, too. But I think this community, in fact, as you were talking about deep collaborations, I was looking at this community, and I think the algorithms folks and the legal scholars here and the philosophers have really been talking very deeply now for a number of years. And there are conferences that are growing up, and they have created this field now with the journals and everything and the publication venues, that de risks it a little bit. And I just hope that here we can do that more broadly. We can, you know, do this in the areas of social welfare and public health and public education, and that we can create this group of scholars who look at the world this way, starting with undergraduates. We have an amazing data science program where I want to start undergraduates thinking this way before they know there's something they should be scared about when it's all new.
00:57:19.240 - 00:57:20.084, Speaker I: Can I.
00:57:21.944 - 00:57:23.456, Speaker A: Sorry, go ahead.
00:57:23.600 - 00:58:20.414, Speaker I: I'm Stacey Dogan. I'm from, from Boston University, and I just want to. I think this is kind of a pivotal moment for these questions. I think many institutions are beginning to think hard about how to solve this very problem. So Bu just established a faculty of computing and data sciences that is trying to do exactly the same thing that Berkeley is trying to do. And if anyone is interested in getting to know the people in that group, please reach out to me directly, or Ron Cannetti, who is also in this, in this session. And we are also, I think someone mentioned yesterday, we're partnering with Georgetown in a project to put together a white paper with guidance on both sort of pedagogical innovations as well as structural innovations within universities who are innovating in the law and technology space at the intersection of law and computer science.
00:58:20.414 - 00:58:48.476, Speaker I: And again, we had sort of an early workshop in April. We were supposed to have another conference this summer that will probably take place in August. But if anyone is either interested in sharing your insights with us on either pedagogy or structural innovation, or interested in hearing from us again, please reach out to one of us, and we'd be happy to bring you into the conversation.
00:58:48.620 - 00:59:16.208, Speaker B: I'm going to say one thing and then turn it over to Linda. Bob Brown, your president, called me and tried to get me to take that job instead of this job. He's a good guy. He thinks very broadly, and I think he's doing very cool things there. And I think Azar, who is your dean, is wonderful. So come to Berkeley or bullet. Okay, Linda, what can I say briefly.
00:59:16.256 - 01:00:07.828, Speaker F: As somebody who's, who's not at RVU, but somebody who went to graduate school at MIT before, there was sort of this idss that there is now. If you want to engage in these problems, Jennifer is right, you need to have a higher risk threshold, but you also need to be very comfortable with having awkward conversations. Conversations. And if you have high quality people at your institution, I think that they will acknowledge that the silence is awkward and then they will make changes. And I went through a set of these where I would ask a question, well, why can't I have this? Well, why is this not a policy? Why is this not standard? The silence is awkward. And then they would come back weeks later and there would be a new policy or they would offer a different project. So I think you do have to have risk tolerance.
01:00:07.828 - 01:00:13.144, Speaker F: You also have to be very comfortable making other people uncomfortable.
01:00:14.844 - 01:00:21.428, Speaker B: Which all of us are. Linda had something she wanted to say here.
01:00:21.596 - 01:01:56.412, Speaker D: Thank you so much, Jennifer. I just wanted to add from the perspective of someone who's really interested and committed to mentorship, to remind us all on the call that we are whole people and with respect to being whole people, as we consider the pathways we want to take in our careers, it's not only about the risk with respect to the work itself and what we do in the context of that environment, but also what it means for other aspects of our lives. So how does what you want to do fit in with other aspects of your life, let's say family life, for example, in terms of your, you know, your trajectory and how are you going to integrate those? Because they're really not separate issues. I'm going to pick on John for just a minute and just say one of the things I've learned from John, you know, over the years is I really had a chance to watch John move through the academy, but to do so with a family with two really brilliant kids, one that is particularly spunky and a wife, a partner who is pretty amazing. To move to Arkansas and to rural Arkansas is not an easy move. So having that particular support was important. So the negotiations not only going on in the job place.
01:01:56.412 - 01:02:18.846, Speaker D: And this is the point that we always forget. It also goes in on, in other aspects of your life, because, you know, that has to come together in a lot of ways to give you not only the intellectual space, but the emotional space to do the labor on the job. So I just wanted to point that out as well.
01:02:19.030 - 01:02:33.438, Speaker B: Yeah, life is long. Life is long. So, you know, if you're focusing on your kids right now, life is long. You have time to do different things at different points in. In your life. Yes.
01:02:33.526 - 01:03:10.886, Speaker A: Thank you very much, Jennifer. And thank you very much to all the panel members. It was a really great panel, and we learned a lot about social change and social justice and what's happening in the real world. So thank you. Let me also thank many people. So many people worked very hard in order to make this workshop happen. So, first of all, the Simons Institute and Shafi, the director of the institute and the great staff that helped us a lot, Ashley and Drew, and also Lily from Haifa University.
01:03:10.886 - 01:03:42.614, Speaker A: Thank you very much. I would like to thank, of course, to all the speakers, discussants, moderators, of course, the leaders of the breakout rooms. Worked really hard. Thank you very much. And thank you to my great co organizers, Niva Elkin Koren and Shafi Goldwasser. And especially, I should say, to involve Algam Cohen, who has been the main engine behind all this workshop from its inception. It was great working together.
01:03:42.614 - 01:04:15.754, Speaker A: And finally, many thanks to all the participants. Thank you for all the notes and for all the interaction. It has been really great. Let's conclude with a final poll, just like we did in the first day. So let me share my screen, and it will take 1 minute. So please just scan this barcode into your cell phones. If I forgot someone, please, I really apologize.
01:04:15.754 - 01:04:48.374, Speaker A: Okay, can we start? Let's start. So what's your main takeaway from this workshop? In a tweet mode.
01:05:10.454 - 01:05:13.430, Speaker D: Listen, what do I do if I'm on a phone? Okay.
01:05:13.542 - 01:05:44.574, Speaker A: Lost in translation. We need structure, exciting challenges, deep questions, conversation. More people. Technical is not enough. Algorithmic design matters. Teaching. Teaching is the main challenge.
01:05:44.574 - 01:06:23.664, Speaker A: Okay. More and more is coming, of course, the limitations of each discipline. Yeah, we heard a lot about representation. Wow. There is someone here who says they need. They have so much to learn from lawyers. Okay, so let's go on to the second question.
01:06:23.664 - 01:06:31.864, Speaker A: What was the main challenge in this workshop? In one, two, four, or one, two, three words.
01:06:44.864 - 01:06:45.644, Speaker E: Yeah.
01:06:47.824 - 01:07:07.804, Speaker A: Time. Language. Long hours. Late hours. Yeah. We have a winner here for sure. Wide range of disciplines.
01:07:07.804 - 01:07:09.076, Speaker A: Yeah.
01:07:09.100 - 01:07:09.340, Speaker D: Okay.
01:07:09.372 - 01:07:33.348, Speaker A: Time repeats. Also in the small ones. So if we had a classification algorithm here, we would have had time even bigger. Absolutely. Okay. And, okay, so answers are still flying in too much time. Okay, great.
01:07:33.348 - 01:08:36.314, Speaker A: And the last question, how likely is it that our April meeting will take place in person? So now we are applying now casting from Hellstone. Okay. Well, a bit pessimistic, but we'll see. Great. So, on this happy note, I would like to conclude. Thank you again to all of you for your participation and really hope to see you in April in Berkeley. Thank you, Michael.
01:08:36.694 - 01:08:37.994, Speaker B: Thank you, Michal.
01:08:39.334 - 01:08:39.694, Speaker A: Good night.
