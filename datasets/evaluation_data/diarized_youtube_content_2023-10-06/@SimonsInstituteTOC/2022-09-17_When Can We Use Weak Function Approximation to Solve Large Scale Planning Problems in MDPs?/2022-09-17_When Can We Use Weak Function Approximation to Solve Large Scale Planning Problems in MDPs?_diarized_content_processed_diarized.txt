00:00:05.920 - 00:00:48.670, Speaker A: Right. Thanks for the organizers first, for organizing this wonderful workshop. It's been a pleasure to be here. And so today I'm going to talk about this series of results that we had about how to use functional approximation, how you can use functional approximation, engineering, reinforcement learning. What's the utility of having functional approximation in reinforcement learning and all that? And so, first, I want to thank my collaborators. It is galliard. He's discovering that this was embodied from above.
00:00:48.670 - 00:01:28.954, Speaker A: And you can see the Troy on his face for seeing this. And he's a DeepMind, and he's also a PhD student at UCL and other collaborators, being Philipp, who's a PhD student with Nan. Nan is also there. And Bonabas Yoncer, who is a PhD student in Cambridge. Yasin and Andrash are at the foundations team at DeepMind. Okay, so, I want to start by going back to the year of 1963. Probably none of you at the time, I wasn't.
00:01:28.954 - 00:02:10.094, Speaker A: So I don't remember. I don't have direct memories. But it was a fantastic year because this paper was published that year. Nobody knows about this paper, but it's kind of interesting to go back to these old papers. This is a paper by Richard Baumund, Robert Caliban, Bella Kotkin, and the title is polynomial approximation, a new computational technique in dynamic programming allocation process. So, I want to use this paper as a precursor to explain to you what this functional approximation business in reinforcement learning is about. And it's kind of, oh, my animation.
00:02:10.094 - 00:03:08.754, Speaker A: All right. Anyways, never mind. So it solves what they call an allocation process. It's a very specific instance of you can cast its NDP, USC, that it's casted effectively as an MDP. And so what is this? You're given age, different utility functions, and they correspond to each different projects that you want to allocate resources in between. The resources are d dimensional, and a resource lives in this, in this hypercube, in this d d. So you want to split up your total amount of resources into h different parts, such that you're maximizing the total utility.
00:03:08.754 - 00:04:24.428, Speaker A: How do you do this? Dynamic programming was invented for doing things like this. So you can imagine, picture that these are the nonlinear utility functions, and you are at some stage age, and you're trying to make a decision about what to do there. And so far, you decided to allocate this much of the resources s so this is one dimensional note so that I can draw it. And as you go, you can imagine that from this point on, things are going to go on optimally. So you can define the optimal value achievable in the remaining period. So, for the remaining projects, given that you have already used up these machine sources, and it's not hard to see, and this was kind of like the thing that Richard Baumund was actually exploiting in many different ways, that these optimal value functions satisfy this recursion. So the optimal value function for some stage is a function of the optimal value function of the next stage.
00:04:24.428 - 00:05:17.796, Speaker A: We just have to think about that. Okay, how is the optimal value for this stage? Well, I need to make a decision for this stage, for the remaining resources. So I have to subtract from the resources I have, what I've already spent. And so this is the interval where I can make a decision for that. I'm going to get a payoff for this, for this project, and I will also consume some resources and I will end up somewhere else, and I will continue optimally from that point on. So, because of the additive structure, the optimal value is going to be the sum of these two things. And you just have to choose the best action for this stage, and you have a boundary condition for the final stages.
00:05:17.796 - 00:06:20.230, Speaker A: Well, and the question that they are asking in this paper is how to compute the optimal value, and specifically, what is the allocation? It's a very simple problem, and this looks beautiful. And they're thinking about, they were trying to solve this for specific cases when there is some extra structure, but they saw that that doesn't quite generalize very well, so they turn to numeric s solutions. So they were thinking previously about discretizing basically this state space, discretizing the action space. But then they noticed that, well, if you discretize a d dimensional space, you want to have some good accuracy. Then your research needs for computing this thing are going to blow up exponentially in the dimension. So the question is, can we do something about this? And so this is what this paper was about in 1963. So what they propose is very simple.
00:06:20.230 - 00:07:03.144, Speaker A: So go back to the one dimensional case, maybe the state positions this minus one for me. Okay, well, change it a little bit. And you can imagine that you will have some basis functions. So they are thinking about orthogonal basis functions of various kinds, and you are going to linearly combine these basis functions to represent the optimal value function in approximate manner. And then somehow you plug this in into the recursion that we had before. So what does this do? So what's the new idea here? The new idea is that we are, well, okay, this shouldn't be d. That's a different d.
00:07:03.144 - 00:07:46.202, Speaker A: So the new idea is that we are decoupling the dimensionality of the state space from how many basis functions we are using. And if you have a good guess about kind of the smoothness of the optimal value function, or what basis functions are going to capture the optimal value function, well, then maybe you can get away with just using a few coefficients. And all that you are left to do is to discover these coefficients for. Okay, you have to do it for every stage, but that's the only thing that you have to do. And so that's indeed what they propose. And they have. So we have this recursion, and they call this successive approximations.
00:07:46.202 - 00:08:35.802, Speaker A: You can write it with operator notation as well. And then they just have this idea that we just have to plug in these basis functions and somehow calculate the coefficients. And then the question is, okay, if I have coefficients for the next stage, how do I get coefficients for the current stage? And specifically, what they propose is that, well, you just do this calculation in some way, this successive approximation, at some notes, at some points, and then you extrapolate or interpolate. You just saw the function fitting. So this function fitting is going to basically project this function into the span of the features. So that's what they propose. And today, we would recognize this as an instance of fitted value iteration.
00:08:35.802 - 00:09:25.308, Speaker A: So, in reinforcement learning, this is what people call fitted value iteration. So they use this, I don't know, ottoman basis and gaussian quadratures for approximating the projection, because they're overqualified. For this job, we would just use Monte Carlo. All right, so what are the results? Any good paper has to have some numerical results. They have two dimensional problems. They were very happy that they calculated nice tables, and they conclude that if they combine these techniques, polynomial approximations, long range multipliers come in because you have constraint action set with that of successive approximations. There should be very few allocation processes with steel desistor.
00:09:25.308 - 00:09:44.624, Speaker A: So they're kind of ambitious by modest at the same time. Right. Like allocation processes. But it's. There is. There is definitely optimism there. All right, so why the optimism? It's because they kind of think that they figure out how to avoid description.
00:09:44.624 - 00:10:44.774, Speaker A: So, no. Curse of dimensionality. So, bamboo coined this term curse of dimensionality due to the dissertation, that they were really not a fan of that. Okay, so there are two questions that arise, and the first question that arises is the question that I defer to our colleagues in the econometrics and other departments. So there's a question of, like, how to choose this polymer or how to choose the basis functions to approximate the optimal value function. And I rather want to focus just on the computational aspect of the problem, which is that if someone already chose some basis functions, can we really use these basis functions to get these coefficients and get the optimal value, good approximation to the optimal value, or get a good policy in particular. So we're going to have mdps.
00:10:44.774 - 00:11:21.066, Speaker A: There are finitely many actions. So that's going to be a simplification compared to what they were doing. So a is a number of actions, age is going to be number of projects or number of time steps. Horizon these the number of features and some accuracy, epsilon that we desire. And we ask the question, are there algorithms out there? They are able to deliver this accuracy regardless of the size of the state space. That's a question. All right, any questions? Okay, so the problem is clear.
00:11:21.066 - 00:11:49.484, Speaker A: Good. So we are done with the first two parts, just one slide with some notation and some terminology, because, well, I will need to rely on some of this. So we are going to be more ambitious. Look at MVP's. It's not allocation processes, but it's a very close relative. We're going to have states. So we have the bam and optimality equation as before.
00:11:49.484 - 00:12:29.546, Speaker A: We're going to have states, we have actions. They're in a feasible set. But I'm going to simplify, the feasible set is always going to be the full action set. Don't think that at this moment that simplification is, I think, tolerable for every stage and every state. And action, if you take that action in that state, you receive an immediate reward of r, and then you transition to an x state, which is a random next state. So there is some randomness acting on a transition. So this is a transition function, and then the optimal deactivation looks like this.
00:12:29.546 - 00:13:10.854, Speaker A: And so, one little innovation that. Well, okay, it's actually quite a nice thing that comes from the machine learning literature. Chris Watkins innovation is that this right hand side, if we just add these two things together, you can call it the action value of choosing action a in this state. And sometimes it's very beneficial to directly approximate the action value function. Why? Because if I knew the actual value function, I would know how to act optimally. Yes. Sorry, could you go back to slide the realizability assumption for all value functions or just the optimal one? We'll come to that.
00:13:10.854 - 00:13:38.482, Speaker A: There will be a whole discussion about this. Yeah, so this is just for now, notation. That's clear. So, one more notation is that. So I'm going to drop this notation. For most of the time in the MDP literature and machine learning, you usually have just a transition cutout. And my notation is going to be that, like if you have a state s and action a, then this is just a probability distribution over the next state.
00:13:38.482 - 00:14:25.044, Speaker A: So I can write something like the index product of this probability distribution with a value function that gives you the expected value that you're going to get if you transition according. So just to, so that I can simplify some notation. All right, so, yeah, back to the content. So we are done with these first two things. And I told you in the panel, you have to concentrate on algorithms. So I'm going to focus on some algorithms. So I will talk about optimistic constraint propagation, which is an argument by Jian Wang and Ben Munroy from 2013, which is addressing this problem in the thermistic setting.
00:14:25.044 - 00:15:00.872, Speaker A: And then I'm going to talk about a bunch of other algorithms under misspecification. And then I will talk about results that I call weak functional approximation results. And that's going to be the back of the talk. Actually, we've done with this. So the first part is this optimistic constraint propagation algorithm. This is for deterministic algorithm, sorry, deterministic mdps. So we have these basis functions and we want to figure out what the coefficient is.
00:15:00.872 - 00:15:42.996, Speaker A: The coefficient is shared between all the stages without loss of charity. And if you make this assumption, then you can plug in your, your functional approximator into the bam and equation. You can subtract one side from the other, and then, no, it becomes a root finding problem. So you just want to find this theta parameter that makes what we call the TD error to be zero at all the state action pairs. We have a bunch of constraints. If we find the right constraints, maybe you can just find these equations. Sorry, find the root for these equations.
00:15:42.996 - 00:16:20.580, Speaker A: So the TD should be zero at all the stages and all the transitions. And so we're going to focus on trying to shrink the parameter space. So initially we are ignorant, we don't know the solution. All the thetis are possible. So what we're going to do is that we're going to start with all possible parameters. So we assume that the true parameter has a known l, one bound b. It lies in this version space, and we're going to iterate.
00:16:20.580 - 00:17:03.136, Speaker A: And in every iteration of the argument, the first step, we're going to choose an optimistic parameter. So what is the optimistic parameter? It's like definition of optimistic parameter is that like this parameter was true parameter, we would get the highest value at the initial state we have some initial state. We're going to run experiments from there. So it's like online learning. And then once you have this parameter, you can just do as if this was a true parameter. So, this is going to be verification. We are going to generate a bunch of new constraints.
00:17:03.136 - 00:17:32.414, Speaker A: So we roll out with the resulting policy up to the end of the trajectory. We collect all the data that. So we collect basically a trajectory. It's deterministic. So there is only one trajectory up to the end. And then you can measure these TD errors along the trajectory, and you're going to enlarge your constraint set by adding the constraints that all these TD errors need to be there along the trajectory. So you shrink the.
00:17:32.414 - 00:18:24.300, Speaker A: Potentially shrink the trajectory, shrink the version space, and you're going to stop when you cannot shrink anymore. So if you ever find a trajectory such that you're not shrinking the version space, then you stop. Well, you need to ask the question, is this computationally efficient? Yes, you can implement this computationally efficiently. And the other question is, is this going to really work? Is this going to return an optimal action? And so, that's the genius of these two gentlemen. They show that it is. And I will later show you an algorithm that generalizes this algorithm. And I will explain a little bit more, that algorithm, about why this algorithm works, and then you will understand clearly if this other algorithm works.
00:18:24.300 - 00:18:54.214, Speaker A: This must work. Yes, this algorithm work is. I see that you need maybe a little bit more. So this is just online. Right. So I only need to. We have the ability to execute trajectories, but the initial state is random.
00:18:54.214 - 00:19:23.874, Speaker A: Then I think it's a little bit more delicate. Like, you will need to add extra stuff, but you can accept it. I missed the beginning. So, is this equality, the first one, the quality function is phi times theta star. Is that an assumption or without loss of generality? No, that was the assumption. So that's what Bauman told us, that we have these basis functions, and then you just need to find the coefficients. And I know we are asking a question.
00:19:23.874 - 00:19:47.242, Speaker A: Can we find the coefficients? Yeah, absolutely. Good question. All right. Okay. All right. So they proved this theorem that for any of these theorem systems, blah, blah, this algorithm stops, and it's poly time and poly number of queries are sent to the environment. Right.
00:19:47.242 - 00:20:29.118, Speaker A: This is great. So this was, by the way, like 50 years after Bamboo's written on paper. There's been a lot of other developments, but somehow this question was not really thoroughly looked at. And even in the interim period, people were busy with other things. But eight years later, this paper by Du Kaka de Wong and Young appeared, and that inspired us, among others. And this then sparked a number of other results. And so now, basically, I want to tell you about all these exciting developments that happened since then.
00:20:29.118 - 00:21:28.374, Speaker A: All right, so the setup of this du Kaka de Vang young paper, it has a lot of different results, is basically the paper that starts to ask these computational questions and query efficiency questions in a very clear language. So that's a big contribution of this paper. And so one of the setups that they look at is that you can, if you have a policy for each policy that is also an action value function, that describes how much this policy is going to make. If you start it in a state and you take an action, and then you follow the policy. So that's the action value function of the policy. And one of the setups in the paper is that they assume that for all the policies, the action value function lies in the span of the features up to an error of epsilon. So that's the approximation error, and it's a uniform approximation error.
00:21:28.374 - 00:22:30.674, Speaker A: And the genius of this is that they started to ask the question like, even if we have this much stronger assumption, sorry, much. So it's strong and weak at the same time. It's a strong assumption because, no, for every action value function, for every policy, we assume that the features are good. So the features good for many different things, but there is some residual error. So there is this epsilon uniform approximation error. Today I ask a question under this setting, is it possible or not to come up with a query efficient algorithm, an algorithm that would just go in, in the MDP, and there are different query models. So here that they consider the generative model, or I call it random access model, or global random access, where you can poke any state action pair and get a transition from there and get corresponding reward.
00:22:30.674 - 00:23:38.528, Speaker A: They ask the question, is this possible? And then they prove that this is not going to work. So if you have a target accuracy of delta, and this target accuracy is not low enough, sorry, is not high enough. So if the target accuracy was like, compared to the approximation error is inflated by a factor of square root of d, then you may have a chance of having a polynomial Argoton. But if this is not the case, then you have no chance. So that's kind of like a big wake up call for everyone who was hoping for something much better, because it seems that, well, on the one hand, we have uniform approximation errors. So there's like al infinite errors that are already big and know, if you're not inflating those by a factor of square of v, at least then there is no hope for a computationally efficient solution, because there is no query efficient solution either. Right.
00:23:38.528 - 00:25:05.532, Speaker A: So this was very kind of innovative at a time. And you can pair this with a positive result that says that, well, okay, if your target accuracy is exceeding this critical threshold, then polynomial computation is available. And that's what we were doing in our paper to have both sides. And so what's the insight here? So, the insight is that in reinforcement learning, you really need, or in this MDP calculations, you really need to deal with uniform approximation errors, because you're looking at maximum values, the optimal values, and some of the pair much better. They force you into this. Just working with uniform approximation errors, and this uniform approximation errors, if you are only making so many measurements of a function that is just nearly realizable, then that can blow up. So, extrapolating from polynomial many queries without blowing up the approximation error by a factor of square root of d is just information that it will not possible sometimes.
00:25:05.532 - 00:26:06.568, Speaker A: So, caveat is that this is specific to some choice of the basis functions, or this feature map, as we call it in machine learning, so you can choose much friendlier ones. There is no blow up. So that's an interesting direction, maybe, to try to understand what's happening there. And so now back to Emma's question. So what are the assumptions that we are making? So, there are a bunch of assumptions that you could make, like this one that I talked about, that all the old policies, action value functions, lie in your function space. Or you could make an alternative one, which says that under the bam and optimality operator, this function space that's induced by the features of the basis functions is closed. So these assumptions concerning the features, I call them the strong assumptions, right? Because, know, the features have to do a lot for us.
00:26:06.568 - 00:27:22.054, Speaker A: We are asking them to do a lot. And the weak assumption is what I think Belmont and these other people in the early days had in their mind, that it's only the optimal value function, or the optimal action value function lies in the span of the features. So there are these two alternate ways of thinking. But here we already saw that if you add approximation errors, then unless you're willing to explode those approximation errors by this square rooty factor or something that depends on the features, there is no hope for query efficient algorithms. So that is the question of, like, okay, here we are assuming even less. Can it work? Well, the errors need to be blown up, or if you're interested in negative results, or we think that maybe the results are not going to be positive, then you can ask the question, okay, what if you have exact realizability? So that is, the value function lies in the span of the features. Can we do it? So that's the question that we're going to look at next.
00:27:22.054 - 00:28:08.902, Speaker A: So in a way, this is more ambitious, but there is usually no misspecification. If there was some specification, you can tolerate it, but the errors blow up. How pathological are those lower bound instances? And that's pretty pathological. It's like the lower bound instance on the previous slide. It's just like a big tree, and it relies on constructing the features. It's basically packing the unit sphere with the feature vectors, and that creates this blow up of the errors. If you have friendlier features, then don't expect this blow up.
00:28:08.902 - 00:28:51.084, Speaker A: Then that result is really about the blow up, that this blow up in the worst case is necessary. But you can ask the same questions for the coming negative. Yeah, well, I think that they are insightful. White pathological. All right, so we are done with this. And so let's get to results about this weak function approximation. All right, so, as you're going to see, the single most important determining factor in these results is going to be how big is your axial space.
00:28:51.084 - 00:29:26.628, Speaker A: So the action space could be relative to the num, the number of stages in the problem, and the number of features or basis functions. It could be polynomial, or much less constant, or much more exponential in the smallest of the two. And we're going to see that this determines a lot. So I will call this first two cases the few action skates. And this one is the math one. So let's start with the many action skates. We have already seen this result.
00:29:26.628 - 00:29:49.820, Speaker A: So I'm going to build these tables. So there are all these, like, you can choose this, you can choose that. And like, we're going to have a lot of fun building these tables. So we have seen this result. This is the dimension Ben von Roy desire from 2013. And just to explain the notation. So the optimal parameter vector lies in this bolaf radius.
00:29:49.820 - 00:30:14.800, Speaker A: B. D is the number of features, age is the horizon is the number of actions, and the number of features is assumed to be bounded by one. And in the tables, we're going to have source, like how many actions we have. What is the MDP class? So explain this notation. So m is for MDP. These are the parameters. This q star means that it's q standardizable.
00:30:14.800 - 00:31:27.024, Speaker A: So that means that the features are such that are handed to the algorithm that the q star function, this optimal action value function lies in the span of the features. And this intersection just means that this other class, this is all deterministic mdps. So if you consider only deterministic mdps and the QSTR function is realizable, then this checkmark means that there is a poly query complexity, in this case also computational complexity algorithm, that solves this problem. So there's the first result. So the next result is from a paper, of course, that says that if you have a large number of actions, exponential in the smallest of the dimension and the horizon, and you give up undeterministic mdps, then this nice result is gone. And this is also going to be based on some pathological construction, if you want. And so that's pretty sad, but.
00:31:27.024 - 00:32:07.022, Speaker A: Okay, so one more thing. So this global access, it just means this generative model. So it's like you're interacting with a simulator, and then you actually know what the state space is. It's like a pretty strong condition, because when you have a simulator of a robot, what do you know? Which states are legal or how to call even the states. So that's a pretty strong assumption. But for a negative result, we want to have a strong assumption. Right? So even if the simulator has global axis, the transitions p is transition.
00:32:07.022 - 00:32:47.886, Speaker A: The transition kernel can be deterministic, but the rewards can be stochastic. So it's enough to make the rewards stochastic. Of course, you can push it to the other end. Now here is the shock. So if you have any number of actions and you have both v and q realizability, and you have global access, no, you can do it. So that's pretty interesting. That's quite surprising.
00:32:47.886 - 00:33:09.524, Speaker A: That's quite efficient. It's not computational efficient. Basically, that algorithm needs to read the whole state space, do a bunch of calculation. It's doing this version space pruning. It's pretty aggressive, and it really needs this global axis. We will see it on the next slide. Right.
00:33:09.524 - 00:33:52.554, Speaker A: So I already talked a little bit about the simulator access models, but I just want to clarify it a little bit more. So we talked about this global access. The algorithm gets a description of the full state space. It gets all the features of all the states upfront, it can do any computation with it. And some of these algorithms in quotation mark, do this and can ask for a transition at any state, actually local access model. I'm going to talk about local access next. It doesn't get the description of the full state space, it's much more realistic, only gets the features associated with states that are being visited.
00:33:52.554 - 00:34:42.670, Speaker A: Simulation always starts at some initial state you can get back to, and the simulator can be reset to a previously visited state. So there is checkpointing in the simulator if you want. So you can implement this by having a simulator, and then as you go, like, you ask a simulator to save it in internal state, and then later on you say, well, step number 152, when whatever state you were at, like, let's go back there because it was interesting. So that's what you can do with local access. You said your result was not efficient in a global access model, but what does efficient mean in the global access? Yeah, I mean, like, it could be that it doesn't use global access that way. Right. The argument could choose not to use this.
00:34:42.670 - 00:35:10.134, Speaker A: All this part. Yeah. Okay. And finally, the weakest of all of these assumptions, online access, which is like, when you're interacting with the real environment, that's like what you have. You can't reset back to previously visited states if you have deterministic transitions. Of course, you could do that by saving how you got to some state. But if you have stochastic transitions, then that's not possible.
00:35:10.134 - 00:35:47.420, Speaker A: And so, again, like, we kind of see, uh, um, the difference between stochastic and deterministic environments. Just kind of interesting. Okay, so the few action skates. So if you have only a constant number of actions, then we have lots of check marks here. So let me go through this rows one by one. So there is v star realizability, and under that you can get positive result. There is q styleizability, but the transition should be deterministic.
00:35:47.420 - 00:36:43.954, Speaker A: The rewards can be stochastic now, and that's kind of like, if your initial state is also stochastic, you kind of, like, fold it into here, then you can do it. And if you have Vstar and q realizability, similar to. So you have two feature sets on the previous slide as well. If you have b star and Q style kind of work hand in hand, you have two feature sets of two set of basis functions, but this only holds under the states that are reachable from the initial state. So that's a distinctly weaker condition. It turns out that then, assuming that Vstar, Q star are realizable at all the states. And so under this, this results also holds on the local axis.
00:36:43.954 - 00:37:43.770, Speaker A: And now if you increase the action count, say, permits the actions, action count determines everything here to be lower, the polynomial of the minimum of the dimension and the horizon, then you get negative results pretty much for all the cases. So you can start with v stylizability and you can slash in determinism there. It doesn't help. Q star reliability. You got the negative result and v star, q star reachable deterministic transitions, you get a negative result. So notice that, like in the previous slide, we had this positive result on the global axis for the same set, or like for the, for the, under the stronger assumption that v star and q are realizable at everywhere. Can you give us an intuition of why does just as much more space fractions cause it to be? Why the actual space? Yeah.
00:37:43.770 - 00:38:09.034, Speaker A: Why is it so important to. Yeah, move us. Yeah. All right. Okay, so I actually want to start with the green rows. Why these problems are not hard. And I'm just going to talk about this setting, and you can generalize this setting to the other settings.
00:38:09.034 - 00:38:56.844, Speaker A: I've done that. So we have constant number of actions. We have stochastic transitions, restoration and local access. So the algorithm is going to be called tensor plan. It turns out that this is just a slight modification of this constraint propagation algorithm of Cenkpen and Ben van Roy. So we have the same notation, except that now we have not action wave functions, but state value functions. You have the TD error, which is the difference between, like, how much reward you're predicting to make under this theta parameter versus how much reward you're making.
00:38:56.844 - 00:39:59.250, Speaker A: And so this is the new thing. So rather than saying that, well, I would need to know, like, remember in the TD error previously there was a maximum reactions in the optimistic constraint propagation error. There is no maximum reaction here. But what we are doing is that we are taking the TD errors and we are multiplying them all. So these TD errors are just like numbers, right? If any of them was zero, then the product is going to be zero, and the product is only going to be zero if one of them is zero, right? Like, it's pretty elementary. So you can basically rewrite the Balmond equation by saying that under this condition, the product of the TD errors at all, the state action pairs, is going to be zero. Because if you take the optimal action that the optimal policy gives you, that makes the TD error zero.
00:39:59.250 - 00:40:41.172, Speaker A: So that's one direction. And in the other direction, if you have this, well, that doesn't quite imply that you have the optimal, um, that doesn't quite imply that you have the optimal policy excluded, that there is some other policy for which these equations hard. But we're going to fix that, so there's no problem. All right. Um, and uh, later on, it will be clear why its advantages to get rid of the maximum and, and write this product. So how does the algorithm work? It's basically copy pasting the previous algorithm. We start with the full parameter set.
00:40:41.172 - 00:41:24.802, Speaker A: We are iterating, we are going to be optimistically choosing a parameter vector. And after that we do a rollout. We don't do one rollout because we have stochastic transitions. So we do a bunch of sufficiently high number of rollouts, we collect the trajectories, and along the trajectories we're going to measure approximately the TD errors by using a local access to the simulator. So if you're in a trajectory, you were choosing a state action pair, you need to measure the TD error there. What is the TD error? So there is this averaging happening there. So you're going to use your simulator reset to the state action pair a number of times, and then you approximate quite closely the TD error.
00:41:24.802 - 00:42:17.944, Speaker A: So that we are using very, very strongly the local access model, that you're able to do that in the online access model, where you can only run trajectories, it would be really hard to get back to exactly the same state action pair. So these are strong use of simulators. And once you did that, you're shrinking the parameter space and return if the parameter space didn't change in the actual argument, it's slightly more complicated than the file. You seem to have made a sort of careful choice to approximate the value function and also use a q function like. That's right, yeah. Is there some insight into why or. Well, it's because we want to get rid of the max.
00:42:17.944 - 00:43:27.984, Speaker A: So the max is not playing valve is all the linearity and everything, and we are going to see that the product plays much better with linearity in the cube, we would have a max. Yeah, that's why we can only deal with deterministic transitions when you have q standard. All right, so questions? So why do a tensor plan this algorithm start changing the parameter vector? Well, so if you look at, so why is this product a beam compared to using a max? If you look at what this product means, you can write it out as an inner product. Reorder determines introduce these tensor products. And then what you're going to see is that this can be viewed as a leader, uh, constraint on the polynomial of the parameter vector. Right. Just the standard product of the parameter vector.
00:43:27.984 - 00:44:30.748, Speaker A: And so this is a bunch of linear constraints. And so that was why. And what was advantages? Because we can solve linear problems or we know, like for how long? You can just add constraints and, and eventually find the solution when, when you're exhausting all the dimensions. Right. So after you adding d plus one over a constraint, then you shrunk the parameter set and you found the solution. That's why we are switching from the max to the product. The other question is, what was the role of optimism here? I already gave you a hint that the algebraic biomass doesn't go both ways.
00:44:30.748 - 00:45:21.420, Speaker A: There could be policies for which the algebraic biomarker equation holds, which are not optimal. Polystyrene. And the way to get out of this, and you want policies which have high value. So a natural choice then, is just to be optimistic with respect to the initial state, because when that happens, when that happens, then you can guarantee that the policy, whenever you're stopping, the policy that you're going to get, if you haven't terminated the optimal policy, is going to be optimal. I have two minutes left. So this is the result that you can prove. And so, back to Emma's question.
00:45:21.420 - 00:46:31.090, Speaker A: So what's the insight about why the action set has such a huge influence? We were doing this tensorization, this algebraic trick, because of that, d to the power of a appears, and you can predict from that that things are not going to go well if the number of actions is going to be higher. Now, the question is, is that really true? So, that's another paper. I won't have time to explain it. It's basically an information theoretic argument that somehow, having already polynomially many actions in an age horizon problem, is going to throw off all the algorithms. Like, you can hide rewards, and no argument is information to be able to find them. All right, so one slight note about this is that in bandits, this is totally not true. So you can have large extreme spaces, you have a horizon of one, and disregarding computational problems, you can solve all your online problems.
00:46:31.090 - 00:47:04.484, Speaker A: Right? Like need a bandit is an awesome topic, regardless of the number of actions you can have infinitely. Anyway, so this is the full table. And then there is a question of whether you can get. This is just great efficiency. So when can you do it computationally efficiently? We know that in this case, you could do it, and then you should talk to Gaurav. I don't know where he's. He's not at the moment here, but he's staying at Simon's.
00:47:04.484 - 00:47:53.708, Speaker A: And yeah, he'd been working on this problem. We've been working on this problem. And in short, there are already some resides by them, published at CoD this year, that show that under some conditions, some of these problems cannot be solved computationally efficiently, and the prediction is that, well, unfortunately, this is going to be true for the remaining problems, too. Okay. All right, so, wrapping up. So, bamboo's modest optimism didn't hold up well, as far as successive approximations are concerned. It seems that when the algorithms can do something, they're not really doing that.
00:47:53.708 - 00:48:45.424, Speaker A: So that's kind of like a very interesting point to me. If you try to do the successive approximation type of algorithms that they were introducing, they incur a lot more errors. So they're blowing up errors way faster than this version space shrinking algorithms. Algorithms. Yet, if you think about computational efficiency, it seems that they are onto something, because the version space shrinking algorithms can only work in very specific cases. And in all the other cases, it's more like a DP solution that we are having. And another big conclusion is that even if you have strong function approximation, you will have to leave this approximation error blow up due to exploration.
00:48:45.424 - 00:49:53.442, Speaker A: Unlike in bandits, large action spaces are going to be a nightmare. And I have a bunch of open problems, but I just stop here. Like, one question, could you say a little bit about the open problems? So, yeah, given everything that was built up, it's very easy to state them. So we don't know whether the query complexity can be kept, uh, polynomial in this case. When you have few sterilizability and you have constant number of actions and the transitions are stochastic, we only know there's a positive answer for deterministic transitions, for stochastic transitions. Just, we have no idea. It's very intriguing.
00:49:53.442 - 00:50:47.164, Speaker A: The next one is, what is the computation complexity when you have constant number of actions and you have restructured stylizability globally. And the next one is that if you have online access, so you don't have this under couple of things. If we had global access or we have a newer paper that says that if you have local access, then you can get every single time. And, like, it's beautiful. But we don't have the same designed for online access, which is kind of the real deal. If you're interacting with real environments, maybe there is a separation between when you're interacting with the simulator, even if it just provides you local access. Could be that you can solve some of these problems and you can't solve them if you only have online access.
00:50:47.164 - 00:51:23.662, Speaker A: And then, like, there are vaguer open questions like, how to do this nonlinear functional approximation. And Ben's paper obviously, was considering counting section spaces. Okay. There was a lot of other structure there, but clearly, from this work, if the number of actions exploding in worst case, like, not something can be said, but that can't be the end. And then it seems deterministic dynamics is helpful. You can do things with this factorization. I don't know.
00:51:23.662 - 00:51:39.654, Speaker A: Like, these are. These are bigger open questions. The first few are. I see. Thank you. Back here to left 15.
