00:00:00.240 - 00:00:47.234, Speaker A: Today's workshop is about proof complexity of solvers, and the first talk is going to be by Paul Bean. Paul doesn't really need an introduction. He's legendary in proof complexity and complexity theory in general. But what I would like to focus on, or as I introduce him, is the fact that this entire area of applying proof complexity to sat, one could argue, was initiated by Paul with his seminal results in the early two thousands, where he showed equivalence between CDCL with some pre processing and resolution. So without any further delay, Paul, the online floor is yours. Thank you.
00:00:47.614 - 00:01:27.494, Speaker B: Thank you, Vijay. Thank you for the generous introduction. So I'm going to talk about a story that connects proof complexity and verification, and illustrates some of the issues in going back and forth between the two. So this is about using resolution and cutting planes for verification of nonlinear bit vector properties. So let me talk about what that is. So in general, this involves so what is bit vector verification? This could be in hardware or software. I've shown a little piece of software.
00:01:27.494 - 00:02:28.684, Speaker B: In general, you have some code that involves bit level operations on things like integer variables plus arithmetic operations, and you have some specification that also might involve some of those operations. So the general way things work with bit vector verification is you convert the integer variables into blocks of bits, 32 bit bit vectors, for example, if they're int's, and directly model the int operators in arithmetic operations and bit level operations. And then you write a formula over that theory asserting that the code does not follow the specification. So you send that formula to the solver. If the solver says that formula is unsat, then you found that the program follows a spec. Okay, so that's the basic form. There are variations in hardware and software.
00:02:28.684 - 00:03:52.098, Speaker B: A big challenge of bit vector solvers is nonlinear arithmetic. So there's actually been tremendous success if all the arithmetic in your code is linear, or if you're your target, spec is linear. But there have been major problems with nonlinear arithmetic and no bit vector solvers even close to working well in general at verifying hardware implementations that involve multiplier circuits or software that involves multiplication operations and pretty much anything else. Though in parallel with some of the work that I'm talking about, there's actually been significant recent progress on verifying multiplier circuits in isolation. And Daniela Kaufman gave a great talk in this in the Beyond SAT workshop, and I highly recommend looking at the recording of the talk. So exactly how does this challenge come in? So, in general, in hardware, you directly model the circuits using boolean logic and gate variables, and software you model, but you won't have these arithmetic operations in software. You're going to directly model the operations, including arithmetic and the bit character language, and you will actually apply theories directly to simplify or prove via pre processing the various properties.
00:03:52.098 - 00:04:36.084, Speaker B: So you can use theories of uninterpreted functions or arithmetic identities at that point. But often that only gets you so far. You've simplified some of that arithmetic away, but there, for example, still may be a bunch of it. So if you don't have anything more you can do, you what's called you bit blast the formula. There's a more pedestrian term of flatten it to convert it, the arithmetic theory, to fixed bit width, say. And typically for practical interest, you'd need at least 32 or 64 bits. And what does that mean? So we have these arithmetic operations that are supposed to do addition, multiplication and so on.
00:04:36.084 - 00:05:16.874, Speaker B: But you only have boolean variables now. So what you do is you pick some circuit that evaluates that property and you add gate variables and constraints for that circuit in order to evaluate it. Now the entire result is a boolean formula, and you send that, say, to a sat solver. So that's the general approach. And here's an example of what you might do as your circuit, for example, for addition. So if you have a one bit adder, you would have a single full adder circuit outputs the majority, sorry, the carry out is the majority. The output is the parity.
00:05:16.874 - 00:05:54.154, Speaker B: Actually, for things later, we're going to want to think of this in terms of a conservation of weight constraint, which is an arithmetic constraint. So the normal thing, for example, you're translating to clauses. You translate that majority in parity to clauses. We're also going to look at the case where we want to translate this to the conservation of weight formula. And then you stick these full adders together to get, for example, a ripple carry adder circuit. It doesn't have to be depth optimal or anything like that. What you want is ease of verification.
00:05:54.154 - 00:06:41.116, Speaker B: For multipliers, there are even more options. Here's an example circuit, which I'm going to use as a running example, and it's actually maybe the most simple and natural circuit. So you write out the ordinary bit tableau that you would do using the elementary school method except over bits, and then you add up the bits. And this is a nice way of doing it as you're going along using the one bit adder. So that's the array multiplier. And I'm going to call that layout of the bits from the tableau, which is the elementary school method. And the tableau is a nice way to look at what's going on.
00:06:41.116 - 00:07:27.014, Speaker B: So that's what you'd have as a circuit, for example, if you wanted to verify that a multiplier was commutative. So you fix some bit with n, you construct the bit blasted sat formula for involving the gate variables for each of the two circuits. So it's the same shape circuit. We've put in the X and Y inputs in different positions, so they're in some sense different multiplier circuits. And then we add in the constraint, which says that one of the corresponding output bits differ. So that's the natural formula for this question that you get from this bit blasting. And here's what happens if you try to run it into a Sat solvent.
00:07:27.014 - 00:08:00.760, Speaker B: You can see that we wanted 32 or 64 bits. You get timeout, for example, with minisat, you can get a little better with others, but it's just a convenient example. You get a timeout at very few bits. The formulas aren't actually that big. There are thousands of variables, hundreds of thousands, actually. It breaks right around 1000. And then Armand Berry observed that Sat solvers cannot solve this for 16 bit vectors, which might be the first sort of interesting case for any multiplier circuit.
00:08:00.760 - 00:08:37.214, Speaker B: Doesn't matter what one you stick in. And he gave them some of these as challenge problems for the SAT race, for the SAT competition. And the solvers were simply unable to do this. So, I'm going to be discussing arithmetic identities like this as indicators of complexity. And this is actually what Armin suggested as well. If you didn't have a specification, you'd actually have to use these identities even to define what you meant by correctness of a multiplier. You can use these identities to do that.
00:08:37.214 - 00:09:24.430, Speaker B: But in general. So in general, it turns out for the linear case, they're easy to check, but it's hard to check in the nonlinear case. Commutativity is just one example. It's actually one of their others as well. Some can be even worse. And the key point is these are indicative of the kinds of issues that one is going to encounter when you start combining these identities, which might get preprocessed away in your bit vector solver, but you want to combine it later with some other properties, property of your circuit. So you've got a multiplier, but you're using bit level logic as well.
00:09:24.430 - 00:09:28.078, Speaker B: I have a question, Paul. Sure. Just to get to an intuition.
00:09:28.166 - 00:09:33.806, Speaker C: So, if y in the commutativity identity, if y where a constant like five.
00:09:33.950 - 00:10:01.924, Speaker B: Would it be easier that's fine. That's in that multiplicative identity. Up top. So why linearity? If one of the arguments is a constant, that is something that current solvers can handle, but they can't handle the things. And typically smallish constants, but in general, they're fine with constants. So things are linear in that sense because they've got constant coefficients. Yeah, thanks.
00:10:01.924 - 00:10:21.744, Speaker B: Yeah. So we're interested in the general case. You might, for example, be interested in software. Hey, let me check the hash functions. And I really care what they are in my cryptographic generator, things like that. So there are lots of instances where we really want real multiplication. So.
00:10:23.444 - 00:10:24.584, Speaker C: Another question.
00:10:24.884 - 00:10:25.624, Speaker B: Sure.
00:10:25.924 - 00:10:29.904, Speaker C: Do we know what is the complexity of resolution for commutativity?
00:10:30.244 - 00:11:36.794, Speaker B: Okay, that. Fancy that you should ask that. This is resolution complexity a fundamental obstacle? You just anticipated the next slide, and this was actually a conjecture a few years ago, is that the reasons, and it's pretty obvious you can actually plot it, that they're actually taking, the solvers are actually taking exponential time to decide this, just even commutativity. And the reason conjectured was that resolution proofs require exponential size. And there was a strong feeling about that. So this is what actually got me interested in the question of resolution complexity of these questions that got us interested, I should say I forgot to at the start. This is joint work with my PhD student, Vincent Liu, who graduated last year, Jo Devrint and Jan Alfers and Jakob Nordstrom from Lund and Copenhagen.
00:11:36.794 - 00:12:11.450, Speaker B: I forgot to say that earlier. So Vincent and I started looking at the problem. And so now, if you think about the question of telling that these two circuits are different, a natural thing is start checking the bits from right to left, seeing, okay, they're equal. They're equal so far. Okay, I found an inequality. If you're thinking in sort of DPL framework, that's the right thing. If you're thinking of resolution proofs, think of those as the last steps of the resolution proof.
00:12:11.450 - 00:13:09.104, Speaker B: But now we've got these two circuits, and we've got an output difference. But if you look at the values, they're sensitive to all the prior bits. Wow. I have to be really careful what I touched. In general, if you would look at this and go through the obvious resolution proof using the properties, the obvious proof that this is unsatisfiable, that it's not possible to have this combination, is exponential. And key thing that Vincent realized is that there's a critical strip of just the prior log in columns that suffice for this unsatisfiability in fact, we can complete. Well, we already knew we could ignore the bits to the left, but we can also completely ignore the bits to the right.
00:13:09.104 - 00:13:41.724, Speaker B: And so we get these critical strips. And in these critical strips of log n bits, notice the input bits are completely unconstrained. So they just have. They only feed in. There are no values for those. And this is also unsatisfiable. And the reason is, the internal bits in the strips, if you're doing commutativity, are actually the same bits, but they're be muted in different places.
00:13:41.724 - 00:14:24.134, Speaker B: The carrion bits may be completely different, but if you had your circuit there, you'd know that they added up to the same sum. But even without that circuit there, the difference in value is at most n. If you think of the right hand entry as being like the ones column. And if you add that up, that's not enough of a sum to create a difference in the high order bit, say, to have the left one be one and the right one be zero. And so we own. That's why it's unsatisfiable. And so now, the one other thing we need is that each critical strip.
00:14:24.134 - 00:14:52.864, Speaker B: Wow. Has a polynomial size regular resolution refutation. And this is because this graph has log n path width. In fact, it's a simpler graph than general, simpler proof than general regular resolution. It's ordered. There's a fixed order in which you can do the resolution on every path, and that's polynomial. And that result goes back to Dektor in 96 and others.
00:14:52.864 - 00:15:34.804, Speaker B: So the net result of this is that for all of the multipliers, all the standard multipliers, like array, diagonal, and booth, there are no. Sorry. There are poly sized resolution proofs. In fact, you can extend this not just to multiple commutativity, but any degree to identity. Vincent and I had the paper on this in Cav and journal version in JCM this year, last year. So, how big are they? Well, they're okay ish. So, the proof size for bit width n is about n to the six, where we should compare with the circuit size, about n squared.
00:15:34.804 - 00:16:20.924, Speaker B: That's potentially on the edge. Some identities are a bit worse. This is pretty much the worst of the ones because of the computations back and forth. But sometimes we know this was just a theory result. Often we find there's other proofs that the solvers see that maybe we don't. So the question is, now that we've got polynomial size, how does this work for practical CDCL solving? Well, we do the strips version. I put it side by side with the full version, and we get some improvement, but we don't really get much further.
00:16:20.924 - 00:17:16.714, Speaker B: So, okay, so we worked a while. Can we give some hand holding, can we make some tweaks to these solvers? And we tried a number of them, but minisent was just the simplest for us to work with, which is why I'm reporting those. It doesn't find these proofs. Even given the division into strips, they're at least a factor of n bigger. They do seem to have polynomial dependence, but it's horrible. And the polynomial crossover point is actually not much different from the exponential. And with extreme handholding, force fed order lots of other things, we can't get closer empirically than a factor of square would have been of the best end of the six seems too large.
00:17:16.714 - 00:17:59.418, Speaker B: In any case, we want to target much bigger values than this. So the only solution was to go to a stronger proof system. And here's all a number of proof systems we've heard about and their relationships. We're going to focus just on three. So, cutting planes and polynomial calculus in addition to resolution. And the polynomial calculus story started in parallel with our work. So, reminding you, polynomial calculus, you have, you convert each line into a polynomial equation, you have rules on derivation.
00:17:59.418 - 00:19:16.794, Speaker B: If you've got something at zero, their sum is zero and their product is zero, and you use properties of boolean variables to say that the square is equal to the value of the variable. Polynomial calculus has implement, you know, models, the steps of the Grosvenor basis reduction algorithms. So that's in part its origins. And we know that polynomial calculus is stronger than resolution. So we'd like it to be the case. That means the Grubner basis reduction would be more efficient than sat up to usual implementation factors. Well, no, first of all, in most algebraic problems, and there are a number of issues with this, one of which is that Grubner basis reduction algorithms destroy the twin variables or dual variables that one actually needs for the polynomial calculus, sufficient simulation of resolution, and the other is just general overheads, of course, and it's just not particularly well tailored for things involving clauses and things and such.
00:19:16.794 - 00:20:03.164, Speaker B: However, for certain algebraic problems, you can like these multipliers in isolation, it works extremely well. So Kaufmann et al. Were able to verify 1024 bit integer multipliers using Grosvenor basis reduction techniques. And in fact, their proof in polynomial calculus actually is order n squared, which is optimal. It's the size of the instance, and it's at the word level. And notice the coefficients here required are relatively large. So they're big coefficient, and you can actually generalize this to any word level, ring identity, and not just degree two.
00:20:03.164 - 00:20:56.494, Speaker B: Now this is great. And this was kind of the state of the art before we started work on the next project with the team at Lunden, Copenhagen. And one thing we realized is that these polymel calculus proofs are treating the numbers themselves as individual units. They're at the word level. If you actually wanted to integrate this with logic, you'd need those values at the bit level. And it turns out, if you just have the statement that two words are equal, here I'm shown the them as x, y and yx, but they're just two abstract words. And you want to know, we want to derive from that, that a given pair of bits is equal even if you had twin variables.
00:20:56.494 - 00:21:44.126, Speaker B: The proof size to derive that is exponential in the number of bits of the word. So, integration of the polynomial calculus with the bit level things is itself a tricky process. So that is one of the things we're actually already starting to look at. Cutting planes proof. And so, in cutting planes you have linear inequalities where the coefficients are integers, and in general they can be arbitrary integer variables. Xi, but we're going to think about them just for boolean variables. So we have non negative linear integer combinations.
00:21:44.126 - 00:22:16.308, Speaker B: So if l one and l two are non negative, so is their non negative linear integer combination. And also division. If we have a linear inequality where the left side has a common factor, we can divide it out. So the left side has integral values. And now the right hand side is a fraction that could be rounded up because we know the left hand side is an integer. So that's cutting planes. And if we're going to use cutting planes, we're going to need something that implements them.
00:22:16.308 - 00:23:14.408, Speaker B: And the best pseudoboolean solvers use features of cutting planes proofs. Some do not, but the very best ones do. And we'll see that that's going to be absolutely essential for the success of using them. So what about this issue that was a problem for bit vector solvers? Well, if we already had the word level equality, for example, and it's not required that it be a word level equality, you have these constraints, then cutting planes can in fact derive all n bit inequalities, all of them at once, in order n steps. And this is with rounding sat a particular pseudo boolean solver, one of the best current solvers. And we can do it in fractions of a second for numbers that are plenty big enough. In contrast, this is not true for every pseudo Boolean solver.
00:23:14.408 - 00:23:46.946, Speaker B: So saf four j has multiple instances, including one that is cutting plane space. But if you use the resolution version of them, it fails. And naps also fails on very small bit widths. I mean, you'll see that's 28 just to extract these bit level bit equalities. So that's great. Assuming we can get the word level equalities correct. And in fact we can get small cutting planes proof.
00:23:46.946 - 00:24:37.896, Speaker B: So the word level equalities for not all ring identities, but a large class that we call too colorable. We don't know you can't for others, but these are the ones we were able to do. And examples include commutativity, distributivity and others like that. And of course, because of the previous results, there are n squared length cutting planes proof for bit level ring identities, which is the first one that can do identities at the bit level. And so, so that's very nice. I want to give you a basic idea of how this works. We can do, because these are nonlinear properties, but we're using a proof system that only deals with linear constraints.
00:24:37.896 - 00:25:26.584, Speaker B: So the general idea is we can use only a little nonlinear linearity of a time at a time. We actually found it convenient to describe these in terms of a slight extension of cutting planes proofs, where we allow a small amount of nonlinearity. So we've got k terms where each of degree d or less, and each term, well, it can actually have a one linear component and then just multiplied by a monomial, or it could just be a monomial itself. And you just have the constraints on the combination rules, is that it retains this property. So how do you simulate it? Well, here's a bit of intuition. So here's the theorem about how you can simulate it. So you can expand the number of lines a little bit.
00:25:26.584 - 00:26:39.070, Speaker B: You can simulate this little bit of non linearity, sort of a geometric intuition. If we had some degree two curve that was splitting out some points, we would just replace it by two lines. You can think about the general form that simulation isn't, you know, there's some tricks to it, but you should be. It's an intuitively not a difficult one, but there are some nasty technical difficulties if you're not careful. So we get the two colorable identities by showing that we can get n squared length cutting planes, proofs where they're in this little bit of non linearity where k and d are constant, d is two and k is, I don't know, three or four. So with this constant overhead simulation, we get an n squared type size proof in standard cutting planes. So what about finding these cutting planes proofs via pseudoboolean solvers? And the two solvers that we worked with are sat for j in its full cutting planes mode, and rounding satisfaction, which I mentioned earlier.
00:26:39.070 - 00:27:19.852, Speaker B: And these have two very complementary approaches to how they derive and learn new inequalities. Satvage is saturation based, which you should check Jakob's bootcamp talk for the details, but basically involves thresholding coefficients by the right hand side. And it turns out these are fast for proving the word level equalities. Rounding set is division based and fast at extracting bit level equalities. Okay, so what?
00:27:19.908 - 00:27:22.344, Speaker A: So there is a question, Paul.
00:27:22.924 - 00:27:24.036, Speaker B: I'll read it out.
00:27:24.180 - 00:27:24.884, Speaker A: Yeah.
00:27:25.044 - 00:28:21.526, Speaker B: So meena Mahajan asks, what are the nonlinear KD terms interpreted in terms of the ky partial product? Well, you've got to do a very basic thing. I mean, there's a very basic one that's, that's three easy inequalities that I'll mention for the case of just the tableau. I've actually got that on a later slide. In general, for example, for distributivity, there's a little bit is kind of at the edge. So there are just a few things that are representing partial products as you would sweep through left to right, comparing the undistributed and distributed version. So I don't have, I can't write them down, but they're very short. So does that answer the question? Yes, that's okay.
00:28:21.526 - 00:28:54.870, Speaker B: Thanks. Thanks. Yeah, so these, so what we have to do is we take a combination of these two, which nicely work on the same representation. We're able to do 256 bit commutativity at the bit level. We can do multiplier equivalence checking. We can check against the spec and things like that. It does require that value based version of one bit adders rather than the causal representation.
00:28:54.870 - 00:29:41.654, Speaker B: So that's essentially if you're going to use pseudo boolean solvers, you cannot reduce things to clauses first. Otherwise, at the moment, even though cutting planes proofs can handle it, pseudoboolean solvers at the moment, if you give them clauses, will behave like slow versions of CDCL solvers. So it's absolutely essential that you do this. But that's an obvious thing that any system that's going to use them in the context of bit factor solving is going to do. It's going to know to hand off these one bit adders rather than in this nicer form. Here are some running times. So they're in the matter of not too many seconds.
00:29:41.654 - 00:30:40.000, Speaker B: At the moment, we're limited. They can't handle the more complicated identities such as distributivity. And we don't have a single solver with a single heuristic decision thing that will combine these two methods. To do this, we have to combine them in a more abstract way rather than sort of at the low level. It would be really nice to be able to combine at the no level and a low level and know when to use saturation based or division based reasoning. So one of the things that we can also see is there's another approach to bit blasting. So if you actually want to use these for in bit vector solving, we think there's an even better way to use cutting plane solvers.
00:30:40.000 - 00:31:47.090, Speaker B: And one thing, by the way, for all of these solvers, I guess I'll skip it. But for all of these solvers, 256 bits, that means that we've got to be able to represent in our solver things like two to the 256 or two to the 511, something like that, or 510. So it's important that these solvers be able to use large coefficients. In the original version of rounding stat that we began with, we only had restricted bit widths. Later it was expanded to the many more bits. And so some of our results are with bit level restricted, but it's not just suitably insolvent, but suitably in solving with large coefficients. And so I didn't mention earlier when I was doing the, there was a failure of one of the solvers on the bit extraction, and that was because the coefficients weren't, it couldn't handle large coefficients.
00:31:47.090 - 00:32:22.878, Speaker B: Anyway, sorry, back to this message of approach to bit blasting. So we need this equation, and this is two inequalities. It's written as an equation, but it's really two inequalities in cutting planes instead of what would be 14 clauses. But the key is actually how we can do the inference as a result. Okay, so we can do something more. We could actually represent that addition without the circuit. So we don't need to fully bit blast to a circuit.
00:32:22.878 - 00:33:03.286, Speaker B: We can represent it directly and implicitly. That's a reasonable way. That would be nice to use even the polymer calculus results. But unfortunately that's not quite enough because we can't of the bit extraction problem. But we can do more. We can represent multiplication without a circuit. So we just put in the n squared tableau constraints for those tableau products, and we just write in a sum constraint for that tableau and bypass all the one bit adders and just put in the specification of a full adder.
00:33:03.286 - 00:34:00.590, Speaker B: Now, the tableau constraints themselves are not pseudoboolean, but we can represent them by these three equalities. This is like the simplest case of a small amount of nonlinearity being simulated by pseudo boolean constraints. So how does this work? So this is using this, we can beat bit vector solvers without any other handholding, without combining with this algebraic representation of multiplication. So we did a simple example that just as a test to mix multiplication and bitwise operations. So imagine you have a large constant k. You want to say k times z is at least some input x anded bitwise with k and multiplied by z. So that's a new integer.
00:34:00.590 - 00:34:49.776, Speaker B: We multiply by z. Clearly the coefficient is smaller, so that should be true. And rounding Sat is able to do it. Now, we did these experiments with the earlier version of rounding set, so there's limited bitwidth, but as you can see, boolector times out even at 28 bits. And it's really focused on the bit vector case and z three, which is more general. So a bit slower also times out on these. So we think that this approach of combining these two sorts of solving is really going to be important going forward, future directions, so we can do better.
00:34:49.776 - 00:35:33.794, Speaker B: There's much more to be done in terms of integrating these types of things, using preprocessing to make things great for pseudoboolean solvers. Do this. Algebraic. Can we get good performance on industrial benchmarks that involve multiplication? So, that's an application question. The other side is the fundamental engine side of this cutting plane is powerful, but pseudo Boolean solving implements only a small piece of it. We were not able to solve the more complicated identities, the heuristics and inference rules are not. Well, are not uniformly integrated.
00:35:33.794 - 00:35:55.724, Speaker B: CDCL. Everybody builds on everybody else's ideas, and there's sort of a. A great path that's been going forward. There have been these crucial, sad, insolvent sat improvements. We think this application might be motivational at the end. So, karam, I see you have a question. Please.
00:35:56.064 - 00:36:01.168, Speaker D: I'm gonna wait until you're done. It's kind of a random question, so I'll wait for you.
00:36:01.336 - 00:36:19.840, Speaker B: Okay, so, finally. So this is, you know, relatively, you know, fresh territory. You know, go there. Pioneers. We think this is a great place to. For future work. Thanks.
00:36:19.840 - 00:36:21.844, Speaker B: I'm done. Go ahead, Kara.
00:36:22.624 - 00:36:26.964, Speaker D: Okay, so something struck me, which is these circuits are very regular.
00:36:27.464 - 00:36:27.960, Speaker B: Yes.
00:36:28.032 - 00:36:59.474, Speaker D: Is there a place for induction on these things? Like if you can show the distributivity holds for eight bits, can you learn something from that? And I know that these circuits are generated, you know, from a, you know, by some tool that increases bit width and so on. So let's assume that that generator is correct. Is there something that you can learn from a narrow bit width multiplier that you can then say, I can then, you know, lift it to 128 or 256?
00:37:00.474 - 00:38:03.996, Speaker B: So it's really unclear. So in general, first of all, this would be a multiplier in the like, we've described things in terms of multipliers, somewhat in isolation or compared to each other or compared with a spec, but in this larger combination where you have some software that, where you are checking the integration with, of a multiplier with other things. For example, if you wanted to verify things like AE, where you're doing bit twiddling operations, there is no induction that's going to get you to work. So yes, in isolation we have the theories that we could use as preprocessing, but we want to avoid that because those are bypassing, those don't help you. And the induction for those does not work in the context of other bit operations. That's why we, we don't think about that. So otherwise you could say, well, it worked well for my eight bit multiplier.
00:38:03.996 - 00:38:08.064, Speaker B: That should be good enough. I'll just trust it for my 128 bit multiplier.
00:38:09.164 - 00:38:10.104, Speaker D: Thank you.
00:38:11.124 - 00:38:21.212, Speaker A: Any other questions? If not, Paul, thank you very much for a fantastic talk. I think Ross has a question. Do you have a question, Ross?
00:38:21.388 - 00:38:22.504, Speaker C: I have a question.
00:38:23.364 - 00:38:25.024, Speaker A: Yeah, well, go ahead, Moshe.
00:38:25.704 - 00:39:04.894, Speaker C: Well, you describe here reasoning about this, let's call it the tableau product, but people usually implement much more sophisticated and more economical multipliers. So the real and the correctness there, the real challenge to me is not to prove, let's cumulativity of tableau, which is a nice warmer. But the real honest to God challenges prove that the sophisticated multiplier with this recursive partitioning and maybe with all kind of tricks to reduce the number of beats, is equivalent to the tableau multiplier.
00:39:06.554 - 00:39:40.384, Speaker B: Sure. So actually, so there are actually two different. So on the hardware side that's true, on the software side, that's not what you're trying to do. So you don't care what multiplier you're doing. Now, in fact, these techniques will. So I just described array multipliers. So the question if you're doing these partial, if you're computing these partial products, as soon as you have them, you could do wallace trees and whatever.
00:39:40.384 - 00:40:24.856, Speaker B: There are other multipliers or other tricks where you, you know, you don't use full adders, you do other things. I totally agree with you. If what you want to do is check your multipliers in isolation, then actually, that polynomial calculus approach is not a bad approach. There are some issues there as well. Cutting planes, in principle, can handle all that. It's just our pseudo boolean solvers are too weak to convert those clauses into constraints. So if we did something like crypto Mini sat, where we could take those linear constraints and infer linear constraint from clauses to do that type of thing, then we would actually be fine with testing those.
00:40:24.856 - 00:40:47.084, Speaker B: But it is a great area and actually, how you combine those two. So I think some of that is, that's super important as well. And what I've shown so far doesn't address that. In principle, cutting planes can do things like that. But we just showed a simple example.
00:40:49.624 - 00:40:54.616, Speaker A: Mina, you had raised a hand. Did you want to type in your question or.
00:40:54.720 - 00:40:57.312, Speaker B: No, no, that was my mistake. I lowered it also.
00:40:57.448 - 00:40:59.568, Speaker A: All right, Ross, did you have a question or you.
00:40:59.616 - 00:41:25.342, Speaker B: Well, if you have a time, a quick question. Paul, great talk. Other lessons in how to accelerate sad instances of these problems. So. Right, so the. In general, I mean, this is, this is not from this. So in general, the issues with accelerating sadness is this is related to some earlier work I did quite a long time ago.
00:41:25.342 - 00:41:53.212, Speaker B: What you're trying to avoid is getting yourselves into subproblems that are unsatisfiable, and that's difficult. And in general, we just get intuition about trying to make sure that the SAT problems are avoiding. These solvers are avoiding nasty unsat sub problems. There is a question in the chat supertik, do you want to ask it live?
00:41:53.388 - 00:41:55.264, Speaker A: Quick, quick question. Suprathik.
00:41:55.824 - 00:42:10.832, Speaker C: Yeah, thanks, Paul. So, I mean, are there properties for which we know that, I mean, it's better to use resolution than cutting planes because it's going to blow up badly with cutting planes.
00:42:10.928 - 00:42:24.694, Speaker B: No, no. So, in fact, we know that cutting planes is always more efficient than resolution. And you only need coefficients, too. At the biggest coefficient. The issue is practical implementation.
00:42:25.034 - 00:42:28.054, Speaker C: Yes, I'm talking about for practical solvers.
00:42:30.074 - 00:42:48.904, Speaker B: It'S a factor. It's a generic uniform factor. It's just you can do the local decisions incredibly quickly with SaT solvers and there's just more data structures to manipulate. There's no special blow up. It's just a factor worse. That's all.
00:42:50.324 - 00:42:55.244, Speaker A: Thank you. Thank you for the great talk, Paul, and thank you, everybody, for the fantastic questions.
