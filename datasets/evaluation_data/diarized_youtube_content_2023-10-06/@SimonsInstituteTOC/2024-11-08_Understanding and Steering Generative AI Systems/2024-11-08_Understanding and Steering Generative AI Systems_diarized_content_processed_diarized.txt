00:00:00.400 - 00:00:10.264, Speaker A: From statistics to robustness and now more alignment. He's also worked a lot in forecasting. So today he's going to tell us more about interpretability and understanding what's going on inside these models.
00:00:10.392 - 00:01:02.690, Speaker B: Great. Thanks, everyone for coming here. So I'm going to be trying to give, I know, a little bit of tutorial and a little bit of talking about some recent work from our group. So I'll start by focusing on interpretability and then kind of generalizing a bit outward from that to generally how to understand AI models. So first, I guess, what's kind of, you know, why do we care about interpretability? So interpretability is sort of about trying to understand the representations of neural networks and ideally kind of the mechanisms underlying those representations. In principle, if we can do it well, we can do things like predict outer distribution, behavior repair models, improve our understanding. There's kind of, you know, various nice papers that I'd encourage you to check out that show ways of doing each of these, but often it involves.
00:01:02.690 - 00:01:52.670, Speaker B: It requires you to kind of do things that are fairly specialized and kind of by hand. And so we want ways to kind of scale these approaches to large models. And so I'm going to, I guess, start by just giving a case study on how we can do this via kind of decomposing a model into smaller pieces automatically and then building it back up. This is from a particular paper that I worked on with Alio Scheffros and Yossi Gondelsman recently. But the techniques in it are kind of fairly general and kind of characteristic of some ideas that are currently generally being applied in this field. So I guess let me start by just giving some background. The model that we were trying to understand is a model called Clip.
00:01:52.670 - 00:02:45.968, Speaker B: It's a joint text image embedding model. So that. What does that mean? That means that there's kind of a text encoder and an image encoder, and you can either hand it an image I or a textt and it puts them into some shared embedding space so you have some kind of semantically shared vector space. And the way it was trained to kind of get this property is that you train it on images and their captions and you want images, the embedding of an image and its caption to have high cosine similarity and an image in a random caption to have low cosine similarity. And it's kind of the backbone of a lot of other models like stable diffusion, Dall E, midjourney, and so on. So it's a Kind of natural model to try to understand. And so I'm going to talk to you about how we can kind of try to look inside this model, decompose its internal representations, and understand different things.
00:02:45.968 - 00:03:17.782, Speaker B: So, for instance, you might want to understand how different directions from specific components of the model affect the representation. So this is showing, for instance, a few different attention heads. I'll explain more about those in a bit that do different things. So you might imagine if I put in this image of a purse, there's one attention head that has to do with animals. And so it says, okay, I see this image. What do I see? Well, I see something that's more like a tiger than a cat. If I'm only thinking about animals, that's what I'd think.
00:03:17.782 - 00:03:53.376, Speaker B: Another one that is more about classifying common objects. And it says, okay, this is kind of like a purse, kind of like a bag. And then maybe another head that's just thinking about texture, and it says, okay, this is more like polka dots than, like, stripes. And you might have a lot of different components of the model that specialize in these kind of different semantic pieces that then combine together into a final representation that kind of tells you everything about this purse. And so we'll talk about how you can kind of do these sorts of decompositions. Another type of decomposition you might want to be able to do is understanding how different locations affect the representation. Right.
00:03:53.376 - 00:05:03.070, Speaker B: So if I ask, okay, if I'm trying to figure out if this is a bag or a tiger, which parts in the image help me differentiate that, Maybe these straps really tell me it's a bag. But if I look at just this, it could easily be a tiger or a leopard. And this is another type of insight that could be useful to get that could also tell us if the model is actually attending to the same things we are and taking the same things away from an image that we would take away, why would it be useful to be able to do these things? Well, one is you might be able to identify and fix spurious cues. So an interesting example of this is that if you show Clip this image and ask it how many people there are, it actually thinks that there's six people. Why is that, you ask? Well, it turns out that there's an attention head that is, like, generally about numbers, but it simultaneously is handling the symbolic representations of numbers and counting as a quantity. You'd ideally want those to be different, different mechanisms. But in clip, it's actually the same, same attention head.
00:05:03.070 - 00:05:37.960, Speaker B: So it kind of gets confused. Another is that you can kind of get things like attribution and localization for free. So if you say, okay, what in this photo makes it a photo of a pyramid, it might highlight the pyramids. If you ask it what makes it a photo of a camel, it might highlight the camels. And that can be useful for actually like identifying the specific kind of segmentations of objects and images. And we'll actually see that you can get really strong segmentation algorithms just by seeing what the model itself has learned about what to pay attention to. Then finally, you can do things like property specific retrieval.
00:05:37.960 - 00:06:17.448, Speaker B: Maybe I want things that are similar to this object, but only on the axis of what animal it reminds me of. And so then I want to look for the components of the model that are looking at animals. Or I want only the object it reminds me of. And so I get the components that have the same object. So these are different things you can do to be able to do this. I want to walk us through a kind of combination of mathematical claims and empirical claims. So this first one is an empirical claim mostly.
00:06:17.448 - 00:06:49.140, Speaker B: The second one is mostly a mathematical claim. And then these are maybe a mix of the two, although more on the empirical side. So let's start with this one. Clip's output is generated by attention heads. So I need to first maybe tell you a bit more about clips. So this is maybe a helpful general tutorial on just what do transformer models look like? Or have we already gone over that earlier in the day? Or is this a good reminder? Good reminder. Okay, great.
00:06:49.140 - 00:07:28.754, Speaker B: So here's what clip does on an image. So first it takes an image, it splits it into a bunch of patches. So think of it as just like little small k by k patches for some small value of K. And it embeds each patch as just some vector. So there's some linear transformation that goes from this patch, which is itself a vector, because it's a collection of pixels, into some embedding space. And then there's some iterated set of transformations that get done. There's two types of things that happen if we ignore some details like normalization.
00:07:28.754 - 00:08:29.960, Speaker B: There's only two major things. One is called multi head self attention, and one is called multilayer perceptron. So what does multi head self attention do? It takes this input and then there's a bunch of different things, different attention heads, uh, each of them, what they do is they, at a given token, will kind of pick other tokens to attend to, take a weighted combination of the representation so far at those tokens, and then apply Some low rank linear transformation to get some new output that it then writes. And then this, what it writes out is a sum of of these outputs of all of the heads. And this actually gets added to what was already there. So most transformers have what's called a residual structure where the output at one location is kind of a running sum of everything that's happened so far. So you start with the embedding, you apply these attention heads that are going to maybe attend to the embeddings of other tokens, transform them, add things up.
00:08:29.960 - 00:09:20.810, Speaker B: Then the next sort of thing that happens is called a multi layered perceptron. What that does is kind of what we're more used to in neural networks, where we kind of take this, we expand it out to some larger latent space, we apply some nonlinear transformation to every coordinate, like say a RELU or a gel U, and then we project back down into the dimensionality of this residual stream space. And so we kind of keep alternating between these. There's like these attention layers, these NLP layers. We do this like a couple dozen times roughly. And then you get to an output, and then this output actually gives you a vector at every single token, every single, or I should say every single image patch. But we want just like a single output.
00:09:20.810 - 00:09:59.654, Speaker B: So we actually create a special token called the class token. And the class token is where we actually read the output from. And this is for an image. So there's a separate thing that happens with text that I'm not going to worry about right now, but we want the image and the text to kind of have aligned meanings. So there's also a final linear transformation called the projection layer that pushes this representation of the image into some kind of jointly aligned image text space. Okay, so basically the things to think about here are there's like, you know, this decomposition into patches. There is, and then there's this decomposition across layers.
00:09:59.654 - 00:10:46.470, Speaker B: And actually within a layer you might also, for the heads, have a decomposition across heads. So there's kind of various axes you can break things down in. And so the first thing to note this is just a mathematical point, is that this final output is actually algebraically equal to this linear map P times the sum of the outputs of all of these layers. Because we said that this just takes a running sum. So this is just like mathematically what the output is equal to. Now note that the output of the L th MLP layer depends on the output of the L minus first MLP layer. So we could kind of like recurse this, but we could also Just think of this as a function purely of the input and you have this algebraic decomposition.
00:10:46.470 - 00:10:54.066, Speaker B: So this is going to be kind of nice, but we're going to iterate this more. But before I. Yes?
00:10:54.178 - 00:10:55.570, Speaker A: I don't know about the initial text.
00:10:55.650 - 00:11:01.116, Speaker B: Yes. Oh, like when you say image text.
00:11:01.308 - 00:11:03.840, Speaker A: In my mind, I think the text is also part of the input.
00:11:04.740 - 00:11:25.556, Speaker B: So this. So the way this model is designed is that it could either take. It's. It's free to take either an image input or a text input, or both. Well, actually it will. So it will. I think we should actually think of it as two separate functions.
00:11:25.556 - 00:11:52.206, Speaker B: One is an image embedding function and one is a text embedding function. The text embedding function would. Sorry, I was shining the laser the wrong way. The text embedding function would kind of look like this as well. Instead of image patches, it would be like tokens. It's trained, basically, going back to here. So I'm kind of describing this, which is one function.
00:11:52.206 - 00:12:16.330, Speaker B: There's this entirely parallel separate function which is the text encoder. And it's been trained to make those two functions consistent with each other. But you should, like, kind of mathematically they're just totally separate functions. Does that make sense? Great. Yeah, definitely. Feel free to interrupt with questions. I want to dive as much into the details as people are interested in.
00:12:16.330 - 00:13:10.740, Speaker B: So, at training time you collect a large collection of images and their captions. And so at training time you have an image and its text description. At test time, you don't need to assume that. In fact, often you would use this as a way to caption images by embedding an image and then like searching over text, text descriptions that have high cosine similarity with that image. Or actually, more commonly, you would use this to generate an image from text by embedding the text and then doing some kind of gradient flow over images to maximize the cosine similarity. And this is how, like, if any of you have played around with like Dall E or Sora or GPT4V, like, this is how all of these work. They either use clip or some variant like some other model that is trained in a similar way to clip.
00:13:10.740 - 00:14:01.062, Speaker B: Okay, great. So this is a mathematical claim. Now let me make an empirical claim, which is that in this sum only this term matters. Like the output of the MLP does not matter. This isn't saying the MLPs don't matter, it's just saying the only way in which the MLPs matter is the way that they indirectly affect the attention heads. So how can we check this? Well, what you do is you just. I guess first of all, if you get rid of every single, like if you just replace all of these with a constant, like you just replace them with their mean across some data set so that it no longer depends on the input, this doesn't change performance at all.
00:14:01.062 - 00:14:05.168, Speaker B: Yes. You're not including them in the final.
00:14:05.224 - 00:14:11.904, Speaker C: Answer or you're just removing them. Because I mean this, I. That you have those inputs also actually depends on.
00:14:11.992 - 00:14:32.434, Speaker B: Right. I'm just not included in the final answer. So it still gets to be used in intermediate computations. If I took it out of intermediate computations, this would definitely create problems. Yes, yes. I'm just saying that like in terms of what's writing out to the final output, the only thing that's directly right into the output is actually these attention heads. And in fact it's only the last few attention heads.
00:14:32.434 - 00:14:59.640, Speaker B: So I can get rid of all of these, replace them with a constant. The formal thing we do is called mean ablation. So you can replace it with zero. This is bad because sometimes there's implicitly some bias term that this is computing that has some systematic mean that's not zero. And if you change it to zero, you're in trouble. But you could replace it with a constant that's just its mean across some large data set. This is called mean ablation.
00:14:59.640 - 00:15:52.390, Speaker B: If you do this, you don't hurt performance at all. Then you can start to mean ablate the attention layers, starting from the beginning, going towards the end. And it's only when you're in like the last four, roughly attention layers that anything bad happens. So this is kind of saying that everything that is being written to the output is being written by these last four attention layers. The way I think you should intuitively think about this is if we go back to this picture, there's like a bunch of computation that's happening kind of, you know, localized to these image patches up until the last four layers. And then in those last four layers, this class token is kind of like grabbing all of that information as like an aggregator and then applying like a little bit of cleanup on top of it. And like, you know, that's not something I can prove, but I think this is like the mental model you should have of what's going on here.
00:15:52.390 - 00:16:08.084, Speaker B: Okay, so we could worry about these layers. And in fact, I'll worry about these layers later in the talk. We'll talk about how to unroll this. But for now, let's just look at these direct effects. Yes.
00:16:08.132 - 00:16:08.720, Speaker C: Question.
00:16:18.460 - 00:17:11.222, Speaker B: Probably weird things would happen. Yeah, I haven't thought about it. Um, okay, so now let's go to a now like truly mathematical claim. So remember that you know, by part one we can make this empirical reduction that the output of clip is approximately this linear map P times the sum of the attention layers. Now if we think about what an attention layer does, it is taking a weighted sum across all the other tokens. And since we're reading from the class token, we just have to worry about what is it pulling to this class token or output token. Now we want to open up and say what does an attention head actually do? We can take the sum over all the heads because things are additive across that head within a head.
00:17:11.222 - 00:18:12.660, Speaker B: What happens is it takes a sum over image patches of the attention it pays to that patch. So these are going to be weights that add up to one times some low rank linear transformation times the activations we had at this patch. Now the way this attention is computed is itself via a kind of dot product followed by a softmax operation. That isn't going to actually matter for this talk, but sometimes you would want to open this up further as well and talk about the inputs into this. But the main thing here is I can now actually write this as like the three way reduction of basically like a third order tensor, right. I have this like, you know, three like three index tensor vector C that's indexed by image patches, layers and head indices. That's just equal to this number.
00:18:12.660 - 00:18:56.560, Speaker B: And so this tells us the contribution to the output from a given patch, head layer tuple. And so this is kind of nice because it's saying okay, I can really understand everything here as just this triple sum. And now this is going to let me attribute things back to individual components in a structured way. Token. This is like here we take the image and we just like split it up into little patches and each patch is a token. There's no left to right structure. It's just, it's the only thing that's being predicted is this thing right here.
00:18:56.560 - 00:19:52.662, Speaker B: Yeah. So it's different from a language model in that with like traditional large language models you're trying to predict each next token. This is just trying to compute an embedding of the image. Also for the text model for clip, it's the same that you're just trying to compute a final embedding so you don't have to worry about next token prediction. You're just trying to make these embeddings match well so that's like one way of training models, which is for something like GPT4, what you would typically be doing. Or for like Llama, where's the clip thing? Clip is trained in a different way because it's being used for a different thing. Like I'm not trying to use clip to predict tokens or to predict images.
00:19:52.662 - 00:20:28.126, Speaker B: I'm just trying to use clip as a way to like semantically align images and text. So you should think of it as like a more primitive object in the machine learning stack than something like Llama. If you want llama or gpt4 to be able to handle images and text jointly, then you need something like this to align their semantic spaces. So this would be a component of those models and then this like next token prediction would be kind of its own thing. Okay, great. Okay, so let's go back to this 3D sum. So we've attributed things to patches, heads and layers.
00:20:28.126 - 00:21:08.932, Speaker B: So what can we do once we have this mathematical decomposition? A lot of interpretability is actually like going back and forth between some mathematical decomposition that comes from the kind of inherent dynamics of a model and various empirical observations that allow us to make some progress. So one thing you can do with this decomposition is attribution. I could group things by patch. So I could say for each image, patch I, I could say that CI is the sum across all layers and heads of the contribution of those layers and heads. So I'm marginalizing out over the last two indices. This gives me the contribution of patch I to the output. Now the problem is the output is just a vector.
00:21:08.932 - 00:21:56.624, Speaker B: So how do I do anything with that? Well, I could take the dot product of that vector with the embedding of different text. And now this, for each element in the image is going to give me a number that says how much that patch thinks. This is, for instance, an image of an echidna. And in general, this is probably the nicer one to look at. So if we say an image of a RAM or adult male sheep, this actually kind of outlines the sheep reasonably well. Lots of other attribution methods that were kind of in the literature before do a much worse job at this. So this kind of gives you a way of doing attribution that's intrinsic to the model.
00:21:56.624 - 00:22:42.110, Speaker B: It's saying what was the model trying to write from each token that is now telling us about what's going on in the output. And this is actually pretty good. So you can quantitatively measure how good this is at segmentation by. I could turn these into a segmentation by Just binarizing. I say if the attribution is above some level, it's part of the sheep, and if it's below some level, it's not part of the sheep. And so you can evaluate this on a Data set called ImageNet Segmentation, which has a bunch of images annotated with ground truth segmentation of the objects. Now there's specialized methods that just fine tune to be good segmenters which would do better than these.
00:22:42.110 - 00:23:19.592, Speaker B: But these are kind of the zero shot methods that are just trying to, without additional data, get good segmentation. And if you do this, you actually kind of do better than everything else out there. So it's kind of a nice thing you get almost for free. You can also go further than this though. And so in addition to looking at these patches, I want to also try to understand what each attention head is doing. This is now kind of like the patches in some sense are easier because they're this extrinsic object. Like the patch just already exists as a meaningful physical quantity.
00:23:19.592 - 00:24:10.650, Speaker B: But now these heads are kind of a bit less interpretable, so we need to do a bit more work. Right, so how are we going to do this? We're going to take the same 3D sum and we're going to now group by head so I can marginalize out over image patches to get the contribution of a head, which is associated with a layer as well, to the output. But now how do I interpret these contributions? Like they're these random high dimensional vectors. I mean, I could ask like, I could do the same thing where I say like, how much did this head contribute to it being an image of a sheep or something. But probably almost all of the heads are not going to contribute at all and maybe one or two of them will matter and this is going to be like much less meaningful than this patch decomposition from before. Nati, you look like you have a question.
00:24:17.110 - 00:24:27.390, Speaker C: Through this final result. I mean, if I look at, I mean this is very specific to resnets. It's like saying that in a non resnet nothing at all matters except for the top layer, which of course is.
00:24:27.510 - 00:24:32.124, Speaker B: Right. Well, I guess I would just say in a non Resnet this would be like not the right approach to take.
00:24:32.212 - 00:24:47.532, Speaker C: Right? But yeah, but this makes me question whether why isn't it the rest that it is the right approach because it's contributing in other ways, right? I mean the fact that you can remove it from the final summation, I don't see how that means it's not contributing.
00:24:47.676 - 00:25:04.164, Speaker B: So, okay, I think you'll be happier in like five slides when I get to like. You could think of these as first order effects. I'll get to second order effects in maybe like five slides. And. And then in principle we could unwind this to like nth order effects. Yes, Amit.
00:25:04.292 - 00:25:18.996, Speaker D: So there was actually, if you go back, I didn't get where the capital L song the capital L go. You had the. Yes. So you had L letters, right? Capital L letters.
00:25:19.028 - 00:25:21.636, Speaker B: Ah, yes. So I'm at the bottom.
00:25:21.668 - 00:25:29.510, Speaker D: There is when you said that this is going to be. We can easily write it down. There are little else, but there are no.
00:25:29.810 - 00:25:50.308, Speaker B: So I'm saying if we look at one term in this sum, we can decompose this as a two dimensional sum and therefore the overall sum is a three dimensional sum. Like 1, 2, 3. Yeah, so I'm. Yeah, good, good question. I was a bit sloppy with notation, but does that make sense? Yeah. Cool. Okay, great.
00:25:50.308 - 00:26:31.650, Speaker B: So, okay, so let's go to talking about these heads. Um, so how can we interpret the contributions of these heads? They're just these kind of high dimensional vectors. And so what I'm going to do is I'm going to do some like a kind of unsupervised learning algorithm where I'm gonna try to come up with a basis for each head where it's. This is gonna look kind of like pca, but I want each basis element to be text interpretable. So how can I do that? So the idea is, okay, how would normal PCA work? I'd just kind of like, you know, take all of the outputs of this. I would like get a big data set. I would look at the different vectors that this head outputs across different inputs in this data set.
00:26:31.650 - 00:27:12.938, Speaker B: And then I would look at the direction of largest variance, I would orthogonalize that out. I'd look at the direction of next largest variance, orthogonalize that out. How could I do that in a way where I make sure that each direction is actually text interpretable? So what I could do is, okay, first let's just get all of the head outputs. So these are gonna be a bunch of vectors. So this is the same as how you would start with pca. You're just collecting your data set, then let's just make some really long list of different sentences or different text descriptions, like tens of thousands. In principle you would want this to be like all possible descriptions, but since we're on a computer, we only get to have finite objects.
00:27:12.938 - 00:27:50.094, Speaker B: So it makes a really long list. You could imagine doing search to make this effectively A combinatorially sized list. But for now, we're just gonna have a fixed list. And then we're just going to greedily take across all of, like, right? Each element in this list, if I embed it under the text encoder gives me a vector. So I'm just going to greedily take the vector that explains the most variance. So this is like pca, but I'm restricting to only vectors that have an underlined sentence that support them. So maybe it's the color yellow that I find the direction with the most invariance.
00:27:50.094 - 00:28:26.680, Speaker B: Maybe it's an image with triangles. And then I keep going, and I keep going until I've kind of explained most of the variants. And so you can do this. And then this means that for each head, you get a set of principal components that are actually backed by text. And so these are some of the results. I apologize that the text is a bit small, but for instance, like this one, layer 21, head 10, you get the top one is timeless black and white. The second is vintage sepia tones.
00:28:26.680 - 00:28:58.760, Speaker B: The next is image with a red color. The next is a charcoal gray color, basically, then soft pastel hues. This one's a bit weird. It says picture taken in Laos, but like, for the most part, this is a color head. And like, each of these components is different colors. And you can see it like, kind of also matches, right? This is black and white, this is sepia, this is red. These are the images that have the highest dot product with each of these directions.
00:28:58.760 - 00:29:32.906, Speaker B: And if you actually just look at this across lots and lots of heads, you actually see that many heads specialize to specific roles. So layer 21, head 11 looks at different geographical locations. It has Arizona desert, Alberta, Canada, Rio de Janeiro, Cyprus, Seoul, South Korea. This is the one that gets at the spurious queue I mentioned before. It's a counting head, but it has both. Image with six subjects, image with four people, and then image of the number three. And so this is.
00:29:32.906 - 00:30:32.736, Speaker B: This is where the problem comes, right? This should be an image of three objects, not image of the number three. So we just generated a large database of about, I think about 30,000 different sentences, combination of manual and GPT4, aided generation. And so we didn't, like, look at this ourselves and come up with these words. This is, if you ask for the text direction whose embedding explains the most possible variance in the activity of this head, this is what you get. And so it's actually quite interesting that they're semantically similar, because actually, if anything this procedure is asking for them to be as semantically different as possible because you're orthogonalizing out. Like, you orthogonalize out everything that had to do with six subjects, and then like, the next highest thing was four. You would think it would actually be something that's like, as different from numbers as possible.
00:30:32.736 - 00:30:59.400, Speaker B: But this is saying these heads, like, really do specialize. Yeah. So the. You could do the training sentences. I guess the problem with the training sentences is that you kind like, in some sense, you want to get at, like, atomic concepts. And the training sentences are like, often like compositions of lots of things. Yeah.
00:30:59.400 - 00:31:05.960, Speaker B: Okay. And you, like, this kind of continues. You get like wildlife shapes and so on. Yes.
00:31:06.460 - 00:31:11.760, Speaker A: You were saying that, like, heads specialize, but can they also, like.
00:31:13.500 - 00:31:13.972, Speaker B: Can they.
00:31:13.996 - 00:31:19.910, Speaker A: Also be activated with several things? I guess I'm thinking on the lines of superposition.
00:31:21.050 - 00:31:54.040, Speaker B: Yeah. So I think you probably can get superposition. I mean, you can think of this as a simple type of superposition. You also often will have multiple heads active on the same input that kind of are like, jointly answering a question. We didn't look at, like, more extreme forms of superposition where you have totally different things. I'll actually get to that now, I guess. I think in three slides where if you look at neurons, you get tons of superposition and you get, like, totally different concepts that.
00:31:54.040 - 00:31:55.164, Speaker B: That match.
00:31:55.252 - 00:31:55.548, Speaker D: So.
00:31:55.604 - 00:32:02.240, Speaker B: So we'll see that pretty soon. Okay. Yes. Oh, yes.
00:32:08.580 - 00:32:11.948, Speaker A: Original prompts. But just make we maybe use taxi.
00:32:11.964 - 00:32:13.772, Speaker B: BDM to kind of come up with.
00:32:13.796 - 00:32:16.020, Speaker A: New prompts related to that in a sense.
00:32:17.000 - 00:32:40.158, Speaker B: Oh, how did we come up with the list? So we came up with, we took all 1000 Imagenet classes, which all have a name. And then for each one, we asked ChatGPT to brainstorm 30 descriptions of images that would be relevant to that class. And so that's how you get a diverse set of descriptions. Thank you. Yeah.
00:32:40.264 - 00:32:44.002, Speaker D: I'm still wondering what would the visualization look like for the counting unit?
00:32:44.146 - 00:33:25.240, Speaker B: Great. Maybe I have that. I have something kind of like this. Oh, no, I do have the counting. Ok, so let me answer that also in, like, two slides. So the final thing I want to say here is just like, what can you do with this list? One thing you can do is you can remove spurious queues. So there's this kind of famous data set called the waterbirds data set that has birds, different bird species whose native habitat is land or water, which then also are on, like, water or land backgrounds.
00:33:25.240 - 00:34:07.094, Speaker B: And this kind of spurious cue can reduce accuracy. But if you remove, like, these Heads, the ones that have to do with locations. Then like, you know, in some sense you might think this makes the model strictly worse, but actually it, it sort of significantly improves the accuracy of this model in the presence of this spurious queue. So one thing you can do if you understand what the components are doing is you can kind of do surgery on the model. We're not the first to do this, actually. I think the first was this nice paper by Evan Hernandez and Jacob Andreas all the way back in 2019. But it's a generally nice thing you can do with interpretability.
00:34:07.094 - 00:34:07.558, Speaker B: Yes.
00:34:07.654 - 00:34:12.294, Speaker A: Do you remove the head in that decomposition or do you actually remove the head completely so it affects all the.
00:34:12.302 - 00:34:14.982, Speaker B: Other representations just in the decomposition?
00:34:15.126 - 00:34:16.950, Speaker A: It's actually still active. Technically.
00:34:17.030 - 00:34:39.070, Speaker B: It's still technically active. Yes. Although probably it doesn't do anything because these are all fairly late heads. So the only way it could be active is if one of the heads in the like two layers after it and it's. These are always in the last layers because only the last layers are writing directly to the output. Yes.
00:34:39.570 - 00:34:49.226, Speaker A: If I go back to your previous slide. So those heads are the heads that can be, you know, kind of perfectly explained with the five faces you have.
00:34:49.298 - 00:34:49.910, Speaker D: Right.
00:34:50.210 - 00:34:59.032, Speaker A: Have you also got you to look at, you know, how much of this, the percentage of the heads that you couldn't really perfectly explain with the basis?
00:34:59.176 - 00:35:37.906, Speaker B: Yeah, it's a good question. So generally, I guess what we did is for each of these, we came up with about a 70 dimensional basis for each head. If you do this and then you just project onto that subspace clip's accuracy only decreases by a couple of percentage points. So, so this is saying most of the things are captured in the top 70 dimensions. In some ways this isn't that surprising though, because if you multiply 70 by the number of heads, you get roughly the embedding dimension. So it's almost full rank. I think if you.
00:35:37.906 - 00:35:51.590, Speaker B: We have a plot somewhere. I don't think I have it in these slides. If you were to only do like 10 or 20, then you lose a lot more accuracy. Although you do get things, you do get like non trivial performance. Yes.
00:35:51.930 - 00:36:05.682, Speaker D: What's the difference between like removing the effect of the head entirely like surgery wise and just looking at the decomposition like. Because if you in the decomposition you still have the effect of the output of the head in like future like layers, Right?
00:36:05.786 - 00:36:06.750, Speaker B: Yeah. So.
00:36:08.730 - 00:36:13.730, Speaker D: In the decomposition versus just pretending the head wasn't there in the first place or setting it zero or something.
00:36:13.770 - 00:36:53.110, Speaker B: Right. So removing in the Decomposition means I take out the thing that it's like writing to the final output, but I allow other intermediate components that happen after it to still use what it's writing. If you were to say, I'm like, well, I'm doing something a little bit different, it's a slightly weird operation. You could almost think of it as just subtracting the output of this head from the final output. Whereas, like, a full surgery would also be like forbidding any later components from using it. Okay. Yes.
00:36:53.770 - 00:36:59.870, Speaker C: Are you looking only at the output of the class or all the tokens? Like, only the class token when.
00:37:00.330 - 00:37:20.446, Speaker B: Only the class token. Yeah, because that's the only thing that gets read from. Yes. So, like, if you were doing this with. I mean, we actually, like in ongoing work, we are doing this with Llama, which is a lot more complicated because all of the tokens read stuff. And this makes your life, like, much, much harder. So there is a reason we started with Clip also, which is that it's a big model.
00:37:20.446 - 00:37:34.174, Speaker B: It's like half a billion parameters. But it's still easier architecturally than in LLM. Maybe I'll skip this slide because property specific, which, you know, you query.
00:37:34.302 - 00:37:40.220, Speaker D: You want to query a particular property of English, like color.
00:37:40.800 - 00:38:21.634, Speaker B: Yes, yes, yes. But maybe I'm going to skip that slide actually because someone had a question about these shapes or. Sorry, about numbers. What happens if you query numbers? I guess, first of all, one nice thing here is once you have these specific things, you can now query the specific components. So, like, there's this shape head with isosceles triangles. You can ask about isosceles triangles in this, like, pyramid thing and it will, like, give kind of the outline of the pyramids. Now someone asked what happens with numbers? Well, okay, so it's a symbolic representation of the number.
00:38:21.634 - 00:39:08.956, Speaker B: Like you would expect that it just, you know, outlines that symbolic representation. What about counting? Is it going to outline the, you know, is it going to outline, like, the two objects? Turns out not. So what it actually does is it outlines the perpendicular bisector between the two objects. If there's two, if there's four, it gives like, you know, it like splits it into like, you know, like Voronoi regions with three. It, like, depends a bit on their, like, relative geometry. So it seems like the counting operation that Clip has is like doing something pretty weird. Like it's not like doing some abstract count.
00:39:08.956 - 00:39:55.936, Speaker B: Like it actually has some, like, I think some sort of special purpose thing that is exploiting, like the combinatorial geometry of small numbers of shapes. There actually are papers saying that Clip can't really count above five or six very effectively. So this could be why Yossi spent a long time trying to figure out what the heck it was actually doing. Can we understand how it's actually. What is it actually storing? Why the perpendicular bisector? What sort of algorithm could possibly be behind this? And we have no idea. So if any of you are interested in fun scientific challenge, it would be figure out how Clip can count to three. Even three would be.
00:39:55.936 - 00:40:13.140, Speaker B: Would be good. Four would be amazing. And I'm not convinced it knows how to count to five, so that's maybe the furthest I would go. I think there's another. The other deferred question was superposition in neurons, right? Yes.
00:40:23.020 - 00:40:25.556, Speaker A: So here's this object you created, right?
00:40:25.628 - 00:40:27.920, Speaker B: Yes. You don't know if it can count.
00:40:28.380 - 00:40:32.596, Speaker A: So when you say like a science experiment.
00:40:32.788 - 00:41:27.584, Speaker B: Well, so I do believe that it can count to three in that if I give it images of like two versus three objects and I ask whether it has higher cosine similarity with 2 or 3, it like pretty consistently gets 2 versus 3. Right. But then the question is, like, what algorithm is it using? You would think it's using like the obvious algorithm of just like, identify the objects and then like have an accumulator somewhere. But I don't think that's what it's doing. Partly because it doesn't like, generalize up to higher numbers, but also because of this, like, attribution pattern where, like, if you ask it why are there two objects, it points to the perpendicular bisector between the objects, like there. I want to know why, like how, like, what's the algorithm. Like, what algorithm.
00:41:27.584 - 00:41:36.980, Speaker B: Would, like what algorithm would want to compute a perpendicular bisector to decide that there's like two of something versus three of something? Yeah, that's kind of the mystery.
00:41:38.840 - 00:41:42.304, Speaker D: That bisector should depend on the image itself.
00:41:42.352 - 00:41:42.560, Speaker B: Right.
00:41:42.600 - 00:41:54.050, Speaker D: Because the 3F on the left is on the right total difference between the location separately. So I wonder why we're consistent.
00:41:54.550 - 00:42:09.730, Speaker B: Yeah. So it's a bit weird. Like, if you make the. So if you have them in a row, it has like. If you have three apples in a row, it has like the two bisectors. If I think if you have a two by two grid, it like does a cross. And if it's like four, it has like the three bisectors.
00:42:09.730 - 00:42:25.790, Speaker B: So it's kind of like Voronoi. Although. Yeah, it does. Like, it's weirder if you just start putting them in arbitrary locations. So Also, I don't know if this like perpendicular bisector Voronoi thing is actually the like generalized algorithm it's using, but it's like a thing that shows up in special cases.
00:42:25.910 - 00:42:35.640, Speaker D: My hypothesis, because the clip has seen lots of image on the website, and on the website it had lots of like symmetric image one by one.
00:42:35.790 - 00:42:43.764, Speaker B: I see. So maybe it's just like memorize that this particular pattern 4 image means the fixed pattern.
00:42:43.812 - 00:42:48.868, Speaker D: So it will. Because that comma appears a lot. So that will remember that.
00:42:48.924 - 00:43:11.978, Speaker B: Yeah, that's interesting. So maybe just like if there's three of something, it always appears in a small number of configurations and it's just memorized all of those configurations. I think that's actually plausible. I hadn't thought of that, but I think it's a good hypothesis. Okay, any other questions before we go on to neurons and second order effects? Yes.
00:43:12.154 - 00:43:17.230, Speaker D: Actually have the same similar functionality. Can you distillate the model?
00:43:18.210 - 00:43:46.820, Speaker B: Depends on the. It depends on the type of functionality. Probably ranges from like 1 to 5. I would guess like, like concepts that are either like very complex or very common are probably more like 3 to 5. And concepts that are more specialized might be just like one or two. Okay, so let's talk about. Oh, yes, sorry.
00:43:46.900 - 00:43:48.920, Speaker A: Can you fix the number of hits?
00:43:49.820 - 00:43:51.040, Speaker B: You mean like.
00:43:54.540 - 00:44:01.250, Speaker A: You allow the data to tell you them? Okay, this is what happens right now, but could you fix it ahead of time?
00:44:01.630 - 00:44:27.330, Speaker B: So you're saying like, like I only want there to be two heads that have to do with shape. I'm not sure it would be a bit tricky because like this structure is not something that I had any control over. This just like came from the learning process. So it's kind of like we're kind of just looking at what's there. I think it's an interesting question, but I don't, I don't really have any good answers for you.
00:44:28.970 - 00:44:36.670, Speaker D: So in general, how do you train a model that recognize other than how many objects are there in the future?
00:44:37.850 - 00:45:15.148, Speaker B: Well, in this case there's just a lot of captions with counting and that's probably where it comes from. Okay, I want to move on to this part of the talk, so maybe I'll move forward. One thing you might want to do is I guess this is kind of giving some very coarse grained stuff. Like these heads do some specific thing like geographic or counting our objects. We might want something much more fine grained. And so if we want to be fine grained, we need to go to smaller components. In this case, neurons as opposed to heads.
00:45:15.148 - 00:46:16.210, Speaker B: So there's around like 16 attention heads, but, but several thousand neurons in these models per layer. So then multiply by the number of layers. But the point is there's many times as many neurons as attention heads and they capture much more fine grained concepts. And so we might want to be able to take a neuron and say, what is this neuron doing in the model? What concepts does it care about? This is trickier for a couple of reasons. So the main reason is because of Nati's objection, right? So actually none of the, we know that none of the MLP layers have a direct effect on the output. So if we just did the same thing we did last time and looked at these like direct effects on the output, we would just see that every neuron doesn't do anything. The only way that neurons do anything is as mediated by an attention head that's like grabbing its information and pulling it to the output.
00:46:16.210 - 00:47:16.594, Speaker B: So this actually turns out to be fine. You can just like recurse this algebra 1 level back and say the second order effect of this neuron is equal to the thing that this neuron writes out times the sum over all attention heads of the amount that that attention head attends to this. Sorry, the sum overall token. Okay, let's try again. Let's talk about a neuron at a given image patch. So what this neuron at this image patch is doing, right, is it's like writing to the like residual stream of that image patch. So then I can say what's the effect of that on the output? I take the thing that it wrote at that patch, I sum over all attention heads of the amount that that attention head is attending to this patch times the low rank transformation that it's applying times the thing that this neuron is writing out.
00:47:16.594 - 00:47:57.560, Speaker B: And so that's what I'm kind of drawing here, this kind of diagram. And then I multiply by the final linear map P. And if I want to know overall what is this neuron doing across all of the patches, I could just sum over patches. So the point is, this gives us a way of knowing what a neuron does. But it is like this second order effect. And also it's a bit weird because this effect will like ideally we would want to say like this neuron cares about like dogs or something. But the thing is, this effect actually is different on every single input, right? Because it depends on the attention pattern of all the attention heads.
00:47:57.560 - 00:48:33.820, Speaker B: So what we have here is like for each neuron we have like a function. This Function outputs a vector on every single input image. And we want to somehow understand this function. So this is where there's a nice empirical claim. The empirical claim is that this function is approximately rank one. So if we just find the single direction, that explains as much variance as possible in this direction, it's about 50% of the total variance. And so you can just approximate it as a kind of single direction.
00:48:33.820 - 00:49:13.930, Speaker B: Once you have that direction now you can say, okay, this neuron is like mainly caring about a direction. What do I do with this? This is now where we get to superposition. Also, I'm kind of like glossing over a bunch of math, so feel free to stop me if you want me to write these things out. The point though is that each of these neurons is associated with the vector. Now I want to understand this vector. What am I going to do? I'm going to do something kind of like pca, like we did with the attention heads. This time I'm actually going to do a different algorithm, which is sparse coding.
00:49:13.930 - 00:49:39.394, Speaker B: The reason I'm doing this different algorithm is because I've already reduced this to rank one. So I've already done the pca. Now I want to understand this vector. I'm going to write it as a sparse linear combination of a bunch of atomic concepts. So I'm going to do the same thing where I embed a bunch of different concepts. I write the vector that this neuron corresponds to as some sparse sum of these concepts. So this is like an actual example.
00:49:39.394 - 00:50:37.930, Speaker B: It turns out there's a neuron that the thing that it most is writing to is dogs, but it also writes to elephants and to the abstract concept of value and to cabbages and to the sun and a bunch of other things. So this is the superposition you were asking about. Why is this helpful? Well, this actually gives us a way to automatically. It gives us basically an auto tester for neural networks where instead of having to either come up with by hand hard inputs or try to generate them randomly, we can actually say, hey, I want to take a cat and make Clip think it's a dog. How do I do that? Well, I have a picture of a cat that's lounging in the sun with a group of elephants and a value sign in the foreground. You can then draw this image. You ask an AI to draw the image, because I'm not a good enough artist to do this.
00:50:37.930 - 00:51:11.620, Speaker B: It draws this image of a cat with an elephant in the value sign on a sunny day, and Clip thinks it's a dog. Um, and this comes just from finding these, like, decompositions where the model will get confused. And you can actually do this very repeatably. So these are just like, five random pairs of classes that we tried. Um, my favorite one is this one. So this is a frog that becomes a bird. How do you turn a frog into a bird? Well, you put a Happy Thanksgiving sign above the frog.
00:51:11.620 - 00:52:05.420, Speaker B: And then it thinks that, well, this frog is underneath the Happy Thanksgiving sign, so it must be a turkey, which is a bird. The other ones are maybe more expected. Like, if you put a horse on a highway, it might think the horse is an automobile. So what's interesting here is that this, like, to some extent, you know, like, there's a sort of polysementicity where there's, like, you know, like, totally unrelated concepts that appear next to each other. But also this just reveals spurious cues where there's, like, you know, things that kind of are related to each other and the model confuses them. I think my two favorite ones are if you put. If you take a piano and you put it in a bed of tulips, it thinks it's a violin instead of a piano.
00:52:05.420 - 00:52:47.274, Speaker B: Why is this? Well, tulips and violets are very similar. And violet and violin under, like, the tokenization of clip have, like, a shared token of, like, V, I, O, L. So they get confused. Another fun one is if you have a stop sign and you put banking services in the background, it thinks it's a yield sign because banks give you yields. So you get a lot of really fun stuff here. This is actually one of my favorite things that we've done with trying to interpret the latent structure. You can think of it as reading the source code of a neural network.
00:52:47.274 - 00:53:19.280, Speaker B: If you're trying to debug a program, you would want to not just look at the inputs and outputs, but read the code. Even if you didn't understand the code in detail, it would still be useful because you could do things like coverage analysis and find places where there's weird collisions. And this is kind of making the same point. I definitely don't understand everything about clip. We would also care about the third and fourth order effects. And I could tell you about some really weird things we found that definitely aren't explained by this. But even getting this partial understanding can get you pretty far.
00:53:20.470 - 00:53:27.650, Speaker C: So can you use it to improve training? I mean, basically you're getting your. If you know, just training examples.
00:53:28.550 - 00:53:58.900, Speaker B: Yeah. So I think this is a good idea. We haven't tried it yet. There's like, I Guess I guess it would probably work, although it's a bit finicky. So we're like trying to do this in a kind of similar setting. One thing you have to worry about is like the training set of clip is like much, much bigger than anything we're going to generate here. And also there's like some noise in this pipeline so you have to worry.
00:53:58.900 - 00:54:26.220, Speaker B: Right. So points on the margin are very valuable. The problem is like sometimes this gives you things that like it says are wrong but are like actually right. And so like the label noise you get from this might outweigh the margin benefit. So I think you can get it. It's an ongoing type of thing we're trying to do, but it's not. So it's not trivial.
00:54:26.220 - 00:54:30.892, Speaker B: You actually have to be careful. Ah, yes.
00:54:30.956 - 00:54:44.850, Speaker A: So how does that compare to these adversarial samples that people came up with other types of models early on? Right. Like changing a few pixels to make stop sign into E3 or.
00:54:45.310 - 00:55:22.556, Speaker B: Yeah. So I think the main thing is that like these are more semantically meaningful and they reveal actual kind of systematic problems in the model. So you can see this through these attribution maps. These are saying what caused it to think that it's a frog versus a bird. And you can see that it really is like the Thanksgiving sign that makes it think it's a bird and actually the frog it still thinks makes it a frog. Whereas if you were to do the same thing with adversarial examples, it would just be kind of random dust specs. The other thing is I didn't have very fine grained control over this.
00:55:22.556 - 00:55:29.580, Speaker B: I typed in this text, I had an AI system generate the image. So it's not like I was optimizing this image to be bad. Yes.
00:55:29.740 - 00:55:35.856, Speaker A: How are the concepts of mind when you try to explain your own in the first place? Yes.
00:55:36.028 - 00:56:19.170, Speaker B: Oh, like where did I, how did I come up with these? I think this was like something like the 10,000 most common English words. There's like a nice like data set that's kind of used for NLP models. That's like the most useful single words that you would want to pass into something. And we kind of just like downloaded that and use it. I guess the last thing I'll say is you can actually also use this for segmentation by doing similar spatial attribution stuff. And this does actually even better than using the attention head. So you can push these up like a little bit higher.
00:56:19.170 - 00:57:15.026, Speaker B: I don't think there's anything else I want to say Because I only have two minutes, I guess I can say, ah, yes, the really crazy thing that is definitely not captured here. So we've more recently been trying to extend this to large language models like llama. So we're looking at llama 8 billion. There's a neuron we found which seems to be a kind of just like abstract aggregator. We can't really tell exactly what it's doing, but if you look at its activity, when you have sequences of kind of like somewhat related concepts that have some pattern, it either like gets increasingly positive or increasingly negative. But like when it becomes positive or negative, it has like no real discernible pattern. Like it definitely like cares about things that have to do with sort of like repeated concepts.
00:57:15.026 - 00:58:27.210, Speaker B: And so our theory is there's this kind of general, this other line of work on function vectors or task vectors which hypothesize that the way in context learning is happening is via some internal aggregator and some vector space that's keeping track of different tasks. So we think this neuron might just be one of the dimensions in this abstract space. If that's true, the only way you could possibly understand is you could never understand it by doing this sort of thing. You would have to actually be able to say there's this cluster of 20 neurons that are working together to be aggregators in some semantic space. And this 20 dimensional vector space is used later by something to actually implement the thing that the aggregator learned. Basically, I guess it kind of reminds me for the theorists in the room of the K junta problem, right? It's like, can you understand things in isolation or is the only way to understand it to look at some kind of clique of components that are working together? And I think there are probably things in this latter case, and this is like to me probably one of the biggest challenges right now. Okay, I'll stop there and take questions.
00:58:30.510 - 00:58:32.070, Speaker A: Can we go back to the next slide?
00:58:32.150 - 00:58:37.846, Speaker B: Yes, this one, when we decide is it.
00:58:37.918 - 00:58:45.120, Speaker A: Well, I want to put this in context. This seems like, is it failure or is it creativity?
00:58:48.260 - 00:59:05.840, Speaker B: It's a good question. I'd probably say a little bit of both. Or like you could use this for creativity, but it's probably not the behavior that you were hoping for. Yes.
00:59:07.700 - 00:59:14.470, Speaker A: If you think of that piano tulip example. Now I want to have a painting that shows me that.
00:59:15.650 - 00:59:17.670, Speaker B: I'll try to send it out to the list.
00:59:19.090 - 00:59:30.190, Speaker A: Is that something that is stable and happening all the time? This kind of misinterpretation, or any of the ones that you've seen is That a reliable.
00:59:32.850 - 01:00:09.934, Speaker B: So I'd say it's not reliable in the sense that like you always get misclassifications. I mean CLEV's pretty robust. So the actual misclass, like you have to try several times to get an actual failure. But I think there is a thing where like it's like correlated with failures. I think like relatively robustly. Like if you put this, I guess it's hard to say what it means for something to be robust if it's a stochastic phenomenon. But like I would think that generally putting violets in an image would push towards the violin.
01:00:09.934 - 01:00:17.370, Speaker B: I guess you could try to do a systematic experiment there. Like I don't have that. That's just my kind of like anecdotal experience with the data.
01:00:18.710 - 01:00:27.358, Speaker D: Do they actually transfer from one model to another? And they said it is a bird.
01:00:27.534 - 01:01:15.640, Speaker B: So in general failures tend to transfer across models. I don't have that for this particular pipeline. There's another somewhat related pipeline we tried where it's both the case that you get a reasonably high transfer rate of less than 50% but maybe 30ish percent across models. I don't remember the exact number, but that's I think what you should be thinking. But also if you were to run this pipeline anew on a new model, you would actually get different failures that are specialized to that model. So I would say it's both the case that there is meaningful transfer, but also that there is a lot of stuff that is model specific. And it also depends a bit on the models GPT 2 and 3.
01:01:15.640 - 01:01:21.512, Speaker B: You have a lot of transfer between them. Probably like GPT2 and Claude, you have somewhat less transfer.
01:01:21.576 - 01:01:34.222, Speaker D: The reason I ask is that for some you don't have access to internal working, so you cannot actually do it. You cannot see how the attention had. So you can actually come up with such an example.
01:01:34.366 - 01:01:58.250, Speaker B: That's right. So you do. Yeah, that's right. So I think you can get transfer from open source to closed source models. Although if you really wanted to test a closed source model, I think you would want to find some way to get more white box access to it. Potentially the log probes can be enough in some cases, but I think this is kind of like an ongoing question.
01:02:01.450 - 01:02:02.002, Speaker C: Okay.
01:02:02.066 - 01:02:03.830, Speaker B: Oh, sorry, last question. Yes.
01:02:04.250 - 01:02:26.400, Speaker A: You were mentioning this like weird token in 3 where you said that you're trying to figure out like what it's doing and I was wondering. It sounds like we are basically trying to see something in those things and I was wondering how we can make sure that we're like very rigorous and ascribing certain things to these models.
01:02:26.740 - 01:03:17.270, Speaker B: Yeah, so it's a good question. I guess what we've been doing is you know you can take some hypothesized description of how the neuron behaves and then on held out examples you can try to use this description to predict its activation values. In fact you could ask another large language model to try to predict the activation values given the description. And so then the correlation of that prediction with the truth gives you a score of how good your description is. Now it could be good maybe because it's correlated instead of the true thing. So I think there are problems with this but actually this was how we found this particular case. We looked for the 10 things that had the lowest correlation scores and this was one of them.
01:03:17.270 - 01:03:25.110, Speaker B: So it kind of gives you a nice way of seeing what you haven't explained yet. All right, well thank you very much everyone.
