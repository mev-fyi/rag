00:00:00.840 - 00:00:01.712, Speaker A: I make it short.
00:00:01.862 - 00:00:02.758, Speaker B: Daniel is awesome.
00:00:02.806 - 00:00:04.954, Speaker A: And brace yourself for more awesomeness.
00:00:08.894 - 00:00:59.922, Speaker B: I'm the same Daniel as yesterday. Okay, so yesterday, this brief introduction to the basics mathematics of lattices. But we didn't really do anything that interesting or surprising. I mentioned some of the problems I will talk about today, and that's what we'll do now. Today, the whole day will be pretty much on average, case complexity of lattice problems and applications to cryptography there are two problems that are used in cryptography that are related to lattices. One is the SiS short integer solution problem. That's what I'll be talking about.
00:00:59.922 - 00:01:39.832, Speaker B: And then later in the afternoon, Chris will talk about LWe, the learning with error problem. So what I want to cover today is a number of topics, which. So I will start with the definition of the problem. I already sketched it yesterday, but we'll go over the definition again connected to lattices, and then show that the problem is hard on the average, which is the harness which is needed to build the cryptographic applications. I will not give a full proof. I condensed the proof on a single slide by something that is pretty clear. It has all the ideas of the proof.
00:01:39.832 - 00:02:32.292, Speaker B: It has some technical inaccuracies, but other than that, I think it gives a very precise idea of why the problem is hard. And after that, we'll move into efficiency issues. What is required for lattice cryptography to be efficiently implemented, and the approach to efficient lattice cryptography is based on the use of a special class of lattices. So you will hear more about it later. Today, there will be two talks. One on the standard standard LWE problem, one on the efficient algebraic LWE problem. In the SIS setting, the algebraic problem has a simpler treatment.
00:02:32.292 - 00:03:25.602, Speaker B: So I will not talk a lot about it, but I will start introducing the ideas behind these efficient lattices in the SIS setting, which is much simpler. And then I will illustrate how this simple lattice problem can be used to build a number of representative cryptographic applications. And again, LWE has a much broader range of applications. It is an easier problem, so it is a stronger. Assuming that LWE is hard is a stronger hardness assumption. There are classical reductions from LWE to sis, but the only way that we know how to reduce sis to LWE is using quantum computers. So in some sense, there seems to be a gap between the two assumptions based on the current knowledge.
00:03:25.602 - 00:04:18.140, Speaker B: Of course, showing that the two problems are perfectly equivalent, even in the classical setting, is a very interesting open problem. It might well be the case, but we don't know how to do it. Okay, so let's start with this sis, or short integer solution problem. So let me first, before we get into the cryptographic definition, recall the geometry behind this problem. And this applies to LWE as well. So these are problems that are associated to CVP, the closest vector problem where you are given a lattice represented by a basis and a target point t, and you want to find a lattice vector which is close to the target. So if you put a bol around the target, you can grow this ball until it includes a lattice point, and that's the closest one.
00:04:18.140 - 00:05:26.088, Speaker B: Or equivalently you can think of this target t as being the result of starting from a lattice point and then perturbing this lattice point with some error e, which is exactly the distance between t and the lattice. And so yesterday you learned everything about a dual lattice. So let's use it. So one way to formulate this problem, which is also quite close to the way the Sis problem is defined, is using the dual lattice. So this is the set of, if you remember, is the set of linear functions from the lattice to the integers. So, dual vectors or the dual basis, a matrix that generates the dual lattice is a linear function from rn to rn, which when you apply it on a lattice point, it produces as a result an integer. But if you apply this function to an arbitrary target, then the result will not be an integer.
00:05:26.088 - 00:06:45.924, Speaker B: And you can compute something that uniquely characterizes the position of the target t modulo the lattice by taking this linear product and reducing the result to modulo one. And so since the target is the sum of v with an error e. So this scalar product is linear. So you get d times v, which is an integer, and this appears modulo one plus the product d times the error, which, using class standard coding terminology, is called the syndrome of the error, because it is something that can be computed from the received word t. But it only depends on the error, it doesn't depend on the message on the lattice point that was perturbed. Okay, so one way to think of this problem is, okay, you are given a shifted copy of the lattice, that is all the lattice points that have all the points in space that have a certain syndrome. So I give you the syndrome and I take all the possible points that map to it, and you want to find the point in this shifted copy of the lattice, which is as short as possible.
00:06:45.924 - 00:07:48.180, Speaker B: And that's perfectly equivalent to finding the lattice point which is closest to the target. So you can shift. So instead of considering lattice centered as zero and a target in an arbitrary position, you can always shift the lattice by the target position and then try to find a point in this shifted lattice which is as close as possible to the origin to zero. So the shortest point in the cosette. So this called the syndrome, the coding formulation of the problem. So, okay, so in this geometric setting, you can consider the following general function where. So it is a function that is keyed by a lattice point by a lattice, the green points in the picture and the input is a vector x, which is short, is a bounded norm.
00:07:48.180 - 00:08:39.679, Speaker B: So this is very pale. So I'm not sure if you can see it, but there is a shaded circle here around the origin. And what this function does is to take this short input vector and reduce it modulo the lattice. This can be done either by multiplication by the dual matrix, or it can be done by picking a standard representative in a fundamental region of the lattice. And notice, if you increase the size of the error, these regions will start overlapping. So if the error is bounded by lambda one over two, that's the unique decoding radius of the lattice. Then you get an injective function, and that's the one that will correspond this afternoon to the Lwe problem.
00:08:39.679 - 00:09:43.034, Speaker B: But you can consider larger regions where after reduction modulo delattis, this sphere will have overlaps. So you see there are some darker regions here in the overlap, and at some point it will cover the entire space, but not in a uniform manner. But at this point, once the radius equals the covalent radius of the lattice, you have a surjective function from the ball offshore vectors to the fundamental parallel pipeline. And most typically in cryptography, you increase this radius even more beyond that, so that the resulting function is not only subjective, but it is almost regular. It produces an almost uniform distribution after you take a short vector and you reduce it modulo delaptice. So sis corresponds basically to this last setting. Why lwe is dysfunction when the error is small enough to make the function injective.
00:09:43.034 - 00:09:51.514, Speaker B: So the question is, are these functions hard to invert? Are they cryptographically strong? One way functions.
00:09:55.534 - 00:10:00.992, Speaker A: Fixed. And the lattice is either a dense lattice or a sparse lattice for sis versus lwe, right?
00:10:01.008 - 00:10:13.484, Speaker B: But no, the lattice is the key to the function. The lattice is fixed, so it is a family of function. It's not a fixed function, but it's a function family. And by picking the lattice, you are choosing a key.
00:10:15.504 - 00:10:21.524, Speaker A: Do you think about sis? Lattice is a very dense lattice. Quite typically it's a kernel of.
00:10:23.344 - 00:10:40.894, Speaker B: Okay, yeah, yeah. In terms of density, yes, yeah, yeah. So if you normalize, you can instead of growing the size of the input, you can also normalize, you said, oh, the input s length, square root of n as a binary vector. And then you can take a lattice that has a larger or smaller determinant.
00:10:41.014 - 00:10:45.754, Speaker A: These are not just morally equivalent, they're actually equivalent. These two views are actually equivalent.
00:10:46.174 - 00:11:12.020, Speaker B: If you restricted to integer vectors, of course, you cannot scale by arbitrary factors. But other than that, yes. So in this setting, which is geometric, yeah, there is no. So here I'm basically illustrating the problem as an instance of the boundary distance decoding problem, or the absolute distance decoding problem that we defined yesterday. And in the real setting, yeah, you can just scale it. You can always normalize the determinant of the lattice to one. That's easy.
00:11:12.020 - 00:11:47.654, Speaker B: In the cryptographic setting, you typically, in applications, you want to work with integer vectors, but not because of the harness, just because they are easier to implement for efficiency, practical efficiency reasons. And there are a few cases where integers come up in a more substantial way. But for the standard sis and LwE problems, yes, morally, yeah. You can think of these two things equivalent as using a larger input or a larger, smaller, denser lattice.
00:11:48.454 - 00:11:53.822, Speaker A: The first version is defined with real errors, not integer ones.
00:11:53.958 - 00:12:26.336, Speaker B: Yeah, but the lattice is still an integer lattice. So, yeah, you still cannot scale it arbitrarily. And it is perhaps a little bit counterintuitive that sis, which I have here, is usually defined with binary errors, while lwe is defined with errors. It doesn't have to be. But the most common instantiation in, you said that. Oh, and the error is a vector with binary components. In lwe, you want a vector where components are at least a square root of n.
00:12:26.336 - 00:13:00.794, Speaker B: So at first, it may seem that lwe is using longer error vectors than sis, but that's just if you compare it to the determinant of the lattice, it's exactly the opposite the situation. And in lwe, you need the errors to be at least square root of n size for some other technical reasons. But then you can compensate for it by setting the dimension and the determinant of the lattice appropriately. Okay. Yes.
00:13:01.174 - 00:13:03.314, Speaker A: So the hard problem here is to.
00:13:03.694 - 00:13:08.924, Speaker B: So once you shift or reduce it with the lattice, basically, you know, like.
00:13:08.964 - 00:13:11.244, Speaker A: There is one corner of this hypercube.
00:13:11.284 - 00:13:13.620, Speaker B: Like structure, this parallel bit.
00:13:13.772 - 00:13:16.944, Speaker A: And the hard thing is to figure out which is the closest corner.
00:13:17.924 - 00:14:09.766, Speaker B: It's not necessarily corner. If it were a corner, then it would be a much easier problem. So the lattice point closest to a target inside the parallel pipette may be a lattice point that is in somewhere else in this case it is not the case. So this is a fairly nice basis for the lattice, which is almost orthogonal. So yes, most likely it is always a corner of the parallel pipette. But if I give a very skewed basis for the lattice, and then I give a point in the middle of this skewed parallel pipe, then the closest lattice point is likely to belong to some, some other puppet that doesn't even touch the one that you are using as a fundamental region. Yeah, what do you mean by inverting?
00:14:09.790 - 00:14:11.294, Speaker A: It is like finding a pre image.
00:14:11.334 - 00:15:14.688, Speaker B: Or specific finding a short pre image, finding a point that is congruent to the target by this short. Now yeah, the picture is a bit misleading in the sense that the vectors here, they look shorter as those there. But that's just because in two dimensions, and if you want the picture to be readable, there is not much you can do. But yes it is considering all the shifted copy of the lattice centered around a target in here. And of all these points, find the closest to the origin, which typically is not the one inside the parallel pipe, but it is something else. So put this in the sort of geometric context of general lattices that I described yesterday. So in cryptography the problem is usually formulated in a much simpler elementary, where it involves only integers and the itai Sis function.
00:15:14.688 - 00:16:03.094, Speaker B: Sis is the short integer solution problem and the name comes from. This formulation is defined as follows. The problem is parameterized by some integers, two dimensions n and m, and a modulus q. And this is a function family that is keyed by a matrix a which is an n by m matrix with entries which are integers modulo q. And this matrix is chosen uniformly at random, and you can think of it as defining a random q ARR lattice a random lattice which is periodic modulo q. Yesterday we saw two different ways to the final lattice. From a, you can either take the image of the rows or the kernel, and we'll see that the kernel is the one that is most relevant in this setting.
00:16:03.094 - 00:16:38.686, Speaker B: And the function is very, very simple. In fact, it is just a linear function that takes an input vector x and outputs a times x modulo q. Now this function for arbitrary inputs is very easy to invert. You can find a pre image just solving a system of linear equations. Solution is not unique, you will find some solution. And what makes the problem hard is that the function is defined as having as a domain a set of small vectors. So for concreteness here, I'm using zero one to the m.
00:16:38.686 - 00:17:09.814, Speaker B: So vectors with binary entries. But what matters is just that these vectors are short. You can generalize it to other sets or vectors that have a bounded euclidean length. And once you do that, using linear algebra, you will be able to find a preimage of the target, but it will not be a short one. In general, you will typically find a preimage which is not short at all. You will find a long one, so it is not in the domain. So that's not solving the inversion problem.
00:17:09.814 - 00:17:51.366, Speaker B: And the function is computed modulo q. A is a matrix modulo q. So I've written is reduced modulo q. Now, in the discussion of dual lattices yesterday, you may remember that the kernel and the image of a, they are related by duality up to a scaling factor q. So reduction modulo q here corresponds to reduction modulo one in my previous geometric description of this function. So, and here everything is done using integers which allow for easier implementation. So, okay, so what do we know about this function? So we know that the function is hard.
00:17:51.366 - 00:18:55.344, Speaker B: In a very interesting sense, this was the result that somehow marks the birth of lattice based cryptography, the use of lattices for the design of secure constructions. So iti proved that if the dimension m, n and q satisfy a certain relation, in particular, if m is big enough, then this function is one way it is hard to invert. Assuming that lattice problems, in particular the SIVP problem, the short integer solution, the shortest independent vector problem, is hard at all in the worst case, to approximate within some factor. So we'll go back to it in a bit, but for now, just to get a sense of this problem. So here is a specific instance. It's a small one. It's something that with a computer you could solve right away and just pretend I didn't give out the solution here to start.
00:18:55.344 - 00:19:17.988, Speaker B: Okay, no, this one is not a solution. So this one is just an input. So the matrix is a four by six by seven matrix. Here we are performing the arithmetic modulo q. So the numbers are single digits. You can multiply a by this vector x, which is a binary vector. You get this as an output, assuming I didn't do any errors.
00:19:17.988 - 00:20:10.496, Speaker B: Now if I were to hide this, and I ask you, oh, can you find a zero one solution to this problem? Could you do it right away? Doesn't look easy. And in fact it's something that may is reminiscent of the it's a subset sum problem, so it's an autonomously np hard problem. Subsetsum usually is defined over integers. Here we are performing a subset sum over vectors. So we have a good sense that, yeah, we don't expect that finding zero one solutions to be easy, as easy as finding arbitrary integer solutions. And so this problem inverting the function is exactly the cryptanalysis problem problem that corresponds to inverting this as a one way function. So we'll see other notions of hardness for this function.
00:20:10.496 - 00:20:57.418, Speaker B: We'll see soon that the function is not only one way, not only it is hard to find a binary pre image, it is also hard to find collisions. Now, even if we are restricting the domain to a set of binary vectors. So if you remember here with a condition that m was bigger than n log q. So that means that the set of binary vectors that you can pass as input is bigger than the set of possible outputs. So you have q to the n possible outputs, because the output is a vector modulo q with n coordinates, and you have two to the m possible inputs, the set of all possible binary vectors. So this function is never injected. So it has a bigger domain than a range.
00:20:57.418 - 00:21:57.196, Speaker B: There exist collisions, there are points that map to the same output, but those points will be hard to find. So what is the connection between this description and the geometric description of the function that I gave before this point? You can probably see it already on your own, but let's review it together. So you are given this function that is not mentioning lattices explicitly. It's just a matrix vector opera multiplication restricted to binary inputs. So where do lattices come up? So it can be formulated as a lattice problem as follows. I'm illustrating the connection to lattices as a possible way to solve the problem. So how can you solve this inversion problem? You can first solve solve the easy problem of finding an arbitrary prey image at equals y, where t is not required to be short, t is not zero one.
00:21:57.196 - 00:22:44.760, Speaker B: So this one, everybody can do it. Just linear algebra and gaussian elimination, especially if q is a prime, is particularly simple over a field. And you can just use standard gaussian elimination. So, but this one doesn't solve the inversion problem. It's easy to find, but it's not a solution. But once you find it, you know just from linear algebra that all the solutions to the system of equations ax equals y mod q are the sum of this specific solution that you just found, together with the solution to the corresponding homogeneous system. And this, the homogeneous system, is exactly the kernel of the matrix.
00:22:44.760 - 00:24:30.882, Speaker B: A is the set of vectors x integer vectors such that ax equals zero modulo q. So you want to find a binary vector, or more generally a short vector in a shifted copy of this integer lattice, which is exactly the closet formulation of the shortest of the closest vector problem that we described before in the general geometric setting by drawing a piece. So there is no picture here. It just equations, matrices and vectors, which often is the most convenient way to work on these things, to design new cryptographic functions, to implement them, to produce pseudocode or code, and even to carry out proofs. But you can draw a picture behind that code, respond to these problems, which I think at first is very useful to get a good intuitive sense of what is the problem being solved, because geometry in the end plays a very important role in pretty much all the algorithms, or most algorithms that are used to solve these problems, even when restricted to binary input vectors. So, okay, so this the SIs problem, the definition. And so with this connection, you see how the SIS problem, this short integer solution problem, the problem of finding a short solution to a system of linear equations, modular q, is an average case version of the closest vector problem.
00:24:30.882 - 00:25:20.964, Speaker B: It is an instance of the closest vector problem, or the close. It's not necessarily the closest is approximate clauses or generally closed vector problem. Or more technically, it is actually this absolute distance decoding where you want to find a vector within a certain bound. The bound is the binary cube in this case, but instantiated with a specific distribution on the class of lattices. These are the lattices which are generated by picking a uniformly random matrix a. So this defines the probability distribution on these kernels, on these integer lattices, and also a distribution on how the target is chosen. It is chosen by picking a random binary input vector and then applying the function to it.
00:25:20.964 - 00:25:26.244, Speaker B: So this establishes a first connection between these two problems.
00:25:28.224 - 00:25:36.094, Speaker A: So close here doesn't mean close in any norm really, because you would get minus 10 one vectors.
00:25:36.874 - 00:25:54.922, Speaker B: Yes. Okay. So in order to think of this as close, instead of zero one, you want to take zero one minus one. Okay, which is just as fine. And that's what I have here. In fact, I'm using zero one because it makes it easier to illustrate a connection with collision resistance. But yes.
00:25:55.098 - 00:26:15.544, Speaker A: Second question is, if you change the domain from zero one to the nice, to vectors of norm, much less than square root m, square root m is the smoothing problem. Then, you know, we don't have a subjective function anymore. And do you think that this function is equivalent to Lwe?
00:26:16.044 - 00:26:17.504, Speaker B: It is, yeah.
00:26:18.324 - 00:26:19.908, Speaker A: This function is hard to invert.
00:26:19.996 - 00:26:58.370, Speaker B: You're saying inverting this function is exactly Lwe, but written differently using via duality. The function is implemented using the dual basis rather than the primal basis, but other than that. Syntactically, sis and Lwe are equivalent via duality, are equivalent formulations of the same class of problems. The real difference between sis and lwe is not that lwe is using as error or y lariat, is that the error relative to the determinant is different. But yes, if you take the density.
00:26:58.402 - 00:27:16.242, Speaker A: Of the lattice, if you write both of them as the same form, a times c equals in both cases. Right. So here the lattice is very dense. It has like tons of short vectors. The way it's written down, yes, the lattice is much sparser. You have the same error length as square root.
00:27:16.338 - 00:27:41.582, Speaker B: Yeah, yeah. So, yeah, yeah. So, yeah. If you prefer, you can scale that, but if you fix the lot, if you scale it, so that the lat is the same and what is different is the length of the error is. Yeah, so then, yeah, yes. Here, q can be arbitrary, or it doesn't have to be prime. It is pretty much arbitrary.
00:27:41.582 - 00:28:31.344, Speaker B: The factorization of q, it hardly plays any role in determining the complexity of the problem. But we will see soon that you want q to be large enough. For example, if you were to take q equal to two in the extreme case, and then say that, oh, I want vectors which are short in the hamming metric, of course, zero one vectors is the entire space modulo two. So this learning parity with noise problem. So it's a classic problem in the theory of linear codes and machine learning. And we do not know how to prove harness the type of harness that is proved for lattices. Proving the type of harness for q equal to is challenging.
00:28:31.344 - 00:29:11.312, Speaker B: And there is some kind of harness that can be proved. But yeah, typically you want q to be at least n. I'll talk later about how small can you reduce q and still have the type of harness that I will show. Other than that q is an arbitrary integer and it is not a large integer. So it is an integer with only logarithmically many bits. Typical values of q that are used, that can be used in practice, can be as small as like 250 if you want to prime. There are some cryptographic functions using q equal to 257, which is a prime.
00:29:11.312 - 00:29:48.458, Speaker B: It has also some other nice properties. And numbers are essentially a single byte. So it's a small int, it's a small arbitrary integers, primarily of queues. And time is convenient to make certain operations faster to implement in practice, because you can use. Yeah, it's nicer to work in finite fields. Okay, so collision resistance. So this function not only is one way, but it is also collision resistant and in fact, it is possible to show that the collision resistance follows for free.
00:29:48.458 - 00:30:30.794, Speaker B: It follows from the fact that it is one way, provided that you slightly change the domain of the function. Consider the following problems. So we know that this is a lattice, this kernel, and a collision to the function corresponding to a is a pair of distinct vectors x and y, such that ax equals ay modulo q. Now, if you take the difference between these two vectors a, x and y are two distinct binary vectors. So their difference will be a non zero vector with coordinates which are zero, one or minus one. So it is still a vector with coordinates which are bounded by one in absolute value. So it is a short vector.
00:30:30.794 - 00:31:02.322, Speaker B: And just by linearity, if you apply the function a to z to the difference, you get exactly ax minus ay, which is zero. Because that's a collision. Those two values are the same. So finding a short vector in this kernel corresponds to finding a collision to that function. In this case we have. Yeah, so you can just talk about norm. Yeah, you care about the length of the vector.
00:31:02.322 - 00:32:11.814, Speaker B: So if here, instead of taking zero one vectors, you take vectors in a certain ball, collisions will give you vectors in a ball which is slightly larger, because when you take the difference, it gets a little bit, a little bit longer and okay, so I'm using zero one again, because in practice those are the easier to implement. You want to pick an input to the function at random. In practice, it is much easier to pick a random binary vector than picking a vector in a ball, which requires some, I mean, still can be done, but certainly is not as straightforward as picking a sequence, a sequence of bits. But this, just for convenience of implementation, has no match to do, if anything at all, with the security and harness of the function. And for binary vectors, the problem is that of finding an input that has infinity norm equal to one, so it should be non zero. It's easy to find that zero is always mapped to zero by the function. You want to find something that is nonzero.
00:32:11.814 - 00:33:08.722, Speaker B: Now this shows how this SIs inversion or collision finding problem can be interpreted as an average case lattice problem, as an instance of a special case of the close vector problem on a certain random class of lattices. But there is a much deeper connection than that. So this is the intuitive connection showing that, oh, if you can solve the lattice problem, then you can break the function. But there is something much deeper that can be shown. And this is the second topic of today's talk, which is average case hardness. So in cryptography, you want functions to be hard to solve on the average, so that when you pick your key at random, you know that your key is hard to break. You don't care that, oh, there exists a key that is hard to break.
00:33:08.722 - 00:33:33.784, Speaker B: No adversary can break every single key. So that's something that you want when working on algorithms. You want your algorithm to work for any instance of your problem. You give the graph, you find the shortest path. You don't want the algorithm to say, oh, you gave me a hard distance of the shortest path problem. You want to solve all of them. An algorithm that works is an algorithm that works all the time.
00:33:33.784 - 00:34:32.058, Speaker B: Cryptographic functions, which is hard, is a function that should be hard to solve, if not all the times, at least most of the times with very high probability. So, average case harness is really the essence of cryptography. And so we want to show that the previous connection between sis and lattice pretty much said that if you can solve the lattice problems, then you can solve Sis, because it is a specific average case instance of the lattice problem. So we want to do something that goes in the other direction. We want to show that the only way to solve Sis is to solve a lattice problem. So we want to give a reduction from lattice problems to Sis, which is the standard methodology in modern cryptography, to gain confidence in the harness of a function. And that's what is done typically in also number theoretic construction.
00:34:32.058 - 00:35:32.976, Speaker B: Consider, for example, Rabin's function, which is a squaring modulo a large integer, which is the product of two primes. So the product of two primes n, is the key to the function. And it is a well known result that being able to extract the square roots modulo n is equivalent to factoring n, which we believe is a hard problem. So you can say that inverting the function is at least as hard as factoring. And notice in this connection, there is a one to one correspondence between function keys, numbers to be factored, and function keys and function on the other side. So, if you assume that this problem, factoring n, is hard for most numbers n, or at least for most numbers n, chosen according to this distribution product of two large primes, then you get this function is hard to invert for most keys. So it's a one to one correspondence.
00:35:32.976 - 00:36:27.374, Speaker B: And what we showed for lattices is so far, it's something that is similar to this. So we have a class of lattices which are chosen at random, a corresponding class of functions, and each function correspond to a lattice. And it's solving the shortest vector problem in the infinity norm in this lattice is hard on average, for most lattices, then this is a one way function. It's a function which is hard to break for most keys. So, but there's not quite enough to. So how do we know that these problems are hard? How do we know that these the right class of lattices to use the right distribution? They're a good way to pick lattices at random, because after all, average case hardness depends on the distribution. Consider the factoring problem.
00:36:27.374 - 00:37:15.176, Speaker B: So I give you the factoring. You are given a number n, and you wanted to factor it into the product of two integers bigger than one a times b. So, is this problem hard on average without quantum computing? I know that there are a good number of quantum computing people in the audience. You are not allowed to use your pocket quantum computer, your Q phone. And so we. So it depends on the distribute even with classical computers, even with a pencil. If I pick a uniformly random integer, 50% of the time is even, and you can immediately factor it as two times n over two.
00:37:15.176 - 00:38:14.714, Speaker B: Okay, so the distribution. So the fact that the factoring in the classical setting is believed to be hard is something that is specific to a certain distribution. So, and this one is not a good one. Choosing n uniformly at random. So, how do we know that choosing a uniformly random a is a good way to pick a lattice at random that results in average case hardness? It could be the case that, similarly to integer, if you pick a uniform at random, then there is some trivial straightforward way to do it, so that a is an easy lattice and the corresponding function is not one way for that key. So, in order to address this issue, we need a different type of reduction, which is randomized. And given any lattice as a starting point, it can map that lattice, not just to a single function, but it can randomly map to an arbitrary function.
00:38:14.714 - 00:38:55.304, Speaker B: So, any lattice can be mapped in principle to any function, to any matrix a. And this done in a random way. So, assume we have such a reduction. So this reduction is somehow forgetting which lattice you started from. And no matter which lattice you started from, it produces a uniformly random matrix a. Now assume that, but this function is not one way, meaning that there is a small set of keys for which your attack works, which can be broken. Okay, maybe this could be still hard most of the times.
00:38:55.304 - 00:39:56.506, Speaker B: But assume that the fraction of keys, the white circle here, where your attack works, is non negligible. So if you start, if you are given an arbitrary lattice l and you apply the reduction, you will get typically a function where the attack doesn't work. But if the attack doesn't work, then you can just run the reduction again, and you will get a different matrix a, and then you will get another one. And you keep performing the attack, the reduction with different randomness, until hopefully you map l to one of the easy instances. And once you do that, you can break the cryptographic function and solve the corresponding lattice problem. Okay, so unless you can solve the lattice problem for every lattice, assume there's no technical definition for it, but let's say that l is the hardest lattice in the world. Okay, so you manage to find the hardest possible lattice for CVP.
00:39:56.506 - 00:40:59.274, Speaker B: That's the specific lattice that you wanted to solve. Yes. So if you want to know what it looks like, Leo will show you after the talk. And so you can solve CVP on that specific lattice by mapping it over and over again to a random matrix, say until you hit an easy instance of the function. So if you assume that hard lattices exist at all, there exists at least one lattice where CVP is hard, then this fraction of easy instances of Sis must be negligible, must be so small that it will take a very long time, it will take forever before you hit a point that is in there. So that's the type of reduction that we want to give. And this is what Aitai did in 96, and brought lattices under the spotlight of problems that can be used for cryptography.
00:40:59.274 - 00:42:00.404, Speaker B: And there's been a number of refinements and improvements on Itai's reduction. The reduction by Oded, Regev and myself in 2004 is closer to the pretty much the best reduction. There have been some further refinement after that. Vinod and Chris had some further refinements that using the discrete gaussian sampling, improve the modulus q that is used in the reduction. But it is the core of the result. And it says that if SiVP is hard to solve, approximately within a factor n linear in the dimension, so this is a factor, I tie it a much bigger polynomial factor. So this work is the one that managed to bring the approximation factor to n, which is still today the smallest polynomial known to give this type of hardness.
00:42:00.404 - 00:43:05.600, Speaker B: So assuming that this problem is hard in the worst case for some lattice, then the function f is one way, and it is also collision resistant. So how do we do that? How can we perform such type of reduction? So, from the picture, you see that the reduction should in some sense forget which lattice point you started from. So I give you a lattice, you give me a uniformly random a, I give another lattice, you still gave me a uniformly random a. So no matter which lattice, which lattice you start from, the reduction should somehow clear this lattice, make it disappear, shouldn't disappear completely, because otherwise the reduction would be. You cannot just pick a uniformly random a from scratch. So this matrix a should still somehow convey useful information about l, but should have a distribution which is independent of L. And this can be done using the idea of smoothing a lattice or blurring a lattice.
00:43:05.600 - 00:43:55.542, Speaker B: And that's where the smoothing parameter defined yesterday comes up. So remember, so given a lattice, you can start adding noise to your lattice. And the noise at first gives these joint spheres. As the noise gets larger, the spheres will start overlapping. At some point they will cover the entire space. And notice, the moment that these spheres cover the entire space, every point in space, space a a point in rn can be written as the sum of two vectors v and r, where v is a lattice point and r is a short vector. Now how short? Well within the covering radius of the lattice, which we know is at most square root of n lambda n.
00:43:55.542 - 00:45:06.326, Speaker B: So this gives a way to represent every point in space as the sum of a lattice point and a short vector. And of course, this decomposition depends on the lattice. But the final result is an arbitrary point in rn, and the function uses even larger noise which produces a distribution which is close to uniform in space. But still, you can represent every point in space as the sum of an error vector, together with a small perturbation. And if you take this vector a so you can go in the other direction, you can first start from a vector v, add an error r to it, and then you reduce the result modulo delapis. This should be close to the uniform distribution over the fundamental region of the lattice. So we mapped a point in space to a uniform vector in a parallel pipette in a way that depends on the lattice.
00:45:06.326 - 00:45:36.020, Speaker B: But the output distribution is uniform regardless of which lattice you study from, if you are using a point in the fundamental region. Of course, the shape of the fundamental region depends on the lattice. But think of it as just as a point in zero, one to the n. So the geometric shape of the region doesn't matter. We just look at the coordinates of the point with respect to the lattice basis. It's a torus. Yes, it is a torus.
00:45:36.020 - 00:46:17.154, Speaker B: And if you use the dual lattice formulation, it is multiplication by d. So it is a vector with zero, with coordinates between zero and one. So on a torus, typically you also consider the geometry of the torus. So here we don't care about it, we just care about the fact that it's uniform. So you can apply linear transformation and it doesn't matter. What matters is that the original point can be written as a sun in this way. So why is this good? Well, you can already see that if the idea was to forget which lattice you started from, in some sense we achieved it.
00:46:17.154 - 00:47:01.274, Speaker B: So another way to describe what's happening here is that is saying that all lattices, if you look them from far enough, they all look the same, they all look like a dense. Once you look at them from far away, the points will be very close to each other. It's like, I mean, can you see the pixels in this screen? I can from here, but perhaps from back there, you don't really see the projection here. You cannot see the fine structure of the dot matrix. It looks like a continuous, continuous space. So if I tell you that, oh, these are hexagonal pixels, you cannot tell from if you are far enough. And.
00:47:01.274 - 00:47:46.618, Speaker B: Okay, so gpv, so this is the work of v node. Chris Craig, what he did was instead of using rn, it used a discrete set lambda divided by q, which is a number of nice properties. I will not get to this level of detail, so I will use real numbers. So sis is an integer problem. It is enough to describe the idea. So real numbers are enough to describe, describe the ideas behind the reduction. For the reduction to be technically sound, you do need to somehow use a discrete subset over n.
00:47:46.618 - 00:49:02.264, Speaker B: But it is a much finer grid where you take your lattice and you divide it by a large integer. So intuitively you can think of it as a continuous space. So how is this used? So we know that these points, so you can generate these points by first picking a random lattice point, picking a random error of this length and adding them up, or if you like, first picking the random error, reducing the module of the lattice, and then computing v as the closest point to the reduced error. So you sample these values and you know that this a will be uniformly random. So it is a way to sample uniformly random values a's, but knowing that they can be written as a sum in this manner, and if we ignore the fact that these are real vectors in my sketch, think of them as integer vectors in a fine grid. You can use them as columns of a matrix a. And since each one of them is distributed uniformly at random in this cube, in the unit cube, this will be a uniformly random matrix.
00:49:02.264 - 00:49:59.714, Speaker B: And you can think of it as a key to sis. If you are working a modular q, if you are scaling the lattice modulo q, instead of being vectors in the unique cube, it will be vectors in zq to the n. And that's where the arithmetic modulo q comes up. And you can pick as many columns as you want in this matrix. So the dimension of the lattice is the number of rows of the matrix, while the number of columns is something that you can choose any way you want. So you can pick the number of columns large enough so that this function is surjective, at which point is a good Sis matrix. You can pass it to an oracle that supports, finds collisions, finds solutions to the sis problem, get a solution.
00:49:59.714 - 00:50:56.674, Speaker B: And then what does this solution tell us about the original lattice problem that we wanted to solve? So, we started from a lattice l, which is not even a QR lattice. It's an arbitrary lattice in euclidean space. We use that lattice to pick vectors modulo Q in the discrete setting with uniform distribution, but in a way that geometrically is related to the lattice that we started from. Because every column of a can be written as the sum of a lattice vector in our worst case, lattice, plus a small perturbation. Okay, now, a hypothetical attacker finds a collision to the function and says, oh, I can find a zero, one minus one vector such that this sum is equal to zero. So this is a proof for contradiction. We're assuming that an attacker can find the collision.
00:50:56.674 - 00:51:47.358, Speaker B: And now, using this collision, I want to show you that I can solve the original sivp problem in the lattice that we set up from, which has nothing to do with the kernel of a. So, the kernel of a is just the kernel of a random matrix modulo q. And there is this lattice l that we started from that got somehow hidden in it in a way that cannot even be detected by the attacker. The attacker is given a uniformly random matrix a, and he doesn't know that it was picked using this specific process. But we know that. So, after the attacker comes up with a collision, we can use this side information that we have from the way we pick the random matrix a. So, we know that each of these vectors AI is the sum of a lattice vector v, plus a small perturbation r.
00:51:47.358 - 00:52:19.474, Speaker B: Okay, so let's just rewrite this differently. Let's bring all the v's on one side and all the r's on the other side. So we know that this sum is equal to zero because this is a collision. So you get that the sum of vizi is equal to negative sum of ri zi. I'm just writing this question. This sum is equal to zero as the equality between two vectors. And we know that z is a short integer vector.
00:52:19.474 - 00:52:41.754, Speaker B: It's a collision. It's a short, nonzero integer vector. So we know that these two vectors are the same. They are the same vectors, and each one of them tell us something interesting. The equation here on the left tell us that this vector is a lattice point, because it is an integer combination of lattice vectors. Z I's are integers. V I's are in the lattice.
00:52:41.754 - 00:53:13.588, Speaker B: So this is a point in the lattice. And what else do we know? Well, we know that this vector can also be written in this way. And this is a combination of short vectors, which are not in the lattice, but they are short, and they are combined with short coefficients. Zi is a small vector, so the coordinates of z I are small. So here, we don't care. We don't know if it's in the lattice or not, but we know that it is short. But since these two quantities are the same, this vector is simultaneously short and it belongs to the lattice.
00:53:13.588 - 00:53:51.446, Speaker B: So we found a short lattice vector, so we did it. Okay, so that's the idea of the proof. So, there are various technicalities that are omitted. One is that at first, I was using real numbers, so you needed to make this more precise using a fine grid. So here there's a chance that even if z is a non zero vector, or perhaps after you add all these vectors, then the sum is zero, which is short, it belongs to the lattice. By doesn't solve SVP. But there is enough information that is hidden in this process that you can show that even all powerful adversary will not be able to do that.
00:53:51.446 - 00:54:32.272, Speaker B: And that's why you need a function which is not injective. If the function were injective, then that would be the only way to get, to get zero. As a result, to get the zero vector here, there are different vectors that map to the same point. The adversary doesn't know, which is the one that we will be able to reconstruct. So the one that we will be able to reconstruct is going to be non zero with high probability. Okay, so this concludes the proof, or proof sketch that the SIS problem is hard on average. Okay, so let's get now to efficiency and then cryptographic applications.
00:54:32.272 - 00:55:14.386, Speaker B: So, efficiency, I'll just give some elements just to tell you what are the type of things that come up when you want to use these problems. And I will first briefly tell you about the choice of the modulus, which is, I think, a technically interesting problem. Because, in principle, we would like these prongs to be hard, even when q is as small as two, but we don't know how to prove that. So this is one of the main open problems in the area. So how small can we make q for this to work? Some constructions even use very large queues. And sometimes using very large queue makes things easier by the harder assumption. So the smaller queue is, the easier is the problem.
00:55:14.386 - 00:55:59.038, Speaker B: So we would like you to be small, and small q is also useful for efficiency. It means that you can implement your functions using a small number arithmetic. You don't need the long precision integer arithmetic used in the implementation of factoring Bayes, the discrete log or other number theoretic constructions. So ItaE in the original proof was showing that if q is an arbitrary polynomial sufficiently large, then the function is one way. And in fact this queue was arbitrarily large. So depending on the probability of the adversary breaking the function a, then you needed a larger q. So this q, there was not even a constant bound on the exponent.
00:55:59.038 - 00:56:46.504, Speaker B: But then this was brought down in a sequence of paper, first I think to 16. Oh, you can set q to be just n to the power of 16. And then this was improved in a sequence of papers. The paper that I mentioned before brought the modulus q down to a small power of n n to the power 2.5. And this is what in 2008 was improved to be just linear in that dimension. And using a smaller queue is something that is not easy to show that when you formalize the proof sketch that I showed you before, and to make it into a real proof, you will see that a larger q allows you to take vectors and make them shorter. If q is not large enough, you will not be able to get these short vectors.
00:56:46.504 - 00:57:37.914, Speaker B: So one question at this point is, is the problem hard if you take q smaller than n? N, after all, is the dimension of the worst case lattice. So it could be a sort of natural bound below which maybe the function becomes computationally different. Okay, but so in 2013, Chris and myself show that, oh, you can actually make it smaller, you can bring the modulus down to as close as you want to square root of n. The function does not necessarily get more efficient, because as we do that, we need to increase the dimension of the function. But there is no conceptual reason to believe that q has to be at least n. So we don't know now how to go below square root of n, but we know it is possible. Yeah, so n is not the answer.
00:57:37.914 - 00:58:33.154, Speaker B: And let me tell you how the proof works. Is a simple combinatorial proof that works by reducing Sis to itself. So assume for simplicity that the input is binary, and the proof can be generalized to short vectors. But I'll make explain explicit use of binary vectors that make things much simpler and assume that we can solve Sis for some values of n, m and q with probability one. And that's why I call this a toy version of the reduction you want, a reduction that is robust and works even for attackers that work with small probability. Now how can you use that to solve Sis with a bigger modulus? By using also a bigger dimension, m. Notice as the modulus gets larger, the range of the function gets larger.
00:58:33.154 - 00:58:46.426, Speaker B: So for the functions to still be subjective, you also needed to increase the size of the domain. So the fact that we are increasing both m and q makes sense. But is there?
00:58:46.490 - 00:58:50.922, Speaker A: Yeah, subjectively it's sufficient to make it two m instead of m squared, but.
00:58:50.938 - 00:59:37.898, Speaker B: Q for the proof to work, yes, and that's why the final result. From a practical efficiency point of view, you might be better off using the larger queue. Okay, and so we do this, we take this larger matrix and we break it into m blocks, each with m coordinates. And each of these matrix is a matrix modulo q square. But you can take think of these numbers modulo q square squares as two digit numbers in the base queue. So each of these matrices can be written as a matrix, a one prime modulo q plus another matrix, a one double prime modulo q scaled up by a factor of q. So these are the two digits in base q of your matrix.
00:59:37.898 - 01:00:25.586, Speaker B: You split them so they become two different matrices modulo q. So now each of these matrices now is a good east answer to the original Sis problem. And the parameters are chosen in such a way are chosen in such a way that, yeah, there are collisions. So you can try to find zero one collisions to each of these matrices modulo q, a one prime, a two prime, and so on. So you find zero, one plus minus one vectors such that this particular product is equal to zero modulo q. And so this involves solving m instances of this Sis problem. But assume that we can do that with probability one.
01:00:25.586 - 01:01:17.346, Speaker B: So we solve all of them. And after you do that, we can subtract this matrix from this one, and we get q times a one double prime, which is a multiple of q, so we can divide it by q. So we can take a I prime plus qa times q. When we multiply this by z, the first part goes to zero, the second part is a multiple of q, so we can divide this by q. So let's call these values b. We have one, we are multiplying. So we are mapping each of these blocks to a single vector by multiplying it, each one of them, by a small collision.
01:01:17.346 - 01:02:00.414, Speaker B: We find a collision here and we get a vector, we get a vector which is zero modulo q, but it is not zero modulo q squared. But we get all these vectors, which are multiples of q, we divide them by q. Now we have m vectors modulo q, and we combine them into one last sis instance, and we find a collision to this instance. So we take a zero, one minus one combination of these vectors. So this is zero, one minus one combination of zero, one minus one combinations of the original vectors. And that's why I'm restricting to zero to one. So that, yeah, products, it is still a zero, one minus one combination of the original vectors.
01:02:00.414 - 01:02:21.112, Speaker B: It is non zero. And we found a solution to the original problem. So, and the solution is precisely some kind of a tensor product between the last collision that we found and the m collisions that we found in the previous step.
01:02:21.288 - 01:02:27.064, Speaker A: The reason this doesn't extend beyond two, you know, like q. Q. Yeah, yeah.
01:02:27.104 - 01:02:45.406, Speaker B: So all the details that I'm omitting here, they show. So in fact, yeah, so if you do this, you said it seems like that, oh, you can always increase the q. So if you can use this larger q, then you can do the smaller one in principle. Yeah. If you take, you can go from two to the n. No. Yeah, it doesn't work, but yeah.
01:02:45.406 - 01:03:08.594, Speaker B: So if you read the paper, it will be apparent why. Yeah, it doesn't. In fact, square root of n is where the reduction. So we cannot go below square root of n. So if you fix all the technical details in the reduction, then you need q to be at least the square root of n, the square root of n somehow natural, because the vectors are of norm, square root of them. Yes. Yeah.
01:03:08.594 - 01:03:26.938, Speaker B: It is relative to the normal vectors, but I don't have a good intuition of why. Yeah. And we cannot achieve x, we cannot achieve square root of n. We can get n to the power of one half plus epsilon. We cannot even get the square root of n. And it's a good open problem. Yeah.
01:03:26.938 - 01:04:07.650, Speaker B: So it's a kind of technical question. So it's not a question that people really tried really, really hard to address, but getting either a good theoretical explanation of why square root of n is a barrier for the modulus q would be useful. Breaking the barrier and go below square root of n would also be very useful. And before this result, n was considered the natural barrier. Okay, so it is not possible to go below n. That's why the binary case is hard. And yeah, maybe the binary case is still hard, but another barrier is square root of n.
01:04:07.650 - 01:04:39.636, Speaker B: And maybe you can get to fourth root of n or something else. So the actual proof is using discrete gaussian sampling. It is giving a reduction from discrete gaussian sampling to itself. So there are many more technicalities, but it's not a hard. Comparatively to other lattice papers is a fairly, fairly simple and self contained reduction. Because everything is average case. There is no worst case to average case connection.
01:04:39.636 - 01:04:43.584, Speaker B: Everything is sis to sis reduction that we are doing.
01:04:43.884 - 01:04:47.924, Speaker A: There is no reason they should be inspired here. Isn't it trivial?
01:04:47.964 - 01:04:50.144, Speaker B: No, I'm hiding what.
01:04:52.244 - 01:04:54.096, Speaker A: Matters, right?
01:04:54.280 - 01:05:09.624, Speaker B: Oh yeah, yeah, yeah. Here I was using m. So that once you break it into blocks. Yeah, yeah. So that all the calls to sis f dimension m. Yeah, yeah, yeah. Fix the many things that are done more generally in the paper and on the front.
01:05:09.624 - 01:05:44.434, Speaker B: Yes, also in the paper you have like, if you have a bounded l two norm, then you can do better, right? It's not just an infinity norm, but also a bounded l two norm. Don't you have a. You can go below squared n or. I'm just remembering our main goal was to reduce q and we couldn't go below square root of n. So, I'm not saying that it cannot be done, but please do it. And I in fact encourage you and everybody to do it. But yeah, if you do it, it's something that Chris and I tried to do and we couldn't find a way to.
01:05:44.434 - 01:05:51.514, Speaker B: Straightforward way to do it. Maybe there is a straightforward way to do it and I'll be happy to see.
01:05:51.674 - 01:06:00.826, Speaker A: Actually, I was saying something rather trivial. Maybe I'm mistaken. So if you can break the n two m version, you can break the n by n squared version, right?
01:06:00.850 - 01:06:05.814, Speaker B: Because intuitively, honestly, I don't even remember exactly.
01:06:08.874 - 01:06:13.406, Speaker A: You have this m squared matrix. Just drop all columns except twelve. Give it to the guy.
01:06:13.470 - 01:06:19.790, Speaker B: No, but there are no collisions. Modulo Q squared, modulo to m. All.
01:06:19.822 - 01:06:28.486, Speaker A: M needs to be is bigger than n log q squared, which is two n log. Yeah. So then there are collisions. The guy will find you and then. That's good. You're good.
01:06:28.590 - 01:06:55.826, Speaker B: Yeah, but it doesn't. I don't think it gives you what you want. But anyway, so we can. Yeah, so we can. But yeah, so there are many, many details which are hidden under the rug. And honestly, I don't remember. I don't have all those details fresh in my mind, but yeah, I would be surprised if there is a completely trigger reduction that allows to reduce the modulus Q and not reduce the modulus.
01:06:55.850 - 01:06:59.034, Speaker A: Q, but reduce the m squared to m. That's all I'm doing.
01:06:59.194 - 01:07:14.064, Speaker B: But you don't care about M. In some sense, M doesn't. M can be anything. In the worst case to average case reduction, the number of columns can be anything you want. So it is reducing Q. What? The goal here is to find a way to reduce the Modulus Q.
01:07:18.564 - 01:07:21.544, Speaker A: You don't want to set m to be like m squared, you know, like.
01:07:22.044 - 01:07:57.404, Speaker B: Oh, yeah, yeah, yeah, you can do it. Oh, yeah, definitely. Yeah, you don't need it. The reason I went all the way to M squared is just to give a simple thing. In fact, even adapting this from something that works, when you can find collisions with probability, one to find in a more robust reduction, it introduces a number of complications that make the actual reduction in the paper look quite a bit different from this. So this is only for the. To give an idea of how this type of things can be.
01:07:57.404 - 01:08:22.432, Speaker B: Okay, so there is another efficiency issue that I'll. Okay, so this one, I will just go through this. What time? So I started at ten, so I don't have 20 minutes left. Correct. Okay, so let me just go through this in our, let's say skip it almost. So there's a. But just mention at least more substantially.
01:08:22.432 - 01:09:04.521, Speaker B: Important issue related to efficiency is the following. So, okay, you can instantiate this function with fairly small numbers. So there are even some instantiation where is, say, eight bit num, a single byte. The dimension is 64. So you get a function that is mapping 1000 bit to 500 bits, and we don't know how to break it. In practice, it seems to have a good harness properties. The numbers, they look small, but once you put all these numbers together, what is the size of the key? What is the size of this number? You have n squared times log n.
01:09:04.521 - 01:09:40.714, Speaker B: So these functions, even for the simplest possible instantiations, they have fairly large keys, and they require a time, a computational time, which is proportional at very least to the size of the key. So they are not that good. From an efficiency point of view, they are polynomial time. But the complexity of computing these functions, which are matrix functions, grows quadratically with the dimension. So, can we get. So, in practice, we would like something that is linear in the dimension rather than quadratic. The dimension captures the security level of the function is the dimension of the lattice.
01:09:40.714 - 01:10:39.696, Speaker B: So it is the main security parameter in cryptography. So the idea that can be used here is the following. So, instead of using an arbitrary matrix, a use a matrix that has some kind of structure, for example, a circular matrix, a matrix where every row or every column is obtained by taking the previous one and rotating cyclically its coordinates. So this allows to represent each of these matrix blocks using only n coordinates, and the other are implicitly represented by a single vector. And so this was proposed as a one way function in paper of mine back in 2005. And it was using a class of matrices that is closely related, very similar to the one used by the entru crypto system, which had been proposed a few years before. But there was just a concrete proposal to perform encryption.
01:10:39.696 - 01:11:24.246, Speaker B: The focus on my paper was try to prove something about it, try to give some kind of evidence that these functions are hard. And 1 may you can still apply the average case to average case reduction, in the sense that, oh, this is still a lattice problem. If these lattice problems are hard on average, then the corresponding function is harder to invert on average, and it is efficient. So we should think in say that, okay, so maybe everything works fine also here, and we get more practical, perhaps you heuristic instantiation. So, let's consider a specific instance. This is using dimension four, so it should be fairly easy. So let's see what happens.
01:11:24.246 - 01:12:06.962, Speaker B: So you see, these are circulant blocks and a modulo ten. And you want to find a collision, you want to find a zero, one minus one combination of these columns that adapts to zero. Here there is an input vector which does not adapt to zero. You want to find something that gives zero. As a result, just by counting, you can show that a collision is guaranteed to exist. So, can you find a collision to this function without writing a computer program? So, when I wrote the paper, I couldn't find a collision to this function, but I couldn't prove that the function was collision resistant. So in the paper, I said that, oh, I can prove it's one way.
01:12:06.962 - 01:12:43.966, Speaker B: So it's a one way function. Collision resistance open problem. In 2007, collisions were found independently, simultaneously by Vadim Lubashensky, my student at the time, and I and Aloha Rosen and Chris Pikert in parallel. And they can be found as follows. So, multiply these blocks by 1111 by the all one vector. And since these matrix are circulant, these products will give constant vectors. Here, all the coordinates will be equal to six.
01:12:43.966 - 01:13:10.646, Speaker B: Here they will all be equal to nine. It's just the sum of the entries of the vectors. This reduces the collision funding problem to a one dimensional problem. Now, can you find a zero, one minus one combination of these four integers that add up to zero, six plus three equals nine. So you find it very easily. And this gives a collision to the original function. So the function is not collision resistant.
01:13:10.646 - 01:13:42.436, Speaker B: It is very easy to find the collisions. And the fact that collisions are easy to find is closely related to the fact that the circular matrices correspond to polynomial arithmetic modulo x to the n minus one. So, this is commonly used in the theory of c cyclic codes, in coding theory. And this polynomial factors, it has a linear factor. And this linear factor is exactly what brings this down to a one dimensional problem. So for this, so it is still one way. I could prove it is one way.
01:13:42.436 - 01:14:44.964, Speaker B: There is some harness, but it's not collision resistant. And so this can be fixed by using a similar type of structure. For example, it the dimension is a power of two. You can take negacyclic matrices, where when you perform rotations, you change the sign of the entry that goes around. And these correspond to working modulo x to the n plus one, rather than x to the n minus one, which, when n is a power of two, is an irreducible polynomial. So it doesn't have a small factor, and not only it breaks the previous attack, but it also allows you to prove harness. It is possible to show that if you can find collisions to this function, then you can solve worst case lattice problems in a corresponding class of lattices that have this same type of structure.
01:14:44.964 - 01:14:49.864, Speaker B: So there is some restriction on the type of lattices, but it is still a worst case to average case connection.
01:14:50.604 - 01:14:52.868, Speaker A: It was also discovered in the penalty.
01:14:52.956 - 01:15:41.204, Speaker B: Sorry. It was also discovered in the penalty. So this is from my work with Vadim, the negacyclic structure. What Chris and Alon did was to work with in lattices that have a prime dimension. So if you take x to the p minus one, it factors x x minus one times x to the p minus one, x p minus two, all the way down to x plus one. So Chris and alone suggested to solve the problem using this structure, while I think we mentioned both of them. And then we focus on the power of two, which is also much easier to implement.
01:15:41.204 - 01:16:49.554, Speaker B: And now, these are the lattices that are most commonly used in lattice cryptography, also in the LwE setting. So, most implementations of lattice cryptography uses now these mega cyclic lattices that were introduced to address the collision resistance problem there. Then together Lm and Pr, we wrote together a paper where we implemented these functions, showing that you can even implement it and run it with a very fast implementation running in just a handful of microseconds, milliseconds. I don't remember the exact number, but was that speed comparable to block cipher? Type of hash functions like ShA one type of functions which for lattice constructions, for constructions based on mathematics, was something that was much, much faster than say factoring based hash functions. And. Ok, so let me jump to the cryptographic applications. Ok, so ten minutes.
01:16:49.554 - 01:17:25.130, Speaker B: So there are, yes, three minutes per application. Ok, so what I want to do is to show how this sis function has some very interesting useful properties. And each property can be used to build interesting, useful applications that go beyond a simple one way function. Properties are compression regularity, the fact that the function is kind of linear. And I'll show you how to build the collision resistance hashing commitment schemes and digital signatures. Don't worry, each one of these is a single slide. So it's not that, I mean, there's a property and then the construction.
01:17:25.130 - 01:17:57.630, Speaker B: So, compression, this function is proved one way, using a setting where the domain is larger than the range. So we know that it is compressing the input. So you start from m bits and you get something that is n log q bits, which is smaller than m. So typically you make it twice as large. So it is compressing the bit size of the input by a factor two. So good. We have a function which is compressing the input.
01:17:57.630 - 01:18:49.574, Speaker B: So this is very useful in many applications in cryptography. So this is a hash function if collisions were hard to find. So these can be used. For example, to build Merkel trees, you can take a large dataset and then using this function to compress it two blocks at a time, you can compute a single value digest that somehow captures the entire big original data set. And then you can reveal the value of a leaf, just revealing the values along a path in this tree. So these are called merkel trees is one of the most useful and widely used constructions in cryptography. You don't need lattices to do that, but can you do it using lattices and say that, oh, breaking this is as high hard as solving lattice problems in the worst case.
01:18:49.574 - 01:19:27.968, Speaker B: Now this can be done using sis. I already suggested the thought, it's already collision resistant to start with, but it is also possible to show that you can go from one wayness to collision resistance in a black box way. So if this function is one way, then zero one is collision resistant. So assume that you can find a collision to function a. Now we want to invert the function. We want to find a preimage and notice we want a preimage of some value y. So a preimage of zero would be easy to find.
01:19:27.968 - 01:20:15.886, Speaker B: Those will be the collisions. But now we are also given we don't want to get zero. As a result, we are given a value y and we ask oh, can you find something that maps to it? What we can do is just to add the target value to one of the columns of the matrix. The matrix is random, so perturbing one column still gives the uniform distribution. And when you multiply it by your collision, so the collision are two different binary vectors x and x prime. If one of these coordinates is one and the other one is zero, then you can bring the y to the other side and get a pre image of y is just a linear manipulation that allows to show that you are achieving collision resistance. So property number two regularity.
01:20:15.886 - 01:21:07.864, Speaker B: So since we are above the smoothing parameter of the lattice, since the output is big, the input is bigger than the covering radius, we know that the output, the function is subjective. And not only that, it is also mapping the input to to an almost uniform distribution. And this follows from a pairwise independent argument. So if you fix two different inputs, x one and x two, not random, fix any two vectors x one and x two. If you pick the function a at random, these two outputs are statistically independent. So if I give you one value, if I only give you two values, I give you one, and then I ask you, oh, can you compute the function on the other value without knowing what a is? You don't know what value is. It is hidden in an information theoretic sense.
01:21:07.864 - 01:21:56.124, Speaker B: So it follows from pairwise independence that so independence together with compression implies regularity. So it's the standard result, independent of lattices, that the output is guaranteed that to be very close to uniform. So what is good for. So let's just take this property so that you can go back and look at the slides and look up the leftover ashlemma later if you like. But why is this useful, this regularity property useful? It's not just nice, because you can build it. We can use it to build commitment schemes. Now, a commitment scheme is a method to take a message and lock it in a box in such a way that the box does not reveal any information about a message, but still it protects the message.
01:21:56.124 - 01:22:36.832, Speaker B: The message cannot be changed, it is locked in the box. So it is like an encryption scheme, but without decryption key. So without a method to encrypt and decrypt. So formally, a commitment scheme is a randomized function that takes a message together with some randomness, a commitment. And the output of the function is called a commitment to the message m, which is obtained by computing the function on the message and on a randomly chosen value array. And how do you open the box? You reveal which r was used to compute the commitment. So that's what you do in order to open the box.
01:22:36.832 - 01:23:24.638, Speaker B: So what we want from this function, we want it to be hiding. We want that the output of the function, when r is chosen properly at random, should not depend on m, should be completely independent. The output should have no connection to, to m, because this value was chosen at random, and this randomizes also the output. So the box is black, so you cannot see inside. On the other hand, you don't want to be able to open it in two different ways. So it shouldn't be possible for an adversary to come up with two different openings, two different values of the randomness, or even the same one where the same one wouldn't work, that open to the same commitment. So they result in the same box, but they reveal two different messages.
01:23:24.638 - 01:23:50.662, Speaker B: So this should be hard, it shouldn't be possible to do that. So how can we build this type of function from sis? Well, we can take not one, but two Sis matrices. And just to anticipate vinod question, the dimension of the matrices doesn't have to be the same. I'm just fixing the matrices to have the same dimension, just for simplicity. So I have only one letter on the screen. Slide. Yeah, okay, let me finish.
01:23:50.662 - 01:24:27.520, Speaker B: Okay, so, and then just put these two matrices one next to the other. Think of them as a larger sis matrix. And so this is the function. It's the usual matrix vector multiplication, but what you get as a result is a one times m plus a two times r. And I claim that this is a good commitment scheme. So why so? Well, it is hiding, because even if you just look at the second part, we know it is closer to a uniformly random vector, so it will completely hide what was added to it. So the output distribution, even for fixed m, is completely random.
01:24:27.520 - 01:24:59.444, Speaker B: So this is the hiding property, hiding commitment. So can you open the commitment in two different ways? Well, that's also hard. And this time it is computationally hard. Finding two different openings is exactly the problem of finding a collision to this sis problem with a larger number of columns. So our proof, average case, worst case connection was working. You could pick as many columns as you want. So yeah, it is still a good distance of Sis, and it is collision resistant.
01:24:59.444 - 01:25:38.768, Speaker B: So this gives another interesting application so 1 minute left, one application left. So this function f is also homomorphic, linearly homomorphic. It's a linear function. So if you apply the function on the sum of two values x one and x two, you get the sum of the two outputs. So it's not exactly a homomorphism, because the domain is not closed under addition. It is a sort of, it is an approximate closure property. The sum of short vectors is still a short vector, but it's like a longer one, but still it is useful.
01:25:38.768 - 01:26:24.398, Speaker B: So you can perform a bounded number of additions and still get the short vectors. So what is this? So this property is trivial to show. And there is also a corresponding property showing that it is linear in the keys, which is also useful in some applications. So if you apply two different functions on the same input, you can adapt the keys together. So vinod made the use of key homomorphism of lattice functions in many interesting fancy lattice cryptographic applications. And here I will use it for a very simple application, which is that of one time signatures, that is, digital signature schemes that allow to sign one message. I mean you can sign more, but if you do, then it may not be secure.
01:26:24.398 - 01:27:21.070, Speaker B: Social Security only if you sign, if you sign one. So like a signature. So in a signature scheme, there is a key generation algorithm that produces a public key and a secret key. The secret key is used to produce a digital signature of a message, is a function that, given the key and the message produces some kind of cryptographic value, which can be verified using a corresponding verification algorithm, which will accept the signature as good or, or not. So when is a scheme secure? If an adversary which is given the public key, then it chooses a message and asks you please can you sign this message? And it is given the signature, because the scheme is used once by the honest user on an adversary chosen message. And after seeing this signature, the adversary can come up with a forgery, it can come up with a signature of a different message. So this is an attack.
01:27:21.070 - 01:27:52.944, Speaker B: So this should be hard. So it should. So if the adversary does this, then either the signature is not good, the verification algorithm fails, or it is the same as the original message. So it's not really a forgery, it's just the signature that was produced by the legitimate user, otherwise. So this is a win for the adversary. So we want to build a cryptographic scheme that has these properties, and we want to build it out of Sis. General signature schemes are similar, but the adversary is allowed to make multiple queries.
01:27:52.944 - 01:28:44.504, Speaker B: You can ask for signatures of arbitrarily many messages, so what do we do? We do the following. So, the key to the one time signature scheme will be a matrix consisting of short vectors. The tablet key is the hash of the secret key. You just multiply the secret key by a, and that's the public key. So how do we use this? We can take a message m, and we define a signature of m as a linear combination of the secret key. So, we have our secret key x here. So, I'm dividing into two parts, big x and small x.
01:28:44.504 - 01:29:17.246, Speaker B: Big x is a matrix. Small x is a vector. You can combine all of these in a single matrix, if you like. And a signature is a linear combination of the columns of the secret key, where the coefficients of the secret combination depend on the message m. And I call this a signature. And notice you can use the linearity property to verify these signatures if you apply the transformation a to the signature. This is a times xm plus x bilinearity.
01:29:17.246 - 01:29:53.784, Speaker B: This can also be expressed as a linear combination of the public keys. So the verification algorithm takes the signature, takes the message, takes the public key, and checks that the secret key, the signature, hashes to a good combination of the public key. So this for verification. What about security? Oh, you can also make this more efficient using cyclic lattices. But let me skip the cyclic lattices details. So, this is what we did. So, our focus in the paper was to get efficient construction.
01:29:53.784 - 01:30:52.208, Speaker B: So the main claim was about, oh, you can do this very efficiently. So why is this secure? Because if the adversary comes up with a forgery, which is a signature, which hashes to this value, we can also use the secret key to come up with our own signature of the same message. And you can show that with hyperability, these two signatures will be different. So there is no way for the adversary to guess which would be the signature that I would produce using my secret key. So, if the adversary can produce a signature, then you have two signatures hashing to the same value, which is a collision to the SIS function, which we know is hard on average problem. Okay, so, this concludes a brief sketch of the type of cryptographic applications you can build from SIS. You will see many more applications from LWE stuff starting today.
01:30:52.208 - 01:31:15.084, Speaker B: Tomorrow, Vinod will talk about you talk about, not today, today scriss about the theory of Lwe, the harness of Lwe. And then tomorrow we'll go into applications of LwE. But is a brief how to say appetizer about just using the simpler sis problem of why sis is hard and why it is useful in six alternative.
01:31:28.764 - 01:31:39.884, Speaker A: Going once. Okay, either everyone is hungry or everyone understood everything or both.
