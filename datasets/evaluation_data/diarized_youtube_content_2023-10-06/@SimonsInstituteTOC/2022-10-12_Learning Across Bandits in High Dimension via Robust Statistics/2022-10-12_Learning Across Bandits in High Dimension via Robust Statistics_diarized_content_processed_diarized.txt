00:00:00.240 - 00:00:29.144, Speaker A: Right, let's get back to it. So, thank you, Hamza, for taking up the mantle of the last talk of the day. But the good thing is, she's an amazing speaker, so we'll be fine. So, yeah, I should introduce Hamza Bastani, and she's Sheath Borden school, University of Pennsylvania. And it's great to have you here again. And she'll talk about learning across bandwidth. Thank you, Shipra.
00:00:29.144 - 00:00:55.644, Speaker A: Oh, this was on. Okay, great. Thanks, guys, for sticking around for the last session. And apologies for making you stick around to the last session. I'll try to keep it snappy. This is work primarily by my PhD student, Khonshu, who is on the job market. And so I'm really going to be borrowing heavily from his work on learning across bandits and high dimension via robust statistics.
00:00:55.644 - 00:01:27.612, Speaker A: I always like to start off with a little motivation about why we should care about these problems in practice. I think generally, if you work in resource constraint settings, we often face practical small data challenges. There are several reasons. You might have a new predictive test like a new drug or a new disease like Covid-19, you might have imbalanced or rare outcomes. The outcomes that you're looking to measure might have very, very high variance. Because of that, uh, you might have non stationarity. I think this is always true in practice.
00:01:27.612 - 00:02:11.894, Speaker A: But if it's excessive, then, um, you start having a very limited window of training data that's representative. And finally, you often have high dimensional features. And so that means that that doesn't necessarily increase variance, but it means we need more data to do effective inference, uh, which makes problems hard when we're faced with these kind of small data challenges. Right. Uh, and so two popular approaches for dealing with this kind of, uh, small data problem. One is to do bandits so adaptively collect data in regions where our model is most uncertain, so that we try to be efficient with the limited budget that we have. And another thing is to do transfer learning, try to leverage auxiliary data streams to reduce variance of the current estimator that we have.
00:02:11.894 - 00:03:01.804, Speaker A: And these two literatures are largely independent, although there is a growing stream of literature kind of at the intersection. And I think they actually go really well together, because often when you try to deploy bandits in kind of small data settings, the bandit always ends up always exploring, because the variance of every arm is kind of prohibitively large, and so it doesn't actually get to exploit within a small time horizon. T transfer learning can kind of be critical to reduce the variance so that we actually move from exploration to exploitation so I'm going to give an example based on some work with key. So he's going to talk about this as a teaser. I mean, consider this a teaser. Next week, two weeks, okay, very soon in the next workshop and he'll do a much better job than me. So you should definitely attend that talk.
00:03:01.804 - 00:03:43.074, Speaker A: But we did some work with Vishal Gupta on targeted Covid-19 testing. Basically we were looking at tourist arrivals to Greece in the summer of 2020. We had a very limited budget of tests to allocate to these arrivals, so only about six to 8000 people could be tested. And the goal was to identify as many infected travelers as possible. So this is a pretty standard kind of bandit problem. You have people coming in with some features that we use to predict how risky they are for Covid and then based on that we decide whether or not to allocate a test to them. Given a limited budget, we get feedback back from those tests that those feedback into our system to build new predictive models and then we kind of continue this feedback loop.
00:03:43.074 - 00:04:23.904, Speaker A: So the objective is basically, I'm not going to go into any details to maximize the number of expected infections caught. And we have a standard exploration exploitation trade off in the sense that we want to test enough passengers of different types so that we know whether or not they're risky. And then on the other hand, we also want to use these estimates to basically figure out who is the most risky so we can find and quarantine them. Right. So standard solutions, you can use upper confidence stomps and sampling whatever you want. So Kimon will probably talk about how there were a lot of other practical challenges that we also addressed, but I'm not really interested in that for this talk. I just want to focus on the small data challenge.
00:04:23.904 - 00:05:05.324, Speaker A: We had very, very imbalanced data here. If you test about 500 people, only one of them will test positive, which is excellent from a public health perspective. If a lot more people than that were testing positive, you have a real problem and you shouldn't be opening your borders. But this also means that even if you test thousands of people a day, you're really only getting a handful of positives. And so you have huge variance. And on top of that, the main feature we're using, origin location, is kind of a very high dimensional categorical variable with many, many levels. So this means that if you actually try to look at the variance of your estimator of risk for any kind of subpopulation, it's huge.
00:05:05.324 - 00:05:28.064, Speaker A: So Vishal made this cute plot where on the x axis. We basically have this empirical base, like prior strength. Uh, the challenge here is that predicting just zero for everyone's risk is already a very, very good estimator. Right? Because one in 500 people test positive. So if I just said no one is positive, I would get 99.5% accuracy. Yay.
00:05:28.064 - 00:05:55.164, Speaker A: We win. We call it a day. Right. But, of course, this is prescriptively useless, and so our goal is to do better than that. If you use a classical estimate that doesn't kind of do any transfer learning, you can't really beat this. Or at least we weren't able to beat it. And so we use a heuristic that basically use lasso for dimension reduction from that original high dimensional feature space, and then empirical base to kind of do some knowledge sharing across types.
00:05:55.164 - 00:06:28.494, Speaker A: And so this actually was better than the zero estimator. I won't talk any more about this example, but it was effective. We caught more cases than random testing. But this really brought us to this question, which is, how do we learn in a principled way, not some sort of heuristic across many simultaneous, heterogeneous contextual bandits in moderate to high dimension. Right. Which is really the focus of this talk and what Khan has been working on. So, there is a lot of prior work on kind of studying multitask bandits.
00:06:28.494 - 00:07:13.636, Speaker A: They usually use, like, you need to relate the parameters in some way. They kind of use ridge regularization or a shared Bayesian prior or something like that. Unfortunately, none of these really improve, and sometimes they actually worsen the regret bounds. And I think this is sort of unescapable, unless you want to impose some sort of structural assumption. So, what we do here is, basically, we look at a bunch of real data sets, and we try to impose what we think is kind of a reasonable structural assumption, which we can provably show improves performance. And we want to pay special attention to these data poor instances. So, locations or arms that have very little sample points, because those are the ones that kind of hold up the bandit in its path.
00:07:13.636 - 00:07:40.132, Speaker A: If you need, and please interrupt me if you have any questions. Thoughts? So, this is a pretty general problem. It's not, you know, just for COVID testing or something like that. So, if you were thinking about, you know, do dynamic pricing or inventory management, you can think of each bandit as, like, a different store. Every store is different. It faces a different customer population, different trends and so on. And so you want to kind of personalize it to that location.
00:07:40.132 - 00:08:14.662, Speaker A: On the other hand, you want to do some knowledge sharing so that you actually converge faster. Similarly, imagine that you have a network of hospitals or health facilities. You can treat each of those facilities as a separate bandit instance, but at the same time, you want to do some knowledge sharing across locations, because ultimately, these are all kind of small data problems. Okay, so let me jump into formulating this. So, this is a single contextual bandit. So, imagine that you have basically one hospital or service provider. You have a bunch of arms, you have k arms, each with some parameter vector.
00:08:14.662 - 00:08:43.430, Speaker A: And so we observe at each time step, an individual arrived with some feature vector x one, x two, x three, and so on. We're just going to consider a linear contextual bandit. So if you choose an arm k with unknown parameter beta k, we observe reward, just x transpose beta k, plus some sub gaussian noise. So the very, very standard framework. Um, and we all know how to solve this. There's been tons of work done on this. You know, kind of pull up any of these algorithms.
00:08:43.430 - 00:09:24.814, Speaker A: If I didn't cite you, I apologize, I ran out of space. Now, you can imagine now, taking a meta perspective, right? Let's say I have many different bandit algorithms operating at the same time. Now, I have n different instances context, or people are going to arrive at each of these service providers with some probability PI. So these p's will sum to one, and each instance has its own set of unknown arm parameters. So note that these are now different. So the same treatment or intervention at a different location might actually have kind of a different parameter vector due to the idiosyncratic biases that are associated with a specific provider. So I'll show you a numerical example in a little bit.
00:09:24.814 - 00:10:12.626, Speaker A: And so now we have individuals arriving. So with probability p one, they come to the first provider. Then with probability p three, maybe they come to the third provider, and so on and so forth. And you'll see that depending on what these values of pr, some locations might be data poor, because it's very unlikely that they actually get customers, because maybe they're small or they serve a rural location. Other locations might get a relatively proportionally large number of individuals. Okay, so this is some regressions that Kahn ran on diabetes diagnosis data. So this is actually claims data across 13 different hospitals where he was trying to predict diabetes diagnosis on the next visit.
00:10:12.626 - 00:10:47.010, Speaker A: So, fairly standard kind of prediction task. And so what we're looking at is the adaptive error rate over time. As we look at the number of patients, know that the horizon length is pretty small. In healthcare, it's not like you can look at thousands of patients, like 175 is actually fairly large. And this gray line here is the oracle. So the first thing you'll see is that the green line, if you were to just ignore all of the other hospitals and do one bandit per hospital, which is a single bandit, you get a lot of variance. So eventually this is going to converge to a no regret policy.
00:10:47.010 - 00:11:11.940, Speaker A: But there's a lot of variance because you have very little information to work with. Right. If you do data pooling. So I just kind of treat all of the hospitals data points as the same. You actually get this blue line over here, which originally in the short time horizon does much better because I have more samples. But then as I continue, it actually flattens out. And that's because this is a biased estimator.
00:11:11.940 - 00:11:36.404, Speaker A: I'm not treating this hospital's idiosyncratic biases differently from the other twelve hospitals. And you'll see that hospital data sets, especially models, don't transport well. And so this is important to incorporate. And so this actually ends up not doing much better than the single bandit and the algorithm. We'll talk about the red line over here kind of tries to gracefully interpolate between these two extremes.
00:11:40.744 - 00:11:44.324, Speaker B: So just to make sure. So your reward here was like, did you get the prediction?
00:11:44.864 - 00:11:49.404, Speaker A: Yeah. Later I'll show you, like the regret. But this is just for illustration. Yeah.
00:11:50.724 - 00:11:59.004, Speaker B: So you have two parameters here. One is for any given bandage, how many data points you have of past.
00:11:59.164 - 00:11:59.644, Speaker A: Yeah.
00:11:59.724 - 00:12:02.484, Speaker B: Then you have the number of different bandits.
00:12:02.604 - 00:12:03.196, Speaker A: Right.
00:12:03.340 - 00:12:08.944, Speaker B: So what is the kind of, what is the regime?
00:12:10.964 - 00:12:35.344, Speaker A: We're thinking of n to be like fairly large, like, you know, omega d ideally, but you know, 13 a ton, anything. Number of instances. The number of arms are dependent as kind of the same as in the regular contextual band aid because we're not doing sharing across arms. You could do that, but we're thinking about sharing for the same arm across instances. So we don't really change that as.
00:12:35.384 - 00:12:37.104, Speaker B: The number of instances grows.
00:12:37.184 - 00:12:37.844, Speaker A: Yeah.
00:12:38.224 - 00:12:41.760, Speaker B: Would the blue and red become closer?
00:12:41.912 - 00:13:12.174, Speaker A: So the blue is hopelessly biased because it's treating data. This is a very naive baseline. It pulls all the data from all instances. And so if I'm basically bringing in data from a predictive model, that's not the correct predictive model. So it's never going to converge to the oracle, whereas the red is going to discard data from these proxies, the green will eventually meet the oracle. Yeah, exactly. Yeah.
00:13:12.174 - 00:13:38.400, Speaker A: This might become clearer when I say the assumption that we're going to make. So in general, you might say, okay, beta one, beta two, beta three are, like, totally different. Right. For each hospital. I mean, that is kind of what the multitask banded literature has done. Like, maybe they're small, but, like, as we take t to be large, it doesn't really mean it's a constant. Right.
00:13:38.400 - 00:14:21.098, Speaker A: Um, and so what we're going to impose instead, uh, is a slightly stronger assumption, which is that each arm parameter can be decomposed into the sum of a shared global parameter. So, note that these vectors are all different. They're colored differently. Uh, but then this there, I'm going to write it as a sum of the same vector plus a sparse instance specific vector. Right? So white squares here means that that component is zero. Basically, we're saying if we're talking about a diabetes treatment or something like that, there is a specific shared predictive vector. That's true for all locations, but then each location has some bias, which is going to corrupt that predictive model.
00:14:21.098 - 00:14:54.162, Speaker A: That's why models don't transport well. But that model is somehow of lower complexity, so we can project it down into a lower dimensional learning problem. So we're going to say that this, this delta here is sparse with l zero norm s, where s is much smaller than d. That's really what we're gonna leverage. So you might say, why are you making this assumption? Well, okay, so if we look at the diabetes hospital data, this is what it looks like. Let me break this down a bit. So this is after we've removed what we've estimated to be the global shared parameter.
00:14:54.162 - 00:15:30.894, Speaker A: So, on the x axis, we have all 75 ish features. On the y axis, we have all 13 hospitals. Basically, a single row of this matrix is going to be the delta, the idiosyncratic bias term corresponding to that specific hospital. So this is it for hospital seven. And a colored square indicates that there's a statistically significant non zero value at that point. What we're seeing here is basically every hospital does have some sort of bias, and those biases are totally different. They're completely not aligned.
00:15:30.894 - 00:16:20.308, Speaker A: We actually dug into some of these. For some of them, there might be something like the reason they're happening is because there are patient population differences or biases in measurement, such as, aria has done a bunch of work on this. For example, you might look at impaired fasting glucose, which is ICD nine, code 790.21. This is actually super predictive of having diabetes, because if you have impaired fasting glucose, you basically have pre diabetes. But unfortunately, it doesn't get recorded for most patients, and it actually varies by hospital, how often it gets recorded, because it actually requires the patient fast. Depending on how on top of things your provider is and how conscious that patient population is, this feature may or may not be censored. If it's not censored, then it's very predictive.
00:16:20.308 - 00:17:04.678, Speaker A: If it is censored, it isn't predictive. That means that if I train my model on a hospital where people are consistently measuring this feature, and then I use it on a hospital where they're not consistently measuring that feature, it's not going to work. Well, these are the biases we're trying to capture. That's easy to capture in as far as Spectre, because I just need the few features that are correlated with that particular feature. Similarly, there's other issues, like, if we look at BMI data, the BMI distribution across hospitals is kind of similar, but some hospitals actually record an obesity diagnosis for the, uh, the tail of distribution of those patients. Uh, but that's not true everywhere. Of course, obesity, when present, is predictive of diabetes.
00:17:04.678 - 00:17:09.474, Speaker A: Uh, but then you can't transport that model to a hospital that doesn't record it, right?
00:17:12.214 - 00:17:12.994, Speaker B: Yeah.
00:17:21.094 - 00:18:05.974, Speaker A: So we have a plus vector, and then it's just saying that this is the sparse matrix. So most entries are zero. And I'm also going to empirically note the fact that there are no, like, columns. There are not many columns that are all zero. So these, the non zero entries are kind of randomly spread out, which makes the problem harder. So, again, let me just briefly bring this back to the related literature. So, there's been a lot of work on multitask transfer meta learning, but again, without imposing this sort of an assumption or some structure, it's hard to basically improve performance bound.
00:18:05.974 - 00:18:47.484, Speaker A: So that's really what we're leveraging here. And so we introduce a new multitask estimator. It turns out this structure is non trivial to leverage. So we combine kind of robust statistics and lasso to do this, which, as far as I know, is a pretty novel combination for an estimator. And then we embed this into a multitask bandit algorithm to show that we can actually get regret bound improvements, which, again, I think are not present in literature. So, first, I'm going to just talk about the supervised learning problem, and then we'll stick this into a bandit. Okay, so let's say I have data across n neighboring instances, and each one has some sparsity level s for my idiosyncratic parameter.
00:18:47.484 - 00:19:16.860, Speaker A: So what does my data look like? Each instance has NJ observations. So xj is my feature matrix. Yj is my response column. And so, based on my assumption, I can write beta j as a global parameter, plus the instance specific bias, and then plus noise. And my goal is to estimate each beta j. I'm going to consider two cases, the standard case. The standard case is when my hospital has a similar data size as other hospitals.
00:19:16.860 - 00:19:46.034, Speaker A: So I'm kind of an average hospital and the data poor case. This is especially the one we're interested in, where I'm in a rural hospital or something like that, that gets very few observations. And so maybe my sample size is way smaller and I really need to transfer them. Okay, so we'll start with to build some intuition, using some simple baselines. The dumbest thing you could do is to just do the single OLS estimator. I'm not going to do any transfer learning. I'm just going to fit.
00:19:46.034 - 00:20:12.514, Speaker A: I know it's a linear model. I'm going to fit an OLS estimator. This is unbiased, but of course it's going to have high variance when my sample size, NJ, is small. And so what am I going to get? So, in the standard regime or the data poor regime, I get something like d over root NJ. I'm looking at l one error, and this is actually a lower bound, well known. So you might say, okay, that's silly. Like, obviously I have data from all these other hospitals.
00:20:12.514 - 00:20:42.394, Speaker A: I want to kind of bring that in, right, to do better. So, one thing you could do, which some of my colleagues in Wharton stats did, is to kind of average. Like, this comes from federated learning, maybe. I average all my linear regression parameters across all the locations, right? That's one thing you could do. Another thing you could do is to pull my data together. That's kind of the red line that I showed you before. So I treat all the data from all my hospitals equally, and then I just fit a single linear regression, right? And then you get this estimator.
00:20:42.394 - 00:21:34.110, Speaker A: It turns out both of these actually have the same lower bound, and there are low variance. Now, I have a lot more data, but high bias, particularly in the standard regime, I'm going to get a big improvement on my variance. Instead of d over root n j, I get d over root be gennj, which is tiny. But then I get this delta j l one norm bias. So that comes from the fact that I'm not accounting for this hospital specific idiosyncratic behaviors. So like that obesity encoding or something like that, in the data for regime, my variance is even lower, right? Because I know that my sample size is much smaller than my neighbor's sample sizes, but I'm still not getting rid of this bias, which is kind of really the bottleneck at this point. So you might say, okay, let's try to debias the shared model estimate.
00:21:34.110 - 00:22:21.534, Speaker A: So I'll also try to estimate delta j. So what might you do? In the first step? Maybe I average together my linear regressions to get an estimate of that shared parameter, that global parameter. And in the second step, I'm basically going to run a lasso on instance j's data to try to estimate delta j, which is as far as parameters. So hopefully the sample complexity isn't too bad. The problem is that step one does not converge to the shared parameter. It converges to the shared parameter plus the average of the biases across all n locations, right? Yes. You don't need to know s for this estimator, but the bounds depend on s.
00:22:21.534 - 00:22:54.514, Speaker A: Usually you just pick the lambda via cross validation. That's what we did in the plots. Other questions? Okay, so let's see what happens here. So, if so, again, this is a similar plot as before, except now I've transposed it. So every column is a delta corresponding to each instance. So I have six instances here, and the rows are basically like the feature components. Right.
00:22:54.514 - 00:23:40.478, Speaker A: And so in the nice case where all my corruptions are non zero values kind of aligned in the same, so along the same features, this is actually fine, because then the average of my bias is also kind of sparse. And so I can actually just use lasso to debias this in the second step. So nothing bad happens. But I'll remind you that when we looked at real data, this was not the case. The nonzero terms were kind of spread out all over the board. And so the real, more realistic picture looks more like this, where, like, you know, the blue squares and non zero components are kind of, like all over the place and they're not well aligned. If that's the case, then the average of the bias terms is also not going to be sparse, which means that you can't use lasso in step two to debiasis.
00:23:40.478 - 00:24:08.854, Speaker A: So if we actually compute the lower bound, we actually see that we've made no progress. So, like, this variance is basically the same. This bias is actually possibly larger. When n is large, NS is going to be much larger than D. And so we haven't actually made any progress over just the independent OLS estimator. So those are the simple things that we started off trying. But this actually gives us a pretty good idea.
00:24:08.854 - 00:24:43.820, Speaker A: So I'm going to introduce what we call the robust multitask estimator, where we're actually going to split up our components like our D different feature components into two subsets. One subset we're going to call the poorly aligned components. So that means that you, in any given row we only have a few of these non zero blue squares. So obviously we're not going to know a priori what this is going to be. I'm just going to argue that we don't actually need to know it. So this means that fewer than zeta n of the instances have support along these components. And then the remaining I'm going to call the dark blue squares, the well aligned component.
00:24:43.820 - 00:25:16.614, Speaker A: So that means that maybe more than theta n or the majority of these components actually have support. Right. And by pigeonhole principle we know that we can't have too many well aligned components. In fact we can't have more than s over zeta of them. The reason being I know that each of these columns is s sparse. So there are sn, sorry, sd possible entries in here, sorry ns possible entries in here. And if I want basically a lot of things where I have more than zeta n terms in a single row, I can have at most s of those components.
00:25:16.614 - 00:26:00.168, Speaker A: That's going to be another kind of the key argument, right? So there are not too many of these like bad components. So in the first step, before we use an average, instead of that we're going to use a robust statistic. So we're just going to use the trimmed mean. You could use the median, but it doesn't have as nice properties or it's less general. And so what does a trim mean do? Basically it'll converge to what our global parameter component is going to be for those poorly aligned components. So let's say that we have, we take this particular component d, which has very few nonzero entries. Let's say it's 30 zero -400 the trimming will basically order these.
00:26:00.168 - 00:26:29.784, Speaker A: And then once we order these, it trims off the tails. So we scratch off Zeta N values on both ends and then we take the average of what's left, right, and that average is zero, even though this average was not actually zero. Right. So basically just kind of removes these corruptions, treating them, these non zero values, treating them as outliers. So that's great. And then we're just left with the well aligned components. Well, we know that the well aligned components, there's only a sparse number of them by pigeonhole principle.
00:26:29.784 - 00:26:55.028, Speaker A: And so we can just include that as part of our loss of regression to actually debias those as well. So we're done. So basically, this breaks up into a sum of three quantities. So beta j minus beta hat rm zero. So that's my robust estimate of the global shared parameter is going to be beta j minus beta zero, which is a sparse term. I can debias that in my second step, beta zero. I'll get back to that.
00:26:55.028 - 00:27:23.418, Speaker A: The second. This third term, which is just noise. So it's not sparse, but it's small. It satisfies standard tail inequalities. And then we've shown by pigeonhole principle that this guy is s over zeta sparse. And so this whole thing is sparse or approximately sparse. And so all we're doing is that we're using, we're taking this robust statistics to estimate the shared components, and then after that, we're going to debias it for it to learn my instance specific model using lasso.
00:27:23.418 - 00:27:25.214, Speaker A: So it's a two step estimator.
00:27:29.114 - 00:27:40.014, Speaker B: Can I ask a very nice, is there a reason that we cannot do lasso together for multiple tasks by explicitly learning v zero and delta?
00:27:40.594 - 00:27:48.346, Speaker A: So we're not, for us, like, our parameters are all dense, so they're not group spars or anything like that, is that what you're asking?
00:27:48.410 - 00:27:54.354, Speaker B: So what I was saying is I basically do the progress version, but instead of breaking down two steps.
00:27:54.434 - 00:27:54.850, Speaker A: Yeah.
00:27:54.922 - 00:28:05.342, Speaker B: I'm just going to solve one problem where I'm learning this. Share beta and delta. And delta is supposed to be have like sparse norm or you're doing this right and for multiple tasks together.
00:28:05.438 - 00:28:21.598, Speaker A: So you can do this for average and multitask. You're absolutely right. Although we don't know how to do lower bounds for that. But you could do it in one step. But the second one you can't because we're in the. After we are estimator in the first step, we're now going to do this component by component robust, trimmed mean. Right.
00:28:21.598 - 00:28:28.670, Speaker A: So that argument breaks if you try to do it together. It's not just a regression problem, it's also sort of an outlier removal problem.
00:28:28.862 - 00:28:35.384, Speaker B: But if I do it in one step, I will get this type of base delta task. I just add it together.
00:28:36.124 - 00:29:10.204, Speaker A: Right. But that actually doesn't converge. So the way we stated our model is kind of weird, right? Like, beta zero isn't like identifiable. Right. There's actually a big l zero ball of potential estimates. From what we could tell, we couldn't actually get something that was in that l zero ball without doing something like this. But I'm not arguing that there isn't, other than.
00:29:10.204 - 00:29:40.878, Speaker A: Okay, so if we actually plug this in to get upper bound, we can now get upper bounds. We want to compare our upper bounds to the lower bounds of baselines. So if we can say that the upper bound of what we've done is kind of lower than the lower bounds of the naive baselines, then we've actually made progress. Right? So that's all we're trying to do here. So in the standard regime, we basically see that we shave off a factor of square root d and we replace it with square root s. So that's nice. And then we still keep the low variance associated with the averaging estimator.
00:29:40.878 - 00:30:15.504, Speaker A: So we basically have this variance term from over here. We've removed the bias completely, but then we pay this extra cost. But I think it's more interesting in the data poor regime where we actually shave off the, the dimension dependence completely. So I've hidden log factors here, but you can see that it only depends on s over root mj. It's actually really intuitive to imagine why this happens. I'm basically using all of the other hospitals to learn my global shared parameter. Once I know my global shared parameter, it's just a sparse learning problem, but it's nice to see that it recovers that limit.
00:30:15.504 - 00:30:48.864, Speaker A: Great. So now we've all only talked about supervised learning. I'm going to go back and stick this into a multitask bandit algorithm. You can essentially embed this into any linear contextual bandit, but because there's a lasso in there, it's a little hard to analyze without embedding it into a high dimensional contextual bandit. So that's what we did. So we're basically building on prior work here. But there's two innovations that we had to make.
00:30:48.864 - 00:31:34.228, Speaker A: One is that the multiple task estimator actually introduces annoying correlations that you don't have in a single bandit instance, because now you're learning across locations. So that global shared parameter is sort of endogenous. And so we introduce a batching strategy to basically kill those correlations. And the second thing is that how we trim in that trimmed mean also has to be tuned to deal with a bias variance trade off. So in particular, like when we have very few patients, the single bandit, so green here is variance, red is bias. The single bandit actually has a lot of variance, and the robust bandit has like, you know, some bias and some variance. If we don't tune that bias variance, straight off, eventually, the single bandit is actually going to take over, right.
00:31:34.228 - 00:32:18.504, Speaker A: Because its variance keeps dropping. Our variance keeps dropping, too, but our bias kind of never goes away, right. And so what we actually need to do is make sure that we aggressively trim so that that trimming parameter zeta that I showed you has to keep on increasing with your time horizon so that you can actually match how the bias dies off with how the variance dies off. And so if we plug this in, we get some regret bounds. So in the first row is a single banded regret bounds from the literature. And in the second row is kind of a robust multitask bandit bounce that we get, and again, broken down by standard instance or data, for instance. So in the standard instance, again, you see that we shave off a factor of d and we replace it with s.
00:32:18.504 - 00:32:46.702, Speaker A: It is worse by a factor of log t. And I think this is just because the high dimensional banded algorithm is worse by a factor of log t. I don't think that's actually supposed to be there, but we don't know how to get it rid of it. But what's really interesting is the data. For instance, again, now we get this exponential improvement in D. We used to have a D squared, now we have an s squared. Right? So again, highlighting that, you know, transfer learning is kind of most important for instances where we have very few samples.
00:32:46.702 - 00:33:15.844, Speaker A: That's why we needed it for COVID testing, because most countries were data poor countries. And so this was sort of necessary. Right. So I'll make a few remarks. So I think a couple of things that we try to do when designing this algorithm, or rather contrary to doing designing this algorithm. One is that it's a federated approach, so you don't actually need to share raw training data. We only share the aggregate regression parameters when we take the robust mean.
00:33:15.844 - 00:34:01.228, Speaker A: And so that's nice because hospitals usually don't like to share raw EHR data. Second of all, it's also fair in the sense that we're trying to, we get the larger improvements for data poor instances, which we think is useful because a lot of hospitals that are kind of data poor tend to be rural hospitals or one serving minorities. They're also the ones that suffer from more of these idiosyncratic biases because they don't follow up with patients as much. And so I think there's something interesting to be said about how transfer learning could reduce kind of the price of fairness literature that's kind of been studied in Ops. And then finally, this is really easy from a production constraint standpoint. Because, you know, we're doing it in a batched way. And so you only need a few batched offline updates to your models.
00:34:01.228 - 00:34:42.894, Speaker A: This is not something that's going to adaptively run in real time, which is nice or easy from an engineering standpoint. Any questions before I jump into experiments? So let me show you two experiments, one on diabetes diagnosis and another one on dynamic pricing. So this one is, again, this is the same 13 hospital example I've been showing you, but in more detail. So we've had 4000 ish patients. The mean hospital only serves about 300 ish patients who are relevant to kind of the diabetes diagnosis test. So the cohort is relatively small. The median is about 300 patients.
00:34:42.894 - 00:35:09.498, Speaker A: We have about 76 features gotten from their demographic information, patient conditions, medications. These were the features that we saw before. These were sort of hand selected based on what people considered relevant for diabetes. Right. And you can see that this is somewhat high dimensional. We're talking about, you know, like 300 ish patients and we have, you know, 75 features, so that's not great. And so we have about two arms.
00:35:09.498 - 00:35:56.064, Speaker A: So whether or not we assign the treatment, we're going to model a binary reward, which is that if you give them the right treatment, that's good. And then we're going to compare to the oracle, which is the estimated true model, using all the data, the kind of the OLS banded algorithm, a high dimensional banded algorithm and one multitask banded algorithm from the literature. And basically this is what we see on the x axis is the number of patients. And so the first plot is for a data poor instance. The second plot is for a relatively data rich instance. And you can see that in all cases, like the RMB kind of hits the oracle much faster and it struggles more for the data poor instance. That's just because the time horizon is kind of pulled out over longer.
00:35:56.064 - 00:36:02.980, Speaker A: So during the same amount of time, we're seeing a lot more patients accumulate in this plot than we are in that plot. Right?
00:36:03.052 - 00:36:06.144, Speaker B: Yes. I wonder how you chose.
00:36:09.324 - 00:36:32.444, Speaker A: I think cross validation. We just did like a grid and pick the best looking plot. But that's a question for Khan. Yes, there are a lot of hyper parameters in developers, but we just, I think, individually search each one in a group. Yes.
00:36:33.424 - 00:36:35.056, Speaker B: You probably need some assumption on like.
00:36:35.080 - 00:37:18.344, Speaker A: The population covariates, because an actual population covariates. So we're using actual medical data, so we're not assuming anything, we're just throwing it at the data that we're seeing. The only assumption maybe is like we don't actually have time stamps, so we don't know the exact sequence in which patients come. So we take a random permutation of the patients. Great. And so we see that there are about 30% fewer errors from doing the kind of multitask robust approach than not using cross hospital information. So that's just an argument for saying we should share information.
00:37:18.344 - 00:37:57.400, Speaker A: Sharing information can really, you know, help speed up learning for whether you're a medium sized hospital or, like, a data poor hospital. And then we did a similar thing with a dynamic pricing data set, which I think Khan found on Kaggle. So this is a meal delivery company. It has about seven stores. I think it's something like hellofresh or blue apron. They have a weekly sales of 51 different meal plans over 145 weeks, but not all of them have the full coverage. And so the mean is that we see about 6747 week plan like product observations, and there are about 18 features.
00:37:57.400 - 00:38:38.358, Speaker A: So this is less high dimensional, if you will. There's a lot more observations, fewer features, and the features are things like email promotions, whether it was featured in the homepage, the cuisine type, the food category, and so on. He assumes a linear demand function. There's been work on this from the dynamic pricing folks like Ben and Kaskan on dynamic pricing with features. We adapt this to what our algorithm might look like. We just plug in different estimators where they use ols. You might have an OLS version, a lasso version, this goblin version, and then the robust multitask version.
00:38:38.476 - 00:38:40.174, Speaker B: So what are the actions here?
00:38:40.474 - 00:39:02.978, Speaker A: The action is a continuous action now. So it's the actual price that you want to give. Yeah. So that's why we're just sticking our estimator into a standard dynamic pricing algorithm. So this isn't the bandit anymore. So it's more like a linear bandit or quadratic bandit, I guess. You have demand is like a linear function of price, and then you're optimizing revenue, which is price times demand.
00:39:02.978 - 00:39:24.824, Speaker A: So it's kind of quadratic interaction. Yeah. So here we get to see the. So we see the stores policy observed from data. And so it's not as clean as a diabetes example. But given the linear function and the whole data set, we can extrapolate what might be an optimal price. So we see a price path, and then we extrapolate from that.
00:39:24.824 - 00:39:34.284, Speaker A: In the diabetes example, we have actual counterfactuals because then the patient shows up next week with diabetes, not next week, hopefully. That'll be really sad.
00:39:37.744 - 00:39:46.544, Speaker B: So you're assuming that the demand coefficients themselves are all perturbations, are all sparse perturbations with the linear sparse.
00:39:46.664 - 00:39:57.160, Speaker A: Oh, yes, yes. Excellent. So, like, the model is. I believe so. That's. We sold this from Bannon Keskin. So it's like beta, x times a plus b times beta.
00:39:57.160 - 00:40:13.914, Speaker A: So there's like a linear transformations of the features, and then each feature is also productive with the price. Right. So you get a price feature dependent outcome, but it's all linear. And so that's the beta that we take. And then we assume that those are sports perturbations. Excellent question. I should have put that in here.
00:40:14.374 - 00:40:20.166, Speaker B: Are there like, is this like you have to worry about the cost elasticity or anything?
00:40:20.270 - 00:40:20.798, Speaker A: Yes.
00:40:20.926 - 00:40:25.326, Speaker B: So did you. Yeah. Do you put sparseness on the cross elasticities as well?
00:40:25.430 - 00:41:02.124, Speaker A: Yeah, so I think you can. I think we have product specific taste effects that we product that capture some cross elasticities, but I think we don't have cross elasticity across instances, because I guess we just assumed there wouldn't be substitution, but there might be. Yeah, I guess we could try that. Cool. I know I'm hearing everyone's patience. So we'll wrap up quickly, as on the x axis, we have basically the number of observations. So these are the weak plan observations.
00:41:02.124 - 00:41:26.608, Speaker A: And the y axis, we have regret. And so this gray line is basically the estimated stored policy. Of course, they're following a static policy, so we expect it to get linear regret. It's not optimized with respect to this data. And then we see, like, kind of the other bandit algorithms compared to, you know, what we think is the optimal policy that are all learning. Right. And so, again, same kind of pattern.
00:41:26.608 - 00:42:01.272, Speaker A: Here we see about a 6% estimated revenue increase than not using cross store information. So if we just treated each store separately, then we would, you know, do worse, which, you know, you kind of might expect. And notice that this number is much smaller than the hospital example, I think, because this is a much less high dimensional problem. So I think this really underscores the need for transfer learning and kind of high dimensional problems, because that's really where we struggle to learn in these small data settings. In this problem, you know, less. So I can pretty much get away with learning independently if I needed to.
00:42:01.328 - 00:42:01.924, Speaker B: Right.
00:42:03.984 - 00:42:43.404, Speaker A: Cool. So, just to summarize, we're working on transfer learning effectively across simultaneous heterogeneous bandits. We impose an assumption of the sparse differences kind of motivated by real data sets on how models transport. And then we had to design a robust multitask estimator that basically is able to exploit this structure so we improve regret bounds in the context dimension d. Notably, it's exponential for any kind of data poor or more high dimensional instances. I think there's a lot of exciting work kind of in this area. We've done some work on combining short and long term outcomes in clinical trials or trying to learn across many Thompson sampling experiments.
00:42:43.404 - 00:42:49.164, Speaker A: So there's a lot of interesting, I think, exciting things to do here with that. I'll wrap up. Thank you.
00:43:01.954 - 00:43:05.706, Speaker B: What about learning across arms?
00:43:05.890 - 00:43:36.084, Speaker A: Yeah, yeah, we can totally do that. In fact, that is what we did for the COVID testing project. The data sets that we have right now are just not aligned with that. But I think you can impose the same assumption. Right? Like all arms kind of have a shared parameter. Like maybe if you're in like representation systems or something like that, you have observable attributes, and then maybe you add some instance specific in using credit bias that you try to learn. I think it would be mathematically equivalent.
00:43:38.544 - 00:43:43.804, Speaker B: What if you have instead of the linear estimates?
00:43:43.904 - 00:43:44.660, Speaker A: Yeah.
00:43:44.852 - 00:43:46.260, Speaker B: So how would this case.
00:43:46.372 - 00:44:23.054, Speaker A: Yeah, that's an excellent question. So we've done this for quadratic neural nets with a single layer. So very specific, because quadratic neural nets basically, you know, boil down to regression with some rotation symmetry. And so if you want to add the sparse deviation, that's fine. We're trying to understand how to do this for like relu activations. It's a little trickier because, you know, you have to design a specific algorithm, but I think even there it might work because you might want to like Zoom into the linear part of the reboot and then, you know, maybe you could do something similar. But the answer is, I don't.
00:44:29.154 - 00:44:31.106, Speaker B: What was exploration done here?
00:44:31.290 - 00:44:51.874, Speaker A: Exploration for this. So we're building off of the high dimensional bandit literature. So basically trade off between four sampling and having kind of an myopic sample estimator. We didn't do anything. Yes.
00:44:53.214 - 00:44:56.714, Speaker B: Related to Nicole's question, can we use this for data?
00:44:59.334 - 00:45:18.164, Speaker A: There's small change, like if there's like change point detection, but it's like small or sparse. Yeah, I think, I think you have a detection problem there, which we don't have. Right.
00:45:21.104 - 00:45:24.872, Speaker B: I'm just assuming that whatever change happens, it is sparse.
00:45:24.968 - 00:45:30.328, Speaker A: Right. Also, every time there is a change point, I'm only going to add a sparse additive parameter.
00:45:30.416 - 00:45:34.282, Speaker B: Maybe I don't even need to detect it. I just take small amount of data.
00:45:34.418 - 00:46:00.144, Speaker A: Right, right. That's really interesting. I have not thought about that. I guess if you told me when the changes would happen. I would just worry that by the time I detected there's a change using change point estimation, I already have collected enough data to do estimation. Or at least my experience with change point estimation has not been great. Like, I usually need a large horizon, whereas you find.
00:46:05.644 - 00:46:07.212, Speaker B: Decisions that it automatically.
00:46:07.308 - 00:46:14.148, Speaker A: I see. Yeah. That's cute. Yeah, thanks. I. Oh, yeah.
00:46:14.276 - 00:46:28.788, Speaker B: Okay. Actually, yes. The question is just like. Rather than assuming that, like, sort of, like, the difference between the tasks is sparse, you could model this as just saying the difference is captured by some function class which has, like, low complexity.
00:46:28.876 - 00:46:29.508, Speaker A: Right.
00:46:29.676 - 00:46:31.372, Speaker B: You thought about modeling it that way at all?
00:46:31.508 - 00:46:35.628, Speaker A: We did, and then we got stuck on this. That makes sense.
00:46:35.676 - 00:46:38.020, Speaker B: I mean, this is like, non trivial already, so.
00:46:38.132 - 00:47:00.268, Speaker A: No, I agree. I think getting this, like, shared parameter is, like, a little bit tricky because it becomes sort of like an outlier approval problem. But I think once you nail that, I can imagine that. Yeah. If you're willing to do any additive low complexity model, I think you would probably generalize, but I'm not sure because the way we're removing the corruptions is very specific to instructional.
00:47:00.316 - 00:47:01.620, Speaker B: Yeah, for sure. For sure.
00:47:01.812 - 00:47:05.572, Speaker A: So I think once you fix that, it will work. But if you have any ideas.
00:47:05.708 - 00:47:06.824, Speaker B: All right, sounds good.
00:47:10.364 - 00:47:21.704, Speaker A: Other questions? Thank you. It.
