00:00:00.480 - 00:00:11.254, Speaker A: Also, hi to our online participants. It's a pleasure to introduce Nika, who's a professor here, who's going to tell us about learning and incentives and how they interact. Thank you, Nika.
00:00:11.414 - 00:00:55.218, Speaker B: Thank you. Thanks for being here. I'm really glad that I'm here and I can see you guys in person, and I hope to get to see the people who are online at some point this semester or in near future as well. So, yeah, so the title of this talk is learning and incentives. And when I was thinking about so what am I going to talk about? Learning and incentives. I've had tutorials about this topic, but it's so broad, I thought maybe a better title for this specific talk I'm going to give today and tomorrow would be learning with strategic interactions. I mean, they're very related, but at least this allows me to tell you about specific types of strategic interactions that we're going to be dealing with today and tomorrow so that we can both remain grounded.
00:00:55.218 - 00:01:57.024, Speaker B: But also think about the applications of where these interactions actually come into play. And we're going to develop a principal way of talking about these interactions using tools from theory of machine learning and outwardly, economics. So, learning and learnability. If I go decades back, the question of what can be learned from data and limited observations has been something that really started the foundations of theory of machine learning as a whole field. We've been wondering for decades, what can be learned from data? And even how do we even talk about learnability? The usual goal is that we have some process that's spitting out some data. We have some limited observations of this, and then we want to use these limited observations to learn some concepts, some facts. These concepts and facts are really abstract, and theory of machine learning will tell us what facts and concepts can be learned from observations.
00:01:57.024 - 00:02:55.658, Speaker B: So this is where learning and learnability comes from. If I go back to the seminal work of Valiant, who was really talking about foundations of learning, you know, what can be learned, what types of concepts and facts can be learned from data. I was intrigued to find that the example that was given in that paper was of familiar objects, images of familiar objects such as tables, chairs. I can add other examples. Today we usually hear about cats and dogs and cars and ships. And I think it's actually really important to think about why such an example was chosen. If we go back, the most basic setting of machine learning, and especially theoretical models for machine learning, really involves these distributions that are about something stationary that's happening in the world, and you get to get some observations from it.
00:02:55.658 - 00:03:51.242, Speaker B: The world does not respond to you. The decisions you make as to whether or not an image includes a cat or a dog or a table or a chair has no bearing on that table or chair. They're not going to be offended, they're not going to change their appearance, they're not going to leave your data set because you didn't recognize them. And that's not what we are seeing today in the applications of learning. Today, we have all of these interactions that I was talking about because our learning algorithms are being applied for decision making, not just for recognizing patterns that may be in some images, especially patterns of objects, but about actually using the observations to make decisions. And these decisions are going to impact people or other entities who were involved. So what are examples of this? Even if I talk about image recognition, but now I'm talking about human faces, this is a completely different story.
00:03:51.242 - 00:04:35.484, Speaker B: People are going to be affected by this. If I talk about image recognition and I talk about whether or not my MacBook is going to let me access my computer by just recognizing my face, that's one that, again, there's an interaction I can put on glasses, take it off, put on a mask, I can try to change that. There are many other applications. I tried to put a couple of these here. The examples I chose here were to highlight three different types of interactions that we're going to see today. I'm going to be talking about strategic, generally speaking, and specifically, specifically speaking, some adversarial interactions. I'm then going to be talking about collaborative interactions, which is really going to be tomorrow.
00:04:35.484 - 00:06:13.826, Speaker B: But something to note about all of these is that they involve type of strategic manipulation that can really mess with our learning algorithms. And we're going to see this. So when we talk, actually. So let me just talk about some of the examples here. So, an example of ride sharing, Uber or Lyft, you as a driver have a lot of flexibility to strategically manipulate your availability, the location of your car, whether or not you're going to accept the ride, and all of these, there's ample evidence that people actually do these to change the price and the ride that they're going to get immediately after, because they know our algorithms are going to respond, our matching algorithms are going to respond to how they're going to be matched with writers. Similarly, content moderation, like YouTube, the algorithms that are trying to catch illegal data on YouTube, they need to be constantly updated because as soon as something about the patterns that they're recognizing gets noticed by people who want to actually post illegal content, they're going to change the illegal content, maybe add, you know, like some red box around it, just change it so that our algorithms are not going to be catching them. So those are examples of the YouTube content is an example of an adversarial interaction, the uber and Lyft is an example of, broadly speaking, strategic interaction, strategic interactions that are not necessarily malicious or adversarial, but opportunistic, or in many different places.
00:06:13.826 - 00:07:33.710, Speaker B: So some other examples are what you see, for example, in admission, lending, hiring, search, you're trying to perhaps improve your chances of getting through a screening to get interviewed. Then for your favorite job, you will go and decide how you're going to actually represent your experiences on your cv, or you might actually take specific courses or tutorials or certificates, so that not only hopefully you're actually improving your qualifications, but also you're sending a better signal about your qualifications. So these are, broadly speaking, strategic interactions that could be beneficial, but there are things that we definitely need to actually account for, because if we don't account for them, or if we think that they're just malicious, we are losing an opportunity to actually make changes. And the last part is going to be about collaborative interactions was probably the hardest to actually show in this slide. There's an emerging market for data and sharing of data. This is happening because a lot of the time, data is held by strategic stakeholders, let's say banks, hospitals, medical centers, who have parts of data. They want to learn concepts or facts that are related more broadly.
00:07:33.710 - 00:08:28.504, Speaker B: Like banks want to figure out if something is happening, if there is a fraud going on. And having access to other banks data is something that's going to really help them. Similarly, with medical centers or hospitals, a lot of genomic studies, they require so much data that's not found in one location. But we want to actually enable people, enable these centers to work on data and find important scientific findings that's going to help everyone, but still keep their data either safe, private, or also make sure that the ownership of data is left with them. So there are all these questions that we are going to be really thinking about today, on tomorrow. As I said, there are three categories that we are going to be focusing on. Adversarial, which comes first.
00:08:28.504 - 00:09:40.928, Speaker B: Then I'll talk about strategic, and then I'll talk about collaborative. And I've highlighted four questions that we are going to broadly ask. They sort of are scattered around the four parts of the tutorial, but I thought it's nevertheless helpful to just highlight them, even though they might not map to a specific part of the tutorial. So, as I said, we're going to really start understanding the learnability part. We're going to talk about the question of what concepts can be learned from data in presence of strategic behavior. And for that, we're going to talk about adversarial and strategic behavior and also try to understand how lessons that we've learned from decades of work in this area is going to address our needs for today and especially today's world and today's applications. That part is mostly statistical, but there's also an algorithmic aspect to this, which is how do you design algorithms for strategic and adversarial environments? Of course, there are questions about computational tractability that I will talk about very briefly only, but there are also questions about what are general principles about how to use and how not to use data when I know that there are these feedback loops.
00:09:40.928 - 00:10:53.094, Speaker B: In practice, we'll also talk about how to design collaborative environments to encourage learner participation. So here we want to incentivize and learning algorithms to participate in data sharing protocols, and we want to, at the same time, deliver solutions that are optimal both for individual learners and also for sort of, society as a whole. So that sort of every stakeholder and society are going to benefit from this and at a very high level. And the three parts of the adversarial, strategic and collaborative learning, there are mathematical tools that are connecting them. Most of these mathematical tools, I'm going to actually introduce them to you pretty early on, but I will highlight them again. How do these learning paradigms connect to one another as we continue with the tutorial? That's the plan. My hope is that today we'll talk about adversarial interactions, and we'll start talking about general strategic interactions that go beyond adversarial interactions.
00:10:53.094 - 00:11:24.964, Speaker B: This might be wishful thinking, so we'll see if we can hit the very first one here. I'll be very happy, but I don't want to rush us so we can sort of choose our own adventure and figure out what are the parts that we want to spend more time. And then tomorrow, I'm going to talk a lot more about of general strategic interactions and then collaborative interactions. This is Wednesday and Thursday, right? Yes. I guess I'm a day behind. Sorry. Yeah.
00:11:24.964 - 00:12:21.244, Speaker B: Any questions? I know I didn't tell you anything substantive enough to ask questions, but if you have questions nevertheless, go ahead. Okay, so here's a plan for the first part, the adversarial interactions part. I'm going to be talking about five small parts here, and let me apologize to those of you who are experts in learning theory. We're going to go through some basic concepts because I want everyone to be on the same page. But I still promise you that by the time we hit these two parts, I'm already going to give you results from the past twelve months. So there is something to look forward to, even if you know a lot of these topics. Okay, so we'll start with sort of the most basic bread and butter kind of learning theory.
00:12:21.244 - 00:13:16.662, Speaker B: And that is what's sometimes referred to as offline learning, agnostic learning. I like to call it stochastic for the purpose of this tutorial. But sort of what you want to think about in this setting is that the type of usage example you want to have in mind is the example of the table I was giving you is when you want to learn to detect some natural phenomena or some objects, trees, animals, things that essentially are not going to react to whatever you do. So you could assume very easily that there are distributions of these items that you're trying to understand. And the idea here is that in this world, data is assumed to be generated stochastically from an unknown but nevertheless fixed distribution. And then the goal of the learner is to learn a function from this data. And we say that the learner was successful if it can achieve good performance for measured on the same underlying distribution.
00:13:16.662 - 00:13:59.334, Speaker B: So this is not concerned with robustness, it's not concerned with strategicness. It's just about seeing data from some distribution. The formal setup of this in the stochastic setting is that we have t interactions between a learner and a world. And at each time step this world is going to generate instances x, t, yt from this distribution. For the sake of simplicity, we can assume that xt are images, yt is yes or no. So does this image have a tree in it? And then the learner has to pick a prediction rule. This prediction rule is shown by f of t.
00:13:59.334 - 00:14:42.874, Speaker B: And while the learner is given access to a base class class h, it doesn't actually have to pick anything from that class necessarily. It just has to come up with some prediction rule. And the learner gets to observe after every round what was the instance and how well it did. And if it made a mistake, it's bad, it doesn't want to make mistakes. So the way we measure the success of this algorithm is something that we are going to call average regret. If I call it regret, it's the same thing. And this is the average number of mistakes the algorithm is making over t time steps versus the average number of mistakes the best fixed classifier in our base set of h would have made.
00:14:42.874 - 00:16:01.024, Speaker B: And the idea is that you can learn if you get vanishing average regret as t goes to infinity, as t goes to infinity if this is zero. Essentially what you're saying is that on average, the algorithm did as best as it could have done in hindsight. So this is a very basic model of stochastic learning. Now, notice that the really important part of this model, in fact, I've chosen a model that's slightly weird but unusual, perhaps, is really about the emphasis on this world generating data IId, in fact, a more common way of viewing this in a slightly different model, but for our purposes is essentially the same, is the offline learning model that many of you are familiar with, and the offline learning model to really emphasize on the fact that there is no interaction, I'm just going to remove the fact that the algorithm has to make predictions successively at every round and just say that actually the world creates a sample set. The sample set is created IID from a distribution. Again, emphasis on IID. And after the learner observes everything in the sample set, it has to come up with that prediction.
00:16:01.024 - 00:17:10.419, Speaker B: I would still measure the success of this algorithm using something that is essentially the average regret here. Another way to say that is that the average regret is the, this is the performance of my predictor on the data, not just on the observations I received, but actually on the underlying distribution. And then I'm trying to compete with the best hypothesis, best classifier measured again on that unknown distribution. And I want this to be, again something that goes to zero as I increase t. Usually we think about these as sample complexity bounds in the sense that I say, well, if I want to get epsilon here, how many samples do I need to see? Something like one over epsilon squared times something that we are going to see. Or I could talk about the average regret in terms of the dependence of it on the number of observations I've had, which is t. So I just introduced you two models.
00:17:10.419 - 00:17:27.663, Speaker B: For intent and purposes, they are the same, although I will address the fact that they're not actually, but just assume that they're the same. They're questions. That's an h, right? Not an h. This is an h. Yes. Yeah, sorry, there was a mistake here. This should have been h.
00:17:27.663 - 00:18:33.962, Speaker B: Any other questions? Okay, so let me ask you as just, I want to get a sense of how much people have already seen. If I give you this type of setting, do you know what characterizes its learnability? Those of you who have seen this, could you please raise your hands? Okay, good. So about half people, this is half of the people. This is still some, there's some freshness to it, so I'm happy. Okay, so we're going to start with our very basic question of what is going to characterize what I have not called offline learnability. Well, the thing that characterizes this is a notion of complexity of your class of hypothesis that h, and it's called Vc dimension. So what is VC dimension? Pictorially, for me, the easiest perhaps way to do this is to put into a matrix, associate the rows with hypotheses that the learner has access to.
00:18:33.962 - 00:19:40.408, Speaker B: Those are the base class hypotheses on the columns. I put all the instances in my domain, and then these are the predictions that it's making. So h two is saying x two is minus one. And then I look at this table and I say, well, the Vc dimension of this hypothesis class is the largest d, where there is a sub matrix of D columns where if I only look at those D columns, I'm actually seeing every way of labeling those D columns, which is two to the d different rows. Okay, so each row is a different way of labeling basically d different instances. And Bc dimension, put another way, is the largest sample set that you can label every possible way, and that's Bc dimension. The nice thing about BC dimension is that it's a simple combinatorial object and it actually characterizes what is learnable in the worst case, that is.
00:19:40.408 - 00:20:34.634, Speaker B: So for any class of hypothesis h, the optimal sample complexity, optimal than I think about sort of the worst case optimal over all possible unknown distributions, is the one that says it's, oops, sorry. It's about, there's a tilde, some logarithmic terms, it's about Vc dimension of that class over epsilon squared. Or I could have thought about the average regret. Also, I need a theta tilde. Theta here is about Vc dimension over tilde. So this here BC dimension is appearing both as upper and lower bound for telling us how much data we need or what regret we can expect in the worst case. Any questions about this? Let me give you an example of Vc dimension.
00:20:34.634 - 00:21:53.744, Speaker B: It's a very simple example, but I think it's nevertheless interesting. So the simplest example of Vc dimension is typically thresholds on a line. So what I'm thinking about is that my hypothesis class is just these thresholds, that they're essentially zero, zero, and then they switch to 1111. And such a class, I ask, what is SVC? Dimension, it's not hard to see that it's just one, because, well, it's very easy that I can get just a single point and I can label it two different ways by just having two different thresholds. But no matter what two points I pick, there is no way that I can produce a plus on the smaller one and minus on the larger one. So this here, the fact that VC dimension is one is telling us that if I had a distribution, I can learn this concept class really easily, because it's just the dimensionality of it is essentially one. Okay, so now that we have seen an example of VC dimension at a high level, why VC dimension is the right thing here, and I'm not going to give you a proof of this, you can pick your favorite learning theory book and read it, but I will tell you a high level of why this is expected.
00:21:53.744 - 00:22:31.708, Speaker B: So you should have expected VC dimension to play a role as a lower bound on the number of samples you needed. Let's talk about samples. I think it's a little bit easier. Why? Let's say that I only observed d minus one number of samples, and because I only observed this, and now I'm given a new point. This new point, I have no idea. Like any hypothesis that I have fixed its labels, still, half of them can say plus one year, half of them can say minus one. I could have explained this as a plus or a minus.
00:22:31.708 - 00:23:43.908, Speaker B: Regardless, in some sense, the minimum number of things I need to see before I can actually generalize from data is the number of things that I can fully explain no matter what. Because once I have seen this, now I'm creating some bias towards actually hypotheses that match these. But this is the underlying reason as to why Vc dimension plays a lower bound role in sample complexity. The more interesting part is why does it play an upper bound? And there sort of another way to think about this is that if I had a finite class of hypothesis I was trying to work with, then I could use very elementary probability statistics and use concentration bounds and union bounds and get something that looks like this. What is this saying? It's essentially saying that forget about that there is finite d many h. Just say that there was a single hypothesis. It's just that I'm seeing a coin and I know that I want to know what is the bias of this coin? Is it half? Is it 0.7?
00:23:43.908 - 00:24:41.424, Speaker B: What is it? So to understand that single coin, I would have needed these many samples. This is just something like a gaussian tear. Now that I have many different hypotheses for each of them. I want to measure this, and union bound is going to tell me that my estimation, the probability of sort of making a bad judgment, is going to blow up by h. So that's for a finite class. What least dimension essentially does is that it says that actually it's not just about finite classes, but I can try to talk about how many labelings I can produce on my sample set by itself or something that looks like my sample set. And if that size is very small, then I can replace this with essentially that effective size of those labelings, the effective number of those labelings, rather than looking over all my hypothesis costs.
00:24:41.424 - 00:25:43.714, Speaker B: And it turns out that VC dimension is exactly what is going to capture the effective sides of a hypothesis class, because it's talking about the power of a hypothesis class producing all possible labelings on a sample set. So this is at a high level why BC dimension works here. Okay, are there any questions about this? We're going to build upon this. We're going to make this a little bit more interesting. Okay, so next, I'm going to work my way towards something that's referred to as online learning. And here the example of usage that you want to have in mind is sort of that adversarial behavior. So you could think about you're trying to control the content quality of YouTube videos, and you're facing these adversarial manipulations for future instances that are essentially going to react to what you have done and you need to at the same time make decisions.
00:25:43.714 - 00:26:29.344, Speaker B: You get a new video, am I going to accept this or am I going to reject it right now? So that's an example of this, and it's also used in learning, in games. We've seen this course as a tutorial, Ebba's tutorial as well. You've seen a lot of this before. The idea is that because there's an adversary in the loop who is reacting to you, there is no distribution anymore. It's just that observations are evolving in unpredictable and adversarial ways, and we are successful. If I look back at my data on that adversarial generated data, and I can still say something positive I did on it. So this is very robust to any adversarial now reactions, as opposed to the previous model we were working with.
00:26:29.344 - 00:27:10.044, Speaker B: So if I start with the stochastic setting of the previous setup that I was talking about, let me tell you how you can change it to make it online. So the first thing is you remove the distribution. The second thing is that you replace the world with an adversary. So now this adversary is picking X TyT, and it's picking it knowing everything. So the history of the place so far, what algorithm you have, it's quite powerful. But the measure of what we use to actually measure success, average, regret, all of that, they still remain exactly the same. So it's just that the world change and we remain the same.
00:27:10.044 - 00:27:54.874, Speaker B: So this is a model of online learning. It's referred to as full information. I'm not going to use these keywords much today, but for those of you who are familiar with it, what that means is that the algorithm actually sees the instance after each prediction. So it could compute these hypotheticals like, oh, what if instead of the current classifier, I use this other classifier so it can sort of know how well every other classifier would have done as well. Can I ask a question? In the YouTube saying the ground truth keeps changing? Right. The ground truth keeps changing in terms of what? In terms of things that used to be pornography in the past might not be pornography in the future. So that's a good question.
00:27:54.874 - 00:28:14.394, Speaker B: But that's not. You're assuming it's always the same. I'm assuming that ground truth is defined in the future. So it is the part of the model. Yeah. So if you look here, this min h is saying that I'm trying. So in fact, I'm not even defined, I'm not committing to a ground truth.
00:28:14.394 - 00:28:41.612, Speaker B: The ground fruit is defined as the best thing that in few years I would have looked back. So the label is time t. Are you trying to guess the label at time t? I am trying to guess the label at time t, but. So let's look at this. So the ground truth again. So I think there are two different ways of sort of, I think, I thought you were talking about benchmark. So the benchmark is defined in the future, the ground truth, white teas, they don't change.
00:28:41.612 - 00:29:19.596, Speaker B: And I think maybe pornography is a good example of that. But I'm thinking more like maybe copyright, maybe children's like inappropriate for children's type content. Like these are things that the instance itself doesn't change. The appropriateness for children changes, I think hasn't changed a lot. But there are situations where definitely there are situations that they do change. I think that's interesting. You know what, when we come back to strategic interaction, we should talk more about that because there's a slightly different concept of regret that talks about certain types of changes.
00:29:19.596 - 00:29:40.784, Speaker B: I don't think it will exactly capture what you have online. But nevertheless, it comes with something like that. Another way to think about this is that at the end of the day, I'm still measuring what my algorithm did versus what, given the same way that I labeled, the true labels I generated would have been. So that's the notion of this kind of regret.
00:29:42.884 - 00:29:49.034, Speaker C: Probably you referred at the beginning. Is the horizon well known from the beginning?
00:29:49.574 - 00:29:50.726, Speaker B: That's a good question.
00:29:50.870 - 00:29:53.606, Speaker C: I got that from the question, yeah.
00:29:53.790 - 00:30:14.408, Speaker B: So the simple answer is, whatever you want, if you want it to be known, it can be known. If you don't want it to be known, it's not known. Essentially, there are algorithms that you can change any algorithm if you're willing to play a little bit so that it works with on the horizon. I'm not going to talk about that.
00:30:14.606 - 00:30:29.864, Speaker C: My apologies. I agree you can do doubling tricks, etcetera. But my question is, if the horizon is known for the adversary in order to submit the YT from the beginning or not.
00:30:31.644 - 00:30:39.924, Speaker B: So adversary can be adapted. Okay. So even if it knows the horizon, it doesn't mean that it has created the sequence out of time.
00:30:39.964 - 00:30:40.914, Speaker C: Okay, fine.
00:30:41.084 - 00:31:31.864, Speaker B: And the adaptivity is actually important. Okay, any other questions? Cool. So let me give you an online learning example. So, the way to think about this picking is, you know, we keep the distribution fixed, and that's selectively sampling from this distribution, or is the distribution changed? So there's two ways to think about this. I can always add a distribution, but in a very naive way, by saying that an adversary at every round takes a new distribution, having known the whole history. But that distribution, I'm allowing it to be focused exactly on one point, so that picking, it doesn't matter, that it's a distribution. We'll come back to this in the second half of the talk today.
00:31:31.864 - 00:31:58.214, Speaker B: But another way is just like give full flexibility to adversary. It's a very powerful adversary. There is no distribution at every round. It looks back exactly. It has kept track of the algorithm. What is it done? And it says, today, this is the instance I'm going to produce. Okay, so it has full power of doing, picking any instance in the domain and assigning it any y that it wants.
00:31:58.214 - 00:32:22.530, Speaker B: It can generate any pair that he wants. What does it mean in the domain? In this context, pick anything in the domain. So you have to usually commit to something like a representation of the problem. So the domain x could be all images, or it could be all real valued, like anything in RN. That's the domain. It's just that it has to be. To define these settings.
00:32:22.530 - 00:33:07.184, Speaker B: We need to know what's the domain? Because a predictor is a function from the domain, two labels. So we kind of need to commit to that domain before we start talking about predictors. Okay, any other questions? Yeah, just out of curiosity, why is the learners. Good? So, a nice way to think about this is that I'm giving a lot of power to the adversary. Okay, so let me first say it means two things. It could mean that the function itself is random, or it could mean that it randomly picks a function. Okay, let's think about the randomly picks a function.
00:33:07.184 - 00:33:46.404, Speaker B: So, because I allow the adversary to actually know the algorithm plus the history, the adversary can actually compute the algorithm in its head. So if the algorithm were to be deterministic, the adversary would know what the next thing that the learner produces is and will generate something that's bad for it. So kind of randomness here is actually a really important power that the learner has to be able to use so that it remains unpredictable, so that even when the adversary sort of plugs in the history to the algorithm, it doesn't know exactly, like, there's no deterministic way of abusing its power. Other questions?
00:33:48.364 - 00:34:04.668, Speaker A: So we have one from the online audience. So the question is, are there any applications for this model? The adversary chooses the data for the learning algorithm, and, you know. Yeah, I mean, yeah.
00:34:04.716 - 00:34:52.534, Speaker B: So, actually, I will partially refer you to the previous tutorials as well. Unless you are one of the people who gave a tutorial, then in that case, I do not. So, yes. So, first of all, there are a bunch of applications of this, theoretical applications, actually, this type of online learning. And assuming an adaptive adversary is an important thing in game theory. Some of the stuff that EVa talked about on Monday, I guess, would have required you to have a regret algorithm that has regret guarantees against an adaptive adversary. So when we think about this, applications, and game theory in particular, sort of adaptivity is an important aspect, although we can do better than even worst case adversaries, other places that this is very useful.
00:34:52.534 - 00:36:01.880, Speaker B: This has been. So it's even right now versions of this, like, sort of this on maybe steroid like kind of saying that, oh, you cannot have some observations, the world is evolving even less predictably that you saw it, and you're trying to compete with something that's even harder to compete with. These are definitely sort of the basis of reinforcement learning and a bunch of other sort of approaches for tweaking algorithms in online ways. So that you're providing both, like, for content quality, for matching, for a lot of different things. So this is the high level answer to that. But if you're thinking about the theory of it, I actually do find the game theory and sort of adaptivity of adversary as a nice example to have in mind, where the environment is actually is responding to, and you don't want to know exactly how it's responding, and you're willing to assume that it's an adaptive adversary. Okay, so now that we know what this model is, what it's used.
00:36:01.880 - 00:36:38.994, Speaker B: Used for. Yes. I was going to give you an online learning example, and I'm going to use the same hypothesis class that I showed you has a bc dimensional one, so it was really easy to learn. For an offline learner, we're going to see that such a simple class of hypothesis is actually impossible to learn in an online setting. What does this look like? So, I still have these thresholds on a line. The algorithm has to predict labels of an adaptively and adversarially selected point per step. So what the adversary will do is to start with any point.
00:36:38.994 - 00:37:08.594, Speaker B: It's easy to just say it starts with half and then asks you to label it. No matter how you label it, the adversary can tell you that actually you were wrong, and the true label was this other label, and then it can continue. It's a very kind of an inductive structure. They'll ask you about a quarter. Again, no matter what you do, it says that you are wrong. Da da da da da. Whenever it wants to stop, or even if it doesn't stop, it can continue this process until the end of time.
00:37:08.594 - 00:37:41.064, Speaker B: The adversary can still give one guarantee for itself, which is that the adversary was consistent. He might have, in his mind, have had some kind of a consistent threshold that every gave you actually matches this. But you. Oops. But you were. But you actually did pretty poorly with respect to this, and it's important to know that. Why did you do poorly? Because at every step, I made sure that you made a mistake, so you made t mistakes, and the best hypothesis would have made zero mistakes.
00:37:41.064 - 00:38:12.934, Speaker B: And it's important to realize that it's not just this path that you went through, could have walked through any path, so any way that you would have made a prediction, there would have been a way for the adversary to continue. Yes. Just a small part is doing the label before you. So I gave you a model, and immediately I didn't follow it. So that was a very good catch. Lastly, why would. I told you? It's essentially the same thing.
00:38:12.934 - 00:38:40.344, Speaker B: Another way we could think about it is the adversary would have picked one of these passage at Rattlesnake. Okay. And because it's picket at random now, it's consistent. The X and Y's are fixed, even though you don't know it, and now you don't make a deterministic mistake, you still make a mistake with probability. Good. So it's still, nevertheless, it's nice to keep in mind this kind of is the x, iv and x. And then I'll tell you the y.
00:38:40.344 - 00:39:47.334, Speaker B: The moral of this story is that in either of these models, you're making either t over two or t mistakes, and there would have been a perfect thing with no mistakes. So the average regret is a constant. It's not going to go to zero and I'm in trouble. So at the very least, what this example should have taught us is that Vc dimension is not the thing that's going to characterize online learn, because I had something of Bc dimension one and I could not learn it at all. So here really, the fact that finite mitigation is not sufficient for online vulnerability is that Mitsra mentioned is really focusing on a labeling a set, whereas I was talking about trees. And to really be able to talk about how expressive your hypothesis class is, you need to also talk about trees or all these sequences. And here is where the definition of little stone trees or shattering trees come in.
00:39:47.334 - 00:40:25.544, Speaker B: The way that they're defined is that I just have a full binary tree of some depth. The nodes of it, especially the interior nodes, are x's. These are the instances that you're supposed to label, and I refer to them actually by their position here. This is x plus because I'm saying that going right is a plus and going left is a minus. This is x plus plus because I started from the root and I went pluses twice. This is x minus plus because of that. And now I can refer to any path by these y's.
00:40:25.544 - 00:41:11.434, Speaker B: This is the y of the empty set being plus, the y of the next one being minus. Okay, now this is a little stone tree. And for every path that I define this way, the x's are labeled as their corresponding y's, the y that actually came after them on this path for the single h here. So every path has a consistent h, and h is consistent in the sense that every x on that path is x empty set and y empty set. Follow the predictions of that. That hypothesis h the same x plus and y plus followed that the same age for each path. That's a little stone tree.
00:41:11.434 - 00:41:39.364, Speaker B: And now the depth that or the height of the largest little stone tree is what's known as the little stone dimension. This is, this could be a little bit hard to grasp. So if you have any questions, it's a good time to ask. You're much more expert the first time I saw this. I know. Okay, do you have a question?
00:41:39.784 - 00:41:48.204, Speaker C: It's more the feeling that you don't have the correct question to ask. What I'm trying to understand is.
00:41:50.694 - 00:41:51.006, Speaker B: Do.
00:41:51.030 - 00:41:53.114, Speaker C: I have the excise yes or no?
00:41:55.334 - 00:41:56.510, Speaker B: So this is about excise.
00:41:56.542 - 00:42:00.194, Speaker C: I'm trying to understand optimization problem. I mean, you're not there.
00:42:01.334 - 00:42:03.606, Speaker B: You're not there. The optimization problem of making.
00:42:03.670 - 00:42:07.834, Speaker C: Yeah, yeah. What are they, what is the rule? What I'm trying to optimize.
00:42:08.134 - 00:42:39.168, Speaker B: So you want to make the largest of these trees, because if you remember, I was talking about vc dimension as the largest thing, you can sort of label everywhere. So you want the largest tree that can have this property. Okay. It's very easy to have a tree of size one. You can just take one and then label it with the right way. But as soon as you start branching, there are two to the d kind of paths that you need to keep consistent. Okay, I will give you an example of this.
00:42:39.168 - 00:43:16.004, Speaker B: Just because I know this is a little bit hard to ask. Let's look at this tree. This is a little stone dimension. Little stone tree. If you look into the mirror, just because the pluses and minuses are exactly reversed. So this is, the left is becoming minus one, the right is becoming plus one. Because at the end of the day, I always, no matter what path I took, I had this hypothesis that said, hey, this guy was consistent with this, this guy was consistent with this, and this guy was consistent with whichever direction I went.
00:43:16.004 - 00:43:32.844, Speaker B: So this is an example of an infinite leather stone tree. Now let me give you an example where you cannot have an infinite lucian truth. Do you have a question about this? I think maybe one more example is going to help.
00:43:33.704 - 00:43:46.224, Speaker C: Just to be sure they know this. The alpha, the number one half, one four, et cetera, is the threshold.
00:43:49.004 - 00:44:07.824, Speaker B: So this is the x's. So remember, the nodes of a tree have to have instances. Those are the x's. The edges are the labels. The edges play two roles. They're both the labels for that point. And they actually define a path.
00:44:07.824 - 00:44:40.814, Speaker B: So the label that you're defining and the path have to be sort of consistent. That's why. Okay, let me give you one more, actually, I have two more examples. Let me give you an example of a small, little stone dimension tree that you cannot grow any further. So think about a class h. This class has this property that is like plus one on just d place d. Of the instances, it can choose any d, it has to be zero or negative one everywhere else.
00:44:40.814 - 00:45:25.116, Speaker B: Now my claim is that the little sum dimension of this class, the biggest tree you can make, biggest full binary tree you can make, has dimension d. Why? Because once I start from the root, there is at most d things that I can d times. I can go plus one. If I ever repeat a point, I'm already unfortunately done because I cannot label it positive ones and negative ones, so I cannot repeat. And if I'm not repeating, there is at most d things that could be positive. So I can go to the right only d times. So that's one way of knowing that this little slow dimension tree is going to be at most height d.
00:45:25.116 - 00:46:15.432, Speaker B: Have a question? Good. If I discretize my previous example, do you want to tell me what would have been the d number? It would have been the log of the size of, right? Yeah. So if I said that actually I'm looking at only integers from zero to two to the d, then I would have been just d. That little son dimension can never be greater than log of. I will discuss if you can do Hassan. Okay, one other example of a large little sun dimension, and this is going to come back after the break, so it's good to try to understand it. This actually has this property of being on the natural numbers.
00:46:15.432 - 00:46:44.254, Speaker B: I kind of like that. But it's slightly different in the sense that it has two ends to the threshold. It's like an interval, but not any interval. It's like intervals between any integer and twice it. And my claim is that this still has a little. Some dimension that's infinite. The reason is that if it was fine, I'm essentially going to rule out its finiteness by saying that it has to be bigger than any dimension.
00:46:44.254 - 00:47:27.438, Speaker B: As soon as you fix the d, I can choose sort of the farthest into the line, like between two to the d and two to the d plus one. And if I just focus on what's happening on two to the d and two to the d plus one. So I actually have all possible thresholds now. Like, let's ignore what's happening outside, okay? But on the inside, I have all thresholds between two to the D and two to the d plus one. Now, it's natural numbers, but as you said, if I have all the thresholds, then at very least my bc dimension. Sorry, my little sum dimension is going to be d. Here I can repeat this for any large d.
00:47:27.438 - 00:48:03.506, Speaker B: I'll just pick something farther, closer to infinity. Okay. And that's why the little sun dimension of this class is alternative. Any questions about this? Let me just highlight something, and I'm going to come back, but I think it's worth highlighting here. I gave you two different ways of creating an infinite little sun dementia. One is here that technically speaking, there's no single tree that you can grow it infinitely long. But for any depth, I tell you, you can make a fresh tree of that depth.
00:48:03.506 - 00:48:45.508, Speaker B: Row me. Because if you want to, like at some point, you're going to hit hit a block, you cannot go deeper. But the one I gave you in the last slide, there's a single tree that you can grow infinitely long. This turns out to be an important distinction, so I'll come back to it, but have have it in mind because I think it's interesting. Okay, so now what characterizes online learnability happens to be this little sum dimension. Again, both upper and lower bound. That's why we call it a characterization and sort of why should it? Lower bound is essentially the binary search trick I did with you.
00:48:45.508 - 00:49:24.412, Speaker B: There was nothing special about thresholds. As soon as you have this kind of a tree, you could let the adversary pick x, y uniformly. It's a path uniformly at random. You have the same dilemma. No matter what, you're going to make a mistake with some probability a constant, and there's going to be a perfect classifier. This is really good if you want to think about infinite little sum, because you will never finish this tree. But actually, if you're thinking about finite, the trick here is that instead of because this tree never grows necessarily after lunch, after a little, some dimension depth.
00:49:24.412 - 00:50:25.124, Speaker B: So you keep repeating the first thing many times and then repeat the second thing many times, and you make sure that you label them at random, so there is no information from one noun to the next. So this is kind of that at a high level, how you would make sure that the littlestone of little stone dimension will enter your regret this way. Now, why upper bound? Really intriguing question. Partly because little stone trees are, by construction inductive, and because of that, they actually lead on inductive algorithm. The easy case is, say, I really did keep a perfect classifier. So I just want to know the number of mistakes you are making. And one way to think about this is that, well, the algorithm that should really use the mention is the one that, of course, at every step, it rules out any hypothesis that has ever made a mistake.
00:50:25.124 - 00:51:29.144, Speaker B: There's no point keeping them around. They're not going to be the real answer. And then for everything else, I will take a look at which way if I predict, the majority class of my prediction is going to have the most power to, or complexity in its predictions. This is kind of abstract. This is kind of like thinking about just majority vote, except not everyone, not everybody is getting a vote of one. Instead, what I want is to look and say that if I were to say the label is plus one, or if I were to say the label is minus one, then I look at the set of active hypothesis. Had I made a mistake, then I want the little stone dimension of the remaining set to be as small as possible, because every time I made a mistake, I want to have created a significantly easier hypothesis class to be working with.
00:51:29.144 - 00:52:14.770, Speaker B: Which is kind of the dual of the saying that I want this situation, that if I was correct, to be as complex as possible. So the way you do this is by actually measuring the littlest dimension of the set of your hypotheses that are still valid and would remain valid given your next prediction. This is called a simple, sorry, standard optimal algorithm. And actually, I don't know if it's standard or simple, I have forgotten this, but it says this perfect. This is, this is an existence proof. This is not a real algorithm. And so why is this hard? Actually, I might have a slide about this.
00:52:14.770 - 00:52:58.112, Speaker B: No, I don't. This is essentially, actually, this is like Bellman equation, okay? And we already know that those things are difficult to deal with, so that's why this is by itself more like a scientific type of algorithm. But there are other algorithms that are really interesting. Okay, so this wraps us up. We have five minutes, and since I'm short on time for today's material, I'm going to just give you the introduction to the next part, but I'll definitely make sure that you'll get time to have your break. So the next question that you're going to be looking at is not about learnability at all. It's actually about the Minmax theorem.
00:52:58.112 - 00:53:58.114, Speaker B: And again, we've seen a lot from specialty courses, tutorial as well about these topics, but kind of the usage example here is any zero sum game that you have in mind, like most board or card games between two people or competitions between two firms. And we're going to talk about sort of equilibria between two people. So what's the setting here? You have two players. Again, I find that the game matrix will continue to give us a good picture, and they each have set of actions, and they have utility functions that are sort of in this matrix, u one and u two of xY. And we are going to focus on zero sum games only for this part of the talk, not for the future parts. And sort of the idea here is that zero sum games means that the sum of the two utilities to be zero. So my gain is your loss.
00:53:58.114 - 00:54:55.066, Speaker B: On the other way around, I'm going to talk about loss for player one as the minus of its utility. I try to remain consistent, but I'm sure that at some point I might not use, whenever I use like the upper, like the capitals of u, the uppercase, it essentially means that it's the expectation of the same quantity. Okay, so we've seen many different solution concepts. We talked about min max and max min values before. But just to go over them very quickly, this is a setting that you think about each player taking a distribution over its actions, over as pure strategies or distribution. Essentially, the role player is taking a distribution over the rows, the column player is taking a distribution over the columns. And now we're asking, hey, would you have rather gone first or second? And that's a min max versus max min.
00:54:55.066 - 00:56:01.996, Speaker B: So min max is player one goes first, gets a p, and then pL2 is essentially maximizing its q, and max min is exactly the opposite. There's also the notion of Nash equilibrium, where it's very similar. The idea is that you pick the p and q, and neither of you can sort of unilaterally just change your own part of the strategy and improve your utility or loss. And the nice thing about zero sum games is that we know there is a celebrated theorem that says that for zero sum games, these are all essentially the same. So the min max value, the maximum value, and the payoffs of the mixed Nash equilibria for the first player are essentially the same under some mild assumptions. Some of those assumptions, they talk about x and y being compact. Some of them talk about the set of distributions over x and y being compact.
00:56:01.996 - 00:56:54.092, Speaker B: This doesn't come for free, especially it does not come for free when your x and y, or the game that we were talking about is now infinitely size. So if I talk about infinite matrices, game matrices, whether or not min max theorem holds is the question. And that's what you're going to see. So this is almost equivalent of our learnability question. For learnability, we were asking, I have these infinite hypothesis classes, am I going to be able to learn them? And now for zero sum games, we're going to say that I have infinite dimensional gain. Sorry, infinite strategy spaces. Am I going to be able to guarantee that min Max theorem holds? And that's what we are going to start talking about in the second part.
00:56:54.092 - 00:56:59.744, Speaker B: So this is a time for break, I assume. Okay, but I'm here if you have any questions.
00:57:06.344 - 00:57:28.438, Speaker A: So let's start with one question that's on the queue here from the online participants. So, the lower bound you gave for littlesome dimension, the error to the bound on littlesome dimension, applies to oblivious adversaries, right? So the set of learnable classes are the same, whether the adversaries oblivious or adaptive, it's the same.
00:57:28.456 - 00:58:24.168, Speaker B: Okay, so maybe actually even a better question, the better answer is that for min max type, regret guarantees. And I know I just talked about min max in terms of games, it's the same thing. So, as in the best regret guarantee that you can give that whole sort of worst case, oblivious and adaptive adversaries are more or less equivalent. Maybe not even say more or less. They're essentially equivalent. I want to emphasize on adaptivity, because in the next part of the talk, I will start talking about a slightly different model, where adaptivity is a significant, both challenging aspect and interesting aspect of the problem. But otherwise, so far, obliviousness and adaptivity are essentially like, they wouldn't have changed any of the things I talked about.
00:58:24.168 - 00:58:33.204, Speaker B: And you can see I was talking about the adversary essentially picking a path, which is essentially like being oblivious, like ignoring the algorithm's choices.
00:58:35.584 - 00:59:00.460, Speaker A: Are there any other questions? So I guess one comments, last remark. Amin's also related to Amin's comment. So the optimal algorithm is not a proper one. Right.
00:59:00.612 - 00:59:05.868, Speaker B: That's a good question. I will talk about this next time. Next in half an hour.
00:59:05.916 - 00:59:07.504, Speaker A: All right, sounds good.
00:59:11.564 - 00:59:12.052, Speaker B: Okay.
00:59:12.108 - 00:59:13.860, Speaker A: All right, let's go get some coffee.
00:59:13.932 - 00:59:15.854, Speaker B: Okay. See you in half an hour. It.
