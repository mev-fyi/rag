00:00:00.040 - 00:00:20.314, Speaker A: Welcome to day five of the Interactive learning workshop. And the last day, sadly. And our first speaker today is Brian Stasolati from Yale, who's been doing all sorts of, like, fascinating stuff involving human robot interaction.
00:00:21.574 - 00:01:12.164, Speaker B: Great, thanks so much. How's that? That's better. Okay, I want to thank everybody. This is a fantastic opportunity. The communities that I circulate in, we often value the solutions that we bring to particular application problems. So I'm going to give what would be considered a terrible robotics talk in that I'm going to give you lots of problems and relatively few solutions, but I think that'll be the best way for us to spark some interesting conversations and some good questions. Now, most of the learning that happens in robotics is based around our traditional idea of what robots do, and that's based on factory automation.
00:01:12.164 - 00:02:59.980, Speaker B: The idea is that we train robots to do specific tasks, to do them with high accuracy, to do them with a large amount of accuracy, a large amount of uptime, repetitive tasks, over and over and over again. And we typically think about this when we're working in the research lab as some kind of sequential manipulation task. That is, if we're assembling, for example, a piece of furniture, that we observe humans dealing with this task, solving this task, we characterize that problem as a sequence of world states, and we then monitor the transitions between these world states with the hope of then having a robot being able to replicate that process by moving from one state to the other. So we build representations that are these nice, easy to derive representations from this kind of a linearized example. And that usually ends up with some kind of semi Markov decision process or some other flat state based representation, where we observe multiple iterations of a person putting together this chair, and we end up with this giant mess of states. And part of the problem here is, in order to deal with the complexities of the world, our states have to be very fine and precisely defined, which means we have a very large dimensional space to deal with, and that means we have lots and lots of transitions as we go through even relatively simple tasks. And our goal in robotics is usually to look at this mess and somehow find an execution policy for the robot.
00:02:59.980 - 00:04:19.354, Speaker B: That is, what should the robot do when it's in any one of these given states, in order to complete the task at hand? That's been our traditional method, and that's what we see in most of robot learning at this point. And we get better and better representations, not just smdps, and we get better and better mechanisms for reducing the dimensionality and we get better and better mechanisms for finding those execution policies. But the challenge that I want to bring to you today is that as we start reconsidering what robots are doing as an application domain, the kinds of problems that we are looking to solve are changing as well. So in manufacturing, as an example, it used to be that we had these wonderful assembly lines that were all robots, and we could precisely define everything in the world and how things transition from one pace to another. Now we're starting to see a desire to move towards what's called collaborative manufacturing, where the idea is that robots are good at some things and they're terrible at other things. They're great at doing very high precision tasks, but they usually have terrible dexterity. They're not good at handling soft or foldable materials.
00:04:19.354 - 00:05:30.814, Speaker B: And so what would happen if we put people and robots together in the same space? Now, there's certainly safety concerns with this, and I'm not going to touch on those today, but those are starting to become solvable problems. So what happens when we put this unpredictable human into these wonderfully well constrained robotic domains? And that changes a lot of our basic assumptions. So what we would like to be able to do in the ideal world is we'd like to keep many of the pillars that we've already built and already invested a lot of time and research energy into. We'd like to still be able to observe these linearized sequences of tasks, these manipulation tasks, and learn from them. We'd still like to be able to use many of the existing methods that we have for learning from demonstration, and many of the same models that we've been been building. And we still need to focus on execution policies. We still need to be able to focus on what it is that the robot's capable of doing, given some state around it.
00:05:30.814 - 00:06:10.484, Speaker B: But in order to adapt to these interactive settings, we're going to have to change some of our fundamental assumptions. And I'm going to try to make three points today, and I'm a firm believer in always telling you what they are before I make them. So here they are. Here's the whole point of the talk. Number one, we need to move away from these flat representations. They're just not adequate for dealing with the complexities that arise when you put that unpredictable human in an environment with the physical robot. Second, we need to move away from divide and conquer based planning mechanisms.
00:06:10.484 - 00:07:20.614, Speaker B: I'll tell you what that means in a few minutes. And finally, we need to consider collaboration in a very broad sense. And that means changing what is the desired goal and the desired outcome of these systems, and what we optimize our learning in order to be able to achieve. Okay, so let's take these one at a time. Why do we want to move away from flat representations? Well, as we start to interact with these higher and higher dimensional spaces, as we start to worry more about safety and concerns that operate over flexible and deformable materials, our search space becomes really terrible. So one of the benefits of moving away from this flat structure is that if we can somehow represent this and break that space up, if we can somehow find a way of reducing the dimensionality, that's going to be a giant benefit for us. So hierarchical structures, and I'll use that in a generic sense for a few minutes, hierarchical structures allow us to break down the dimensionality of the problem and deal with contained subspaces that we can hopefully work well within.
00:07:20.614 - 00:08:54.584, Speaker B: The second big thing that we get from this, though, is that we can start talking about multiagent task allocation with a linearized sequence of observations. It's very hard to take that sequence and convert it into some parallelizable execution plan between a robot and a human, or even multiple robots and multiple people. That is, with a more structured approach, with some type of hierarchical, some type of structured representation, we can start to identify portions of the task that are performable, perhaps only by one agent. Only the human can fold this piece of material where only the human is dexterous enough to get this tiny screw into place, and other things might only be performable by the robot. We only trust the robot to do this high temperature arc welding, and we can start looking at the structure of this task and thinking about it not just as a single execution thread, but as something that we can optimize the performance by dividing it up. So the third benefit that we really gain, and this is perhaps the most important that we actually, right now, have the least to say about, is that we gain some transparency with the human that we're interacting with. It's extremely difficult to take these large, flat, state based representations and even describe what's happening at that moment in time.
00:08:54.584 - 00:10:18.794, Speaker B: Knowing that you're in state 1732 doesn't give any information to your human partner. However, knowing that you're assembling the back leg of the chair, that's a much more useful piece of information. So it allows us to build structure within these representations that are communicative with the individual. Now, of course, the disadvantage of all of this is where do we get this hierarchy from? We have all of these techniques and methods that are designed to operate on these linearized sequences of observations. If I'm swapping out the ram in my laptop, I observe three different people doing it, or two different people doing it, and they follow a sequence of steps, and maybe they don't follow the exact same sequence of steps, but all I get are these linearized sequences of observations. How do I take that and extract some hierarchical representation? Now, there are good techniques out there for doing it. I'm going to show you one really simple technique that I have to credit Brad Hayes, who is my former grad student, with identifying, which is that we're going to treat this as a discovery problem and we're going to take those traditional models that we use and just consider the conjugate of them.
00:10:18.794 - 00:11:31.154, Speaker B: So, traditionally, let's say we're assembling that little chair that you saw a video of a few minutes ago. We would treat states of the world, real physical states, object locations, tool locations, assemblies as nodes within this graph, and actions as transitions between one state and another. That is, an action allows us to translate from one world state to another. What we're going to do is to swap that and consider what we've been calling a conjugate of this graph, which is that we're going to consider actions to be the nodes within this representation, and transitions will be the world states that that are required in order to enable that action. Now, the first thing to notice is that we can automatically derive this from the typical SMDP. It's trivial to be able to take one representation and swap it into the other. But once we're in this representation, we can start to look for particular structure within that graph and it becomes a recognition problem within that graph space.
00:11:31.154 - 00:12:33.544, Speaker B: So what are the structures we're looking for? Well, the first thing we're going to look for is that we're going to look for sequences or chains of states, because those chains of states indicate sequences of actions that have to be performed one after another. The other thing that we'll look for are cliques, fully connected subgraphs. Those cliques will represent sequences, or rather collections of activities that must all be performed. But where the order is invariant, or, sorry, the order is arbitrary, we can perform those actions in any order. So let's take this example. So here's one of the little subgraphs from this chair assembly task we'll look through, and we'll consider it first to be one completely flat representation. That is, there's one node in the hierarchy that represents the whole structure, and each of the individual nodes is an action within the leaf of that hierarchy.
00:12:33.544 - 00:13:02.444, Speaker B: We'll then start looking for cliques and chains, and we'll iteratively look for those two structures. So first, let's identify a couple of chains. So there's a pair of chains here in which I have to get the left peg and then place the left peg into its. Into its position. And I have to get the right peg and place the right peg into its position. And I have to do those in order. And anytime I start with getting the left peg, I have to do placing the left peg after that.
00:13:02.444 - 00:13:34.084, Speaker B: So we'll identify those chains of activities. We'll treat those as a step up in the hierarchy, and we'll condense the nodes within that graph and treat that as one entity. Now, once we've done that, we can then recognize that. Now, there is a clique in place here that is from wherever I go outside of this entry. I always come in. I come into either one of those two. I then have to complete the other before I move on and exit that.
00:13:34.084 - 00:14:15.614, Speaker B: That is to say, I have to do the left side and I have to do the right side. But it doesn't matter which one I do first. And then finally, there's one big chain that I've formed now, between the starting state, this large purple intermediate clique, and then the two last actions. And that's one higher level structure. So what this does is it forms hierarchical structure where the internal or higher level nodes indicate either actions that must be performed in sequence, the chains, or can be performed in any sequence. Those cliques. Now, it's certainly not the most intelligent hierarchy.
00:14:15.614 - 00:14:16.078, Speaker B: It's.
00:14:16.126 - 00:14:24.486, Speaker C: Yeah, I noticed the get frame and place frame that forms a chain. But you didn't group them together. Is there like a difference?
00:14:24.550 - 00:14:52.610, Speaker B: Yeah. So we could do that. It turns out that you can create multiple hierarchies that are identical. And in fact, you need to define a way to preference one type of representation over another. It's the same as sort of the socks and shoes problem. I need to put on two socks and I need to put on two shoes. It doesn't matter whether I do both socks, then both shoes or I do left sock, left shoe, right sock, right shoe.
00:14:52.610 - 00:15:06.554, Speaker B: But there are orderings that I can't do. I can't do left sock, right shoe. So we need to be able to discriminate those. And we do have some methods, and I don't have time to talk about that. But that is something that we deliberately didn't talk about right now today. Yeah.
00:15:08.134 - 00:15:20.154, Speaker A: So along the edges, the sort of the preconditions that are needed here, they're stated as kind of a logical entities. So, in general, would that basically just be a classic?
00:15:20.614 - 00:15:33.854, Speaker B: Yeah, so those can be. Those are the same things that you're going to derive from looking at these world states. And what are the actions that are being performed in order to transition from one to the other? Those are the same kinds of things that we get from a learning from demonstration based system.
00:15:35.634 - 00:15:41.130, Speaker A: But in order to sort of generalize these modules or something that I suppose.
00:15:41.322 - 00:16:42.214, Speaker B: One would, yes, there are a few things that you need to do. I've skipped over a couple of steps here, but there are a few things you need to do to match up those and to condense and simplify them. Okay. All right, so that's one simple technique that we can use to start to generate some form of hierarchical structure. Let me make the argument now that not only do we need to build these structures so that we have ways of communicating between the person and the machine, but that we also need to change the way that we can say what the machine is doing. So imagine that I'm putting this chair together again. Typical methods that we have for identifying what the robot should do, whether it's a hierarchical based planning system or a flat based planning system, is to identify which actions, which subtasks could the robot perform.
00:16:42.214 - 00:17:52.564, Speaker B: Every subtask that the robot can identify is one step toward the goal. It's one of the leaf nodes in this representation. However, that's a very limited form of the kinds of interactions that we see when we look at real partnered teams. And let me again, jump to the end of this story. First, there are many other behaviors that you don't find through a divide and conquer based approach to planning that are useful things for a collaborator to do. For example, handing you material when you need it, or seeing that, in this case, you're having trouble with one part, and maybe you would actually have a better time with this screwdriver instead of trying to put that screw in by hand or being able to recognize that you're having a lot of trouble getting this to actually fit together. And maybe what I should do is just hold that piece of material in place for you so that you have fewer things that you have to manipulate.
00:17:52.564 - 00:18:17.596, Speaker B: These kinds of things I'm going to call supportive behaviors. None of them are the kinds of actions that you could find by a divide and conquer based approach. No divide and conquer based approach to looking at the task of assembling this chair is going to come up with. Hold the material steady or hand over the screwdriver, doesn't matter what level of granularity I pull this apart to.
00:18:17.660 - 00:18:27.788, Speaker D: Yeah, just to make sure I understand. So if I screw, if I need to use a screwdriver, I will take it. So you can look at it as one thing that I did do in a sequential.
00:18:27.916 - 00:18:36.364, Speaker B: Yes. Right. So there's some things where that's going to be the case. There are other things. Like what I might do is I might clear the workspace for you.
00:18:37.184 - 00:18:37.992, Speaker A: Okay.
00:18:38.128 - 00:19:11.682, Speaker B: No part of our planning system is going to identify that. Okay. Me supporting the structure. You're already supporting the structure. Why should I need to stabilize it for you? Okay, so we can, depending upon the level of granularity, some of these may become parts of the decomposition, but there's no level of granularity where we're going to get all of them. So in this case, we had pre programmed. In the video that you saw, we pre programmed certain behaviors that when a certain condition was triggered, the robot would respond.
00:19:11.682 - 00:19:47.470, Speaker B: If it saw that you were fumbling around and taking too long on a particular task, there was a particular action that it offered. You're taking too long assembling the back of the chair to the side of the chair. I'll offer to stabilize this for you. We'd like to do better than that. So, in most of our demonstration based systems, the human, through some form of deliberate instruction, tells the robot what to do, when to do it. We'd like to move to the point. Well, we have moved to the point where we're one step better than that.
00:19:47.470 - 00:20:33.024, Speaker B: The robot recognizes when to take an action, but then it responds with this one canned response. We'd like the robot to move beyond that, to the point where it figures not only how to help, what it should be doing, but also when it should intercede and help. And that means that we move a little bit from a traditional learning perspective to what in robotics, we'd call planning. But I'm going to argue that they're pretty similar. Anyway, in order to do this, we're going to link a symbolic planner with a motion planning system. And these are typically considered as separate tasks within the robotics world. There's a symbolic planner that tells me what task I'm supposed to do, and then a motion based planner that tells me how to execute that particular goal state.
00:20:33.024 - 00:21:38.094, Speaker B: And we're going to do some amount of perspective taking. That is, we have to consider things from the other agent's point of view in order to be able to do this. Okay, so I'm going to highlight a solution. This is not the best solution, and this is not in any way the final solution that we would like to have, but it's enough to give you a sense of where the problems are. So what we're going to try to do is we're going to have the robot initially hypothesize future states. That is, we're going to generate future hypothetical states that the robot could achieve things that it could possibly do based on the plans that it has and the actions that are available to it. We'll then look at and predict what the user might be able to do given that future state and figure out what the robot would need to do in order to achieve that state.
00:21:38.094 - 00:22:13.378, Speaker B: Okay, so imagine some point in the future. Imagine what the human user would be able to do going forward from that point and going backwards. Figure out what it would take me to get to that point. Well, then weight that policy and look at it and say, is this something that's worth doing? And if it is, then let's execute it. Now, like I said, this is not the perfect solution. Ideally, we'd like to have some more guidance than just imagine a bunch of possible future states, but for now, that's going to be good enough. All right, so let me give you some examples with a simplified assembly task.
00:22:13.378 - 00:23:10.974, Speaker B: So rather than building a chair, now, I'm going to build some flat pack furniture that involves a small bench. Okay? It's got four pieces. It's got two legs, a horizontal support and a flat piece to sit on bench itself. And we've made our job somewhat easier in that we've put QR tags on these so that we don't have to solve lots of difficult vision problems, because we've got enough problems to give everyone. We'll let the vision community figure that out. Okay, so here's some of the supportive actions that in this case, a Baxter robot, a commercial robot, could use in terms of constructing supportive behaviors for that assembly. So the robot has a sequence of pieces that it can gab here, it's going to offer to hold one of the pieces in place while it provides the other components to a human user.
00:23:10.974 - 00:24:13.304, Speaker B: And we're showing this at four times speed because even with the best that we're doing now, the robots are still kind of slow, and we run them deliberately slow to keep things safe in the lab. But this is the kind of activity that we're trying to generate. Now, no part of the plan here involves deliberately giving the robot instructions about handing one object over to its other hand about holding that steady. Now we are using some simplified vision, as I said, to do this. The QR tags, as you can see, are letting us localize this and select out pieces. But it also introduces one interesting piece, which is that we can make small changes to these pieces so that some of them fit together and others don't, and the robot can discriminate between them and the human user can't. So it allows us to build a representa or build a very simple task where there are some things that the human is not good at or that we want to not allow the human partner to do.
00:24:13.304 - 00:26:04.414, Speaker B: And this is an online planning process, so there will be failures that come along. In this case, we're going to generate a failure by having the human user come along and select, decide that one of the pieces is faulty and get rid of it, at which point the robot needs to adapt what it's doing and replan on the fly to generate not only the actions that needed to be done to support assembling this piece, but also those supportive actions as well. And finally, we can also start to model preferences in terms of task activity. So we may have one user who likes to assemble things and then put them, put the two legs together and then put it down and put the bench on, and other users who might prefer to do everything while the robot is holding things. So the challenges here, and this is again, just a system that's designed enough to give you a set of ideas about what should happen next, is how can we look at this kind of a process and build a representation that allows the robot to flexibly select and learn to select those different kinds of supportive activities? All right, let me take this one step further. Once we start considering supportive action in addition to just task based or task directed activity, we need to think about the kinds of ways in which we model collaboration. And I'm going to do this with a different application domain.
00:26:04.414 - 00:27:10.224, Speaker B: We're still going to use an industrial grade robot, but we're going to use a very simple kid's toy to demonstrate this. So this is an electronic building kit called Snap circuits, where the idea is you've got these rigid pieces that are wires and these rigid pieces that are resistors and these rigid pieces that are battery packs, and you can build circuits out of them by connecting them together and just clicking them together. It's nice because it's the kind of thing that we can easily characterize. It's also great because while there's a limited search space for this, you can only put them down on certain pegs, you can't sort of position them any which way you want. There's still a relatively rich description of the way in which you can define problems and not have a specific solution in mind. So I can ask you to build a switched light bulb. And there's probably, there's not an infinite number of solutions, but there's a large number of solutions given a finite set of parts.
00:27:10.224 - 00:28:30.964, Speaker B: Okay, this brings to the forefront problems with resource allocation. Who's going to grab which piece at which time, how do we use the space of the board? How do we assign who's doing which piece? And how do we parallelize those different subtasks? Now, we're going to use this same supportive agent pipeline, but the piece that I'm going to focus on and the piece that I'm going to modify in order to make this point is this magic little piece up here that says somehow we weight the two policies between the human and the machine predicted action that the human would take if the robot carried out this one supportive or task based activity. How do we weight those two when we consider the full execution? Okay, so we can capture this by a plan evaluation function. And for right now, we're just going to consider duration as our key metric. Okay, how long does it take you to complete this task? So we're going to minimize over all of the possible activities that we could take. Which support policy should we be selecting that minimizes that duration. Now, the only part to this that we're going to do anything to change at all is this weighting function that we're applying to the duration.
00:28:30.964 - 00:29:15.494, Speaker B: Okay? So there's a fixed duration for every possible configuration of the world of what would take to go next, and we'll weight them in different ways. So one thing we could do is we could just apply a uniform weighting function, put a w of one, a constant one in there. And what that's going to do is it's going to make the robot take some action that will optimize or improve one of the possible plans that are out there. So in this case, we're using a smaller kuka robot arm, but it's actually a much more dangerous robot to be around. So we run it much, much slower. This is an industrial strength robot. It's powerful enough to tear itself off the table.
00:29:15.494 - 00:29:52.182, Speaker B: So think of what it can do to your fingers. And that's why we run it slow. So that's the only reason it's moving at the speed you'll see it. So what's going to happen is we have these different pieces from the snap circuits available on the table in these little cups, so they're easy for the robot to grab. And a human who's got a specific task in mind. In this case, they're trying to build a switched led, and there's one nice three wire component there, and there's another three wire component there. And if we use a uniform weighting function, the robot is going to take some possible plan and optimize it.
00:29:52.182 - 00:30:18.334, Speaker B: In this case, it's going to take that three element wire that's really far away from the person and move it close to them because there's a possibility that they would use that piece. However, it's an extremely unlikely plan because they already have a three wire piece right in front of them. The robot has made one possible plan better, but there's already a better plan out there, so there's really no reason for this to happen.
00:30:18.954 - 00:30:25.174, Speaker A: So, in terms of notation, the PI is the possible thing the human could be wanting to do.
00:30:25.894 - 00:30:36.354, Speaker B: Yes. So there's a piece that is, what should the robot do? And then based on the state that that ends up in, what is the human action that we predict will happen following that?
00:30:36.734 - 00:30:37.594, Speaker E: Okay.
00:30:38.654 - 00:31:33.554, Speaker B: All right. Second thing, we could try and instead optimize with a slightly different weighting function, where we'll weight the plans proportional to the similarity to the best known solution. So the closer you are to the best known solution, the best solution that we have planned at this point, the better we'll weight it. So what that's going to do for us is we will, in effect, favor making our good plans even better. So in this case, the person is assembling, again the same circuit. We see that they've selected a part, we know which part they're going to potentially need or which kind of parts they're going to need to finish that off, and the robot will go take that piece that we know that they're going to need next and make it easier for them to get that piece.
00:31:37.694 - 00:31:42.166, Speaker A: So in some of these situations, it seems like speech would be helpful, right? Some sort of.
00:31:42.190 - 00:31:44.294, Speaker B: Yes, absolutely.
00:31:44.754 - 00:31:51.174, Speaker A: But I guess this is a situation where you wouldn't want to have.
00:31:51.674 - 00:32:14.184, Speaker B: Yeah. And that's. And that's. There are times when speech is essential, and we can talk about that. That's actually one of the things we're working with right now. It's a very hard domain to integrate speech into because the kinds of utterances that you get from the human are extraordinarily difficult to ground. The person in this case, will say, you know, get me the next piece.
00:32:14.184 - 00:33:00.564, Speaker B: What's the next piece? Or the most common utterance that we actually get when we have real people do this is they'll just say, help. Thanks. That's great. That means, I know you want me to do something now, but it hasn't given me any selection over or what that action should be. Let me give you one other thing, and this is the point I wanted to get to. One of the other ways in which we can modify that waiting function is that we can penalize bad plans. So rather than trying to make a good plan better, we will penalize the really poor performing plans.
00:33:00.564 - 00:33:58.724, Speaker B: Okay. Effectively make a bad plan worse. What this accounts, what this amounts to is the robot basically assuming that it knows better than you and it doesn't trust you not to do something that might look like a good plan to you, but is really a terrible idea. So, for example, in this video, we're going to have a plan that it would be a terrible plan to execute using the wrong piece. But that piece that they're considering is this suboptimal but easy to reach piece. Okay? So normally when we consider planning, what we would do is we would take one of the three component, three length pieces, one of the good pieces, and move it close to you. But if we change this weighting function instead, what the robot does is it takes that easily available but faulty piece and moves it so that it's really hard for you to get to it.
00:33:58.724 - 00:34:38.103, Speaker B: This is a different kind of collaboration. Okay. This is the kind of thing that we need to start considering. It's not the case that we would want to do this all the time. In fact, it would be nice to have a model of how expert your system is and how expert the user is and adjust the weighting function based on that. But we do this with each other. When I sit down and I teach one of my brand new undergraduate lab assistants how to solder, you can bet that I take the really expensive parts and I move them really far away from them so that they don't try those out first.
00:34:38.103 - 00:35:21.954, Speaker B: So when we start considering real collaboration situations, we need to move beyond the idea of just optimizing the best plan. Sometimes optimizing the best plan is the really the best thing we can do. But sometimes maybe we want to take one of those bad plans and make it even worse. Okay, let me close up and then I'll take both questions. I just need to say thanks to the people who actually did all of the work for this. Brad Hayes, who is my former PhD student who's now at Julie Shaw's lab at MIT, Alessandro, Olivier and Francesca. Alessandro and Olivier are my current post docs, and Francesco is a visiting student.
00:35:21.954 - 00:35:30.610, Speaker B: And that's thanks to all of them for supporting the robots as well. Okay, let me go back and take questions. So, first in the back and then. Right.
00:35:30.762 - 00:35:46.902, Speaker C: This makes perfect sense. I just wonder currently, are these behaviors learned from past experience? Because it all boils down to theory of mind. Can humans be confused by that action and try to really grab that for a piece?
00:35:46.998 - 00:36:31.234, Speaker B: Yeah. So there's definitely a question of how would the person respond to this particular action? Okay, one of the things that we have that the robots do naturally, not naturally. There is no such thing as naturally for a robot. One of the things that falls out of either, any of these kinds of planning is that the robot will actively, if it has nothing better to do, it will actively try to make your job easier by pulling things out of the way that are not relevant to the task at hand. So effectively, it cleans up the workspace. We like that in part because what it's doing effectively, it's limiting the cognitive load on the user. And if you trust the robot, then that's an effective thing for it to do.
00:36:31.234 - 00:36:57.134, Speaker B: If you don't trust the robot, though, it's a terrible thing to do, because every time the robot grabs something and takes it away from me, I'm immediately thinking, wait, maybe I want that. And it distracts the user every time. So there are a lot of different kinds of scenarios that we might consider as we look at these. And it definitely comes down to a bit of theory of mind and perspective taking a moment. And that's a lot of my interest in this.
00:36:58.514 - 00:37:05.506, Speaker D: So it seems to me like, at least from the person's perspective, they have explanations to what the robot is doing.
00:37:05.650 - 00:37:06.282, Speaker B: Yeah.
00:37:06.418 - 00:37:07.778, Speaker D: Much less annoyed when they.
00:37:07.866 - 00:38:00.894, Speaker B: Yeah. And one of the things, and one of the things that we can do, because this is represented in a non flat system, is that we can actually either deliberately ask the robot, what are you doing now? And it can say, I'm assembling the rear leg structure, or it can say, you look like you were having trouble, I'm offering to help, or it can say, you really don't need this. I'm just getting it out of the way. And those are things that we can identify internally as structure. One of the current projects that we have right now is that we're working with two natural language folks to try to produce those kinds of grounded utterances. It's not something we have the capability to do right now, but it's something that we are structured to do. That's exactly where we want to be.
00:38:01.674 - 00:38:06.570, Speaker E: So you said the user has this goal in mind. Does the robot know the goal in this instance?
00:38:06.722 - 00:38:12.806, Speaker B: In this particular instance, yes, but not always things that are clearly not going.
00:38:12.830 - 00:38:13.750, Speaker E: To fit in the.
00:38:13.822 - 00:39:05.544, Speaker B: Yeah, and that's. And that's one of the things, actually, that we use as an active learning mechanism to figure out whether something is part of the task or not. So when the robot doesn't know the task at all, one thing you can do to figure out whether something's important is to try to get rid of it. And as soon as, when you get stopped, you suddenly know that's an important piece. Okay, it sounds silly to some extent, but when you're trying to identify, there are three parts to this. Ideally, what we would have in my wonderful vision of the future is that we would effectively do what surgeons call their mechanism for watch one, do one, teach one. You watch it first, watch someone else do it.
00:39:05.544 - 00:39:30.404, Speaker B: Then you work together and you do it collaboratively, and then you bring in somebody new who's never done it before, and you have to teach the process to them. Closing that loop between those three very, what we consider very different tasks right now. Watch somebody and learn how to do this. Learning from demonstration, collaborate with someone. Okay. Maybe this kind of stuff. Teach them how to do it.
00:39:30.404 - 00:39:46.324, Speaker B: We have other work that's on that tutoring side of the domain, but how do you build something that actually spans across those three domains? That's what I think is the really interesting part to this, but you're not going to get it unless you have the underlying representation. Right.
00:39:48.064 - 00:39:55.492, Speaker A: So in some of the domains you've been working with, is it the case that different humans would actually appreciate different kinds?
00:39:55.668 - 00:40:24.938, Speaker B: Yeah, absolutely. Even in really simple things, even when we're building those little Ikea chairs, we bring in naive users. We give them a set of instructions, they study the instructions, and then we say, here you go, build the chair. Okay. Some people love this kind of thing. Sure enough, we bring in a couple of Mekhi undergrads, and this is, you know, fun time for them. We're a liberal arts campus.
00:40:24.938 - 00:40:52.714, Speaker B: You bring in a theater studies major, and this maybe is not their kind of thing, and they're miserable with it. Those two different individuals have different preferences about what they want to do and what they want help with. Now, those are caricatures of people, obviously. I know theater study folks who are much better at assembling things than, unfortunately, some of the Mekki students. But those kinds of preferences come across. We want the system to be able to identify those.
00:40:54.254 - 00:41:00.558, Speaker A: And would this be something that's done with like a little initial interaction or sort of along the way one graphs?
00:41:00.686 - 00:41:49.314, Speaker B: Yeah. So, you know, one of the reasons we cast this in the collaborative manufacturing domain is the idea that you're going to be working together for a reasonable amount of time. You're going to be doing some things repetitively, but you are going to be changing tasks as you go along. And that's the kind of thing that you're seeing small and medium scale enterprises being interested in. If you're Mercedes Benz or your Volkswagen, you can afford to dedicate a multimillion dollar factory towards one particular line of product. If you're the local bakery, you can't do that. So instead you need a system that's going to be flexible enough where in the morning it's helping, I don't know, box croissants to send out for the morning deliveries and later in the afternoon it's helping to roll out dough.
00:41:49.314 - 00:42:04.674, Speaker B: How do you shift between these different kinds of things? So, as I promised, I brought more problems than solutions today. But these are the kinds of things that I think make for interesting collaboration across domains.
00:42:05.074 - 00:42:14.298, Speaker E: Have you tried the medical domain? I mean, since you mentioned surgeons, you imagine like a surgeon's assistant gets a very precise, unambiguous instruction, and this kind of goes back to the speech.
00:42:14.386 - 00:42:36.084, Speaker B: So it turns out that the surgeons. So this is one thing. So our natural language collaborators actually worked in the medical domain before this. And it's one of the things where surgeons are very particular about the staff that they work with because they learn the patterns of what's expected. And the worst thing you can do to a surgeon is to give them a different score. Scrub nurse. They're basically.
00:42:36.084 - 00:43:09.774, Speaker B: The thing you want to know going into surgery is that this team has worked together before a lot. That's the best predictor of success. And it's because it's not the case that everyone has a very well defined job. It's that different people have preferences and the overall responsibilities of the same, but the distribution of them when they happen, how they happen, it's not the same from team to team. Okay, I think I kept us mostly on time.
00:43:16.354 - 00:43:24.174, Speaker A: Okay, so the next speaker is Sinha Sabata, who will be talking about active nearest neighbor.
