00:00:00.360 - 00:00:09.474, Speaker A: Lecture on TCP theorem given by Dana. She will talk about optimal inapprox mobility in particular.
00:00:10.454 - 00:00:55.324, Speaker B: Thank you. Okay, so, yeah, so this is the last talk. We'll talk about how to prove really fine sharp inappropriability thresholds. And in this talk it will finally be clear, kind of the depth of the connection between PCP and approximately. And geometry, it's very, very tight and interesting connection. Okay, so let me start with that. So what's the connection? The connection is that the best known algorithms for many, many combinatorial optimization problems, our geometric algorithms.
00:00:55.324 - 00:01:26.224, Speaker B: Okay, so it's not true for all problems. It is true for very wide families of problems. So let me give you, as the running example, we'll do Maxcat. So Maxcat is the following problem. It was mentioned before by Rit. You're given a graph and you're trying to find a cut in the graph. So a partition of the vertices into two, such that there are many, many edges that cross the cut.
00:01:26.224 - 00:01:56.054, Speaker B: So one endpoint of the edge is on one side and the other endpoint is on the other side. It's a very simple combinatorial optimization problem. I want to tell you about the best algorithm we have for Maxcat, which is purely geometric. This is the Goman's Williamson algorithm. We call it Gomans Williamson, even though Michel's name is Michelle is belgian. His name is. But we all call it Gomans Williamson.
00:01:56.054 - 00:02:56.594, Speaker B: So here is how it goes. So this is this Commons Williamson algorithm. So what it says is let's identify a vector in Rn with each vertex and what we are really looking for. So we're going to have all those vectors and what we are hoping for is that vertices that are joined by an edge actually are very anti correlated. So maybe one points the other way. So all those vectors will be unit vectors and. Right, so we'll have those Vi's and all of the VI's are going to be unit.
00:02:56.594 - 00:03:55.524, Speaker B: And we hope that what we'll ask for is that across edges will have as much anti correlation as possible. Right, so we're going to look at the inner product of Vijay when IJ is an edge, and we'll try at least on average. Let's suppose that our edges are just, our vertices are just numbers between one and n. So for every edge on average over edges, we want this inner product to be anti correlated. Okay, so we're going to have lots and lots of. So they're all supposed to be unit. Now there is this trick we do.
00:03:55.524 - 00:05:02.976, Speaker B: Right, so unit vectors, right, so if you look at edges. Either you have an edge or you don't have an edge, right? So it either zero or one. But then this inner product, what we want is that. Right? Well, we want to count, we want kind of this mapping, right? So this is x minus x over two, right? Map one to zero and minus one to one. So really what we're going to consider is half, minus half of this. And this will tell us that when we count how many edges are separated, then minus one is counted as plus one. And if it's a, if they're the same, then it's going to be counted as zero.
00:05:02.976 - 00:05:59.410, Speaker B: Right? So this is what this linear transformation does for us. Okay, so we'll see a lot of like half, minus half of inner product, right? Because this is the way to translate between, between inner products and just zero one. Okay, so this is what we, this is the geometric problem we're going to try to, we're going to look at, we're searching for unit vectors in rn such that we want to maximize this half, minus half sum of the inner products. I see there's a questions. Question, Dana. I think the comment is just that the sum should be outside of the. Sure, yeah, sure.
00:05:59.410 - 00:06:31.884, Speaker B: We can do, we can make it into an average. Okay. Or multiply everything by. Yeah, okay, thanks. Anyway, so this is, this is the Goeman Sweden algorithm you want to solve, Maxcat. In fact, instead solve this, this problem on finding unit vectors that try to be anti correlated on edges. Now, the nice thing about, about this problem, this geometric problem, is that it has an efficient algorithm.
00:06:31.884 - 00:07:22.204, Speaker B: So this is known as semi definite programming. If you try to find vectors and all you have is linear constraints and linear objectives on those pairwise inner products, then semi definite programming is an efficient algorithm that solves this problem. So this is what Goman Swilliamson algorithm does. It converted our combinatorial problem to a geometric problem. And of course, the vectors that the algorithm may find are not necessarily just would give you plus one or minus one. You can have all kinds of angles between different vectors. So maybe you have vi and v j.
00:07:22.204 - 00:08:21.702, Speaker B: And the angle here would be general theta. And the question is, how do we take this and get a cut out of that? So we got the vectors of the algorithm. How do we have the cut? And what they do? So let me maybe switch a color. So what they do is they say, let's pick random half space. So now pick random half space and then, and then partition. Then this induces the cut. So everything on one side of this, on one side of the random half space is going to get to be on one side of the cut.
00:08:21.702 - 00:09:20.974, Speaker B: Everything on the other side is going to be on the other side of the cut. Okay, so things that are very anti correlated are not going to be on the same side of the cut, but things that are very correlated are likely to find themselves on the same size side of the, of the cut. Okay, so this is, this is the algorithm I saw. There's a question. No question. Okay, let's, let's see if this is a, hopefully for, for the geometries of you. This is an algorithm that, that makes sense, right? This is maybe what you would try to do and, okay, so how good is it? So let's see, so the probability of separating, probability to separate vi and vj when the angle is between them is theta.
00:09:20.974 - 00:10:19.464, Speaker B: What is theta over PI or theta over two PI? And meanwhile, what you pay for the, in the inner product world, half minus half the inner product, right. Is half minus half cosine. Cosine theta. Okay, so right in the geometric world we have this. In the, in the, in the Maxcat world we have this. And then the question is what's, what's the ratio between them? So maybe, let me tell you here, mean the ratio between theta over PI and half minus half cosine of theta. This is about 878.
00:10:19.464 - 00:11:40.642, Speaker B: And this is the approximation that Goemon Williamson gets. Okay, so you're going to, to get not the best cut, but a cut that is about 88%, has about 88% of the number of edges in the maximum cut, which is, you know, pretty good if you're in the case where, in the special case where half minus half cosine theta is one minus delta, is one minus delta. For very small delta, you actually have that theta over PI. Oh man. Let me maybe just say that in the special case, well, well, this what you pay half minus half cosine theta is very close to one, is one minus delta. The theta over PI becomes one minus two over PI square root delta. So this is, if you remember it reads talk or she had one minus delta versus one minus square root delta.
00:11:40.642 - 00:12:17.814, Speaker B: This is where it came from. Anyway. So I mean, but this is it, right? This is, this is the whole Gomans Williamson algorithm. It gives 88 approximation ratio for Maxcat. Okay, start a new board. And for the geometric problem doing this random half space, this is the best you can do. So this was theorem of Tiger and Schechtman.
00:12:17.814 - 00:14:34.434, Speaker B: Random half space is optimal partition to cut the vectors. And the instance, the graph that they consider is this graph where you have, say, all vectors on the sphere, and you connect in an edge two vectors where the inner product is some fixed row, and you can see, right? So this is a parametry that random half space is the best, is the best cut the very surprising theorem. So this is this KkMO theom that Rit mentioned, Khodkindler, Mosellen, Odonelli, and they showed that not only random hub space is the best thing to do. In fact, this Williamson algorithm, Goemons Williamson algorithm, is optimal computationally. Computationally for Maxcat, as in it's NP hard to do better, to approximate better, right? NP hard is right. This is what we are able to, this is the notion of hardness. We're able to prove that it's as hard as solving coloring efficiently.
00:14:34.434 - 00:16:49.828, Speaker B: At least this is true if you assume this so called unique games conjecture. Okay, so what I want to do is prove, to you at least sketch the proof of this theorem, okay? So, right, it's a remarkable theorem that somehow this geometric, very simple geometric algorithm is optimal computationally, right, you can, it's np hard. It's unlikely that you'll be able to do better than this. And what's really remarkable is that this is not just about Maxcat. So, such a theorem is true for every, for every concern satisfaction problem, if you remember the definition of those from Merit's lectures. So, and it's a very interesting, interesting proof, because what you do is you take the optimality of the geometric, of the geometric solution, like what we had Feige Shechman proving that you can partition the vectors better than then taking a random half space, and then you lift this to computational optimality, at least if you have this unigames conjecture. So, it's a direct way to, to lift optimality in the geometry world, to optimality in the computational world, which is amazing.
00:16:49.828 - 00:18:13.964, Speaker B: And then Prasad, Rago Vendra actually generalized that and showed how to do the same. Same can be done for all constraint satisfaction problems. So, using geometric algorithm is not new. I mean, it was new with Gomez and Williamson did it in the beginning of the nineties. Before that, there was linear programming, when instead of searching for vectors, it's search for points, points in r, and that satisfy some linear constraint, trying to satisfy some Linux objective. But definitely sans, Gomez, Williamson. This has been what's in the toolbox of algorithms, people, you give them combinatorial problems and then they say, oh, but let's embed it in RN and let's try to find vectors and argue about correlations of them.
00:18:13.964 - 00:19:31.344, Speaker B: And those theorems show that they will write algorithms, people to do this, because indeed, it's a very good way to solve, this is like an optimal way to solve computational optimization problems. Optimization problems just by reducing to geometry. And it's really, this, the geometric problem is basically equivalent to the computational problem, which, I mean, it's mind boggling that you can, that this is true and that you can prove that. Okay. So I'll be happy to answer any questions just about the big picture connection between geometry and computation. And if there are no such questions, what I'll do is I want to remind you of this unigense conjecture, and then I'll sketch the proof of this KKMO theorem. Okay, question the mapping to rn.
00:19:31.344 - 00:20:24.298, Speaker B: Not sure that I. So the, the mapping to rn coming up with the vectors. This is what the semi definite programming algorithm does for you. What you ask the semi definite program to do is find for me unit vectors, such as to maximize this linear form in the pairwise correlations. The magic is that this is possible to do efficiently right now. I mean, of course we can talk about semi definite programming and linear programming and how those algorithms work. And this is a fascinating topic, but not a topic that I have time to discuss right now.
00:20:24.298 - 00:20:46.526, Speaker B: But this is the amazing thing. There is a way you can tell an algorithm I want vectors like that. And please find them for me. Okay. Okay, good. So let me remind you of the unique games conjecture. And the unique games conjecture talks about this problem called unique label cover.
00:20:46.526 - 00:21:19.484, Speaker B: So this is a new combinatorial optimization problem. It goes like that. The input is a graph. So in Irith's lecture, it was a bipartite graph for unigames. It makes more sense to have it non bipartite. You also have an Alphabet, and usually you denote the size of the Alphabet by K. This is some finite set of colors of labels.
00:21:19.484 - 00:22:18.594, Speaker B: And then you also have one to one permutations on the edges, right? So you have those PI, e from Sigma to Sigma for every edge. Okay. And this is one to one mappings. And the goal, you want to find the labeling of the vertices. Labeling. So a form v to sigma that satisfies as many edges as possible, satisfies as many edges as possible, and satisfies an edge. This means.
00:22:18.594 - 00:23:22.874, Speaker B: So let's say that the edge is uv in e. Let's call it e. What we want is that if you see what, what's the label that you get, and then the mapping those permutations give you, what's the label that you expect for v? And you want you want to get this? You wanted the label to be, would be what you expect. Okay, so this is this unique label cover problem, and then the unique games conjecture. And I'm doing this first because you redefined it on Friday. So the New England conjecture. So this is Cott's conjecture from 2002, that for every epsilon and delta larger than zero, there exists k.
00:23:22.874 - 00:25:36.574, Speaker B: K is a function of epsilon and delta such that it is np hard. Given a unique label cover, instance input, say g, sigma and sigma, we usually get, we call the number of labels k and those PI is, um, to distinguish between the two cases, one is all edges, not all, almost all. Um, at least one minus delta fraction of edges can be satisfied versus at most epsilon fraction of edges can be satisfied. Okay, so this is the unigames conjecture. The reason subhash was conjecturing that was that this is known as a theom for standard label cover, right? What we call projection games. If the, if the PI es weren't supposed to be, weren't, weren't one to one, then this is true as a theorem. In fact, extremely strong things are known, right? You can take delta equals zero and epsilon depending on n going to zero.
00:25:36.574 - 00:26:25.574, Speaker B: We know that there are extremely good reductions, very efficient reductions like that. But Subhash was bold enough to say this is true. This, he hoped, would be true not just for normal label cover, but for unique label cover, where all those permutations on the edges were, well, one to one functions. Okay, so this is the unique games conjecture. Again, if there are any questions, I'll be very happy to answer them. And now what I want to do is prove to you this, this Kkmo. But first, let's maybe have the picture in our heads for what's going on.
00:26:25.574 - 00:27:22.134, Speaker B: So, over many, many years, beginning, I guess, in the eighties and nineties, formed this paradigm for how to prove good proximity results. And it started with this basic PCP theorem. So this is a theorem like we discussed in the, in the first and second lectures, right? Well, what you say, basically, you say it's NP hard to distinguish between the case that the graph is three colorable and the graph is like, far from three colorable. Isn't unique. I have a question. Isn't unique label cover a special case of label cover? It is. I'm confused how UGC can be a theorem when we don't demand uniqueness while UGC itself is a conjecture.
00:27:22.134 - 00:28:09.814, Speaker B: No. So what I said is that it is NP hard. So let's maybe say something about what it means for something to be NP hard. It means that there is this, there is a construction, a way to take. You want to distinguish whether a graph is three colorable or not three colorable, and then you can reduce it to a label cover such that if the graph is three colorable, then the label cover is fully satisfied. And if the graph is not recolorable, then the. And the graph is far from satisfied.
00:28:09.814 - 00:29:07.364, Speaker B: But this is true as a theorem. In the case of the PI es are not one to one in the sense that there is a construction. Well, the PI's are not one to one. And then you can prove such a reduction. And if you require that the PI's are one to one, we just don't have such a construction that generates this sort of, these sort of instances. Okay, maybe, maybe the panelists would be able to answer, okay, yeah, when we don't demand uniqueness, this is known to be to be a theorem, even with extremely good parameters. But when we demand uniqueness, we don't know how to prove it as a theorem.
00:29:07.364 - 00:30:16.130, Speaker B: Okay, so good. So the way one proves optimal, inappropriate results. One starts with a basic PCP theorem, like what we discussed in the first two lectures. What if we described on Friday and get this label cover without demanding uniqueness? And then there is a very nice paradigm that I'll show you an example of today based on something called the long code to get results like, say, the three lean result that Rit kept mentioning, and many, many other optimal inapproximate results. So this is the basic paradigm, right? I should mention that there are various proof of the basic PCP theorem. One doesn't have to go for parapet. There are other ways to go to label cover and so on.
00:30:16.130 - 00:30:59.846, Speaker B: But this is the basic scheme. And what Subas suggested now, almost 20 years ago, was instead, if we could go to unique label cover, right? So demand uniqueness, demand one to one permutations in the label cover. So this is what we don't know whether this is true or not true. Then we'll be able to do things like. So he gave all kinds of examples in his original paper. The point is that you can even prove hardness of results for problems that look at two bits. Eventually Maxcat was proved.
00:30:59.846 - 00:31:56.054, Speaker B: This is the KkMO result. This is what I'll do today. There is also a vertex cover is another, for those who know is another example. This was a paper for dead Regev and Subhash and many, many other theorems that you couldn't prove with the standard paradigm. You can certainly prove once you assume this extra, extra permutations are unique. Okay, so, question, what is the best many to one ratio known in the reduction for general label cover? So you can actually get delta equals zero right in the. Instead of the Unigames conjecture for label cover, you can get delta equals zero and epsilon that equals one over polylogan.
00:31:56.054 - 00:32:25.584, Speaker B: Yeah. It's actually open. Whether you can get one over polyn for some, for the right polynomial. Okay, so, hopefully this answers that. So, we have extremely good, good theorems like that in the case where we don't demand one to one. But when we do demand, we have no idea whether how to do it, or. I mean, we have some ideas, but not yet a proof.
00:32:25.584 - 00:33:09.284, Speaker B: But if we had the proof, we can use this paradigm to prove optimal in a possibility for problems like Maxcat. Okay. And it's really. I mean, the reason Subhash came up with this question was that he knew very well what was this transformation from label cover to thrilling based on long code. And he just realized that everything would be much, much easier, much, much simpler if you could assume uniqueness. We'll just simplify everything, and we'll let you prove things that you couldn't before. Okay, so this is what I want to do now.
00:33:09.284 - 00:33:36.998, Speaker B: How does. Right, so let me. So I want to do this. I want to do this and talk about long codes. So, this is the plan. I see a question. What is the weakest instance of label cover still known to be hard with functions that are at most two to one will still be harnessed? Yes.
00:33:36.998 - 00:34:00.614, Speaker B: This was proved a couple of years ago. If the functions are just two to one. This was the two to two conjecture. So this was done two years ago? Yeah. The one to one we still don't have. But we have various. We have an approach for how to do that.
00:34:00.614 - 00:34:46.213, Speaker B: Just haven't. It hasn't yet. Wasn't finalized yet. Okay, good. So, this is what I'll do next. Let me just give credit, when credit is due, that this long code paradigm that I'm going to present, it started with this work of Belara, Goldreich and Sudan, and then Johan Hastad introduced free analysis here, and this completely revolutionized everything. Let people prove optimal results, and then I'll do the kkbo, which is how to get Maxcat from the uni games conjecture.
00:34:46.213 - 00:35:38.956, Speaker B: Okay, good. So let me maybe define the long code. So, for those of you who listen to my talk, the second talk will may look natural to you. So, this is long code. So, suppose that we want to encode some symbols in an Alphabet. So what we'll do is we'll consider all the functions, we'll take all the functions from sigma to say plus minus one. And this will be done coding.
00:35:38.956 - 00:36:21.204, Speaker B: So instead of writing sigma, we're going to have a gigantic and gigantic array that has, for every little function f from sigma to plus one, plus minus one. So every boolean function on sigma put f of sigma. Okay, and if you remember the second talk there, we had all the quadratic functions and now we'll just consider all possible functions. And this is called the long code because it is the longest binary code possible. So, right, without, without repetitions. So all possible linear combinations of the, of the input bits are not linear. Right.
00:36:21.204 - 00:37:10.114, Speaker B: All possible functions on the input bits are going to be bits in the encoding. This is the long code. Okay, so good. So now that we have this definition, I can show you the reduction. So I want to show a reduction from unique games to Maxcat. Okay, so I'm going to start with writing games. So I have a graph with some Alphabet for the labels on the vertices and some permutations and I want to generate from that a Maxcat instance.
00:37:10.114 - 00:38:13.094, Speaker B: So some other graph. And while I'm searching for the Maxcat, okay, so the convention looks, so I'll consider suppose that the unique games instance, let's say that this is given by g and sigma and PI. Eric and Maxcat. I'll just describe directly what are the vertices and what are the edges. Okay, so Maxcat vertices. So I just need to define for you a graph, that's it. So the vertices are going to be um, for every, for every vertex in, in, um, in v where v is right, g is ve.
00:38:13.094 - 00:39:50.896, Speaker B: So for every vertex of the, the unique game, um, I'm going to have vertex of the maxcad, but also for every, I need two edges, u and v and u prime v and e. Right. So in the unique game you have v, you have u and u prime. These are two vertices that are, that are neighbors of v and therefore the labels, induced labels for vlog. And also for every function f from sigma to plus minus one I'm going to have, oh just a sec, this is going to come a bit later, let me, so for every f and for every function sigma two plus minus one, I'm going to have vertex that I'll denote, denoted f of sigma v. Okay, so really what I want to do is heaven encoding a long code encoding of the label of v. So the u and the u prime will come in a second.
00:39:50.896 - 00:40:21.932, Speaker B: When I describe the edges I got mixed up. Ok, we'll do that in a second. Right. So for every vertex, so I have a vertex v of the uni game and then there is some label sigma v that it's supposed to get. So label in the Alphabet that it's supposed to get. And I'm trying to encode this label. So I'm introducing write encoded with the long code.
00:40:21.932 - 00:41:25.794, Speaker B: So for every f like this, I have a vertex that corresponds to, that is supposed to be applying this function on this one label that is supposed to be for Victoria. So these are my vertices. For every v I have like a block of many many many possible many many vertices. And all of them together correspond to supposed encoding of the label of v. Okay, and remember that in maxca the solution, what we ask the algorithm to do is to find the cut, find a partition of the vertices into two, right? So for each one of those vertices it will tell it to either be on one side of the cut or the other side of the cut. It will give it some boolean value. And this boolean value is supposed to be f of the label of v.
00:41:25.794 - 00:42:27.534, Speaker B: So these are the vertices. Now as to the edges, so let me use the same convention that I had before. I'm going to show you how to pick a random edge. Okay, so here is how you pick a random edge. You take v, then you take this u and u prime. So pick uv in e and then u prime in v in e. So two random edges that touch on v and also pick f.
00:42:27.534 - 00:44:21.794, Speaker B: And also we're going to pick some noise to apply on f. So the noise, so I'm going to denote it squiggly f. So squiggly f of sigma. For every sigma squiggly f of sigma will be f of sigma with probability half plus rho over two rho will be our parameter and minus f of sigma with the remaining probability one minus. Okay, and then the edge, let me erase this. So the edge would be, so the edge is going to be between what's called f of PI e sigma u and f of PI e prime this e and e prime sigma. You okay? This is kind of a bit much to now, but let me kind of break it down for you.
00:44:21.794 - 00:44:48.464, Speaker B: Right. So what I want, right, an edge means that I want those two endpoints to have different boolean assignments. Okay. Okay, good. So let's, let's try to see what's going on here. But I mean, this is it, this is the whole construction. So what I ask there is supposed to be u is supposed to get some assignment which which I denote sigma u.
00:44:48.464 - 00:45:33.460, Speaker B: It's not clear that this assignment actually exists, but, but in the good case where there is a good assignment to the uni games, there is some, some for every u, some sigma u same for u prime, there is sigma u that are that satisfying many of the edges of the, of the uni game. So in each one, so sigma u gives some value for v. So this is what I call PI e of sigma u. Let me switch to a pointer. Right. So sigma u is the value I'm supposed to get. The label that is supposed to be for u, sigma upright, the label.
00:45:33.460 - 00:46:23.694, Speaker B: Suppose that supposedly the label that upright gets, the label that you get induces some label that we supposed to get, the label of u prime induces through PI e prime, the label that v is supposed to get according to u prime. And. Sorry, I minus here and squiggly and. Right, so f of. Right, so this and this are supposed to be the same. And if they are the same, they are the same when you apply the same function on them. So they are supposed to be the same, which means that if you look at this and minus f, they're supposed to be different.
00:46:23.694 - 00:47:18.676, Speaker B: And I'm adding this squiggly, which is kind of checking this robust version of this. This should be true even if I change the function a little, which is not supposed to change much with high probability. Okay. Yeah, so this is a lot to digest if this is the first time that you see all of this, but this is the construction, I'm happy to have ten minutes, so in the interest of time, I won't repeat this explanation. Right, maybe. Right. This and this are supposed to be the same in the variety.
00:47:18.676 - 00:47:49.904, Speaker B: If the unigame thing is the same, so f applied on both is supposed to be the same. So f and f are supposed to be different and squiggly shouldn't change much. If f squiggly is the same as f with good probability. Okay. Ha. I'm running out of time. So this is the construction.
00:47:49.904 - 00:48:25.494, Speaker B: Wow. I have very, very little time left. Okay, so I want to sketch for you the proof why this reduction works. Questions? The edge is there. If that condition holds anotherwise. No, the edge. The edge is here.
00:48:25.494 - 00:48:59.184, Speaker B: The edge is here, always between the vertex that is labeled this way and the vertex that is labeled this way. Okay. And if there are ways, if there are sigma u's that are good, then it all makes sense. And if they're not, it makes less sense. More questions. Will one side of the cut have all satisfied edges up to noise and the other, all unsatisfied. No, no.
00:48:59.184 - 00:49:22.684, Speaker B: So this is. Right. An edge. An edge means that they have different down, different sides of the cut. So in the good case, most of the edges are on different sides of the cut. Okay. There is inequality between those assignments.
00:49:22.684 - 00:50:04.684, Speaker B: Maybe I'll just. Wow. I won't actually have time to do much, but let me maybe go back here. So I described to you this reduction. I haven't analyzed it. I could analyze it if I had ten more minutes. It's actually, the interesting thing about the analysis is that it relies on gaussian isoparimetry.
00:50:04.684 - 00:51:10.386, Speaker B: So the understanding is that when we check this, so you can look at all the vertices that end up on one side of the, the cut, right? And you can think of this as a set. And here the number of edges that cross the cut is exactly the surface area of this set. And saying that. Right, right. And this is a set of function, about half. So the interesting thing is that the analysis here for why this construction works just follows from gaussian isoform. There is a nice theorem that shows that this.
00:51:10.386 - 00:52:24.498, Speaker B: Right, so the construction was purely on the boolean hypercube, but it turns out that the quantities that we analyze without loss of generality, we can analyze them in gaussian space. And the analysis completely follows from this and from gaussian ISO palimetry. Somehow, gaussian isopolymetry gave us this production that we were looking for. And had I had even more time, I would have told you. Right, so the interesting thing here is that, right, so, proximity of Maxcat, which is a computational problem, which lives in computational world, is basically equivalent to gaussian isopolymetry. Both. The Gomans Williamson algorithm is an algorithm that uses Gaussian as a perimetry to partition those vectors.
00:52:24.498 - 00:52:54.712, Speaker B: And also the Gaussian ISO polymetry is what proves optimality. And this is kind of in leaves in geometry and geometric wall. Dana, can I ask you a quick question? Yeah. Are you claiming that you can derive gaussian isoperimetry from approxim inapproxibility of maxcut or approximately of Maxcut? So what I said was that we prove inapproximability of Maxcat from gaussian isopolymetry. Right. But your arrow here goes the other way, too. No.
00:52:54.712 - 00:53:11.238, Speaker B: So this is because the gaussian isopilimetry gives us the best algorithm. And optimality of gaussian isopilimetry gives us the optimality of the approximation algorithm. So in this sense. Right, thanks. I now understand the double error. All right. Okay.
00:53:11.238 - 00:54:13.592, Speaker B: Maybe, maybe it would be, I mean, maybe it would be worth it to take like an extra, you know, 510 minutes just to give it a little bit more of a flavor. So what I'm thinking is that I can kind of talk a bit about the high level picture for the few minutes that I have left, until twelve, until my 12th. And then I want to do my original plan, which is to actually show you the analysis and the connection to isopolymetry. And whoever wants to stay for that, can stay for that, and it will be another 1015 minutes at the most. Ok, but I want to, let's talk a bit about the high level picture. So, somehow, approximation for Maxcat is completely given by gaussian isopilimetry, both the algorithm and the hardness, which is very remarkable because one lives in computational world and the other lives in geometric world. This is true only assuming the UGC.
00:54:13.592 - 00:54:57.438, Speaker B: For now, the Uni games conjecture. It's a conjecture. It's like this part here that we don't know how to prove for now. There is another interesting point I wanted to make, is that still, these sort of reductions are interesting, just from the geometric perspective. So, in the time I don't have. But I'll take, I wanted to also mention that, right, you can continue living in. Right, even if you don't assume the UGC, even if you don't care about the geometric world, just in geometry world.
00:54:57.438 - 00:56:00.742, Speaker B: Right. This is an interesting, this is an interesting thing, that you have a reduction from one problem to another problem. And this sort of reduction was used to show that l two squared cannot be embedded with constant distortion in l one, l two squared with, as a metric, assuming that it satisfies the triangle inequality. And this was just proved by taking kind of looking at the geometric problem that is associated here, and seeing, um, and improving optimality here, and then using these sort of reductions. So there, it's not Maxcat, it's a related problem called spouses cut. Um, and, and getting something here, then the sparse cut is basically equivalent to this l two squared to l one embeddability. Um.
00:56:00.742 - 00:56:34.944, Speaker B: Right. So, so even if you continue, if you just in the geometric world, you don't need the UGC. You can actually get very interesting results just from, from this sort of stuff. Okay, so this is, this is the plan for the 15 minutes extra that I'm going to take. I'll describe to you this, I'll show you the analysis of this, and then I'll talk to you about this non embedded ability result. Okay, good. So, right, so there is this gather town and whoever wants to stop now can stop now.
00:56:34.944 - 00:57:48.354, Speaker B: But yeah, whoever wants to see this analysis of unique games to Maxcat reduction, I'm going to describe that. Okay? So I mean, let me, let me kind of first switch to more familiar terms and I'll do this for every, okay, so for every vertex in the uni game, really what I have is a function, let me denote it, fv. It's a function from plus minus one to the k to plus minus one. So let's see what we had before. This is the construction as we had before. So this is a bit of a cumbersome notation. So I want to kind of present this in a more familiar way.
00:57:48.354 - 00:59:21.564, Speaker B: Right? So it's really right. So I have the, all those, all those, right, I'm supposedly evaluating all possible functions on the supposed label of the vertex, okay? Now notice that the function f from sigma to plus minus one is exactly equivalent to a vector that has here all the sigmas and here has f of sigma, right? So this is the truth table, right? I can identify a function with its truth table. It's the, it's the same thing, right? And really what I, what I want this long code encoding, I kind of tell the person finding the labels, well, there is some label that you're trying to encode the label to this vertex. So I just want you to every function, just give me what the function does on sigma. This was my definition of the long code for every function. For every vector. Just tell me how it evaluates on the label that you think that this vertex is supposed to get.
00:59:21.564 - 01:00:21.802, Speaker B: So such functions are known as dictators. So this is supposedly long code is the same as what we call a dictator. So supposedly we take, right, we take f and we give f of sigma and we can think of it as a function fv, right. Takes this vector and gives this the sigma location, right? So this is the long code, okay? Right. Every function is supposed to give them, get the evaluation on sigma. It's the same as, right. The function is given by the truth table.
01:00:21.802 - 01:02:33.324, Speaker B: You just want one location from this truth table. Okay, so this is what kind of I prescribe, this is what I prescribe, this is what I ask kind of the labeler to do, or the algorithm for Maxcat for every, right? So for every v, give me, give me, for every f, give me f of, of the assignment to the label of, to v. But in fact, the algorithm may not give me a dictator. It may give me a general function, a general boolean function. Okay, so this is what the, what the, we ask the algorithm, the cat algorithm is expected to do, right? So this is for, take the actual label sigma v for v, and tell me for every function, what's the valuation of the function on, on this label, right, which corresponds to this dictator function that takes a function and just gives you one location in the truth table. But then there is what the algorithm, what the algorithm may do, what the cut gives, it gives some boolean function, right? Because the cut is some boolean function on the vertices, right? So for every v, for every, right, each f is like, is like a vector like this. Each f is a vector like this and it gives it some boolean value.
01:02:33.324 - 01:04:55.464, Speaker B: Okay, good. So let's just write with this terminology. So yeah, every time one does this flip, right, it's kind of hard to continue following online, but the hope is that it's recorded. So it's possible to kind of pause and understand kind of this conceptual step that we just made. So the situation is, I have my uni game, right? So I have all those vertices and in the Maxcat each vertex is replaced with a bunch of, with many other vertices that represent an encoding of it. And you can think of the labeling to those vertices as a function fv, right? So for every v you have those new vertices and you have this new function fv that is supposed to encode an assignment to v. If fv were a dictator, it would have encoded an assignment to v, but otherwise, I mean it's some function, okay, so with this notation, let me try to write what's the test that we do? So the test that we do, we can write say, the probability that the test accepts, right? So really what we, okay, yeah, it's a bit, the important term here is that we look at fv of f and f v of f squiggly and for random f.
01:04:55.464 - 01:06:11.924, Speaker B: And we asked what's the correlation between, between those two functions? Once when we apply it on f and once when we apply it, oh, sorry, let me make one more definition before I do this. Let's define fv as follows. Fv of f, it takes a random edge uv and e. Let's call it e. And then it looks at the vertex that this is its name, right? So we take a random neighbor v, call it u, and then there is supposed to be some label for u, and then the label for u induces some label for v. And then we want to evaluate, evaluate f on that label. So this is what we're trying to do.
01:06:11.924 - 01:07:27.634, Speaker B: And then we can write the probability that, or the fraction of edges cut, we can write it as half minus f sigma v fv, f f v f squiggly. Okay, so this is, right, so this is how we defined edges. We looked at, at f and we looked at squiggly f. And, right, one comes from some u and the other comes from some u prime. And, right, and writing in the language, right. So we could have written it in the language of how many edges are cut. And this is how it's written in the language of correlation between the functions.
01:07:27.634 - 01:09:07.588, Speaker B: And then this actually looks much more familiar. Right? Because really what we check here is what's the noise stability? Right. So this stem here is the noise stability of fv. So this is what we achieved with the construction I showed you that the fraction of edges cut is exactly, exactly relates to the noise stability of this function that we get. Okay, so this is how ISO perimetry comes in. Okay, let me write down the analysis just for completeness. Okay, so if, um, the fraction of edges cut is at least arc cosine rho divided by PI plus epsilon, which is kind of, if you do this calculation of Goemons Williamson.
01:09:07.588 - 01:10:06.674, Speaker B: Right. This is what we are trying to do to prove that if this happens, then those FV's must be in some way a dictator. This is what we're trying to do. If there are many edges cut, if there is a lot of noise stability of those FV's, then those FV's must be like dictators. And because they're kind of like dictators, they actually correspond to a labeling for the unique game. So this is what we're trying to prove. So just by averaging then for at least epsilon over two fraction of the v, we get, let me call it asterisk, that squiggly fvfv.
01:10:06.674 - 01:10:47.264, Speaker B: Squiggly f here is at most one minus two over PI cosine of rho minus epsilon. Okay? So, right. If many, many edges are cut, then for many, for many those of those, right, by average, in many of those vertices, we get noise stable. We get noise stable. This should be noise stable. Right. It should be the other way.
01:10:47.264 - 01:11:46.404, Speaker B: We get noise stable functions. Okay, now the interesting thing. So the analysis proceeds in two steps. One is what we call the invariance principle. So this was beautiful theorem by Mussolo Donnell. And that says that this expression, the noise stability when f is over the boolean hypercube plus minus one to the k to plus minus one, this is the same as if you evaluated it in gaussian space. Sorry.
01:11:46.404 - 01:12:29.620, Speaker B: F in plus minus one to the k. This is the same as f is gaussian and the same expression. Well, the squiggly becomes gaussian noise. And then we can use gaussian ISO perimetry. So this is a theorem of Borrell to say. Well, I mean, it just. This is impossible.
01:12:29.620 - 01:13:22.064, Speaker B: You don't have functions that are. Oh, I'm sorry. This is true. If the influence of every index on the function, the low degree part of the function, is always small, then you have this. If all the influences, if we have a function where all the influences are small, then the situation hypocube is the same as the situation of a gaussian. But then. Right, and the issue is that over gaussian, it can't be.
01:13:22.064 - 01:14:28.844, Speaker B: So, gaussian isopolymetry tells us that, that asterisks cannot happen in gaussian world. Gaussian. Right. So combining the two, the only way that asterisks can happen is if there exists some sigma in sigma. There exists something here where the influence is large. And this is going to be the label that we're going to associate with v. Right? So hence, asterisks implies some sigma with large influence, which this will be.
01:14:28.844 - 01:15:39.884, Speaker B: This will be the label two v. This is the analysis. I see that I have some questions. Oh, no questions were answered. This is it, right? I mean, this is at least the scale of this kkmo and mu analysis of the construction. I mean, so really what it is saying is that when I define for you this kind of weird construction, really what I did is make the Maxcat problem into this isopilimetric problem on those functions associated with, with each vertex. And what we showed here was that if I have many edges cut, it must be because this function is actually associated with a dictator.
01:15:39.884 - 01:16:09.424, Speaker B: Okay, I have a lot more to say about the inverence principle and stuff, but I won't say it for now because I'm running out of time. Yeah, so this is what I wanted to say about this. Sorry. Right. About this part. Right. So how Maxcat is really all about gaussian ISO polymetry.
01:16:09.424 - 01:16:23.994, Speaker B: I could tell you about this non embeddability of l two squared in l one, but maybe the panelists can tell me. I mean, I already took the 15 minutes.
01:16:26.254 - 01:16:29.834, Speaker A: Maybe we can stop here because there is another talk soon.
01:16:30.414 - 01:16:33.034, Speaker B: Okay, good. So this is it for today.
01:16:34.574 - 01:17:02.984, Speaker A: Okay, but is there a quick question that we can take? Okay, I guess we can all start this again with the video in lower pace. Thank Dana for the lecture and we will come back at 1030. There is eleven minutes break. You can hand out in the gather time. Thanks, Dana.
01:17:03.404 - 01:17:03.924, Speaker B: Thanks, Dana.
