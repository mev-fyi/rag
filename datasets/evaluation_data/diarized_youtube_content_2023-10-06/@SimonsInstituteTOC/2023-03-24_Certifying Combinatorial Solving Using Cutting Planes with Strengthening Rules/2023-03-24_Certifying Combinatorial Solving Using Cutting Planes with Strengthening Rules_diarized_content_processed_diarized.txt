00:00:01.080 - 00:00:29.054, Speaker A: So welcome MLB to day four of this workshop. The first talk is by Yakub so that there is no strength. Thank you. Thank you for the invitation to speak at this workshop. So unfortunately, it seems like I brought the wrong set of slides. I understood now that this is a proof complexity workshop, and here I am talking about common tort optimization. So let me very, very quickly try to make some connections.
00:00:29.054 - 00:01:38.664, Speaker A: So, just as proof complexity, combinatorial optimization is a rich and beautiful area of mathematics and computer science. Even maybe slightly more than proof complexity, combinatorial optimization has had an impact outside of math and computer science. Probably the reason that most of you are here is that your airline, the airline company, which you were traveling around some kind of combinatorial solver to figure out the plane schedule. If this talk gets boring so that you check your laptop or cell phone, then someone has been running a combinatorial solver, verifying the hardware. Another interesting application of combinatorial solving just for motivational purposes. If, if at some later point in time, it turns out that you need a kidney transplant and actually a sort of combinatorial optimization solver will try to figure out how to match recipients and donors to maximize the number of people who survive and get kidneys. So there are lots and lots of applications of combinatorial solving and optimization.
00:01:38.664 - 00:02:37.654, Speaker A: As we all know, these problems are typically very hard. They're np complete, sometimes np hard. So sort of as a proof complexity theorist, this means that I already know that this is exponentially hard. There's no hope. Unfortunately, the practitioners didn't pay any attention to this. They insist on solving these problems in pretty much in linear time, using a lot of different combinatorial optimization paradigms, such as Boolean satisfiability solving, constraint programming, mixed linear programming, integer linear programming, satisfiability, modular theory. So there are amazing algorithms that solve many, many challenging combinatorial solving and optimization problems out there in the real world and with like millions of variables, with the one big caveat that how do we know that they give the correct results? So, in fact, we know that they don't.
00:02:37.654 - 00:03:37.374, Speaker A: It's fairly well established in the literature that even the very best, most mature commercial solvers sometimes get the answer wrong, and we don't know when. Obviously, if they output the solution, you should be able to check feasibility, although solvers sometimes even get that wrong. So they output solutions that you can see are not solutions to your constraints. But how do you check if a Sat solver runs for a few hours and says that your formula is unsatisfiable? How do you actually know that that is true? Or if your combinatorial solver that was distributing kidneys between donors and recipients says that this is the optimal solution, then we actually really want to know that the solution is optimal. How do we know that this is true? Like your airline company, if the solution wasn't optimal, then you're just losing a little bit of money. But here we're losing lives and non optimal solutions are just completely unacceptable. And yet we're in a setting where even the best combinatorial solvers do get their answers wrong from time to time.
00:03:37.374 - 00:04:13.444, Speaker A: So what do we do on this? Well, one thing you, obviously you do, you do extensive software testing to try to weed out the bugs. The only problem is this doesn't work. Like empirically, it hasn't worked. There's another mathematical approach that is very principled, very attractive. You write down exactly how your solver is supposed to work. You give a mathematical specification, and then you use like an interactive theorem proverbs, something to prove that what you have implemented adheres to this formal specification. And now you have a mathematical proof that your software is correct.
00:04:13.444 - 00:05:15.504, Speaker A: And there are tools that can do this, like all the way down to machine code level. One problem here is that modern combinatorial solvers are far too complicated for these techniques to work. They just don't scale. There's no chance. So formal verification would be very nice, but it just doesn't work in practice. So what seems to the most promising approach, or what I'm arguing is the most promising approach is to make solvers certifying or to make them use proof logging, which means that we're asking them now to give not only an answer to the problem, but they should also provide a proof, a proof that this answer is correct. And now what this means is this solver algorithm should not just, when we run it, we get an answer, but we also get a machine verifiable proof, and we should be able to feed the input, the answer and the proof to a proof checker.
00:05:15.504 - 00:05:56.834, Speaker A: And then we check what the proof checker says. And if it says that everything is okay, then we can trust that the solution is okay or the answer is okay. And otherwise we know that there's an error. And now you see that this proof checker, this is just a standard like, you know, cook recall, proof verify. So now we're actually getting close to proof complexity, but from a slightly different direction than what is usually the case. So what do we want from proofs used in practice? Well, we. So one thing that it's important to understand the scenarios here.
00:05:56.834 - 00:06:57.508, Speaker A: One thing that what's not going to happen is that we design a proof system and then we start to rebuild solvers from scratch for like constraint programming or satisfiability, modular theory or something. There are like decades of very, very sophisticated coding has gone into this and we'll have no chance to replicate this work. So we have to take the solvers as is, and that means that the proof system that we have will have to be very efficient, very concise. If this is going to fly, it should be easy to basically take an existing state of the art algorithm and then just add concise explanations of what the solver is doing. And our proof system should be strong enough to express the kind of sophisticated reasoning that the modern solver is doing. It has to be a strong, concise, very expressive proof system. On the other hand, the whole point why we're doing this is that we want proofs to be dead easy.
00:06:57.508 - 00:08:10.314, Speaker A: I mean the whole point with this workflow is that the solver is a very, very complicated piece of software that we don't understand. But this checking process, the proof, should be so simple that checking the proof is more or less a triviality. A good undergrad should in principle be able to code up a proof checker so that once we know that the proof checks out, we're really confident that everything is okay. And that means in particular that whatever proof system we design, it shouldn't be like a one time hack that is tied very specifically to this particular solver. So that I have to know how the solver works, I somehow have to trust that it's correctly implemented. It should be an abstract proof system for which I can check the proofs without necessarily knowing what the solver is doing. Okay, so, and this is to highlight that there's like a clear conflict here between, on the one hand, the proof system, I want it to be very, very expressive, and on the other hand because, because I want to support efficient proof logging, like the process when the solver generates the proof.
00:08:10.314 - 00:09:04.674, Speaker A: But on the other hand, in order to make verification trustworthy, I want the proof to be very simple. So it's sort of there you can ask like are we making any progress? It seems like we're having a very, we have two completely opposite demands on our proof system. So are we, you know, will this work? Well, there's one thing that's in our favor. Potentially we could get this to work, because now we're not trying to prove the whole complicated solver correct. We are somehow just trying to abstract away a concrete proof and verify that proof. And maybe that's a simpler task. And in fact, if we get this to work, then at the end of the day, we want to use formal verification, but not for the solver, but for the proof checker, because the proof checker should be simple enough that you can actually make the proof checker formally verified.
00:09:04.674 - 00:09:17.974, Speaker A: So you have no idea what the solver is doing. But if the proof turns out to be okay, then you're fairly confident that the proof checker is formally verified. And so now we can trust the result. Was there a question over there isn't like the primal dual methods, don't they give you this?
00:09:19.114 - 00:09:20.554, Speaker B: Like if you have a primal solution.
00:09:20.634 - 00:10:18.860, Speaker A: And a dual matching? I mean, if I was doing, for instance, linear programming, then sure, but if we're doing like zero one interdilinear programming, the shortage there's no. Yeah, but you're absolutely right that something like that would be great, but we don't have it right for discrete. But now the point is that here is a point where proof complexity can contribute, because suddenly everything that is causing trouble for us in our daily lives is now suddenly an advantage. We're struggling to prove lower bounds because we study like, you know, proof systems like say, polynomial calculus or cutting place or bounded, that trigger it's hard to prove lower bounds because these proof systems are like, can do unexpected smart things. But now we can turn all of this in our favor. Now we want the proof systems to be strong. We want them to be able to have short ways of certifying complex reasoning.
00:10:18.860 - 00:11:17.128, Speaker A: So if we have struggled to prove lower bounds against the proof system and understood that it can do lots of strong things, then now we could use this in our favor to design a method to certify correctness of combinatorial solver reason. And just to say that if we can do this, this is much more about, much more than just certifying the correctness of the output of a certifying solver. It gives you something that formal verification cannot give you. Suppose, for instance, that you have a formally verified piece of software and it runs on a supercomputing cluster and you have a hardware failure, which you will have on a large scale computation. Now, even if your software is formally correct, I mean, once the node goes down, you have no idea what will happen. But if you have proof generation, then the proof will be wrong. So if you don't get a correct proof, then you see that there's something wrong in the computation.
00:11:17.128 - 00:12:34.210, Speaker A: And if you do get a correct proof, then you know that the fact that this node went down did apparently not affect the end result. One thing that has been the case in all, pretty much all papers so far, actually implementing proof logging in state of the art solvers, is that even in extremely well tested software, you discover new bugs. And the way you discover this is you have a bunch of test cases, they all pass. So you get the right result and then you switch on proof logging and you see that the solver is taking shortcuts in order to reach the correct answer. Because you can see that, for instance, some case analysis and the proof, the solver claims that, okay, I've done an exhaustive analysis of all cases, now I'm backtracking. And the proof checker says, wait a minute, isn't this this weird corner case, that it's true, it will never ever happen, but I don't see that you considered it and such errors will be all over the proof, even though like the output is always correct, because this very weird, especially case never triggers. Now you can also potentially use the proofs generated by combinatorial solvers to understand that a much sort of finer level of granularity than before, what the solver is actually doing.
00:12:34.210 - 00:13:28.844, Speaker A: How is it reasoning? And if you analyze the proof, you can for instance trim the proofs and see which part of the proofs were really used. Well, that can give you information about which kind of reasoning seems to be more or less important, and maybe also give you ideas for how to improve the reasoning in your solver, maybe for new heuristics and stuff. So this is like the proofs, you can mine them for information that will maybe drive improvements in algorithm development. And almost by definition, once you have run your solver on a problem and you have not only the answer but also proof logic, then at any given time in the future you can check that the solution to this problem is indeed what it is claimed to be. And you don't even need the original solver because you have the proof. And you can run a proof check which in the end you could also envision like a future where you use this kind of thing. Also, a proof will tend to explain.
00:13:28.844 - 00:14:31.100, Speaker A: It will tend to certify not only that an answer is optimal, for instance, but why an answer is optimal. So you can envision that you will combine this with when you solve a problem to optimality, the proof will not only certify that this solution is optimal, but it will tell you, because of these constraints and because they combine in a certain way, you cannot get a better solution. And there are a number of applications, for instance, in artificial intelligence, where what people are asking for today is, I don't just want the answer, I want to know why the answer was what it was. And it's, this seems to be something that proof of. So this is trying to pitch that this is like an interesting technology, if we could do it, and that it's basically, all of this is a proof complexity question, but from a constructive viewpoint. So does this exist? Well, it turns out yes, it does exist in the Boolean satisfiability community. There are lots of different proof logging formats in the SAP competition.
00:14:31.100 - 00:15:41.718, Speaker A: I think going back till 2013 or 2014, proof logging is actually compulsory in the main track. If you want to take part and win the main track of the competition, you have to give machine verifiable proofs if you claim that a formula is unsatisfied. But this success story has stayed with SAT. We don't have anything similar for, say, constraint programming or for SAT based optimization or mixed into your linear programming. And in fact, even for SAT, a problem these days is that if you want to use the most advanced techniques, like if you want to use, if you're solving a cryptographic instance, you want to be able to do parity reasoning, or if you have a highly symmetric instance and you want to do symmetry breaking, then it's not known in these proof formats how to do proofs that certify the correctness of breaking symmetries, for instance. So we have a somewhat paradoxical situation today in the SAT competition where you have to switch off techniques that we know give an exponential improvement in reasoning power, and you have to switch them off because you can't generate efficient proofs. This is not entirely satisfactory.
00:15:41.718 - 00:16:08.330, Speaker A: Is that a question? Yeah. So, two questions. So, first of all, in practice, when test servers are used, is proof hovering actually used? I think it depends. The point is, it should be in principle. So proof logging should have extremely small overhead. So it should not be an extra cost to run your solver with proof logging. Whether they actually do it, that intel or something, I don't know.
00:16:08.330 - 00:16:22.084, Speaker A: I would have to ask. But in principle, basically, I think also if you run your solution proof logging for thousands and thousands of instances, and the proofs always work out, then at some point you get confident.
00:16:23.064 - 00:16:30.968, Speaker B: Yeah, I mean, the storage requirements are probably the issue in some of the long roots. Right? You coupled with that?
00:16:31.136 - 00:16:52.294, Speaker A: Yeah, I mean, for applied problems, I think proofs tend to be not too large. I mean, proof logging has also been used in, I'm not mentioning this in the slide, but proof logging has also been used to solve, for instance, problems in extreme combinatorics. And these are typically very, very hard problems where what the SATs owners do is a sophisticated form of brute force, and then you get gigantic proofs.
00:16:54.754 - 00:16:59.294, Speaker B: Doesn't it happen that actually checking the proof takes longer than the solver ran?
00:16:59.834 - 00:17:32.900, Speaker A: Checking will typically take longer, yeah. So like within a factor of five or ten longer. And the reason for this is sort of the way this is going to work. And this is. Yeah, so this is a good point. And it is different from when we, in a TCS setting, when we talk about proofs, we usually think about maybe probabilistically checkable proofs, where you check them like in logarithmic time or something, but then you sort of, in that setting, you know what the answer is. And you can use a lot of nifty mathematics to build very, very sophisticated proofs.
00:17:32.900 - 00:18:04.544, Speaker A: Here. You should think of, the solver doesn't even know what the answer is, but it has to build the proof in an online fashion on the fly. So anything it does, it's going to write down, okay, now I did this, now I did that. I still have no idea whether I'm satisfiable or unsatisfiable, but let me tell you what I'm doing. And then after 1 hour, you suddenly decide, oh, it's unsatisfiable. Okay, then and there, the notes you wrote down should be a value proof. You don't have time to go back and sort of patch it up and do something nice.
00:18:04.544 - 00:18:16.814, Speaker A: So typically checking the proof will actually take longer than generating the proof, but it should be like a small component. Yes.
00:18:17.474 - 00:18:18.850, Speaker B: Conceptual question.
00:18:19.002 - 00:18:23.074, Speaker A: Some 25 years ago, there was a lot of excitement about the area of.
00:18:23.114 - 00:18:32.174, Speaker B: Proof getting code that George Nekulak had developed. Used to be a professor here. How conceptually, what's the relationship with any.
00:18:32.994 - 00:19:07.496, Speaker A: I guess that would be closer to formal verification. I think that like the general problem with these techniques is that they're extremely attractive, but they don't scale, I think to like real world applications. I'm not an expert on proof terrain code. I know that for formal verification, if you talk to the experts in formal verification who actually work on this, they just said it. People have been trying to make certified versions of baby SaT solvers. It's highly sophisticated, very, very interesting work. In terms of performance, it's nowhere close.
00:19:07.496 - 00:19:08.484, Speaker A: Thank you.
00:19:10.704 - 00:19:29.632, Speaker B: My question was, when you first output, when you output the first proof, you said that it takes longer to verify it than maybe generate. But after you do the first verification of the proof, my understanding is that sometimes you get like you clean up.
00:19:29.648 - 00:19:58.044, Speaker A: The proof as well, and so maybe the you could clean up the proof and shipment. I'm not going to talk about that. But you're absolutely right that like I can work on the proof. I could see what part of the proof do I actually need verified the second time? Yeah, yeah, yeah, sure. It varies, but typically you can trim proofs a lot. So if you would store the proof for posterity, then you would probably try to trim it significantly on this. So, where are we? Okay, so that's the current situation.
00:19:58.044 - 00:20:51.780, Speaker A: Like proof logging would be wonderful. It would have all of these amazing applications. We know in SAts solving that it partially works, but not beyond sat solving, and not even for advanced sat solving techniques. Okay, so why are we here today? So what I'm going to tell you about is that if you take basically what the SATs only people have been doing, but you generalize it to zero one integer linear inequalities instead of clauses, and instead of the resolution based proof, you switch to cutting planes and you extend it in a clever way with suitable rules. Then you can do not only all of Sat solving, but it seems you actually get a general purpose proof system that so far has been able to do anything in combinatorial optimization that we have looked at. So, this is what I want to tell you about. So, I'll start by quickly reviewing a little bit about Sat solving and making the connection between CDCL and resolution, which I think many of us know.
00:20:51.780 - 00:21:35.192, Speaker A: But I'll try to do it quickly and tell you about the extension rules that are used for sat proof logging. And then I'll tell you about more advanced SAT techniques and how sort of cutting planes comes into the picture. And hopefully tell you a little bit. I'll be able to tell you a bit, little bit about more advanced techniques such as constraint programming, symmetry breaking, I hope. I definitely want to define. Also, since it's a proof complexity workshop, I want to define the formal proof system in the cook recalcance that comes out of this. And that gives rise to some interesting questions like how strong is this proof system? Actually, do we need all of the strength of this proof system? So, just to make sure we're on the same page, we're starting with the SAT problem.
00:21:35.192 - 00:22:28.044, Speaker A: So we see an f formula, conduction of clauses. Every clause is a disjunction of literals, that is variables or negated variables. Bar is negation assets always ask to decide whether a formula like this is satisfiable or not. So how, what do we do? Well, there's a DPLL method from the sixties which is basically just an intelligent form of backtracking. And then what has driven the revolution in SAT solving is conflict driven clause learning, where when you want to backtrack, not only do you backtrack, but you compute a new clause that is implied by your formula, you add it, and it will cut off parts of the search space going forward. And you also use the conflict analysis to guide a lot of other heuristics. We don't care about this for proof logging purposes.
00:22:28.044 - 00:23:07.576, Speaker A: All we need to do is to justify backtracking and to justify when we're adding new clauses, why it's actually valid to add these new clauses. So this is a general thing, like for proof logging, how the SAT solver or another solver figured something out, what heuristics it used. It's sort of proof logging is oblivious to that. We just need to certify the correctness of the conclusions found. We don't care so much how these conclusions were found. Okay, so quite quickly, if we have a formula like this, what would a SAS solver do? Well, it would start by assigning some variable, maybe setting p to false. And then a lot of the time a SAS solver does unit propagation.
00:23:07.576 - 00:23:34.006, Speaker A: So it looks at clauses. Here, for instance, if p is set to false, then you see that the first clause says that, well, u also has to be false, or else I've immediately violated a clause. So that's called unit propagation. And the SAT solver will write down that it propagates you because of the reason clause p or not you. And you always propagate until saturation. Now there are no further propagations. So now the SAT solver maybe decides to set q to false.
00:23:34.006 - 00:24:11.642, Speaker A: And then this second clause propagates that r has to be true. The third clause propagates that w has to be true. Again, there are no further propagations. So now maybe we decide on x being false. And now, since, and the sat story is keeping like a trail of, which is an ordered list of the assignments made so far, now, u is false, x is false. So this clause propagates that y has to be true. So I write that down with not with u or x or y as the reason clause, and then this clause, x or not y or z propagates that z has to be true.
00:24:11.642 - 00:24:36.054, Speaker A: But now we have a conflict because we're violating not y or not. Back in the sixties, we would just say, oh, apparently x was a bad decision. Let me flip it. Now, a modern sat solver would instead say, let me analyze. I made three decisions and then I made some propagations. I have three decision levels. I'll go over what happened at the last decision level and try to figure out what went wrong.
00:24:36.054 - 00:25:16.508, Speaker A: And here for proof complexity theorists, what will happen is just we'll do a very simple resolution proof. We'll do a trivial resolution proof. So we start by the conflict clause and the last reason clause. They disagree on z. So just by removing said and not z and taking the union, the rest of the literals, you realize that any satisfying assignment to these two clauses has to satisfy X or not Y. And then we can look at the next reason clause and this clause that we have derived, and they disagree on Y. And it follows by the same type of analysis that any satisfying assignment to these two clauses will also have to satisfy U or X.
00:25:16.508 - 00:26:01.218, Speaker A: And now I'm doing this very quickly, but at this point, the SAT solver will say, aha, I only have one variable left from the last decision level. Now this is a good place to stop. I'll add this clause to my formula and I'll backtrack. Okay? In fact, it will back jump. It will say, it will look at how much can I trim the trail while still having this clause propagating? And it turns out that for this conflict, this decision on q and all the propagations were completely immaterial. So I'll trim them away, but I'll keep the decision on p and the propagation on u, and observe that given those two assignments, this new clause propagates x to true. And now I'm back again.
00:26:01.218 - 00:26:25.244, Speaker A: I'll do exactly the same cycle of propagations and decisions. Except in this case, I don't need to make any further decision. I get a propagation on not x or z, and then I immediately get a new conflict. The conflict analysis here is very simple. I just resolve these two clauses. I get not x. I've learned now that in any satisfying assignment, x has to be false, and this is true regardless of anything else.
00:26:25.244 - 00:26:51.538, Speaker A: So now the solver trims the whole tray, starts over from the top, and says, I know x is false. Well then this clause implies that u has to be true. Well then this clause implies that p has to be true. But that's a conflict with this clause. And now note that I haven't made any assumptions, no decisions. So now I know that the formula is in fact, unsatisfiable. One way of seeing this typically here the solvers would actually terminate.
00:26:51.538 - 00:27:45.804, Speaker A: I could run one final conflict analysis, and I'll see that I actually derive the empty clause. Okay, so if we want to do, and from this description, you can already see, like how would I do proof logging? We'll just look at what the conflict analysis is doing. It's using the resolution proof system that from C or X and D, or not x, I can derive C or D. And so basically the main loop in conflict driven clause learning is just generating resolution proof. So this is ignoring tree and in processing we'll get there in a minute. But sort of morally, most of what a SAT solver does is actually captured by the resolution rule. So if I wanted to do proof logging for the main loop of a CDCL solver, I can just take this execution, take the conflict analysis, and string them together and write down this resolution proof.
00:27:45.804 - 00:28:29.774, Speaker A: And this would be my proof loop. And this is not quite the way it's done. I don't, for reasons of time, I won't go into the details, but in principle, like a SAT solver could do a proof logging by writing down a proof which is clearly, efficiently verifiable. And you see, the proof will scale with the running time, and it will be again, you can see also verification time will scale with the execution time, but the scaling will be like. So that's basic conflict driven closed learning. Now, satsolders will do more than this. Sometimes they will say, oh, I want to keep track of the value of the conjunction of x and y.
00:28:29.774 - 00:29:15.294, Speaker A: I want to introduce a fresh variable that actually takes the value of this conjunction. Now, the way you could do this is I can just, if you think about it, I could add these three clauses to my formula. Is this okay? Yes, if a is fresh, this should be okay, because, you know, given an assignment to x and y, a will just propagate to whatever value it should take. So we want, if we should be able to do proof logging for SaT solvers, then we need a rule that supports something like this. And it's clear that the resolution can't just do it for fundamental reasons. I mean, these resolution can only derive implied clauses, and these clauses are not implied. But of course, we know that the extended resolution proof system can do this.
00:29:15.294 - 00:29:59.184, Speaker A: So it turns out that I don't want to use extended resolution. I want to use a slightly nicer rule. There are a number of different rules in the literature for this for deriving so called redundant clauses. So what's a redundant clause? A clause is redundant if adding it doesn't change the formula from satisfiable to unsatisfiable. That's like what at least what sad people mean when they say redundant. You can discuss whether this is good terminology or not, but this is like established term in the literature. And all of these different rules are captured by something that we call redundance based strengthening, building on previous work by by bus and top end.
00:29:59.184 - 00:30:50.034, Speaker A: So all of these different rat rules and propagation redundancy rules are basically special cases of the following rule. And here comes like the first interesting proof, complexity rule. So I claim that we should be able to add a clause c to a formula f if and only if I can find a substitution omega, which I will think of as a witness. So a substitution, it's mapping variables either to truth values or to other literals for which the following implication holds. So f and not c should imply f and c under this substitution omega. If this holds, and I will need some kind of efficiently verifiable way of checking this implication. If this holds, then you should allow me to add c, is what I'm saying.
00:30:50.034 - 00:31:27.894, Speaker A: And if you see this for the first time, it might not be obvious why. So let me try to very quickly sketch, like, the interesting direction. So, which also tells us how we should think about omega. So what we're worried about is that we have an assignment alpha that satisfies f, but that this solution is invalidated when I add c. That is sort of the bad case. Okay, but if that is the case, then alpha satisfies f and not c. So in that case, by the fact that we have this verifiable implication, alpha satisfies f and c restricted by omega.
00:31:27.894 - 00:31:41.894, Speaker A: And now just by unpacking this definition, this means that alpha, composed with omega satisfies both f and c. I'm just sort of doing this very quickly, but I'm just unpacking the definition of what a restriction means.
00:31:42.434 - 00:31:43.854, Speaker B: How's your composition?
00:31:44.274 - 00:32:25.014, Speaker A: I'm applying alpha. I'm applying omega first, and then once I've applied omega, I apply alpha. So sort of, you should think of omega as a local patch. So if alpha is not quite a good assignment because it violates the c, then by applying omega first and then alpha, I get a patched truth value assignment that is guaranteed to take care of both what I had before and the new clause. And this is essentially sort of, in some moral way, this is sort of what extended resolution is also. But this is, I think this is a nicer way of thinking about it. And it also generalizes the so called rat rule.
00:32:25.014 - 00:32:55.724, Speaker A: Which I won't even explain what it is, but what is being used in just a. Very quickly, see that I can get these clauses by this so called redundance based strengthening rule. So note that I have to specify. It's very important that you can't guess omega. So I have to tell you what it is. So, suppose I want to drive these closest in order. First, I want to drive a or not x or not y, where a is fresh.
00:32:55.724 - 00:33:16.852, Speaker A: My witness will set here a to true. Okay? And now you realize that nothing happens to f. And on the right hand side, the witness sets the clause to true so that it vanishes. So I don't have to prove anything. So this. Now I have derived this clause by the redundant space strengthening rule. In the next step, I add this clause.
00:33:16.852 - 00:33:44.648, Speaker A: Now, my witness is the opposite. This sets a to false. That takes care of the new clause. It doesn't take care of this clause. But now the assumption not c on the other side, actually says that x is false, and that satisfies this clause, so it again banishes. So I have nothing to prove, and the same thing happens for not a or y. So these are like very, very simple cases of redundant space strengthening.
00:33:44.648 - 00:34:08.004, Speaker A: But this tells us that this is like a nice rule. And you can see that if I specify the witness, then I can get, like, verifiable derivation, the second and the third, you need the previous ones. Or could you do them? I could do this in any order. I don't. I mean, the things that I'm adding before, they don't sort of help me. They're more like, they mess my life up a bit. I could do this in any order.
00:34:08.004 - 00:34:36.744, Speaker A: It's valuable. Okay, so this, if I take resolution and I take this redundant rule, this is sort of extended trigger. And now you could say, aren't we done now? Because sort of extended trigger can do any reasoning known to man. No. And the problem here is that maybe that is the case, or maybe not. That's actually one of the open questions that we'll get to in the end. But the problem is, we really care about efficiency.
00:34:36.744 - 00:34:52.644, Speaker A: So saying that. Yeah, maybe extended frigate polynomial simulates everything. But if you have, like, an m cubed blow up in the simulation, then that's a non starter, because then you're going from linear time to cubic time. And if you're looking at a million variables, this is not going to flow.
00:34:53.694 - 00:34:57.074, Speaker B: Is the converse obvious effect that represents.
00:34:57.814 - 00:35:07.342, Speaker A: No. Okay. Yeah, but I think there are some. I'm blanking on the reference. There are some people who have worked on this, I think maybe, I don't.
00:35:07.358 - 00:35:16.834, Speaker B: Know if somebody forgot who the original person was. There's pretty simple proofs to just define the value of the variable.
00:35:21.034 - 00:36:25.750, Speaker A: But it's not like, yeah, you have to, this is sort of, it's believable, but you have to improve it. Okay, so what are examples of what sat solos might do that d wrap doesn't know how to do? Well, if I have these four clauses over x one to x four, then you look at them and you realize that this is just a cardinality constraint, saying that at least two of these variables are true. Why whiteness may helpful? Well, if you can extract cardinality constraints, that you could solve the pigeonhole principle in linear time, whereas for resolution we know that it's exponential time. There are solvers implementing this. There hasn't been known any efficient way to support this with proof logging. And this is sort of the first motivation why it might be nice to go to zero one integer linear inequalities, except that when you talk to sat solving people, you should refer to them as pseudoboolean constraints, which is a much cooler name, obviously. So from now on, let's not say cutting planes.
00:36:25.750 - 00:36:51.204, Speaker A: Let's say pseudoboolean reasoning. Clearly, everybody wants to use pseudoboolean reasoning. It sounds extremely powerful. But it's just cutting planes. We have zero one integer linear inequalities, integral coefficients. For formal reasons, it's nice to have like literals as variables in their own right. If I want to, and I can say that x and x bar just cancel to leave a one variables are still true or false, zero or one.
00:36:51.204 - 00:37:51.294, Speaker A: Obviously clauses are just special cases of pseudo boolean inequalities, cardinality constraints. Or we can have completely general constraints with arbitrary integer coefficients, and we reason with these constraints just using standard cutting planes. So just to make sure that we're on the same page, if I have these two constraints, then I can take a positive linear combination, say this constraint plus two times this constraint, and then I just collect terms and I guess this, then I can, I know for all literals that they are non negative. So I know that, for instance, z bar is non negative. I can multiply that by two and add to this and then cancel out. And when the dust settles, I guess that get the three w plus six, x plus six, y greater equals seven. And then the sort of main rule of cutting planes, if you wish, is division, which then says that I can divide through by three, and then on the right hand side I can round up.
00:37:51.294 - 00:38:39.512, Speaker A: This is cutting. And now it's clear that if I just write down my clauses in pseudo boolean form, add them up, and divide, then I can get this. So this means that if my proof logging system would be able to reason with linear equalities, then I have a very concise, efficient way to certify this type of reason. I still need the redundance rule, and it will look exactly the same. The only reason difference now is that c is no longer a clause. It's a general zero one integer linear inequality. And this gives me basically extended resolution, except for cutting planes in particular.
00:38:39.512 - 00:39:13.704, Speaker A: This is an extension of the drought proof logging system that some people use. What is not obvious at all and wasn't obvious to us before, but it just is an empirical observation, is that this turns out to be a surprisingly powerful process. It is very convenient. You can take a lot of different off the shelf solvers and look at what they do. And it turns out that a lot of what happens in combinatorial optimization actually is cutting place based reasoning. You're counting different things, you're comparing your different counts. This is, you know, cutting planes can do this.
00:39:13.704 - 00:39:52.592, Speaker A: I mean, there's a reason why we struggle to prove cutting planes lower bounds. It's a fairly intelligent proof system. It can do non trivial things. One thing that it can do, and that we haven't had proof logging for before, is suppose you're actually given a pseudo boolean formula, or like a zero one integer linear program. One way of solving this is to introduce extension variables and say that, well, x one plus x two plus x three plus x four greater or equal to two is equivalent to this formula. And this is something that is actually being done. You get like a zero one integer linear program.
00:39:52.592 - 00:40:51.968, Speaker A: You take some smart translations to clauses, you run a SAT solver, and if the SAT solver says unsat, then you believe that your zero one integer linear program is on set. The only problem is like, is it obvious that this is actually the same as this? It's not. And again, these are the kind of solars where you would actually see bugs from time to time. And what I don't have time to tell you about, but what we have a paper about this summer was that using this cutting planes with this extension rule, you get fairly slick proofs that this kind of representation can be derived from this. Okay, so that means that you can first prove the correctness of the translation to CNF, and then you concatenate this with a sat solving standard proof logging proof. And now you actually have end to end verification of the whole process. The SATs over never reasons with anything other than clauses.
00:40:51.968 - 00:41:37.814, Speaker A: But the fact that we are reasoning with zero one integer linear inequalities helps us to certify the correctness of the whole process. Another thing that SATs solver would do that I mentioned, for instance cryptographic problems, is parity reasoning. So suppose I have these four clauses and these four clauses. Then I claim that I should be able to derive xor, not w and not x or w. Why is this? Well, it's easier if you write down these four clauses are just saying that the parity of x and y and z is odd. These clauses, if you look at them, say that the parity of y, z and w is odd. And now if you just sum up these two parties, clearly x and w must have even parity.
00:41:37.814 - 00:42:11.448, Speaker A: But we know that resolution doesn't support this because saping formulas are hard for resolution. There are solvers using this. Again, if your solver uses parity reasoning, you can't take part in the main track of the set competition. This is an example of where we know that drag proof logging can do this. But all suggested solutions so far just don't scape. We pick up something like a linear or quadratic factor, and that's just too bad. And now, what is not so obvious is that cutting planes with extension variables can do this in a very nifty way.
00:42:11.448 - 00:42:53.804, Speaker A: By I'll write my parity constraint in the following way, that x plus y plus z plus two a equals three, where a is a new fresh variable. If you look at this, you realize that this is just a stupid way of saying that the party is old. And in the same way this constraint says that the party is older. And now when you add them, you get x plus w plus an even number times lots of things is equal to an even number. And now you realize that this constraint actually says that the parity of x and y w is even. And then if you work a little bit more, it turns out that you can actually efficiently. This is not obvious, you have to do a little bit of work, but now you can extract efficiently.
00:42:53.804 - 00:43:34.474, Speaker A: These inequalities is our exactly these. And this scales very nicely. So again, this is something that we have implemented, and you get an order of magnitude faster. Proof logging and proof verification that has been the case and going beyond that. So I'm running a little bit out of time. I'll say this just very quickly. So it turns out that you can also take cutting planes and you can certify what state of the art graph solvers for like for instance for maxclick, for subgraph isomorphism, for maximum common connected subgraph what they do, you can write down efficient cutting planes proof.
00:43:34.474 - 00:44:31.318, Speaker A: Although cutting planes doesn't know what a graph is, but still, the way these solvers are reasoning about graphs fit nicely with cutting planes. Although cutting planes doesn't know what a general integer is, you can actually encode an integer as just this exponential sum of the bits, and then you can do things like that constraint programming solvers would do and provide efficient proofs. There's one caveat here. If I'm given a graph as an input, and I want to solve subgraph isomorphism, then I somehow have to pre translate the graph subgraph isomorphism problem into a zero one integer linear program so that cutting planes can understand the program problem. Okay, this is something that should be formally verified. We don't have it formally verified at this point. Again, this is another point where you should use interacting theorem interactive theorem provers or something.
00:44:31.318 - 00:45:31.412, Speaker A: It's completely within reach, but it's work that needs to be done. But this is sort of for many of these problems. Again, if you think about your favorite np complete problem, it's very easy to write it as a zero one integer in the program, and it's not hard to prove that indeed, like this is the correct formulation. What about symmetries? So, symmetries show up in optimization, they show up in hard benchmarks, if you think. Again, if you go to the proof complexity literature, turns out like most of the formulas that are hard are actually hard because of symmetry. The only example, maybe that we don't, don't have, maybe random Casey enoughs are not symmetric, I guess, but otherwise many formulas that we have lower bounds for are symmetric one way or another. Again, the style of symmetries that you have for pigeonhole principle formulas can be broken by draft, but it's complicated enough that it actually hasn't been implemented.
00:45:31.412 - 00:46:21.674, Speaker A: In principle. There should be solvers that break pigeonhole style symmetries and emit kick and take part in the sub competition. There are no such solvers. So what can you do with symmetries? One thing you can do is like break symmetries to get, for instance, the lexicographic small solutions. You could also, if you don't want to change the set of solutions, you can do something that is called symmetric learning, where anytime you derive a consequence, you say oh, any symmetric version of this consequence should also be valid. There have been one proof logging system proposed for symmetric learning, which basically just says, take a proof system and extend it with knowledge of permutations and symmetries and that kind of thing. There are two reasons for not liking that.
00:46:21.674 - 00:47:08.824, Speaker A: Firstly, because it turns out that if you do it in this way, then this is incompatible with all the pre processing techniques that are also used. So it's not very useful. And it's also, in some sense, it's clear that whatever kind of reasoning you want to do, you could also add, and always add an ad hoc rule that does say parity reasoning, or cardinality reasoning, or symmetry breaking. But the problem is, when you take all of these rules together, your proof system is getting very complicated and soundness becomes hard to understand and to verify. So somehow the aesthetic is that we really want to keep the proof system as simple as possible. We don't want it to know about symmetries or permutations or anything. If we can get away with just knowing what zero one interlinear inequalities are, then we would be very happy.
00:47:08.824 - 00:48:12.256, Speaker A: So we'll try to do that, but at the price of adding one more extension rule, which I think Neil will talk more about after the break, because this is an interesting rule, and understanding how powerful this rule is, and whether it's really important in order to introduce this rule, we'll think about optimization problems. So in this talk, I consciously focus just on decision problems because it's easier and it's a SAT workshop. Our proof logging system also works in a much more general optimization setting. But even if we don't have an optimization setting, I can invent, nothing stops me from inventing an objective. So my objective will be some interlinear combination for integers, w literals, liquid. And I think of my problem as say, minimizing this subject to my formula. What is a proof of optimality? Well, a proof of optimality would be first you check that you have a satisfying assignment, and then you have a proof of contradiction from your original formula.
00:48:12.256 - 00:49:22.404, Speaker A: And then together with the requirement that you should find a better solution, well, note that strict inequality is just syntactic sugar. Since we're doing discrete problems, there is no strict inequality. It just means less equal, minus one plus whatever. This is actually a little bit important, because this is something you can run into problem with if you start doing proof logging for more like linear programming and make sense of linear programming solvers. And now once we have this more general optimization setting, nothing stops me from solving a decision problems while at the same time minimizing, say, two to the I times exile. What does that mean? Well, I'm sort of searching for the lexicographically smallest satisfying assignment. So how does the proof system change? So standard cutting planes is still okay? Once I've found the solution, I will allow constraints that allow me to cut branches that cannot have better solutions.
00:49:22.404 - 00:49:59.784, Speaker A: I also need to be aware of my objective when I use the redundance rule. It's not okay just that the witness flips to a satisfying assignment. I also want to make sure that it's not okay if I flip to an assignment that gives me a worse objective. So this is this new thing that says that whenever I flip with respect to omega, it doesn't increase my objective. Okay, so now with this adding objective, our rule becomes this. And now here comes the key observation. If my witness is good enough, I could actually make a much more aggressive rule.
00:49:59.784 - 00:50:33.644, Speaker A: So note that here from f and not c, I have to prove all of f and c under the restriction omega plus that the objective doesn't increase. But now if I can prove that actually the flipping strictly decreases the objective, then I don't need. Then I only need to reprove f. I don't need to prove the constraint that I want to add. So note here that the difference between these rules. I have removed the proof target. Now there's no c here.
00:50:33.644 - 00:51:23.944, Speaker A: But on the other hand, I'm proving strict inequality. So why is this okay? Well, very quickly, it is okay for the following reason that suppose again, what are we worried about? We're worrying about Alpha satisfying f, but violating d. Well, in that case, I know that alpha composed with omega satisfies f and has strictly smaller objective value. And now if this new alpha composed with omega satisfies d, then I'm happy. Otherwise, I can run this rule again, and I get alpha composed with omega composed with omega, which gives me a strictly smaller objective. And now either I satisfy D or I run this again, and I get a chain of new assignments that have smaller, smaller objective. And this has determined.
00:51:23.944 - 00:52:23.074, Speaker A: So this dominance rule is kind of some kind of limited induction rule that says, you know, that if I ran this rule enough many times, then I would finally find a satisfying assignment. So let me just do this in one trick, and in fact, you can do it even slightly stronger. It turns out that if you have derived new constraints before, and I want to add an MS constraint, I don't need to reprove these constraints. I only need to reprove my original formula, and it's basically the same proof as before. And this turns out to be. So this is like the cutting planes with this redundancy I told you about before. And this new dominance rule turns out to be able to do pretty much all techniques that have ever been used in modern Sat solving except for symmetric learning.
00:52:23.074 - 00:53:05.294, Speaker A: And it turns out that it can also do symmetry breaking in more general paradigms such as constraint programming. But time is flying fast, so I don't have time to say too much about that. So I want to now to say, okay, like what is actually the formal proof system? If you want to do proof complexity about this, I won't give you the full formal version. I'll give you a version for like decision problems and slightly simplified. So because of these, you need to know, for these kinds of rules, you need to know what constraints you are currently having in your database. So it's a configuration based proof system. A proof is a sequence of configurations.
00:53:05.294 - 00:54:07.968, Speaker A: Every configuration contains a set of core constraints which you could think of as the input formula, and then another set of constraints that you have derived. And you sort of know at all times that if you really need to, you can recover the derived constraints from the core constraints. So you don't need to worry too much about those. And then if you're doing symmetry breaking or if you're doing optimization, you have some kind of objective, some linear objective, you can always, you can always apply standard cutting planes, rules, and anything you derive, you put it in the derived set and let's maybe ignore the rules for how to deal with optimization problems and just focus on sat down. Sat. Now here are the formal descriptions of our strengthening rules. So the redundant space strengthening rule, it says, if I have the core set c and the derived set d, and I want to add a new constraint c, then I need to show that c from c union d union.
00:54:07.968 - 00:55:07.584, Speaker A: The new navigation of the new constraint, I can derive all of them under this witness, substitution omega, and I'm not making my objective, whereas the dominance rule has a stronger requirement on the objective, it says that the objective should decrease strictly. But note that I got rid of a lot of proof targets here. I got rid of all of the, and also I got rid of the new. I only need to reprove essentially my original formula. And note that, for instance, now if you're doing symmetry breaking, I'm saying this very quickly, but basically if you're doing symmetry breaking, then this witness would typically be a symmetry, so that syntactically this is the same as this. So you don't even have to write down the proof, you just have the verifier should check that everything is exactly the same in order to make this into a proof system. It's important that the witness should be explicitly written down.
00:55:07.584 - 00:56:03.640, Speaker A: And also this is a derivation symbol. So there will be like a sub derivation for each of these constraints. Or if the derivation is obvious enough, then sometimes you can just sort of ask the proof check to fill in the details. This will be important in practice, it's not important for the theoretical analysis of the proof. One thing that happens with this type of proof system is that suddenly deletion becomes very important. You have no restrictions on deletions from the derived set, but you cannot derive from the core set in general, like you can only erase something from the core set if you can derive, if you can rederive it from the core, minus the thing that you erase, either by cutting planes or by the redundant space frankening rule. And the reason is that actually if you don't have this, then it turns out that with the proof system, you can take a satisfiable formula and prove that it's unsatisfiable.
00:56:03.640 - 00:57:01.984, Speaker A: This is slightly subtle, but so you have to be a little bit careful in practice. If you want to do proof logging for like real world combinatorial solvers, then there are also some hacky special cases that you can add and that you would add, but again, like the abstract version would be this. And now you realize that if you want to delete, then it might be useful to shift things that you have derived into the core set. So there is a way of, there's a rule for just saying, okay, I want to shift some constraints from the drive set to the core set in order to be able to do deletion, for instance, or in order to change the order, because I can. When you're doing symmetry breaking, it's sometimes useful to break with respect to different symmetries. They might sometimes be encoded by different orders. And if you think about it carefully, then you realize that changing the order should be okay if you make sure that the derived set is empty when you change the order.
00:57:01.984 - 00:57:43.246, Speaker A: I'm doing this. It's like extremely fast, but there's details in the paper. Okay, so that's it. So, wrapping up, it turns out that with this proof system in ongoing work, we're now showing that you can lift this from sat solving to Max sat solving. So sat based optimization, we already are starting to do it for general constraint programming. There's a completely different proof system being explored in Berlin that is trying to do mixed into the linear program, which is somewhat similar in spiritual satisfiability. Modular theories would be nice to do.
00:57:43.246 - 00:58:24.410, Speaker A: But then you probably need richer logics because you have these logical theories. That would be like a third application of maybe interacting theorem provers or stronger theories on the proof complexity side. Three questions symmetric learning I mentioned that we don't have proof logging for that yet. There is a way of doing it, but it is not super efficient. It would be really nice to understand whether you can do substitution proofs efficiently in this proof system. I know how to do it, but not as efficiently as I would like. We don't know if this fancy dominance rule is necessary to do symmetry breaking.
00:58:24.410 - 00:58:56.350, Speaker A: We know it's sufficient. I still think it's a very reasonable question to ask whether extended Frege could do fully general symmetry breaking efficiency. We just don't know. I think it would be interesting also to understand this dominance rule. Does it add anything? Are we still within extended Frege, or are we actually somehow outside of extended Frege? I think this is an interesting 2.50 question that maybe Neil will comment on after the break. And there are many, many more interesting questions and applications.
00:58:56.350 - 00:59:36.434, Speaker A: We're aiming to take over the whole world of combinatorial optimization and change it to get certifiable proofs of correctness. You're welcome to join the revolution if you want. We're hiring and with this. So yeah, so although this is a proof complexity workshop, I started talking about combinatorial solving and optimization. The connection to proof complexity is that ensuring correctness is like the elephant in the room here. Certifying solver seems to be the answer that's really applied proof complexity. We can use all the tools that we like, but to prove upper bounds, not lower bounds, just empirically.
00:59:36.434 - 01:00:02.944, Speaker A: I don't know why, but it just seems that cutting planes is a wonderful proof system. It just works. It's beautiful. And I think it's interesting when you extend cutting planes with this dominance based and redundant space strengthening, what kind of proof systems do you get? How does it relate to other proof systems that we know and love? I think these are theoretical questions that are worthy of further study. So thanks.
01:00:14.684 - 01:00:24.978, Speaker B: I had a couple tipping on my answer too long. First, when you said you couldn't change the order, did I understand correctly? You have to be careful if you change the order when you bring your dominance?
01:00:25.116 - 01:00:29.366, Speaker A: Yeah, if you want to change your dominance order halfway through the proof, you have to be really careful.
01:00:29.430 - 01:00:36.078, Speaker B: But could you just not, just not start throwing away the previous DS or something like that like you indicated? But it still works.
01:00:36.126 - 01:01:07.510, Speaker A: I mean, what you would do is basically you do your dominance breaking with respect to some order. And then the things that you want to keep, you can keep them, but now you have to transfer them to the core, which means that they will then be among the proof targets when you're doing symmetry breaking with a new one. And it's not so hard to see that, like if you don't have this, then you can break symmetry so that every pigeon goes into, like the largest pigeon goes into the smallest hole, and then I'll break symmetry so that the smallest pigeon goes into the smallest hole. And then I immediately have a contradiction, even if there are enough holes. Right.
01:01:07.662 - 01:01:21.774, Speaker B: Makes sense. And the other question was, is there for d rat style systems? Or we have this unit propagation makes this sort of, this one implies is an easy to check things analog and cutting planes of an easy to check.
01:01:21.854 - 01:01:42.142, Speaker A: Unit propagation transfers exactly to pseudo boolean constraints, to zero on interdilinear inequalities. There's like the natural definition and indeed that would, I'm sort of sweeping that under the rug. A lot of our proofs would be based on captain planes. Unit propagation. Reverse unit propagation. That will be the workforce.
01:01:42.198 - 01:01:49.994, Speaker B: I mean, you use the double term turn style in your definition there. Is that what you use in the final thing?
01:01:50.074 - 01:02:21.294, Speaker A: What happened now? Why is it always in the final formal definition here? Yeah, maybe I wasn't like here I actually have a single turnstile. Right? So there will be proofs, but I will have sort of, my proof system will allow me. If something follows by reverse unit propagation, then I can just claim it. So you can basically you add reverse unit propagation as a rule. And that's okay because it's a polynomial timer function. Okay.
01:02:25.274 - 01:02:40.422, Speaker B: I was wondering if there is some fragment of these new proof systems, maybe without using the extension variables, for which you can still prove some lower bound, even though maybe it's stronger.
01:02:40.558 - 01:03:29.134, Speaker A: That's an excellent question. Yeah, that's an, yeah, that's a great question, which I should have added and I didn't. Yeah, so you can, that's, and I think we're sort of going to have a talk on a somewhat similar notice this afternoon. You can take these wonderful strong proof systems that, and you can say, because this, this dominance rule, for instance, or the point is that what you can do with these rules is that C could contain new variables. Right? But I can take both of these rules and I can say, no, you are allowed to use these rules, but you can't use new variables. And the question is, does this make sense? The answer is yes. For instance, resolution with redundancy strengthening can solve Satan efficiently, it can solve pigeonhole principle formulas efficiently.
01:03:29.134 - 01:03:39.062, Speaker A: And then it's a very natural question if I forbid new variables, but you still have these rules, can I prove lower bounds? That's a great question. And it might be within reach.
01:03:39.158 - 01:03:44.274, Speaker B: Understand that, you know in the resolution world that there are some results like this, like PR.
01:03:44.854 - 01:04:10.474, Speaker A: Yes. I'm not on top of it. I think Sam is the person to talk to or maybe EmrA. Well, I think we'll hear about kind of this this afternoon. Yes. Yeah. But I think even for, even for resolution, I mean even doing this for resolution in full generality would still be interesting because I think even the resolution, if I remember correctly, have further restrictions.
01:04:10.474 - 01:04:29.034, Speaker A: But it's a great question. It's like it might be within reach. It's clearly very hard but might be doing okay, let's stop now and take a break. 11:00.
