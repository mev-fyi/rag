00:00:04.760 - 00:00:08.074, Speaker A: Okay. Could you see my screen?
00:00:08.614 - 00:00:09.394, Speaker B: Yes.
00:00:09.974 - 00:00:49.424, Speaker A: Okay, sounds great. It's my great pleasure to be here to give another UTR Steam park after school. So we are in. This talk is about again related to safety learning score just mentioned. I will focus on a specific topic on how to constructing non isotopic confidence bounds for off policy evaluation. So we'll propose what we call optimization based algorithm, which is framing the construction of confidence intervals into optimization problem, and we can apply primal do methods to kind of solve it efficiently. We'll see how it goes.
00:00:49.424 - 00:02:00.084, Speaker A: Okay, so I guess I don't need to give a heavy introduction to the safety questions after scores talk, but basically the idea here is that reinforcement learning has been really powerful at cases when we have very good simulators such as video games and Alphago. But on the other hand, for many real world problems, we don't have high fidelity simulators and we only have observational data. So this is especially the case where we want to apply reinforced learning to medical diagnosis education robotics, when the robotics are very expensive and you don't want to damage them. So in these cases, we only have data observed from previous experiments, possibly in a very messy, unstructured and unorganized way. In this way, in this case, the information about system is very limited. So when you apply reinforcement learning, there's lots of uncertainty. You cannot ensure whether you are doing the right thing.
00:02:00.084 - 00:02:45.298, Speaker A: So here, uncertainty estimation is really important, especially in high stakes areas such as health care. It's actually not enough to just get the arbitrary uncertain estimation. You actually want to make sure that your uncertain is theoretically verified. It's guaranteed, it's regressed. This is why we are motivated to talk about non asymptotic boundaries instead of approximation algorithms. So here's my definition of the problem. So the problem is that we are given a policy PI and we want to construct a non asymptotic confidence interval for the expected reward purely from observational data, right? So there are lots of challenges.
00:02:45.298 - 00:03:57.840, Speaker A: First of all, the data is on off policy, which means that it's collected from some arbitrary policy or even no policy, right? So arbitrarily. So if you want to reason about a particular pie that hasn't been tested before, it requires some sort of counterfactual reasoning ability, right? Another challenge is that the data collected in some black box way, which is unknown to the algorithm, right? So the algorithm has to be behavior agnostic in order to take that into account. And also, as we are doing reinforced learning, we have long trajectories, and the trajectories are dependent with each other because of time. So we don't have the IID assumption. That was very nice for supervised learning, but here we really don't have that. And this actually caused a very strong difficulty for constructing nunc, as I'm talking about. And also, that's something called a curse of horizon, which means that if you are reasoning about non horizon, if your decision making has lots of steps, it's very difficult to reason about the future rewards just because you are considering reasoning multiple steps mathematically.
00:03:57.840 - 00:04:30.440, Speaker A: It turns out this is also very difficult for traditional methods. So these are kind of challenging problems, but more or less. We actually propose an algorithm, I think quite nicely, surprisingly solve all these problems. We provide a non asymptotic bound, has guarantees and behavior agnostic, works for infant horizon. It's off policy, actually works for data with arbitrary dependencies. So it's very nice, flexible. And again, the idea, I will talk about that.
00:04:30.440 - 00:05:14.124, Speaker A: It's a kind of optimization view when sort of primal do labor. Okay, so let's talk about the background. The setting is very simple and standard. We have a target policy PI, which is given we have unknown environment, p is unknown to us. So basically, every time the agent take action based on state, and then based on the action state, you have an environment that transit into another state as t plus one. Also we receive the reward. Both the way to receive the reward and the next state are unknown to the user.
00:05:14.124 - 00:06:22.168, Speaker A: To the algorithm, we want to estimate the expected reward over an infant horizon, which is summing over all the time up to infinite of the reward, discounted by a factor gamma t. Starting from some initialization, mu zero, assume. Here we assume mu zero is fixed, is known as well. Okay, so now the problem is given. In reality, we don't know the true model. So we assume that we have transition pairs, right? So I can actually label. So we have a collection of transition pairs, right? I will talk about what's the condition of the transition pairs, but basically we observe a bunch of things, a data set, and then we want to construct a confidence interval, which is a function of the data such that the probability that the confidence interval catch the true value, the true expected reward is larger than one minus delta.
00:06:22.168 - 00:07:15.474, Speaker A: Delta is a confidence level. So here the probability is taken with respect to the randomness of data. So the interval is a standard confidence interval thing. So the interval is a random number versus the true reward is fixed, but want to make sure the interval actually catch the true value with high probability. Okay, so this is our formal definition of the data assumption. It's actually a very mild assumption. So the idea is that we assume we collected a set of transition pairs, right? And we assume the transition pairs are collected sequentially such that for each data point I, the SA, the state and action is collected by some arbitrary procedure that is unknown to us given the previous data points.
00:07:15.474 - 00:08:05.672, Speaker A: And then given the state action pair, the next state, the next action, the next state, and the reward is collected from the unknown true model. So really the only requirement we have is that the next state and the reward, given the previous state and action has to follow the model, the underlying environment. But otherwise it can be arbitrary. So the state action could be, you could use it in any way to decide your essay at each time, right. So it's very flexible. In fact, I cannot figure out a case when this is not satisfied. The only case I can figure out is when somehow that's a loopy dependency where you can travel back to time.
00:08:05.672 - 00:08:08.848, Speaker A: But that doesn't seem to be realistic.
00:08:08.976 - 00:08:27.204, Speaker C: So just a second on this assumption, a quick question. So you say that this data is gathered sequentially with this black box, but it's not very clear to me whether you're assuming that these tuples would be independent of each other.
00:08:29.144 - 00:08:32.544, Speaker A: So this is a sequential process, right. You can think of as a map.
00:08:32.664 - 00:08:36.112, Speaker C: And so like, is there going to be some mixing assumption here?
00:08:36.168 - 00:08:49.564, Speaker A: Because there's no mixing assumption. That's the key. Right. So I, maybe I will, I think on this slide it will make it very much clearer. So the idea is that you can kind of view this as a two point setting. I think this is really a kind of shoe.
00:08:50.054 - 00:08:52.542, Speaker C: I got it. Like you're going to assumption.
00:08:52.598 - 00:09:13.566, Speaker A: Yeah, yeah, yeah, yeah. So basically you can view s a as, as your feature, right. And then your y as your label, right. So it's like a supervised menu. You want to give an x, you want to put it y. And really what I'm assuming is that y given x, follows the model, but then I don't assume anything about the data, right. The data is collected arbitrary.
00:09:13.566 - 00:09:23.534, Speaker A: This is really the kind of, kind of supervised learning setting. Like what we really don't need to assume the.
00:09:25.274 - 00:09:27.474, Speaker C: Where the data comes from even more general.
00:09:27.594 - 00:09:27.946, Speaker A: Right.
00:09:28.010 - 00:09:34.322, Speaker C: So the question I'm asking whether the xi's could be sequentially correlated, like the black.
00:09:34.458 - 00:09:38.054, Speaker A: Yeah, sequentially correlated. They could be deterministic, they could be.
00:09:38.394 - 00:09:49.934, Speaker C: The black box might decide, for example, that. I like this previous exercise. Let's just go back and let's pick that again and 100 times more and stuff like that, right?
00:09:51.114 - 00:10:16.634, Speaker A: Yeah. Okay. Yeah, sounds great. So, yeah, the point, again, I think the point is that the data is really arbitrary, right. So Xi could be dependent in some arbitrary fashion. The idea is that when we want to construct a confidence bound, we really don't care about how the data was generated. We only care about the information that data is carried to me.
00:10:16.634 - 00:10:49.834, Speaker A: Right. So when you generate data, you have all kinds of assumptions, right. But once you get the data, the data is fixed. Right. So that we really, we don't need to care about how the data was generated, because by the time the data is given, it's already deterministic. Okay, so traditional way to construct non anesthetic components bounds. It seems that the only way to do this is using important sampling plus concentration inequalities.
00:10:49.834 - 00:11:34.204, Speaker A: That's a large literature, very classical literature on this. Many of the people here contributing heavily on this topic. And the idea is that you collect a set of independent trajectories from some known behavior policy PI zero. In this case, you have to have multiple trajectories that are independent, and you have to, the trajectory has to be drawn from some known policy PI. So it's not arbitrary dependency, it doesn't follow the assumption that I just mentioned. And also it's not behavior agnostic. And also you have to truncate the horizon to be some finite horizon.
00:11:34.204 - 00:12:25.190, Speaker A: In this case, you can use a standard important sampling estimator and basically assign weights on the trajectories. And then you can get a concentration bound using non isympathetic bound using any constitution inequality, right? So obviously there are lots of drawbacks, right? It doesn't, so actually it doesn't have the efficiency. So when you have a large horizon, t this algorithm actually tends to degenerate. That's because your importance weight is a product over the time, as you have lots of time, time goes to infinite, the variance of your importance rates grow exponentially. So that's very bad. There's something we call cursor horizon. And also this algorithm has to require independent assumptions between the trajectories.
00:12:25.190 - 00:13:16.466, Speaker A: If you don't have independent assumption, you can still give an estimation, but you cannot provide non asymptotic bound without providing a bound over the mixing coefficient of the Markov process. So as I want to emphasize here is that our algorithm doesn't require the mixing condition, and also it's not behavioral plastic, so it doesn't work for our case. There are obviously many other ways to get confidence bounds of policy. So there are bootstrap algorithms that developed recently. School also had a work on this. But bootstrap is an asymptotic algorithm, so it doesn't give non asynchronous guarantees. If we care about safety, we should go with non asymptotic, because that's a real guarantee.
00:13:16.466 - 00:14:11.814, Speaker A: And there are also bayesian methods that quantify uncertainty in reinforcement. But Bayesian doesn't have, have frequentist coverage, so it doesn't have the kind of guarantee that we want. Okay, so how do we solve this problem? Well, it starts from a line of work that I was getting to, together with Ste Hong and many other collaborators on infra horizon value estimation. So we have a sequence work on estimating the reward without, first of all, without confidence estimation for the infinite horizon case, when you have the cursor horizon for importance sampling. And then we talk about, and then this sort of work gradually move towards to confidence bound estimation. But it more or less provides a foundation for my method. So I want to introduce this line of work.
00:14:11.814 - 00:14:51.794, Speaker A: So, it turns out that there are two major algorithms for infinite horizon value estimation. One is based on a q function, another is based on visitation description. So the q function idea is very simple. Basically, if you can estimate the q function of the policy, the q function is the cumulative reward. Given that you start from a fixed state action pair, where here I'm using x to represent the state action pairs. Then if you know the q function, you can just take the average expectation of your q function over the initialization, over the initial states. That will give you an expected reward.
00:14:51.794 - 00:15:33.632, Speaker A: This is one way to kind of think about expected reward. Another way to think about reward is that you can calculate what's called a visitation distribution, which is the discounted average of the distribution of the states across time. So here, d PI t is the distribution of xt. If you run the policy PI for t steps starting from initialization, then you can get the estimation as well. So the expected reward is going to be the expectation of the local reward, taking expectation under the pack. Right. So you can sort of see these two ways, different ways to calculate the expected reward.
00:15:33.632 - 00:16:08.668, Speaker A: They are really just their duality each other. They have a primary relation. I will talk about this. Many people also observe that. But I think the difference is really that when you talk about this summation over infinite, right? So when you have this q function representation, the summation is pushing inside the reward versus when you have the visitation distribution, the summation is pushed into the probability. There are two ways to kind of count the summation. Okay? But then if we can estimate either with visitation distribution or q function, we can, we can construct estimation.
00:16:08.668 - 00:17:17.734, Speaker A: If we can ask the most we can use, we can do w bus estimation. Okay, so how to estimate visitation distribution? Well, we have this idea that you can actually put some importance weights on the data point. So here w is some weight distribution over the data points, right? We can actually define this weighted empirical distribution over the distribution where each data point is assigned with a weight. And then we want to find the weight such that the weighted distribution of the data actually follow matches the visitation distribution. If we can do this, then we can approximate the expected reward using the weighted sum over the reward over the data set. Kind of like important sampling, but we are applying important sampling on the transition level, and it's for infinite horizon. So the importance weight w here doesn't have a closed form solution, doesn't have a closed form representation, and we actually need to estimate it using optimization methods.
00:17:17.734 - 00:17:57.860, Speaker A: Right now, the way we estimate it is to define a reward function. The reward function we defined it is a minimax reward. So the idea is that. So basically, if you assume the policy for giving the policy, that's Markov chain, essentially you are trying to estimate the station distribution of the Markov chain. And then what you can do is you write down the fixed point equation for the Markov chain and reframe it a little bit. It turns out you can show that we can define this delta function. I don't want to get into detail about what this function does, but basically this is a function that is fully computable.
00:17:57.860 - 00:18:48.048, Speaker A: You can directly calculate it from the data, and then you can show that this delta function equal to zero for n in q. Here, q is a kind of a discriminator function that we introduce such that if the dw exactly equal to the d PI, then this delta equal to zero for any distribution queue. So it quantifies the deviation of delta to the kind of true delta deviation of w to the q w. Right. So in this way, we can define this worst case kind of loss where you take a set of q function as, as a discriminator. It's kind of like a Gantt style where you find the worst case q, right. And if delta equal to zero for every q, then this loss function has to be zero, otherwise it will be positive.
00:18:48.048 - 00:19:20.790, Speaker A: If the q is symmetric, then this defines a loss function. And in addition, if we choose q to be a reproducing kernel, here space, it's a kernel space with a positive definite kernel related to think as doing some kind of kernel regression, then you can actually write down the closed form solution for the loss function and you can minimize delta, minimize w to estimate, minimize the loss to estimate w. So that's, that's a very quick overview of this idea.
00:19:20.902 - 00:19:27.190, Speaker B: And you have about four minutes left. Sorry, do you have about four minutes left?
00:19:27.342 - 00:20:05.994, Speaker A: Oh, four minutes left. Okay, so I need to do really quick. Okay, so another approach is to do q function estimation. And again, the idea is that we are going to introduce a kind of discrete loss and the loss is defined in this way. We again, we use a minimax loss function to define the loss. And again we introduce a discriminated function. If the discriminate function is a kernel arch address, we can actually show that it has this very nice closed form that can be calculated easily.
00:20:05.994 - 00:21:39.622, Speaker A: And further, the key result that we have is that if the data is collected following the previous assumptions, then we can show that the kernel loss of the true q function is bounded by this non asymptotic bound. And this is important. Basically, the concentration equality actually doesn't require any sort of dependency among the independence of xi. Right? So given this concentration equality, what we can do is we can construct this concentration, uh, this feasible region for, uh, for q function, right? So uh, if we define this function set where q is some model space that is a prior information that capture q, q PI, then this set has to be a confidence set for the, for the true function, right? Uh, it puts the non syntactic guarantee and then we can actually transform this feasible region of q PI into a confidence interval for the expected reward by finding the extreme q functions, either maximize it, minimize it, we will get a confidence bound for the expected reward. This frames the optimization problem which can absolutely be solved in practice. But the disadvantage is that we have to solve this optimization to absolute global optimal in order to guarantee our methods. This is not ideal.
00:21:39.622 - 00:22:30.180, Speaker A: So what we did was to divide upper bound of this confidence interval. So the upper bound is a minimization problem. It's guaranteed to be larger than the bound that we have and depends on some w. In fact, if you look at the upper and lower bound, it has this very clear primal dual structure where the primal bound is framed as optimizing q function versus the dual bound is actually framed as optimizing the density ratio. So in fact you can sort of say the kernel Bauman loss for q, and here is the density, the loss for the density ratio as well. So now you can use for any w, you can actually construct a confidence bounce in this way. It's like assigning an error bar around the density ratio estimation.
00:22:30.180 - 00:23:04.198, Speaker A: And we can actually find the best w by minimizing the loss. In this case, the idea is that you don't have to solve the thing into global optimal solution. You just want to find whatever the best, because you will always guarantee the one delta condition. It's just that if you optimize delta better, you will get a tighter bound. Okay, so this is in fact our method. It's a very simple algorithm. So basically you, you find a density estimation, density ratio estimation by minimizing this loss.
00:23:04.198 - 00:23:35.108, Speaker A: Try your best. You don't have to find the global optimal and then construct your bound in this way. So here's a few discussions. I'm not sure I will have time, but the idea is that our method actually critically based on the assumption that the q function actually belongs to a family, model family. It's very similar to Shupa's learning, where we have to assume certain model family. We cannot do statistics without any assumption. Right.
00:23:35.108 - 00:24:22.890, Speaker A: It turns out we cannot actually positively verify the assumptions. Again, it's the same thing as the statistics. Well, we cannot absolutely verify the model assumptions we can reject, but we can never absolutely satisfy. So that's a model specific model misspecification problem. And we can actually show that even if we cannot verify that assumption, we can still kind of interpret our method as trying best as trying to get upper bound of some kind of oracle estimation that is accessible given the data. So I wouldn't get detailed, but if that's a question, I can explain more. So, we have some simple example here.
00:24:22.890 - 00:24:48.740, Speaker A: So this is going to be what it looks like. If you check the gap between the, the confidence intervals actually decays nicely with the data. So it's a consistent algorithm. These are a few comparisons with other methods. So basically what we observe is that. So the green line is our methods, right. And if you use the important sampling with concentration bound, you will guarantee non asymptotics as well.
00:24:48.740 - 00:25:12.414, Speaker A: But it will be much looser than our bound. And then if you use other methods, like the bootstrap, for example, you cannot guarantee non asymptotic bounce. You will have the risk of not catching the true value, which is if you have the non synthetic guarantee, you can always, with hyper b, catch the two errors. Okay, so I'm stopping here. I'm sorry that if this is over time.
00:25:14.994 - 00:25:23.934, Speaker B: Thank you so much, Chiang. I think there was some discussion, some questions from Chaba about independence assumptions. So I'll open the floor for questions and we can start that.
00:25:25.294 - 00:26:03.662, Speaker C: Yeah. Hey, very nice. I read a paper a while ago and I kept confusing myself about multiple things, I guess maybe at a very high level. You also said at the end is that this realizability is highly crucial. And what you're doing is that rather than assuming that you would estimate the model, you just assume that a function or features are given to you. Maybe you can push it to infinite dimensional spaces, whatnot. But so you're given these features or archeage space, and you assume reliability.
00:26:03.662 - 00:26:06.742, Speaker C: And that's the way you avoid the mixing assumption.
00:26:06.838 - 00:26:08.234, Speaker A: Exactly, exactly.
00:26:09.574 - 00:27:11.164, Speaker C: People would normally just estimate a model and then based on the estimated model, you can evaluate any policy, and then you can get confidence bonds with that way as well by assuming realizability in a way, but this way you're kind of like. So you could do the same thing. What I want to say in linear MDP is with style linear mdps, but then that's a little bit stronger than the assumption that you're doing. And then you kind of avoid estimating the MVP, go directly for estimates the action value function, and you just show that everything like including confidence intervals, work out super nicely, which is very neat. So with that, let's make this a question. So, do you have any thoughts about what to do in the unrealizable setting? How bad is that? I have some dark thoughts about that.
00:27:12.844 - 00:27:34.944, Speaker A: So do you mean the case when the model assumption doesn't satisfy? Of course, yeah. Right. So I think this is a very tricky question. Here's the thing. Right? So we are taking the model of space to be an architecture space. It's a kernel space, it's infinite dimensional. So we can, so there will be two cases.
00:27:34.944 - 00:28:11.928, Speaker A: One if the, so we have, the data is given, and as I mentioned here, we can actually, this is a very special thing about this algorithm. What we can do is we can take the data and just look at data very carefully, using any algorithm to figure out a queue. And the queue can actually depend on data in an arbitrary way. This is unusual. This is what's different from the answer by its name. We can look at the data and construct a queue such that it's large enough, such that we cannot statistically reject the assumption. So there were no model misses in this case.
00:28:11.928 - 00:29:00.540, Speaker A: The only difficulty here is that when I construct a very large queue such that I cannot reject my hypothesis, I can still, I still cannot guarantee that my true queue will in fact be inside the queue that I have, because this is because the state assumption that we had earlier was really weak. We didn't assume anything about the distribution of the state. So because of this weak assumption. In fact, statistically, we cannot identify case reject. We cannot positively decide a case when q is indeed in the model space. So we don't have the model misses specific problem. We more like have the model non identifiability problem working.
00:29:00.540 - 00:29:16.074, Speaker A: We cannot decide among set of models given the information that we have. We fundamentally cannot decide the model. But it's not about mission, because if there's a miss specification, we can actually reject that.
00:29:16.534 - 00:29:18.814, Speaker C: How does rejection work? Exactly?
00:29:18.974 - 00:29:33.582, Speaker A: So rejection is that you can calculate the loss function of the functions inside the queue, inside your distribution, and if.
00:29:33.638 - 00:29:48.190, Speaker C: All the queues you're writing so I can't see it. Yeah, like you have to make that recording go away. I don't know whether it's on everyone's screen, but I write some other point. Yeah, right. Okay, good.
00:29:48.342 - 00:30:14.284, Speaker A: Oh, okay. That's what we have. Okay. Yes. So the idea is that you can calculate the kernel loss for every queue inside of model space. And if every, if that lower bound is large, then we'll reject, basically, if every data point, if every queue inside the function space has a large kernel loss, Bellman loss, then you reject it.
00:30:15.864 - 00:30:36.534, Speaker C: Okay, so you basically say that there is no way I can support the data with my function class. Then you say that my function class was not big enough. Is that going to happen if you have a universal arcade space?
00:30:36.954 - 00:31:50.704, Speaker A: Yes. Assume I have a function. Let's say I'm thinking about shoe prize learning problem. I have a set of data points. I observe the value of final number points, and I want to feed a curve, right? If you don't have any assumption at all about the curve, you cannot learn anything, because what you observe is on what you observe. You can never extend your observation, your information, to the points that you don't observe unless you make some assumption, right? And that assumption cannot be verified from data itself, has to be verified from, as a prior knowledge, right? So, for example, in order to fit this curve, I have to assume that the curve is sufficiently smooth so I can do any kind of learning, right. If I don't make any assumption, that the model could be arbitrarily kind of messy, right? So even if you learn a model that fits the data perfectly well, you can never guarantee anything about unseen data unless you have some sort of assumption.
00:31:52.174 - 00:32:04.874, Speaker C: Sure, I was asking the other direction. I didn't know you were saying that. Okay, if you assume smooth test, then obviously if your data is very jagged, then you're gonna say that, okay, I reject it, or something like that.
00:32:05.814 - 00:32:19.374, Speaker B: Okay, I think on this note, possibly we should unfortunately move to Bose, talk to. Sorry, Chaba, but maybe we can continue this over the chat. Thank you so much, Chiang, for the talk.
