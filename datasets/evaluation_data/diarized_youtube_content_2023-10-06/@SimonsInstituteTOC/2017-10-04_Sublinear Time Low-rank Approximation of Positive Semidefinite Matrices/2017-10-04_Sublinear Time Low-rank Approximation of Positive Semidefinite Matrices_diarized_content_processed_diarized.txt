00:00:07.160 - 00:00:36.796, Speaker A: So the next talk is by David Ludwig from CMU on the sublinear time low rank approximation of PSD matrices. Okay, so thanks to the organizers for inviting me. This was done with when I was still at IBM, Ahmadine with my intern at the time, Cameron Musko. So yeah, so, sublinear of time low rank approximation of posi seminary definite matrices. Um, uh, is the title. And let me first, uh, uh, define the low rank approximation problem. So, uh, we're given an n by d matrix a.
00:00:36.796 - 00:01:15.704, Speaker A: Think of the n rows of a as being points in r to the d. For example, a might be a customer product matrix a indicates how many times a customer I purchased some item j. And in many applications a is well approximated by a low rank matrix. There might be some underlying latent variable model or whatever, but typically a is well approximated by a low rank matrix. And the goal is just to find a low rank matrix approximating a. So a low rank matrix is easy to store in factored form. It has many fewer parameters.
00:01:15.704 - 00:02:11.126, Speaker A: You know, if it's an n by k times a k by d matrix, I only have n plus d times k parameters. Um, it's quick to multiply by a low rank matrix because I can multiply a vector first by the first low rank factor, then by the second low rank factor. And really, if a is only high rank because of noise, you can think of this low rank factorization as denoising, uh, uh, your data. So, um, what is a good low rank approximation? Uh, the standard way of getting a low rank approximation is through the singular value decomposition, or SVD. So recall that any matrix a can be written as u sigma v, where u has orthonormal columns. Sigma is a diagonal matrix with non increasing entries as you go down the diagonal, non negative and v as orthonormal rows. And the standard way of getting a low rank approximation is via the truncated SVD.
00:02:11.126 - 00:03:08.314, Speaker A: So what you do is you just take sigma there, decreasing as you go down the diagonal. So you just take the top k singular values, the diagonal entries of sigma, leave them alone, and zero out all the remaining singular values. This gives me sigma sub k, and this effectively selects the k leftmost vectors in u and the k uppermost vectors in v. This gives me a rank k matrix, which I'm calling a sub k. And so this approximates a up to some error matrix e, which depends on all the remaining singular values. Um, okay, so why do people use the truncated SVD for low rank approximation? Well, a sub k is the best rank k approximation for a under a variety of different norms. In particular, it's the arg min over rank k matrices b of the frabinius norm of a minus b, where the frabinius norm is just the sum of squares of entries to the one half.
00:03:08.314 - 00:04:03.134, Speaker A: So the issue with the truncated SVD is actually that computing a sub k is expensive. Okay, so computing this exactly, um, using the SVD, uh, can take, uh, n d squared time if a is an n by d matrix. And we want to get much faster, uh, algorithms, uh, than those based, uh, uh, on the SVD. So, um, people have been looking at a notion of approximation for low rank approximation, which is a relative error notion, uh, defined as follows. So the goal is to still output a rank k matrix. Let me call it a prime, so that the forbidden norm now of a minus a prime is at most relative error, one plus epsilon times the best possible low rank approximation cost given by the SVD. And you allow some probability of failure.
00:04:03.134 - 00:05:11.592, Speaker A: And with this relaxation, you're able to get significantly faster algorithms. Um, in particular, uh, in work, uh, with Ken Clarkson, um, we showed that, uh, with this notion, you can get an algorithm which runs in time n and z of a, where n and z denotes the number of non zero entries of your input matrix a. Okay? So this algorithm is sensitive to the sparsity of your matrix a, uh, plus what is typically a low order term, uh, n plus d, the sum of the two matrix dimensions, times a polynomial in the rank parameter k, which is typically small, and one over epsilon for a one plus epsilon approximation. Okay, so just some perspective. Even for dense matrices that have, say, nd entries, the svd would take n times d squared time, whereas this algorithm runs in only n d time. And if n and z of a is significantly less than that, like around n plus d, then this could be close to an n plus d time algorithm. Okay, so, um, and stop me at any point if there are any questions.
00:05:11.592 - 00:05:59.434, Speaker A: Um, let me just, uh, uh, go over roughly how you use, uh, a technique called sketching, uh, to achieve these, uh, these algorithms that run in time, uh, proportional to the number of non zero entries of a. Okay, so, um, we're given this n by d input matrix a, and we think of the rows of a as being n points here in r to the d. And the first step of these algorithms is to choose a random matrix s. And I'll discuss just a few families of matrices in a moment. But the important property about s is that it's a very wide fat matrix. Okay? So it has a very small number of rows, um, has roughly k over epsilon rows. Okay.
00:05:59.434 - 00:06:34.190, Speaker A: And the number of columns is n. And what we do is we compute s times a. Okay, this is the first step of the algorithm. Now what is sa doing? It's taking random linear combinations of these endpoints, the rows of a in r to the d. And we can look at s times a. So it's a k over epsilon by d matrix. We can think of the span of the rows of s times a as being a k over epsilon dimensional subspace of r to the d, which I've drawn here as a line.
00:06:34.190 - 00:07:20.734, Speaker A: Okay, so this line represents the row span of s times a. And the next step of the algorithm, after computing s times a, is to take each of the rows of a these points and to project the point onto s times a. Right, just move it to the closest point in s times a. Okay. And now I still have n points, but they're all sitting inside the row span of s times a, which is a very low dimensional subspace of r to the d. So recall that the SVD, it took number of points times dimension squared time. Now, I still have n points, but the dimension of these points, since they sit in this caver, epsilon dimensional subspace is only k over epsilon.
00:07:20.734 - 00:08:01.644, Speaker A: So I can afford to run the SVD now on these projected points if I use the coordinate representation in this low dimensional subspace. Okay, so that's the idea. So one issue with this algorithm is you can't actually afford in n and z of a time to project these points exactly onto this space, s times a. But projection is just least squares regression. And there are many sketching algorithms, randomized algorithms for approximately doing projection. So you can approximately move these, you move them to an approximate closest point and the algorithm still works. Okay, so this is how you get, um.
00:08:01.644 - 00:08:58.988, Speaker A: Um, this is the basic paradigm for applying sketching, uh, to low rank approximation. And a natural question here is which, uh, random families of matrices s do you use? Um, so the first one that might come to mind is a matrix of IAd, normal random variables. Uh, this matrix s works, but, uh, computing s times a is slow because s is a dense matrix. Um, uh, what Sarlo showed, uh, is instead of choosing this random family of matrices, if you choose a family of matrices from a much more structured family called fast Johnson linens Ross transforms, then you still have roughly k over epsilon rows. But now s times a can be applied in roughly nd time, which is very fast. It's linear in the product of the dimensions of your input matrix. However, one drawback about this is s can't exploit sparsity in a.
00:08:58.988 - 00:09:48.124, Speaker A: So if your input matrix a has a small number of nonzero entries, you could hope to do better. And what we showed in that paper with Clarkson is that instead, if you choose s to be, um, what is called a count sketch matrix, um, it has a bit more rows than before. Instead of just k over epsilon rows, has roughly k squared over epsilon rows, roughly, um, but still independent of n and d and still very small. So it's still a wide fat matrix. But a nice property about this count sketch matrix is that it's extremely sparse. Okay? So each column of this matrix s only has a single non zero entry in it. That entry is chosen at a uniformly random position in each of the n columns.
00:09:48.124 - 00:10:49.706, Speaker A: Okay? And that position is set to be one with probability one half and minus one with probability one half independently for each of the n columns. Okay? And the nice thing about this count sketch matrix s, um, is that s times a can be computed in n and z of a time. Okay? So for each column of a, for each non zero entry of that column, it corresponds to a column of s, and there's only a single non zero entry in that column. So I can update the matrix product s times a. And you can show that this, uh, this, this approach of using sketching for low rank approximation turns out to give you a good relative error approximation with large probability if the number of rows of s is an appropriate polynomial in k over epsilon. Okay, so this is the sort of, this was the state of the art for using sketching for low rank approximation. It gives n and z of a time.
00:10:49.706 - 00:11:44.254, Speaker A: Um, recently, uh, what we've been looking at is, uh, what I call, uh, structure preserving low rank approximation. Um, so, uh, now let a be an arbitrary n by n matrix. Not only do we want, uh, a low rank approximation, a generic low rank approximation to a, but we also want our low rank approximation to have a certain structure. Okay? So suppose we require our rank k approximation, a prime, for example, to be positive, semi definite, or PSD. Okay? So the just, the definition is that a prime is a symmetric matrix and all the eigenvalues are non negative. Okay? So now can we, can we still solve this problem very quickly if we require this additional property about a prime? This occurs, PST matrices occur all over the place. For example, covariance matrices, kernel matrices, laplacians.
00:11:44.254 - 00:12:33.166, Speaker A: Um, and, uh, your matrix a, uh, that you want to approximate itself might not be a PST matrix. For instance, uh, maybe it was supposed to be a PSD matrix, but because of round off errors in an application, uh, turned out to not be PSD. But still knowing that it should be PSD, you want your low rank approximation, a prime itself, to be PSD. Okay, so, um, uh, yeah, so the goal here is just to output, uh, a rank k PSD matrix, a prime for which the frabinius norm of a minus a prime is small. To solve this problem, you can first assume that the matrix a is actually a symmetric matrix. Okay, so right now this, we have this arbitrary n by n matrix. We're assuming nothing about it.
00:12:33.166 - 00:13:16.724, Speaker A: We just want a rank k matrix approximating it. And I'm saying that this arbitrary n by n input matrix, we might as well assume it's a symmetric matrix. Why is that? Well, you can write any matrix a as a symmetric part a sym plus an anti symmetric part a. Asym. How do you do this? Well, the symmetric part is just obtained by averaging the corresponding pair of off diagonal entries, aij plus aji over two. And the asymmetric part is obtained by taking the difference of the off diagonal entries divided by two. And the important point is that an asymmetric matrix, if you think of it as a vector, it's orthogonal to a symmetric matrix.
00:13:16.724 - 00:14:10.744, Speaker A: If you think of that as a vector, because you'll have two corresponding entries that are equal in the symmetric matrix. But in the asymmetric matrix, they'll have opposite sign, so the inner product will be zero. And because of that, if I'm approximating a by a symmetric matrix a prime, then by the Pythagorean theorem, I can write the squared fabinius norm of a minus a prime as the symmetric part of this matrix, which is a minus a prime. The squared norm of that plus the squared norm of the anti symmetric part of the matrix. Okay, this is just the pythagorean theorem, since these are orthogonal. And notice that this second part doesn't even depend, uh, on a prime, you know, the rank k matrix that you're trying to find. So any, any rank k matrix, a prime that you find is going to have to pay the cost of the second part.
00:14:10.744 - 00:15:12.344, Speaker A: So I might as well just assume that my input matrix a is symmetric, because I can compute its symmetric part in n and z of a time, right? I just average the off diagonal entries, and now I have a symmetric matrix. Okay, so a is symmetric. And what is the best PSD rank K approximation, which I'm going to call a sub K plus to this matrix a. So if I give you an arbitrary symmetric matrix a. Can you tell me what it's best rank KPSD approximation is? So you stop at the forsyth at the last, it's non negative or the gate one, whichever one comes soon. Yeah, I mean, so you zero out all but the top k positive eigenvalues of a. And that gives me, yeah, a sub k plus.
00:15:12.344 - 00:16:02.142, Speaker A: Okay, so, um, what is known about this problem? Uh, so in earlier work this year, uh, with Clarkson, we showed that you can still get input sparsity time. Um, and you can find PSD rank k matrix a prime which has this relative error guarantee. So it's, the cost is at most one plus epsilon times the cost of using this optimal, um, a sub k plus. Um, and this, uh, this improves several previous works. So one thing you might be familiar with is what's called the nicerom method, which is a method which occurs in, uh, um, in, in kernel, in kernels and machine learning. Um, so this is a very fast method, but it doesn't have provable guarantees. It's based on sampling columns of a.
00:16:02.142 - 00:16:55.954, Speaker A: And in order to get provable guarantees, you need to make incoherence assumptions on your input matrix a, whereas our running time statement holds for an arbitrary a. Um, there's also a bound by giddens and Mahoney which showed that you can find a PSD, a prime with a weaker error guarantee. So the error, instead of being epsilon, times the frabinius norm of a minus, the optimal matrix is epsilon times the nuclear norm, the sum of the singular values instead of the sum of the squares, which can be quite a bit larger. Um, there's work by Wang et al. That, uh, uh, gets, uh, a bit worse of a running time, roughly n squared k over epsilon time, and has a larger rank caver, epsilon. But. Oh, okay, so, so this is input sparsity time for PSD low rank approximation.
00:16:55.954 - 00:17:33.670, Speaker A: And, um, so the question is like, how good are these algorithms? So, um, so for general matrices a, uh, there is an n and z of a time lower bound. You have to read almost all of the entries if you want to get a relative error low rank approximation. Um, now why is that? So, here is a very simple example. Matrix a. Um, it has a bunch of ones, and it has some zeros, and one of these entries is infinity. Okay? And it's placed at, say, a uniformly random position. And you have no idea where this entry is.
00:17:33.670 - 00:18:32.924, Speaker A: If you don't read most of the nonzero entries, then you're not going to find this infinity entry with large probability. But if I'm a good rank one matrix. You know, the first thing I'm going to do is I'm going to assign infinity to this entry, you know, because I'm taking the difference of this in a rank one matrix, I better wipe out that infinity term. So I need n and z of a time if I want a good relative error approximation. And you can actually show that there's also an n and z of a time lower bound, even if you require outputting a PSD matrix to an arbitrary input matrix a. Okay, so in fact, these lower bounds, they're even for an easier problem of just someone gives you a matrix a and asks you what is the norm of this matrix? I mean, that I still have to read n and z of a entries to find this infinity. If I don't do that, I'm not going to estimate its norm very well.
00:18:32.924 - 00:19:29.124, Speaker A: Okay, so there's already a lower bound for this problem. So now the next question that we had is the following. Suppose the input matrix a, this n by n matrix is, is itself PSD, right? So earlier we were trying to approximate an arbitrary matrix by a PSD matrix. Now our input itself is PSD. And you know, maybe we just want to approximate a PSD matrix for efficiency reasons. The question is, is there an n and z wave time lower bound for low rank approximation of PSD matrices? Or stepping back to this even easier question, is there an n and z of a time lower bound just for estimating the norm of a PSD matrix, you might only need the diagonal. You might treat the diagonal and get an idea.
00:19:29.124 - 00:20:06.694, Speaker A: Or. Yeah, so the diagonal is very key to this problem, and that's the right intuition. And it turns out that you can estimate the norm of a PSD matrix reading roughly only order n entries. Okay, so let me just give, this will help for intuition for the low rank approximation. So let's see how to estimate the norm of a PSD matrix in sublinear time. So I'm looking at the squared Frobenius norm of some PSD matrix a. Now, a is PSD, which, which means that I can write it as b times b transpose.
00:20:06.694 - 00:20:31.644, Speaker A: I can think of b as being the square root of a. This is well defined since a is PSD. So what is the squared frabinius norm of b b transpose? Well, that's the sum of squares of all the entries. Well, what is the ijth entry? That's just the inner product of the ith row of b and the jth row of b. Right. The jth column of B transpose. And I'm looking at the sum of squared entries.
00:20:31.644 - 00:21:23.464, Speaker A: Okay, um, well, what do we have? So, by Cauchy Schwarz, let's look at one of these entries. Inner product of Bibj squared by Cauchy Schwarz, this is at most the squared norm of bi, the ith row of b times the squared norm of the jth row of b. Right. And what are these entries? These are the diagonal entries of a. Right. So, okay, so suppose all the diagonal entries of a were one, just for some intuition. And what does this statement mean? Well, this Cauchy Schwarz is then saying, well, then all the off diagonal entries have to have absolute value at most one.
00:21:23.464 - 00:22:05.530, Speaker A: So now I can't hide this infinity in an off diagonal entry because it means that one of the diagonal entries must also be infinity. And now let's suppose the sum of squares of the off diagonal entries actually contributed a lot to the fabinius norm of a. In particular, the sum of squares of the off diagonal entries was at least an epsilon fraction of the overall fabinius norm. Which means, suppose it's at least an epsilon fraction of the contribution to the fabinius norm on the diagonal alone. Well, all the diagonal elements are one. That was just by assumption. This is one.
00:22:05.530 - 00:22:50.404, Speaker A: So this means the sum of squares of off diagonal elements is at least epsilon times n. If it's not, then it doesn't matter. Like if we know the sum of squares of the diagonal elements is n, every diagonal element is one. And if the off diagonal elements, if their sum of squares, contributes to the overall squared FRAbinIUS norm, well, its sum of squares had better be at least epsilon n. If it's smaller, then we don't even need to look at the off diagonal elements. Okay? But now we're in a situation where we have a bunch of small terms. All of these terms bibj inner products squared, or at most one, and they sum up to something very large, like epsilon n.
00:22:50.404 - 00:23:28.756, Speaker A: So what do you do when you have a bunch of small things summing up to something very large? Yeah. So uniformly sample. Right. So uniformly sampling. If we sample roughly n times a polynomial in one over epsilon of these off diagonal terms, you know, we average them and then we scale back up by the reciprocal of the sampling probability. We get a good feeling, we get a good estimate for the sum of squares of the off diagonal terms. So we only had to sample n off diagonal terms, and that's it.
00:23:28.756 - 00:24:21.296, Speaker A: So we had an order n sub linear time algorithm. So, just pictorially, we were assuming that the diagonal entries were all one. The sum of squares of off diagonal entries was at least epsilon n, and each off diagonal entry is at most one in absolute value. These were these two conditions, and they imply that uniformly sampling entries gave me a good approximation to the squared frobenius norm. And something very similar happens when the diagonal entries are not all equal to one. You just do importance sampling instead of uniform sampling. So, um, just quickly, uh, define the probability p I j of sampling an entry ij to be the squared, uh, rho norm of bi times the squared row norm of bj divided by a normalization factor.
00:24:21.296 - 00:25:37.644, Speaker A: This factor is the sum of squares of all of these. Um, so notice that this probability pij can be computed just by reading the diagonal entries, right? This is the ith diagonal entry of a. This is the j diagonal entry, and this is the sum of all them. So if I just do important sampling, if I just let my random variable x be the squared value of the ijth off diagonal entry, I mean I j th entry divided by the probability of sampling it, then just, the standard thing here is that the expectation of x is the sum over all entries I j of the probability of sampling the entry times the squared value over a probability of pij. This is my random variable. If I sample it and I get that the expectation is the squared norm and I can upper bound the variance by this second moment and just the sum overall ij of the probability of sampling that entry times the squared value if I sample that. And now I just use the fact this Cauchy Schwarz, again, I cancel one of the probabilities and I use Cauchy Schwarz to upper bound this inner product in terms of the squared norms of the rows.
00:25:37.644 - 00:26:58.624, Speaker A: And so I can, overall, you can show that the variance is at most n times the squared expectation. So this means that if I take roughly n samples, I'll get a good approximation, uh, with high probability, okay, I mean with good probability, okay, so PsD matrices, it's possible to estimate uh, their norm and um, so the previous lower bound strategy doesn't apply. And our main result is actually that uh, given an n by n p s d matrix in roughly n times k squared poly one over epsilon time, we can output a rank k matrix a prime, which achieves relative error that the Frobenius norm of a minus a prime is at most one plus epsilon times the optimal rank k approximation cost. Okay, so this is much faster than these input sparsity time algorithms that we saw based on sketching earlier in the talk. This corresponds to say one of the low order terms in those input sparsity time algorithms. Um, uh, in fact, the number of entries we read is roughly about n times k for constant epsilon. And um, if the matrix exponent multiplication, the, the exponent of matrix multiplication is two, we can improve this to n times k.
00:26:58.624 - 00:27:39.724, Speaker A: Um, and we do show a, uh, a lower bound that you need to read roughly nk over epsilon entries. So at least for constant epsilon this is roughly tight. Um, okay, so uh, let me give you, uh, yeah, some intuition of how the algorithm works. So the starting point is a connection to uh, what's called adaptive sampling. Um, and this is an algorithm proposed by Deshpand and vampala. So uh, the idea is as follows. You'd like to, you have this n by n matrix a and you'd like to sample a small subset c of its columns, which contains, this small subset, contains a good rank k space inside of its span.
00:27:39.724 - 00:28:15.584, Speaker A: This is your goal. And how are you going to do this? What you do is you start off, you have no columns chosen and you just choose a column proportional to its squared norm. That's the first step. Now I have one column. Now I look at all my remaining columns and I compute their square distance to the column I've already chosen and I sample proportional to that square distance. Now I have two columns. Now I take two columns and I look at the span of these two columns and I look at all my remaining columns and I sample a column proportional to the square distance of the span of those two columns.
00:28:15.584 - 00:29:05.624, Speaker A: And I repeat this process. Okay, so I iteratively choose a column proportion with probability proportional to its square distance to the span of the columns that I've already chosen. Okay, so that's all this is saying. You know, I initialize my columns to empty set and I repeat this process roughly k squared over epsilon times. Just sampling proportional to the square distance to what I've already chosen. That's what this statement is saying. And what they prove is that if you do this, then there exists a k dimensional subspace v inside of the span of these roughly k squared over epsilon columns that you sampled, such that if I were to project my columns of a onto this k dimensional space, so I get a rank k matrix, then this is a good rank k matrix.
00:29:05.624 - 00:29:44.450, Speaker A: It means that the cost of using that rank k matrix is at most one plus epsilon times the optimal cost of any rank k approximation. Okay, so, okay, so this was a very simple intuitive algorithm. Um, but there's something, uh, very, uh, important to note about this algorithm, which is that, um, it doesn't look at many inner products between pairs of columns of, uh, uh, of a. Right. So I have n columns and there are n squared, uh, pairs of columns. And I could be looking at the inner product between all n squared pairs of these columns. You could imagine algorithms that do that.
00:29:44.450 - 00:30:05.346, Speaker A: This algorithm is not doing that. What is it doing? It's actually looking at a very small number of inner products between pairs. In the first step. What is it doing? It's just sampling a column based on its squared norm. Well, that, that, that's, that's just looking at n n pairs of columns like a column with itself. Right. Because its square norm is just its inner product with itself.
00:30:05.346 - 00:30:31.934, Speaker A: I'm looking at n of these, then I sample a column. Then I want to look at my remaining n minus one columns and find their distance to the column I've already sampled. That just involves knowing the inner product of that column. With the column I've already sampled, that's just n more inner products. Now I have two columns. And now I need to know these n minus two columns. I need to know their inner products with both of the columns I've already sampled.
00:30:31.934 - 00:31:23.804, Speaker A: So in fact, this algorithm, it only needs n times the size of the number of columns, which is k squared over epsilon pairs of inner products. It only has to look at a very small number of pairs of inner products of columns of your input matrix. Okay? So this will be a very important, uh, uh, property on the next slide. Um, yeah, so as I just said, it looks at, just, uh, the algorithm only looks at much less than n squared of the total possible number of inner products it could look at. And so the idea, I mean, so here, here's one idea. So, um, again, since a is PSD, we can write it as say b transpose b. And the nice thing about this is suppose we wanted to run our adaptive sampling algorithm on b.
00:31:23.804 - 00:32:11.680, Speaker A: Then a has already come and we're given a. Then a has already computed all the pairs of inner products that we might need, right? A is the matrix of inner products of all the columns of b. So what I can do is I can just run the adaptive sampling algorithm on b without actually knowing b. I just read the inner products that I need to run that adaptive sampling algorithm. And those inner products are entries of a. And we just said, well, we only need to read a very small number of entries to run the whole adaptive sampling algorithm. So I can run this whole adaptive sampling algorithm in sublinear time, roughly n k squared over epsilon time.
00:32:11.680 - 00:33:01.284, Speaker A: I just use this matrix a as an oracle, whenever I need an inner product for a pair of columns in b. Okay, so this will give me an output, a rank k approximation, which is a good low rank approximation to b. I mean, that's the adaptive sampling algorithm that we just said, right? So a is like this table of all inner products that we might need. The problem with this, though, is we wanted a good low rank approximation to a. And it turns out that if you use the same, okay, so b and a, b is the square root of a. B and a have the same singular vectors. So if there's no approximation involved, if I took the best rank k space for b, it would also be the best rank k space for a.
00:33:01.284 - 00:33:55.424, Speaker A: But since there's approximation involved, actually this is not approximation preserving. So you can show that this if you use the same space formed from that small set of columns by running adaptive sampling on b, if you use that same space for a, you can get an arbitrarily bad approximation to a. Okay, you'd really have to crank down this epsilon to be very small as a function of the spectrum of a, actually. Um, so it doesn't quite work. But what we're gonna do is we're gonna, um, but by the way, this is already useful on its own in the sense that, um, sometimes you want a low rank approximation to a factor of a kernel matrix, and you can think of a as being a kernel matrix and this being a low rank approximation to a factor form. But that's not what we're going to do here. So we still want a low rank approximation of a, and we'll see how b can still be helpful in that regard, just in a different way.
00:33:55.424 - 00:34:51.084, Speaker A: Um, okay, so, uh, what we're going to do is we're going to use the notion of what's called a projection cost preserving sketch. Now this is a sample of a set c of columns of a, which has the following property. So this was in a beautiful work of Michael Cohen et al. Very tragic news recently. And what they showed is the following, that they defined the following notion that c is a re weighted subset of columns of a, which has the following property. It has a corset like property, which is that if you give me any rank k projection matrix p. Suppose I look at I minus p, the projection onto the complement of p.
00:34:51.084 - 00:35:26.792, Speaker A: Suppose I look at the norm of I minus p times c. Then that's the same as the norm of I minus p times a up to a one plus or minus epsilon factor. Okay, so what does this mean? I have my n columns of a, and I want to compute the sum of their square distances to p. But I don't want to keep a around because it's too big and it takes a lot of memory, etcetera. So I'm not going to keep it around. I'm just going to compute this c, which has a much smaller number of columns. And every time you ask me to, what is the distance to your favorite rank k space? I'm just going to use C and compute it in terms of C.
00:35:26.792 - 00:36:35.904, Speaker A: And I know that's a good approximation to a. Okay, so it's a core set in the sense that it holds for every matrix p simultaneously every projection matrix p. Um, okay, so um, a nice thing about, uh, uh, um, projecting cost preserving sketches or pcps, is that if you find a rank k projection matrix p, which minimizes the left hand side, it also minimizes the right hand side. Right, because it preserves everything up to one plus epsilon. So, you know, what does that mean? Well, it means that if somehow, someway, I was able to get a PCP for a, for the columns of a with a small number of columns, say a number of columns, which is roughly like polycave or epsilon, then I could just take C and I could find its best, its top k singular vectors, its best k dimensional space. Why? Because its best k, its top k singular vectors, minimizes the left hand side. And so because it's a PCP, it's also a good one plus epsilon approximation for the right hand side.
00:36:35.904 - 00:37:10.994, Speaker A: So once I have c, forget about a, I just find the top k left singular vectors of C, and I get a good approximation for a. Yeah, and if C has a small number of columns, I can afford to read all the entries in C. There are only n entries in each column of C. And I'll get like roughly nice polycaver epsilon time. Um, so really, if we can find this, this PcPC, um, also in sublinear time, like roughly n time, then we're done. Our goal is just to find this PCP. Okay.
00:37:10.994 - 00:37:53.584, Speaker A: Um, yeah, so how are we going to do that? So, we want to sample columns of a to build a PCP. So we're going to use a notion, um, called the ridge leverage scores. And it's not for this talk, it's not so important if you understand their definition. But this was introduced in several different works at the same time, similar times. Lee, Miller, peng, kaprila, Vidal, Illawi and Mahoney. Basically these are so it's just some quantity. For each column of a, I define what's called a ridge leverage score, tau sub I of a.
00:37:53.584 - 00:38:51.088, Speaker A: Um, what this quantity is, is it gives a smooth version of the standard rank k leverage scores if you've seen these before. So um, yeah, if you haven't seen these before, it's not, it's really not important what they are for the talk. But um, the rank k leverage scores, what they are is if you take the top k left singular vectors of a matrix, um, these, these top k left singular vectors, uh, they um, like if I take the top k left singular vectors of a matrix u sub k, then I know the squared column norms of this matrix are all one because these are orthonormal vectors. And the squared row norms, the ith squared row norm is called the ith, uh, uh, leverage score. The ith rank k leverage score, okay. And these can be arbitrary, I mean, they sum up to k. Since the sum of the columns is k, the sum of the rows is also k.
00:38:51.088 - 00:39:34.448, Speaker A: But in any case, the ridge leverage scores, they're useful as they give sort of a smoother version of this. I mean they're robust to slight perturbations in a, et cetera. The sum of all the rank kick, all the ridge leverage scores is at most two k. So slightly larger than k as it is here, but it's still small, which is what we'll need for sampling based algorithms. Um, and there's some connection to ridge regression if you've seen it before. These correspond to the standard leverage scores of taking your matrix a and concatenating them with a scaled identity matrix. Um, but uh, again, it's not so important if you haven't seen it.
00:39:34.448 - 00:40:29.252, Speaker A: Um, so how are we going to use the ridge leverage scores? So it turns out that they provide a PCP. Okay, so let me say that more precisely. So in work of Cohen, Musko and Musko, they showed the following. Suppose you have overestimates tilde tau I for each of your actual ridge leverage scores. Tau I. Let's define a probability distribution on the columns of a p sub I corresponds to the probability for the ith column and it's just proportional to the tilde tau of I, the ith approximate ridge leverage score. Then we're gonna sample a set c of t columns of a, ignore for the moment what t is, but we're going to do it with replacement.
00:40:29.252 - 00:41:22.804, Speaker A: And the ith column of c is just going to equal the jth column of a with probability p, sub j scaled by roughly square root of t times the probability that we sample that column. Okay, so we're sampling t columns. We scale by roughly one over the square root of t times the probability that we sample it. Okay, um, the number of columns you sample, it's proportional to the sum of your overestimates to the actual ridge leverage scores. Okay, so if you had the actual ridge leverage scores, the sum of them would be k and you would sample roughly k log k over epsilon squared columns. And yeah, if you have, you know, sort of crude estimates, you have to sample more. Um, but they show that if you sample according to this distribution, you get a PCP.
00:41:22.804 - 00:42:23.324, Speaker A: And we said if you have a pcP, then we're done with our problem. Okay. Um, but the question is, you know, how do we get the ridge lever scores or how do we get good approximations to them in sublinear time? Okay, so I mean, it doesn't seem obvious how to, the ridge leverage scores are defined by some complicated expression. How can we get these in sublinear time? Um, so, uh, what we're going to do is use the connection that I mentioned earlier to this literature on uh, uh, on kernel matrices in machine learning. So we said a is equal to b b transpose, since it's PSD. So we can think of a as being the kernel matrix of what's called a linear kernel. And there is work, uh, on uh, by Musko and Musko in last year which shows how to approximate the ridge leverage scores of b up to a constant factor.
00:42:23.324 - 00:43:00.814, Speaker A: And the time that you need is n times k. Kernel evaluations. And now we're going to use this connection again, that one kernel evaluation is just an entry of a. Right, because this is a dot product. A kernel evaluation for a linear kernel means a dot product. So there's an algorithm that can compute the ridge leverage scores of b, each up to a constant factor, by reading only nk entries of a. Okay.
00:43:00.814 - 00:44:01.884, Speaker A: Um, and what we show is that the ridge leverage scores of a are related to the ridge leverage scores of b. Okay? Even though we said if you get a good low rank approximation to b, it could be arbitrarily bad low rank approximation to a. But what we do have is that at least the ridge leverage scores of b, if we scale them by roughly root n, then they're overestimates to the ridge leverage scores of a. And okay, so what is the sum of all of our overestimates? This is the, this is the tilde tau I we're gonna use. It's going to be root n over root k times the ridge leverage score of b. Well, we said the sum of all the ridge leverage scores of a matrix is roughly k, and the number of columns that we need to sample is proportional to the sum of these approximate ridge leverage scores. So when we sum all these up, we're gonna get roughly root n times root kick.
00:44:01.884 - 00:44:52.974, Speaker A: And this works. It turns out that if you sample roughly root n, root k columns of a according to the ridge leverage scores of b, then you get a PCP for a. Okay, and this PCP. So this already gives us an algorithm we know, in order n time, we can find all these good approximations to the ridge leverage scores of B. Then we know that if we oversample by a root n factor, if we sample roughly root n k columns of a proportional to these, then we'll get a subset of root n columns, which is a PCP for a. And therefore we can just find the best rank k approximation inside of there. That already leads to an end to the three two time algorithm, which is already, you know, sublinear time for certain values of n and z of a.
00:44:52.974 - 00:45:29.914, Speaker A: The final step, which we wanted, actually, an n time algorithm. What we've shown is how to take a sample roughly root n columns to get this matrix c. Um, there's one other step, is we show how to sample roughly root n rows of this matrix c to get a matrix r, which is root n by root n. And it turns out that actually the sampling probabilities are the same ones that we used for C. They're again the ridge leverage scores of B. And we said we could compute those in sublinear time very quickly. Um, but I won't get into the details of that.
00:45:29.914 - 00:46:11.440, Speaker A: Um, but the, but the point now is r is a very small matrix root n by root n. And now you can do whatever you want on this. Okay? So you can afford to run these input sparsity time algorithms on R because they'll take at most n time. Okay, so since r is so small, you can use the earlier sketching techniques I described to find its top k right singular vectors. Since R is a PCP for C, its top k right singular vectors can be used to find the top k left singular vectors of C. And since C is a PCP for a, its top k left singular vectors are a good low rank approximation to a. So that's the overall like logic behind the algorithm.
00:46:11.440 - 00:47:03.594, Speaker A: Like you, you reduce it to a small square matrix, do whatever you want, the top k singular vectors. Then there's a procedure of using those top k singular vectors to find the, because r sits inside of C here. So I find its top k directions and I can actually sample from them to get k directions inside of C that are very good for C. And since those are very good for C, they're also very good for a, because C is a good representation of a, it's a PCP of a. So overall, the algorithm runs in Nk poly one over epsilon time. So just to conclude, um, yeah, we give the first sublinear time algorithm achieving relative error, low rank approximation for a natural family of matrices. So for pse matrices, which bypasses this n and z of a time lower bound for general matrices.
00:47:03.594 - 00:47:55.524, Speaker A: Um, in fact, our concrete bounds are fairly tight. I mean, we're getting n times k entries and that we can prove a lower bound for that, you might ask for other error measures. Not everyone is just interested in the Frobenius norm. It turns out that if you want spectral norm error, that's impossible in sublinear time. We can show that, but you can get sort of a mixed guarantee where it's like a mixture of a spectral and a Frobenius guarantee. You output a rank k matrix, a prime for which the squared spectral norm difference is at most one plus epsilon times the optimal squared spectral norm difference plus a term that depends on the frabinius norm. So epsilon over k times the squared frabinius norm of a tail that you can do in sublinear time.
00:47:55.524 - 00:48:22.914, Speaker A: I mentioned this earlier problem in the talk of sometimes you want to output a matrix which is itself PSD. You want your low rank matrix to be PSD. We can do that here too. So we're given a PSD matrix and we output a rank k PSD matrix in roughly n time. And the open questions here. So a natural question is to get tight bounds on this epsilon parameter. Our upper bound is roughly like one over epsilon to the 2.5,
00:48:22.914 - 00:49:27.196, Speaker A: and our lower bound is roughly one over epsilon. And sort of more generally like, are there other natural families of matrices for which you can bypass skirt around this n and z of a time lower bound and get like a truly sublinear time algorithm? So that's all. Thanks. Maybe one quick question. What if the matrix is just a little bit indefinite? So, you know, I see, so you have like your PSD matrix plus an error matrix if it's small enough, and if you just get a, in certain cases, if you just get a low rank approximation to the PSD part, use these techniques. I'm not sure how robust it would be in that case. Certainly if the error is small enough, then I think it should go through, but I don't know exactly what the limit of that is.
00:49:27.196 - 00:49:46.284, Speaker A: Yeah, good question. Yeah. Okay, so while the next speaker is setting up, I just wanted to make a final announcement regarding the short.
