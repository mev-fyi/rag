00:00:00.760 - 00:00:25.994, Speaker A: Thanks a lot for the invitation. It's really a pleasure to be here. Let me tell you what this talk is about. It's going to be really about the use of semi definite programming and in particular something called, generally speaking, semi definite hierarchies for problems in quantum information. And here's another. I mean, this is going to be pretty introductory. Hopefully if you have questions you'll stop me and we can clarify those.
00:00:25.994 - 00:01:23.544, Speaker A: And I really want to emphasize somehow the basic ideas and results and intuition, I mean, why these kind of things work rather than perhaps incomprehensible or overly technical. And somehow the kind of things I want to focus is on just very shortly, at the beginning at least a geometric viewpoint on semi definite optimization, which I think is very useful. Then somehow I'm going to describe shortly at least one motivation to come into all these hierarchies, which is what happens as you move on from choralic forms of polynomials. And we're going to see immediately how that allows us to answer questions, for instance about consistency of moments, or questions about, for instance, entanglement and separability. And then we'll move on to kind of the non commutative versions because we want to see that somehow it's exactly the same thing. I mean, you move on from commutative to non commutative stuff, but this formulation of the semidefinite problems is essentially the same. So what semi definite problem? I guess we used it already, but I want to again emphasize a couple of things.
00:01:23.544 - 00:02:03.130, Speaker A: It's really like a generalization of linear programming to symmetric matrices. So instead of having linear inequalities in scalar variables, now what I'm going to have is inequalities in matrix variables, or positivity is going to mean that things have to be positive. Semi definitely. I like this geometric picture of semi definite trauma. I will not write things in coordinates as much as I can avoid them. And the idea is that what semi definite problem really is, is about we have the cone of positive semidefinite matrices we're intersecting with inserting affine subspace, let me call it l. So my feasible series intersection of this affine subspace and my cone, and I'm minimizing some linear function over this intersection.
00:02:03.130 - 00:02:39.050, Speaker A: So it's really a cone, a subspace, and I optimize over this intersection. And just as a comparison, if this cone was negative or something, this would be exactly linear programming. But in here we're changing the conduit, something which is not polyhedral and somehow many of the same features. I've seen will apply, although a few things are going to be slightly different if there's tons of applications. I mean, the last 20 years or something, there's really an explosion of applications of semi definite problem in many, many different areas. In particular in combinatorial optimization, control theory, and of course quantum information. And say a little bit about this and why this is nice.
00:02:39.050 - 00:03:12.204, Speaker A: Again, I'm going to abstract out how do we actually solve it. But the reason why this is nice and convenient is because it's a nice class of convex finite dimensional optimization problems, but a little bit more than that. I mean, it's not the kind of things, for instance, that we could just apply the ellipsoid algorithm, a general purpose complex optimization method. But there's a lot of structure in here. This is defined by determinants. We have nice computable borrower function. So there's a lot of things that we can do in here that parallel very much what we can do in the linear programming case.
00:03:12.204 - 00:03:45.162, Speaker A: For instance, there's a beautiful duality theory that pretty much has exactly the same features as in linear programming. And this is going to come in very useful. We're going to see, even in the kind of things I'm going to describe today, that somehow it's very, very convenient to not just look at your primal formulation of your optimization problem, but you also want to look at the same time what happens in the dual. And you can carry a lot of information from one place to the other. And these things are essentially solar important. So is the essentially caveat. Well, essentially caveat has to do depending exactly on your.
00:03:45.162 - 00:04:17.344, Speaker A: So in here, for instance, I suppose with linear programming. In linear programming, if data is rational, solutions are rational in here, that's not necessarily the case. So then, so what you're talking about in here really has to do with computing epsilon approximate solutions. Yeah, and. Yeah, and encoding issues. Solutions can be exponentially big, actually. So semi definitely problem in quantum information, as I said, linear kind of, it has, there's been a lot of applications in the last 1015 years.
00:04:17.344 - 00:05:13.752, Speaker A: These are some names, if your name is not there, and it should be like, let's rewind and write it. And most of the only things I'm going to focus on is a little bit on some work that we did like ten years ago on entanglement and separability. And somehow kind of the corresponding, I want to use this as a segue into perhaps one of the things that I think many of you are kind of interested, which is how do we characterize a set of quantum correlations of moments, and how that allows me to compute perhaps values of games or approximate values of games, I should say. So in the title, we have all these semi definite hierarchies. I describe specifically what I mean, but maybe, perhaps take a step back and try to discuss at a high level what these things are actually trying to do. And to me, the two key features that these things are trying to do. On the one hand, so of course, you're really trying to approximate some difficult object that you don't know how to handle.
00:05:13.752 - 00:06:13.844, Speaker A: And this could be a set that perhaps it's non convex, or even if it's convex, it could be like, it could be, it could be very computationally difficult to describe, and we're going to see some examples of it. And somehow, the way that these techniques work, or again, there's many, many ways of interpreting them. But one in particular, I think it's useful, is that essentially what they try to do is instead of looking at this object globally, what they try to do is they try to satisfy local consistency conditions. And I write local in here in quotes, because I don't really mean local, depending exactly on whether we're talking about LP's or we're talking about semi definite programming, then we're talking about some kind of reduced, if you want consistency conditions. And somehow, of course, this is all parable whatever, of the blind people trying to fill an elephant. And that's exactly what happens in here. I mean, each constraints that you impose will be a description, but there's nothing that will actually guarantee that all these things can actually fit together into a nice global object.
00:06:13.844 - 00:06:48.544, Speaker A: I mean, if you like topology, it's kind of homology issues. And again, we want to see exactly how these kind of things appear. This is not a formal definition, but somehow I really want to emphasize that there's kind of these two things. On the one hand, how do we deal with approximation of sets? And the way that we actually do this is by somehow imposing some kind of low dimensional consistency conditions of a certain type. So this is something that, I mean, at least in spirit, appears all over the place. In particular, in optimization. We've been doing this for Lenox 50 years or something, in different versions.
00:06:48.544 - 00:07:41.680, Speaker A: Of course, now we have a much more sophisticated way, hopefully, of thinking all the things. But for instance, when this idea of approximating convex sets using linear programming, linear programs of a particular type, I mean, people have used things called linearization, reformulation, roof duality, cherali atoms, relaxations all the time, and those produce somehow polyhedral approximations of complex sets, that they have nice properties. And there's a lot of philosophically related ideas in a number of areas. If you like probability, the idea of locally consistent margins, you have a joint probability distribution between many variables, and perhaps along every subset things are consistent. But there may not be a global object, right? And there's a number of other areas where the same kind of thing is true. So one thing at this level even, yeah, sorry, I should think of the hierarchy as going less and less local. Less and less local, exactly.
00:07:41.680 - 00:08:36.926, Speaker A: It will become clear later. Yeah. By essentially imposing stronger and stronger conditions that hopefully will bring you closer to the existence of a global object. So if you want, if you like LP, for instance, and you want to understand a little bit, what's the difference between the kind of things that we did before with LP and the kind of things that you can do with semi definite problem, think about LP as imposing, again, in particular situations, as imposing consistency conditions between subsets, right? So we may impose, if we have a joint probability distribution, for instance, that whenever I restrict this candidate joint probability distribution to all subsets, somehow what I see is actually consistent, semi definite forming will do something different. They will say that on every subspace, things will have to be consistent. And of course you could pick subsets that are subspaces that are aligned with coordinates, some subspace of the form, one, one, one and all zeros. But there's a lot of other subspaces.
00:08:36.926 - 00:09:09.282, Speaker A: And somehow semidetic brain is going to be very strong in that, or much stronger than LP in that sense, because it's going to impose stronger consistency conditions rather than just about subsets. Sorry, you mean linear subspaces? Yeah, linear subspaces. You couldn't formulate with linear, because you have infinitely many subspaces. When you look at substances of fixed cardinality, cardinality k, you'll have n choose k. If you look at all possible subspaces, you'll have an infinite number. Something like that doesn't fit into finite dimensional LP. It's really infinite number of constraints.
00:09:09.282 - 00:09:44.514, Speaker A: And that's exactly semi definite problem. So what's nice about this is that we have very compact ways of describing these things, as we're going to see somehow, in a sense at least. I mean, of course, that's in the eye of a beholder. I think the semi definite picture is much easier than the, somehow the healthy picture, because everything becomes linear algebra instead of combinatorics. So somehow, of course, depending on your taste, if you like combinatorics, then you love these kind of things. But somehow, linear algebra, at least, it's a lot easier for at least certain things. You have a lot more symmetries, actually, that makes things very, very nice.
00:09:44.514 - 00:10:29.224, Speaker A: Okay, so let me start perhaps with. I mean, there's no technical content in here, but anyway, if there's any question or. Okay, so let me start with something perhaps more concrete, and let me try to address a very old question, or very natural question in applied mathematics, which is, let's imagine I have a polynomial, a multivariate polynomial, and I want to try to understand whether this polynomial is non negative. So I have a polynomial in n variables in here of degree v, whatever d will have to be even. And I'm trying to understand whether this polynomial is non negative whenever I plug in all possible variables. Right? And in here, I'm working over the reals. Everything that I'm going to say will extend to other domains.
00:10:29.224 - 00:10:53.624, Speaker A: But just to make things simpler and not have to make things difficult, I'll just deal with global non negativity of the problem. So, of course, if p is quadratic, that's easy. I have a quadratic form. I check eigenvalues, or actually, I just check if a matrix is positive, semidefinite, I'm done. But if the degree is greater or equal than four, then that's already a big. I give you a quartic polynomial. We cannot efficiently decide whether this polynomial is non negative or not.
00:10:53.624 - 00:11:36.534, Speaker A: It's a function of. There's an obvious, I guess cubic is trivial, because if you have a cubic term, odd degree. So if you have any odd degree polynomial, as you go to plus or minus infinity in some direction, there's an obvious sufficient condition. If I have my polynomial, I can write it as a sum of squares of some other polynomials, q, sub I. Then obviously this implies this, right? And what I like about, I mean, what many people like about this, of course, is that you don't have to trust me that I got this, that I'm telling you that my polynomial is non negative. I actually have a certificate. These Q survives in there.
00:11:36.534 - 00:12:22.330, Speaker A: If I just verify that these two things are exactly the same, that's actually certifying that my polynomial is non negative. Not as if I were to run, let's say, a quantified elimination algorithm task is Heidenberg or something like this, that will run in your computer for three weeks, and it will not end up telling me yes or no. And we don't know whether my computer, my program was fine or had a bug, or anything like this will actually happen. Many of the quantifier limitation software is very buggy. So what's nice about this, again, somehow they're easily verifiable certificates from negativity. And much more interesting for us is that this is a semi definite problem. The existence, deciding the existence of such a decomposition, whether I give you a polynomial, and checking whether my polynomial is a samos course or not, is a semidevic program.
00:12:22.330 - 00:12:45.262, Speaker A: I'll tell you very briefly how it works. It's not crucial for what we're going to do. I mean, it's crucial that it's a semi definite problem. The details are not crucial. And this is really the building block. I mean, everything that we do afterwards, they're just kind of either dualizing or putting slight variations on this. So if there's one thing I want you to remember, is that whenever you're faced with non negativity, that's a horrible problem.
00:12:45.262 - 00:13:07.254, Speaker A: You're faced with some absorbed condition. That's great. And we're going to see why. For instance, if you care about entanglement, you actually care about these polynomials, even though you may think that you don't. And this sufficient condition, is it like in typical cases, always true? We'll talk later. How about for now, the only thing I'm saying is that this is sufficient condition that implies this. Let me perhaps make a couple of comments.
00:13:07.254 - 00:13:41.608, Speaker A: Two cases where you do know that these things are equivalent. One is, of course, if p is a quadratic form. Right? If p is a quadratic form, it's quadratic. Then the eigenvalue, the composition, the factorization, choleski decomposition, whatever you want, will tell you that this is true, because if you have a quadratic form, you factor, it's a Samos quads. The other case, it's a little bit less trivial, but equally trivial is a unified case. If I have a polynomial in one variable, you write the roots, you play around with the, you factor the polynomial, you play around with the roots, and you'll be able to easily write a decomposition of this type. And there's a few other cases.
00:13:41.608 - 00:14:11.548, Speaker A: Hilbert actually exactly characterizes the cases where these things are. Yeah. Following up on Scott's question, in the very case, we also understand the odd. You can talk about odd degree if you just look. Yeah, same thing. I mean, there's versions of this. If I want negativity, not on rn, but I want, let's say, over a compact region or a cone or something like this.
00:14:11.548 - 00:14:43.610, Speaker A: All these machinery applies slightly different versions, but essentially the same thing. Is it also true for matrix valued polynomials. Yes, it's essentially true for matrix value polynomials. Actually, the univalid case, it's essentially a result by choice. Actually, many people report that. But in particular here you're saying that semidefinite programs are useful to test non negativity because they can test for some. I don't directly check this.
00:14:43.610 - 00:15:25.702, Speaker A: I check this. Is there a sense in which this problem is universal? Well, it's reducible. I mean, you can go either way, right? So any SAP is checking for it. Essentially, you're really checking. I mean, when you have an SAP, you really have an affine family of polynomials and you're trying quadratic polynomials and you're trying to find one on that family that is Esamos quartz. Perhaps you cannot reduce it to the case of is this single polynomial, is hemoglobus or not? But what you do is you form an pencil or an affine family of polynomials, and you ask, is there one which is the sum of squares in this family? And that's actually going to be important somehow. We're going to move in just a second from the fixed polynomial case.
00:15:25.702 - 00:16:00.958, Speaker A: I give you p and you tell me yes or no to, I give you an affine family of polynomials and you'll tell me if there's one that's a sum. Of course. Let me just describe in one slide. I don't think the details are terribly important, but at least I want to give you a plausibility argument. Why it's actually a proof, but why this checking this condition is a semi definite problem. So the idea is very easy. Right? So the idea is that if I have a polynomial of even degree b, it's not very difficult to see that my polynomial is going to be a sum of squares.
00:16:00.958 - 00:16:56.410, Speaker A: If and only if I can find a positive semi definite matrix q in such a way that I can express my polynomial as a quadratic form in a vector z, where z is essentially a basis for the space of polynomials of degree up to d over. And in particular, I can just pick this polynomial basis, these monomial basis. And why somehow this an if and no leave? Because if I have my polynomial and it's a sum of scores, I can just write my sum of scores as linear functions on this vector of monomials. Then when I put those two things together, I get a matrix q. And conversely, if I found a matrix q that has its property, I factor it and I will get a decomposition. Now, the only thing that we need to show is that this condition, there are essentially linear conditions in q, right? And that's easy to see. Or it's a semi definite problem.
00:16:56.410 - 00:17:19.604, Speaker A: What do we have in here? Q has to be positive, semi definite. Q lives in my cone. What is this really? Imagine expanding this expression. What I have in here is a polynomial. What I have in here is another polynomial. If I want these two things to be the same, I will just match coefficients. What are the coefficients on this side? They're just linear expressions of the coefficients of q.
00:17:19.604 - 00:17:47.953, Speaker A: So somehow the identity between these two things, they're just linear equations between the qs yjs in here and the coefficients of p. That defines a subspace. If I want to find whether this is true or not, I have to find a positive semidefinite matrix on that subspace. So this is exactly a semidefinite problem. If you're concerned about the size of the semidefinite problem, how big is Q? Let me use a board. So that's essentially of size. I have all the monomials of size up to over two.
00:17:47.953 - 00:18:17.146, Speaker A: So that's n plus d. Choose two plus 1d, choose two, remember D seven. And this is actually, let's say for fixed d. This is polynomial. Polynomial in n, sorry, actually polynomial in both. Whenever you fix any, it's polynomial in. So it's clear again, the main point, if you hand me a polynomial, and you ask me C decimal scores or not, I'll write this semi definitely won't write it.
00:18:17.146 - 00:18:53.796, Speaker A: I'll tell my computer to write this in my definite program to solve it. It will tell me yes or no. And if it is, it will give me a certificate or a matrix q that I can verify that it's not. If it's not true, it will give me a certificate that this polynomial is not sms one and we'll talk later on. But it's kind of questions. Okay, so why do we care about this? Why do we care in optimization, all these kind of things? Because it's a one step to go from a fixed problem, from a fixed polynomial to optimize it. The only thing I need to do is to really think about, ok, let's say that I'm trying to minimize p.
00:18:53.796 - 00:19:32.904, Speaker A: I have a polynomial p, like this one algorithm, convex, whatever, and I'm trying to minimize it. Now let me write in a different way, instead of trying to go down in my polynomial, let me try to go up from the other side and try to find out whether I can find a lower bound gamma on my polynomial. And finding the minimum is exactly finding the largest gamma in such a way that the polynomial p of x minus gamma is non negative. So I have a polynomial. I'm trying to find a minimum. I can just take constant, push it up as much as I can. And as long as this polynomial is non negative for all x, that means that p of x is greater or equal than gamma for all x.
00:19:32.904 - 00:20:21.704, Speaker A: And of course now we can go from this non negativity condition, so I told you they're ugly and difficult, to a condition of this type where I try to find the largest gamma that satisfies that p of x minus gamma is the sum of squares. And this we can do just with a semidefinite problem exactly the same way that I was doing it before for xp. Why? Because I play the same game writing. The condition that this has, this quadratic representation and matching coefficients is going to give me linear equations between the entries of q and the coefficients of this polynomial. The coefficients of this polynomial are linear functions of gamma. So I have on the one hand linear functions of q, on the other side linear functions of gamma. These are linear equations and that's fine, I can just maximize gamma subject to those equations.
00:20:21.704 - 00:20:47.546, Speaker A: So this is again really important that just in the same way that we do, we deal with a single polynomial p. I can just deal with affine families. I can think about, I have enough affine subspace of polynomials. I'm trying to find one in that family, which is samoscox. Yep. Can you somehow incorporate the gamma into the variable? Yes. Gamma is a decision variable.
00:20:47.546 - 00:21:41.326, Speaker A: Yeah, exactly. So now the decision variables of my semi definite problem, they're simultaneously going to be gamma and the q sub I's and I solve for both at the same time. Or let me perhaps take abstracted away if before I told you that semidefinite programming was this idea of I have a cone of matrix and a subspace. Now let me just abstract all this stuff out and just say that now the problems that I can solve there I have the cons of some squares and I have subspaces of polynomials and then I don't care anymore about semidefinite problem. Semi definite problem is under the hood. But the only thing that I care about is as long as I have, I find families of polynomials and I have a cone of sum of squares, I can find points in these intersections and underneath I will write them as semi definite problems. I don't need to ever write a semi definite problem if I want to deal with this.
00:21:41.326 - 00:22:00.462, Speaker A: Is this a special case of SDP or. It is, I mean, because in the particular case where you take polynomials with quadratic. These are quadratic forms. These are made so it's not known how to reduce a general SDP to this form. Well, if I allow more parameters, it is. Ah, okay, okay. It is exactly good.
00:22:00.462 - 00:22:25.016, Speaker A: So this is on the polynomial side. Let me just mention very briefly what happens on the dual. I told you that every time that you have a complex problem, there's a corresponding dual. And I just want to mention it briefly. There's a nice interpretation somehow in terms of moment consistency. So somehow the question that you want to add again, forget all I said before for a second. So the question that I want to answer, for instance in here, is, imagine that I actually give you a candidate set of moments.
00:22:25.016 - 00:23:09.648, Speaker A: And just to make it easier in these examples, let's imagine that I'm doing in the univariate case, and I'm interested in the measure, for instance, in the existence of a measure or a probability distribution which is consistent with a given set of moments. So, for instance, in this case, I may be interested in knowing if there's a probability distribution that has zero moment, one, first moment equal to zero, mean equal to zero, second moment equal to two. Of course, the answer in that case is of course, this one. Right? What's an example? So this is just an organization condition. This is serum in. And I want variance equal to two. So just pick, you know, Gaussians, whatever, you pick the Gaussian of two times the standard square root of two times the standard Gaussian.
00:23:09.648 - 00:23:48.746, Speaker A: And I know that will tell you that there's a density which is consistent with Gaussians. But what happens in this example, right? I mean, can you find the priority distribution whose first moment is one that just normalization. I want the mean equal to two, and I want the second moment, the expected of x squared equal to three. Sorry, violates other, essentially, Cauchy Schwarz. Yeah, but, yeah, yeah, sure. I would cite my most general inequality also. Yeah, so this would, I mean, it's easy way of saying this.
00:23:48.746 - 00:24:20.542, Speaker A: For instance, if you remember, the variance is always non negative. The variance is the expected value of x squared minus the expected value of x squared. If you computing here, you'll have that the variance of this random variable, if it exists, it should be equal to minus one. So clearly, these numbers are not consistent with an underlying probability distribution. So let me try to. And of course, you're seeing where I'm heading, right? I mean, you're interested, many of you are interested not in probability distributions, or you should be, I guess, but. Or you are, but you're particularly just in the quantum case.
00:24:20.542 - 00:25:26.644, Speaker A: And somehow if we understand this case, then essentially the same techniques are going to generalize. Right? So why this polynomial stuff that I was telling you before gives an answer to this? Because of course there's an equivalent characterization, I'll tell you what that means in a second, of when the set of moments is actually, they're actually consistent with underlying priority distribution. Essentially the only thing that you need is at the expected value of all functions, they have to be non negative or non negative functions, they have to be non negative. And I'm working here on a compact set. So that's why, because polynomials are dense, I can just look at non negative polynomials. So the point is, and let me perhaps make this a little more explicit, instead of my software, that hop argument about variance, let me try to explain that argument in a different way. Let me say, for instance, I am smart and I'm trying to compute the expected value under this, whatever probability is, if it exists, of something like x minus two squared.
00:25:26.644 - 00:26:10.264, Speaker A: Right? And let's compute what this is going to be by linear, and by expanding this, I don't even know how to expand the square. So this is minus four, x plus four. Then I can use linearity of expectations in here, right? Because I know that the expected value of x squared is going to be equal to three. The expected value of x is going to be equal to two. So this is minus eight plus four and this is minus one. So what happens in here? I'm computing the value of some polynomial that should be non negative. And if there's truly a distribution, this should give me a positive number.
00:26:10.264 - 00:26:42.644, Speaker A: But in this case I get something negative. So the point is, if I were to test in here against all possible non negative polynomials and I passed, this would be great. I know that at least in compact sets, there has to be a corresponding probability distribution. This is what's called the truncated model. Sorry. So I would in here put all polynomials of this form, and I would require that this is positive for all polynomials. And in here I'm failing.
00:26:42.644 - 00:27:46.644, Speaker A: So somehow, what happens in here? What's the way that we're producing semi definite approximations of this, that instead of checking against all polynomials of all degrees, we're just going to check on all non negative polynomials? We're somehow only going to impose the consistency conditions that I have to produce non negative values whenever I compute expectations with respect to polynomials which are somehow, this will give me some kind of outer approximation of this subdivision. Think of essence, let's get to quantum. Finally, so why should you care about polynomials if all this stuff was a little weird? Let me tell you that this directly, I mean, just by essentially turning the crank, this immediately tells us a very nice way of approximating the set of separable sets. Right? So I think here I'm looking at bipartite quantum states. We know, you know better than me. I'm looking at essentially convex combinations of product states. Yeah, I'm looking here at the density matrix.
00:27:46.644 - 00:28:54.526, Speaker A: And of course, a question which is pretty hard is, if I give you state in here and give you a row, I want to try to understand whether it belongs to this convex set or not. And notice that convexity doesn't actually help me in here. Remember what I said at the beginning, that even if I start with a convex set, if my convex set is difficult, then that by itself doesn't really help me a lot. And of course, 1 may be interested not just in membership to this set, but if you're interested in solving some other problems, you may want to optimize over this set and understand, for instance, what happens when you optimize over this. So what I want to see is that it's fairly natural or essentially translating what I said before to this domain, we can easily produce nice approximations for this set. And this is all work that we did like ten years ago or something with Andrew and Felico. Okay, so what's the idea? Somehow, what is a key is that if I have a convex set and I have a point which is not on the set, then the obvious way of certifying that that point is not Kanbana separating hyperplane will tell you.
00:28:54.526 - 00:29:16.244, Speaker A: Find a separating hyperplane that puts your state on one side and puts your. You have several states, you have set on the other side. And this picture is nice and wonderful, right? I mean, it's very easy. I have a convex set. I have a point. I find it. What's the difficult part in here? The difficult part is given this subspace, to actually check that this set is on that side.
00:29:16.244 - 00:30:11.394, Speaker A: This part is easy, right? I mean, if I have a subspace verifying the difficult part in here is to verify that this set is completely contained in the half space defined by this. And it's kind of well known. Let me, I mean, again, many of you know a lot better than me that there's a perfect identification between these kind of separating hyperplanes that leave the separable states on one side with something called positive maps, essentially maps that will map positive semidefic matrices into positive semidefic matrices. And I'll tell you why. That's essentially equivalent to looking at non negative polynomials of a certain type, what's called biharmitian polynomials. These are going to be polynomials in two sets of variables that are quadratic in one set and quadratic in the other. And again, this has a number of names in the quantum information literature, but this is a well understood or operator theory, and this is kind of pretty well understood.
00:30:11.394 - 00:31:07.808, Speaker A: So why somehow were polynomials appearing here? If you're, no, this is, if you're not exposed to this viewpoint line algorithm, it may not be obvious in here that you have somehow some nice underlying polynomial here. It's actually pretty easy. The idea is if you want to check whether a hyperplane is non negative on a convex set, it's of course enough to just check on the extreme points of your convex set. And I can easily parameterize the extreme points in here, because the extreme points in here are exactly tensile products of rank one matrices. And if I write it down, if I write, let me just write it this way. If I write the trace, this linear function applied to a separable state, what I get when I expand is exactly a polynomial that looks like this, which is bi quadratic. When I fix the x's is quadratic in y.
00:31:07.808 - 00:31:54.742, Speaker A: When I fix the y's, it's quadratic in x. And when I mean quadratic, I mean hermitian in here we're doing it complex version, right? So it really looks like this. For, if you give me w, let's say w is your favorite witness in here, I mean, just a whole bunch of numbers, then the condition that this actually is on this side, that the set is on this side of my hyperplane, really boils down to the question of whether this polynomial is non negative or not for all values of x and y. So I'm doing, I'm sweeping overall possible extreme points of this, and I have to get a non negative number. Of course, at this point, I'm setting you up. We cannot do this. This is non negativity what we're going to do.
00:31:54.742 - 00:32:32.570, Speaker A: We just want to write the sum of squares condition on this. And again, don't worry about complex versus real. I mean, we have sum of hermitian squares, same stuff, right? So what happens if we do that? This is clear questions. So what happens to this? This is great. It does give you an approximation of the set. And of course it is something that people did know about in somewhat different form, like in those terms, this class of positive, remember I told you there's an isomorphism between these different things. So somehow the polynomials that have some of scores weakness, they correspond to something called the composable maps.
00:32:32.570 - 00:33:07.854, Speaker A: In operator theory, they correspond, or the sets that those things can detect is something called PPT state, possible partial trace states in quantum information. So it's great. I mean, we do get an approximation of this set, but we cannot refine this. Or yet, right, the way I present it, we just get another approximation that may be bad or maybe good, we don't know. But now it's exactly where the hierarchy parts enter. We're not only, I'm going to be able to produce this, but I'm going to be able to produce more and more and more approximations in here. And I'll tell you in a second how it works.
00:33:07.854 - 00:33:50.712, Speaker A: Is this clear why we get, remember, somehow, instead of identifying all the possible non negative polynomials, I'm just identifying the smaller subset, which are the ones that are Samsung squares. So in this picture, that gives me a coarser approximation of my set. So the approximation is going to be bigger. It's just one. I make things smaller on one side and get bigger on the other. How do we make this? Dorit, you have a question? Yeah, I'm not sure. What is the PPT states? PPT are the ones that, when you apply a partial transpose, you get a positive semidefic matrix under states.
00:33:50.712 - 00:34:14.330, Speaker A: Like the ones in here. Yeah, like the ones in here that will have a. Yeah, exactly. Yeah. They got seen entangled. But, okay, so what happens in the, how do we make this stronger? So let me just explain it first, because it's easier in the polynomial case. I mean, let me ignore the quantum stuff for a second.
00:34:14.330 - 00:34:38.928, Speaker A: But the idea, again, it's a one slide, actually one equation, which is this. But if I want to put conditions that are somewhere in the middle between p being sum of squares and p being non negative. And again, for simplicity, let me just work on the sphere. Let me make, sorry, p or homogeneous. Let me deal with a homogeneous polynomial. Then something that I can do is just do something like this. I can take my polynomial and I can multiply it by something positive.
00:34:38.928 - 00:35:14.174, Speaker A: Let's say in particular, the sum of the squares of my variables raised to some power, k. It doesn't matter. K will matter. So it's, again, immediate that this condition is weaker than this. If piece is sum of squares, when I multiply it by another sum of squares, the product is also going to be sum of squares. But this still implies non negativity. So as I increase in there, the multiplier the power k that I'm using, I'm getting better and better conditions that approach this non negative.
00:35:14.174 - 00:36:00.726, Speaker A: And it's not very difficult to show in full generality, people have done this extremely complicated settings, but let's say on the sphere or the homogeneous case, elementary arguments will tell you that as k goes to infinity, then essentially what approach in this comp. So if you want, in this picture I have before, if you have one set and another set, you're getting layers and layers between one and the other. And essentially, as k goes to infinity, this filter. So are you saying that any non negative polynomial can be approximated in the limit by a product of sum of squares, polynomial ratio of sum of squares. A ratio of sum of squares, right, well, I have that. This is sum of squares. So that means that p is a sum of squares divided by this.
00:36:00.726 - 00:36:44.408, Speaker A: And that's essentially what Artin proved right now when he solved Hilbert 17th four. Okay, so we take that, we translate it in here. If before we just looked at weaknesses for which this expression was a sum of squares, what am I going to do? I'm going to take this, I'm going to multiply it by something, and I'm just going to check whether this is a sum of squares or not. I'm going to be looking for weaknesses of this time. If you do that, you'll get nicer classes in here that get closer and closer to the set of separable states. And again, as k goes to infinity, these things converge. So here's an example, if you want.
00:36:44.408 - 00:37:08.198, Speaker A: This is from the paper, I don't remember which state you're looking at one of these difficult states. So you look at a particular witness, you multiply it by this x transpose. And what you'll see is that this actually has a nice decomposition, even with integers of. This is just an example. Yeah. So here you made the choice of multiplying by this, this. Yeah.
00:37:08.198 - 00:37:41.174, Speaker A: So are there cases where there would be a better choice or a different. Yeah, and certainly, for instance, if this is fixed, I can search for a multiplier that's also a semi definite problem. If I fix this, if I fix my witness, then I can actually search for the best multiplier in here. Yeah. So in general, what you do is you just use this. I mean, certainly the things that we defined, we just fixed kind of that multiplied to that. Yeah.
00:37:41.174 - 00:38:28.710, Speaker A: I mean, in the stuff that we did, we just fix it to this. Okay, so let me mention very briefly the dual an interpretation that also, I think many of you are familiar with. Which is somehow like before I told you, there's a dual story. There's also a dual story in here. If you don't like entanglement witnesses and you'd rather think just directly in terms of the state, is the particular choice of textiles square to the case, you know, you could optimize it, but can it vary by a lot in terms of the degree? Or is there. That's something that we don't. Even in the general case, we don't understand very well.
00:38:28.710 - 00:38:53.290, Speaker A: Even in the polynomial case, there are certainly cases where, if you pick that very, very carefully, you can get away with certificates of law degree. If you use something universal like this, then a few may have to somehow make things longer. I mean, higher levels. And that's a huge difference. We don't know examples or. There's a huge difference. Well, look, I mean, the fact that deciding separability is NP hard.
00:38:53.290 - 00:39:07.514, Speaker A: Right. Or, you know, optimizing over separable states is NP hard, sort of tells us that there must be some cases where K has to increase. Of course. Yeah, yeah, yeah, yeah, yeah. Right. Yeah. But that's a different question.
00:39:07.514 - 00:39:23.468, Speaker A: Right. Okay. All right, fine. In here. Oh, an arbitrarily positive polynomial, if you know any positive thing to put in there. But then you have to have a certificate that's actually positive if you actually want to prove something. Yeah.
00:39:23.468 - 00:39:58.230, Speaker A: Does this have anything to do with the hierarchy of K extendable states? Of course, yeah. That is. That is that hierarchy. I mean, that's what we define that paper. That's what this slide is. Can you dualize the duals? Can you identify the dual differences, like, for, you know, for PPP? Well, you get the class of. I mean, it's hard to characterize it other than in terms of this, but they're essentially the things that have certificates of this type that you only need to multiply once by something until they become sums of permission squares.
00:39:58.230 - 00:40:28.820, Speaker A: It's somehow tautological. We don't have an intrinsic definition of. What do they mean? Before, it was also just decomposable. Right. Wait, Pablo, is this an accurate characterization you want to witness? There's only, like, a measurement on systems, Ab, but it should be one that if you write m on AB plus M on AB prime, then that resulting operator should be positive, semi definite. Yeah. So that's kind of like, essentially what will happen here.
00:40:28.820 - 00:40:48.504, Speaker A: But, I mean, when you do the extension. But if you say on the first level, what are the witnesses? Well, those are the decomposable witnesses. But then if you say, what are the witnesses that correspond to a higher level of the hierarchy, would they be the ones that, you know, satisfy that property, as I said, or. No, I'm not sure. Okay. I think. Yeah.
00:40:48.504 - 00:41:16.264, Speaker A: Are there entangled states that you reach for Noke or is it. So what, really? No, so we don't get, you don't get, in general, you get that the closure of this will give you the whole. Sorry, you get that the closure of this, or you take the union of these for our case, and you take the closure of this, and that will give you the set. Sorry, the intersection. Right. You get the set of several sets. So essentially, if I would run my algorithm forever with increasing case, I would at some point.
00:41:16.264 - 00:41:22.784, Speaker A: Yeah. It will always succeed. Yeah. For anything. Yeah. It's a complete hierarchy. I mean, that's exactly.
00:41:22.784 - 00:42:05.094, Speaker A: So what's a dual viewpoint again? Like I told you before, you have consistency of marginals in the, in the classical case, what happens in the quantum case is history about symmetric state extensions. Right. So instead of thinking of my original bipartite state, I may think, okay, if my state is actually separable, then I can actually write down explicitly some kind of extension of my state to more parties. Right. Or somehow the property that this state will have is that it's going to be symmetric between these two parties, let's say two in here, or k in general. And also, when I do a partial trace over all these other components, I get back the original state. And it turns out that you can decide that separable states will always have these extensions.
00:42:05.094 - 00:42:34.734, Speaker A: Entangled states cannot have these extensions to an infinite number of parties, or, sorry, to a sufficiently large number of parties. And this is under, if you check the existence of the conditions, sometimes the only thing that it's involved in there are either linear equations or semi definite conditions. So this is a semi definite problem. In fact, it's exactly the dual of the stuff that I described in the previous one. So somehow this makes very clear the picture I was telling before, that it's really about consistency of a certain type. Writing here. It's consistency.
00:42:34.734 - 00:42:55.996, Speaker A: Sorry. Yeah, it's exactly the same case. Yeah. So if I want in here, extensions of k parties, then that gives. Yeah, exactly. This is what you see the sum of the x y squared, essentially, because I exactly copied this in the other components. Yeah.
00:42:55.996 - 00:43:25.424, Speaker A: If I were to pick something different in there, then I'm doing something different in here. And I cannot quite understand. I mean, I do know what it is in the probabilistic case. I don't quite know exactly what happen here. Essentially, will correspond to normalizing the cone in a different way. I mean, what you're really doing is you're making this compact by taking a section, the sum of the xy is equal to one, and then you're taking a different compact section of the same cone, which I think will be like a different normalization condition. But again, I don't know the exact answer.
00:43:25.424 - 00:44:28.190, Speaker A: Very quickly, the three slides that I have, two minutes, I have. So one is, of course, this should ring a bell. And it's exactly, somehow the equivalent of the classical definitive theorems in probability. Definitive theorems in probability will tell you that if you have a sequence of random variables that's infinitely exchangeable, then that's equivalent to being a mixture of iabs. And somehow it's underlying early stories, some finite versions of that, that were worked out by diacones and Freeman, and somehow the quantum versions of these, both infinite and infinite, that were worked out by the number of people. And in particular, something very nice that comes out of all this work is that you can actually quote something that we certainly didn't do, which is it gives you a nice way of quantifying how well these things do as a function of k. So if you want to know how well you're approximating the sets, and you're giving a certain error, and you want to understand what k you need, and think of k again, either as a degree of this exponent, of this multiplier, or the number of copies, then these kind of theorems will actually tell you how you should pick k.
00:44:28.190 - 00:45:06.682, Speaker A: If you want to guarantee that this will happen. And there's a number of different results, I decided not to put any, because they really vary in terms of how you pick norms. If you're measuring things in one norm to norm velocity norm, and of course, Fernando and I guess a number of other people are kind of who you should ask for these kind of things. So, in the 1 minute that I have, let me just tell you how this generalizes, for instance, for the non committed. And the idea is, it's exactly the same. You don't need to even think about this, at least if you want to write the corresponding conditions, if you want to analyze them, of course you have the work. But if you just want to understand what happens, it's exactly the same thing.
00:45:06.682 - 00:45:29.774, Speaker A: What do you have now, if you have a non committed polynomial, you can have a non commutative polynomial, polynomial in operators or matrices, if you want. There's a natural notion of positivity. I want this to be positive. There's a notion of sum of squirts. I want to have a quadratic representation of a certain type. The semi definitely problem that you write is exactly the same. You write this, you write monomials, you expand, you match coefficients.
00:45:29.774 - 00:45:53.654, Speaker A: And essentially what you want to understand is how to approximate this kind of nested convex bodies that we saw last time. Right? I mean, there's a polytope of classical correlations. There's a quantum convex set, and there's a non signaling polytope. This is easy. This is combinatorial hard. This is a essentially cat polytunnel. And this is a thing that we don't understand very well, which is exactly the kind of things that you want to optimize over when you want to compute values of games.
00:45:53.654 - 00:46:45.752, Speaker A: And what you would do in this case, for instance, if you want to compute the value of, let's say, some game of this type, and this is in the paper by Andrew Young Bentoner and Stephanie Werner. So what you're really trying to do is to maximize a function that looks like this over a's and b's, a and b's. And there are the povms of the two players. So, instead of requiring that this polynomial be non negative, I mean, the spectral value, an eigenvalue condition on this is essentially a polynomial being bounded below by gamma. You just try to find a certificate that that thing is sum of squares. And if you ask, increase the degree of the multipliers, you get exactly the kind of semi definite hierarchies that they obtained, or dually, the ones that now as quesad, asin and pyronia, they obtain in this case. So it's exactly the same picture.
00:46:45.752 - 00:47:21.864, Speaker A: What's the tricky part? These arguments, which I have to say I don't really understand very well on the question of finite dimensionality, whether you can achieve these things in the optimal values in finite dimensions. And I don't think these things are yet tell us anything about that, perhaps as a way of extracting something from primary dual solutions. But I really should wrap up. Shameless, sad. We have this nice book with a number of people on the nail with all this stuff at the beginning. You don't have to buy it. You can search it on the web and you'll find it.
00:47:21.864 - 00:47:47.396, Speaker A: But the idea is that, again, somehow there's a very nice way of just approaching this. Say, I am will not be happy if I say this. That's fine. Actually, no, they allowed us to do, do this. So I'm not doing anything wrong either. More than your legally. And the idea is that, again, somehow, if we just focus on these sum of scores, type conditions, I mean, many of these things, they really come very naturally.
00:47:47.396 - 00:48:19.944, Speaker A: I mean, it's either positivity conditions, uncommutative variables, or non commutative variables. And the techniques are exactly the same. Nothing really difference. Sure. Thank you. Any questions for popping? Yes, it does. So there's a very nice result by plenty of.
00:48:19.944 - 00:49:16.128, Speaker A: Oh, I didn't write it. Was it plain? Yeah. Plenty of squares of war that they show that the convergence rate that you get is actually better when you include the PPT conditions than if you just look at the symmetric extension condition. Essentially it goes from something like one over l to one over l squared. Nava, square, Plinio and Owari. Excuse me, what's l? L? They go from one over k, if you don't impose a positive condition, to one over k squared, the equivalent. Sorry, say that again.
00:49:16.128 - 00:49:50.736, Speaker A: Between symmetry and exchangeability, I mean, changeability and substituting is false. So this theorem fails. It's true in the combination setting, but fails in the long term. So there are variable variables which are, which are sort of getting more and more independent over their tail, but they're not exchangeable anymore. Yeah, yeah, yeah. In here, essentially, what you use, or what provides completeness, are these results by Helton and Maculum. So they prove the non commutative version of the positive shell and cells.
00:49:50.736 - 00:50:32.154, Speaker A: That kind of applies in this sense. Yeah, but that's. Yeah. Okay, just to make sure I understood the last part of what you said, for the field theoretic value of a non local game, you can approximate it, sort of from above, like converge on it, but you can't just approximate it because you can't get lower bounds. Is that correct? The question what's not clear in here is. So the way that this formulation works is working from below. And what it's not clear is whether you're actually converging to the true value, whether there's a finite dimensional strategy that will produce that value.
00:50:32.154 - 00:51:06.880, Speaker A: You're producing lower bounds. But it's, I mean, we don't know, apparently, again, I'm not an expert on this, that we can achieve these ones in finite English. Right, exactly. Yeah. Way back in your first slide or something, you made a comment that I see how seriously, you know, that you were making an analogy, I think, to the relaxation, to use the crypto weekly broker basis relaxation, yes. And some finite fields and everything. So essentially what you do, I mean, it's very similar to what you're doing here.
00:51:06.880 - 00:51:30.004, Speaker A: Instead of looking at quadratic equations, you linearize. Sorry? At polynomial equations, you linearize. You introduce new variables for finite products, and then you just treat them as linear equations in this. Now, you're doing exactly the same thing. The difference that you also impose, the positivity conditions are coming from this. And the problem is, of course, we cannot do this over finite fields. At least, I don't know.
00:51:30.004 - 00:51:54.608, Speaker A: But I mean, conceptually, it's exactly the same kind of. You just linearize. The point is in here, over reals. We know more valid constraints that are true, and those are coming from that. Maybe if I could steal a second question. So you, in fact refer to the argument. So, does anything here work? So, if I start with a rational function that I want to show, so does any of this framework.
00:51:54.608 - 00:52:11.634, Speaker A: So, this is much more general than what I described. Essentially an arbitrary semi algebraic problem. You can write this kind of relaxations. As long as it's semi algebraic, you can do these kind of things. Okay. There are no further questions. Let's thank Pablo again.
