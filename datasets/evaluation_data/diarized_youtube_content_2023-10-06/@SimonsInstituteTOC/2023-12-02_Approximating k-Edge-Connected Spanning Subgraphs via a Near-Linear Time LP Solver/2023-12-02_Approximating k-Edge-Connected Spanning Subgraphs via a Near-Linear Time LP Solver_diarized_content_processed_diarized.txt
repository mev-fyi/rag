00:00:00.160 - 00:00:26.552, Speaker A: I think we should just get started. Let's bid welcome to our final speaker, Sora chai. Thank you very much for staying around until the very last talk. I'm Sora chai. Today I will tell you more about the problem called KH connected spanning subgraph and how to approximate it using fast lp solver. This is joy. Work with Parinya.
00:00:26.552 - 00:01:13.120, Speaker A: Tell them. So let's jump right in. So let me first define the problems kh connected spanning subgraph. First, a graph is k is connected if it remains connected after removing k minus one edges, or like the min cut is at least k. So that's what I mean by k is connected. And the problem definition is the following. Give you a undirected graph and a cost function on each edge and the output we need to compute the mean cost kh connected spanning subgraph, meaning that computer subgraph that is spanning that contain all the nodes and also computer has to be kh connected.
00:01:13.120 - 00:02:17.518, Speaker A: And I want to minimize the cost for this problem. When k is equal to one, it is kind of easy to solve, right? Because what is connected is a tree. So this is minimum spanning tree problem. But the problem becomes Apxrt as soon as k is at least two, even on the bounded graph, about a degree graph, and benignly cost function. And the best known approximation for this problem is two, which is known very long ago. And the Kh 22 spanning subgraph is a special case of the survivable network design problem, which is a much broader class. So in terms of the complexity of this problem, for the case of various small k, let's say for two ecss, you can do three approximation in the quadratic time, and for general k you can do also two approximation in MNK, or you can do something like two k minus one approximation in quadratic time and so on.
00:02:17.518 - 00:03:10.560, Speaker A: But also there's a algorithm by Jen that gives like two approximations that solve much broader class, not just k css. And today I will tell you more about the new result in this area. Meaning that we can show that you can give two epsilon approximation if you in nearly linear time, if you can output only the estimate, output the value. But if you want to output the solution, we need some more time addition like n to the 1.5 times k squared over s one squared to get the integral solution. And the key main technique is that we just solve the Lp, solve the lp in nearly linear time, and then we do the rounding. So let me talk about how to solve lq for this problem.
00:03:10.560 - 00:04:14.084, Speaker A: And okay, so here is the Lp formulation for k css problem, I want to minimize the sum of the costs on the variable xe, and then we want to select each edge such that the for each cut it cover at least k amount for per cut. And then there's a restriction that you cannot pick more than one edge. So there's an upper limit on the box constraint here. And for the purpose of this talk, when I say cut here, I mean the set of edges instead of set up nodes. So for example, when I say cut ab, it means literally just two edges there. And how fast can we solve this k css Lp? You can solve it in mn time over epsilon squared by Fleischer really long ago. And it turns out that if you want to solve, if you want, there's a slightly easier problem which is called KECSM.
00:04:14.084 - 00:05:20.278, Speaker A: If you can remove the buck constraint, meaning that if you can select multiple edges, then this problem is called KECsM ke connected, spanning multi subgraph and similar LP can be solved efficiently. And the LP is exactly the same, except that you don't have bulk constraint. And the key recent work by Sekuli and Kondrat give this, say that this particular LP can be solved in nearly linear time. However, the framework cannot handle the buck constraint. And the key contribution of this work is basically, we show that, well, their framework can actually be extended and solve this bulk constraint. That's the main message. Okay, now what are some application of this? How, what is a useful utility of the LP solver? So we can use it to estimate the optimal integral solution between factor two, because of the integrality gap is two in this case.
00:05:20.278 - 00:06:21.404, Speaker A: And we can also use this to get the integral solution that is running time on this form of m over epsilon square plus m to the epsilon square. But the key approach here we follow the standard template that also even outlined in the other works that we use. We solve the KsS LP up to one p accuracy, and we specified LP solution using some form of graph compression theorem from Benzo Carter. And we show that we can specify the graph so that the number edges is of the form k squared and epsilon minus two to the mass two. And then we run the combinatorial algorithm bicooler whisking on this support, on the support of specified graph and enhancement gets this kind of running time. But for today I'm more interested. I want to focus on how to solve the LP solver in nearly linear time.
00:06:21.404 - 00:07:00.798, Speaker A: So I want to focus on this main result. Now, next I'm going to talk about the multiplicity update framework, and I want to highlight the key challenge that appear in handling the bulk constraint. Okay. And look it. So, okay, I know that everybody's familiar with this framework. I want to phrase this in a language that we know in a homo language here. So let's talk, let's take a look at the covering LP, the general covering LP that minimize c transpose x such that ax at least one.
00:07:00.798 - 00:07:57.634, Speaker A: Now you can, one can think about this as a really tall implicit metrics in a sense that the number of rows can be exponentially really large and order entry is positive. So, I want to solve this covering LP and the standard multiple update framework from gag. And Kahneman says that, well, you can reduce this problem to solve a sequence of the simpler problem, which is called the minrol problem. And the mineral problem is the following. I give you a weight vector, which is basically the weight of each column, and then the minimal problem asks for the minimum row such as the inner product between the row of the matrix and the weight is minimized. So that's the main role problem. And the key theorem is that, well, you, you can obtain the one part s approximate solution to this covering LP by solving a sequence of this problem for m log m over s one squared iteration.
00:07:57.634 - 00:09:06.812, Speaker A: Now, the proof itself is also not very difficult. I also highly recommend you to check Ombre's talk on the dynamic graph workshop that talk also the same LP topic. And then he provide the elegant one slide proof of this theorem. But for us today, it's only for us, the goal is to compute this fast enough, so we will focus on what is the mean role problem in our setting, and then how far can we solve it. Any question on this part? If no, then okay, good. Next, I'm gonna give you one example of how to use this framework, how to use this framework to describe the mean law problem of Kec SMLP, meaning that the kh connector spanning multi subgraph, so it's the same thing as k css Lp, but there's no bulk constraint. Okay, and then you can write this matrix a as a cut and as incident metrics.
00:09:06.812 - 00:10:16.434, Speaker A: And what is the mineral problem in this case? So for example, so the cut incident method, you can see like c one represent one cut and then it incident on which edge and so on. And the mineral problem is the following, that I give you an edge, I give the weight function on each edge, and then ask for the minimizing role here, any minimizer role here, which correspond to the min card problem. So basically, if you look at this LP and then you forward look at this format of min role problem, then the oracle problem here is the classical min card problem. However, if you want to solve this LP, you cannot quite, it cannot quite do that directly because of the annoying bulk constraint over there. That is, that makes the problem not, not LP, not really a PO covering LP. If you want to do this in all setting, we need a PO covering LP. And then there's a standard way to get out our disk bulk constraints by adding the extra inequalities, also known as NASA call inequality, which is a very generic technique, but to be specific for our setting.
00:10:16.434 - 00:11:24.134, Speaker A: So we do the strengthen the LP in the following way. First, look at the cut like for each cut in the constraint, we add the following. Pick any subset of the edges inside that cut or size at most kick, and we're gonna, because we want to remove the bulk constraint, we're gonna add the constraint in a way that we fought the solution to pre purchase the set of HSF already f of them. I will force the solution to pre purchase that, so that the remaining part you need to satisfy k minus f. And I do that for all the subset of size k inside f. And by doing that we can get rid of the upper bar constraint, because the constraint here kind of enforce you not to pick the larger one. And we can rewrite this constraint in this form.
00:11:24.134 - 00:12:29.586, Speaker A: And we can ask the same question, what is, we can ask the same question now, what is the oracle problem of this kind of po version of this k Css? So now we can write this in terms of the cut free incident metrics, where each row corresponds to a cut and a set of the free edge. When I say free edge, it means that the free edge set inside that cut. So now it's a cut and free edge at incident to an edge matrix, right? And it's a standard fact that you can solve this. It's enough to solve this po hovering version. And okay, what is the mean role problem? Now, it's also instructive to take a look by inspection, what would be the constraint metrics of this extreme version of the LP. So let's fix the cut, fix some cut that contain three edges, for example. So in the metrics, the cut incident metric would look like this, that have one, one, one on the first three column.
00:12:29.586 - 00:13:53.244, Speaker A: And if I add, let's say the free edge, if I add zero, free edge, it means that I need to satisfy k, right? So I get the first column that is one over k on the first row, and then in the next row I could also make is a single edge free when I make a single edge free, meaning that I don't need to worry about to satisfy one of the edge and then I need to satisfy less than k, k minus one and so on. And then this came happening for like one and also for all pair, for all two and also on. Okay, now looking at the formula for the mineral problem, you just take look at this matrix a with adding the new constraint and look at the weight function at its edge. And mineral problem asks for minimizing the inner product between this constraint matrix and the minimizing the role of the matrix and input of the role matrix and the weight. Right? And then mineral problem turns out to look like the following function that you want to minimize or overall cut and then each cut you allow to kind of ignore up to f edges and then f here can be at most side of at most k minus one. And then you need to also normalize by k minus f. And here, when I denote ws, it means the sum of the weather inside the set.
00:13:53.244 - 00:15:00.774, Speaker A: The upshot is that, the upshot is that this is the new mineral problem in the case we assess problem. So we get this, this expression which we call the normalized free card problem. And notice that in the version of Kecsm where we don't have the bulk constraint, the mineral problem is just simply a min cut. But now if you have to deal with a bulk constraint, the mineral problem becomes this normalized version of the free cut problem. So the upside is that there's a technical challenge that if you remove bulk constraint, apply this multiplicative update framework on this covering lp, then you get this, this normalized free cut problem. Any question at this point? Very good. Now let's spend some time trying to understand what this normalized free cut problem a little bit more.
00:15:00.774 - 00:15:51.228, Speaker A: So what do we know anything about in the literature? So what do we know so far? So one thing to think about is that look at this cut. So you look, the nominator means that you can fix, let's say fix a car c, right? And then denominator means that you can ignore up to k minus one edges. So basically you should ignore the most expensive edges in there. But when you, the more you can ignore the less normalizing factor that you have, right? And in other words, you can also write this problem and rewrite it expression by the following way. So I define lambda I to be the mean cut with I edges being removed. So we can, let's say you can remove I edges and then this is a lambda I. I call this I free min cut.
00:15:51.228 - 00:16:55.054, Speaker A: Okay, now you can also rewrite this expression to be the mineral problem, to be like the minimum between lambda I over k minus k minus I. So basically if it's a minimum between the normal mean cut and normalized by k, or you ignore one s, but you normalize less or up to the k minus one. Okay, and how fast can we solve this problem? What is the complexity? So even, is it even polynomial time solvable? But this is a question to look at. And so let's say, let's imagine that I want to compute lambda I. How do I compute lambda I, right. So basically I can do the following way that I can do like and choose I basically to compute I free mincut. The way to one brute force way to do is to select look at a graph, remove I edges from the graph and compute min cut, right? And then you can do that and choose itime.
00:16:55.054 - 00:17:31.914, Speaker A: And then you get like call of min cut. But I mean, but if you need to do all that, then it's going to be like om actually. Yeah. And if you do do all of that, this is like would be exponentially many calls, I mean type. So this is not the like the right strategy. And it turns out that computing lambda I is also known as the connectivity interdiction problem, where it is known that you can compute it in polynomial time m two to the four. And therefore if you plug in this result by St.
00:17:31.914 - 00:18:34.238, Speaker A: Louisian, we can also show that, well we can solve this in km into the four and that the technique there is based on some cut innomination problem. But if you, but for our purpose we need much faster than that and we also need to be able to maintain this cut as well. So this seems to be really, really difficult problem. And the main result to disable is that, well this mean row problem is not that difficult in a sense that it's actually we can reduce this to the min cut problem, actually. So this, the main technical results in this work is that we show that we can solve this mineral normalized min cut problem by the reduction to polylock, many of them of the min cut. And furthermore the technique also works for the event dynamic setting. The reduction can be done in a, so in the dynamic, basically the main dynamic mineral, another result is that we can maintain it dynamically under the weight increase of the multiplicity rate of the framework.
00:18:34.238 - 00:19:29.786, Speaker A: And then the total time is still the same as the dynamic main card problem in the secular and conrad in the, that they provide for KCSM problem. So now I will get a little bit technical on this part, how to give you a sense of how this can be done on the static version. Now I will explain how to do this and then I feel like it's very simple trick. So let's say that we focus on the following problem. I have a weighted graph and then I want to output this normalized mean cut like in a linear time, and I want to output one plus epsilon approximation. So let's set some notation a bit. So first, denote that opt is just the optimal value of this mean row problem, the normalized mincut problem.
00:19:29.786 - 00:20:06.004, Speaker A: And now let's say that we can guess the value of this opt up to one plus one. So let's say that row, this is the guess value that is exactly one plus x one opt. Then I define the weight function in the following form to be the mean for each edge. I just truncate that if it's exceed role, then I will just set it to row. Otherwise I will just set it normal weight. So basically just do weight truncation and by row. Okay, and by doing this, the key structural property is the following.
00:20:06.004 - 00:20:56.276, Speaker A: So take a truncated weight and look at the minimum cut. The key claim is that you can always find a free set of size k at most k, such that the value, the normal life value is one part approximation. And then if this lemma says that, well, it's enough to just solve mincut, and then you can find a face set automatically inside this cut. So given this lemma, then the algorithm is very simple. Just find the min cut, find the optimal f star, the optimal fee cut, and then that's the deduction to the main cut. Okay, next, I'm going to explain why we should expect this mister one to actually, we even gave a full proof of lemma one. One question.
00:20:56.276 - 00:21:43.402, Speaker A: Yeah, so this also works when actually zero, if you, oh, you mean like if you, okay, the question is if, if row is exactly off, then will it work? Yeah, the answer is yes. So it's actually, the mapping is exact. So this will also solve the exact problem in your near term. Well, this, you mean the exact problem for the mineral, for the, at least I believe so, yes, yes. So the mapping is exact, but the way we guess we cannot get there. Thank you for the question. Okay, so let's take a look.
00:21:43.402 - 00:22:28.894, Speaker A: Oh yes. Does it imply anything for the main interdiction problem too, or not? Oh, actually that's a great question, because there's a follow up work that the technique that we use here, we actually solve the open problem in the interdiction problem as well, where there was an open problem that can you get the FP task for the general interdiction problem. It turns out that this technique can be kind of generalized and then the faster interdiction problem. But it's different topic. Yeah, but thank you for that question. Yeah, so basically this transition and this one can be also used to speed up the event, the interdiction problem. Yes.
00:22:28.894 - 00:23:17.164, Speaker A: Okay, now let's take a look at the proof of the lemma one. And this proof is very simple. I can even write a full proof here. So let's say that the lemma says that you can find the optimal set in the minimum cut with respect to the new weight, right? So let's split into two claims. The first claim is the following, that if the weight for any cut, if you can find a weight is smaller than k, one price a lot, optimize, then you can find a preset that we are looking for. And the second claim is the following, that the mean card satisfies that bound. The mean count will respect to that weight.
00:23:17.164 - 00:24:19.384, Speaker A: So to be divided two step. So let's first prove the claim number two, which is very, quite straightforward. So let's take a look at the definition again. So this is a truncated weight. So the weight is like the minimum between the original weight and the threshold, okay? And let's say that the optimal normalized mean cut is of the form d and f, okay? Then we can first show that the truncated weight is at most the original weight here, because by definition of this truncated weight, and now by applying definition of the being the optimal normalized mean cut, this is the same as k minus f times epsilon. This is just definition, right? And also, and also the next line is look at the next the way of f. We also know that it's at most row time f because well we do truncated, so you cannot go beyond draw.
00:24:19.384 - 00:25:10.344, Speaker A: And that's the binary channel is like one transform f. Now we just add them together and then we get this minimum normalized cut is this satisfy like one plus one k opt. And therefore there has to be a min cut inside this way that is smaller than 1001K opt. And then we are done with claim number two. Now, now we do the claim number one. So now we gonna prove the first claim. So let's f.
00:25:10.344 - 00:25:50.500, Speaker A: Let's say I fix any hard c satisfying the condition in clinical one where that the weight in the truncated is at both epsilon truncated times opt. Let f be the set of heavy edges. In this set, the heavy edges means that the weight is at least one part sl. So I define as heavy, otherwise light. Now clearly this set of the heavy edges has to be, cannot have more than k because otherwise the weight could be far bigger than this one. Up. Good.
00:25:50.500 - 00:27:00.358, Speaker A: So there, so we know that there are only at most k minus one which is good for us. And what about the weight of the cut that is not heavy inside? If it's not heavy it means that when you truncate then you don't change anything, right. And then you can write this form, the second form into the original weight and then subtracted by the truncate weight. And now for this part you can write roll to be one opt. And therefore maybe what I want to say is that if you following each of this step then they get the weight of c minus f is going to be at most exactly k minus f, this 1% opt. And then that's the whole proof of the lemma. Yep.
00:27:00.358 - 00:27:44.790, Speaker A: This at least gives you a sense that the key thing is that you said do the correct threshold and then it basically become a minimum cut problem and then so on. But the question for the dynamic version of the problem is the following, that maybe the, when you maintain the weight, when you do the multiply weight update, they can keep increase and so on. And do you need to like set the threshold differently and so on. And the thing is that, and then to handle that case we're going to use the batch version. So let me first review the dynamic version. So now let's leave you the multiplayer update framework for Garnet again. Now the algorithm looks like the following.
00:27:44.790 - 00:28:27.088, Speaker A: First I initialize the weight and somehow there's a way to initialize it. And the key point is that repeat the following, that compute approximate minrol and then for each edge inside I will update the weight. So that's the normal framework and the weight can keep increasing and so on. For the, today's, today's talk, I will not deal, I will not deal with how to do the second step efficiently, how to do the update the weight efficiently. I just focus on how to maintain the minrol efficiently. Okay, so there are two key issues that we need to be aware of. First, how to update this role and every time.
00:28:27.088 - 00:29:24.088, Speaker A: And then second is how to identify the free set quickly because the free set can be upsized up to m. So identify the free set already takes time even if I know the optimal cut for you. So first, to handle the first part we're gonna batch off into the range of power of I, power of f one part s one I because in the same batch then you can use the same threshold and then it's gonna work out beside the same same batch. How do you how to do the batching? So this is exactly the framework, epoch based framework of the multi tablet update. So this is at least first node that I can trace up is by fly show that you can do this kind of batching for the multi commodity flow. But it can also be very general. So the framework is the following, that you also initialize the way the same way first compute the lambda which is the mineral at the current weight, and repeat the following epochs.
00:29:24.088 - 00:30:20.484, Speaker A: So you need to do this only log and western square epochs and how to do in each epoch. Then in this epoch computer the min role problem, which is CNF here and such, that normalize weight is within this range and update the weight accordingly and repeat. And then we do this until the main role has increased by at least one part SLR. And then we are, and then this leaves the end of, and then we gonna, and then this is the end of the epoch and we start a new epoch. And it turns out, and then if you notice that in this the same, in the same epoch, then the same, they have the same threshold, so you can just use the same threshold throughout the epoch. So this is works really nicely. So now I'm going to focus on how to speed up and implement in this report.
00:30:20.484 - 00:31:15.524, Speaker A: Now the goal is to implement this in time running time off linear time plus the number of cuts, number cut out that output inside the epoch. So if you can do this, then if you can do this implementation, then the total running time would be of this form. But you have some design time over all the epochs. And then you can show that it's going to be nearly linear running time because the number epoch is only log and os on square. But the number of the cut that you produce in each iteration that is used is exactly the same as the number iteration in the non epoch version. So you can charge this to the number iteration, charge the number of cut to be nearly linear. Okay, so now everything is clear.
00:31:15.524 - 00:31:52.366, Speaker A: So now I'm gonna tell you how to do this. To do this reduction to the dynamic min cut. Basically we, for simplicity here, let's not worry about the size of edges. Let's say that the number of edges is always bounded by polylock factor. And we're going to use a data structure from security and conrad to handle the general logistics. So let's see how we know how we do this. Okay.
00:31:52.366 - 00:32:50.638, Speaker A: And then before that, there's also another issue that you can focus on, which is how to identify the best free set quickly. And then the key answer is that wherever you do thresholding, it will automatically give this free set for you by the thresholding. So this can be summarized, the thresholding can be summarized in this mapping theorem in the following. So fix a threshold that is one place on lambda and did not opt you to be the mineral optimal and also the truncated weight here to be the truncated weight that we use. Now there are two statements. First of all, say that if as long as the mineral is inside the range, then the minimum cut with respect to that is going to be inside a certain range. So there are two space.
00:32:50.638 - 00:34:03.476, Speaker A: One spread is the original weight and the second space is on the truncated weight. Right. As long as op is inside this range, then the min card is going to be inside the smaller range. And if the second property is that whenever you find a cut such that the weight of the truncated bit of that cut is smaller than k row, that is a good cut, meaning that you can take that cut together with the free edges defined by the set of edges that is heavy, meaning that you just take the edges that is heavy but that has at least row. Then you can show that if you give this, it automatically gives you a good free edge and then this is a good approximate free cut to the mineral problem. Okay, let's see how do, how, how do we use this to, so basically the proof is very similar to the static case. So it's very small modification, then you will get it.
00:34:03.476 - 00:35:29.090, Speaker A: So I will not present the proof here, but I will show you how to use this to implement each epoch. But basically the output is the following. So we initialize the way the bin you row, I mean we know the, let's say we have at the starting of the epoch we initialize the weight of the min euro. And then the goal here is that we just run the initialize the dynamic algorithm of by Chakuli and Conrad with respect to the truncated weight. And the goal here is the following, that we just keep listing the cut in this range when, because by the theorem, if we can leave the cut by this range, we are in good shape, we will be able to find a good cut and then we keep update the wave and repeat. So basically it's almost a black box reduction with small control that we need to be careful so to see how it works. Like for example, so this, so let's say that at the first iteration of it inside the range, by the range mapping theorem, on the first condition, we know that there has to be some cut inside this range in the second space, because there is some cut, then you can find, you can use a dynamic data structure to list this cut.
00:35:29.090 - 00:36:48.218, Speaker A: And then because this cut in this range, inside this range, you can use the second condition of the range mapping to say that, oh, this cut, you can also identify the corresponding, the free edge set. And then you will get something that is close to optimal cut automatically from, from Ethereum. And then when you do the updates, updates the weight, this optimal value and the cut, and the weight of the cut also increases a bit. And then the goal here, just like keep, keep repeating listing this cut until everything go out, until the cut in the second range go out of this range, go out the row, and then this kind of finish of epoch. And then you can show that this simple range mapping theorem will give you all the other ingredients that you need. And in here, I hide all the details about how to exactly implement the reduction, but the high level of the range mapping will go along this way. Okay, so actually that's pretty much all I want to say in this talk.
00:36:48.218 - 00:37:39.354, Speaker A: So I just want to conclude here that I would love to also see more systematic theory of the fast approximation algorithm. I would love to see some more universal theory on how to do fast LP. On one more of the combinatorial optimization problem. Can we develop fast routing algorithm? Can we do the nearly linear time for two s connected? It's still unknown, we don't know how to do that in linear time. And can we solve LP for even broader cast on the survivable network design problem? Can we get that faster? And even like, can we solve the same LP here with high accuracy in nearly linear time? It's also very interesting. In my view, this is because many incentive routing alternative needs high accuracy software. Yes.
00:37:39.354 - 00:38:25.314, Speaker A: So you just need one higher solution or every route. Yeah, you mean for it to be routing? So yes, I'm just asking like, do you need to solve one LP then, or many? Many? Okay, yeah, iterative routing means like you solve many LP. With that, I would like to conclude my talk. Okay, then let's think Sora chai again.
