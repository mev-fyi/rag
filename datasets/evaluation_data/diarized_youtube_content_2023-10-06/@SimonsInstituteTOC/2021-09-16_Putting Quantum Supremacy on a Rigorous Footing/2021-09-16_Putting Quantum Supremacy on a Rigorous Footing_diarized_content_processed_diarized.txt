00:00:25.634 - 00:00:26.174, Speaker A: It.
00:00:55.624 - 00:01:02.164, Speaker B: On the program. Reading the program this morning, I thought it was going to be put there.
00:01:10.144 - 00:01:34.476, Speaker A: Sorry, I was going to keep this short, but maybe, maybe not. Yeah. So, as you all know, core ingredients to establishment privacy is exhibiting, presumably, our average case nature, which are hard for.
00:01:34.500 - 00:01:37.868, Speaker C: Classical polynomial time algorithms, but which you.
00:01:37.956 - 00:02:37.904, Speaker A: Have efficient quantum algorithms. So we're really excited to have Umesh tell us about recent work on putting this. So my charge was exactly to say, try to say, what are the open questions to build up towards the main questions that remain open in terms of within quantum supremacy on a rigorous setting. So let me, let me try to. And, you know, I want to. So I want to get to the questions that are, that are really at the core of this, and hopefully, you know, some that you can, you can tackle without really doing too much. Okay, so, so here were the, here's the backdrop.
00:02:37.904 - 00:03:43.008, Speaker A: A couple of years ago, Google announced, announced their quantum supremacy experiment based on 52 qubit quantum circuits, depth 20, with gate fidelity about 0.99. Okay, so that was roughly 1000 gates, each of which failed with probability one in 100, which meant these were very noisy outputs that they. And then last year, there was a, you know, at the University of Science and Technology in China, there was an announcement about Boisson sampling experiment, which was effectively a larger Hilbert space of roughly 72 qubits. 76 qubits. Okay, so now, so how do we get to understand what coordinates. So let me just give you a little bit of a backdrop to that, and then, okay, so what's quantum supremacy? Well, for the. To understand that, let's just go back to the early history of quantum computing.
00:03:43.008 - 00:04:29.282, Speaker A: So the computer science interest in quantum computing really started with this notion that quantum computers violate the extended church thesis. Now, here's one way to look at it. You know, so why is. Why is quantum supremacy so important? Well, there's one way to look at it. So the classical description of an n qubit state is this exponential superposition. So it's a two to the n dimensional complex vector, where each amplitude corresponds to the amplitude of a particular configuration, a particular n bit string. And of course, when we measure, we don't get to see the superposition.
00:04:29.282 - 00:06:19.870, Speaker A: We get to see almost no information at all, just an embed string. And so for all practical purposes, we could think that this quantum state is a black box, that nature protects her secrets extremely, guards them very closely, so that. So that, you know, you might think that, in fact, nature is being so scrupulous about it that there's no way that we could ever experimentally you know, probe and verify that this, that there's this exponentially large Hilbert space. And so in the, you know, in the early days of quantum computing, it was, it was, it wasn't lost on us that, that once you have a violation of the, of, of this extended church doing thesis, you get implicitly a way to, a way to probe Hilbert space and verify that the Hilbert space was exponentially large. Right? So, so, for example, you know, well, if you did, if you were able to do this experiment demonstrating an exponential speed up, then you would have a computational verification that there was an exponentially large hoop space. And so, in fact, quantum supremacy is sort of this very jazzy new term for an experimental violation of this extended chestering thesis. Now, also already in the early days of quantum computing, it was pretty clear what you should do in order to do such an experiment.
00:06:19.870 - 00:08:26.754, Speaker A: As soon as we had Shor's algorithm, that was as long as we didn't care about the actual experimental difficulty involved. Because this was the early days, you may as well assume whatever quantum computer you wanted to, then Shor's algorithm was the natural way to carry out this experiment, because simple idea, right? You input a number, you get the factors from the quantum computer, you multiply them to see if you got back that number, and then you say, okay, well, that's it. I now know that classically, it would take an exponential amount of effort to do this. And therefore I have a real verification that behind the scenes, there must be an exponentially large object, like now, this notion started changing around ten years ago because there was a prospect that experiments would get close enough. So now they were back off the envelope calculations, saying that if you wanted to do this with factoring, then you would need thousands and possibly millions of qubits, where we are just going to be nowhere close to that. And so the question was, could you actually do this without appealing to quantum algorithms, the way we were thinking about it? And so there were two, two proposals that spoke about how one could do this in the near term using the kinds of quantum computers one could envision, which would be very noisy. And so these were based on sampling tasks, so where instead of actually solving a computational problem like factoring, you would just use the quantum computer to sample from some distribution.
00:08:26.754 - 00:09:35.254, Speaker A: And so there were two such proposals, one due to Ernst and archipelago boson sampling, and the other due to Brevner Joseph Shepard. This was called IQP. Okay, so, at the end of the day, what do we want from this, from this sort of an experiment? So we want to say look, we have some. Some device that claims to be quantum, and we are going to collect some statistics from it, and then we are going to. We are going to perform some statistical analysis on this, on this collected data. And so we have some statistical classifier, which, for which we say, okay, our collected data lies on this side of the line. And then together with that, we also have some theorem that says any classical device must sit on the other side of the line.
00:09:35.254 - 00:10:42.082, Speaker A: Any classical device that's efficient couldn't possibly have generated data that lies on this side of this dividing line. Okay? And so we have some statistical measure that distinguishes between a quantum device and any classical device that's efficient. So that's what the goal of this enterprise is. So there are two parts to this enterprise. One is this a statistical measure. Once you get this data, how do you evaluate it? And the second is some kind of a proof, a complexity theoretic proof, saying that any low complexity classical device must sit on one side of this. Okay, so now, how do these sampling tasks go? So, of course, we knew from the, you know, early days of quantum computing that this is what quantum computers are good for.
00:10:42.082 - 00:11:22.628, Speaker A: And, in fact, you know, they sample from probability distributions. And, you know, even the most basic thing you do with them is Fourier sampling. So, you know, that's what that would. That's what comes very naturally. Right? And but there was, there's also this sense in which probability distribution generated by quantum circuits look very different from all quantum computers, look very different from those generated by classical computers. And and, you know, this was. But somehow, you know, this.
00:11:22.628 - 00:11:51.678, Speaker A: This was clear in this. In this proof that BQP lies in gap p. It's the difference, you know, if you. It's. You can solve it by solving a counting problem. But the kind of counting problem you solve is you take the difference of two, two sharp p functions. And the way this comes about is, let's say that you have a quantum circuit, c, which works on input, say, zero to the n.
00:11:51.678 - 00:12:43.614, Speaker A: And now your output is, you know, you run this quantum circuit and you measure the output, and you see what you get. Okay, so now we want to understand, what's the probability that we'll actually measure x? Okay, so how do we figure this out? Well, what you can do is what you do. If this was a probabilistic sort of circuit, then you would say, well, look at. Let's look at all possible computational parts that lead from the input, you know, through all possible branchings, and that eventually output x. And let me add up the probabilities of all these parts. So, the probability of a path is the product of the. Of the branching probabilities along the path.
00:12:43.614 - 00:13:37.744, Speaker A: And now we just add them. Now, the point with a quantum computer is that these branching probabilities are actually complex numbers. But, in fact, the complex numbers don't matter. All that matters is that you're allowed to have positive and negative numbers. And so, when you look at the probability to associate with a path, it's actually probability amplitude, and it's either positive or negative. And so you'll have, if you want to look at the probability of x, what you'll have to do is look at all, all those parts which give you a positive amplitude, all those that give you a negative amplitude, and take your total amplitude is the sum of these two. So it's a plus minus a minus, and then the probability of actually cx is the square of this.
00:13:37.744 - 00:14:20.054, Speaker A: And so when you square this, what you will get is actually a plus squared plus a minus squared minus two times a plus a minus. So, this is where you get the difference of two quantities. And you can write each of these sums down the positive quantity and the negative quantity. You can just sum them separately, and you get two Sharpie functions. Okay, so, this is a basic difference in a probabilistic circuit, computing the probabilities in Sharpie. Quantum circuit, computing the probability that you output x is in gap B. And, in fact, it's gapi hard for the worst case circuit C.
00:14:20.054 - 00:15:07.364, Speaker A: All right, so, so far, so good. You know, at the time, you know, we just said, okay, it's in Shabti. You know, it's slightly different from Shabti. You know, it's in Gatti. Okay, but this was really a crucial point, and this was really something that Aronson Arkhipov and Rabna Shep had really noticed that. That this difference is really quite extremely important once you start basing trying to base this quantum supremacy on sampling tasks. And so why is that? Well, okay, sorry.
00:15:07.364 - 00:15:35.974, Speaker A: Already it was evident that there's a. There's a. There's a major difference, which is this thing, you know, uh, if you want to. If you want. If you look at the probability that. That a classical circle circuit will output x, it's in Sharpie, but you can also estimate it with an np oracle, right? So. So, approximating this probability lies in the polynomial hierarchy.
00:15:35.974 - 00:17:04.170, Speaker A: Once you're given the difference of two shopee functions, estimating that difference is still Sharpie hard. Right? So, even though these two look very similar to each other this way, in terms of computing the value once you go to approximate them, this one is much simpler than that. And so quantum and classical are very far apart. Once you talk about approximating these probabilities. Now there's a but, okay, so, but there's a crucial step from there, which was a really important, I think, you know, it's a simple step, but I think it's sort of a very interesting, you know, it's a, it's simple but not entirely obvious step, which was this. So what they, what Einstein, Akopoff and Rabna Joseph Sheppard observed, which was really, I think, quite, quite important, is that they said, well, look, suppose that you have a classical computer that can sample from the output distribution of this, of this quantum circuit, or even from a close by distribution. So now you can use stockmire to say, well, if you can sample from the distribution, then you can also approximate this output probability right in, within the, you know, with an NP oracle.
00:17:04.170 - 00:18:08.054, Speaker A: So within the, within the polynomial hierarchy. And now once you've actually approximated this, this output probability, you've actually separated these two. So, so, well, okay, so on. Shouldn't, shouldn't be done. You know, shouldn't we now have a rigorous proof that, that if you solve this sampling problem, then you couldn't possibly have done it classically, unless the sharp, the polynomial hierarchy collapses, you have sharp t being approximated inside the collapse down to the polynomial hierarchy. So the answer lies in this fact that this is for a worst case circuit. So this is for the worst possible input, and who knows what that is.
00:18:08.054 - 00:19:26.824, Speaker A: So what we really want to do now is we want to say, actually, it's, you know, it's, it's hard to compute this, this output probability or even to approximate it for a random circuit c. So that's, that's the crux of the matter. So that's how this, this. Okay, so, so what was the, what was the experiment that Google did? It actually did exactly this experiment. They had this superconducting qubit with these other qubits. Those are the couplers through which you can program the gates. And so, so what they did was they fixed a particular sequence of, you know, of settings for, for 20, you know, for depth 20.
00:19:26.824 - 00:20:04.126, Speaker A: And so that was, that was your random circuit C that was fixed. They initialized all the qubits to zero, and then they ran the circuit, measured the qubits to get a 52 bit random string x. Okay, it's a sample from the, you know, assuming that this was really a quantum computer, then it was, it was the output of that quantum computer, except it was noisy and very noisy. Right. Because there were thousand gates, each with one in 100 chance of error. Now. And then they repeat.
00:20:04.126 - 00:21:06.974, Speaker A: Right, so for that same random circuit, initialize again, you know, pick another sample and keep doing this. And then, okay, so now in order to do their statistical test, they also needed some help. So they said, okay, now let's use a supercomputer to compute what's the probability that X should have been output if this device that they had was really this quantum circuit C that they had picked. So that requires an exponential amount of time. But luckily, 52 was not that big and they could use some heuristics and so on, work this out. And now they needed to check whether the sample axis were consistent with that was the whole experiment. Okay, so, okay, so there were two challenges.
00:21:06.974 - 00:22:19.084, Speaker A: You know, one, how do you design this statistical test? And two, how do we know that approximating this output probability for a random cohort circuit is elite constant? And actually, if we, if we do know that, it's hard, then by Stockmyer sampling, you know, we know that something from any distribution is constant. Distance from this probability distribution is high. So at least we know that. And then we need to design our statistical test to check the constant total variation. Okay, so now let me just briefly talk about what we know about with respect to this second question. How do we know that it's out for random quantum circuits, or to what extent do we know that? And then a little bit about the statistical test. Okay, so how do we know that approximating this output probability for a random quantum circuit is not? Well, we want a first case to average case reduction.
00:22:19.084 - 00:23:18.518, Speaker A: Okay, so the first thing we're going to do is we're going to model, we're going to do have a simple simplifying assumption. We'll assume that the random quantum circuit actually implements our random unit, you know, perfectly random unitary on n qubits. So this is something that appears to be true once you get to depth n circuits, of course. Well, in the Google experiment, they aren't getting quite as far, but, but they seem to have they experimented with it? It seems like it's a decent assumption. Okay, and then what we want to do is we want to model, model this worst case to average case reduction after victims, permanent reduction. So how do we want to do that? Well, let me just ask a question.
00:23:18.686 - 00:23:34.998, Speaker D: Yes, I'm sorry for interruption this, David, good morning. There's a borderline trivial argument to show that the random classical circuit is not, is hard just by counting.
00:23:35.046 - 00:23:35.230, Speaker A: Right.
00:23:35.262 - 00:23:51.984, Speaker D: So what goes wrong in this case? Because you bounding the depth. I'm sorry if I missed that. Or what goes wrong in this case? Just a counting argument that if you randomly sample the circuit, then it's just the number of distribution we'll produce is too much for to be computed.
00:23:52.604 - 00:24:28.830, Speaker A: But you see, so it may be too much to be computed. But we are saying, look, you are even given an NP oracle and then a TD, right? So we are trying to distinguish quantum from classical. In the classical case, we are going through a stockmire and saying, well, once you have an NP oracle, it's easy to compute these things. In the quantum case, we want to say it's even hard with NPR, you know, it's hard anywhere in the hierarchy. That's what we want to really get to. I see. Yeah, yeah.
00:24:29.022 - 00:24:29.954, Speaker D: Thank you.
00:24:35.374 - 00:25:50.294, Speaker A: Okay, so just, you know, just to make sure we're on the same page, let me just do a very quick a review of, you know, the way I'm thinking about Lipton's argument, just to get our notation to match up. So in the permanent case, well, you have some, you have some matrix a who's permanent. You're trying to compute, and so you define this other, you know, a one parameter family of matrices. You pick a, pick a random matrix R. And in this case, you're working over a finite field, and so you let a of t be x plus t times r. And now a of t is random for any non zero t. And then there's this vein, you know, and then you observe that permanent of a of t is a degree and polynomial in, in t, and that what you're really interested in is permanent of af zero, which is permanent of x.
00:25:50.294 - 00:27:04.774, Speaker A: And so if you, if you were to compute the permanent of a of t for n, different value non zero values of, of t, and these would be n random permanence. And in the process, you would have computed random, which is a non random. Okay, so I'm going to describe to you what it takes to actually carry out this, this sort of thing in the, in the context of quantum circuits. Yes, but the quantum circuit, approximately, you need depth m for two design. So, you know, and then there are results saying even along the way, you get, you converge towards it. Of course, you'd want for har, and then you'd want. Okay, but, you know, but this is the simplifying assumption.
00:27:04.774 - 00:29:05.000, Speaker A: Okay, so let's try to put the pieces together for such a reduction and see what all we need. Well, one thing that would be convenient to have is a polynomial somewhere, right? So, you know, and so what, you know, the first thing you know, sort of, this is, this is sort of the easy thing to say, that value output probability of the quantum circuit is, you know, if you have a formant circuit, m gates, then you can write it as a, as a, as a polynomial in the parameters of the circuit, right? If you let the gates be c, one through c, and then you can sort of, again take a, take a sum over all parts. Okay? But now the place there's a problem with is, is if you, if you're trying to, if you're trying to, you know, somehow, you know, reproduce that previous argument, Lipton's argument, then we run into a problem straight away, because we can't take, you know, given a circuit c, we cannot, we cannot sort of take c plus t times r. If this is a unitary and this is a unitary. When you, when you take a linear combination of them, you won't get a unitary. Okay, so how do we, how do we sort of go from a circuit to a hard random circuit? Well, there's an easy way to do it. What you could do is choose, choose a bunch of r random gates.
00:29:05.000 - 00:29:49.396, Speaker A: So, and, you know, basically replace each gate by that gate, followed by a Horandon gate. So that gives you, you know, if you start with any circuit, you'll end up with a circuit, which is horror. So that's their first. Okay, but now there's nowhere a univariate polynomial structure, right? So which connects this, the circuit we started with this new random circuit. So we should do something different. Okay, so here's what we can do. So the thing we can do.
00:29:49.396 - 00:30:32.916, Speaker A: So the interesting thing in a quantum circuit is that if you can apply a gate, you can also apply the half of that gate, the square root of that gate, or any fraction of that gate. And so what we can do is we won't quite apply this r random gate. We'll just dial it back by a tiny amount. So that's this theta here. So theta equal to one. Then, of course, we just dialed it back totally. And so we got back to the original circuit.
00:30:32.916 - 00:31:16.638, Speaker A: If theta is zero, then we totally applied this random gate. And so it's where we were in the previous slide. But now we'll assume we'll take theta as small. And so now each gate is very close to Horandon, and now we have the single parameter family again, because we're going to apply different random random gates for each gate of the circuit. But we'll dial it back by the same amount. Theta, but it's, theta is the corresponding parameter to t in the elliptin. So now we have a univariate polynomial.
00:31:16.638 - 00:32:24.674, Speaker A: Well, we take a small theta and then we want to work with that. Okay, so now there's still a problem. The problem is that this dialing back, this is not a polynomial in theta. And so that's, of course, something that's very crucial to the average case reduction. So the solution is to take a truncation of the Taylor series, and now you can get very close, and you do end up with a polynomial in theta. And so you can now go through the, through the template. Okay, so now, of course, when we do this truncation, it introduces a small error.
00:32:24.674 - 00:33:43.604, Speaker A: It turns out that there's a different way of doing this, which is a little bit more complex, but this is what Moba Sark did. He showed that, in fact, there's a different way to do this where you use, use a kleepath interpolation and, you know, which, which actually stays unitary throughout. So you don't even need this. Okay, so, all right, so now let me, let me talk about the error that, you know, how robust this. You know, so, so this gives us a reduction, but we have to talk about how robust it is. Actually, while talking about the robustness, I'll actually talk about the recent improvement that was done by two groups there. Okay, so, so how do we understand that? So we, you know, we should understand that in terms of polynomial interpolation.
00:33:43.604 - 00:34:20.084, Speaker A: And this is over the real. So what are we doing? Well, we have some small interval, you know, within which theta is allowed to sit. And we chose theta to be very small with a small perturbation. And, of course, where we want to go out to is theta equal to one. That's where the worst case circuit sits. So this is where we collect the data. And.
00:34:20.084 - 00:35:29.324, Speaker A: Okay, so the range in which we collect the data is. So we let this, this interval be of size one over m, where m is the total number of gates, because these are going to add up, and that's where we want these to sit. What we'll collect is about m squared samples of equally spaced thetas. And now we are going to assume that at least two thirds of these samples that we picked are delta, close to the actual value that we were supposed to get. So we are assuming that this quantum computer is noisy. One third of the time it might just give us complete nonsense, but two thirds of the time, it gives us something that's an approximation to what the true value is if you were to use the true quantum secrecy. And now, of course, you know, with this noisy interpolation, that would be hard.
00:35:29.324 - 00:36:13.764, Speaker A: But remember, we have an NP oracle, so we don't need to sweat that. And so we'll just find a polynomial. And now we need to bound how far away? Okay, so, so on the, you know, in our, in the. In the previous paper, what we had shown was that you would. That you would be getting Delta, which is, which is how close these samples were. That would get amplified by exponential in m times beta. Beta was this, which was one over m so n squared.
00:36:13.764 - 00:36:46.550, Speaker A: But, but, you know, ideally you want, you want to choose Delta to be roughly two to the minus n. So we really have to improve this a lot. So that's, that's what these people do. They. They actually improve this to order m times two to the order m times log of theta. But as you can see, order m log m is still not good enough. So, one, you want it to be linear, right? Because we want to say, well, this distance, you know, how bad.
00:36:46.550 - 00:37:17.722, Speaker A: You know, so eventually we want to say, well, the worst case circuit is, you know, the output probability is hard to approximate to within. Within how much? Well, we want Delta to be sort of small there. Like we want to say, well, within constant factor. It's hard to approximate or something. So certainly we want Delta to be, you know, of, you know, two to the order minus order n. So certainly we can't afford these log m factors here. But even worse, we can't afford m here.
00:37:17.722 - 00:37:42.342, Speaker A: It should be n, right? M was the number of gates in the circuit. There was n times d, where d is the depth. We really want two to the MInuS n. So this is really, you know, it's. It's a CRuCial thing that one. One has to, has to improve for these ResUlts to be Robust. Actually, the improvement to get down to m log m is really quite nice.
00:37:42.342 - 00:38:31.582, Speaker A: The idea is to substitute theta as x to the K. So you sort of focus your attention on this interval. And what that does is it leaves the endpoints unchanged. But now beta goes to beta to the one over k that becomes m log n to the one n. So it sort of focuses the results in the. Now, the situation is much nicer for Boson saMpling, because there. I'll just go over this very quickly, but let me just give you a sense of it.
00:38:31.582 - 00:39:09.266, Speaker A: So you have N bosons, N squared modes. The degree of the polynomial is going to be n, but the dimension of the Hilbert space that you're working in, you know, these are n bosons in n squared mode. So you get this, you know, it's this balls and Bins, right? These are identical particles. So it'll be the dimension of the Hilbert space would be n squared plus n minus one plus n. So it's as though, you know, this n log n is the. Is the, is the. Is the important parameter here.
00:39:09.266 - 00:39:51.594, Speaker A: And so we, you know, we don't mind delta being one over two to the n log n. And of course, this m was now n. That's the degree of the polynomial. So everything seems fine except for the constant up here. And so for boson sampling, this result is within a constant of six or eight of being meaningful. So there. It's not clear that there's any fundamental barrier to just charging ahead and improving gives you a meaningfully robust result.
00:39:51.594 - 00:41:07.134, Speaker A: Well, because, you know, you expect the probabilities to be roughly uniform, right? And then you want to do that. Okay? So try to save. Just spend another 510 minutes just wrapping up at this second question. Okay? So it's quite a statistical test to check whether the sample x is consistent with this observed with the theoretical probabilities of generating x. So the measure that was eventually used in the Google experiment was linear cross entropy, which is just the expected value of. In other words, you actually measure x, you compute p, sub c, of x. And now you.
00:41:07.134 - 00:41:36.066, Speaker A: Now you. Now you compute the sample average of that. So you're coming up with an approximation for the expected value of p, sub c, of x. So, initially, they started with a different measure, which was linear, which is cross entropy, which would have been expected value minus log of p, sub c of x. And that also is actually quite interesting. I can say a little bit about that. But when they ran the experiments, this one did better.
00:41:36.066 - 00:42:13.024, Speaker A: And it turns out it's also a good measurement. The intuition is, well, the output distribution is not going to be uniform. It's actually exponentially distributed. And so it's almost uniform. But there are some, you know, some strings are somewhat higher probability than others. And, and now you would expect the higher probabilities, outputs to show up more often. And therefore, this expected value of p, sub c, of x should be.
00:42:13.024 - 00:42:52.450, Speaker A: Should be larger than one over two to them, which is what it would be if it was completely flat. And in fact, if you work it out, then for random quantum circuit, this expected value is exactly twice what it would be if it was uniform. So it's two over two to them. So, so, well, when they, when they ran the experiment, you know, the, the estimate was 1.002 over two to ten, which they said, well, this is larger than one over two to them. Therefore, it must be. Actually, they did a little more than that.
00:42:52.450 - 00:43:15.110, Speaker A: They said, well, look, this. This is pretty. It seems like, you know, this is only a small signal. How do we know it's. This is really a signal and not noise? And so they repeated this, this experiment thousands of times and tried to. Well, there's some. Some justification for this, but where do the.
00:43:15.110 - 00:44:14.864, Speaker A: Does. Do these justifications come from? Well, various things, but let me show you one particular way of justifying this again. And. Okay, so this notion that typically, you will see, you're more likely to see heavy outputs than light outputs. So this was formalized by Aronson and Chen in this challenge. You know, hall, which is when you're given a random quantum circuit C, can you generate several samples, x one through x k, so that at least two thirds of them have a probability larger than medium? So if you. If you look at the parameters of this exponential distribution that you get, this is, you know, your quantum circuit will satisfy.
00:44:14.864 - 00:44:45.868, Speaker A: This seems like a real challenge to kind of. Yeah. What role does the parameter k play in this? What role would they. Yeah, so, so it would. It would. Okay, so you could. I think then, you know, you can.
00:44:45.868 - 00:45:05.928, Speaker A: You can define sort of a dual problem, which I'll. Which I'll do down here. And then if you want them to correspond to each other, you. You work with the trade off between k. So let me. Let me just maybe. Let's see what it would play out here.
00:45:05.928 - 00:46:04.316, Speaker A: Right, so, once you move on to linear cross entropy, you actually have to think about this slightly differently. So there's a corresponding version of this heavy output generation, which really deals with more quantitative. And for now, the challenges. You're given a random quantum circuit c. You want to generate a bunch of samples such that the average of the average probability of this trig in the sample under the circuit circle is at least one plus b times two to the n, two to the minus n. So it's. It's larger than the.
00:46:04.316 - 00:46:59.104, Speaker A: Than the expected probability by this small amount. And so this is exactly the kind of. Kind of problem that, you know, this is. This is almost exactly a translation of the actual experiment, right? Except that. Except that in the actual experiment, well, was the deviation from one over two to the n? Was it. Was it some constant or one over polynomial times two to the minus n? Or was it actually more like one over two to the n, where m was the number of gates and I? You know, that's, that's the one thing that one has to worry about. But within that distinction, n versus m distinction, which is a source of many problems here and something that we have to think through.
00:46:59.104 - 00:48:08.098, Speaker A: But this is a direct translation up to that. And then you can come up with a conjecture which is in the, you could think of it as some sort of exponential time hypothesis or, or a type of it for quantum circuits, you know, so it's sort of going out on the limb and assuming a lot of things, but there's no polynomial time algorithm that, on, on input a random quantum circuit C. Now you want to, you want to understand, with what probability does this random circuit output the all zero string. So produces an estimate for p zero, which is the probability that this circuit output zero to the n with the, with the following minimal requirement. So we want to produce an estimate which is just slightly better than the trivial estimate. So the trivial estimate would be, would be, you would say p sub c, you know, that p zero must be two to the minus n. Okay? So all you want is to beat that by a little bit in the following sense.
00:48:08.098 - 00:49:17.950, Speaker A: You want that the, you know that the expectation, expected square deviation from p zero should be, should be slightly smaller than if you had guessed two to the minus seven. So you look at the expected square deviation here, and you just want to beat it by some, some other exponentially small amount. Now you can, you can actually show with various parameters that, that this assumption actually implies that you couldn't possibly have carried out this task classically. And therefore, if a device manages to do this, it must have been important. And basically the way you do that is, first you say, well, in these quantum circuits, you could always have either flipped or not flipped every output bit. So whether you're looking to for this probability for the string zero to the n or some other string, it's all the same. So you may as well replace this all zero string by a completely random string.
00:49:17.950 - 00:50:15.894, Speaker A: Because, you see, because this, this, you know, okay, so we want to search x quote implies this. So you say, well, if there's an algorithm for this, there's an algorithm for this, but algorithm for this, maybe, you know, who knows? You know, it may be producing strings in it in a particular way so that it never produces zero to then, so the way we'll do this estimation is, we want to say, okay, so we'll first say our task is to figure this out for a random string. And then the algorithm would be, well, we just run this algorithm, the presumed algorithm for x, Bog and then just see if this random string occurs in these outputs. We'll say, oh, yeah, it has higher probability. Otherwise it's same as one over two to that, right? So. So you say, well, it must have probably one, one over b times two to the minus n. Otherwise you say it's zero.
00:50:15.894 - 00:51:17.634, Speaker A: And then you just apply some sort of Markov inequality to say, let's say, okay, so I should, I should really wind up here. So let me just say a little bit about all this. So, there's this one issue that I said is really quite important. It's the number of qubits versus the number of gates for random circuit sampling. As we saw, we really need to not just get rid of this log M factor, but also do this m versus n thing. It actually shows up also in this, in this kind of argument, this n versus MSU is really. And it's an interpretation thing with the actual experiments.
00:51:17.634 - 00:53:00.784, Speaker A: I should also say that, you know, as I mentioned, there's also, instead of linear cross entropy, you could be looking at cross entropy itself. And then for that measure, it's possible to show that, in fact, if you see the right measure, the right kind of output on the cross entropy measure, then, in fact, you're within total variation distance. So much of the given distribution that ties the cross entropy measure to this west case to average case reduction, but under a certain assumption. And the assumption is that if you look at your, your quantum computer, then, then the output entropy that it puts out is at least as large as the output entropy of this quantum circuit that you were trying to simulate. You had in mind. You know, you thought you were programming in a quantum circuit c, and on input all zero to n, there's some output distribution which has some entropy. Now you're saying, well, whatever the quantum computer in the lab is, its output distribution has at least as high entropy.
00:53:00.784 - 00:54:18.744, Speaker A: Of course, that's a reasonable assumption in the sense that it's a noisy quantum computer, but it doesn't follow. So it's. So that's something that is a consequence of PSCH, inequality. Now, I should say that there's a whole other way of doing these kinds of experiments through using cryptographic schemes. But for those to kick in, the, the specifications of these quantum computers people are building will have to improve somewhat, both in terms of number of qubits and the fidelity of the qubits. So, depending upon how optimistic you are, you could say, well, maybe we'll get there in the next couple of years or not. But I would say that in terms of proving the robustness of these bounds, which, you know, which don't, which are for these experiments that don't rely on cryptography.
00:54:18.744 - 00:55:52.164, Speaker A: I think that not only is it relevant for the near future, but it's also going to be relevant later, because I think that at the end of the day, there are two or three motivations for all this work. One is, it's a very important scientific experiment in the sense that you're looking into Hilbert space and you're saying, well, we are verifying something about Hilbert space which isn't a priori, won't have thought it's possible to verify. And moreover, it's something that we should feel obliged to do because it's such a, you know, what, what a, you know, it's such a, you know, such an important, you know, sort of, it's, it's, it's claiming so much about what's, what nature is doing, so we should be verifying it. Another reason to do it is, you know, it's a, it's a step on the way to, to building better quantum computers. And in particular, these techniques are being used to benchmark quantum computers, and I don't think that's going to go away. So I think, you know, even if we have these other schemes for supremacy, proofs of quantumness, it's not, you know, the earlier things are still going to be of interest. So.
00:56:06.204 - 00:56:45.114, Speaker C: Thanks, Mish, for agreeing to bring some quantumness into our workshop. I have a hopelessly naive question. So, you know, for sampling tasks, we have canonical classical types of algorithms, Markov chain Monte Carlo type algorithms. Is there, okay. And one thing that we have a lot of people here who are interested in is proving unconditional lower bounds on such algorithms for supposedly hard problems. Is there some canonical classical family of algorithms that one would use if there were, if this problem were easy, that one could study to prove unconditional lower bounds against.
00:56:48.034 - 00:57:16.422, Speaker A: Um. Right, so, so, um, for these, you know, there are, there are a few classes of algorithms one could look into. You know, one is, of course, just the Feynman part integral, which is just another, you know, for us, it's another name for. What would you call it, where you just, just look at all computational parts. It's. I'm sorry, maybe. I'm sorry.
00:57:16.558 - 00:57:17.914, Speaker B: Matrix multiplication.
00:57:18.734 - 00:57:57.906, Speaker A: Yeah. But for a general computation. Right, okay. Another, another set of techniques would involve tensor networks, right? You try to, you try to break up the quantum circuit in some ways. And I mean, those techniques, you know, those techniques were actually used in. So there were brute force computations up to a certain end for computing piece of CFX. And then beyond that, there were tension techniques.
00:57:57.906 - 00:58:16.494, Speaker A: And actually, there's even a paper now about spoofing the Google experiment, which, you know, which relied on a very clever observation about.
00:58:32.994 - 00:58:56.402, Speaker B: So in, in your worst case to average case reduction, you put a random unitary in between each layer of the circuit. And I was a little confused, because if we already believed that the circuit of the entire circuit is in essence a random unitary. Why didn't we just.
00:58:56.498 - 00:59:01.214, Speaker A: No, no, we didn't. Why did we?
00:59:03.504 - 00:59:06.804, Speaker B: So the model seems to be that a circuit is a.
00:59:07.104 - 00:59:46.678, Speaker A: No, no, so, sorry. Okay, maybe I didn't make this clear. So what we are starting with is, you know, we are starting from the point where I said, look, if you wanted to compute the output probability for quantum circuit, in the worst case, it's hard for gap b. And even approximating it is hard, whatever, right? So, okay, so we start from that point. Now that gives us a quantum circuit. That's where the hardness sits. Now we want to do a worst case to average case reduction.
00:59:46.678 - 01:00:21.084, Speaker A: So we start with that circuit, and now we insert random gates in there. So now we have randomized the circuit, and now this random distribution we want to claim is the hard distribution. So this is sort of the simplifying assumption, but we are going from a worst case circuit to a random circuit. And then we are saying, well, look, in the experiment, we are just picking a random circuit. So what do we care? It's this one or that one, all the same.
01:00:21.154 - 01:00:27.644, Speaker B: But. So those h sub I's were like random two qubit gates. Okay, good. That's, that's what I didn't understand.
01:00:33.784 - 01:01:00.652, Speaker E: This may be a bit of a stupid northern question, related a bit to the answer to the Sam's question. So something that bothers me a bit here. The. So if we're convinced that there's no. That you cannot. So we said, if we can do, if we can solve these problems in polynomial time, then we're convinced we have quantum supremacy. But the thing is that what we're doing is we're solving them for a problem of size 52, and we're solving only.
01:01:00.652 - 01:01:05.036, Speaker E: Or Google is solving them for problem size 52 and also only, approximately.
01:01:05.060 - 01:01:05.196, Speaker A: Right.
01:01:05.220 - 01:01:09.664, Speaker E: So there's lots of other parameters here. You can get the edge that you get.
01:01:10.804 - 01:01:12.556, Speaker A: I'm sorry, I didn't get the last.
01:01:12.620 - 01:01:29.928, Speaker E: There's lots of other parameters here other than the size, like the edge that you get because we don't. The edge because you don't get you. We don't solve it. Exactly. We only solve it approximately. We get some edge over random sampling, right? So we only get it with a small edge. And it seems, you know, 52 is not such a, you know, two to the 52 doesn't.
01:01:29.928 - 01:01:54.328, Speaker E: Is that such a large number? But then when you get. Since we only get an edge and it seems maybe I can do it in two to the 30, which is something I don't know, I can do on my watch or something. Is there a way to quantify, like, what we should be, you know, based on maybe a stronger assumption about how, you know, what, what kind of edge and for what size problem would convince us that really this is not something we can do with class computation?
01:01:54.456 - 01:02:39.094, Speaker A: Right? So I think that was the point of these, you know, this, you know, what I referred to as sort of the analog of the exponential time hypothesis, right? This thing saying that, well, if you have a quantum circuit, you now you're trying to, trying to estimate the probability with which it outputs a certain string. And this assumption that Ernst and Gunn make, it's a hypothesis that they're making, but they're saying there's no algorithm that will give you an edge of one over three to the. Nice.
01:02:39.284 - 01:02:43.250, Speaker E: Okay, so that it is not a prime, it's not a pulley edge, it's an actual.
01:02:43.362 - 01:02:54.410, Speaker A: Yeah, it's an actual thing saying, you, you, you know, you, you would imagine that, okay, we're gonna compute this probability. But no, you know, and it's close to one over two to the n. But, but you can't even get.
01:02:54.482 - 01:02:55.810, Speaker E: But we're not getting that edge.
01:02:55.922 - 01:04:01.976, Speaker A: Yeah, and, but, but, you know, but the thing that the. And so, you know, that seems like, you know, so far, we don't know of an algorithm that achieves that even that small, small amount. It's, you know, it's a bold conjecture, but it's. And then the question is, is it bold enough, right? Should it be one over three to the n, or should it be one over three to the m? Or one, one over something to the m? So, if you use Feynman path integrals, you can, you can get in edge of something which, which is exponential in m. But now if you dial it back just a little bit and say, okay, you know, I know you can do it using these brute, this brute force technique, you can get an edge of this much, but try to get me even, anything even slightly larger as a function of n. Now, one could, you know, one could go out on the limb completely and say, well, you can't even do that. And then, well, if you, if you make that assumption, then, yes, you.
01:04:01.976 - 01:04:21.404, Speaker A: You would say, okay, yes, we should. Maybe this was, you know, one would have to check, but maybe one would say, okay, well, maybe just this small signal was good enough, but then you have to go back and say, how confident are we that there's no technique to get us down?
01:04:22.424 - 01:04:27.924, Speaker B: All these things seem to require two to the n, an exponential number of samples.
01:04:31.184 - 01:04:38.924, Speaker A: Sorry. So are you saying in order to do this, you have to choose exponentially many samples?
01:04:39.464 - 01:04:50.724, Speaker B: Well, I guess I was confused about the x class thing, because the probability of getting a particular output, on average, is one over two to the n. And so most of the time, you could say zero.
01:04:51.384 - 01:05:32.074, Speaker A: No, no, no. But in Xcoth, you're not saying you're just giving an estimate. You want to say, so you want to give a number, p, right? So you're looking at the circuit and you're analyzing it, and then your challenge is to output a number. And, for example, one number you could output is two to the minus seven without even looking at the circuit. And you do, you'd get, you'd get, you know, you'd get whatever score you do on this. And now your challenge here is to beat that trivial answer by just this much.
01:05:34.014 - 01:05:57.678, Speaker B: I guess where I was confused is that one way to attempt to solve these problems is if it turns out that there is a classical box which produces outputs with the same probability distribution as the quantum box, then one type of algorithm for these problems is to just sample from that many times. But that's not actually the regime we're thinking about here, right?
01:05:57.726 - 01:06:18.626, Speaker A: I mean, no, but, no, that's, that's a. I think it's okay to think that way. But then at that point, what you would say is, well, if there's such a classical box, then you arm yourself with the power of NP, and then you, then you arm yourself with stockmaier, and you say, okay, well, now I can. Now I can go to the races with this.
01:06:18.750 - 01:06:23.934, Speaker B: So then we strengthen x quas to say there's nothing in the polynomial hierarchy that can do that.
01:06:26.914 - 01:06:57.674, Speaker A: Excellent. You know, so, so, so eventually, I. Yeah, I. Maybe. Maybe that's a, that's, that's the way to go with it. But, but I think this is, this is just sort of saying, look, it's just, it's just taking aim at it directly and saying, look, you know, did you have a classical algorithm or not? Well, if you did, it looked like this and do this estimate, then you couldn't have created just a direct implication.
