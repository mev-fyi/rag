00:00:03.600 - 00:00:58.714, Speaker A: Hi, I'm going to be talking about block rigidity, multiplayer parallel repetition and lower bound for Turing machines. This is joint work with Ron Ross at Princeton University. Our main result is the following. We show that a strong multiplayer parallel repetition theorem implies the existence of explicit block rigid functions, which further implies super linear lower bounds for Turing machines with advice first, I'm going to be talking about lower bounds for Turing machines. Turing machines are the standard model of computation used to define complexity classes. Lower bounds for Turing machines are known by the time hierarchy theorem. Even more lower bounds are known from the seminal work of Paul Pipenger, Samaradi and Trotter who showed a separation of non deterministic and deterministic linear time for multi tape Turing machines.
00:00:58.714 - 00:02:06.634, Speaker A: No such theorems are known in the non uniform setting. The model we are interested in is deterministic multi tape Turing machines that compute multi output functions. What I mean by that is that we assume that the tape that the Turing machine has separate input and output tapes and at the end of computation the machine writes the output on the output tape and halts. The machines we are interested in will also be non uniform. That means that the machines are allowed to use an advise string which is a function of the input length and is written on a separate advice tape at the beginning of computation. Our main theorem regarding lower points for tiering machines is the following let t from the natural numbers to the natural numbers be a function such that t is omega of n. Assuming what we call the main conjecture, there exists a function f such that on inputs of length n bits, the output of f is of length at most n bits.
00:02:06.634 - 00:03:22.824, Speaker A: The function f is computable by multitape deterministic Turing machines in time and the function f is not computable by any multitape deterministic Turing machine that takes time off n, that is, linear time, and these machines can even take advice. So the main technical ideas we use to prove this theorem are the following. The first is the notion of block restricting Turing machines which was defined by Hopcroft, Paul and Valiant, and the second is a graph theoretic result by Paul, Pipenger, Semeradi and Trotter, which was used to show separation between deterministic and non deterministic linear time. These ideas are very interesting in their own right, but I will not go into them in this talk. In the rest of this talk I will talk about parallel repetition, block rigidity and the main conjecture I mentioned earlier. Next I'll talk about multiplayer parallel repetition. What are multiplayer games? A multiplayer game can be defined as follows suppose we have k players p one up till pk and a verifier.
00:03:22.824 - 00:04:10.834, Speaker A: The game starts by the verifier sampling k questions x one up till xk from a known distribution mu. The distribution mu is known to all of p one up till pk as well. Then the verifier sends x one to p one, x two to p two, and xk to pk. The players based on the questions they receive, output answers and send them to the verifiers. For example, p one sends y one to the verifier, p two sends y two, and pk sends yk. The verifier then evaluates a joint predicate of these questions and answers. That is, x one up till xk and y one up till yk, and based on this predicate, announces whether the players win or lose.
00:04:10.834 - 00:05:08.594, Speaker A: We define the value of the game denoted by Val G as the maximum winning probability over all the strategies of the players. By a strategy of a player, we mean the function that maps the players questions to their output. A natural thing to consider for games is the repeated game, which we denote by G raised to the n. What happens in this repeated game is that the verifier samples n copies of the questions. These are denoted by x one I up till xki for each I in one to n, and these n copies are sampled independently of each other. The first player receives the first question for all the copies of the game, the second player receives the second question for all the copies of the game, and so on. Based on all of these questions, the first player gives answers to the first copy of the game.
00:05:08.594 - 00:06:06.324, Speaker A: Sorry, the first answers for each of the copy of the game. The second player gives answers to the second question for each of the copy of the game, and so on. And we say that the win condition for the players is that the players must win each of the end games, that is, for each I in one to n, the predicate must evaluate to one on x one I up till xki and y one I up till yki. The value of g to the n is defined as the value of this game, which is basically the maximum winning probability over all the strategies of the players. Note that one important point here is that all the n answers that are given by player one can depend on all the n questions received by player one. So the players have the option to correlate their answers among different copies of the games. Next, we'll talk about what is parallel repetition.
00:06:06.324 - 00:07:02.644, Speaker A: It basically refers to the following question. What can we say about the value of g raised to the n. One can easily see that the value of g raised to the n is at least the value of g raised to the n. This can be seen by the following observation. So suppose there is a strategy for the players for the game g. Now, when a player receives n copies of questions, the player independently uses that strategy on each of the copies, and it is easy to see that the value achieved by this strategy is val g raised to the n, since the players must win in all copies. But is it true that this inequality is an equality? What is known is that that is not necessarily true even in the case of two player games.
00:07:02.644 - 00:08:03.910, Speaker A: So then what is interesting is whether we can find bounds on the value of the repeated game in terms of the value of the initial game. For two player games, the following is known. If the value of g is less than one, then the value of the repeated game goes down exponentially. In Nice this was first proved by Ron Ross and was later simplified by Hollenstein. Tighter bounds are also known when the initial game has some small value. Two player parallel repetition has found many applications, for example in pcps and hardness of approximation, in the geometry of phones, in quantum information, in communication complexity, and many more and many more to come as well. Now we asked the question, the same question for multiplayer games.
00:08:03.910 - 00:08:45.246, Speaker A: That is the case when k is at least three. We know that very less is known in this case. The main result known in this area is one, proven by Verbitsky in 1996. So they showed that if the value of g is less than one, then the value of the repeated gain goes down to zero as n tends to infinity. The bounds for this turn out to be one over the inverse Ackerman function, which is a very slow decay. These bounds are basically the result of a black box application of the density hills duet theorem. In some special cases, for example, we know exponential bounds.
00:08:45.246 - 00:09:44.854, Speaker A: For example, Dinur, Harsha, Venkat and Yuan exponential decay in some games which they call expanding games and homegrown and Ras proved a polynomial decay for a game which is called Ghg game and which does not fit into the category of these expanding games. But not much more is known than these results. Further, we do not know any application of multiplayer games that is parallel repetition for multiplayer games. In this work, we give the first major potential implication of a parallel repetition theorem with more than two players. I now mention the game of interest. So we fix an integer k and the function f, which takes in k bits as input and outputs k bits as output. We also fix sets s one up till sk of the set one to k, each of which is of size k over hundred.
00:09:44.854 - 00:10:43.244, Speaker A: I would like to mention that this one over 100 is arbitrary and we can replace it by any small constant. The verifier first samples the k inputs to the functions which are k bits uniformly and independently. Player j is given the bits that are indexed by the set sj and is required to answer back a bit aj. The varying condition for the players is that aj is equal to the jth bit of f of x for all j. Basically, this game asks whether the function f can be computed in the following form. We want that each bit of f can be computed based on a small number of bits of x, and the value of the game will denote on how large a set of the inputs can this be done. So this is a recurring theme that we'll see later when we are discussing rigidity.
00:10:43.244 - 00:11:35.804, Speaker A: We state the following conjecture. Let f be such that the value of g is two to the minus omega of k. What we mean by this is that for all these fixings of the sets s one up till sk, the value is exponentially small in k. Then we conjecture that the value of the repeated game is two to the minus omega of k times n and this again we want to say for all fixings of the sets s one up till sk. I would like to mention that, or I would even leave this as an exercise, that it is easy to prove that the value of the repeated game goes on exponentially in n. But what is interesting is to prove an exponential decay in k times n. This conjecture will imply the main conjecture and hence will give us the lower bound for Turing machines that we said earlier.
00:11:35.804 - 00:12:56.724, Speaker A: I would like to mention a special property of this game. In this game, once we fix the random string of the verifier, there is a unique correct answer for each of the players. Like here, if we fix x one up till x k, we definitely require that the jth player answers the jth bit of f of x and we call such games independent games. While these games do not fit into the category of expanding games and other special cases for which we know exponential decay, we hope that this very strong property of independent games will help to prove strong parallel repetition. Next I would like to I will talk about block rigidity. What are rigid matrices? A matrix a, which is an n cross n matrix over gf two is said to be an r comma s rigid matrix if it cannot be written as the sum b plus c where b is of rank at most r, and the matrix C is s rho sparse, which means that C has at most s non zero entries in each row. What this means is that a matrix is rigid if it is far in having distance from a low rank matrix.
00:12:56.724 - 00:14:00.284, Speaker A: Matrix rigidity has found many applications. For example, Valiant, who introduced matrix rigidity, proved that if a is a rigid matrix with some parameters, then the function determined by a cannot be computed by linear size logarithmic depth arithmetic circuits. Razburov has also proved lower bounds in communication complexity based on rigid matrices. Valeant in a seminal paper also showed that random matrices are very highly rigid with high probability. The challenge, as for many other things in complexity theory, remains to find explicit rigid matrices. That is given, and that is we want to give an efficient deterministic Turing machine that on input one to the n outputs a rigid matrix of size n times n. There are many known constructions for the above, but all of those are quite far from what is needed to get real lower bounds.
00:14:00.284 - 00:14:52.440, Speaker A: We state the following observation. This is very easy to prove. It says that if a is an n crossin matrix over g of two, then the following conditions are equivalent. The first is that a is not an r comma s rigid matrix. This means that a can be written as the sum of b and c, where b is of low rank and c has low sparsity and the second condition says that there exists a subset of f two to the n of size, at least two to the n minus r, and an s row sparse matrix c such that over this subset capital x, the output of a and the output of c are the same. The proof is basically the following. In the forward direction we let x be the null space of b, which is of size at least two to the n minus r.
00:14:52.440 - 00:15:57.304, Speaker A: Since the rank of b is r at most r and in the reverse direction, we note that the rank of c minus a must be at most r. Based on the above observation, we extend the definition of rigidity to general functions. Now we say that a function f, which takes in as input n bits and outputs n bit, is said to be an rigid function. If the following holds for every subset x of zero one to the nice of size at least two to the n minus r. It is not possible to find functions g one to gn and sets s one to sn such that the output of the function f over this set capital x can be described by these g one up till gn and s one up till sn. So the parameters here say that the function is g one up till gn taken as input s bits and output one bit. And each of these si's is a size s.
00:15:57.304 - 00:17:00.430, Speaker A: So what we are trying to do here is to express the output of f, that is each output bit of f as a function of a few input bits of f that is only s of those. And we want to say that a function is rigid if this is not possible. Yeah, just to recall, basically not possible that for many inputs x, each bit of f of x can be written as a function of a small number of bits of x. One can easily show that random functions are rigid with high probability. And following the proof by valiant, one can even show that if f is a rigid function, then f cannot be computed by linear size logarithmic depth. Boolean circuits by boolean circuits we consider circuits which taken as input two bits and are allowed to output an arbitrary function of these two bits. Now I would like to state a conjecture.
00:17:00.430 - 00:18:12.354, Speaker A: This conjecture is not related to our main conjecture, but it is still interesting in its own right. We conjecture that if a matrix a is an rs rigid matrix, then the corresponding function from f two to the n to f two to the n is also a rigid function. We note that this conjecture is closely related to a conjecture of Eucnet Schnetzer on the linearization of depth two circuits, which is also related to the question of whether data structures for linear problems can be optimally linearized. Now I would like to talk about block rigidity, which is a relaxation of the notion of rigidity. First I'll talk about block rigid matrices. A matrix a which is of size nk times nk, is said to be an rs block rigid matrix if it cannot be written as the sum of two matrices b and c, where the matrix b is of rank at most r and the matrix c has at most s non zero entries in its block row. So the way we will look at a is that a is composed of n cross n blocks and there are k square such blocks.
00:18:12.354 - 00:19:10.044, Speaker A: And what we require is that the matrix c has at most s non zero entries in each block row, and each of these entries mean a block. So basically the matrix c has only s non zero matrices in each block row. We know that the case n is equal to one is basically is the same as that of rigid mate of phase. And similarly, we'll extend the definition of rigid functions to block rigid functions. Here what we want is to express the each output block of the function which is of size n as a function of a few input blocks of as a function of a few blocks of the input. So these are what is denoted by these s one up till sk and g one up till gk. So I won't go over the formal definition because it is a little hard to pass, but it's easy to understand it in this sense.
00:19:10.044 - 00:20:08.714, Speaker A: So we proved the following theorem. Let k be any function that is super constant, and we'll mostly be interested in the case when k is much smaller than n. So suppose fn is a family of omega of nk omega of k block rigid functions. Then this family is not computable by any multi tape deterministic Turing machine that takes advice. I would again like to mention that block rigidity is a weaker condition than rigidity. Hence this theorem gives an extra application of rigidity as well, since rigid functions would also satisfy this property of not being computable by multi tape deterministic tiering machines that take advice. Assuming the earlier conjecture which said that rigid matrices are rigid when seen as functions as well, this also gives new applications of rigid matrices.
00:20:08.714 - 00:21:12.334, Speaker A: Finally, I would like to state our main conjecture, which is the following. We stay we conjecture that if f is an omega k comma omega k rigid function, which is a very strongly rigid function, then the function f tensor with itself n times is an omega nk comma omega k block rigid function. I would like to state what this tensor function is. Basically it takes as input nk bits of input, where this input can be arranged in a matrix of size k cross n, and then f is applied to each column of the matrix. And now this new k cross n matrix is thought of as the output of the function f tends out with itself n times. Now I would state our main result again. We show that strong multiplayer parallel repetition implies the existence of explicit block rigid functions, which further implies superlinear lower bounds for Turing machines with advice as for other directions.
00:21:12.334 - 00:21:57.764, Speaker A: What is interesting is whether we can prove strong block rigidity for other functions. This would imply superlinear lower band for those functions. What our regime of parallel repetition proves is block rigidity for this tensor function, but it is very well possible that it is easier to prove block rigidity for other functions. For example, some candidates are the following. The first is matrix transpose. Given an n cross n matrix a in terms of its rows, we want to output it output its transpose and the second is matrix product. Given two n cross n matrices a and b, we want to output the product matrix a times b.
00:21:57.764 - 00:22:45.514, Speaker A: The showing block rigidity for these functions translates to finding the value of very simple games. I would like to explain the game for matrix transpose and I think it is a very nice problem. Let n be a natural number and s one up till sn. We subset from one to n each of size n over hundred. Again, this n over hundred can be thought of as any small constant times n. Consider the following n player game the verifier first samples are uniformly at random n cross n matrix, each of whose entries is zero or one. Player j is given the rows of x indexed by the subset sj and is required to answer the jth column of the matrix.
00:22:45.514 - 00:23:38.434, Speaker A: We conjecture that the value of the above game is two to the minus omega of n squared. Again, I would like to mention that proving two to the minus omega of n in this case is not hard, but we conjecture that the bound can be improved to two to the minus omega of n square. This conjecture would imply that the matrix transfers function cannot be solved by multi tape deterministic linear time Turing machines that take advice. I would also like to mention that the above problem is very interesting from a combinatorial perspective as well. What it says is that basically does there exist a large family of incers and matrices, each of whose columns can be written as a function of a small number of rows? I would like to end my talk here. Thank you all.
