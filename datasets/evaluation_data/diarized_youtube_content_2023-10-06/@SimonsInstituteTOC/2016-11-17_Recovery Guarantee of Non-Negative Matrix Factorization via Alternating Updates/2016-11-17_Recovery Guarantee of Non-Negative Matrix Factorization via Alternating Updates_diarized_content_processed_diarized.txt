00:00:00.200 - 00:00:07.394, Speaker A: Welcome to the Thursday morning session, and it's great to have Ying Yuiang talking about non negative matrix factors.
00:00:08.014 - 00:01:06.612, Speaker B: Okay, so this is joy work with Yanzhi Li and Andrei Riscski, both students from Princeton University. So let's begin to consider recent advances in machine learning applications. So, we have seen significant advances along these machine learning applications, such as dictionary learning, majors completion and especially deep learning. And a lot of new things happening along with these advances, we see a lot of non convex problems. For example, all those applications listed here, I mean, in their general form, just non convex problem, although there is like convex relaxation. But usually the problem is formulated in a non convex way. So these problems in their general form are just hard to solve.
00:01:06.612 - 00:02:12.574, Speaker B: But it seems that the practical instances are not so bad. So basically, people are getting used to formulating problem in a non convex way and just solve them by some heuristic and get good results, so they are not scared by this non convexity. And especially often, we use some simple method to get some local improvement. By that, I mean this simple method often has multiple iterations, and at each iteration, we just look at the information at the current solution and try to get some local improvement. So the most commonly used one is so called gradient descent, and its variants such as stochastic gradient descent. So, suppose we have some objective function and we want to minimize it. Then we can just take the gradient at the current solution and move a little bit along the negative direction of the gradient.
00:02:12.574 - 00:03:00.614, Speaker B: So this is a very basic algorithm, but works really well. So it's the basis for the, for example, the deep learning. And another approach is this so called alternating update method. So usually the problem has two blocks of variables that you need to determine. So you fix one of them, you update the other, and then you fix the other and update the first one and repeat this procedure. The advantage is that many problems, if you fix one of the variables, updating the other is quite easy. So this is very popular in practice for such kind of problem if they have the structure I just mentioned.
00:03:00.614 - 00:04:03.028, Speaker B: So this is a concrete example, factorization. We are given a matrix y, and we want to find two matrices, usually load rank, and then we want y equal to the product of a times x. So then after doing some initialization, we can just say fix a and update x. Basically like find the best x that fix the that fit the current a, and then we fix x and then find the best a that we can get. So it's kind of like some best response dynamics. So it turns out that this tool, very simple method, or heuristic works really well for y families or problem including like those I listed on the first line. But there's no, generally there's no theoretical guarantee.
00:04:03.028 - 00:04:26.686, Speaker B: So the interesting questions are when and why do such algorithms actually work for the hard problems? So the goal here is first make some natural and clean assumption about the practical instances. And then we show that these simple methods can actually recover what you want.
00:04:26.790 - 00:04:41.394, Speaker C: Excuse me. Right, I have a question. Someone say it walks. Oh yeah, it's fast because you stop it after like ten minutes or gives a good value solution because you compare it to something or.
00:04:41.854 - 00:04:56.308, Speaker B: Yeah, that's a good question. So basically, if you have an objective function, by works, I mean you can minimize, you can get a solution that is close to the global optimal, either in object function or in the solution.
00:04:56.476 - 00:05:01.704, Speaker C: How do you know the global optimal? Because if you know the global, probably you have a different algorithm that works better.
00:05:02.804 - 00:05:45.834, Speaker B: Yeah, that's a good question. So basically, for some of the problems, you may be able to show that all the local optimals are just global optimal, you mean? Yeah, proving. So for example, major completion under some assumption like incoherence and random Teng Yu is not here. Yeah. So basically for some problems you can show that. Another way to show that it works is that you assume some so called generative model and in which you actually have a ground truth. So you want that ground truth, you don't care about the object function, you care about the ground truth.
00:05:45.834 - 00:06:00.434, Speaker B: So what you want to show is then this algorithm, given the data from that distribution, after a while you actually get close to the underlying ground truth. So, yeah, so, yeah.
00:06:03.694 - 00:06:10.038, Speaker C: This is by simulation, I mean on simulated data, it's not on real data. The general model.
00:06:10.086 - 00:07:09.852, Speaker B: Yeah, yeah. On generic model, if you want to empirically verify, usually you simulate, because you have the generated model, you can just generate data. But our hope is that the generating model is natural in the sense that it maybe is close to the actual data distribution that we observe in practice. So that is the hope. Okay, so this work is about analyzing alternating update for non negligible matrix factorization. So then what is non negative matrix factorization? It's just a factorization problem under the additional requirement that the entries of the factorized matrices should be non negative. So in some cases you only require the entries of x is non negative.
00:07:09.852 - 00:07:58.208, Speaker B: But for simplicity, let's just say we required both are non negative. So this leads to a non convex problem. It's a important theoretical concept, basically relates to non negative rank and like log rank injection communication complexity. But here we care more about its application in machine learning. It has many applications here. The typical one, I mean, is the topic modeling, which is a mixture model for analyzing data such as tags, images or even networks, and many other types of algorithm. So here in this setting, the Y majors.
00:07:58.208 - 00:08:44.978, Speaker B: So let's say each column of Y majors corresponds to a document and each row corresponds to some word. So each column is basically the word distribution. In this particular document, for example, this document has like 0.04 fraction being the word rank. It's just a synthetic example. And this matrix a is called the topic mages. Each column is called a topic, basically just a distribution over the words, but it's meaningful in the sense that it corresponds to some concept.
00:08:44.978 - 00:09:41.224, Speaker B: For example, like this topic weather has a large probability on this word rain, and has more probability over some other not so related words like soccer, physics and things like that. So this is the top images, and this x images is each column of it corresponds to a document. And it's just mixing coefficients of the topics in this particular document. Let's say this document has 0.2 fraction of the topic weather and has zero point a fraction of the topic signs. So y equals a times x. Basically means that by mixing the distribution of the topics, you get the distribution of the words in this document.
00:09:41.224 - 00:10:29.434, Speaker B: So you can imagine that by analyzing, by discovering the topics among a large set of documents, you can get an overview of the themes of these documents. So it's very useful and popular in social science and also like information retrieval and many areas. Ok, another example comes from computer vision. So you apply non negative matrix factorization on images. Say this example is coming from a nature paper. So you are given some images about human faces. You want to find these so called pattern images.
00:10:29.434 - 00:11:17.384, Speaker B: So each small square is actually an image of the same size as this original face image. So we shrink it because we need to have space. So then you want to find a bunch of pattern images and then by a non negative linear mixture of these pattern images, you can recover the face image. So by piecing them up you can get this face image. So now you don't know this pattern image, you don't know the weights. You are given this a bunch of images. So each image is just a column in the, in our images y.
00:11:17.384 - 00:12:14.624, Speaker B: Then you run this non negative images factorization using alternating update rules. Then you, you can discover some interesting patterns. Some of the images, let's say this, this might correspond to the mouth of on human faces and some others might correspond to eyebrows and things like that. So you can get a part based representation of human faces. So this is another. So it also has many other applications like network analyzing, networks and like information retrieval and things like that. But as similar to some other machine learning problems, usually we have a large gap between the worst case bound and what happens in practice.
00:12:14.624 - 00:13:09.984, Speaker B: So Aurora Guang Kannan moisturize show that there is no sub exponential algorithm assuming eth. And they also give an algorithm running exponentially in k. K is the number of columns in the matrix a. So you can see that this basically is impractical for any practical use. But what happens in practice is just this heuristic suggested by the computer vision guys Lee and song that you can just do alternating update after setting an initialization, so usually set by some human experts on this data set. And then you can just repeat the two steps. You compute X from a, you compute a based on X.
00:13:09.984 - 00:14:12.774, Speaker B: So I'll be more specific about this algorithm later on. But this is just a simple algorithm, you can just run and get very good results. So this large gap just requires some analysis for practical instance rather than worst case instance. So how do you model practical instance for machine learning applications? So the most common way to do so is so called generative model where you assume there's a ground truth solution. So here, let's say we have a ground truth, a star matrix, and then you assume that the data is generated from a distribution defined based on this ground truth. So it has some information about the ground truth, and you want to discover the ground truth based on the data. So another approach to do a beyond worst case analysis for machine learning problems is so called warm star.
00:14:12.774 - 00:15:11.794, Speaker B: Basically it's assuming the initialization is not far away from this ground truth. For example here it's not far away from the ground truth. This is motivated by the fact that the initialization is often set by some human experts. So it's reasonable to assume that is better than just random guessing. So you can assume that it's not too far away. Okay, so what do we know about NMF? This paper proposed the so called separability assumption motivated by topic modeling. The assumption says that each topic, basically each column of the matrix a has a so called anchor word, or what is an anchor word is just a word that appear in this particular topic, but do not appear in any other topics.
00:15:11.794 - 00:16:25.814, Speaker B: So under such kind of assumption, there is this efficient algorithm for this problem and you can just run it. And actually there's a, a lot of subsequent work along this line and there are very practical algorithms. Another related work is by Avasti and Risteski, they analyze this variation inference method. Basically it's an alternating update method for topic modeling. They, alternating over a, a and x, try to minimize the KL divergence between the columns of y and the columns of a times x. So in this work, basically it requires quite strong assumption, an a and also a warm star depending on the dynamic range of the entries in the matrix a, so which is not so realistic. So and also the assumptions may not hold for some other time of data beyond tanks, for example, like computer vision, the assumption is a little bit off.
00:16:25.814 - 00:17:34.443, Speaker B: So the interesting question is, can we show provable guarantee of the simple method under more general assumption? So now let's turn to our work. I would describe the model algorithm and then provide the results and an overview of the analysis. So this is our generative model. We just assume that each column of y, basically a data point is iid example from this distribution, a star times x star. Here, a star is deterministic ground truth matrix and x star is a random variable, a vector. So then we can, for simplicity, let's assume that we have infinite data and then the only assumption about the matrix a star is linear independence. So this is very my condition first of all is a generalization of the angle work condition.
00:17:34.443 - 00:18:20.848, Speaker B: And also um, we kind of expect the columns of a to be convex independent. Otherwise, um, is even non identifiable in the sense that you may have like infinite number of a that satisfy the data. So this is so, although there's a difference between linear independence and convex independence, but we don't know how to make use of that. So we just assume linear independence. Now we also assume some distribution on the random vector x star. We assume that the coordinates are independent. So this is the key assumption.
00:18:20.848 - 00:18:50.864, Speaker B: And we also assume that it's just zero. One is one with certain probability as zero, otherwise. So in expectation this is a vector with s non zero entries being one and all the others being zero. So just a very simple model. So our analysis generalize to some other more general assumptions, but this is sufficient for demonstrating our intuition.
00:18:54.084 - 00:19:00.118, Speaker D: Doesn't this look a lot like the sparse coding setting as well? Because X is a sparse vector and.
00:19:00.166 - 00:19:22.942, Speaker B: A is our results generalize to non sparse vectors. So sparsity is not a key assumption here, but independence is the key assumption. So it's also called non negative ICA independent component analysis in the empirical papers. But we don't find any theoretical analysis for that. So we still, yeah, so s, I.
00:19:22.958 - 00:19:26.966, Speaker D: Should think of as being somewhat close to k or k or something, right?
00:19:26.990 - 00:19:32.434, Speaker B: You can think of it as that it can be sparse and it can be non sparse, but it doesn't matter.
00:19:33.574 - 00:19:41.302, Speaker A: So if s was one, this would be easy. K is the number of, k is.
00:19:41.318 - 00:19:44.198, Speaker B: The size, the number of columns in a.
00:19:44.326 - 00:19:47.230, Speaker A: So if s was one, then you would be seeing some columns of a.
00:19:47.262 - 00:20:22.374, Speaker B: In your y, right? Yeah, that would be the easy case. Okay, let's continue. So this is our algorithm. We have some parameters that I will explain later, but basically it's just the alternative update. In the first step, we compute x. We call it decoding, just from like computer vision literature. Basically means that for each example y, you try to get the corresponding x.
00:20:22.374 - 00:21:02.842, Speaker B: How do you get it? You just multiply by the pseudo inverse of the current solution. That's why we need linear independence. And then we pass it through a nonlinear function, looks like this, and then get x. So what is the nonlinear function? It applies to each coordinate of the input vector. And for each and each coordinate, if it's smaller than alpha, then sets it to zero. Otherwise just keep the original value. So this is called ratify linear unit relu in deep learning.
00:21:02.842 - 00:21:12.594, Speaker B: And it turns out it's crucial for proving the convergence resolve in our setting. So there might be some connection to that, but we don't know.
00:21:13.054 - 00:21:20.846, Speaker D: Sorry, I'm not quite following. So how is the loop dependent on t and what is a, could you explain one more time?
00:21:20.910 - 00:22:03.240, Speaker B: Oh, okay, so here I just omit the subsquared t. So let's say, because let's, for simplicity, we assume that you have infinite theta. So each time you just sample a lot, a lot of y, and then for each y you use the current solution. Let's say this is at, you use the pseudo inverse of the current solution at. And then you get the decoding. And then I will use these decodings in updating at to at plus one. Let's say, I'm not sure you remember the notation.
00:22:03.312 - 00:22:05.288, Speaker C: He's factoring y into a times x.
00:22:05.336 - 00:22:11.592, Speaker D: Right, right, but I wasn't sure what was happening. I guess the algorithm's not completely there yet.
00:22:11.768 - 00:22:17.004, Speaker C: Oh no, he hasn't told you that? No, the dagger is also a t, the e dagger.
00:22:20.464 - 00:23:02.174, Speaker B: Okay. Yeah, so there should be a, yeah, there should be a superscript t or subscript t inside this dagger. Yeah. So, okay, after getting the decoding, we just do the update. It's also quite simple. We just compute some direction and then move along that direction. So what, the direction is just the expectation over y minus y prime times x minus x prime transpose so this is a vector, this is a row vector.
00:23:02.174 - 00:23:27.094, Speaker B: So you get a matrix out of this. So here y and y prime are two independent examples and x and x prime are their corresponding decodings. So we need to use some polynomial number of example. But here I just take expectation for simplicity.
00:23:27.874 - 00:23:31.294, Speaker D: One more question. That y has nothing to do with the y of the previous line.
00:23:33.674 - 00:23:55.594, Speaker B: You mean this y? So you can think of it as, let's say, m copies n m examples y, one up to ym. Right. So we just use the first half as this, the second half as this. Let's see.
00:23:56.694 - 00:23:59.910, Speaker C: Wait, that's confusing. Those are just two independent examples.
00:24:00.022 - 00:24:00.534, Speaker B: Right.
00:24:00.654 - 00:24:01.714, Speaker C: What's the half?
00:24:04.254 - 00:24:22.914, Speaker B: Okay, so we, let's say we draw two sets of examples. Yeah. So the first set of example, we decode it in this way. The second set we also decode in this way. And then we use the first set as y, the second set as y prime. And then we did it. Yeah.
00:24:23.034 - 00:24:24.974, Speaker C: Do you need to use fresh samples.
00:24:25.354 - 00:24:41.492, Speaker B: For each for analysis? Yeah, because it simplified things a lot. But it's just logarithmic iteration, so it's okay. With some extra words, you, you may be able to show that you don't need independent sample, but they just complicate things.
00:24:41.628 - 00:24:49.132, Speaker C: Yeah. I mean, in the matrix completion setting, which is kind of similar, it's really, it was difficult to get rid of this requirement.
00:24:49.188 - 00:25:28.010, Speaker B: Oh. So here is slightly different. It's not so difficult to get rid of the fresh sample. Okay, so this complete the specification of our algorithm. Now let's do some sanity check. Suppose we have the ground truth, what would happen? Let's first check the decoding. So if you plug in the pseudo inverse just cancels out the ground truth, and then you get x star because it is non negative.
00:25:28.010 - 00:25:47.674, Speaker B: So Relu doesn't do anything. So you, you get a ground truth, x star, it's also zero. Yeah, yeah, yeah. Alpha, so you can think about Alpha has a small constant between zero and one. Yeah. So I didn't specify. Yeah, sorry.
00:25:47.674 - 00:26:09.402, Speaker B: Yeah. Is that okay? Yeah. Okay. Okay. Now let's check the update. So in this case, if we plug this in and also plug the two decoding, then we get that it's proportioned to a star. So you are moving along the right direction.
00:26:09.402 - 00:26:49.148, Speaker B: But there's some scaling here. So we need to have different step sizes to take into account the scaling issue. But anyway, the update direction is just the ground truth, just pointing to the ground truth. So this seems reasonable. Okay. But it turns out that because we only assume linear independence of the columns of a. So if the matrix a is far away from the ground truth, basically we cannot say anything about the algorithm, so weird things can happen.
00:26:49.148 - 00:28:16.512, Speaker B: We have some examples showing that you can get very weird things. So for simplicity, sorry, we just assume that the initialization is close to the ground truth up to a constant arrow. What do I mean by arrow here? Basically it means that the columns of a is a linear combination of the columns in a star, and in particular is a times a star times a large diagonal matrix plus a small of diagonal matrix. So to pass what this means, let's look at a column of a. Then if you expand this out, basically means it's a combination of the corresponding ground truth column v, one a and some other, some components from other columns in a star. So the assumption basically means that you have a large fraction along the one that you want, and all the others are just a constant, just taste a small constant fraction. So we can relax this a little bit, but we cannot make it larger than one two.
00:28:16.512 - 00:29:09.744, Speaker B: So beyond a half, it's not even like identifiable and weird things happen. So now under these three assumptions, we can have this clean theorem saying that after log of one over epsilon iterations, you basically get the arrow to be epsilon. So here we still maintain this to be one minus l. But if you know that the columns of the ground truth has not has column norm one, so you get, then you can just do normalization, and then after normalization you can get this to be one minus all the epsilon. So basically it just means that you can get epsilon error. After logarithm iterations.
00:29:22.724 - 00:29:31.944, Speaker A: You get a y. So you look at the, at the columns of your current. Yes. Of a that have a component and you keep those and then you decode x.
00:29:31.984 - 00:29:47.164, Speaker B: No, in this particular simple generative model, yes. But it turns out that the same intuition can be generalized to the case that you might not even be able to recover the support of X star.
00:29:48.984 - 00:29:50.764, Speaker C: This is for the non sparks case.
00:29:51.284 - 00:30:09.844, Speaker B: Yeah. For the sparse case with zero and one, it's easy to recover the support and things like that. But the intuition here that I'm going to describe applies to more general setting that you may not be able to recover the support. Even.
00:30:09.964 - 00:30:12.996, Speaker C: So, I'm asking is that the non sparse case or which case?
00:30:13.140 - 00:30:14.436, Speaker B: Yeah, non sparse case.
00:30:14.580 - 00:30:15.772, Speaker C: Yeah, yeah.
00:30:15.908 - 00:31:28.168, Speaker B: Right. Okay, so how do we analyze? So it turns out that this expression is the beginning of our analysis. So we just try to show the method can maintain large sigma and at the same time just make e the arrow matrix e decrease so we need to have some measure, some measurement to measure the size of this arrow matrix. So we define this potential function, which is basically take the norm of the positive part of e, take the norm of the negative part and do a weighted combination. So it seems very weird potential function. But anyway, if we can show these two things, we can prove the theorem right. So now let's focus on this arrow matrix and this potential function to see why we want this weird thing here.
00:31:28.168 - 00:32:20.330, Speaker B: Why not just use the norm of the arrow matrix as a whole? It turns out that it's basically because the relu non unit function, nonlinear function change the positive part and the negative part in different ways. It can remove the negative noise, but it cannot remove the positive noise. So you need to treat the two parts in two different ways. Okay, to be more specific now, let's first look at the decoding. Let's simplify things by assuming that the sigma matrix is just the identity, because we always keep it to close to one. So let's just say it's identity under this assumption. Let's see how the arrow matrix is changed by the update rule.
00:32:20.330 - 00:32:58.944, Speaker B: Let's first look at the decoding. If we plug the expression of a into the decoding rule, then basically it's just this expression. You take the inverse of identity plus the arrow matrix and times x star. So if we do a Taylor expansion so it looks like this. So we also need to handle the higher order number. It's not so easy to handle. But the intuition is that it consists the grout truth that we want and also some arrow related to this arrow matrix.
00:32:58.944 - 00:34:02.736, Speaker B: Let's say some noise related to this arrow matrix. You can imagine that the nonlinear function the relu here is just to remove this noise, let's look at the entries of this x star minus e times x star. So it looks like this. The green bars stands for, say, the non zero entries in the ground truth x star and the yellow ones are just the noise introduced by this second term. So it basically looks like something like this. And it turns out that although the total amount of noise is large, they are spread out. So you can imagine if you set this alpha properly, you can remove a large fraction of the noise while maintaining a large fraction of this grout truth that you want.
00:34:02.736 - 00:34:14.514, Speaker B: So you get a much cleaner version after doing this relu function. So this is basically the yvne relu. Okay.
00:34:15.654 - 00:34:19.390, Speaker C: If alpha was zero, you would not remove the positive noise.
00:34:19.422 - 00:34:19.686, Speaker B: Yes.
00:34:19.750 - 00:34:22.714, Speaker C: So we, if alpha can't be zero.
00:34:23.014 - 00:34:24.766, Speaker B: Yeah, it can't be zero.
00:34:24.950 - 00:34:28.434, Speaker C: Positive noise. Adds up if that happens.
00:34:29.574 - 00:34:50.858, Speaker B: If you have positive. Yeah, it has positive noise. Right. So sometimes you have positive noise that can, for example, you have a positive noise into this. So basically relu cannot have a good effect on that. That's why later on we need a delicate potential function.
00:34:50.946 - 00:34:57.202, Speaker C: Is there some relation between alpha and s? So if you knew that x was going to be very sparse then maybe you want to.
00:34:57.298 - 00:35:25.374, Speaker B: Right. So in this simplifying. So in a more general setting alpha has, it depends on quite a few things but there is this one in this simplified setting. Yeah, I think it, I didn't did a calculation but yeah, it also depends on the sparsity but let's see. Yeah, I think it depends on.
00:35:28.034 - 00:35:30.814, Speaker C: So if you don't know the sparsity then you.
00:35:31.274 - 00:36:25.030, Speaker B: Right, so basically this is something that we need to think more about basically because our guarantee is something like this is a set of parameters, alpha and step size, eta one, eta two, such that you can get this thing. But yeah, okay. Yeah I'm getting close. So after doing the decoding you can plug in into this and do a bunch of calculations that is not so interesting. But at the end of the day the update on the arrow matrix looks like this. So this is the original arrow matrix, I mean in the previous iteration and after one update you just modify by moving along this update direction. And this update direction is the green and the brown.
00:36:25.030 - 00:37:21.114, Speaker B: Oh yeah. The green stands for the positive entries in arrow matrix and the yellow one stands for the negative. Of course I just made them into two parts but they can be like scattering around just for simplicity. And it turns out that the update direction. So this delta matrix here has a very specific form. You first take the original error matrix, each, transpose it and then you scale down the positive entries by small constant. Why? Basically it's because as I said, if you have a positive entry here, its effect on the decoding, let's say on the decoding is contributing a negative noise.
00:37:21.114 - 00:38:31.464, Speaker B: So you can imagine that it kind of like can be, most of the time it can be like threshold by this non dis function. So right, this is, but for the, for the other parts, the negative edges, you don't have this nice property. But it turns out that this more constant on this positive part are sufficient. So basically if you set error one and eta two properly, you can make sure that the sigma matrix stays close to one. And you can also make sure that this positive part will not be blow up too much by this negative part. And on the other hand the negative parts would be decreased because of this small constant. So then by a weighty combination, we can show that this actually decreased.
00:38:31.464 - 00:39:10.682, Speaker B: But if you just use the norm of the arrow mages as whole and do not use the weight combination, it can actually sometimes increase a little bit. But if you consider this weight combination, then you can show that it always decreases. So then we get our result. Okay. So this kind of analysis can also be applied to some other more general setting. For example, we have noise to each column of y. This is basically because you can handle this kind of noise in the same way as before.
00:39:10.682 - 00:39:41.914, Speaker B: You can imagine that you're just adding an additional noise term here, and then you can argue using the previous intuition. So it turns out that you can tolerate large adversary noise. And if the noise is zero mean, the noise can be a square root k larger than the l one norm of the, of the signal. But we. Yeah, so this is very strong denoising effect by the relu.
00:39:42.974 - 00:39:46.154, Speaker C: So it's adversarial. So it's not iod or.
00:39:46.454 - 00:40:31.724, Speaker B: Yeah, no, no assumption. I mean, in this setting is no assumption. Yeah. If you have zero mean, then you can, it's basically because this and the decouple and the coupling of the two parts, you can use this analysis to show that actually you don't need much assumption. But the key assumption is that the noise should be spread out across different entries. So I should say that it's not totally adversary, but it's adversary on each entry. So basically you have an upper bound on each entry of this noise vector, but it can be any distribution engine wise.
00:40:31.724 - 00:41:23.424, Speaker B: And. Yeah, so it can also be used in some other distribution of the X star. We only need to satisfy some moment condition about the first moment and second moment. So it's quite general, but we still need the coordinates to be independent of each other. Why? Because when we, from this step, this decoding, to showing this, basically you need the independence of the coordinates so that you get a lot of cancellation here. But. Yeah, that's the high level intuition.
00:41:23.424 - 00:41:55.284, Speaker B: So let's summarize. Basically, we have a good analysis of this problem under the assumption that is fair in mind, but we still need a warm star. And then we discovered a very strong denoising effect of the relu, so it can make use of the non negativity constraint in this particular problem. So it's. Yeah, okay. Yeah. That's all.
00:42:00.184 - 00:42:02.684, Speaker A: Quick question, while the next speaker is setting up.
00:42:07.184 - 00:42:13.494, Speaker C: Do you need to know about. The rest of you have to do binary search or can you.
00:42:14.954 - 00:42:31.474, Speaker B: Yeah, indeed you can do binary search. You can do binary search, right? But indeed our algorithm need to know the Alpha in advance so you can do some estimation. But yeah, we need. Thanks.
