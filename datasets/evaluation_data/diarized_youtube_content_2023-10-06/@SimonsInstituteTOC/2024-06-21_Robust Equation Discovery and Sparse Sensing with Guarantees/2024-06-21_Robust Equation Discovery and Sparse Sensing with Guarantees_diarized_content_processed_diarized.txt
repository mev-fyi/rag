00:00:14.520 - 00:00:58.164, Speaker A: Thank you, Aditi and Jen, for inviting me to speak here. I'll caveat that there's very little proteins in my talk. There is a little bit of engineering. So if you are looking for a healthy break from proteins, I hope you enjoy. I will be talking about something I guess Aditi and some others left off with in the panel discussion, which is a robust discovery of odes and PDE's from data. So that's what I mean by a robust equation discovery and sparse sensing so certain sparse resource allocation problems in engineering applications. Now you can see from my funding.
00:00:58.164 - 00:02:01.714, Speaker A: Acknowledgments Idaho national lab resources are very sparse in nuclear reactors, particularly sensors and actuators. That's something that I usually start off with, but given the context I might start off with, I'll start off with equation discovery. I also want to just for some context, I'm in a mechanical engineering department at University of Washington, and my background is in applied math. I'm also part of the NSF AI Institute in dynamic systems, which is led by Nathan Kutz and Steve Brunton at University of Washington. So some of you might be familiar with their work. I am the thrust lead for optimization and sensing, or AI for optimization and sensing, which supports the other fundamental research thrusts in AI for modeling dynamical systems and AI for control, optimal control of dynamical systems. Feel free to interrupt at any point with questions.
00:02:01.714 - 00:03:29.074, Speaker A: Okay, so AI in engineering settings, or at least the settings I work with, is trustworthy, predictive AI models. So this includes models that are required to be interpretable and generalizable to new parameter settings, to new environments by changing the model parameters so that alone places constraints on the types of models you can use. We require models with uncertainty quantification. So, for example, in the nuclear reactor setting, we want to be able to predict when a reactor is or a reactor component is in off normal conditions. And so UQ plays a big role in these types of applications. And of course, sensors and actuators in a lot of in nuclear and aerospace engineering, which are my two primary collaborative efforts, are extremely sparse and expensive. So in a nuclear component like this one, you might only be able to deploy two thermocouples or one fiber optic sensor to be able to infer the entire temperature or pressure field from point measurements, point noisy sensor measurements.
00:03:29.074 - 00:04:37.136, Speaker A: And in aerospace manufacturing, you might only have a few robotic actuators or control modules to manipulate in order to deform some large scale assembled structure to meet nominal standards. Regulatory requirements in both of these fields are extremely stringent, so there's tremendous need for theory and trustworthiness of AI models. This is quite a different setting than what you're used to. So most of the models I'll focus on are predictive. I want to also mention sparse data driven equation discovery, which is going to be the first half of this talk. That is the discovery of governing dynamics of ode trajectory data, or PDE data, directly from the data itself. So, this is kind of related to the problem of learning your PDE operator.
00:04:37.136 - 00:05:45.594, Speaker A: But in certain methods, like physics informed neural networks, you know what physics constraints you have. You're imposing the dynamics on your model, but here you're learning the dynamics directly from, ideally, experimental data. And the constraints that your model has to be interpretable and generalizable, I will argue, translates to a sparsity constraint in your space of identified models. So, there are two perspectives on discovering governing laws from trajectory data, and I'll talk about mostly the latter one. Either your ML representation learns its own very complex representation, like a neural network or some arbitrarily complex model, or we can constrain to human interpretable representations. And that's a familiar idea. It's known as symbolic regression, where you learn natural laws from experimental data.
00:05:45.594 - 00:06:18.704, Speaker A: And this has work on this has been going on for quite a while. So I'll highlight symbolic regression using genetic algorithms by Schmidt and Hodlipsen. So, that was back in 2009. And there are several challenges to using this type of symbolic regression. One of them is you want to identify something that's meaningful. You don't want to identify cosine squared plus sine squared of some input feature equals one. That's kind of a trivial conservation.
00:06:18.704 - 00:07:14.720, Speaker A: You want to identify conservation laws that are related, explicitly related, related to the dynamics or the derivatives, or the time derivatives of your state variables. And that's exactly what they do here. And they discover the equations of nonlinear pendulum based on this symbolic expression tree. Okay, so, optimization in this space is a little challenging because there's no guarantees that you would have found the sparsest expression or that you've identified the correct model. You just have your experimental data and the deviation from the fit of this function. There's no notion of a nearby function, in other words, as well. More recently, there's sparse identification of nonlinear dynamics, which essentially brings linear algebra to symbolic regression.
00:07:14.720 - 00:08:17.034, Speaker A: How many of you have heard of Cindy? Well, perfect. Perfect. Okay, so the idea is basically to discretize your state variables and take the time derivatives of your trajectory data, which are given to you, and set up a linear system of equations where the features you're trying to fit are nonlinear functions of your input variables. So in this case, these are the monomials of your input variables x, y and z, up to order five. And this is for the Lorentz systems, which has three state variables. And the idea is to find the sparsest combination of these functions that describe the data. And this optimization function is extremely efficient to optimize so it can be deployed on board, for example, a robot with very sparse or limited computational resources.
00:08:17.034 - 00:09:19.110, Speaker A: So, I'll walk through this in detail. So, I'm glad you guys are familiar. So, the goal is to identify the OD, or the functional form of f, your right hand side from trajectory data, which is given to you, and you impose a small library of of nonlinear functions. These can include monomials of your state, quadratic interactions all the way up to some order. And you impose that your identified coefficients are sparse. So, for the Lorenz attractor, for a chaotic system like this, you can set up a regularized least squares regression problem where x dot is equal to theta of x, our nonlinear function library times c. And when you solve this using a sequentially thresholdedly squares algorithm, you identify the correct model coefficients.
00:09:19.110 - 00:10:43.184, Speaker A: For example, sigma is ten, x is driven by dynamics in x, and y is driven by three nonlinear terms, one linear, no, two linear and one nonlinear. And z is driven by xy, a nonzero coefficient times xy times z minus beta times z. Okay, so, we've identified the canonical model of the Lorenz 63 equations by just, oops. Discretizing the data and fitting linear least squares, regularized least squares problem. I would argue that this is the natural kind of prior to kind of constraint to use for identifying governing models, is that the models are sparse in your space of possible functions. This, as it turns out, is the argument made in this paper, which is that most physical laws or governing laws that we, that we model are parsimonious. They are modeled with the fewest possible terms that describe the dynamics, and therefore they are interpretable and generalizable to new settings.
00:10:43.184 - 00:11:04.560, Speaker A: So this launched know a proliferation of approaches based on. Cindy. No. Up to. We limit it up to a single or up to a certain order. For example, for example, up to order five. So z to the fifth.
00:11:04.672 - 00:11:18.384, Speaker B: Within that order, they're all equally likely, yes. Does that like, reflect reality? I don't know anything about these systems. Like would you think, like, I don't know. In the space I work in, as they get more and more higher order, they're kind of less likely.
00:11:19.324 - 00:11:39.264, Speaker A: So, as it turns out, a large class of systems in fluid and atmospheric dynamics that we, that we study have these quadratic nonlinearities, and it's. There are systems with, you know, higher order nonlinearities, but this is kind of a very good assumption for a lot of problems.
00:11:41.154 - 00:11:44.854, Speaker B: Sorry, the assumption that it's equally likely up to some fixed order.
00:11:45.514 - 00:12:26.814, Speaker A: Yes. And so you can have, you can specify any order, but you're limited by, you know, the storage and computational resources available to you. I will point out that if you don't include kind of the correct terms, like, for example, the nonlinear pendulum, if you don't include the sine theta in your library, then what it does identify is the Taylor expansion sign in these monomials. So theta minus theta, theta cubed that expansion.
00:12:27.714 - 00:12:44.374, Speaker C: Can I ask, sorry about. Maybe this is a related question about the data that's available to you, because if you have a lot of data, relative points relative to the number of columns in this matrix, that it all turns out to be fine. And is that the regime that you're in most of the time?
00:12:44.834 - 00:13:32.694, Speaker A: Yes. So that is the question we try to answer with the work I'm going to talk about. So how does this depend on the length of your trajectory? How does this depend on the noise measurement noise? So, to your right hand side. So we cast this in a bayesian framework, essentially, and this is just a background for those of you. I figured this would be useful. So there have been a proliferation of models that extend this to identify PDE's from the data based on taking additional derivatives, like spatial derivatives and putting them in your nonlinear function library and adding all of those interactions. And it found that you can identify Navier Stokes from PDE from simulation data.
00:13:32.694 - 00:15:26.644, Speaker A: The question of noise in your data has been addressed by these two works, or a weak form of cindy, where you avoid having to take that dx Dt, the derivative with respect to time. Obviously, derivatives are amplified by noise, so they solve a weak form of cindy, which they call Windy. As it turns out, slowly the alphabets are being taken up. Esindi, which basically does a bootstrapping of the terms of the points, data points, and basically ensemble averages the identified model and uses that to simulate the trajectory to forecast the system forward in time. This is Ecindi, and I want to point out an intrinsic challenge with this is that for chaotic systems, you don't know whether you've identified what is the notion of a correct model, because after a certain time, your predictions will start to diverge. Your forecast will start to diverge after the characteristically up and off timescales so what is the notion of a correct model when you haven't, you know, a priori generated data from a model, you know, and then, and then run Cindy and found that you've identified the correct model? So that is one question that I'll try to answer with our recent work. I also want to mention that kind of like, the grand challenge here is to be able to, you know, point your cell phone at a moving object and derive Newton's laws of motion.
00:15:26.644 - 00:16:53.114, Speaker A: And so there have been kind of deep learning embeddings to discover these kinds of laws of motion directly from video data. So what this does is impose that sparse library fitting penalty on the latent representation after an encoding step, which is then decoded to reconstruct, you know, the next time step in your trajectory. So this is called Cindy autoencoders, I believe, by Kathleen champion and collaborators. So that's kind of the grand challenge. Like, can you simultaneously discover the right coordinates to observe and the right equations and just giving you context for that before I switch to my particular, what my lab works on. Okay, so this is the functional that's optimized by Cindy. You want to get a good fit to your trajectory derivatives using a linear combination of these library functions that you pre imposed, um, and you impose a sparsity penalty, um, usually motivated with an l zero norm, which is the number of active nonzero terms in your coefficient model xi.
00:16:53.114 - 00:18:37.034, Speaker A: But in practice, this is relaxed to an l one norm, uh, which essentially is basically lasso lasso regression. Uh, and in practice, they solve this with, uh, a much faster algorithm call sequentially thresholded least squares. And there's the real kind of driver behind this optimization problem was that series of work carried out like in the two thousands with compressed sensing and l one convergence to l zero by donohoe, tipshirani and collaborators. So, the model that we identified is very sensitive to our choice of our sparsity penalty and this goodness of fit, this resolution parameter, as we're calling it. And we want to try to answer the question, as you've asked, when is recovery even possible as a function of the trajectory length? How long do we need to observe the sampling time step and the noise? These are still open questions, and we also want to avoid this kind of trial and error search over the sparsity penalty lambda. So what's usually done is you have this pareto frontier, which is a family of Cindy models identified by different choices of lambda, and you choose one of the models that live on the Pareto optimal frontier. And this is the standard way of knowing, I guess, that you've selected the correct model, but there are several models that live on this frontier.
00:18:37.034 - 00:20:03.720, Speaker A: So I'll talk about this work by my postdoc, Andre Kleeshen and collaborators Joe Bakarji and Nathan Kutz, on basically answering these exact questions. Okay, so this brings me to basically statistical mechanics, very light statistical mechanics of dynamical system identification, as proposed by Cindy. So, by choosing a clever choice of prior, which is the combination of delta functions plus a wide prior, which we choose to be Gaussians, we can show that the map estimate is basically the l zero regularized Cindy loss function, which no one ever tries to solve. But it gives you a very useful post processing step on the identified models. So the posterior, in this bayesian inference framework over your coefficient sets, gamma. So gamma are all your coefficient sets. And cardinality of gamma is denoted by this absolute value of gamma expression.
00:20:03.720 - 00:21:00.506, Speaker A: And the free energy of each coefficient set is defined by this term. The free energy is, you know, the negative log of these statistical weights, that kind of weight your posterior over each co, each different coefficient set. Okay, so let's look at this free energy in some detail. So there is a, we want this to be as small as possible. So the optimal coefficient set will correspond to the lowest free energy. And the free energy expression for each coefficient set has a term that depends on the size of the set, which we call the natural sparsity penalty, and several data driven terms. So this is a data driven term, but this is the same for all coefficient sets.
00:21:00.506 - 00:21:46.208, Speaker A: So we ignore it. This term introduces kind of the goodness of fit. So v is our library derivative covariance. So think x dot times theta of x for each different coefficient set, and c is our library library library library covariance. So all of the library terms identified by this coefficient set, that covariance matrix formed across all time. And this is the, what's known as the explicit sparsity. And you can see that, again, it contains the cardinality of gamma in it.
00:21:46.208 - 00:23:01.824, Speaker A: So these two terms penalize the number of terms identified in your model, and these promote, this term promotes basically the goodness of fitness. So, let's actually look at the identification of the optimal coefficient set and show that it's consistent with the real model, the true model describing the Lorentz 6.3 dynamics. So here you can see that the lowest free energy set identifies the true dynamics for each of the variables. So y and x are the lowest coefficient set associated with x dot x y and x z for y dot and z and xy for z dot. This also shows you kind of what is the next best coefficient set, which is interesting because we did not have that with the Pareto front. So if you get two different Cindy solutions from different sparse regression solvers, you have no way of telling which is the actual driving dynamics for your PDE or ODE.
00:23:01.824 - 00:23:49.158, Speaker A: The bottom plots show the free energy, the free energies for all the different terms coefficient sets as a function of time. And when there's this natural stratification of these free energy sets that's introduced by the explicit sparsity term. So you can see that there's a collapse of the true coefficient set to zero, where this is the free energy minus the lowest free energy. So we had to actually compute all the coefficient sets to get this plot. Okay, excuse me. Yes.
00:23:49.286 - 00:24:05.714, Speaker D: Could you, hopefully doesn't drop too much, but could you give more intuition about the prior over, like the Bernoulli gears, like, I think prior that you used, like, why that one was. Why that, why that one in particular? Because it seems like it would inform all the free energy inference.
00:24:06.054 - 00:25:08.764, Speaker A: Yes, exactly. So that's what introduces all of those terms in the free energy expression. Okay, so here. So when you solve least squares regression without that sparsity penalty, you're basically assuming gaussian prior, not this. This explicitly introduces that, that sparsity, that delta function centered on each of the coefficient sets. And this form of the prior has been used for analyzing l zero regularized v squares, because the choice of this prior explicitly leads to this loss function for the map estimate when p is equal to zero, which has kind of been a thorn in the side of sparse regression problems. Like, we relax to l one, but does it solve the l zero problem?
00:25:12.544 - 00:25:29.084, Speaker D: So, connecting to Jennifer's point, you've already made some choices by reframing it as this b squares optimization problem. And now there's this canonical way in which we can, like, choose a prior that actually, then you can approximate yellow zero exactly.
00:25:29.204 - 00:26:26.594, Speaker A: Exactly. And you can do this for any choices of those Cindy variants that I've shown before, the either identification of PDE dynamics. And there's a group, I think, Shaecilia claimanti, who identified, who formulated Cindy for stochastic, stochastic differential equations. So it's possible to identify those as well. And this form of analysis can kind of give you a confirmation that you actually found the correct model and the distance of that model from other identified model when you have noise and other things, or not enough data. Yes. Is the assumption that there's a single coefficient set and the rest are zero, or that there is a small number of coefficient sets.
00:26:26.594 - 00:26:42.694, Speaker A: The assumption is that there is a, there is no assumption that there's a single coefficient set. Because what if there are a couple of free energies that are very close?
00:26:42.854 - 00:26:44.654, Speaker C: Yeah. Okay.
00:26:44.814 - 00:27:52.144, Speaker A: And then I was wondering, does that mean in the posterior, that you want to look at all of the correlations between the posterior sets? That's a good question. I don't even know if, when identifying two equal coefficient sets, equal free energy coefficient sets, when that would happen. But, for example, with Cindy, for the Lorenz model, the chaotic system, we have discovered similar attractors that kind of have the same properties as the Lorentz attractor, but it's not exactly the same model identified here. I guess there could be two with equal free energy because they both equally explain it. Or is it that there's like multiple ones that you will have to. It makes me think about, you know, existence and uniqueness. But yeah, there should always be like a numerical difference between two free entities.
00:27:52.144 - 00:29:05.418, Speaker A: Okay, so we have this interesting, we have studied the dependence of noise on the sparsity penalty and the length of the trajectory. And I'm just showing the choice of sparsity penalty here. But there are different, there are sharp transitions between correct and incorrect identification as a function of the sparsity penalty. And we see this interesting power law which suggests that, you know, our sparsity penalty should scale us the cube of the noise and Zc. This is what we're calling it because of the z partition function. This can actually do better than, or comparable comparably to ensemble Cindy, because it can handle quite a lot of noise in the z variable and y variables, though it's not clear why. Here we see these sharp transitions of coefficient sets with increasing sparsity penalty.
00:29:05.418 - 00:29:46.384, Speaker A: So here is a correctly identified model. In the light yellow for the y dot is the three term model here, and for x, it's the x and y term model. And these have the largest areas. But you can see that the y is a pretty, pretty sensitive to the choice of sparsity penalty. And if you increase it beyond a certain threshold, you can identify an even sparser solution, but it has a higher free energy, so it tells you that that's wrong. And the discrepancy in free energies, though it may be small. Ah, this might answer your question.
00:29:46.384 - 00:31:23.244, Speaker A: So, the probability of selecting the suboptimal set decays exponentially with trajectory length. So as you increase the length of your trajectory, you're introducing a bigger gap between gamma and gamma star. And this leads to exponentially decaying probability of identifying the incorrect coefficient set. So, here's a comparison to ensemble Cindy, where we can see that the strategy of bootstraps ensemble models, by bootstrapping different choices of your data points, randomizing over your data points, misidentifies this constant term, which doesn't actually exist in the Lorentz 63 dynamics, whereas Zcindy actually identifies the true posterior. Of course, this requires computing the free energies for all the power set over gamma, so two to the n, where we considered a ten term library, so 1024 coefficient sets. But you don't need to do this if you just want to compute the relative free energies for two coefficient sets identified by, you know, l zero l one optimization or sequentially thresholded least squares. So you should think of this kind of as a post processing step when you don't have the true model and you have experimental data.
00:31:23.244 - 00:32:20.952, Speaker A: Have I identified the correct model? Okay, before I go into this, I'll just summarize the takeaways from Zcindy. It gives you the best model from the library in terms of free energy, which the free energies balance sparsity and goodness of fit in kind of predictable way. So there's no need for a pareto front. It identifies critical levels of noise and appropriate choices of lambda for correct inference. And it has, it's actually, this needs to be studied further, but it's actually very closely related. I mean, it is ld or regularized sparse regression. So this could be kind of a way to study feature interactions and selections for all sparse regression.
00:32:20.952 - 00:32:36.814, Speaker A: So anytime you're solving a sparse regression, you can study the interaction of the features you identify, the sparse combinations of features you identify. I'll stop here and see if there are any questions. Yes?
00:32:47.674 - 00:33:12.274, Speaker E: Maybe you have partial access. Have you or others kind of looked into using some missing variables? Maybe you have a partial observation. Maybe it implies certain constraints on your things that you're not directly observing, and then you might want to send it on top of some sort of.
00:33:18.654 - 00:34:28.264, Speaker A: Yes, let me think about that. So, I think we talked about when the library is incomplete, if one of your variables is missing, then it would just identify whatever closest model. But as for how we can go beyond that, so we can do things like delay embedding. So where you. Because the, the assumption here is that you're, this is, this is Markov process, and you're not, you're identifying all the relevant variables in your dynamics. If you have partial observations, tokens, delay embedding suggests that you can delay embed your dynamics and identify kind of do system identification in that space, which is essentially what's done in system identification, like Eigen system realization algorithm. This is linear models, but people are working to generalize to nonlinear with Koopman operator methods and things like that.
00:34:28.264 - 00:34:38.367, Speaker A: Yeah, good question. We can talk offline more about that because that's kind of an open problem. One more question.
00:34:38.475 - 00:35:10.024, Speaker D: Kind of the same kind of question, but rather than partial observability of your variables, if what you actually observe is some non linear function of your, like, dynamical variables, it might still be monotonic, but maybe it's, like, weird. Or maybe there's like a distribution of possible functions that might be kind of hard to invert. Have you guys tried using SimD plus some, like, inversion step to, like, map from your observables to your underlying dynamical variables?
00:35:10.484 - 00:35:50.524, Speaker A: Yeah, that's. I think that's kind of what Cindy autoencoders does, which is if you're not observing the correct variables, if you're. If you have a video of, you know, of a pendulum swinging, then it uses an autoencoder structure to enforce the Cindy model in the latent space, in the latent representation, as opposed to the original coordinates, which is pixel space. Yeah, I hope that answers your question. And we have some stuff on Koopman embeddings from partially observed systems as well. I don't know if I'll have time to get to that.
00:35:56.484 - 00:36:16.104, Speaker E: Maybe a really good question. So suppose, like, my dynamics are not directly predictable for my instantaneous variables, so there's like hidden variables. I do some sort of hysteresis. How would you have different mechanisms for CD? Or is there, is there.
00:36:16.724 - 00:36:21.304, Speaker A: I'm actually not sure. I'll have to think about that.
00:36:25.024 - 00:37:33.564, Speaker F: I was just wondering, you talked about the example where if you didn't provide the sign x as a possibility for the double pendulum model, then you would end up seeing the expansion of that. When you see like two, multiple two models that have a very similar free energy, does that suggest that there might be like a different term that's missing or that like some, like a dot product or something, that there's some term that's missing? Like, how do you think about, like, you obviously can't put everybody sign function and every Fourier expansion as a basis in this, but is there like, a best practice for putting some of them in there or trying to identify, like, what's a signal that you are missing, that you have an incomplete basis? Does anything have a similar free energy suggest that you have a complete basis?
00:37:38.484 - 00:37:59.756, Speaker A: So that is a limitation of the Sindhi approach is that, you know, you're assuming, like, monomials up to a certain order. The. Yeah, the free energies will not tell you if you're missing a term, if.
00:37:59.780 - 00:38:01.024, Speaker E: That'S what you're asking.
00:38:01.644 - 00:38:10.978, Speaker F: Do you find it, like, worrying if you have lots of terms that have similar amounts of free energy, like lots of different combinations that have a similar.
00:38:11.026 - 00:38:11.802, Speaker D: Amount of free energy?
00:38:11.858 - 00:38:35.714, Speaker F: Like, if you have one great answer and a bunch of really bad ones, you can feel, like, wonderful about the results. But, like, does it make you uncomfortable if there are several ones, that there's a couple that are similar? Does that, like, scary, or is that, like, more closely at what the two solutions are?
00:38:36.014 - 00:39:55.834, Speaker A: I think you would look at the trajectory simulation if you have something like this where you have very close free energies and you don't have the correct term in your model, but it usually will include the correct term in the nearby models. As you can see here, the question of stability was raised before, and I included this work. So Cindy is also used to discover Cindy Galerkin models, which are reduced order models with quadratic interaction terms for Navier Stokes equations. So, using this, if you do Cindy on the projected dynamics, then you can identify interesting things like the HoF bifurcation for cylinder flow and the transition to a limit cycle that was in the original cindy paper. However, this does not directly enforce stability of energy preserving dynamics. So there is a theorem, the Schlegel and Nowak theorem, which derives conditions on the global stability of the projected reduced order model based on conditions on the linear part and this Q tensor here. And you can explicitly enforce these stability conditions via something called trapping Cindy.
00:39:55.834 - 00:40:55.062, Speaker A: And you can see that actually the radius of this trapping region is determined. It depends in a somewhat complex way, but in a linear way, on l and q. And you can see that the Cindy Galerkin ROm reduced order model loses stability, whereas the Sindi with the trapping region enforces stability and is stable for a long time. And looking at the kinetic energy corresponding to this reduced order model, you can see that trapping Cindy enforces stability. And this is really important. When you're predicting physical quantities like flow behind a cylinder, you want to enforce energy preserving stability. I noticed that I don't have a lot of time left, so I'll talk a little bit about sparse sensing in safety critical applications.
00:40:55.062 - 00:42:09.954, Speaker A: So this is where the choice of measurements affects all downstream decision making and basically the performance of your control laws and inference tasks. And the same problem shows up all the time in various applications where your sensors are extremely expensive and extremely constrained in components of a reactor or in large scale assembly, where you can only deform certain locations of a structure. So this brings us to the optimal sensor placement problem, which is a design problem on top of an inference problem. So we use, in a lot of our work, we use the fact that data is low rank using truncated SVD or pod. This is how we get the Galerkin projection ROM. And in a lot of data, the effective rank is less than the, much less than the ambient dimensionality of the data. So for flow behind a cylinder, you can basically truncate at 40, because it's a physical simulation data, very clean.
00:42:09.954 - 00:43:15.804, Speaker A: For image data sets like faces, there's slower signal decay, but it's still around. There's an optimal truncation rank associated with it that was derived by Gavish and Donahoe. And this dataset is compressible to basically 150 singular vectors. So we use this idea to solve or efficiently solve the design problem, another combinatorial scaling problem where you're trying to optimize p sensor measurements. We're assuming point sensor measurements to reconstruct a high dimensional state x, which we're assuming is a flow or some kind of image. It's high dimensional. And we exploit the fact that our state is low dimensional in a representation basis that has been learned from simulation to basically solve the sensor placement problem much more efficiently.
00:43:15.804 - 00:44:12.284, Speaker A: So the inference task is easy. We know how to, given zero mean gaussian noise, we know how to invert this equation to solve for our mode coefficients and reconstruct x as a linear combination of phi times a. That's called gap ePod. We can, the design problem on top of this is to optimize those selections of or rows of this operator to minimize your some metric of error of your estimation. We choose the log determinant of this Fisher information of measurements. Or it's also this data. Transpose data is also sometimes is equivalent to the observability when the states are sampled from an ode, for example.
00:44:12.284 - 00:45:02.676, Speaker A: This has a correspondence to a convex optimization problem. So if you relax this binary selection to, you know, a fractional weights over your state space, then you get this convex relaxation objective. But this is order n cubed, where n is your number of candidate locations and n in, you know, fluids is intractably large. It's millions of states. And so these problems are usually limited to spaces with thousands of states. Again, the solution is in linear algebra. So we use the pivoted QR optimization to maximize the determinant of theta transpose theta.
00:45:02.676 - 00:46:02.504, Speaker A: The Fisher information of measurements which automatically kind of maximizes the diagonal entries of R. We think it's taking some sort of a greedy step because this optimization is also submodular. So the pivot indices in QR factorization correspond to sensor locations. So it chooses the sensor location or the row of theta or the row of phi with maximal to norm, does the orthogonal projection that maximizes the current leading entry of R. And because a log determinant is equal to this, this is a greedy procedure for optimizing this. So our optimal selection operator, near optimal selection operator is given by the permutation matrix in QR pivoting. So this is order nr squared, which makes it tractable for the problems we work with.
00:46:02.504 - 00:47:18.364, Speaker A: We also look at this optimization in terms of the statistical mechanics perspective by identifying free energies of sensor placement. So you can see that these pivots identify important features in the data, in this case pixel locations corresponding to facial features. And you can see that we can quantify how well how good this reconstruction will be with respect to a given sensor set and with random placement. If you look at the log determinant, it's going to be very, very suboptimal and you can see that the reconstruction is ill conditioned. So this motivated the perspective of looking at this objective and identifying it with a Hamiltonian of sensor placement. So this in terms of one sensor interactions and two sensor interactions, which is repelled from correlated sensors. So you want to pick sensors that are not correlated with each other to identify a minimal sensor set.
00:47:18.364 - 00:49:08.464, Speaker A: So we've done this for several very high dimensional search spaces, such as the sea surface temperature, which has around 40,000 grid points. And you can see that using this bayesian inference framework, using a data driven prior, improves the reconstruction significantly to the point where with this data driven prior, we need fewer sensors than the rank of the model, whereas the competing approaches, like the interpolation methods use, require at least the number of sensors to be at least the rank of the model or more. We benchmark it against the leading approaches. And the purple line is the QR algorithm combined with this bayesian reconstruction framework, and it achieves kind of optimal reconstruction with minimal error with the smallest number of sensors. We also deploy this for with our collaborators in the subsystem of a nuclear reactor. This is a reconstruction of physics simulations of the temperature distribution, and you can see that a small distance in the log determinant between placements translates to a very small difference in the error. So you're perfectly fine using a suboptimal sensor placement, by incorporating constraints that are necessary, that are imposed by the engineering setting or the accessibility of the sensor.
00:49:08.464 - 00:49:53.024, Speaker A: And this is interpretable because you get a distribution of each of the mode coefficients. And with higher noise, you can see that the predictions are basically within distribution. At low noise, the model, the model truncation is too low. So this is, that dominates the noise. And that's why it looks like this for the low noise scenario. And it also gives you this kind of uncertainty heat map of a given sensor placement without having to do the reconstruction. This is given a priori.
00:49:53.024 - 00:50:38.714, Speaker A: I think I'm close to my ending point. So there are real world applications of this, especially in large scale aircraft assembly. So this is the engineering bit. So if you think of aircraft assembly, it's like assembling Ikea furniture, which is already challenging, but scaled to the size of this building. And, you know, composite structures deform kind of easily, and there's, it's dragged down by the weight of the. Under its own weight and also the weight of the, of the tooling around it. So there's a bunch of components holding these structures in place.
00:50:38.714 - 00:51:18.184, Speaker A: So one of the major challenges is to predict the gaps that occur between joining parts without actually fitting them together. And the way that's done is by programming these metrology lasers. It's really expensive to slowly scan over each part. So you can think of this wing to body join as kind of like a box box type structure, but you don't want to fit the wing to the fuselage. It's called dry fitting to measure these gaps. Because if you think about a 747 wing to body joint and an entire person, probably half this room could fit inside that join. That's how big the structure is.
00:51:18.184 - 00:51:59.830, Speaker A: So there's really a great need to reduce that programming and reduce the cost of taking these measurements because they're different for each different aircraft. So we can take the data from individual aircraft. There aren't many. You can't train an auto encoder from 50 different aircraft. So we do the low rank compression using robust PCA. There are, in fact, outliers, so you can't use standard PCA. So these essentially point clouds are digitally aligned, and that's how they estimate these gaps and manufacture shims to fill those gaps.
00:51:59.830 - 00:53:27.422, Speaker A: So they were looking for a way to bypass that procedure because taking those high fidelity measurements with metrology was extremely expensive. It turns out that with a subset of those, those measurements, we can still accurately predict those shims to within manufacturing tolerance. This is really challenging because in aerospace manufacturing assembling, think about that Ikea furniture, you scale it to the size of the building, and it has to meet your nominal shape to within the width of a human hair. So that's how strict regulate the regulation is for manufacturing tolerance. So you have to meet that. And as it turns out, with these strategic measurements that are optimized for this process, you can predict 99% of the unknown gaps with 30 times fewer measurements. This has actually been deployed into the 787 production workflow along with this, beyond optimizing, you know, sensor placement in this kind of linear estimation problem, we're also learning, using deep learning to learn embeddings of extremely sparse measurements to reconstruct turbulent flows.
00:53:27.422 - 00:54:34.488, Speaker A: So for turbulent flows, you don't need to, there's less dependence on where to optimize these parse measurements because there's a lot of scale mixing. And basically any location is informative. But I will caveat with more localized dynamics, like with the nuclear setting, you do have to optimize sensors. And in this work, they are able to reconstruct to a much higher accuracy than with this linear gap y estimation framework using the shallow decoder LSTM framework. So this motivates the idea of kind of, you know, what is information contained in your measurements when you have these deep learning embeddings. Developing some of these observability metrics and information criteria for these kinds of embeddings will help us deploy it in nuclear and aerospace engineering applications. So I'll stop here.
00:54:34.488 - 00:54:36.644, Speaker A: I don't think I have much more time.
00:54:53.204 - 00:55:14.344, Speaker E: It's a really interesting talk. And for this last example, when you choose these different positions and you're decoding it out, is this for like, do you train on a particular trajectory, or is this for like, arbitrary turbulent flow within some constraints? Or I guess, like, how much information does the model need to see?
00:55:20.694 - 00:55:27.462, Speaker A: This one obviously requires a lot more training data, and it's a long trajectory of one specific turbulent flow. Okay.
00:55:27.478 - 00:55:41.794, Speaker E: And then reconstruct for that flow. So it's kind of like a, like a data compression for that data set, rather than like for an arbitrary, like, oh, I'm going to transfer this to some arbitrary other turbulence case.
00:55:43.274 - 00:56:19.094, Speaker A: Exactly. It's basically fitting to hold out data from the same data set. The real contribution here is the sparsity of the measurements we're taking. And because we're delay embedding them and learning those sequential connections using this LSTM layer, intermediate LSTM layer, able to learn the long term dependencies and is able to then decode it to the entire turbulent flow much better than. Yeah.
00:56:22.554 - 00:56:54.584, Speaker G: This might be massively over, over interpreting picture figures. But in the airfoil and reactor sensor placement, I noticed me and some of the figures, like, there. I felt like there were like, clusters or kind of some sensors that were, like, closer than you'd expect. Is that so? And is there anything kind of about the nature of the solution or the character of the solution geometry in those domains that leads to this?
00:56:56.084 - 00:56:56.824, Speaker A: So.
00:56:57.444 - 00:57:02.464, Speaker G: Or, like, it doesn't look as maybe symmetric or maybe the reactor example for.
00:57:02.604 - 00:57:37.904, Speaker A: Yeah, so there, there are very strong spatial correlations that are making the sensors go close to each other. This is where the, you know, the constraints and looking at those sensor interactions really comes into play. If the sensor fails, select a placement nearby, you know, induced by interactions between the other sensors. This is really kind of the same idea between Zcindi and this. Look at this closest one in that energy landscape.
00:57:47.844 - 00:58:27.574, Speaker C: Thank you. I used to look out of my window and watch the leaves swirling around in the window. And I would wonder what the dynamical whole wind structure around my building was. But all I had was these point measurements from my leaves that were moving instead of fixed. And so I guess I'm wondering if that problem is kind of almost the same as this one, or whether it's fundamentally different. And if you could do something like placing leaves at an arbitrary point, but then they would follow the path of the trajectory rather than being fixed. Is that, you know, is that an intractable problem?
00:58:29.274 - 00:59:24.244, Speaker A: So that requires kind of a different perspective, like eulerian, lagrangian. You do want to. There is some work, I forget the name, but there is some work looking at coherent structures or estimating coherent structures from atmospheric flow and deciding where to place particle tracers that add back along the coherent structures. So the question you're asking is closer to that, because you want those leads essentially to be, you know, be picked up and follow that. Follow that structure. We are looking at drifting sensor drift in the reactor setting. That's a really important problem, because when they shut down the reactors, sensors have really good.
00:59:27.664 - 00:59:38.984, Speaker B: All right, I think in the interest of time, but I want to make one small request. There's supposed to be four short talk speakers. I think I only see two slide dogs. I'm not sure if you're giving a short talk.
00:59:39.024 - 00:59:40.728, Speaker A: Can you just come to the front.
00:59:40.776 - 00:59:47.354, Speaker B: Now as we go into the break, so I can understand what our situation is? Okay, now we can join the break and let's think.
