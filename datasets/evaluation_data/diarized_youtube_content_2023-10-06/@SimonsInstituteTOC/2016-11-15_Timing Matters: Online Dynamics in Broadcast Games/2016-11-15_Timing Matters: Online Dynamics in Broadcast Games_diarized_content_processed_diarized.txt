00:00:00.280 - 00:00:02.070, Speaker A: Okay, so welcome to the last session.
00:00:02.102 - 00:00:05.542, Speaker B: Of the morning, and this will be by Shuchi from Scottsdale Medicine.
00:00:05.678 - 00:01:35.154, Speaker A: Thanks, Ravi. Okay, so this talk is joint work with Deb Mallya, who is here, and William, who's in the back there, and Sefi nower and Mohit Singh. And so, the topic of this talk is to understand how well systems perform under strategic behavior. This is one of the overarching themes in algorithmic game theory, and there's been a lot of work on this question. So imagine that we have a system that has many participants, and as a central planner, you might have some global objective that you want to achieve through the system, but every participant has their own individual goals that they want to optimize for. And so the participants take actions that may not align with the objective of optimizing the global behavior of the system. And so the question we want to ask is, in equilibrium, when everybody takes the action that is best suited to them, how well does the performance of the system compare against the optimal that could be achieved if everybody was cooperating? Okay? And in many situations, there are many different equilibria that can arise in settings such as this.
00:01:35.154 - 00:02:55.146, Speaker A: And so, going with the beyond worst case theme of this workshop, you know, the question is, which equilibrium are we analyzing? And in this talk, we are not going to be looking at the worst equilibrium or the best equilibrium, but rather something that emerges through some sort of natural dynamics in this game. Okay? And we'll see that, you know, the, this question is very closely related to thinking about how well algorithms perform when there's some sort of restriction on the set of actions that the algorithm can take. Namely that every such action should correspond to behavior of a self interested agent. Okay? So, um, uh, we're gonna focus on a specific setting where, um, there's a big gap between what worst case equilibria look like and what best case equilibria look like. So that, uh, this question, uh, becomes interesting, and, uh, we're gonna give an analysis for, uh, that specific game. Uh, but before we move forward, I want to start with a disclaimer. Uh, there's been, uh, uh, a very nice emphasis over the last day and a half on, you know, practical motivations towards algorithm design.
00:02:55.146 - 00:04:08.704, Speaker A: I'm going to depart from that and do this purely theoretical exercise with the goal being to understand, you know, what kinds of approaches can we take to analyzing algorithms within this sort of very restricted action space? Okay, so this is the game that we are going to talk about. This is called the broadcast game, and it's a special case of a class of games that are called network design games. So, uh, the goal here. So we're given, uh, this graph, or a network, and there are costs on edges, and there's a root, okay? And every, um, agent in this game is, uh, an agent that resides at one of these nodes, okay? And the goal of these agents is to buy a path that connects to the root. And so, um, the solutions that we expect to see here are, um, essentially spanning trees, okay? And the global objective that we're trying to minimize here is the cost of such a spanning tree. So, for example, um, you know, this could be one particular solution. In this, uh, uh, example, uh, the total cost of the solution is 15.
00:04:08.704 - 00:05:01.832, Speaker A: And this also turns out to be the minimum spanning tree, uh, for this graph, okay? And the agents are going to share the cost of the spanning tree. And so we're going to look at what's called the Shapley cost sharing, namely that the cost of every edge is shared equally by every person that uses it. So, for instance, if you look at this edge, okay? So in green here are the paths that the different agents are taking. And so this edge is shared by all five of the agents in this graph. And so its cost is going to get shared equally by all of these five agents. This edge is shared by three, and so its cost is going to be shared by three agents and so on. Okay? And so in particular, the cost share of this guy here is four plus three by three plus five by five.
00:05:01.832 - 00:05:36.964, Speaker A: Okay? So that's the structure of the cost shares. We can write down these cost shares for every terminal for the solution. And every particular terminal in this graph is interested in minimizing their own cost without worrying about the total cost of the solution. So, do you have a question? Yeah. In that case, is it obvious that a tree is in equilibrium with Shapley cost sharing? Very good question. So indeed, an equilibrium is always going to be a tree, and I'll shortly tell you why. Yeah.
00:05:36.964 - 00:06:35.964, Speaker A: Okay. So every agent's goal is to minimize their own cost share. So if you look at this particular agent over here, this guy is currently paying the cost of six, but it could in fact switch from its current path to this one and pay the cost of this entire path by himself and still save on his cost share. This terminal is going to want to switch. And when he does that, of course, the total cost of the solution goes up. So you can see that the optimal solution here is not necessarily an equilibrium in the sense that everybody is happy with that solution. Okay? And, of course, when this guy moves away from this path, everybody else on that path sees their cost share increase, okay? And so, potentially, now some of these guys are not happy anymore with their paths.
00:06:35.964 - 00:07:40.444, Speaker A: And so, for instance, this guy now is paying two plus three halves, plus five fifths on his path. And if he shifts to this one, then he only pays three plus five fifths. And so this particular terminal also wants to shift, okay? And now it turns out that this particular configuration is an equilibrium in the sense that no terminal wants to deviate from their current path to a new path unilaterally. Okay? So that's an equilibrium in this game. And you can already see from this example that equilibria could be more expensive than the minimum spanning tree in this game. Okay? Okay. So the question that we want to ask is, well, how good are these equilibria? How large can their cost be relative to the minimum spanning tree? And as I mentioned earlier, one problem with such a question is there could be multiple equilibria of different qualities.
00:07:40.444 - 00:08:34.134, Speaker A: And so, to deal with this, you know, with the worst case mindset that we tend to have in theory, could Sufius and papa DiMitro define this notion of the price of anarchy of a game? And this is defined to be, this is like an approximation ratio. And it's defined to be the approximation ratio achieved by the worst equilibrium in the worst instance of the game. Okay? Now for the broadcast game, this ratio can actually be really large. And let me show you an example where it is really large. So here this is the root again, and we have n agents residing at this other node. And there are two pads, one of which is very cheap, and the other is really expensive. It has a cost of n.
00:08:34.134 - 00:09:17.420, Speaker A: And of course, if everybody was to take the cheap edge, then this would be an equilibrium. But this is not the only equilibrium in this game. So if you imagine everybody taking this long, uh, path to the root, then everyone's cost share is n divided by n or one, and nobody wants to switch to the cheap path unilaterally. Okay? And so for this particular example, for this equilibrium, the price of anarchy is of almost a factor of n. Okay? Okay. So that's pretty bad. Okay? But observe that this is a really strange example.
00:09:17.420 - 00:10:21.694, Speaker A: So you'd really have to force all the n agents to go on that long path in order for this to be an equilibrium. So unless they all collude to pick a bad solution in this particular example, that's not the equilibrium that you'd expect to see. Okay? And that's kind of the point of what we're trying to do here. We don't want to think of this n as being really the performance of this game, because this equilibrium is never going to emerge. Okay? So the broadcast game and a more general family of network games was proposed by anchovies et al. And they came up with the concept of the price of stability to deal with precisely this issue that you don't expect to see, the kind of equilibria that I showed you on the previous slide. Okay, so the price of stability, um, measures the quality of the best equilibrium, uh, in an instance of the game.
00:10:21.694 - 00:10:32.550, Speaker A: And again, you're taking the worst case over all instances, but the best case over equilibria of that instance. And then again, it's an, it's like an approximation ratio.
00:10:32.742 - 00:10:34.674, Speaker B: Sorry, why is that called price of stability?
00:10:35.054 - 00:11:56.168, Speaker A: Good question. So it's called the price of stability because, um, it's basically saying that, uh, you know, if a central planner were to come and suggest a solution to every agent, then the agents would actually be happy with that solution in the sense that it's in equilibrium and no one wants to unilaterally deviate. So the best equilibrium is telling us that if we could somehow enforce the solution, then it would be stable, nobody would want to move. Anchelovich et al also showed that the price of stability in any such shapley network game is small relative to the price of anarchy. And this is basically through a sort of potential function argument. So there's a class of games called potential games, where you can assign a potential function to every possible state of the game with the property that if anybody wants to deviate from their current strategy, then the change in their cost share is captured by the change in potential. Okay? And that allows us to do various sorts of arguments on the existence of equilibria and such.
00:11:56.168 - 00:12:53.140, Speaker A: So let me describe the potential function for this particular game, and it looks like this. So there's a sum over the edges of the cost of the edge. And note that if we didn't have this extra term here, then this would just be the cost of the current solution, all of the edges that that somebody is using. And let's denote by n, sub e, the number of people that are using this edge e. Then the potential for this edge is this the n, sub e harmonic number. Okay? So you take one plus one, two, so on, till one over n, sub e. And the observation here is that the last term in this potential function for any particular edge is precisely the cost share that any terminal using that edge sees for this edge.
00:12:53.140 - 00:13:59.884, Speaker A: So if there are any people on edge e, then the cost share for any one of those on this edge is precisely c, sub e over n, sub e. Okay, so what does that mean? That basically tells us that if you, if you take an agent and you move them from one path to another, then uh, the potential on the edges in the old path decreases by precisely this last term, which is the cost share of the, the terminal on those edges. And on the new edges, the edges in the new path, it increases by precisely what the new cost share of the terminal is on those edges. So the change in potential from one agent moving from one path to another is precisely, uh, the change in the agent score cost share. So if an agent wants to move and decrease his cost share, the potential decreases. And now you could imagine thinking about a series of such moves where people in sequence try to improve their cost shares, and this potential keeps decreasing every time.
00:14:01.264 - 00:14:02.856, Speaker B: So what's a potential game?
00:14:03.040 - 00:14:09.324, Speaker A: A potential game is just a game where you can define such a potential function. It doesn't have to be exactly of this form. But.
00:14:16.524 - 00:14:21.700, Speaker B: Second, fine, what do you mean you can define such a potential function? What do you mean by such?
00:14:21.852 - 00:15:04.174, Speaker A: I mean you can define a potential where every improving move is going to decrease this potential. And the change in potential is precisely the change in the cost share of the agent. So what are some implications of this? So first of all, this potential starts out at some finite value, decreases every time. This process is going to converge to some local optimum for this potential function. And any such local optimum is going to be an equilibrium of the game. Okay, so equilibria always exists. There could be multiple equilibria because there could be multiple local optima.
00:15:04.174 - 00:15:57.858, Speaker A: And for this specific game, one can use the form of this potential function to also argue that the price of stability is small. In particular, think about the global optimum state for the potential function. Then the cost of such a global minimum is going to be no more than the potential at the minimum minimum spanning tree, which is no more than a logarithmic factor off of the cost of the minimum spanning tree. And all we are using here is that this potential is at least as large as the cost of the solution and no more than a factor of log n off. Okay, so that shows that there's some equilibrium that is at most, oh, than the MST in terms of cost. Yeah.
00:15:57.906 - 00:16:01.178, Speaker B: So that's the potential function for general congestion.
00:16:01.346 - 00:16:05.226, Speaker A: Yes, it is the same potential function for general congestion.
00:16:05.330 - 00:16:09.974, Speaker B: People are just choosing passes rather than a subset of the edges. Does it change anything?
00:16:10.394 - 00:16:39.374, Speaker A: So I'm going to talk about improvements on this particular bound. This is an upper bound. So for general network design games, actually this analysis is tight. There's an example where the price of stability is exactly log of n. And here's the example. So here, this is the root. The terminals are all here, okay? And we have an extra node here.
00:16:39.374 - 00:17:28.438, Speaker A: And every terminal has two potential paths to the root. And observe that this graph is directed. So there's, every terminal has only two different paths. One goes through this extra Steiner node and has this one plus epsilon cost. The other path has different costs for different terminals. But observe that a solution where everybody tries to follow the cheap path here, which is in fact the optimal solution for this graph, is not a stable solution because everybody sees a cost share of one plus epsilon over n. And so this last guy who has an alternate path of cost one over n, is going to want to switch.
00:17:28.438 - 00:18:25.304, Speaker A: But once this guy switches, everybody sees a cost of one plus epsilon over n minus one. And then the second to last guy here is going to want to switch because his alternate path costs one over n minus one and so on. So the only stable solution in this example is one where everybody takes their individual edge to the root and the total cost of the solution is log of n as opposed to the optimal solution that has a cost of one plus epsilon. This is the only equilibrium. In this example, anchelovic et al. Basically gave a tight bound for the price of stability. And observe that this is already an exponential improvement over the price of anarchy bound that we saw, which was n in the worst case.
00:18:25.304 - 00:19:07.224, Speaker A: Okay. Um, but we're going to be looking at a special case of this game, one where you don't have any Steiner vertices. So every vertex in the graph is a terminal and wants to connect to the root. And furthermore, the, the graph is undirected. Okay, so here, uh, the fact that the graph is directed was important in showing that you get a price of stability of exactly log of n. Okay, so there are improved bounds that are known for price of stability. Lee showed that in undirected graphs, uh, you can get a slight improvement from the log n bound.
00:19:07.224 - 00:20:20.748, Speaker A: And when we talk about broadcast games, namely those where every vertex in the graph has a terminal residing at it, then there's a succession of papers that gave progressively better bounds, and this terminated in an o of one bound. So there's some large constant such that you can always find an equilibrium of the gain that's within this constant factor of the optimal solution. So there's all of this nice work, and much of it basically proceeds by starting with the optimal solution and then making some very careful modifications to it so as to convert it into an equilibrium. And these modifications, these are pretty involved algorithms, and none of them is, for instance, computationally efficient, so that you can't even compute these nice equilibria and then enforce them or suggest them to the terminals.
00:20:20.876 - 00:20:23.156, Speaker B: So, computing them as NP hard or what?
00:20:23.340 - 00:20:33.824, Speaker A: No, it's not known that it's NP hard, but there's no algorithm known for computing any equilibrium efficiently in the scheme.
00:20:35.744 - 00:20:40.432, Speaker B: And I think the lug n type for the exam, like, I mean, we.
00:20:40.448 - 00:20:46.416, Speaker A: Don'T know this one. Not sure. The palaya might know. Not known to be tight.
00:20:46.600 - 00:20:47.444, Speaker B: Okay.
00:20:51.264 - 00:22:19.184, Speaker A: So they're somewhat unsatisfying in the sense that even if you were to believe that, you know, a central planner should compute a good equilibrium and then suggest it to the terminals, we don't even know of an efficient way of doing that. Okay, so, to come back to the question that I stated at the very beginning, our goal is to think of some sort of natural dynamics that the agents can follow to converge to an equilibrium. And the question we want to ask is, what kinds of dynamics lead to a good equilibrium? What kinds of dynamics lead to a poor equilibrium? Okay, so what do I mean by natural dynamics? We're going to allow three kinds of moves here, so terminals can arrive in the system one by one. And when a terminal arrives, it follows the best path, the current path, to the root. Given the current state of the system, terminals can depart, and in between terminals can make improving moves, which means they can switch their current path to a new path with a smaller cost share. Okay, so this sort of process was first proposed by Chikhouri et al. And they looked at the following kind of simple dynamics, which is split into two phases.
00:22:19.184 - 00:23:19.248, Speaker A: So in the first phase, every terminal arrives in some adversarial sequence, and in the second phase, terminals make these improving moves in some arbitrary order, and there are no departures. And they showed that for this kind of dynamics, this always converges to an equilibrium that's no more than a factor of square root of n times some poly log of n times the optimum. Okay, so it's an improvement over the price of anarchy bound. And the analysis of the same dynamics was later improved by Charikar et al. And they showed that, in fact, the worst equilibrium you could arrive at through this process is no more than o of log squared of n, worse than the optimal. Okay, so this is a huge improvement over the price of anarchy bound. It doesn't quite get down to o of one of the price of stability bound, but it's still pretty good.
00:23:19.248 - 00:23:26.720, Speaker A: Okay. Okay. And the analysis of this process. Yeah.
00:23:26.832 - 00:23:34.992, Speaker B: Is that independent of the initial condition for the dynamic system, is the efficiency watts bound independent of the.
00:23:35.048 - 00:23:58.274, Speaker A: So the initial condition is that the graph is empty. There are no terminals, there are no paths. Okay, so you start from an empty graph and then people arrive one by one, and every person that arrives looks at the current state and takes their best path at that point of time, and then they make improving moves. So the initial condition is just that the graph is empty. Yeah.
00:23:58.774 - 00:24:06.314, Speaker B: Does this include bounds on the rate of convergence or just statement?
00:24:06.694 - 00:24:34.324, Speaker A: So the first phase is just n arrivals. The second phase, the analysis of the second phase is just in saying that the potential function decreases as the process goes on. And again, there are no bounds on how long that process takes because the potential function starts out at a finite value, but in some steps it might make only exponentially small improvements. And so the length of the process could well be exponential.
00:24:35.784 - 00:24:39.384, Speaker B: The cost gets low even at the beginning of the phase two, is that correct?
00:24:39.424 - 00:24:40.064, Speaker A: Yes, yes.
00:24:40.144 - 00:24:44.812, Speaker B: The cost gets low fast. It just may take a little while to beat exactly an equilibrium to actually.
00:24:44.968 - 00:25:45.594, Speaker A: That's right. That's right. So the way that this analysis proceeds is that they basically use a careful dual fitting to bound how large the potential is at the end of the first phase. And this basically looks somewhat like the online Steiner tree algorithm, except that the greedy step is a little bit different because every person is minimizing their cost share rather than just their connection cost to the current solution. So once you're able to bound the potential at the end of this first phase, though, then things are in good shape, because what happens in the second phase is that the potential successively decreases. And so the cost of the solution at the very end is no more than the potential at the very end, which is no more than the potential that you started out with at the end of phase one. Okay, so this o of log squared n is basically coming from the bound on the potential at the end of this first phase.
00:25:45.594 - 00:27:12.574, Speaker A: But the length of the second phase is not necessarily bounded because the potential decreases in very small steps sometimes. So this is what Charikar et al. Showed. And what we do in our work is to ask, well, first of all, what if you had departures in this process as well? How does that affect this analysis? And second, what happens if you allow these things to happen in some arbitrary interleaving of these different steps. What can you then say about the equilibria that emerge? Okay, okay, so um, what are some of the challenges with doing something like this? Well, first of all, it's very difficult to maintain any sort of structural invariant once you let, uh, people just make improving moves as and when they want. Um, in fact, uh, you know, I said before in response to a question that equilibrium states are always trees, but intermediate states of this process might look very strange with different cycles, paths of different terminals converging and diverging. Yeah.
00:27:12.574 - 00:28:12.958, Speaker A: What is an op if people are departure? I'm sorry, what is opt. Okay, very good question. So I stated before that in broadcast games every vertex has a terminal at it. So essentially what that means is that sometime in this process, process, a terminal is going to arrive at every works and potentially leave after that. So the optimum is going to be defined as the solution over the entire set of terminals or the minimum spanning tree. And when you allow for departures, it's important to compare against that opt and not just the minimum steiner tree over the remaining terminals because that could be very far off. So the cost of your solution is like when the person departs at that time, what was the cost? So we're going to compare the cost of the entire state at the end of the process against the minimum steiner minimum spanning tree.
00:28:12.958 - 00:28:56.092, Speaker A: So even if a terminal departs, the node stays in the graph and then we are allowed to include that in opt. Yeah. Any other questions? Okay, so that's one of the difficulties, that it's tricky to have any sort of structural invariant as the process goes on. And if we could just somehow argue that the potential stays bounded, as in chorica et al's analysis, then we'd be in good shape because improving moves only decrease potential. Um, departures only decrease potential. The only thing that increases potential is arrivals. Okay.
00:28:56.092 - 00:29:13.124, Speaker A: Uh, but then again it's, it's really tricky to argue about how much uh, the potential could jump up by for any arrival if you don't have any kind of structural uh, property on what the solution looks like at that point of time. Yeah.
00:29:13.244 - 00:29:22.924, Speaker B: So can we think of it as like there is an underlying graph which costs on edges and then there, there are no agents at the time zero. And then agents arrive, they sit on vertices.
00:29:25.024 - 00:29:42.044, Speaker A: Yes. So indeed. So there's some underlying graph and you'd like the agents to make their decisions based on what they see at the current moment and not other nodes that are not yet in the process. And that's indeed how our results proceed.
00:29:44.424 - 00:29:46.880, Speaker B: That's connected to a vertex that hasn't yet arrived.
00:29:46.952 - 00:30:38.698, Speaker A: That's right, that's right, yeah. And if edge costs satisfy the triangle inequality, then that's going to happen for arrivals. Okay, so let me describe our results and then we'll go into some details. Okay, so first we show that if the sequence of arrivals, departures and moves is completely arbitrary, then things can be really bad. So we can have equilibria that are poly in n far from the optimal solution. And in fact these examples, the bad examples that we have, look like a sequence of arrivals and departures in the beginning. And then once that sequence terminates, the system is at equilibrium.
00:30:38.698 - 00:31:30.288, Speaker A: So in fact there are no improving moves at all. So you could think of this as just a slight departure from what charker et al do, in the sense that you add departures to the first phase. And adding departures just makes things a lot worse. Okay, um, so that's, that's a negative result. And then you might ask well, how can we restrict this process to make the equilibrium better? So our next result says that if between every successive arrival and departure, we're allowed to make improving moves to bring the system to equilibrium. So in other words, arrivals and departures can only happen at equilibrium, then the worst case ratio with respect to opt decreases to o of log n. Okay, so this is even better than what Charker et al.
00:31:30.288 - 00:32:38.300, Speaker A: Get, and I put worst case here in quotes because this is, this does need a little bit, a little extra assumption, which is that um, between every batch of arrivals and departures, uh, the sequence of improving moves is going to be a carefully constructed, algorithmically constructed sequence of improving moves. So an algorithm suggests terminals one by one to make an improving move and that uh, that restores equilibrium. Um, but the ordering of arrivals and departures can be completely adversarial. Okay, so those are the results. And in the rest of the talk I'm going to focus on this positive result and I'll describe why this algorithmically generated sequence is necessary. Okay, so here are some of the key ideas that we need for this to work out. So I said before that if the dynamics is completely arbitrary, then one trouble is that um, intermediate states might not look nice like trees.
00:32:38.300 - 00:33:53.716, Speaker A: And so uh, we try to stay within the space of tree solutions, um, in two ways. So when, uh, uh, so if you have a tree solution at some point of time and some terminal wants to make a move, then we restrict that move to be of the form where this terminal takes an edge to another node in the tree, and then follows the tree path to the root. And then we also enforce that the entire subtree of at this terminal switches to the same new path. And so after the sequence of moves, the state is again a tree, and these are all improving moves. Okay? And then our main analysis consists of charging the cost of such a tree to a particular dual solution for the minimum spanning tree problem. And the key thing that we take care of is to make sure that every cut in the dual is charged at most once. One trouble with this is that this can be temporarily violated as we go through a sequence of improving moves.
00:33:53.716 - 00:35:02.344, Speaker A: And then basically what we show is that whenever this is violated, there exists some improving move that is going to fix this overcharging. So this is a little bit different from how dual fitting for online algorithms typically works, where you maintain some charging structure and you stay within that. Here we allow violating that invariant, um, but then fix it through improving moves afterwards. Okay, so a very quick, um, background on how dual fitting works for, uh, the minimum spanning tree. Uh, so observe that, uh, if this is your graph, and if you can find some number of nodes that are all pairwise distance at some distance from each other, then this gives a lower bound on the size of the minimum spanning tree. In particular, if you have k nodes that are each pairwise at distance of l apart, then the minimum spanning tree has to have cost at least l times k minus one. And we can certify that by drawing balls around these points, each of radius l over two, and these are going to be disjoint.
00:35:02.344 - 00:35:52.958, Speaker A: This is a way of constructing a dual solution to the MST. And in fact, you can construct the solution iteratively in a greedy manner. So pick a point, draw a ball of radius l around it, um, pick a point outside of that ball, outside of that ball, draw another ball of radius l around it, and continue doing this until the entire graph is covered. And then the sum of the radii of these balls within a constant factor gives a lower bound on opt. Okay, so this is the kind of dual that we will charge the cost of our solution to. And in fact, we will construct such a dual at many different distance scales, let's say powers of two. And it turns out that there are only logarithmically many distance scales of interest.
00:35:52.958 - 00:36:46.364, Speaker A: The rest are too tiny to be of interest. And as I said before, so every row here is a feasible dual within a constant factor. So the sum of the radii of all of the duals that we have here is, um, within a factor of log n of the optimal cost. Okay, so what are we going to do with this? Uh, well, uh, we're going to try to charge the cost of a tree to these duals and in particular we're going to make every node in this tree responsible for its parent edge. Okay, now this is different from the cost share of a node. The cost share of a node is the sum of its cost shares along all of the edges to the root. But for the point of view of charging, this node is going to be responsible for just its parent edge.
00:36:46.364 - 00:37:32.352, Speaker A: So suppose you have a node v and its parent edge has length roughly two to the I. We're going to find a row in these duals that is of radius roughly two to the I, or a little bit smaller than that. And then we're going to find the ball that contains v in this row and we're going to charge this edge to that particular ball. Okay. And if we can do this in a way that every ball gets charged at most once, then we get a bound on the cost of the solution. So what is the bad case here? The bad case is that you could have two vertices that have edges of roughly equal length, two different parents, and they're very close to each other. Okay.
00:37:32.352 - 00:38:08.246, Speaker A: And when that happens, the hope is that one of these vertices is going to want to make an improving move here. In particular, one of these vertices would want to switch to a path that goes to the other vertex and then follows its path to the root. And why is this a good idea? This is a good idea because the switching, there's some extra cost that you incur, which is tiny. And then by sharing these edges, hopefully there's enough of a saving in cost share that makes up for this extra cost here.
00:38:08.390 - 00:38:13.422, Speaker B: So you don't allow them both to do this simultaneously.
00:38:13.558 - 00:38:17.958, Speaker A: We're going to show that one or the other is a good idea. Both might not be.
00:38:18.046 - 00:38:21.014, Speaker B: The dynamic doesn't allow simultaneous changes.
00:38:21.054 - 00:38:24.134, Speaker A: That's right. The dynamics only allows one move at a time.
00:38:24.914 - 00:38:29.574, Speaker B: What's that I'm saying here? It seems important because otherwise they will both switch.
00:38:30.914 - 00:39:05.694, Speaker A: That's right. So one will switch at a time and then the other path goes away. So it's going to always be one node at a time. Yeah. Okay, so when does this not work? So if it's the case that there are lots of people on these blue pads, for instance, if there's a big subtree under u and another one under v, then um, v's goal is to switch to u so that it gets some extra saving from extra sharing along this path. But that's not going to happen if there are already lots of people. The extra saving is tiny then.
00:39:05.694 - 00:40:09.894, Speaker A: Okay, so basically what we show is that if one of these was a leaf in the tree, then you get enough, enough of a savings so that either u or v wants to switch. In particular, we can write down the new cost of v from switching to use path in this manner in terms of the old cost of u. And we can consider a similar switch for u and write down its new cost in terms of the old cost of b. And if you add these up, you see that either u improves its cost or v improves its cost. Okay? And that's the idea here. So as long as one of these guys is a leaf, and when you are in this bad situation, one of them is going to want to switch and correct that overcharge on the dual. Okay, so what does this look like overall? So we have some charging of the dual, where at equilibrium, where every dual is being charged at most once, and then some arrival happens and creates an overcharge.
00:40:09.894 - 00:40:46.734, Speaker A: Um, and then this overcharge is fixed by moving one of these nodes, making an improving move. But this only moves the overcharge potentially to another dual. And then you move one of those nodes and this shifts the overcharge again. And we continue doing this until everything, every dual is again charged at most once. Okay? And this might not be an equilibrium, but you can go through this process until it converges to an equilibrium and the potential always decreases. So eventually this process converges. Okay, so we can do this more generally when there's a batch of arrivals and departures.
00:40:46.734 - 00:40:57.978, Speaker A: And that's the basic idea, if there are, if a dual is getting overcharged, we can fix these overcharges. Okay, so to summarize, yeah, sorry, I.
00:40:57.986 - 00:41:10.166, Speaker B: Was trying to get some naive intuition about why the log nice. Is the following true? If all the age weights were one, any tree has cost at most log n times solved, isn't it?
00:41:10.190 - 00:41:31.764, Speaker A: Because. So if all of the edges were of cost one, then we'd only be worried about a single dual, and actually we'd get a constant factor cost. So the log n comes from having to look at log n, different distance scales and edges of different lengths.
00:41:32.704 - 00:41:40.000, Speaker B: All the costs are within a constant factor. Just by counting, I mean, I agree that you get counting parts, you get log n, right?
00:41:40.072 - 00:41:43.912, Speaker A: That's right. But in that case, we can actually get a constant, is what I'm saying.
00:41:43.968 - 00:41:45.896, Speaker B: Yes, there's a constant factor.
00:41:45.960 - 00:42:31.664, Speaker A: That's right, that's right. Yeah. So the log n here is coming from the different lengths of the different edges. There could be a log n, different scales for these lengths and that's where that comes from. So, to summarize, this work gives an analysis for a min cost spanning tree algorithm under this restriction that it needs to respect the agent's preferences. And an arbitrary sequence of moves looks pretty bad. But if we insist on restoring equilibrium after each arrival or departure, then we can bound the cost of the equilibrium that emerges.
00:42:31.664 - 00:43:06.704, Speaker A: And I said earlier that we don't know of any way of arriving at an equilibrium within polynomial time. So indeed, the process that I described could also take a long time to converge between every sequence of arrivals and departures. Okay, so, and it's also open whether we can further generalize in what order people can make improving moves. Okay, so that's it. Questions?
00:43:12.684 - 00:43:25.158, Speaker B: Is there any sense in which your tree is sort of consistent with a greedy tree? That is, you wouldn't use a higher cost edge if you have a lower cost edge which doesn't form a cycle with lower edges.
00:43:25.286 - 00:44:04.438, Speaker A: Yeah, so the greedy online Steiner tree algorithm tries to find the cheapest connection to the existing tree, whereas this one, every agent is going to look at not just the cost of this cheapest connection, but also how much its cost share is along the path to the route. And so those choices could be quite different, actually. And part of what the charika et al. Analysis does is to show that they're not too different in some sense, whereas here we don't have as much structure as the process goes on. And so that's not necessarily true, that those two choices are going to be similar.
00:44:04.606 - 00:44:07.274, Speaker B: It is sort of approximately true.
00:44:09.094 - 00:44:23.854, Speaker A: Yeah. Yes, in some sense it is true. You can bound the, you can write down inequalities that describe the increase in potential, essentially, or the cost share that the agent faces.
00:44:27.994 - 00:44:31.898, Speaker B: So it seems like all the analysis hinges on a proportional cost sharing mechanism.
00:44:31.986 - 00:44:33.202, Speaker A: That's right, yeah.
00:44:33.378 - 00:44:43.154, Speaker B: I mean, if you relax the requirement that you be budget balanced to revenue adequate, is there a reasonable revenue adequate mechanism that could improve the worst case? Nash?
00:44:44.854 - 00:45:05.914, Speaker A: That's a good question. I haven't thought about it at all. There is some work on people looking at other ways of sharing cost so as to lead to better equilibria, but these are always budget balanced. So your question, I believe, is whether you can move away from budget balance and do something better. And I'm not sure.
00:45:08.964 - 00:45:16.104, Speaker B: So, regarding your last open thing, if I wanted to find a good equilibrium. Is that known to be hard?
00:45:18.524 - 00:45:28.844, Speaker A: I don't think that a hardness result is known. Any questions? Okay, so, time for lunch.
