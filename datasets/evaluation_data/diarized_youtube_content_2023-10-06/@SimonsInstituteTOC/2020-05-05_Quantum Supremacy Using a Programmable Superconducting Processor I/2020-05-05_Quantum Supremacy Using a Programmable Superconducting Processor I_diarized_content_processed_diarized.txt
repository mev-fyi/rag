00:00:00.200 - 00:00:49.124, Speaker A: Excellent. So, yes, I'm very excited to be here. It's an extremely exciting workshop and very exciting day, and I think that's because, in particular, we're at a very exciting period in sort of the history of quantum computing, where we're seeing this sort of unprecedented convergence of experiment and theory. And this is sort of what this workshop is all about. And in particular, I think one of the best examples of this just happened at the end of last year, was this quantum supremacy experiment that Google published late last year. And we're really happy to have today one of the leads of that experiment from the Google team, Sergio Boisio here to tell us more. So, Sergio, let me pass it to you.
00:00:51.184 - 00:01:14.116, Speaker B: All right, thank you. I guess I will start my presentation. Let me share my screen. Okay. Is that working? Yeah, I can see my screen perfect. Yeah. So thank you very much for the nice introduction, Bill.
00:01:14.116 - 00:02:21.814, Speaker B: So I'm going to talk a bit about this experiment that we performed last year, and the paper that we published is called quantum supremacy, using a programmable superconducting processor. So, let me see. Okay, so this is the team working on this experiment, pretty much everyone at Google, a lot of experimentalists, and a frame of theorists as well. So, just to give a very basic overview of what we were trying to do in demonstrating quantum supremacy, we choose a computational problem, which is just to sample that distribution of a random quantum circuit. And then we compare the best quantum strategy, which is just to run the quantum circuit and a quantum processor with the best classical strategy available, which is to to sample the same circuit using a supercomputer. And to sample the same circuit, we actually have to simulate the circuit. That's one of the reasons why we choose this particular problem.
00:02:21.814 - 00:03:16.742, Speaker B: And then we compare the processing time on the experimental quantum processor, which was sort of order of 200 seconds to collect 3 million beta strings for fairly large sample against the classical processing time on a supercomputer. And once the classical processing times becomes unfeasible, at least in practice so far, then we decide that we have achieved quantum supremacy. So that point is when we publish the paper. So I will start first by talking a little bit about the complexity theory underpinnings of this experiment. As Will said, this was a nice mess of experimental theory. So, to sample random quantum circuits, we only know how to do that with classical computers. We actually estimate the probabilities.
00:03:16.742 - 00:04:19.684, Speaker B: We don't know of any other ways to simulate the sampling, but to actually simulate the circuit and estimate probabilities and the computational cost, at least for 2d circuits, I'll talk a bit about this, because this has changed a bit lately with some new papers. But on 2d circuits, the computational cost is proportional to the fidelity. And it turns out that if you have a polynomial classical sampling algorithm, then this implies that you could estimate the probabilities using an empirical. So, if you say, well, assume I have some other simulation method that doesn't actually calculate probabilities. If this is a classical algorithm, it will have random bits. And if you have random bits, you can use an empirical to sort of count how many assignments of the random bits give you a particular. And that will imply that with an empirical, you can estimate probabilities.
00:04:19.684 - 00:05:10.020, Speaker B: In contrast. Well, just a detail that this also holds for globally unbiased noise. So, if you think that the noise in your algorithm or your processor is globally unbiased, even if you have a lot of noise, you can still estimate these probabilities with an empirical polynomial time. In contrast, if you're actually sampling with a quantum computer, there are no random bits anywhere. It's just intrinsically quantum randomness. So, even with an empirical, you cannot estimate probabilities if you can sample. So this difference between, let's say, classical random bits on a classical polynomial algorithm and intrinsic randomness in quantum sampling without any classical random bits or any random bits of any sort, allows you to try to do a separation in terms of complexity theory.
00:05:10.020 - 00:06:17.240, Speaker B: And this is related to this famous collapse of the polynomial hierarchy. So that's sort of the underlying difference between classical and quantum sampling for this problem. So, what do we know about the complexity, theoretical complexity of sampling random circles? Well, we know that estimating one probability is Sharpie hard in the worst case. So it's something that we cannot do even with an empirical is worst case sarpy hard? Now, calculating probabilities exactly is also sarp in the average case, and I think Bill will talk about this later. What we don't know is if the average case estimation of probabilities is also hard. And if that was hard, that would imply that random circuit sampling is theoretically hard. So that's the result that we're missing.
00:06:17.240 - 00:07:24.732, Speaker B: We know exact average harness, and we know worst case approximate hardness, but we don't know average case approximate hardness. Nevertheless, we conjecture that this is still hard. So that's a good motivation to use this particular computational task to prove quantum supremacy. And I will nevertheless mention that without error correction, the fidelity decays in an experiment exponentially in the number of gates, not just the number of qubits, and therefore, asymptotically, we're not going to be able to sample or asymptotically. Put in another way, asymptotically, classical algorithms, in terms of complexity, will win, because classical algorithms scale exponentially in the number of qubits. And with atoral correction, quantum sampling decays exponentially in the number of gates, which is a bigger exponent than the number of qubits. So once we set ourselves to work on these computational tiles, we need to choose a fear of merit.
00:07:24.732 - 00:08:09.828, Speaker B: And what we develop is what we call cross center prevention marking, which is really an estimate of the fidelity. So we're going to be running random circuits. We want to estimate the fidelity. This works well in previous proposals, like randomized benchmarking. It's also related to quantum chaos, in the sense that if you have any discrete errors, you will expect the trajectories without errors, and with errors to sort of diverge maximally in some sense. So this will allow us to detect the fidelity, because we're sensitive to basically all the errors. We're going to calculate the fidelity for a full circuit with up to 53 qubits.
00:08:09.828 - 00:09:03.662, Speaker B: So this is system fidelity, not just the fidelity of two qubit gates, and it's a good measure to see if everything is working. So, this is one of the reasons why we choose this experiment, and we focus on this experiment for quite some time, especially in the experimental side, is that it allows us, or it forces us to make sure that the whole system is working, not just isolated to qubit gates, but measurement, simultaneous two qubit gates. It has to be fast, et cetera. So it's a good benchmark, moving towards fault tolerance condition. The downside is that this particular estimator of fidelity requires to actually calculate the ideal probabilities that you will expect for a given random circuit. And that requires simulating this random circuit, which is exponentially hard in the number of qubits. And in practice, it just becomes very expensive.
00:09:03.662 - 00:09:54.354, Speaker B: So, once we get to the point where we cannot calculate these probabilities anymore, but we can still sample with a quantum computer, and we extrapolate, and we see that the fidelity is good. Nevertheless, we do more things than extrapolation, then that's when we decided, well, this is what we meant by quantum supremacy. Nevertheless, there are other things you can do that allow us to calculate the fidelity of the whole system, such as patch XCV, which I will mention later. But it's basically you cut the circuit in half, and we see experimentally that if you remove all the gates connecting the two halves, you get a fidelity. In our experiment, which is very close to the fidelity of the original circuit. So this is much more efficient to do because now you have two disconnected circuits and allows you to calculate the fidelity for fairly like systems. So let me explain a little bit.
00:09:54.354 - 00:10:57.142, Speaker B: What's the theory or what are the assumptions behind this estimator of fidelity, which we call cross entropy metric? So the first thing that we can do is, well, we write the output density matrix as a mixture of the ideal output with some depolarization PWT p, and then some noisy noise density metrics where we simply craft the effect of all the errors. If you want to think in terms of quantum trajectories, you're going to have discrete errors. If it's in a stochastic map, it's even simpler. You're going to have poly errors basically. So you connect all the trajectories with errors in this operator, and we're going to define observables that are going to depend on the ideal probabilities. This is the part that is expensive, because we need to run this expensive classical simulations to calculate these numbers. So we calculate for some output bit of string q, what is the ideal probability to measure q.
00:10:57.142 - 00:12:29.624, Speaker B: So r observable is going to be some function of these ideal probabilities. And then what happens is that if we try to estimate, what will the observable be for the noisy operator? Noisy density metrics chi, the one where we keep accumulating all the trajectories with errors, then, well, this is just the expression that we got. We have these probabilities from the noisy operator and this function of the ideal probabilities. And now we're going to make the assumption that because this is a random shortcut, the noisy probabilities are going to be uncorrelated with the ideal probabilities. And if they are, they're going to average to one over two to the n, or one over d, where this is the dimension of the hilopert space. And if this assumption is correct, then in the sum here we can use the central limit theorem because we're summing some values with sampling, which is uncorrelated, and what we get is just the mean right with some fluctuations that go like one over the square root of the number of terms in the sum, and we're summing in this particular expression over all the different probabilities. So from the central limit theorem, if you want, we spat that the value of the observable on the noisy operator is actually statistically independent on the details of the noise.
00:12:29.624 - 00:13:38.320, Speaker B: So we know what the value of the observable noise operator is, and that will allow us to estimate the fidelity. A slightly more formal and general way to say the same thing is that we're going to assume concentration of measurement. We're going to assume that for our operator that we use to estimate the fidelity if we estimate the statistical period, because this is a random circuit, even with discrete noise, but this is still very random. From concentration of measure, what you expect is the average of the Hilbert space. So Gilbert space is a very high dimensional sphere, if you want, plus fluctuations that are like one over the square root of the dimension of space. This is a fairly generic result. Concentration of measure in the case of the sphere scolatis lemma has been used before several works, I think by Jens, for instance, who's going to talk later in quantum computing, and it turns to this kind of like a geometrical equivalent of the central limit there.
00:13:38.320 - 00:14:24.878, Speaker B: So as long as the concentration of measure works, then we don't really care about the details of the noise. We know what the observable will give us when applied to the noise. So that's really all we need, either this concentration of measure in the geometrical sense, or just the central limit theorem. If the probabilities from the noise are uncorelated. And once you have that this was again are observable, which is just a function of the do probabilities, we know that the observable applied to the noise is independent of the specifics of the noise. So we know what the value is without knowing too much about the noise. And then the tricky part is to design an observable which actually gives you a different signal for the ideal output.
00:14:24.878 - 00:15:20.490, Speaker B: So, you know, concentration of measure is very powerful. If we don't design our observable carefully, the observable will also concentrate for the DL output. So it will give the same value for the DL output as for the noisy part. And that's why we need to calculate the ideal probabilities. When we define an observable, which is a function of the ideal probabilities, is if you want an observable or, you know, function of the ideal probabilities is very specific to the output state that we expect in the DL case. So that's how we avoid concentration of measure in the DL case. So then we're going to have some value here that we can tune because we choose observable for this very specific state, some value here which is generic and that allows us to solve for the depolarization fidelity, or which is, you know, for enough, high enough dimension is just the fidelity, and that's what we do.
00:15:20.682 - 00:15:22.498, Speaker C: Sergio, question?
00:15:22.666 - 00:15:23.614, Speaker B: Yes, please.
00:15:24.084 - 00:15:31.748, Speaker C: Can you go back one slide? So how do you verify this concentration of measure that is part of the experiment? You verify this or.
00:15:31.836 - 00:16:35.610, Speaker B: Right? Yeah, that's two slides on the road. We verify this numerically. So what we do, basically, we look at assume you have an stochastic map, which is the simplest case, and it's quite reasonable if you have random circuits. Then what we do is we calculate the observable for a circuit with just one poly error, and then we compare with what we expect from concentration of measure, which is just the average with a totally mixed state or the average space. And then we see that in this, the difference between what we obtained for our fidelity estimate, or let's say in this case it was logarithmic cross entropy, and the average value is exponentially small. So indeed, it goes like one over the square root of the dimension for every poly error. So in this particular plot, the s axis is the depth of the circuit.
00:16:35.610 - 00:17:15.625, Speaker B: So you expect concentration to work once you randomize enough, once the depth of the circuit is large enough. And the y axis is basically the difference between these two values as measured by. And the dash lines here is one over the square root of the dimensions for different circuit size. And we're plotting here the median, because there are outliers. For instance, if there is a zero just before the measurement, and you measure on the z basis, that zero doesn't do anything. So, taking the average, because you're trying to look at an exponential quantity, the outliers change the average too much. But if you look at quantiles, you see that the median indeed converges to one over square root of d.
00:17:15.625 - 00:17:29.593, Speaker B: And the error bars here are quartiles. And you see that, you know, most of the errors, typical errors, do give you a concentration of measure. And we. Yeah, and we look at this numerically.
00:17:29.633 - 00:17:47.474, Speaker C: Basically, this is numerical. This is not an experiment. Of course, there is somebody who's raised their question, raised their hands, and I'm just looking at, oh, yeah, Benjamin Obert. Does he want to ask a question? Okay, maybe not.
00:17:47.814 - 00:18:15.314, Speaker B: Yeah. What we did experimentally is we check for two qubits, for instance, that where we can do randomized benchmarking, that indeed, what we get out of cross entropy or these estimators, is the same as what you get with randomized benchmarking. That was experimental. I think Julian will talk later this week about that. And also, we look at measurement errors. For instance, you know what happens if you only have, if measurement errors dominate. Yes.
00:18:15.314 - 00:18:19.674, Speaker B: Do we get the correct measurement errors? And that also works experimentally.
00:18:21.054 - 00:18:21.834, Speaker C: Thanks.
00:18:22.774 - 00:18:56.056, Speaker B: Yeah. Thank you. That's a very good question, because indeed, this is a very important assumption for us. So we check it numerically and experimentally with two qubits when we can compare with randomized benchmarking. So these are the specific observables that we choose. We started working with this cross entropy estimator, logarithmic cross entropy, because that's the more standard form of entropy, and it was inspired by machine learning, if you want. So then this is how you will get the experimental fidelity, p D.
00:18:56.056 - 00:19:21.620, Speaker B: Here is the dimension of Quilbert space, which we know, of course, because we know the number of qubits. And gamma is just the Euler constant. So it's just a number. And the average here is average over the beta strings that you sample. So you sample, in our case, up to 3 million bit strings. You have to calculate these ideal probabilities. You take the logarithmic value of the ideal probabilities and you just average over the probabilities that you sample, and that allows you to get p.
00:19:21.620 - 00:20:29.474, Speaker B: The bar denotes you also can average over circuits. So we did, in the largest experiments, ten circuits with 3 million bit of strings each, and we average all of them. And that's the best estimator we have of the actual experimental fidelity. There is also actually in the experiment, we do use this other form, the linear cross entropy, which we call a still cross entropy, because it's related to this general idea of using concentration of measure for an observable that depends on the deal functions. And it's also called linear cross entropy in some other branches of machine learning. But what it really is, is just a max likelihood estimator of the depolarization fidelity p if you assume a totally depolarizing channel. And this version, the linear cross entropy is very related to the probability of heavy output that Scott introduce introduced, which is using quantum volume, which is another measure of how well you're doing with large circuits in experiments.
00:20:29.474 - 00:21:12.452, Speaker B: So what we actually did in the experiments is we tried both of them and actually even the heavy output probability, and we checked that all of them give you again the same fidelity, because basically the assumption of concentration measure applies in all three cases. You can define different observables. The reason why we end up preferring linear cross entropy is because it has a smaller variance. Okay, so we check the concentration of s four numerically. We just went through these. And this is how the algorithm looks like for these quantum supremacy experiments. So we start by choosing some random circuit we sampled around a million times in the experiment.
00:21:12.452 - 00:21:47.864, Speaker B: It was more like 3,000,400 circuits. So we sampled from the, from Maxwell experimental processor. We calculate the ideal probabilities for the circuit. So this is the expensive simulation part. And, for instance, once you have the ideal probabilities, you can calculate the cross entropy or the linear cross entropy. And that's our estimator of fidelity. And the goal is to be able to do this with circles that are large enough so you cannot calculate the probabilities anymore, or not the probabilities in the sample.
00:21:47.864 - 00:22:20.240, Speaker B: That, again, is what we call supremacy. So this was the chip that was used in the spearman. It's called sycamore. It has 54 qubits. 53 of them were using the experiment, basically because one cable wasn't working when they called the chip. And then the chip worked so well that they didn't want to warm it up anymore until we finished the experiment. But it has 54 qubits, and it also has 196 control knobs.
00:22:20.240 - 00:22:46.188, Speaker B: And one important feature of this chip is that the couplers. So here in this picture, the gray crosses, those are the qubits, which are superconducting qubits, flavor of transmons. The blue boxes, those are couplers. And the couplers are controllable. In this chip, they are adjustable couplers. They have an independent control knob. So we found out we have to use this chip.
00:22:46.188 - 00:23:34.644, Speaker B: We started with a different chip that didn't have controllable couplers, because that allow us to tune the couplers, the coupling between qubits very close to zero. And that's the way we could address crosstalk with our previous chief resale con that had more qubits, less controls, but more qubits. It was very hard to get rid of sufficient crosstalk for this payment to work. So, as I said, one reason why we focus on this experiment is that it gives us a measure of system fidelity. It was a very good system engineering goal, and that's one of the results of trying to do this. We moved to an adjustable coupler design, which we think is actually probably needed for fault tolerance as well, because you also want to remove crosstalk if you want to build a photo and quantum computer.
00:23:35.384 - 00:23:55.304, Speaker D: Sergel? Yes, there's a question from the, from the audience out there. This questioner Jonas Helson, asks, I was wondering how sensitively the value of XEB depends on the details of the random sampling of the circuits. That is, do they need to be perfectly har random?
00:23:55.804 - 00:24:37.170, Speaker B: Yeah, that's a good question. They are not perfectly high random, because for two qubits, you can do perfectly high random circuits, but not for 53 qubits. The depth to be perfectly horrendous will be exponential in the number of qubits. So I guess in principle, you just needed to be able to design. And there has been indeed a lot of interesting work on trying to see what depth do you need for it to design, even theoretically. Maybe Jens will talk about it later. In practice, we checked it numerically, you know, so up to even after 53 qubits, we could do numerics for circuits that are not the hardest circuits.
00:24:37.170 - 00:25:33.524, Speaker B: And we basically check that the distribution, the output probabilities, look random enough. The simplest check that we do is if you have a hard random circuit and you look at the distribution of the ideal probabilities, the distribution of the ideal probabilities itself is exponential, which is called, in this context, a porter Thomas distribution, because it's related to quantum chaos. So we check that we have enough depth to get a porter Thomas distribution. And we also check other statistical properties of the distribution of ideal probabilities, like the inverse participation ratios. And so it would be great if we could prove that we have a two design, which is probably good enough, but in practice, we just do it numerically until we're confident that we have enough depth. But they are not perfectly horrendous. You just need it to be random enough for concentration of measure to work.
00:25:33.524 - 00:25:41.084, Speaker B: And I think concentration of measure tends to work fairly well. I think two design is good enough for concentration of measure, probably.
00:25:41.584 - 00:25:42.484, Speaker C: Thank you.
00:25:44.104 - 00:25:47.484, Speaker E: Hi, Sergio, I wonder if I can ask you very quickly.
00:25:47.824 - 00:25:48.232, Speaker B: Sure.
00:25:48.288 - 00:26:00.044, Speaker E: So is there something more you can say that would be understandable to a theoretician about how these controllable couplers help you, help you deal with crosstalk?
00:26:00.624 - 00:26:27.472, Speaker B: Right? It's basically, yeah. So the difference is in standard super, I mean, you need to couple qubits, obviously. Right? So in most supercomputing qubits platforms, the qubits are always coupled. There is always some capacitance or inductance between, between the qubits. The qubits are these superconducting circuits. There is always some inductance, for instance. So that means there is always a coupling between them.
00:26:27.472 - 00:26:59.564, Speaker B: Always. So what you do if you don't want to have the coupling is you basically turn the qubits of resonance. You, you know, the coupling can be, I don't know, like 20 MHz. You set them if you can, like a gigahertz apart. So they have resonance. And in the rotating wave, for instance, you know, like the company is perturbatively small and you sort of ignore it, but it's always there. Now, the problem is, when you want to do gates, then you start, you have to move the qubits close to each other.
00:26:59.564 - 00:27:36.496, Speaker B: And when you move to qubits close to each other, so, you know, they are not of resonance anymore. And this coupling that is always there becomes effective. Then they tend to get also close to other qubits in the neighborhood because, you know, they start colliding in frequencies, so you get more crossed out. The difference with these adjustable couplers is the qubits are now always close in frequency. And there is no, you know, the inductance that you have in the middle is sort of an adjustable inductance. So you can actually turn the inductance off. And that's complicated to do.
00:27:36.496 - 00:28:08.724, Speaker B: They are actually, the couplers themselves are sort of qubits also. They're sort of transmons. They are just working on a different regime. And then what you do is you just kind of turn the transmon in the middle of resonance without actually having to move the qubits around. And that gives you, you know, more liberty to make sure that you don't introduce, you know, unwanted coupler with other neighboring qubits. So it's kind of an adjustable inductance. Basically, you built a circuit which is an inductance that you can adjust and you turn it to zero.
00:28:08.724 - 00:28:45.814, Speaker B: Okay, thanks. Questions? Um, so let's see. Right, so these are sort of experimental results at the level of two qubit gates. So the black line here is single qubit error rates. And this is integrated histogram. So you can see the error rates of all the qubits in this particular chip, all the 53 qubits. And you see there around 0.15
00:28:45.814 - 00:29:29.182, Speaker B: poly error rate. We think it's better to quote poly error rates, which is a bit higher than one minus average fidelity, which is the standard number that is quoted. But the polio error rate is what extends to larger circuits. You can apply the poly error rate to any is arbitrary of the dimension of the system in which you measure it. So it's pretty good fidelity. And simultaneously, this is very important, is when we do single qubit gates in all the qubits at the same time. So you see that the simultaneous rate is very close to the isolated rate.
00:29:29.182 - 00:30:03.250, Speaker B: So even if you're doing single qubit guys in all the qubits at the same time, you are not introducing more errors because of crosstalk. Now, with two qubit gates, it's a bit different. The isolated polar rate is 0.36, which is also pretty good. And this is for a particular gate, which we call sycamore, which is close to an iswap. The two qubit gate, when you measure it simultaneously. So that means you are doing as many two qubit gates as you can across the chip.
00:30:03.250 - 00:30:34.670, Speaker B: So every qubit participates in a two qubit gate. The error rate doubles. So yeah, we have these adjustable couplers. That removes a lot of cross style, but not enough, because if it was exactly, you know, if all the crosstalk was completely off, then simultaneous and isolated will be the same error rate. So it's double, but it's not too bad. It's still good enough to run the larger circuits. And then the readout error in this particular chip wasn't very good because of some design problem.
00:30:34.670 - 00:30:55.834, Speaker B: This has science improved. And it was around 3.8% readout error when measuring all the qubits at the same time, which was fairly high. So this is how we run random circuits. We basically do simultaneous gates on all qubits. It's kind of a general purpose algorithm. I'll talk about this more at the end.
00:30:55.834 - 00:31:41.424, Speaker B: And we just run sequences of single qubit gates and two qubit gates for some number of cycles, where a cycle is two qubit gates in all the qubits and single qubit gates in all the qubits. So the single qubit gates take around 25 microseconds. They are microwave gates and the two qubit gates, they are very fast. That's another advantage of using these adjustable couplers. When you can turn it on, you can actually turn on a fairly large amount of coupling. So the two qubit gates take only twelve milliseconds. So this is the experimental results for some circuits that we call verifiable circuits.
00:31:41.424 - 00:32:34.390, Speaker B: So, the x axis is the number of qubits. The number of cycles was always 40 in these data sets. So 28 cycles, you count single antiquit gates. The first interesting thing to note is that we measure the poly error rates in simultaneous node. That was a couple of slides ago. And if you just multiply the probability of not having an error, where you get the poly error rates from the simultaneous polyurethane measurements, then you get this line, the black line, and that tends to aligned really well with the fidelity that we measure with percent of the benchmarking for the full circuit. So these particular circuits turns out to have some hidden symmetry.
00:32:34.390 - 00:33:14.926, Speaker B: That's what we call them verifiable circuits. So we were able to actually perform cross center image marking for the full circuit, 53 qubits at 40, that was around 5 hours with 1 million cores. So it's 5 hours, but, you know, a lot of cpu hours, 5 million cp hours per circuit, and we did ten of them. So this was more than 50 million core hours. To get the red circle there. Here some of these circuits with 42 and 43 qubits that were done in the Julic supercomputing center. These are the up to 38 qubit.
00:33:14.926 - 00:34:00.324, Speaker B: You can do it in a workstation if you have up to four terabytes of ram, that allows you to do 38 qubits. So those are the red circles, just the crescent of image marking of the full circuit. Now, if you cut this cut in half, as I mentioned at the beginning, then the cross entropy is easy to do because you only need to calculate the probabilities on half as many qubits. So half 54 is still 27. That's a circuit which is easy to simulate. And what is interesting is that even though now the circuit is cut in half, so you don't have entanglement between this, there is a lot less complexity if you want. This is easy to simulate.
00:34:00.324 - 00:35:12.656, Speaker B: When we cross entropy for this on the left and the circuit on the right, and then we just multiply fidelities, we get these blue crosses, which align again with the red circles and with the estimate from the discrete error model. So what this tells us is that basically the fidelity that we're getting is sort of independent of the amount of entanglement and the complexity, if you want, of the particular circuit that we're running. So that's kind of the best case scenario, you know, aligned with the check that the discrete error model is working when you measure the errors in the simultaneous cross entropy. So this is good sign for future fault tolerant roadmap, at least to the level of sensitivity of these experiments. Of course, you need to do experiments with more sensitivity to measure cross style at smaller levels, but to the level of sensitivity of this experiment. This is promising. And then we do something similar with the lighted circuits, which is something in between, where we only remove some of the gates across the cut.
00:35:12.656 - 00:35:49.936, Speaker B: And it turns out that with this algorithm that was playing in a minute, this makes the simulation a lot easier. And again, everything aligns. So these were the circuits that we could verify because they have this hidden symmetry. Actually, what happened is we thought initially these were like our supremacy circuits, but then in the process of checking that they indeed could not be simulated. We found this symmetry by looking sort of decompositions of gates close to the cat and things like that. And it turned out that we could simulate them. So one nice consequence of that is that we could do this full circuit.
00:35:49.936 - 00:36:23.814, Speaker B: And the downside is that for a while we thought we were not going to get supremacy because we could simulate these circuits. But then we found out how to remove the symmetry by sort of changing how we do two qubit gates across the cat. And those are what we call now the supremacy circuits. And what we did at that point is we were now interested in new supremacy because we already had this data. Cross entropy seem to work well. The digital error model seemed to work well. So we just took circuits at 53 qubits at different depths, all the way from twelve to 20.
00:36:23.814 - 00:36:53.584, Speaker B: The black line again is the discrete error model. And we do Eli that patch XCB. So patch XB again is you just cut the circuits into halves. It aligns well with what the discrete error model predicts. But we couldn't check the full strings anymore. We pull s all the data. If you go and look for a paper in nature and look for the link to the data, you can download the bit strings for all the circuits on all the full circuits that we have not verified.
00:36:53.584 - 00:37:38.982, Speaker B: So this is, you know, sort of a nice check for the future. Whenever algorithms or hardware improves enough. So you can actually do this full cross entropy, then you can check if it aligns with the rest of the circuits or not. Okay, so let me tell you a little bit more about the algorithm that we use in the simulations beyond 38 qubits, including these 50 mah, or these circuits with 53 qubits and 14 cross entropy cycles. So it's what we call answering a Feynman hybrid simulation. And basically what you do is you have these two qubit gates in the cut. Remember, in patch XCB, we just remove all the gates in the cut.
00:37:38.982 - 00:38:46.566, Speaker B: Now we want to simulate the circuit with two qubit gates across the same cut. So to sort of separate them for the simulation, we just do a smid decomposition of the two qubit gates across the cat, which will look like this, and it will be a sum of gates that go on the left and on the right, and the sum is weighted by this mid coefficient. Now this will be, you know, for one two qubit gate, you have force coefficients for this particular two qubit gates that we use, which are related to isocates. If you have many two qubit gates in the CAD, then you basically have to sum the products of all the possible Smith decompositions of the gates across the cut. So you need to perform this sum, and the number of terms in the sums goes. Goes like, well, it's exponentially the combination. Well, it's the number of Smith coefficients that you can choose, you know, possible combinations.
00:38:46.566 - 00:39:36.534, Speaker B: So it's exponential in the number of two qubit gates across the cat. So you have to do a lot of simulations, but every one of them is now easy to do, because for every one of these choices of smitty composition, for the two qubit gates across the CAD, you just have circuits with 27 qubits, which you can do efficiently. So you end up doing millions of simulations, but every one of them is kind of easier. And that's how we did these 53 qubit simulations with that for it turns out that if you don't need, if you're not trying to do cross entropy, but just sort of simulating sampling, then you only need to sum a number of terms in this sum proportional to the fidelity that you want. So, for instance, in the sprayman, it was around 0.2% fidelity. So if you only sum 0.2%
00:39:36.534 - 00:40:45.764, Speaker B: of the paths in this sum, then that will be equivalent computationally to what the quantum computer itself is doing. Now, for 1d circuit, this is for 2d. For 1d circuits are actually efficient, noisy simulations. These are some new results which you can do with matrix product states, but for the at least asymptotically, you cannot do this. So, in any case, this is the technique that we use to calculate decorous entropy for 53 qubits that 14. So, this is the data in the supplement of our paper where we basically compare the best algorithms we could run with cost of sampling in the quantum computer. So, one thing that you can do is if you can start the full wave function, well, then that tends to be fairly efficient.
00:40:45.764 - 00:41:22.184, Speaker B: But at least using ram in the summing supercomputer, you can only get around 48 qubits or something like that. We were using 53 qubits. Now, there is some proposal to use the disk. So that would be interesting to see if it actually works. And I guess our philosophy here, which I will comment later on, is we wanted to compare an experiment on a processor with actual computational experiments. So we didn't consider this, because nobody has run simulations of this size in this ever. So it will be interesting to see if you can actually do it.
00:41:22.184 - 00:42:36.602, Speaker B: And then we use, after we run out of these, which are the simulations that people actually do in practice, then we do this Rodger five mana algorithm, which has smaller memory requirements, but the computational cost is now exponential on the number of gates across the cut. So what happened is that for the circuits that we could actually run if we just extrapolate from simulations that we perform, so we don't do the full simulation, we don't do all the paths, but we, we do a subset of the paths, and the cost is linear in the number of paths, because there is no communication between different paths. Then we extrapolate like 600 years for Dev 16 and all the way to 10,000 years for Dev 20 using this algorithm. So, of course, algorithms will improve. That's kind of what we spat. They've been improving for the last years. But we think it's important to actually perform the experiments, computational experiments as well, you know, because it's hard to really find out what's the actual runtime on an algorithm when you run it in practice.
00:42:36.602 - 00:43:31.414, Speaker B: And, for instance, it's not clear if you can run using the fault disk of a supercomputer, because it has never been done before. And then in the meantime, we keep increasing the number of qubits. So we hope to be, we hope that, you know, we have cross supremacy. Eventually, algorithms will catch up, probably, but we will have a better processor. And we think that this sort of a transition, once we cross supremacy, the experiment will improve faster than classical algorithms and classical hardware. Now, in terms of classical algorithms, there are, there has been some other new advances which I want to touch upon. So another method that we didn't use for most of our computations in this particular paper is these, what we call Feynman algorithms, or tensor neighbor constructions.
00:43:31.414 - 00:44:24.354, Speaker B: So we work on these, but it just didn't work for these very large circuits in our implementation. So what you do is basically you have a quantum circuit, and this is the standard representation of a quantum circuit, where the boxes are single qubit gates and two quid gates. And you can think of one of these quantum gates as a tensor. So you map the circle to some sort of tensor representation or graphical model, and then you basically want to contrast all these tensor operations. Now, synthetically, we know that the cost of the tensor contraction is going to be exponentially. Neither the minimum, well, if these are to the circuits. So in the minimum of either the depth and the lateral dimension, which is the number of qubits, or if you have enough depth, then you just contrast, sort of as you will do, the wave function simulation.
00:44:24.354 - 00:45:01.974, Speaker B: So it's just exponential number of qubits. Good. And what happened recently is Johnny Gray has been improving algorithms to find out what is the optimal contraction ordering. So, the tricky part about this, performing these Feyman simulations, or tensor network simulations, is that you have to contract all these indexes. And the cost of the simulation itself is very dependent on how smart you are about what indexes you contract first. So, this is a difficult problem. Johnny Gray has improved significantly.
00:45:01.974 - 00:45:52.124, Speaker B: What are the heuristics for this problem? So, these are plots from his paper, and in particular for Sycamore 53. So, for dead circuits in an experiment, our best contractions were basically the ones we used are these yellow squares, q flags, which is a very efficient algorithm in terms of computational efficiency. It's very efficient as using all the computational cycles properly. So we run this algorithm in summit, and we achieve a 92% peak efficiency. So 92% of the theoretical maximum efficiency in summit, and the average efficiency was bigger than 50%. So it's a very efficient algorithm. It was designed to be efficient, but it's not really flexible in terms of contraction orderings.
00:45:52.124 - 00:46:29.484, Speaker B: And this is what Johnny Gray found with some heuristic, which is based on community detection. I won't get into the details, but you can read the paper. He found contraction orderings that are, in terms of width, sort of 20 times. Sorry. The width decreases by 20. So that means the exponent decreases by 20. And if you translate the width into computational cost, so the number of operations that you expect to find out, well, it has decreases significantly from ten to the 27 for q flags at the 20 to, to the 18, sort of flop operations.
00:46:29.484 - 00:47:10.662, Speaker B: So Johnny Gray actually run this algorithm on one single Nvidia GPU, and it calculates that for Dev 20, which are supremacy circuits, if you want to estimate just one amplitude at 0.5% fidelity, that will be around 7 million. Second, those are these numbers. Now, summit has 28,000 cpu's, sorry, GPU's, so that would be three days. But the GPU's in summit are more efficient than the GPU's that Johnny use. So that will be at most 4 hours. So using this new algorithm or this new contraction, or you can actually calculate at least one amplitude with 0.5%
00:47:10.662 - 00:47:37.726, Speaker B: fidelity. And summit. So I think that's very interesting. Now, if you want to calculate 3 million amplitudes, as we did in the experiment, that will still be three years. So I don't think this is doable, but it's certainly better than the 10,000 years of the hybrid algorithm. So I think this is a significant improvement. And if you want to do cross entropy, which of course, is what we needed to do for the experiment? Well, that's still 3 million years for the larger circuits.
00:47:37.726 - 00:48:52.134, Speaker B: Unfortunately, we cannot do cross entropy and check the data for the larger supremacy circuits yet. Okay, so what did we learn from this experiment? Well, we learned that the fidelity that we get with chrysanthemum benchmarking was the same for the full circuits, the patch circuits, when you cut it in half and you have significantly less entanglement and complexity. And it was actually the same that you get from the predicted error model. So the errors, if you measure errors, at least in simultaneous mode where you're already taking into account the crosstalk with your neighbors, doesn't seem to depend on the entanglement and computational complexity as you will expect from quantum mechanics. But, you know, this is something that you have to check as you are increasing the amount of complexity in the system zone. We check that the discrete error model works, and this underlines fault tolerance or quantum error correction. So that's good news, at least to the level of sensitivity of the experiment.
00:48:52.134 - 00:50:09.536, Speaker B: We increase complexity, sort of by more than ten magnitude, I think, over previous experiments, in the sense that we roughly build this quasi random state in a very large Gilbert space with dimension ten to the 16th, and quantum mechanics seems to work. We think this is interesting, actually, where in the paper where John Preskill introduces the name quantum supremacy, was actually talking about this highly complex frontier as one frontier in science that you want to explore experimentally. So I think that's what we do in this paper. And finally, we kind of challenged the extended choose three thesis in the sense that we have this experimental processor, which is sort of one synthetic. You know, it needs this larger dilution refrigerator, but the chip itself is pretty small. We're kind of competing, you know, with the largest supercomputer summit, which is sort of two foot fields or basketball fields worth of, you know, transistors. And, you know, so far, we're performing a computational task which is, you know, fine tuned to quantum computers, but it's still a well defined computational task, which we can do with this centimeter chip, but you cannot do with a supercomputer.
00:50:09.536 - 00:50:57.674, Speaker B: So we think this is a challenge to the standard shifting thesis that says that all computational models are equivalent in terms of efficiency. And this seems to be true. Sorry, this seems to be false according to quantum computing, and it seems to be false experimentally as well. So in the last four minutes, I will just mention an experiment that we have done recently, which is just to mention it here, basically to make the point that this is indeed a general quantum processor. So we calculate the ground state energy for hydrogen chain of up to twelve hydrogens. We use hardrifocus states. So this experiment is simple to simulate, because hydrifocus midfield, you can simulate this classically nicely, but it's a good benchmark to see how the quantum processor is working.
00:50:57.674 - 00:52:14.112, Speaker B: This is the circuit that we use to prepare the current state, well, the hard t four current state, and then variationally, and then measure sort of the terms that you need to measure the energy variation, which turns out to be just the one side reduced density matrix expectation values all the way to twelve qubits. We compile these two q, which are given rotations in a freemanic language with the square PI swaps, or sort of the square root of the sigma gates that we used in the previous experiment. We get all the way to 72 square root of iswabs and 108 two qubit gates, sorry. And 108 single qubit rotations. One thing that was interesting in performing this experiment is that even though, well first of all you can do post election, because this is a midfield state, her focus state, these given flotations conserve the number of fermions, or basically conserve the number of ones in your beta strings. So by throwing away beta strings that don't have half occupancy, then you post select, you reduce measurement errors for instance, and sort of t one errors. Even with post selection, you get fidelity as measured by fidelity.
00:52:14.112 - 00:53:16.174, Speaker B: Witness that ports for these fermenting gaussian estates, which is under of only 10%. This is error. But then you can do purifications, because this is a hard to focus state. You have the one rdms, and using representability properties of these miful states, you can sort of project to the closest peer state, and that increases the fidelity significantly, or produces the errors to sort of around 90 the dares to around like 2% or something like that. And then if you do bquez, you tune your parameters to sort of minimize, then irradiation are even lower. So I think the interesting part of this experiment is sort of improving. These are mitigation techniques which are indeed going to be necessarily, we want to really perform quantum simulation in noisy near term experimental processors.
00:53:16.174 - 00:53:20.844, Speaker B: So with that I would like to conclude. Thank you very much.
00:53:22.344 - 00:53:28.724, Speaker A: Okay, great, thanks. So let's see, should we ask, should we see if there's any questions from the audience?
00:53:41.624 - 00:54:12.586, Speaker C: Maybe I can ask a question whilst people are still thinking about other questions. Of course there are 187 people on the call and maybe take the time to ask a question. So one question about. So you were experiments on the bristlecone chip that you did before. Right. And so you can, that didn't have tunable couplers, I think so I want to know, you know, like what experiments failed, right. In the end, this worked, but on the bristlecone chip, it wasn't good enough.
00:54:12.586 - 00:54:15.674, Speaker C: And how did you see that? And. Right.
00:54:18.094 - 00:54:57.688, Speaker B: Yeah. What happened is the result on chip, in a nutshell, you could perform two qubit gates, right? The two qubit gates were a bit different because of the particular details of how you implement two qubit gates when you have qubits of resonance, because the coupler is always solved. So there were control c or control not two qubit gates. And you could just choose two qubits and calibrate them and do a two qubit k with those two qubits. Fine, right. And all, you know, the qubits around you were sort of on the ground state. You like, they don't exist.
00:54:57.688 - 00:55:43.530, Speaker B: They are in the ground state. And you could do these two qubit gates perfectly well. You could get good fidelities. Now, the problem is for these random circuits, you need to half two qubit gates basically in all qubits at the same time, right. Otherwise it would be just too much depth and you are going to be killed by t one, just the coherence. And the problem is, if you try to do two qubit gates in many qubits at the same time and neighboring qubits at the same time, then it was just too hard to calibrate those qubits correctly to remove crosstalk.
00:55:43.642 - 00:55:46.466, Speaker C: So once you basically you got too much crosstalk.
00:55:46.570 - 00:56:27.078, Speaker B: Basically you get too much crosstalk. You could probably remove a lot of crosstalk by calibrating better, because, you know, if you're moving your neighboring qubit to some other place in frequency, it's a start affecting this one qubit. But you can in principle, you know, it's sort of a coherent coupling. So you can in principle try to calibrate it away if you are smart enough with a smarter control to sort of remove the coupling with control because it's coherent. It's not just the coherence, but the problem is just too hard. It was too hard to try to engineer the controls for all the qubits simultaneously to remove this coherent coupling. So, yeah, in essence, we just couldn't get away from crosstalk.
00:56:27.078 - 00:57:07.504, Speaker B: If you do keep working on it, maybe you're smart enough and eventually you can improve your control because it's coherent, but it was just too hard. So at the same time, well, people were working actually on this chip with tunable copper for quite some time, people like Charles Neil and Pedram and others, and it just turned out to work really well. And as people were struggling really hard to remove the cross, also control, at some point, people said, well, this is just too hard to do. This chip is working really well. And everybody kind of moved to the tunable copper sycamore chip.
00:57:08.164 - 00:57:21.344, Speaker D: Sergio, there's a question from the chat, but maybe it's a question for part two of this talk during experimental Tuesday. But the question is, how come the single qubit gates are slower than the two qubit gates?
00:57:21.724 - 00:58:24.238, Speaker B: Right? That's an interesting question, and indeed, Julian caprally answered that better. But the basic thing is that with these tunable couplers, you can turn it off, but you can actually turn them off to some fairly large value of coupling. And your other qubits are sort of quiet. And so you turn up a fairly strong coupling between two neighboring qubits without turning on the other couplers. It's just the value of copper that you get. So I guess the more interesting question is why you don't do single qubit gates faster, which are microwave gates, so you could increase the amplitude and then you could do them faster. And I guess the main reason is that these superconducting qubits, they have not just zero on one, they also have the second energy level, third energy level.
00:58:24.238 - 00:58:50.794, Speaker B: And if you try to do single quid gates very fast, you're trying to drive excitations between the ground state and the first estate, but you start leaking into the second excited state and software, because you're, you know, it gets harder to decouple from higher energy levels. So you start getting leakage. So the basic answer is you're limited to 25 and a half seconds, at least currently, because you want to avoid leakage to higher energy levels.
00:58:53.144 - 00:59:25.314, Speaker E: I have a quick question, Sergio. So in terms of error models, you said your results are consistent with having local, you know, maybe single qubit errors or possibly two qubit. So would you have been able to tell the difference if, let's say that you had errors spread out locally over over ten qubits, you know, you had these patches of errors. Would you have been able to tell that that was not the case?
00:59:30.014 - 01:00:23.518, Speaker B: Well, it depends on the probability, I guess. I mean, this experiment, you know, has, I guess, a limited sensitivity, right? Because we're talking about, you know, error probabilities of further 0.3% or, you know, for two qubit gates. So I think if you have spread errors which are ten times smaller and they are really spread between many qubits, then you will not be able to detect them with this particular experiment. Right. Because, you know, they will be masked by the higher errors that, that we're getting. So, you know, it's quite possible that there are still significant correlated errors at, you know, just a lower probability than the sensitivity from this experiment.
01:00:23.518 - 01:00:28.674, Speaker B: So it's very important to do more tailor experiments to look at that particular question.
01:00:40.874 - 01:01:17.434, Speaker D: I think I'd like to step in as the conference organizer and say we would like us to keep fairly close to the schedule. There is in fact, quite a lot of further discussion showing up in both in the Q and A and the chat. But my reading of is most of it also addressed, can be addressed by Julian Kelly tomorrow. So I would suggest we adjourn. And thanks, Sergio, thank you for this talk. And we will have the promised, well, not ten minute break, but four minute break anyway until Jens Eysert is ready to go.
01:01:22.574 - 01:01:24.594, Speaker A: Okay, great, thanks, Sergio.
01:01:25.774 - 01:01:26.574, Speaker B: Sergio, thank you.
