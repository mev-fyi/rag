00:00:01.280 - 00:00:31.360, Speaker A: Thank you, John. And thank you for the organizers for giving me this opportunity to talk. And for also, I thank you to stay, that you stayed here late in the afternoon. And I'm going to hopefully finish by five, so you can go and have your first beer by 530. Okay, so I'll talk about the degree of central curve in quadratic programming. And this is joint work with one of my master's students. Now he's done, and he's a PhD student at Carnegie ML and or department.
00:00:31.360 - 00:01:10.576, Speaker A: And, you know, part of his thesis is sort of part of what I'm going to talk about today. Okay, so what's a quadratic program? Quadratic programming is an optimization problem. Okay. We're trying to minimize a quadratic objective function given by a n by n, symmetric positive definite matrix. And there's also linear part two, subject to linear equations. And the variables are non negative. Okay? Again, in this case, the q is a positive definite matrix, c is some cost factor, a is a d by a matrix of rank d.
00:01:10.576 - 00:01:52.324, Speaker A: So keep in mind, d is the number of equations and n is the number of variables. So in my formulas, the d and n are going to appear. So as I stated, this is a convex optimization problem, because when I have a positive definite matrix up there, it's got convex objective function, and feasible region is also convex. This is sort of all the nice technology we can apply, right, of optimization, the convex optimization. So how would you solve such a problem? Here's one way of solving. So like a 10,000ft point of view of interior point methods, you change your problem just slightly. You know, you put in the non negativity constraints into the objective function.
00:01:52.324 - 00:02:28.240, Speaker A: Lambda is a positive parameter. There right now as lambda. So you should imagine that you're going to solve this problem as lambda starts, big plus infinity is going to go down to zero. Objective function is still convex. And when lambda is really super big, then we can get the optimal solution for this problem very easily, because the original objective function parts of disappears, that's not going to affect. And then as Lambda goes down to zero, the original objective function is going to take over. And we'd like to use that.
00:02:28.240 - 00:03:02.668, Speaker A: And how can you turn this into equations? Optimization problems is really solving systems of equations. I can write down what's called the KKt conditions, the Koreshkuhntucker conditions, and here they are. The first two are the feasibility conditions on x ax equal to b x non negative. And then comes the gradient of the objective function. That's the first part there. And then that's equal to a linear combination of the rows of the matrix a. So y is a vector of variables Y one through y d.
00:03:02.668 - 00:03:39.552, Speaker A: So there are dv y variables there. So the nice thing is that, so we have a convex problem. So if you solve this problem, it's going to be a unique solution when lambda for each lambda is positive and the x component of that solution is going to give you an optimal solution for the bulk problem there. So we're going to do that. And so again, for every lambda there is a unique solution. So for every lambda, there's some solution here where x star is going to be the optimal solution up there. Okay? And then we can vary the lambda right, and plot the pair of optimal.
00:03:39.552 - 00:04:13.796, Speaker A: So actually the y sort of the optimal solution for the, for the dual, we can plot that and then we're going to get a curve in r to the n plus one plus d. So this is a small example that I'm going to use to illustrate some of my things here. Okay, here's a quadratic optimization problem, very simple quadratic function there. So the q matrix is actually a diagonal matrix now, okay. And the linear part, and then two equations and four variables. As you can see, you know, I chose my equation such a way so that x three and x four can be easily expressed in terms of x one and x two.
00:04:13.860 - 00:04:30.472, Speaker B: Just stupid question on the last slide to make sure I'm not lost. So your actual solution is just the first solution where all the x's are positive with the smallest lambda, because you have this curve and that's the point on the curve you take.
00:04:30.628 - 00:04:33.936, Speaker A: Say again? So the lambda is super big. Yeah.
00:04:34.000 - 00:04:45.912, Speaker B: When lambda is zero, then positivity, but eventually it's going to force it to be positive. And you're saying that first point, it's forced to be positive is the actual solution to your original question on the previous slide.
00:04:46.008 - 00:04:57.664, Speaker A: Now, what I'm saying is I start with lambda being super big. Okay? I solve a sequence of problems as lambda is approaching zero. I'm going to also approach the optimal solution to the original problem.
00:04:57.744 - 00:05:01.352, Speaker B: Why does that keep the x positive as lambda goes to zero? I don't see that.
00:05:01.368 - 00:05:06.244, Speaker A: Because of the logs. 0000, it's a barrier function.
00:05:06.704 - 00:05:07.904, Speaker B: Yeah. Okay.
00:05:07.944 - 00:05:21.376, Speaker A: Yeah, sorry. Okay, so that's my problem. So again, so after that, I'll come back to my example right away. Let me make central definition, okay. Central definition of central path.
00:05:21.440 - 00:05:21.720, Speaker C: Okay.
00:05:21.752 - 00:06:06.614, Speaker A: So, okay, so if I take this chart that I get for every lambda, right, at every point, I get the point on this curve for every lambda, I take that and I project it on the x coordinates. Okay? Get it in real curve. That's called the central path of quadratic programming. Now, meanwhile, maybe I should have said earlier in my original optimization problem, the quadratic program, of course, if I didn't have the quadratic part, if q were equal to zero, right, then I will have a linear program. So what I'm saying here can be all set for the linear programs as well. So the central path can be defined in a similar way using the same kind of optimization problem. And we will get a central path for linear programming, too.
00:06:06.614 - 00:06:33.188, Speaker A: So what are the equations defining? These are the KKt equations defining the central path in my problem. So you see the diagonal q matrix here. You see the cost vector c up there. The lambda over the variables appear there as well. And right here, you see the transpose of the a matrix. And then the constraints ax equal to b constraint down there, too. So we solve this.
00:06:33.188 - 00:07:06.080, Speaker A: For every lambda, you get a curve projected on x. That's the central path. Okay? So how we use this. So here's a, here's a cartoon idea. And when lambda is really big, we can solve the problem very easily and we get something called analytic center of the feasible region of the polytope. Okay? And then we start from there and we start making the lambda smaller. As we're doing that, we're gonna, we're going to trace, we're going to follow the central path, okay, all the way to the optimal solution.
00:07:06.080 - 00:07:28.484, Speaker A: Okay, so that's one way of saying what, how the interpoint algorithms work in this, you know, cartoon picture. You know, you should not be fooled. Okay, I'm sorry. I had again, the linear programming in mind. The optimal solution is going to be at the vertex. Of course, it doesn't have to be that way for the quadratic programming. And there's a unique optimal solution somewhere, but it doesn't have to be at the corner there.
00:07:28.484 - 00:08:14.340, Speaker A: But that's how the linear programs, the interior point algorithms work. Okay, so now let me go to the algebra or algebraic geometry. I define a central path by projecting the earlier curve on the x coordinates. Now, but the equations defining it were not polynomial equations. I'm going to make them polynomial equations by clearing the denominators. Okay, so what you see here is the KKt conditions equations, but I'm going to multiply each equation by the corresponding xi because they were in the denominator. And we're going to put polynomial equations, okay, so, and I'll take the complex solution set of them together with the ax equal to b, that's a curve in c to the n plus one plus d.
00:08:14.340 - 00:08:49.824, Speaker A: I'll project that on the first n coordinates. I'm going to call this a central curve of quadratic programming. All right. Okay, so this is central curve defined by both equations. I cleared the denominators. Well done. Okay, so one more time I'll project, so the ideal generator PI, these equations I'm going to call j and the ideal, the vanishing ideal, or the ideal of polynomials vanishing on the projection, the central curve I'm going to call I sub c.
00:08:49.824 - 00:09:44.008, Speaker A: Okay, so in our running example, if you do the computations that was n was equal to 4d is equal to two equations. For two equations and variables, the central curve, after you do this projection will be defined by the linear equations ax p and one extra polynomial of degree four. This is that. And you see the real picture of the curve that the wall is the ax equal to b, two dimensional affine space. The lines you see are the intersection of the coordinate hyperplanes with the, but the defined space, our feasible region, original feasible region, is this quadrangle. Right away. Okay, that's the one.
00:09:44.008 - 00:10:10.134, Speaker A: And you see other feasible regions there. If I change the signs of the, of the variables and the rest, you know, the curvy stuff is the central curve. Okay. And central curve contains all the central paths of all the feasible regions. Okay. And central paths will follow from the analytic center of that particle region to the optimal solution. Is that clear? Okay, so it's a nice degree four curve.
00:10:10.134 - 00:10:38.886, Speaker A: Okay. If you only cared about one of the regions, could you have a simpler curve? So the central curve is, the way I defined is I take the, I forget about the non negativity. You know, I went to the complex numbers, I take the whole thing and then project it so I get the whole. Okay, so I think, so this is an irreducible cure. Okay. Yeah. This is an irreducible curve, I guess.
00:10:38.910 - 00:10:43.278, Speaker C: The other boundary chambers, if you change the signs of the variables, those would then be feasible regions.
00:10:43.366 - 00:10:45.486, Speaker A: Exactly. Yeah. So there are one of those.
00:10:45.510 - 00:10:48.674, Speaker C: There's two components of the curve, right?
00:10:50.574 - 00:10:51.622, Speaker A: This one here?
00:10:51.798 - 00:10:54.514, Speaker C: No, no, to the lower left. Up.
00:10:55.414 - 00:10:56.258, Speaker A: Lower up.
00:10:56.366 - 00:11:02.618, Speaker C: No. Oh, here, you mean just to the right of that. Just this one, yes.
00:11:02.666 - 00:11:03.122, Speaker A: Okay.
00:11:03.218 - 00:11:05.054, Speaker C: There's two components of that curve in there.
00:11:06.834 - 00:11:10.626, Speaker A: Right. Maybe it's close for a minima. Yeah. Okay.
00:11:10.770 - 00:11:12.018, Speaker D: Anyway, nothing.
00:11:12.066 - 00:11:21.678, Speaker A: Mistake. Maybe. So I'm not sure. Are the coordinate axes. Oh, I'm sorry. These are not the parts of the curve. Yeah.
00:11:21.678 - 00:11:23.954, Speaker A: These are cornered axes. Oh, okay.
00:11:24.734 - 00:11:32.318, Speaker C: That's a poly, it's a polyunal decomposition of the plane. And for different choices of, say, each one of them become a feasible region.
00:11:32.366 - 00:11:32.870, Speaker A: That's right. Yeah.
00:11:32.902 - 00:11:36.286, Speaker C: In that feasible region, that triangular region, there's two components of your central curve.
00:11:36.430 - 00:11:38.874, Speaker B: But they're saying that these are coordinates.
00:11:39.694 - 00:11:45.994, Speaker A: These are not part of the conditions, I think. I'm not sure why. Answer why. There are two components there. But anyway.
00:11:47.724 - 00:11:52.236, Speaker E: There'S also, this quadratic form has some minimum over all of the plane.
00:11:52.260 - 00:12:06.184, Speaker A: The plane, yeah, I know what it is. It's in here, actually in the original feasible region. And it's not on the boundary. It's in the middle here somewhere. There. And the central path follows from analytic center down there. Yes.
00:12:06.184 - 00:13:17.694, Speaker A: Everything else, all the optimal solutions for all the feasible regions are on the boundary except that middle guy. Okay, so the goal here is I like to compute the equations of the central curve and try to compute the degree of this curve. That's why the degree of the central curve. So why, okay, so this work of studying the central curve and interior point methods using differential geometric methods were started actually by Dave Byron and Jeff Ligarios in the late eighties. But then sort of recently, two new things happen. One of them is that my office mate Gregorio Malayevich and Mike Shoup and de Dieu, how should I say it, in 2005, looked at the central path and looked at the total curvature of the central path for linear programs. So why would you want to look at the total curvature of this curve? Well, as I just described to you, oops, the other way.
00:13:17.694 - 00:13:46.460, Speaker A: Let's go back to the cartoon. So if I'm going to follow this path, I have to make steps. I cannot continuously deform lambda from positive infinity to zero. I have to make steps. So that means I'm going to be approximating this curve by a polygonal path. The curvier the path, the higher the total curvature, it's going to be more difficult. So the higher the curvature, the total curvature, then there's going to be the interior point.
00:13:46.460 - 00:14:39.384, Speaker A: Algorithms are going to take more time, more iterations to get to the optimal solution. So that's a rough idea. So they looked at that. And so their idea was that, well, okay, the total curvature, it actually can be computed by looking at the arc length of the corresponding Gauss curve. So you look at the unit tangent vectors to the curve, you plot them on the unit sphere, and you compute the arc length. And that arc length can be bounded, can be bounded by data coming from the central curve itself, including the degree so that was the idea. And then comes Jesus and Barrington and Cynthia in 2012, actually earlier that's published in 2012, and they actually gave a bound for the central curve in terms of d and n.
00:14:39.384 - 00:15:17.744, Speaker A: So say that the degree of the central curve for linear programming is n minus one. Choose d. N is the number of variables, d is the number of equations, and this bound is sort of exact when all the data, the a matrix, the b and c, and everything is generic if it's randomly chosen. So that's the degree of the central curve. In particular, it implies a bound on the total curvature that I'm just putting down there. So that's a bound center curvature. Now, one of the motivations of all the study is that there's been a conjecture in the back out there.
00:15:17.744 - 00:16:03.830, Speaker A: It's called continuous use conjecture that says that for any d, you don't have to fix for any d. For linear programming, the total curvature should be a linear function in n bounded by that. It turns out it's a very new result by Michael Josvik and his co authors that actually spectacularly false. The total curvature of linear programming can be exponential if you allow D to also vary. And it's very interesting paper. I have to look at it more carefully, but it involves heavy duty tropical geometry in there. And then finally, one of my motivations here is that quadratic programming, so we deal with linear programs.
00:16:03.830 - 00:16:24.844, Speaker A: The ultimate goal is really look at semi definite programming. It has its own central curve and whatnot, and quadratic programming is a one step in between. Although, of course, semi definite programming is a little far away from quadratic programming in a sense. And I know, you know, smart people like Cynthia have thought about it, and it's a hard problem, but that's sort of the goal.
00:16:25.824 - 00:16:26.328, Speaker C: Okay?
00:16:26.376 - 00:16:38.656, Speaker A: So I hope the problem is clear. I want to look at the equations of the central curve for quadratic programming. Computer degree and. Yeah, so. And these are the reasons. Okay. Okay.
00:16:38.656 - 00:17:08.280, Speaker A: So this is my embarrassing slide. Okay. Ben already mentioned whether this curve is irreducible or not. To dilute it, I don't know. In generality, it should be reducible. But here is one simple observation I'm going to use. This says that if I take the upstairs curve and remove the corner hyperplanes and then close back again, nothing changes, which means that the central curve does not have any components.
00:17:08.280 - 00:17:25.024, Speaker A: If there are any in the coordinate hyperplanes, of course, irreducible, we're fine. It's not going to happen. Okay. And. But the only thing I'm using is this result anyway. Okay. But I agree with Bernd that this curve should be, should be reduced to hold in general.
00:17:25.024 - 00:18:03.494, Speaker A: Okay, so here's our first observation. Okay, so now we're going to Berent and Cynthia and Jesus have a way of computing the degree using some meteorite theoretic methods. Okay? N minus one, choose. D is actually the number of bounded regions inside a real hyperplanear arrangement that comes from a and b and c. But we're going to sort of now move away from that idea. And here's our first result. It says that if I wanted to compute the degree of the central curve, what I would do, I would intersect it with a random hyperplane and count the number of points.
00:18:03.494 - 00:18:23.464, Speaker A: It turns out all I can, I can do is I can take the upstairs curve and intersect it with a hyperplane, okay? Only in axis. Then the number of solutions in the torus, in the corresponding torus is going to be equal to the number of solutions downstairs. And that's exactly the degree, okay?
00:18:24.244 - 00:18:28.492, Speaker D: So why do you only intersect with the x's and not with the generic hypotenuse?
00:18:28.508 - 00:18:53.550, Speaker A: Okay, so one more time. If I, if I projected on the x variables, right, then I get the central curve. And I want, you know, to compute this degree. Degree, I will intersect it with a hyperplane like this. What I'm saying is that do the same thing upstairs, okay. You will still have the same number of intersection points upstairs, okay? All right, so another thing I have to tell here, this was actually crucial point.
00:18:53.702 - 00:18:55.390, Speaker C: It can't be several to one.
00:18:55.582 - 00:19:39.478, Speaker A: It's not, it's not right. It cannot because of the, the AI is here, the a matrix there. So it's one to one. And now one more thing is, so I'm assuming here, by the way, everything is generic. But already in my example, I was using a quadratic diagonal matrix. It turns out if you replace everything in here, where everything is generated by q, is a diagonal matrix with generic diagonal entries, then the number of solutions you would get for the corresponding system is the same as the number of solutions you would get for the general system. Now you might say, what's the big deal? Every symmetric matrix can be turned into a diagonal matrix.
00:19:39.478 - 00:20:34.584, Speaker A: But that's not the reason here, because if you do that, if you do that in your, let's see, you will screw up your system back here. If you do that one more time, if you make the change of coordinates, these guys in the denominator is going to become some complicated linear forms. You have to clear those denominators. And the problem is going to get complicated. So the reason why I can go down to go look for the number of solutions doesn't change when I move to the diagonal programs because I can have a homotopy from this, the general case, to the diagonal case, and nothing happens in the sense that I don't lose solutions when I do the homotopy. So this is very useful if you think about it. If I want to compute the numbers, I can only use examples where the matrix is diagonal.
00:20:34.584 - 00:21:13.542, Speaker A: And that makes the computations a lot easier. So when we tried using grobnar basis methods to eliminate and then compute the degree, we were able to compute a very small portion of this table. So here, these are the degrees indicate for generic cases where that d is the number of equations and is the number of variables, we're able to compute a very small portion. But for the diagonal case, we can complete a big chunk of the table. And then, in fact, we did this earlier, we realized it and then went back and approved the earlier theorem. So these are numbers. Hopefully you think that these numbers are nice.
00:21:13.542 - 00:21:28.646, Speaker A: Okay. All right, very good. Like the first row, you know, fantastic. Okay, exactly. Oh, wow. Frank is fast. Okay, so you see that, right? What is it, five plus eleven is, this is equal to that.
00:21:28.646 - 00:21:45.816, Speaker A: You know, it's very nice. All right. So, okay. Okay, so how we're gonna do this, I'm gonna do is I'm supposed to count, I'm supposed to count the solutions in the torus. Okay, I'm gonna do that for you first for the linear program. Right. So this is the linear program version.
00:21:45.816 - 00:22:16.454, Speaker A: So I'm gonna compute for you the, the center, the degree of the centrica for linear program. Okay, I need to count the solutions here. These are d plus one equations and n variables. Right? I change, make a change of, I write the solutions. So this is n minus one, n minus d minus one dimensional affine space. Every solution looks like that. So v one through vn minus d is a vector in the kernel of the matrix.
00:22:16.454 - 00:22:52.478, Speaker A: So I introduce new variables, t one through tn minus d minus one. I take that, I plug it in upstairs where I have the x's, I expand it out. So I have n equations now in variables, and the support of those variables are exactly these guys. So what are those? I mean, the supports of the equations. Okay, note that. So the lambda I can ignore. Okay, one, t one to tn minus t minus one, that thing multiplied with the y, one, that multiplied with y two, etcetera.
00:22:52.478 - 00:23:44.036, Speaker A: Okay, that's the, that's the support. Okay, so what is the, what is that? That's also known as the n minus d minus one simplex cross d dimensional simplex. And every equation has that support. So in order to compute the mixed volume, I'm going to compute the normalized volume. I, the volume of this guy, the volume of this guy, also known as the, as the klee polytope of, what is it? D copies of n minus d minus n, one dimensional simplices, I'm sorry, d plus one copies of them. And the volume is computed like that. It's actually n minus one.
00:23:44.036 - 00:24:18.464, Speaker A: Choose d. And I'm going to write it in this form. So you can get this form as if you, if you look at the staircase triangulation of the polytope. So this is not very complicated. So what I'm trying to say here, let's say if n was, if n was seven and d is equal to say three. Okay? Then six choose three, right. Can be written as, what is it? Five, choose two plus four choose two.
00:24:18.464 - 00:24:42.714, Speaker A: Plus three choose two. Plus two, choose two. Okay, but, sorry, I mean n minus one choose. D in this, in this view has this nice decomposition into sums. Okay, so I'm going to do exactly the same thing for the quadratic programming. Okay, so same thing, just the quadratic part has included here now, okay, I'll do the same substitution. I do that.
00:24:42.714 - 00:25:27.664, Speaker A: And this time I'll have monomials in the support of the following kind. What do I have? Now here I have first two times the simplex, n minus d minus one simplex, and then, and then d copies of the, of the standard unit cplex shifted in the directions. This is the Klipolo top of two times unit simplex, and then d times the unit simplex. D copies of the unit simplex. So we can compute the volume of that polytope by using the stick as triangulation two. Okay, but this time you get the exact same thing with the volume factor. Each one of those guys going to have two to the k.
00:25:27.664 - 00:25:52.528, Speaker A: So that's the ie I'm going to put here. The volume is going to be. So it's going to be two to the zero plus two to the one to the square two cube. Okay, so that gives us the degree of the central curve for the quadratic program. It's very, very close to linear program inform. And then we get that.
00:25:52.656 - 00:25:55.384, Speaker C: So in this case, then the Pkk bound is exact.
00:25:55.464 - 00:26:14.972, Speaker A: Exact, yep. For the generic data. Exactly. Do I have two minutes? Two minutes I have. So the triangulation. So let me just point out something here in the case when again, q is a diagonal matrix, we also have the equations. Equations.
00:26:14.972 - 00:26:44.936, Speaker A: For the central curve, we use a nice idea to eliminate the variables. And if I have time, I explain it to you. But I don't have the time. But the point here is that with respect to a degree, reverse x order x minus x n, let's say for d is equal to two, a is equal to five. I get the following grammar basis, plus, of course, ax, equal to b. The equations, right. The initial ideal is generated by those four monomials.
00:26:44.936 - 00:27:27.704, Speaker A: Here is the decomposition, and if you add up the degrees, it's going to be equal to eleven, which would be the case if you erase the two s, you're going to get the broken circuit complex of the uniform metroid of rank three on five elements. So using actual degree bound, we were able to prove that these are the equations that define the curve as well. I'll stop with that, thank you very much. So, any quick questions? Yes, Cynthia.
00:27:28.444 - 00:27:33.704, Speaker E: So this, this thing that these equations define doesn't depend on b?
00:27:34.844 - 00:27:53.804, Speaker A: No, it depends. So, so you see these numbers here, the numbers here. So what doesn't depend is that the terms do not depend. So I chose a specific problem, but I generated my B's and C's randomly and did my, my positions earlier.
00:27:54.184 - 00:28:00.280, Speaker E: And you could eliminate these sort of, this long equation with lambdas and C's.
00:28:00.312 - 00:28:01.884, Speaker A: And Y's and all of this stuff.
00:28:02.224 - 00:28:06.444, Speaker E: The long ones didn't have any b's in them, right?
00:28:07.464 - 00:28:12.784, Speaker A: Oh, I see. That is correct. Yes.
00:28:12.904 - 00:28:13.784, Speaker E: I was wondering if there was.
00:28:13.824 - 00:28:18.444, Speaker A: No, you're right, you're right, exactly. This doesn't have any data that comes from b. I agree with you.
00:28:19.174 - 00:28:24.594, Speaker E: Geometric description of what this large variety is.
00:28:26.134 - 00:28:29.102, Speaker A: This is now the central sheet, right, for the quadratic programming.
00:28:29.198 - 00:28:32.142, Speaker C: So the part from b is just slices. You slice this.
00:28:32.238 - 00:28:37.262, Speaker A: Yeah. With ax equal to b. Then I slice it. That's right. But you're right, this doesn't contain b.
00:28:37.278 - 00:28:38.514, Speaker E: Is the central sheet.
00:28:40.214 - 00:29:01.072, Speaker A: I mean, it is the central sheet. You know, I'm calling the central sheet, but I don't have anything like that. That comes from like spires and broadfoots type of result. I don't have that. But it's, I mean, this, this monomial ideal. Hands, hands. This initial ideal for this, for this guy in general, you know, this is a, this is a como coli initial ideal.
00:29:01.072 - 00:29:15.260, Speaker A: Okay, so we can compute the h vectors and in fact, so that actually we can get a good bound on the central curve. Any other questions? Yes, Matt.
00:29:15.332 - 00:29:18.908, Speaker D: Yeah, so you mentioned that this is for all generic data.
00:29:18.996 - 00:29:19.260, Speaker A: Yeah.
00:29:19.292 - 00:29:28.700, Speaker D: So then what about if you say for that q term, that Q matrix, what if this guy was sort of ranked, efficient? I mean, so it's no longer generic anymore. How does this affect things?
00:29:28.772 - 00:29:33.036, Speaker A: It should go down, but I don't know how in a controlled way it's going to go down.
00:29:33.100 - 00:29:39.344, Speaker D: Do you think the curve would still be irreducible then, or do you think it might split up into different irreducible proponents?
00:29:41.444 - 00:29:49.644, Speaker A: I don't know. All right, let's thank certainly.
