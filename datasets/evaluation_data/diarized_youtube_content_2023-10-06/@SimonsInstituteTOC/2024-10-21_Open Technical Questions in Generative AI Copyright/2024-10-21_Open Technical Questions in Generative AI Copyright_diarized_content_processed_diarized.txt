00:00:23.840 - 00:00:25.245, Speaker A: Technical questions in general.
00:00:29.795 - 00:00:55.223, Speaker B: Can you hear me? Okay, cool. So hi everyone. It's my great pleasure to talk about open technical questions in general copyright today. So a very quick self introduction. I recently graduated from Princeton in July and joined Google as a research scientist in New York City. So most of the work I'm going to cover today are back in Princeton. So it doesn't really represent most of the group opinions like disclaimer.
00:00:55.223 - 00:01:34.329, Speaker B: Okay, so yeah, today I'm going to talk about open technical questions in Genai copyright. But as you may imagine, this question is not only a technical question. So I will actually start with what is not a technical question. I will roughly spend 10 minutes on that and then we'll talk about what is partially a technical question. And finally towards the end, we will spend the majority of the time on what are important but understudy technical questions and disclaimers. Again, I work on the technical side, I'm not a lawyer, so maybe I will make factual rules, try to check things online. And this talk is mainly for discussion and all the opinions here are on my own.
00:01:34.329 - 00:02:07.575, Speaker B: And most of discussion here focuses on US copyright issues because copyright is a global thing and most of the things here are based on like the Fair Use Doctrine, which is a US thing. Cool. So let's start with the first part. What is not a technical question? If I had to name a non technical but very important question in the Genai copyright regime, I would say that is what is and is not copyright protected. So let me start with a very simple question for you guys. Do you think Mickey Mouse is protected by copyright? Raise your hand if you think yes.
00:02:10.565 - 00:02:12.389, Speaker A: Later one is the earlier one.
00:02:12.437 - 00:02:30.005, Speaker B: Yeah, okay, you already predict my slide. Yeah, cool. But for this one I'm asking for this one. This is copyright protected because the later one. But there's also a very, very first version which is shown here. This is the very first version of Mickey Mouse. I think it's in the film called the Steamboat Willie.
00:02:30.005 - 00:03:09.265, Speaker B: It's back in 1928. And if I asked you the same question last year, your answer is should already Sorry. Should still be yes because. Sorry, because actually for this version of Mickey Mouse, it's not copyright protected since January 1st of this year. The reason for this is because in the U.S. according to the Copyright Office, we have this statement for our artwork. The copyright only induced for a term of 95 years after its publication or for 120 years after its first creation, whichever it comes first.
00:03:09.265 - 00:03:23.885, Speaker B: So actually back in 2023, towards the end of the year, both Mickey Mouse and its friends, Minnie Mouse. They are no longer copyright protected. So indeed this is not a technical question, but it's an important one.
00:03:23.965 - 00:03:28.025, Speaker A: And was that term of years extended specifically because of Mickey Mouse?
00:03:29.095 - 00:03:55.481, Speaker B: I don't know. I'm not a lawyer. But yeah, maybe you can ask the same question in tomorrow's talk. Yeah. So I think another question I want to pose here, which is becoming more and more complicated given like the Genai involvement, is is machine generated content copyrightable? That's a very hard question. So let's imagine that yesterday I was very boring. I was just playing with Gemini and Gemini created something like this for me.
00:03:55.481 - 00:04:26.488, Speaker B: Okay, So I do not know how to draw, but can I claim the copyright of this work? Because I write the prompt for this work. Do you believe it makes sense? Raise your hand if you think like I can own the copyright of this work. Okay. It seems like you have good legal education. Actually, this thing I just mentioned is not something hypothetical. Indeed there's a case around that. So back in 2022, there's a person who use midjourney to help him create a comic book.
00:04:26.488 - 00:05:03.119, Speaker B: And the book's name is called Diary of the Dawn. So what the person did is like he used some prompts and then used a prompt to generate the images. And also he has some human written text on each of the comic. And he put all this together and that comes a comic book. And in 2022 he tried to claim the copyright of the book. It takes a US Copyright Office quite a long time. And finally at the end of the day, the judgment is this part of human written text is copyrightable, but the images generated by the midjourney is not copyrightable, which is kind of like very mixture decision.
00:05:03.119 - 00:05:38.227, Speaker B: And the major reason behind this is because the traditional elements of the authorship for this artwork, which is basically like the pictures are not executed by human, they are executed by machine. So in that sense the copyright office believe it is not copyrightable. But the things are evolving. Like starting in last year, the copyright office are actually asking for more inquiries or suggestions on this piece, especially on the copyright in AI generated text. And later on this year, I think they put out a summary of all the opinions. The summary is quite long, roughly seven pages. So I haven't got a chance to read it.
00:05:38.227 - 00:06:09.423, Speaker B: If you guys are interested, feel free to check it out. I also want to share some personal opinions or discussions around that. This is actually coming from a very old but potentially relevant case. So these discussion is relevant to a technology which was still a very novel technology. 150 years ago, but not a technical technology nowadays, which is photography. So back then there was also discussion why the photographer owned the copyright of the picture they took. So the case is like this.
00:06:09.423 - 00:06:53.315, Speaker B: So this is original picture took by this photographer and the subject here is actually the famous writer Oscar Wilde. And then there's a company that tried to make a lithographic copy of this picture. And then the photographer is kind of mad, like you kind of like copy my work. But back then there's no argument saying like pictures or photos as copyrightable. So it take a while for the choir to debate around that. Because actually in this piece of work, the human participation is that the photographer makes some decisions on how you compose the elements in the picture and how your arrangement and the lightning and then he use the camera, which is a tool to capture all the things. It seems like the camera is taking all the roles here.
00:06:53.315 - 00:07:27.651, Speaker B: But at the end of the day, the core decision is humans should own the copyright of this work. And the large reason is because all of these elements talking about here are considered to be creative elements back then. And due to this case, nowadays, if you like photography now, you can still own the copyright of your work. So let's try to make analogy here. All of these are my opinions. If we view AI as a tool as well. So in the human AI case, the human participation, if we can also think that as something creative.
00:07:27.651 - 00:08:16.679, Speaker B: For example, I spend a lot of time in writing my prompt and you know, like nowadays a lot of people are actually working as prompt engineers. So it kind of indicates that you have some creative elements inside of that. And then you use AI as a tool to project your thinking onto the word. And then you create an image or a paragraph. In that sense, should human at the end of their own the copyright of the work, that's still like an open ended discussion. But if we pray, prefer to view AI as a tool rather than kind of like a creator itself, maybe at the end of the day humans should also own the copyright of the world. But something more complicated here is like what about the companies that create the models? So all of these are non decided things, but I think there will be a long debate around that.
00:08:16.679 - 00:08:39.815, Speaker B: And all these things are quite legally complicated. So nowadays in order to avoid some of this complications, the companies are just using contracts to bypass all the things. So Basically, for example, OpenAI is telling you that you own all the rights of the inputs and the outputs of the model. Yeah, but we will see the things to evolve in the future. But all the things Discussed here are now. Technical question. Yeah, go ahead.
00:08:39.815 - 00:08:56.265, Speaker B: Probably want to take the ownership because they can get sued if artists, you know. Yes.
00:08:56.685 - 00:08:57.505, Speaker A: Whoever.
00:08:58.325 - 00:09:16.895, Speaker B: You know, if the person who created the comic book didn't realize that it actually was using the size of another comic. Yes. So yeah, there are so many complications here. Yeah. So I wondered like there is that aspect as well which is very different from. A little bit different from photography. Yeah.
00:09:16.895 - 00:09:34.051, Speaker B: Versus maybe if you are considering. Okay. I don't know how they consider copyright for when they do. I guess collage kind of works. But. Yeah. So I was just wondering what your thoughts were on that.
00:09:34.051 - 00:09:56.355, Speaker B: Yeah, I think that's a very important question and I might not be the best person answering the question. So maybe you can raise the same question tomorrow's session. Yeah, so sorry about that. Yeah. Cool. Yeah, so I think we covered the first piece about what is not technical question. And I think there are some parts which we saw some interplay between the technical part and the legal part of the question.
00:09:56.355 - 00:10:25.973, Speaker B: And also I will spend a few more minutes on that before we get to fully technical questions. So if I have to give an example here for what is partially a technical question, I will say that is the legal and technical decision on what is copyright infringing. Yeah. Oh, yes. So technical is kind of like quantifiable. And you can use quantifiable evidence to show things. Yeah, no, it's not illegal.
00:10:25.973 - 00:10:52.121, Speaker B: It's my personal term. Yeah. Or would you mind? I'm sorry. Yeah. For instance, let's talk about like for example for the infringement thing. If a technical part is like if my content infringe on the original copyright content by N gram. Well, N is a fixed term.
00:10:52.121 - 00:11:11.815, Speaker B: Then I can see that it's infringing. But this is not exactly the same that happened in the court because even if you have substantial similarity, whether it mattered by N gram similarity or kind of semantic similarity, whatever you measure. But at the end of the day, that's not the only part to make the judgment of infringement or not, which I'm going to cover in a bit as well.
00:11:12.195 - 00:11:18.787, Speaker A: Definition in the copyright. I'm just trying to understand what technical.
00:11:18.931 - 00:11:44.221, Speaker B: Yes. We can go deeper into this copyright engraving thing and you may get a clear of what I mean here. Yeah. So yeah. So there are so many lawsuits nowadays around the copyrights infringement. And the current issue around why judging infringement is very complicated is mainly going to like how you interpret the doctrine of fair use in United States. And it relies on two important components.
00:11:44.221 - 00:12:16.319, Speaker B: The first thing is something in the morning Talks. The speaker already mentioned that you need substantial similarity of the content that the model generates compared to the content that is copyrightable. Um, even though that definition is not clear enough. For example, when we're talking about similarity, it could either be like verbatim match. For instance, on the left you are seeing the original document, on the right, this is the exact match of document. And even if that is exact match, it also matters how long the overlapping. For example, 50 grams or 200 grams.
00:12:16.319 - 00:13:06.207, Speaker B: But let's see, if you generate something like A, it, you might think that is kind of encouraging. How about you generate something like B? It's fuzzy matching, not exact match, but still very, very close. And what if you generate something like C, which is like semantic similar to the original content? It's almost like a rewritten version of the original content. But would you regard that as infringement? So currently in the core, there's no clear definition of how you can quantify this measurement. But in this work, which I'm also going to give you more context in the bits we try to spell out, what are the potential spectrum that we can use technical tools to measure the similarity. But the second part is more tricky. It's like in order to say that this is copyright infringement, you also need to prove that the effect of the models used on the potential market for the copyright works.
00:13:06.207 - 00:13:42.587, Speaker B: For example, in the New York Times lawsuit case, what the New York Times may want to prove there is if the OpenAI model generates the New York Times article, it will potentially negative impact the New York Times market. And that is something relatively hard to prove. And actually there have been previous cases. Well, the substantial similarity holds. But on the use case side, things that use doesn't hurt the potential market, then the case is not considered to be infringing. And that's something actually related to Google, which is the Google Book use case. So Google Book is actually a search engine for you to search books.
00:13:42.587 - 00:14:23.663, Speaker B: And you can search a book here and you will get a large chunk of the preview of the book. And usually the previews can be several chapters which can already be viewed as substantial similar. But it takes quite a while to decide whether Google use Google Book is a fair use of the copyright content. But at the end of the day they think this is a fair use because the search engine on the search platform is actually trying to promote the underlying market of the book rather than undermine this. So finally, even if you're having substantial similarity of the content put on the search engine of several chapters, at the end of the day, this is Not a copyright infringement. So yeah, that hopefully this kind of answer your question on why this is a complicated both technical and legal question. Cool.
00:14:23.663 - 00:15:08.915, Speaker B: Yeah. So let's go to the final part of the talk which are the important and understudied technical questions. So before I go to that, I firstly want to spell out a few assumptions here. So the assumptions before the questions I'm going to explain is we are assuming like the model deployers here in the market are playing a good role. Well, they have tried their best to exclude copyright domains from training, for example, exclude the articles from New York Times and also they have tried their best to implement the guardrails, whichever the guardrail is, for example the DP production in the morning session talk. But even if you do that, will there still be something going wrong? Yeah. Give me an example.
00:15:09.295 - 00:15:21.355, Speaker A: Well, I mean you mentioned dp. I mean it's just one element. Adjacency requirement, adjacency constraint. One of two people together sued. Sued for copyright violation.
00:15:22.535 - 00:15:49.675, Speaker B: Yeah, that's, that's one potential way of going around. But something I'm going to show here is actually something quite easy but you might not get it correct. For example, you exclude the New York Times domain, but there are also copies of New York Times articles in Reddit. Actually in Reddit there's a subreddit from New York Times and people post articles here. Like as a general company you just square all the data. You might get lessons from Reddit but Reddit doesn't necessarily get licensed from New York Times. There still are issues.
00:15:49.675 - 00:16:57.625, Speaker B: So I'm trying to spell out the technical questions in three different ways. So firstly, what have been currently overlooked in the current deployed systems in terms of production and what might be expected way of triggering something going wrong? Also secondly, what if something going wrong and also based on the condition that the model deployer model trainer are trying to play a good role, what has a better way out than requiring this model deployers to destroying the model? Because back in the New York Times and OpenAI lawsuits, the New York Times is asking OpenAI to destroy the ChatGPT. Finally, at the end of day we do find there are infringements. Is there a way for the model trainers to fairly compensate the copyright holders? I will firstly start with the first thing I will try to show you. Actually it's very easy to have the current system going. Well let me give you an example. In the image generation system, as average user I might be interested in playing with model and maybe I will ask it to generate me some images.
00:16:57.625 - 00:17:27.013, Speaker B: For example Nintendo smartwield. If you send this prompt to most of the production level systems, usually it will say, sorry, I cannot generate because it already have some copyright protection inbuilt under that. But what if I ask you something else? I'm not asking for Mario, but what I'm asking here, give me something of Video Game Plumber. Maybe I'm asking for some general video game Plumber. Or maybe I'm like a bad guy asking for Video game Plumber. You guess it. Or give me superhero Galton.
00:17:27.013 - 00:18:16.465, Speaker B: You'll say probably Batman. Right. So when they try to send these prompts into the models and for both, like the open source model, like the playgrounds, and also the production level system like Dall E3, they can generate this copyrighted characters. This will be a severe issue because for instance, a generation of the copyright characters mainly to potential liability of the model deployer, for example Dall E3 or the playground model deployer. Also in some cases, maybe as a user, when I come to the system, I'm actually not asking for Mario and maybe I don't even know Mario at end of day. I just want a video game plumber and you give me Mario and use it for some downstream application and I may suffer from the liability of copyright infringement. So that's a huge issue.
00:18:16.465 - 00:18:29.265, Speaker B: And actually in this work we try to systematically study this question. We find that for 20 out of 50 characters we collect, we can use as few as five keywords to generate them without mentioning the name.
00:18:30.205 - 00:18:36.195, Speaker A: So if you take it back when they tell you this is actually copyrighted, you have the image now can I just put it back and ask the question?
00:18:36.885 - 00:18:52.225, Speaker B: Usually the model will refrain from answering the question. Usually the model will refrain from answering whether something is a copyright character. For example, you just put the image into the system. But if you ask do you think this is Mario? Then it will give you some author.
00:18:52.805 - 00:18:56.173, Speaker A: What if you ask for a video game plumber, that's not Mario.
00:18:56.269 - 00:19:07.715, Speaker B: Yeah, that's also a good thing. We tried. It will still give you Mario. And also even if you put Mario as an active prompt in the model, it may still give you Mario, which we're showing the paper, but I didn't put in the slides.
00:19:07.835 - 00:19:11.415, Speaker A: Yeah, maybe just Video Game Plumber should be copyrighted.
00:19:13.875 - 00:19:49.777, Speaker B: Yeah, there's only two of them. Which is the other one. Like sorry, if I generate that carry term, I may use that for my downstream application and I get lawsuits. Yeah, but yeah, I think. Let me tell you a bit more how we find this keyword actually very simple. So the first thing is you can try to use embedding similarity potentially maybe words that share similar embedding to the character's name, for example Mario are lucky keywords. You can also ask your favorite LLM to come up with keywords similar to Mario.
00:19:49.777 - 00:20:24.255, Speaker B: And something else is like you can use some paragraph level co occurrence with a character's name in some popular training data. For example, this is a piece of description of Mario in wikitext and you can see Mario plumber co occur Mario video game co occur. You can also do that for all the possible pre training compasses. And indeed in this work we do that for C4 pile open web text and also lion data set. While lion is a multimodal data set. So then we try to test which ones are most effective. So different groups of bar are corresponding to different number of keywords we're using and different colors correspond to different methods.
00:20:24.255 - 00:21:07.429, Speaker B: In most cases, using a few keywords can even be more effective than using a very long description while the description also doesn't contain the character's name. Also it seems like co occurrence with a Lion dataset is especially effective because it's a multimodal dataset. Something else we try is like ok, let's just try to pressure test Dallistry and these are all the cute characters we get from Dallas 3 without mentioning their names. We have a lot of examples in our website, so feel free to look it up and also play with it. We also have some disclaimer in the paper. We're not trying to break the system but try to show the vulnerability of the system. So hopefully I kind of convince you that there are various ways that potentially things can be overlooked even if you try very hard.
00:21:07.429 - 00:21:13.389, Speaker B: Yeah, Question generation model did you like.
00:21:13.437 - 00:21:25.551, Speaker A: Are either open source such that you know what that state what that filter looks like. For example, like for simple diffusion you can like go into the code and you can see what the safety filter looks like. So you just like I'm just like.
00:21:25.623 - 00:21:28.191, Speaker B: Not in the sense of like just.
00:21:28.223 - 00:21:31.631, Speaker A: To like try and map like what some of the keywords like that you.
00:21:31.663 - 00:21:33.655, Speaker B: Got that work really well to like.
00:21:33.775 - 00:21:36.711, Speaker A: What the filter is doing or like are both of these closed source?
00:21:36.743 - 00:22:18.593, Speaker B: Because obviously what's going on? Oh yeah, actually we do know something what is going on under the. So I'm not quite sure if you guys have noticed like when you send some prompting to the Dall E generation system they will quietly rewrite your prompts and you are able to see the rewritten prompt if you click on some Kind of a circled I on top of the system. We indeed find that these characters, these keywords, bypass the schedule. But going back to the original question, I believe indeed in the open source models they don't have guardrail. For example, for the playground one, it's very easy to break it. Yeah, doesn't have a guardrail.
00:22:18.609 - 00:22:20.409, Speaker A: Doesn't have a guardrail. Okay. Okay. So it's not rewriting.
00:22:20.497 - 00:22:29.445, Speaker B: Yeah. So we basically try to find these keywords using the public models and see how they transfer to the open source. Sorry, to the proprietary ones. Yeah.
00:22:30.145 - 00:22:51.255, Speaker A: Is there a correlation between the uniqueness of your prompt and in the sense that this. There's relatively few people in the training set who would match that. Like the plumber case versus something like immortal superheroes. And then you have like hundreds of them and this thing is capable of like finding a hybrid.
00:22:51.915 - 00:23:23.155, Speaker B: Yeah, I think that's a good point. I think things could hold on both. And for example, if you do see a lot of popular keywords co occur with Mario's name a lot of times and the model may kind of automatically connect those two terms and lead to the generation. But on the other end it could be the case that you have some very long tail prompts and that's very like only tied to a single character and then that would trigger the long tail memorization happening. So I think we haven't explored the long tail end. We are mainly looking into the more popular end. But that's a good point.
00:23:23.155 - 00:23:56.245, Speaker B: Cool. So let's continue to the second part of remedy. Oh, sorry. If you ask it specifically to generate something that is not copyrighted, I'm curious because they technically have information about the licenses of the images that along with the description. Yeah, that's a good question. So we actually tried that in our paper. But also a caveat is like most of the image generation system, especially the public ones, they don't support the system prompt.
00:23:56.245 - 00:24:32.193, Speaker B: Yeah. But we also tried it, even if it doesn't support the system prompt. But the thing I'm going to show in nas, which is for the language model, we do try that in the system prompt saying do not operate characters and copyright content and it's not very effective, which we're seeing. Cool. So let's go to the second part of like, okay, let's say we already find that they are infringing content. Like for example, for the Mario case or for the New York Times case. Are there better ways out other than destroying the infringing model? I think that's a very hard question, but usually when you have a hard question, maybe you can look into back into the history to see how people solve things.
00:24:32.193 - 00:25:13.707, Speaker B: A while ago, for example, in the very early age of the Internet, similar question may also apply to the website copyright. So back then in the Internet age. Sorry, we're still in the Internet age. Back in early search engine age. Let's see. As a user, if I find a potential website is potentially infringing on my copyrighted content, then what the user can do is to submit a takedown request to the search engine platform and then the search engine would have a certain period of time to take the content down. As long as it take down in this period of time, then the search engine is not liable for for the encroachment.
00:25:13.707 - 00:25:55.987, Speaker B: And that is enforced by the dmc. Copyright takedown. Yeah, by law. Is then Google responsible to check whether the claim of infringing is actually right? Because you can have somebody. Yeah, I think the pipeline is more kind of like Google, sorry, the user will submit through the DMCA harbor. And yeah, is it the agency that is supposed to determine whether it is actual infringement or. I'm also not quite sure who's actually making the call, but there's a pipeline for doing that.
00:25:55.987 - 00:26:57.799, Speaker B: Yeah, yeah. So I think the question we actually try to ask in a recent work is can we also kind of operationalize this kind of copyright takedown in language models? So let's see New York Times find something fragging and then it comes to the chatbot builder and saying like please take down my content. As long as you remove the content in a few days, then the chatbot does not need to be destroyed. So the most important question here is like how can you take down a piece of content from a trained model? Because for the search engine case you just do a simple deletion from the database, then that's fine. But how can you delay articles from the trained model? So there are actually various ways or proposals around this and we try to evaluate like a subset of them in our very recent work. So going back to your question, one very naive and straightforward way is just to do direct system prompting. And actually we find that for a lot of system prompts in the current production level system, they do have some wording around copyright protection.
00:26:57.799 - 00:27:36.463, Speaker B: For example, this is a wording from the GitHub Copilot. This is a wording from Databricks DVRX. Well, actually the system prompt is trying to tell the model you were not trained on copyrighted books. I'm not quite sure whether the model beliefs that, but yeah, and also as a naughty author lyrics. Yeah, and also for the Microsoft Bing chat you will also find something similar like that. So that seems to be a very simple way, but also possibly it relies on the model's understanding of what is copyrightable and what is not. The second thing that you can try to do here is at the going time, as long as you have a whole database of the things that don't want to generate, you can check and rewind.
00:27:36.463 - 00:28:15.485, Speaker B: Let's see, this is the thing that we don't want to generate which is a copyright content. Our goal is to prevent certain type of similarity. For example, we prevent the 15 gram consecutive overlap. You can try to swap in different goals here depending on how you define the infringement. And then during the decoding time you can just try to check at every point and as long as you find the potential infringing in this case 515 gram overlap, you rewind and then try a different word. And then this will automatically avoid the goal that you're having here. But it also depends on how flexible your goal is and also another kind.
00:28:15.485 - 00:28:16.625, Speaker B: Yeah, minutes.
00:28:17.975 - 00:28:20.335, Speaker A: What's the impact overhead of doing this new ip?
00:28:20.415 - 00:29:05.635, Speaker B: That's a good point. We try to a very efficient implementation in the paper which is based on Boom filter and that's almost no overhead but it also is because our data store is not too large as several trillions of tokens. And the third thing is you can use some sort of training based algorithm. For example on learning there have been a lot of proposals of how you can potentially remove some knowledge or content from a model. We tried to do the evaluation and we found none of them can balance the utility and non exact matching. Risk reduction. What I'm reporting here is the risk reduction or the similarity measurements when the utility is balanced.
00:29:05.635 - 00:29:36.225, Speaker B: These are the results for vanilla case. The first block is exact match and the second one is for near duplicate match was fuzzy matching and third one is semantic similarity. And this is when you use system prompt. So on average you can reduce the similarity but in the worst case this is a violent plot. In the worst case you cannot prevent it from happening. And this is Memphis decoding which is like the inference time check and rewind. For exact match cases it handles it quite well.
00:29:36.225 - 00:30:26.905, Speaker B: As you can see you reduce the bar by quite a lot. But for fuzzy matching and semantic similarities they are not very helpful. And these are all unlearning Masters still cannot work very well for non exact matching scenarios. I also want to spend a bit more time on learning because I always have some troubles. Every time I come across some unlearning work, I know some of the audience is working on learning, so please spare me about that. I think there are a lot of definition of unlearning, but to put it fuzzy, I would call unlearning to be masses to update model weights so that updated model behaves as if it was never trained on certain data. I think I always have trouble in understanding first, what are the expectations of unlearning? When we are talking about unlearning, we indeed have a work on that to better dive into this.
00:30:26.905 - 00:31:31.175, Speaker B: Usually when people are talking about unlearning nowadays it's either due to privacy motivation, which is actually the original of unlearning work that's due to the GDPR and the right to be forgotten or or due to the copyright kind of like motivation and usually like the data owner will just submit an online request. For example please remove this book from the target model and then the target model takes a request and do their own learning. So you will see like there are two stakeholders in the system. One is like the data owner who might just want their work to be removed and it doesn't care about how the removal side effect will be on the final model. And the other part is like the model deployer and they might have completely different expectations. For example for the data owner side, depending on its downstream applications of unlearning, it could be either be to reduce the rebating memorization, reduce some knowledge memorization or no privacy leakage. But on the multi deployer side they not only need to be responsible for the data owner, they also need to be responsible for the other owners or the other user who are still using the system and using their productions.
00:31:31.175 - 00:32:04.065, Speaker B: First thing first, they want to preserve the utility of the system. Also they want the system to operate in an easy to maintain way. For example, the unlearning method should be able to scale for both smaller data sets and larger data sets. Also it should be operating a sustainable way. Every day there might be some user coming in and say please remove my data. We need to operate this on learning request in a sequential way. You cannot have your own learning method to be only able to accommodate maybe three consecutive requests and then afterwards it will fail.
00:32:04.065 - 00:33:07.633, Speaker B: So we try to define this six different ways of evaluating on learning algorithms. We're not calling for that every unlearning algorithm should satisfy all the needs, but instead we just take the very popular online learning algorithm which are eight of them out there for the language models and we try to Evaluate to what extent the satisfy the need or not. And we open source all the evaluation code in this website and also we have the leaderboard while we also welcome people to submit new on learning algorithm to see to what extent they satisfy this needs. And another thing I kind of have trouble about learning is that people are all talking about like maybe you want to deploy on learning in the future, but that's a new kind of interface for user. Every time you have a new interface, it may be also introduced new potential security vulnerabilities. In another recent work, we try to understand what type of new system vulnerabilities could unlearning introduce. As the average user, you have some data, you can submit your data and then you unlearn it.
00:33:07.633 - 00:33:41.635, Speaker B: But this also means that an attacker can also do the same thing. Let's say we have attacker, they can craft some unlearning example which look very similar to to the original Valley unlearning example. But as long as you run unlearning on top of that your model will reach zero accuracy. We indeed managed to do that. This is an example from the original ImageNet datasets with valid unlearning all of this our original training images. And this is the attacker's submission. If you do not carefully check that they are almost looking at authentic and also valid images.
00:33:41.635 - 00:34:19.969, Speaker B: But if you run learning on this examples, the accuracy of the final model after unlearning actually go below 5%. The tech is actually very simple. So basically we optimize in the input space such that the model ongo the unlearning will finally have a very high loss. And basically you can formulate that as a way that you have a hypothetical computation graph for the computing the loss. And then you try to unroll all the graph and optimize in the input space. This is actually something very very similar to meta learning algorithms. And we actually borrow their implementation in trying to do our attack.
00:34:19.969 - 00:34:58.975, Speaker B: And we are able to mix attack very fast. Each attack takes fewer than one minute on the single A100 GPU. Which means that these vulnerability is very hard to patch. In our paper we also discuss what are the potential ways to catch them. Now the model strain for ImageNet recognition task. Like what task is the accuracy going below 5%? Oh, it's a task accuracy like the image recognition task general on the task just removing some subset of images. Yes, because we craft these images such that it will maximize the loss of the model after you run learning.
00:34:58.975 - 00:35:46.391, Speaker B: Yeah, and the perturbation is very slight. It's Kind of an adversary example, but in a higher order sense. Yeah. Cool. And the third thing I want to talk about here because more recently when people are talking about unlearning, they are opening up a new potential use case which is for safety. Another question we answered recently is can unlearning also be applied to remove harmful knowledge? There is a famous paper from Scale AI and also center for AI Safety, which is the WMDB benchmark, where they propose a benchmark and also they propose a way to remove harmful knowledge from a model by using unlearning. Um, so the question we ask here is like is the harmful knowledge actually removed from the system? Or they are just kind of like hide it or obfuscated? Um, yes.
00:35:46.391 - 00:36:20.927, Speaker B: So they define the knowledge. So they just do the QA accuracy on some bioweapon like dataset and they, they use that as a way to say like the knowledge is there or not. Yeah, so we're just using their definition and try to see whether the system debuted is robust. So we try to do some tasks in order to answer this question. So basically we will show the QA accuracy on the bio or chemical hazards. And this is original accuracy for the model before unlearning for safety. And this is the accuracy for the model after you unlearn.
00:36:20.927 - 00:37:07.875, Speaker B: It seems like the unlearning is indeed playing a role here because the model's accuracy answering this very harmful task get reduced. But you can use various ways to kind of pressure test the model in a non malicious way. For example, you can fine tune on benign data like as long as the model has already forgotten all this harmful knowledge. If you find you on benign data, the model should not be able to increase its capability in answer harmful questions. But if you find your benign data increases and also you can try to perturb the activations of the model in the inference time. As long as your model genuinely forget about harmful knowledge, it will not increase accuracy. But no, and also you can do prompt optimization, you trigger the model to answer the question or you can remove partially of the weights of the model.
00:37:07.875 - 00:37:57.167, Speaker B: So by doing all this proxy testing they can all substantially increase the model's accuracy on this harmful knowledge. And we also do the test on another learning method called NPO and we observe some similar pattern. So this makes me kind of pessimistic about the way forward for using unlearning for safety. We haven't tried that because we directly take the checkpoint released by the model and then do the testing for integrity. But yes, I think if you try to combine this maybe if they are vulnerable in different cases, maybe you even see more kind of increase in the final accuracy. But if they kind of like can help each other the panel will be different. But we haven't done that testing yet.
00:37:57.167 - 00:38:46.663, Speaker B: Yeah, cool. So yeah, towards the end of the talk I'm not going to show you more work but I want to discuss something that I'm very interested in working on and I think it's an important question. It's like how can we fairly compete as a copyright holders Because I believe copyright takedown is now the end of story because if I can generate New York Times article it means like actually use our data for training. Taking down of that doesn't actually compensate the New York Times. So people have been talking about various way of doing the data evaluation. So one of the most popular method is called sharply value as a Nobel Prize winning solution but it struggles with the pre training scale. There are also some solutions that could scale to like the pre training scale but they didn't win Nobel Prize so they may not convince all the stakeholders.
00:38:46.663 - 00:39:11.415, Speaker B: So we have a dilemma here. Like some solutions might work but yeah, it's hard to convince everyone to adopt them. But for the other side like some solutions have proven guarantee but you cannot have them actually work in the system. I think it's a very important question to solve but currently I don't have a very good idea. I think they are all the brilliance minds sitting here. So maybe if you guys are interested in spending some time on empirical work I would suggest that will be an interesting area to work on.
00:39:11.995 - 00:39:12.467, Speaker A: Cool.
00:39:12.531 - 00:40:10.159, Speaker B: So yeah, the takeaways, we have some very quick takeaways. So for the gen 9 copyright I think we have very important non technical and also partially technical questions and I think for that front the legal definitions and also legal effort may need to evolve together with the technical inputs. And also we have a lot of important technical questions, at least some of them in our talk. I think the best way forward is for policymakers and also for technical researchers to work more closely with each other to advance the fields faster. Actually my colleague at Google, Catherine, she has been running the center for Generative AI Law and Policy Research and she has organized a lot of workshop in ICML last year and also in ICML this year. Also in the middle they organized a workshop at dc. Well there are half audience action from the policy community and half the audience are coming from the technical community.
00:40:10.159 - 00:40:28.955, Speaker B: So I think that's kind of like a good starting point and you guys are all welcome to participate in all these discussions. And finally, I want to express my gratitude for all my collaborators. Some of them are sitting in the audience with Jia and also dougl. And here are all the references to all the work I covered here. Thank you.
00:40:32.875 - 00:41:08.107, Speaker A: Questions? Is there any work on like something like a revocation list? So just like you have rags, you have new content and you have. Obviously the neural network and the rag have to work together. Imagine having the delete list, right? I mean people do this for keys, certificate authorities. I mean they haven't solved the problem. But why don't you take all of this content that supposed to be copyrighted and put it in the database and then I guess change the model architecture? I guess I'm asking. I don't know anything about machine learning.
00:41:08.251 - 00:41:15.787, Speaker B: Yeah, you're talking about like for your model weights. They are only trained on permissive data and you have a database, you get.
00:41:15.811 - 00:41:38.669, Speaker A: Trained on whatever you want. But when someone wants you to unlearn or someone asks you to take things out, put it in a delete list, put it in a revocation list. And then when you respond to a query, just like you would respond to a query looking at the rag to get the latest response, you check this other database just to make sure you're not responding incorrectly.
00:41:38.757 - 00:42:25.617, Speaker B: Yeah, I think that's actually something similar to what we talked about for the decoding time intervention. But the problem there is first thing first, it requires the goal to be quantifiable. You don't want some model to output, let's say N gram overlap with content, that is something quantifiable. But currently we don't have a quantifiable term in the legal space. And second thing is, as Venice just mentioned, if you're having a very huge takedown list and every time you're checking against that during the decoding time, there will be some computation overheads. Another thing relevant is actually almost all the piece could be copyrightable as long as it's written by humor. So if you do want to include all the copyrighted content, then it's kind of like an endless database.
00:42:25.681 - 00:42:53.759, Speaker A: Yeah, thanks. So the question about the related question about the training data, whether it can. So what's your personal, maybe legal position? Because one option is you can never include, you know, copyrightable data and learning. Hopefully a lot of questions kind of disappear. But on the other hand, you said that there are reasons maybe to include it. Like Google has a fair use case of Quaker edible data. Maybe almost everything is Quaker.
00:42:53.759 - 00:43:04.343, Speaker A: So what is the current position, both legally and your personal or whether Code data should be part of the training set and to what extent I don't.
00:43:04.359 - 00:43:08.675, Speaker B: Think I can have opinion on that. Sorry about that because I recently joined. Yeah.
00:43:10.615 - 00:43:33.501, Speaker A: So you said it's currently as part of the training set because it's hard to exclude those engineering things through Reddit or whatever. Are you saying legally it's actually clear that you're not allowed to have comparable data in the training set, that it's just solved by coincidence because it's hard to exclude it or actually there is no loss? Maybe. So why is basically yeah, I think.
00:43:33.533 - 00:44:07.955, Speaker B: In the legal space sometimes the court will favor from the dependent perspective, it's like if you try very hard to prevent the copyright infringement from happening, even if it happens, then it may not it reduce your liability. So in that sense, if you try very hard to exclude the copyright domains in your pre training data, even if for example Redis you can have some copy, but you try very hard and also you try very hard to deploy some guardrails at inference time and also at training time, then you can show this evidence to the court and then it depends on their judgment. Yeah.
00:44:11.655 - 00:44:25.195, Speaker A: So I'm not familiar with the unlearning literature. Can you give me some intuition about why it's possible since quite daunting task. Can you give me some why seems like you believe that it can be done. So what's the intuition behind why it's possible?
00:44:26.215 - 00:45:10.263, Speaker B: I actually am not taking the position that I think unlearning is possible. So my current position is unlearning, especially like the definition of unlearning in terms of compliance with regulations are not possible. Because if you do want to unlearn for downstream use cases, for example for privacy protection or copyright protection in the provocate, you need to prove that the model after unlearning is almost exactly the same as the model trained from scratch without the data. So I don't see that to be possible in the large model regime. For smaller models there used to be work to show that you can do that or you can approximately achieve that in an alpha beta way. Yeah but for the large model thing I'm very kind of pessimistic about that.
00:45:10.399 - 00:45:14.191, Speaker A: Why small model make it possible the same architecture.
00:45:14.263 - 00:45:19.915, Speaker B: Yeah. If you are kind of like logistic regression model, you can have some proof of. Yes, yes.
00:45:21.455 - 00:45:34.577, Speaker A: Yeah, I'm not sure. Can you train like a separate model that just tries to distinguish between is copyrightable, it's violent copyright and not violent copyright.
00:45:34.721 - 00:45:42.601, Speaker B: Yeah, I think that's also something relatively complicated because it Actually depends on the definition of copyright violation, which I.
00:45:42.713 - 00:45:55.905, Speaker A: My point is separate model can be very flexible. It does not involve this retraining. So it's just separate model. And whenever you change your definition, you know, whenever you change your database of copyrights, pictures and so on, just.
00:45:56.765 - 00:46:36.969, Speaker B: Yeah, I think for the text domain maybe that could be potentially possible. And I'm not quite sure whether that will be something the legal space will favor, but I see that is kind of possible. But for the image domain, especially when you are talking about copyright characters, how can you measure the similarity in that space? Like embedding similarity or those sort of. Kind of like automatic similarity may have a lot of paid fors in those. Yeah, so I think it depends on the modality you're talking about. But I also think there are having some discussion around whether you can train that model. So for example, in general, they have been one or two papers around.
00:46:36.969 - 00:46:39.845, Speaker B: How can we build a better copyright detector? Yeah.
00:46:42.625 - 00:46:51.485, Speaker A: This is my. Again, I don't. I know nothing, Right. So earlier on you had this comment that right now machine generated stuff is not.
00:46:51.785 - 00:47:20.375, Speaker B: It depends on like whether the machine generated concept part is like the major piece of the. So if the traditional element of authorship is done by the machine, not by the human, so according to this judgment, it's not copyrightable. But there's another case, for example, if you are writing your article and then you use ChatGPT or whatever language model you have to rewrite, then that's kind of subtly different, but it also is kind of like case by case analysis. It's hard to make a decision.
00:47:20.835 - 00:48:09.335, Speaker A: So I guess kind of a crazy question, right? But you could imagine a world in which you say, instead of training one model, I will train three. I'll take my giant enormous data set, partition it, deduplicate it across three disjointed sets of data for sampling all kinds of stuff. Okay, you take one and you tell it. Now you generate prompted generate synthetic version of this one and this one. Then now you have a new set of data train on that. Now everything has been. Nothing was generated with respect to itself.
00:48:09.335 - 00:48:40.837, Speaker A: The model never saw anything. Right? From the perspective that is entirely machine generated. If this kind of procedure doesn't suffer from the problem of training on generated data. So if this could be. That's where the third model comes for aligning. Okay. Presumably this could sanitize everything from at least the fact that.
00:48:40.837 - 00:48:56.735, Speaker A: Wait a minute, I mean, you touched the data set, the third of the data set to generate rather synthetic data. So certainly from a Privacy standpoint. That's not right. Yes. It's not differentially private. I mean, it could be different. You could make it differentially private by.
00:48:56.735 - 00:49:25.793, Speaker A: By privatizing the synthetic data generation process. But I don't think that's what you were saying. I'm saying that if you say this stuff generated by machines from a legal. Oh, you're talking for the purposes of this year. This is like the year to sanitize every single thing through this cleansing process. And then you say, hey, that year it wasn't copyrighted. And now you checkpoint it this time and you're like, good to go.
00:49:25.793 - 00:49:36.833, Speaker A: I think they changed a lot. Like, can't retroactively change it. Is this like insanity? Probably it is, but I just want to know why.
00:49:37.009 - 00:49:44.845, Speaker B: Yeah, I. I still feel like I'm not the best person to answer this question. Like it seems like. Yeah, I think this actually makes technical question.
00:49:48.305 - 00:49:54.765, Speaker A: Is this insanity is a technical question? No, no. Is doing this a technical question in your sense of text?
00:49:57.265 - 00:50:23.775, Speaker B: Yeah, I think if you think like the authorship, like for example, the traditional elements of authorship for the thing that you just mentioned, it's just like generate the text. We agree on that conclusion. Then I think that piece is kind of like a viable way of creating. Yeah. But also this definition will also potentially evolve in the future. So we don't know.
00:50:24.555 - 00:50:26.107, Speaker A: Very important feature.
00:50:26.211 - 00:50:26.935, Speaker B: Yeah.
00:50:28.955 - 00:50:41.771, Speaker A: So in your example of last image generation model generates some copyright characters. So how do you decide that is copyrighted? Using some matrix or just human image?
00:50:41.963 - 00:51:14.615, Speaker B: Yeah, that's a good point. We have both alterators and human evaluations. So yeah, we mainly use operators, but we calibrate that against human evaluation to show that they have very high consistency. But because we try to do the kind of like pressure test for very large scale kind of model. So we cannot afford have every one of them annotated by human. Okay, thank you. We can bring a question offline.
00:51:14.615 - 00:51:14.855, Speaker B: Yeah.
