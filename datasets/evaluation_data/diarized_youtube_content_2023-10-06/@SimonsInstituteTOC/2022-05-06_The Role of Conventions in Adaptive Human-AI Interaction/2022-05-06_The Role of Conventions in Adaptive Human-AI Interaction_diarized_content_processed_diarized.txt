00:00:00.080 - 00:00:17.126, Speaker A: I was just do one last introduction. Yeah. All right, thanks for coming back, everyone. So, yeah, for the very last session of the program, we're super happy to have Dorsa Sadiq here from Stanford telling us about the role of conventions in adaptive human AI collaboration. So I'll let you take it away.
00:00:17.230 - 00:00:27.678, Speaker B: All right. Okay. Thanks for inviting me. Excited to be here. So, last talk of the program. This is very much practical. It's very different from the talks that you have seen in this workshop.
00:00:27.678 - 00:00:54.958, Speaker B: I'm Dorsa. I am a roboticist. I work on robots, working with humans, working with each other. So a little bit, like, not as much theory in this stuff, but maybe there are some insights that we can learn from when we think about the theoretical side of things. And I've learned quite a lot during this workshop, like, going back to the robotic side of things. I think there are a lot of interesting insights and making connections. So what do I work on? So, I work on robots that generally work with different types of other agents, other intelligent agents like humans.
00:00:54.958 - 00:01:47.712, Speaker B: And you're starting to see a lot of these types of robotics agents really making them out of factory floors and working with us. An immediate example is autonomous cars. So we're starting to see autonomous cars interacting with other drivers, human drivers, other autonomous cars, pedestrians, things of the form of service. Robotics is really like getting there, having robots in homes, trying to help us do various types of tasks, or assistive robots, which is an application that we're actively working on. Very interesting, very difficult problem. So in all of these applications, we have some sort of interaction between an autonomous intelligent agent, an AI agent, interacting with another intelligent agent, usually a human. And oftentimes one way of modeling this type of interaction in these types of systems is trying to take a game theoretic perspective, a very simplified game theoretic perspective, where you bring the two agents together.
00:01:47.712 - 00:02:23.754, Speaker B: This is a very common thing that we do in human robot interaction. And you think about the human building a model of the robot and having some representation of the robot's policy, the robot state, and what the robot is trying to do. Quite a bit of work in human robot interaction goes into this, like, what is the model of the human, of the robot? And then similarly, how does the robot model the human is also like. A lot of work goes into that. But once you have that, you pair up these agents, and then you get them to work on a task like playing chess. This is a made up example. In general, we are looking at mostly cooperative types of tasks.
00:02:23.754 - 00:02:53.404, Speaker B: And if you pair up these agents in this game theory and do theory of mind modeling. As we know, that type of recursive belief modeling is going to be quite expensive. So that is not very practical. So in every single field that does some sort of theory of mind modeling to actually get it to work, we end up making some sort of approximation. And the approximation is, well, let's not solve the full game, let's just cut it at some time. Step solve an nth order theory of mind. Let's do a stack over game, and that is maybe sufficient for what we want to do.
00:02:53.404 - 00:03:32.092, Speaker B: Again, this shows up in natural language processing, often referred to as pragmatics or rational speech act, shows up in multi agent reinforcement learning as opponent modeling, partner modeling. Modeling shows up in robotics, and that type of approximation is fairly good. So, like, this is something that, for example, we have done in the space of autonomous driving, where we basically modeled an autonomous car as an agent who's trying to optimize its own objective. So sum up rewards, and that is usually dependent on its state and its actions. So the actions here are continuous. This is steering angle and acceleration of the car, and it also depends on what the other agent is doing. And this other agent is a human driver.
00:03:32.092 - 00:04:31.826, Speaker B: So a good question to ask is, well, how does a human act in the world? And a very common model is to say, well, humans are intelligent decision makers, so they're also probably optimizing some sort of reward function. And at some point someone will come and say, well, humans are not exactly intelligent decision makers, so they're not exactly optimizing a reward function. So we start making approximations about what the human does. And there's quite a bit of work around modeling humans as these noisily rational agents who are approximately optimizing something like RH. So again, a lot of work in the space of robotics and human robot interaction goes into learning what these reward functions even should be like. That is not even clear, like what the objective should be when we're trying to optimize for a car, driving down the road is very complicated. How do you think about comfort, and how do you think about safety and all these other objectives that you care about? It's a multi objective optimization in practice, and we are trying to pack all of that in a single reward function, and that tends to be a pretty difficult problem.
00:04:31.826 - 00:04:58.738, Speaker B: So I would say like a particular direction in robotics and AI in general is figuring out reward design and what this reward function should be, and figuring out how humans drive. Like behavior modeling is also a big part of it. But imagine we have all of that. If we have all of that, then we need to solve this game. Theoretic interaction actions of my robot car depends on the actions of the human. Actions of the human car depends on the actions of the robot. As I said, that is computationally very difficult.
00:04:58.738 - 00:05:39.830, Speaker B: So let's just cut the game. Let's assume that my robot is influencing the human, but the human is observing the optimal actions of the robot. So I solve it as a Stackelberg game, and I go based on that. And then we've demonstrated that this type of behavior allows us to get behaviors from autonomous cars that are more interactive, more assertive. For example, if a car wants to change lanes using this type of model, what ends up happening is that our autonomous car ends up nudging in, and by nudging in, it influences the other driver to slow down and make room for it. So let's say that this driver was going 80 mph. Most prior autonomous driving work, what they do is they want to be safe.
00:05:39.830 - 00:06:06.892, Speaker B: They look at that 80 mph, like, oh, this car is going to continue going 80 mph. So I can't change lanes. And they don't really change lanes. Here's actually a video of a Waymo car not being able to change lanes because of exactly that reason. So it is driving in an exit lane. It starts signaling at some point. It's like doing all the rules of the road, like all the right things to do, but it's not doing anything assertive, it's not doing anything interactive.
00:06:06.892 - 00:06:53.356, Speaker B: And because of that, it is stuck in this lane. It's exit, come back around, try again, which is not that ideal. So it's kind of silly that you think autonomous car companies, for the longest, didn't really have these types of more complicated interaction models, but they actually didn't. For the longest, roboticists were thinking, we just want to build better, good robots that are good at driving, but they weren't really thinking about interactions between other drivers. And using this game theoretic model that I was talking about earlier, we could actually get behaviors like this, where you start nudging in. So this driver that was going 80 mph, it's not going to go 80 mph anymore, it's actually going to slow down. And because this vehicle has a model of how it is affecting this other driver, if I nudge in front of it, it's actually going to slow down.
00:06:53.356 - 00:06:56.532, Speaker B: And that is the knowledge that we get from this game theoretic modeling.
00:06:56.668 - 00:07:16.550, Speaker C: Just to be clear, I was wondering, I'm also using this window, and I hope it's you know, for way more. Like, presumably they want to minimize, make the probability of an accident, like, really, really, really small. Right. Is that part of the. I mean, like, what is in their calculations?
00:07:16.622 - 00:07:21.166, Speaker D: Yeah. So it kind of goes back to what should be this reward function that the robot is optimizing.
00:07:21.270 - 00:07:22.726, Speaker B: And if it is just safety.
00:07:22.790 - 00:07:37.084, Speaker D: Right. Like, if it is just that I don't want to hit any car. Like, the best thing you can do is just not leave the, like, house. Right. Because that way, like, you won't hit anything. But it's really, like a combination of things. And figuring out that right balance is pretty difficult, like, coming up with the right reward function.
00:07:37.084 - 00:08:09.184, Speaker D: I remember I was talking to one of these driving companies, and they were having a lot of difficulties when you would sit in their cars, you'd feel nauseous at the end of it because it would, like, every plastic bag you would see would slow down because it'd be like, oh, there's an obstacle. I got to slow down. And that's very uncomfortable if you keep slowing down and breaking. So again, it goes back to what should be the reward function you're optimizing. And similarly, if you're not modeling interactions, you want to bring down accidents. So you don't want to get close to anyone, because if you get close to anyone, like, being more assertive, it is also being more aggressive and actually.
00:08:10.324 - 00:08:14.780, Speaker C: For them, like, but being out from a single accident, it's, like, huge problem.
00:08:14.852 - 00:08:15.036, Speaker D: Right.
00:08:15.060 - 00:08:19.064, Speaker C: I mean, like, Vancouver, you know, ran over this bicyclist.
00:08:21.644 - 00:08:27.572, Speaker D: Yeah. Yeah. And that's a very interesting point. Yeah. Because car companies these days are definitely more on the conservative side of things.
00:08:27.708 - 00:08:29.484, Speaker B: Which is interesting because when I see.
00:08:29.524 - 00:08:51.050, Speaker D: An autonomous car, I know it is going to be on the conservative side of things. So I probably cut in front of it because I know it's going to slow down and that kind of multiple learning agents in the same. Yeah. Okay. So this is wonderful. Right? So I can do game theoretic type of modeling.
00:08:51.082 - 00:09:23.164, Speaker B: I'm not solving the full game because I was arguing that it's difficult, but these types of approximations work kind of. Okay. But the thing I want to spend most of this talk today is actually not this type of game theoretic modeling. The reason for that is there are a lot of tasks, coordination tasks, which I don't think in these coordination tasks, we as humans make game theoretic modeling. So, for example, imagine you want to move an object, like two robots, they want to move an object together. Let's say each one of these robots has partial observability of the world. They only see the obstacles close to them.
00:09:23.164 - 00:09:34.024, Speaker B: This robot doesn't really see these obstacles. This other robot doesn't really see these obstacles. And they want to move this rod together. They don't communicate with each other. They're decentralized. They need to figure out how to do this.
00:09:38.884 - 00:09:54.698, Speaker D: Is it a successful trial or not? This is a successful trial, yeah. So the task is don't hit the obstacles and then move the rod and put it on the table. Down to the table. Not to the obstacles, no. Yeah, not to the table. Avoid the obstacles, but they don't see the obstacles. They only see, like, the obstacles close to them.
00:09:54.698 - 00:10:12.862, Speaker D: So it's kind of a partial observability multi agent, and it's difficult. Right. Like, if you want to do the game theory type of modeling, it's actually pretty difficult to do this. But consider two humans moving a table. But people can do this, like, easily. And this one might argue they're using language. Tell them to not use language.
00:10:12.862 - 00:10:23.422, Speaker D: Blindfold them. People are able to, like, move a table, like, move a cache, like, very much like, easily by just, like, feeling the forces. So. So the question is, what are people doing when they're moving the couch or moving a table?
00:10:23.478 - 00:10:23.750, Speaker B: I don't.
00:10:23.782 - 00:10:25.860, Speaker D: I don't think they're doing theory of mind modeling.
00:10:25.942 - 00:11:10.272, Speaker B: I don't think we are building these belief structures in our head. One thing I know about humans is that they're bounded rational, they have bounded memory, and they're not going to be able to capture more than a few bits of information in every interaction. The question we have is, what is it that people are doing? And could we incorporate that in our multi agent interactions? Here's another example. We started looking at playing a game of air hockey, mostly because PhD students wanted to play air hockey in the lab. So we ended up getting an air hockey table. But when you're thinking about playing a game of air hockey, if I were to do this game theoretic modeling, and this was my partner, what would I do? I would look at the action space of this partner. What is the action space of this partner? So the partner is using their arm.
00:11:10.272 - 00:11:48.934, Speaker B: That is a seven degree freedom space. So a human arm has seven degrees of freedom, like any other fancy robot arm has seven degrees of freedom, lives in r seven. So if I want to play with this partner, I really need to build a belief structure in that seven degree freedom space, which, again, I think, computationally is very difficult. But if I'm playing with the partner. I think what I do as a human is I'm very effective in capturing the sufficient information for this task, which I would argue is just a direction of the puck. In this task, maybe I'll just look at which direction the puck is being pushed. Is it to the left or right or center? And that is oftentimes sufficient for me to play a game of our hockey.
00:11:48.934 - 00:12:16.692, Speaker B: At this point, you might say, okay, this direction is really the intent or goal. And there is a ton of work on intent modeling and doing belief modeling over intense. So what is the big deal? But most prior work, they're like, oh, there is a three degree freedom like space for intense. That's the belief modeling over that. But I'm not assuming that I have access to this. I'm not assuming that I know what is that intense space. What I'm really like getting at is we, as humans, we quickly figure out what that space is.
00:12:16.692 - 00:12:33.694, Speaker B: And ideally, I would want to have robots that are able to do that. Robots that can, just, based on the interactions, learn what is that low dimensional representation that captures the interaction and use that low dimensional representation for coordination. So that is really what we would like to do.
00:12:37.034 - 00:12:42.122, Speaker D: It seems like in many games, it's enough for practical purposes to estimate the.
00:12:42.138 - 00:12:48.986, Speaker A: Amount of conservatism or aggression of your counterpart.
00:12:49.170 - 00:13:20.732, Speaker D: Does that intuition hold up? So, like, that intuition holds up. I guess the thing that I'm trying to get at is, let's bring the dimensionality of what you're like, how you're representing your partner. And what I'm arguing is that humans are really good at figuring out context and figuring out what that space, that action space is like. In a lot of, like, a lot of times, we sit down and say, oh, action space is a. And we assume that that is given. But in practice, right? Like, if you take the action space that is given, that is a seven degree freedom space of the human hand. I don't want to deal with that.
00:13:20.828 - 00:13:22.004, Speaker B: I want to deal with an action.
00:13:22.044 - 00:14:05.192, Speaker D: Space that's much smaller and the action space that's much smaller, I would argue, is this intense directionality here. And the question is, can I figure that out by just looking at the data? Can I learn what that action space is? May I ask one question? So, yes, the humans have the ability, first of all, whenever they participate somewhere, to understand, is it exactly what you're saying or I'm trying to understand. Would you like something, like, to do mindful learning, to do lower dimensional representation of the robot somehow? That's. Yeah, that's I'm arguing. Humans are really good at that.
00:14:05.288 - 00:14:05.848, Speaker B: I'm arguing.
00:14:05.896 - 00:14:40.776, Speaker D: Maybe we can try to do that. Maybe we can do some sort of dimensionality reduction on the trajectories, and maybe that will give us the space that we need to work in. The dimensionality reduction in the previous example is that the arm has seven. But I just look at the other direction, okay? I just look at direction in practice, and I don't want to tell the robot to just look at direction, because that's a lot of information. So the robot or direction is the thing that matters. It might be something else in a different scenario. All right, so, yeah, so I'm going to refer to this low dimensional representation as a convention.
00:14:40.776 - 00:14:46.780, Speaker D: I'll come back to why I'm calling it a convention. All right, so we decided to do this with two robots.
00:14:46.812 - 00:15:07.344, Speaker B: So we have an ego agent. This is my robot that I'm planning for. This is the robot that I really want to play this game. And I have this other agent. I'm calling it other agent because, like, what I'm doing is not making assumptions about this being cooperative or competitive. It could be an opponent, it could be a partner. And this other agent, it's following a policy.
00:15:07.344 - 00:15:30.134, Speaker B: It has seven degrees of freedom. So the policy, from the perspective of the Ego agent, is the seven degree freedom space. It doesn't even observe that. But I'm telling you guys, that this other agent is really pushing the puck either to the center or to the right or to the left. I'm telling you guys, that is the policy that this other agent is following. What the ego agent is seeing is a trajectory. That trajectory is a sequence of state and actions and reports.
00:15:30.134 - 00:16:03.078, Speaker B: States are the states of the puck, states of its own robot, like its own joints. Actions are its own actions, like what the ego agent is doing. Again, it's not seeing anything about the other agent, and it sees rewards. So was I able to block the puck, or was I not able to block the puck? So it doesn't really observe anything about the action space of the other agent, and what does the other agent is doing? It's actually not like this is not a competitive scenario. This is not zero sum. What this other agent is doing is following a very hand coded policy. It's a history dependent policy.
00:16:03.078 - 00:16:25.902, Speaker B: What it is doing is it's pushing the puck. And if it gets blocked, it's going to try out a different direction. If it pushes the puck towards the right, it gets blocked. It's going to try left or center. In the next iteration, so that is what I'm hand coding for what the other agent is doing. If I wanted to make this competitive, this is rock, paper, scissors. The best thing the other agent should have done was play randomly.
00:16:25.902 - 00:16:55.574, Speaker B: I'm not doing a competitive setting here. I'm actually putting a partner model. Then the question is, how do I learn this low dimensional representation that I'm talking about? So what we do is we take a trajectory. A trajectory is the style at episode k minus one. And then what we try to do is we try to encode this trajectory into a low dimensional representation, ZK. And I'm going to refer to this ZK as the latent intent. This is what my ego agent thinks the other agent is going to do next.
00:16:55.574 - 00:17:33.566, Speaker B: This is really trying to capture that dimensionality that I was talking about earlier. How do we make sure that ZK at episode K is learning the right thing from an episode of Data? Well, the way I make sure that happens is by training this objective, by training this using a prediction loss. Basically what that means is I'm going to look at an episode of data, episode at k minus one, a sequence of trajectories. I'm going to try to encode that into a low dimensional representation. And using that low dimensional representation, I try to predict what will happen next. What would the next episode look like?
00:17:33.630 - 00:17:35.422, Speaker D: Is that an auto encoder?
00:17:35.598 - 00:18:09.138, Speaker B: It's an auto encoder like architecture. Yeah, exactly. It's not doing reconstruction, it's doing prediction. So it's taking the k minus one episode and trying to predict what will happen next. But it is very much an auto encoder like architecture. The reason this would do something reasonable is that if I see my trajectory and I am the ego agent and I know what I am doing, and I know the state of the world, for me to predict the next tau, the only thing I need to know is what the other agent is doing. And this DK is trying to ensure that that is captured in a low dimensional space.
00:18:09.138 - 00:18:11.974, Speaker B: It's brought down into a low dimensional space.
00:18:15.594 - 00:18:20.746, Speaker D: I mean, if you just put one deep network, wouldn't it also learn a.
00:18:20.770 - 00:18:24.734, Speaker A: Simple function, kind of like, what does it gain you?
00:18:28.794 - 00:18:40.090, Speaker D: Yeah, yeah. It is working better in practice. Second? Yeah, it is a lower dimensional space to deal with. And this can have dynamics, right? Like this. Like the way that I defined the dynamics of this ZK earlier was that.
00:18:40.162 - 00:18:40.810, Speaker B: You push the puck.
00:18:40.842 - 00:19:09.924, Speaker D: If it gets blocked, you're going to push it towards a different direction. So in some sense, in this low dimensional space, there also exists this dynamics. And for, like, by living this space, like, I can easily, like capture that dynamics and non spatiality of the other agent in some sense in this lower dimensional space. So that's like part of the reason. And it's just that makes the space that you're dealing with, like a simpler space. If I had like infinite data and I was just learning, like reinforcement learning, it would probably be able to do this too. But this just like helps me to bring down complexity.
00:19:10.944 - 00:19:12.032, Speaker C: Yeah, I guess the notation.
00:19:12.088 - 00:19:14.044, Speaker D: So like.
00:19:16.904 - 00:19:17.568, Speaker B: Oh, yeah.
00:19:17.616 - 00:19:22.056, Speaker C: So what are the rewards are collected in the end or at every step of interaction?
00:19:22.080 - 00:19:33.764, Speaker D: Yeah, it's episodic. So over an episode. So episode is Horizon Age. So it would be like a horizon type of data. Yeah, yeah, that's right.
00:19:34.504 - 00:19:34.816, Speaker B: Yeah.
00:19:34.840 - 00:20:03.956, Speaker D: So basically we're trying to learn this encoder and decoder. The parameters of the encoder and decoder are P and Psi. And what we're trying to do is we're trying to predict the log likelihood. So it's a log likelihood loss, predicting the future. And for predicting the future, remember the way I was writing Tau was a sequence of state, state actions and rewards. And I know my own actions. So, so the things that I'm trying to predict is what will be the state at t plus one, and what would be the reward that I would get, actually? Like, it would be the reward at.
00:20:03.980 - 00:20:19.704, Speaker B: That particular t. So that particular time step, given my previous state, given the actions that I've taken and given the full trajectory, horizon age, that I've seen in my past episodes, you could. Yeah, you could.
00:20:20.764 - 00:20:52.254, Speaker D: So can we see the dimensionality somewhere in that objective? In this work, we are not hard coding it. In this work, it's actually learning like a continuous low dimensional z. We have some follow ups where we are learning categorical Z's. Like we're learning like three dimensions and you're actually like encoding that. But yeah, here we didn't. And actually, like it is learning a low dimensional z that we think is probably those directions, but it is actually not very interpretable. It's not like when I look at that disease, it's like a four dimensional thing.
00:20:52.254 - 00:21:09.730, Speaker D: In this work, we didn't make a categorical to be three things. In some polks, we have had versions of it. So it is again a vector of seven variables or not. No, it's a lower dimension. I don't remember what the dimension is, but it's lower dimensional. Like, it's not seven dimensions. Yeah, it's not in r seven, it's actually lower dimension.
00:21:09.730 - 00:21:24.910, Speaker D: I think it might even be, like, a single continuous second. I don't. I don't remember, like, exactly like, what? There is an answer to the question that from seven, we went to one. Yeah, yeah. But it's not, like, discrete. It's a continuous z. That's what I mean by not being categorical.
00:21:24.942 - 00:21:25.918, Speaker B: Because I could have made this to.
00:21:25.926 - 00:21:40.236, Speaker D: Be just left, right, center, and, like, I could have made z to y. And, like, even in my seven freedoms, they are continuous. I mean, there are limits. I cannot do that. No, it is continuous. It is a norse. Yeah.
00:21:40.236 - 00:21:51.944, Speaker D: So in a few slides, I'll get to, like, that differentiation that I'm making, because in some settings, we are literally, like, going to three discrete things. But here, all I'm saying is it's a continuous but lower dimensional.
00:21:53.724 - 00:21:58.236, Speaker A: So what is the exact observation agent gets? Is it like, camera input or.
00:21:58.340 - 00:22:09.188, Speaker D: Yeah, so the observation that it gets is it's. Yeah, so it sees where the location of the puck is. So it has, like. It basically. Like, it takes where the plaque is. And it's like. It basically looks at its own distance to the puck.
00:22:09.188 - 00:22:12.316, Speaker D: And it also has, like, its own state, basically.
00:22:12.500 - 00:22:15.412, Speaker A: But it doesn't observe, like, the arm moving.
00:22:15.508 - 00:22:16.796, Speaker D: Yeah. Observes its own arm.
00:22:16.820 - 00:22:17.636, Speaker B: Like, I know it's like.
00:22:17.660 - 00:22:28.300, Speaker D: It's only my arm. Right? So. But it doesn't observe anything about the other arm. So it's not also, like, the opposite arm has separate policies to do the same thing. Yeah, yeah. The opposite arm. Totally accepted.
00:22:28.300 - 00:22:31.944, Speaker D: Okay. It doesn't object. Like, my legal arm doesn't object.
00:22:33.764 - 00:22:34.584, Speaker B: All right.
00:22:35.144 - 00:22:41.416, Speaker D: Okay. So once I have this representation, I'm doing basically this representation, learning from some experience buffer.
00:22:41.440 - 00:23:16.914, Speaker B: I'm getting this ZK. Once I have that, I can use my favorite planner to plan with that. And in this work, what we are doing is we are wrapping this inside a reinforcement learning loop, meaning that what we're doing is we're just maximizing expected return. And this expected return is going to depend on disease that we are learning these low dimensional representations that you're learning. So our policy is really conditioned on states and these Z's that are coming out of this representation learning component. So there's kind of like two learning loops happening here, like feeding into each other. There's an iterative process happening here.
00:23:16.914 - 00:23:36.774, Speaker B: I try to maximize my expected return based on whatever estimate of Z that I have right now that generates a policy. That policy generates some data. That data goes into my representation learning component. From that, I learn a New Zealand. And I'll pass that in and I keep doing. And that's why the Z can actually change over time. The Z can have dynamics.
00:23:38.354 - 00:23:42.654, Speaker C: So do you have to have to loop? So you can just lock it all in one?
00:23:48.594 - 00:24:15.982, Speaker D: Practically, we weren't able to get that to work. There's also like, yeah, the fact that the z keeps changing. So we kind of wanted to have the evolution of that zoo. But you could technically probably put it in practically. Like, it made things easier to be in this iterative process. Um, and what that gives us is that gives us this agent that can react to other agents. So, uh, let me show a very simple point mass example, and then I'll show the, uh, air hockey thing afterwards.
00:24:15.982 - 00:24:26.684, Speaker D: So. So this is a very simple point mass example. I have. I have an agent. Let's call that my ego agent. That's like my ego robot that I was talking about earlier. I don't know if you guys can see, but there's a circle here.
00:24:26.684 - 00:24:36.728, Speaker D: And there is this other agent that moves around the circle. Okay, so the setup is, if my ego agent is inside of the circle.
00:24:36.776 - 00:24:56.480, Speaker B: If it, like, does a trajectory, and at the end of the horizon, it is inside of the circle. My other agent moves counterclockwise. But if it ends up outside of the circle, my other agent moves clockwise. So it's a very made up example that we are creating here. What are we trying to do? We're trying to. My ego agent. What it is trying to do is it's trying to capture this other agent as quickly as possible.
00:24:56.480 - 00:25:44.788, Speaker B: So that is what it is basically trying to do. So if you're using soft actor critic, soft actor, critic like, state of the artery enforcement learning doesn't really capture a model of the other agent. We are starting from this point. And because it doesn't model what is happening with the dynamics of the other agent, it goes in the center here, and it's not really able to capture the other agent. But if you're using this algorithm, literally learning and influencing latent intent, what ends up happening is that we end up building a model of how this other agent is moving. If I go outside, it makes sure it gets the dynamics of the other agent to be flipped, moving counterclockwise to clockwise and vice versa. It learns this strategy that it goes outside, and then it's able to capture.
00:25:44.788 - 00:25:53.684, Speaker B: Capture the other agent because it realizes by going outside. Right. Like, I can, I can have a model of what the other agent is doing. And based on that, it's able to capture.
00:25:54.544 - 00:26:01.216, Speaker C: The result of this is it has a lower dimensional representation. So you can do more exploration. What is the.
00:26:01.360 - 00:26:28.724, Speaker D: Yeah, yeah. So basically, it has a low dimensional representation that kind of is clockwise, counter clockwise in this very simple setting. And because of that, you have much lower samples. You can actually realize that by going out, you could capture the other. In the game of air hockey, after 4 hours of training, this is training on the robots. We are able to basically model the partner and be able to block the clock no matter where it is sent, which is, again, a difficult task to get these systems work. This is 4 hours of training on the real robot.
00:26:28.724 - 00:27:01.356, Speaker D: It's pretty difficult to get that to work again. If you look at success rate, if you use soft actor critic, what it ends up doing, soft actor critic is it learns how to block, it learns how to go towards one direction. Maybe it always goes to the left because it learns that blocks box once in a while. But using Lilly, we are able to get a success rate around 90% because we can really capture this model of the partner in an effective way. All right, so what have I been talking about so far?
00:27:01.380 - 00:27:58.736, Speaker B: So I've been talking about basically a way of doing dimensionality reduction and modeling the partner strategies using these low dimensional spaces and then doing reinforcement learning based on that low dimensional space. And that helps us to coordinate and react, interact better. But one interesting thing that we can do that can maybe help us to go beyond this is to optimize for long horizon objectives. If I just put a sum outside of this expectation, then instead of optimizing expected return within an interaction, I would end up optimizing it across multiple interactions that I have. If I optimize expected return across multiple interactions, then that allows me to actually leverage this dynamics of the z that I was talking about earlier. And that allows me to not just react, but also influence the other agent in this game that I was showing earlier. If I want to capture this other agent very quickly.
00:27:58.736 - 00:28:37.854, Speaker B: Wonderful, Lily. What it does is it tries to model the agent. It goes and captures it. But the best strategy that Lilly can do is for Lilly to really influence the agent to be as close to it as possible and capture it as close to it as possible. Because the agent is not starting from the centers, the agent is starting from top here. So the best thing that it can do is to, in some sense, influence this other agent to move towards it. And by optimizing for the long horizon objective, what is happening is I can move out in ways that kind of like get the other agent move counterclockwise and get to me quickly so I can capture it more more effectively.
00:28:41.154 - 00:28:42.934, Speaker D: What was an interaction?
00:29:22.394 - 00:29:41.450, Speaker B: Optimizing multi episodes. Because when you are optimizing multi episode, you realize how your actions are going to influence the other agent. You start learning the dynamics of what the. How your actions are influencing the dynamics of the other agent. If I'm going this way, the other agent is going to go that way. So I need multiple episodes to realize that dynamics.
00:29:41.562 - 00:29:42.210, Speaker C: Oh, is it?
00:29:42.282 - 00:29:55.654, Speaker B: Yeah, well, yeah, the way that we are encoding it. So, yeah, the reason we are optimizing over multi episodes is that we get, we learn that dynamics. So then in test time, when we run it, like, we know exactly what to do, get the other agent to end up at a place that is useful for us.
00:29:57.914 - 00:29:59.882, Speaker D: Why do you need, why can't you.
00:29:59.898 - 00:30:01.694, Speaker A: Just do it in one very, very long?
00:30:02.634 - 00:30:19.514, Speaker D: It's really like, I guess like a practical choice here. So the way we are. Yeah, the way we are formulating this is that, for example, in this case, right, like, the way I encoded the dynamics was if you end an episode is that you either end outside of the circle or inside of the circle. And that affects which direction the other agent is going.
00:30:19.594 - 00:30:20.690, Speaker B: So that's how I kind of like.
00:30:20.722 - 00:30:44.830, Speaker D: Encoded how the other agent is being affected. Yeah. You could probably have, like a really long episode and get these types of behaviors, but it's probably just cleaner to get that behavior. So the soft actor critic here, none of these examples would have any memory of the previous episode, right? Yeah, yeah. It's not slightly unfair. Yeah, one could argue that is. I agree with that.
00:30:44.830 - 00:31:02.394, Speaker D: What do you expect to happen if the SAC system did have memory of the previous? I think it would convert. It would just take longer. Okay. Just take a lot longer. And then in the Arawaki example. So the way that we created this idea of influencing was that we artificially made it so that the robot agent.
00:31:02.434 - 00:31:20.156, Speaker B: Is blocking the puck on one direction. On the left. It is going to get higher rewards. So it gets reward of plus two if it blocks the puck on the left. If it blocks the puck anywhere else, it gets a reward of plus one. So the Eagle agent really wants the puck to go on the left because it's like really like, good at that for like, whatever reason that we are making here.
00:31:20.260 - 00:31:22.572, Speaker D: Why like the left hand?
00:31:22.748 - 00:31:24.180, Speaker B: We're making it artificially.
00:31:24.252 - 00:31:25.444, Speaker D: So that is the better report.
00:31:25.524 - 00:31:26.532, Speaker B: I'm just trying to show that if.
00:31:26.548 - 00:31:48.660, Speaker D: You artificially make it like that, you can influence the other agent to push the puck more often to the left. That's what I'm trying in practice, you could imagine that maybe your robot is really good at going to the left. Because other you could artificially make it so that not even artificial, in fact, it's right. There are some actions that are easier for the robot, and some actions in this case, like I just said, symmetric. It doesn't matter.
00:31:48.772 - 00:31:54.284, Speaker A: But just in the interest of time, why don't we keep the rest of the questions for at the end of the talk?
00:31:54.444 - 00:32:14.144, Speaker D: I can. I had three sections. I can make it one section. No, no, you're good. You're good. How much time do I have? Maybe like twelve minutes. Okay.
00:32:14.144 - 00:32:15.332, Speaker D: Okay.
00:32:15.468 - 00:32:17.028, Speaker B: Tell me when to stop. I can stop anytime.
00:32:17.076 - 00:32:17.916, Speaker D: Sounds good.
00:32:18.100 - 00:33:01.654, Speaker B: All right, so just like going back to this figure, right? So when we are doing this, influencing, it actually takes a little bit longer for it to converge. But what ends up happening is that with influencing, you can really get the puck to more often go to the left, as opposed to the other case, without influencing where you're not really influencing the other agent. No matter where your actions are not going to influence where the puck is being pushed, it can go uniformly to any of these directions. All right, so what I've been saying so far is that when you're interacting with other agents, oftentimes humans. I haven't shown anything humans here. But when you're interacting with other agents, maybe human partners or robot partners, they're often non stationary. And by non stationary, what I mean here is that their actions are influenced by our actions.
00:33:01.654 - 00:33:35.410, Speaker B: Like, even the car example I was showing, right? Like, when we nudge in front of the other vehicle, that vehicle slows down. Like, us nudging in front of them is going to be the thing that influences them. So when we have these non stationarities, one way of capturing that is by trying to do this low dimensional representation. And having a latent representation of the other agent's intent. And really leverage that for reacting and even influencing the other agent. And what Lily is doing is a version of that. So, that's wonderful.
00:33:35.410 - 00:34:21.332, Speaker B: But one thing that we started thinking about, and this is maybe not very practical, but I want to briefly on two slides, talk about it, is that when you're talking about these types of non stationarities, that is oftentimes a problem. In these multi agent or situations, we have this other agent. We need to figure out what is the dynamics of the other agent? What are they trying to do? We also need to focus on learning and doing our own task. And if non stationarity is the problem, I mean, like, a very simple question to ask is, can we get rid of it. Can we reduce non stationarity? So here's a fun example. So imagine that you have, you have a house that is very messy, and you and your partner, you want to clean this house and you want to wash the dishes, and you don't know how to wash the dishes. You have to learn how to wash the dishes.
00:34:21.332 - 00:34:59.006, Speaker B: But then your partner is in the kitchen too. So you need to deal with your partner now and what your partner is trying to do. And that can get really confusing. So one way of doing this task that makes it a lot easier is if you could lock your partner in the room, then your life would be so much easier because you can really like focus on the task of like, washing the dishes, and your partner can focus on like, cleaning the room or doing whatever in that room. So that's the idea. Like role assignments in general is a way of reducing non stationarity. If you can come up with these, like, roles, like, that would be really a good way of reducing non stationarity, having, having an easier task.
00:34:59.006 - 00:35:49.208, Speaker B: And one question is maybe if we optimize, like how do we get role assignment? If we optimize for stabilizing the partner, like if I want to clean this house, if I optimize for making sure that the other agent is not moving so much is actually stable, then I could potentially end up with a simpler task to solve. And then that is kind of like the idea that you're trying to get at here. So I want to just talk about this again over one slide. Nothing. I don't want to talk about this in too much detail, but the idea is we can do the same representation learning that I was talking about earlier. This time it might be categorical event like these that we are learning. But once we learn it, the objective that we are optimizing in our reinforcement learning, it could simply be a combination of achieving the task, like whatever task objective that we had before, and stabilizing the partner.
00:35:49.208 - 00:36:48.188, Speaker B: And by stabilizing the partner, what I mean is we had this low dimensional representation, this Z, and we can make sure that that Z doesn't change so much. We can make sure that that Z doesn't move so much across interactions. This is a very difficult thing to train because I'm trying to learn disease. And not only I'm trying to learn disease, I'm trying to keep them stable too. I'm trying to not get them changed as much, but with this very kind of simple objective of stabilizing the partner as well as trying to achieve the task together, is actually pretty useful for a lot of collaborative tasks. So we've actually demonstrated this working across a number of domains where you can actually have higher task reward as well as higher stability reward in driving tasks or robotics tasks, or simple pursuit evasion type point mass, point mass type of tasks. And then here I'm comparing this algorithm, silly stabilizing and influencing learning latent intent compared to Louvi, what I was showing earlier, and a number of other baselines.
00:36:48.188 - 00:37:17.332, Speaker B: Smeral, for example, is a surprise minimizing RL algorithm. So what smeral does is actually interesting. It's trying to minimize surprise over the environment. What we are doing is we are trying to in some sense minimize surprise in the partner policy. So we're trying to keep the partner more stationary, and by keeping the partner more stationary, we are able to do interesting things. So going back to this point, mass example, um, I'm changing this. So the agent is not moving continuously around a circle, it just jumps around.
00:37:17.332 - 00:37:36.860, Speaker B: Um, I'm actually changing it. So if you end up outside of the circle, the agent doesn't change to move around. It like gets like, it's actually like stays like, it becomes stable. But if you end the episode inside of the circle, it moves clockwise. So if you do something like this, again, soft actor critic doesn't really know what to do. It kind of gets stuck in the middle. Stable.
00:37:36.860 - 00:38:07.464, Speaker B: What stable does is it just tries to stabilize the other agent. So it just realizes that if it goes outside, it makes the agent stable. What Lily does is it does partner modeling. It's actually able to capture it, but it keeps jumping around. It doesn't really stabilize the partner. What silly does is it actually goes out and stabilizes the partner and then afterwards goes and captures it. The reason I'm interested about this, I think this stabilizing idea is interesting, is I think it even goes beyond these multi agent interactions.
00:38:07.464 - 00:38:37.844, Speaker B: For example, a common robotics task that we are interested in is bimanual manipulation. So how do we use two arms to do a task like cut a stake? Why do we need two arms to cut a stake? If you think about it, why don't you use just one arm to cut a stake? The whole thing the other arm does is it holds the stake. And why does it hold the stake? Because me predicting the dynamics of the stake, it's a very difficult task. If I want to do that. If I hold the stake, I don't need to predict the dynamics of the stake. It's actually a lot easier. It stabilizes it.
00:38:37.844 - 00:39:06.738, Speaker B: I think this idea of stabilizing part of the environment or part of the non stationarity of the environment. It's a very common idea that shows up in a lot of bimanual tasks. And, uh, bimanual manipulation is difficult. So, and that's something that we're actively working on. And I think the stabilizing idea could be one way of going about that. All right, so, uh, let me summarize. So, so kind of last point is that if non stationarity is the problem in multi agent RL, maybe we can reduce non stationarity.
00:39:06.738 - 00:39:49.260, Speaker B: And one way of reducing non stationarity is maybe stabilizing the other agent. Um, I think I'm gonna skip things. Um, maybe let me just tell you what the problem is. Okay, so, so I had, I don't want to go into details anymore, because I don't probably don't have that much time, but, um, let me just define a couple of other problems that we are interested in, just, just to tell you about these problems. And then I'll wrap up afterwards. So, building on top of this idea of these low dimensional representations, these conventions that we have, one thing that in general is also pretty interesting in these human robot, human AI interactions is what happens over repeated interactions. What happens when you pair up agents and they want to coordinate with each other over multiple runs.
00:39:49.260 - 00:40:28.280, Speaker B: For example, I might have Alice and Bob here, and Alice and Bob are trying to put their blocks to build some sort of block architect structure. And I might have, again, partial information, meaning that maybe in this case, Bob really knows what we are trying to build, but Alice doesn't know what the goal configuration looks like. And it's a collaborative task. They need to do this task together. The first time I pair up Alice and Bob, no reason for them to be able to do this task because Alice doesn't know what to do. Bob doesn't know how to signal to Alice. But if I pair them up over multiple runs, after 20 runs, they would be able to build up a convention, a low dimensional representation that they build together to be able to do this task.
00:40:28.280 - 00:40:56.058, Speaker B: For example, in this case, what Bob can do is Bob can put the red block at the top corner here. Alice will just observe it. Bob moves the red block to the bottom corner. And just this transition is something that is going to help Alice to know where to put her blue block, and it would be able to do this task. And what is, this is an arbitrary convention that they have come up with. This is a collaborative setting. They could have come up with any other signaling strategy, and they came up with this after multiple repeated interactions.
00:40:56.058 - 00:41:48.194, Speaker B: So a question that we are interested in is in these collaborative situations where we have arbitrary number of good solutions, like how do we come up with, how do we deal these conventions with other agents, including humans? And could we build AI agents that can coordinate with humans and build conventions that are actually meaningful with humans and these conventions, we see them all over. We see them in our driving behaviors. There are different equilibria that we end up at driving on the right side of the road or left side of the road. That's definitely an arbitrary choice that we have come up with. Again, either one could be a good solution, but that's a convention that we have built up. Built up. So ideally, the question is, how can we build AI agents that can do that? I'm not going to talk about how we do it, but I guess at the high level, we are coming up with a modular architecture that tries to separate what is special about the task and what is special about the partner.
00:41:48.194 - 00:42:04.920, Speaker B: And by separating them, it is able to learn partner specific conventions. I missed Kalisha's talk, but she might have talked about Hanabi and overcooked and these types of. Did you talk about on Abhin or. You didn't talk about coordination. You talked about coordination.
00:42:04.992 - 00:42:05.176, Speaker D: Yeah.
00:42:05.200 - 00:42:57.476, Speaker B: But the type of games that you're interested in are, like, these types of coordinations, like game of hanabi or Game of overcooked. Like Game of Overcooked is something like this where you pair up agents and you try to, like, get them coordinated each other. And on these games, like, what we are interested in is could we adapt to new partners effectively and quickly by separating out conventions and rules, specific parts of the game? And we actually demonstrate that you can do that in these coordination tasks. And similarly, if you work with the same partner, could you adapt to new tasks more effectively with the same partner, and you kind of, like, demonstrate that that is also possible. So this was kind of my second section that I skipped over very quickly. And the last section, let me also very briefly mention this last section and promise I'll wrap up after that. The last section is something that I just added to think about how we formalize some of these types of multi agent problems.
00:42:57.476 - 00:43:37.146, Speaker B: So one way of going about formalizing this is formalizing it as a collaboration. Multi arm bandit. So what I mean by that is, well, we have, like, two agents, maybe we have a leader and we have a follower leader, let's say, like, similar to the last example, it was our Bob. Right. Leader sees, leader actually observes, like, all the rewards when arms are pulled, the follower here has partial observability, meaning that maybe with some probability it sees the rewards and it doesn't always like see the reward when an arm is being pulled and they take turns. And that is kind of what the reward structure that we are setting here. So, um, standard way of going about this is to say, well, we have a centralized algorithm, we do UCB, and with a centralized UCB, we are going to have logarithmic regret.
00:43:37.146 - 00:43:59.308, Speaker B: Wonderful. But if I have decentralized UCB, meaning that these two agents don't talk to each other, there's no reason for that to work. And that actually gives us linear regret. The reason for that is if my leader is, has sees all the information, my leader might think that this is the arm that we got to get right. Like this is the arm that my leader has converged to. So my leader is going to choose a row. My leader is a role player.
00:43:59.308 - 00:44:32.652, Speaker B: It's either going to choose the top row or the bottom row. So my leader decides to choose their top row because it really wants this arm, but my follower, because it's like it doesn't see all the information, it might be still in exploration mode and my follower might really want to explore this arm. So my follower really wants this arm. So my follower might decide to pick this column. And the mismatch basically ends up giving us an arm that none of them really wanted. So we could end up with these suboptimal situations where like we end up in arms that no one really wanted. And because of that, decentralized UCD is just going to give us, give us linear regret.
00:44:32.652 - 00:45:07.296, Speaker B: The question is, could we do better in a decentralized way? And what we have demonstrated in this work is a very simple algorithm and kind of like analyze that very simple algorithm that does this partner modeling in this setup. So, so this very simple algorithm basically says, hey, leader, you just do your UCB, don't worry about anything. You do your UCB. You just take an arm, but take it l times. And the hard work here really goes for the follower. So what the follower does is it basically tries to build like distribution of what the leader is trying to do. So it looks at the latest actions of the leader.
00:45:07.296 - 00:45:29.202, Speaker B: It construct a distribution of what the leader is trying to do. And then within that it tries to do its own exploration and its own UCB. So that is literally like the idea of this algorithm. It's very simple. Follower is doing the smart thing, leader is just doing normal UCB. And we demonstrate that this algorithm actually has logarithmic regret. I think it would be interesting to think about smarter ways of doing this.
00:45:29.202 - 00:46:39.548, Speaker B: We tried to make the leader smarter, to do something smarter than just doing UCB and try to teach, but we weren't able to show anything on that. So, in general, I think this idea of collaborative multi arm bandits is an interesting way of formalizing some of these multi agent collaborations, coordinations that I've talked about earlier. And, yeah, like, would be cool to have better strategies for the leader than what I just showed, or have ideas around learning algorithms that can get us these types of behaviors. We have demonstrated this on a real robotics example, and even in practice, you start seeing the difference between these two algorithms. So the regret is not just asymptotic, it's actually like, you actually see this within a difference between these two types of algorithms, within, like, 4d interactions. Um, and I think, yeah, we should have more intelligent leader strategies and potentially generalize some of these ideas to mdps. All right, can I say one last thing, Dylan? All right, so, okay, so, so I've talked about this idea of partner modeling, right? Like, I think partner modeling is pretty important for human robot collaboration, human AI collaboration.
00:46:39.548 - 00:47:09.614, Speaker B: I talked about it in the setting of low dimensional representations as an effective way of modeling the partner. I talked about stabilizing that as a better way of coordinating. I very briefly talked about repeated interactions and then collaborative multi arm bandits as a way of maybe formalizing it and being able to give some sort of guarantees. Nothing I showed worked with actual humans. Everything I showed was two robots working with each other, two AI agents working with each other. Let's see how things work with actual humans. So this Lilly algorithm was showing 90% success rate.
00:47:09.614 - 00:47:34.636, Speaker B: If you pair it up with an actual human, the success rate goes to 70%. And this is a human that is trying to be as close as possible to that robot strategy. So it's not even trying to be intelligent. This is a human that is trying to be exactly imitating what that robot was doing. And even then, success rate gets dropped, because human behavior has a lot more variability than what we see in the robot behavior. The friction, like the push, is going to be slightly different. Different force friction is going to be different.
00:47:34.636 - 00:48:09.480, Speaker B: That will mess up everything. Similarly, it's the behavior that other people have seen. This is work from Berkeley, where they showed that if you do self play in this overcooked environment, it's just not going to work with humans, because humans build different conventions. So the conventions that I talked about, it actually matters if it is human conventions. And in a lot of these types of work, what we end up doing is we end up modeling the human as this intelligent decision maker that is doing these game theory type of models. And we model the human as the best robot decision maker that we know. And then what we do is when we do human studies, we ask our humans to be as robot like as possible.
00:48:09.480 - 00:48:57.980, Speaker B: This is like a very common practice in this field, which is very unfortunate. And I feel like it's becoming more and more of a problem. Like, even in simplest robotics tasks that we are interested in, like learning how to push an object, robot generated data is very different from human generated data and messes up our algorithms so much. So, in general, I think we need to look back at our assumptions and what it is that humans are doing, um, and model that a lot more carefully than just assuming there are these intelligent decision makers. Um, as an example, I think why people are bad at playing chess or solving a math problem is a very different issue than why they're bad at flipping a pancake or finding their keys from their backs. And I think there are different types of suboptimalities that we really need to formalize and understand. And we are doing some work around capturing these suboptimalities and thinking about active teaching to humans.
00:48:57.980 - 00:49:03.754, Speaker B: Uh, with that, I would like to thank the group. Yeah, I can take any questions.
00:49:05.014 - 00:49:17.454, Speaker A: Thanks a lot. There are a bunch of questions. Let's see. Oh, Kosis. Yeah, yeah.
00:49:17.494 - 00:49:33.674, Speaker C: So I wanted to revisit your very last point about modeling humans. So I guess I'm not sure I understood the point is the point that humans are much smarter than what you model them as being when you try to learn policies?
00:49:33.754 - 00:50:02.442, Speaker B: No, I think what is happening is not that. I was going to play this one sec. So here is an example of how humans are different from robots when they are pushing an object. So, there is argument from cognitive science that when people are doing simplest tasks, like pushing an object, they're not just pushing an object. They're doing probably five other things at the same time. They're thinking about the dinner that day, and they're paying attention to something else. And because of all these different processes that are happening, people are not just pushing the object.
00:50:02.442 - 00:50:24.130, Speaker B: They might like, they have random pauses when they're pushing an object. They have like these, and you might treat it as noise, like so far, like the field, the way that they have been treating it, as that human data is just noisy. It's just a noisy version of that robot data. And if you just add enough noise that will capture it. But it turns out that it is more than that. It's actually, like, not just noise. It's a very systematic type of noise.
00:50:24.130 - 00:50:50.838, Speaker B: So there's a lot of robotics work that shows that things work wonderfully on scripted data. The moment you add robot data, performance goes down. No one got human data performance goes down because it's just not from the same distribution. And you have these out of distribution issues. This is one example of the type of differences that human data has. It's just more variable. But the other argument that I was trying to make is that that variability really depends on the type of the task.
00:50:50.838 - 00:51:05.002, Speaker B: Sometimes it is because they have partial observability and maybe, like, they don't know how to solve the game or go down the game tree. Sometimes it's because, oh, they have limited time, and they're doing this suboptimal thing within the time that they have, or they have limited memory. They can't really capture everything, like, in their memory.
00:51:05.098 - 00:51:05.282, Speaker D: So.
00:51:05.298 - 00:51:12.898, Speaker B: So there are different reasons, inductive biases that could cause humans being suboptimal. And I would argue that we got to, like, model that. It's not the same thing in all.
00:51:12.986 - 00:51:46.242, Speaker C: But I guess there's a counterpoint to that, which is sort of like that if I'm cooking together with my robot, like, in the same way that, you know, like, when I'm making queries to Google, I adapt the way I ask Google so that Google gives me good answers. Like, in some, at least collaborative tasks, I could imagine that kind of like, yeah, when I'm cooking together with my robot, I will, you know, like, you know, you know, my behavior will be mal. Like, it's malleable, and it's gonna try to bring up the best of my robot that I can bring up, right?
00:51:46.398 - 00:52:19.600, Speaker B: So I think that's to some extent, it depends on the complexity of tasks. So there are two versions of this. Like, what I've been talking about here or on the previous slide, is not a collaborative setting. It's more of, oh, I'm collecting human data, and I'm trying to learn intelligent behavior from that. What you are getting at, which is, like, the collaborative thing, is something similar to this overcooked thing, right? Like, so if you pair up two agents, right? Like, maybe you pair up two AI agents, and they do self play, and they come up with a best strategy of doing things. Now, you pair up that AI agent with a human, and you would expect your human to adapt. And if it is a simple task, the human might actually adapt.
00:52:19.600 - 00:53:08.414, Speaker B: Like, okay, this is a convention that you have come up with. I'll go with your convention. But if it is actually complicated, if that convention is complicated, then I think it becomes harder for the human to adapt. For example, going to this block example that I was showing earlier, like, this example, like, this convention is such an unnatural convention for Bob to put the block here and then move it down here, and that being a signal for the human to, like, do this other task, that could be, like, a very unnatural thing and could be, if I make this block to, like, I don't know, three by three, like, structure, as opposed to two by two. Like, people start, like, losing track of these conventions that are maybe optimal from the perspective of two AI agents being paired up with each other. So that's why I would argue that these conventions should be kind of, like, guided by how humans build conventions.
00:53:09.014 - 00:53:09.754, Speaker D: Yeah.
00:53:17.094 - 00:53:17.446, Speaker C: Yes.
00:53:17.470 - 00:53:39.060, Speaker A: So, thank you. Very interesting. So, with regards to the first part, the lily and silly part. So the SAC also has a function approximator who could basically learn a model of the environment, a lower dimensional model, but I guess it doesn't, because the reward signal is too sparse. So, basically, you give it an intrinsic motivation. Right. To predict the environment and to influence environment and later also stabilize the environment.
00:53:39.060 - 00:53:46.388, Speaker A: So is this work framed in terms of intrinsic motivation or curiosity or. Because maybe I missed it at the beginning of your talk.
00:53:46.436 - 00:53:46.572, Speaker D: Yeah.
00:53:46.588 - 00:53:51.220, Speaker B: So there are kind of two parts to it. The Lilly part is not really, like, doing any reward shaping in some sense.
00:53:51.252 - 00:53:51.516, Speaker D: Right.
00:53:51.580 - 00:54:15.148, Speaker B: The Lilly part is just doing partner modeling. It's just coming up with an effective representation of the partner, and that partner is changing over time. So it's in some sense, capturing that change over time in a very, like, kind of efficient way. And it's basically doing that. The silly part is exactly that. It's basically a reward shaping approach, and that reward shaping approach. The shaping term is the stabilizing term, and, yeah, it is really an instance of that type of work.
00:54:15.236 - 00:54:15.652, Speaker A: Okay.
00:54:15.708 - 00:54:16.012, Speaker C: Yeah.
00:54:16.068 - 00:54:53.400, Speaker A: Thanks. Are there any other questions? All right, maybe we should stop it there. So let's thank Dorsa again. All right, that kind of brings us to the end of the workshop. So I just want to say, you know, thanks to everyone for coming. You know, on behalf of myself and the rest of the organizers, I believe there's, like, a Cinco de Mayo celebration happening outside, so everyone should go and enjoy that. But, yeah, let's just say, yeah.
00:54:53.400 - 00:55:04.654, Speaker A: Thanks again for coming. Hopefully this leads to some more interesting.
