00:00:00.200 - 00:00:07.966, Speaker A: Everyone, our next speaker is Sahan Negaban from Yale University, and he's going to talk about approximation guarantees for 3d low rank approximation.
00:00:08.150 - 00:00:58.822, Speaker B: Okay, so thank you all for coming. So this is joint work with a few guys from UT Austin, Rajiv and ethan, our great grad students there. And then Alex Demakis is a professor in the ECE department. And so what I'm going to talk about is greedy low rank matrix approximation problems. And we're going to establish our goal would be to establish new bounds for doing low rank matrix approximation in a greedy way. And we're going to do that by drawing connections with sub modular optimization and sort of taking ideas from that community in order to establish new approximation guarantees from the optimization perspective and the statistical perspective. All right, so our goal is to provide sort of a novel analysis for greedy low rank matrix approximation by establishing these connections with some modular optimization.
00:00:58.822 - 00:01:41.052, Speaker B: And so what we want to do is we want to maximize some function, some l of theta, subject to the non convex constraint that theta has bounded rank. And our goal, our method of doing this will be to just greedily add rank one components in every iteration. So at every iteration, we'll have some theta k. We want to figure out what's the right rank one component to add, and then just add to that. All right? And so, informally, the result that we show is the following. As we take k iterations. So when we take k iterations, at every iteration, we add a rank one matrix, which means our matrix is at most rank k.
00:01:41.052 - 00:02:20.632, Speaker B: What we're able to show is that our loss or our objective that we want to maximize. Guys, the greedily approximated one, the theta k has a multiplicative lower bound approximation with respect to the best rank r version. So we let theta star be the best rank r matrix that actually solves this problem. And we're able to show this multiplicative, this multiplicative guarantee. So, to be clear, we're allowing our approximation to be of larger rank. So when we look at this exponential, it's e to the minus c, k over r. So c will be some constant.
00:02:20.632 - 00:02:47.854, Speaker B: That only depends on the loss. K is the number of iterations that we take. So it's the rank of our solution. And then we're trying to compare to the best rank r solution. So, as one obvious example, in low rank matrix approximation, we have our theta star. It can be decomposed in terms of some factors. And then what we might have are noisy observations of the form phi of theta ij.
00:02:47.854 - 00:03:13.970, Speaker B: And so this is what you have in an exponential family. So, for example, I might have theta star, which are underlying sort of desirability of a product. So I might think m is users, n are the products. And I have some underlying desire for that. But I don't actually observe a number noisy, a noisy corrupted observation of that number. Instead, I might just see like or dislike. Another example is a co occurrence matrix of words.
00:03:13.970 - 00:03:52.114, Speaker B: I might just see words across words, and then we'll know sort of how often one word follows another word. So I can see a co occurrence matrix. And then that might have a low rank, because we might think that we can embed words in some lower dimensional space. Similarly for the users and the ratings, we might think that we can embed users and movies in some lower dimensional space. And so what we want to do is basically take these observations and recover the underlying matrix, theta star. And so this is connected way back to exponential family PCA of, uh, of colons, uh, et al. And so basically what we want to show are sort of theoretical guarantees for doing this recovery.
00:03:52.114 - 00:04:56.662, Speaker B: So we're going to do that by connecting the problem to atomic approximations. All right, so what does that mean? We want to estimate from some set CK, where Ck is the set of arc of all r combinations from some atoms. So Ck is the sum from I equals one to r of c, sub I, a sub I, given that a sub I come from some atomic set. And what we want to do is we want to find theta hat, that's the arg max over the set of theta. So obviously, for rank r matrices, we can just take the atomic set to be rank one matrices of operated norm bounded by one. And so now this general framework has a very, very long history with all sorts of different types of recovery guarantees, sometimes rediscovered, sometimes applied for different settings. And so in our case, we're focusing explicitly just on the rank one matrices, that's our atomic set.
00:04:56.662 - 00:05:46.654, Speaker B: And then, so we are able to treat this problem now as a kind of a set optimization, where our set is something that we select from the set of atoms and then we optimize over those. So to be more precise, so to be precise, suppose we have some algorithm that can select a new atom. So for us, that means it selects a new rank one matrix and then let l be the set of indices of those selected atoms. So now for us, in this case, we're going to think of the atoms as both some left singular vectors and right singular vectors. And so you might have a column space in a row space indexed by l. So l selects the column space vectors and it selects the row space vectors. And so we have u sub l and v sub lift.
00:05:46.654 - 00:06:48.414, Speaker B: And then with that we can define a set function. So we define the set function f of l is equal to the max over this matrix h, which is l by l of l of u sub l h v sub l minus l of zero. So now once we've specified the row space in the column space, then it's just a much easier problem. If l were concave, then this is an easy problem to optimize over. And so thinking of, say, a special case like in sparsity, maximizing over h would be like in sparse optimization if I selected an appropriate subset of indices and I only optimized on those weights. So for us, so the equivalent sort of problem in sparsity would be something like maximizing over some f of s, where the cardinality of s is upper bounded by k. So for us, we're selecting some set that has a bounded cardinality, but it's coming from an infinite size set.
00:06:48.414 - 00:07:38.574, Speaker B: So now this links us to submodular functions. So submodular maximization is all about maximizing functions over some cardinality constraints or some set constraints. And so a set function f that maps, say, p subsets to r is submodular for all a and b subset of p. So where p is one, two p. If f of a plus f of b is lower bounded by f of a union b plus f of a intersect b. So the set function is normalized if f of the empty set is zero and then its monotone. If I take any b that's a superset of a, then f of b is greater than or equal to f of a.
00:07:38.574 - 00:08:44.634, Speaker B: And so for us, that's like saying if I select more low rank matrices, then I'm able to optimize my objective l, the objective l better. And so one way to optimize over these problems is to just pick an element that maximizes. So pick an element over the set of remaining coordinates that maximizes the objective and then just add that to the set. So we just greedily keep adding an element from our space and then just work with that. And then, so we know this result by Nemhauser from 78 that says if we take the greedy style algorithm, then for a monotone submodular functions, the greedy guarantees a one minus one over e approximation. So that is, we get that lower multiplicative bound of one minus one over e. So the greedy method, a simple method, can still do pretty well when we compare to the, to the best case search method.
00:08:44.634 - 00:09:21.276, Speaker B: Okay, so now we can talk about something called weeks of modularity. So this form of sub modularity is a bit strong. And for us, our problem won't necessarily satisfy sub modularity. And indeed, whenever we're talking about some modularity, we're talking about a finite class. But for us, again, it's over an infinite set of atoms. So what does that mean? And so for the first step, we use this notion of weak sub modularity that was introduced by Das and Kempe. And now they introduced this for analyzing sparse linear regression problems.
00:09:21.276 - 00:10:14.154, Speaker B: So these are just linear regression problems where the weights have to be sparse, only a small subset of the indices matter. And so they introduced the following concept. So suppose we take s and l to be disjoint sets, and we take f as a set function. Then the sub modularity ratio of L with respect to s is given by gamma l comma s, where, which is the sum over j and s of f of l union j minus f of l. So now this is like taking l and if I add each individual element from j and then sum up how much each individually contributes, and then we divide that by l union s. So now we just take s all at once, put it all in, and see how much better we get. And we take this ratio.
00:10:14.154 - 00:10:31.384, Speaker B: So the sub modularity ratio of a set u with respect to an integer k is then given by gamma u. K is the min over all disjoint l, and s, where l is a subset of u, and absolute value of cardinality of s is upper bounded by k.
00:10:33.484 - 00:10:34.892, Speaker A: So what is u here?
00:10:35.068 - 00:11:09.378, Speaker B: So u is just an arbitrary set. Now, sorry. All right. So if you have this sub modularity ratio, this gamma, then what one can show, and what Das and Kempe show, is that you can, instead of getting that one minus one over e approximation, now you get a one minus one over e to the gamma approximation. And so what that says is you'd like gamma to at least be strictly positive. And indeed, if the function f were submodular, then gamma is at least greater than or equal to is one. Whoa.
00:11:09.378 - 00:11:44.578, Speaker B: Yeah. So f sub modular is if and only if gamma is greater than or equal to one. And so we can see that from the previous definition. And if you're familiar with sub modular optimization, that's kind of like the, the incremental gains as you add to a larger set. The benefit isn't as, isn't as large. Now, the issue with this is that it's defined for finite sets and it's not directly applicable to our setting. So for us, if I have two atoms, I can have one atom, a set of atoms, and another set of atoms that are completely disjoint, but for the most part are almost the same.
00:11:44.578 - 00:12:23.134, Speaker B: I can just take an epsilon perturbation of a rank one matrix, and they're basically, in terms of my approximation, roughly equivalent. So instead, what we do is we generalize this notion of the submodularity ratio to atoms. So for, and specifically now this is for low rank matrices. So for us, we take SNL to be subsets of the atom set. So for us, that's rank one matrices and they're disjoint. And now the elements of s are orthogonal with respect to the elements of L. So now we're saying s and L can't even be, they can't really be close.
00:12:23.134 - 00:13:11.630, Speaker B: The information that s is giving on top of L is sort of orthogonal to it. And now we take the cardinality of L to be k, the cardinality of S to be r, and then f is some set function. Then we define the sub modularity ratio now in exactly the same way, just gamma of L comma r is the sum over a, and s of f of L union, a minus f of L divided by f of L union, s minus f of L. And then again, we can define the sum modularity ratio of the set of another set of atoms, u with respect to some k, just by the min over all of these sets. And so now this basically just generalizes the notion of weeks of modularity.
00:13:11.702 - 00:13:24.854, Speaker A: Yeah, yeah. So I just don't know. So what you're saying is that s and L are respectively included into octagonal subspaces. Is that what it means?
00:13:25.234 - 00:14:11.904, Speaker B: So, yeah, I'll try to be. So L is a set of atoms. So just some arbitrary set of rank one matrices, and then s is another set of rank one matrices. But where we're assuming that the elements in s, the row spaces of each atom, is orthogonal to the row spaces of the atoms in L, and the column spaces of the atoms in s, are orthogonal to the column spaces of the atoms in L. And so then this is the notion of weeks of modularity that we want satisfied for low rank matrices and sort of generalizing, just kind of like infinite sets. Okay, so now with that, we take another element. So we have this weak sub modularity notion.
00:14:11.904 - 00:15:16.750, Speaker B: And now we take an idea from statistics and from optima and from constrained optimization, the notion of restricted strong concavity and restricted smoothness. And so all that means is that if we have our matrix variate function l, so remember we're maybe thinking of l as our log likelihood. If we have our matrix variate function l, then it's restricted strongly concave with parameter m omega and restricted smooth with parameter m omega. If for all x and y in some appropriate set omega, then the first order Taylor approximation of L is lower bounded by a quadratic and upper bounded by a quadratic. So it's just smooth and strongly concave like we're used to. But now we're doing it over restricted sets. All right, so l, when we say is m sub I strongly concave, we mean that it's m sub I, strongly concave over rank I matrices.
00:15:16.750 - 00:16:21.458, Speaker B: So coming back here, the set omega for little m is rank I matrices. So x is an arbitrary rank I matrix or rank I matrix, and y is an arbitrary rank I matrix. For the smoothness part, we need that l is m. Twiddle one smooth over the following set omega, where the rank of x minus y is just one. So whereas in the strong concavity part, we need x and y to be arbitrary rank I matrices, for the smoothness part all we need is y and x their difference to be rank one. And this is going to, this has to do within the algorithm we're always updating by a rank one matrix, and so the next iterate is only different by rank one. So in pictures, that basically means over our constrained set, we have our blue function, which is our actual desired l that we're trying to maximize over that log likelihood.
00:16:21.458 - 00:17:19.082, Speaker B: And then we just have some upper and lower bound control on those on the function. All right, so this is then our main result. The restricted strong convexity concavity and restricted smoothness of the function l actually implies weeks of modularity. So recall that we define our set function in this way, f of l. So l is the set of atoms that we've selected is equal to the max over this h of l of u, sub l h v, sub l minus l of zero. And so now we want to lower bound the sub modularity ratio and we get the following so if l is a set of k rank one atoms and up to r, additional atoms are all orthogonal to atoms in L, and they are greedily added. Then gamma lr, that submodularity ratio for low rank matrices is lower bounded just by the strong concavity over the strong smoothness.
00:17:19.082 - 00:18:06.476, Speaker B: So m sub r plus k divided by m, sub one. And then so with this we can basically immediately prove guarantees on a greedy method for solving the rank r approximation problem. So this sort of thing extends a previous result by Ellenberg et al. And us that gives you weeks of modularity for general sparse optimization. But it should be important to note that in this case, and in that case, we're not implying actual sub modularity, it's just this weak sub modularity. So we can get, now get greedy approximation bounds. So we have two greedy algorithms that one can consider.
00:18:06.476 - 00:18:54.064, Speaker B: One is orthogonal matching pursuit for matrices. And then so that's, we can think of approximate greedy, the gecko algorithm or Admira, or you can just do the full on greedy method, which is a forward stepwise selection. So if we're taking l as some log likelihood of a statistical model, then this is guaranteeing us some type of log likelihood for the parameter that we, that we achieve. Now there is an issue with the greedy method. So we could just directly plug in the gamma and we'll obtain convergence guarantees. But now the problem is for low rank matrices. If you want to do the full on greedy approach for anything but quadratic functions, it's intractable.
00:18:54.064 - 00:19:38.882, Speaker B: In the sparse case, where you have a finite set, you could at least search, and so you're searching over all p things. In our case, you have an infinite number of things that you're going to have to search over. And the only place where it's easy to do, and we know how to prove things on is then in the quadratic case, because in that case you can use the power method. So what's nice then is rather than doing the forward stage wise greedy, you just do the orthogonal matching pursuit, or gecko, which we'll describe now. So the OMP selection. So it's described in a lot of nice detail in chalet, Schwartz and others, we'll choose the atom that satisfies the following. So we're at our optimization l.
00:19:38.882 - 00:21:24.654, Speaker B: So we've selected the l atoms, we take the gradient with respect to that, and then we want to find the rank one matrix that satisfies this lower bound, so it's greater than or equal to tau for some tau strictly greater than one, less strictly greater than zero, less than one of the max over u and v of this. And now, so if we ignore the tau, well, if we take tau to, say, be a half, then what this basically means is we can effectively just run the power method up to approximation half to get this rank one, this rank one solution that maximizes it. And what's interesting is then, with this scheme, if s is the solution that the greedy method pulls so after k iterations, then we can show that f of s is lower bounded by one minus e to the minus tau squared. This weeks of modularity ratio times k over r and then times f of s star. So if we take for a second tau equal to one, say you can do the power method perfectly, and then you just run it. This basically exactly kind of recovers the solution from das and Kempe, where they had the weeks of modularity, except now you have an additional tau squared term because of this, this approximation. So what we'll next talk about are sort of other results in this realm of atomic optimization, which I alluded to before, because this, this problem of atomic optimization is quite old and there's a lot of different results in it, and a lot of those results come in a few different flavors.
00:21:24.654 - 00:22:15.586, Speaker B: So, in order to define the results, first I'll define the atomic norm. This is also the total variation norm with respect to a dictionary, which is what is called in Baron et al. But I think a lot of people are familiar with atomic norms, and so I'll just stick to that. And so an atomic norm v is just equal to the inf over the sum of absolute value of c, sub I, where v is a linear combination of elements in the atomic set. So if the atomic set were coordinate vectors, this would be the l one norm for us, the low rank matrices, this is the nuclear norm, or the sum of the singular values. And then, so a lot of the bounds that exist in the literature are of the form l of theta hat. K is greater than or equal to l of theta star minus some epsilon.
00:22:15.586 - 00:23:03.670, Speaker B: So we have a multiplicative bound. Most of the results in the literature prove, prove an additive, an additive bound, and then so we can kind of compress a lot of the different results. And you get kind of three different types of bounds with respect to this epsilon. So, one of the type of bounds is just the atomic norm squared of the optimal solution divided by k. And so remember, k is like the number of iterations that you've taken. And so this you can do kind of in the general case using kind of ideas from gradient descent and Frank Wolf optimization. So another method is if the function is strongly concave, then in that case, people have shown exponential decrease of the form alpha to the k l of theta star.
00:23:03.670 - 00:23:49.414, Speaker B: But in this case, alpha is usually of the form e to the minus one over d one, where the matrix is say d one by d one, and then another form that also exists in the literature if it satisfies restricted strong concavity so this is strongly concave over the entire space. This is restricted, strongly concave. Another result is l of zero r over k. But what ends up happening is oftentimes we'd like epsilon to be of the order r times d one plus d two over n. The reason is this is frequently the statistical guarantee, where in this case, r is the rank we want. Our matrix is d one by d two. And then n represents the number of observations.
00:23:49.414 - 00:24:30.548, Speaker B: So say for instance, in a matrix completion problem, if you observe n items from the matrix. So for instance, different users tell you if they like or dislike a movie, and you only have n of those. Well, a lot of results from the statistics and machine learning community show that the error behavior of estimating the theta star is of the form r times d one plus d two over n. And these are, these are minimax. So these are basically optimal. So you'd like to, to have this type of bound. But then in that case, in order to have our error bound epsilon to reach this small, we frequently have to take k, the number of iterations.
00:24:30.548 - 00:25:25.154, Speaker B: So remember, r is the target rank, k is the number of iterations. Then k has to end up growing linearly in n or d one. And then that's a problem, because for a lot of problems, the rank r might be much, much smaller than d one or the number of observations. So for r bound, what we end up getting is epsilon behaves of the order e to the minus gamma k k over r. So what that means is, for us, we basically need to take k to be like r times log n. So we still need to take our rank to be larger, but by a much smaller amount than the existing work in order to achieve sort of the desired statistical, statistical guarantees. So, and we can see, we can see that we can unwrap our result.
00:25:25.154 - 00:25:56.362, Speaker B: So if we take any rank r matrix. So this is some arbitrary rank r matrix. This isn't like the best one. This isn't one that comes from a statistical model. This is just any rank r matrix. And if we denote that to be theta star, then we can show the following optimization bound. The greedy algorithm achieves the following error rate achieves theta hat sub k minus theta star in Frabinius, norm squared is upper bounded by e to the minus gamma r over k times l of zero.
00:25:56.362 - 00:26:34.554, Speaker B: So this is like our approx. The optimization error. Then plus r plus k, the operator norm of the gradient divided by gamma star gamma squared. So for a second, if we ignore this, this is the sort of the familiar statistical error bound that we would achieve. Typically, you have your, the operator norm of the gradient, if you are coming from some statistical model, is going to behave like the square root of the dimension of the matrix. And then so you end up with the statistical bound. And then plus this convergence, this optimization convergence.
00:26:34.554 - 00:27:33.426, Speaker B: And so that's exactly where this bound comes from. We just set that to be this epsilon. So we can also see some experiments to see, well, how well does this type of idea do in practice? Dasgupta and Collins and their exponential PCA paper have a lot of nice experiments, but we also ran some of our own. So in this case, we just look at the stochastic block model, but in a very simple form, where the probability of inside connections is p and outside connections is one minus p. And then all we do is we estimate the matrix after observing the binary variables with this type of log likelihood. So this is just the log likelihood that you would get if you're going to assume that the observations are coming from some exponential family. And so when we look at that, one thing that we thought was interesting, and so we only are comparing against spectral methods.
00:27:33.426 - 00:28:15.328, Speaker B: One is a normalized method, one is just the silly unnormalized method, and then one is the greedy method. And then what we have here is just a generalization error. So the probability of correctly putting people into the right cluster, into the right group, and here we have five groups. And so what we see is we have different sort of ranks, 3510. And in the greedy case, one thing that we thought was interesting is that it seems more robust to picking the right rank. We see similar type of results for reconstruction error. So instead of looking at probability of correct clustering or assignment, we look at reconstructing the parameter matrix.
00:28:15.328 - 00:28:40.364, Speaker B: And again, we see something similar. Similar. So at the end, what we have is we've reinterpreted this low rank approximation problem as a set optimization over an infinite set of atoms, and showed a new analysis for the greedy optimization. And I heard people talk about word embeddings before. We have some word embedding experiments in our paper as well. All right, thank you.
00:28:44.904 - 00:28:45.752, Speaker A: Any questions?
00:28:45.808 - 00:29:28.296, Speaker B: Comments? So, how about being a little more complicated than having away steps? Having what? Steps, sometimes called away steps, where you not only take greedy steps, but you throw away things. Yeah, yeah. So that the away steps, I guess also in the statistical side, they come like a forward backward optimization. So we're actually looking at that right now. And at least in the sparse optimization setting, the forward backward method works well in terms of not needing to go above the rank. You would just need to be, say, at two times the desired rank. The problem is at least the forward backward algorithm or the away steps that we see in the sparse setting.
00:29:28.296 - 00:29:38.044, Speaker B: Computationally, it starts getting way worse to start deleting things. But, yeah, we're actually working on it right now. Cool. Thanks.
00:29:38.984 - 00:29:54.494, Speaker A: One quick question. Down depends on L. If not or yes. So in the, say, gaussian case, that would be the distance from your starting point to your. That's dimension dependent. Do you know how I mean? Can you control it? To be always dependent polynomial in a dimension?
00:29:56.154 - 00:30:23.024, Speaker B: So, in the bounds that I showed, the recovery bounds, in order to even have restricted strong convexity, we kind of need the frabinius norm of the theta star to be bounded in some, in some unit ball, basically. And so that assuming that our parameters are in some bounded in some bounded ball removes at least the d dimensionality, but then we still have root n dimensionality. But then that gets killed because of the exponential. Thanks.
00:30:23.564 - 00:30:24.460, Speaker A: All right, so let's. Thanks.
00:30:24.492 - 00:30:24.844, Speaker B: Aha. Again.
