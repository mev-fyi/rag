00:00:03.920 - 00:00:31.116, Speaker A: Welcome, everyone, to the Simons Institute for Theory of Computing. My name is Sampath Kannan. I am the incoming associate director at the Simons Institute, and I work in theoretical computer science. And for many of you who might not be familiar with the Simons Institute, I wanted to give you a brief introduction about the institute, what we do, and so on. Simons Institute is the leading venue for research and theoretical computer science in the world. Some would say the only one. I mean, the one of it.
00:00:31.116 - 00:01:06.134, Speaker A: So it's clearly the leading one, but there's nothing like it anywhere else. And not only do we focus on what computation itself is, that's the kind of core focus of Simons Institute and the theory of computing. We also have. We take an algorithmic lens on other disciplines and see how thinking algorithmically or thinking from the point of view of compute theoretical computer science can tell you something about other disciplines. So. And we have many interdisciplinary programs. Chief among them, we are very big in quantum computing, and we work with physicists, but we have programs like this one.
00:01:06.134 - 00:01:47.254, Speaker A: We have a program on whale communication. So we have. We also worry about, you know, languages and how. How other organisms can communicate and things like that in various things. We also want to understand the brain and consciousness and all kinds of I endeavors. Theoretical computer science has a history of taking up tasks that are poorly understood, seemingly unquantifiable, unmathematical, and giving them a mathematical formulation that has gone on to become very powerful. The most salient example of which is machine learning, which one of the founders of the field is sitting right here in the audience, less valiant.
00:01:47.254 - 00:02:46.566, Speaker A: So, I guess, again, we hope that this workshop. This workshop, we hope you'll ask the question, what can theoretical computer science do for us, if in any way you can? But also, what kind of problems can we pose that could be interesting theoretical computer science? And so there will be people in the audience who are coming from that community would love to know the answers to either question. Okay, so that's the brief introduction. And I want to say thanks to the organizers sitting right here, Sherry, Philip, and Nate, who helped put together this program, and I guess the organizers of the workshop as well. Just a couple of logistic comments before I yield the floor. First is this auditorium has had a policy, strictly enforced from the beginning of no food or drinks in it. And that's how we've not had to reupholster the chairs over the twelve years of its existence.
00:02:46.566 - 00:03:18.764, Speaker A: So please adhere to this rule. There will be food in the morning, as you saw, and there'll be food during the breaks. But lunch is on your own, and you probably have many local Berkeley people who know where to go, and they can guide you to good lunch spots. And then in case you want to leave valuable things here during lunch, there are lockers on this floor which you can access for the day. And leave your backpacks, laptops, whatever you want while you go out to lunch. Have a great workshop and a great program to continue.
00:03:29.264 - 00:03:30.044, Speaker B: Hello.
00:03:32.224 - 00:04:17.598, Speaker C: Okay. Hi, I am Shiri, and this has been my dream for a very long time, so I'm so excited that you all could come here and actually make it reality with me. And I wrote just. I wrote several words, and maybe we'll see if I have time for that. Okay, so there's been a lot of talk recently about how we are approaching some form of AGI. But current efforts, like large language models, focus on scenarios that are kind of detached from how humans and animals behave in the real world. Text data is easy to scrape off the Internet to train large models, but language is only the most recent addition to human intelligence.
00:04:17.598 - 00:04:59.264, Speaker C: It relies on so many layers of capabilities for which we still need to develop sound models. So in this month long cluster, we're going to be here. Some of us are going to be here for a month. We're going to directly tackle the rich array of capabilities underlying natural intelligence systems. This week, we're going to think about the lower level capabilities such as perception, physical and causal learning, and the sensory motor stack. And then towards the end of the month, we're going to focus on language, analogy, world model, social learning, and consciousness. Such capabilities rely on learning in complex real world environments for multimodal, causal, and constantly changing data.
00:04:59.264 - 00:05:45.804, Speaker C: Humans and animals easily understand these data types, but current AI efforts cannot yet utilize them to make any progress towards understanding and modeling intelligence in the real world. We can't work in our little stinger discipline bubbles. We have to combine efforts from all the disciplines to study intelligence from AI, psychology, and neuroscience. Now, the goal here is twofold. On the one hand, we want to understand and model natural forms of intelligence like humans and animals, using tools from AI. And on the other hand, we want to build AI that's grounded in the real world. However, we almost never have an opportunity to engage in actual deep discussions with people outside of our field in academia.
00:05:45.804 - 00:06:16.902, Speaker C: In fact, academic incentive structures seem to even try to prevent us from doing it. Bringing together people who study intelligence is precisely why I decided to organize this cluster and spent over a year putting it together. This is a rare opportunity. You should take it if you're a computer scientist, you should talk to a primatologist. If you're a neuroscientist, take a developmental psychologist to lunch. And students. You should not be afraid of the Turing award winners.
00:06:16.902 - 00:07:08.474, Speaker C: I talked to them all and they are very, very nice. I am very excited that you all decided to take up this quest with us. This is truly a group of friends and family. If you're here today, it is because either I or one of my excellent partners in crime, Alison Gopnik, Doris Zau, and Amanda Seed, believe that you are one of the most brilliant thinkers today and also a lovely person that we would love to work with. I want to thank Jitendra Malek for the idea of organizing assignments program. Shafi Goldwasser, Sampat, Kanan, Sandy Irani, and the rest of the assignments, leadership and board for taking a chance on us crazy non theoretical AI folks. On Carolyn, Natia, Ashley, Frieda, and the rest of the assignment staff for doing all the legwork to actually get all of you situated here today.
00:07:08.474 - 00:08:10.634, Speaker C: Finally, I want to thank my co organizers for signing up before this became cool and putting together a winning proposal. We have such a fantastic list of incredibly famous speakers and a super packed schedule that we want to focus on the content rather than the fluff. And we're going to keep introductions to a minimum, except I reserve the right to introduce at length our first speaker. But before that, let me introduce you to our session chair, Filipizola, who is tenured Professor Minos Epsilon at MIT. So if you're writing letters, this is the time to think of him. Wizard of Computer vision, deep thinker, philosopher, and recently also the author of this beautiful computer vision textbook. By the way, this is your rare opportunity to get your copy signed because this week with Antonio to Alba and Bill Freeman, who is somewhere, here he is.
00:08:10.634 - 00:09:25.914, Speaker C: We actually have all three authors in the same room for the same for the first time, I think, in the west coast. So despite his very gentle nature, Philip will act as the emcee for this week, and he's going to keep the show running and all of us in time, except for me, strictly according to East Coast MIT standard instead of our lax west coast tendencies. Okay? And now to our first speaker, Professor Shimon Ulman from the Weizmann Institute of Science in Israel, from whom I first learned that AI could serve as a computational laboratory for experimental hypotheses from psychology and neuroscience. At some point distant point in the past, I knew I needed a new field of research, but I didn't know what that might be. I worked as a software engineer across the street from MIT, and I did what anybody would do in an existential crisis. I snuck out of work twice a week to take classes from the living history of AI, Marvin Minsky and Patrick Winston, who sadly cannot be with us here today. Patrick Winston's class was about the origins of AI.
00:09:25.914 - 00:09:58.638, Speaker C: We read Turing, brooks, Mar, and then a series of papers from Shimon Ullman. I don't remember what happened in the rest of the class because for me that was it. I was hooked. The depth of the problems discussed, the simplicity of the solutions proposed, and the beauty of the prose made me a convert. I decided to study computer vision, and thank you to Alba for letting me cross your class. Next. So folks, if you've never read, I brought with me Sheema Nolman's book.
00:09:58.638 - 00:10:14.714, Speaker C: I highly recommend it, especially if you are in computer vision, because it might have been published a long time ago, but many problems here are still open questions in the world of deep learning. And with that, I give you Shimon Ulman.
00:10:22.334 - 00:11:24.716, Speaker B: Thanks, Shiri, for the generous introduction, and I'm very glad to be here. I think we are all fortunate that these are such exciting times with all the advances in AI and together with it, the counter interactions with brain science and with cognition. And it's not only very broad, but I feel it's very deep, many levels. We go all the way in these exciting times from application and from technology, technology that is changing the world through the sciences that we are speaking about, and all the way to philosophy and some real implications to philosophy. So for the younger of you, it's a. These are special times, it doesn't happen all the time, and we should all be glad with it and appreciate it. Okay? And I'd like to talk today about a number of related topics that all related to combining innate concepts with learning to perceive the world.
00:11:24.716 - 00:12:15.498, Speaker B: And I'll talk about mostly about visual intelligence, although as you'll see, at least towards the end, the scope is supposed to be broadly about AI. But my emphasis will be on learning and getting intelligent in vision. When we as adults look at complicated scene like this, which is somewhat unusual scene, we immediately understand and get the meaning of it. We know what the people are doing and what happened just before this picture was taken and so on. But what I want to focus in the main, most of the time today is how it all starts. And I will focus on vision. And I said, so here is a baby looking at these numbers.
00:12:15.498 - 00:13:12.584, Speaker B: They supposed to represent pixels. They look at life intensities in the world, starting without knowledge. They watch the world, and they develop useful and meaningful representations of the world that they see. I should mention that people can also learn without vision. And that's an interesting question that we can discuss later, and maybe tomorrow at the discussion. So I will talk about these issues, and we'll end with more sort of general discussion of the kind of conceptual structures that are building in inside the learning person, the learning baby, and later the person. How it happens in humans and how it happens in models, basically is an open question for further discussion tomorrow and beyond.
00:13:12.584 - 00:14:08.024, Speaker B: Now, we were interested for quite a long time in this very early learning from a computational standpoint. How do you look at the world? Pixels are moving around. There is no real the initial time, there is no real supervision, and you just get the information out and learn to recognize important classes and events in the outside world and understand what they mean. And we started with two of those initially, hands and gaze. And we selected those because they are particularly challenging. Hands were known, I think sometimes were treated in computer vision as some of the most difficult classes to learn because they're so flexible and so on. And gaze is also something quite unusual, because gaze doesn't exist in the real world.
00:14:08.024 - 00:15:02.904, Speaker B: It's a three dimensional vector that somehow you have to pick up, learn to extract it and use it. So these are known as difficult problems, and these are things that humans pick up very, very early, very early in the few months, first months of life. And so this somewhat older world, older work from a paper called from simply innate biases to complex visual concepts for some time ago. So I mentioned that hands are difficult because they're very flexible, have many appearances, and often very small in the image. You can basically almost not see them, very difficult to see, and you still know about them, where they are and what they are doing. In models, this is being solved by a lot of supervision. You just have many images with annotated hands.
00:15:02.904 - 00:15:57.496, Speaker B: But this is not what children need, infants need. And we were worrying about this and thinking about this and what may indicate that the presence of hands reliably in the outside world, that you can pick it up and automatically learn it without any supervision. And we, looking at the literature and available data related to infants, a number of things came up that sort of indicated for us the possible way. And we believe it's probably maybe close to the correct way in which infants learn about hands. It turns out that infants are sensitive from birth. From initially, it looks to be innate or very close to the earliest time you can measure. Human infants are sensitive to some specific patterns of motion.
00:15:57.496 - 00:16:44.994, Speaker B: And this has been explored famously by Michaud to talk about these patterns and their relation to causality and to other things that we learn. You can see here, too, parts of the two examples of the kind of motion events that Michaud studied. In the first one, you see this red object coming in contact with the green one and sort of launching it. He called it a launching pattern that, for him was the essence and the beginning of understanding causality. And the other one is sort of carrying along. You can see the red thing contacting the grain and carrying it along. And people are, as I said, infants are very sensitive, and they look a lot when they encounter such patterns.
00:16:44.994 - 00:17:45.728, Speaker B: And so this is one thing. The sensitivity to mover events sort of resonated with the role of a hand, of touching an object, manipulating it, moving it around. And indeed, it also turns out that when infants look initially at hands, it's not, for example, their own hands. They look at other people's hands, and particularly at the instance that the hand makes contact and starts to manipulate an object. So it looks like this kind of interaction, the launch and the carry, is something that hands do. And one question was, is it mostly hands? And what happens in the. If you look at sort of random videos taken from people around people moving and doing things, looking for mover events? So here's an example of an image, in which case, it's slightly sort of favoring this kind of interaction, but this works in the real world as well.
00:17:45.728 - 00:18:38.506, Speaker B: And you can see here. Sorry, let me see how I get this working. It's a video work for me. If not, I will tell you what's. What's in the video. Okay. It works so you can see the person moving their head, their hand, and you can see that when they make contact and manipulate an object, this mover detector fires up just motion in the world does not do it.
00:18:38.506 - 00:19:39.444, Speaker B: It requires the interaction with the object. Here you can see it again, and it turns out to be very reliable. You get, from time to time, false alarms, not motions like this, but when one thing bumps into things and sets into motion, you can see the sort of the false alarm. But it turns out that if you do this and just automatically have a camera, and we build sort of this kind of mover detector, as we called it, and the details are not very important. It's relatively easy to build something like this, having it as proposed, innate structure that already exists, that it will fire almost only and quite reliably whenever there is really a hand in the image, getting in touch with object and manipulate it. And you see here some examples of kind of images that are taken automatically. When you take an image, a region surrounding the location where a mover detect event took place.
00:19:39.444 - 00:20:03.836, Speaker B: And initially, if you just take this as your initial detector, the performance relatively low. But you just now use a DNN, you just say these are training images. And from now on use your standard Ways of using this. Although it's not an entirely clean data set, use this as your data set. It's a ground truth. Learn what the class is. You learn hands and you can see here I will not go details.
00:20:03.836 - 00:20:47.160, Speaker B: There are a number of stages. For example, I mentioned one, it turns out, and again we did it after the data that really exists in infants. When they watch somebody manipulate an object and they trace their hands, they look between a second and 2 seconds and then they go off and do other things. But you can think about taking a number of frames of the hand moving the object for about 2 seconds. So we started just with one frame and the 2 seconds. You can see here some of the development, but the end result is that you get a very strongly performing hand detector. It's the green curve here going through the stages.
00:20:47.160 - 00:22:02.312, Speaker B: The red one is with having more data in existing data sets and complete careful annotation. So even the initial stage that takes just a few dozens of videos automatically gets you a hand detector that is coming close to what you can do with really very carefully crafted hand detector. So you get hands, they learn very well. There is no supervision. Everything is sort of internal supervision and are based on the assumption of a pre existing special detector that then hands over the results to standard learning, which creates the hand detector. We assume that it's a module in the brain and it has some special connectivity to other areas of interest that starts the understanding of hands and causality. So it's an interesting combination of it's not just the detection and the connectivity together with standard DNN learning that does the job.
00:22:02.312 - 00:22:50.294, Speaker B: Let me mention also that it's more just that label here is a hand, but automatically you get some kind of what a hand is about. For example, this is from an experiment by Rebecca Sachs at MIT with Sue Carey in which they showed infants this disembodied hand moving across the screen carrying a glass with it. And the infant looks at it and then a little bit after that they see either the hand moving alone or the cup moving alone, and are not surprised to see the hand moving alone. But they are quite surprised seeing the cup moving on its own. So clearly they understand that the hand is sort of the cause of this thing. It's the mover of things and not otherwise. So I think that's an important issue.
00:22:50.294 - 00:24:00.670, Speaker B: And you'll see it coming again in other examples that together with the more natural human learning, infant learning, based on some kind of promoting innate structures, it comes together that you can label and recognize the thing, but it comes immediately with the main semantic properties that this object is connected with, which is an important contribution to understanding the environment. Let me go to the second one, which is gaze. I mentioned that gaze is again, a challenging one because it's not really there in the outside world and they depend on subtle cues. I don't know if you can see here from the way you're sitting, but those of you are sitting in front. Do you think that this person is looking at you or looking down at the objects down here? What do you think? Yeah, here are the eyes of the person in large. It's a subtle cue and we just look at it and we usually, we cannot even escape noticing it. We know where people are looking at.
00:24:00.670 - 00:24:47.126, Speaker B: So it's surprising that we do it, and we do it very early on. We, I mean, the humans, three months of age, four months of age, we already do this at the high level. Nobody supervises us for us. And the question again, how come, where do we take our signal that this person is now looking at this object in this direction and so on? And we think that it's closely related to what we just discussed about hands, that they do it. And again, there is evidence, empirical evidence for it, that they take their sort of selected images where they know where the gaze is. Again, in this case, it's a little bit after hand learning. It's they look at the hand where the hand is making contact with an object.
00:24:47.126 - 00:26:35.118, Speaker B: When I have an object and I hold it in my hand, I can talk to you and move my hand and I don't have to look at it, but if it's here and I want to pick it up, the usual thing is that I have to look at it, make the contact, and then I can continue to whatever I want. But the instant of making the contact to manipulate the object is always associated with making a glance and looking at the object. So what you can do and what we suppose that you do, and what we implemented in a simple program is that whenever there is a hand involved in a mover event, we shift our model's attention to the face, and then we take an image of the face. And you can see here that we have people sort of manipulating objects from time to time. Whenever there is the minute of the second of creating the contact, we take a frame of the face, we connect the face with the location of the manipulated object and this is the data set to guide vision. It was interesting to note later on that we had to have not just DNN, just a network, but here a small program that does things, that whenever you detect the contact, then you shift your gaze, you take an image at a different place, place of the face, and you use it and so on. And then we found out about the paper that existed in fact a couple of years before we did this work, but in which slaughter in Austria, in Australia noted and wrote a paper about this issue, that whenever an infant sees another person touching an object, they shift their gaze to the face of this person.
00:26:35.118 - 00:27:05.146, Speaker B: So, you know, may or may not be a coincidence, but we were of course positively encouraged that, you know, something that is not entirely obvious. It goes along with the way of creating gaze detector. The gaze detector. Again, it was relatively simple, early learning. It gets encouragingly close to the best performing gaze detector in the world.
00:27:05.290 - 00:27:22.730, Speaker D: Yes, on that two slides ago. Yeah, the previous one earlier actually. Yeah. So here, did you do have any delay, a few frames delay between the.
00:27:22.762 - 00:28:31.736, Speaker B: Moving and the gaze or it's in the same frame? It's in the same frame. Effectively we sort of assume the buffer of a few frames in which you have a few images, including the one that is actual touching of the object. But the algorithm itself is really quite low level and simple. It turns out just mover, the primitive mover as it is, without verifying that it's also a hand also work, but not as well. So we think that hand comes first and then when you have a mover event and you verify that it's, it's a, it's actually a hand touching the object. Then you activate the second stage of the algorithm, shift to the face and, you know, you may be behind a little bit, but it was enough in realistic videos like this to get you very, you know, maybe there is, if there is a delay at how long, there is some other evidence that the gaze actually precedes. Right.
00:28:31.736 - 00:29:41.694, Speaker B: But maybe, you know, the infant doesn't know about it, but it also, the minute that, but the minute the contact is created, maybe the gaze is already there, you know, a couple of seconds later, but it's still a bit, it's a reliable instance to catch this. Now gaze, like we talked about, hand, it's not just again a class or here is a vector in space, and I know where the vector is. Gaze plays an important role in many things. It's probably many of you know about the phenomenon of gaze following and establishing common attention. That here is a taken from a picture, taken from an experiment where the mother is making, in this case, sort of a big move, but shifting her head and eyes to a new position. The baby look at this and looks at the same object that the mother is looking at. They establish a mutual attention, both looking at the same object, and this helps the interaction between the mother and the infant.
00:29:41.694 - 00:30:28.634, Speaker B: So in the way the model is learning, this gaze detection, again, is not just here is a vector in space, but it's a vector pointing to an object of interest. It's a place where the person is now putting their hand and manipulating an object. And it turns out that there is here and in other places. This progression, trajectory of learning, that you learn the initial event or initial object, but then you understand and it becomes more and more complicated. Gaze is used for various things in social interactions. Gaze is also used. Direction of gaze is also used in language reference.
00:30:28.634 - 00:31:34.912, Speaker B: When an infant hears a noun or a verb that they never recognize before, they look at the person, the speaker, and they try to follow their gaze and do some disambiguation of what he's talking about. So this little gaze, starting with the mover and attention, and then it naturally goes into attention. What's important, what does this other person have in mind? So for humans, all of these are not just give me labels, and I create more and more detectors for different classes. I think it comes in this nice setting in which you. It's being connected to the important things that are happening. We had an interesting prediction, this is more recent work, and you can see 2022 with someone I know from the Hebrew University, who works with people, children in Ethiopia, who are born with bilateral two eyes, congenial cataract, very strong cataract. It's not like older people cataract that you still see, but not as well.
00:31:34.912 - 00:32:11.648, Speaker B: This infant cataract is devastating. In severe cases, it's complete blindness. They could not see anything. In some lighter case, they see some light, light and dark, but it can be complete lack of vision. And this one, it's following Pawan Sinha, that many of you know at MIT, who started this in India. Then Udi Zohari from the Hebrew University, did similar things in Ethiopia. It's partly research and partly just doing good things.
00:32:11.648 - 00:33:05.474, Speaker B: They really find these young infants, children, and they have ambulatory equipment that they can do treatment in the. In the village of the. Of the patient and fix the cataract. And they start to see, they bring their vision, vision back. And we approach it with the prediction that we thought that this guidance of this innate ability to find you, to learn gaze, we had reasons to believe that it's temporary and it's not there for life. It's during the first year, two years of life, that infants use it and then it may go away. And our prediction was, and we wanted to test whether infants were congenital.
00:33:05.474 - 00:33:32.620, Speaker B: Initially sort of blind, and then they were corrected, but it was corrected at a later age, say six years of age, 818. Will they have this gaze ability? In particular, the gaze following that people use is automatic. When you see somebody look at another place, it's. People do it sort of automatically. It's not. Okay, interesting. Let's see what is, what the person is doing.
00:33:32.620 - 00:33:57.290, Speaker B: And it can be tested psychophysically in the following way that it's shown below in the experiments. And it was all big operation. But I'll just tell you the sort of the bottom line. So the experiment goes that you. You see a person, you first see just the person. You see the person here. Following a balloon here is sort of the experiment.
00:33:57.290 - 00:34:25.930, Speaker B: I will not go into the detail, but is the person looking directly? And then the patient needs to touch, touch his nose. And this is just to establish a connection. And then the person looks either right or left. And this is all in video. When I say touch his nose, it's not a real person there, it's a video. So then the person looks either right or left. And 300 milliseconds later, a balloon appears.
00:34:25.930 - 00:35:18.748, Speaker B: And the balloon appears either in the direction that the person was looking at or in the opposite direction. So it's either a compatible balloon or an incompatible balloon. And the subject needs to touch the balloon as soon as possible. And what you see in normal people is that there is a gap in reaction time. You are much quicker to touch the balloon if the balloon is compatible, if the balloon that came up is in the direction that the person was looking at. And what we found is what they found. We participated in this, that patients who were early treated in the first year of life, the cataract was detected and fixed, then they had the complete usual reaction time, and they showed this automatic gaze following people who were restored.
00:35:18.748 - 00:35:53.674, Speaker B: The vision was restored at a later age, late. These are late treated. They do gain close to perfect visual acuity. They really can see the eyes. They can tell you the eyes are pointing to the left, the eyes are pointing to the right, and so on. But when you do the experiment with balloons, there is no effect whatsoever. This arrow here shows the reaction time, the difference in reaction time between the compatible and incompatible.
00:35:53.674 - 00:36:29.556, Speaker B: It's completely random. They don't show any effect. The ones who are early treated are not different than normal. They completely recover this. Okay, so I talked about. I wanted to get set and tell you about much of what I wanted to say. There are more to say and more examples about trying to find computationally about this early learning, how it comes about and what it is associated with, how is the entire trajectory of learning new things and how they developed into more complicated projects, topics.
00:36:29.556 - 00:37:20.090, Speaker B: I will discuss this a little bit more, but just to tell you, I'll talk a little bit more about this. We called it at the time, the digital baby project, the modeling of early learning in vision. I will say less and less. This is becoming more recent and less research, but I will talk a little bit about higher level vision, sort of action recognition. And then I will discuss more generally the innate components within the larger conceptual structure that we have in our head. And just start with some thoughts that I hope will continue tomorrow. We have another discussion on this early learning and late learning and evolution and so on.
00:37:20.090 - 00:37:49.724, Speaker B: So, some questions for, for the discussion tomorrow. The next thing we wanted to discuss is learning to perceive coherent objects. It's impressive to see how lack of segmentation infants have. For example, when infants look at something like this, the evidence is they do not parse it into two objects. It's one thing, and that's it. They don't do anything special about it to break it into parts later on. They look at.
00:37:49.724 - 00:38:41.934, Speaker B: Sorry, they look at something like this, and there is a lot of segmentation to do here, and they do it easily. So somehow unsupervised will go from here to here. And what we investigated is how you learn it. I think that the number of people at about the same time, slightly after us, also propose that it's all based on motion or people. In principle, in psychology, sorted segmentation may be aided by emotion. And let me show you what we've done in it. And it takes advantage of two parallel, two complementary capacities that infants have that we know, that are wired in as close to birth as we can.
00:38:41.934 - 00:39:06.838, Speaker B: In motion segmentation. Here you can see the power of motion segmentation. You can see a piece of texture moving around. There is no change in shape or something. It's just the motion that separates the moving entity from this, from the background. And we immediately can segment it and do figure ground segmentation based on motion. So this is one component for young, very young infant.
00:39:06.838 - 00:39:39.366, Speaker B: This is an object moving, and the object is created based on motion. The other part that we have innately is a surprisingly accurate. I started it initially, many years ago. I was surprised by the accuracy of this other component, which is finding motion discontinuities, boundaries between two moving objects. And I'll show you here we see the boundary very sharply, although it sort of does not exist. It's a motion boundary. And we also make an immediate decision which one is in front and which one is behind.
00:39:39.366 - 00:39:55.112, Speaker B: So look at it and tell me which was in front. This will start moving. You can see a very nice boundary, so I hope. And can you tell which was in front? Right. So we immediately do it. And we see it. No super, no supervision, and we do it very well.
00:39:55.112 - 00:41:01.418, Speaker B: And we combined these two channels, one that groups moving pixels together to be one object, and the other one is to delineate objects based on this motion discontinuities. And you get, when you, when you do it enough, you really can get a very nice internal supervision for advanced segmentation of including static objects. So there are other, some pieces of work that support this. We went as closely as we can with what infants have, and it seems clear that you can get based on, based on these motion signals, very nice segmentation. And it's also interesting. People thought, many people thought in psychology initially, that babies are learned with some gestalt principles of common motion and smoothness of motion and so on. It turns out that you need much less than what psychologists suggested initially.
00:41:01.418 - 00:41:49.012, Speaker B: And this leads to related thing, which to me was very surprising to hear on work. When looking at work in psychology about spatial relations in general, you have two objects. They can be far, they can be closed, they can be one above the other, they can things like that. It turns out that one of the earliest, if not the earliest spatial relation that people ever learn is containment, that one thing contains another one. I think, without knowing this, if I had to write a list of the hundred first things that baby learns, I don't think I will have containment or containers in them. And it turns out that they learn it, and they create this class of containers that may be different in shape. These are two containers already for infants.
00:41:49.012 - 00:42:35.252, Speaker B: This is an object contained in another object. And this is learned very early. And I want to tell you, where is the magic here? How do you know about segmentation? And to me also, how come that it is in the, in the list of 100 earliest or even in the list of three earliest relation that we learn. And we think that it depends on something that we call paradoxical occlusion. And I'll show it first in the way children learn it initially, containment. They learn in a dynamic setting and not in a static setting. So here is a dynamic setting of containment.
00:42:35.252 - 00:43:04.136, Speaker B: Something getting into a container, object a, into c. And you can see that eventually, initially, a occludes object c, right, object a, occlude object c. As it goes further below, C starts to also occlude a. So you have this paradoxical occlusion. If you want, you can see it here on the left. It simply a occludes b. One occludes object, object one, occludes object two.
00:43:04.136 - 00:43:48.118, Speaker B: But in the right picture, you have this paradoxical occlusion. One occludes the other, that occludes the first one in turn. And if we can think about it, first of all, it's a very good cue that something that happens in occlusion. And also you can see from the previous example of the. Of the boundaries that infants are very sensitive to, which is in front, which is behind. So you are in a mode at this stage of learning that you do segmentation and you try to find who is in front of whom. And suddenly you get that in this usually simple world, a occludes b and it's in front, or b include occludes a, it is in front.
00:43:48.118 - 00:44:26.090, Speaker B: You have a is in front of a, b, that is in front of a. You have a special event here. So perhaps it's not surprising that this is at least an interesting event that they pay attention to and look at it, and it turns out to be a signature of occlusion. If you just look for these paradoxical occlusions in the right way, you find occlusions in. You can build a container and containment detector that works. That works very well. It even goes through certain stages.
00:44:26.090 - 00:45:03.738, Speaker B: And it turns out that the stages that the model predicts were actually also discovered, the stages in learning containment in humans. But I will not go into it. I will just mention one last thing about containment, which is, again, I think, interesting, and shows this trajectory of learning from classes themselves to what they mean and what happens in the world. And that's learning that containers can have this transport capacity. They can move things from place to place. You can put something in a container, either an object or stuff. It can be liquid or it can be powder.
00:45:03.738 - 00:46:02.436, Speaker B: Move the container from place to place, and the thing would move with the container and this is for an experiment by Bellajon and HesPers and showing the distinction that Infants made between something that you put behind and something that you put into. So if you look in the first row, this sort of represents an experiment, an experiment in which an experimenter is behind the curtain. Here is a container, this cylinder, and the experimenter takes this object, and in this case, puts it behind the container. And having put it behind the container, this thing disappears. It's no longer visible. The cylinder is moved, and the object is revealed behind the. As you move the occluding one here, it's a different thing here.
00:46:02.436 - 00:46:37.510, Speaker B: You put the object inside the container, rather below it, you move the container. And if in the experiment, what happens is, and they arrange it, there was a second object hidden here. So when you move the container, this object seemed to have appeared. And then the babies are. They don't understand what happened. It's clear that they expected the object not to be there, to be transported with the container. And look, it's based on the differences between putting into and putting behind can be very small.
00:46:37.510 - 00:46:52.918, Speaker B: They're very sensitive to it. So it's not just here is a labeling of. Here's an event. Something was put into something else. But they have the clear notion that now it's inside. And when this thing is being transported, they keep being connected. This is inside.
00:46:52.918 - 00:48:05.324, Speaker B: So you can see that they learn some interesting high level semantic aspects of how the world behaves. And for this, probably, they continuously try to predict where hidden objects are located. And I will not talk about this model at the moment. The last thing that I want to show about the digital baby, which gets into more sort of complex setting, is the woodwork task, which we looked into recently and still are looking at various interesting continuations of things related to learning about objects and interactions and goals and agents and animate entities in the world and inanimate things in the world. So at this level. Okay, so maybe I'll jump this, because unless you give me five more minutes, I started a little bit. I'll go quickly through it, but it may be okay.
00:48:05.324 - 00:48:38.114, Speaker B: Okay, so this is a very famous experiment by Woodward, and it was repeated many times. It's considered a particularly robust and thing that you can recreate easily. What happens is that you see a video of a hand. In this case, the three pictures are frames of a movie. They see a hand reaching. There are two objects on two pedestals. You can see a teddy bear on one of them and a ball on the other one, and you see the hand moving and touching the bear.
00:48:38.114 - 00:49:23.878, Speaker B: And then after that, another short movie comes, comes up. And the thing is that you have the hand moving, but the position of the ball and the teddy bear was switched. So the teddy bear was on the right. Now the teddy bear is on the left. And as you can see here, now the teddy bear is on the left. And the question is what the hand will do. Will it go to the same object that it went before, like the teddy bear, like this, going to the other direction? Or it will keep moving in the same trajectory it moved before and go according to the previous motion, not the previous held object.
00:49:23.878 - 00:50:12.426, Speaker B: And it turns out that what children expect already at four months of age, is that things that are animates, like hands or an animal, they go after the object that they went before, they will switch and we'll go with the object. And if it's a non living object, and it can be similar to a hand, maybe difficult to see, but this is sort of a road with something attached to the end of the road. So they expect the road to go in the same direction that it went before, not to the same object, and expect the hand to go the. To the object. So people go according to the goals. They have a goal, they like the teddy bear, they go with the teddy bear. Speaking in sort of.
00:50:12.426 - 00:50:38.776, Speaker B: Maybe it's more than they have in their head, but roughly, animate things go. They have goals and they go according to the rules of that. They want something and they follow it. And in animate thing, just follow physics. And if they push to the right, it will keep moving in the same direction. And it's a robust finding. And here you don't have to tell the.
00:50:38.776 - 00:51:22.588, Speaker B: They already know about hands and non hands, so you don't have to tell them that one is animate and the other inanimate. It's sort of. They know it and they use it. And what we wanted to see is what happens if you take a model now, it's an AI model, the VLM model, that you teach to do things. And you want to compare two possibilities. In one, you already give it the term that one object is an animate object and the other one is not. And you compare it with a model in which you just see the frames in the video just evolving over time, but without giving the.
00:51:22.588 - 00:52:12.940, Speaker B: The pre notion that one of the moving agents, the ECTO, is animate or inanimate. And note that we are not telling them that animate has properties that we do not assume. Innate knowledge that the baby know that animate things follow the goals, but just that he has a distinction between some things in the world are animate and some things are not. And now you're looking at the world and you try to understand what's going on and to make predictions. We assume that you still do not know this eventual knowledge, that there is this difference, that agents, living agents, pursue their goals. You don't know that you have to discover it. So both models need to discover this property about the world.
00:52:12.940 - 00:52:32.424, Speaker B: The only thing is that you add the concept. There is a concept of animate versus animated. Here it is. I'm telling you, it's animate. Use it, do whatever you do you want. So it's giving them some hint, but very minimal, without telling them what's going on. And I will spare you the.
00:52:32.424 - 00:52:57.202, Speaker B: I will spare you the details of the experiment. We do not have time, but I just let you know, show the results that everything is the same. One model has this initial innate concept animate. The other one is not. It doesn't know the model what animate means. It's just in the representation. Once you represent the image, we inject also a symbol for animate inside.
00:52:57.202 - 00:53:32.914, Speaker B: And do with this symbol whatever you want. And what you see here is the results and I'll just show you. All it sees is. What we see here is this toolbar is how the model that has the word or the symbol animate inside its representation as it forms the task. You see the training and the test. And the test is having the new objects in the new targets, replacing the initial targets, whether to go the teddy bear and the ball are replaced by other objects. That's the test.
00:53:32.914 - 00:54:04.516, Speaker B: So it lands perfectly, very quickly. Within these are hundred epochs. And the other model that sees exactly the same videos, but without this added symbol. This one is animate, is a trend on both in training and test. And then it learns slowly and not as well. This learns within 30 epochs. This gets to an asymptote for the first time after 16,000 epochs.
00:54:04.516 - 00:54:53.264, Speaker B: So it's a factor of 500 in terms of how many iteration you need. And it never reaches the accuracy that just adding the term this one is an animate gives you. So it shows that an interesting aspect is that giving hinting or giving the notion of a useful concept and adding it to the cognitive system of the thing that learns can. Can help the learning. Even with. If you do, you create and incorporate this concept in the. In the representation without giving anything about the implications of what this concept is eventually going to do.
00:54:53.264 - 00:55:53.004, Speaker B: Okay, for the first of benefit of time. I'll do this very quickly. We looked at vlms, models of vision, language models, and they learn about the world a lot. They see the world, they read all the Internet, you know, they see billions of, sometimes the large ones see billions of images. And the question is, if you play the game that the VLM is a baby, and you give it this videos and you ask it without training just to see, what do you predict? Will the hand go to the ball or will the hand go to the. To the teddy bear? After seeing that initially it went to the teddy bear, and you do the same with inanimate things. And the VLM is at random chance it doesn't, the VLM does not know the woodward, the woodward effect that babies know.
00:55:53.004 - 00:56:47.522, Speaker B: Okay, so I finished the presentation of data, just few slides about implications and higher level thinking. And with this I'll finish. So, first of all, just to summarize that, I think that about innate structures, I think it's interesting to know that the complex concepts, complex, relatively complex, but thinks about goal and things, about gaze and so on, are neither learned on their own or not innate. We do not have innate hand detector, we do not have an innate gait detector, gaze detector and so on. We have what I call sometimes photo concepts about this mover and paradoxical occlusion that they exist. But together with learning, we get to the final solution. We get to.
00:56:47.522 - 00:57:51.324, Speaker B: So we do not have hands in our innate system, but you have. But I assume that we have movers in our innate system, and they guide the system to develop meaningful representations. They provide their own internal supervision, and they can extract meaningful concepts like the gaze, which is very subtle, can be sometimes very subtle and occlude the container. Putting into the container things which can be missed are weak cues, but the system can deal with them. I also mentioned that it's not just you have a set of things that you learn this and this and this and this using innate concept. But the innate concept sort of pull us again along. Certain learning trajectories from mover to hand, hand helps gaze, gaze help social interactions and even language references.
00:57:51.324 - 00:58:40.840, Speaker B: Some people, when we talk about innate concepts, some people, in the context of AI, ask the obvious question, is there something special about innate having systems with innate pre existing notions? After all, it's all big learning. You can think about evolution and individual learning as one big learning. So, you know, I've seen only a thousand horses and I recognize horses, but my forefathers saw hundreds of thousands of horses. And this set up something into my system. And now I have to only do the fine tuning, but a model. We have computers that can run for a long time, they can do both. They will need many more images, but they will do it.
00:58:40.840 - 00:59:16.196, Speaker B: But I think that that's not the case and it's not equivalent and it's not going to work. And I'll tell you briefly why. And again, I hope to continue some of this maybe tomorrow, because when a baby is born, he's not born with almost perfect horse detector that he sees. Yeah, I know it's a sort of a horse, maybe not. It doesn't know anything. So it's not the fine tuning. What evolution produced is not an almost working horse detector, but some kind of a general machinery that can now quickly recognize any other objects.
00:59:16.196 - 00:59:54.286, Speaker B: And it's not clear how you do this. And it's not equivalent, I think formally it's not equivalent to, to just one long learning. To me it's more like this. And we have valiant here, maybe we can, he can tell us more about it, but roughly, I'm not saying it's exactly the same, but roughly we want to find a solution to something, some target function in a big space, and this may be very difficult. And what happens is that evolution help us get a reasonable hypothesis. Space, sort of a more concise space. And now the learner now has to search, but within a much more constrained environment.
00:59:54.286 - 01:00:52.786, Speaker B: And I will not pursue this analogy later, but I think this is more reasonable in terms of division between evolution and the agent learning. They are not just one big continuum, but they are different tasks and the evolutionary learning is different and it's not done by gradient descent on, on a DNN. So this we can discuss. I want to mention that we have innate, fascinating, clever, innate machineries all over the animal kingdom. So to think that we do not have them will be, you know, people say, hey, how could evolution put cognitive sophisticated things in our brain? It can and it does it all the time, so we should expect it rather than sort of try to see whether we can deal without it. This is a picture from the navigating ant in the sahara. I will not tell you about it.
01:00:52.786 - 01:02:05.056, Speaker B: It's fascinating. I'll mention one thing because it's perhaps most surprising. This ant can go hundreds of meters away from its nest, foliage and so on, and it wanders around and then it goes directly home. It has an integration in its head and it knows the direction and the length, the distance to go back to the nest, the distance by the way, it counts the steps, it counts how many steps, it is moved, and scientists know it, because what they do is you can see it in the image. They put ends like this on stilts that make the legs twice as long, and then they go twice the distance. They overshoot the. And when they integrate, and they have to know what distance, it's not just counting the steps, but they take geometry and cosines into effect, because if you go uphill, the distance that you covered on the real terrain is just this, the cosine, right? So they do this calculation and they put it into account in order to compute the right distance to the, to the nest.
01:02:05.056 - 01:02:38.114, Speaker B: So I will not go into, you know, this system any longer. But you look at one system and you get struck by the complexity of combination of innate things. And they have some learning, learning landscapes in the terrain. And it's a wonderful ceiling. It's a 0.1 milligram of brain that the ant has. And evolution puts in their sophisticated innate, innate stuff.
01:02:38.114 - 01:03:10.218, Speaker B: The last point that I want to make is I talked. What I emphasized so far is these proto structures make the learning more efficient and with minimal supervision. But I think it's also the cue to, it helps us a lot in understanding the world. And, for example, when you look here, all of these people are drinking, although some of them are drinking with a container. The other one is using a water fountain. Very different things. So as images, they're very different.
01:03:10.218 - 01:03:52.114, Speaker B: But. But what drinking is about is transferring liquid from into the mouth and swallow. And if you know this somehow and you don't just look at patterns in the image, then even things like this, you should realize that this is drinking and this is drinking. And I think that systems can be built in which they will never see such images. But easily we generalize, oh, sure, that's some other instances of drinking, because we really know what they're drinking. And I suspect that the innate concepts play an important role in this. Let me just note that in current models, vlms, this is lava.
01:03:52.114 - 01:04:39.296, Speaker B: Lava looks at these images, and it called this all of this drinking. And you can see these are not drinking, and these are not drinking because these things cannot support liquid, and these things cannot let the liquid enter into the mouth. And the kind of learning that infants learn that immediately with learning about containers, it turns out that at five years of age, infants know that surfaces with holes, perforated surfaces, cannot support liquid. It will go through, and you cannot carry liquid using something with holes and perforated. They know it at five years old age. So for an infant looking, this is not really a container, it's a fake container. It's not something that can transport liquid from one location to another.
01:04:39.296 - 01:05:47.948, Speaker B: So it relates to what I said before, that you learn not only to classify and name things. This looks like a container to me. It's a cylinder opening at the top, but it's essential, can it occur, cannot support liquid. So let me, let me skip this and the final slide. What I skipped is things that, anyway, general thing that we can discuss, discuss tomorrow about the role of the innate part to get better generalizations in understanding semantic things in the world. Just to sort of summarize with a sketchy sketch that are, I think, three levels that I described are innate concepts, which are shown here with the hooks because they make connections to the world. These are the existing detection units that can respond to specific classes or events in the world and help the system work.
01:05:47.948 - 01:06:33.228, Speaker B: And then you have behind them, you have sort of proto concepts like gaze, for example. There is no gaze detector, physical gaze detector, that fires whenever there is a gaze, but it gets its connection directly from the other ones. But I suggest that gaze already exists as a vector somewhere, as a small piece in the brain, connected to other things like hand and mover. And once they fire, they activate the gaze and it learns very quickly. So these are things that are innate in terms of the connectivity patterns. They are not things that are immediately activated by what happens in the world. And then there is the vast majority of other concepts.
01:06:33.228 - 01:07:39.484, Speaker B: We learn lots and lots of concepts, like communism, for example. You don't see it, you don't, I don't. You know it, and you learn it by language. And so the last question that I ask and leave for tomorrow is, what is the role of the innate concept within a much larger conceptual structure, in which most of the concepts are not learned by this early mechanism? And at least one possibility is that although the minority, they still have a large influence of how we understand the world. It's because they come early and because of the trajectories that we saw that one thing looks to another, and because they focus naturally on the semantic important interaction with the, with the world. So it's not the knowledge that they carry, but it's for those of you who follow a little bit philosophy for what Kant called the synthetic, a priori things that we have in our brain, in his mind. But it's not things about the world, but it's about how we see the world.
01:07:39.484 - 01:08:42.775, Speaker B: We say that objects have extent. It's not about the true statement, but it's the concept that we have objects and we have extent, and that's how we see the world. So I think that there is something similar to this, that the part of the innate concept is they create our perspective, our looking glass, our way of looking at the world. And therefore they play, I think, a very important, important role. And it would be interesting to look at them and maybe to incorporate them in computer systems, either by studying cognition and transferring it to models or by sort of new way of modeling learning. The more evolutionary part, how to computationally can study and decide what are the optimal ways of putting innate structures into computer system. So this is the end.
01:08:42.775 - 01:09:17.836, Speaker B: Here is just a slide, putting together various things that we talked about. And please, tomorrow we have a discussion on innate the role of evolution. How do we reach understanding all of these things? People who want to comment, to ask, to say on, please get in touch with me, send me email, catch me if you want to say something. We have only 45 minutes. It will be great. We have a great collection of people here, knowledgeable people here. So if you want to help, want to contribute some point of view, please let me know.
01:09:17.836 - 01:09:26.584, Speaker B: It will help me organize the discussion tomorrow afternoon. So, sorry about running all the time, and thank you very much for.
01:09:34.264 - 01:10:14.018, Speaker D: So let's take one or two questions and then we'll have our break. So, thanks for the talk, shubhat. So, one thing I would take issue with is that you have neglected the motor side of learning. So it's not just, I mean, even me. Before I started working in robotics five, six years ago, I realized that our vision people, we are both, we think, try to solve the problem purely in terms of vision, but we shouldn't. So if you think of your hand example, I mean, so you emphasize how you can learn from the exo views, right. Views of other people.
01:10:14.018 - 01:10:42.564, Speaker D: But also the child has access to the ego views. So when the child is looking at her hand and has all the appropriate signals. So in the first three months of life, the child keeps touching her body, his or her body, and gets a sense of the body image. The proprioceptive system is developed looking at the hand, but it's essentially not just a vision problem. My, my motor system is telling me.
01:10:42.604 - 01:10:43.836, Speaker B: I agree. Okay.
01:10:44.020 - 01:10:47.924, Speaker D: That makes the problem much easier. So in a sense, you need to do both.
01:10:48.004 - 01:11:37.330, Speaker B: Yeah, you need to do two quick comments. As I said, people looked at the first hands that you learn to recognize are other people hands, and you're not responding in the same way to your own hands. And it's also represented, at least in the primate brain, in a slightly different area, so it seemed to be related concepts. But the first one that you learn is other people's hands, and then you add the notion of your own hands, so it plays a role, but apparently they're somewhat separated. Let me also say that we ignore a lot of things. As I said, people without vision learn the world completely, and they have internal representation of the world. People have studied this and it's very difficult to distinguish between, if you have run questionnaires and so on, to know whether the person is a sighted vision or non sighted vision.
01:11:37.330 - 01:12:24.362, Speaker B: You can learn without vision even, and you can learn without manipulating the object. There are hundreds of people who were born without hands, without limbs, and the studies about them, and they learn completely, perfectly and at the same stage. So the result of redundancy, that we converge to a similar conceptual structure and we can do it in more than one way. And obviously manipulation is very important. And I agree, you can bring a lot of examples in support of the importance of manipulation, but it's true and it's helping and you can do without it. And you can have, you have this system that, based on vision, that can do most of the work. And even if this doesn't exist, you still know most of the things that you need to know about.
01:12:24.362 - 01:13:13.138, Speaker B: And that's interesting. And it's, in effect, I think it's something that we will be very nice to study tomorrow. Whether the convergence of, for example, of the visual apparatus, of our conceptual structures, general as humans, is very similar, regardless of many, many differences in sensory experience. And how come? I think that it's because of the power of the innate pre existing structure that are common to all of us and lead us in the same direction. Some people think that there is a very large role there to play in language. We all speak and we get all, everything through speech and everything else is not very important. Vision is not very important and manipulation is not very important.
01:13:13.138 - 01:13:40.474, Speaker B: The main role is vision. Vision is language. Language. We have similar, not the same language, but similar languages. And the convergence is more due to language than to the scaffold of innate concepts. We can discuss this more. So anyway, so your question brought, I think, a big issue that we should really look into.
01:13:41.974 - 01:13:51.774, Speaker D: I think we should go to break and continue the questions and discussion over the break. So we'll come back at 1105 right here to start the series talk. And let's thank people.
