00:00:01.760 - 00:00:32.086, Speaker A: That's like the least inspiring introduction that I've ever been given. It's like we've already heard the same material from other people, but we haven't heard it in Kevin's voice. So let's do it again. Actually, I'm really going to make an effort to tell you things that you won't have heard from the various other spectrum auction talks, even to the level of slides of my own, which are beloved by me, which have appeared in other talks. I'll leave out from this particular one. So if you don't know how spectrum auctions work, then I have bad news for you. I'm not going to tell you that much about the answer to that question.
00:00:32.086 - 00:01:16.564, Speaker A: I'm really going to try to focus on the computational stuff. So let's dig in. Before we do, let me point out that this is joint work with a couple of my students, Alex Flechette, who has been poached by Google, and Neil Newman, who so far has not yet been poached by Google and is here with us today. So thanks to both of them for their efforts. So the FCC is running an incentive auction. It's the first auction ever to feature incentives or something. And basically, the idea of this is to sell more radio spectrum to telecom companies so that they can make your phone work better, get more bars on your phone, and download those videos you want to download on your commute.
00:01:16.564 - 00:01:59.674, Speaker A: The problem is, there just isn't any more spectrum for them to sell. And so for a while, they've just been selling unused spectrum, and it's been a great thing. They've been improving the economy, reducing the national debt, and making everybody happier. But at some point, you run out of different colors of invisible light and you have to do something else. So what they're doing now is repurposing spectrum that was previously allocated to one purpose and moving it over to another purpose. And in particular, in the early days of radio spectrum, some really prime spectrum was given to broadcast tv because that was almost the only thing that spectrum was being used for at the time. And now it turns out that not very many people watch tv that's broadcast over the air, and a lot of people would like their mobile phones to work better.
00:01:59.674 - 00:02:50.388, Speaker A: And so there's kind of an opportunity to repurpose things. So, broadly speaking, the way the incentive auction is going to work is that spectrum is going to be bought back from television broadcasters, who are going to be paid to stop being broadcasters, to go off the air, and it's then going to be repackaged and sold for other uses, most likely mobile phones. This is a big deal. Of course, we don't know how much money an auction is going to make until the auction happens, but indications are that it's going to make a lot of money. So the Congressional Budget Office is the branch of the us government whose job it is to make impartial determinations of how much things are going to cost the government. And their official ruling was that it was going to net between ten and $40 billion for the government. That's NAT.
00:02:50.388 - 00:03:42.976, Speaker A: So the government has to spend money to buy out the tv stations, and then it's going to turn around and repackage that spectrum and sell it off. And it's expected to be making this kind of money. From what I understand from Paul more recently, I think that there are now some indications the numbers might even be getting bigger based on some sort of public estimates of the values of things. But it's clear that this is an enormous application of auction theory, albeit one that's going to happen only once in the US, and it's coming up really soon. So in the end of the first quarter next year, this is projected to happen. So mark your calendars for March 29 and be prepared to see a real victory of algorithmic game theory and practice start to lift off. So I'm going to, as I said, say very little about how the auction works.
00:03:42.976 - 00:04:34.016, Speaker A: So this is my only slide, really, about how the auction works, which is intended to give some scaffolding to those of you who might not have heard the many spectrum auction talks that have happened at Simons already. But broadly speaking, there are going to be two different auctions happening. So there's going to be a so called forward or ascending price auction in which spectrum is being sold off to people who want to buy more spectrum. Probably telecoms and prices in each region are going to be increasing as long as demand exceeds supply. At the same time, there's going to be a reverse or descending price auction for broadcasters in which broadcasters are being offered money to buy out their television stations and free up this spectrum that is ultimately getting sold. And essentially, I'm going to speak only about this reverse auction. The forward auction is pretty traditional at this point.
00:04:34.016 - 00:05:18.638, Speaker A: This is similar to what's been done in the past, and there are a few wrinkles for this new auction design in the forward auction. But first of all, not any that I've been particularly involved in. And secondly, the real innovation has happened on the reverse auction side. I think it's fair to say so in this case, you've got kind of a funny thing. Many of you might be used to thinking about combinatorial auctions. When you think about spectrum auctions here, what's combinatorial is the constraints, whereas the bidders are really simple. We have single minded bidders, at least to a first order approximation, where every bidder is a television station who is trying to decide, should I stop being a television station, or at least should I stop being a broadcast television station, or should I hold onto my station? And so they just have to make a yes or no decision.
00:05:18.638 - 00:06:24.358, Speaker A: What's combinatorial is the decision that the auctioneer is making about which stations to buy out, because the auctioneer has a constraint that across the whole country, it wants to clear a certain amount of spectrum. And in order to do that, it doesn't want to buy out too many stations in one market, and it certainly has to make sure it doesn't buy out too few stations in another market. So that's where the kind of combinatorial optimization is going to come in here. Now, in order to ensure that this auction makes money for the government rather than the reverse, we need to ensure that we don't end up paying a lot to clear a lot of spectrum and then not getting very much money to sell it. And so in order to ensure this, the two auctions are linked. And I'm not going to say much about that, but there's a revenue target specified in legislation to say that the auction has to cover, essentially cover its costs and cover the, the retuning costs associated with moving around the broadcasters who don't get bought out. And so basically, if we haven't yet achieved that amount of revenue, then the two auctions have both stopped.
00:06:24.358 - 00:07:08.056, Speaker A: Then what we can do is just say we were wrong about how much spectrum we were going to clear. We were originally running an auction where we were trying to buy out some particular amount of spectrum, some particular number of tv channels. Let's just reduce that amount of spectrum that we're trying to buy out. Now, this will allow us to continue running both auctions, so then we'll have less demand in the auction where we're buying and more supply in the auction where we're selling, and maybe the other way around. But anyway, we can keep running things so we can ensure we have positive revenue. So here's the thing that I am going to focus most of my time on today. This is the question of feasibility testing, which is kind of inherent to the way that the auction works.
00:07:08.056 - 00:07:56.424, Speaker A: So in this reverse auction, what's going on is you're a television station, and I'm going to come to you and say, my starting price for your television station is $50 million. Would you like to relinquish your broadcast rights in exchange for $50 million? And these numbers have been chosen to be deliberately, really high. So the hope is very many tv stations are going to say, that sounds fantastic. Of course, my broadcast rights are worth $50 million to me. I can keep serving most of my customers on cable anyway. I'd be happy to sell to you. And then we get an initial situation where there are a lot of broadcasters who have agreed to go off the air, and in fact, there are many more than we need to meet the clearing target that we have, which is lucky, because these prices are astronomically high for the government to be paying.
00:07:56.424 - 00:09:03.186, Speaker A: So then what happens is we start seeing whether we actually need each of the broadcasters, and if we don't need them, which is to say everything would be feasible if that broadcaster wasn't to go off the air, then we offer a lower price to that broadcaster, and we keep kind of iterating round robin through the broadcasters until everything stabilizes, until basically we need everybody, or until people decide that they don't like the prices they're being offered, and they decide to go back on the air, and they stop participating in the auction. So from a strategic point of view, this is great. It's very simple, because bidders are being made a series of take it or leave it offers that are just descending in price. So it's easy to see that bidders have a dominant strategy in this auction. The computational problem, though, is figuring out whether we actually need the bidders. That's the part that I'm worried about here. So what this means is every single time I want to go to any bidder, and there are 2991 stations, so there are nearly 3000 agents who might be participating in this auction, depending on who makes these initial decisions.
00:09:03.186 - 00:09:59.016, Speaker A: To participate, they have to go to every one of them and say, would it be possible to not take this station? Would it be possible for this station, who's currently told me that he's willing to go off the air? Would it be possible to put him back on the air, given the reduced number of channels that I'm trying to make everything fit into? And asking that question is more or less a graph coloring problem. It's not quite a graph coloring problem because we have side constraints. So turns out, the way that radio propagation works, if I'm on channel 17, it might be that Costas, who broadcast near me, also can't be on channel 17, but it might be that he's so close to me that he also can't be on channel 18. So we have these kind of adjacent color constraints in the graph coloring formulation. Um, but, but broadly speaking, it's kind of a graph coloring problem. And that's bad news, because even just as a straight up graph coloring problem, it's easy to see, you know, that's np complete. So I, in principle, I'm going to have to solve many, many, many of these problems.
00:09:59.016 - 00:10:42.186, Speaker A: And in simulations, it's on the order of about 100,000 problems per auction that need to be solved, because we're going to have to solve it once for every guy in the auction, for every price tick. So every time I want to lower somebody's price, I have to ask this question. Um, and, and, uh, and the problem is enormous, and not just in the number of stations, but also in the number of constraints. Sorry, let me just finish my sentence. So, we have, uh, almost 3 million interference constraints, um, between all of these different stations. The reason the number is so high is that actually the, you know, Costas and I might interfere on channel 17, but oddly enough, we might actually not interfere on channel 26 because these are different frequencies. So the propagation characteristics are actually different.
00:10:42.186 - 00:10:47.882, Speaker A: Their signal might be more absorbed by trees in one frequency than another. So I saw a couple of hands over here.
00:10:48.058 - 00:11:02.994, Speaker B: So the constraint, the coloring constraint, is it that you just have a number of colors? Is it just you trying to clear the same frequencies across the country? Or maybe I have these frequencies on the east coast, these different frequencies on the west coast.
00:11:03.114 - 00:11:04.490, Speaker A: Is it combinatorial, or is it just.
00:11:04.522 - 00:11:06.074, Speaker B: A number of colors that you have?
00:11:06.894 - 00:11:47.930, Speaker A: The number of the colors that I have sounds combinatorial to me. So I'm not sure what the distinction is that you're asking throughout the graph. Or is it kind of a little more or less? I mean, there are the various places where something might be held out for some kind of complicated reason. I mean, anytime you dig into something practical, there's sort of irritating complexity that creeps in because of all sorts of special cases, but sort of to a first order approximation that, yes, we're going to say, I want to take all of the tv channels which are now in between. I think it's 14 to 52, and I want to change that. So it's 14 to 29. And so everybody who's in any of those higher channels, they're going to have to get moved down, which is fine.
00:11:47.930 - 00:12:19.840, Speaker A: I don't care where they go, but I'm going to have everybody who's willing to participate in the auction off the air. I'm going to have some people who weren't willing to be in the auction all on the air, and I'm already having to repack them. I'm having to assign them different channels than they used to have because I've gotten rid of some of their channels. And then when I going round robin trying to decide whether people's prices can lower, I have to ask, is it possible to, I have this problem that's solvable for all the people who are currently on the air? Is it also solvable when I add you, and I have to ask that for everybody?
00:12:19.952 - 00:12:31.998, Speaker B: Yeah, I think you said that you do this unilateral test. If I drop this station, am I still able to me, by slight constraint, why is it unilateral rather than a set to stations that could all be simultaneously dropped?
00:12:32.166 - 00:12:34.814, Speaker A: Well, because we can make these price decrements one at a time.
00:12:34.854 - 00:12:38.314, Speaker B: So I wanted to change the price asynchronously.
00:12:38.814 - 00:13:31.356, Speaker A: Well, you can sort of see, you can think of one kind of round of price lowerings as just one pass round robin through all the stations. But the order does matter, because if I go to you first and you say, I'm not going to accept your price, I'm going back on the air, that might change whether it was possible to deal with Shaddon or not, still wants to be on the air, but it's, the frequency at which it's operating is so off that it is going to cause problems. Will you still be happy? Yes. So actually, Congress had to pass legislation to make this auction possible to happen for, among other things, that reason. So stations who want nothing to do with this auction don't have to give up their tv station, but they're not guaranteed that they're going to end up on the frequency that they're on now. So what they are guaranteed is that they're going to end up on a frequency that doesn't have significant interference with other stations. And so that's what all these constraints are.
00:13:31.356 - 00:14:16.840, Speaker A: They're ensuring that everybody who's on the air is in a decent place where they can reach the people that they're reaching now. Okay. So initially, when I first got involved in this project, some skepticism was expressed to me about whether this problem could be attacked at the national scale. And people were thinking about things like, what if we just look at some sub area when I'm trying to think about a given station, I look at some geographic radius around that station to try to make the problem smaller. And even then, it was a pretty challenging problem that really doesn't work very well. And I think if you look at the graph of the United States with the true constraints drawn on here, even though it's a bit small, you can see why that is. It's just really densely connected, particularly around here.
00:14:16.840 - 00:14:55.664, Speaker A: And so just the constraints can propagate pretty far. And so making some kind of artificial restriction is just going to be cutting a lot of edges for no reason. You're going to take a lot of problems that might be feasible and make them be infeasible. Something I should say that's really important is given that this, I'm trying to solve an np complete problem here, you might wonder what happens if you just fail to solve it. Okay, Laden Brown, you can make your fancy fast AI algorithms, but you're not going to change the, the laws of math. And so sometimes you're just not going to succeed in solving something. So something I think that's really cool about this auction design is that it's kind of robust to occasional failures.
00:14:55.664 - 00:15:48.950, Speaker A: So what happens is every time we want to lower a price for somebody, we can only do that when we can prove that it's possible to put that person back on the air if they want to leave. If in the amount of time we have, we're just unable to prove that, then we just have to act as though we've proven that it's not possible to put that person on the air, which means their price has to just freeze for all time at that level. And that's costly for us, but it's not a complete disaster. It only affects that one bidder and the rest of the auction can proceed as it does. And so what that means is if once in a while we're unable to solve problems, that's going to have a kind of direct financial cost. I don't think I know an example of solving combinatorial optimization algorithms using heuristics where there's as direct a relationship between incrementally solving a few more problems and making huge amounts of money, but nevertheless, it sort of degrades very gradually with the number of problems that we can't solve.
00:15:49.062 - 00:15:57.342, Speaker C: Yeah, you visit a bidder, you try to lower his price. You cannot repack him, but you can also not prove that he's not repackable.
00:15:57.438 - 00:15:58.230, Speaker A: Exactly.
00:15:58.382 - 00:16:07.282, Speaker C: This other guy, will you reback or whatever then? Do you ever consider going back to the original guy in case the instance became easier.
00:16:07.438 - 00:16:24.578, Speaker A: No, for incentive reasons. Right. We don't want to give people complicated incentive reasons where they might try to affect the hardness of the instance through something. And basically, if I ever make you an offer where I say, okay, your price is frozen, then if I want this thing to be simple and strategy proof, I just leave it.
00:16:24.706 - 00:16:32.214, Speaker C: Say you try to solve it, not commit that you are done, but sort of like there is some room. They don't know the order in which you.
00:16:32.914 - 00:16:49.802, Speaker A: This is a more sophisticated question. Various designs of this forum have been mooted. I'm not actually sure what situation we're in relative to these rules. I think the official party line right now is what I've told you. Yeah.
00:16:49.938 - 00:17:11.394, Speaker B: The SEC, back in the day, used to be very worried about seemingly little things like tie breaking and kind of the vagrancies of the way you do your computation affecting the outcome of the auction. How comfortable are the policymakers with what you just said, that the price dynamics somehow depend on the ability to solve the computational problem?
00:17:13.254 - 00:17:52.748, Speaker A: I would be a fool if I tried to answer a question about how comfortable the policymakers felt about anything. And so I won't. I mean, there's been a lot of discussion about all sorts of, you know, very fine details of this auction, as there should be, because as all of you know, from the practical, you know, high stakes auctions that have been held in the world, sometimes, you know, getting some small detail wrong can matter a whole lot. And so I think there's been a lot of care given to those sorts of things. I think the, the policymakers have expressed kind of, you know, competing interests. They want to save as much money for the government as they can. You know, they want to be transparent and understandable to bidders.
00:17:52.748 - 00:18:01.476, Speaker A: They want to have this thing happen on schedule. And so I think they're weighing all these sorts of concerns. Anyone else? All right. Yeah.
00:18:01.660 - 00:18:05.304, Speaker C: How much time do you have to solve? You say if you don't solve it in time, you move on?
00:18:06.364 - 00:18:25.914, Speaker A: That's a question under active discussion and consideration. I hear Paul Milgrom laughing because he knows how much time I spend thinking about this question. On the order of a minute, let's say. I mean, if we did fancy things like Costas is proposing, then maybe longer in the bad case instances. But if we do what I've been saying now, then something like a minute.
00:18:27.334 - 00:18:31.574, Speaker C: Is the station exited? Just waiting there for you to let them know in a minute. Are they good?
00:18:31.694 - 00:18:48.594, Speaker A: We would only do, like, a couple rounds of the auction per day. So there's only going to be on the order of a couple of price movements per day. So this auction is not like an auction for a Picasso at Christie's, that everybody gets to go home in the evening and someone's got the painting. This is going to unfold over a long period of time.
00:18:49.314 - 00:18:59.770, Speaker C: You run this one problem, and you find out that you're going to offer this station a lower price, but you can't move on to the next station till you know what the station did.
00:18:59.802 - 00:19:00.374, Speaker B: Right?
00:19:00.914 - 00:19:45.792, Speaker A: Right. So I think we have basically conditional information from stations about how they would respond to the one movement if it were to come. Okay. So, anyhow, my punchline here, as you've been able to read in the last, like, five or ten minutes on this slide, is that we're indeed able to solve these problems, or else I wouldn't be here telling you about it. I think part of the point of this talk is just to tell you about not the inner workings of all these methods. I don't have time for that. But the kinds of methods that we use and the fact that they work, because I think this sort of broad program for how to attack a hard computational problem about which we know a lot in advance distributionally, is one that is pretty broadly applicable outside this area.
00:19:45.792 - 00:20:03.376, Speaker A: Yeah. How do you know so much distribution? Where did you take your distributional information from? Well, for one thing, I know that we don't have to run this auction in China. Right. I know where all of the television stations are in the United States. That's huge. Right. I know this master constraint graph that everything is generated from.
00:20:03.376 - 00:20:41.798, Speaker A: Secondly, I know the population distribution in the United States, and I know that the value of a tv station has a lot to do with the population that it serves. I know the signal strength of every tv station in the United States. And so, along with population and some geographical characteristics, I can know something to a first order about valuations. That tells me something about what would happen in an auction. I don't want to believe that too firmly, but I can put some pretty big error bars on that and end up with a distribution. Okay, so let me start telling you some actual answers from data. So, for the first, I'll tell you two different sets of data in this talk.
00:20:41.798 - 00:21:26.358, Speaker A: The first one is a set of data that the FCC told me were allowed to publicly release, and I think we'll be releasing soon, which is, at this point, very out of date. It's the interference data is already public. The set of constraints, which is based on a clearing target of trying to get everybody into 19 UHF channels. And the data that I'm soon going to be able to release is this. Results of some simulations which are based on the smooth ladder auction mechanism and valuation assumptions, kind of along the lines that I was explaining to EvA. Turns out there's some complexity about if you're really old and you've ever watched broadcast tv, you might know that there's this one thing called UHF and this other thing called VHF. There aren't very many VHF channels, and so the repacking problem in VHF is pretty easy.
00:21:26.358 - 00:22:06.640, Speaker A: So for this entire talk, I'm just going to focus only on UHF problems, which are much harder computationally. And we did a sort of appropriate test training methodology here, where we took all of these problems. We randomly pulled out 10,000 test instances and 1000 validation instances, and we left everything else to train on. You'll understand better what I mean by training, as I go on. And as I was saying to ashish, we have a 1 minute cutoff. So when I first got involved in this problem, there were a bunch of or people already working on it and being or people they like using mixed integer programming solvers. So that's what they were using.
00:22:06.640 - 00:22:55.158, Speaker A: They'd actually thought quite a lot about generating fancy kinds of constraints and doing some relatively sophisticated things, which I actually don't have their code, and I'm not really at liberty to try to recreate it. So what I've done here is maybe a little bit less good than what they did, but I think it's not really that different. What I've done here is just take off the shelf cplex and Garobi, which are the two kind of high end mixed integer programming solvers, and I'm trying to solve them on our distribution. So let me walk you through what this graph means, because you're going to see a lot of graphs that look like this. So on the x axis here, I have runtime, and it's on a log scale. So this is a 10th of a second, and this is a second, and this is 60 seconds, which is my cutoff time. And on the y axis, I have the fraction of instances that I've solved within that amount of time.
00:22:55.158 - 00:23:38.240, Speaker A: So, for example, you can see that within 1 second, CPlex, which is the blue line, has solved a little bit more than 10% of the instances in our test set. And by the end, CPlEx has rallied and managed to solve about half of the instances, whereas Gurobi is doing a bit less well. So this is pretty terrible. Right. You would really hate to run an auction with a 1 minute cutoff using these solvers, because half of the time, this isn't even half the stations, but half the time you ask the question, you're going to end up saying, sorry, I'm not able to tell whether that price could be lowered. I'm going to have to not lower it. People are going to end up, on average with prices pretty close to the starting points, which are high on purpose, so that they encourage participation and the government is going to be bleeding money and it would be a disaster.
00:23:38.240 - 00:23:42.704, Speaker A: So that's why people were thinking that doing this at a national scale didn't seem like a good idea.
00:23:42.784 - 00:23:45.044, Speaker C: Is this on a laptop or on a super.
00:23:45.744 - 00:23:55.084, Speaker A: This is on a high end cluster computer. I mean, I'm running on one core, these are all single core algorithms, but this is on a cluster at UBC.
00:23:55.204 - 00:24:01.412, Speaker C: If I were to continue for like, I don't know, like ten minutes, would I get 90% or would I get like, I don't know.
00:24:01.548 - 00:24:41.464, Speaker A: Well, I mean, you can kind of see like it. You know, these curves tend to sort of flatten rather than suddenly becoming spectacular. It's on a log scale. You can sort of extrapolate in your head and see that within any kind of reasonable amount of time that would be consistent with running a couple auction rounds a day, you know, you're not going to suddenly be getting 90%, and even 90% is not all that could be wished for. So the first thing that struck me when I got involved in this is that it's a bit of a funny choice to be using mixed integer programming to try to solve this problem, because there's nothing being maximized here. There are no real values in the constraints. It's really a purely combinatorial decision problem.
00:24:41.464 - 00:25:22.172, Speaker A: As a person in AI, if I think about a purely combinatorial decision problem, the most obvious thing to think about is a reduction to sat. So that's what we did. I won't walk you through the reduction, but this is all very straightforward. What I will point out most of all is that the reduction produces mostly clauses of length two. If you just kind of skim through these constraints that I've drawn here, you'll notice that most of the constraints are pairs of disjunctions, and only one of them is a disjunction of many variables. And that's a good thing for Sat solvers in practice, because I'm sure you all know that two Sat is easily solvable in polynomial time. And in practice, if I have a clause that is mostly two clauses.
00:25:22.172 - 00:26:11.644, Speaker A: I can more or less search over the non two clause part and then sort of propagate constraints through the two clauses and solve things pretty quickly. So what actual sophisticated solvers do is a little bit more complicated than that, but it's a good intuition to have that the two clauses are kind of a good case for SAT solvers. So every year, there's a SAT competition where the SAT community makes their best SAT solvers and puts them all head to head. There's been amazing progress in the SAT community over the last few decades in incrementally improving these solvers. And not only are they doing a great job of making their solvers faster, but they're also making them publicly available through these competitions. So it becomes possible to download a bunch of solvers and try them all. We tried a large number of solvers, I think, made, like, a couple of dozen solvers, and the ones that I'm graphing here are just the best ones to avoid giving you a really cluttered graph.
00:26:11.644 - 00:26:47.220, Speaker A: But what we found is that they pretty much dominate the mixed integer programming solvers for this problem. It really seemed to be a good idea to work with SaT solvers. The best SaT solver that we had is DCCA, which is actually a local search algorithm, so it's not able to prove unsat at all. Um, but it turned out most of the problems in our test set are satisfiable, so that was okay. Um, and, uh, you know, the. Even the worst one here is an algorithm called clasp is doing about as well as our best, best mixed integer programming solver and lies above it most of the time. So, on the whole, sat really seem to be doing pretty well.
00:26:47.220 - 00:27:27.246, Speaker A: Uh, but even so, we're getting, you know, uh, less than three quarters of the problem solved in a minute using our best algorithm. That's still a lot less than we want in practice. Oh, and here's I'm putting underneath these cumulative density functions a histogram of satisfiability status, showing you whether the problems being solved were satisfiable or unsatisfiable. So, in this histogram, I have a blue area if the problem was sat, a red area if the problem was unsat, and a gray area if it times out. So, of course, all the timeouts are over here. You might be forgiven if you think that there isn't any red anywhere. In fact, there is.
00:27:27.246 - 00:28:07.814, Speaker A: It's always at the top. So you can see some little regions of red, but there's really not very much. It's kind of a side effect of the way the auction works, because once I encounter an unsatisfiable problem, that station gets frozen and I stop seeing problems that have that station in it. And so I tend to spend most of my time in the course of an actual auction generating satisfiable problems. So that's kind of good news, in particular for things like DCCA that are satisfiable. And generally, in practice, it's faster to prove satisfiability than to prove unsatisfiability, because you get to stop when you encounter a solution. On the other hand, it's sort of discouraging because that means we might expect that a lot of this region is also satisfiable, which means that's money left on the table.
00:28:07.814 - 00:29:04.648, Speaker A: So what do we do to make things better? Well, it turns out a topic near and dear to my heart that my research group has been focusing on for more than a decade now, something we call algorithm configuration. And the idea is to think about the design of heuristic algorithms, not as a process of human ingenuity and exploration, but instead as a process of combinatorial search. Really, in the end, you have these various different design decisions you could make in designing a heuristic algorithm. And what we usually do, the kind of canonical approach, is that you have some intuition about what would work well, and you run some experiments and you see how it does, and then you look at those experiments and you think, maybe I should have changed something. And you changed that thing. And you run another experiment, and you kind of iterate a few times until you get to the paper deadline, and then you send that to aaai. The problem with that is, that's a pretty lousy search algorithm.
00:29:04.648 - 00:29:43.424, Speaker A: I'm doing kind of coordinate descent. I'm changing only one thing at a time. It's very likely that these heuristics impact on performance is correlated. It's likely that my starting point is not the best place to start, and I'm in this high dimensional combinatorial space, so it's really likely that there's just great stuff there that I never find. So what we've been working on is the design of algorithm configurators, which are meta algorithms, in the sense that they're algorithms that take other algorithms as part of their input. So you give me a set of problems that you want good performance on, a way of measuring performance, and an algorithm which has many parameters, and these parameters can be absolutely anything. It's a black box.
00:29:43.424 - 00:30:34.984, Speaker A: So you can use these parameters just to express all of the different heuristic ideas that you might have had. And then you kind of throw this thing on a computer cluster and wait for a few days and it does this exploration for you. So it basically draws on the statistics literature on experiment design and black box function optimization, does some kind of clever machine learning on the inside, and in the end comes back with a configuration that it can show to statistical significance, performs better than anything else that it can find. And so it seemed like a really good target application for algorithm configuration here, because as I was saying to Eva, we know that we're facing the us constraint graph. We know a lot about the kinds of stations that we're facing. We really are happy to build something that kind of narrows in just on that part of the problem. We want to be careful that we don't overfit our training data.
00:30:34.984 - 00:31:24.774, Speaker A: That's why we do this training test split, so that we're sure that we do generalize within our distribution. Otherwise, though, this seems like a good thing to do. In particular, we used an algorithm that we call sequential model based algorithm configuration, or smack. And the way that this works is that we build a response function in parameter space. So you can kind of imagine a space that has one dimension for every parameter, and one more dimension, a response dimension for runtime. And you can kind of picture in your head that there's some true underlying response surface that says, for every parameter, what's my average runtime in this distribution for that joint parameter setting. And then what we do is we use random forests of regression trees, which we've already heard a little bit about at this conference, to build this response surface in real time.
00:31:24.774 - 00:32:15.046, Speaker A: Based on some initial exploration, we find a place that looks good in terms of that response surface, and then we go actually find out whether it is good. We conduct an experiment, which means we run the algorithm actually on those problems, we see how they do, and if they look good, then great. But even if they look bad, that allows us to learn more about our response surface, and we just keep iterating until we run out of time, basically. So that's what we did. I'm happy to talk more about this, if anyone's interested in it. But the upshot is that we built a new solver called clasp h two, for reasons that I won't tell you, but we built this new solver, which is just clasp, but with different parameter settings. So something I didn't tell you before is that clasp is actually an algorithm that was made by its creators specifically to work well in this paradigm of algorithm configuration.
00:32:15.046 - 00:33:04.492, Speaker A: So they exposed many, many parameters that do all kinds of different things, and they really wanted to leave it up to people like me to find things that are going to work well in their distribution. And so this algorithm, which actually started out as our worst algorithm, you might remember, improves so much that it's the green line up here. It becomes far better than the best thing we had seen before, and we're now at the point of solving about 85% of of the problems. There's one more thing that I might do. I might ask, can the previous assignment be something you might notice about the way the auction works is I've always got a previous assignment. I'm always having a solvable problem and trying to add one station to a solvable problem. So one thing you might ask is, can I just add the station without changing anything? It turns out the FCC software already does that without even talking to me, and that solves 80% of the problem problems.
00:33:04.492 - 00:33:57.664, Speaker A: So everything I'm talking about here is something that they gave me, which means that didn't work. But the next best thing that I might do is say, can it be augmented by fixing every station that doesn't neighbor the station that I'm interested in to its value in the previous solution and just solving the induced subproblem in the immediate neighborhood of the station that I've got. If I ended up proving that that was unsat, I would learn nothing, because it might just be that I had to make bigger changes in the graph as a whole. But that might be a really fast way of driving myself towards a sat solution. So we built a sort of presolver that just tries this for a short time and sees if it works, and then goes and tries to solve the real problem. And we put that together with this configured version of Clasp, and we ended up getting this performance that you see here. So here's the configured clasp, and here's configured clasp with the presolver and also some kind of software engineering trickery to make things faster.
00:33:57.664 - 00:34:30.055, Speaker A: And this was called SaTFC feasibility checker based on satisfiability 1.0, which was released by the FCC last November. So about a year ago. And so that was our kind of previous benchmark we got written up in the press that cares about such things. Amazingly. Tvtechnology.com dot I'm sure you all follow it, but if you don't, I was particularly happy here that he says, I doubt many individuals will have the time or resources to generate their own feasibility checks.
00:34:30.055 - 00:34:58.824, Speaker A: It sucks up computer power. So the people were clamoring for us to make this faster, literally coming to my door. Okay, not literally, but indeed we were still leaving a bunch of money on the table. So we really wanted to make this thing faster. And so we looked for some other things we could do to leverage this specific problem. One thing we can do is take local search solvers like DCCA and actually seed them with the previous solution, rather than having them just work at random. That turns out to help a bit.
00:34:58.824 - 00:35:38.776, Speaker A: We've tried, incidentally, many things that I'm not telling you about, because they didn't help. All of these things did help. Another thing we can do is we can take the induced constraint graph from a given problem and try to decompose it into disconnected components. And if we find disconnected components, we can solve each of them separately. And for example, if I was to find that one component was unsatisfactory, then it doesn't matter if I can't even solve some other component. I've already shown that the whole problem is unset. And one more thing that I can do is I can say, is there a station which is unconstrained in the sense that for every assignment of the other stations, it's always the case that this station can take some feasible value, maybe a different feasible value in every case.
00:35:38.776 - 00:36:19.724, Speaker A: If I can prove that this fact is true about some station, then I can just emit this station from the problem, because I know that I could repair any solution that I found to fix this station. And it turns out this can actually really help. Furthermore, if I do this first, I can end up getting more graph decomposition than if I didn't do this, because it involves dropping some stations. So I end up disconnecting my graph a bit. So looking at how much under constrained station solving helps. Here's a CDF showing the degree of stations that we find in the real problems that we encounter. So we're seeing problems with up to to station degrees of 60, so stations that interfere with 60 other stations that are really already on the air.
00:36:19.724 - 00:37:15.370, Speaker A: And here I'm saying, how often was I able to detect and remove a station as being under constrained? And we had an initial heuristic that we used, and then we ended up finding something fancier and better. So the initial heuristic that we used is able to get up to about 8%. And you see that it sort of tops out at stations with very low degree. We ended up with something much faster, based on our consistency, that gets us up above 45%. The reason I'm telling you about both of them, is that the results I'm showing you right now are based on this green line, and the results that I'll kind of gesture at the end are based on the red line. The next thing that we did was to build algorithm portfolios. So, just as a financial portfolio works by saying, let me exploit variance in the performance of different stocks, by owning many stocks, we can do the same thing with algorithms for solving hard computational problems.
00:37:15.370 - 00:38:00.474, Speaker A: So, it turns out that they tend to have very high variance in their runtimes, and that these variances are often uncorrelated. And so it can be a really good strategy not to run one algorithm, but to run many. Even if you had to task swap them on the same process, or even if you're not getting parallel computation for free, you can really gain something by running many things at the same time. And again, this is something my group has been working on for a long time. We developed something called satzilla, which is pictorially described in this image here. And it's been winning a lot of SAT competitions over the last decade or so, just basically by taking other people's SaT solvers, which I don't really understand, and putting them together with machine learning and making them outperform their constituent part. So it really shows the power of this kind of machine learning approach in combinatorial optimization.
00:38:00.474 - 00:38:52.756, Speaker A: Another thing that we can do that we've proposed a bit more recently, is something we call Hydra, which basically says, what if I start with one highly parameterized solver, and I want to build a portfolio out of it? So I want to end up with a bunch of uncorrelated solvers that can be put together, that perform well together. But my starting point is just this space that you have to search, you have to make these things yourself. So Hydra's a methodology for doing that, and that's actually what we did in the FCC problem. So we actually ran Hydra many times, both kind of configuring our existing solvers and choosing among them to eventually find a set of things that worked well together. And I'm happy to tell you offline how Hydra works, but the upshot is we made a four algorithm portfolio, and so we're going to run. SATFC actually runs on four processors at the same time. And it looks like this.
00:38:52.756 - 00:39:27.876, Speaker A: So we actually ended up, we looked at a couple dozen sat solvers, but amazingly, we ended up choosing two clasp variants, even though they're so similar. So clasp really was powerful for our problem, and we ended up choosing a version of DCCA that augments based on the previous solution. And we ended up building up a presolver based on DCCA. And if we put all of these things together and run them in parallel, then, and we look at which thing solves first, then I get the performance of this maroon line, which is starting to look pretty good here, and now getting pretty close to 100% of the problem solved, something like 98.
00:39:27.980 - 00:39:41.024, Speaker C: So, to understand this graph, the set of instances you're looking at, you simulate running the auction like you simulate several auctions, and for each auction you get a lot of such problems.
00:39:43.244 - 00:39:55.864, Speaker A: Yeah. So just to be clear, I then take that set of problems, I separate them into two sets, I do my algorithm configuration on a training set, and I evaluate the results on a held out test set. And what you're seeing in every picture I show you is test set results.
00:39:56.604 - 00:40:01.684, Speaker B: Yeah, it's held out from auction instance to instance, not from round to round.
00:40:01.844 - 00:40:41.260, Speaker A: So this is held out just from instance to instance, because the data set that I'm telling you about doesn't identify the auction that things came from. At the very end of the talk, I'll show you some results that are held out auction by auction, which is obviously more interesting. Ok, so the last idea we have that I'm going to have to go through very quickly, which is a shame, because I think it's a really cool idea, is caching. So it turns out that I really would be willing to leverage a huge amount of offline time if I could make my solver faster. And I know the constraint graph in advance, I have this distributional information. If I could somehow use this to run on a cluster forever and ever, I could save billions of dollars for the government. It seems like a good idea.
00:40:41.260 - 00:41:18.564, Speaker A: The problem is, I can't just solve every problem offline because there are two to the n possible problems, and n here is 3000. And so I can't generate every possible problem, even in the us constraint graph. Of course, all these 3000 problems don't occur in practice because of what I said about populations and how that affects valuation. And all of that way, the auction rules actually work. So I might hope that there's more structure to what happens really in practice. So maybe I should build a cache and just remember what I've actually seen in practice and see if I ever encounter it again in a different auction. And does that work? No.
00:41:18.564 - 00:41:42.514, Speaker A: Basically, I never see the same thing again. Almost never. So we built a cache on a training set of 200,000 problems. We evaluated this test that I told you about of 10,000 problems, we were just never seeing the same thing again. So this idea was just not working. But here's the insight that made things work. If a station set is repackable, then every superset of that station set is also repackable.
00:41:42.514 - 00:42:21.174, Speaker A: If I can solve a given problem, then I could drop some parts of that solution and I would always still have a valid solution. So this is good because there are exponentially many supersets of a given set. So this dramatically increases my chance of getting a collision in my cache. So you might think I should go to my cache and query to say, is there some superset I've ever seen before and use that as my cache. That's great. The problem is I need to find a way of querying this cache because there are now an exponential number of different keys that I want to be able to hit with a given query. So I need to find some querying algorithm that runs in real time where I can say, here's my query string.
00:42:21.174 - 00:43:28.600, Speaker A: Do you have some superset of this query string in the database? And basically we found a way of doing this where we build a traditional cache that we can hash into and a secondary cache that basically allows us to map queries in the original queries into queries that are actually going to hit something. I can't tell you exactly how it works, I think, in the time that I've got, but it's kind of a cool randomized algorithm that basically works by randomly ordering the different items in my secondary cache based on their sort of bit string representation under a random ordering of the bits, doing binary search, and then using the property that a superset is going to be ordered higher in the list. And if I have many different lists, all according to different random orderings, then I hope that this prefix is going to be short in at least one of those lists, and I'm going to find the thing I'm looking for quickly. If that was too quick for you, I'm happy to talk about this part offline. But instead let me tell you how the cash actually did so on this original set of problems. The cache did spectacularly well. The cache is the dashed line here.
00:43:28.600 - 00:44:00.602, Speaker A: I'm just putting it up against the portfolio I showed you before. This graph is saying how did the cache actually work? And it's saying there's a lot of stuff that I never hit in the cache, ever. There's a lot of stuff that I hit only one time. Some of it saves me almost no time, and some of it saves me enormous amounts of time. And then, interestingly, there are also keys that I just hit over and over again and again. Some of them saved me very little time and some of them saved me a lot of time. So it seems like the cache really was working for us in the end.
00:44:00.602 - 00:44:34.360, Speaker A: We're solving 99% of the problems in a fifth of a second and 99.6% in 60 seconds. So we were pretty happy with these results. The main thing that we, and we released all of this as SATFC 2.0. The main thing that we were concerned about with all of this is this is all about one data set. So the last thing that I want to tell you about, I just want to put up some graphs that are really hot off the press as of late last night of much newer simulation results. So the FCC has been generating data in the last couple of weeks.
00:44:34.360 - 00:44:57.826, Speaker A: So we have data that was generated on auction simulations between October 22 and November 6. So really recently they generated 1.1 million instances in total. And all of these are instances that were not directly solvable by augmenting the previous solution. And this comes from 80 auctions. And all of these auctions are based on different simulation assumptions. The FCC didn't do this to make a dataset for me.
00:44:57.826 - 00:45:43.028, Speaker A: They did it because they want to test the effect of all kinds of crazy things that might happen in the auction. So these are really a wide range of different kinds of assumptions, including assumptions about valuations, who decides to participate, how much spectrum we try to clear and how long we run SATFC for, and how that affects the progress of the auction. So taking all of this stuff and kind of throwing it together, here's how SADFC does. So here I'm showing you 80 different cdfs on the same graph. And I've drawn dark the median cdf median by its the point that it hits at the end here. So, um, you can see that, you know, overall we're doing very well. Um, our, um, uh, again, I'll say we, we see the same pattern we saw before of mostly satisfiable problems.
00:45:43.028 - 00:46:21.696, Speaker A: Um, if I look at the solved percentages, and I should say all of these experiments were done with the cache turned off, because that complicates the analysis. So the, the true performance would be better if we had the cache running. Um, but it, it's complicated except to think about what we would train it on. If I look at the solved percentages here, drawn as a histogram, you can see that most of the time we're solving more than mid to high nineties percents of instances. There are a couple of outliers that catch your eye, and the other one we're solving very few. Luckily, those are really pathological cases where the FCC was trying things that they really don't expect would happen. We haven't retrained on all this data yet.
00:46:21.696 - 00:46:46.638, Speaker A: We intend to rerun algorithm configuration based on all this data. We imagine, again, that would only improve performance. This is trained only on the previous data set that I showed you. The last thing I want to show you is we tried to test how the cache works. And what's tricky here is that the FCC was not trying to make things easy for me. So they often reran more or less the same auction with some very small change in it. And what I wanted to do is what David was kind of asking me about before.
00:46:46.638 - 00:47:25.192, Speaker A: Some kind of leave one out analysis where I leave out a complete auction, I train the cache on everything else, and then I see how well the cache is doing on that auction, and I kind of try that everywhere, sort of cross validate. The problem is that gets complicated by the fact that two auctions are more or less the same auction. I end up sort of training on my test data. Testing on my training data. So what we did here is an analysis where we took every auction and we looked at how similar they were in the sense of how many problems did they share in common. What fraction of the problems do they share in common? So, density on this heat map is by what fraction they share in common from 0% to 100%. And we put a threshold at 20%.
00:47:25.192 - 00:48:11.574, Speaker A: So we're coloring. It's the same luminance, but the hue is red when it's above 20% and it's blue when it's below. So Neil and I decided that 20% was a good threshold for saying anything that's above 20% is too similar. We're going to call it the same auction, and anything that is below that we think is it's different enough, we're going to call it a different auction. So, under that similarity measure, here's how the cache does. So, what I'm looking at here is, for each of the 80 auctions, I built a new cache containing all of those auctions that weren't overly similar to it, which, of course, sort of biases away from what the cache is good at. I'm trying to get problems where there would be few direct cache hits, and then see how well I do on these superset queries.
00:48:11.574 - 00:48:42.424, Speaker A: And what you see is, again, there are a few pathological outlying cases where the simulation assumptions were really different from everything else. And we do very badly. And most of the time we're not doing quite as well as we were before in that previous case, which doesn't surprise me because we weren't doing a leave one out auction analysis. But our hit rate is still really high. So our median hit rate is over here. It's about 40%. So something like 40% of the problems that we encounter are solvable just by looking at problems that we've solved before.
00:48:42.424 - 00:49:26.144, Speaker A: And of course we can make this better and better by just having a bigger and bigger cache. So we're obviously not going to stop here. We're going to do as much cache training as we can before the auction really happens to try to push this curve up. So I'll conclude there. Thank you for your attention and happy to take questions. So the real action that the relapse is still be running. How do you know that there is no box in the code? Like, did you write the code? Are they rewriting your code? Are they trusting your code? Or how are you trusting their code? As you can imagine, I've been asked this question before.
00:49:26.144 - 00:49:53.606, Speaker A: They're doing many things to worry about. This one thing that we're helped by the fact that these problems are np. Because if I claim that a problem is unsolvable, there's no calamity that happens. If I'm wrong, claiming that a problem is unsolvable wrongly is like timing out on a problem. It's exactly like it. And so that would be a shame. And we don't think that we have those kinds of bugs.
00:49:53.606 - 00:50:44.858, Speaker A: But if we did have those kinds of bugs, it's not terrible. If we encounter a case where I claim that a problem is satisfiable and I'm wrong, that would be really terrible because I could get the auction into a trajectory where I can no longer repack everybody and just awful things would happen and people would come kill me in my sleep. But luckily we just had three independent teams write different code that is in the critical path that checks a claim of satisfiability and make sure that it really is satisfiable because why not? It's really fast to do so that we really do do for robustness. Then of course they just do all of this kind of enterprise software validation to protect it from everything the us government is concerned about. As you might imagine, after Obamacare, software quality has become a primary concern of the us government. So I think they're doing a pretty good job with just making sure that everything works. The answer is that you wrote the software that they're going to run.
00:50:44.858 - 00:50:50.754, Speaker A: Yes, they're going to run our software. Yeah. Okay. Thanks very much.
