00:00:00.200 - 00:00:24.914, Speaker A: So we have the next speaker who needs very little introduction. A lot of us learned RL from reading his lecture notes and books and articles. So Chaba special, he's like two places right now, I guess, one in University of Alberta and one in Google DeepMind. And he's going to talk about multi criteria mdps today. I look forward to it.
00:00:25.864 - 00:01:28.676, Speaker B: Thank you. Shivra, right, started. How many people here know about this multi criteria mdps? Constraint mdps? Okay, so I do a short introduction about those. And basically the purpose is to discuss whether these, solving these is harder or not than solving mdps in a very particular narrow setting. And so I wouldn't venture to jump into the conclusion that in general you can expect that the results are going to hold for more general settings. But at least in these narrow settings, I think that we found something quite interesting. So first, the standard framework for reinforcement learning is that you have maybe an agent or brain.
00:01:28.676 - 00:02:15.084, Speaker B: It interacts with an unknown environment, sending actions to it and receiving states and rewards from it. And the goal is to design some brain for this agent such that whatever environment is going to face it can maximize the potassium of expected discounted rewards. In this case, I'm going to assume this content. And so you have this little gamma factor here, comma, is a discount factor. It's between zero and one. And future rewards, rewards received later are discounted by raising this discount factor to a higher power. So that means that they're exponentially less important.
00:02:15.084 - 00:03:03.860, Speaker B: If you would plug in reward of one, this series converges and the limit is one over one minus gamma, which also corresponds to. You can think about that as the effective horizon that the planner needs to look ahead to plan for good actions. Because beyond that horizon, nothing really matters because things are so small. Okay, so the difference to the standard setting that I'm going to consider today is that instead of having one reward function, we're going to have at least two reward functions. I'm going to keep it simple. So we are going to have exactly two reward functions. One reward function, I call it a task reward.
00:03:03.860 - 00:03:33.158, Speaker B: You want to maximize it as usual. The other reward is kind of like a constraint reward. So that's why it's called a c. It's kind of confusing. You might think that it's a cost, but it's not a cost, it's still a reward. So you're summing up this constraint rewards in a discounted fashion. Take expectation in the usual way, and you want to ensure that this constraint reward is high enough in the task that you're solving.
00:03:33.158 - 00:04:39.864, Speaker B: So that's the formulation. This is called constraint mdps, and you can motivate constraint mdps in various ways. Generally, it is just that sometimes it's more convenient to state the problems in a constraint formulation than to state it as a pure optimization formulation. It kind of broadens the class of problems that we can describe. And that leads to my next slide, which is about a paper done by my excellent colleagues at DeepMind, recently appeared in the AI channel, and they're arguing that a single reward should be enough for everything. So this goes into the face of that, because here I'm saying that, well, some problems are just going to be much easier to state if you have two rewards. So maybe under some conditions, you could reformulate things in such a way that you can get away with a single reward.
00:04:39.864 - 00:05:52.194, Speaker B: But in general, that's certainly another case. For example, it's really, and we will see that it's really easy to come up with examples where the optimal policy for this constraint formulation needs randomized, whereas for a classic formulation, when you have a single reward, there is absolutely no need to randomize, ever. So the class of optimal policies is already different. So clearly there is a discrepancy here. But the question is, to me, whether it's not whether a single reward is enough. But maybe I would ask the question in such a way that developing algorithms for the single reward cases, enough to handle maybe these more complicated cases, and we will see it today, that this is kind of like, really intriguing question. So, is there a non trivia cost to be paid, for example, if you're debating from the single reward formulation? And secondly, whether the algorithms that you are developing for the single reward case are going to be extendable to this case.
00:05:52.194 - 00:07:04.876, Speaker B: And, yeah, it's kind of very curious. All right, so this is just the symbols that I'm going to use throughout the talk. So I promise that I will keep everything super simple. So we will have finitely many states, finitely many actions, no generalization with functional approximation, no monkey business with that, keeping it super simple. So you have s states, a, actions, they are identified with integers, let's say, between one and s and one a, and you have rewards. And for most of the talk, I'm going to assume that there will be a planner, and the planner is going to be given the rewards ahead of time, but the planner is not going to know the state transitions. The state transitions describe this markovian dynamics of how the state transits from a state to an ex state, given that the agent is taking some action and in this formulation, because we are talking about a constraint formulation, it's going to be important that we will have an initial state distribution, a state distribution with initial state.
00:07:04.876 - 00:08:21.384, Speaker B: So every time we are evaluating a policy, which for our purposes, it's enough to think that it's just a map from states to actions, maybe it should be stochastic. It's like you need distributions of reactions, at least. So whenever you have such a policy, you roll out from the initial state chosen from row, and then following the policy that generates a sequence of states and actions, you receive the rewards along the way, you discount them, sum them up, take expectation, and the goal in standard mdps is just to maximize this. And this can be done regardless of the initial state distribution that exists, policies, stationary deterministic policies for that matter, which achieve the best possible. This is called a value at every state, just single policy, and for notation. So, since we're going to change the reward function, deviation from the standard notation is that I'm going to put the reward in the index of the value function. So that's going to be important.
00:08:21.384 - 00:09:07.654, Speaker B: And so, one more concept. An epsilon optimal policy is any policy that comes epsilon close to the optimal value at every step. All right, so this is standard just to recap notation. And if you know everything, then value iteration, policy iteration, linear programming. All of these excellent tools can be used to find a policy and avoid an exponential cause that you would encounter if you just try to enumerate all deterministic policies. All right, so that's dynamic programming. So in the constraint formulation, you have this constraining reward function, which also takes values in zero and one, and recycling the notation that we had before.
00:09:07.654 - 00:10:12.994, Speaker B: You can just very concisely write the problem as maximizing the value given an initial state under the Husk reward subject to a constraint, and the value that is generated by the constraining reward function. So the planner is going to be given this b, and maybe it's given this role and it's given c and r, but maybe it's not going to know the transition probabilities. And we're going to be interested in how many samples you need to generate from the model in order to be able to solve this problem. If you knew everything, then you can reformulate this and write this as an LP. You just write the discounted state occupancy measures. And these values are just the reward integrated with respect to this discounted state occupancy measure. This is also similar to that seducer linear in these discounted state occupancy measures.
00:10:12.994 - 00:10:53.660, Speaker B: And these measures live in a very nice polytope. So you can optimize this over this polytope using Lina programming. And from that you can just marginalize with respect to the actions and read other policies. That's how people would normally think about solving a problem like this. And so in the talk, I'm going to make this feasibility assumption, without which there is no life. Just have to assume that this constraint can be satisfied by some policies. So in other words, b is not too high.
00:10:53.660 - 00:11:09.824, Speaker B: So if you just want to maximize the constraining reward function, the maximum value that you could achieve is the best policy that doesn't care about the task reward, just cares about the constraining reward would exceed b, and there is a positive gap there.
00:11:11.724 - 00:11:15.260, Speaker A: I'm imagining the optimal policy is no longer stationary.
00:11:15.452 - 00:11:39.054, Speaker B: The optimal policy can be a stationary policy, but it will be randomizing if you think about what the LP solution does. Okay. So you would need to prove that. Okay, like that. That's the case. But if you are rewriting this, if you're thinking about just stationary polysystem, you can write this with discounted occupancy, but you can go beyond stationary. You can go to markovian polysis.
00:11:39.054 - 00:12:12.054, Speaker B: They have the same state occupancy measures as the, as the stationary markovian policies. So it's all fine. Right. So you can just think about this as the LP, and then from that you will see that you can always find a stationary markovian policy. And there is even a theorem that says that the number of states at which you have to randomize something like that is dependent on how many constraints you're going to have and so forth. So in this case, it's one or something. Yes.
00:12:12.054 - 00:12:58.282, Speaker B: How large are these LP? And how do you solve this LP? Right, right. So I'm dealing with a toy situation here. So I'm going to postpone anything related to going beyond really small state action spaces where you could enumerate everything. And that's absolutely not practical. That's not how you should be thinking about these things. What people would do normally is that they would throw in function approximation here, approximate the value functions using some features, neural networks or whatnot, and then you can scale up. But that's exactly what I'm not going to talk about today.
00:12:58.282 - 00:13:35.084, Speaker B: And I don't even know what generalizes from what I'm going to talk about today. So this is a least practical talk ever. Yeah. The reason you do this is because in the past, we have seen that whatever intuition you're gaining on studying a city problem like this, a lot of it transfers. So whenever you can push things to work with function approximation, it kind of is rooted in ideas of city problems like this. But I have no better reason than this to study this problem.
00:13:38.864 - 00:13:45.676, Speaker C: If you had made this a pathways constraint, if you had said, on average, sorry. On every sample part, the cost has.
00:13:45.700 - 00:13:48.476, Speaker B: To be at least three, right? It's even worse.
00:13:48.620 - 00:13:54.624, Speaker C: No, but then it's just MDP, so then you're guaranteed to just. Because now it's just a constraint, so.
00:13:57.684 - 00:14:05.444, Speaker B: It should be easily dependent. You're accumulating stuff like you have to expand the state space. You just made everything even much bigger.
00:14:05.564 - 00:14:14.064, Speaker C: Sure, but it's not going to randomize. So I guess maybe just a philosophical question. Why sort of marks expectations subject to expectation?
00:14:14.584 - 00:14:55.074, Speaker B: Right, okay, so that's a very good question. Why take this formulation as opposed to the million other constraint formulations? I know why it's the simplest one that comes first, and like, it's classical. I'm not saying that this is the best unit to every, everyone's practical problems. It certainly does make sense. Like, on the average, the constraining reward needs to exceed something. It's like a soft constraint, because you are averaging over those constraint rewards as well. Right? Like, it's not even that hard version of it.
00:14:55.074 - 00:15:06.794, Speaker B: You expect that the hard version requires even more prior knowledge. It's going to be harder to solve. So it's like, this is the smallest deviation from mdps you can imagine.
00:15:06.954 - 00:15:10.406, Speaker A: It's like a multi objective MD, which you're making one objective.
00:15:10.450 - 00:15:39.498, Speaker B: That's right. Like, it's like you have two objectives that may be conflicting. And then, like, if you wanted to draw the whole parietal front, then you could sweep through b and like try to do that. This doesn't quite explain why this formulation, this is one of the formulations. It's just one of the formulations. And then I invite other people to study the others. I don't know, you can only do so much.
00:15:39.546 - 00:15:41.894, Speaker A: You have to start problem session on Thursday.
00:15:42.434 - 00:16:10.074, Speaker B: Yeah, I mean, even which problem formulation to choose, it's like, it's not very clear. So in this case, it's just like shooting in the dark. Okay. Anyways, excellent questions. Just not going to answer that. All right, so this is the sample complexity of planning. So this is the city toy model.
00:16:10.074 - 00:17:41.150, Speaker B: You have the full state action space given to you, and it's all enumerated, and you can choose any state action, and you will get a transition from the underlying kernel, probably the kernel, you see the next state randomly sampled from that, and then you ask, if a planner needs to solve one of these problems, and then I'm going to have two versions of these problems, then how many samples are we going to need? And, yeah, this is the. So the generative model is hugely unrealistic, and we've worked really hard on the MDP setting to get away from it, and there is a lot of literature on it. But if you can't do something on the generative setting, or you find that something is already hard in the generative setting, it's going to be even harder under the more general settings. So it's kind of like just a simple starting point. So what are the two settings that we're going to consider? So, in one setting, I allow a constraint violation of size epsilon, and in the other setting, no constraint violation is allowed. Maybe the justification for that is that, like, come on. Like, if you set your mind on having a constraint, then, like, just keep it, like, why change it, right? Like, otherwise you can just, like, shift it up.
00:17:41.150 - 00:19:08.858, Speaker B: I mean, there comes the question of, like, if you shift it up by epsilon, then maybe you're exceeding the best possible reward that you could get. And, like, you will get in trouble. So when you are in this small Slater gap, and then the epsilon is large compared to the Slater gap, then it's kind of like an interesting scenario there, right? Then it will be interesting to see how things interact if you have this strict constraint. All right, so is everything clear? All right, so what do we know about this problem setting for MVP's? There's been a long line of research, starting with this paper from 1999. And then Mohammed Azhar and wonderful coworkers had some paper in 2013, and then other papers came along and everything got refined. And now we have matching low and upper bounds. Finally, after, I don't know, 22 years of research on tabular MVP's, in the generative setting, matching lower and upper bounds, the sample complexity of planning is scaling linearly with the number of states, the actions it scales cubically with the horizon.
00:19:08.858 - 00:19:52.282, Speaker B: And as usual in statistical learning, it scales invariably with the square of the desired precision. So that part everyone immediately understands. Maybe this one, like the lower bond, it's easy to understand. At every state action pair, you need to gain some information. The question is whether in the upper bond, why you're not seeing an S squared squared a would be the size of the transition probability matrix. And we're getting away, um, with linear dependence on s. So that, that's a, that's a much more, uh, dedicated question, and we're going to talk about that a little bit.
00:19:52.282 - 00:20:39.924, Speaker B: So that's not trivial. To get. Uh, the cubic dependence on, on the effective horizon just comes from studying the sensitivity of the value function to the transition structure. You write some silly example, you take some derivatives and you see these cubic dependence popping out. So it's like a sensitive, if you don't know some transitions and you jitter them a little, the way you can jump around cubicly with the effective horizon. So the precision that you're going to need is appropriately needs to be tightened to deal with this scaling up. So an MDP is of course a special case of a constraint mvp.
00:20:39.924 - 00:21:36.744, Speaker B: So immediately get the lower bond that the CMDP learning problem is at least as hard as the MDP learning problem. And the question is that, is it any harder or is it maybe the same? And so we are not the first one to ask this question. There's been a lot of algorithms designed, and then people were getting better and better results. And the state of the art before, or paper was something like this, there's like 20 of different papers, but this is kind of like taking the price at the moment, or we're taking the price at the moment. So this paper allows constraint violation and gets an upper bond that scales quadratically with the number of states. Otherwise the scaling looks fine. Okay, so that's maybe not idea.
00:21:36.744 - 00:23:05.646, Speaker B: So another paper allowing constraint violation gets the linear scaling with number of states, but scales with the fifth part of the horizon. So what's going on then? There is a paper by Bori at Orell, and they were updating and updating, and in the most recent version of the paper, if you rewrite it with notation and normalization, what you get is that you get linear scaling with the number of states and number of actions, but you get a fourth power scaling with the effective horizon and the slaughter constraint, or Slater, I don't know. Slater constraint appears in this fashion. And this is for the slick constraint setting, and for the non strict constraint setting, I think that they also have a result where also the dependence on the horizon is suboptimal. Okay, so this is the state of the art. So what's going on? Is this really necessary that we have this deviation from the mdps or not? And so first of all, where is this dependence on the slope? Latter Slater constraint comes in. So we can consider a very, very even simpler problem than what we looked at before.
00:23:05.646 - 00:23:35.774, Speaker B: So just Bennett, you have a single state. I love Bennett. And the task reward is either one or zero. If you're taking one of the actions, first action, you get a task reward of one. Otherwise you get task reward of zero. So obviously the first action is what you want to take as often as you can. But there is a constraining reward, and the constraining reward goes the other way.
00:23:35.774 - 00:24:13.464, Speaker B: It's like you get less reward if you're taking the first action, and actually you get less reward than the constraint. The threshold that you have is b. So if you would always just take this action, you wouldn't satisfy the constraint. So you have to balance between taking this action and the other action. And this is nicely set up in a symmetric fashion, just for the sake of keeping the example very simple, such that to meet the constraint, you have to randomize with equal probability between the two choices. Then you're just meeting the constraint. And of course that's maximize this the task reward as well.
00:24:13.464 - 00:24:27.994, Speaker B: So you get back to the same thing, same state every time step. So every round is the same as all the previous rounds. So it doesn't matter whether you're discounting or not. It's just a bandit.
00:24:28.074 - 00:24:28.934, Speaker A: Totally.
00:24:31.954 - 00:24:51.906, Speaker B: Yeah. Okay, so let's say gamma is zero in this case. So that, yeah, I guess it's just like, I just want to do bandits. I, my heart is still with the bandits. Yeah. Good catch. So here we set gamma to zero as the simplest explanation, because otherwise.
00:24:51.906 - 00:25:58.630, Speaker B: Yeah, like the b should be scared, and everything should be scared, but everything. All right. And so now the question is, so what happens when you're, when you're getting noisy information, when you need to interact with this generative model, you're going to have estimation errors. And so let's imagine that in this case, you are estimating this constraint reward, and you have an error of u, and if you have an error of you, and you're just like planning as if that was a true reward, we can, like, you know, just to gain some intuition, we can look at how the value, how the policy is going to change. So let's imagine that instead of c one, which is b minus x, the reward that we are working is c one plus u, which is b minus x plus u. So if u is positive, then c one is a little bit higher. If u is negative, then c one is a little bit lower.
00:25:58.630 - 00:26:52.864, Speaker B: If c one is a little bit higher, then that means that we can put a little bit more probability on this action. We're going to be happier if u is negative, then we have to put a little bit less probability on this action. And we are going to be less happy about the task reward, and ultimately you can work out what, what the optimal solution is, and you will find that the optimal solution is roughly one half plus u. So that's the error in estimating the constraining reward divided by four x. And so the sensitivity to the error is inversely proportional to x. And what is x here? Well, x is just the room that you have between your constraint and the maximum possible value that you could get. So that's exactly the slatter constraint.
00:26:52.864 - 00:27:54.866, Speaker B: So you see that because you have this constraint, if you're misestimating things in the constraint, you're going to pay in proportion to the room that you had in satisfying the constraint, which is invariably proportional to that. So the Slatter constraint, or Slater constraint, is something that we expect to see in the bonds if you need to keep the constraint. This explains the presence of the Slatter constraint, or Slater constraint. I will never learn it. And lo and behold, you can refine this argument. You can come up with some more or less standard construction. So take the MDP lower bound construction, modify it a little bit so that you are mostly playing with the constraint rewards, and you can prove that no argument can get away with less than s eight times.
00:27:54.866 - 00:28:49.324, Speaker B: And you see that the horizon blows up to the fifth bar, and otherwise you have the usual terms, except that the Slater constraint appears invariably as kind of expected from given the previous slide. All right, and the explanation for all of this is that. So the sensitivity to misestimating the constraint leads to that you need to estimate the constant reward rated quantities up to a higher accuracy. That's just like this. And then that's the source for the extra factor of two in the effective horizon, because this one, minus gamma is just like the inverse of the horizon, and also the source of the Slater constraint appearing in a bound.
00:28:51.524 - 00:28:58.308, Speaker A: So that's straight constraints. You do not allow the violation here.
00:28:58.356 - 00:29:48.330, Speaker B: Yeah, here, there is no constraint violation alone. If constraint violation is allowed, then we're still hoping that maybe it's not worse than MDP. So the first thing that we were doing is that we wanted to understand the appearance of this later constraint and how exactly things should be behaving on the horizon. But you sort of like conjecture that this is going to be important only when you have strict constraints. So you talked about some of the earlier papers. They had age to the four and various things. So they're wrong or they're different, or, no, everything is fine, because the first two papers, they allowed constraint violation, so they're not.
00:29:48.330 - 00:30:31.194, Speaker B: Yeah, like, this was not the case when we found some box in some papers. Good point. All right. Okay, so I think that. Okay, I see that I messed up that that's probably eight to the six. Yeah, I think I did mess up there, because what happened is that they started to normalize things such that the values are in the zero one range, and then that rescues everything. Yeah, that's an extra h squared or something.
00:30:31.194 - 00:30:54.584, Speaker B: And so if you add that h squared and you get the h to the six. Yeah, yeah, that's correct. Like, I need to correct my slides. H to the six. All right. All right. So we see that that paper almost got it right, except that there is an extra age factor.
00:30:54.584 - 00:31:23.844, Speaker B: Okay. So the next task is to get some matching upper bonds and or good old friend is just a plugin approach. Right. So what works in MDP should work in CMDP. What works in MDP is that at each state action pair, you get the, the same number of samples. There's no expiration issue. You just get like some uniformly good information at all the state action pairs.
00:31:23.844 - 00:31:59.566, Speaker B: And then you just compute the empirical, an empirical estimate of the transition. Oh, what is that? Okay. Just basically counting transitions and. And you solve the resulting MDP. And if you do that, then after 20 years of work, you can prove that you can match the tor bond. So we saw that maybe the same thing is going to. Like, it would be beautiful, wouldn't be, this would be the simplest approach if it worked.
00:31:59.566 - 00:32:38.292, Speaker B: And this is the first thing we tried and we failed at it. Like, we can't prove that this works for a number of reasons. And I'm very curious whether other people will be able to prove that this works, or maybe there is some reason this shouldn't work. So what am I talking about? What is the plugin solver? Any method that solves the empirical MDP up to an epsilon accuracy. So take your favorite IP solver or whatever, and you read all this policy, and then you would need to prove that it didn't matter how you arrived at this policy. It's epsilon optimal. You should be good.
00:32:38.292 - 00:34:38.722, Speaker B: And there is a strong feeling that this should be true, that if the plugin method doesn't get it, why would any specific method get it? But we will see later a little spoiler alert that we do have some other methods for which we can get some nice positive results that we couldn't prove for this simple plugin approach. So what are the problems? So, one problem is that, okay, if you just solve without thinking much, then the policy that you read out might be infeasible. So you have to set up things in such a way that the constraint threshold is modified a little bit, maybe increase it by an epsilon, or you increase the precision a little bit. So to make sure that the empirically Optima policy, any Epsilon Optima policy that solves the empirical constraint MDP, is going to be feasible for the original MDPs fund that you can take care of. But then the other problem is that normally what we can do is that we can control the deviation between the value functions of for a fixed policy, or we can also do that, control the deviation between the optimal value function between the empirical MDP and the true MDP. But here what we have is that we have a random policy that we read out, and then we need to control, well, okay, there should be a hat over there, the deviation between the value of it in the empirical MDP versus the true MDP. And of course, people like this is the essence of the problem in the MDP setting as well.
00:34:38.722 - 00:35:40.184, Speaker B: So people have been quite busy trying to figure out how to do these things. But usually what people end up doing is that you use some sort of balmond equation, rely on optimal value functions and so forth. But if you solve this, let's say with an LP, then, okay, like you do have bam on equations for the individual policies, but not for the optima. Like the optimal value function per se doesn't, it's not the thing anymore, right? Because it's like you just have this policy that is not optimal for the task reward. It's not optimal maybe for anything. You just got it. And like, okay, what are you going to do with it? You could try to apply union bonds over all the policies, but that's what leads to this suboptimal scaling with the number of states, for example, or other quantities.
00:35:40.184 - 00:36:45.814, Speaker B: So long story short, can't make this work. So then we had to go back to the drawing board, and we had to rethink our strategy. And after this much failure, we realized that the only way we will be able to control the terms that we need to control is if we are relying on MDP, like solving mdps. And so now how do you reduce solving a CMDP to solving maybe not one MDP, but a series of mdps? We have seen this in previous talks, similar things. You do the Langrantian and you have maximum formulation. And if you solve the maximum formulation in an iterative fashion, where you're playing a best response to some Langrantian that you're updating with, I don't know, with gradient descent. Uh, then you come up with this iterative method.
00:36:45.814 - 00:37:24.222, Speaker B: Um, and the nice thing that happens here is that the language yen is combining the value functions, uh, under the task reward and the constraining reward of a policy. And this is linear in the rewards, right? So you can just write this as the value function underlying the linearly combined rewards. So you can push everything into the index there. And this is just solving an MDP now, right? So in every step we need to solve an MDP. We are not happy about that. That, like, we don't care, but we can solve LP's. That's not a problem.
00:37:24.222 - 00:38:03.704, Speaker B: It's not numerical optimization issue. It's a statistical issue. We are happy about solving MVP's because we know how to control the statistical error of getting right, the solution of mdps, the sample complexity. We have very refined tools for dealing with that. And so those are the tools that we want to use to control these deviations now. But we have a number of other issues that we have to take care. We have a bunch of these policies, and I mean, like, there are some stability issues and so on, so forth.
00:38:03.704 - 00:38:28.118, Speaker B: It turns out not in PI, but you don't really need that. You don't need it. You can always go to the occupancy measures if you really cared about that. But you don't need it.
00:38:28.206 - 00:38:30.394, Speaker A: Like the best lambda may not need.
00:38:33.174 - 00:39:06.584, Speaker B: There is a term that says that it does need. And the way to imagine that is that the occupancy measures a corresponding one to one with the policies. So the policies, just an indexing into the occupancy space. And in occupancy space, you have nice complexity and you have zero duality. Get, therefore you have zero duality gap e as fast. So it's just like the parameterization is just inconvenient parameterization of the optimization problem. But it's, it looks like as if you didn't have it, but you do have it.
00:39:06.584 - 00:39:14.804, Speaker B: No, it doesn't mean that.
00:39:14.964 - 00:39:19.144, Speaker C: So for any fixed lambda, there is a non randomized optimum.
00:39:22.954 - 00:39:25.442, Speaker B: No, no. So you can't flip the mean and.
00:39:25.458 - 00:39:27.334, Speaker A: The max without making it.
00:39:28.154 - 00:39:38.934, Speaker B: Yeah, yeah. So if you, if you flip the mean and max, then the max will be over. Mixture policies. Know the mixture policies. You're good, right? Like, that's the convex space.
00:39:39.274 - 00:39:40.810, Speaker C: Even if I give you the optimal.
00:39:40.842 - 00:39:52.274, Speaker B: Number strand, you'll see you need to randomize. Yeah. The example that I showed shows that without randomization, you can't get away.
00:39:53.614 - 00:40:00.814, Speaker C: Technically, you showed one optimal for this. You're saying that also shows that rules out any non randomized optimal for this.
00:40:00.894 - 00:40:12.714, Speaker A: This rules out. Right. To exchange, like to close the gap, you need to make it. Mixture. Yeah, to make the world the whole. You're going to that space.
00:40:13.864 - 00:40:42.760, Speaker B: So it may be confusing what I'm saying. So that there is no duality gap doesn't mean that you can flip the mean and Max and you get the same value here. That's not true. The best response could very well be a deterministic policy, which is not optional. Maybe a related question is just, is the final thing you output from this algorithm. It's not going to be like PI hat big t, right? It's going to be some mixture. It's just a mixture.
00:40:42.760 - 00:40:59.330, Speaker B: Yeah, there. Oh, not there. There now. Oh, okay. Yeah, yeah, there. Yeah. It's like take the average of all of these policies, and that's a mixture policy that you output.
00:40:59.330 - 00:41:29.674, Speaker B: So we effectively like searching the space of mixture policies, and you can find a mixture with not too many components. That's absolute. Does it make sense? So in particular, last iterate convergence. I mean, you can't just. This rules out there. Absolutely. The last three can very well be deterministic.
00:41:29.674 - 00:41:59.354, Speaker B: So there's no way you can do anything like that. You could try to come up with stochastic policies along the way and maybe you could have last three conversions there, but that's different story. We were absolutely not doing that. This is just plain best response. These policies could be deterioristic, actually. We want them to be deteriorating. This makes our life easier.
00:41:59.354 - 00:42:35.692, Speaker B: All right. Okay. All right. So some sanity check. The algorithm actually solves the CMDP, right? So that's the first thing. You don't need too many iterates to solve the CMBP. So if you want an epsilon optimized solution with some relaxation of the constraint, I mean, like, you can increase this by the essence, and you have the constraint exactly matched, then you don't need too many iterates.
00:42:35.692 - 00:43:24.020, Speaker B: And then you can solve this algorithm, the CMBP, the empirical CM. So this doesn't tell us that we are solving the original MVP up to any accuracy. It just asks you that this optimization algorithm is not silly. We just wanted to have some way of solving the CMDP where we can talk about the policies that we came up with along the way. Underlie some MDP with some reward function. There are optimal policies in some mdps with some reward functions, yes. So in the projection, you need to.
00:43:24.020 - 00:44:08.556, Speaker B: So there are two things here, here. And there are kind of fine details and I hope to just conveniently not to talk about them, but since you're asking. So you need to estimate the larger value the Slater constraint should take. And I think that that will be something with the effective horizon. And you can prove that divided by the Slater constraint. I think no, to estimate the Slater constraint under feasibility, it's not really a problem. You can have a doubling algorithm that makes a gas, and then, like the gas is the largest litter constraint possible.
00:44:08.556 - 00:44:45.838, Speaker B: That's the effective horizon. You half it, you half it, you half it. And then you keep checking whether, by just solving the MDP for the constraining reward, whether that Slater constraint guess was correct or not. And this is going to stop as soon as you are below the actual slit constraint by a factor of two. So you can do that effectively. So, yeah, like eventually in this algorithm, you need to choose a bunch of parameters like the ETO and I didn't talk about this. Later on, maybe we can return to that.
00:44:45.838 - 00:45:23.484, Speaker B: We are going to do some running as well of these langrantian multipliers to a grid to prevent some nasty things happening. Basically, we don't want too many unit bonds. And so that's like put an at on the reasonable range of the long range and multipliers. That's fine enough. Do the rounding. You're not going to introduce errors that are too big. So you might have said this already, but in the lower bound setting you had a strict constraint, but here it's non strict.
00:45:23.484 - 00:45:58.144, Speaker B: Or am I missing something? It's both. We're going to. At the end of the day, the same algorithm can be used to solve both settings. I didn't tell you, but here we have b prime instead of b. So to satisfy the strict constraint, we're going to take b prime slightly bigger md maybe. Then why can't you do the same thing for the lower bone and get the. Well, that was the example is the sensitivity, right? Like if the Slatter constraint is pretty small, then.
00:45:58.144 - 00:46:27.384, Speaker B: All right, this clip. So. Exactly. Let's keep asking this question. I have no idea. I hate it. I love it.
00:46:27.384 - 00:47:10.858, Speaker B: This reminds me of something I did long ago. Okay. Also, constraint optimization. What we did was we just took the x one, found the optimal dual. So what? Exactly. So just try to directly find optimal lambda. Well, we don't have the machinery to control the value adder and the primary vod unless we are solving mvp's.
00:47:10.986 - 00:47:25.414, Speaker A: So the solving, finding, solving that problem for best lambda is not that easy because the way even the max min is holding is by expanding the space to all possible policies. Like, it's not a linear program.
00:47:26.174 - 00:48:32.952, Speaker B: I mean, this is not a computational issue at all. Like, we're just making our life much harder by coming up with an iterative formulation, rather than just saying that solve is lp with whatever way you get some empirically optimal language and you prove some stability results. You can do that. We were trying to do that, but at the end of the day, we need to control the statistical accuracy of the policy that we lay out. And then trying to do that, you suffer all these extra factors of, like, an extra s factor, an extra h vector if you're not careful about these things. And we were not able to do things like, okay, so you will see some slide where I will uncover the magic of using mdps, and, like, that will explain to a large degree, why choose this way? So, this. This algorithm is only motivated because we can analyze it and get optimal results.
00:48:32.952 - 00:48:53.904, Speaker B: There is one thing is, if you only care about your. Yes, you still do that, right? You still do the Lagrangian, the combinations, all the. Okay, but you would not do the lambda through this grid in this end. Okay, so you could just, like, solve for every value of the grid and like, yeah, I think that that works fine. I think that that. Yeah.
00:48:56.684 - 00:49:00.784, Speaker C: If there's any monotonicity, you could be able to do bisection.
00:49:03.924 - 00:49:05.268, Speaker B: The lambda is one dimensional.
00:49:05.316 - 00:49:06.824, Speaker C: Lambda is just one dimensional.
00:49:13.124 - 00:49:50.284, Speaker B: Yeah, probably. Probably something like that can be made if you're briefly thinking about that. I don't know, choose it anyways. So the benefit is that we can just, like, think about this issue. So we're solving mdps with some reward, where the language multiplier comes from a grid. And so we have this finitely many choices for the reward function. We are just owing all those empirical mdps, and you can feel that that union bond is not a problem.
00:49:50.284 - 00:50:46.486, Speaker B: And then we want to argue about these deviations and notice what's, again, like, something different happening than in the MDP case. We're solving an MDP for a reward that includes the constraining reward, and then we are looking at the value functions, not for that, but the reward. Without the languangian, the language is set to zero. So we have that gap that we also need to cover. Otherwise, you could just, like, use MDP techniques and you would be done. And. Okay, so in, like, the general look at this is that you have some alpha rewards and you have some beta rewards, and you are solving the optimal policy standard of rewards.
00:50:46.486 - 00:51:34.504, Speaker B: And then you want to argue about value functions and the beta rewards. And can you do this? And long story short. Well, I have. I don't know. I think three minutes is hopefully going to be enough that there is all this wonderful work that was done in the MDP literature, and you can kind of adapt it. So the first work is by Chan Li and others from Europe's 2020. They have this theorem that says that this deviation that is of interest for us can be controlled for a data dependent policy, arbitrary data dependent policy, as long as some benchtain type of inequality holds.
00:51:34.504 - 00:52:26.744, Speaker B: Here, n is the number of samples. So it's like you have a variance term and you have a faster term, and there are some value functions which are defined in some complicated fashion. Don't bother about that. The important thing about this, though, is that these benchtain type of inequalities need to hold for these value functions that depend on the policy, which is chosen in a data dependent way. So then the next question is, okay, can we get this benchtain type of inequalities? And if you have a fixed value function v, then you have a standard benchling type of inequality of this type. So these are close relatives. So one hopes that somehow it should be possible to get this type of inequality from the standard bashing type of inequality for mdps.
00:52:26.744 - 00:53:21.410, Speaker B: And in fact, these MDP papers kind of worked out pretty much all the tools that we need to do this. So we want to control this, and we want to use the previous lemma with this empirically optimal policy for the offer rewards. And this is the quantity that we need to control in the bench line bond. So we have the dependence on a policy in two places. Here, the value function depends on the policy, and also the kernel depends on the policy. The dependence in the kernel from the policy can be removed by considering instead of all state action pairs, and then proving a uniform bond that only requires a union bond over the number of state actions. That's cheap.
00:53:21.410 - 00:53:46.320, Speaker B: So we can do that. So now we are left with controlling this term. And so that's very nice, because previously we had this. And so the only difference to this standard bash stain bond is that the value function that we have depends on the random policies. It's random itself. So we have to be careful with how to deal with that. The question is how to deal with that.
00:53:46.320 - 00:54:25.768, Speaker B: Well, there was this construction by Agarwal and coworkers from 2020. Sham and Lin were on this paper, but they introduced this beautiful technique of absorbing mdps. So the observation is that you want to control this deviation at this state action pair sa. And this value function depends on the whole of data. If they were somehow decoupled, everything would be nice. So this is data dependent. This is data dependent.
00:54:25.768 - 00:55:12.028, Speaker B: This depends only at the data state action pair sa. This depends on all the data. If we change somehow the MDP in such a way that this guy is not going to depend on the data state action per s a, then we achieved independence, because otherwise the data is independent that you obtain the different state action pairs. So it's a decoupling technique that they introduced through this observing MDP construction. It's basically modifies the MDP so that at the s is, something else happens. You go to a special state, you stay there, you get a special reward there. It's called u.
00:55:12.028 - 00:56:11.228, Speaker B: And if you set that special reward in some special way, you get exactly the value function that you want it to have if you're, if you care about optimal value functions. So that was their construction. And so they put on an epsilon net on these values, and all is fine for them because everything works in a nice and Lipschitz way. But here, the problem is that you could put an epsilon net on the possible values for this reward that it could take. But the problem is that you're not directly controlling, with this observing MDP construction, the value there, because you solve for the optimal policy for that observing MDP. And so that policy changes potentially in some other way. So that arises the need to show that, well, this is just not happening.
00:56:11.228 - 00:57:14.048, Speaker B: So how could this not be happening? If you are like, you just want to show that you can have a fine enough grid such that if from the grid you're choosing the value of the reward in some specific way, and then you're optimizing for the policy in the observing MDP, then what you get from there for one of the values on the grid is going to be exactly the policy that you started with. The way to arrange for this is to, well, visual thinking. Assume that there are big action gaps. If there are big action gaps between the action value of the optimal action action versus the action value of all the suboptimal actions, then small perturbations of the reward are not going to change the identity of the optimal policy. You can kind of like feel that that must be true. And so how do you arrange for action gaps? You don't get action gaps for free. Well, except that you do.
00:57:14.048 - 00:58:19.976, Speaker B: If you add random noise to the rewards, then you can show that that introduces this controlled probability action gaps of a controlled size, just enough so that you can arrange for a fine enough grid which still have polynomial many values such that everything works out. So this technique has been worked out in the paper of Chen liquid in this 2020 New Year's paper. And so that's how they had the result for the random policies. And we can just adapt this technique to our setting. So what we need to do then is to add random noise to the rewards. Set the magnitude of this random noise, not too large, not too big. But if you set the noise magnitude too large, then of course you're not solving the original MDP.
00:58:19.976 - 00:58:58.824, Speaker B: So the noise magnitude cannot be too large. It has to be related to the precision with which you want to solve the MDP. So it's constrained by the precision, so that constraint is the size of the gap, but the size of the gap is not going to be much smaller than that. So it's still scarce with epsilon. And so the net is going to have one over epsilon points on it. So all fine, you suffer logarithmic increase in sample complexity, but all is fine. And the blow up in terms of the other quantities is the number of states, actions, and the horizon is also not bad.
00:58:58.824 - 00:59:37.232, Speaker B: So this is what we need to do. The argument needs to be changed such that in the first step, you just add small random perturbations to the reward. Kind of mysterious. So for this we can prove otherwise. The algorithm is the same, you output the same mixture policy, and for this you can prove these two guarantees. The first guarantee is for the relaxed setting where you allow constraint violations. You get optimized sample complexity.
00:59:37.232 - 01:00:32.074, Speaker B: You also get the optimized sample complexity for the strict setting. So that's the end. All right, so big open question. Does the plugin based approach work? Is there a simpler proof for that? The CMDPs basically fix constraint variation, have the same sample complexity as normal mdps. If, on the other hand, no constraint violation is permitted, then there is a sharp increase in sample complexity. So to me, the most interesting question is like, why does it have to be so complicated? There should be some simpler proof, but we are completely stuck there. And then there are lots of other interesting questions, like how to extend this to online setting, function approximation, and so forth.
01:00:46.594 - 01:00:54.574, Speaker C: In a special case, suppose for my constraint, all the costs have the same size. They were all non negative.
01:01:02.454 - 01:01:43.554, Speaker B: They are one sided. As far as, I mean, like everything can be shifted from this perspective. It doesn't matter what range you're in. It's like the constraint is going to matter. Doesn't matter. You can shape the rewards, everything relative to the optimum, right? It's like it's easy. Yeah, you must be thinking about stochastic shortest path or something.
01:01:43.674 - 01:01:44.374, Speaker C: Yeah.
01:01:47.234 - 01:02:12.444, Speaker B: No, yeah. Some other settings, it's gonna matter. Not in this one. That doesn't matter either. Right? Let's thank Java.
