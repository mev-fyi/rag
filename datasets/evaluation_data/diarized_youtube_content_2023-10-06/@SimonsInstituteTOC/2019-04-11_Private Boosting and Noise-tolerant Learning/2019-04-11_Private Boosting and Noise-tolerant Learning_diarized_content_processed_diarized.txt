00:00:01.480 - 00:00:12.554, Speaker A: Okay, so our next session is about private learning. The first speaker is Mark Ban, who tells us about private boosting and noise development. He's a fellow here and after all, going to be here.
00:00:13.894 - 00:01:04.134, Speaker B: Thanks for the introduction, Ori. Thanks to everybody for coming. I'll be talking today about some work done in collaboration with Marco Carmacino and Jess Sorrell from UC San Diego, which actually started sort of at the tail end of last semester, Simon's program on lower bounds and complexity theory. And maybe we'll see the connection between the topic of this talk boosting and complexity theory a bit later in the talk. So I'm going to start out with a disclaimer. Oh boy, my slides are missing a bunch of content. There we go.
00:01:04.134 - 00:01:45.378, Speaker B: So I'll start this talk with a disclaimer, which is that this is still a work in progress. So I'm going to try to restrict myself to telling you things that I think are true. But as we all know, differential privacy can be a bit subtle. So if anybody during the talk identify something that they see as a serious blunder or something, please feel free to call me out on it. Um, all right, so the topic of this talk is going to be about boosting, which should be a familiar topic to many people in the audience. Uh, the setup is as follows. We have some data domain x and we have some unknown Boolean function over this data domain, which we'll call f.
00:01:45.378 - 00:03:02.004, Speaker B: And we have some unknown distribution d over, uh, this, uh, this data domain x. So a strong learning algorithm is going to be given a sequence of labeled examples, which are iid draws from this distribution d together with the label determined by this Boolean function f. And the goal of the strong learner is to produce a hypothesis which is itself a Boolean function over the domain x and which hopefully agrees with the target function f and generalizes to the underlying distribution in the sense that with high probability over a fresh sample from the underlying distribution, it agrees with the target function f. Okay, so boosting, uh, is a technique which relates strong learning to a different notion called weak learning, which is like essentially the same thing. It's the same setup where you have IId draws from a distribution and your goal is to produce hypothesis. But this time the accuracy guarantee on the hypothesis is a lot weaker. So instead of, um, agreeing with the target function f with high probability very close to one over fresh examples, you only need to agree with it, uh, with slightly better, um, probability than just random guessing.
00:03:02.004 - 00:03:55.174, Speaker B: So like one half plus c for some small parameter c. Think of it as like inverse polynomial or something. Okay, so again, boosting is a technique for relating these two notions of learning. And in particular it's a technique for converting these weak learners which do slightly better than random guessing into strong learners. All right, so uh, give you a brief overview of how uh, number boosting algorithms in a literature work. Uh, so this is sort of the template that, um, a lot of important algorithms, in particular the uh, famous adaboost algorithm of Freund and Shapiri operates. Um, so again, uh, the input to what will eventually be a strong learner is a labeled sample size, um, and it's given uh, access, or like oracle access you can think of uh, to a weak learner which has some advantage c over random guessing.
00:03:55.174 - 00:05:34.824, Speaker B: Um, so the strong learner is going to operate by um, repeatedly calling the weak learner on, uh, the sample s, but with uh, different weights, um, where the weights on the sample s are going to be chosen adaptively depending on the performance of the weak learner in previous iterations. So uh, this boosted algorithm is going to start by initializing the uniform distribution over samples. It's going to feed the sample weighted by the uniform distribution. So basically just the sample itself, the weak learner produce a hypothesis, use the hypothesis to sort of focus attention on the parts of the sample s where the first hypothesis h one was doing badly, and then feed this new distribution over samples into the weak learner, again produce a hypothesis and so on and so forth, producing this long sequence, h one through ht of hypotheses. Okay, so the way that these intermediate distributions over the sample are determined is according to this important multiplicative weights update rule, which again is going to place mass proportional to like exponential in like sort of cumulatively how badly the previous sequence of hypotheses was doing on a sample. All right, um, and uh, at the end of the day, once you've done t equals roughly log one over alpha over c squared iterations, you can just output sort of like the majority vote, uh, of the hypotheses that you fed into this. Okay, and then this is going to produce a strong learner which has uh, error at most alpha with respect to the original sample s.
00:05:34.824 - 00:06:39.500, Speaker B: Okay, any questions? All right, um, so let's see a nice example of boosting an action. So I'm going to give the example of learning a large margin half space, which um, is very closely related to the notion of um, um, learning functions with low margin complexity that vitaly introduced before the break. Um, and this particular way of using boosting to learn a large margin half space, uh, is due to servideo from 20 years ago. Um, like almost all of my research projects, they begin by looking at some paper Rocco wrote 20 years ago and like doing some incremental thing to it. Um, so uh, in this problem we're given uh, a data domain x, which is just the unit l two ball in rd for some d. Uh, the target function uh, is going to be uh, a half space over this domain. So it's the sign of the inner product of some unit vector w.
00:06:39.500 - 00:08:08.530, Speaker B: With um, the example x, we're going to make the following large margin assumption on uh, the pair f comma d, where d is the uh, distribution over. Examples where we uh, enforce the condition that um, for every point x in the support of the distribution d, the inner product between w and x is at least gamma. So that means, you know, every point in the distribution is going to be at least distance gamma away from the separating hyperplane. Okay? So servideo constructed a very nice, simple, clean weak learner for half space, which works as follows. So the input to the weak learner is going to be now a pair consisting of labeled ex, labeled sample s together with weights for how much attention the weak learner should be placing on each of the samples. Okay? And what the weak learner is going to do is it's going to produce a, you know, the thing that goes into the sine function when defining half space. So the inner product between a unit vector and domain item x where this uh, unit vector w is obtained uh, by taking a weighted average of um, uh, the label times the example for each uh, point in the sample, okay, where the weights are chosen according to uh, um, the weights passed into the weak learner.
00:08:08.530 - 00:08:56.141, Speaker B: So um, one thing to note about this is that uh, it doesn't quite follow the syntax of weak learning that uh, I introduced in the first slide. Um, don't worry about it. Like uh, uh, these frameworks are like fairly adaptable and um, defining a weak learner in terms of a real valued predictor is like useful for um, downstream analyses. So we'll just do it this way. Um. Okay, so one way to think about what this weak learner is doing is it's uh, looking at the points in the labeled sample and for all of the points labeled minus, it's um, reflecting them to be uh, uh, you know, the negative of that point and then assigning them the positive label. And then it just computes sort of like the center of attention of this like cloud of positive points.
00:08:56.141 - 00:09:51.464, Speaker B: And this should ideally have some correlation with the underlying, um, vector w that you're trying to predict. All right? Okay, so, uh, here's what, uh, Rocco showed about, um, this weak learner for half space. So if, uh, the labeled sample, s indeed comes from a half space that has margin gamma, then this, um, uh, real valued hypothesis that you produce is going to have advantage at least like gamma over two, or gamma is again the margin. Uh, and this is going to hold with respect to, uh, the sample. All right, so, um, how do you prove such a thing? Uh, it's extremely simple, but probably the hardest part of the proof is defining what it means to have good advantage for a, a real valued learner. So it's going to be the advantage. So like your advantage over random guessing is going to be given by this expression.
00:09:51.464 - 00:10:35.954, Speaker B: Um, so where does this come from? Well, if, uh, h is just, you know, doing no better than random guessing, then it's, then you can take h to be zero on all points. So what you get here is half minus half, which is advantage zero. If h is perfectly predicting y, so h agrees with y on every sample, then these gaps are all zero and you have advantage one half. And then this is just the natural, like, linear interpolation for real valued h. Okay. Um, so to prove this advantage bound, you do some algebra. Um, you can compute, uh, the advantage is half of the l two norm of the thing that goes into w before you normalize it.
00:10:35.954 - 00:11:06.030, Speaker B: Meanwhile, you can get a lower bound on this expression using the margin condition. Again, you do some algebra. Use Cauchy Schwarz somewhere. Use the fact that w has l two norm one, compare these expressions, and then you get advantage gamma over two out of this. Okay, so, pretty simple fits on half a slide. All right. Um, so let's see what you can, uh, do by feeding this weak learner into, uh, city adaboost algorithm.
00:11:06.030 - 00:12:01.584, Speaker B: Um, so again, uh, we know that this weak learner has advantage at least gamma over two. Um, this, uh, theorem of friendship period shows that if you plug this into the adaboost algorithm, um, then after about log one over alpha over gamma squared rounds, you're going to be able to guarantee that the hypothesis is error alpha with respect to the sample s. All right, so getting good error with respect to the sample is nice, but ideally we'd like to have the hypothesis we generate generalized to the underlying distribution. So this requires a little bit more work. And servideo was able to show that if the hypothesis is that the hypothesis actually has error, um, alpha with respect to the underlying distribution, as long as the number of samples is at least about one over alpha gamma squared. Okay. And this, um, basically matches what you can get for this problem of learning a large margin half space using like the perceptron algorithm.
00:12:01.584 - 00:12:51.534, Speaker B: Um, and the proof of this result of this generalization result, uh, goes by way of, uh, uh, a more detailed analysis of the adaboost algorithm, which shows that, uh, the thing in the sign here, the thing, so h itself defines a half space. Um, uh, this improved analysis of adaboo shows that, uh, this half space itself has large margin with respect to the sample. And then one can use this, you know, beautiful and deep theory about, um, generalization from, uh, classifiers with good margin to show that, uh, once you have the sample complexity, this, uh, hypothesis will generalize. So this, uh, Sir Rocco's theorem here is with respect to like a specific algorithm, right? It's not a general statement for, with respect to the adaboost algorithm in particular. That's correct.
00:12:53.114 - 00:13:04.334, Speaker C: The adaboos produced large margin over the ages. Like if I have new features, which are the ages, then I have large margin in the l one. The infinity margin makes the multiplicative weights work.
00:13:07.714 - 00:13:08.730, Speaker B: Sorry. Identified.
00:13:08.842 - 00:13:14.114, Speaker C: So this h over h is a margin right over the hypothesis.
00:13:16.774 - 00:13:17.358, Speaker A: Let's see.
00:13:17.406 - 00:13:26.262, Speaker C: So hyperplane over different hypotheses. So I take each x and I say what if hypothesis one gives it that will be feature one what hypothesis two gives it, that will be feature two.
00:13:26.278 - 00:13:26.670, Speaker B: Dah, dah, dah.
00:13:26.702 - 00:13:31.094, Speaker C: Then I do a hyperplane over there node over the original features.
00:13:31.214 - 00:13:41.414, Speaker B: Yeah. So this is a, but the hypotheses that you generate are these like real valued inner product things. So when you sum them up, you again get.
00:13:46.514 - 00:13:47.734, Speaker A: The sum of.
00:13:49.754 - 00:13:58.014, Speaker C: Because of its real vectors, these ages. So it's what goes under the, not the Adaboost, it's the servidio weak learner. That makes.
00:13:58.594 - 00:14:37.254, Speaker B: Yes. So it's a combination of an analysis of Ada boost with servidio's weak learner. And this exploits, I think, what you're saying, which is that the underlying hypotheses are these real value things. But I think there are analogous results for adaboose that predated this particular result, which show generalization using margin balance as well. Yeah. Okay. All right, so what do we do in this work? Well, we're interested in doing learning with differential privacy.
00:14:37.254 - 00:15:34.974, Speaker B: And in particular, what we'd like to be able to do is design boosting algorithms that preserve differential privacy in the sense that they can take as inputs a differentially private weak learner and then produce a differentially private strong learner. So this exact problem was considered by Salil Guy and Cynthia. Um, and this is probably like the third most important result from that amazing paper. Um. Uh, and what we do in this work is we revisit, uh, this private boosting algorithm that they provided and, um, give a few tweaks to it to simplify and generalize it. Um, as an application of this boosting framework, we give an algorithm for privately learning large working half spaces. And this recovers basically similar, basically the same sample complexity guarantees as this recent paper of Nguyen, Ullman and Zacintinu.
00:15:34.974 - 00:16:36.960, Speaker B: And finally, at the end of the talk, I'll give a few musings about the relationship between differential privacy and generalization and talk about how differential privacy automatically guarantees generalization in spite of, you know, not necessarily being able to produce these margin bounds. Uh, and this leverages this really nice connection between differential privacy and generalization for adaptive data analysis. All right. Um, so, uh, again, the focus of this talk is going to be on differentially private learning. Um, so this is the, uh, this is going to use the framework introduced by cleaners, um, ten years ago. Um, and again, th this framework should be, uh, pretty familiar, but I'll go through it anyway. So, um, just as in the ordinary strong learning setting, we're given unknown function, unknown distribution iid samples from this.
00:16:36.960 - 00:17:54.202, Speaker B: And, uh, learner is supposed to produce a hypothesis that generalizes to the underlying distribution. Um, so, uh, this paper of cleaners augmented this model with a privacy condition which says, uh, sort of in the worst case, over, um, input samples. So input samples that could be just like arbitrary, labeled, uh, examples not necessarily coming from a distribution or anything, um, the output of this learner should be differentially private with respect to the sample s. Uh, meaning that if I take, uh, this guy Stephen in the data set, um, and replace him with somebody who's, uh, cooler in every respect, the distribution of hypotheses output by the learner l should be statistically close in the sense of differential privacy. So more formally, we can define private pack learning for your favorite notion of differential privacy, meaning for all pairs of neighboring samples s and s prime, you get the epsilon epsilon delta differential privacy condition, or common concentrated differential privacy. Okay. Um, we'll also need a notion of private weak learning.
00:17:54.202 - 00:18:54.716, Speaker B: And I'll be a little bit more precise about the notion of private weak learning that we'll need. Um, so again, uh, our weak learners are going to not necessarily output boolean hypotheses, but they're going to output hypotheses that could be real value and giving you something between minus one and one. Um, and our goal is to design private weak learners which give you advantage at least c for some parameter c. Um, and uh, sort of this more general notion of inputs for uh, weak learners necessitates, uh, a little bit more um, finesse in how one defines differential privacy. So, like the input to uh, a weak learner is now, uh, a sample together with a sequence of weights for each of the examples. Um, so one possible way to do this is to say that, um, you have, uh, uh, for some parameter sigma. If you have, uh, two weighted samples, so say s weighted by some distribution p and s prime weighted by some distribution p prime.
00:18:54.716 - 00:19:35.048, Speaker B: If you compute the histograms of those, um, two samples where the weights of the histograms are determined by the distributions, then those two histograms are close in statistical distance, close by most sigma. Okay. Um, and then the notion of differential privacy is that, uh, uh, the output of the learner when run on these two things are statistically close. And again, you can define this for epsilon, delta or CDP or whatever. Um, notice that this definition collapses to the usual definition of differential privacy. If you take sigma to be one over n and you take p to be the uniform distribution all the time. All right, all right.
00:19:35.048 - 00:20:43.352, Speaker B: So the main question is whether differentially private weak learners can be boosted to differentially private strong learners. And again, we'll try to work up to this result of dork, rothbum and vedant and tell you about our modification to it. So the first attempt that you might try to make in designing a differentially private booster is to just use adaboost. Um, but uh, there's a problem with this. So if you look at this, um, multiplicative weights update method, uh, it's going to be, you know, in every round piling on mass proportional to exponential in, um, whatever your margin of your last hypothesis was. So if you're really unlucky and you're in a situation where every weak learner is doing a terrible job on one particular example, x one, for instance, then you're joining us going to be like piling on exponentially more mass on that example throughout the course of this learning algorithm. Um, and this is bad for privacy because, um, uh, at the end of the day, you're going to end up with a distribution that places like a ton of mass on one point.
00:20:43.352 - 00:21:51.374, Speaker B: So if, you know, you change that one point in the input sample, then you're going to dramatically change what's being fed into the weak learner and you might fail to, uh, guarantee differential privacy. Okay, so, um, aggressive weighting is bad for privacy. Uh, so sort of the key idea for, uh, getting around the situation is to restrict the distributions that, uh, the booster is going to produce over the course of its execution. So in particular, um, be interested in this notion of smooth boosting, which does exactly this, um, uh, notion of um, capping how much mass, uh, the uh, the booster can place on any one example. Um, so formally for some parameter sigma, suggestively named, um, a boosting algorithm, is sigma smooth if the mass that it places on any point I, uh, is at most sigma for any I in, uh, one through. Nice. Okay, so for the complexity theorists in the audience, this is just another way of saying that, uh, the distribution p always has min entropy, at least log one over sigma.
00:21:51.374 - 00:23:20.888, Speaker B: Okay, so, um, this notion of, uh, smooth boosting has been considered, um, in a number of contexts, both in learning theory and outside learning theory. Um, to the best of my knowledge, the first use of smooth boosting came in this completely different context of hardness, amplification and complexity theory. Um, so this goes back to, uh, Impagliazzo's, uh, hardcore lemma from uh, 1995, where he showed that, uh, if you have some boolean function which is sort of like mildly hard to compute, um, on average, by circuits of a given size s, um, then it's actually, uh, very hard to compute by circuits of, uh, slightly smaller size. So in particular, you can find, um, a relatively large hard core of boolean inputs, um, for which, uh, circuits of slightly smaller size can't do much better than random guessing on that small set of inputs. And, uh, the connection to smooth boosting is, um, that uh, sort of the, the bigness of this, uh, hardcore set of inputs corresponds to the smoothness of, uh, a boosting algorithm, um, which t implicitly used to, uh, do this amplification. And this, um, connection between hardness amplification and boosting was made explicit in this beautiful work of cliven cervidial. Um, within learning theory itself, smooth boosting has been used, uh, to design noise tolerant learning algorithms.
00:23:20.888 - 00:24:18.234, Speaker B: So sort of using similar intuition for why smooth boosting is useful for differential privacy. Um, it's also useful for noise tolerant boosting because, uh, intuitively you don't want your boosting algorithm to pile like, arbitrarily large amounts of mass on examples that might have been like, noisy, you know, misclassified, or like, not actually have anything to do with the distribution that you're trying to learn over. Um, it's also been used, uh, to design boosters for agnostic learning and uh, to design algorithms in this extended statistical queries model where you ask sq. Queries, um, but uh, they're allowed to be defined with respect to a distribution of your choosing rather than necessarily the target distribution. Okay, um, so give a quick overview of some of the smooth boosting algorithms available in the literature. Uh, so to do this I want to introduce like a little bit of notation. The first piece of notation is this notion of a, a measure.
00:24:18.234 - 00:25:06.844, Speaker B: And you should think of a measure as a function whose only purpose in life is to be normalized into a distribution. Um, so it's going to be a function from uh, n into zero one, um, which is an unnormalized distribution. You turn it into a distribution by normalizing. Um, so we're going to define the uh, total margin at a point I, based on some sequence of hypotheses that you've produced to be uh, the sum of the individual margins on those points. H is a real value hypothesis. Um, and a number of these algorithms will use uh, termination criterion which says like once your um, measure stops being big, so uh, if it becomes smaller than one over sigma, then you'd better stop. Um, and so they're going to use the invariant that this um, measures at least one over sigma over their execution.
00:25:06.844 - 00:26:29.564, Speaker B: Okay, so um, just uh, to put what we know into this um, notation, uh, the non smooth adaboost algorithm, um, computes its measure by uh, just taking exponential of um, the total margin. And uh, what these smooth boosting algorithms, for instance, this algorithm of impagliazzo do is they restrict these measures, um, to be stuck between zero and one. Um, and also use this uh, invariant defined by the termination criterion that the total mass is at least one over sigma. So because everything is stuck between zero and one, once you normalize by the measure, by the size of the measure, you get something where all of the points have mass at most sigma. Okay? And then servideo gave a different smooth boosting algorithm which uses this different definition of the measure. So another thing you can do beyond these sort of like explicit smooth boosting schemes is what I think of as sort of cheating. You can use this framework of fragment projections where you take this measure m, do the typical ada boost kind of update to it, and then just project it into the space of smooth distributions.
00:26:29.564 - 00:27:51.234, Speaker B: Okay, any questions? All right, so uh, now we're ready to move on to the uh, boosting for people algorithm, which uses these ideas of smooth boosting, um, in order to produce a differentially private booster. So this again goes, uh, comes from the work of dork, Rothblom and badan. Uh, they showed that if you start with, uh, say an epsilon sigma dp, weak learner, where we're going to take Sigma to be one over alpha n. Um, so sort of the intuition for why we want to take Sigma to be one over alpha n is as follows. Um, kind of like the worst, uh, thing for the strong learner that you get at the end of the day is going to be, um, a distribution which is uniform over, uh, a set of like alpha n examples in the sample, um, which corresponds to like a sigma being one over alpha n, smooth distribution. Um, so we'll kind of like work with this fixing of the parameterization throughout the talk. Um, okay, so what they do is, is the following, uh, you know, for t running, um, between one and capital t being like log one over alpha over c squared, you run the weak learner on the sample weighted by your distribution.
00:27:51.234 - 00:28:51.114, Speaker B: Um, you can do like a private check to see if you've done a good job so far. Um, and then you, uh, have to do a reweighting of the distribution example. So what they do to do the reweighting is they compute this differentially, private noisy cap, which you should think of as like the size of the measure they're using. And they use this noisy cap together with something that basically amounts to a Bregman projection in order to guarantee smoothness of the distribution of these. So I guess one thing I want to point out about this is that they do the multiplicative weights update here with respect to the normalized distribution pt that they've, that they're trying to create. And then Bregman project this down into the space of smooth distributions. So, question here, what's the purpose of step two here? Privately checking the error page.
00:28:51.114 - 00:29:15.274, Speaker B: Yeah, you don't really have to do that. It just allows you to do it. Early termination. Oh, it's okay. All right. Okay. So what we do in this work is we give a slight simplification of this that works in a slightly different setting where the weak learner is allowed to have real outputs and it's guaranteed to give real advantage c.
00:29:15.274 - 00:30:08.454, Speaker B: It's like pretty similar. You run the weak learner on the sample weighted by the current distribution, distribution, do some hypothesis, and then do some reweighting. And then for the sake of simplicity, we can just not do any early termination anything, and just output the sign of the sum of the underlying hypotheses. Kind of like the key salient difference I want to point out about our algorithm is that we do the multiplicative weights update on the unnormalized measure MT, which I think saves you from having to do anything with this noisy cap and stuff. And then we Bregman project this unnormalized measure onto the space of some distributions. And this gives you a slight simplification. Okay, so a little bit more about this lazy normalization.
00:30:08.454 - 00:31:20.610, Speaker B: I don't know, it's kind of like this one cool trick that makes machine learning people hate you maybe, but yeah, it just uses the fact that multiplicative weights and boosting converge even when you apply updates to this, like, unprojected, unnormalized measure, and then only like normalize it when you feed it into the weak learner when you need to. And this property has been exploited in several works by Sue Roth, Rothgarden and Ullman. In other contexts where you want to privately solved like two player games and other applications. There, we kind of like use this trick for yet another thing that multiplicative weight solves, which is boosting and sort of the underlying limit. You don't have to read this. That drives the privacy analysis of these things is that if you have like two unnormalized, unprojected measures which disagree in only one coordinate, as long as those measures are, um, uh, not too big, then when you project them onto the space of smooth distributions, you get distributions that are close in statistical distance, which is what you need for privacy. All right.
00:31:20.610 - 00:31:56.534, Speaker B: Um, so here's, uh, the theorem that we think we can prove. Um, so if you start with a weak learner, which is, let's say CDP, so that we get advanced composition, epsilon squared over two, comma sigma, CDP for sigma being one over alpha n, then you get a boosted learner, which guarantees a CDP criterion you want. So, Epsilon squareds add up. It's going to have error alpha with respect to the sample, and all of the intermediate distributions that it generates are going to be sigma smooth. Okay. Yes.
00:32:00.634 - 00:32:02.442, Speaker C: This is the number of iterations that you run.
00:32:02.498 - 00:32:49.464, Speaker B: That's right. It's just counting the number of iterations. And CDP lets you add up the epsilon squared. Okay, so, um, the analysis of this goes by way of, um, analyzing, uh, approximate equilibrium computation. So, you know, you think of pure row strategies for the row player as corresponding to examples in the domain. Pure, um, column strategies are going to correspond to weak hypotheses that the weak learner can produce. Um, and you want to show that these, um, that, uh, smooth boosting corresponds to, like, repeatedly playing this game and converging to an equilibrium, which corresponds to, uh, a strongly accurate hypothesis, together with, um, a small, like, alpha n size set of bad examples on which that hypothesis does poorly.
00:32:49.464 - 00:33:33.954, Speaker B: Um, so this is like, a fairly standard type of analysis. If you want more details, ask Aaron, not me. So, moreover, doing the analysis this way guarantees that if you have real valued weak learners, h with good margin, then the boosted learner that you get has a good margin on almost all inputs. And it seems to be pretty flexible to substituting in different update rules. For instance, in polyyathosis. Okay, um, so that's the general framework. Uh, I want to spend a few minutes telling you about our application to privately learning large margin half spaces.
00:33:33.954 - 00:33:59.044, Speaker B: Um, again, so, so here's the problem. Um, your data domain is the unit ball in RD. You want to learn a half space with margin, at least gamma. Uh, your target error rate for the, um. For the classifier you produce is alpha. And you want to guarantee epsilon differential privacy. Or actually, what we get is epsilon squared over two, CDP.
00:33:59.044 - 00:35:04.093, Speaker B: Okay, so, um, there have been a number of algorithms that consider this problem. Uh, probably the first goes back to, uh, before the definition of differential privacy. Um, using this, like, sublinear queries framework of Blumdork, McSherry and Nassim, which gave, like, sort of a statistical queries implementation of the perceptron algorithm. And if you work through the sample complexity you get for this, it's something like polyd over alpha, epsilon, gamma. Another thing you can do is cast this problem as a convex empirical risk minimization problem using the hinge loss. And then this, like, very nice line of work on doing, um, uh, stochastic, um, gradient descent and convex optimization shows that you get a similar kind of bound. Um, so, a very nice recent work due to hui, John and Lydia, um, gave the first sort of dimension independent bounds for this problem.
00:35:04.093 - 00:36:02.124, Speaker B: Um, they showed that you can get sample complexity which doesn't depend on d. So sort of morally similar to what you can get from the perceptron algorithm in the non private case. And what they do is they use this trick that Vitaly mentioned in his talk, where you first use dimensionality reduction, in particular the Johnson Lindenstrass lemma, to project your d dimensional domain into dimension, roughly one over gamma squared. And then you can use like, uh, techniques for private convex empirical risk minimization to, uh, solve the problem in the lower dimensional space. Um, so in this work, uh, we use the boosting framework, which I mentioned, which I've talked about before, uh, to give basically the same sample complexity guarantees using, like, a completely different method. All right. Okay, so here's how the algorithm works.
00:36:02.124 - 00:36:49.064, Speaker B: We basically adapt Servideo's weak learner in the most obvious way. So again, Servideo's weak learner takes as input some weighted sample computes this normalized centered of attention of the example. And we saw that if you have a large margin half space, then this weak learner is guaranteed advantage, at least gamma over two. Okay, so, um, do the most obvious thing to get a CDP weak learner, which is that, uh, instead of reporting this true center of attention, you add appropriately scaled gaussian noise to it. Okay? And then, uh, if you walk through the calculation, you can show that the advantage of this weak learner decreases by, um, sigma over epsilon. And you get, uh, epsilon squared over two sigma CDP out of this. All right.
00:36:49.064 - 00:37:42.764, Speaker B: Um, so let's see what happens when we plug this weak learner into the boosting framework. So, uh, we have this CDP weak learner that has advantage gamma over two minus sigma over epsilon. Uh, our goal is to learn a half space with margin, gamma up to accuracy, alpha with privacy loss, epsilon total. Um, so, I mean, here's the algorithm, uh, from before. Let's see what sample complexity we get out of this. So again, we're going to take the smoothest parameter, sigma to be one over alpha n. So if you plug this into, uh, the guarantee for the weak learner, um, you'll see that if you want to get advantage, uh, which is proportional to gamma, you should take, uh, sigma over epsilon to be proportional to gamma.
00:37:42.764 - 00:38:24.224, Speaker B: Um, now, Sigma, we said, was one over alpha n. So working this out, you get that n has to be at least about one over alpha. Epsilon gamma. All right, so using advanced composition for CDP, um, the total privacy loss of this algorithm as a whole is going to be, um, something like square root of the number of rounds times epsilon. Um, plug, uh, epsilon total over root log, one over alpha over gamma in for epsilon here. And you get the final sample complexity bound of, um, one over epsilon alpha gamma squared. Okay, so just some algebra.
00:38:24.224 - 00:39:25.514, Speaker B: All right, so, um, there's some cool properties of, uh, this learner for half spaces using boosting. So the first nice property is that, um, it automatically tolerates malicious noise. So, um, servedio showed that, uh, smooth boosting tolerates malicious noise. Um, uh, and in addition, his, you know, weak learner for half spaces, uh, is the same. Um, so, uh, just plugging all this stuff in shows that our algorithm also tolerates the roughly the same level of malicious noise uh, another nice property is that our algorithm is like very simple and computationally efficient even for getting a CDP guarantee at the end of the day. So this other work of Wynn, Ullman and second, Pinocchio showed actually two variants of the result. One which is inefficient for pure differential privacy, and one which is efficient, but only guarantees approximate differential privacy or truncated differential privacy.
00:39:25.514 - 00:40:31.004, Speaker B: Another nice feature is that basically the same algorithm generalizes to this, uh, p norm generalization of the, uh, basis of the margin problem. All right, so, uh, I just want to spend a few minutes talking about generalization, which is kind of like the last piece of the puzzle here. So I showed, um, that boosting gives you an algorithm that gets good empirical error alpha. But I mean, of course we want these to, uh, generalize to the underlying distribution. Um, so again, I want to draw your attention to this part of, say, sir Video's analysis of the boosted learner for large margin half spaces. So sort of the general question for generalization bounds is to identify conditions on the learner h. That will guarantee that if h has low error, say, alpha with respect to, to the sample s, then it has roughly the same error like alpha over two, say, with respect to the underlying population d.
00:40:31.004 - 00:41:43.654, Speaker B: So the first, like, general tool for doing this is using combinatorial properties of the class functions from which h comes from particular can use the Vc dimension. And these results show that if your final hypothesis h comes from some class big h, uh, then it's enough to take the number of samples to be at least the Bc dimension in order to do that generalization. Um, in the case of half spaces, when, uh, the output hypothesis that you produce is like itself, a half space in d dimensions with no other like knowledge about where it comes from. Um, the Vc dimension of this is d, which is unfortunate. Um, so, uh, this work of, uh, servideo got around this, um, to give something better based on, um, margin based generalization. So, uh, if you know instead that, uh, your hypothesis h can be written as the sign of some real valued function where f comes from some class of real valued functions, f. And moreover, uh, all right, this is a typo.
00:41:43.654 - 00:42:34.452, Speaker B: This should be a little f. Moreover, the margin of this hypothesis is good on all points in your input sample. Um, then it suffices to take a number of samples that's at least the fat shattering dimension of, um, this class of real valued functions at scale, um, depending on your margin. All right, so I won't define what the fat shattering dimension is, but, um, for d dimensional half spaces, uh, this comes out to, uh, one over zeta squared if zeta is your margin, lower bound. Okay? And this makes po very happy. Um, so servideo showed that, uh, the h produced by adaboose, when run on his weak learner, produces a hypothesis of margin at least gamma over two. Um, you put all this stuff together and you get that it suffices to take n to be at least one over alpha gamma squared.
00:42:34.452 - 00:43:40.484, Speaker B: Okay? And then this margin analysis works for our result as well. So this is enough samples, uh, to get our, um, hypothesis to generalize. Okay, so I want to point out potentially a, a third way of proving generalization bounds, which is, uh, just to invoke, um, uh, the kind of generalization you can get from differential privacy itself. Uh, so there's this, you know, very nice line of work, um, showing that, uh, differentially private algorithms run on IId samples automatically generalize. And this, uh, um, sort of like, um, the killer app for the stuff is, um, doing adaptive data analysis. But it works, you know, just in the, in arbitrary settings where you want to use a differentially private algorithm over IIDC samples. Um, so sort of the, the punchline for this work is that if you want to get, uh, this type of generalization, it suffices to, uh, have h come from a differentially private algorithm where epsilon is, uh, order alpha.
00:43:40.484 - 00:45:17.750, Speaker B: Okay, so, um, for our particular result about privately learning large margin half spaces, this isn't terribly interesting, because if you set, uh, epsilon to be, um, alpha, you get that the number of samples has to be at least one over alpha squared, gamma squared, which is not better than what you get using a margin. Um, but, uh, if you, if instead of looking at the traditional pack learning case, you look at, um, the setting of noise tolerant learning, this gives you something somewhat interesting. Um, so remember that in the, in the pack setting, we're given some samples that come IId from some distribution d. Uh, in the framework of noise tolerant learning, um, you're given, uh, a different example, oracle, which with probability one minus eta, gives you like a clean example, which is, um, independently generated from the underlying distribution. Uh, and with probability eta, you get some like, arbitrary, adversarially chosen example, both in terms of the point and the label. So servideo showed that his smooth boosting algorithm per half space is able to tolerate malicious noise in this model at a rate of, um, eta being about alpha times gamma. Uh, and using, um, margin based arguments, uh, showed that this generalizes as long as n is at least one over alpha squared.
00:45:17.750 - 00:46:10.776, Speaker B: Gamma squared. Um, so our private booster um, also works in the setting of noise tolerant learning. Um, and so even if you don't care about privacy, uh, if you set epsilon to be alpha, you get the same generalization bound that servideo got using margins. Okay. Um, so I want to conclude with a few, um, further directions, uh, most of which I think should be doable, but I'm less confident, uh, than the stuff that, uh, about this, than the stuff that I talked about already. Um, so, like another, um, you know, a good question is whether you can get like genuinely new sample complex sample complexity balance for noise tolerant learning. Yes, vitalik, you sort of need a.
00:46:10.800 - 00:46:16.164, Speaker A: Differentially private weak learner in this setting, right? To get generalization.
00:46:16.464 - 00:47:07.744, Speaker B: Yes. Yeah, so I'm talking about using the differentially private weak learner. When you set epsilon to be alpha, then you will get a noise tolerant learner that also happens to be differentially private. But generalizes for the particular algorithm. For the particular algorithm I said. So, yeah, it'd be good to get genuinely new sample complexity bounds for noid tolerant learning using this connection to differential privacy. It's also probably likely that we can get private boosters for agnostic learning, um, which is, uh, in some sense, uh, a stronger, um, or like a more challenging setting than, uh, noise tolerant learning.
00:47:07.744 - 00:48:13.494, Speaker B: Um, another question is whether boosting based algorithms can be practical. So sort of like, um, I've been told that adaboost was like the first practical boosting algorithm because of this like adaptive thing where, um, the multiplicative weights updates can be made to depend sort of on how well previous hypotheses were doing. And so this can dramatically decrease running time and sample complexing itself. So it'd be nice to come up with adaptive boosting algorithm for differential privacy. And finally, it'd be great to come up with sort of more general connections between differential privacy and noise tolerant learning in broader settings. So, for instance, in robust estimation, which is different than these kinds of classification problems we've talked about where you want to estimate parameters of distributions and stuff, where some of the samples that you're given have been corrupted arbitrarily. Thanks.
00:48:13.494 - 00:48:20.694, Speaker B: Any more questions?
00:48:21.924 - 00:48:36.444, Speaker A: So basically, the only thing you need to get the agnostic boosting is the optimal smoothness parameter, making sure you actually do it with alpha n as your smoothness parameter.
00:48:36.484 - 00:48:37.132, Speaker B: Yeah.
00:48:37.308 - 00:48:39.020, Speaker A: So do you not get the optimal.
00:48:39.052 - 00:48:55.920, Speaker B: Smoothness concept for your booster? I don't know. We have to work this out. I think that's probably okay. And we just haven't like looked at doing agnostic questions.
00:48:56.072 - 00:49:35.812, Speaker D: Yeah, this is kind of a question for both you and Vitaly. In some sense, it seems that the thing that a margin gives you is that the sort of optimal solution is really, really well defined. Like a whole area of solutions that are all really good and stable. And I wonder, but it's very specific to classification, on the other hand, and I'm just wondering if there are other notions of kind of well definedness of.
00:49:35.868 - 00:49:36.464, Speaker B: The.
00:49:40.484 - 00:50:27.074, Speaker D: Solution to a problem problem that would also like. Those seem like notions that lend themselves well to generalization, right. But also to privacy. For the same reason that like in some sense, if the solution is kind of over defined, then one, you know, moving around examples isn't going to change it too much. And I guess it's just like a kind of an open ended question. Are there other notions of well definedness of a solution that might make sense in this kind of application, and also that might sort of make just as good sense for non, for either unsupervised settings or things where you don't have like binary labels.
00:50:32.514 - 00:50:34.386, Speaker B: Clustering and things like that? I'm not too sure.
00:50:34.410 - 00:51:22.034, Speaker A: It's kind of what you get is that you get big ball of solutions doesn't necessarily mean that the solution is well defined by the data, at least in this specific example of first of all, we know that there is some canonical solution, there is evolve and it is kind of robust. So all of its perturbation is still solution. But there might be various practices, solutions out there which do reveal a lot about the data. So just about this intuition, I'm not so sure if that. So there is a setting in which things are better defined as let's say if you have like strong convexity, then there things are indeed well defined and we know that they're closely related to privacy, generalization and friends.
