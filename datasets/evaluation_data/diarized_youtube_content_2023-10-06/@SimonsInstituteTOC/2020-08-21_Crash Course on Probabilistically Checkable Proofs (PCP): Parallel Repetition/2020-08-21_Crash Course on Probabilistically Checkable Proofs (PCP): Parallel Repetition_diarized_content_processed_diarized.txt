00:00:00.160 - 00:00:08.314, Speaker A: So welcome everyone, to the third course, the third lecture in the course on PCP theorems. We're happy to have back to tell us about parallel repetition.
00:00:09.814 - 00:01:03.834, Speaker B: Okay, so thanks. Yeah, so thanks for coming again. Hopefully today the Internet will, on my side be a bit better. So I apologize for last time. So, in the past two lectures, we gave an introduction on pcps and Dana gave a very nice lecture about proving the a weak PCP theorem today. And in the last lecture on Monday, our goal is to talk about more advanced topics in a more survey like fashion, and kind of describe connections to high dimensional geometry, which as far as, or at least the kind of geometry that we think that maybe will interest our workshop participants. It's always hard to predict what other people care about anyway.
00:01:03.834 - 00:02:12.621, Speaker B: So what is parallel repetition and how is it related to what we talked about before? So let's remember the basic PCP theorem, which basically said, if you have a graph in the language of three coloring, it is NP hard to decide if a given graph, if a given graph has a perfect three coloring, meaning you can color the vertices so that all the edges are happy, or every three coloring falsifies at least 1% of the edges, so it has value 0.99. So that's what we call the basic PCP theorem. And I happen to describe it in the language of three coloring. Then Dana showed how you can also describe it in the language of quadratic equation. And there are many other constraint satisfaction problems that you can cast this into. And a couple that we talked about were thrilling. Oops, thrilling.
00:02:12.621 - 00:03:03.064, Speaker B: Which is when you have linear equations mod two, and each equation, each equation has three variables. And I quoted the theorem of Hasdad, which says it's NP hard to tell if the value of a given thrill instance. So let me denote it by equations. This is just an instance of thrilling. If the value is large, it's almost one. Or if the value of the same instance is at most half plus epsilon. And we said that this is tight.
00:03:03.064 - 00:03:47.348, Speaker B: So unlike the situation here, where 0.99 versus one is not necessarily tight, it's just already making us happy. Because there's a constant gap here, the parameters are actually tight in the sense that you cannot expect a larger gap in linear equations because there's an algorithm that shows that actually telling between a larger gap is not NP hard, it's actually in polynomial time. So this is tight. Of course, assuming P is not equal to NP, this is for three. Lin and hast also showed that it's NP hard. Given a three SAT, given a three set instance, let me call it, I don't know, psi.
00:03:47.348 - 00:04:23.532, Speaker B: So, three set instances. You know, you have boolean variables, and you have clauses of the form x one or x two or not x three, x one or x five or x seven, and so on. This is a three step, and here you prove that the value. It's NPR to decide if the value of this instance is one, meaning there is actually a satisfying assignment, or if the value is less than 0.99. No, actually, 0.99 is not very interesting because this is the same kind of behavior we had here. There's nothing special about the number 0.99.
00:04:23.532 - 00:05:06.912, Speaker B: So what is the number that I should write here that's actually tight? This is seven eight plus epsilon. And the reason for seven eight is that, again, there's a very simple polynomial time algorithm that finds an assignment that satisfies seven eight of the clauses. And what this theorem is telling us, actually, you cannot do any better. Okay, so just again, to say what is value here? This notation is different depending on the problem. When I talk about three set, the value is the fraction of clauses. When I talk about three lean, the value is the fraction of equations. And when you talk about three coloring, it's the fraction of edges.
00:05:06.912 - 00:05:59.854, Speaker B: It's always the fraction of constraints. So how do you prove such a theorem of hastat? And how general is it? I mean, are the three sat and three lean problems that I'm showing here? Let me highlight this. So I have here three sat, and I have here three lean. And is it actually, is this theorem, does it generalize to every possible CSP? So, 1st, first of all, let me just say that when this came out, it was incredibly shocking that even one can prove such a type theorem. But no, you cannot continue and prove it for every CSP. For this, this is still open, but it does follow from something called the unique gains conjecture. So this will be discussed, I think, mostly on Monday, but we will return to this today as well.
00:05:59.854 - 00:07:04.692, Speaker B: So let's talk about how Hostad proved his result, and the way he did it was by moving to one CSP code, label cover. And this is just a specific constraint satisfaction problem, just like you have thrilling. And three sat, you also have label cover. And then using parallel repetition, which I denote by this tensor product d, because parallel repetition is a term that, it's a kind of tensor ization of the instance. It's a kind of direct product of the instance, you get into another label cover instance, but this time, let me call it strong label cover. So really, what's going on? Here I also write the word parallel repetition. There's a transformation from a CSP with some gap that is more of the form here, which is just a gap between one and something slightly less than one.
00:07:04.692 - 00:07:53.314, Speaker B: And you get a transformation into a gap that's between one and zero, or more correctly, between one and almost zero, one and epsilon, which is the largest possible gap you can have. And once you have such a very strong pcp. And so this we sometimes call the strong PCP theorem. Once you have such a strong pcP, you can apply all kinds of local encodings like gadgets and get these very strong results. Hasta and many more. This is already going to be described next week. Okay, so let me move to talk about color repetition and label cover.
00:07:53.314 - 00:08:54.438, Speaker B: So what is label color? Let's define labor cover level cover instance is given, it's given by a bipartite graph. I should instance is given by a bipartite graph g, whose vertices are a, b and edges e. Okay, so I have here vertices a, I have vertices b, and I have edges. Let's say this is u and this is me. And there's also a parameter so level cover with parameter ar. Okay, parameter r says what is the size of the label set? So you should think of r as some constant parameter, but it's kind of arbitrarily large. It's like the Alphabet size.
00:08:54.438 - 00:09:39.534, Speaker B: So the vertices here are going to be like variables of a constraint satisfaction problem, and they get values in some, we call them labels here, but it's not like zero or one or maybe a color in one, two, three. It can be a number in an Alphabet of size r. Okay, so there's two label sets now describing what the label cover instance is. So there's two label sets, La and Lb. These are label sets whose size is it most? R. So that's how the parameter comes in. And for every edge, for every edge you also have a function fuv.
00:09:39.534 - 00:10:22.616, Speaker B: There is a function fuv from the label set a to the label set b. So actually what I'm describing here is called the projection gain level. However, there's a projection from the labels on the a side to the b side. I'm just going to call it label cover for here. So the label cover instance is given not only by the graph, but also by, for every edge, you need to give me this little function from la to lb. Okay, now we define the value of a labeling. So now how is this a constraint satisfaction problem? Someone needs to come and label the vertices.
00:10:22.616 - 00:11:37.704, Speaker B: So labeling is some assignment from the vertex sets to the label sets vertices in a should get labels in la, and so on vertices in b labels in ab. And the value of this labeling sigma is defined to be the fraction of edges that satisfy this function. So what does it mean to satisfy the function? If I label this vertex by some label alpha and I label this vertex by some label beta, it needs to be that f of this edge of alpha should equal beta. This means, this is, this means that the edge uv is satisfying. Okay, so this is the constraint on every edge. Fuv really specifies a constraint for me. And the value of the entire labeling of the entire graph is just the probability over all edges in the graph that f u, v of sigma of um equals sigma of v.
00:11:37.704 - 00:12:44.304, Speaker B: Okay, so the probability that this overall edge I could have written, instead of this kind of heavy notation I could have just written that this edge is satisfied, uv is happy. Okay, so the probability that this edge is happy, this is exactly defined as the value of sigma with respect, of course, to this instance, and the value of the label cover is the maximum. If you go over all possible assignments sigma, you want the labeling that is the best. This is the value of your label cover. Now it's very easy. So this is a simple claim to see that the basic PCp theorem implies that it's NP hard to the side on a gap of a one minus, I don't know, let me write, I don't know, 0.999 and one label cover in this notation.
00:12:44.304 - 00:13:04.404, Speaker B: We didn't have it before. Okay, label cover. This notation gap and two numbers. It's shorthand for having to write. It can be hard to decide if the value is one or if the value is less than 0.99. Okay, so it's just notation. You have a gap between these two values.
00:13:04.404 - 00:14:03.380, Speaker B: Why is this? As you've seen already in the last previous lectures, there are many different CSP's and it's kind of easy to move between them. Sometimes this moving between them or doing a reduction is not so evident. For example, when you move from three coloring to quadratic equations, we say it's easy, but really you need to understand how to do the reduction. Here it's actually really easy if you know, for example, that graph recoloring is NP hard with a gap. You construct the label cover instance by putting a vertex on the a side for every edge and a vertex on the b side for every vertical vertex in your graph that you're trying to three color. And the labels on the edges should tell you a coloring of both endpoints of the edges and the labels on the vertices should tell you a coloring just of the vertex. Of course, the labels on the a side should only allow legal colorings.
00:14:03.380 - 00:15:05.350, Speaker B: So you only have three times two possible labels for every edge because the color should be different. And the fun the FUV will just check consistency. If the edge on the a side is between some vertex v one and v two, then you want that the label on this edge is consistent with the label on the other side. So it's just a very simple transformation and it shows you that this gap level covers very, very easily. Okay, not only that, if you notice in my, the way I described it, the label set, the label cover that I constructed was small. It was size six, size six for the big player and size three for the small player. Now we already had this in the first lecture.
00:15:05.350 - 00:15:51.200, Speaker B: So sometimes you can describe something in a very kind of CSP or kind of static way. But there's also a more personified dramatized view. And this is the two player game. So let me call this the dramatized perspective on label cover, which is two player, one round game. So what is the story here? Let's look at this picture. I like to describe this when I look at the picture of the graph. Okay, so I have this in mind, this graph, and there are two players.
00:15:51.200 - 00:16:23.904, Speaker B: One is called Alice, the other is called Bob. And there is a referee. The referee selects an edge in this graph at random. And then the referee sends one endpoint u he sends to Alice, and one endpoint v he sends to Bob. So Alice only knows you, she doesn't know which, which edge, which v, and Bob only knows v. Alice needs to respond with some kind of answer, which is a label in LA. And Bob needs to also respond with some kind of answer, which is a label in.
00:16:23.904 - 00:17:09.178, Speaker B: And then the referee looks at these two answers, alpha and beta, and makes some computations which amount to computing whether f of uv computed on alpha equals beta. And the referee says yes or no. And in this game, the value of the game is the probability that the referee says yes, the probability that the players win the game. Okay, so this is called the winning probability of the game. And you can see that it's exactly the value of the label color. It's just describing this whole situation as a game. And notice that indeed the value of the game is the same as the value of the lever cover.
00:17:09.178 - 00:17:59.354, Speaker B: Because from the point of view of a and b, Alice only gets in one round of the game, which is we only run it for one round. Alice just gets a question which is one vertex and she needs to answer with some labor. So to describe what Alice is doing, to describe her strategy, all you need to tell me is for every vertex what label Alice is giving. So a labeling of Alice's vertices is a strategy for Alice and similarly for Bob. Okay, so the success probability is the value of the little columns. The same thing. Okay, so to summarize what I said, we have players a and b and b.
00:17:59.354 - 00:18:58.864, Speaker B: Referee, referee selects a edge. Uv at random, sends u to Alice and v to bob. A replies with alpha, b replies with beta, and the referee accepts. This is one round because the referee sent something and the player's answers and the game is over. Yeah, there are three, except if and only if f u v of alpha equals beta. All right, so so far we just described what a two player game is and now we're ready to describe the parallel repetition. Okay, so it's very, actually much more convenient to describe parallel repetition when you talk in this personified way.
00:18:58.864 - 00:19:38.024, Speaker B: So what is the parallel repetition? Repetition. Well, the referee runs d different games in parallel. So what does that mean? The referee selects not one edge, but the edges u one, v one ut, v two, and so on. Udvd. Okay. Independently in the edge set. Okay.
00:19:38.024 - 00:20:22.714, Speaker B: And after, therefore he selects these edges, he sends the first endpoints, u one up to ud to Alice. And let me call it Alice, not a and v one up to vd to Bob. And Alice and Bob look at their information that they got the questions. And then Alice responds. Alice sends alpha one up to alpha D. Bob sends beta one up to beta d. So it's a d topple of labels and therefore accepts if.
00:20:22.714 - 00:21:14.444, Speaker B: If, what? If for every I, f of u I, v I of alpha I equals vet I if they are right on every single coordinate. Okay, I is in one to d. Okay, so that's the parallel repetition game as you describe it. The reason it's called parallel repetition is to distinguish it from sequential repetition. When people think about different kinds of interactions and rounds, then it's very natural to say, first I send, therefore he sends an edge, and then the players respond. And then the referee sends another edge and then they respond again. This would be two rounds, and if you do it sequentially, d rounds, it's a different type of object than this object where it's still one round.
00:21:14.444 - 00:22:04.268, Speaker B: There are three select something sense to the players, the players respond and that's it. So it's still the same type of object as we had before. It just relates differently to the level cover instance. And really what's going on here is, what do we have here? We really have a different type of level, just a different level cover instance. We're really working not with the initial level cover graph, but with a tensorized version of it, where the new a, it's really a to the d here, the vertices correspond to tuples, u one up to ud of elements in a, and the new b is b to the d. The vertices correspond to tuples in b. Right.
00:22:04.268 - 00:22:52.434, Speaker B: These are the questions that Alice can get u one up to ud, and these are the questions that Bob can get v one up to ud. The label set is also la to the power d and lb to the power d. So I guess I shouldn't here, I shouldn't use tensor product. It's just, it's just the product lv to the d. Yeah, and the functions, I'm not going to write it, but we understand what the function is. It's the function exactly that describes this behavior of the referee. You're checking a label alpha one up to alpha d matches with beta one up to beta d if they match in every single.
00:22:52.434 - 00:24:07.264, Speaker B: We basically took a label cover instance and we tensorized it in a bipartite way and we got a new label cover instance. So by the way, if you think of just the underlying graph of the label cover, remember a label cover is a graph plus these constraints. So if you just think of the graph and you write the adjacency matrix of the bipartite graph, so you have a here and b here, and you tensorize this matrix with itself, d times you will get the adjacency matrix of this graph. So that's why it's really a tensor ization. It's just a bipartite tensorization, but, okay, so now it's very natural to ask, okay, what happens to the value of the game? Okay, so question, oops, wrong color. Okay, question, what happens to the value of, let's call it g to the d compared to the value of g. Okay, so if the value of g was one, suppose to the initial game, the players had a perfect winning strategy.
00:24:07.264 - 00:24:39.244, Speaker B: Then clearly also now they have a perfect winning strategy. They should just answer in a way that's perfectly consistent with the previous strategy in every coordinate. Okay, if val g is one, then val g to the d is one. This should be clear. Sorry, my check marks are reversed because I'm left handed. Okay, this is one thing. What happens if the value of g is smaller than one? I don't know, it's maybe one minus epsilon, then.
00:24:39.244 - 00:25:21.222, Speaker B: Well, I can still have the players answer each coordinate independently, and then they will have a value of one minus epsilon to the d, because with probability one minus epsilon, the referee chooses d edges. So the first edge with probability one minus epsilon is actually a good one. The second edge, again with probability one minus epsilon, is a good one where they actually know how to succeed. So if they answer in a product strategy, the probability of success is one minus. Absolutely. So I can definitely write this now. It's very tempting to actually write equality.
00:25:21.222 - 00:25:47.524, Speaker B: And initially, when these games were studied, people mistakenly wrote that. But there's no reason to expect that the players always play a product strategy. The players can actually maybe play a different kind of strategy. Okay. And so it could be that the value is actually much higher than one minus epsilon degree. Okay. And this was the parallel repetition question.
00:25:47.524 - 00:26:10.674, Speaker B: Can you prove that as d goes to infinity, as you repeat the game more and more that at least the value goes to zero. Okay. And this was shown. At first, it was shown that it goes to zero at some rate. Then it was shown to go to zero, polynomial in d by Phi Ganglion. And then run Ra's proof, the famous parallel repetition theorem. This was in 95.
00:26:10.674 - 00:27:02.458, Speaker B: It shows that the value of g to the d goes down one minus epsilon to some power. I guess maybe it was 32, I don't know, to the d. So you should read if the initial value was one minus epsilon, then as you repeat, the value goes to zero and it goes to zero exponentially fast. Okay. So that was a big achievement. And it gives us what we need, because as we said, the graph g to the d, or this operation of parallel repetition of the game is giving us a new label cover. And if initially we had a gap between one and 0.99,
00:27:02.458 - 00:27:53.158, Speaker B: then as we take d to be larger and larger, our gap would be between one and 999 to the power of d. I moved from 0.99 to 0.999 to kind of take into account this 32. Okay? So, but of course, you take D to be as large as you need, and of course, this value will go to zero in the null case and remain at one in the yes case, and you have a gap, a very strong gap for parallel, for parallel repetition. So the corollary of this is that the label cover with gap in epsilon versus one is np hard. And remember I told you that level cover comes with a parameter which is the Alphabet size.
00:27:53.158 - 00:28:41.768, Speaker B: The Alphabet. It needs only to be polynomial in one minus in one over epsilon. So it's a constant. It depends on epsilon, as it should. And it's only polynomial, so that's sometimes important. Okay, so I told you that initially people, until things started to fall into place, it was not clear whether the value of the parallel repetition is perfectly decreasing exponentially. And then all kinds of strange counterexamples were found, where you have a game whose value is something like three quarters, and then you repeat it twice and the value remains three quarters.
00:28:41.768 - 00:29:22.378, Speaker B: It doesn't even decrease at all. So all kinds of weird behavior happens. But slowly things became more understood. And I'm not going to describe to you now a counterexample like that. Instead, we will soon see the geometric interpretation of this value. And when we talk about that, you will see a geometric example of how the value doesn't have to go down as fast as it does in the product strategy. As I said, Raz proved that it does go down exponentially fast.
00:29:22.378 - 00:30:15.744, Speaker B: But the question is in this case, how fast? Before that, I just want to say that people have worked on trying to understand the proof and get maybe better parameters. For example, this exponent of 32. And there were many words on this, and eventually Hollenstein and Rau showed that for this kind of gains, for projection gains, the theorem can be improved so that instead of 32, you can have two. So we know that the value of g to the d is less than one minus epsilon squared to the d. Okay, this is for projection games. All the games we talk about today are projection games, so doesn't matter. In particular, unique games are projection games, and so on.
00:30:15.744 - 00:31:20.910, Speaker B: Okay, at this point, in order to prove, for example, Hasta's results, this is not important at all. But now I'm going to take a detour and discuss a little bit unique games and Max class that, because this direction of research actually started to care very much about this constant tune. So, as I told you before, let me scroll up a little bit. As I told you before here, when we talked about three sat and three lean, Hastan managed to prove tight results for a number of very central constraint satisfaction problems, but not all of them. And for example, one very basic constraint problem is the problem of Maxcut. What is Maxcut? Maxcut is like three coloring, except not three colors, but two colors. You need to, you have, you get a graph as input, you need to color the vertices by two colors, or namely to split the graph into two parts.
00:31:20.910 - 00:32:10.094, Speaker B: And you want to maximize the number of edges that see two different colors, namely that cross the cut. So basically you need to cut the vertices into two parts and maximizing how many edges go inside the cut. So, unlike three coloring, you need to cut there into three parts. When you need to cut in only two parts, it's easy to decide if you can cut 100% of the edges or not, because this is just whether or not the graph is a viper type graph. Okay? So it's actually polynomial time to decide two color abilities. But Maxcut is not a polynomial time optimization problem, because if you're getting a graph that is not a bicycle graph, and you need to maximize the number of cut edges, in that case, we don't know. It's actually, it's np hard to do it.
00:32:10.094 - 00:33:19.304, Speaker B: And the best known approximation algorithm is the celebrated Gomez Williamson semi definite program and the Gomns Williamson algorithm. So let me go back here. Here, the Gomez Williamson algorithm. So, Max cut, the GW algorithm shows that if the value of the, of the graph is at least one minus epsilon, meaning when I say value of the graph, in this case, I just mean the graph has a cut that cuts almost all the edges, one minus epsilon of the edges, okay? Then their algorithm, the value of the algorithm, or the fraction of edges that they managed to cut, is one minus square root of epsilon. So it's a very, very good algorithm, but still, there is a difference between epsilon and square root epsilon, okay, square root epsilon is bigger, so the algorithm is cutting fewer edges. Okay, so this is the algorithm. And you might wonder whether or not this is tight.
00:33:19.304 - 00:34:47.016, Speaker B: Maybe there's no algorithm that does better. And indeed, in a very important paper, Kotkindler, Mosel and O'Donnell showed that actually, the unique games conjecture, I haven't described what unique games are, and I will now, if this conjecture is true, and it's talking about a kind of label cover, then it implies that the GW algorithm is the best possible. In other words, it implies Max cat is hard, with a gap of one minus root epsilon and one minus epsilon. Okay? So this gap is exactly the correct gap. This is, of course, assuming that the unique games conjecture, okay, so I should say here, unique games conjecture, this was a conjecture made by Subhashko just a couple of years before that, in maybe 2003 or two. And this paper, I don't know if it was zero, four, I don't remember, was something, and this was a very big surprise. So, in this paper, by the way, this super influential paper, they didn't quite manage to prove this reduction.
00:34:47.016 - 00:35:21.988, Speaker B: They needed something called the majority stable x. And so they formulated this as a conjecture. And very quickly afterwards, in a famous paper by masala, donor and alejkevitz that introduced the invariance principle. This was proven. So all this Dana will describe on Monday, I believe. I just want to highlight that Maxcat is kind of a very basic CSP, because it talks about a constraint between two variables that's as small as possible and their boolean variable. So it's kind of the smallest possible constraint.
00:35:21.988 - 00:36:59.432, Speaker B: It's very basic, and so you would expect to be able to analyze how hard it is. And until today, we don't know whether or not this is true if we don't want to assume the unique games conjecture. So let me say, what are unique games? It's the same as a label cover, except that the fuv are one to one. Okay, so a unique game is where did we have, so we had here label cover instance here I defined label cover, and a unique games label cover is a label cover, where in addition, you know that these functions fuv from la to lv are one to one. When you hear it, you don't know what does it mean? Is it making the problem easier or harder? So it's making the problem easier because the constraints are from a more limited class. And it turns out that if you assume that this kind of label cover has a gap between zero and one, like we already know, for the more general, so assuming we believe the parallel repetition theorem, we know that label cover has a gap between Epsilon and one. For any epsilon, the unique gains conjecture says, oh, you know, if you assume a gap between Epsilon and one minus epsilon when the constraints are one to one, this is a very good assumption for me.
00:36:59.432 - 00:37:48.216, Speaker B: And I can prove, for example, that Maxcat, I can prove that the Maxcat is the semi definite program algorithm of Gomez Williamson is tight. Okay, so sorry about this detour, but the reason I want to talk about this is because now this leads me to talk about strong parallel repetition. Okay, so KkMo showed basically a reduction from unique games to Maxcat. Now at that point you can say unique games is kind of a new construct. I don't know about it. Whereas Maxcat is a much more well studied problem. And I know that it's hard to, to beat the GW algorithm.
00:37:48.216 - 00:38:29.304, Speaker B: So it would be much more convincing if you can prove a two way reduction. Can you prove that if Max cut the gap of one minus epsilon and one minus root epsilon? Suppose I assume that this gap is tight. Does this imply the unique gains conjecture? So really what you're asking is whether this direction is also true. Can I reduce gap Max that back to unique things? And that's a very natural thing to do. Moreover, there's a very, very natural reduction to do it. It's just parallel repetition. Okay, so it's a very natural.
00:38:29.304 - 00:39:21.494, Speaker B: Can we show hardness of one minus root epsilon one minus epsilon? Max cut implies hardness of epsilon one. Or let me call it delta one minus delta. Unique gains. This is just the unique gains conjecture. This is what it says. Okay, and how will we do it? We can just take a Maxcut instance. Notice that the maxcut itself is a unique game, because what are the constraints here? The constraints are between two adjacent vertices.
00:39:21.494 - 00:40:17.170, Speaker B: This vertex has value on one side of the cut, and the other vertex should have the opposite value. So there's only two values, zero and one. So if this guy is zero, this one needs to be one and vice versa. So the constraint is one to one. And when you take a one to one constraint and you do parallel repetition on it, you remain a one to one constraint. So if you parallel repeat a unique games, sorry, a Maxcat instance d times, you will still get a unique gain. So also plus parallel repetition is a very natural candidate reduction, because parallel repetition of maxcat is a unique one.
00:40:17.170 - 00:41:26.144, Speaker B: Okay, so what do we need? We need to show that if we had, if the value was of the graph, the max cut is one minus epsilon. I know already that the value of the repeated instance is at least one minus epsilon to the power d. This is just because I will take the product strategy so I can always get this value of one minus epsilon to the d. However, if I was in a null instance and my value was less than one minus root epsilon, then what kind of theorem do I need to prove? I need to show that the value of g to the d is less than. Well, it would be nice if I could prove one minus root epsilon to the d, or even with some constant. Okay, let me ignore the constant. This would be, would be wonderful, right? Because why would this be so wonderful? First of all, before I say what it would be not wonderful, we don't know this.
00:41:26.144 - 00:41:47.750, Speaker B: What we know is that you need to take this and raise it to the power two. Right? This is what I told you that Hollenstein approved. It used to be 32. They moved it down to two. But of course, if it's two, then you have here one minus epsilon to the d, and here one minus epsilon to the d. It's useless. Okay, so what you really need is to get rid of the two.
00:41:47.750 - 00:42:31.804, Speaker B: Once you get rid of the two, then you can choose d just by choosing d to be somewhere, let's say, much smaller than one over epsilon, but much bigger than one over root, epsilon. Okay, if it's in this range, then this guy would go to almost one. It's still near one. And this guy would go to almost zero. Okay, and great, you have a gap between almost one and almost zero. This is what the uni games conjecture is telling us. So this is why people really cared about getting rid of the two to get a reduction from Xtac back to unique games.
00:42:31.804 - 00:43:09.186, Speaker B: Okay, so this was studied by Feige, Kindler and O'Donag. This was their starting point. Feige, Kindler and odor. Actually, I don't remember the year. Sorry about this. Maybe zero, eight, I don't know. But they said, okay, let's not talk about Olivia, all games, let's only talk about unique gains.
00:43:09.186 - 00:43:43.520, Speaker B: All I need to do is parallel repetition with. They wanted to prove, oh, maybe, let me call it. So this, this is the so called strong parallel repetition conjecture. This is what I called. Wonderful. Okay, so the conjecture is that this inequality holds without the two, okay, this is the conjecture that you take a game of value one minus epsilon. Sorry, one minus root, epsilon.
00:43:43.520 - 00:44:17.794, Speaker B: And when you raise it to the power d, the value just raises to the power d, maybe with some constants. Okay, so they wanted to prove the strong parallel repetition conjecture or check if it's true or not. And they said, okay, we only need to prove it for unique games. We only need to prove it. Actually, for Maxcat type of games where the Alphabet is boolean. And actually, let's focus on a specific game. So focus on a very special case called the odd cycle game.
00:44:17.794 - 00:45:03.606, Speaker B: So what is the odd cycle? It's a graph, right, with an odd number of vertices, and it's not bipartite, although it's like almost bipartite, right. You can do a plus minus plus minus plus. And so there's one edge that's kind of ruining the bipartiteness for you. And as the size of the cycle grows, there's still only one edge. That's problematic. So this is the old cycle game. And if you think of it as a cycle on m vertices, that's call it cm, then the value of cm is roughly one minus one over n.
00:45:03.606 - 00:46:05.194, Speaker B: Okay, you can kind of cut the vertices in a way that, okay, only one edge is not happy. And the question is, okay, when I take this specific, this specific graph, and I look at the parallel repetition of this specific graph, do I get a, is it true that when I take this Cm and I raise it to the power d, is it true that I get one minus one over m to the power d. This is my question that the FKO asked. Now, there is some issue here that I, uh, neglected to tell you, which is when you move from Maxcut to a labor cover because label cover has a bipartite description. But Maxcut on a bipartite graph doesn't make sense. So you need to move to the double cover of the graph. And the way to do it is that to describe the odd cycle gain.
00:46:05.194 - 00:46:47.376, Speaker B: So what is exactly the odd cycle gain? It's the following thing I'll tell to you in the dramatized version. So there is a graph, everyone is looking at it. It's a cycle. The referee with probability half, referee with probability half chooses an edge. Uv sends u to alice and v to bob. And with probability half, the referee chooses just one vertex, um, and sends it to both alice and bob. Okay, they don't know what happened.
00:46:47.376 - 00:47:14.580, Speaker B: And when he sends the same vertex to both guys, he wants the answer to be the same. When he sends vertices on an edge to the players, he wants the answers to be different. So this is like, are you still seeing me or did something bad happen? It's okay. All right, so this is the odd cycle game. Exactly. So it's like a, it's like a double cover of the graph. It's not important.
00:47:14.580 - 00:48:16.228, Speaker B: So let's not dwell on it. Okay. And the question is whether the value of this game goes down like something is happening. Okay, so my computer, I guess, is still functioning, but the iPad has disconnected. Okay. While it's disconnecting, I tell it to you without an iPad. So, okay, so they studied this question about the odd cycle and how you.
00:48:16.228 - 00:49:03.914, Speaker B: Okay, now it's a good time for a hotspot. Okay, this looks a bit better. Share content screen start. Okay, we're back. Right? Yeah. Okay, sorry about that, guys. So, um.
00:49:03.914 - 00:50:38.064, Speaker B: Okay, so, okay, so they tried to study this, and it turns out that this question is related to a question about foam. Okay, foam, like soap bubbles that make foam related to foam in r to the d. In other words, it's related to the following question. Okay, what is the least area, the least surface area of a shape that tiles r to the d by shifts of z to the d? So this question turns out that people have studied it and it goes under this as word of foam. And the surface area here, notice that because you are tiling r to the d. So maybe the best way to think about it if I change here to. So imagine this is r to the two and I need to put some shape around every point for example, I can put the square that if I shift it like this, I'm tiling the entire space.
00:50:38.064 - 00:51:23.404, Speaker B: Right? So square actually works very well. Or, you know, in more generality, this kind of shape, the cube ties r to the d very nicely. But the surface area here is roughly d the surface area of this cube. And what is the surface area of the smallest? Kind of. And of course, if you're tiling, you need your volume to be one because you're tiling the entire space with this kind of shift. So when you're tiling and your volume is one, your surface area can certainly not be smaller than the smallest geometrical shape whose volume is one. So the shape is, of course, at least square root of d.
00:51:23.404 - 00:51:49.918, Speaker B: This is how this sphere behaves, okay. And this is how the cube behaves. Okay. It's clear that it's d right in every direction. You need to pay this and this part. So it's one 2d directions, okay? So you need to behave like the cube to tie the space. And your surface area is going to be at least square root d.
00:51:49.918 - 00:52:57.138, Speaker B: But what is the right answer? Is there like a crazy shape that actually looks like a sphere of its piece tiles r to the d? So this turned out to be an open question people didn't know the answer to. The best thing that was known in the time that this question was discovered by fko was some kind of shape that's better than the cube for two dimensions. It's slightly squished and then just tensor eyes. And turns out that this question is exactly related to, to the value of this odd cycle gain. So what is the relation? So notice that if you tie r to the d by shift of z to the d, really your shape, shape naturally lives in r to the d divided by z to the d, which is nothing other than the torus. So, you know, when d equals two, we usually draw it like this. It's like a doughnut, right? This is the two dimensional torus, and we're talking about the d dimension.
00:52:57.138 - 00:54:07.424, Speaker B: Okay? Now with a context switch going back to the odd cycle game, I want to try to explain how the odd cycle game is related to this thing. So what is the relationship? What is the graph underlying the odd cycle game? So the one shot game with probability half, the referee chose an edge and send both endpoints to the player and with probability half a vertex and send both them to the player. So I'm asking myself, if alice got queries, questions u, one up to ud, and bob got questions v, one up to vd. What do I know about the relationship between these guys? So I'm saying either ui can be equal vi. This is something that happens half the time, or u I equals vi plus or minus one mod m. So let's now think of kind of, we name the vertices by numbers from zero to m minus one modulo m. That's just convenient.
00:54:07.424 - 00:54:41.136, Speaker B: Okay, so all in all, the vector u is connected by an edge to the vector v. If and only if. If I look at them in l infinity, their distance is at most one. In every coordinate, the distance is either zero or one. So this is the kind of the graph that describes what's going on here. Okay, and if you like to think, topologically, you have a cycle, that's the odd cycle game in one dimension, and then you multiply it by another one. Now you get the torus and you multiply by another one.
00:54:41.136 - 00:55:15.184, Speaker B: So if you're used to these things, it's not hard to convince yourself that the graph is also just the torus. It's nothing other than the torus. So the graph underlined this game is the torus. Okay, so now at least we had the word torus appear twice, once in r to the d divided by z to the d, and once here is the graph. So really it's not that torus. It's a discretization of the torus. Discretized.
00:55:15.184 - 00:56:06.336, Speaker B: All right, so now here's an observation. If you look at this discretization of the torus, and you look at the part of the, of this graph that has no non trivial loops. So non trivial loops in the, in the torus are exactly the kind of loops here. Seeing the doughnut that go around here or that go around here, these are non trivial from a topological point of view. And, you know, this is a very well studied area, and you can define it through homotopy, whether it's homotopic to a point or not. So this kind of look going here is trivial, but the one goes around here or here is not trivial. So if you take an area in the d dimensional torus, you start kind of growing a shape inside this torus.
00:56:06.336 - 00:56:35.454, Speaker B: And in this shape, there are no non trivial loops. Then you can convince yourself that inside this shape, you can actually give a strategy to the players that satisfy all of the edges inside this shape. Because the only problems that kind of. How does the referee detect a problem? When you try to go along a cycle, you do plus, minus, plus, minus, plus, minus, plus, minus. When you close the loop. Oops. You see that there is a problem.
00:56:35.454 - 00:57:01.568, Speaker B: And there's a bad edge. But as long as you don't close the loop and you avoid closing the loop, you can actually kind of grow this strategy nicely. So this is kind of an important observation. Any shape, oops. Oh, I'm running out of time. Sorry. Okay, can I take another five minutes? Sorry.
00:57:01.568 - 00:58:21.948, Speaker B: Okay. Any shape with no non trivial, without non trivial loops has a strategy, has a, has a strategy without bad edges. Bad meaning unhappy edges. Okay, so if we manage to find a very large shape, then the only edges that kind of might be bad, and I need to count them out when I'm calculating my probability of success, are the edges that go from the shape outside, not exactly the surface area. Okay, so really what's going on here is that the size of the surface area is exactly the fraction of rejecting edges. So if you look at the paper or if you think about it for a few minutes, this is easy to understand. Maybe it's not easy in a talk, but it's really a beautiful connection and there's nothing hidden here.
00:58:21.948 - 00:59:10.728, Speaker B: You think about it and you will see this. So really what's going on here is that if you find a tiling of r to the d with small surface area, then inside the shape of the tile, you can, you can grow this strategy. And the only problems, the only places where the strategy will fit is in the surface area. So what FKO showed or proved is that if there is a foam, a small surface area, then a strong parallel repetition is false. But if you can prove that all form all the ways to tile the space with a shape like this must behave like a cube, then strong parallel repetition is true. And then you get a reduction from Maxcat to unique games, which would be really nice. Okay, so that's what they showed.
00:59:10.728 - 00:59:59.436, Speaker B: And this kind of motivated further people further looking at this, and then actually parallel, strong parallel repetition is false. So one minus epsilon squared, the two there is actually necessary, is necessary. So the bound of the one minus epsilon squared to the d is actually tight. This is what the unproved. And actually, how did he show it? He looked at the odd cycle game, this specific game, and he showed that for this game it's not true that the best strategies is one minus epsilon to the d, one minus. Here, epsilon is one over m. So we said that the value of a single game is one over one minus one over m if you raise it to the power d.
00:59:59.436 - 01:00:43.276, Speaker B: If strong parallel position were true, the value should be this, one minus one over m raised to the power d. Now ran found a much better strategy. Okay, whose value is one minus one over m squared to the d. Okay. So we found this strategy later on in a paper by Barack Rao, Regev and Stoiler. I think I named them all, kind of understood this construction of Ras in a more generalized way, and they showed really what's playing. The key role here is the semi definite programming relaxation of the game in any unique game.
01:00:43.276 - 01:01:19.172, Speaker B: And the odd cycle game is a unique game. In any unique game, there is a very natural semi definite program that is a relaxation of this game. The value of this program is bigger or equal to the value of the game. And the way this goes is that each variable, each player, instead of giving back an answer, it gives back a vector. Maybe Dana will go into it, or maybe not. Probably no time for all of these things. But if you look at the semi definite SDP value, semi definite programming value, this is the value that the parallel repetition captures.
01:01:19.172 - 01:02:10.886, Speaker B: So really the odd cycle game, the value is exactly this one minus one over m squared. This is the SDP value of the odd cycle game. And so this is the correct value of the parallel repetition. And so strong parallel repetition is false, and you do not get this kind of counterexample. So just to finish my talk, just to say, okay, so the chiropractic part is false, but what did we learn about foams? So, in another work by Kindler, O'Donnell, Wigdelsson and Rao, they actually managed to take this construction of ras and use it to construct a tiling of r to the d with surface area, square root d. So it's some kind of strange shape that on one hand, it ties r to the d perfectly by shifts of z to the d. And on the other hand, its surface area is like the sphere, it's only square root of d.
01:02:10.886 - 01:02:25.394, Speaker B: Of course, with constants, I don't know the constant. And I think actually Nogalon and boss Clautag worked on this and got very nice constants. I don't remember, there was some follow up work on this. Okay, so I'm over time. And so I stopped there.
01:02:28.714 - 01:02:49.298, Speaker A: Okay, well, thanks very much for the talk. And I think there is one question left that hasn't been answered, but I don't understand what it says, which part was natural? So I don't know if you understand.
01:02:49.346 - 01:03:09.034, Speaker B: What that means, but I probably talked about the reduction from Maxcat to unique games to be natural. When I said that it's, oh, New Yorker is talking to us. When I said that, it's a natural reduction to do a tensor ization to move from Maxcap to emit games.
01:03:09.654 - 01:03:16.754, Speaker A: Okay, if anyone has any further questions, or if whoever asked that question would like to clarify in a follow up.
01:03:21.514 - 01:03:24.130, Speaker C: Irith, can I ask you a different question please?
01:03:24.162 - 01:03:24.894, Speaker B: Go ahead.
01:03:25.514 - 01:03:51.846, Speaker C: So, if you want to construct tilings with, you know, nice properties, my, I mean, I've never thought about it, but the thing that I would try to do is I just draw a bunch of random vectors and IId, and I would construct the lattice from that, and I would use the tiling generated by that lattice. Does this not have this nice surface area property? And this is usually the type of examples that give you very, very nice properties.
01:03:51.990 - 01:04:06.846, Speaker B: Really? Wouldn't the surface area, I see. I have never thought about these questions, so I don't know, unfortunately, maybe someone in the audience knows more than me. Sorry.
01:04:06.990 - 01:04:10.584, Speaker C: No, sounds good, thanks.
01:04:13.764 - 01:04:33.544, Speaker B: Actually, a big open question in this area, which your construction doesn't really solve, but is that the construction of coming from Raz's example and so on, they're all randomized, like selecting random vectors and so on. We don't know of a deterministic shape, as far as I know.
01:04:34.444 - 01:04:43.712, Speaker C: Yeah, there are many examples of that, right? I mean, we also don't know of a deterministic graph that has a clique of logarithmic size or maximal clique of logarithmic size.
01:04:43.768 - 01:04:44.944, Speaker B: Oh, of course.
01:04:45.064 - 01:04:49.124, Speaker C: I mean, so there's, yeah, this is a very big, big issue.
01:04:50.504 - 01:05:53.616, Speaker B: Here's one more, one more example. So someone asked, why was tensorization a natural? Oh, okay, I guess that's arguable to me. What is a reduction? A reduction, you take some object, you do something to it, you get another object. So one of the most basic things to do is to tensor eyes, what can you do, really? You can do some local replacement, you can tensor eyes, what else can you do? Are there examples where it's useful to consider multiple round games? Sure. In many cryptographic settings, when you have more rounds, the, you have much more power, the referee has much more power, so you can get much better properties, but it doesn't have anything to do with the hardness of approximation. So you're moving into other regimes, but I don't know if you meant one sequentially or morons in parallel. If you mean in parallel, sequentially, yeah.
01:05:53.616 - 01:06:28.424, Speaker B: So sequentially, yes. There's a lot of, a lot of crypto situations where you want to do many rounds and you get properties that we cannot dream of when you do one round, actually. Why does the referee have more power rather than the player? Because the referee tells the player. The answer. The referee keeps challenging the players, and if they fail, he catches them. Right. So, if after one round, they didn't fail, and now you're adding another round, it's just more opportunity for them to fail.
01:06:28.424 - 01:06:34.844, Speaker B: Okay. The more tests you take, the more probabilities you fail. I don't know.
01:06:34.884 - 01:06:39.584, Speaker A: But also because the referee can use the answers from the previous rounds to try and get them in a trap.
01:06:40.564 - 01:06:51.064, Speaker B: Yeah. Thanks, Joe. Yeah. So, you answer something in the first round, and the referee still remembers it. Right. So, kind of. I don't know.
01:06:51.064 - 01:07:15.384, Speaker B: One more question that I wanted to mention is three player parallel repetition, which looks naive, like. Oh, it's like two player. If we know how to analyze this, we know how to analyze that. This is wide open to prove exponential decay. We don't know how to do it. There's kind of some new work on it recently by Ran and Justin Holmgren.
01:07:22.624 - 01:07:27.604, Speaker A: Cool. Oh, so, this is not a question. This is a reply to Ramon's question.
01:07:28.304 - 01:07:56.438, Speaker B: But it does not correspond to a lattice's basis. So, how do you understand the question? The answer there is a measure called the Siegel measure, which I don't know. And if you select from that, then you do get the surface area, that.
01:07:56.486 - 01:08:05.494, Speaker A: Square root D. I think he's claiming that you don't. I don't know. Maybe Nike, you can. I don't think I can unmute you, Daniel.
01:08:05.534 - 01:08:10.274, Speaker C: Sorry to Daniel saying something about. No, we can unmute, can't we?
01:08:10.794 - 01:08:14.642, Speaker A: I think I do not have the power to unmute, but Nike does.
01:08:14.778 - 01:08:23.034, Speaker B: Yes. Just a second. Daniel, you should be unmuted.
01:08:23.194 - 01:08:24.930, Speaker D: Yeah. Can you hear me?
01:08:25.042 - 01:08:25.898, Speaker C: Yes. Yep.
01:08:26.026 - 01:08:48.906, Speaker D: Yeah. No, so, there are different ways of generating lattices in a way that you would call random, and. Yeah, the. The Siegel measure is. Is sort of. How would you do, like, there. Yeah, it's a bit complicated to describe.
01:08:48.906 - 01:09:37.394, Speaker D: There's, like, a poor man's version of it, which is not so complicated to describe, which I can do, maybe, which is like, basically, you pick a. Run a random sub lattice of the integer lattice, where you basically take all integer points that satisfy a random modular equation modulo p. And that's like a poor man's version of this thing. And if you look at what are called the Voronoi cells of this lattice, so just all the points that are closest to the origin than anyone else, that induces a tiling. And that tiling will basically look like a ball. Like, the Voronoi cell will look like a ball.
01:09:38.254 - 01:09:40.606, Speaker C: So it will have surface area, square root D is what you're saying.
01:09:40.630 - 01:09:54.994, Speaker D: Yes, but if you pick the lattice to have a basis of gaussian vectors or something, then the basis is kind of orthogonal, which will give you.
01:09:57.554 - 01:09:57.866, Speaker B: Like.
01:09:57.890 - 01:10:01.494, Speaker D: A Voronoi cell that looks more like a cube than like a ball.
01:10:02.314 - 01:10:06.250, Speaker C: Well, it depends on how many of these you pick. Of course, if you pick an exponential number.
01:10:06.402 - 01:10:11.370, Speaker D: Yeah, yeah. But if you pick more than n vectors, then. Then. Then you won't have, like a basis.
01:10:11.442 - 01:10:13.854, Speaker C: Ah, that's right. Okay, now I'm with you.
01:10:15.154 - 01:10:20.534, Speaker B: But will you be tiling with respect to your own lattice or with respect to z? To the d?
01:10:20.914 - 01:10:21.290, Speaker A: Yeah.
01:10:21.322 - 01:10:23.686, Speaker D: So you tile with respect to your own lattice.
01:10:23.810 - 01:10:24.070, Speaker B: Yeah.
01:10:24.102 - 01:10:34.014, Speaker D: That's why this question is very different than. Yeah, it's. It's. It's quite. It's quite different than tiling with respect to zn.
01:10:34.174 - 01:10:43.110, Speaker C: Well, the answer is that. That my. The gist of my idea works, but my description of how you generate a random lattice is not a useful one.
01:10:43.302 - 01:11:19.814, Speaker D: Yes, yes. But I think some of the tiling with respect to zn is. Yeah. Is kind of a different question in some sense. It's like if you were to change this, if you were to take the tiling induced by the Voronoi cell, and then you could apply the linear transformation, or a linear transformation that turns the lattice into zn, that would destroy the surface area completely. So somehow, the question specifically for zn is of a different nature.
01:11:20.194 - 01:11:21.134, Speaker B: Somehow.
01:11:25.354 - 01:11:28.014, Speaker C: Thanks, Daniel. Andy. Rit.
01:11:31.034 - 01:11:35.334, Speaker B: I'm glad someone was in the audience who could give us some more insight here.
01:11:50.254 - 01:12:01.454, Speaker A: Okay, cool. So if there are no more further questions, then I'll just remind everyone that there's a discord server that you can continue to discuss. I think this room just stays open right until the next talk. Yeah, the room will take.
