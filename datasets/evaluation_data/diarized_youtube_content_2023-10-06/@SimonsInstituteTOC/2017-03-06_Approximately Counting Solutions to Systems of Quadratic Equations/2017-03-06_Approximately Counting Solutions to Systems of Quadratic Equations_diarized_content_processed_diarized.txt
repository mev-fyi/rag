00:00:00.480 - 00:00:05.834, Speaker A: Ryan Williams. He'll just take a minute to set up and then we're ready to go.
00:00:09.654 - 00:01:32.024, Speaker B: I guess he turned up the volume. Hi. Thanks to the organizers for inviting me. I don't really consider myself a pseudo randomist by nature, but I have something. It's unpublished, but I told Luca and Omer and other people about it and they said they didn't know how to prove it. I'm going to tell you how to do it. And what's really nice is that, because this is at the beginning of the workshop, if you have any ideas about what else to do with this, there's a lot of time to do it.
00:01:32.024 - 00:02:22.456, Speaker B: So this is about approximate counting for systems of quadratic equations. So the problem we're going to look at is mqs. So you're given some polynomials, p one through P's. Okay. They're f, two polynomials in n variables. Okay. And, well, usually you just want to know, is there some satisfying assignment to all of them? So you want to know, I'll usually just drop the vector notation, just talk about x.
00:02:22.456 - 00:02:56.998, Speaker B: So you want to know, you'll have an x that satisfies all these. So this is Mp hard, NP complete. In fact, we're going to look at the counting problem for which exact counting, sorry, these are quadratic. It's important that these are degree two polynomials. Well, already for degree two, it's np hard counting. The number of solutions is sharp, p hard. We're going to look at an approximate version of this.
00:02:56.998 - 00:03:53.390, Speaker B: So call it approximate mqs. And now we're given these things, we're given a parameter epsilon, and we want to know a value that approximates the fraction of satisfying assignments. So I'll just write it out here. So if you take v minus the probability of a random assignment that all these equations are true, this the differences most epsilon. Okay, so we want to get an additive approximation to the fraction of satisfying a sinus. Now, random sampling can do this, right? It can do it very efficiently. And let's say.
00:03:53.390 - 00:04:39.804, Speaker B: So I've got like s equations and variables, Polyn over epsilon squared time. Okay? So just take order one over epsilon squared, random samples, compute the fraction from that. You're good. Okay, so that's easy. So the theorem here is that this, you can get a completely deterministic algorithm with exactly the same running thing. Well, exactly, I mean, if you allow me, polynomials. So the theorem is that there exists a deterministic, that time algorithm for this approximation problem.
00:04:39.804 - 00:05:12.880, Speaker B: So that's, that's the theorem I want to talk about. And there are some. So the field being two doesn't seem to matter so much. It makes the presentation a lot easier. You have less of a headache. The degree two part, that seems trickier to generalize, generalizing the finite fields not seems so tricky to do. Okay, let me say a bit about the prior work.
00:05:12.880 - 00:06:05.106, Speaker B: So, just to put this in context, the interesting thing here is that the dependence on epsilon is just one over epsilon squared. Okay, so just, this is the best firework I know of. Perhaps you know some more and I'd be delighted to hear about it. So Treveson showed that for approximating the solutions to a D CNF. So think of this is, so it's an and of ors, and each or has the most d variables negated or not in it. Okay, so this is, you can think of this as a very special case of, say, degree two, polynomials over degree d, polynomials over f two. And so there he gets a polynomial time algorithm.
00:06:05.106 - 00:07:11.672, Speaker B: So let s be like the size of it, and he gets an algorithm which has dependence epsilon of the c d times two to the d for some constant c, at least one. Okay, so the dependence here is pretty bad, even for d equals two. So over there we get absolutely actually epsilon squared. So, and so his is a white box in the sense that it has to look at the CNF to get the right answer. And mine is also white box in the sense that it has to look at the system and determine what to do. There are some black box things best known to my knowledge is that of viola, who gives a sort of pseudorandom generator for fooling degree d polynomials. And this gives you a degree d approximation for degree d f two polynomials, for example, that runs in order n to the d.
00:07:11.672 - 00:08:21.834, Speaker B: Well, I mean, degree d polynomial could be that big times one over epsilon, another c times d times two to the d. So again, these dependencies are not so great. One thing I want to mention which will be useful for us is that, so this is from Aaron Kropinsky from 1990, that if you wanted to exactly count the number of solutions to a single polynomial of degree two, then you can actually do that. So degree two, f two polynomial, if you want to count exactly, you can do that in order in cube time. It's mainly, the idea is mainly finding some nice basis change for the variables. So you get some very nice normal form, in which case the number of satisfying assignments is easy to compute. I mean, I won't say any more than that, but this has been generalized to finite fields.
00:08:21.834 - 00:08:58.714, Speaker B: So for any finite field, you can get some polynomial in time algorithm for counting the number of solutions to a single degree two polynomial. Exactly. This was by woods. All right, so what's the idea here? So the dependence on Q's, is it like, you know, if q is huge, like exponential? I'm not sure. I think it might be exponential in Q. I'm not exactly sure. Have to look.
00:08:58.714 - 00:09:33.794, Speaker B: Yeah. So the idea, the first idea, we don't have to do this, I just want to say, but it makes the rest of the acquisition pretty easy to think about. The first idea is we want to count solutions to sort of the complement. So we're going to count solutions to p one vex one or p one sx one. So we want to find solutions that make at least one of these equations one. Okay. Over f two.
00:09:33.794 - 00:10:11.616, Speaker B: All right. So to that end, let s sub I be the set of those points that make the ith polynomial one. Okay, so what we want is to approximate the cardinality of the union of these different sets. Okay. So to do that, I'm going to make a definition. There's probably another name for it, but I really like my name. So we're going to look at an operator on a collection of sets.
00:10:11.616 - 00:10:42.144, Speaker B: So let a one through am be some collection of sets. And we say that this sort of o plus, I guess in latex of the different a sub is, is the set of x. The x occurs in the odd number of the a sub I. Okay, I call this the otter section. Okay. I don't know. Okay, fine.
00:10:42.144 - 00:11:45.444, Speaker B: Do you know, do you know, is there actually a standard name for this thing for two? Yeah, yeah. Outer section. Okay. And then I call this sort of thing the oddity. All right, so the idea is that you're going to represent this cardinality of the union that we want as some linear combination of a small number of oddities of, say, subcollections of deterministically chosen subcollections. So there's going to be some poly s over epsilon squared things we're going to sum up, and it will be, there'll be some coefficients, say c, sub k's or the k different things here. And there will be some, the s of ij something, you know, different things from the subcollection.
00:11:45.444 - 00:12:46.652, Speaker B: And the point is that each of these oddities can be counted in polynomial time. Okay, well, why is that? Well, it's pretty easy to see that if I did this, if I took the oddity of some subsets of this, right, then that's just a set of x such that the sum over the same j's, the same pj, sub I x's. So let me write it. Index these, just some arbitrary sub collection of the polynomials. I'm just looking at the number of solutions that make this particular equation over f two true. Okay, so the idea is I want to approximate this union. Well, the fraction of assignments in this union up to some epsilon, I will exactly count oddities.
00:12:46.652 - 00:13:15.056, Speaker B: So in other words, I will just take some deterministically chosen linear combination of the polynomials in my system. I will count the number of solutions to each one, and then I'll take a linear combination of that. Then that will be additive approximation. And then this is, so what you're looking for is a spark probabilistic polynomial or approximating polynomial. Yeah, I want a non probabilistic. Yeah, yeah. I want a deterministic.
00:13:15.056 - 00:14:05.736, Speaker B: Yeah, yeah, yeah. So it actually will prove something a bit stronger along the way. I mean, this, what this representation is saying is that actually, let's say it off here to the side. If you've got an epsilon approximation, so you're approximating the fraction with an additive epsilon to the number of solutions to, say, s degree d polynomials. Again, over f two, we can always reduce this to making our epsilon, say, slightly smaller epsilon cubed over s squared approximation, the number of solutions to one degree d polynomial. So that's nice. I mean, it's a general reduction.
00:14:05.736 - 00:14:43.864, Speaker B: I mean, it would work. So in other words, independent of the degree d, what I'm saying is going to work. So systems seem much harder to count solutions to than a single one, especially in this case. So it's nice. I hope there's more to say in this direction. Any questions so far? All right, so let me, let me tell you how you formulation of the same result, or it's a different result. I mean, it's basically saying, if you've had this representation, this theorem follows.
00:14:43.864 - 00:15:14.448, Speaker B: Yeah, yeah, yeah, yeah. Like, it will, it will follow because this polynomial in s is actually s squared. And so, like, having this kind of approximation means I get sort of epsilon times one over this total number. So getting an approximation to each of them, which is really that tight, will give us epsilon approximation, the whole thing. Yeah. Do you have a bound on the sum of the absolute value of cks? Yeah, each ck is actually very easy. It's like one over two.
00:15:14.448 - 00:16:12.310, Speaker B: This number, will this imply, like, what I'm wondering is, will this imply a prg? Like, oh, let me tell you how it works and then you can tell me. Yeah, so we'll use, oops, maybe I should use the pin. We use epsilon biased sets. So what are these things? These are really wonderful objects constructed by Nior Nur at first in 1990. So take some subset of f two to the n. It's epsilon biased. If for all x and f two to the n, this thing will closely approximate the parity of, say, a random inner product on x.
00:16:12.310 - 00:17:25.294, Speaker B: So suppose I've got a uniform random r and f two the n, and I'm looking at the probability that this thing is zero. Well, that's going to be within epsilon of the probability over this epsilon bias set of the same. Okay, so this is, so you can think of this epsilon bias set as sort of the output of a pseudo random generator that's fooling degree one polynomials. So linear functions over f two. So this is all like mod two mod two stuff. And what we'll want to use is the fact that these can be very efficiently constructed. So for example, a theorem of Alain Goldreich Hofstad Peralta from 92 says that in fact we can get in polynomial n over polynomial n over epsilon squared time algorithm that will construct.
00:17:25.294 - 00:18:07.362, Speaker B: So we construct an epsilon biased s, and the cardinality of this s is, say, most n squared over epsilon squared. Now, if you were to pick a random collection of s, you can get away with n over epsilon squared tiny bit better, depending on how error epsilon and n depending on each other. But this is the best known explicit construction for what we want to do. What we want to do is have one over epsilon squared error. There are better constructions if you want. If you allow epsilon cubed, you can make this thing. And there's other things, other trade offs like that as well.
00:18:07.362 - 00:19:03.904, Speaker B: But for us, we care about getting this optimal sort of epsilon basically matching random sampling. Okay? And so the main lemma here is, suppose I've got some set r1 through rm. It's a subset of f two to the t. And let's suppose it's epsilon over two biased, and also let a one through at the arbitrary subsets of some finite set. Um, okay. Then I can approximate the union of these things in the following way. If I take one over two m times the sum from I ranging from one to m of various oddities, which are determined completely by these vectors here.
00:19:03.904 - 00:19:43.196, Speaker B: So the point is for every single vector here, I will look at the coordinates that are indexed one, right, they're t coordinates. And I will just compute the oddity of the subsets that are one there. So I'm gonna say j such that sub ij equals one raised of j. Okay, so if I take these two things, this thing is actually less than epsilon. So this is exactly the quantity I'm going to use to approximate this thing. Yep. Oh, sorry.
00:19:43.196 - 00:19:58.108, Speaker B: In fact, I mean less than epsilon times the size of the universe. Thanks. Yeah, yeah, yeah, yeah. I guess I'm like mentally dividing, dividing through by the universe. That's right. Thanks. Okay.
00:19:58.108 - 00:21:19.774, Speaker B: And this lemma follows from the fact that the or function actually has a sparse representation in the Fourier domain. I guess I could do it here. I don't have a whole lot of time left, so I will just say what claim it follows from, and not so hard to think about how this kind of approximate formula follows. So, it follows from this claim that for all inputs, say x on t bits, this is due to the earliest reference I can find is the lawn and Brook from 94. The or function on those t bits is very closely approximated by one over two m. Some of these I's. And then I compute for each of these I's, this j r I j equals one x, sub j mod two.
00:21:19.774 - 00:21:46.598, Speaker B: Okay. So this function here should think of it as taking an integer, okay. And outputting a zero one value. So it takes this integer sum, computes mod two, renders that as an integer zero one integer, and sends that out. Now you're computing this sum, okay, it's over m things. Okay. And I'm using this two over m to kind of average out what's going on.
00:21:46.598 - 00:22:13.742, Speaker B: Okay, the idea is that if the vector is all zeros, the or should be zero, right? All these sums mod two are going to be zero. If all the bits are zero. Okay, so it's zero minus zero, and it works perfectly then. Okay. Now suppose there's a bit that's set to one here. Okay. Then if I chose a random.
00:22:13.742 - 00:22:44.836, Speaker B: So there's a bit in here set to one. If I chose a random vector and took an inner product with it, it's probably half of being zero. Exactly one half. Okay. With epsilon biased, I'm off by epsilon from half. All right. So this quantity here, the sum is going to be between m over two minus epsilon, m over two, because I'm using whatever epsilon over two biased in my set and m over two plus epsilon m over two.
00:22:44.836 - 00:23:17.632, Speaker B: I'm dividing by two over m. So this is actually one minus epsilon, one plus epsilon. Okay. So even when, so when the or is one, this is actually approximating it to an epsilon as well. Okay. And the way you would get this limit from the claim is just imagine rewriting what the union means the union means the or of some indicator variables on all the elements here. So it's like you sum over all of the elements in the union and you take the or for every single set, whether that element is in that set or not.
00:23:17.632 - 00:24:01.650, Speaker B: Okay? And so you can write the union as just a sum of like or functions being applied, right? So then you would, using this lemma, say, oh, well, that's actually equal to the sum over all the elements of this thing being applied. You basically exchange the sums until the sum over all elements is within here. And when the sum of all elements within here, this is actually computing exactly these oddity things. So I think that's kind of all I want to say about these limos and claims. I hope it's somewhat clear. Let me sort of tell you what the algorithm is now. So the final algorithm after this dust is clearing.
00:24:01.650 - 00:25:04.274, Speaker B: So you're given an MQs instance of these p one through p's, each in n variables. So think of this as a vector of in bit variables. And so the first thing you do is you build these epsilon over two biased sets, but it's over f two to the s, okay, the number of the polynomials. And then this epsilon bias set will have a bunch of vectors in it, not too many. But these vectors will determine which of these sort of subsets of polynomials you will sum together and count their solutions to. So then you will count the number of solutions to size of s. So this size of s degree two polynomials, each of which is determined by some subset, some, basically some, mod two of some of these.
00:25:04.274 - 00:25:37.322, Speaker B: And then this will give us an estimate, say w of the union. And then we're just going to output one minus that estimate. Right, because we're actually getting the union of the, yep, sorry, I'm confused. Is it the size of s many degree, two polynomials, or the number of elements in the epsilon biased set? Many. This is the number of elements in the epsilon bias set. The set is s. The set is over.
00:25:37.322 - 00:26:22.514, Speaker B: Oh, sorry, sorry, are you, you're getting confused about notation. So the s of I means the set of solutions to this polynomial, the set of points that make this polynomial one. And then t is a set of vectors in s bits. And so each of those vectors will determine some subset of those polynomials to sum together mod two. And then you count the number of solutions to those things, to each of those things. And so the point is, this thing is right order s squared over epsilon squared of what we have here, and each of these estimates. So each of these counts will take us order n cubed time.
00:26:22.514 - 00:26:40.634, Speaker B: Okay. By the fact that for a single polynomial, we can compute that. And so we get polynomial in n in s divided by epsilon squared. But note that. So we had this little trick where we were wanting the number of solutions. This we just look at the complement. So then we started working with.
00:26:40.634 - 00:27:10.626, Speaker B: Or you could get a polynomial for Ann directly if you wanted. But it's a lot messier to write down. There's like some minus ones and like other weird sums you have to carry around. So this sort of makes it a bit cleaner. I have like 1 minute left. I have some remarks, I guess. So, in retrospect, what's going on is kind of similar to lineal and Nissan.
00:27:10.626 - 00:27:53.630, Speaker B: So they had some work on approximate inclusion exclusion, where they showed that the union of a family of sets. So the cardinality union of family of sets can be, well, approximated by sums of small intersections. So up to like square root of the number of sets. And the way they did this is they just applied the fact that over the reals, you can get approximating polynomials due to Nissan and segue for the or function. So this is kind of taking that but getting a different representation instead of intersections xor's. But here it's really nice because xors of f two polynomials are just f two polynomials. I think.
00:27:53.630 - 00:28:07.714, Speaker B: I don't know. I think it works for finite fields, but it's a longer talk. Let's see. Yeah, we can talk later if you have ideas about it. Thanks.
00:28:15.614 - 00:28:20.104, Speaker A: Thank you very much. I do think we have time for one or two questions. Quick questions.
00:28:20.444 - 00:28:53.240, Speaker B: So, Ryan, what about iodine? Sorry, what? Iot degrees? Oh, yeah. So it would suffice to say for a degree three, to have an approximate, approximate number of solutions to a degree three thing. Actually, you can just take what Viola has. So this thing here. Okay. And if you want to say epsilon prime approximation, you would just plug in, say this quantity for this epsilon, and you get something there. So Mac and can write out what it is.
00:28:53.240 - 00:29:32.144, Speaker B: But you can also just take this and plug it in and see it's like s over epsilon to the order d times two to the d for like any degree, any system of degree d polynomials. Yeah. Yeah. Then you can just use the same epsilon, the same epsilon. Well, if you want to apply what I just said, when you're just doing a pseudo random generator, there's no reason to do this. Different composition. If you just understood the average of the things over all sets.
00:29:32.144 - 00:30:02.072, Speaker B: I see, I see, I see. I mean, I guess the point is to end up with an epsilon approximation at the end. I need a close approximation here because I'm summing one over this many times epsilon stuff here, and I want them to, like. I don't want the error to accumulate some of the sizes of the coefficients. Yeah. I mean, can't you reduce the higher degree case to the degree two case? But since you're looking at systems of polynomials. Yeah.
00:30:02.072 - 00:30:17.352, Speaker B: Introduce new variables, you can to be quadratic functions. Yeah. That messes up your fraction though, right? Because you start out with, you start out with two to the n total assignments. Now you get to the end. Yeah. It'd be really nice if I could just like apply np hardness reductions. Yeah.
00:30:17.352 - 00:30:27.844, Speaker B: If I were. Yeah, yeah. If it were a different approximation problem approximation, that would work. Yeah. For additive approximation, it totally destroys it because you're epsilon times two. The end. Like stuff getting stuck in there yet.
00:30:29.464 - 00:30:36.264, Speaker A: Great. We'll reconvene at eleven outside. Let's thank Ryan again.
