00:00:00.240 - 00:00:14.110, Speaker A: End of the hierarchy was bounded depth Frege or Ac zero Frege. Our next speaker, Kili Andrisse, will tell us about the latest news in some lower bounds for bounded depth Frege. Approved. So, Kiliman, please take it away.
00:00:14.182 - 00:00:28.130, Speaker B: Yeah, sure. Thanks for inviting me. This will be a bit more technical than previous talks. I'm sorry about this. Then. It's trying to work with Johan Hostado. Uh, one of my.
00:00:28.130 - 00:00:34.370, Speaker B: I don't know, is he my ex advisor or school advisor? I don't know, but he was my advisor until, like, last fall. So.
00:00:34.402 - 00:00:34.778, Speaker C: Yeah.
00:00:34.866 - 00:00:38.090, Speaker B: Okay, so, just to give you a picture again of where we are.
00:00:38.162 - 00:00:38.322, Speaker C: So.
00:00:38.338 - 00:01:10.982, Speaker B: So, right, we have resolution here. And then we have these, like, hierarchy of logic proof systems. So we have resk, and then we have res log n. Then we have found the depth Frege and extended Frege. Then we have, like, one semi algebraic proof system, say, cutting planes. And here we have, like, the algebraic proof systems, polynomial calculus. And if we go really crazy, we can consider proof systems, such as the ideal proof system, which are very strong, but you really just know, like, lower bounds for CNF formulas for these somewhat weak proof systems.
00:01:10.982 - 00:01:52.748, Speaker B: Though I was very happy that Robert referred to bounded depth fig as a strong proof system. So I'm going to show you how to prove lower bounds for this strong proof proof system under depth frigate. So, yeah, I'll first introduce, like Frege, state our results, and then try to give you some idea of how to prove such a result. Okay, so, how should we think of the frigate proof system? So it's just a sequence of boolean formulas over some basis we'll use or. And not. But. Because I don't want to torture you, I'll use this short tent called and for not or not.
00:01:52.748 - 00:02:08.860, Speaker B: And then we can, like, write out like, a Boolean formula here. So it's tree like if we ignore the leaves where the variables live. And. Yeah, you can assume that it's or not, or end. And so on alternating. And this circuit happens to compute XoR. Okay.
00:02:08.860 - 00:02:49.566, Speaker B: Or no formula. So, yeah, and then each line, we have to derive it or it's a clause. So, either we take a clause from our original CNF, or we derive a new Boolean formula from two or a constant number of previous Boolean formulas. And for this, we have to use these derivation rules. It's not so important what they are precisely like somehow, as long as every derivation rule depends only on a constant number of assumptions. Like all these systems, polynomials simulate each other. But this is one proof system that we may consider it's like somehow the first four rules are kind of boring.
00:02:49.566 - 00:03:50.544, Speaker B: They just manipulate some of the structure of your formula. And really the interesting rule is this last one, which is a generalization of the resolution rule. So what does it say? Well, if you have a formula of the form q or p, where q and p are formulas themselves, and you also have to formulate not p or r, then you can actually resolve over p and not p, and then you can derive q or r. So you should really think of this as like a generalized version of resolution, where we now can branch over more complicated objects. And because we want the reputation that the CNF has a satisfying assignment, we want that the final line is constant, false, so that there are no satisfying signs, some standard size measures, or measures that we consider. So we have the length of a proof, which is the number of formulas occurring. In your proof, we have the line size, which is the maximum size of any formula in your proof.
00:03:50.544 - 00:04:44.654, Speaker B: Then we have the size, which is just the sum of the sizes of all formulas occurring, and the depth which is the maximum logical depth of any formula occurring. Then like, I mean, one of the long standing open problems is to actually prove like super polynomial length lower bounds. For, unfortunately, we really don't know what to do about this or how to prove such a, such a lower bound. And what we do is we somehow restrict all our proofs, namely, we restrict our proofs to use like only say constant depth formulas. And that, like to demonstrate that this is a real restriction. Like if you take this circuit and you request or require that it's more shallow, well, then the size explodes, right? So like restricting the size of the depth of each formula may blow up the proof considerably.
00:04:45.114 - 00:04:49.654, Speaker C: Okay, when you calculate depth, you're allowed to use this.
00:04:51.844 - 00:05:36.804, Speaker B: Yeah, like I'm assuming I've unbounded Fannin, because otherwise, like in constant depth, I can only talk about many variables. Yeah, it's really, it's really the logic that number of or not alternations. Any other questions? So, yeah, so I also want to prove a little bomb for particular. And we will consider plating. The formula is defined over graph. Connect the graph. We have boolean variables for every edge, and every vertex is associated with an axiom claiming that an odd number of edges incident to each vertex is set to one.
00:05:36.804 - 00:06:14.234, Speaker B: Okay, so in particular, this is a satisfying assignment, while this one is not, because it like, violates the vertex axioms at these two points as well. There an even number of edges set to one x. It's not so hard to show that the Zetin formula is satisfiable even only if the number of vertices is even. And so from now on, we'll just assume that our graphs always have an odd number of vertices. And in fact, I'll fix the graph that we'll consider. We'll consider the two dimensional n by n torus. What do I mean by this? Well, it's this n, like this mesh defined over, over a two dimensional doughnut.
00:06:14.234 - 00:06:52.490, Speaker B: I'll not draw this picture from now on, but I'll put it into the plane and actually I'll drop the edges wrapping around, because it's impossible to draw. So if I draw a grid, it's understood. Now that I actually talked about the torus. And real quickly, let me just tell you what the pigeonhole principle is. Well, it claims that n plus one pigeons feed into n holes, so that every pigeon has its own hole. Formalize this by introducing boolean variables for each pigeon hole pair with the intended meaning that if it's set to true, then pigeon p goes to hole h. Every pigeon claims to fly at least to one hole, and every hole is occupied by most one pigeon.
00:06:52.490 - 00:07:30.932, Speaker B: So just really quick, I suppose everyone knows this. Okay, so some history. So what is the game? Improving bounded that three lower bounds. So we want to prove super polynomial refutation size lower bounds for as deep a proof system as possible. Okay, so the first lower bound was by Aetai who showed that for any constant depth trigger proof system, these repetitions need to be of super polynomial size. And then this d was not explicit. And follow up work then made it explicit to log sar of n, still not very strong.
00:07:30.932 - 00:08:17.700, Speaker B: But then this, these works by Krajiek, Putlak, woods and Pitasi beam and actually managed to extend this up to that login. And we were really stuck at this step for a long, long time. And I'm going to ignore for now the developments on the staking side. And now really since the last few months hosted managed to actually push this log round up to that login overload on the sapien side. If you look at the satin formula, like there was Spencer and worker hard in the late nineties who considered Satan formula and they showed it up to depth log n. This was after these papers. And this was then by a really nice work was extended to like depth login.
00:08:17.700 - 00:08:39.074, Speaker B: And this was the first one to get this depth. So I. Yeah, it's really this work that got the first time like this kind of login depth. And then Hostad actually also closed this line of work off by his login. Or log n depth. And these are essentially all the lower bounds we have for bounded depth. Three game.
00:08:39.074 - 00:09:20.832, Speaker B: If we restrict ourselves as cnfs and they're proof by the same technique, we have one technique to prove it and nothing else. So please develop new techniques. It's unbearable. So, yeah, but. Okay, so let me go, like, tell you what Hossad actually proved. So he showed that any frager repetition of the zetian principle defined over this two dimensional n by n powers, if these gravitations are restricted to depth t, then these require size exponential n to the one over 58 d. And you cannot hope for a significant improvement on the dependence on d in the second exponent.
00:09:20.832 - 00:09:55.270, Speaker B: If you would do this, we would get regular rounds. Okay, so at least in my mind, this was all done. Like, I don't see any way how to improve this interest in an interesting manner. And then Pitasi, Ramakrishnan and Tan came along and asked this beautiful question of, well, we can restrict the depth of each line, but we could also restrict the size of each line. Right? So what happens then? Can we then prove actually stronger low bounds? And you actually managed to do this. I think this is really cool. I was surprised that you can do this.
00:09:55.270 - 00:10:53.974, Speaker B: So if you restrict the lines not just to depth d, but also to line size n, then you get this exponential in n divided by some factors, just to illustrate the parameters, like if you restrict the line size to be polynomial in n, and this lower bound is like almost fully exponential in n absolute depth, like square root log n, whereas the previous lower bound that I showed you, as soon as d is a growing function in n, then the lower bound is the form exponential n to the lower one. So these lower bounds are much, much, much stronger. Okay. And they conjectured that this is actually not the end of the story. And one should be able to improve this lower bounds to be exponential in n over log entity. Okay, so, yeah, our result, our main result is basically resolving this conjecture. And we showed that indeed, like, like such reputations require this length.
00:10:53.974 - 00:11:23.228, Speaker B: And just to give you again, a feeling for parameters. So now we can go in quasi polynomial line size as well as up to depth log n over log. Nice. And we still get this almost fully exponential normal on the length of repetitions. Yes. Why b minus one from the previous slide? Because circuits computing x or. Oh, sorry.
00:11:23.228 - 00:11:32.422, Speaker B: Yes. So the question is, why do we have a b minus one here in the program? Because the lower bound, this is kind.
00:11:32.438 - 00:11:33.502, Speaker A: Of the tight upper bound.
00:11:33.558 - 00:12:22.264, Speaker B: Yes. We don't have a real syntactic proof, but we have, if you consider somewhat semantic proof system, then you can actually prove that this should be, it comes really from bounded depth xor lower bound, say, where you also have an exponential in n to the one over. Yeah, I believe we get d minus one here if I write the polylogn here. Okay. And along the way, we actually improve this size lower bound of hosta to be exponentially n to the one over d minus one, which previously was n to the one over 50 ad. It's a big improvement. But this should be now tight.
00:12:22.264 - 00:12:47.444, Speaker B: I don't have proof that this is tight. It should be tight because this is the size that you need in depth t to represent an xor over n variance. And I'm suppressing poly law vectors in this one question matching. No, we don't have any matching. I mean, we have a construction of the semantic proof which achieves this, but. Oh, but it's semantic. Yeah.
00:12:47.444 - 00:13:38.544, Speaker B: So all the lines here are really formulas, right? Yes. You're not like allowing, you could imagine like bounded depth applications of the extension rule, and that could shrink it even more, right? Yeah, no, but our lore bound also applies for circuits. Oh. One of the only problems to actually distinguish this. But I mean, because we have a polyblog here and here I'm also having some polyvo. But like, we cannot see this difference yet, but like, it would be nice to actually get like expansion b and z. But yeah, yeah, I, I don't know how to combine Rossman's, like, notion of this type.
00:13:38.544 - 00:14:22.808, Speaker B: I have no idea what. Anything else. Okay, some ideas how to prove size robots. So, um, yeah, so, so, yeah, we talked already about the bounded depth circuits, so it shouldn't be so surprising that we actually use machinery developed to prove bounded depth circuit load bounds. So let me like really, really like give a really rough outline of how this argument. So what do we do? Well, suppose you're given a small circuit of depth p that computes 30 on endpoints. Then we want to hit the circuit C with the restriction row that keeps each variable alive with probability p, meaning it does not assign it.
00:14:22.808 - 00:15:20.534, Speaker B: And with probability one minus p, each variable independently sets it to zero or one uniform address. And then you want to argue that, oh, under this restriction, my circuit actually shrinks in depth by one. Then I repeat this d times, and I'm left with the constant circuit computing constant. But hopefully my function is still non constant. The main technical step in actually completing this proof is to prove a switching lemma. Let me tell you what the switching lemma does. So what the statement of the switching lemma now guarantees you is that if you take a DNF with bottom fanning bounded by t, and you hit it with a restriction row, then you can write this, rewrite this formula as a CNF with bottom planning at Moses, so f under row can be represented as a CNF with low bottom fanning.
00:15:20.534 - 00:16:10.734, Speaker B: And then there is some failure probability, like over the randomness of your choice row, that this actually does not happen. And by a classic result of all that, you can actually download this by, by something that is independent of n. I still think this is a bit surprising. And yeah, then just how do you apply this? Well, you look at the bottom two layers, you switch them applying this switching lemma, and then you can collapse those layers and you're left with that d minus one. And so we want to apply this machinery to frigate proofs. So how do we do this? This is actually not how the proof goes, but let's pretend we can do this. So we hit the Frege with a restriction row, so that the depth of every line shrinks by one.
00:16:10.734 - 00:16:38.724, Speaker B: This is not all. What I also have to guarantee is that I don't suddenly introduce contradiction in my initial formula. If I hit my formula by this restriction, I don't want to have contradiction already in my formula. Then it becomes very easy to repeat. So I additionally need to guarantee that I can maintain the structure of my original form. And this is somehow where all the complications arrive. You really want to go from a large sapien instance to a smaller.
00:16:38.724 - 00:17:19.289, Speaker B: And yeah, again you prove a switching lemma. I carry all the details, but the original proof of hosta somehow managed to get this failure from bluid with this S 27. It turns out that you need to do union bound over all subformulas occurring in your proof. So you need this to be less than one over the inverse of the size of your proof. And this S 27 really like caused this ugly second exponent like expansion NP one or 58 d. And our new proof now manages to replace this by a log n. And then yes, again, skipping a few steps, we get these expansion.
00:17:19.321 - 00:17:32.273, Speaker C: And yes, is it true that what you've just said, that if you introduce contradictions, it becomes easy because you can have hard contradictions?
00:17:32.353 - 00:17:42.533, Speaker B: No, I mean like, you violate the maximum of the table, like you said, even number of variables to zero next to one variable.
00:17:45.774 - 00:17:48.194, Speaker C: It's the fact that it's an easy contradiction.
00:17:49.334 - 00:18:48.234, Speaker B: Yes, I have to like, maintain the hardness or supposed hardness of this problem. Okay, so, yeah, so this was for the size of our bound and then I also should maybe tell you something about the remainders of how one proves this. So here we really follow piracy. Ramekrishna and Tan, they show that you can use multi switching, another notion from switching terminology to prove these frigate trade offs, which is really surprising because multi switching was devised to actually get these correlation after bounds for circuits. But it turns out that this is also useful tool to prove these fray gate trade offs like line size versus length results. So we have to prove multi switching. So I'll try to explain to you again what the main, what the statement is, but I will not prove it.
00:18:48.234 - 00:19:38.854, Speaker B: So first, the switching mega guarantees you actually something better. It does not just tell you you can go from DNF to CNF, but it tells you that you can go from a DNF into a bounded depth decision tree. So what is the decision tree? You query a variable x and depending on its value. Now, if you remember from the proof outline, we had to do this union bound over all subformulas occurring in your proof. And you want to not do that, you want to do something smart. So what is a reasonable thing to do? Well, you could consider many DNF's together at once, right? And then lies somehow, then having some object and then say something. And what a multi switching lemma does is it tells you that if you consider many DNF's, then you can under this restriction role represent them as the following.
00:19:38.854 - 00:20:31.650, Speaker B: You represent them as a partial common decision tree of bounded depth. And then each EMF that you consider has like these small decision trees hanging off this partial decision tree, so that the combination of this large tree and these small trees actually decide your DNF under restriction rule. Okay, so for different DNF's, you have these small decision trees like Bangladesh. And then now again, you can analyze the failure probability of what's happening, and it turns out that you can bound it by this expression, which is essentially the same failure probability as you have multispitch. You might not accept this factor, which is due to the constraint. So it kind of catches. Okay, so I'm, how much time do I say?
00:20:31.682 - 00:20:32.370, Speaker A: Four or five minutes.
00:20:32.442 - 00:21:18.164, Speaker B: Four or five minutes. Okay, so, yeah, but like all the fun in these proofs happens in how you design this restriction row. But like this restriction row, how you choose this is like really important. So I at least want to tell you what this restriction is and what it does and giving you an idea of how it works. So we use the same restriction, or essentially the same restrictions as hostile and KLK paper. And yeah, just to remind you, what you want is we start with the Satan instance over a large bit and want to end up with the states instance over a smaller bit. And what we do is we hit the proof with a f fine restriction, meaning that we every original x variable we either set to 10 or a new variable or a negation of either.
00:21:18.164 - 00:22:11.094, Speaker B: So how do we come up with such a restriction? So first we hit the entire proof with a restriction, setting every edge one or zero so that the vertices should survive, have an even number of ones next to them and all others have an odd number of ones next to them. And then we want to really represent this as a variable y for five each path. And what we do is we replace like negative things by the negation of the corresponding variable and positive things, positive things along this path. And you do this, you get this restriction. And this is the restriction that we use. And it has this nice property that any vertex on the path you can verify that under any assignment to the y variables you actually have an odd number of ones next to it. And these vertex accents have still the same form of an axiom.
00:22:11.094 - 00:22:14.950, Speaker B: So you're really left with this smaller instance or description.
00:22:15.102 - 00:22:18.914, Speaker C: What is defined restriction? And this seems like just a restriction.
00:22:20.374 - 00:22:22.766, Speaker B: Yeah, yeah, it's the restriction with substitution.
00:22:22.950 - 00:22:24.474, Speaker C: So why is it defined?
00:22:25.534 - 00:23:14.012, Speaker B: Well, I mean it has to respect negations or, I don't know, it's a name that people have chosen and I'm just reusing this name. I do not fully agree with names, but it's, so this was the restriction. Now I have to tell you how to actually choose subject restrictions. So we start with this large grid and we want to end up with this mod and you could just like pick any like subgrid and choose this as a restriction. But the issue is that you get a lot of dependence between nodes, like which one you choose and so on. And you want to prevent this. So what you do is first you like cut your whole grid into smaller pieces and you guarantee that you pick one vertex from each such subgrade.
00:23:14.012 - 00:23:51.622, Speaker B: So then you get independence in a sense, which is important. And then again as before, we pick a solution to the formula where the blue nodes now have an even constraint. So this is now a satisfying instance. And then we connect them by some paths in a very careful manner. It's not important what this precisely is. And then we connect them and we end up with this like smaller pores. And actually the main issue with this is that if you look at this restriction, it's very easy to find these nodes, because these are just the nodes that have an even constraint.
00:23:51.622 - 00:24:26.394, Speaker B: All others satisfy the odd constraints or at a node. Okay, so we want to obfuscate this, and therefore we need an intermediate restriction to hide where these lunar are. What you do is like you pick two vertices in adjacent subsquares and you connect them by path. You flip the value along this path so that these also have an even constraint. And then we do this a bunch of times, you're then somehow left with this restriction. Or let this pretend to have this restriction. And the main property that we have is now that it's difficult to tell whether the blue node is this one, this one, this one, or this one.
00:24:26.394 - 00:25:19.636, Speaker B: So somehow we use this in and yeah, the key difference, like for anyone who has ever read these proofs, the key difference is that previous restrictions had roughly as many such vertices in each subsquare. And we now have lower. One limitation of this technique, very annoying, is that you need to assign a huge number of variables for one step. And so this means that we can only apply this technique to these formulas that are almost satisfying or like minimally unsatisfying. And we cannot apply them to like formulas that are only locally satisfying, like say random Cnx or something. So yeah, that's it for me. So just reiterate what we proved.
00:25:19.636 - 00:26:20.664, Speaker B: So if you have line size n and that's t, we get this exponential n over log n to d, and if you only switch to depth d, then we get this exponential n to the one over d. Some open problems, I think I listed them like from easy to hard. Let's see. So the first one is that our lower bound is somehow exponentially the square root of the number of variables, because this formula has n squared many variables. Can we actually get a lower bound that is exponential in n to the one over d with n variables? So like improving like a square root, and like an obvious candidate would to try this, at least to try this rotation over its pattern. I think this might be approaching the next question, which is due to Jubal filmus, actually, and it's a question that you asked before, is like, our lower bounds cannot distinguish whether you have formulas or circuits of load depth on each line. And can you actually make this like distinction in a sense, like, yeah, prove this exponentially d times n to the one over one.
00:26:20.664 - 00:26:36.024, Speaker B: And then, yeah, like really nice problems, but I have no idea what to do. Proof that regular bounds for like supposedly hard formulas, such as like the truth table topology, cleek, or random scenes, I just don't know what to do.
00:26:54.284 - 00:27:37.114, Speaker C: Is there really a way to actually prove these constant depth regular bounds without the proof? So just to have specific properties of switching dilemma and the topology unique without discussing proof complexity, well, you just have a set of properties that you need your switching to abide. That's it. And then this basically reduce the problem of constant depths to switching them on to a circuit. Is it correct?
00:27:39.814 - 00:27:47.954, Speaker B: If I understand correctly, the question is whether we actually reduce the problem of truing on depth regular bonds to the problem of.
00:27:50.454 - 00:28:11.094, Speaker C: No, just is there a way? My understanding is in constant depth regular proofs, at least the way hasta did it, you don't need to analyze the proof, you just need to have a set of, of statement of properties for the switching level. And then you say from known result K evaluation and so forth, I get the load.
00:28:11.174 - 00:29:08.660, Speaker B: Is it correct? Yes. Okay, so, okay, now I think I understand the question. So the question is whether we need to analyze the structure of the proof in order to apply our switching number. And no, we do not like we just switching them on by better analysis for like say circuits versus formulas. Would then some proof theory need to come into it, or do you think a strong enough switching limit would just be able to tell the difference like the raw spinning style? Robert is asking whether such proof there had to come in for this second of. I really don't, I mean, just trying to apply this Rossman like notion of time. Do you think there is some kind.
00:29:08.692 - 00:29:21.556, Speaker C: Of a super critical regime where if you restrict all of the parameters too much, the other parameter go out, or is everything okay?
00:29:21.580 - 00:29:52.274, Speaker B: So the question is whether at least in training you can balance that. We can also do this bounded trigger. Yeah, I believe, yes. Okay. Yeah, I'm not 100% sure. I know you can do it for trigger, you have to pay a little bit for the depth, like go up a little bit.
00:29:53.814 - 00:29:56.834, Speaker A: So let's thank the speaker again.
00:30:00.974 - 00:30:01.286, Speaker C: And.
00:30:01.310 - 00:30:13.086, Speaker A: Then we convene at 02:00 p.m. for an afternoon of more applied talks on the algorithmic side of continuing our discussion.
00:30:13.110 - 00:30:14.854, Speaker B: Of algorithmic aspects of this.
