00:00:00.570 - 00:00:43.650, Speaker A: Yeah, we will have a section on it, I think after the first, not sure when in the program, but we will introduce that. But if you in parallel want to, I don't know, already, publish some funny pictures there or whatever, then please do and they will be shared later. So right now you can kind of just get some faucet, some shadow XD from a faucet and. And you fit. I know we haven't started the call yet, but I think the reality of the value that blobs are going to bring hasn't really sunk in yet. We're just not ready for it. So exciting.
00:00:43.650 - 00:01:15.770, Speaker A: Okay, we are live on YouTube, so I will hand it over to Armand to kick it off. Hello everyone. Thank you for your patience for the beginning process. So welcome to Nosischain Dankun hard fork call. My name is Arman. I'm working at Gnosis as a validator communication lead. We are delighted to have you here today and we are experiencing the Danken hard fork altogether.
00:01:15.770 - 00:01:57.666, Speaker A: In this program we will both experience the hard fork and also engage in discussions about the various aspects of gnosis chain with our guests. Firstly, let me provide a brief overview of the program flow. Our first discussion will be about client diversity. We are hosting amazing people for this panel. The second discussions will focus on L two on gnosis chain and we will discuss L two landscape on gnosis chain. And we have an amazing guest for this discussion. In this part, Philip and Dave will introduce blob FM.
00:01:57.666 - 00:02:47.990, Speaker A: I am super excited about the Danken creative works and our fourth discussions will be about shutrise between chain Shutter Network team will join us and we will explore encrypted mantles on gnosis chain. And finally we are adding the artwork together. And of course we will have a special club for this call. And now let's move on. The first discussion and it will take 20 minutes and I would like to introduce our guest. Our first discussion, as I said, our first discussion will be about client diversity. And our participants are Nixon from its taker.
00:02:47.990 - 00:03:28.466, Speaker A: Superfys is an ethereum decentralization advocate, Jiom from yet Ahmed Pitar from Nethermind, and of course lion. So I will hand it over to Nixo. Yeah, let's get started. Hi, I'm Nixo. So I'm the executive director of Estaker. Estaker is an organization that supports solo stakers. Advocates for solo stakers, helps educate them, supports any tooling and infrastructure that helps them figure out how to run and maintain and been.
00:03:28.466 - 00:04:25.314, Speaker A: Estaker in general has been an advocate of client diversity, which is something that we're actually trying to move to call supermajority risk recently, because client diversity sort of implies that the word diversity comes with all sorts of political, economic, or social connotations. And so supermajority risk really highlights the self interest that's involved in it. It's not necessarily like, an altruistic endeavor to not run a supermajority client. Yeah, that's me. Superfizz, you want to introduce yourself? Hey, yeah, I'm superfiz. I've been obsessed with Ethereum, the EVM, since 2014. I began Estaker, I think, in 2017, 2018.
00:04:25.314 - 00:05:15.388, Speaker A: I was very lucky to welcome Nixo on board, and I've sort of gone off my own direction for a while. But I'm so proud of the leadership she's provided to Eastaker. And yeah, I obsess over the value of these chains, Ethereum, gnosis, being grounded in decentralization, like anti capture, capture resistance. And so anytime I can share a perspective on the value of decentralization, I'm glad to do that. Jill, are you on the. I am. I mean, I assume you were talking about me, so.
00:05:15.388 - 00:05:55.412, Speaker A: Hi, I'm Guillaume. So yeah, no one gets it right unless they speak French. Don't worry about that. I mostly work for the guest team as part of the Ethereum foundation, and my focus is on Virgo trees. But I also work on bringing go Ethereum to dosys chain, making Goetheorem the minority client. It's been a dream of main net, so we're making it happen right now on this chain. And yeah, I just passed 26 million blocks replied.
00:05:55.412 - 00:06:56.718, Speaker A: So it's happening. Lion, are you on the call? He's not. And also we have Ahmed Pitar from Nethermind. I think he's not on the call right now. I can quickly also jump in to fill maybe lines or perspective. So, yeah, I think Superman, I like your reframing of the problem into supermajority risk. We, of course, have the little bit the opposite situation where gas is even not yet available.
00:06:56.718 - 00:07:58.594, Speaker A: So nosis, we have kind of the, I guess. Well, in that case, nethermind is our supermajority risk. So think the same can be said for ethereum. So I think everyone in gnosis is grateful for nevermind, and I think same for guests on Ethereum. So I think when we talk about those initiatives, yeah, it's simply important to understand what the risk is. The risk is simply that if a supermajority client and I think, please correct me if I'm wrong. So that means 66% the client that has more than 66% has a bug, then the chain would finalize on a state that is malicious or that is buggy, and that's simply a very painful situation to be in, because then it's really unclear how to deal with that.
00:07:58.594 - 00:09:38.142, Speaker A: So if no client has 66%, so just whatever, 60% or something like that, then the situation would be much, much better, because then the chain will still split, but none of the two chains will finalize, and then it's much easier to manually intervene and basically say, this is the right one and this is the wrong one, and kind of, please, every validator is here, either switch or takes this emergency bug fix or whatever, and then there's a very high likelihood that the whole outcome or that kind of within a day or something like that, or probably less, the situation is fixed, everyone is on the same chain, and kind of in retrospective it will be an unfortunate 24 hours. But that's essentially it, and the problem is over. So I think that is really a goal worth, that's simply an important goal. And I think to the two of you, how close are we on Ethereum? Probably still have the supermajority situation for guests, but getting better. Do you have recent numbers? It's really hard to tell. Our recent numbers are fuzzy because they're based on survey data and extrapolation from that survey data, because everybody who doesn't report, you just have to kind of assume what they are running. And if we assume 100% of the supermajority client, then we get to above 66%.
00:09:38.142 - 00:10:48.230, Speaker A: But if we assume like 80% of those non reporting entities are running the supermajority client, then we're just below 66%. So hard to tell and really excited for I forget what the update is. It's an update to the engine API that reports the execution layer data to the consensus layer and puts it in the graffiti. And so it sort of becomes opt out data instead of opt in. And opt out data is going to give us a much clearer picture of what the current landscape is. Does anyone have an estimate? So maybe for perspective, so onosis chain again, we currently likely have nevermind, or very likely have nevermind as a supermajority client, because the only second client that is right now available is Aragon. But yeah, together with soon gas, then we absolutely should strive for this goal to get the share of gas and Aragon at least combined to this at least 33%.
00:10:48.230 - 00:11:53.866, Speaker A: So to get Nethermind essentially below 66%. And yeah, also agree having that data more, somewhat more verifiable ish. It's super interesting to have gnosis be in the same position, but with a different client because it really highlights that it doesn't matter what client it is, it's going to be a problem with any client that gets like first mover advantage and is really robust to begin with. Hey, can we back up and kind of talk about why this multi client architecture exists? Because I know people coming from other chains are not going to really understand why we have this problem. And it does in some ways feel like a made up problem. So Nix or Martin, does anyone want to describe why we have this multi client architecture and why we're not just doing it the easy way? Go for it. Sure.
00:11:53.866 - 00:12:56.250, Speaker A: You're fine, Martin, go mean to me, it's really one of the. Sorry, it's really one of the things that made Ethereum stood out, because in a way, in my view, it's all about decentralization. And that means that there shouldn't be a single entity that has too much control over the network. And it's quite complex to kind of really describe who are the people who have control or who are the entities who have control over ethereum. And that's good because there's a balance between exchanges and core developers and foundation and users and ESA holders and stakers and so on and so on. So without client diversity you can end up in a bitcoin like situation where you have essentially just one entity or small group developing the only client. And that is of course also a form of centralization.
00:12:56.250 - 00:13:35.938, Speaker A: So then you essentially have four or five guys with GitHub access rights that can more or have extremely high leverage about the chain and ultimately too much power. So I think bringing their diversity or kind of having different teams with different strategies brings resilience, brings decentralization and just makes the network so much stronger. I see Joe is raising the hand. I think Mark was first. Yeah. Hi, I'm Mark from Nethermind team. I've been working on Nethermind client and different hard forks in Ethereum.
00:13:35.938 - 00:14:31.870, Speaker A: So yeah, I agree with what Martin said about decentralization and that the one entity, one team would control basically everything. But there are a few more technical things why client diversity matters, I would say so. For example, if you do not have client diversity, then your bugs will be consensus. And with client diversity we have a feature that we have at least detection, that something happened, next thing is testing. So we use this feature a lot, that we can compare results between different clients. So if we know that different clients giving us more or less the same result. This is very good for testing.
00:14:31.870 - 00:15:19.020, Speaker A: And there are more things, even not consensus related. Of course, it might be insider attacks, et cetera, but maybe not about this. For example, if we have only one client, then any issue in that client is risky for the network. So it's not only consensus bugs, but for example, clients might have memory issues, or malicious peer might do something wrong. So for example, connecting to the peers and causing out of memory. So yeah, with many implementation, we are much more decentralized and much more stable. Cool.
00:15:19.020 - 00:16:35.334, Speaker A: So just wanted to add a little historical color. Client diversity started unintentionally, and then it became intentional. It started because Vitalik was coding the Python client, Gavin was coding the c client, Jeff Wilkie was coding the go client. And we were happy with that for a while. Some loud voices in the bitcoin ecosystem thought we were crazy for lots of different reasons, but especially because there would be no way to keep these different clients syncing to the protocol, so we'd have consensus failures regularly. And I think it was April 2014, we were all in Toronto, and we did have a very explicit discussion about maybe killing two of the clients and focusing on just one client, so we could put all our resources into it. And the argument was that, hey, bitcoin doesn't even have a spec, we should have a specification, very explicit specification, so anybody could code a client to it.
00:16:35.334 - 00:17:20.900, Speaker A: And it's a superpower in terms of redundancy, in terms of real time monitoring of all the clients, by all the clients. And so we ended up sticking with it. And then other people showed up. Roman Mandalay showed up with a nearly fully formed Java client. History has proven us, I think, fortunate and thoughtful. Who would we credit with that? Do you recall who was all of us, pretty much. I don't know who the strongest voice was to shrink things up.
00:17:20.900 - 00:18:07.600, Speaker A: And because I don't remember who that was, it was a thought that we were chewing on, and it was pretty easy to land on the correct decision. Yeah, that's awesome, because I've obsessed over this idea for so long, and I never knew the origins, so that's really exciting. Yeah. April. So we were in Toronto for Anthony Diorio's bitcoin conference, and we were all sitting in bit decentral, or decentral, his place on the second floor. The whole core group sort of landed that decision after a good discussion. Igor.
00:18:07.600 - 00:18:57.056, Speaker A: Yeah. Since we also participated a couple of times from kind of making like a multi client, let's say, for Polygon ZKVM, a multi client from a single client architecture, you won't believe how much information is not written anywhere in just in the head of a single person and a single organization, why it was done like this. And when you just have like a single client architecture, basically all this information keeps getting hidden. And then you end up with either a code base long term, that you cannot really change. And we see this in some other blockchains, right? So they never ever change. Or that's just a longevity risk because some people quit. And if there is no discussions around some features or some things, I don't know.
00:18:57.056 - 00:19:31.960, Speaker A: Just a funny example that, again, nothing to say, like Polygon. CQM is pretty cool technology, but it was missing in one AIP from 2017. It wasn't just enabled for some reason, and nobody knew why. And things like this are all over the place. When you're starting to build an alternative implementation, you start saying, oh, why did you do it like this? Oh, we didn't really think about this, or this guy knows, or something like this, or look at the commit history. So it's a lot of information. That's a little bit of my anecdote on that, Dion.
00:19:31.960 - 00:20:20.744, Speaker A: Yeah, so all that has been said is perfectly correct. I just wanted to add a couple points. One of them is that Igor was talking about people quitting, which is true if you look at the initial guest team and the current one. The current guest team has been pretty stable, but a lot of the early founders are gone and actually completely disappeared from the map. So we don't even know really what they're doing these days. They're okay, but that's pretty much everything we know about them. People do quit, but on top of that, teams completely disappear.
00:20:20.744 - 00:21:00.344, Speaker A: Because I can name at least three clients that are no longer developed, completely abandoned. Some of them, I don't even know if they've ever worked. Like, for example, the Ruby client. This one, I don't know if it actually ran, but yeah, so you can't just let people hanging like this. A team might just get super rich and completely disappear, and that would be pretty bad for the network. So you need backup, you need incentives, you need redundancy. And the other reason I would say client diversity is good.
00:21:00.344 - 00:21:51.080, Speaker A: It's not even to prevent the supermajority bug, it's just that you talk to more people so you don't find yourself in your little bubble, your own little team of ten people. Because client development teams are usually pretty small. Actually, nevermind might be the exception, but every other team I can think of is really small. So just talking to people, discussing the spec, having a different point of view on the spec itself for me is actually the biggest benefit of client diversity. Thank you all. We have reached time. Thanks for the all participation for the fantastic client diversity talk.
00:21:51.080 - 00:22:39.400, Speaker A: Let's quickly move on to our second discussion, Altu on gnosis chain. In this part we will delve into Altu on gnosis chain and discuss how to danken upgrade affects gnosis Altus and the potential it creates. Our participants, Gnosis co founder Martin Copelman and Gateway CTO egor McDigreen, our head of engineering Philip Schommars and Joseph Lubin, consensus co founder. And this panel moderated by Philip. I will hand it over to Philip. Thank you, Armand. Well, I guess I don't have to introduce anyone anymore, so yeah, let's dive right in.
00:22:39.400 - 00:23:28.786, Speaker A: So let's start with an open question, I guess to anyone. Other people in the call also feel free to interact if you have anything to say. But first off, how do you see the layer two landscape now that Dencon is available? And what are the challenges for layer twos in general? Maybe let's start with Martin. Yeah, maybe to kind of set the stage. Of course we have a long history, Nosis has a long history with Ethereum. So we of course very closely followed the approach or the idea of how to scale Ethereum and we ran into the scaling issues ourselves. We had applications on the chain that suddenly became unusable because they were just too expensive.
00:23:28.786 - 00:24:51.602, Speaker A: So of course we very much followed the l two centric roadmap that ethereum went for. And I would by no means say that was a mistake, but we felt it was incomplete or basically we believe, and that is the big reason why we did gnosis chain, that just going for vertical scaling is not enough. And that kind of essentially the demand for block space or for things to happen on chain is larger than what we can in the foreseeable future just achieve with ethereum l two s. And an example of course is gnosis pay. So for gnosis pay where now daily many users are doing their transactions on chain, we simply definitely need subscent transaction costs and we need them reliably. So kind of we cannot be in a situation where there's, I don't know, some random airdrop on the chain or something like that, and suddenly transaction cost a dollar or something like that. My expectation is that even with Dankoon, layer twos for Ethereum will not reliably have transaction costs under cent.
00:24:51.602 - 00:25:20.566, Speaker A: And again, that's also totally fine. And the way high transaction fees are or kind of decently high. Transaction fees are also a positive sign because it essentially means it's valuable to people. And people are spending millions if not billions for those transaction costs. But long story short, we still think we need more. So we have nosis chain kind of as a horizontal scaling, so very similar to Ethereum. And now we also do layer twos on top of gnosis chain.
00:25:20.566 - 00:26:50.866, Speaker A: And yeah, so the Ethereum tech stake is incredibly strong and it's incredibly hard to replicate all those things. So yes, we are very happy that we now also have 4844. So essentially the two s will work out of the box and you can set up and igor gateway, they make it extremely easy to set up your own layer two, but you can choose and you can now pick gnosis. You will basically be able to completely stick to the Ethereum tech stack and use four, eight four and so on, but do it as a fraction of the cost forwarding to Joe. So I've been guilty, as have a bunch of other people, of calling 4844 our broadband moment, implying that hey, we've got usability, decent amount of security and scale now, and now we can cross the chasm into mainstream culture with some applications. I think that's partly, slightly true, but I think we're going to max things out again relatively quickly. I think it's funny that people think that blobs are going to be cheap as air and maybe not totally utilized.
00:26:50.866 - 00:28:09.330, Speaker A: I think this call started in an amusing way. Blobs are going to get eaten up. We'll add more blobs, they'll get eaten up, and that's great. So I do think that we're at a stage in the extended metropolitan ethereum ecosystem where we can architect applications reasonably well, so that they're treating their l two or their l three fairly lightly, and we can see things like social networks and games and actually cross the chasm into mainstream culture. But if we're going to build a new trust foundation that subserves a new system of the world, a more decentralized system of the world, we're going to need orders of magnitude more scalability. So we've done a pretty good job doing what engineers do, which is building a system, a proof of concept system, and then layering it and modularizing it and sub modularizing it for orthogonality and for scale. And we're going up the stack.
00:28:09.330 - 00:29:36.522, Speaker A: I think in order to see a lot of real usable applications, they're probably going to be game chains or app chains at layer three. And I believed in the thesis that Martin Stefan Frederick have proposed quite a while ago that we're going to need more layer one trust foundations and wouldn't it be nice if many of them at least had the full ethereum architecture because then we can use all of our stuff. So I'm a big fan of that thesis and I think we have to start thinking about how we soonish link up these layer ones with sort of back plane or trunk message passing between them to let them communicate the things that they're going to need to communicate. Igor, maybe you can share a bit. So I think you have been also on the forefront of setting up layer twos and maybe also the challenges with that. Yeah of course I'm very excited for 4844 everywhere it lands. So right now it's just two chains like today gnosis and then Ethereum.
00:29:36.522 - 00:30:58.182, Speaker A: Because I think one of the big things that we totally see right now is the more popular like your layer two or layer three gets. You're getting into this issue with both data availability and kind of verifications. And if we look at the, I mean I'm more, let's say working with zk roll ups and zk L two. So in the case I would say that the majority of the costs that are spent are data availability still. And with four eight four enabled right now the base chain, they have kind of the chance to because the whole area was invented right now with projects like Celestia or near went the same way which is basically to serve as a data availability platforms, which is nice. And I think when the base chain offers this, it kind of makes a lot of the setup much simpler and it's made it much more how to say it's cohesive together. Because right now from the engineering perspective how, let's say ZKE roll up works, it sends its data availability somewhere, or like ZKL two.
00:30:58.182 - 00:32:10.398, Speaker A: If it's a roll up it sends it to the basically L one. If that's a validium, it sends it to alternative data variability and then it sends a proof, like a validity proof somewhere. And usually these validity proofs, they need to be somehow matched with data availability for the external people to be able to verify it because why even having validity proofs if nobody else apart from your team can verify it? So I think it offers a very nice architecture for a fraction essentially of the cost. And I think still there's going to be way to, there's going to be space for both alternative data availabilities because there probably will be still cheaper and they can work independently on working on increasing the capacity. That's the only thing. The second thing is that there will also be space for the roll ups without 4844 for supermission critical things when you don't want the data to go away, like ever. So you don't want it to be in the blobs that will disappear at some point in time.
00:32:10.398 - 00:33:41.242, Speaker A: You want them to be forever, and then the blobs are probably the middle ground. That's good enough for the majority of the use cases, basically. One more thought I have is that I think, well, Ethereum had obviously in that sense, the disadvantage of needing to kind of develop this strategy first. And so I think where Ethereum currently, what I at least see as challenging is that the L two landscape is pretty diverse, which of course is also positive and speaks for a lot of innovation. But it has, in my view, the negative that currently it doesn't seem like there's a real good something like messaging protocol or interoperability between different l two s. So kind of like, let's say just if you would have the goal that you have a safe and you kind of use it on all the l two s, or you are at least able to do that is far from solved and far from trivial. And here I think we have a little bit with nosis, the advantage of coming second and trying to design our l two kind of landscape a little bit more, I guess, with the benefit of already lots of work being done.
00:33:41.242 - 00:34:41.338, Speaker A: So I think this is still kind of open or still ongoing. But my hope would be that we can create an l two landscape where the different l two s on nosis nicely interoperate. And there are specific standards for, for example, having one account to save, let's say, on the main chain where you have your recovery stuff and so on, but then you can fairly seamlessly use it on all tools. Last comment before I go to Joe. So concretely planned for Nosis, we have a bunch of requests or ideas for applications that might do their own l two on gnosis, but the two concrete ones that will come for sure is one gnosis pay. Right now it is running on L one, but it's very clear that it's not a long term or even midterm solution. So we will need to go to an L two just for speed and cost, essentially.
00:34:41.338 - 00:35:45.810, Speaker A: And the other one, we have those agents and AI agents running on nosis chain that also kind of do those micro payments and pay each other. And that's also pretty clear that as soon as transaction fees will rise to a few cents on novice chain that this needs to go to an L two as well. Joe. Yeah, so I think we're on the right trajectory, a healthy trajectory in terms of balancing scalability, the drive for scalability, and the side effect of fragmentation. And I'm pretty confident that through a few techniques we will integrate within layers and between layers. Even so, I expect state channels to come back at some point soon. I expect our different interoperability approaches to get mature.
00:35:45.810 - 00:36:55.270, Speaker A: But we did need to have a divergence phase where we're exploring the solution space with respect to transactions per second, with respect to data availability. We're going to need a mind blowingly giant amount of data availability, and we're going to need different kinds of security guarantees or other kinds of characteristics for available data. Like if you're running something in Switzerland, you may not want that data to leave Switzerland. And so we're going to have to map this technology into how the world needs it to be, even while we're changing how the world is architected. So going to be a divergence phase for quite a while. And the convergence phase looks like wallet level abstractions. It looks like shared sequencers based, decentralized shared sequencers.
00:36:55.270 - 00:37:40.870, Speaker A: Looking forward to the convergence phase of the ecosystem, where I don't need to know where a token is in order to swap another token to it, or I don't even need to know the names of tokens. I might just want to have a sort of much more general intent expressed, like, I want to borrow some money. Just to repeat that. I think this divergent phase was very much needed. Quite happy that we didn't kind of stick to the early, whatever, plasma designs or something like that and kind of just double down on that. So I think, yeah, the innovation phase that happened in the last two years was amazing and very necessary. Maybe we bring in Sebastian.
00:37:40.870 - 00:38:06.640, Speaker A: Yeah, I just wanted to double down on what Joe just said. State channels are absolutely needed. State channels are here. There's projects like us which very actively use channel state channels already today. But state channels need also l two s. Like state channels and L two s are not either or. I think we need both.
00:38:06.640 - 00:39:04.020, Speaker A: And still on an l two you have settlement time and fees, which for some l two s are prohibitive. And that's why I think we still need to continue ideating and engineering stage solutions for specific applications. Those are typically not general purpose, and that's fine. But yeah, I totally see a future for state channels on top of L two s. Yeah. I also want to agree that I think it's kind of a natural state of things when we kind of came up with one layer that verifies, settles things or like a couple of big layers. Right then, right now we kind of distributed compute in their own isolated areas.
00:39:04.020 - 00:40:30.282, Speaker A: And yeah, probably as the next step we need to bring back the experience of a single chain within this multiple execution environments by keeping the execution scaled and basically distributed that way. Any question? Okay, in the meantime, I can just do a little bit of a stat. So since we have this platform, presto, that allows to provision nl two in a couple of clicks and we give away some for free. We have of course a lot of people who are trying to, to create their own l two. Not all of them actually have some activity in them. But overall I think we provisioned about 1700 l two s over both ethereum and notice and about 20% of them are actually on shadow. So that's pretty good.
00:40:30.282 - 00:41:24.000, Speaker A: Actually when I looked up the stats literally right now, I expected it to be a lower number for gnosis because you don't see that, but like 20% is very solid so people understand what that is and somehow provision l two s there. So that's pretty cool. Wow. Heard those numbers for the first time as well. But yeah, I mean, maybe kind of to just also give perspective. Why would you want to do that? Or why would you want to then later run your l two on gnosis often for your l two, let's say you are just some game or some application. It's questionable whether I think there are different scales, but some will certainly not want to do things like have their own integration into exchanges or have their own integration into onramps and so on.
00:41:24.000 - 00:42:50.986, Speaker A: So then of course you could go directly to Ethereum, but then use all the, I mean, still Ethereum has by far by wide margin the largest number of on ramping and kind of connectivity. But of course kind of asking your users to say you first need to on ramp to Ethereum and then bridge to the l two. That's of course for many applications, absolutely not feasible because asking your users to pay, I don't know, $40, $50 transaction fee just to then bridge your stuff to zl two is not going to happen. But on osage chain with stuff like whatever this monarium iban onboarding, I think we have a good number of ways to get from a credit card to XDI with really low costs in the area of ascent and then kind of do your deposit into the L two from nosis. Then you are kind of still end to end from credit cards to having your assets on the L two. The total transactions costs are still in the send area. And that certainly enables essentially a new range of applications that simply you couldn't do before regarding this, since again, we're also looking quite a bit on the applications of kind of l two s blockchains, right.
00:42:50.986 - 00:43:41.190, Speaker A: Not more like an actual ledger than defi and crypto itself. And that also bring a pretty cool use cases to that where you can have your own chain that's fast and cheap. But then it still kind of publishes, let's say validity proofs and data will be to L one for others to audit. So then example for it could be like one of the things that we're aware of is like carbon credits. It's nothing to do with crypto. I mean, there are probably tokens on that, but it's more used for, okay, you running your server of like 100 gigs of, I don't know, SSD space, like, I don't know, some ram, some cooling and stuff like this. So it generates you some carbon credits that you need to cover then.
00:43:41.190 - 00:44:28.980, Speaker A: And then the auditors needs to come and basically verify it. If you do it directly on L one, it's fairly expensive, especially if we're talking about pretty good l ones that are well decentralized, like again, ethereum or nosis. But doing it on l two with validity proofs on L one is, I think a pretty good thing of how can you run it economically, quickly and also in a way that settles on kind of a decentralized and open system. Really exciting developments. Thanks for the amazing discussions. And we are going to next talk. Here is the question.
00:44:28.980 - 00:45:09.210, Speaker A: Can we make creative things with blobs? I think it seems possible with the blob FM. I will hand over to Dave and Philip and they will introduce what is Blob FM? Awesome. I'll just share my screen to show blobfm. Let me know if you can see it. Can you see the it? But you have to switch options. You can do shared screens. Dave.
00:45:09.210 - 00:45:35.332, Speaker A: Yes. Good. Awesome. It's working. Yes. Cool. So this is actually a project Philip and I wanted to hack on at Istanbul.
00:45:35.332 - 00:46:17.812, Speaker A: But we didn't have blobs back then, so we didn't end up doing it then, but we kind of brought it to life right now. And I'll just start by giving a quick overview of the gallery. Because you guys looking watching this on YouTube or here on the call can actually participate in this. This actually allows you to upload images onto blobs on Chiado right now. Because of course, the hard fork hasn't happened yet. As you can see, it's already been heavily used so far on this call with some pretty decent memes already uploaded here. But essentially if you go on BlobFM gallery, it actually creates a local wallet for you here.
00:46:17.812 - 00:46:52.448, Speaker A: So you can just click on the wallet, it'll open it in block scout for you. And then you can just open our faucet here, it'll bring you to the gnosis chain chiado testnet faucet. You can claim some XDI to that wallet. And then essentially you can just upload an image from your PC. And essentially it will just upload that as a blob to the chiado testnet. And after a few seconds you should see it popping up here. So there's the image right there.
00:46:52.448 - 00:48:01.240, Speaker A: And we encourage people watching the stream to just upload some images. And we'll come back to this probably at the end of this segment and after the hard fork to see what images have been posted here. And eventually we'll probably upgrade this to work on nodded chain mainnet as well. And then the other segment of this is actually the radio part. Maybe before we go there, Philip, maybe you can add a bit of the technical stuff. So kind of, if someone uploads such a picture here, meme, what is actually happening under the hood? How does this blob be? I mean that was going to be the next step anyways, but yeah, so actually our idea was to kind of present blobs and explain why they are so interesting. Because of course the main idea is to scale ethereum, right? So they will be used mostly by layer twos, but still it is first and foremost just a way to post arbitrary data on chain, kind of in an ephemeral way.
00:48:01.240 - 00:48:48.230, Speaker A: So data will only stay for up to two weeks. And so basically when you upload a picture here on Blob FM, it just creates a blob. So, well it resizes the picture first because one blob is at most 125 size. So we have to resize that image and then we upload it in a type three transaction. And in addition to doing that, because type three transactions are actually exactly the same as non transactions, except you can also add blobs. We actually do a contract interaction. And so here we post a transaction on the gallery contract, if you will, so that you can tail the logs and basically know in which transaction a blob happens.
00:48:48.230 - 00:49:32.832, Speaker A: Because then blobs are stored on the consensus layer side instead of the execution layer, which is the opposite of how everything works currently for ethereum or EVM in general. So until now it was all on the execution layer side, meaning you had like Infora or any RPC or whatever, you can contact that. Now it will have to be changed because you have to actually call the consensus layer site instead. So you have to do some calculations. It's a bit complicated actually, it's not that easy yet, but you have to find the slot in which the block that contains the transaction was mined is. So that's fairly easy. You can just calculate the timestamp and then there you contact the beacon chain API to get the blob.
00:49:32.832 - 00:50:08.690, Speaker A: Then of course you have to decode it because it's a bit complicated again. But anyways, all of this like Blobfm is of course available open source. You can go through the code. As far as I know, this is one of the first projects where you can actually upload blobs directly from the browser. And there is still no support for in browser wallets, which is why we have to use a local wallet that can actually sign directly from the browser. So it's a security risk. Don't send many tokens to that address.
00:50:08.690 - 00:51:00.064, Speaker A: But yeah, all goes to show we can do many very interesting things with blobs. And actually the next step, which is the audio part Dave can talk about now, maybe quickly. So yes, I understand it's a new type of conduction we have. I forgot what type zero and type one was, but type two was kind of 1155 with a base fee where you can define a base fee instead of. And now we have type three. So type three means it will be most similar to type two transactions. So it will also use base fee, I guess, yes, but it will also have a second, I think if I understand correctly, the block fee market is separate from the regular gas market.
00:51:00.064 - 00:51:33.672, Speaker A: So I guess you will also specify base fee or a kind of gas limit there. Or do you. Yes, exactly. An overall for EIP 1559, there's two fee parameters, which is like the base fee. So max fee per gas it's usually called, and then the max priority fee. And then in addition to that, now there's the max fee. I don't remember exactly how it's called, but yeah, it's a separate fee market for blobs specifically.
00:51:33.672 - 00:52:24.764, Speaker A: So you set gas for blobs in addition to gas for your actual transaction. Maybe kind of just educational. So I think on ethereum they will launch the chain with three blobs per block, correct? Yes. So the target on ethereum is three blocks per blob, meaning that kind of the fees will stay linear if we are at three blocks per blob. But if we go above that with a maximum of six blobs per block, then it will become more expensive. The base fee will get more expensive. And if we do less, I mean, if there are less blobs than three per block, then the fees will go down, or the base fee, four blobs specifically, which is exactly the same as for the base fee for normal transactions and for nosis chain, because we have a much faster block time of 5 seconds instead of twelve.
00:52:24.764 - 00:52:54.970, Speaker A: So 2.4 times faster. We actually only target one blob per block. And with a maximum of two to stay kind of in line with ethereum. And so for that specific part, for the coding part in the FAQ of Blobfm, you can actually. I mean, there's a link to show exactly how to do that transaction. So, yeah, I think you can scroll a bit in the source code, a bit higher, I think it was.
00:52:54.970 - 00:53:35.750, Speaker A: But, yeah, how do blops work? Yeah, exactly. So there's a link a bit exactly there. And there you can see that there is actually this max fee per blop that is added in comparison to type two transactions. Yeah. So those are the three fees, the priority fee, the base fee for normal transaction gas, and the base fee for blob gas. So, yeah, with this code, you can actually post blobs yourself. And then there was this second part that is like audio that Dave can present.
00:53:35.750 - 00:54:10.296, Speaker A: Sure. Yeah. So the other part is the audio where we uploaded some epicenter podcast segments. And this works kind of similar to the gallery, but given the kind of limit of 125 blobs, the kind of episodes are actually segmented into multiple blobs. And then it actually just plays through them. So if I play one, for example, this is epicenter episode 538 with guest Jasper. Before you.
00:54:10.296 - 00:54:36.250, Speaker A: I'll just mute the episode. But you can see the blobs are actually being loaded up. And then the player is actually just playing through these blobs, which are like Philip mentioned, 125 KB each. And it's essentially just streaming through them. And you can actually use the same to also live stream audio. We haven't really implemented that yet. It's just uploading a couple of existing episodes and also music, of course.
00:54:36.250 - 00:55:28.744, Speaker A: And, yeah, Philip, I don't know if you want to talk more about the smart contract and how this kind of works with the audio episodes. Go ahead, Martin. My question would just be, Philip, I know you calculated just to give the audience a rough idea for the mean, I think you did some numbers. What is feasible? Yeah, so I wrote it up in FAQ, but basically the retention time is around two weeks. So very rough math. We have around 125 block, which if you calculate times the number of blocks per day, you get about 2.16 gigs of data per day times two weeks.
00:55:28.744 - 00:56:13.060, Speaker A: So basically, 32GB of data in blobs per total per two weeks. And then that's the maximum data that people have to store. I mean, node runners have to store all that data, of course, but they will never exceed 30 or 32GB of data as the previous one will get automatically deleted. So this represents 2.16Gb per day, roughly, on nosis chain. I think on Ethereum it must be a tiny bit more. But yeah, it would be kind of similar for the segments on the radio part.
00:56:13.060 - 00:57:12.130, Speaker A: So, as Dave mentioned, we can fit around 14 seconds of quite high quality sound in one blob. So in 125 kilobytes, and then you can actually do live streaming as long as you can get at least like 60% of or your blobs in at least 60% of the blocks, at which point you can do like seamless live streaming. And in the player, as they've showed before, you can see basically, I mean, it serves no purpose except an illustrating which blobs are currently being played. But you see, the first element is the index that is currently being played. So he's playing the sound from the first blob out of 32 are currently loaded. So, yeah, just to give an idea, like one episode is around 250 to 300 blobs. So it does take around like half an hour to an hour to upload.
00:57:12.130 - 00:57:50.170, Speaker A: But yeah, I think it's quite neat, the technology. And then before I know we have to move on, I actually implemented or enabled nosis chain now on broadfm. Of course, the hot fork hasn't happened yet, so it will not work at this point. And I haven't been able to test it, of course. So hopefully it works. But when the hot fork goes through in around half an hour, hopefully we will be able to test directly on gnosis chain as well. Super nice, super nice that you were able to hack this together.
00:57:50.170 - 00:58:30.408, Speaker A: Yeah. Thank you for the fantastic idea and demo. Hopefully we can upload this broadcast to blob FM after Dhankun. And our next panel will be about Shatterai's gnosis chain. Our guest Yannick from Yannick and Luis from Shutter Network, and Philip and Martin Copelman from gnosis chain. I will try to lead this panel as a moderator. And let's get started.
00:58:30.408 - 01:00:01.904, Speaker A: First of all, I would like to start this panel with a question. This question, like shutter team, what is shutterized gnosis chain? And what is the encrypted mempool on gnosis chain? Maybe I can take a step. So I'm Luis, I'm doing product at shutter, so I can maybe give a high level introduction and then maybe Yanni can go to more technical detail. So shutter is a distributed key generation mechanism, DKG, which uses threshold encryption, and this sort of decentralized keeper set, which enables various commit reveal use cases, and especially sort of in the area of basic neutrality and information symmetry. So the other use case is besides sort of the shared nosis chain and sort of the l two implementations we're working on, is for Dow voting. So with snapshot together, snapshot, we implemented this feature where any snapshot, Dow can use this sort of encryption method, where then the Dow votes are encrypted during the vote and then decrypted after the vote is ended. But yeah, focusing on sort of the shutter gnosis chain is basically this collaboration between shutter gnosis and Nethermind, where we're building this encrypted mempool, which protects against front running malicious MeV, and also kind of censorship.
01:00:01.904 - 01:01:35.840, Speaker A: And the high level way it works is we are encrypting transactions and have them be sort of committed to and signed off on by the validators while they're still encrypted, such that no one can censor or front run. And this really sort of enhances the front running kind of protection and also increases sort of the real time censorship. Obviously, Nosis chain is already censorship resistant because it's decentralized, but with the encrypted mempool, it becomes sort of even more short term censorship resistant. And yeah, I'm super excited about this, because I think nosischain will probably be the first chain with such an encrypted mempool. And I think it's also something where really, crypto has an actual sort of edge over the real world, because the same or similar sort of information asymmetry and front running problems exist in the real world. I think this is something where crypto can really have an advantage right over the real world. And also, I think it's interesting because just aside from sort of this transaction, MeV supply chain encryption, I think, is also a very nice kind of gadget to have for nosis chain, which can enable other kind of timelock encryption use cases and things like scheduling transactions and other things that we haven't even explored yet, because we have this kind of commit and reveal and time lock encryption gadget and phenosys chain.
01:01:35.840 - 01:01:59.720, Speaker A: And also we have a Ruben from nethermyt team. Maybe he can give some status of the implementation on shutrise. Nonstop chain. Ruben. Yeah. Hey guys. So until now, we were working on our leap P two p implementation and polishing it, and finally it was ready.
01:01:59.720 - 01:02:52.146, Speaker A: We made some progress communicating with the shutter network, doing some basic encryption decryption. So yeah, I hope we'll progress faster from now on, given we have all the foundation we need and given that we have the shutter network deployed on. So looking forward to having it ready for testing very soon. Thank you, Ruben. I have another question to Janit. What are the main trade off of implementing on encrypted mempole on this way? I would say so. First of all, if it's implemented, you don't have to use it.
01:02:52.146 - 01:03:20.540, Speaker A: So you're not necessarily subjecting yourself to these trade offs. But one trade off is you have an additional security assumption. So when you send in a crypto transaction. So Lewis mentioned this. We have this committee of nodes. It's a threshold committee, and the protocol is only secure as long as the majority of that committee is honest. And if that's not the case, then they can censor your front run.
01:03:20.540 - 01:03:46.950, Speaker A: Another. I think the biggest trade off from a high level is complexity. It makes the protocol more complicated. There's more chances for bugs. But of course, the benefit is very clear. I think Luis has highlighted those very well. It's just a more secure user experience for users.
01:03:46.950 - 01:04:56.490, Speaker A: Yeah. Maybe I can also comment a bit more high level on what we are trying to solve here. So, on Ethereum, while kind of many, many parts are really awesome, decentralized, where there is this balance of power and so on, in the MEV space, that's unfortunately not so true. And we know this very well because we actually run off one of the relays, one of the really just like four or five relevant relays. So there's a very small number of relayers and there's almost an even smaller number of actually relevant builders. So despite Ethereum having, what is it, 800,000? Is it maybe already a million? I don't know. Because despite Ethereum having this huge number of validators, the actual number of entities that produce blocks is much, much smaller, or at least 80% of all blocks.
01:04:56.490 - 01:05:40.122, Speaker A: I think it's even more than 80%. I think over 90% of all blocks are just produced by literally four different builders, and that we see as problematic and kind of onosis. We don't have that problem yet. So basically, right now, we have our 200,000 validators, and essentially all blocks are actually kind of locally produced. But if we are honest, that's not because gnosis did something revolutionary so far. That's simply because MEV is not such a big topic on gnosis yet. So kind of the DeX and defi activity is much lower.
01:05:40.122 - 01:07:04.990, Speaker A: So essentially, the sophisticated MeV and builder system has simply not developed on also. So kind of, if we would just go with the Ethereum route, we would probably, or it's very likely that we would end up in the same situation with few concentrated builders and essentially a lot of power or kind of, in a way, giving up a lot of decentralization. And this is something we want to try to address with that proposal, to systematically reduce overall MeV of the system by giving, in a way, by increasing the neutrality of the system, by basically just saying, okay, here's the transaction. I, as a validator, see this transaction, I recognize this and I sign it, but I don't know its content yet, and I can't treat it any different based on its content. So basically, two transactions that pay the same fee, in our view, should be treated equally regardless of what their content is. And yeah, we believe the actual way to do that is to make sure that they are encrypted at the time when the validator makes that decision. And, yeah, that's what we hope to demonstrate on Ethereum.
01:07:04.990 - 01:07:47.282, Speaker A: Sorry, on gnosis, I think if it goes well on gnosis, we would also be happy to bring the same technology to Ethereum, for sure. There are kind of several stages of how it can be implemented. So first, it can be a voluntary validator opt in, and kind of in the later stage. Second stage, you could use something like Eigen layer, where validators kind of commit to additional rules that can be enforced on smart contract level. So that would be stage two, and stage three would be to actually mandate it by the protocol itself. We will certainly want to go to stage two. Whether we go all the way to stage three.
01:07:47.282 - 01:09:02.630, Speaker A: I think that's still kind of an open question for gnosis, whether it's. I mean, of course, first we want to demonstrate it's useful, and then I think we will assess together whether it's, how much value add on it would be to kind of, even kind of make it part of the core protocol, which, of course, comes also at costs in the sense of diverging from Ethereum. In case Ethereum doesn't want to go that route. Fantastic. We have come to end our time, and also we are very close to artwork time. And thank you for all the fantastic panels, all the participants, and all the questions, anyone? So I will hand over to Lyon and Philip for the current status of the hard work. And yeah, we still have a fair few minutes left, so if there was anything left to talk about, for sure, feel free.
01:09:02.630 - 01:09:28.560, Speaker A: It's not for the immediate future. Maybe if there are kind of questions. Yeah, I think Sebastian's raising. Oh, sorry, Sebastian, please go. Yeah, I think my video doesn't work, but it's fine. I think Martin pointed it out towards the end, but just briefly. So my first reaction to this shutter I speak in Shane was, well, this is amazing.
01:09:28.560 - 01:10:32.034, Speaker A: This is innovation, which is actually front running Ethereum and making innovation easier on Ethereum. My second thought was, well, this is actually kind of something where Nosis will be diverging from Ethereum, but maybe you could spend a few more words on how you make it actually kind of like soft fork, kind of upgradable in a sense that we don't need to necessarily make protocol changes to enable that, because I found that one actually pretty interesting that this is even possible. That was not obvious to me from the get go. Yeah. So in the way kind of it happened on Ethereum already, where without essentially what happened on Ethereum almost two years ago now, is that there was this kind of builder proposal separation that is not baked into the protocol, into the core protocol necessarily, but it's kind of built around it. So map boost, essentially. So essentially there was no hard fork or anything.
01:10:32.034 - 01:11:38.262, Speaker A: It was just an additional kind of communication channel, how a validator can retrieve a block from a builder or through a relay, and then essentially use this block when it's their turn. So essentially that was kind of built around Ethereum, and it's now pretty much part of Ethereum, but it was kind of never part of the yellow paper. It's kind of just something that is implicitly possible within the specification of the yellow paper, and the very same thing can be done here. So essentially we can have an extra rule. In the simplest form, we can just create a relay that people connect to the same way how the validators on Ethereum connect to relays. But nosis relay would essentially enforce this shutter protocol, and that would be kind of the weakest form because then. But still, that's how we start.
01:11:38.262 - 01:12:32.410, Speaker A: So that is really a voluntary opt in, and you can actually cheat the system here. So you could then essentially take this block from the relay and kind of then still look at the content and build your own one. So that's certainly only a temporary solution. So the next best solution is kind of to do something similar that Eigenveier is doing. So essentially you as a validator would put your stake, your gno stake into a contract. And this contract essentially can check the shutter rules. And if you violate those shutter rules, then this contract has the right to essentially take your deposit.
01:12:32.410 - 01:13:20.850, Speaker A: Yannick, I think I shouldn't have answered this. You're much better to answer pretty much what you said. One thing maybe to add, it's like a technical detail, but the contract can't check it on its own because it doesn't have enough data. It needs some sort of proof similar to how you slash validators on Ethereum or on losses chain. You give them proof that they proposed two blocks, for example, and then the contract can only check this proof. But other than that, pretty much what you said. It's great that we as a gnosis ecosystem can kind of contribute really to progress on Ethereum.
01:13:20.850 - 01:15:08.138, Speaker A: Anyone who has listened to kind of the all core dev calls on Ethereum know that this is hard. And I guess having an argument of, hey, this works in production on gnosis is actually a pretty meaningful one. Yeah, maybe then just to add this one more thought, so kind of as gnosis is actually running a relay for ethereum, that would also be an easy kind of potential kind of route to offer that to Ethereum to essentially say your validators if you want to connect, or basically our relay on Ethereum will in addition kind of build blocks in this very specific way, or will require from builders, let's say order transactions according to those additional rules. But yeah, enough of that. Let's look forward to Dankun. Yeah, we can move on the open floor and for questions and yeah, I'll just say that gnosis chain in general has really developed a lot of credibility in my mind over the past couple of years. Obviously, I've been an ethereum maxi for many years, and I was aware of gnosis and I participated in some validators.
01:15:08.138 - 01:15:37.140, Speaker A: But seeing gnosis and gnosis chain develop so many new and interesting products, like I'm really getting pilled on the ecosystem. It just makes me want to participate more and learn more. I'm still an ETH maxi, but gnosis is part of the ethereum ecosystem. It's really just an interesting complement. So I'm really glad to be here. Thank you. Fifth.
01:15:37.140 - 01:16:17.430, Speaker A: Appreciate that, Philip. And what are sites you're currently having open or besides your command line. What do you recognize? So we should probably start streaming. I don't know. Lane, are you here? Do you want to stream? Yeah, I can share my screen. Yeah, go ahead. Let me check open system preferences.
01:16:17.430 - 01:16:31.722, Speaker A: Give me a second. I have to figure that out. Oh, fuck. I have to quit until you open. Okay, so take it from here. I'll be back. Yeah.
01:16:31.722 - 01:17:08.310, Speaker A: So while lion fixes his setup, I guess so. We are 70 slots away from houseworking, which is quick. Yeah. So five, six minutes away. So what we are looking at right now is just basically lots of all the clients. I mean, there's nothing to see until it actually happens. After that, we'll see for each client if there are any issues, if we keep the number of peers, of course, if we keep producing blocks, if they keep getting attested.
01:17:08.310 - 01:17:35.810, Speaker A: I don't know. I mean, everything the network should do, basically. So processing transactions, that type of thing. So of course we have a lot of tools for that, which I guess lion is going to show. Yeah. Hello, everyone. Can you guys hear me? Yes.
01:17:35.810 - 01:17:59.272, Speaker A: Hello, I'm Lyon. Sorry if my energy is not right. It's very late here. I'm in Asia at the moment, so, yeah, we have a bunch of custom things. I'm sorry, we cannot share you these links. It's all authenticated, but we have a bunch of infra tracking the chain. So as Phil was saying, we have some key metrics that we are looking for.
01:17:59.272 - 01:18:25.740, Speaker A: We have the basic network health, when to see blocks, when to see transactions, when to make sure that everything is happening as normal. Plus we have something new happening on the chain. That's why we have this fork. We'll have blobs showing up. So that's what we are tracking here. As Philo was explaining, blob FM should be running. And we have another service that would hopefully be producing blobs at the right of the chain.
01:18:25.740 - 01:19:01.164, Speaker A: So we'll have some activity that we can track down. Okay, how do I move this bar? So I'm also looking at logs, making sure that we don't have any unexpected errors here. I'm looking at every single container that we are running in all of our infra. And hopefully we don't see any errors. Things that happen typically with the fork is probably people will not be upgraded. So some nodes will try to produce blocks that are on the invalid fork. And this will show up here.
01:19:01.164 - 01:19:42.570, Speaker A: We would likely see a small drop in participation for everyone that hasn't updated. And this is fine as long as we don't have a drop in participation that goes below the finalization level, we should be fine. The first fork that we did in Nosis on the merge, I think we didn't communicate enough time. So we had a brief drop in participation below finalization, but that was very quickly fixed and nothing happened. Like losing finality is not a big problem for the next fork. Capella I think the community really stepped up and finality barely dropped at all. So great job everyone.
01:19:42.570 - 01:20:12.032, Speaker A: We'll see what happens this time. I have confidence. We have been planning for a while did very good comms, so hopefully participation drops by very little. We will be able to see it here. What we want to track is the target participation. That's what's important, that's what contributes to finality. Head participation is voting on the correct head and due to the fact that Nosy's slots are quite fast, we don't have as good as participation as we have in ethereum.
01:20:12.032 - 01:21:33.740, Speaker A: But that's okay, that doesn't affect the network health at yeah, another important thing to consider is now that we have blobs into the picture, the bandwidth requirements to the chain are higher, so nodes has to do more work in the short period of time between when blocks are produced at second zero and when attestations have to be casted, which is innosis at 1.6 seconds in that time before the fork, the only thing they have to do is receive the block in time, but now they have to receive the block and every blob that is attached to the block. And this is extra data that takes some time to receive. So maybe we can see a spike in block processing times if blocks take a little bit more time to progress. However, as Phil has said, we have adjusted the block reconnaissance in nosis to have a bandwidth requirement that we don't think that it will be a problem. We did extensive testing and research on this before the fork and we believe that with the settings that we have chosen, which is one blob target and two max, there shouldn't be too much of a problem. Like there is enough time with the average setup of people that are running the nosys chain to be able to receive the blobs, process it, and have the correct head on the chain in time to do a proper attestation.
01:21:33.740 - 01:21:55.670, Speaker A: But yeah, we'll see what happens. Hopefully everything goes well. Fortunately it was a very successful fork. We didn't had any issues, participation didn't drop, block times were good, blobs show up instantly. So I'm optimistic. You want to add something else Philip. No, I think that about covers it.
01:21:55.670 - 01:22:19.964, Speaker A: I was also not entirely listening because I was looking at data, but I'm sure it was perfect. Oh yeah, 25 seconds of the fork. Yeah. Time to stream some logs maybe. Yeah, who needs logs? Just listen for the cheers, it'll be fine. Hey blobs, we have. Hey.
01:22:19.964 - 01:22:32.140, Speaker A: Wow. Here happen. Okay, let's see. Errors, no errors so far. Fantastic. Let me zoom in a bit. 2 seconds.
01:22:32.140 - 01:22:57.080, Speaker A: So the participation takes a little bit of time. We will need to spend a full epoch, which is about a minute to see. We will pick this. If everything went fine, we will finalize within two minutes. We are tracking this here. So we are already four slots into the fork. This is expected like it takes on perfect network conditions to epoch to finalize.
01:22:57.080 - 01:23:24.800, Speaker A: So when this thing is at 120 seconds, if this thing hits zero, then we are good and we can go to sleep. Let's see who will be the first one to submit a blob. Oh yeah, let's see. Yeah, I'm sending another one that is a little bit more, a little bit better. Okay, so far no block. That's okay. It see the ASCII.
01:23:24.800 - 01:23:46.392, Speaker A: Please guys, validate it. It's good for you. No errors. All good. 13. So we are close to that image works, Carlo. All right, thank you so much.
01:23:46.392 - 01:24:35.894, Speaker A: Let me try to catch someone without any error, which I think I can. Give me 1 second. There, you got one without any error. Carla, I think that one is better. Let's see, search for blob. Yeah, so far no blobs in the network. We have four blob transactions.
01:24:35.894 - 01:24:59.282, Speaker A: Great. Okay, we are justifying. So participation dropped to 77, 77% on the head, which is fine. Target is 95%, which is excellent. Yeah, we didn't drop at all. That's really good. We'll have to double check.
01:24:59.282 - 01:25:33.020, Speaker A: These metrics are a bit lagging. We talked so much about decentralization earlier. We want to see, not we want, but we expect to see a drop in participation that tells us that we really do have this decentralized chain that a lot of people are running and some people are going to screw that up or just forget. So we want the chain to keep running, but also we hope to see a little bit of variance that indicates we do have a high degree of decentralization. Finalized. We have finalized the fork. That's it.
01:25:33.020 - 01:25:56.110, Speaker A: Super nice backseat from this point. Okay, you see. Okay, now the drop is showing. This is a bit lagging. So we dropped to 91% from 95%. So there you have the 4% of stake. Wasn't updated.
01:25:56.110 - 01:26:11.720, Speaker A: Target. That's great numbers. Congrats. Yeah, this is a win. Not for us. This is a win for the comMunity. Yeah, peer counts are good.
01:26:11.720 - 01:26:35.480, Speaker A: Still no locks. That's pretty impressive. No errors, but sure, I'll take it. Okay. We have steady blobs, so yeah, everything looks nominal. We have good participation. We are finalizing no errors.
01:26:35.480 - 01:27:19.300, Speaker A: Blocks. Yeah, the block rate is stable, so we are producing one block every slot, as we should if participation drop. So in this case we can expect about 9% of the network to be offline. So 9% of miss slots would be what we would expect to see now. But this function has a bit of noise, so it's not immediately obvious. We could go to nauseous scannossischat in that's latest epoch. And we have, yeah, one slot, which is okay.
01:27:19.300 - 01:27:56.632, Speaker A: Yeah, super nice. Do we see that? At least for me? Blob FM seems to throw some errors, so at least I am not able to upload stuff there. Has anyone else succeeded yet to create a type spring? I'm currently patching in prod. Sure, but it still works for shadow. Of course you can switch on top. Sure, of course. BlobFM was one way to do it manually.
01:27:56.632 - 01:28:18.470, Speaker A: Then I think, I guess we had also a script planned to create blobs. Do we know if that is working or is this also not working yet? Or basically, have we yet seen a type three transaction? Yeah, we have. Because here we have metrics of block inclusion. Block inclusion. Block inclusion, block block. Yeah. Okay, nice.
01:28:18.470 - 01:28:46.930, Speaker A: So this is the rate per second of transactions seen in the network. So the cumulative value would be higher. I think this is the instant. So how many blob transactions are seen in the previous block? So there has been blocks with blobs for some period of time. More than a single blob. Here we have gossip. Blob verified total.
01:28:46.930 - 01:29:31.540, Speaker A: We are at eight at the moment. Remind me. So we had two or do we don't have a view where we can see kind of which blocks had blobs? This is really confusing to say, but yeah, I'm not sure if we can display it here. For mainnet we use blob scan, but it's a bit unstable and I'm not sure if you have deployed here yet. We probably have to manually look for type three transactions. Yeah, but yeah, this metric is good. This is all we care about.
01:29:31.540 - 01:30:15.230, Speaker A: There is some activity. Great. Do we also have the forkmon? So Aragon nodes are also doing well. Yeah, we have Forkmon. I've sent the link let me resend it one more time. But all nodes were working fine and they are still working fine. There is no chain.
01:30:15.230 - 01:31:13.782, Speaker A: Yeah, I can see here some nodes took significant time to process the. I think this is the fork block, but all of them went down now, so it should be fine. For some reason they choke at the fork, but they are good. Now we can see here some drop in block production, but I'm not sure if this is noise or an actual issue. But again, there is some randomness and it's not easily attributable that this is explicit for kishu. Nice. Seems like Lucas found a block transaction.
01:31:13.782 - 01:31:45.630, Speaker A: Yeah, super nice. Yeah, amazing stuff. Thanks to everyone who was working on this. Yeah, it's amazing. For the celebrating this successful hard fork, we have a clap. So please claim your clap. I will leave a link on the chat.
01:31:45.630 - 01:32:43.022, Speaker A: I'm over here trying to tweet about my excitement, and it takes like nine tweets to get it right. I have to keep deleting them because I'm so excited I typed the wrong thing ten times in a row. So here we have the ASCII app from Lodestar. I'm heading back to work. Looking forward to reading about it tomorrow. Congrats, everyone. Cool, thank you.
01:32:43.022 - 01:33:26.860, Speaker A: Thank you, Joe. Do you guys want to call it? I have a short presentation if you guys are in the mood to motivate a bit. Why blobs are important. Please go. I'm afraid. Can you guys see the screen? I'm not sure if I'm going. Can you see the screen full screen? Yes.
01:33:26.860 - 01:34:05.394, Speaker A: Cool. So, yeah, blobs are all about scaling. I love the name. Is dank sharding actually a real thing? Why do we pick this name? Well, we are actually scaling Ethereum. It's going to take a bit of time, but it makes sense, so bear with me. So every blockchain takes a different trade off to scale and remain decentralized. What we need to do is to have every single node do the same amount of work while the network keeps growing its total capacity.
01:34:05.394 - 01:35:08.060, Speaker A: So the only way forward is every single node doing a fraction of the job. That's the path that both nosis and Ethereum would take. But that's not the only option. Some other chains, I'm not going to name exactly which, choose a different path and they are scaling vertically. So just asking more resources from nodes so less and less people can run the node, this is fine, but if there is a disagreement and the small cartel of people running nodes, do something naughty, like infinite mean or something nasty, then the rest of the community would have to hold the chain and do a social recovery. This is possible, we have done it in Ethereum in the past, but as you know, it's incredibly damaging and not very scalable. So the question is, why this thing of holding the chain and doing recovery? Why don't we automate it so we don't have to rely on social shenanigans? And that's exactly what roll ups do.
01:35:08.060 - 01:35:45.378, Speaker A: They use advanced techniques like fraud proofs or validity roll ups. They use proof of correctness to ensure that we don't do naughty stuff. So that's fine. We know optimistic roll ups and validity roll ups. They work. But the key problem that they face is that if the data that they use to progress the roll up is unavailable, everything falls apart. In the case of an optimistic roll up, if data is not available, the operator can steal funds because it can send a state transition that is fraudulent.
01:35:45.378 - 01:36:16.214, Speaker A: But if you don't know what happened, you cannot dispute it. So you cannot do optimism. They have to do like a B section game and find which exact step was not correct. If you don't know the whole execution trace, you cannot do this dispute. So basically, the attacker will just run with the funds. In the case of a validator roll up, it's not that bad, but you can lock everyone into a state that you cannot progress. So you know that ethereum, the whole state is a tree.
01:36:16.214 - 01:36:47.574, Speaker A: If you change something, the whole hashes would change. If you don't know all the hashes, you cannot do a successful state transition. So you are locking everyone out forever. And this is really bad, because imagine someone took, I don't know, like ZK sync and did this rock state transition. Then they could say, hey, we own the state. We only ask, the attackers can do another transaction. So either you pay us half of your all assets, or screw you, we don't give you the funds.
01:36:47.574 - 01:37:25.782, Speaker A: So it's on the same order of magnitude of bad, but not as bad. So what we were saying before is, we want the nodes to do less things. So who needs to do data availability checks? Unfortunately, the question is everyone. And that's because proving that something is available is an impossible problem. This is a very famous slide for Vitalik. I highly recommend everyone. You should screen capture this and understand it, because this is kind of the whole key of the game of scalability that we are doing.
01:37:25.782 - 01:38:25.670, Speaker A: So let's see, case one, we have an attacker that doesn't publish something, and then someone claims, hey, watch out, this is not published. We should penalize you or something, and then the attacker publishes the thing. Or in case two, the publisher is honest and publishes everything, but the one that rings the alarm is the attacker and raised the alarm falsely you as an observer after d three, you have no way to tell these two cases apart. So we just cannot build an economic system to punish people that publish or not publish. So we are kind of screwed. This is why everyone has to check, because we cannot build any other system that's sustainable except everyone checking. So yeah, if everyone has to check, the easiest solution is that's what we have been doing now since forever, is everyone downloads everything, it's perfectly safe, but scales terribly.
01:38:25.670 - 01:39:05.874, Speaker A: That's not what we want to do. We want everyone to do less things. So the next best thing is, okay, I'm not going to download everything. I'm just going to download specific bits of the data that has to be available. If you do this, this isn't safe, because only if a tiny bit, like if one byte of the whole data that we publish turns out to be unavailable, this whole cataclysmic steal all the fans things happen. So we cannot allow even for one single bit to be missing. So what do we do? We do this crazy KcG thing, which is the azure code extension.
01:39:05.874 - 01:39:58.680, Speaker A: Why? Because now we don't need to have all the data available, we only need to have 50% of it available. So what's going to happen is we do this extension and we have all the data converted into a polynomial. So imagine like a curve, and if you get 50% of the points, you can compute this polynomial again and just get the data back. So instead of just sample everything, you would just sample random points. And every time you do a sample and the sample turns out to be available, you are 50% more sure that the data is there. So if you do like 100 samples and all of them are available, the chance that less than 50% of the data is available is one divided by two to the exponent of the number of samples you have done. So the probability that you are safe goes really high really quickly.
01:39:58.680 - 01:40:34.930, Speaker A: And that's what's going to do with bank charting. Eventually, we will just do a few samples and then two to the power of 50. That's a super low probability that data is unavailable. So we can safely attest and say that this is available. We will put this extra data, this whole thing in blobs because we don't need this data anymore. The only guarantee that we need to provide roll ups is that the data was available at some point, because that's all they want. They don't need the data to be there.
01:40:34.930 - 01:41:08.462, Speaker A: They just want someone to either dispute the data or someone to continue the state. In the case of ability roll up like history, we can handle somewhere else. As long as the data was available sometime around publishing, everything is safe. So we can do this new thing, this blob, and we don't need to store it. So that's why we can price it way down. Because we don't need everyone to have these huge disks with four terabytes or something. As long as the data for this very short window of time that now it's two weeks.
01:41:08.462 - 01:41:43.782, Speaker A: But probably in the future it will be much, much less as Ethereum moves to only having zk rollups. This window. I mean, I'm not sure how low it could go, but it could even be just one block. It doesn't matter. As long as it was available for even one instant, we are fine. So if we want to do this crazy sampling, we need a way to very efficiently prove that a specific bit of data corresponds to the whole block. So if we want to do this with Merkel trees, which is what we have everywhere today, that's crazy expensive.
01:41:43.782 - 01:42:11.250, Speaker A: It's just not going to work. So that's where this new moon mass, KCG, comes from. KCG is exceptionally good at proving. You can prove that one thing belongs to another with a tiny proof, and it scales wonderfully. As you can see here, it's of one. So we only attach one commitment to the block. And with that one commitment, we can prove any bit of the thing belongs to that thing.
01:42:11.250 - 01:42:57.474, Speaker A: So excellent. That's why we're doing it. And same for if you guys have been following in Ethereum, we will do stateless and we do the same thing. We use KZG because it's crazy efficient for this reason. So to do this whole dank charting sampling thing, that's very complicated, especially because it depends on first new cryptography, new consensus logic, but also some crazy networking contraption. Because doing this sampling at scale is very complicated. You would need in a short rhythm time to figure out from the whole gnosis and ethereum network who the hell has these samples, fetch them efficiently and make sure that they are there.
01:42:57.474 - 01:43:26.970, Speaker A: So we don't know how to do that today. We have ideas, but it's not proven yet. It's going to take some time to develop. So someone said, okay, we want to go there, but we haven't figured out the networking part. So yeah, rollers were, hey, we want cheap data. Now give us something. So what we're going to do is we're going to do the crypto part like the consensus bit, which is clear and we know how to do it.
01:43:26.970 - 01:44:23.600, Speaker A: We will do this now instead of just doing some garbage thing like with mercury proof. We will move the crypto and the consensus part early and we will do this first so that when we figure out the networking, then we just do the networking and boom, we're done, we're fixed. Scalability is fantastic and that's what we're doing. That's proton sharding. We are just doing the crypto part early so we don't have to do that later and just have a lot of debt. Thanks, I hope that was clear. Some questions or any questions? Everything clear, 100%.
01:44:23.600 - 01:44:45.126, Speaker A: Great. You guys are so smart. We just want to let you get to bed. I hear it's very late where you are. It's fine, it's a special occasion. Cool. Anything? Any updates from network? Everything fine.
01:44:45.126 - 01:45:33.770, Speaker A: I guess at least what I see is everything looks good. Yes, everything seems to be great. Actually blobfm kind of works but we are on the last step of fixing. Well, I was mentioning this fact that blobs need to be fetched from consensus nodes and they were not public, which is basically the issue. But that's also an interesting problem for the ecosystem in general because again we will have to have new RPC services for consensus layer nodes and not only for execution layer nodes and this kind of highlights this particular issue. Cool. Then I think we probably going to end the call.
01:45:33.770 - 01:45:56.188, Speaker A: Let's do it again in two days. We have reached the end of our stream. Thanks for all participations and see you next. Hard work Etra and Electra, thank you everyone. Thank you so much for having us. Thanks, bye.
