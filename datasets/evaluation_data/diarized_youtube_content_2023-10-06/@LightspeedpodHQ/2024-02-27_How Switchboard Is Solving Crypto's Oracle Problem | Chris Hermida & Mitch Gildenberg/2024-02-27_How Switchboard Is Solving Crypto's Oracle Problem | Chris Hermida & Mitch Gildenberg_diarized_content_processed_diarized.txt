00:00:00.490 - 00:00:30.166, Speaker A: Before we get moving on today's episode, just a quick disclaimer. The views expressed on this podcast by either myself, my cohost, or any guests are their personal views and do not represent the views of any associated organization. Nothing in the episode should be construed or relied upon as financial, technical, tax, legal, or any other advice. Okay, let's jump into the episode. All right, everyone, before we get into the episode today, I want to talk a bit about access protocol. They're built on Salana and solving the subscription problem in a crypto native way. We'll talk more about them later in the show.
00:00:30.166 - 00:01:16.194, Speaker A: All right, everyone, this episode is brought to you by Monad, an L1, bringing performance to the EVM with parallel execution and both a custom consensus engine and new database solution. You'll hear more about them later in the show. You all right, everyone? We are joined today by Chris and Mitch, the co founders of the Oracle Network Switchboard. Switchboard currently secures over 800 million across twelve protocols and TVs is largely driven by applications. On Salana, though switchboard is multichain, they support EVM chains like Ethereum and many of its L two s, as well as newer chains like Swee and Aptos. But it doesn't stop there. They're even working on new integrations for new chains like Manad, which will be released alongside when the chain goes live.
00:01:16.194 - 00:01:40.778, Speaker A: So, Chris, Mitch, thanks a ton for joining us today. We've got a really exciting conversation centered around switchboard and the Oracle network and Oracle networks as a whole. You guys have been live since 2021, and that's quite a long time in this industry. You're currently on V three, so you do have a very, very rich history. So maybe talk about the problem you originally set out to solve and sort of how the protocol has evolved into solving that problem today in terms of.
00:01:40.784 - 00:02:23.642, Speaker B: The origin genesis of it. So I worked at Circle for a while on their trading desk with some of the folks at CMS holdings and many others, and even back in 2017, 2018. Always had the vision that Dfi would really come to take over a lot of the traditional activity that you see not just in the space, but in traditional finance. I wouldn't say we're quite there yet, but one of the really core concepts and something that you're seeing now is this idea that derivatives, the bitmexes of the world, should move entirely on chain in the form of defi derivatives platforms like ByDx, like drift, like mango, like Hxro, et cetera, et cetera, et cetera. The original vision of switchboard was actually to try to build something in that space really focused on the long tail. And the idea was up and coming.
00:02:23.696 - 00:02:24.170, Speaker C: L1s.
00:02:24.240 - 00:02:52.290, Speaker B: And even to an extent, l two s would enable low TX fees, which would make new derivative markets profitable, or at least to make sense to spin up that otherwise wouldn't on, say, ethan's l one. To do that, you needed an oracle that could easily and quickly support the long tail. And so that was really the genesis of switchboard, was saying, how can we make it as simple and easy as possible to bring esoteric or exotic long tail price and data on chain in an easy to use, fast, cheap manner?
00:02:54.150 - 00:03:13.178, Speaker A: Awesome. I love that. And I guess on the evolutionary side, how has the protocol kind of evolved or created new service offerings? Because maybe there was like a shortcoming in the initial offering, and obviously your initial state's likely not your end state. So talk a bit about the evolution and kind of how you got to the point where you're at today.
00:03:13.264 - 00:04:00.934, Speaker C: Cool. So yeah, I'mitch CTO of switchboard. I've been around the block, and yeah, me and Chris started this back in 2021, and it's been a long way coming. Before that, I was a security engineer, spent a few years at Google before Chris actually sucked me into this space, which I've never forgiven him before, but maybe one day I will. But anyways, yeah, with V one, we wanted to provide an oracle offering that was as adaptive as possible for when salon was really starting, because we needed to spin up markets from scratch very quickly. And when we actually launched our Devnet, that was around April 2021. So we were looking for a platform where we could have some sort of body to decide on what feeds we could spin up to produce these new markets as fast as possible.
00:04:00.934 - 00:04:55.740, Speaker C: And that's really the route we went. And with that Genesis, we got a lot of feeds that we could have the team manage and spin up, but we really wanted to make it a more permissionless platform. And that's really where V Two came in with allowing people to create their own feeds, offering a no code UI for people to choose their data sources and really give that whole permissionless data aspect to switchboard. So I would really say switchboard became fully permissionless at our V two. And then for V three, we have a lot of iterations still coming, but it takes a whole new level of security. By incorporating this whole type of oracle incentive system and oracle layer of hardware attested security to our oracles, along with some scheduling and some free form code execution that we actually allow, you can basically automate any platform and any type of data aggregation that you want.
00:04:57.550 - 00:05:17.618, Speaker A: Okay, awesome. That's a perfect background to kind of jump right into the technical side then, which at the end of the day, an Oracle service is bringing off chain data on chain and into the blockchain world. So there's a couple different models for how data is actually propagated in this system. So can you just quickly walk us through each of the three data models and kind of discuss where the trade offs are with each?
00:05:17.784 - 00:05:50.422, Speaker C: Definitely. I'm assuming you read our op ed on like the push pull versus on demand models. Exactly, yeah. So each of them actually has its own advantages. And I would start with the Oracle push model is the one that you're going to be most familiar with because that's essentially all that existed until mid 2022. That's when all the Oracles push the data on chain themselves, when it meets a certain threshold of variance from a previous price. And that's usually done because pushing data on chain streaming, it is extremely, extremely expensive.
00:05:50.422 - 00:06:51.870, Speaker C: And when you're pushing that much and you want a certain fidelity for the freshness of your data, you have to choose when to optimize for gas costs. So that's really what the push model really does, but it really gives not 100% guarantee of how fresh that data is because you are trusting that the Oracle is pushing in a relatively strong cadence to when price movements actually happen. And those price gaps might have some openings that an Oracle might not directly reflect reality. So that's where we have the two other models, where we have callback and pull based models. The callback model, which is what our switchboard functions actually employs, is where you, as a protocol, could request a new price update for a specific feat that could be sent to our Oracle network and they can derive it right then and deliver that back on chain. So at least in this model, your user action is locked in before the price derivation actually happens. So it really reduces that gap risk of when the Oracle updates and when the user action actually occurs.
00:06:51.870 - 00:07:29.260, Speaker C: The last model is the pull model, which we've been looking very deeply into, and how this can actually best iterate on switchboard services in which users actually can request from an Oracle L1 or an Oracle network for a price when they need it. So this is basically a constantly streamed platform or data layer where Oracle values are assigned. Users can go request this and then bring that on chain themselves, where the users are paying the gas cost for essentially their needs and how badly they need the price and how fresh they need the price, and those are really the three models as we see them today.
00:07:29.870 - 00:08:05.766, Speaker A: Perfect. Okay, so if we put that into a practical context, then if I was a lending market that needed to run a liquidation, obviously the asset prices are incredibly important to making this operation run. And so with the push model, you'd have to trust that the Oracle network was properly pushing that data on chain at a regular cadence versus the pull model. You could say, hey, I need to go update this asset price like I'm requesting this value, but can you just double down on the callback method? I'm just not sure I'm fully grasping exactly how that works.
00:08:05.788 - 00:08:19.850, Speaker C: No problem. So essentially in the callback model, you can trigger oracle action by some action on chain. The oracles will detect this and then go derive some forum price. So the oracle action is explicitly after the user protocol action.
00:08:20.670 - 00:08:32.430, Speaker A: Okay, and so that differs from the pull model is because in the pull model, the oracles already on the back end would be like consistently refreshing this data so that they don't need to wait for the user action. Is that the difference between the two?
00:08:32.500 - 00:08:40.980, Speaker C: Yes. And with the poll model, the user may have some discretion on whether they still want to commit to their action because they know the price they're submitting beforehand too.
00:08:41.670 - 00:08:58.550, Speaker A: So is it fair to say that seems to be kind of the modern way to do it in the industry? Is that like the most exciting way to do it? Or are you guys firm believers in either the push or the callback being maybe a better suited, or is it just case by case, depending on the use of the Oracle?
00:08:59.210 - 00:09:34.530, Speaker B: Look, a lot of it is case by case, and it depends on the specific needs of the protocol. What we could say is on demand, whether it's pull or callback is always going to be cheaper from a gas and TX perspective for the end user and for those protocols than the push? Because by its very nature, every single update is something that is only being called when it's needed. So I think particularly on at least lower fidelity ecosystems, it makes a lot of sense for more things to move there as gas fees pick up and as more and more activity moves on chain going forward.
00:09:34.680 - 00:10:17.086, Speaker A: All right, quick break from the episode here to talk to you about access protocol, the easiest and best way to stay updated on what's happening in crypto. By following your favorite publishers, you can gain access to over 60 publishers, including Coin Gecko, Crypto, slate, and a whole list of independent creators. Most importantly, you can also do this without managing a bunch of different subscriptions. We all know how painful that can be so. How it works is you find your favorite publishers, you stake the ACs token, that's the access token, and once you stake, you gain access to all of that creator's content without the hassle of ads or subscriptions that you can't cancel or lose track of how many you have. Access protocol already has over 225,000 users that are finding new creators reading content and even receiving NFTs from their favorite creators. They're soon releasing V two, so check it out.
00:10:17.086 - 00:10:54.986, Speaker A: The link in the description to go give the protocol a try. It's an awesome product. It's crypto native and it's built on Salana. Okay guys, quick break from the episode to talk to you about Monad, the L1, optimizing the performance of the EVM, the team is working to materially advance the efficient frontier in the tradeoff between decentralization and scalability. The internal Devnet is currently live and public. Testnet is coming soon, and testing on the internal devnet indicates that the chain can handle up to about 10,000 transactions per second, significantly increasing the throughput capabilities of the EVM. This, of course, opens doors for new applications and more interesting use cases, even those with greater complexity and higher usage, to run in a decentralized manner.
00:10:54.986 - 00:11:39.286, Speaker A: Importantly, Manad is fully compatible with the EVM and the Ethereum RPC API, which provides EVM developers with a seamless portability for their applications. Given the popularity of the EVM today, this is really a no brainer to stay up to date with all of the latest developments. Join the Monad community by following them on Twitter and jumping in the discord. They're a lively bunch, so hit the links in the description below. All right, let's get back to the episode. If we look back throughout the history, largely the demand for Oracles has been driven by Defi. So I'd love to know if you disagree there, but if we look at some of the learnings from Oracle usage within Defi over the past couple years, there's a lot of very exciting ones, and maybe there's two I want to hit on, one being mango and the other being LSTs in lending markets.
00:11:39.286 - 00:11:50.830, Speaker A: So maybe we can start with the mango protocol. One, can you guys just briefly walk through what happened with the mango exploit, the role Oracle's played in that and some of the key learnings that kind of the industry as a whole took away from that?
00:11:50.980 - 00:13:20.122, Speaker B: Sure, yeah. So I'll go ahead and answer that. The mango attack was effectively a price manipulation attack on the price of MnGO and for those listeners that are not familiar with it, understand oracles try to relay price on chain and protocols, use that as a metric, whether it's for liquidations, whether it's, in mango's case, for unrealized P L, or for settlement of those positions, and for other things. The way that the MNGO attack effectively happened was the oracle relied on a number of different sources, but primarily places where the majority of market activity were happening, in this case, FTX serum, and I believe, Bitmax, ascendex. What effectively happened was the attacker manipulated the underlying price on those three sources with a sufficient amount of capital over time to artificially increase the price of MNGO and use that to manipulate the settlement system of the mango protocol at that point in time. These types of attacks, we wouldn't necessarily call them oracle attacks per se, we'd call them price manipulation flowing through the oracle. And what we mean by that is, at least in this case in particular, the oracle functioned exactly as it was instructed and as it was told.
00:13:20.122 - 00:14:32.610, Speaker B: What ended up happening was somebody who was willing to pump several million dollars into manipulating the underlying price of that asset for a low liquidity coin, relatively low liquidity coin that, at the time, books were relatively thin, you're going to be able to manipulate the underlying price. And the trade off that happens with these sorts of attacks is, at what point do you say, what's the fair value versus what's the market price? And again, these are not necessarily questions. For the oracle switch, four is configured in a way where anybody can configure their own system to use for this. For example, if you wanted to, say, have a T wop or an EMA, that would lessen the severity of an attack like that. You can do that. But ultimately, the specifics of this really depend on that particular asset and how that protocol happens to manage sort of the risk around it. There are some workarounds, but what I would broadly say is you can't stop a determined attacker who wants to manipulate the underlying price of an asset if they have tens of millions of dollars that they're willing to put through it for a low liquidity coin that doesn't trade that much on a given time period.
00:14:32.610 - 00:15:49.322, Speaker B: So in many cases, I think mango gets a bad rap for that when I think ultimately these sorts of attacks, the way to build more robust systems around it, there are a bunch of trade offs that there is no one size fits all right answer. Because, for example, if you put more risk controls around it, say, stopping withdrawals or timed withdrawals or limitations around things or using more twaps. Well, you hurt the user, experience. The users then can't seamlessly withdraw. They are asking, where are my funds? What is this? Why can't I take my money out? And those are real UX concerns that every DeFi protocol has to go. So, you know, from our perspective, we always take the approach of what is the problem you're looking to solve and how can we help? And if you need help evaluating those trade offs, we always try to be a resource, but ultimately these are not decisions, in our opinion, that oracle networks can make. These are decisions that ultimately every protocol needs to evaluate in their own context and in their own risk, sort of analysis of how they want to handle these sorts of both adverse risks, but also the trade offs between ux and safety, right?
00:15:49.376 - 00:16:12.722, Speaker A: Yeah. I mean, at the end of the day, the lending market needs to have tight risk params if they're going to be working with specific assets that maybe are low liquidity. And I hear you. It's sort of like the oracle gets put in a weird spot there. It's like, well, our goal is to provide you the price of the asset. And if the price of the asset is manipulated, sort of like what do we do? Because that is the price of the asset that we're giving you. And you mentioned Emas there.
00:16:12.722 - 00:16:22.518, Speaker A: I'm curious on your take on if the moving averages are a good way to present a price to a lending market, because I feel like there could be some externalities around that.
00:16:22.684 - 00:17:38.590, Speaker B: So a lot of it, I think if you just take one step back, if you just think about what is this lending protocol trying to do? You're trying to safely loan money effectively against a volatile asset. All these tools as to saying what it's worth, go back to one fundamental thing, which is actually what you want to know is what is the fair value? And the fair value of an asset for a loan may be very different than the last market price. The fair value for a loan will also vary depending on the size of that underlying collateral. You saw what happened with curve and the ave situation last year that a lot of the same sort of concerns were being brought up. So I think it's really unfair to specifically blame one protocol or one particular component when these are ux trade offs that affect everyone. And the way that we always think of it is what are you trying to accomplish? What are the risks? And how can you mitigate this while still maintaining a good ux? And again, I hate to say it, but there really is no one size fits all, every protocol is different. And these concerns also come up with other protocols and other things as well.
00:17:38.590 - 00:17:42.750, Speaker B: There's no way to really prevent all GAAP risk. You just can't.
00:17:44.530 - 00:18:26.014, Speaker A: Right? And that's honestly a perfect segue into the second phase of the question here, which was on lending markets and liquid staking tokens. So I think it was not too long ago, marginfi and Solen got into a bit of a Twitter debate, polite and not in a bad way, but just a debate over how to price Salana LSTs as collateral. And this is why I said it's a perfect segue, is you mentioned fair value. And if the liquid staking token itself, if it's fully redeemable for the underlying collateral, but there's some liquidity discount that the market gives it, all of a sudden, instead of trading at 1.0, now trading at zero point 99, how do you, as the lending market, sort of think about the fair value of that asset? So I'm curious if you guys have any thoughts around that.
00:18:26.212 - 00:20:39.510, Speaker B: Yeah, I think the question is, what's the mark price versus the fair value price, right? And again, when people are trying or thinking of loans, a lot of times they think, okay, we're trying to evaluate the fair value, not necessarily the market value, but that gets confusing for a lot of folks. Now, in particular, in this particular case, there is a fair value formula that you can come to for LSTs using, say, something like phantom, or using some other sort of on chain indicator of saying, if we redeem this LST right this second, for the underlying amount of, right, what would this be worth right here? But all those things. And no matter how good that fair value price is, you can't totally isolate from gap risk, right, from smart contract risk, where, let's say, God forbid, one of these LSTs gets hacked. Well, no matter how good that fair value is, no matter what happens there, that lending protocol or that other tool is still going to be subject to potentially a massive amount of under collateralized collateral if there is some critical vulnerability or some critical attack. And so when we think of these things, at least from our perspective, we think less necessarily of the market value risk, especially if you're using twaps or using some indicator that links it back to the underlying. And more about, okay, how do you think about gap risk? Where if there is some critical attack or critical vulnerability, how can you ensure that your pools or your collaterals are still good for the entire system, that the entire system remained solvent? That being said, in terms of valuation, in practice, unless one of these factors happens, right? Unless there's a massive hack, as long as there's some more closer link if you will, to the underlying, it's less of an issue. And what we mean by that is as long as the liquid market between say, marinade soul and the underlying of soul is liquid enough and there's some redemption mechanism and there are some bounds or protections to potentially prevent against manipulation, the specific approach that one might take matters a little bit less here from the end user perspective.
00:20:40.970 - 00:21:00.206, Speaker A: Right. And that's interesting because at the end of the day there's sort of like a spectrum of risk tolerance and to which parameters you'd choose. And there's going to be lending markets that build on either side of that spectrum. So do you as the oracle provider just say, look, we're going to give you the platform to get you that price feed configured however you want it. Is that kind of the goal here?
00:21:00.388 - 00:21:44.800, Speaker B: That's the goal. I mean, I think in particular for LSTs, what I think is a fairly reasonable design, right, is kind of bounding it by whatever the sanctum prices is up to a certain size. But again, I don't want to say that there's a one size fits all here because every protocol is different. And quite frankly, for a pool of, say, $10 million versus a pool of, say, $500 million, there are very different risk concerns here. And once you get up to the larger numbers and particularly pool concentration, you start to think, okay, how do I protect against, say, gap risk or smart contract risk here, which is, in my opinion, sort of the existential elephant in the room beyond sort of the individual market risk, assuming that you've put the protections in place.
00:21:46.450 - 00:22:10.706, Speaker A: Right on. And I want to switch gears here slightly then, to the current iteration of the protocol V three, which launched fairly recently and is really kind of powered by tees or trusted execution environments. So let's kind of jam on the tech side of this and walk us through maybe an overview of what tees are, how they work, and sort of why they're important in the context of an oracle.
00:22:10.898 - 00:23:14.486, Speaker C: Yes, let's do that. So really what Tes are, are a special type of hardware where when you load a program onto a certain chip, you can actually verify that is this program running. And any output from this program is from, let's say an oracle, from an oracle that's actually registered on chain. So how this actually works is when you load a program onto a te, it creates something called a measurement. And this measurement is in every single signature of every output we can produce from, say, one of our oracles. And as long as we have the signature and we put this measurement on chain, we can go then verify that any single price point that we post has definitely come from this oracle's binary from inside of this chip. So beyond just having economic security, we can actually have a hardware security level making it probably the most secure option that you can go to with an oracle, as well as the most transparent because you can verify the code that actually produced all of those signatures.
00:23:14.486 - 00:23:46.050, Speaker C: So through and through, we try to make the off chain component of oracles as verifiable as possible. Now, since the essence of oracles is fetching data from off chain sources, and how do you really verify something that didn't have something like a reflective signature? On chain is the word I'm looking for. And the way you do that is through some medium that is like a how do you secure operator that people know as oracles. But to actually have a verification of what the oracles are doing is just the next step of, I think, the realm of oracles overall.
00:23:47.830 - 00:25:01.274, Speaker B: And Dan, just to add on know, I think the way that we think of things is really an evolution. Right? You start off with, say, price feeds, you go to for commoditized things like BTC USD, you then go to customized price feeds, and then you go to sort of the latter point, which is really just customizable data feeds more broadly, of saying there's no one size fits all here. How can we make sure that we have the right tech, the right software and the right infra to enable anybody to bring the data they need on chain easily, permissionlessly and securely? And I think it's all going in that direction. And ultimately, what leads us down to choose any tech stack one versus another, or our design decisions really reflect that. How can we be as broad and generic as possible with this to support as broad a number of use cases while having a high quality solution? Because just as an example, we spent a year trying to really explore how can this be done using ZK. And the ultimate conclusion was, from a technical perspective, for our particular use cases and our users particular use cases, the tech's not quite there yet. That doesn't mean, that's not to say that it won't be in the near future or that it won't be down the road, but for the particular problems that are being solved right now, stuff is still early.
00:25:01.472 - 00:25:18.642, Speaker A: Okay, I want to dive down that route because when I was doing some reading up on this, it sounded a lot like you need to prove something without revealing the process of what was being proved. I was like, oh, this sounds a ton like ZK. When you say the tech wasn't there yet, is that largely on the cost side or a different piece of the puzzle here?
00:25:18.696 - 00:26:00.910, Speaker B: Cost and latency, both. Okay, now, to credit, this is getting a lot better, right? We're technologists. I think every protocol and every company in the space should be open to using whatever technology they need to, to make sure that they're solving the problems for their users in the best manner as possible. So I would say that it's definitely something that we're still exploring. But in the short term, the teas were really based off just a simple problem, which is or not simple, but a clear problem that our users are facing, which is you need a safe, secure way to bring customizable data feeds and customizable compute on chain in a latency sensitive manner.
00:26:01.410 - 00:26:20.754, Speaker A: Okay, this makes sense. And so then t's were sort of a way to get there today type thing. And let's say the ZK caught up tomorrow just poof, magic. And zk tech is here. It's scalable and it's cheap and it's quick. Would that be like a net improvement on tease or just sort of like different set of trade offs?
00:26:20.802 - 00:27:02.500, Speaker B: Let me make it even easier. Right. I think if you think of where things are, know, why do you even need an oracle to begin with for certain things? And it's to right and verify that this particular data from this particular website or this particular blockchain or this particular thing is what they say it is. Right. It's sort of trust minimized structures to bring data safely and securely on chain. If ZK helped us do that in a way that met our customer needs tomorrow, of course we would do it. From our perspective, it's what is the best technology for the end user? I think a lot of technologists really get hung up over the beauty of things versus just having something that works and is there and available for your end users today.
00:27:02.500 - 00:27:19.080, Speaker B: And I think the trade offs in crypto are there, and we're very bullish. What's going on in the zk side? We're very bullish. Crypto more broadly and info more broadly. But there are problems today that need to be solved, and we chose the best stack for that based off those problems.
00:27:19.770 - 00:28:07.880, Speaker C: I also want to add to this point. When you're in the world of ZK, you also deal with the world of deterministic start and end states. When you're dealing with an oracle where the inputs are usually produced by another entity, you don't have a provable start state. So for the benefit ad of ZK at a certain point kind of becomes moot because there's no way to prove in this whole ecosystem of say, a bank what they're pricing these assets at. If you're a first party data provider for some type of oracle, where is that zk stack? Terminate or begin, I should say, in this whole provability scheme. So it's really about the cost benefit of the trust model of the processing of data putting on chain versus the production of the.
00:28:09.210 - 00:28:29.200, Speaker A: Right, right, okay, so I do like the pragmatic view there of at the end of the day, we just can't use zk today. So that landed us at T's and you guys have really interesting use cases for these t's and putting them in functions. So walk us through what functions are, because ultimately being able to execute code off chain and then bring that data back on chain and say, hey, we did this.
00:28:29.730 - 00:28:30.682, Speaker B: Why is that useful?
00:28:30.746 - 00:28:33.550, Speaker A: If I'm an app developer? Why is that like, oh, this is an unlock for me?
00:28:33.620 - 00:29:31.326, Speaker C: Yeah, so you can think of it like this. So in our V two, we deal with purely quantitative data and verify that quantitative data in functions. How it works is that we use Tes to verify an oracle's execution of essentially a whole process to create a full transaction. And then the Oracle can sign for that transaction in lieu of some settings you have configured as the user. So you as the user could say, this is a script I want to run oracles, can you do this for me? And then produce a transaction that's definitely an output of my code. And because we're inside Tes, we can emit something called a quote in the te world where it creates a signature saying that the Oracle binary ran your code and this was a transaction that was definitely emitted from your code. And then you can execute that on demand or on an interval such as like a routine if you need scheduling.
00:29:31.326 - 00:29:57.420, Speaker C: And that's really the entire processe there. The cross benefits of that is it's definitely a higher execution path or higher fidelity execution path to just quantitative data. So if you're looking for scheduling and routines, functions is a great path forward. If you're looking for high fidelity, you want to choose your data more carefully in such a way that it is most efficient with the Oracle's processing power to actually push data on.
00:30:01.230 - 00:30:07.358, Speaker A: Users. How are they using functions today? And what are some interesting use cases that you think people will be using in the future.
00:30:07.524 - 00:30:58.826, Speaker C: So with functions there are some benefits with cost. I would say a big benefit is with cost because you can push a lot more data at once on chain on a schedule. So some of our function users push an aggregate of the six major centralized exchanges on chain which can produce 800 data feeds at once at a fidelity of 10 seconds at a time. It's still fairly fast. And for the data that's being produced, that's amazing, that's great. And for some use cases that's all you need. But some people want to have that one slot latency and for that you might need another solution, which is why we're right now working on a switchboard on demand model where it takes advantage of some of these te mechanisms as well as some of these pull mechanisms to only execute code that you want when users request it off chain.
00:30:58.826 - 00:31:10.306, Speaker C: And as the pull model was referenced before, users can then take a signature aggregator from these oracles and submit it with their transaction to then choose the gas cause to submit it in the lowest latency form possible.
00:31:10.408 - 00:31:17.220, Speaker A: What if I have like an API key in my code? Can users like other users on chain? Perfect.
00:31:18.410 - 00:32:08.760, Speaker C: So it's fantastic that we have this enclave set up already because being in enclaves does not only mean your code's verifiable, it also means it's confidential. And we actually have mechanisms to do a two way signature verify between enclaves and distributed key value stores that users can go store their API keys in. So now users can go store their API keys in this key value store that can then be fetched only by their feed or their function. And now you have a data feed that can take these API keys and say fetch data from some subscription model that you have for yourself that you need on chain. And now you've just closed all connections between getting this secure private data on chain without revealing the access to it as well.
00:32:09.130 - 00:32:27.750, Speaker A: Okay, this is really cool. How does the verification work? How can a random third party know that hey, this is the code I ran and I guess I'm missing the gap there between. How does that get proved that the code that I said I ran actually is what I ran?
00:32:27.910 - 00:33:03.750, Speaker C: Yes. So the way that Tes work is that they have an entire certificate scheme for showing when something ran inside a tee. When somebody does this we can verify that certificate scheme and then show that we had inside that certificate scheme. We also signed a public key that we can prove was only made inside the enclaves. And now we have an ED 2519 key pair that we have essentially verified has never left this confidential, secured space and now we can sign whatever we want knowing that it's definitely from a switchboard oracle.
00:33:04.410 - 00:33:31.886, Speaker A: Okay, that is super cool. And I love how that kind of closed the loop there for me on ZK is proving things without revealing what you're proving. And that makes sense because the key pair was created within this set of code, you know, it cannot exist elsewhere. That is a very interesting use case here. Wow, okay. My mind's a bit blown. Okay, so, and then I was rolling through the docs before this as well, and secrets did get mentioned there.
00:33:31.886 - 00:33:45.300, Speaker A: And secrets then I guess are really just the idea of that you mentioned where you're hitting that external key pair so you can kind of connect it to and again not reveal something like an API key or other important piece of information.
00:33:45.830 - 00:34:42.726, Speaker C: Exactly. And we have different schemes for how we do that. For some of them you might only want to reveal it for an entire enclave, which would be your function in switchword on demand. We have one enclave that's running all of the data feeds and we can actually map revealing your secret only to your data feed inside of this enclave. And since it's inside this enclave, we have full control of where that secret is being processed, where it actually gets sent off to a third party to be used, and how often it can actually be used so your keys do not get abused. So essentially we have our oracle binary that has a measurement that we know that is our oracle. It can then ask from this secrets key store saying, hey, here is my certificate of verification that I am in an enclave, here's my pub key, please send me back my secrets with the pub key.
00:34:42.726 - 00:34:52.940, Speaker C: So I have them on the enclave now, or the oracle now. And then in the oracle in the key pair store, we can see which feed they actually want to reveal those secrets to.
00:34:54.670 - 00:35:01.006, Speaker A: This is so fascinating. I just love to see this innovation on this side. Does this exist elsewhere in the industry or is this something that you guys.
00:35:01.028 - 00:35:23.460, Speaker C: Are pioneering for secret information built into permissionless oracles? I think we are the leader in permissionless oracles. I don't see anybody else trying to provide private data to oracles for this permissionless set of verification. So as far as I know, we are the first to go and do this.
00:35:25.590 - 00:35:49.290, Speaker A: Super interesting. And one of the other things you mentioned was your data feed, right? And if I'm running my own code, of course that's going to be my own data feed. What is the setup? If I wanted, just like a standard BTc USD price feed, am I going to create my own derivative price feed there, or am I just going to use a general price feed for that that already exists on chain.
00:35:49.870 - 00:36:54.160, Speaker C: If you've used switchboard v two, I assume you kind of know the context of the feed builder and how that looks by you connecting certain sources, you doing a transformation and that aggregating into a single data point, and then you own that account. In this next iteration on switchboard on demand, we kind of abstract some of that away into this new concept of just producing a feed hash of all of the work you want it to do. And if you want to make a job schema that's identical to somebody else's BTC USD feed, then that's going to be a single data feed. If you want to add one more source, then the hash is going to be different and that's going to be put into a separate data feeds data bucket, as they say, values on chain. So when you're saying BTC USD your data feed, it comes down to really how you want to structure your BTC USD feed. But it will be a shared resource because we can know that all the oracles are executing the exact same jobs and if it executes more often then you would just call it their higher fidelity should be, or more samples in your data set should be no problem.
00:36:54.850 - 00:37:22.678, Speaker A: Interesting. No, that makes a ton of sense as well there. And the last thing I want to jam on, on the kind of services offering side before moving into the more network side. Whenever you start reading about oracles, you always come across randomness. And the idea of bringing randomness or a random number generator on chain, it just always comes up in oracle discussions. Right. And so why is it so challenging to create true randomness? And why is this important in the context of on chain applications?
00:37:22.854 - 00:38:21.946, Speaker C: Yeah, definitely. Well, when you think of randomness, there's plenty of ways you can really think about it. And whenever you create a number on a computer that's not just spun up out of thin air, a lot of computers can take their heat signatures, the number of CPU cycles that executed in a certain time frame to produce randomness. But when you're a third party, how do you really prove to yourself that's how it was executed? And the answer is it's extremely difficult to actually prove that the producer of randomness had no context into what the value could have been. So removing any bias from that system, from these high risk markets is quite an operation. So there's different ways to do that. With switchboard randomness, we actually source it from inside of our enclave and we can produce that whole quote system that we showed that we actually have signed all of this randomness with a key that we've definitely made inside an enclave.
00:38:21.946 - 00:39:41.906, Speaker C: So we can use an enclave secret, which is an embedded hardware secret that we know has not been revealed deterministically by the helicop manufacturer, that those secrets are secured by these TEs. If there's other models to still distribute that trust between multiple parties, which is another model which is commit reveal, in which a user can go and submit a seed to one of our enclaves that produce randomness, and because the seed was not known to the enclave before the request was actually made, and we actually know which code the enclave was running to produce a secret, neither the enclave or this user could have known the output of the randomness before this request was initiated. So it really kind of goes into what parties are producing something to create this output, and when the user action was really taken to decide what they should do as a result of this randomness. So my favorite model is a commit to reveal scheme where you have two parties, but that can be slower because that takes user action from these protocols. And some people need to stream randomness. So if you can trust the enclave models solely, then there is a model. We can stream randomness if you want multiparty.
00:39:41.906 - 00:39:49.206, Speaker C: So it's not solely on the enclave or the whole security model that the enclave provides. There is the commitment real scheme that you can do as well.
00:39:49.388 - 00:40:01.950, Speaker A: So a basic example of why I would need randomness is like some silly lottery game where I have to pick a random winner. But you just said something interesting there, which was stream randomness. When would I as an app builder need to stream randomness?
00:40:02.690 - 00:40:38.360, Speaker C: So there are times that you need this in a very high fidelity manner where, let's say, every single action on your DAP needs some new value produced, and you don't want the user to have to go cycle between calling something on switchboard to produce a new value before this action is taken. In this case you might need a new unique value on every single user action before you can actually execute something. So at some points or some apps, they might have a bottleneck around the production of randomness to essentially act as a crank for their application.
00:40:40.650 - 00:40:58.190, Speaker A: Awesome. No, that's great context there as well. And maybe jumping into the network side of the discussion. Know, we've talked about a lot of these use cases and they fall back to referring to this oracle network. So what does that set up look like for you guys today? How many nodes are in the network and how can you be eligible to participate in this network?
00:40:59.650 - 00:41:47.070, Speaker C: Great. So right now I believe we have the magnitude of 20 nodes. I would have to check the actual stats. I think, Chris, we can pull that up, but we have a number of actual relayers that are all very helpful in producing the best quality network that we can actually offer and for people to actually come and get involved in the network. We are making a very large push for the switchboard on demand onboarding model now where it actually ends up looking like a web two service for you where you are hosting this rest API for users to go call. They can get signatures from your oracle and then you can submit responses on chain. And the incentive system is very similar to d two where the oracles are incentivized for the more responses they actually can fulfill on chain.
00:41:47.070 - 00:41:54.800, Speaker C: We currently have a mint that's linked to wrap soul for them to produce some positive outcome for them there.
00:41:55.170 - 00:41:55.678, Speaker A: Yeah.
00:41:55.764 - 00:42:13.640, Speaker B: And I should add, dan, the node partner program is open to folks. So if you're interested, reach out to us on Discord or via email and we'll get you in that beta program. I should note anybody can run their own queue or their own system of nodes, but the goal is to really ensure that it's open to anybody in the community.
00:42:14.810 - 00:42:45.954, Speaker A: That's awesome. And we'll put the link in the show notes to the discord and the other socials so listeners can go have easy access to finding that as well. And how do you guys think about the importance of decentralization in this oracle network setup? Because obviously if there's one, then only one node, then everyone just ends up trusting this one node. But there are inefficiencies to scaling that to 100 million nodes, right? Because that can introduce latency and trying to get an aggregation from all of them. I'm just curious where you think on that spectrum you kind of want to end up.
00:42:46.072 - 00:43:40.130, Speaker B: Yeah. So I think this is just a more broader question for all oracles, right? Everybody has that main limitation, if you will, where you are relying on some centralized web two service in some way, shape or form, right, for some piece of data. There's no way around that. What we try to do, and really ultimately, honestly, everyone tries to do is ensure that there's enough multiple redundant sources for whatever that might be, whether it's nodes, whether it's data providers. You can insert sort of your choice there so that if there is an issue with one in particular, it doesn't affect the whole network. And so as you might imagine, switchboard does that we have multiple independent operators running nodes and multiple independent network participants. But most importantly, we're the only oracle network out there that lets the end user permissionlessly configure how they want the feed designed so that end user can decide to have multiple redundant sources.
00:43:40.130 - 00:44:20.190, Speaker B: And again, to their own point, if there's ever an issue with one centralized exchange, ensuring that, say, another centralized exchange is there as a backup source, if you will. And so when we think of all of that, it's really just designing it for your particular use case, ensuring that there's enough backups and redundancy in place and making it so that if there's some critical issue or something goes down, your application is not impacted and can carry on from a business continuity perspective. And we always give this example. But to a larger extent, it's also true with oracles, you wouldn't just depend on one centralized exchange, why would you only depend on one data source.
00:44:21.970 - 00:44:56.762, Speaker A: Right? No, and that makes a ton of sense. And we hit there in that previous answer from Mitch with kind of talking about how do you reward good behavior and how do you disincentivize bad behavior, but I want to expand on that into just the broader idea of economic incentives and economic security, if you will. What is the setup today? And is there a staking token? Is that something you want to eventually have? Not necessarily from the token aspect, but more of putting value at stake that can be slashed, sort of like a typical proof of stake model that you see with some of the l ones today. Yeah.
00:44:56.816 - 00:46:17.390, Speaker B: So at the moment we use a combination of economic security as well as trusted enclaves and hardware security to ensure data integrity with requests from just how we think about both factors. I think there's always been the issue in oracle literature and in research of, okay, if the total value being secured or at risk is larger than, say the stake, there's always a risk of an attack. And in some ways there's some validity to that, but in other ways it's just not the case because of the nature and types of attacks and the specific risks. So just as an example, a better way to think about it would be what is the total value at risk at one point in time from one particular transaction or one particular feed? And quickly evaluating sort of how that fits into the rest of your system, you can sort of evaluate, okay, what are the trade offs needed? And just as an example, right, if you have a feed for, say, hamsters, I'm making it up. There was the animal racing meme that was going on a few months ago. If the total value of that feed is say $100 or $1,000 or some smaller notional amount, well, you really don't need to ensure that there's tens of millions of dollars securing it. It's just sort of those cost risk reward trade offs.
00:46:17.390 - 00:47:09.960, Speaker B: And so evaluating that with the context of specific feed creation we think is really important. But the second thing is this is a big reason to use trusted enclaves in our design is to ensure data integrity and ensure security in addition to having the economic security. And from our perspective, our goal is to make things as secure as possible and ensure data integrity at every step of the process. And so if there's design decisions that we can do to make things more secure, we're going to do that. Especially given all the lessons that we've learned and the fact that we're now securing almost over $700 million of TVs. This is top of mind with us. And I really have to thank our wonderful auditors at Ottersec for working together on a lot of these particular issues and ensuring that we have robustness and we have security that can help ensure the growth not just of the sole ecosystem, but every ecosystem we work with.
00:47:10.650 - 00:47:46.410, Speaker A: Yeah, no, that's a great point about if you do have this staking asset and you are trying to bring the world of traditional finance on know, then you sort of need trillions of dollars at stake or else this might not be know like chain lake spending a lot of time thinking about how do you bring the RWA world or that tokenized asset world on chain. What do you think is going to be the largest source of demand for price feeds or just oracle networks in general? Is it going to be some of these more on chain price feeds like BTC USD, or is it going to be like evolve into being tokenized asset prices.
00:47:46.570 - 00:48:45.140, Speaker B: So our thesis and when we started was that the future of everything is going to be on chain. You're going to see more and more commerce happening on chain in some way, shape or form exponentially growing over the next 1020 years. So from that perspective, again, I tend to think that the uses of oracles and the uses of bridging off chain data on chain are going to vary dramatically as the entire ecosystem, 100 x's and overall activity on chain grows from here. Whether that's tokenized stonks and stocks, that's another totally separate discussion there. But I do think you're going to see more and more of this in some way shape or form. Now, what I would say is on the RWA side, a lot of things are limited by the regulatory environment. Don't want to go too much into it, but there are certain blockers that make it commercially unfeasible at the moment.
00:48:45.140 - 00:49:22.670, Speaker B: Whether or not those changes, we'll see. But I do think a bit like Uber, right? It's just a better experience for the end user. Crypto is a better experience, at least on the RWA side, for a lot of different applications than, say, engaging with the traditional financial system. And I mean, if you think about getting a hard money loan, say, against a piece of property versus borrowing against, say, your bitcoin or ETH or Solana, it is far more easier to go borrow on, say, Soland or mango against your crypto there than it is to go try to get a hard money loan and borrow against physical real estate.
00:49:23.650 - 00:49:55.498, Speaker A: As somebody trying to buy a house right now, I do agree strongly. So we've mentioned the total value secured here a couple of times, and you guys have had great success over the last few months hitting and crossing 800 million. But we look at the Oracle market as a whole. That 800 million is about one and a half percent of the total value secured, and chain link is at about 53% here. So I'm curious, when you look out at the market, what do you kind of internally say, like, okay, what does switchboard need to do to become the largest oracle network in the space?
00:49:55.664 - 00:50:56.720, Speaker B: So I want to rephrase that. I think this is kind of phrased in a very competitive way. We try to think of things in a positive sum world, and what we really think is this entire market is going to 100 x from here. There's going to be room for multiple players on different verticals and different ecosystems. So when we're thinking, we're not thinking necessarily competitive focused. We're thinking, okay, how can we help power these new use cases that we think are going to help 100 x the size of this market and bring the next generation of people on chain? So does it make a lot of sense for us to invest tons of resources, say, trying to convince a lending protocol to switch to us versus another oracle? To be frank with you, that's less of our emphasis as much as finding new unique use cases for, say, for those lending protocols to grow with them and help grow their existing market versus trying to compete on a zero sum basis. And you talked about there being, I think, 50 billion of TBS there in total.
00:50:56.720 - 00:51:16.820, Speaker B: I don't see why this can't be 500 billion or trillions of dollars as more and more assets and more and more things are being brought on chain, and in that world, what we care about is more getting some percentage and growing with that pie than necessarily trying to engage in zero sum games that ultimately are less helpful than trying to grow together.
00:51:18.070 - 00:52:04.270, Speaker A: No, yeah, I definitely agree there. I didn't mean to come off in a competitive way, was mostly just trying to give the listener a frame of references. Largest market player in the space today has about 53%, which that is like a more winner take all kind of cornering of the market there. But I do agree with you, there's just so much room to grow that the infighting in this space as a whole is a bit exhausting at times. So it's really refreshing to hear that kind of rising tide lifts all boats take there. And maybe as a closing question, then when you kind of have that sign on the wall, right? You're like Ted Lasso believe sign. Right? What are the key differentiators that you as a team are really proud of and kind of bring you into feeling confident about going forward in the future? What are those unique value drivers within switchboard?
00:52:04.430 - 00:52:47.422, Speaker B: Yeah, I mean, for us, it's really being able to help new and upcoming teams and grow with them and help them quickly and easily bring new products to market. I mean, when I think of what's interesting, right. It's not one particular technical component. It is for us, but we're the exception. When you think of things like that, it's really saying, wow, we just shipped this brand new feature or this brand new market that was uniquely possible with switchboard. And we started switchboard really to help people quickly and easily bring customizable data on chain. And I know it's a mouthful, but the truth of the matter is we're really proud and still excited by that mission, and we want to make it as easy as possible for people to get the data they need on chain to power this next generation of both Defi products, RWA products, but also gamefi.
00:52:47.422 - 00:52:51.700, Speaker B: Interesting use cases that we couldn't have even imagined a few years ago.
00:52:52.870 - 00:53:18.490, Speaker A: No, I love the vision there. And Chris, Mitch, this has been a fantastic conversation. Really appreciate the both of you coming on again. We'll put the links in the show notes to both Chris and Mitch as well as switchboard as a to. This is a random nerd thing here, but the docs are incredible. I love when I dive through some docs and they actually tell the story that we're looking for. So again, switch forward is a very exciting protocol, and I'm very excited to watch it grow.
00:53:18.490 - 00:53:21.646, Speaker A: Going forward. And thanks again for coming on. Y'all have a great rest of your day.
00:53:21.828 - 00:53:22.874, Speaker C: All right? Thanks, Dan.
00:53:22.922 - 00:53:23.260, Speaker A: I'm ready.
