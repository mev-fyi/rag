00:00:09.040 - 00:00:34.066, Speaker A: Hello everybody. I am Mas. I go by rain and coffee on Twitter. I do research at a fund out of Amsterdam called Maven Eleven. I primarily look at interval ability, CK, MeV and things of that nature. And one of the things we're big fans of are IBC. We're big IBC fans and one of the things that we care a lot about is bringing IBC beyond where it is now, which is primarily just a customs ecosystem.
00:00:34.066 - 00:01:41.418, Speaker A: So what this is looking at is why the hell do we even need IBC? And why don't all chains have IBC? And why is it important to have standardization within IBC that is being brought beyond the ecosystem it's currently in? So this is sort of a continuation of some of the stuff that we have done previously. We had a long article on the security and trust assumptions that we have in blockchain bridges, and what extended came down to was that secure and trust assumptions matter. It's pretty important to have strong security and strong trust assumptions when you're bridging, and that generally you don't want to trust any smart contracts. You want to trust like clients. So in a perfect world, no bridges are being handled through smart contracts, but just through lite clients and consensus verification. We've also looked beyond IBC previously, primarily with bringing IPC to Ethereum, but also other ecosystems through serial knowledge proofs and what we call serial knowledge, like client verification. This is both with recursion and aggregation, which can also help bring down costs.
00:01:41.418 - 00:02:09.680, Speaker A: Because some of you might be aware that verifying tendermint blockaders is pretty damn expensive. And I think most chains at this point actually subsidize this cost. And this is just natively between customers chains, which is obviously not ideal. And there's a lot of other teams that are working on this as well. Not just polymer, but plenty of others as well. And there's even hybrids of this that could be quite interesting to look at as well. The reason that we like appchains to begin with is because of the idea of warehousing block space.
00:02:09.680 - 00:02:51.032, Speaker A: You get your own block space, you can define your own sequencing rules. You want sequencing rules for Dex Imm you can do that. Or if you have a specific purpose that's completely different, you can do that as well. But there are big problems still, and that is the fact that most chains beyond customers don't have native interoperability, they don't have enshrined transport layers, and they are trusting third parties, state or middle chains or smart contracts, which is not ideal. But still, despite all of this, the UX is still pretty bad. If you are a native and often user of cosmos chains, it's not exactly ideal. Now, what we've seen happening over the last couple of months and years is this campaign explosion of cosmos chains.
00:02:51.032 - 00:03:22.020, Speaker A: This didn't used to look this way, but this is from a few days ago. We have a ton of chains north of Flex 70 that have IVC enabled. Most of them, if not all built with customers DK and sender mint. But in our opinion, this is just the beginning. I think we're going to go beyond this with IBC, and we are seeing this happen already now. So we have a ton of different products that are working on bringing IBC beyond the ecosystem with various degrees of standardization. I'm not going to name names, but not all of them adhere completely to the IBC specifications.
00:03:22.020 - 00:04:04.790, Speaker A: We also have people like polymer that are trying to bring sort of a native transport layer built on IVC to enable channels that are specific to IPC. And then we have the first part of what we call native cross domain applications. These are products like Catalyst, Squid, Somalia Ume that are natively cross chain and that you can interact with on different chains and do a lot of very cool stuff as well. Now, if you are not completely aware of how bridging works, I'll give like a quick rundown. And this is not just IPC, it's our chains as well. But four things that are quite important is that you always have some monitoring. So you need to monitor that the state of the chain you're connected to is actually correct.
00:04:04.790 - 00:04:28.160, Speaker A: Usually we do this with like clients in IPC. It could also be validators like on XLR that run full nodes and all the connected chains. Or it could be an oracle, like with layer zero, for example. Then we have relayers. They sit in the middle, it's in the name. They relay a message from one chain to another. And then we have some consensus somewhere, which could be natively on chains like we have with IBC.
00:04:28.160 - 00:04:54.992, Speaker A: Or it could be with Axela, where you have a state middle chain that is sort of off chain from the two chains connected, that will sign up on something being true. And they always need to run full nodes for this. And then we have signatures. So this is a way. But how IPC works is that you get some signatures from the validators. You pass that message on and you verify that signature as well. You can also do this with ckbs, so you can verify this signatures of a specific chain and that some state is true.
00:04:54.992 - 00:05:23.270, Speaker A: And you can then post that as a serial knowledge proof or a stability proof. There's no serial knowledge in that setup. Now, with IPC, the way it works is very simple. Each chain wants a lite client on the other, and you have a relay in the middle that relays a message. Now what's important to have is this IPC module that can take in these messages and can verify them correctly. And it's also important to have a handshake. So this is often something we see on governance forums.
00:05:23.270 - 00:05:59.320, Speaker A: If you see a new chain launching, there'll be a governance proposal that says let's open an IPC connection or channel to this specific chain. And that's important to have. So the two value assets agree on a specific like client being the correct one. Now by verifying these two chain state, we can essentially be pretty convinced that you have finality on both chains. So this is what we do with the lite client. So the lite client from chain a verifies the state of chain b and vice versa. And by doing so, we can have pretty high conviction if we trust the full nodes that we have finality on both chains.
00:05:59.320 - 00:06:32.300, Speaker A: And all they do is actually quite simple. So the lite clients essentially go and look at the block headers. So this is a state root, the top of a mogul tree that has all the information of what's in the mogul tree. And then you get state from an honest trusted node. Obviously, if all the full nodes in a system are dishonest, then lite clients are not very useful. You need some honest node to be trustworthy to give you the correct state. And this is quite important because if you don't have that, you are in a lot of trouble.
00:06:32.300 - 00:07:04.622, Speaker A: This is very expensive, as I mentioned, if you want to verify this on customers chains right now, it gets pretty expensive. So it gets subsidized for the most part. Otherwise us sending IBC transactions would be a lot more expensive than it is now. If you want to verify this other chain that has congested or extremely expensive block space, like Ethereum for example, it gets even more expensive, which is not ideal. Now, why don't we just get IBC out of the box? Well, there's quite a few reasons. So we have dependencies on IBC. One of them is a module system.
00:07:04.622 - 00:07:40.498, Speaker A: This is what we have with Cosmos chains, where we have authorization bank modules, IBC modules and so on. We also need some introspection so we can see what the state of consensus was. Sometimes that happens with a full node, like with XLR running full nodes on Ethereum and other chains as well. But we need this for like clients and IVC, reliance on lite clients. And then we need to support timeouts. So I think Jim touched a bit upon this earlier, but we need timeout. So if message doesn't get passed, we can timeout the message so the message doesn't get actually passed and thrown over, and that we don't get weird scams.
00:07:40.498 - 00:08:13.838, Speaker A: So we need some sort of time limit and some sort of concept of time as well. And then we need a port system and this is for IPC to interact with other modules. So if I'm using a near chain account on query, I need to be able to interact with the different modules. Could be an account module that has all the list of who owns what on each chain, but that's quite important. And we need a rollback system so that we can essentially roll back and revert state changes in case of some kind of fraud. And then we need data availability. This is quite important.
00:08:13.838 - 00:08:47.282, Speaker A: This could be from Celestia, it could be from your own chain if you're a monolithic chain. But we need some conviction that the data of a specific set of transactions are actually valid and available. And you'll also want deterministic consensus. So like Ethereum l, one doesn't really give very good deterministic properties. It does to some extent, but actual finale Ethereum, usually between twelve to 15 minutes depending on the active validity of set. Now one thing we can do is use validity proof. So this might, you might notice SDK proofs.
00:08:47.282 - 00:09:32.172, Speaker A: And we can essentially encode the logic of a light client into a circuit and use that to do a very efficient verification of batches of transactions or batches of block headers. And then by using those previous blockades as we have verified, we can sort of construct a chain on chain as well. And this is also a good way to get trust minimized verification of something, because you can obviously verify the computation that's been done correctly. And if we do recursive proofs or aggregated proofs, this is what we see over here. We can actually take a ton of different block headers. So let's say we have 20 blocks, we can verify all the blockaders together for a single proof, and then we can post that on chain. And this means we can have a lot of transactions within a single proof.
00:09:32.172 - 00:10:31.112, Speaker A: So we get a high density, but with the same actual verification cost at the end of it. Now what might have become clear over the last couple of months and years is that non native bridging that's not a part of the protocol is super, super dangerous. So we have seen by far the highest amount of exploits with relation to bridges, which is also why applications that are building stuff with bridges or question measuring protocols is quite dangerous. And you need to really care about trust, assumptions and security. So if you are going out of protocol, like with most bridges beyond IBC, there's third party risks, you're trusting third parties, you're taking multi sig risks, you're taking smart contract risks, and there's no standardization between messaging. So if you're an application and you want to interact with some application using a different standardization, you might not be able to do that, which causes be quite problematic. But by using a sort of native bridging solution, we can also put trust in the social consensus and the signatures of that specific chain that we're connected to.
00:10:31.112 - 00:11:23.664, Speaker A: Now, there's still some issues here that we need to solve. One of these is fungibility, and we'll get into that in a little bit and even change the share same da layer like ethereum roll ups on Ethereum or roll ups on Celestia. Even bridging between this gets quite difficult because we have challenge periods with optimistic roll ups, we have proving times for validity proofs and so on. So there are still problems with speed and latency that forces people to go to third party bridges, even if there was a native bridge involved as well. This common ground with IBC also brings forward a lot of interoperability, which is very, very cool. We get indexing accounts, in essence in query, which is very important specifically for the proliferation of staking that we might see coming out soon. But LSM initiation queries, Internet accounts are a big part of actually getting cross domain staking, which is going to be very cool to see.
00:11:23.664 - 00:12:26.420, Speaker A: And we get these transactions or messages that go from being from this specific chain, being agnostic between a ton of different chains at the same time, which is very nice. Now, one of the things I talked about before was the issue of fungibility. So chains and customers, you can go from chain a to chain b to chain c, but if I have the same token from the same chain, I go chain a, D, e, and over here, then those tokens cannot be used in the same liquidity pools, so they become non fungible tokens, which is a big problem if you're going through like two different chains going to osmosis, I can't interact with a chain with one that I've gone from say XLR neutron osmosis. If I go from XLR, Cosmos hub osmosis, those can't be used together. So what we want here is path unwinding, which is energy, always sending the token or message back to the source chain of that specific token and then back to the destination. So the last chain it touches before the new destination was always the source chain. So they'll always be fungible with all other tokens as well.
00:12:26.420 - 00:13:22.818, Speaker A: And there's also a lot of other chains outside the customer ecosystem that don't have modules, and that can also be a problem as well. Verification cost is still very, very high, and we don't have relay incentivization, which means running a v layer could mean you're running at a loss. And most relayers right now are just validators that are running this so they can actually move tokens and other things around, but it's not incentivized to actually losing money in a lot of cases doing this. But what is very cool is we're building these public goods together that are expanding the abilities of IBC already. One of the things that one of the teams within customers are building, I think strainlove, Strangelove are building packet forward middleware. So this is essentially a module within IPC that will allow tokens to go back to the source chain we talked about with path unwinding and then to the destination chain at the end of it. So we are getting better ux, we are slowly getting there, but we are still sort of missing applications.
00:13:22.818 - 00:13:58.600, Speaker A: Building on top of this, I know catalyst and others are starting to do this to some extent, which should be quite cool. Now, we talked a bit about relayers before, and IVC relayers just passes the measures across, which is quite nice. We have different types of connections as well. So a connection is just one chain has connected to the other chain, and you have different channels that are serviced by different relayers, and within that you can have unordered and ordered channels. If it's unordered, you just check if a message has been achieved or not. So it's sort of first cut, first serve. If it's ordered, you get an order of what needs to be received by the IVC module.
00:13:58.600 - 00:15:17.982, Speaker A: So there are different sort of ways of MEV can come into this, especially if you go with an unaudited channel, you can get a fee market and you can sort of extract MEV as a relayer by requiring people to intent to pay. Tip more if you want to get included first in an IVC transaction. But what's very important with relay incentivization is the fact that we need sort of not just the first come and the fastest and the most sort of geographically centralized located relayer to do this, we need to sort of have maybe a subset of relayers for one specific channel and so on, because otherwise it will always revert to centralization, like a lot of stuff in crypto does quite often. So it kind of be the first, the fastest always wins. Now we come to the modeller idea, and what's happening a lot in celestial, with ethereum is this proliferation of roll ups and app chains and app specific roll ups that are happening, that are using oftentimes the same DA layer. But the big problem with using the same DA layer now and sort of splitting up the stack of what you're using is that we still need to verify everything that's usually just part of one chain. So we have execution, we have consensus, settlement goes into this a bit, and we have data availability.
00:15:17.982 - 00:16:04.312, Speaker A: So by splitting this up, we now require more like clients. So if I am an IBC enable chain, I'm monolithic, let's call this osmosis. I need to be running not just a light client on execution layer to verify execution has been done correctly, but I also need to run a light client on the DA layer to verify that data is actually available. So we get more light clients, more cost, highly likely, and you still need to verify that all this would be correct. So we can either run lite client, or we can also relay through a relayer, the validator signatures from celestia or Ethereum or Vale or Eigenda back to that specific chain that you're connected to. So we need three convictions. We need data availability, we need transaction ordering, and we need execution and settlement of those specific transactions.
00:16:04.312 - 00:17:11.724, Speaker A: So we get a separation of transport, verification, execution, which is quite important to keep in mind if you're trying to bridge between a roll up that's using a DA layer or similar. Now, one thing that we can do is that we can use relayers to sort of aggregate all of these proofs together of DA and execution. So what we can do is that we can post proofs of execution, and that's the correct fork trust rule, the correct chain onto the DA layer, use a relayer to sort of relay that back for pitching back to the specific rollers that are using that DA layer and even outside of that specific ecosystem. So lite clients are quite important, but we can also use lite client proofs to verify these things and aggregate them together and verify them together as well. There's a very cool implementation of this by sovereign labs, which I recommend checking out if you're interested in seeing how aggregated proofs of state can be used for bridging as well. Now one thing we can also do is use shared sequencing. So we talked about before, transaction ordering is important and we need proof of correct transaction ordering.
00:17:11.724 - 00:17:52.868, Speaker A: We need proof of execution and proof of DA. So one thing that we can do is use a shared sequencing layer that is building these mega blocks for specific roll ups. So we could have two rollups, rollup one and roll up two. And we can have a megabloc being built by a shared sequencer that's building and ordering for both of them. Now one thing that becomes a problem is that each of these two nodes still need to verify that the order roll up actually execute those specific conditional transactions. This can be done via like client. So again with IPC you want to like climb the other and the other one from the lite client, the other, but, and these collision transactions are basically just transactions that say okay, I want to go to this specific roll up.
00:17:52.868 - 00:18:34.650, Speaker A: We need to make sure that roll up has executed the exact same transactions. So it's primarily sort of limited to cross domain transactions and messaging as well. What does become hard and slightly more expensive is that both of the roll ups need to execute this exact same transactions with adds to execution costs. And it also makes ethnomicity become a problem because we still need to validate that the transaction was actually executed. Either we are still knowledge proof or via a like client as well. And this depends a lot on block times within this specific two rollups as well. But it's a very cool way to sort of get a very easy interval ability without needing to running too many like clients.
00:18:34.650 - 00:19:13.628, Speaker A: Now what's very important and what's sort of the end goal of this talk is that I want to make sure that people understand that standardization is super important. So we have a lot of developers in the IBC ecosystem that are building applications for various different chains, all wanting to use IBC specifications. And what they have right now is that they need to write logics for various different messaging and transport layers. So they need one for IBC, one for XLR, one for Hiberlin, and one for wormhole, as Larry puts here. And this is what we want. We want standardization, because IBC is no longer just limited to the customers ecosystem. It's going beyond this.
00:19:13.628 - 00:20:07.690, Speaker A: And when we do this, we need to make sure we have standardization, because standardization drives common innovation and oftentimes the group will often outweigh what the few can do, and it also just increases interoperability and composability between chains that we are using. And this sort of also plays into aggregation theories that we want a singular UX with a singular interoperability standard. That means we can bridge and use all different chains within the same UX within specific applications. And IBC sort of enables this. And I think it's very important to make sure that people understand that having enshrined interoperability intuitive protocol is super important because it enables native interoperability without trusting smart contracts and without trusting the trusted third parties. You prefer to just want to trust the two chains that are actually involved in a cross chain transaction. And that was it for me.
00:20:07.690 - 00:20:11.310, Speaker A: Thank you. You can find me at zero x ratingcoffee on Twitter if you want to.
