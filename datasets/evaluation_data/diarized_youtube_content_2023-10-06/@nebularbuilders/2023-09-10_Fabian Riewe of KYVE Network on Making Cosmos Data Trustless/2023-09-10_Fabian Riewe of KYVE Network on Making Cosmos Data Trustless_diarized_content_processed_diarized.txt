00:00:10.400 - 00:01:05.216, Speaker A: Well, all right, everyone, welcome to the quick and easy talk today about making Cosmos data Trust list. I'm Fabian, I'm the founder of Kyiv Network. It's going to be quite easy, a little bit, talking about today's data roadblocks, little introduction about what we do at Kyiv, and then certain ways of how you can access the data that we are storing with kive and all of this stuff around it. So what is the current problem in the blockchain data space? First of all, actually, node syncing and accessing all the big amounts of data is just tedious and time consuming. So if you want to sync a full node on Ethan statistics, weeks or even longer, right. Archival nodes are just not incentivized, except you're running them as an RPC service. You might want to create a service out of this.
00:01:05.216 - 00:01:53.232, Speaker A: You have a high risk of running into invalid and out of date data. Actually, there are some networks out there where it's just even impossible to even recreate the full archive because they changed the technology in between or the hardware that was used, since you would need to find a specific hardware at a specific point in time just to sync that certain timeframe and just getting more and more tricky accessing that data. And then of course, also, if you actually read from a blockchain, you're not actually reading from the blockchain, right? You're reading from an RPC node that is connected to the blockchain. But actually they are kind of faulty. Sometimes. I'm not sure if the result you get is actually correct, you just trust it because, well, the writing part, that is easy, you kind of trust this. But then when you read, you're just like, hmm, is it correct? You might want to ping a second RPC service just to figure that out.
00:01:53.232 - 00:02:23.150, Speaker A: It's just like a big mess. And then of course, you have all the different data requirements and you have different use cases, something we see. We have dual analytics specializing in SQL data access only. You have a graph specializing in graphql data access only. But if you're a developer, you of course want to stay in the text that you're familiar with, or especially stay in the text deck that brings the most value. And maybe graphql isn't the solution you're looking for. Maybe looking for like a streaming optimized database or things like that.
00:02:23.150 - 00:03:08.132, Speaker A: So what is Kive and how does Kiv work? Kiv is a fully decentralized data lake. We are living on the cosmos SDK. We have quite a tech journey behind us. We started out being fully arv based, and we were storing the data on arwiv. Our smart contract logic for the token was sitting on arwyv. And then, well, we realized the arweeve contracts weren't as production ready back then as we needed them. So we transitioned over to solidity contracts, then realized that, oh, actually, when we need to share app space and things like that, it might actually get quite costive for us because we just can't compete with like a uniswap where when you make ten k profit on a trade, and then it's fine if you spend $100 in gas fee, that just doesn't work for like application specific, like, use case specific applications, so to say.
00:03:08.132 - 00:03:49.210, Speaker A: And then in, I think it was February last year, we then transition our logic off to be a Cosmos SDK appchain, and of course, still storing the data on permanent storage, like Arweave or filecoin, or just other kind of storage backends. We are very generalizable. And so basically, if you're a developer, you can choose, well, which data do you want to store, where do you want to store it, and kind of how do you want to interact with it? And everything we do happens on chain. So it's fully trustless and it's immutable. And of course, it's easy to access with kind of like certain data tools. We build around the data like we are building. And this will kind of like, yeah, be what I will be highlighting on today.
00:03:49.210 - 00:04:27.270, Speaker A: We have many active integrations right now. We are very blockchain space agnostic. Not only blockchain, but actually data agnostic. So we started out as a grant, actually, from the parity and Arweave team to archive the Polkadot data onto Arweave. And then we realized that, oh, actually there's kind of cool use case and something that is needed not only in the Polkadot ecosystem, but actually all around the web3 ecosystem. And so we rather quickly extend it into EVM compatibility and other chains. And, yeah, I mean, by now we support, I would say, basically almost every big l one out there.
00:04:27.270 - 00:05:02.356, Speaker A: And also some, yeah, not web3 data source, but actually web two data sources, like weather data, IoT data. But I will kind of deep dive into these use cases a little bit later. So what are the kite data pools? The kite data pools are kind of the core of how we build up our decentralized data lake. They sit completely on chain and they only store the meta information. So what is the name of the data you're storing? What is the code that is executing which nodes are active in this. And basically they can be created by anyone in the governance. We are changing it right now.
00:05:02.356 - 00:06:00.988, Speaker A: So actually you can just create data pool without going to go through the governance, kind of like our end of the year goal. And so basically how do they work? So if I'm a developer and I want to store, let's say Polkadot, I would create, it's called a runtime where I define one function where I define how do I get my data. So for example, I request block number five, would be oh, I request a Polkadot RPC endpoint, get block number five, store it onto permanent storage, submit it on the pool and be like, hey everyone else, here's my uploaded data, please validate it. Then the other nodes, they go ahead, they download the data and they execute the validation function. And the validation function is completely generic and can spend from oh, I want to check if my data is an exact copy to oh, my data is almost equal for some, for pricing information. It's something where you might want to use this, like, oh, does the price from Coinbase is almost equal to the one from binance? And if yes, I'm going to vote valid or invalid on it. And then once all the nodes have submitted, they're voting on the data.
00:06:00.988 - 00:06:40.310, Speaker A: The blockchain or the pool is then computing if the uploaded piece of data is valid or invalid, and if it's invalid, we don't keep a track of it. And if it is valid, then we keep track of it. And then developers can query for that specific piece of data from our blockchain knowing it is 100% correct and has not been tampered with or modified with. Funding a data pool is the most important part and the core of Kaif's tokenomics. The way it works is these data pools, they require funding. And every time a node paid out a piece of data, it gets rewarded in Kaif tokens from this pool. Funding.
00:06:40.310 - 00:07:21.890, Speaker A: If a pool ever runs out of funding and just, and doesn't upload any new data. So if you build your application on top of kite, if you're kind of incentivized to keep the pool funded, so the stream of data you are working with keeps being stored in case a pool runs out and it pauses, it just pauses at the block height or whatever key you specify. So we'll never have any data gaps. Up to 50 projects at the same time can fund the pool. We're also kind of reworking the funding system right now, maybe also a little bit more about this later. And then who funds a pool? It's usually projects that are building on top of the data Kaif is producing. For example, if you're building your own indexer, you always want access to the latest Kaif data.
00:07:21.890 - 00:08:13.976, Speaker A: So you would be keeping the pool funded to make sure there's always new data is being stored and you can use it in your system. And then what are different ways how you can access the kite data? Well the most easiest one is to use our rest endpoint. So you can just query on chain using our like an LCD RPC client for kind of like all the bundles you have going on there. It's a very tricky way because our chain only stores the ide on the storage provided the data is stored. So let's say if you store your data on for example Arweeve, we just say oh, this ArWYF ID is valid and then you would kind of like have to interface it with yourself and download it and then you can insert it into your database. Data is also compressed so you can kind of like choose what kind of compression mechanism you want to use. And I think the storage provider is the most crucial part when you're building your integration.
00:08:13.976 - 00:08:59.030, Speaker A: Thinking about what are kind of my storage requirements, do I need 200 years permanent storage? Well then Arif might be a good idea. Do you only need like five year short term storage? Maybe Fhir, maybe ipfs might kind of like fit your use case. So we kind of build it in a way that developers have it very easy to interact with it. The kind of more robust way and easiest way to interact with KaiF data is to use our ELT pipeline. For everyone who is unfamiliar with ELT, it's a concept out of data science. It's while you first extract the data from the source, in this case it's Kaif, you load it into your chosen destination, it could be your snowflake cluster, it could be your postgres database. And then the client does the transformation on their side.
00:08:59.030 - 00:09:27.544, Speaker A: It's ELT versus ETL. Some other use cases you might prefer ETL over ELT, let's say. Well, you have specific customer data and you know exactly which format you want. On your end you can do the transformation on the fly. It's kind of like saving you a little bit of development work with blockchain data. We found actually that most of our clients want it as raw as possible because it's just such diverse, like let's say ethereum for example. You might find value in like well NFT data extraction, like which users have for which NFT.
00:09:27.544 - 00:09:54.368, Speaker A: Some others might be interested in DeFi, some others might be interested in whatever other application is going on. So kind of working with the raw data and then you can decide what you want to do on your own is the approach we chose there. And so our data pipeline, it's a no code solution. You can just spin it up, it's fully trustless, you can use it in your own system. You also have a hosted version. We're using a framework called Airbyte for this. Maybe some people of you have heard of it.
00:09:54.368 - 00:10:13.040, Speaker A: It's a more generalized ELT framework. They have a cloud version you can use. This Kiv is fully integrated, there don't need to spin up anything. If you decide you want to keep it fully trustless, you can just execute it on your own machine. It's just a docker container and it's very easy. Basically it's a nice interface. I hope if I have enough time at the end and give you guys a quick demo of what it looks like.
00:10:13.040 - 00:11:04.414, Speaker A: But basically you just select oh, I want to get the data from the Polkadot Kive pool. And here's my destination, which you can see airbright supports so many destinations, I think everyone really finds what fits the use case best. And just one click and automatically syncs all the raw data in it. You can define custom normalization if you say, oh, already, my transactions should already live in their own table. On SQL, for example, you can just import everything in and then just do your data analytics on your own, build your own queries around it. A couple of cool use cases spanning from like well the most basic data analysis to actually, for example, maintaining old queries that got removed that were interesting for old dashboards that got built. Osmosis for example, like a good, a good option there where they removed old state queries because they are too intensive to query them from the chain directly.
00:11:04.414 - 00:11:49.734, Speaker A: And so the team from imperator for example, is looking into a solution, how they can recreate that and maintain these old queries by then querying trusted off chain data from like whatever data framework they choose to deliver that. Another use case for Kaif that we as a team have built out is called ksync. What it does is it allows validators or node operators to sync their node without actually having to be connected to the p two p network. So there's a couple of advantages. The first thing is that actually it takes a lot of time for your node. We always call it like the old kids don't want to play with the new kids coming in, actually, it takes up to like 2025 minutes sometimes. Until your node is connected and needs to sync data, it can get kicked out.
00:11:49.734 - 00:12:23.720, Speaker A: So it's kind of like a faster way, using the kite data and just syncing the node directly. We do support dbsync, which allows you to sync your database from your node directly. Or we can mimic a p two p node. So it means that the nodes think it's actually connected to the blockchain network, but in reality it isn't, and it's just talking to a mocked endpoint. And getting the data actually from Kaif in the background has a couple of cool advantages. It eliminates the need for archival nodes because you can just sync to whatever point in time you want and you can just prune to whatever time you want. So let's say you only want to investigate a certain area of blocks.
00:12:23.720 - 00:12:47.372, Speaker A: You can just do it using this. It's very easy to install. It's actually just a wrapper around your daemon. It just executes automatically. It knows exactly which data storage to use, which provider to use. We also work on a state sync application right now. So on blocksync, you have to consume every block one by one, apply to get the state for every block height.
00:12:47.372 - 00:13:26.450, Speaker A: So it also takes some time. And then the cooler thing actually to store is to work with statesync, where you can store the state at every given block height. So it's much more easier to use because you just tap into, oh, I want to see what the blockchain looked like on, I know, 1 April 2022. So you download a block from that date and you just see exactly what it looked like. So it's much easier to do holistic analysis. Another nice little tool we have is called supervisor. A couple of feedback we got when people were using kite, especially on Mainnet, was that it requires you to have a note of a real node that's connected to the network running at the same time where you want your kite node.
00:13:26.450 - 00:13:56.228, Speaker A: So let's say on Mainnet we are running the Cosmos Hub archival right now. So you need to run a Cosmos hub node and the kite node. And of course, because they're talking to each other, the Cosmos hub node needs to sync and basically creates a full archive. And this is of course what we wanted to get rid of, that people need to run their archival nodes. So I created this script called supervisor. And what it does, it, it's communicating back and forth with your kite node. It's actually pruning the data that you are syncing only up to a certain height and only from a certain height.
00:13:56.228 - 00:14:35.286, Speaker A: So let's say when the Kaif pool right now is archiving the block height between 1002 thousand, supervisor makes sure that your node that you're talking to only has the data from let's say 800 to 2500. So you're not running kind of like into an unnecessary cost situation. We're actually running like a way more expensive node than you actually do. And actually in some cases it's the only way how you can even sync all the data because the pure archive would get so big. It just be like no economical sense to run it on your own. Another cool way on how we are sending data from a to b. It's our X Oracle module, it's in development right now.
00:14:35.286 - 00:15:34.110, Speaker A: We're using IBC there and then we can use other IBC bridges like Excela for example, to send the data across the whole blockchain ecosystem. It's already integrated with all IBC chains because we just utilizing the ICS 20 stack there. Yeah, you can use Excel and Squid, you just pay in any token if you want to execute the query. And it's just super handy if you have an Oracle use case for example on your chain and your cosmosm contract and you want to access data that has been stored there, you just basically would query it through our Oracle module and it would ensure it's fully trustless and stays kind of like in the fully on chain ecosystem. So to say, what does the next couple of months look like for Kaif? We launched our mainnet in March. Since then we launched one data pool, which is the cosmos hub data pool there. So we're almost done with Zinke and we have a full archive of that created.
00:15:34.110 - 00:16:09.230, Speaker A: This is the protocol launch token listing is coming up next, as well as the Oracle launch, which is the IBC transfer thing. And then basically what we really focus on for the rest of the year is just data pools. Data pools, data pools. We have very cool ecosystems in the pipeline spanning from all the cosmos platforms to of course the EVM chains again storing their data. And then something we are very, very excited about is we have our first external project building on top of kite. It's called Lockstore. And I don't know if you guys are familiar with like the elasticsearch stack.
00:16:09.230 - 00:16:41.966, Speaker A: They have also a thing, they're called lockstash, which basically you deploy to machines, the machines and all their locks in a decentralized way. And basically they've done it to then collect the data using Kaif and Arwyth. And then make it from there available for developers for the long term analysis of things. Because this is something you have to keep in mind. We're working with Kive because the data gets voted on and uploaded just a normal delay of around 30 seconds. So we get a lot of questions about real time usage. But honestly real time data validation is kind of tricky.
00:16:41.966 - 00:17:05.598, Speaker A: If you want to send the data around the whole world, you already have latency. You need notes to vote on it. That adds latency. So real time is almost impossible. And they have a cool system. They use the streamer network, it's a decentralized pub sub network. So they publish the data immediately and then everyone who subscribed to it just gets it into their database and then they're waiting for Kaif 30 to 45 seconds later to actually confirm the data is valid.
00:17:05.598 - 00:17:42.920, Speaker A: And then they override the data in the database that they didn't get from Kaif to ensure that they working in the fully trustless space, so to say. And yeah, I think it was a very, there you go. It was a very high level intensive overview about that. Happy to answer any questions. We have like five minutes left if there are any, if not. Thank you so much. I'm a bit curious about the choice of database providers.
00:17:42.920 - 00:18:08.378, Speaker A: Yeah, how is the choice being made? Is it up to the user? Yes, exactly. So basically you can decide on both ways. So first, the first decision you can do is when you store the data using Kaif on chain. So there of course we recommend like a decentralized storage provider. ArWYF is usually a good choice and things like that because you can just dump all the data on it. It's cheap and it's stored, but it's, let's say you want to store your data in s three. There's nothing wrong with this.
00:18:08.378 - 00:18:40.462, Speaker A: We have an easy storage interface. You just write in like oh, how you usually connect to s three for example, and then it's getting stored there. And then when you jump back to the elt pipe here. Was this what you were referring to on the ELT pipeline or was it the on chain data storage? No, it's the database provider. I mean, for example, in some cases you might want to restrict where the database is located. I see, I see. No, so this is something you can fully decide on your own.
00:18:40.462 - 00:19:15.442, Speaker A: So you would kind of like stay in whatever architecture you usually use, let's say using Amazon RDs like postgres for example, to store your data. You just set up your normal local database cluster how you always do. You just spin up the ELT pipeline, point it to your database endpoint, click run, and it just syncs it automatically. This is all handled by this framework we're using there. So you can choose whatever database you prefer and whatever you're willing to work with, so to say. Exactly. And it's the cool thing also because we're storing the data with arwave, it's free to access because AYF is free on the data access side of things.
00:19:15.442 - 00:19:44.756, Speaker A: So the moment it's stored, just click one button, it's getting sticking to your database and that's it. We also have a cool tech demo about this online. Also if you want to check out the docs, it's like a little section about like a postgres example and a snowflake example. So feel free to also play around with that. It should be quite an easy start. Exactly. Yes, when you said yeah.
00:19:44.756 - 00:20:29.080, Speaker A: So this is also a cool process. So basically we have 50 nodes active in one data pool, and one of the nodes is randomly selected to be the uploader. So we only upload the data once, and then once the uploader has uploaded the data on chain, the other nodes download it and then they just execute a validation function. So the validation function is just an interface and just needs to return either true false or abstain. And then you can write whatever logic you want in between to validate your data. So something we do for blockchain data, because that's quite easy to validate, we just say, well, does my local copy equal the copy that got uploaded? That is like a very easy validation check. But you can make it more and more complex by for example checking for ordering.
00:20:29.080 - 00:21:07.734, Speaker A: Or let's say if you work with pricing data, like oh, is the slippage not higher than x or y? And then the notes would vote on this. And actually it happens quite sometimes that the RPC endpoint, you connect the two goals offline, for example, and then the nodes just can always abstain. So if you're unsure, vote abstain, and if the majority votes abstain, the bundle just gets dropped. So just kind of like is stuck in an infinite loop until like a human steps in and fixes it. For example, in case the data is not deterministic, the validation function is broken or the endpoint is broken or things like that. Exactly. So how do you provide such functions in the transaction? You provide the function, no.
00:21:07.734 - 00:21:42.828, Speaker A: So basically we have our own JavaScript library and you just override the interface that we have there. You build it and then you just deploy it to the pool. So in the pool you say, it's a shame the Wi Fi here isn't working very well because I would love to show you a little demo in the app. But basically every pool has a fixed runtime, and then all the nodes execute this binary. So you can build a binary however you want. You can add as much customization to you. The people that are building their own stuff on top of Kaif, it's crazily customized because they're using stream and all the other things, but they just ship the binary and all the nodes executed and that's how it works.
00:21:42.828 - 00:22:35.232, Speaker A: Exactly. Curious, what do you feel are the most relevant competitors for Kai? That is a good question. I think for us, we are also still trying to figure that out a little bit in the beginning. A lot of questions like, oh, are you competitive the graph, are you competitive to Dune? But actually, I think we're sitting much deeper in the tech stack because actually the graph is a, I mean, they can be big user of kive, so can dune, right? Because they don't need to worry about building their own data pipeline infrastructure. They can just tap into some already existing data and just pull into their system and make it available immediately. I would say on that level, we don't have many competitors yet. Of course, you have the messaging protocols, they go into the direction, but they only work with deterministic data.
00:22:35.232 - 00:23:01.818, Speaker A: So here, it's kind of cool that you can choose it on your own where you want to store. High modular, I think it's the new cool word for that. But basically, I think even for us, we don't see many competitors yet in that space. I'm happy to do that, actually. That's a good idea. Let me see. Perfect.
00:23:01.818 - 00:23:53.010, Speaker A: If I quit this, what kind of application do you think would be the biggest users? Yeah, I think it would start out definitely with, I mean, well, you guys as validators, right? Because you just are so much into all the node systems work with this, you know, all the pain we go through every day, right? Like how, how fragile some of the networks actually are and things like this. And actually just being able to just sync a note without having to worry about it is pretty powerful. And then this is something also kind of very interesting for us. It's like, well, Kaif grows with every little piece of data we add over the next couple of years. So this is like the most interesting challenge compared to other protocols or products it just launched and usually have access to a big ecosystem. It actually takes some time. And also, I mean, the older the data, the more valuable it is because the harder it is to get.
00:23:53.010 - 00:24:18.680, Speaker A: Like Solana, for example. Solana block zero. I don't know if it even exists on some archival nodes. It definitely exists in the bigquery instance, but I don't know if it even exists on some nodes. That's kind of, like, interesting to see. And, yeah, as you can see, I mean, some other blockchains, honestly, they're not old, maybe like two or three years, and it's already impossible to, like, get their early data and create an archival and things like that. So it's getting, yeah, very, very interesting time for us to look into that.
00:24:18.680 - 00:24:40.950, Speaker A: Let me connect to hotspot and see if I can make it happen. Let's see. Yeah, it looks. I'm actually. Nope. Oh, with me. Nice.
00:24:40.950 - 00:25:38.690, Speaker A: No, it doesn't look like it's connecting. Let's check with the hotspot. Is it open on my screen? Perfect. No. Where is this side? No, my hotspot doesn't want it. Hmm. Let's try the Wi Fi one more time.
00:25:38.690 - 00:26:11.960, Speaker A: Well, I'm sorry, everyone looks like the Wi Fi and the hotspots not. Oh, no, it's. It's not successful in here. But, yeah, feel free to check out Kaif network. I'm happy to stay around, answer any questions once I get a big exclamation mark on the Wi Fi once it's working. And, yeah, happy to answer any questions if they are. And thank you so much for listening in today.
