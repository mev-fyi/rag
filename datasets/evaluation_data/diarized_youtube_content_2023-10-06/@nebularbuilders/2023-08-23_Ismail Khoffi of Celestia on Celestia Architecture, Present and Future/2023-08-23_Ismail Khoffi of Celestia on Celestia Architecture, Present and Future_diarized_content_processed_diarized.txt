00:00:09.080 - 00:00:36.100, Speaker A: Hey, everyone, I'm Isme Coffee, CTO and co founder. Louder. Okay, I got a shout. Okay, again. Hey, I'm Isme Coffee, co founder and CTO of Celestia. Today I'm going to talk about what Celestia is and currently what we're going to launch with and how it might look into in the future. So, like, I give a few future directions.
00:00:36.100 - 00:01:14.100, Speaker A: It's not a full fledged roadmap, but it is like a few ideas what we could work on moving forward. So let's dive into the first part. Let me stand here, I guess let's dive into the first part. So for those who are not familiar with Silesia, I give, like, a very quick overview what it is and how it works and how we implemented it. So from a very high level perspective, Silesia is the first modular blockchain network. And the core idea is to decouple consensus from execution. What does this mean? Right.
00:01:14.100 - 00:02:05.000, Speaker A: Like, every chain today operates the following way. Like, that's also true for all cosmos chains. Roughly, if you, if you produce blocks, your clients validate the following. Right? Like, you validate if the header has consensus, and you also validate the transactions, like all of the transactions. So it's also true for all cosmos chains in the sense that, like, even if you, like, launch, like a cosmosm zone, you just have more transactions to validate. So this is what we call a monolithic blockchain, because it does everything right, like it does all the tasks a chain does in celestia, which we call a modular blockchain. It operates slightly differently in the sense that headers are still validated.
00:02:05.000 - 00:02:46.078, Speaker A: You still validate if a header has consensus. But instead of validating all the transactions, you just validate if the transactions were actually published to the network and are available so that you have a chance to recompute those. But the celestial base layer doesn't do this. That's the main difference. I'm standing in front of my slides, so that's the main difference here. Right? What does it mean for the transactions to be available? It just means that they've been published. And because it doesn't validate, if each transaction is valid from like a state machine perspective, these transactions can be arbitrary blobs of other applications.
00:02:46.078 - 00:03:51.864, Speaker A: Right. You might wonder, if the transactions do not get executed and validated, where does the execution actually happen? Usually, like in a cosmos zone, all the transactions get validated on the same chain. And if it's like a general purpose execution environment, you have like a cosmosm zone or EVmos EVM zone. Then the execution happens still on the same chain in like NvMdez. In the celestia modular blockchain model, the execution happens usually in so called roll ups or sometimes called client side execution. So while Celestia only guarantees that the messages of the application have been published and ordered and there's consensus on the order, the execution is done on a different chain in a so called roll up that just posts the data for ordering and consensus to celestia. So the execution happens there and the like.
00:03:51.864 - 00:04:47.996, Speaker A: The state validity is guaranteed either through fraud state fraud proofs or so called validity proofs. So like a validity proof could be like a ZK proof, right? These are like the two ways, the two main ways how execution works in Celestia. These are the design principles that we designed Celestia with. I gonna very quickly summarize those. So the block validity is done via like, it's only checked if the data is available, right? That I already mentioned that two other maintain goals were that applications only need to care about their data, right? Like they only need to care about the data. They need to execute in their roll up. They do not need to download any other data from any other application.
00:04:47.996 - 00:05:41.256, Speaker A: This is different from like how Ethereum, for instance, currently works. In Ethereum, you download all the transaction data of all solidity smart contracts deployed on Ethereum. Additionally, one design goal was that you're able to prove to an application that you downloaded all the transactions, nothing is missing from your application, and also that you only need to care to execute. I mean, you only download those that you care about. You only download those transactions, the data that you care about. You also only have to execute the transactions of your application and nothing else. Again, that's very different from like for instance, Ethereum's world computer model where all the, where all the state transitions are executed.
00:05:41.256 - 00:06:15.122, Speaker A: It's also different from a COSM wasm zone where all the cosmwasm smart contracts are executed. So how did we achieve this? There's like mainly three ingredients we used to achieve this. First of all, we eraser code the block data. Erasure coding is a technique that most of you are familiar with. When you scan a QR code, there's erasure coding implied. So like if something is missing in the QR code, you still get to the website. If the CD is scratched, you can still listen to the music.
00:06:15.122 - 00:07:13.440, Speaker A: That's erasure coding happening right there. The other way is like how we, like the other ingredient is how we commit to the data. And we commit to the data with a so called namespace mercury, which is very similar to like how tendermint currently commits to the data. I'll explain that later. But like the third ingredient is data availability sampling, that basically, like all these three things, basically make a cosmos, vanilla Cosmos chain to Celestia. So let's look a little bit into the erasure coding, not too much into detail, but so how it roughly works is you take the original block data, right, like all the transactions in the block, and you arrange them in a square, chunking the data into like equally sized chunks. And then you extend the data using that erasure coding three times, actually.
00:07:13.440 - 00:08:22.420, Speaker A: And what you merkelize is not just the original data, like in ten m usually, but what you merkelize is like the whole extended data, the whole square, and you do it once with every row and with every column. So we had to swap out the data route in the tendermint header. So let's briefly look into how we commit to that data. Right, like I said, the rows and columns are mercury using a namespace mercury tree. So the chunks are also prefixed with a namespace, and the namespaced mercury takes the data sorted by namespace. And it also, I mean, as you can see here, it also basically separates each, like the data per namespace, right? So if you want to prove, for instance, the completeness of the data, you can download a namespace and you can prove to the left or to the right that there's other namespaces involved. So you have proven completeness, assuming that the data was sorted, which is also guaranteed via consensus.
00:08:22.420 - 00:09:12.078, Speaker A: So, speaking about data availability, sampling, the third ingredient, that's the main, what's the goal of Celestia is to guarantee data availability. And this is done by this erasure coding that I mentioned here. You can see a little bit in more detail the chunks. So how this works in practice is via the erasure coding. You only need to download a small portion of the block and have very high probabilistic guarantee that all the block data was actually available. So it's very hard for block producers to hide any data. So there's some nuance here.
00:09:12.078 - 00:10:22.510, Speaker A: But like roughly, if you download like only less than 1% of the data, you're almost 100% guaranteed that like all the block was published to the network. That's what we call data availability sampling, because lite clients, or nodes in celestia, sample randomly the block data to assure that it was being published. So from a very high level architecture perspective, we took tendermint, we took a Cosmos SDK app we built a state machine using that. You could imagine Celestia being a very simple proof of stake chain, the simplest you can imagine. We even removed some of the base app modules. We simplified a few things, but we employed mostly the changes that I mentioned, which is validators commit to the erasure coded block and merkelize it using the namespace Mercury tree. So that happens in Celestia app, which is like a vanilla proof of stake app where the changes were employed and the data availability sampling is done on a separate peer to peer network using Libb two p and bitswap, mainly.
00:10:22.510 - 00:11:16.010, Speaker A: So that's completely written from scratch. That part is very familiar to every like or like, very similar to every cosmos chain, just much simpler. And that part is completely new and never has been done before. So we added one transaction type to like base app, essentially, which is pay for blobs. You can imagine you submit a transaction which is an arbitrary blob, and you pay per byte for that. And then Celestia goes, does the erasure coding and guarantees that this has been published and everyone can verify themselves with a very small client that this is actually the case by employing that sampling that I mentioned before. Okay, so through the more interesting part, to those who are already familiar with celestia, some future directions and open problems.
00:11:16.010 - 00:13:03.630, Speaker A: As I said, this is not a full fledged roadmap, but a few directions that might be prioritized moving forward. So first of all, how is it guaranteed that the erasure coding and democratization is done correctly? As you remember, nodes only download the data that they care about. How do you guarantee that the erasure coding has been done correctly? So what we currently do, like what we're going to launch with, is the lite clients can rely on something that is called bad encoding fraud proof. Everyone who downloads a whole row or column can generate these bad encoding fraud proofs by looking at the rows and columns, matching those to the, to the, to the mercallized, like to the mercury tree of that row or column, and then send around the proof that they do not actually match to what, like consensus agreed on, or that the data actually, yeah, that the data is actually different, or like there's no integrity in the block. This is important because rollups, the only thing we guarantee, as I said, is that rollups can post the data and they can download their data to recompute their state. Right? And this bad encoding for proof guarantees that by ensuring that not, like the validator set didn't just commit to garbage or something, the bad encoding for have the downside that lite clients need to wait for them, right? For the block to be final. One way to get rid of that is using so called KZG commitments.
00:13:03.630 - 00:14:13.940, Speaker A: That's for instance what polygonal veil is doing and ethereum was also eyeballing with. And I think we can actually do better than that. Instead. Now we continue as before essentially, but we compute like a zero knowledge proof that the erasure, coding and the construction of the block and the commitment has been done correctly. That has like many benefits because the computation before is still very fast and easy and you can send around a small proof immediately after the prover is done. You can send around a small proof and you have immediate finality and do not need to wait for a bad encoding fraud proof. Another thing that we will most likely need to prioritize or the community will ask for, I'm sure, is having a two way bridge directly from unto Celestia to get the base layer tokens in and out to roll ups and back.
00:14:13.940 - 00:15:37.700, Speaker A: What is relevant is then with that, if you had that bridge, you could deploy a roll up and use the base layer token for gas consumption, right? You don't need your own token if you don't want to, and you have to use the base layer token to pay for the blobs anyways. So it just makes sense. It will feel more natural if there was a two way bridge to get the tokens in and out. The problem with that is that we want to keep the base layer as minimalistic as possible, right? Like I mentioned, I applications just post their data. There's no execution environment and there's no general purpose execution environment such that you could host a bridge there for all the different kind of execution environments that deploy their applications on top of celestia, and that could be very different. So for each of those, if you need a bridge, you would need to have an execution environment running on Celestia. The question is, can we do without that? And thanks to an idea that Mustafa came up with, and thanks to ZK snarks, it is actually possible, like the high level idea without, again, without going into detail, because any of this could be a 2030 minutes talk of their own.
00:15:37.700 - 00:16:33.300, Speaker A: The high level idea is if you look at how a ZK snark verifier operates, it's basically very similar to plain old signature cryptographic signature verification. So what if we took the public key of such a verifier and we used that as a specific address on Celestia, and then the user flow would roughly look as follows so to get TIA out, or like the base layer tokenization, call it here. If you want to get that out of Celestia, you send a deposit transaction to that public key of the Zksnark verifier. Let's call that verification key address. Once that's done, you credit the user with TIA on the roll up. That's very easy. No magic.
00:16:33.300 - 00:17:35.904, Speaker A: It's basically like an escrow. It's just that the public key that you send the tokens to are tied to a ZK verifier. So the way back is a little bit more nuanced here on the roll up, the user, very similar, sends a withdrawal transaction on here, gets verified on the state machine, the transaction gets confirmed, and additionally you generate a ZK proof that this transaction actually happened. Right. And then the magic is you use that ZK proof as like a kind of signature on Celestia. And then there is a, like, the verifier runs on Celestia for like this ZK proof, and then on Celestia, without having, like, a general purpose execution environment, you just have a verifier, right, like an opcode, you withdraw the TIA back to the user. There's a lot of nuance here.
00:17:35.904 - 00:18:14.630, Speaker A: There's a lot of design decisions to be made. That's the high level idea. It's very easy and very appealing. If you're into working on ZK staff, please reach out or participate on our research forum that I will bring up a little bit at the end of the slideshow. Another hot topic that we need to tackle is MeV. There's a lot of research going on. There's, in the modular paradigm, there are a few more nuances because you have these heterogeneous app layer on top.
00:18:14.630 - 00:18:57.656, Speaker A: So there's MEv in these roll ups. There's MEV in the base layer. And then how do you. Yeah, how do you reason about these state of the art research in the context of that? Skip is looking into that. And like other projects as well. I think that's a very, very interesting research field. If you're into MeV, I would recommend to look into the early celestial ecosystem and how to tackle MeV and how to improve censorship resistance, mainly because the roll ups inherit the censorship resistance of the base layer because.
00:18:57.656 - 00:20:07.380, Speaker A: Yeah. Okay, one more thing before I wrap it up. The vision that we have is we want 1gb blocks, 1 million roll ups, ideally, or more, and 1 billion light nodes, right? You need a lot of light nodes to securely scale the block size. And to achieve that, there's a lot of implementation that needs to happen, which is optimization work mainly on the peer to peer layer. So we currently Callum, my colleague, has an optimized mempool, which is called content rational mempool, but there's still to get to the goal of at least a gigabyte blocks. There's still a lot to be optimized in the mempool because the ten m in mempool is not meant for these block sizes. Similar on the celestial node side, there's a lot of work to be done to improve dosing on the peer to peer layer and on the storage layer for the 1 billion light nodes.
00:20:07.380 - 00:21:05.890, Speaker A: An interesting target is the browser. Obviously, because everyone runs a browser, people run wallets in their browser. People use infura and other centralized service providers to interact with chains, but what they actually could do is they could all run a light node that is data availability sampling, have much higher security guarantees, and basically contribute to the security of the network. So if you want to work in browser data availability sampling light nodes, please reach out. There's roughly two ideas how to do this. One is to have a rust client which compiles to wasm, and then you have, yeah, you deploy that in the browser. The other idea is to use the go code and have a WASM compile target go improved recently a lot on WASM support, so that's a very appealing approach here.
00:21:05.890 - 00:21:41.420, Speaker A: Okay, I'm going to skip this slide and go directly to the links and I move aside. If you want to participate on any of the ZK stuff, particularly or any research topic, I would recommend you to go to our research forum and post your ideas there. There's a QR code if you want to work on any of the implementation specific stuff. If you're into peer to peer systems, networking, storage databases, our GitHub repositories are the right address. Please reach out. Thank you very much.
