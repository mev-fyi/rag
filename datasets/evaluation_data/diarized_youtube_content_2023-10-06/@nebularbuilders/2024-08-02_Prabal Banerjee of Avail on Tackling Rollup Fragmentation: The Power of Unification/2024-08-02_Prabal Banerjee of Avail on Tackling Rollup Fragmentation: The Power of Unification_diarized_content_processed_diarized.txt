00:00:00.320 - 00:00:27.416, Speaker A: Thanks. Yeah, I think why we need mouthful titles is because the experience is handful when you want to talk among different angles. And one of the things that we are trying to do at avail is we are trying to unify the experience and the overall infrastructure that is at a, on the various roll ups.
00:00:27.568 - 00:00:28.808, Speaker B: And I would just like to, you.
00:00:28.824 - 00:00:36.940, Speaker A: Know, go through a little bit on how we do it, what are the essential ingredients of it and what is the end of.
00:00:38.880 - 00:01:02.382, Speaker B: The oval. We are building three things. We are building the Owel DA, which is the essential base layer. That's the data availability solution that we have been building for the last three years almost. Um, and it's going to the main net very soon. But apart from that we were thinking that yes, roll ups are proliferating the market. There will be hundreds of roll ups.
00:01:02.382 - 00:01:54.284, Speaker B: I think last, last talk also talked about all the different roll ups that are going to be in the market and how the access is completely broken and things like that. And hence we were thinking with, by solving the data everybody problem, which is what the roll ups are hungry for. What's next? The next thing is about the fragmentation that is happening across the l two s and that's why we have been building on avail Nexus. I will go into the Nexus bit throughout the talk, but I want to also touch upon the fusion. Now fusion is about adding crypto economic guarantees back to the base layer so that you can have a secure base layer on which the, all the infrastructures on top can interplay. Right. So why is avail da special? Right.
00:01:54.284 - 00:03:01.390, Speaker B: What we want to do is we want to have easy verifiability. One of the core reasons why hopefully we are all in blockchain space is that we want end user verifiability across all the applications that we are building in this space and how we are trying to enable it is through casey commitments and data everybody sampling. So what that means is every user would be able to verify within their client, within their wallets whether the data behind the roll up that they're interacting with is actually available or not. I think the scroll is a little bit broken so I would manually scroll this at awhile. Nexus we are now trying to do what we call as proof aggregation and sequence reduction. What does it mean? We want to have a single point of aggregation for proofs within all the roll ups. And with sequencer selection we want to provide a way for different roll up sequencers to come in and bid for them to be able to produce blocks.
00:03:01.390 - 00:04:21.170, Speaker B: The sequencer selection bit is what we are not focusing right now. We are more focusing on the proof aggregation. Now why this is important is only when you have a common DA layer you can verify execution on top and the interoperability constructs work. The moment you go across Da layers your verifiability and the security assumptions completely change based on the foreign chain that you're interacting with. Okay. And in avail fusion we were thinking of, you know, yes, we have the avail token which is there for staking, but can we have additive economic guarantees baked into the consensus of the chain? And that's why we are thinking of how we can include bitcoin, ethereum, Solana and plenty of roll up tokens which are trying to find their place in the market about utility, how we can add them to the base layer in order for them to contribute to the consensus of the chain which is securing them, right? So with avail nexus we are trying to do what we call as trust minimized roll up interoperability. And I think the trust minimization factor is important.
00:04:21.170 - 00:05:03.996, Speaker B: Why trust minimization is important is because think of the interaction that you do today in the typical web two, what you do is you go to a marketplace, you go to Amazon. The Amazon team doesn't exactly know which bank you're interacting with. So what happens is they use a payment aggregator in the middle. So at Amazon you click the checkout, it goes to someone like PayPal, stripe builders, whatever. At that point you give your payment information and depending on which kind of card are you talking about, it goes to someone like a visa or a Mastercard or an Amex. Even them, even then they do not process it directly. They actually send it to the bank.
00:05:03.996 - 00:05:43.090, Speaker B: The bank confirms it. That confirmation goes back to PayPal, PayPal confirms it back to Amazon and then it is done. Only then your payment is done. And we want to have the same spirit in terms of web3 applications because we think that this kind of an architecture is very elegant. This is a very much microservices based architecture where each of these components can scale onto demand. If there is a sale going on, if it's a Black Friday or whatever, Amazon can independently scale up. And if there are, I am a XYZ payment provider and I don't have enough users, I can still have low scale of my product.
00:05:43.090 - 00:06:26.768, Speaker B: And this is what is missing today. And this is why we are building just another l two. Like the previous person said, the other l two is not about solving a problem, it's about being sovereign in the sense of what kind of scalability you need, what kind of latency do you need? What kind of interoperability do you need? It's about that. And that is why we think of a world where applications are not on monolithic chains but are on roll ups. At the same time. We fundamentally agree that the experience is broken and that is why we are talking about it. And this is how a web3 cross chain bridging works.
00:06:26.768 - 00:06:51.424, Speaker B: On Ethereum, you have to lock your asset on, let's say a cosmos or any other l one. You have to mint asset based on the guarantee that this locking has been done. So on cosmos you verify that the locking is there. On the other hand, you burn. When you burn your assets, you show a proof of burn and then do an unlock. Of course, there are many other ways to do bridging. This is a typical lock and mint mechanism.
00:06:51.424 - 00:07:38.948, Speaker B: There are things that are out there which are very different. I was talking to, I was talking to some booth upstairs and they were talking about how they are doing all of this in a single click and so on and so forth. All of that is great, but think of the complexity that it takes to verify one chain on the other. For example, the fact that you have locked your tokens here. What is the guarantee of that? That you can provide here, the only guarantee, the best in class guarantee, which many fail to provide even today. The best in class guarantee that you can provide is light client verifiability, which means that you can verify that Ethereum consensus has agreed to the fact that this has been locked. To be very honest, you don't even do that.
00:07:38.948 - 00:08:16.228, Speaker B: Most bridges, even light client bridges rely on sync committee and not the overall consensus. But let's not go there. And this is how cross chain interrupt among various chains roughly works. You have multitude of these chains, they have their own users. And depending on from which chain to which chain you want to interact, there are plethora of ways. There are of course aggregators on top. People like socket and others are doing fantastic job of, you know, abstracting away all of this complexity.
00:08:16.228 - 00:08:53.552, Speaker B: But that doesn't mean that doesn't change the fundamental fact about how the interaction is happening behind. And as you can see, this is the real problem, right? This is the real problem. When I talk about another l two, is that people are seeing this kind of a mess and they don't want another vertex here. They don't want to have additional complexity just because I went there and figured that I want a new l two. That's the complexity that we are facing right now. And this is what we are trying to fundamentally change. What we are trying to do is we are trying to provide what we call as a veil nexus.
00:08:53.552 - 00:09:37.544, Speaker B: Let's talk about three, four different l, two s, let's say a zksync era, a polygon, ZKvM, a starkware, starknet and a plethora of other altos like scroll linea and others. They all want to talk to each other. What we want to do is we want to have nexus in the middle where you can have like verifiability of all of these chains within nexus. And this is only possible when you have the fundamentally talk about roll ups on top. Because if you think about it, then the roll up verifiability comes from two facts. The role of verifiability comes from the fact that you have verified execution and you have verified data availability. Any one of them missing is actually fundamentally invalid.
00:09:37.544 - 00:10:24.544, Speaker B: It is actually insecure if you do not verify the execution of the other chain. If stocknet doesn't verify the execution of Zksync era, it cannot migrate assets in a trust minimized manner. Similarly, if Zkevm doesn't know what state transition function is crawl doing, it cannot do this interaction. And that is why we are trying to provide nexus as a coordination layer where it acts as the middleware which connects all of these chains. And all of these are right client verifiable from within the nexus. I will talk a little bit about how that happens, but before I go there, this is the kind of interaction that we want. We want to have contingent transactions of one chain on the other.
00:10:24.544 - 00:11:20.912, Speaker B: Think of it like if you want to have an opensea NFT, what you can do is you can have a contingent transaction where your NFT is actually put on a hold state unless the payment is actually processed on polygon, for example. And this is very normal in the web two world, right? If you go and try to buy an aeroplane ticket, it's not that your seat is booked immediately without processing the payment, and it's also not that your seat is actually sold to someone else while you're processing your payment. It's put on a whole state is the same when you want to buy tickets on a stadium, a plane, booking, anything on a marketplace. That is the typical way of how marketplaces work. And this is exactly what we want to enable, but in a trust minimized manner. If there is no trust minimization here, then we are not improving the status quo. We are not here to build web two products with the same guarantees on a new wrapper.
00:11:20.912 - 00:12:15.110, Speaker B: We are here to build fundamentally better products which can give better security guarantees to the end user. Now comes the complicated part about how exactly, how exactly we want to do it is. We want to have a very thin Nexus runtime. What the Nexus runtime does is it checks for continuity among the various participating rollups. But typically how it is done is that each of these roll up proofs are actually taken and converted into an adapter. And why this is important is because if you think about the complexity of unificating all of these chains, there is fundamental complexity involved in being able to roll all this up into an aggregated proof. For example, zksync uses a modified planck, whereas polygon zk EVm uses f flunk scroll uses a rot 16.
00:12:15.110 - 00:13:01.360, Speaker B: Linea uses optimistic plus ZK proofs. And so it's a wild world out there, even if you talk about all being ZK proofable. So that's why we want to create this adapter, where the adapter takes heterogeneous proof systems together to create a homogeneous proof on top. And on this homogeneous proof is where you can do aggregation because homogeneous aggregations are much faster and they have coherent security guarantees on top. And then the nexus runtime is just a coordination engine which just checks that. For example, all these proofs are not only sound but also complete. And this is what the world we want to take a look at.
00:13:01.360 - 00:14:13.958, Speaker B: It's where conditional execution is empowered, right? How it is being done is that a user for the user, all of this will be abstracted, as many of you would certainly understand. The marketplace basically provides the NFT chain with some payment details and an ability to hold the NFT for some time. The time duration is specified and how you verify it is not by timeout, but by showing either inclusion proof or proof of non inclusion. And this is one of the biggest guarantees that you can provide in any other system is very hard to provide proofs of non inclusion. Non inclusion proofs are hard, are notoriously hard unless you build the data structures from the ground up in which you can control. And that's what we are trying to do on the Nexus middle, where we want to not only provide this guarantee to the NFT chain that the payment has been processed, but we also want to give an inclusion proof through the marketplace. And what that means is we can now verify both execution and data availability on the Nexus adapter.
00:14:13.958 - 00:14:53.424, Speaker B: And how is that ever possible? Is that from all of this zksync era Zkvm other l two s you take the proof, but this proof only does state validity, which means that the state updates were done correctly. That's the only thing that you proof. By these proofs, you take it to the adapter, you create an aggregate proof. This is great. And it can be verified by any lite client, which means that I, as a user, I can verify all these chains by verifying a single proof. And this single proof takes 60 milliseconds if it's a garage. 16 in typical hardware that's in your mobile phones, that is what you are enabling by providing an aggregate proof over all the execution.
00:14:53.424 - 00:15:48.320, Speaker B: But as I said, this is not enough. This is not enough because if I change the ordering of the transactions, your execution proofs do not hold true. For example, if I give you five numbers and you have to do an exponentiation for one over the other to the next, and if I change the ordering, then your actual outcome is going to change. And that is why it is important to have the data available for safety or liveness guarantees, depending on what kind of roll ups are you talking about, but also because your ordering guarantees should come from the DLA. And that's why for availability from day one we have been focusing on building a light client which provides ultimate DA guarantees using data availability sampling. You do not need to download the entire blocks, you can just sample parts of the block and be very sure that the entire data and the ordering is available. And this is together.
00:15:48.320 - 00:16:23.176, Speaker B: Both of this gives you the ability to verify in a light client. I think Vitalik has also been talking about like client verifiability of ethereum and such. And these are important because unless you have light client verifiability not only on the execution, but about the data availability, you do not get. Fundamentally, you do not get guarantees. You can have crypto economic guarantees, but cryptographic guarantees are not possible. And hence this is the world that we are trying to look at. We are trying to look at a world where there are n number of modular chains.
00:16:23.176 - 00:16:58.000, Speaker B: I don't care if you go from n to n plus one and build just another roll up that you want to do, but we want to provide a unified interface which is powered by light clients, which can improve not only the user experience, but also the developer experience on this modular world. And yeah, so I will kind of end by saying that we want to accelerate the unification of web3 by providing ways to interoperate in a trust minimized manner. Happy to answer questions and connect later if you want. Thank you.
00:17:09.780 - 00:17:22.800, Speaker C: I'm curious, how much data, what's sort of the throughput of, in terms of how much data could be put in there. And what is the latency of the confirmations?
00:17:23.620 - 00:18:13.338, Speaker B: So, right now, the data size is two mb per block, which is erasure coded to four. But this is a scalable layer, which means that we have tried up to 128 mb and beyond. But the point is that you just cannot provide a huge block with no demand. The economics doesn't work that way. For example, if you are a validator of our chain, you wouldn't want to provide the infrastructure to support 128 mb blocks or two GB blocks or whatever, unless there is a real demand, which is subsidizing your cost. And that's why we have been testing, you know, higher numbers. But right now, we have been stopping it to a two mb because we think that the market is still not ready for a two mb block.
00:18:13.338 - 00:18:48.996, Speaker B: And now, coming to the latency, the latency for our chain is we produce a block every 22nd, but our finality is every two blocks, under the assumption that there is no network partition. If the network partitions, then it will take some more time, unless the partition is resolved, and then you can finalize. And that's typical for every chain, right? So we have a hybrid ledger, which means that if there is delayed finality, which means that there is a problem with the validators not being able to reach consensus on the finality. But the ledger, like the chain, goes on because it has liveness for them.
00:18:49.188 - 00:18:57.150, Speaker C: So every 40 seconds, basically. And then two megabytes currently, roughly, yes.
00:18:57.310 - 00:19:26.770, Speaker B: And just to compare that with other chains, for example, on Ethereum, if you post 4844 blobs, it takes you roughly 15 minutes to get finality. On Celestia, if you post data blobs, then, although you reach instant finality on the block because they use a tendermint chain, but because there is a fraud proof, it can take from minutes to days, depending on whom. In Celestia, you ask.
00:19:32.550 - 00:19:39.930, Speaker C: One more question. Do you see the primary sort of users of this being roll ups?
00:19:40.390 - 00:20:17.610, Speaker B: Yeah, absolutely. The users of us are roll ups. Even today we have been talking to tens of rollups. All the roll up stacks have running integrations with us. So you can, today on the East Global hackathon, you can go and try out our arbitram, orbit, Opstack, Polygon, CDK, Zksync, hyperchain, starkware, madara, any kind of integration you can run. You can even use someone, something like a stacker, something like a sovereign SDK, something like rollkit to power your own roll up using avail.
00:20:20.790 - 00:20:44.836, Speaker C: Okay, two more questions. So, first of all. Well, maybe, but so for two megabytes every 20 seconds doesn't sound very much like how much data would roll up. So transaction volume would roll ups be able to sort of handle to like fit into that.
00:20:44.988 - 00:21:34.198, Speaker B: So very good question, like just to give comparisons, because that's the most that we can do is that give comparison with the status quo? Ethereum blob has a capacity of 370 kb in average and it can go up to a 720 or something kb at the maximum using AIP 1559. But on an average it has for every 12 seconds it has a 300 kb and there is no congestion. Even with arbitrum, op, stack, linea, Tyco, starkware, all of them using blobs, there is no congestion today. And that's why it's dirt cheap to post blob transactions on Ethereum. And that is the lack of the demand that we are talking about. So two mb per 22nd is low. I agree, but the demand is also low.
00:21:34.198 - 00:21:46.150, Speaker B: And that's why I said that when the demand hits, we have a scalable architecture. But unless the demand is there, there is no point in tuning up and creating empty blocks.
00:21:47.370 - 00:21:53.070, Speaker C: Do you see other applications besides rollups that would need to use da?
00:21:53.370 - 00:22:25.760, Speaker B: Absolutely, like any kind of verifiable computation that you want to do. And rollups are just a construct. Roll up is a construct because it's a construct which takes execution off chain and puts assertions on chain. And that can be done for many, many different types of application. It doesn't have to be branded as a roll up. Even within rollups, there are different types of roll ups, and there's definitions of what you want to call a roll up and such. But any kind of verifiable computation off chain verifiable computation needs data availability.
00:22:27.100 - 00:22:28.490, Speaker C: Cool, thank you.
00:22:33.070 - 00:22:50.142, Speaker D: Hi, in terms of roll ups, posting data to avail, I was just curious to know if you guys were prioritizing or if you prioritize any particular roll up framework, or is there something that you in particular are excited about like opstack or arbitrum nitro, something like that?
00:22:50.286 - 00:23:23.908, Speaker B: I mean, all of them are great. I have to say that to be credibly neutral, I cannot stand on stage and prioritize one. But at the same time, I think each one of them have a different focus and that is great for us. I will give you some examples on orbit. From orbit we have stylus, and stylus gives you the ability to provide wasm executions, right? Which is awesome. Think of the smart contracts that you can build and the rust code that you can run and them being talking to each other on the other side. In the op stack, we have seen the gas getting tuned up.
00:23:23.908 - 00:23:53.050, Speaker B: We have seen huge amounts of transaction getting hit on the base, on base. And it is being sustaining. And they are trying to push the limits on giga gas per second that you can process and such. So their optimizations helps us in that way. On the other hand, Zksync is trying to reduce the prover cost. It is trying to show that on a laptop you can provide a proof which is very different than a starkware madara. A madara is trying to push for sovereign chains.
00:23:53.050 - 00:24:17.276, Speaker B: They are not settled roll ups, they are sovereign roll ups. And hence their optimizations are looking very, very different. They are not talking about the smart contract in which you can settle. They are talking about end user verifiable proofs which may be settled on ethereum and also on bitcoin, for example. They have been talking about that. So all these stacks are prioritizing different parts and we are happy to collaborate with each one of them. And then there are other stacks.
00:24:17.276 - 00:24:38.080, Speaker B: For example, a stacker is providing micro roll up, which you can customize your own execution and state and such. In sovereign SDK you are providing extremely good guarantees on a base roll up. Guarantees which are never seen before in any other stack. There is a tyco, there is a scroll. I mean, each one of them are different.
00:24:38.790 - 00:24:49.530, Speaker D: And just to follow up, while you guys go on Mainnet, are there already any particular users that you can already talk about or like somebody, I don't know if that's a bit.
00:24:50.150 - 00:24:55.210, Speaker B: I mean, this is about predicting future, which I would like.
00:24:56.550 - 00:25:01.798, Speaker D: Do you guys already have certain users in the pipeline for Mainnet? Is there somebody that you can already talk about?
00:25:01.854 - 00:25:53.036, Speaker B: Yes. So there's a post, I would love to refer you to the post where we have talked about all our partners and also their stages of development. One of the problems that we see is I can only talk about, I can tell you like, yes, we have 40 partnerships, and then you will say like, okay, but how many of them are going to use on day one of Mainnet? And that is a tricky question to ask, because every day the stack is changing. None of these stacks, none of these stacks of are ready for mainstream on an alternate DA. The interfaces we are collectively building with arbitrum, with op, with Polygon, with Zksync, we are collectively building the interfaces. But even with that, the stable source code is not there. There are forks that we have to maintain and such, which means that whenever you are talking about Mainnet.
00:25:53.036 - 00:26:11.570, Speaker B: We have to be extremely, extremely careful about the security. The audits have to be complete, they have to be stable. They have to be tested on testnet. Some testnets are going on. I mean for example on OP testnet we are hosting for the EiD global hackathon for us to be stress test whether that is complete or not.
00:26:12.150 - 00:26:12.830, Speaker D: Cool. Thank you.
