00:00:04.720 - 00:00:13.662, Speaker A: All right, here we are with Ben Jones, co founder and director of the Optimism Foundation. Ben, it's so great to have you here on the defiant podcast. Welcome.
00:00:13.846 - 00:00:15.770, Speaker B: Thanks for having me. Absolute pleasure.
00:00:16.190 - 00:00:16.766, Speaker C: Awesome.
00:00:16.838 - 00:00:47.684, Speaker A: So obviously, optimism is the second largest ethereum layer, two by total, by locked with now around 7.6 billion of TVL. Obviously, that's central to Ethereum scaling. So very excited to talk about all the things coming up and being worked on in the optimism ecosystem. So why don't we just start with the very basics, if you can just explain optimism in simple terms.
00:00:47.812 - 00:01:50.894, Speaker B: Yeah, absolutely. So fundamentally, what do we have with Ethereum itself? Ethereum is a new computing substrate for a new kind of application on the Internet. And so obviously, we're all familiar with the story of smart contracts and decentralization and proof of work, proof of stake, so on and so forth. But fundamentally, it's the case to date that such systems have been fundamentally limited in terms of their throughput, how many transactions in a single second or in a single block or over the course of a day can be put through the system. And that is fundamentally limited when you have a single chain design, effectively. So the fundamental goal of optimism is to bring forth what we call the super chain, which is a governance system for having a large collection of chains that all share in security from Ethereum and unblock the requirement that we have one single kind of pipe that is a fixed number of transactions per second, and expand that so that we can have many, many, many pipes and unlock the next wave of applications, users, cost reductions, so on and so forth.
00:01:51.062 - 00:02:05.518, Speaker A: Awesome. And the goal of having these multiple chains that are based on Ethereum security is what, like, what's the end goal of this?
00:02:05.614 - 00:03:04.000, Speaker B: Yeah, so the end goal is to take that substrate that I described that we talk about this thing that is very powerful tool for defi, financial freedom, nfts, games, so many different things, and to take that into a context where it can serve a far greater spectrum of use cases that we see on the Internet. So today, a fundamental limitation of having one kind of pipe with transactions going through it at a fixed maximum amount means that the things that we can do are fundamentally restricted. And it also means that we have what's called congestion pricing. So what that means is that the more that ethereum is getting used, the more kind of demand for gas there is, because there's only a fixed size sort of supply. What happens is that the price goes up. And so in practice, the way that we see the scaling problem manifests today is in high gas prices and what we need to do is figure out a way to get around that. And fundamentally what that boils down to is parallelization.
00:03:04.000 - 00:03:32.254, Speaker B: It's taking everything from occurring on one pipe and giving everything that's going on a different pipe to be able to run on simultaneously. So, yeah, that's fundamentally the reason for having multiple chains. It's this notion of parallelism, this notion of we can't have just one pipe support everything. We need to have different pipes supporting different things. And the way that the problem sort of manifests is in gas fees. So you want to lower gas fees, you've got to increase the pipe size or the number of pipes. You can only do so much on the pipe size.
00:03:32.254 - 00:03:34.750, Speaker B: So at some point, you got to go to multiple pipes. Awesome.
00:03:34.790 - 00:03:46.450, Speaker A: And then optimism then, is this network of many different chains that are then basing their security on Ethereum.
00:03:46.830 - 00:04:34.904, Speaker B: Exactly. So the question becomes, how do you accomplish having many of these pipes, many of these chains, and the philosophy, and in particular, how do you do that in a way that doesn't sacrifice on the security that we arrived at in the first place? The naive solution to create two chains is to create what we might call another layer, one chain. Just take Ethereum and copy and paste it. The problem that you run into is that then you have to divide the security between those two chains. So Ethereum is secured by a network of validators. These are these folks who have staked some ETH and are participating in the rules of Ethereum to progress the blockchain and the economic, what we call the economic security. The security of the system and how we kind of measure how much it would cost to attack the system is based on how much value is locked securing the network.
00:04:34.904 - 00:05:12.504, Speaker B: And so you have a problem in the naive solution where if you split that into two chains, you've now divided the security of each of them in halves. Layer two works a little bit differently. Layer two basically introduces a notion of, you might call it a dependency or an extension to the layer one, as opposed to a copy and paste of the layer one. And basically by building it in such a way that everything is fundamentally rooted in layer one. And the l two s evolve underneath. You allow them to inherit the security from the layer one. So instead of splitting the security into two chains, you have inherited the security of one chain into another chain.
00:05:12.504 - 00:05:30.360, Speaker B: And that's kind of the fundamental property of scaling. Yeah, and then, like you said, the super chain is the substrate that optimism, as a network, as a platform, is building, which is a collection of these chains that all share a common language and that all share a common stack and that grow the pie together.
00:05:31.060 - 00:05:31.668, Speaker A: Awesome.
00:05:31.764 - 00:05:50.252, Speaker C: Before we get into the specifics of this solution and how it differs from other solutions, I also want to get to know more about you, like what's your background, what got you into crypto and what led you to start? Optimism.
00:05:50.436 - 00:06:33.490, Speaker B: Yeah, I started off as an undergraduate graduate, as an undergraduate in math and physics. And basically what I realized was that academia was an extremely slow path to take. And I saw a bunch of people that were kind of in a similar boat to me, but doing very cool startup stuff. And that kind of got me excited about that. Many moons ago, I had used bitcoin to play poker online, and so I kind of had known about this thing. It existed. But when I saw the Ethereum whitepaper, that was when it really clicked for me, and I realized that this was a beautiful intersection of something that was extremely exciting, clearly going to have a huge impact right at the cusp and moving very, very, very fast.
00:06:33.490 - 00:07:16.410, Speaker B: And so I jumped on it. What quickly became especially exciting was this scaling problem, which at the time, because, you know, as I said earlier, the way that the scaling problem manifests at first is with congestion and prices going up. At that time, the reality was just that the congestion problem was not as large. We knew because we believed in Ethereum that it was going to be an issue, but in practice, the fees were very low. I mean, it was sense to send a transaction. And so anyway, basically I found myself in the circles of folks at the Ethereum foundation that were very, very passionate about this problem. I found myself getting along well with them, being able to usefully contribute to the problem space.
00:07:16.410 - 00:08:15.180, Speaker B: And, yeah, so that was the first thing, was the Ethereum foundation. Eventually we kind of focused in on this problem statement of something that we called plasma, which is maybe not been in the spotlight for many years now, although it is making a return. And so we started off as a nonprofit organization called Plasma Group, and our goal was basically release research and implementation code for this kind of like tree roll up scaling solution called plasma and just put it out as a public good. So we were a nonprofit, we got some donation funding to stay alive, and we started putting that out. What we realized over the course of that was actually that the thing that we were experiencing was actually a more generalized version of this problem. What I mean by that is that we knew that the scaling solutions were extremely, extremely important. And it was clear that if Ethereum was going to succeed, that was going to happen.
00:08:15.180 - 00:08:53.322, Speaker B: At the same time, most of the funding was going to the application layer. So there was huge amounts of money at that time going into projects being built on top, Ethereum. And it was kind of a moment to say, hmm, all of the funding is going towards applications being built on top. For those applications to be successful, they need this public good, they need this architecture for scaling, and they simply, that is simply not getting the attention it desires. So over time, what we realized was that actually the scaling story was never going to be done. There was always going to be further improvements and extensions and what would become the roll up, and then plasma will return and so on and so forth. And this will be a decades long process.
00:08:53.322 - 00:09:23.276, Speaker B: So the other thing that we realized is that we need to fund the public goods to continuously improve the open source software that makes Ethereum tick. So that was kind of our story. We started off as a nonprofit. We realized that it was very hard to raise money. A lot of the money was being directed to other parts of the ecosystem. And we said, we need to solve this thing. There's an opportunity here for a scaling solution to come in and have a source of revenue and redistribute that revenue to the public goods, the creators of the open source software that built it.
00:09:23.276 - 00:09:26.520, Speaker B: So that was kind of our evolution of where I came from.
00:09:27.020 - 00:09:30.120, Speaker C: Awesome. And so in this evolution from.
00:09:32.460 - 00:10:00.868, Speaker A: Nonprofit, open source public good to optimism, which I understand has a foundation, but also a company that raised funds, that has a token. What was the transition like, and how was optimism able to achieve this more sustainable model?
00:10:01.004 - 00:11:08.388, Speaker B: It's a very good question. I think, in general, a pattern that we see across the crypto ecosystem is very, very challenging answers of some parts of open source applying and some parts not, and some parts of web two traditional pre crypto business models applying and some other aspects of them not, it's very intuitive to say the simple answer, which is a very web two minded answer, which is that if there's valuable code to be written, it should not be open source. That is an example of something that is fundamentally changed when you enter the web3, reshape. So I think ultimately, what it boils down to is having an understanding that the most successful methods in a web3 context borrow some things from web two, but in others, they are just fundamentally changed by the nature of what we are building. So I think what it boiled down to was not being, obviously, we preserved our values. We said, it's very important to us that if we go and raise money, all of this code be MIT licensed. Anybody can take this code.
00:11:08.388 - 00:12:04.726, Speaker B: Do anything that they want with it, with no restrictions, it must remain a public good. And ultimately the real story becomes helping people understand that that is not a values feel good decision alone. It is equally and more importantly, a self interested sustainability decision. Because what we see in the web3 industry is that no one person that owns some code base caused Ethereum to become what it was. What caused Ethereum to become what it is and have such massive success was a community of thousands and thousands of contributors across the globe. And so ultimately, you know, the naive web two answer says Ethereum shouldn't open source its software because that is giving up a moat, that's giving away value in this technology. Instead, what we see is that that is completely counteracted, flipped on its head by everybody being able to contribute to that open source project.
00:12:04.726 - 00:12:32.720, Speaker B: And it just makes the software so much better that it outpaces any concern that you might have in sort of a web two value capture silo setting. So that's what I would say it was about. It wasn't necessarily about convincing VC's that we're some feel good members of society, and it's very important to values to be open source. Obviously, that's true, and that's why crypto resonates with me personally. But what's really exciting is that doing the open source thing is the competitive advantage.
00:12:33.380 - 00:12:35.240, Speaker C: That's so interesting.
00:12:35.620 - 00:13:02.990, Speaker A: And then, so you raised VC funding, over $100 million in total. And then separately, there's also the op token that was airdropped. So how does the token fit into optimism as a network, an ecosystem, and also as your business model?
00:13:03.610 - 00:13:38.160, Speaker B: Yeah, great question. So the OB token is one governance. It's a governance token, but it's actually one component of a broader system, which we call the optimism collective. And the optimism collective is the governance system for the super chain. And it's split into two houses, two governing bodies, if you will, what we call the citizens house and what we call the token house. The token house, of course, being the op token that you referred to. So the basic design principle behind building a two house system is to balance exactly the things that I was just describing.
00:13:38.160 - 00:14:23.654, Speaker B: It's actually interesting. I talk about the differences between web two and web3 business models. But what's interesting is if you actually go and look back at the history of web two, even before, really, like at the start, there was a very similar vibe to crypto of this new ungovernable global cyberspace. And like, you know, you can look at, there's a really, a really well written declaration of independence of cyberspace that was written. That's like a really, really cool read. And when you go and reflect back on it 20 years later, what you see is that the economics of that arose from the system prevented many of those things from being true. And that was a byproduct of the incentive mechanisms that were in place with the Internet.
00:14:23.654 - 00:15:45.366, Speaker B: So because things like the cloud and siloed databases were a, easy to implement on the architecture side, and b, inferred these kind of web two motes that I was alluding to earlier, what we saw was an extremely decentralized substrate because the core protocols of TCP IP is always analogized to web3 because it's very similarly generic, decentralized. It's a communication protocol. But what we saw in practice was that the incentives on top caused consolidation and basically caused that independence of cyberspace to in reality be the Fang controlled cyberspace between Facebook, Apple, Google, etcetera. So anyway, I'm sorry, that's a bit of a tangent, but to go back to what do we need to do in web3? What we need to do in web3 is achieve a governance system that prevents those same methods of capture from being repeated. We don't want to repeat that story. And so we have a two house system of what we call citizens, which is a one person, one vote system, and token holders, which is a one token, one vote system. And together they're meant to have checks and balances on each other to balance these two tensions, to balance the plutocratic incentives that, again, a profitable network will be the one that succeeds in the marketplace, but two, to give the individuals a vote in the system so that the system doesn't fall capture to the powers that be.
00:15:45.366 - 00:15:57.958, Speaker B: And consolidation doesn't occur around a few major centralized players, where we end up with the same thing that we have in the Internet, super decentralized, backend, extremely consolidated, centralized, like front end.
00:15:58.134 - 00:16:06.796, Speaker C: This is such an interesting model. I wonder, how can you achieve the one person, one vote house?
00:16:06.868 - 00:16:25.660, Speaker A: Because, like, the one token one vote is what everyone is using, but I think the one person, one vote is the harder thing to pull off. So how can you actually match that, you know, the identity with the vote?
00:16:25.780 - 00:17:10.946, Speaker B: Yeah, I mean, the place that I would start is that optimism governance is also a very iterative place. So we absolutely knew when we launched our governance system two or three years ago, what we put out initially would not be the end state of affairs and we would not have answers to all the problems. So optimism governance progresses in seasons, and each season we make various improvements and changes to the governance. So the answers aren't all here yet. And you're right that this is one of the big key points. Quite frankly, I don't think just for optimism, but for humanity, as things like AI comes online and starts being able to sound exactly like any human and, you know, speak at their intelligence level. But it is an open question as far as how it works today.
00:17:10.946 - 00:18:08.000, Speaker B: Basically, you can almost think of citizenships as like a referral system where basically there is a set of citizens and they, and kind of each season, each round, each citizen can extend a citizenship to one more individual. So this is, this is often called like a web of trust model, is the traditional kind of like academic name for this, and it currently operates with a very simple web of trust. Over time, I think we're going to see lots of other identity infrastructure come online that's going to be useful. In fact, in this upcoming season of governance, we're starting to play around with using Farcaster, which is a decentralized social media application, as some more of the basically data inputs to the identity layer. And I think we'll continue to build more and more of those inputs as crypto continues to grow and reach more folks. But it is not a fully answered question, and we are here to iterate and succeed. So check jobs, optimism IO, or just come hit up the community and join the discussion.
00:18:08.420 - 00:18:11.080, Speaker C: Very cool. Okay, so that's optimism.
00:18:12.020 - 00:18:18.360, Speaker A: How the op token works as a governance token, does it have any other utility in the network?
00:18:19.140 - 00:19:21.290, Speaker B: I mean, I think that the other utilities and the details of what that looks like. So in particular, kind of to allude to the, I think a question that you're implying is what is the relationship between the token house and the citizens house, and what actions are taken by both parties? And you say checks and balances, what does that mean in practice? So one thing that happens, for example, is that the token house initiates protocol upgrades. The citizens house has a veto over protocol upgrades that the token house passes. So this is an example of a check and balance. So, yeah, without getting into all the specifics of the checks and balances, the main way that you want to focus things is you want the token house to be responsible for the kind of like self interested financial, plutocratic decisions. So one example of this is protocol improvements. It is self interested to improve the protocol, increases throughput, performance, et cetera, because that's going to grow business and grow activity across the super chain.
00:19:21.290 - 00:19:38.070, Speaker B: Another example is grants giving. So there's obviously an ecosystem of applications and chains within the super chain. And the token house has what's called the Governance fund, which is basically a combination of grants programs for different purposes.
00:19:38.610 - 00:20:11.172, Speaker C: Ckverify is revolutionizing how proofs are verified, making web3 networks faster and more efficient. At its core is a modular system designed to decouple proof verification from execution and settlement layers, offloading the heavy lifting of proof verification to a separate network. CK verify cuts down the associated cost by over 90%, offering huge savings and a more robust network performance for web3 networks. And it is live on Testnet. Visit their website today to learn more.
00:20:11.356 - 00:20:31.040, Speaker A: There's two main variations of roll ups. You know, L2, ethereum scaling solutions, optimistic roll ups, and ck or zero knowledge proof based rollups would be great to get. Just like basic difference between the two solutions.
00:20:31.940 - 00:21:28.792, Speaker B: Absolutely. And I think there's a bit of nuance here, and I'll try to capture it in simple terms. Fundamentally, I talked about this notion of inheriting security from the layer one, and the idea that we want to make many of these L2 pipes that borrow from the security of a single ethereum, as opposed to splitting the security of ethereum between the multiple pipes. So that's the fundamental goal. In general, the philosophy behind this approach, which we would call L2 scaling, like, put simply, you don't go to court to cash a check, you go to court if the check bounces. And this is the fundamental principle of scaling. It's basically, how can we make these L2 chains embedded in the layer one such that the layer one is used more efficiently? That is, instead of going to court, going to layer one every time a check needs to be cached, only go there.
00:21:28.792 - 00:22:10.978, Speaker B: When a check bounce was fundamentally the way that we accomplish this was what we would call a roll up. And I refer to this neither with reference to Zk nor to optimistic. And currently the focus of what is in production for the collective is optimistic rollups. But I will note that what we are building is intentionally agnostic to ZK versus optimistic or other schemes. But interestingly, what I just described about using the layer one more efficiently has nothing to do with either of those. All that we do to accomplish this is we say, hmm, we have this Ethereum pipe, this one single pipe. Let's look at how this pipe is being used.
00:22:10.978 - 00:23:04.512, Speaker B: It's actually being used for two different things. One of the things it's being used for is publication of data. So one of the things that happens is just there's these blocks on Ethereum, and every time a new block appears, you have very high confidence that anybody else in the world, through this peer to peer network, can go ahead and download that block. The other thing that Ethereum does is execution. So in addition to taking those blocks of transactions and like making them available globally, we also take those transactions and we execute actions based on that data. So is it your defi interactions, swapping, minting an NFT, so on and so forth, sending an asset, what a roll up does, it says, ah. If you actually look at the breakdown of how much time, how much of this pipe is taken up by the data versus by the execution, it is predominantly the execution.
00:23:04.512 - 00:24:13.450, Speaker B: The time that your computer spends downloading a single Ethereum block is not nearly as significant as the time that it takes to process all of the ledger updates that correspond to the actual execution of those transactions. So a roll up actually does nothing more than say, what if we roll up those transactions? What if we do nothing other than publish the data? What that means is that now you have the option to download that data and do the execution, like do perform the ledger updates that you were expecting. But it's not a fundamental requirement. And so this is actually the fundamental concept of a roll up in general, and how you inherit the security properties, because all of the data publishing occurs through exactly the same method as it does on layer one, Ethereum. So it has all the security. Anybody in the world can download the blocks for the roll up chain in the same way that they can for Ethereum itself. So that's fundamentally the way that we get this performance improvement, where by using the pipe exclusively for data, we can fit a lot more data into the pipe, and then we can do execution across multiple chains subsequently.
00:24:13.450 - 00:24:52.974, Speaker B: So actually, when you get, the question becomes, is such a thing going to be useful? And the answer is kind of. So what I described on its own is you would just call a roll up chain, and it has no notion of ck or optimistic. Sometimes it's referred to as a sovereign roll up. The property of this chain is that layer one does not know what the state of that chain is. We told layer one explicitly, don't execute these transactions, save your energy for the data. And so that got us our efficiency gain. But what we lost is layer one, knowing what the state of those chains are.
00:24:52.974 - 00:25:51.246, Speaker B: So you can download the transactions, do the extra computation yourself, and know what the result is. But this poses a problem, because one of the massive values of Ethereum is composability, right? In a defi context, it's your money Legos, the ability for a uniswap and a Dex router and some custom pool that somebody has made. And the hook system, all of these puzzle pieces go together seamlessly. The problem becomes, if layer one doesn't know what the state of those chains are, you lose that composability. And so the second part of the question beyond how do we get more of these pipes to appear, is how do we get these pipes to communicate information to each other and be able to do this composability thing that is arguably the biggest unlock that Ethereum brought to the table. And so the answer is you need some way to tell the layer one chain so that the layer one chain can have confidence. And you need to make that happen in a different way than the layer one just does.
00:25:51.246 - 00:26:29.740, Speaker B: All of the ledger updates the execution itself, or you would have defeated the whole purpose of what you were trying to build. So that's where you get into what you call a proof system. The idea is instead of Ethereum executing everything, we can simply make a proof to Ethereum that those things have occurred and through some mechanism that proof will be relied on. That's where you get into ZK versus optimistic. And again, all of the rest of it is completely generic. So the op stack does this both for ZK proven systems and optimist proven systems exactly the same, because it's just a data publishing multiple pipe piece of code. But there's two major ways to do the proofs in practice.
00:26:29.740 - 00:27:32.494, Speaker B: The first is called optimistic, which really leads into you don't go to court to cash a check, you go to court if the check balances. So the way that that works is basically anybody in the world can say, hey, ethereum, what I believe the current state of the chain to be is x. And you build a protocol for anybody else to come along and say, ah, I disagree, and I can prove you wrong. And basically through some clever game theory, you can make it be the case that a single honest person that knows the state of the chain can actually make money by proving everybody else around, because you, because everyone who makes a claim that could be proven wrong, posts of money. And so you get this system where the prefill resolves, so long as there is one person watching the chain and watching if somebody lied about the state of the chain and making a little bit of money if they so wish. So that's really, that was where we really lean into the court analogy of don't go to court to cash a check, go to court. If the check bounces, the check bouncing is like somebody coming along and saying, I think the state is x.
00:27:32.494 - 00:28:04.976, Speaker B: And that being. Wow. The other proof system is a ZK proof. A ZK proof basically uses very advanced cryptography to compress the compression into some, the execution into some smaller sort of proof. So it's kind of like a signature on ski steroids. There's insane, extremely complicated, extremely cutting edge cryptography mathematics that goes into that. And it basically says, I came up with this crazy number just in the same way that only I could have signed a transaction with my private key.
00:28:04.976 - 00:28:34.284, Speaker B: When I give you a signature, this is like a signature, but it proves all of the ledger updates, not just that. I indeed wanted to make this transaction. So, yeah, to summarize, hopefully this wasn't too in the weeds. To summarize, using a chain more efficiently is what a roll up is, and the opstack will serve that in a generic way. There's an additional step of proving that makes these pipes more useful. And ZK and optimistic are both valid approaches. We're going to support them both.
00:28:34.284 - 00:28:36.480, Speaker B: And they both have awesome properties.
00:28:37.540 - 00:28:57.930, Speaker C: I love that explanation. Not really. I've been covering obviously this space for a long time and heard different variations of it. But yeah, yours was very simplified and I love the analogy. So, yeah, appreciate it.
00:28:58.010 - 00:29:01.666, Speaker B: Well, likewise, I've got a lot of practice answering that question, for what it's worth.
00:29:01.738 - 00:29:06.226, Speaker C: Yeah, no, I can tell. But it's, you know, it's been worth it.
00:29:06.338 - 00:29:59.870, Speaker A: So. So, okay, so now there seems to be growing consensus, maybe that of these two ways of proving execution, that the CK method will be in the long run the preferred way of scaling Ethereum. Vitalik said something along those lines recently, and yeah, there's been a, you know, other people saying something along those lines as well. So wanted to get your thoughts. Do you think that everything will, or like all roll ups will converge to become Ck? Or is there space for both solutions?
00:30:00.570 - 00:30:52.078, Speaker B: Yeah, so like I said before, definitely we are prepared for both worlds. We are prepared for the world that that comes true, and we're prepared for the world that it doesn't. And in fact, fundamentally, a well designed system should be prepared for that, in that the mental model for there being a difference between a chain and a proof of the chain is actually just very generically useful for creating good, reliable modular code that other people can use and turn into new pipes. With that being said, fundamentally, I think that this conversation boils down to allowing folks to make trade offs on a trade off spectrum. If you look at the ZK use case, it has some extremely compelling advantages. One of the strongest ones is because this optimistic system requires this back and forth. This.
00:30:52.078 - 00:31:25.896, Speaker B: I claim that this data is x. I'm going to prove you wrong. I'm going to challenge that. You're going to prove me wrong again, because it has that there is a period of time that you have to give for people to prove the others wrong. And so this introduces some latency, so it is faster to produce a ZK proof, which says, this is the one thing that is correct. And I have this garbled moon math signature thing that proves it beyond a reasonable doubt immediately. In the optimistic case, what you want to do is wait for some period of time and say, hmm, okay, well, it's been a week.
00:31:25.896 - 00:31:56.946, Speaker B: If this was wrong, anybody in the world could have made some money by proving it wrong, but they didn't. Therefore, it must be correct. But inherent in that approach, you're spending a bit of time waiting. So ZK is really nice from that perspective and what we might call a proof latency perspective. At the same time, there are trade offs on cost. So today it is quite expensive to generate these proofs, because basically the way that the math works out is to make it less expensive for you to do the computation. I make it more expensive for myself.
00:31:56.946 - 00:32:23.610, Speaker B: So, producing this extra signature math special number incurs a lot of overhead in terms of like, the computational power, the amount of math required to perform that proof. So, yeah, I can't say that I have a perfectly formed stance on what the end state of affairs is going to be. In general, we need to be prepared for all futures, and most generally, we need to be prepared for people to be able to make trade offs. And that's, I think, what it all boils down to.
00:32:23.950 - 00:32:51.074, Speaker A: Yeah, that makes sense. I had the understanding that there's also some difference between what can be built on optimistic roll ups versus CK roll ups. I don't know. At some point, I think optimistic roll ups allowed for were more EVM compatible, and in CKS warrant. I don't know if that's still the case.
00:32:51.202 - 00:33:39.328, Speaker B: Yeah, I mean, on the compatibility piece, I'm actually very pleasantly surprised by how quickly ZK is catching up. It was actually the case that ZK made more sacrifices. Basically, certain things in the VM were really, really expensive to do that moon math, generate that proof, ZK proof around. And so by removing those and kind of breaking the compatibility a little bit, you could get cost savings that made it feasible. So it's the case today that, like, what we might call EVM equivalents is most achieved by optimistic systems because they have a little bit more flexibility because of the cost. Fundamentally, it is the case that different trade offs that people make will have different applications that are possible to be built on it. I think in the case of just looking at that on a single chain basis, that's not really true.
00:33:39.328 - 00:34:31.276, Speaker B: I think we will see the same functionality and capabilities. Like I said, ZK is rapidly catching up with regards to compatibility. So in terms of being able to take a smart contract and deploy it and have that exhibit some behavior and build some application, I think between any of these solutions, you're going to have the same set of capabilities of you can write any smart contract to do anything, and that's great. Where you get into differences comes when you want to do things across multiple chains. So we talked about having these different pipes that are different chains. And obviously you might have an application that is so successful that even though it has its own pipe, it fills up that pipe just like it would have filled up a stereo itself were it on there. And so the way you get around that is you have multiple pipes.
00:34:31.276 - 00:35:28.202, Speaker B: But it might be the case that to have multiple pipes that an application is running on, where an application is running on all of those pipes, it may be the case that between those pipes you need to send communications. And so that is an example of part of the new paradigm of multi chain super chain development that is the real paradigm shift from developing just for Ethereum and developing for a multi chain context. And things like latency there do make a difference. If you have an application that's running on multiple pipes and you can only have those communicate to each other after seven days, you may not be able to build the application you're trying to build because seven days later the information isn't valid. Or it's the oracle, price is different, what have you. You can imagine a dozen reasons. So it's not one size fits all, but in general, the place where people will start having to build applications differently is when they have applications across multiple chains that communicate to each other.
00:35:28.306 - 00:35:35.402, Speaker C: And in that case, then the CK model makes a lot more sense because there's no latency between the chains.
00:35:35.586 - 00:35:51.630, Speaker B: Specs. Optimism IO interop. You'd be surprised by what you can do, even an optimistic system. So it's not the end of the road as far as what can be done. But you're right intuitively that there are certain bottlenecks that ZK will help us push beyond, and we're super excited for that.
00:35:52.500 - 00:35:52.860, Speaker C: Great.
00:35:52.900 - 00:36:26.650, Speaker A: And then, okay, so you said that you are prepared for whatever future, whether it's optimistic or CK based. But right now, optimism is an optimistic roll up layer, two chain, and then this super chain, multi chain feature. Where are you towards that development? Like optimism is still a single chain, right? Or are you already like, do you already have multiple chains?
00:36:27.110 - 00:37:01.480, Speaker B: We already have many chains, which I'm super excited about. And in fact, we did a rebrand about a year ago. So the chain that was formerly known as optimism is called op mainet, and we did that explicitly to clarify that optimism is the group of these chains, the super chain, as opposed to any one of them. So I mean, I'm super excited about all the success that we're seeing across chains other than op mainnet base. Obviously an incredible success case. Part of the super chain Zora, incredible NFT activity, part of the super chain mode, Lyra, ancient eight. The list goes on and on.
00:37:01.480 - 00:37:26.234, Speaker B: And there's even really exciting stuff on the core development side. Redstone just launched, which is the first non roll up op stack chain. It's actually a plasma. It's the return of plasma. It allows you to use much cheaper, cheaper data availability to drive prices of transactions even lower. So we're multi chain today, baby. There's a long road ahead, no question, in terms of improving things across developer experience, interoperability, wallet management, RPC.
00:37:26.234 - 00:37:30.590, Speaker B: So not saying the work is done, but we are officially in multi chain mode.
00:37:31.170 - 00:37:59.006, Speaker C: Nice. I just made the connection that, yeah, like Baze and Sora are their own chains and they're part of the optimism ecosystem. So yeah, that's the super chain. I thought it was somehow something else or different, but okay, now I made that connection. Okay, cool. And interesting that these multiple chains don't necessarily have to be the same type of chain.
00:37:59.038 - 00:38:05.286, Speaker A: So there's optimistic roll up base chains. But also, you said a plasma chain.
00:38:05.318 - 00:38:09.768, Speaker C: I had no idea that was part of the plan.
00:38:09.824 - 00:38:47.562, Speaker B: Yeah, it's really interesting. This goes back again to the same thing I said earlier about ZK versus optimistic about enabling trade offs. It's always going to be the case that the different types of chains may serve different use cases. If you look at something like plasma, shout out to the lattice team. They just launched. Go play a bunch of incredible on chain games there. The property of a plasma is basically to say, instead of publishing all of the data of the chain to a pipe, we actually just publish a hash of the data to the pipe.
00:38:47.562 - 00:39:52.880, Speaker B: And that's like an even more compressed form. It's an even more efficient usage of the chain. The tradeoff is that just from that hash, you lose the property that a rollup has that by being broadcast to the Ethereum layer one network, everybody can download the blocks anywhere in the world via the peer to peer layer. Instead, they download just this hash, and they say, hmm, well, I know there's only one piece of data that could fulfill this hash, and they tried to go find it from somewhere else. And that may not be the perfect use case for all applications, because it does make a sort of trade off on the security of the chain. But what it does do is it unlocks incredibly, incredibly inexpensive transactions, even beyond the sort of like, matter of cents, like we're talking subsent transactions, like a penny for a full block full of a ton of transactions. And so, in general, the op stack, which is the sort of, like, code base that powers these chains, powers the super chain, is built to be modular and allow people to tweak these knobs and flip the switches and make the trade offs so they can sit on the right spectrum between cost and security.
00:39:53.620 - 00:40:13.388, Speaker C: Super interesting. And then all these chains, which I'm seeing the main use case for having them is each Dapp will have its own specific chain. How do they all communicate and connect? Are they all connected by some sort.
00:40:13.404 - 00:40:15.372, Speaker A: Of bridge or how does that work?
00:40:15.556 - 00:40:51.308, Speaker B: Yeah, absolutely. This is the topic of interoperability. We kind of touched on this a little bit before we talked about certain properties that ZK proofs have that are valuable. Again, I actually alluded to the fact that we can do a lot more than is the case today, even in the optimistic context. And for the protocol nerds out there, specs optimism IO make a pull, request, help, contribute. Fundamentally, there are two things that we need to do in order to get these chains communicating. The first thing that we need to do is we need to standardize.
00:40:51.308 - 00:41:45.668, Speaker B: So, you know, what's the goal? To state it a little more explicitly, we want to be able to take an asset on chain a, say some heath on chain a, and move it to chain b in as frictionless and quick a way as possible. The first actual unlock, which is kind of, like, counterintuitively simple, is making sure that chain a and chain b have the same security. This is important because you don't want to send ETH to another chain. Think all you were doing was transferring it between one wallet to another, or, like, keeping it on your own wallet, but moving it and suddenly have that ETH be lost because there was a different security model on chain b. So actually, the first, most basic step on the interoperability journey is to homogenize the security. Is to equalize the security, everything should be extremely, extremely secure. And that's kind of a prerequisite to be able to use these chains without kind of knowledge.
00:41:45.668 - 00:42:40.720, Speaker B: And I could say that a little more explicitly. I guess the goal of the super chain is to have a bunch of these chains, but that's true only in the same way that the goal that it's true of the backend, it's the requirement for the architecture, it's not the goal of what you see. So the goal should not be that there's a bunch of chains and the users continue to go to some bridge website to send fems between them and consider the slippage and the security model. And rich hacks are an absolutely massive problem for our industry and literal billions of dollars have been lost to them. So yeah, I mean to say more explicitly, our goal is to make it so that you're not interacting with any one chain, you're just interacting with an application. And just like you don't know what server is, what the ip address of the server, when you go to some website, you just go to google.com and it works.
00:42:40.720 - 00:43:34.710, Speaker B: This is a shame. Same should be the super chain. You just go to an application and it is happening on some chain, but you shouldn't even know about that, you should just be interacting with the application. So the first requirement is to kind of like make the security uniform across all of those websites because it would be scary if you went to one website and it was super secure and then suddenly you went to another website and you lost ne because that website got hacked or something. So the first thing to do is sort of homogenize all these pipes and keep them all secure. Then the next thing to do is to build elegant developer tooling and APIs on top that allow for faster messages to be sent between those chains which enable a more frictionless experience and basically rehaul the wallet process from the bottom up so that your wallet isn't tied to a particular chain and everything kind of just works. So yeah, happy to dive into more detail, but that's kind of the basics.
00:43:35.130 - 00:43:56.858, Speaker D: Protocol 20 introduces Sorbonne, which is stellar's cutting edge smart contracts platform. This creates new surface area for innovation and provides new opportunities for developers to build protocols and products that create access to everyday financial services. So it means a gradual increase in transaction capacity and a chance to fine tune applications. And it's the most transformative upgrade to the stellar network to date.
00:43:56.994 - 00:43:57.870, Speaker B: Woo hoo.
00:43:58.370 - 00:44:13.110, Speaker C: For sure, that would be the ideal way to use not just Ethereum but any blockchain just like going to a single, you know, to whatever dapp you want to use and not carrying what's under it.
00:44:13.410 - 00:44:24.440, Speaker A: So right now, where are we in this, you know, in this goal? Like, how far are we, like, what still needs to be built?
00:44:25.060 - 00:45:23.990, Speaker B: We're in the specs phase of the most fundamental improvements that we know can be made so effectively. Huge shout out to the Op labs team for driving a really collaborative community effort, for specifying exactly how this maps out of and how this ability maps out for different kinds of bridges, for example, ZK versus optimistic, and how to define a universal API that can work and kind of like apply to all of those systems. So today we're in a very nascent state. What's awesome is that bridges do exist. And although bridge hack risk is an extremely serious one, there are sort of application layer bridges that let you move quickly between these environments. So if you go to, like, Superbridge, I think if you go to Superbridge app, you can, for example, get a front end to bridge between all of the different chains in the super chain. So there are things like that.
00:45:23.990 - 00:45:56.300, Speaker B: But fundamentally, we have just scratched the surface of the deep protocol level improvements that we're going to be able to make. And I'm super excited. We work all in the open. MIT license. Go check out the specs for more detail if you're interested. But fundamentally, what we really need to do is we need to improve our definition of a chain, of a L2 chain, to be able to acknowledge more than one other chain. So when these systems were built, the op stack was built originally to serve op mainnet.
00:45:56.300 - 00:46:46.176, Speaker B: It wasn't at the point where the bases and zors of the world could take our code base and run their own chain. We had our hands full just running the one chain ourselves as the folks that created it. And basically what that means is that, and this doesn't just apply, I think this applies to the entire ecosystem, is that most people built chain software to be a single L2 pipe connected to a single layer one pipe. And what we need to do is update our mental models and update the code correspondingly so that the chains are aware of these other pipes that are sitting alongside them. So that's the kind of core thing that you need to do. You need to make it so that one chain can be aware not just of the layer one, but of other L2 chains as well. And this gets into things like you might hear a layer three being thrown around, because that's a L2 chain that's aware on a L2 chain.
00:46:46.176 - 00:47:00.514, Speaker B: So you might call it a layer three, but yeah, that's kind of a hint at the flavor of the work that's been specced. You take the notion of a L2 chain only knowing about layer one, and instead you make it know about multiple chains, including other L2s.
00:47:00.682 - 00:47:01.346, Speaker C: Super interesting.
00:47:01.378 - 00:47:18.402, Speaker A: And then you talked about overhauling the wallet experience. And with base being one of the chains in, like, the optimism super chain, are you working with them closely to do that?
00:47:18.586 - 00:47:53.520, Speaker B: It's going to be a community effort. I think some incredible, incredible work has been done by the Coinbase team on what's called the Coinbase smart wallet, which is an incredibly smooth extension that already makes a lot of improvements over the status quo. So huge shout out to base. Like, lots of chains in the super chains in the super chain are supported there. It's an amazing experience. In general, this story will evolve over years, I believe, and we'll slowly chip away at the problem and then we'll make big step improvements, and then it will all be just magic seamless work. So it's a community effort.
00:47:53.520 - 00:48:20.324, Speaker B: Base is doing some incredible stuff. The category that this falls under is now being called, like, chain. Abstraction is the fancy term for, like, how we talk about multiple chains. And I think it's going to be a community effort. There's a real value in having standards that lots of people can use, not just one entity, and folks are collaborating on it, and we're going to have a marketplace of ideas, and the best ones will become adopted and help us all out.
00:48:20.492 - 00:48:21.476, Speaker A: Yeah, makes sense.
00:48:21.588 - 00:49:00.294, Speaker C: Can't wait for that future of just like, seamless blockchain interactions to be here. Before we run out of time, I really want to talk about retroactive public goods funding, or retro. What is it now called? Retrofunding. Just retrofunding. That's been a really interesting effort by optimism. Can you explain what this program is and also how much money you guys have given away? It's in the billions, so, yeah, we'd love to just touch on that.
00:49:00.422 - 00:49:45.710, Speaker B: Yeah, I absolutely can touch on retrofunding. So we talked earlier about the optimism collective being a two house system, and you asked some questions about what does the token house do? And I said it initiates upgrades. It also does grant allocation. Now, the question might be, what does the citizens house do? The citizens house has a check, a balance over some of those actions. For example, it can veto a protocol upgrade that the token house passes. But the question still becomes, what are some of the fundamental responsibilities of the citizens house? One of the biggest ones is retroactive funding and public goods funding. So what is the goal? The goal is to solve this is exactly relates to the story of us being a nonprofit.
00:49:45.710 - 00:50:27.500, Speaker B: It's literally make that be the last time that anyone had to be a nonprofit to produce good outcomes and put food on the table. So that's kind of like the fundamental place where you start. There are very, very valuable pieces of code that are written that are open source. The community has them and utilizes them to great advantage. But fundamentally, the creators of that code have no guarantee on being rewarded. This is things like a cryptography library that helps you more efficiently compute a hash function, or a database library that the Ethereum node software uses. The Ethereum node software itself.
00:50:27.500 - 00:51:06.098, Speaker B: Those things on their own do nothing, have a business model. They are open source public goods. The goal of the citizen's house is to give those folks a business model. And again, this is not just a feel good, oh, we feel bad about people getting underpaid for open source software and want to fix it. Open source software is the basis for the success of the entire industry and certainly of optimism. And so we view it as a competitive advantage for us to be able to incentivize people and reward them for building the public goods and for in general, contributing value to the super chain. Our mantra on this is impact equals profit.
00:51:06.098 - 00:51:47.494, Speaker B: The impact to the collective should be rewarded by profit to the individual. And retroactive funding is about funding those that have impact that otherwise would have gone under rewarded in terms of the retroactive piece of it. This is an awesome piece of economics design that really came from Vitalik. You can go back to our medium blog that many, many moons ago was a guest post by Vitalik. So when you look at the existing solutions to this problem, most of them center around giving upfront grants. And so what this means is somebody says, hey, I want to build this piece of open source software. It's going to be super valuable.
00:51:47.494 - 00:52:54.372, Speaker B: However, I don't have a business model, so I am applying to some grant to be given some funding upfront to be able to go ahead and complete that work. The issue with this model is that it is very difficult to know in advance whether or not that is going to have value, and furthermore, it is very difficult for a single grants giving body to know whether that's going to have value. Retroactive funding says we are going to provide the, the funding after the work is done. And what this means is that it's much easier to evaluate the actual impact that that work had and the actual improvement and benefit to the ecosystem that went under rewarded serves. The goal of that, however, is not that you might say there's a problem here. How is somebody going to put food on their table if they have to do two years of work to release an incredible code base? The point of retroactive funding is to take is not to eliminate upfront funding, it's to shift it to different entities. It turns out that the question of, hmm, this thing could be done, it might work, it might not, has an extremely solid solution in the marketplace today.
00:52:54.372 - 00:53:41.912, Speaker B: It's called venture capital. Venture capital is an entire industry of people that are trying to make a very similar bet predict that somebody that makes something will have an impact. The difference is that that impact for DC today is restricted to only those things which can extract value. So VC's can't really fund open source, open source development, but they're way better at it in the abstract as a predictive tool. The marketplace, the invisible hand, is way better at performing this than any individual. This is why the United States government does not fund all startups. Instead, a huge selection of competing VC's fund startups and the ones that are good at making the predictions turn a profit and continue to make predictions and so on and so forth.
00:53:41.912 - 00:54:18.938, Speaker B: So the ultimate goal of retroactive public goods funding, or retro funding, is what we call the retro profit organization. This is actually the idea that you have organizations whose basis for a business model is to be able to go to VC's and say, hey, I'm writing this open source software. I can't extract any value directly, but I believe it will provide a massive value to the Ethereum scaling industry. And so you allow the VC's to make really good decisions, but you unshackle them from the restrictions of silos and profit seeking by allowing the retroactive funding to reward instead of a value capture.
00:54:19.034 - 00:54:31.398, Speaker A: So in theory a startup could say I'm going to write this open source software and UVC should fund me because this will probably get a grant from optimism.
00:54:31.494 - 00:54:40.190, Speaker B: That's the premise. Exactly. And so that gets us the best of the VC predictive energy and the best of our ability to actually spend resources where they really had impact.
00:54:40.310 - 00:54:45.982, Speaker A: And where are these funds coming from? How much have you funded so far with retrofunding?
00:54:46.046 - 00:55:33.426, Speaker B: That's a great question. So there is a portion of the op token supply that when the token was created is allocated towards retrofunding. It's a big old number and I don't, the exact number off the top of my head is escaping me and I'm getting panic considering whether I should type it into a tab and do it, but I'm just not going to do that. But it is on the order of hundreds and hundreds of millions of Op tokens. To date, about 30 40 million OP tokens have been distributed in the first few rounds of retro funding. There are four more rounds coming up this year, and sign ups are opening right around the time I think this podcast will come out. So be on the lookout for that.
00:55:33.578 - 00:55:42.350, Speaker A: Okay, so funding for these grants comes directly from the op token supply.
00:55:43.170 - 00:56:32.576, Speaker B: That's one source. That is obviously not the entire answer, because you might say that is not sustainable. So the reason that we started this and realized that L2s were the best way to go and solve this problem is because L2s have a source of funding that is not just some allocation that was put onto a list of the creation of a token. That funding source is transaction fees. And so all of the chains in the super chain contribute a portion of their revenue towards retroactive public goods funding as well. And so this is the sort of like flywheel of sustainability. In practice, the folks that are using the open source chain software contribute revenue to public goods funding.
00:56:32.576 - 00:56:55.278, Speaker B: The public goods funding makes really effective decisions on making that chain software better. That enables more business and more people to create transactions on chains, which generates more fees. And it keeps going. So that's the core of the sustainability. The initial op token allocation is the way to kickstart that flywheel. But as the flywheel spins, it's transaction fees that sustain it.
00:56:55.454 - 00:57:04.660, Speaker C: Okay, yeah, that makes sense. Okay, then a couple of questions I definitely want to talk about.
00:57:04.830 - 00:57:34.574, Speaker A: It is optimism versus arbitrum. So arbitrum is now the biggest L2 roll up on Ethereum. It's also an optimistic roll up, but now it has two times the TVL that optimism has. So wondering what the difference between the two solutions is and why. Like, what explains that gap.
00:57:34.702 - 00:58:51.636, Speaker B: I'm an optimistic man, and I love talking about the positives, so I'm going to take this as an opportunity to do that. I think the first thing that I would say is that fundamentally, optimism is going through a phase shift right now of focusing on the super chain. So I actually spend a lot less time looking at the TDL of OP mainnet than I would have two or three years ago, when that was the only op stack chain. I spent a lot more time looking at the aggregate figures across many, many, many successful chains, be that base, Zora mode, so on and so forth. So I think very fundamentally, that is a really important phase shift that, quite frankly, not only us as the founders and leadership organizations for the collective, but also the community of the collective has been undergoing like a sort of evolution. And if you actually look at this season of governance, which we are just entering season six, the entire premise of that season is orienting around super chain first governance. So to me that's really the most important thing, is that actually looking at any chain's TBL is a byproduct of we had to start with one chain before we went to many chains.
00:58:51.636 - 00:59:40.302, Speaker B: But when you move to a many chain regime, that is by far the most important thing to look at. I'm very excited about the op stack in practice. I think the story that we told ourselves two years ago is coming true now around the open source development of the collective. What I mean by that is that I'm extremely proud. And I think the op stack leads the industry in the number of organizations and contributors that come that are outside of the founding organizations of the Alpine Smith foundation and Op Labs. So if you look at huge improvements that have been made to the op stack, things like 4844, which was this wonderful reduction in cost that basically made transactions a lot cheaper, that developing work was driven by Coinbase. Absolutely epic.
00:59:40.302 - 01:00:29.870, Speaker B: This plasmid design that I just talked about that makes them secure trade offs, but it pushes the cost down even lower. That was built by a team called lattice, which are another core dev to the op stack. So what I'm really excited about these days is seeing the story of open source and a global contributor set outpacing anyone actually coming true in practice. I mean, it warms my heart and it is such a phase shift for me to be able to go from, well, we hypothesize in the future that this open source software will have a global contributor network that, just like Ethereum, will help pace what we could do. So it makes sense to open source software now that is literally coming true in practice. And something like half of the protocol upgrades that we've now passed for the super chain were not actually written by Op Labs or the foundation, but were written by external contributors. So that's what I'm super excited about.
01:00:29.870 - 01:01:02.740, Speaker B: There's obviously different architectural decisions that I think the op stack just really, really powerfully makes. But to me, the proof is ultimately in the pudding of contributions. The architecture matters, but the architecture is there in service of enabling many, many, many people to contribute to this thing all at once. And I'm just absolutely pumped to be able to come on a podcast now and be able to point to real proof points of that actually working and people seriously improving the system in seriously meaningful ways, not just fixing typos in the docs. So I'm just incredibly, incredibly excited for that.
01:01:02.860 - 01:01:35.432, Speaker C: Are you ready to revolutionize how you interact with live content? Introducing Playfi, where AI meets blockchain to unlock real time data from live streams, sports and more with playfi, creators and brands can monetize and engage like never before. Join the future of live content, visit Playfi AI today and discover how you can power the next era of play. Awesome, no? Yeah, great point. And about these Dapps building on the.
01:01:35.456 - 01:01:56.566, Speaker A: Op stack, do you think optimism is more optimized for a specific type of use case or Dapp? If so, what's that? And what do you hope to see being built in? Kind of in optimism? Yeah.
01:01:56.638 - 01:03:06.322, Speaker B: So the answer is, if we're doing our job right, no. One of the core principles of the op stack is modularity, which is like the ability to take the overall architecture and have it be composed of smaller subcomponents that are individually modifiable and improvable. And so this very deeply relates to that contribution stuff that I was talking about, where the more monolithic, the less modular a code base is, the harder it is for a contributor to come in and contribute, because they have to learn the entirety of the system before they can make an improvement anywhere, as opposed to just learning one particular module and making an improvement directly in that module. The other property of modularity that's important is that it allows you to make different modules, and different modules are better for serving different use cases. So I talked about the plasma module as an example of this. There is a module that is called block derivation in the op stack and modifying the block derivation pipeline, whatever the words don't really matter. The point is there was a module that allowed us to swap in the data availability in a very meaningful, very useful way.
01:03:06.322 - 01:04:12.266, Speaker B: I think what that gets down to is that chains are going to be a tool for a lot more things than our brains even grok today because we are so familiar with the one pipe single ethereum model that fundamentally gets congested and can only fit so many transactions per second. So, yeah, anyway, to return to your question, the goal of the op stack is to be general. That does not mean that it's useful at nothing because it's not specified to anything. It's because the generality allows you to plug in modules that are purpose built to to be useful for different components. And so if you look at, for example, the release of op plasma and what lattice is doing. They're building some extremely exciting stuff in the autonomous worlds segment of the market, which is effectively super on chain games that are super composable and fundamentally rooted in on chain. So you can go play a minecraft like game that is running on the op stack, where every block of the world being mined and every item received are all on chain transactions, they're all on chain assets.
01:04:12.266 - 01:05:12.400, Speaker B: And the upper bound on what you can do with that is absolutely phenomenal. Anyway, so sorry, bit of a non answer. We're general, so that other people can specialize and do the use cases. What am I personally super excited about? In general? I think the category of things that I'm very, very excited about is the category of things that are exactly those things that don't seem possible if you're only on a one pipe mental model. And ultimately, I think what you realize as that occurs, as you begin to realize that and become more familiarized with the multi pipe model, is that actually vast swaths of the Internet, if they have multiple pipes, like multiple servers to be run on vast swaths of the Internet, can be replaced with far superior versions that are based on smart contracts on a scalable architecture. So social is like an incredible example of this shout out to Farcaster, absolutely incredible social app. They manage all of their identity on the super chain.
01:05:12.400 - 01:05:42.234, Speaker B: These kinds of things are examples of, you could never imagine every like retweet posts, like every social media action being taken on Ethereum. It just wouldn't work. In a multi pipe model, you can actually make something that is that basic work. And obviously the wallets and everything else that we talked about need to come along. But to me, that's the most exciting stuff, is the stuff that gets unlocked where you thought it was impossible because there was just one pipe. But once you have multiple pipes, it becomes possible. Social is a great example of that.
01:05:42.234 - 01:05:49.530, Speaker B: Gaming is a great example of that. I'm the most excited for the one that I can't think of that some genius out there is going to build on the pstack.
01:05:49.650 - 01:05:50.550, Speaker A: Love that.
01:05:51.050 - 01:05:54.522, Speaker C: And finally, curious to know whether you.
01:05:54.546 - 01:06:18.190, Speaker A: Think that Ethereum and all its L2 infrastructure and technology, is it prepared to take on next billion users, or to actually be the settlement layer for real mainstream finance, mainstream apps.
01:06:18.850 - 01:07:11.536, Speaker B: We're getting close. We're not there yet. It feels like there's maybe light at the end of the tunnel, whereas we certainly not left the tunnel yet. Something that I didn't call out super explicitly, that I'm extremely, extremely proud of the collective for accomplishing is the fault proof release of the kind of 1.0 of the fault proof system is about to be deployed. But the reality is, across the industry today, nobody is at a level of security that is what we might call stage two security, where effectively, in all of these systems, there are emergency protocols in place that could be used in the event of a bug or something fat. I think for us to get to mainstream finance levels of adoption on L2s, we need to be very, very mindful of where those emergency kind of like escape hatches are and avoid them.
01:07:11.536 - 01:08:00.234, Speaker B: And we spent a lot of time in modularizing the lp stack, working towards enabling that future, because it's very, very high stakes. The point at which somebody doesn't have the ability to respond to a security incident is the point at which if a security incident exists, it is no longer capable, the system is no longer capable of recovering. So that requires very, very, very thoughtful building. From our view, it actually just requires redundancy. So, like, in fact, there's an entirely separate implementation of the op stack that's written in rust that shares no code with the initial implementation that was built. These are kind of like the, these are examples of the real serious rigorousness we need to bring to the table when it comes to security. So I think security is a place where we, where great strides are being made.
01:08:00.234 - 01:08:37.830, Speaker B: I'm really, really, really proud of the collective for pushing out this fault proof release, which is like feature complete, super solid, but it still has these emergency fallback mechanisms, and it's going to be a big step to get around that, to be ready for the next billion users. I think the other thing that we need to do is improve the UX. We talked about this, but a billion users implies a lot of pipes. And if switching between pipes is like switching between chains, today with the go to your drop down and change the chain, id select a new RPC provider. That has to go away. And in fact, it has to get to the point where basically you're not even using the wallet. It just feels like you're using a website.
01:08:37.830 - 01:08:57.042, Speaker B: And maybe there's an extension that occasionally pops up once a day or the first time that you visit a website for you to take one action. We're not fully there yet on the UX, we're not fully there yet on the security, but the end is in sight. Great strategy being made. I'm very proud, and I think it's going to come en masse.
01:08:57.066 - 01:08:59.642, Speaker C: When it happened, do you think it's.
01:08:59.666 - 01:09:11.210, Speaker A: Like a one to three year thing, like transition, or like how. It's obviously really hard to say, but, like, if you could give a range.
01:09:11.290 - 01:09:40.306, Speaker B: The rule of crypto is that if a crypto founder gives you a timeline, don't believe. Don't believe them. But I guess I think that. I think that the unblocking strides, it's very reasonable to think of those happening in the next one to three years. I think that there will be potentially decades. I mean, if you look at the progression of Internet infrastructure, Internet infrastructure never stopped in proofing. We had massive, massive improvements throughout the stack.
01:09:40.306 - 01:10:22.830, Speaker B: The cloud became a thing. Sure, it got captured by webtoon, but still, technologically speaking, it's incredibly impressive that there are billions of servers in a warehouse in the desert somewhere that are making it possible for us to interact as if we're in the same room together. So I think that the big unlocks are really right around the corner on the timeframe that you're talking about to meet the bar. There will also be a long, slow and steady improvement subsequently. That I think also is just if you look at the pattern of what happened with Daredevil, the same thing that's going to happen in crypto, and we'll keep having more pikes, and they'll keep getting incrementally better and better and better and better. But I think we're close to unblocking the big step function.
01:10:23.490 - 01:10:24.266, Speaker A: Awesome.
01:10:24.418 - 01:10:52.032, Speaker C: This has been such an interesting conversation, Ben. Thank you so much for taking the time. Love your optimistic view on things. Also wanted to just give a shout out to your music writing or songwriting talented. Just checking out your twitter earlier today, and I mean, you really do have a special talent for writing crypto related songs, so just congrats on that as well.
01:10:52.176 - 01:11:06.580, Speaker B: I figured out a way for my one non crypto hobby to become a crypto hobby. So I will say, of course, it was obviously a pleasure to have the convo. I thought it was really enjoyable. And please don't go too far back in the timeline to look at my old songs, because they suck.
01:11:06.920 - 01:11:09.160, Speaker C: Okay? Promise. Okay, thanks again.
