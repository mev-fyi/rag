00:00:00.160 - 00:00:57.585, Speaker A: Hey folks, welcome back to yet another stream, another impel rust stream. This one is going to be in line with some of the previous streams we've done where we implement sort of following a little bit of a programming challenge, if you will, or we sort of code along with a sequence of challenges. In this particular case, we're going to be following the codecrafters build your own interpreter. And the reason for this is because, well, I've really wanted to do a stream on especially parsing, but also on writing an interpreter. And so now that there is one where there's sort of a well defined path to walk through that feels like a good opportunity to do so. What I'll also add is that I've gotten a lot of really good feedback on the videos where we walk through, especially these sort of well defined challenges that other people can work through on their own time. Because some people like to just sort of have this video running at the same time and try to solve it with me, like sort of pair programming.
00:00:57.585 - 00:01:46.365, Speaker A: Some people like to, after the recording of the stream is up, basically try to do it themselves and after they finish each part go and sort of compare to how I did it and then do that over and over again, which is harder to do for just implementing something completely random that you don't really have a guide to implementing yourself. And so we're going to kick that off in a second. If you are in chat then what I will send in chat now is a link you can use to join so that you can, I think you get like a week free or something. This particular challenge is also in beta, which is hard to read here. It's in beta, so it is freely available. It's also actually available on GitHub. If you're watching this after the fact, I'll put the link in the video description.
00:01:46.365 - 00:02:32.875, Speaker A: Let me see if I can dig up the free one interpreter. It is this one. I'll send that in chat right here. So Codecrafters, all of the challenges are also on GitHub that comes without all the testing infrastructure and stuff, but at least all the challenge text and code snippet and stuff are there. So if you're not willing, if you want to go beyond the seven days here and you are not willing to pay for it, just use that one instead. It's not going to be quite as nice to work with, but it is at least a way that you can continue to work through it. I'll also add the same sort of stipulation that I've done before when I do the codecrafter videos that I'm not sponsored by codecrafters.
00:02:32.875 - 00:03:20.877, Speaker A: I do get a referral if you sign up through the link that is in the video description or in the chat when you do like the seven days free. But apart from that, if I don't like things I will call them out because I'm not being paid to make this video. Excellent. Okay, before we kick off with the we'll dive into it almost straight away. I just want to mention first, and this is going to be bright for the people who don't like bright screens, there is a Discord for my streams. You can get the invite link just by going to Discord John who eu and it'll redirect you to the Discord invite link. And that has information about like the various things that I make, as well as information about sponsorship if that's something you're interested in doing, as well as an announcements channel that you can subscribe to.
00:03:20.877 - 00:03:46.445, Speaker A: And that's where I announce whenever I will do upcoming streams and the like. I'm also on the various social media media. But if you prefer to get it through Discord, then you can go in there and do that. Okay, then I think we are ready. Oh, the Discord link is just Discord John who Eu. I'll put it in the link here as well. Okay, let's dig right into it.
00:03:46.445 - 00:04:38.055, Speaker A: The building your own interpreter challenge. There were two reasons really why I really want to do something like this. The first one is there's an article by Matclad, it's from 2020, so it's not a super recent article, but where Matclad writes about Pratt parsing, which is a particular way to write parsers. That is actually fairly straightforward but also quite powerful. It is. I think it's a really fun article to read if you're not very experienced with parsers or even if you are and you just want like here's a sort of cookie cutter good way to just do simple parsing that allows you to deal with things like associativity and the like will probably follow something like this article in the implementation. So if you haven't read it, I recommend either you pause and go read it or you'll sort of follow through as I do it and you can read the whole article later.
00:04:38.055 - 00:05:05.719, Speaker A: The other reason why I wanted to do this one is I like the fact that it's based on a book, the Crafting Interpreters book, which has a website. I'm not going to open it because it's a very bright page and people apparently don't like that. But I believe at least parts of the book are available freely online as well. Or of course buy the book and support the author. But I like the fact that it sort of follows a well developed resource. So with that I think we're just going to start. Now.
00:05:05.719 - 00:05:45.575, Speaker A: The interesting thing about building an interpreter for any language is that there is an underlying language that you are supposed to interpret, right? Like Python is an interpreted language too. But the Python language is not trivial. I think the idea here with locks, which is the language that is sort of used in this Crafting Interpreters book and that we're going to be working with here, is that it's a fairly simple language. I have not looked at it, so I do not know, but given it's intended for writing your interpreter, I'm guessing that it's at least not a super complicated language to at least parse and work with. It might even be Turing complete, who knows. So you can probably build interesting things with it. But that's where we'll start.
00:05:45.575 - 00:06:15.645, Speaker A: Before starting this challenge, make sure you've read the welcome part of the book that contains these chapters. I have not read any of those chapters, so we will skim through them quickly. Let's see if there's a dark mode for this. I don't think there is. So we're going to do our little trick here and say filter invert for that. And we'll do the same thing here. Inspect HTML, filter invert.
00:06:15.645 - 00:06:43.097, Speaker A: And we'll do the same for this guy, this and element filter invert. I know there are extensions for this, but I don't actually care about dark mode myself, so I don't have them. All right, introduction. I'm going to skim through this pretty quickly. Okay, this is a welcome great. Why learn this stuff? I know lots of reasons to learn this stuff. The languages are everywhere.
00:06:43.097 - 00:06:59.667, Speaker A: That is true. There are a lot of special purpose built languages and sometimes it's really cool to be able to write your own for whatever little DSL do you want. Domain specific language you want. Languages are a great exercise. Agree with that. Too close does the author's heart. That's nice.
00:06:59.667 - 00:07:15.655, Speaker A: Book is organized with code snippets, asides, challenges, design notes. The first interpreter we'll write Our first interpreter JLocks in Java. Well, we're not going to do that. We're going to write it in rust. Of course. I almost said C. We're not going to write it in C.
00:07:15.655 - 00:07:40.515, Speaker A: Okay, don't care too much about that. Oh, in the book, they implemented in C as the second language around. That's cool. Well, we're going to be the third language. We're going to do it in Rust. Map of the territory, Parts of a language. Okay, so this is worth talking a little bit about.
00:07:40.515 - 00:08:17.505, Speaker A: I know some of this from before. So rather than sort of reading this verbatim, which you can do on your own time, I'm just going to talk roughly about like the parts that exist in a language. And I'm not a specialist at this either, so I'm sure I'll get some things right, but at least enough that we can work our way through this. The way we'll think about this is that when you have any programming language, the thing you start with is source code. It's a text file. And ultimately what you need to turn that into is, as you see all the way on the right on the mountain here, machine code. So you start from source code and you need to get to machine code.
00:08:17.505 - 00:08:58.313, Speaker A: The journey you can take to get there is. There are actually a lot of different ways to get there, but all of them start in the same way. The first of them is you need to scan the text and turn it into tokens. So this is often called tokenization or lexing. And what this is is taking the individual, like ASCII or UTFA characters and recognizing words like struct or enum or, or curly brackets as being, you know, special in some sense. That group of characters that make up like struct are actually one logical unit. It is one token.
00:08:58.313 - 00:09:30.343, Speaker A: And so you're turning the individual string of characters into these well defined tokens. After you have the tokens, you then have to do parsing. So parsing is when you go from this set of the sequence of tokens. And in fact, if you look at Rust in the Rust standard library, there's this thing called a token stream. And a token stream is a sequence of tokens, or in the Rust case is a sequence of token trees. So they actually have done a little bit of parsing here. It's not.
00:09:30.343 - 00:10:10.739, Speaker A: It's not fully unparsed. But the idea of a token stream in Rust is that this is what a procedural macro is given. So it's tokenized, it's lexed, and it's a little bit parsed. Like they parse out things like parentheses, for example, like groups. But that's almost the extent to which. And then you'll see that the main thing you have on a token stream is that it implements into Iterator and implements into iterator, where every item in that iterator is a token tree, where that token tree is either a literal, a punctuation, an ident. So this is just a keyword or a group, where group is a delimited token stream, like parentheses, square brackets or curly brackets.
00:10:10.739 - 00:11:04.921, Speaker A: And so that's it's a very like. It is strictly speaking a little bit more than just tokenization because they do things like recognize groups, but it's essentially just tokenization. And then a procedural macro takes this and outputs a sequence of tokens that should be used to parse instead of the original stream. That's really all procedural macros are. They are program from one tokenization that is the representation of what's in the source code to a replacement tokenization that should be the input to parsing. And so then after you give the sequence of tokens to the parser, the parser's job is to turn that into a syntax tree, often known as an abstract syntax tree. A syntax tree is a representation of the sort of logic that underpins the sequence of tokens that you have.
00:11:04.921 - 00:12:00.995, Speaker A: So how they are related, this is where you might do things like recognize. Then async FN is actually one function, like it is, they are logically grouped together as an async FN being a sort of unit. If you wanted to see something like this in Rust, you could look at something like the syncrate. So if you look at the syncrate, it provides parsing. So not just tokenization, but actually parsing. And if we look down at the types that you have here, here you have things like field expressions, generics, FN items, impl items like sort of knows about the logical constructs of the language. And in fact, if we look up the rust reference, which I really should have already opened, so the Rust reference here, you see, it talks about the tokens of rust, right? So this is things like characters, strings, raw strings, bytes, etc.
00:12:00.995 - 00:12:57.563, Speaker A: These are the things that you would tokenize or lex. And then items are the things that you would actually parse. So these say things like this is the syntax for a function, right? It has things like self parameters and it defines what all of these things are, how the, how the tokens go together to form the constructs of the language. And so the output of the parser or the sort of input to the parser is the token stream. And this definition of what the language looks like, the grammar of the language, which is what it's called, and the output of the parser is an abstract syntax tree, which is essentially, I mean it is a tree, as the name implies. That shows you the sort of hierarchy, such as, for example, if you parse a file, there'll be a sort of a node for this file and then the children of that node will be all the top level declarations in that file. Say one of them is a module definition.
00:12:57.563 - 00:14:01.819, Speaker A: Then you'll have a module node, and that module node in the tree will have a bunch of child nodes, and each of those are the items within that module, such as an impul block or a type definition or whatever it might be. And so that's how you end up with this notion of an abstract syntax tree is that is a tree representation of the items that have been parsed out of that file. Okay, so that's the idea of a syntax tree. And then the question becomes, how do you take a syntax tree and turn it into something the computer can execute? Well, there are a couple of ways to do that. One of them is to not actually produce machine code, but instead take that abstract syntax tree and hand it to another program that knows how to execute that. And this is how interpreted languages usually work, is that the output of the syntax tree is really just like, it doesn't compute anything. The syntax tree is just a representation of all of the stuff, all the declarations in the file, but nothing has been executed.
00:14:01.819 - 00:14:43.845, Speaker A: And then you hand that to a program, in Python's case, the Python binary, and it knows how to walk that tree and evaluate the items of that tree so that like if it sees a for loop, it will actually execute a for loop sort of on your behalf. So your program is never directly turned into machine code. Another program is sort of using your input, your, your source code as instructions for what it should execute. And it has all of the machine code. Syntax tree is, I mean it's not really a binary or not, it's an abstract tree. It's never actually written to disk anywhere. It's sort of an in memory representation of the semantics of whatever source file has been, has been parsed.
00:14:43.845 - 00:15:35.537, Speaker A: And so a high level language like Python or Node JS will generally execute an ast. It will traverse the AST and then interpret it, execute it. The other thing you could do is you could turn it directly into machine code. This is what happens in a language like Rust or C, where you take the, you take the astronomy, you walk the ASD using something that knows how to translate different concepts, like abstract concepts into machine code, into assembly code. This is something that you know LLVM is a good example of this. So LLVM takes as input a sort of abstract definition of operations, like for loops or functions or function calls, and then knows how to sort of optimize those and turn them into executable machine code. And that's what you see this path here from syntax tree that goes up here through analysis and to intermediate representations.
00:15:35.537 - 00:16:46.447, Speaker A: So LLVM has a bunch of these intermediate representations and Rust in fact has its own, called mir, which is the middle intermediate representation. They also have hir, which is the high level intermediate representation. And so the sort of abstract syntax tree gets turned into hir, which gets turned into mir, which gets turned into LLVM ir, meaning intermediate representation here. And then the LLVM internally does a bunch of optimization sort of transformations over that graph to do things like move expressions out of loops if they don't depend on the variables within the loop, like optimizations, passes like that, that happen not at the machine code level, but happen at the sort of more abstract semantic level. And then ultimately what you end up with is some intermediate representation that is now ready to be transpiled or translated, if you will, almost directly into code generation. So by the time you get close to getting ready to do this code generation, the thing that actually generates machine code at that point, your intermediate representation is usually quite close to assembly. In the first place, it might no longer have a notion of loops.
00:16:46.447 - 00:17:40.109, Speaker A: For example, like there isn't really a looping construct in machine code. In assembly, all you have is like a go to, you have a jump and so a loop is you walk like you run a bunch of code and at the end you evaluate some condition which is the loop condition, and if the condition is true, then you jump to the first instruction of the loop again. So there's no loop construct in assembly. And so this is an example of something where in the lowest intermediate representation you might already have eliminated all loops and replaced them with go tos, because that's what you're going to need for the code generation step, which is that last bit that takes you into actually executable machine code. And then there's a variant of this which is something like Java, where the thing that the compiler generates is not actually machine code, it's not assembly code that runs like on your cpu. It is actually sort of. It is more like the high level language approach.
00:17:40.109 - 00:19:00.675, Speaker A: But what you handed is not the abstract syntax tree for it to traverse and interpret like Python, but rather a sort of general purpose machine code. It's sort of a meta Machine code that can then be interpreted by the Java virtual machine that can then sort of similar to Python, it has all the machine code and it reads the output of your intermediate representation, your bytecode, and evaluates that to basically execute your program on your behalf. So those are the sort of different paths here. And in our case, when what we want is an interpreter, generally what we mean is something that goes from source code, scans it into tokens, parses it into a syntax tree, and then interprets it by executing directly that syntax tree. So we're not like in what we're doing here, I don't think we're going to do any of the sort of analysis or optimization of intermediate representations or generating any assembly code. I'm going to pause there because that was a lot to take in and do sort of questions if you have them, and then we'll move on from there. Is the AST tree like objects? You can sort of think of it like that, like it's a tree structure where all the nodes describe a particular item in the source tree, like a module or a function definition.
00:19:00.675 - 00:20:33.285, Speaker A: Does the LLVM take IR as its input instead of an, as an abstract syntax tree? So LVM's input is actually also an intermediate representation. So in, in the Rust compiler, what actually happens is you take the, you parse into the syntax tree, you turn that into the high level ir like hir, you turn that into mirror, and then there's a bunch of optimization passes on that intermediate representation and then eventually they move it to LLVM's IR and at that point they call LVM and then LLVM does the remaining bits from there. Not sure, but I think Python also has a bytecode. I think you're right. So I think Python now has sort of a bytecode equivalent where you can sort of pre compile your Python code into like, rather than just giving the raw source code file to Python, you can sort of pre compile it, which is essentially turning it into bytecode, which is really just a byte representation of the syntax tree, if I remember correctly. So that you can then hand that to Python later and it doesn't have to do the scanning and the parsing, it can just directly execute the resulting file. Will that include a gc? My guess is the language here is not compared complicated enough that it needs a gc, but we'll see.
00:20:33.285 - 00:21:35.775, Speaker A: Okay, since Golang is converted into binary, but has garbage collector, how does that work? So a language like Go produces machine code, but the way to think about it is that There's a, there's a bunch of source code that is compiled into your binary, into your machine code that you have not written written. So Go sort of comes with a bundled piece of source code that does the garbage collection, does a bunch of transformations on your source code so that it interacts with the runtime that they ship. But the runtime is sort of embedded in your binary. Whereas in Java the runtime is a separate executable. The Java VM that you hand your compiled Java file to or compile Java Project, rather. Bytecode is compiled assembly. Not quite.
00:21:35.775 - 00:22:13.065, Speaker A: So bytecode is a sort of equivalent to assembly, but assembly is or machine code rather. We should be technically accurate here. Arguably, assembly is really just a programming language, but machine code is usually specific to a particular cpu. It might even like it's, it's, it's to. Not just a particular family of CPU can even be specialized to a very specific architecture, a very specific instance of a cpu. Whereas bytecode is a sort of low level, very low level language that can be run on many different CPUs. Locks does have a GC.
00:22:13.065 - 00:22:36.425, Speaker A: It also has closures and inheritance. Ooh, this will be, this will be interesting to see how far we get through that. I'm excited to see the Java chapter uses Java's gc. In rust I've used reference counting. Oh, so that's interesting. So if locks does have garbage collection, that means that we will have to act as the garbage collector for the language. We might just do that in a sort of simple way with reference counting or something.
00:22:36.425 - 00:23:08.015, Speaker A: We'll see that when we get there. How much performance do you live on the table by doing work in an interpreter compared to compiling it to machine code? It varies. But usually interpreted languages are slower than fully compiled languages because they have to do more work. Right? Because they essentially have to do part of the compilation at runtime. Locks is more complete than you might think. It's like lua. Well then I'm excited to start implementing it.
00:23:08.015 - 00:23:40.435, Speaker A: Go produces something called ssa. This is a single assignment. What's it called? I forget what the first S for. I think it's serial single assignment or something, which is a machine code that is not tailored to a particular cpu. And it's a very useful representation for being able to just execute a program after the fact. Static single assignment. That's what it is.
00:23:40.435 - 00:24:22.315, Speaker A: Okay, so now that we have this sort of. We've explored this map of the parts of a language. We've talked about scanning, we've talked about parsing, we'll get into this a little bit once we started writing the code. I don't want to do too much of this in advance. Static analysis is this idea of after you've parsed, you can then do a bunch of analysis things like type checking, right, where you check that if you try to call a function, then the function you're trying to call actually has the types that you gave for the arguments, for instance. So this is where you add semantic richness. So you no longer have just a function call, you actually have a specific function with a type signature.
00:24:22.315 - 00:25:09.971, Speaker A: And so now you infuse your code with meaning, right? Like if you do A plus B, what does that mean? Intermediate representations, optimizations, Code generation. We've talked about most of this. Now I'm obviously skipping past a bunch of detail. Highly recommend you read this if you really want to get into the depths here. But I'm more trying to give us enough of the context that we need for starting to write some code. Single pass compilers. Yeah, so single pass compilers are a version of a compiler where rather than parse everything, like into memory and then doing all these intermediate representations, they try to be a lot faster at compiling by not doing any of that.
00:25:09.971 - 00:25:52.201, Speaker A: It tries to make sure that you can, as you parse, almost as you parse, you produce the machine code. Do you do a single pass through the file, and at the end you've produced all the machine code. That does mean that you can end up with a way faster compiler. The downside is you can be a lot less smart about things like transformations because you haven't seen the later code yet. So you can't optimize based on it, but also because it forces declarations to happen in a particular way. So this is one of the reasons actually why, at least if I remember correctly, why in C, in older versions of C, you had to declare the signature of a function before you're allowed to call that function. So you don't have to give the body of a function at the beginning of the file.
00:25:52.201 - 00:27:00.835, Speaker A: But like, if you have two functions, foo and bar, and foo wants to call bar, you can't have foo first and then bar. That will not work. You have to have either you have to put bar before foo, or you have to put a declaration of the signature of bar first, and then you write foo, and then foo is allowed to reference bar, and then you put the definition of bar, because it's this sort of single pass compiler. C has since evolved from that, where you don't need to do that anymore. Tree walk interpreters are, as we talked about, things that specifically will take the AST and that execute the AST directly. And then transpilers are things to just take one representation of language and turn it into another one. So this is something like if you compile Rust to JavaScript, like WebAssembly is an example of a transpiler, because you're not producing machine code, you're producing just a different source code language that then gets executed in some other way and then just in time.
00:27:00.835 - 00:27:48.825, Speaker A: Compilation is what Python often does, for example, where you take the. It's basically you, you take the syntax tree, the app stack syntax tree, and when you need to call it, you compile that thing and then you execute the machine code, right? And then the next time it gets called, you already produced the machine code, so then you can just call it again. So hence the name just in time. You compile it just when you need it, which can make the compiler phase really fast because it doesn't really need to compile anything. And instead you pay the cost of compilation only on first call. Oh, Python doesn't do JIT. No, Python does do JIT, I'm pretty sure.
00:27:48.825 - 00:28:36.521, Speaker A: And JavaScript does JIT as well. Yeah, and this is one of those. Like it's a little bit weird to differentiate between compilers and interpreters because interpreters sometimes do some compilation, like just in time compilation, and compilers do kind of interpret things, things like if you have a constant expression in C or a const in Rust that has an expression and not just a value, you actually have to execute code at compile time, which is really kind of an interpreter. So like, these are not perfectly separate circles, they're actually fairly related. Oh, interesting. Yeah. So I guess, at least according to this, there are very few languages that are pure interpreters anymore.
00:28:36.521 - 00:29:11.159, Speaker A: Well, I guess JLocks apparently is, and then purely compiled languages. And then you have a bunch here that do things like jits, where it's. It's an interpreter, but it also does compile. Okay, so now to that chapter, and now the LOCKS language. And this obviously is a. An important thing for us to know, given this is the language that we're supposed to parse and interpret. Now, I don't think what we're going to do is try to fully understand the entire grammar and semantics of the language upfront.
00:29:11.159 - 00:29:42.681, Speaker A: That's going to be very hard. Instead, what we're going to do, and I'm guessing this is what this chapter will do as well, is give us an overview of the language like what roughly does it look like? What's roughly? The syntax. What kind of items does it have? You know, how do you clear declare variables and the like? Yeah, exactly. I don't want to drag you through the reams of language lawyering and specification is before you get to touch your text editor. So it's a gently friendlier introduction to locks. Okay, let's see. Okay, so comments are dash, dash.
00:29:42.681 - 00:30:15.659, Speaker A: Let me see if I can zoom in a little bit here. We're talking. Okay, so we have comments, we have print. We don't have parentheses around function arguments, which is interesting. Yeah, so locks is pretty compact. It's kind of like JavaScript scheme in Lua looks the most. Like JavaScript Lux is dynamically typed.
00:30:15.659 - 00:30:41.595, Speaker A: Okay, so variables can store values of any type. You don't need to declare that this type, this val, this variable holds like a hash map, for example. It just holds what it holds and it can change over time. Errors happen at runtime on types. That seems fine. Automatic memory management. That seems pretty reasonable too.
00:30:41.595 - 00:31:22.147, Speaker A: You don't really want to have to write a bunch of freeze in your code. You want that to just be handled by the interpreter. And I think in our case what we will almost certainly do is we'll do reference counting for keeping track of when variables should be deallocated and stuff. Rather than a good garbage collector, we are going to write our own garbage collector. That's cool. I mean, we might end up doing that as well. I'm guessing that we're not going to get all the way to writing our own garbage collector in this video at least.
00:31:22.147 - 00:31:48.655, Speaker A: But this might be a sort of beefy enough challenge that we end up doing a two parter and then part two might easily be the. Might easily be the garbage collector data types. Okay, we have booleans. That makes a lot of sense. We have numbers. There's only one kind of number. Double precision floating point.
00:31:48.655 - 00:32:24.025, Speaker A: Okay, so integers are floating points. Great. Strings and double quotes. And parsing strings is actually surprisingly difficult. Not strings like this, but once you have to be able to escape characters. What if you want to put a double quote inside of a string? What do you do then? Well, okay, let's say you do well. What if you want to put an actual backslash in the string? Well, now you need to support backslash backslash.
00:32:24.025 - 00:33:07.645, Speaker A: And so it gets interesting pretty quickly and nil interesting. Does it not have. So it doesn't have eraser maps, just as booleans, numbers, strings and nil interesting expressions. It has arithmetic, plus, minus, multiply, and divide. So all four of these are infix operators. So infix operators are operators that sit in between their two operands, and then they also have a prefix operator for negative numbers. That makes sense.
00:33:07.645 - 00:33:47.631, Speaker A: And plus also works on strings. Okay, comparison inequality. We can compare numbers and only numbers with the standard comparison operators. Equality, inequality, inequality for different types, which presumably always mean, always returns false logical operators. Okay, so this is the not operator, which is a prefix operator. So it's not an infix. It doesn't have two operands, one on each side.
00:33:47.631 - 00:34:18.235, Speaker A: It is a prefix operator. So it's an operator followed by its operator end and then and which is an infix operator for logical and or infix for or. Okay, precedence and grouping. Great. So this is the notion of you want. You want to be able to put parentheses. If, for example, you have A plus B times C, then normally multiplication binds tighter.
00:34:18.235 - 00:34:42.287, Speaker A: So you would end up computing B times C and then adding A. But parentheses are the way that you would say, no, I really want A plus B, and then multiply that with C. So you need something like parentheses to be able to express that kind of thing. There's no bitwise shift modulo or conditional operators. Great. Statements. Okay.
00:34:42.287 - 00:35:09.895, Speaker A: An expression's main job is to produce a value. Yeah. So expressions are things like this. This would be an expression, and this is a statement. So the statement is usually the thing that ends with semicolon. A statement is usually things like calling a function or assigning something to a variable, whereas an expression is just a thing to compute. Expressions.
00:35:09.895 - 00:35:46.015, Speaker A: An expression followed by a semicolon promotes the expression to statementhood. An expression statement that makes sense. And you can have blocks, curly brackets, we know these from Rust. That are sequences of statements. And so, for example, the way that you write a function, I would assume, if we get there, is that the way you write a function is that a function is the function keyword followed by a signature followed by a block. Variables. Variables default to nil that can be given an initializer.
00:35:46.015 - 00:36:06.155, Speaker A: You can access and assign variables using their name. That makes sense. Control flow. If conditions. Okay, so conditions are surrounded by parentheses. So our conditions in while loops. And we have for loops.
00:36:06.155 - 00:36:40.315, Speaker A: Okay. Functions look the same as in C. And in Rust you'd find them with the fun name name of the thing, argument list, which has no types, so that makes it easier to parse. There's no named return type and then followed by a block. Okay. And you can return inside of a function that makes sense. And if there's no Return, it implicitly returns nil.
00:36:40.315 - 00:37:25.161, Speaker A: So unlike Rust, the last expression, a function is not allowed to, or a function block is not allowed to end an expression and have that be the return value. Okay, closures. Oh, it has first class closures. Closures are going to be their own kind of interesting thing to implement. So here you can see that add pair is actually a function. And so here we are passing add pair to a function as an argument. And so this function just takes a parameter whose type has not been declared at all and just returns that same parameter value.
00:37:25.161 - 00:37:48.857, Speaker A: And so when we pass add pair as an argument, that ends up having this thing return ad pair back like identity. It just returns what it's given. And so this ends up calling add pair. So this means that ad pair is actually a. Like functions are a first class type, if you will, in this language. This alone is not closures, though. This is just first class functions.
00:37:48.857 - 00:38:15.283, Speaker A: And then this is a closure. So you can declare a function inside a function. And crucially, the, the way in which that matters is like you see here, you can have a. A variable in an outer function, and you can have an a function inside of that outer function that references variables that are declared in the, in the defining scope of this function. And this is, this is what is a closure. It. The.
00:38:15.283 - 00:38:56.071, Speaker A: The nomenclature here comes from. It closes over its environment. It gets to have access to the space in which it was declared, the scope in which it was declared. Okay, so inner has to hold on to references to any surrounding variables that it uses so that it stays around even after the outer function is returned. Yeah, and this is the other thing that's interesting here is that notice here, this is a thing that's actually kind of hard to do in Rust. It's not impossible, but it is hard. Which is here we're returning this closure and this closure captures this variable.
00:38:56.071 - 00:39:45.055, Speaker A: And so that means that for this, this closure to be valid after this function returns, that means that this needs to capture this variable in some way so that when this function returns. You know, normally in Rust terminology, we would drop this variable value when this function returns. And therefore you wouldn't be allowed to call the return value the closure that gets returned anymore because the variable it references has been dropped. But in locks, that's allowed. And this is basically because it has something equivalent to a garbage, collector, right? It knows that the closure is still using this value for it to remain. For it to remain valid to access. And so therefore it's fine to return this closure and then call it down here.
00:39:45.055 - 00:40:16.295, Speaker A: Even after the function returned. And it also looks like print is maybe special here, that print is not a function. You see, print doesn't have any, any parentheses for calling print, but functions do. And so this makes me think that print is actually a first class primitive in the language. So it is, it is special. It is not just a function where you can emit the parent. Yeah, it's a built in statement.
00:40:16.295 - 00:41:05.615, Speaker A: Classes. Oof, it has classes. Interesting. Okay, is it a class or a prototype language? Let's see. I'm interesting. Okay, so locks has classes, not prototypes. That's fine.
00:41:05.615 - 00:41:58.715, Speaker A: So you can have a class, you can have methods on the class, and the methods on the class don't require the fun keyword. Okay. When the class declaration is executed, LUX creates a class object and stores that variable name that in a variable named after the class. Oh, I see. So when we parse and execute this, when we interpret this class definition, then we create a class object and we create a variable that's named after the class. We get a variable called breakfast that is itself a reference to the breakfast class object. So not an instance of Breakfast, but the class itself.
00:41:58.715 - 00:42:36.693, Speaker A: Interesting. Okay, and the way you create a new one is we don't have a new or anything. We just have a sort of implicit constructor that takes no arguments. Okay, you can access fields. That makes sense. This is how you refer to the current instance and you declare an initializer. You don't have to declare the fields anywhere.
00:42:36.693 - 00:43:06.465, Speaker A: The fields are just. They just magically appear, I suppose. Great. And in it is sort of a specially blessed method name that is used for instantiating things of that class. And we can have single inheritance. So a class can sort of extend another class. That's fine.
00:43:06.465 - 00:43:41.767, Speaker A: Every method defined in a superclass is also available to its subclasses and you can call supers init. Yeah, this is very standard Java version. Okay. Standard library, a functionality that's implemented directly in the interpreter and that all user defined behavior is built on top of. This is the saddest part of locks. Its standard library goes beyond minimalism and veers close to outright nihilism. Okay, great, that makes me happy.
00:43:41.767 - 00:43:59.367, Speaker A: For the sample code in this book, we only need to demonstrate the code is running and doing what it's supposed to do. For that we already have the built in print statement. Later when we start optimizing, we'll write some benchmarks and see how long it takes to execute code. That means we need to track time. So we'll define one built in function clock that returns the Number of seconds since the program started. And that's it. Great.
00:43:59.367 - 00:44:27.965, Speaker A: Okay, this is a standard library that will be okay to implement, I think. All right, let's. Now I'm going to keep that one open in a background tab to see where we go from here. Update. Ooh, an update during a stream. Scary. Okay, how about.
00:44:27.965 - 00:44:56.431, Speaker A: Okay, empty file is going to be very easy and then it moves on from there. There are a lot of challenges here. I wonder how far we'll get. They claim that these down here are hard, but I think they might actually not be that bad, like scanning. I actually think Pratt parsing is going to get us through surprisingly quickly. Parsing is a little trickier evaluation. I think this is going to be fun.
00:44:56.431 - 00:45:06.689, Speaker A: I'm excited. Okay, where's the start button? Start. Start building. Okay. Welcome. Thank you. I'm going to do it in rust.
00:45:06.689 - 00:45:21.205, Speaker A: Thank you very much. Language proficiency. I'm going to go with advanced. Great. Next question. How often do I want to practice? Once a month. Accountability.
00:45:21.205 - 00:45:46.915, Speaker A: No, thank you. I do not want accountability for my actions. Okay, let's go ahead and do this. Codecrafters interpreter rust. Codecrafters interpreters rust. Okay. Push an empty commit.
00:45:46.915 - 00:46:07.549, Speaker A: I believe in us. Git Push Origin Master Turbo test run. Oh, no, we failed. Tokenize Test locks expected line number one. Non standard out to be. End of file null. Got nothing that.
00:46:07.549 - 00:46:19.785, Speaker A: I mean, that makes sense. Okay, great. Because we haven't implemented anything. Fireworks. Git Push Received Activated. Okay. Comment A code to get you started.
00:46:19.785 - 00:46:42.081, Speaker A: Uncomment the code and submit. Yeah, this is probably just a sort of tokenize. Oh, I see. Okay, great. So this is just. If the file is empty, then we just print the thing that the tester wants. Great.
00:46:42.081 - 00:47:03.025, Speaker A: Love it. Yeah, I did that. Okay, I don't. What? I'm confused it Uncomment the following code. There's some stuff at the top it wants me to uncomment too. Oh, I see. If I print to standard error.
00:47:03.025 - 00:47:37.351, Speaker A: Why. Why do they do this instead of just e print line? Can I just e print line instead? Feels excessive to be very explicit about writing to standard error. Right. I'm going to go with this is probably better. Question mark. Great. It could be that.
00:47:37.351 - 00:48:02.357, Speaker A: Didn't know that was a thing and now I don't need these things. Great. Git diff. No hacks Git Push Next step. Test passed. Okay. Stage marked as complete.
00:48:02.357 - 00:48:31.645, Speaker A: Look at us racing through. This is easy. Read the instructions. Your task. Oh, interesting. So this is. This makes me a little bit sad, but we might be able to.
00:48:31.645 - 00:49:07.779, Speaker A: We Might be able to still do this with Pratt parsing. So this is where I'm gonna. I'm gonna leave the challenge for a second and I think I'm just gonna. I see. So the language sort of defines this, the sort of output for what it should look like as you walk through when you lex. That's fine. So this is just the lexing part.
00:49:07.779 - 00:49:35.185, Speaker A: And the lexing is not where we need a Pratt parser yet. Let's see what the book wants us to do. All right. Filter inverse, inverted. Okay. In each turn of the loop, we scan a single token. This is the real heart of the scanner.
00:49:35.185 - 00:50:14.495, Speaker A: We'll start simple. Okay. It just. Yeah, we just walk one character at a time and keep track of every character and turn it into a token. Okay, so are there other files in here already? No. Great. So I'm first of all going to start a lib rs and in here we're going to have a puppy num which is going to be token.
00:50:14.495 - 00:51:23.323, Speaker A: And then here we're going to use code. What's the name of this again? Codecrafters interpreter. We're just going to use everything from there. And then in lib over here, we're then going to implement. How do we want to do this easiest? I think we're just going to have a freestanding function parse lex, which is going to take any string. So the input here is going to be a str and it's going to return a result. And actually, if we want to be really fancy here, if we want to be really fancy here, then we pull in a dependency on Miette instead of anyhow, because that way we can give errors that highlight specific parts of the input.
00:51:23.323 - 00:51:44.065, Speaker A: Let's. Let's do that. Let's be bold. So miette, which is 7, 2, 0. Okay. And I don't actually want this error for this either. And we're going to use miat wraper and error.
00:51:44.065 - 00:52:37.965, Speaker A: In fact, we can just grab. No, I like having my own result. So we're going to do this and it is hopefully going to return an impl iterator where the item is token. And so the other question we need to ask ourselves here is, do we care about this being streaming? Because there are sort of two ways to lex. One of this is a streaming lexer where you walk the input, and as you get tokens from the input, or as you get characters from the input, you produce tokens from the output. So it's sort of a transformer of an input to output stream. The other alternative is to take the whole string, lex the whole string and return.
00:52:37.965 - 00:53:49.285, Speaker A: Like here's the vector of all the tokens I found. It's actually kind of tempting here to change this slightly and say that the input is actually into iterator where the item is a character. And then what we're going to produce is an iterator where the item is a result of either a token or an error with a sort of. Once the iterator returns error, it will only return none. So the idea here is that we're going to be grabbing characters from the input, trying to lex them, and whenever we successfully lex a token, then we yield the thing from the outgoing iterator. Ultimately though, either we successfully yield a token and then we keep going and we successfully yield another token, or we might yield a. We might end up basically with a.
00:53:49.285 - 00:54:28.835, Speaker A: Like for example, we encounter a character that wasn't supposed to be there that we weren't expecting in the input, at which points we will make the iterator then return an error and from that point forward the errors. The iterator is sort of invalidated. Because now what we can do is we can say input is. And this is where usually I think you might need a peekable here, but we might actually not. Someone said don't you also need a wait state? And the cool thing here is yes, you kind of do. You need to sort of accumulate characters, but at the same. I guess.
00:54:28.835 - 00:55:49.511, Speaker A: I guess really what we want here is a generator, right? We kind of want a generator here. You can, however, do this by doing like. Essentially what we do here is we define a struct that is sort of the lex state, right? And that's going to hold something and we are going to do. Uxtate is a lex state of nothing. And then we're going to do. I'm trying to see if there's a smart way we can do this here such that it's actually a lazy iterator. Because the problem, right, is that you might get one character and then you might get another character that should be a part of the previous token.
00:55:49.511 - 00:56:31.605, Speaker A: So the first thing doesn't yield anything. And so that's sort of trivial to write with a while loop, right? The problem with a while loop is what you really want to do is for C in input, do some stuff. And one of the things is going to be, you know, if completed token then you want to yield, which is not a thing. We don't have generators in rust yet. Then you want to yield token for C so far. But we don't have a way to express that in Rust today. The closest you can do is you have a struct that sort of holds the state instead.
00:56:31.605 - 00:57:57.665, Speaker A: And then you. Yeah, the way we're going to do this is we're going to do here new over input and then we're just going to return that. You'll see how this works in a second. And then we're going to impul, I guess, really lexer and at this point we don't even need the function. We're going to have a new that just takes a this and returns a self. And then we're going to implement Iterator for lexer where the item is going to be this and this of course is going to be a to do. But one of the things we're going to have to keep in here is this is going to be generic over the Iterator and this is going to be I.
00:57:57.665 - 00:58:42.219, Speaker A: This is going to return a self where the iterator is input or is input dot into iter rather. And then here we're going to return where I is an iterator whose item is a character. Great. And of course this is going to be public. That's public. And the Iterator implementation is automatically public. And then in here next, this is where the actual lexing is going to happen.
00:58:42.219 - 00:59:49.597, Speaker A: So whenever we get asked for the next output token, that's sort of what this is asking for. Like what is the next output token in the stream? Then we need to figure out. Then we need to figure out, okay, what is the next output token? And that's going to depend on what the inputs are. So in the call to next here, we basically need to figure out the next token. And the way we're going to do that is while let C is self Iterator next, we might actually not need any other state in the lexer than the Iterator. We might be able to just keep it as local state in the next method. Okay, so the question now is how do we lex? Well, C here is going to be a character and the thing we want to match on here is we want to sort of construct up if we go back to the things that we should be able to lex.
00:59:49.597 - 01:00:50.291, Speaker A: And this is where I want to see the list of all the tokens. Okay, so here are the base tokens for the language. Now these of course are slightly different in Rust. The syntax here is going to be this and this is going to be token. Okay, now for all of these ones, we actually just want to return some of this right. Not. And in fact, we can do let sum is this.
01:00:50.291 - 01:01:33.793, Speaker A: And then we can do question mark. Because if the inner iterator returns none, then we might as well just return none ourselves. This is actually a little bit special because it might be later on here that if we get an opal open double code, for example, and then it's the end of the input stream, then at that point that's actually an error. It's an unterminated string literal. But we'll handle that in a second. And so now we want all of these to be variants. Great.
01:01:33.793 - 01:02:20.325, Speaker A: So all these are now variants up here. And we can also get help from Rust here to rename all of them. Why is it okay with these? Oh, I think I've seen a bug report about this and it's actually kind of annoying to fix colon and tar. And then we want to implement display for this so that people can print tokens back out, which is what we're going to want to do in order to print this whole. This whole output that it's looking for. Remember how it wants a very specific output of which tokens were found? Well, we're going to give it to him. So we're going to match here on self.
01:02:20.325 - 01:03:10.601, Speaker A: And this was left paren. And this can actually just be right F and this and that. And then this is going to be right paren. This is going to be left brace. This is going to be right brace, comma. There are crates that can help you do this a little bit with less boilerplate in practice here. I don't think it would be worth even finding one and setting it up, because it's.
01:03:10.601 - 01:04:05.825, Speaker A: It's not that much work. Right Star. Okay, so now we have these tokens and we can also derive debug, we can derive partial E, can eek and we can derive hash. And in fact, we can also derive clone and copy. No, not copy, because we're also going to want to support literals here. And in fact, this is one of the reasons why it might be a little annoying to do this token down here over character inputs, because it means that if you, for example, want to parse out a, you want to tokenize something that has a string literal, then what we're going to do here at the moment is we're going to parse out every character one by one, which means we're going to reconstruct the string one character at a time. So it's going to be slow, but it's going to be possible.
01:04:05.825 - 01:05:05.575, Speaker A: The alternative would be for this to Take a STR with some lifetime and then you return a lexer with a lifetime tick A as well so that it can refer back into the string. That's actually pretty. Okay, that this is how SERDE works, for example, is that the deserializer has a lifetime bound of the input precisely for this reason. And in fact, maybe that's something we want. Now that I think about it, maybe it's not that useful for this to be an iterator over characters. Yeah, maybe we actually do want this to be A. All right, okay, I'm sold.
01:05:05.575 - 01:05:48.475, Speaker A: And in fact, we can sort of follow the SERDE nomenclature here and say that you need to give me a de. And then that's going to return here. And the iterator here is actually going to be rest, where rest is sort of the remainder of the input. And this is now going to be over de. There's no longer generic I. This is just going to be a de str. The reason I call REST here is because.
01:05:48.475 - 01:06:37.383, Speaker A: Is because initially it would be the whole string. And then once we've read the first character, we're actually going to move the string reference, the start of the string reference so that it points to the remainder of the string instead. And the reason we want to do the D here is now token can be given a D over here. So this is giving me generic over the thing that's being deserialized. And so now once we get a string literal, for example, in here, literal, then that literal can actually be a de str. Now you're going to see this in a minute when we get there. You can't always use this as a string reference because sometimes you need to do DE escaping, for example, or unescaping.
01:06:37.383 - 01:07:38.675, Speaker A: Right. Where if there's a backslash in there, you don't want to preserve that in the literal. But we will not deal with that right now. So for now we're going to stick with this. And so now what this is going to be is if. No, we're going to do self.rest. how do I want to do this? I want to do split so I can write it the sort of dumb way first, which is cars.next
01:07:38.675 - 01:08:44.015, Speaker A: and then I think there's like chars. Yeah, and then self rest equals self restriction from C dot is a bytes. What is the name of the method on straight on characters characters? Lenny Divid. Like, so there we go. But I think there is a way to do like split at and. Oh, it's a split at and a byte index which isn't Quite good enough. Okay, so what we're doing here is we're pulling out the first character of the string, and then we are.
01:08:44.015 - 01:09:04.907, Speaker A: That returns an option. If the string is empty, that option will be none. And so therefore, we return with a question mark. So C here is the first character that we haven't lexed yet. And then we reassign rest to point to the remainder of rest. So we sort of bump the start of the slice pointer. And now we can match on these.
01:09:04.907 - 01:09:56.025, Speaker A: Now, this is dot, minus, plus, semicolon and star. And we also have to remember that this is actually okay like so, because, again, remember, we can return an error at any one token that we're walking. Oh, Cars Aster. I don't think we can use Cars Aster, because I think its lifetime is tied to the lifetime of the Iterator. But I could be wrong. Let's try that. So cars itself, Rest.
01:09:56.025 - 01:10:18.737, Speaker A: Cars. And then C is cars. Dot next. And then self rest is cars. As Str. Oh, great. Aster returns a string whose lifetime is tied to the original input string.
01:10:18.737 - 01:10:52.057, Speaker A: Perfect. That's what I wanted. That way we don't have to do any of our fancy UTF eight bits. Now, the reason I want to specifically return here is because there are some characters for which we're going to fall through, such as open, double quote for those things we don't want to return. So it's specifically these. We want to return early, and anything else we actually want to fall through. So for now, what we'll do here is we will return.
01:10:52.057 - 01:11:48.945, Speaker A: And this is where Miette is fun, because we can do met bail. And technically, what we would want to do here, I'll be. I'll actually keep hole here. So whole is input and rest is input. There we go. And then I also want to keep track of the I that we're at. So initially we're at I0.
01:11:48.945 - 01:12:10.471, Speaker A: Or I guess this could be byte. It might not be byte. It might be. Yeah, it'll be I. So this is like, what. What character are we at? You'll see why in a second. So this will be self.
01:12:10.471 - 01:12:42.605, Speaker A: I plus equals one. So if we look now briefly at Miette, you'll see that in Miette, when you output an error. Where's my. Where's miat? Bail. Bail. That is not the one I wanted yet. Where's the actually useful one? Diagnostic.
01:12:42.605 - 01:13:12.679, Speaker A: Ah, okay. Yeah. So you can give. I think Miette. Miette had the same description. So you can give a severity, which is fine, and code and help and stuff, but the interesting thing is how this is rendered. So labels lets you label a particular subset of the input and say this part of the input is the one that's interesting.
01:13:12.679 - 01:14:01.835, Speaker A: So I think if we go to report maybe and then diagnostic code is the one that I want labeled span. What is byte offset? Great. From the beginnings of a source code. Okay, so we do actually want byte offset here. And so then this will be C utivit lenutivit. Technically we could write a. We could write a reader that keeps track of how many bytes it has yielded and stuff, but that gets a little too annoying and I don't think it's going to be all that useful.
01:14:01.835 - 01:14:35.965, Speaker A: But what's cool here now is that if we now go back to the. Oh, I need to keep better track of my tabs in delayed source code. No, I don't want delayed source code. I want the one where you actually give the source code directly in. Aha. Source is what it's called. But I want to do it with the macro instead of having to construct the big type.
01:14:35.965 - 01:15:29.625, Speaker A: Because snippets is the thing that I want. I forget how you pass this to the macro. I don't want to use the error type. That's frustrating. Ah, oh yeah, you're right. It is just with dot with source code. Code.
01:15:29.625 - 01:16:34.595, Speaker A: Yeah, right here. Okay, great. Okay, and then I'll have to grab labeled span Some error of this self whole expected literal. Right. Because I also need to give here I want to say unexpected token in input. Does that really need to come first? That doesn't seem right. But here we can then do self byte.
01:16:34.595 - 01:17:48.845, Speaker A: I guess actually we want to do this here. Self byte to self byte plus cnnut this character. What am I doing wrong here? O Len UTF8 oh, and this probably has to be tostring now is it happy? Maybe? Yeah. And then double quote for example is going to just pass through. Okay, fantastic. Implicitly returns nothing at his body has no tail expression. Right.
01:17:48.845 - 01:18:10.535, Speaker A: That's be. But there's a question mark right there. Right. And I don't have a. I need to do over here. Okay, so now we have a lexer that will at least recognize these tokens and produce them as we see it. And so now, I mean we can, we can try this out in our main.
01:18:10.535 - 01:19:06.655, Speaker A: Right. So down here we want to say let lexer is or mute Lexer is lexer colon colon new from file contents and then we want. What was the output structure that they wanted here? They wanted I they wanted the type and then the contents and Then what is this last thing? Why are they all null? Oh, it's interpreted value, so I'm guessing for strings.
