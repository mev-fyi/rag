00:00:00.200 - 00:00:32.678, Speaker A: Hi folks, welcome back to another stream. This time I kind of wanted to call this a crust of rust. I'm not sure whether it is. The boundary is a little ill defined. The basic idea here, the basic idea here was to do a stream where we take an arbitrary crate that happens to be the one that we implemented in the previous stream. But that shouldn't be too relevant to what we're going to do this time. And look at, ok, you've written the code like you have a thing that works.
00:00:32.678 - 00:01:26.994, Speaker A: What do you do next? This is the sort of polish part of a crate, but it's not just like surface polish, it's things like setting up CI. What kind of steps do you want in your CI? Setting up prop tests maybe? What do you put in your readme? How should you structure your docs? What kind of options do you have for your docs? How do you get coverage testing like that sort of stuff? So in some sense maybe you could call it hygiene, maybe polished, maybe some combination. And I'm aiming for somewhere around 2 hours here, sort of cross the rust length. We'll see what we get to. I don't really have an endpoint necessarily in mind as much as the goal is to just go through things that I do. When I have a crate like this one, where I have one commit, I have a readme that just is a link to a stream and then I have the source for the crate. I don't know why this loads slowly.
00:01:26.994 - 00:01:47.470, Speaker A: That's going to be pretty frustrating. Also, I don't know why I'm not signed into GitHub. That's also going to be frustrating. Let me go ahead and fix that right now. Oh, now I need my key. There we go. All right, let's try that again.
00:01:47.470 - 00:02:20.834, Speaker A: Source. Fantastic. Yeah, so we have a bunch of code here. We also have a little test suite that we wrote. It's not super advanced, but we have one. And the question is what do we do next as we go through here? If I do anything that doesn't really make sense, then just call me out on it and I'll try to dig into more. I've done this a few times, so it might be that I rushed through something and yeah, this is, this is going to be a shorter video in the sense of 2 hours.
00:02:20.834 - 00:02:45.910, Speaker A: It might be shorter actually. Well, we'll see how far we get. But things like if we really down dive down the rabbit hole of prop test, then like it's going to be longer. There's an argument here for maybe this should be a separate stream on prop testing. But I think it's a. It's a pattern, more so than teaching you how the prop test crate works. And so I'm hoping that it's not really suitable for just its own stream.
00:02:45.910 - 00:03:06.070, Speaker A: Great. I'll get into what prop test is in a second. Okay, so let's start with CI. So I have here a checkout of. I did rodstop update night. So I have a checkout of the crate here. The only diff I have is the patch that we made to cargo.
00:03:06.070 - 00:04:03.450, Speaker A: This one, actually, now can be a git patch instead because it's landed upstream, but that's not terribly important. Oh, actually, it means it won't build. All right, fine. We'll fix this up here real quick and say this is going to be no rustling cargo. The reason I want this to not be a path anymore is because otherwise CI wouldn't be able to run. And then I guess we can say we can sort of link to the pr that landed public closed. I just want to find the pr from last time, this one new project kind.
00:04:03.450 - 00:05:00.920, Speaker A: So that's the one that landed during our previous stream. So what I'll do is I'll say it's this one remove once cargo for rust 169 is released 2023.0 420, which is when that release lands, and then we can just do a cargo check to see that's actually true. Why do I have a config as well? Oh, I have two cargo config files. That's silly. Let's have just one of those. Okay, so that builds just fine.
00:05:00.920 - 00:05:36.950, Speaker A: Interesting. Skipping duplicate package. Interesting. There's a lot of warnings. Someone pointed out that for the cargo. Tommel, why is cargo zero six eight? So the versioning for the cargo crate is a little weird. It is the current rust version.
00:05:36.950 - 00:06:40.720, Speaker A: The current rust version, but zero point instead of one point and plus one. So for rust one x, the cargo version is zero dot x one. I don't know why. I guess we could keep this in here just for our own reference. And I guess, in fact, what we really should do then also is something like turn this into 70, because otherwise the patch won't get picked up. The alternative here, actually is to do which commit to this land in this landed in this commit. So if we want to be real specific here, which I think maybe we do, we say rev equals this one, and then this is going to be zero 70 because it lands in rust 69.
00:06:40.720 - 00:07:29.004, Speaker A: Cargo T. This skipping duplicate package is a little weird to me, but this feels, like, different from what this feels like a problem when you take a git dependency on cargo. Interesting. All right, let's see if this builds. So this is the first step to getting your, getting CI to run right, is to make sure that your package builds somewhere that's not on your own laptop. And in this case it wouldn't in the past, right, because I had a patch path override. And so this is just fixing that up.
00:07:29.004 - 00:08:09.660, Speaker A: And hopefully this should now build without issue. Yep. And all the warnings are about this skipping duplicate package bit, which we can ignore. I think this is because the cargo project includes a bunch of test crates and those seemingly confuse cargo itself here, which is weird. I would like this to not be the case, but anyway. Okay, so git commit, avoid path or cargo patch now available upstream on git. I can push that.
00:08:09.660 - 00:08:58.300, Speaker A: Okay, so what do we do next? How do we get this to actually run any kind of cir? Well, I have a collection of CI scripts that I normally use that I have just in a separate GitHub repo. It's not something I've documented very well, but it is just like, this is where I keep all of my GitHub CI config. And if GitHub decides not to be annoying today, great. So when you configure GitHub CI, you put all your config in a GitHub folder. You see here, I have configurations for dependabot. I have configuration for code cover, which is code coverage and workflows, which is all of the CI steps you see here. I have different workflows for different kinds of things that I may or may not want for a given crate.
00:08:58.300 - 00:09:38.724, Speaker A: So no STD, for example, is not going to be relevant to this particular crate. Safety is probably not going to be relevant for this particular crate. This is for things like if you want to run Miri and address sanitizer and thread sanitizer. So we won't dig into these two because we're not going to use them. But check scheduled and test YAML, we're all going to look through. And the reason I have this in a separate repo is because I can do this, get remote, add CI, get fetch CI. So this is basically adding a, adding another git remote to the current git repository, fetching that, and then I do git merge, allow unrelated CI main.
00:09:38.724 - 00:10:20.460, Speaker A: So this is going to do is it's going to merge that the history, the git history of that other repo with the history of this repo. And normally git disallows you from merging completely unrelated git histories, but there's nothing that prevents you from doing so in the, in the sort of git data model. So by doing this now you see, it created all of these GitHub files. And the reason this is nice is because in the future I can run git fetchci. If I make changes to that other repo I can do git fetchci and then I can just merge from here again and then I get normal, like git merge conflicts if necessary. It'll just handle any local changes for me. And so this makes it really easy to keep the CI up to date.
00:10:20.460 - 00:11:11.880, Speaker A: I could have some automation to check whether I need to do a merge from CI and stuff. That is actually an interesting CI step I could add to this, but we're not going to ignore that for now. And if we look at these files, let's start with Dependabot. So Dependabot is GitHub's tool for scanning your cargo toml and your cargo lock to look for essentially look for outdated dependencies or dependencies with known issues and automatically file prs to you when anything has changed. And you know, the default configuration for dependent bot I don't always love, but this one I've found works pretty well. So the first here is you see, you can opt into specific types of updates. In my case I'm opting into update notifications for GitHub actions.
00:11:11.880 - 00:11:59.378, Speaker A: You'll see this when we look at the actual scripts. But I'm using actions like there's a cargo action, for example, and those are all versioned. And so if my CI is using a version of an action that is itself outdated, then this will file a pr to update that action for me. And then I also have one for the cargo ecosystem. I set it to run daily, it runs at the root of the crate and I wanted to ignore all dependencies if the update is a patch or minor update. And the reasoning behind this is I don't actually, because this is a library. I don't actually care about updating my lock file, updating the minor version of my dependencies, because whatever I have in my lock file is not going to affect my consumers anyway.
00:11:59.378 - 00:13:12.906, Speaker A: They build with their own lock file that's independent of mine. All that matters is what's in my cargo Tamil. But if there's a major release then I want to know, because then I as a library author have to take action and update my crates cargo Tomml to move to the new major version. And then as it says here, if you have a binary then it does matter because your build is going to include exactly that set of dependencies, and so you don't want to ignore patch and minor versions, although then daily might be too much for you. So that's pretty neat. What else do we have? So Codecov, we'll talk about coverage a little bit when we look at it later, but this is really just configuring code configuration code coverage reporting so that it's a little bit less aggressive about false negatives so that we have a higher standard, rather than saying like zero to 100, we actually care that it's decently high. Ignoring the test directory because I don't want coverage for tests, and then making the code CoV comments the code CoV makes on prs and stuff less verbose and then we can look at the actual real jobs.
00:13:12.906 - 00:13:47.480, Speaker A: So if we go through here and look at let's look at check first. So you'll see here there's a configuration at the top. This is all like GitHub CI actions, CI configuration. So there's a bunch of stuff you can put in these files. Let's just walk through what's actually there. This is when does this workflow run? And I've said it should run on push to the main branch. The reason I say only the main branch here is because if I make a pr to my own repository, the way I do that is I create a new branch and then I do a pr from that branch into main.
00:13:47.480 - 00:14:52.388, Speaker A: But without this rule, when I push to my branch, it'll run CI on that branch, and then it'll also run CI as a result of opening the pull request. So you're doubling the amount of CI, which seems unnecessary since that I say when pushing domain or when creating a pull request is when I want this. The name of this workflow is check, and the jobs to check runs are format and clippy and doc. Oh, and hack. Forgot about hack and msRv, right? I keep forgetting that I keep adding to this. So format just runs cargo format check on the stable compiler, you'll see here that it checks out the code, it installs rust stable with the rust format component, runs cargo format check, and it does this through the actions Rscargo action, which configures cargo and sets up some caches and stuff. And I believe also ensures that the output of this goes back into the GitHub CI checks stuff.
00:14:52.388 - 00:15:29.624, Speaker A: So this is like it will actually show up in line as warnings in the file diff view of a printhead. So it's a little bit easier to spot later on. The other check I run is clippy and clippy I run both stable and beta. The reason I run formatting only on stable is because, I don't know, it seems more reasonable. It's the thing that people are going to have installed. I don't really want to hold them to the bar of beta for Clippy. I run stable for the same reason, and I run beta partially to help test new versions of Clippy, and partially because Clippy tends to have like a lot of new lints that come every beta.
00:15:29.624 - 00:16:09.910, Speaker A: And I like to get a little bit of a heads up if something is going to start failing in the future. I've also said fail fast here because usually if something fails in one, it's going to fail in the other. Steps here are the same it's a checkout install rust with Clippy, run Clippy and right, it's given a GitHub token here so that it can echo back to the GitHub check API. So this suggests that I'm actually not doing this for formatting, which I think is fine. The formatting is usually obvious. If it fails, you just need to run cargo format. Doc is run separately and is run on nightly.
00:16:09.910 - 00:17:27.640, Speaker A: The reason I run Doc is because there are a bunch of checks when you run cargo dock that don't run when you run any other command. So if you run, let's say, cargo check all targets, it doesn't actually do things like check that you're intra doc links are correct and not dangling to get that you actually need to run cargo dock. At least that used to be the case. I think it probably still is, and I run it on nightly because there are some really useful nightly documentation features that are commonly used on something like Docs Rs for example, where people will docsrs builds your documentation on nightly. But it also means that you can use some nice features like config doc or doc doc config, which means that you can mark a particular function or type or whatever in the render documentation as only being available if a certain configuration is true, for example, only if you're running on Linux, or only if you're in x 86. In order to be able to make use of those kinds of features, I run Doc on nightly. I run it without any depths because I don't want to spend time documenting dependencies when all I'm really doing is checking that the docs work.
00:17:27.640 - 00:18:07.860, Speaker A: And then cargo hack is this really neat tool for checking combinations of feature flags. So in cargo there's a requirement that your features are all additive. So that means your crate should compile with any combination of your features. The reason this requirement exists is because, because of something called feature unionization. So essentially if you have, you have some crate x, you have two dependencies, a and b. A enables one feature of X, b enables another feature of X. What cargo will do is only build X twice, sorry, only build X once with both of the features enabled.
00:18:07.860 - 00:18:41.628, Speaker A: It will not build it twice, once with each feature. And as a result, depending on what the dependency graph is, you might end up with any combination of feature flags. And so cargo just requires that your features are additive. And what hack does is it allows you to check a particular combination of features or a set of combinations of features. So in this case, I've told Cargo hack to use the power set of the features. That means every possible combination. I wanted to run check and I wanted to check libs and tests, and that works really well.
00:18:41.628 - 00:19:20.282, Speaker A: Sometimes you have to tune this if you have benchmarks or nightly only features and stuff. And hack has ways to specify this. Have you checked out dtlne rustoolchain? It doesn't really matter which one you use. I don't think it makes that much of a difference. And then the final step that I have is MSRV. So MSRV stands for minimum supported rust version. And this is essentially to figure out whether the crate continues to build with the version of rust that we claim, let's say in the readme that it builds with.
00:19:20.282 - 00:20:00.170, Speaker A: So in this case it just defaults to 156.1 because that is the version that released the 2021 edition, which is what all new crates default to when you run cargo new. So this just seemed like a good default to have in the sort of CI template here. And it really just does the same as the other steps here, which is it checks out, installs, and then runs cargo check against MSRV. It doesn't run the full test suite. Arguably it could, but this was more like you really just want to make sure that you still compile against that version. The test suite is somewhat less important when it comes to MSRV.
00:20:00.170 - 00:20:46.420, Speaker A: So those are all the check steps that I have in there. And then for safety we're going to remove this one because we don't actually have any unsafe code in this crate. So this is running Miri, running the leak sanitizer, the thread sanitizer, the address sanitizer, and all of that good business. It also runs loom, which again doesn't matter for our particular crate. So I'm just gonna remove safety. Remove safety. And no STD is similarly, it makes sure that your crate still builds against targets that have no standard library and no allocator, or targets that have an allocator but not a standard library.
00:20:46.420 - 00:21:33.170, Speaker A: Also is not relevant to us here, so we can remove it. The next thing is, let's look at scheduled. So the scheduled CI job runs in this case I think it runs every day. But scheduled jobs in GitHub are a little weird because they only run as long as the project is still active. But sort of conceptually it is more correct to think of these as rolling jobs because, and the rationale here is they might break independently of any given pr and that's the reason you want to run them on an ongoing basis. So the first example of this is building on nightly. New nightly releases come out every so often, specifically nightly.
00:21:33.170 - 00:22:07.238, Speaker A: And when they come out they might have break your crate, there might be a regression in nightly. And so as a result, you really want to run a build on nightly, nightly, or at least fairly regularly. And that's indeed what this does. It checks out, installs nightly, and then this thing is a little bit interesting. I'll get back to what this does when we talk about testing about why I have this bit here. But essentially it runs cargo test and it runs it with dash hash lock. So it runs it with the lock file that you have checked in.
00:22:07.238 - 00:22:49.910, Speaker A: And this little bit is just if you don't have a lock file checked in, it generates one for you. And this is to ensure that we are actually testing nightly and not updates to your dependencies. And then this one is update. So this is sort of the inverse. This is saying we're still, we're going to build on beta and we're going to run cargo update and then see the test still succeeds. Right? So the idea here being we want to make sure that if you run with a newer version of the dependencies, then your test suite still passes. The reason this one is important is because you're consuming the crates that consume you, the crates that depend on you.
00:22:49.910 - 00:23:19.172, Speaker A: They are going to have an independent lock file from yours. So they might use newer versions of dependencies than the ones you previously tested with. And so you want to regularly make sure that they don't get broken as a result of your dependencies getting updated. And that's what this will do. You see here. Also, I've specifically set this job to fail if there are any new deprecated items in my dependencies. Okay, so those are the scheduled jobs.
00:23:19.172 - 00:24:33.600, Speaker A: And then finally it's the running the actual test suite. Again, you see the on here is the same for all of these. So this one runs your test suite on stable and beta beta here is entirely just to help test out new versions of the compiler to catch regressions and such. It has the same little bit here, and now we can actually look at this in a little bit more detail. This is GitHub CI kind of magic to say if this file does not exist, then run cargo generate lock file the reason I have this bit in here is because I want to be able to merge in this template regardless of whether the current project, the project you're merging this into already has a lock file checked in or not, and this way it'll work either way. But I do think there's a strong argument for checking in your lock file and then this command will not run, and instead you're just going to run cargo test locked, which will test with your locked dependencies. The reason I think we should check in lock files is because it allows people to do things like bisect your crate, like they can check out an old version of your crate and it should still be able to build by virtue of using the exact dependencies that were present at the time.
00:24:33.600 - 00:25:37.470, Speaker A: But this way it's not required as a part of the CI template. And then minimal only runs on stable, but it happens to also install nightly because minimal versions is an unstable flag. And what it does is it runs the it runs cargo update with the Dash Z minimal versions flag. This is an unstable flag that causes cargo updates behavior to be sort of inverted. So rather than choosing the newest version that's available for your dependencies, it chooses the oldest version that's still subject to the dependency requirements that you set in your cargo toml and the idea here is sort of the same as for why you want to test with cargo update, which is you don't have control over what versions of your dependencies your consumers use. The only thing you know is that cargo, when running your consumers is going to pick some version that's compatible with what you put in your cargo toml. That's the only thing you know.
00:25:37.470 - 00:26:18.194, Speaker A: And you know cargo update is going to be a way to test that. If everything is the newest, it still works. Cargo lock or the stuff that's in cargo lock is going to check that whatever's in your lock file also still builds. And then this is going to be if some consumer only got those dependencies through you and hasn't updated in ages, they'll still be able to build this is basically to check that the specifications you have in your cargo toml file are accurate. So for example, let's look at our cargo toml here. Take temp file. We could specify template file as just three, right? Because it major version three, that's the API we built against.
00:26:18.194 - 00:27:27.020, Speaker A: But imagine that in our crate one of the things that we did is make use of, let's say the, I don't know, the foo function in temp file was added in version three dot two dot one. Well, if we give three that means that one of our dependencies might choose to use temp file three dot 0.0 because that's compatible with the specifier. But if they did, then our crate wouldn't build because we actually require three 2.1. So minimal versions is a way to try to tease out these kinds of problems to ensure that your consumers don't end up in weird states. And so this is why usually I recommend that if you take a dependency, you add the full version number, not just the current major version, to this, precisely to ensure that you don't or it's the real thing you should do, right, is look back at the version list and find exactly the minimal version that you need. In practice no one can be bothered to do that, but it's better than to pick the version that is the current one because you know that has everything you're going to use rather than trying to pick some like generic minimal because it's not clear that that'll actually work.
00:27:27.020 - 00:28:41.006, Speaker A: But minimal versions then is a way for you to ensure that the minimum requirements you've set in your cargo tunnel are actually appropriate. There is some work, there are some annoying parts about minimal versions because it chooses minimal versions transitively, which means if one of your dependencies has an incorrectly specified minimum versions, like imagine back here, imagine temp file takes three dot three dot zero, but then it depends on foo equals three, but uses things from foo, from foo, three dot two dot one. Then when we run our resolve with minimal versions, then the food dependency which we get transitively through temp file, we're going to choose three dot zero dot zero. That's what minimal versions tells us to. But temp file actually needs foo three 2.1 and so they have a bug in their cargo toml but as a result we suffer. And so that's why minimal versions is a little bit of a pain sometimes.
00:28:41.006 - 00:29:26.874, Speaker A: Like I definitely have some crates where I end up doing things like to make minimal versions happy. And then down here I said like foo equals three dot, two dot, one via temp file. And so this is one way to basically force that minimal requirement, because temp file hasn't specified it themselves. And it's really sad to have these. It changes the edges in your dependency graph, so cargo builds can't be quite as efficient. So it's really unfortunate that these are here, but usually it's not too bad to work around. But as a result, there's another proposal which is Z direct minimal versions, which chooses the minimal versions of your direct dependencies, but the highest version of all, transitive dependencies.
00:29:26.874 - 00:29:53.400, Speaker A: So it's sort of a combination of these two. And I think this is a good thing to test too, but it's not. I don't think it's going to be available on nightly until the nightly for 170 comes out, or rather on beta until that comes out. On nightly. It'll be available, I think, pretty soon because I think it's landed. So that might be an interesting thing to try, but I haven't played around with it yet. The next test step we run is OS check.
00:29:53.400 - 00:30:39.090, Speaker A: So this is just to make sure that the crate test suite also runs on macOS and windows using the stablecompiler. Nothing special here, just locked all features, all targets, and then we run coverage testing. There are a bunch of different ways to do coverage testing in rust these days. I tend to like cargo lVM Cov. It uses the new, I guess it's not that new anymore, but the instrumented coverage. So rather than using a sampling coverage checker like what we used to have before with something like KCUV, this instruments the code when you build it, and then runs the instrumented code which logs all the lines that are executed. And then you get much more accurate reporting because it's not sampling.
00:30:39.090 - 00:31:32.750, Speaker A: And cargo LVM CoV is a tool that makes it easier to build your code with instrumentation for coverage and then running and collecting that coverage. So this again checks out, installs, installs this helper binary, and then runs cargo LVM CoV, which essentially just runs your test suite with the other arguments, and then it outputs it in a way that codecov IO understands. And my choice here of Codecov IO is sort of arbitrary. It's one that I know I picked up years ago when I was looking initially at code coverage, and it's worked fine for me ever since. And it's free to use for open source projects. So it's just been very easy. Okay, so that means we now have we deleted no sudden safety and the rest of this in theory should just work.
00:31:32.750 - 00:32:44.612, Speaker A: So if we do add CI bits, actually, here's what I'm going to do. I'm going to create a new branch called CI and go back to Main and we're going to reset this to Oregon origin slash main, and I'm going to check CI and then I'm going to push CI to origin. This is just because I want to do a PR of this to see that the PR machinery works as well. So if we now go back here and open a pr from this one, add CI, and then I'll put the link there, create pull request, and just for those in chat here, I'll put that there in case anyone's curious. And so hopefully now you see this now brings in, because we did a merge with this other repo, it brings in all of the commits from that other repo as well. But that's fine. This is a one off thing.
00:32:44.612 - 00:33:22.430, Speaker A: And now you see it kicked off all these jobs and it'll be interesting to see whether these actually pass. Hopefully they should all pass, but we'll find out. I think the one that's most likely to, the two that are most likely to fail are minimal versions and minimum supported rust version, because neither of those we've checked. Also, who knows about macOS and windows? It's always a little bit of a coin toss, but I don't think we're doing anything particularly os dependent. And clippy, I don't know if we ran Clippy on this actually back in the day. Better to preserve history or do a squash merge. I preserve history for this merge.
00:33:22.430 - 00:33:57.480, Speaker A: And the reason for this is because in the future I'm going to want to merge from rusty Iconf again. And if I do a squash, then I'm going to have to deal with a lot more merge conflicts because git doesn't know about the connection, the existing connections, and merge history with that other repo. So this, I specifically want to be a merge. Let's see. Nightly dock works fine. Cargo update works fine. I forget whether we checked in our cargo lock.
00:33:57.480 - 00:34:49.039, Speaker A: That's another good question here. We did not. So that's the other thing that I'll want to do here, is git add cargo lock bin Gitignore. I don't want to ignore cargo lock and I want to add my cargo lock check in. So that's the next thing I'm gonna want to do. Something else failed here. Okay, so minimum supported rust version failed, and that's because that's interesting.
00:34:49.039 - 00:35:53.832, Speaker A: Crates index zero, one nine. Is there something weird about crates index zero one nine that it doesn't work on 156 fascinating. Crits index if we go to the repo for this cargo tomlike ah, so it has a rust version zero 60. But this error is very unhelpful. So what we'll do then is we'll go to our GitHub workflows. Check. Oops, I mean tests.
00:35:53.832 - 00:36:32.570, Speaker A: No, I mean check MSRV. And this is going to be one dot. I guess really what I want to do here is something like show me 0190. Yeah, so it too has rust version 160. So what we're going to do here is 160.0 for crates index bump and this rv for creates index. See what else might fail.
00:36:32.570 - 00:37:28.904, Speaker A: I like to just deal with I'm committing each of these, but I like to do all of them before pushing so that we avoid having to wait for CI each time. Minimal versions it's not terribly surprising. No version new found for version. So this suggests that OpenSSL SYS has an incorrect specification of its dependency on rust c version. So if we go to can type OpensSL sys, the reason I go via docsrs is just a really easy way to get to the GitHub repo. So let's see if we find OpensSL sys. It's cargo toml.
00:37:28.904 - 00:38:15.610, Speaker A: It has a dependency on build main on rust C version. Is that still true? That's the other question. If we look at build main, you know what I think? Yeah. I think that OpensSL sys has moved away from using that dependency at all. Now it uses auto config, but we're using an older version of OpenSSL Sy's as well. So this suggests that this was a bug in an old version of OpensSL Sy's. OpensSL is particularly annoying when it comes to this stuff.
00:38:15.610 - 00:39:00.170, Speaker A: So what we can do here is cargo tree dash I openssl sys and look at what we find. Okay, so we get an OpensSL sys through a bunch of different things. We get it in through curl, we get it in through curl sys and we get it in through OpensSL. So there are a couple of ways that we can try to solve this problem. My first instinct here is actually just a bump C minimal versions. Again, this is where it's annoying to use minimal versions because this isn't the problem in our specification. It's a transitive problem of minimal versions.
00:39:00.170 - 00:39:36.980, Speaker A: But what we can do is just pin a much newer version of OpenSSL. So currently, you see, we're using OpenSSL version 00:10 45 if we go to this. Nope, that's not what I wanted. OpensSL is currently what is currently 1045. So we're using the newest version of OpenSSL, interestingly enough. Oh, right, I haven't, never mind, that's false. I need to do cargo plus nightly update C minimal versions.
00:39:36.980 - 00:40:33.400, Speaker A: And now we can do cargo tree Opensslsys, see what we get. Okay, so we're bringing in OpenSSL version 00:100 so that's pretty old. So let's try to do a sort of binary search here. So if I do 20 and then I do cargo update and then I run cargo check, does it build? That builds. Okay, so what if I do ten binary search, cargo check that builds. That's interesting. Oh, it's because I'm not using all targets, I think.
00:40:33.400 - 00:41:22.816, Speaker A: Yeah, so this breaks for a number of reasons, but let's do so 45 here. Update minimal versions. Go check all targets, because we fail somewhere else too, which is Rand Isaac. So why are we getting in cargo tree? Rand Isaac brings in through, brought in through rand 0.6.0. So let's see, what's the newest version of Rand? Well, zero eight five. Okay, so there's a much newer version of Rand. So where did we get that from? We got that in through cross Beam channel, which comes in through cargo parking lot.
00:41:22.816 - 00:42:07.110, Speaker A: Alright, so let's just see what versions of zero six there are. Oh, these are all yanked. Zero six five. Okay, zero six five, check all targets that still fails. So then let's look at random and Isaac, and again, this is why people can't be bothered to use minimal versions because it's actually pretty annoying. Zero one one. Sarand Isaac zero one one oops.
00:42:07.110 - 00:42:48.770, Speaker A: Update minimal versions. Great. Okay, so then we might not need the specification for Rand here. The nice thing is, once you've done this once, then it sort of fixes itself. Okay, libgito can't find range version, so the range version here is method on. It's a method on package configured pro version. Range version.
00:42:48.770 - 00:43:33.160, Speaker A: Okay, package config oops. So we're bringing in package config version 0311. And so this was clearly added in some later version than 0311. So let's do cargo. Toml didn't need that, but package config is 0.36. Why not? Oops again. Let's see.
00:43:33.160 - 00:44:13.004, Speaker A: Cargo check. I think maybe we did need rando six four here. Now it's elliptic curve. Why are we even getting that in? JK df through P 384 through proceed. Oh, through cargo. Oh, man. All right, what do we have here, docs? HK D.
00:44:13.004 - 00:44:35.390, Speaker A: This is actually one of the worst I've seen. It's because cargo brings in a lot of dependencies to 00:12 three. For this one, HKDF is 0.123. This is another thing where I. Someone should write a tool for this because it's very mechanical. Yeah, this is insanely annoying. I agree.
00:44:35.390 - 00:45:07.710, Speaker A: Great. Oh, you're right. It was zero six five. And I think demonstrating how annoying this is is useful. Okay, so P 384 has a problem, too. P 384. Wait, what? 00:11 so what's a newer version of 00:11 0112.
00:45:07.710 - 00:45:50.834, Speaker A: Now, note that the part of the problem here is it seems like some of these have actually been broken by new versions of Rust. Maybe. Either that or their dependency specifiers are, again, just straight up wrong. Okay, Rand AC is 0.1. That one doesn't have a new version. Okay, so Rand AC gets brought in through rand zero six five. Was there a newer version of Rand on the zero six branch? There's not.
00:45:50.834 - 00:46:22.890, Speaker A: Okay. And I think it's going to be. Oh, so the. The other thing we can do here is actually just bump. Do we take a dependency on Rand specifically? Yeah, just that one. So we're getting rand through parking lot, through crossbeam channel, actually. So what we can do is instead of trying to do this, get ran to a newer, major version, which we get by getting a newer version of crossbeam channel.
00:46:22.890 - 00:47:04.896, Speaker A: So cross beam channel. So crossbeam channel is zero five now, but let's say zero three nine, instead of zero three six, we do cross beam channel. Zero three six. No, zero three nine. Cross, cross bean. Let's see. So that got rid of the old version of parking lot, which is what brought in the old version of Rand.
00:47:04.896 - 00:47:28.000, Speaker A: And then we also make sure that we don't accidentally bring it in again. So now you see the old version of Randis is completely gone, which is what we wanted. Now let's see if it builds. This looks pretty promising. Never cross the beans. Okay, so that builds. So we can now get rid of this.
00:47:28.000 - 00:47:56.542, Speaker A: Let's see if we can also get rid of that, for example. Nope, that's still needed. Then. You know, we can do a bit of a binary search here if we actually want to be a little bit nicer about this, because 00:11 was the one that failed and 00:26 works. So we can just sort of dig our way through here. Okay, that seemed to work. So eleven worked.
00:47:56.542 - 00:48:51.874, Speaker A: So let's do 16 and see. Great. So that works. I don't want to spend too much time fiddling with these. Openssl is the same, like we could do somewhere halfway between zero and that one that fails. So 30, this is just so that we don't have to go all the way through the binary search, but we can at least go a little bit further, right? So 38, great. So that seems like it builds package config.
00:48:51.874 - 00:49:50.450, Speaker A: I think these we can just leave and then we want to get checkout, cargo dot lock and then we want to cargo check just to have it reflect the changes we made here. None, no changes were needed because everything was in our cargo lock was at least these versions anyway. Fixes for badminimal versions. You see, I've written that comment before, so now we can get rid of all these and this and then we can go back here. What else? Failed, minimal versions we dealt with coverage failed and MSRV failed, which is fine, we can do with that. Ah, could not find a repository. So this is for code covenous.
00:49:50.450 - 00:50:45.172, Speaker A: You need to, before you can start getting CI, you need to like go to login with GitHub. This should also always open in dev. And so now I can do this, at least in theory. Really, why? Interesting. There we go. Okay, so now it's been created, that was all that was really needed. And now that step should work as well.
00:50:45.172 - 00:51:29.940, Speaker A: So now we can do a git push and then we see now I haven't added any actual approval rules here yet, which is the next thing we're going to do. This is stuff like what do we actually require before being able to merge something. Let's see here. Okay, and then the other thing I want to do is make sure that all of the workflows actually ran. So if we go back to check here, rolling and test, because sometimes there's like a syntax error in your YAML or something, and then it basically gets hidden from this list. And you have to go in here and see that it's actually running. But that seems like it is.
00:51:29.940 - 00:52:23.150, Speaker A: Nightly now fails. Why does nightly fail on nightly? The lock file needs to be updated. Locked. Oh, interesting. Ah, alright, new lock file. So this is, I guess I didn't rerun cargo check after I checked it out and updated these dependencies. So the reason that the diffs here are things like we now have multiple versions of FF, for example, being brought in as a result of the minimal versions that we specified.
00:52:23.150 - 00:53:10.410, Speaker A: Did I leak a secret? Is there a secret in here? That's fine, I don't care if you want to take over my code cover for this one. That's fine. I think I can also just erase that after the stream. But thanks for the call out. All right, so now at least, hopefully there were three rockets and now there are only two. Makes me sad it's not a rocket anymore. Let's see if everything goes to plan and these should now all turn green.
00:53:10.410 - 00:53:48.350, Speaker A: I wish they would just like all run in parallel and all run fast. But I suppose GitHub is like donating CI for free for open source things. Yeah, I know, it's, it's crazy that they just show the secret without a button to reveal. Come on, CI details. Rolling. No, don't auto scroll. They're all running.
00:53:48.350 - 00:54:21.080, Speaker A: Test isn't even running yet. Can I cancel that run? I want to cancel that. That's the execution from the previous push. Same with this, I don't actually care about that one. And same with this, I don't actually care about that one. Because for open source projects, GitHub CI limits the number of jobs that can run in parallel. And so that's why some of these haven't even started yet.
00:54:21.080 - 00:55:10.028, Speaker A: And so by canceling the runs for this push, so you see some of them have now been canceled, I'm freeing up more resources to run this one faster or more in parallel. Hey, now there are three rockets again. Nice. And Windows takes forever to start because they presumably have a limited number of Windows hosts. As a software engineer, doesn't this work feel like a waste of time due to the minimal benefit versus time used? I mean, if it's your free time, it's your choice, but you might get paid for your time. When you say this work, what do you mean? Do you mean doing streams or do you mean like the minimal version stuff? If it's doing streams, I have all sorts of good answers. If it's minimal versions, yeah, it's a waste of time.
00:55:10.028 - 00:55:49.412, Speaker A: We should have tooling that does this for us. And I would like dependencies to fix the problem. Now, is it a waste of time in the sort of grander scheme of things? Unclear. Like, I do think it's actually valuable to have a CI check the to test your minimal versions. And I think it can save you a lot of time down the road when your consumers come to you and say, hey, my thing doesn't build with the dependencies you specified in your cargo toml and especially for things like I tried to build and it failed. And it fails in really weird ways if they're using a dependency that isn't new enough. So this is a way for you to catch it before all of your consumers catch it.
00:55:49.412 - 00:56:17.408, Speaker A: And it's better to catch it in one place than many places. So I don't think it's wasted to have the correct minimal versions and check for it. I think the work required to get to that point is a lot of waste and we should really build tooling for it. Yeah. So that's why if it's for the minimal work stuff, that's the reason why I think it's valuable. Clippy seems like it passed, so that's nice. Oh yeah.
00:56:17.408 - 00:56:50.840, Speaker A: I think we did a pass on that in the previous stream. MSRV succeeded, so that's good. Cargo hack succeeded. Coverage we don't know yet. MacOS we don't know yet. Minimal versions we don't know yet. Tests passed on beta, so probably pass unstable updated passed last time is unlikely it will fail this time.
00:56:50.840 - 00:57:34.228, Speaker A: Would be interested in hearing the answer for streaming as well. Yeah, why would we wait for CI? So the reason why I think streamings or streams like this are not a waste of time is partially because I try to build things that are useful. If I stream something where I just build something completely throwaway, then it's less clear that or that part of the value sort of goes away. But I do do those streams too. Like if you take the wordle stream for example, it doesn't matter, it's not a thing that needed to be built. It was just fun to build. But then I think the other value with streams is that at least my hope is that they're educational as a part of going through this.
00:57:34.228 - 00:58:10.280, Speaker A: Even just the stuff that we've done so far. Hopefully there was a bunch of stuff there that some subset of the audience for each thing haven't seen before, haven't been exposed to, haven't thought of. And so, you know, it's a, it's a way to try to teach many things all at once by showing the real development process. And I think that's hugely valuable. I think it has a long term impact. It has a broad sort of fan out in terms of who it teaches what, and I think it teaches things that are otherwise hard to pick up. Like it takes a long time to pick up a lot of these small nuances, things to think of in CI.
00:58:10.280 - 00:59:03.260, Speaker A: So at least my hope here is that these streams are actually have a pretty meaningful impact and are definitely not a waste of time. Me spending two or 3 hours for a shortish stream, and then as a result, having, you know, what is currently 189 people and possibly more when they watch afterwards spend the same amount of time, but hopefully they are learning new stuff as part of that time. Seems to me like a pretty reasonable trade off. Like my time is valuable, but it's not as valuable as 200 other people's same amount of time. Windows failed. That's. I hate windows so much.
00:59:03.260 - 01:00:50.000, Speaker A: Okay, windows failed because ah, I've seen this before. Where did I fix this? Repositories sort by last updated flurry word search imap inferno GitHub this one doesn't even have GitHub yet. Yeah, found it. So on Windows you have to do this little thing and really I should just add this to my standard setup, but essentially under steps you have to do this, which is really, if you're on Windows then you need to specifically install openssl. Install openssl for Windows. Now for many projects this doesn't matter because you just aren't taking a dependency on OpenSSL in the library. But when you do, you run into this one and macOS I think is probably going to pass.
01:00:50.000 - 01:01:22.992, Speaker A: The reason I don't want to push this yet is when I push it's going to restart all the CI. So it's a little tempting actually to merge this and then deal with it later. Let's see. Oh, we got coverage too. I think so in theory there should be a comment coming in from code cov here anytime soon. Let's look what it says. Does it have coverage in for now? Right, so it only has for the CI branch yet because we haven't merged and coverage here is about 68%.
01:01:22.992 - 01:01:53.390, Speaker A: So that's not great. So we have terrible testing of index. What's in index? That doesn't seem right. Oh, it's because we never have a test that tests features or dependencies. That's what it's telling us. And that's totally right. Like that's a thing that we really should have a test for.
01:01:53.390 - 01:02:29.310, Speaker A: Mac Os passed. Great. So now what I'll do is I will git push and then when this kicks off, assuming the YAML is still valid. So this is where I'm just going to go here, go back test. Yep. So it picked up all the jobs and then we're just going to go here and merge. Yeah, finally that in confirm merge.
01:02:29.310 - 01:03:10.120, Speaker A: So now delete branch. Boom, we have CI. And then the question becomes, okay, what do we put in the readme? Because readmes are hard. I think the last place I did this was here. Yeah, look at the readme here. I tend to reuse this a lot, which is here. Main polonmain and then for all of this, it is replace furry with crates index transit.
01:03:10.120 - 01:04:08.170, Speaker A: That's what we called it right now, cargo index. Really? It should maybe be crates indexed transit, now that I think about it. But, but fine. And it's the main branch, not the master branch badges. So this is a crates IO badge, you know, which you want to show what version docs link, which goes to docs rs dependency list, which is this thing that just shows what your specifiers are and whether they may be problematic. My guess is we're going to get prs from dependabot here pretty soon about, well, cargo is going to be a little weird. I don't know whether we get a pr for that, but for crossbeam channel and for P 384.
01:04:08.170 - 01:05:08.286, Speaker A: Unfortunately though, we can't actually update these because they're there for minimal versions. But we'll see whether it comes in and then we can ignore them and then code CoV should upload once the code cover step of this actually completes. All right, so that means we now have CI, we have the badges and the readme. Where do we go next? Well, there are a couple of paths we can go. One is on writing documentation, which I kind of don't want to spend a lot of time on writing documentation here, even though it's valuable, but it's also kind of straightforward. So I think what I want to do next is look at improving the test suite. So for testing, there are a bunch of different ways to write tests, and it's not as though you should only do it in one way, but one way to write tests is something called property based testing.
01:05:08.286 - 01:05:44.584, Speaker A: And there are a couple of tools that you have in rust for doing them. Two of the main contenders are prop test and quick check. Quick check is made by burnt sushi by Andrew Gallant, and it's really good. I think the recommendation these days is to generally favor prop test. The reason for that is because quick check is it has a little bit of a simpler model of how to do property based testing. It's faster as a result of having a simpler model, but it's not, not clear that it's better along meaningful axes. If you look at the.
01:05:44.584 - 01:06:02.230, Speaker A: Sorry. Let me see if I can make this be dark. That's interesting. Why don't they have the little. There's usually a little icon. No, I don't want to print this book. There we go.
01:06:02.230 - 01:06:27.650, Speaker A: Rust coal, navy iu. Iu. Looks nice. So they have an entry here on prop test versus quick check. And I think, you know, there's a you can read this on your own time if you want to. I'll put it in chat too. But basically they're saying prop test is, has a more elaborate way of doing property based testing.
01:06:27.650 - 01:07:28.120, Speaker A: But the real value in quick check is that it's faster because like generating the complex values in prop test can be much, much slower. But we're going to start with prop test here, and I won't give you a full introduction to what property testing is or all of the nuances of it, but the basic premise here is rather than writing a, rather than writing individual test cases, you write a, you essentially write the pattern for test cases. So if we dive into an example, you know, here they have a parse date function. So it takes a string, gives you back the components of the string, of the date in the string. And you know, normally you might write a test like this. We just parse individual dates and you see whether or not they return something reasonable. Unfortunately though, this means that you have to sort of hand code a list of dates.
01:07:28.120 - 01:08:08.830, Speaker A: You also have to come up with the patterns. And if there are patterns you didn't think of, you're not going to be testing them. The other way to do this is with a prop test or property based testing where you say, for example here for any s, that in this case is a regular expression. For any string that matches this regular expression, call parse state and don't even check its result. We just want to check that it doesn't panic. So here what this is going to do is it's essentially going to generate a bunch of different strings that match this particular pattern, run it through parse state, see that it doesn't panic. And then if it does find a panic, it'll tell you what input it gave and what actually happened.
01:08:08.830 - 01:08:55.766, Speaker A: You see here it ran 102, it ran that test 102 times for different inputs before I got to a panic. The other thing you can do is you can do this. So for example, here they say for I want all the s's, so all the strings that match this pattern. So these are things that should be valid dates, or at least should be parsable as dates. And here you want to check that the result is actually some, it is like fuzz testing, like property based testing is in some sense a structured form of fuss testing. And you're right, it's also not deterministic. Now they have some mechanisms for trying to make it a little bit more deterministic, but it is exploring an infinite space.
01:08:55.766 - 01:09:37.485, Speaker A: Or in this case it's not quite infinite, but it is very large and so realistically it has bounds on how long it keeps running for. And you can configure that. But the idea here is that just straight up fuss testing we actually give arbitrary inputs is a little bit less useful. And instead you want to be able to express constraints on the inputs to more efficiently explore the space or more intelligently explore the space. But it really is fast testing. It's a sort of subcategory of fuss testing. And then they have some, I'm not going to dive into all the details, but they have some mechanisms that allow you to save the state of past runs to ensure that you hit the same ones if you run the test suite.
01:09:37.485 - 01:10:03.990, Speaker A: Again, there's a bunch of that kind of mechanism in here, but. So in this case. So, yeah, so running prop testing can take a long time, just like fuzzing. And in practice, what you do is you limit the number of steps that it actually runs. I think what we would do is set prop testing up to run as a separate CI job, but, but it is slower. Like it does take longer, there's no doubt about that. But it also much, it tests your program in a much, much better way.
01:10:03.990 - 01:10:54.722, Speaker A: And the question becomes, you know, how do you generate these inputs? And if you generate them from relatively small domains, then your testing is going to be, you might actually be able to fully explore the space and then it allows you to do things like actual ranges. So instead of just saying, you know, any arbitrary date or any arbitrary string, we're actually going to generate a bunch of valid potential inputs. And again, it doesn't fully generate all of these. It tries to semi intelligently explore the space here, and then we can do whatever we want inside to test that things work correctly. And again, it's going to run a bunch of inputs and tell you what input it might fail for. I don't think I would use this to test performance. I mean, you could, but it's okay.
01:10:54.722 - 01:11:48.430, Speaker A: So the way you could use this to test performance is it sort of randomly explores the input space. And as a result, performance benchmarks often end up weird because you use the same input over and over in a loop and this is going to force you to handle multiple different inputs. The challenge is even just generating the inputs takes a bunch of cycles. This is one of the reasons why fastest thing can be slow. And prop testing in particular is because you have to generate inputs that actually match the constraints, which is not true for fast testing, which is just purely random. And as a result, if you try to do benchmarking based on this, it gets a little weird because you're probably going to benchmark the prop testing suite just as much or the harness just as much as you're going to test the actual online code you see here. For this case, it generates a bunch of things that match this pattern.
01:11:48.430 - 01:12:52.842, Speaker A: And you see it's not just a straight list, it doesn't do a linear scan. It tries to be more intelligent than that and it ultimately refines down to this test case fails. And there's a bunch more in here. We're not going to get to all of it, but I wanted to see if we could get some, some kind of basic prop test setup for this. So let's look at the tests that we currently have. So we have this roundtrip function, which is it constructs a cargo project for you. It expects that you're going to make some modifications to the cargo toml then it packages that up and then runs it through all of the different steps where it converts, it sees that it's convertible into a normalized manifest, see that it's convertible into a crate version that can be published, that it can be turned into things that are compatible with these other crates.
01:12:52.842 - 01:13:19.330, Speaker A: And ultimately you can create an index entry. So it just does this full round trip. And currently our round trips are super simple. This one doesn't modify the tree that you get from cargo new at all. This one we didn't even implement. But the idea here was to add dependencies and I think we could actually do this with prop tests. And I'll show you in a second how to do it.
01:13:19.330 - 01:14:02.280, Speaker A: So if we wanted to add prop test to this, the first thing we're gonna have to do is where is the. They're missing a crucial step up here, which is adding the dependency. Ah, there we go. So as per our previous discussion here, they claim that you want to do one dot 0.0. I don't know if I believe them. I'm going to go ahead and make that one dot zero. And now if we go back here, we use the prop desk prelude.
01:14:02.280 - 01:15:41.900, Speaker A: Now I wonder here whether we even want this to be in the same file. Here's what I'm gonna do. I'm gonna make this pub. You're gonna hate this, if I remember correctly here. Oh, why that's interesting. I think I need to go ahead and do this. I might just be confused.
01:15:41.900 - 01:16:49.940, Speaker A: Interesting. Check out cargo lock, huh? I don't know why it's now complaining about these. I guess it's because we added prop test, but I'm not. This isn't running with minimal versions. That shouldn't make a difference. In fact it doesn't even build with the latest versions of everything found. Create bytes compiled by an incompatible version of rust compiled by rusty beta.
01:16:49.940 - 01:17:32.870, Speaker A: Interesting. Oh, my environment variable is no longer being set, that's why. So it feels about that. Now I specifically didn't want to run cargo clean because I have a kind of weird target directory setup, although apparently it wasn't using it anyway, so it wouldn't have made a difference. So that looks like it builds. Check out cargo lock. Cargo check.
01:17:32.870 - 01:18:23.770, Speaker A: That's very strange. Looks like we got into some weird fingerprinting issue here. Alright, well that seems to now be working and with. All right, that builds. Now the reason I did this is because I want via cargo to just be a standard integration tests file where we have like just imperative tests. And then I want the prop tests to be separate so that we can choose whether or not to run them. We might even ignore this whole file.
01:18:23.770 - 01:19:17.860, Speaker A: Now the challenge is this round trip function, I actually want to be usable in both of these is what I do is I make the via cargo thing a module that I then use something from. Actually this might not work the way I want it to because it might end up running those tests twice. There are other ways to do this, but the auto detection the cargo does for test suites is actually a little bit annoying. Here, you can turn it off, but let's see, we get out of here. So many dependencies. Oh, you know why? I think I know why. It's because, no, not that one.
01:19:17.860 - 01:20:49.420, Speaker A: It's because I changed my cargo config to include this and I think that caused it to get confused when trying to reuse old artifacts. So I think cargo test is actually now going to run the unit test and via cargo twice. Yeah, see it runs it via here as well. All right, fine. So what we'll do then is we'll make a subdirectory called something like utilization. And then we will copy tes via to test util round trip mod rs and then we remove the unit test from here and then we go back to via cargo. We say modest util and we use util on trip and then we can get rid of all these.
01:20:49.420 - 01:21:29.150, Speaker A: And then we do the same in prop tests. And now let's see what we get. Why does it complain this is dead code? Oh, because it thinks this is a main. That's fine, we'll do allow code. Okay, so back to prop test. So now that we have this, and it only runs our test once. And the warnings are all from prop test.
01:21:29.150 - 01:22:41.300, Speaker A: Okay, so you see how the, it prints the output from cargo as a part of this test. So that's another thing that we're gonna want to fix here. And the fix for that is actually a little bit annoying. It's probably here that what we want is. So when we invoke cargo as a library to do things like builds, what it's going to do is it's going to print to standard out and STD error, but it does so in a way where it prints directly rather than through the print line machinery, which means that it doesn't get captured by the mechanism that rust uses to capture test output. I thought there was a way to do this. So if we look at the docs for cargo and we find shell in here, shell is the thing that it uses to sort of emulate the output location.
01:22:41.300 - 01:23:36.320, Speaker A: But what we want here is to probably do it from right, I suppose so if we do from right. I forget whether. Hmm. This makes me wonder actually what the rust test suite uses from that. For this? No, from right. Okay. It just uses a box new vec.
01:23:36.320 - 01:24:09.940, Speaker A: I see. It uses a. Yeah, I guess we can just use a file here instead. So what this is effectively going to do is just drop all output, which might be annoying. We might actually want that output, but I think in this case it's rare that we'll want the output from cargo itself. That's fine. All right, so back to round trip.
01:24:09.940 - 01:25:29.766, Speaker A: So what we want here, we can just start from one of the examples that they give here. And if we go back to via cargo, we can actually steal this whole test. So we want here is, you know, this is the modify the workspace before packaging. And this is check the final, check the output structs, or check the various structs, the various transit structs. Great. So the question here is, what are we actually prop testing over for this? And this becomes a question of, well, what are we actually interested in varying? It could be something like features. Right.
01:25:29.766 - 01:26:02.342, Speaker A: We want to make sure that as you go through all of the steps, regardless of what the feature is named, it'll just get passed through appropriately. It doesn't seem all that interesting to prop test. If we go back to dot create, look at what we have here. So these are sort of the kind of things that we might expect to do prop testing over. Right. Or the fields that we know are going to be read in by our thing. And so this is like stuff that might be in cargo toml.
01:26:02.342 - 01:26:42.726, Speaker A: So it's the name of the package. We could prop test over the name of the package. It seems unlikely that the contents of the name matters. It's just an arbitrary string that's passed through, through. It's not really parsed. The dependency specification is though, right? So here we could do things like we want to make sure that when dependencies are added, the appropriate, when dependencies are added, the appropriate like version specifier, optionality and stuff ends up being passed through. So that seems like a reasonable thing to do.
01:26:42.726 - 01:28:15.880, Speaker A: And we also want to prop test over whether it's dependencies, debit dependencies, or build dependencies. So I think that's actually the thing we're going to prop test over here is this is going to take a, probably a vector of dependencies, I think is what we want. So let me go back to the strategy here. So you can do things like where's the prop compose here? No generating, filtering, generating enums, where's the prop for getting a vector? Higher order strategies. There we go. So here the kind of inputs that we want for this is steps. But really I want to see if we can do this in a better way because, all right, this might work actually, because what we want here is a function that generates dependencies.
01:28:15.880 - 01:29:33.770, Speaker A: And so it's going to generate a, I guess it'll actually just generate a string like a vector of string, where that string is really a toml specifier. Right. And so what we want is some, and we could specify that just by regular expression, but it feels more appropriate to generate the structured representation and then turning it into a string. Now what I'm curious about is the right way. Aha. Canonical, any, right. So I think what we want here is something like struct dependency, and a dependency can be enum, kind can be normal, build or dev an appendency has a kind, it has a name, it has a version and it has a features.
01:29:33.770 - 01:30:39.140, Speaker A: Is there anything else we have? It has an optionality, I guess we can actually go back and look here what else gets pulled in. It has an index field, which is also arguably something we want to include here. Public is still nightly default features. Arguably. This is just, we want to generate arbitrary for this, we want to be able to generate an arbitrary dependency. So in fact I think we can just do that. So maybe that's the thing we're going to do instead is to have a way to generate a normalized manifest.
01:30:39.140 - 01:31:27.540, Speaker A: I think that's actually what we want. So, right, so there's a trait called arbitrary. And arbitrary is also used in quick check. And it's essentially a way to say it's a trait version of saying generate me an arbitrary version of the struct. And we can use the prop just arrive, it's experimental, but it seems pretty reasonable for us to start with this. So what we would do is go over to, but we can't use that in a unit test. Right, right.
01:31:27.540 - 01:32:38.156, Speaker A: So the temptation here is to do derive arbitrary. The problem is if we derive arbitrary, then now we have to take a dev dependency, no, a normal dependency on arbitrary for this crate, which doesn't really feel right. The other thing we could do is we could do a configured fig attest derive arbitrary. But this doesn't work either, because test is only set for the library crate when the library is being unit tested. When you're running integration tests like prop test, then the library will be compiled not in test. And so this won't work. So instead what I think is we do actually have to replicate this, which isn't the end of the world, but it suggests that maybe instead of this we should just do, we should just do this.
01:32:38.156 - 01:33:57.530, Speaker A: We should say struct dependency should just hold a cargo index transit dot crate normalized manifest. So really what we want here is a, we want to say that a dependency is a combination of kind and normalized manifest of string is fine. Oh, I forgot we had all these things. Oh no, actually we want a dependency. We can do this. And I guess, actually I think we have a definition of dependency somewhere too. We have dot create and I think it's an index where we have, this is not an index, it's in publish maybe then.
01:33:57.530 - 01:35:04.030, Speaker A: Yeah, and publish dependency kind. There we go. And then what we want to do is we want to implement arbitrary for dependency. Now implementing arbitrary is a little bit annoying. Oh, do they have a helper for this? Right, so this is the kind of stuff that we want, right? We want an arbitrary, maybe we don't need arbitrary here, maybe we want ARb Dep, right? That's what they called this. So something like arb dep. And it implements a strategy for generating a dependency.
01:35:04.030 - 01:36:12.780, Speaker A: Oops. And what we wanted to generate is it's not quite going to be that, because it's going to be we want a compose. So this is for generating recursive things which we don't actually need. But I do think what we need here is where's the compound strategies? No, prop compose. That sounds like what we want. Yes. So prop compose.
01:36:12.780 - 01:37:37.250, Speaker A: So an arbitrary depth kind is going to be any in kind in zero u eight to three and it generates a published dependency kind. So this is just how do you generate an arbitrary dependency kind? Zero is going to be publish. Arguably we could just replace this now with this. That's going to be a dependent dependency kind. Dev one and two, so normal build and dev and then this is going to panic. So now we have a way to generate an arbitrary dependency kind and then we're going to do the same for arbitrary dependency, I guess listing, which is to generate the second part. So that's going to generate a dot create dependency string.
01:37:37.250 - 01:38:29.150, Speaker A: And we'll figure that out in a second. And it's not going to take a kind, but it's going to take some other stuff. And then we're going to say that to compose a arbitrary depth, which is ultimately what we get to, we want kind in where they write this. Yeah, you just call the function. So Arb dep kind and listing. In Arb dep listing that's going to give you back a dependency. And this is going to be dependency of kind and listing.
01:38:29.150 - 01:39:57.454, Speaker A: And we can derive debug here. That seems fine. Arb deposits find multiple times. No, not anymore. And we'll still have to figure out exactly how to write this depth thing that generates a vector of these, but that's fine. And to generate a listing, what do we actually need to generate a dot create dependency, fill the struct fields. Okay, here we're going to take version, which is going to be the version specifier is a regular expression and it's something like, I guess really here we're trying to have a regex for semantic versioning specifiers.
01:39:57.454 - 01:41:24.520, Speaker A: Right. So we'll do, it can be a caret or an equals, but that's optional. Then it's followed by a zero to nine, any number of, but at least one. And then it's followed by a actual dot followed by zero to nine of at least one, followed by potentially like build specifiers and stuff, which I suppose we could include. So there's a dash followed by I think arbitrary ASCII characters, but I think it is limited to ASCII. So that's an optional one. And then same with build meta information.
01:41:24.520 - 01:42:23.910, Speaker A: So this is like if there's a pre release specifier, this is if there's a build meta. And maybe this is like too extreme, but we'll start there. So that's the version. The name is just any string really. And maybe we're gonna regret making it any string because it's not actually right. It's it is a to z, zero to nine, dash and underscore. I don't think capital letters are allowed, and I think it has to start with a letter, but it's allowed to be a single character.
01:42:23.910 - 01:43:16.770, Speaker A: Okay, so this is going to be the name. Ooh, no, it's nothing. Right. We actually need to generate a name for this as well, remember? So the way that these are structured is actually that the, the name of the dependency is separate from the dependency specifier. So this is the dependency specifier and there also needs to be a string, which is the name. And so this is really the package. And then when we do arbitrary depth, it separately has to dictate a name.
01:43:16.770 - 01:44:40.426, Speaker A: And there's an interesting case of if the name is equal to the listing, that actually changes our logic. We do not, we're not currently encoding this. That might be an interesting thing to try to encode. So version is a version. So version is actually going to be sember, version, rec, parse, unwrap. And there's a, there's a way here in prop to specifically say prop, assume so we can do rec is equal to this. This is basically a way to say that if it's not event a valid semantic versioning number, then we're not going to bring it in at all.
01:44:40.426 - 01:45:09.162, Speaker A: So we're going to assume that rec dot is okay and then rec is rec unwrap. I put package instead of version. Oh, you're right. Thank you. So version takes rec, package takes package. It's also going to take optional within. It's a good question.
01:45:09.162 - 01:46:15.950, Speaker A: How do you specify boules here? Doesn't really say, does it? There's got to be an easier way than saying it has to be, oh, we can use any bool and say with default features. So optional is going to be optional. Default features can be default features public. For now we're gonna set to true. Actually we can set to none. This, I guess optional is gonna be option of this. So it's fine.
01:46:15.950 - 01:47:44.150, Speaker A: Package is gonna be, oh, that's interesting. I guess we can do this arbitrary package and say it's going to take this and it's going to return an option string. That's not really what I want. I wonder if there's a nice way to do, actually, I guess we can do it this way. We can say package. It's going to be an option string. And so it's either going to be none or it's going to be, I guess like this, assuming this is what they want this to look like.
01:47:44.150 - 01:48:34.910, Speaker A: Prop map s and this is going to be sum s. I think that's what they're after. And that way this can go away. Package is going to be Arb package. And so that way we're actually testing now both the case where the package is set to something that's different from the name of the crate, and the case where it's not set, which is what we wanted. Why do I need to wrap this in. Okay, that doesn't seem right.
01:48:34.910 - 01:49:30.130, Speaker A: Features for now we can just leave as none. We're going to want to generate this eventually, but for now we'll make it simple. And same with target, it's going to be none. Now it's complaining that this has to generate, okay, now it complains that it expected to be a closure that returns dependency string, but it returns result of that. Actually this doesn't need to be prop compose is the reason I think. Or maybe it does. Proph compose.
01:49:30.130 - 01:50:42.360, Speaker A: Interesting. I don't know why this one is being unhelpful because I feel like this is supposed to generate this. Oh, but then it can't use prop assume. What does it say under filtering? Oh, you can only use prop assume inside of prop tests, that's why. But we can use it locally without. So we're going to have to do prop filter s semware version rec parser. Okay.
01:50:42.360 - 01:51:33.870, Speaker A: And that way this can go away. Is it gonna let me do that? Expected two arguments to prop filter. Not a valid semwerec. Oh, right. But it might be. Is there a filter map? There is a filter map. So s dot and then this.
01:51:33.870 - 01:52:36.710, Speaker A: Okay, that's also none of what I want. I think I'm just gonna leave this as this for now and just use the unwrap. We can't use any dependency kind because dependency kind doesn't implement arbitrary and we don't want to take the dependency to make it implement arbitrary. Okay, so now we have arbitrary depth and for arbitration depths we want to say that it takes. Right. So any, I think we now should be able to say ARb deployment zero to, I don't know, I don't think we actually need all that many whoops. And I don't think we need to map over it.
01:52:36.710 - 01:53:52.310, Speaker A: We do actually. We want to prop map d and we want to turn it into a string, which is going to be something like format. Nope. But it returns string. I don't want that. I want, maybe it's just map vex strategy is not an iterator. Okay, prop returns string.
01:53:52.310 - 01:55:04.260, Speaker A: Actually, this is going to be easier if we say that ARb Dep, which is going to be any depth and returns a string and it just formats that. And then this now is just Arb Depster. That doesn't work either because the we want to branch with section of the file we put in. Depending on the kind. Maybe we ignore kind for now. That's fine. We can deal with this.
01:55:04.260 - 01:56:10.090, Speaker A: So this is going to be something like the name. We can use a raw string here, make it a little bit nicer to write. It's going to be equal to, oh, turning this into a string is actually gonna be a little bit annoying, right. Because we want something like depth name or dep zero, right, which is the name. We're gonna ignore depth one, which is the kind. And then we're going to say. And then we kind of want to test the case where there's no map here.
01:56:10.090 - 01:57:18.520, Speaker A: But that's actually kind of annoying to do. So instead I think what we want to do is something like if let sum depth depth version then s pushster format. Oh, this is way grosser than I wanted it to be. Version equals one of the expressions fields. Oh, dep two. Oh yeah, we can write, you're right. We can do write instead into SDE mute s, which is marginally better.
01:57:18.520 - 01:58:12.700, Speaker A: But then we have to use StD format, right, for that to work. The version is always there, right. And then we're going to have it ultimately output s. So this would actually, this gets us to something that's kind of working. Now the to do here is also write out other props, including getting comma. Right. I just want to see that the basic infrastructure we have works.
01:58:12.700 - 01:59:11.422, Speaker A: We're not going to do this. We're going to take depths, which is going to be in arb depths. And then here we're going to do ctomml is to open read to string p join cargo toml. So we're going to read in the existing toml. This is one of the reasons why this really slow because it operates on the file system as well for every test case. And then we're going to do push a new line. Remember that the cargo toml, the cargo new generates has a, it already has an empty dependency list.
01:59:11.422 - 02:00:00.470, Speaker A: So we should be able to then do for depth in depth ctmal. I guess we can write to ctomml. We don't even need to that we can do this step. And here we'll use standard format write as well. And then we'll do write to p join cargo toml and we'll write back out the modified toml. And then we still need to do this check later on. But that's fine.
02:00:00.470 - 02:00:39.160, Speaker A: Let's see what happens. I just want to see here, if we do prop test, there's a warning here, we don't use self. There's also a warning saying online 69. Right. We don't unwrap this. And I guess here we could, for our own sanity, print out what it actually does. So what is the list of depths that it's trying to inject? Well, it does something.
02:00:39.160 - 02:01:45.300, Speaker A: File failure, persistence. That's fine. It doesn't find Librs or main rs. Oh, actually I wonder whether the prop tests are supposed to not be integration tests, but actually be test binaries. Oh, I guess it is not entirely clear where they intend that this should go. They have a main, but they don't actually say where. Like they have a main function here, which is a little interesting.
02:01:45.300 - 02:02:33.360, Speaker A: That's fine. I don't think that matters. But it does say that my regular expression is invalid. Repetition, repetition missing in this pattern. It might be this dash, actually. So it's complaining about my thing here. I'm gonna simplify it a bunch and see if it still complains.
02:02:33.360 - 02:03:20.250, Speaker A: It does still complain. Value of minor version exceeds use 64 max. Right. This just. Right. Okay, so we probably here want to say instead of this being arbitrary number, realistically, versions, numbers are going to be between not very many digits, and same here, probably between one and three. See where we get to with this.
02:03:20.250 - 02:04:01.270, Speaker A: Well, this is interesting. Certainly seeing some, some panics here could not parse a equals version equals. So that's correct. So that means our printing is wrong, which is correct. There should be a double quote around this. Let's see. Okay, it ran for a little bit longer.
02:04:01.270 - 02:04:26.640, Speaker A: Wow, check out this list of dependencies. So this is all the stuff it tried. So here's a crate named r underscore underscore dash, underscore o underscore six, I underscore underscore zero y dash. That's the name of the crate. And it, we take a dependency on that on version two. Sweet, thanks. Great.
02:04:26.640 - 02:05:24.918, Speaker A: Although this is, this is what we asked for, and it failed on this one because invalid leading zero in minor version number. Ah, yes. Okay, so this is, this is our regular expression again, where it actually needs to be one through nine and then maybe zero through nine. And we could, we could do this with zero one instead. And this too has to be, it has to start with a non zero. That's also not true because it can just be zero. So it has to be either zero or one through nine followed by any number and same here.
02:05:24.918 - 02:06:11.416, Speaker A: This has to be either zero or one through nine followed by zero to nine one two times. Ah, regular expressions are the best. Man. It's running for longer now. Okay, it's doing stuff. Let's have it not capture the output. Okay, so it's generating some like reasonable dependency specifiers here.
02:06:11.416 - 02:06:39.170, Speaker A: And remember this is running through the full round trip test. So this is turning it into all of the intermediate formats. Now of course there are a bunch of things we're not doing at the moment. So for example, we're not writing out all the other properties. So here we're going to say if let's, I don't actually remember what's in here. Depth two dot public. Oops.
02:06:39.170 - 02:07:45.320, Speaker A: What else do we have? We have optional public we're not currently writing out. So optional, that's frustrating. Default features, what else do we generate? Optional, default features and package. So let's do those first. Then we're going to write out optional equals Boolean, same with default features. It's going to be default features equals that and then this is going to be for the package and that's going to be package equals. And we're going to quote this one and for all of these because we know version comes first, we're going to add a comma at the beginning of them.
02:07:45.320 - 02:08:14.962, Speaker A: So now if we run this, let's see what we get. So we just halted for a second here. Like look at this dependency specifier. It generated version equals caret zero dot zero. Optional equals true. Default features equal true. And we can go further here too.
02:08:14.962 - 02:09:04.586, Speaker A: Like for the this, it doesn't have to be those, it can also be less than, it can also be greater than, it can also be greater than or equal. So let's say it can be one of these or it can be less than or equal, or it can be greater than or equal. Technically it can also have commas in it, but we're just going to ignore that now. Now what's interesting is it seems to, it biases these a little bit. It seems like it tries to like pick every branch equally often and stuff. But I don't think that's too bad. Option equals false, default features equal true.
02:09:04.586 - 02:10:01.910, Speaker A: Package equals this other random string that's not the same as that random string. Nice. And what's interesting here too is that, you know, this is actually testing. Remember how if you watch the previous stream you'll remember that there's a bunch of logic around whether package is set how that gets transferred between the different like representations and here like we're going to generate all of them. Now currently we're just checking that this doesn't panic, right? Because down here we're writing out the dependencies, but we're not actually checking that this is present in the, the ultimate thing that we end up with. So there's a bit here missing, which is in the, in text. We want to check that for dep index dot.
02:10:01.910 - 02:11:15.524, Speaker A: Now I don't remember what the dot dependencies, the regex for minor versions is wrong because it should be zero. It should be one to nine. Oh, right. Yeah, this should be zero or two. Yep, that's right. So we want to loop over the dependencies that are actually listed in the, in there. And for each one we want to make sure that final dep input depends.
02:11:15.524 - 02:12:27.480, Speaker A: This scan could perhaps be more efficient. But we want to check here is that the dependency that ended up in the final index entry is actually one of the ones that we gave in the input. And this is going to be check because what we want to do here is panic if we ever get here. So we want to see if final deploy dot zero, which is the name is no, if the input depth zero is equal to the final dep name, then we want to break outer. Realistically that's not actually, we need the full check here. No field zero on string. Right.
02:12:27.480 - 02:13:04.408, Speaker A: I think I actually want this as a helper. I think I don't want this to generate a vector of string. I think I want this to generate a vector of dependencies. And then I want this to just be a helper function, just dep to toml. And it takes a dependency and returns a string. Because realistically that's what we've built here. It's nothing about that function.
02:13:04.408 - 02:14:03.260, Speaker A: That's actually about prop tests. And then here we're going to write out depth to Toml of Dep. And then down here this is check. So this is just checking that, this is just checking the name. It's not checking that any other property is, is maintained. Let's just see that that isn't wrong line 71. And I guess this can be here.
02:14:03.260 - 02:14:50.630, Speaker A: And this is going to be something like here. All right, let's see. Great. Okay, so this at least suggests that for all of the final dependencies are in, the input dependencies are one of the input dependencies. We actually want to check that they match. But there's also the inverse here, which we have to check, which is num found is zero. Then here num also this should continue.
02:14:50.630 - 02:15:40.210, Speaker A: Check num found plus equals one. And when we get down here, right, this should be a panic. And this should be if num found not equal to depth len. Or we can just do cert equal num found input. So the assertion here is that, no, this is depths. The assertion here is that we actually found all of the input dependencies and the output dependencies. And again we're not actually checking that these are correct beyond just the name is present.
02:15:40.210 - 02:16:46.440, Speaker A: See what we get. Okay, so this is just running through now and my guess is it would have panicked long by now if any of this was obviously broken here. Though we can now start to do things like assert that the input FD is final dep ID is input depth and we want to assert equals the version is equal to the final version specifier. Right. So here we need to actually map these fields for the input depth. I thought it was version. So why is it not version? Oh, this should be input depth two.
02:16:46.440 - 02:17:31.944, Speaker A: And the final depth here is of type index and the index dependencies. So registry dependency and a registered dependency has a name and as a requirements, and I think requirements is the one we're after. See if that works. And as you can see, you know, we could go on with this for ages, just adding lots and lots of checks. Now I'm just adding them mostly for completeness. Right. So there are a bunch of, we want to check that optional matches.
02:17:31.944 - 02:18:29.539, Speaker A: We want to check that default features made it all through. We want to check that package made it all through. So optional in the index. Optional is not optional and that's because it defaults to false. And then we want to check that here. Default features defaults to true package here as deref, dereference, both of them and I. Then they should be matchable twice.
02:18:29.539 - 02:19:21.810, Speaker A: One of these is a calstring, this one is a calstring. So here we're going to do has rep so that they're actually comparable map s. There we go. Just needed more stars. And then what else? Is there anything else that we actually write out here? Version optional, default features and package. Now one thing that's interesting is this will actually break if it generates two dependencies by the same name, which we haven't told that it shouldn't do. It's also kind of interesting.
02:19:21.810 - 02:20:29.920, Speaker A: Nice. So this now could run basically forever and generate every possible dependency list. But at least now we have something that does the round trip check with a bunch of different configurations of package options. We could add more things here that make it into the cargo toml that are not just in the dependency list, but at least now we know that that dependency transformation does something reasonable. So now that we have this as prop test, I think the other thing we want to do here is actually, I think we want to ignore this. The reason I think we want to ignore this is because this, this particular test is actually pretty slow. And so what I want to do here is now go into test and say that not minimal, not required, actually required is probably fine.
02:20:29.920 - 02:21:14.986, Speaker A: So we're going to go down here and say prop test is going to run only on stable. I don't think this is one where we need to run it on multiple ones. Toolchain stable. We don't need to generate the lock file separately. And here we want to test prop test ignored. This is not all features and all targets, it is test prop test ignored. So this will actually run the test.
02:21:14.986 - 02:22:11.090, Speaker A: So the idea here is that the, we're not actually going to run the prop test in the normal test step that's going to be ignored because otherwise we would end up running it on windows, on Mac OS like in all the different configurations where we run test. So we want a separate job that's actually going to run prop test. There's even an argument here for saying this job should only run if the basic test suite succeeds. And I think there's a way to do that. So if we go to GitHub actions workflow syntax jobs needs is the one I'm after. So we can say here that this needs required. So the idea here is that we're not even going to run the prop test job, which is going to run for a long time.
02:22:11.090 - 02:22:55.578, Speaker A: We're not even going to run it unless the basic test suite succeeds. And now if we go back to prop test, one of the things we'll see is there's a configuration down here, test timeouts and forking. That's not actually what I want, configuring a prop test. So because we know that our prop tests are pretty slow. Okay, so currently 256 executions have to execute for a test as a whole to pass. Now 256 for us is actually going to take a while. Like you saw how slowly they executed.
02:22:55.578 - 02:23:23.620, Speaker A: But I think it's probably okay. I don't, I don't worry too much about this. I think we can also skip this print line. I don't think it's that important. But 256 might be a little high. Let's see how long the CI job takes to run prop test. The other thing you'll see is we now have a new test slash prop test regressions thing.
02:23:23.620 - 02:24:13.680, Speaker A: Prop test regressions. And this has the seed for failures so that it replicates over time. They recommend that we check it in. We're not going to check in the initial one we had, but over time it's a file that we're probably going to want to check in and we could even here do. So it's going to run for a little bit. And now you see it didn't degenerate the file because it only generates that on failure. Okay, so git add commit, add protests and then git push you prop test origin, prop test.
02:24:13.680 - 02:25:12.948, Speaker A: Right. So now we open a pull request and let's see how this turns out in RCI. So it should now be the case that it kicks off a bunch of different CI rules just as it did previously. If we go to details here and we go not to check but to test and go in here. Yeah, here we go. So we see here that there's this required and then there's this one which hasn't started executing yet, and this is prop test. So it won't run this one until that one's finished, which is going to make the total CI time longer.
02:25:12.948 - 02:26:30.700, Speaker A: But it also means that we're not going to start running prop testing until we know that the basic tests actually work correctly. So while that's going, what we'll do is we'll go configure the dot dot dor branch protection. So for main add branch protection, what do we want? We want to require a pull request. We want to, these are nice, but they matter less for a repo that only I work on. I do require status checks to pass, and in particular I want, I want stable formatting to pass. I want Ubuntu stable, which is the just the regular test suite to pass. What's annoying here is these are the workflows but not the individual jobs.
02:26:30.700 - 02:26:54.640, Speaker A: So it can be really annoying to dig through. And I want features. So these are going to be required. I don't want to require minimum supported rust version, for example. I don't want to require minimal versions. Those can be pretty annoying. I don't want to require nightly, I don't want to require beta, but these ones are clearly ones that should succeed.
02:26:54.640 - 02:27:40.974, Speaker A: I don't require linear history, don't require deployments create. Let's go ahead and do that over here. Sweet. Okay, so now we have a branch protection rule that's going to start to kick in. Windows test failed. Why did windows test fail? Hate windows. So much.
02:27:40.974 - 02:28:15.130, Speaker A: Sometimes I think this is just an actual random failure. Ignore this. But let's go ahead and see whether our tests actually, oh yeah, you're right. And ignore equals would be useful here. I forget whether that's stabilized yet. So there's this new thing where you can say ignore equals and then give a reason. And in this case we're ignoring it because prop tests are slow and should be run explicitly.
02:28:15.130 - 02:28:58.060, Speaker A: And so now if I run this, hopefully it should print that ignore message. Yeah, that's a good call out. I'll put that into the check. So if we now go back to test, add prop test. So these are still running. Why are these still running? Why are they so slow? Oh, it's because building our dependencies takes forever. Yeah, I think there's a, given the size of our dependency closure, especially via cargo, there's a decent argument for caching the cargo home directory, which I just haven't set up at the moment.
02:28:58.060 - 02:29:55.614, Speaker A: But there is like in, no, not there, in here. I forget exactly what the command is, but there's, there's like a step you can add here that's like cache home slash cargo or something. Or in our case I think what we actually care about is the target directory. And by caching the target directory it's slower if you don't have very many dependencies because it means you have to. It means the GitHub CI actually has to copy over the cache directory from the, from previously, from previous builds. And that has a lot of binary artifacts which are slow to copy over and it's faster, just build them from source. But in our case we actually have a decently chunky dependency tree that takes a long time to build cargo in particular.
02:29:55.614 - 02:30:39.640, Speaker A: And so doing the copy of those binary bits might actually be faster. Okay, so that kicked off. So now we should see this one run. We'll look at the summary in a second. Let me see if I can find job. And yeah, it's tricky whether you use caching because now also if your dependency closure updates then it's annoying. This is one of the places where checking the lock file also really helps.
02:30:39.640 - 02:32:28.210, Speaker A: Oh, interesting. They don't have a, there's a specialized version for a bunch of languages, but not including rust. So we can say here, yeah, so what we want here is we'll probably just use this for required. One of the annoying things, right, is that when we for example build with MSRV, then that has to be a different cache. And so this is why you have to define a cache key. So it's a little bit annoying to set this bit up. And so we would need to do something like target cache restore, target and key here is going to be matrix toolchain restore and then there's a separate like save step which we run after Kurgo test.
02:32:28.210 - 02:33:54.720, Speaker A: It's going to be save cache target, it's going to be here save, ooh, steps, dot target, cache, restore, outputs, cache, primary key. And then in theory at least we should be able to use that same cache down here. We're going to restore but we're not going to save as afterwards. And in fact we could restore the cache here for things like minimal versions too, at least in theory. But I don't actually think I want to do that. And for prop test the restore here is just going to be stable target. So that's the thing we can try.
02:33:54.720 - 02:34:41.360, Speaker A: We'll see whether it's makes a meaningful difference here. So now the prop tests are running and they're taking forever to run because of course they are. It's prop tests. The reason I want to do this in a separate commit is because I want to see to what extent it makes a difference. And that means I need to know how long it takes to run when we're not doing this. Huh. It finished compiling in four minutes and then it's gonna run the test.
02:34:41.360 - 02:35:39.710, Speaker A: Explain ignore. And then I'll go ahead and push now because I don't think it's going to cancel this one while it's running. And we'll see. It'll be interesting to, it's not going to speed up the first time because it has to populate the cache, although it might speed up the prop test from the very beginning. Oh it completed. I want to see the output of this step. So prop test, that's not too bad, right? It finished compiling after what, four minutes and then it finished the whole job in 458.
02:35:39.710 - 02:36:25.030, Speaker A: So the actual prop test only took, let's see. Yeah, the actual prop test only took 51 seconds. So that's running 256 different combinations of this input space. Now we are exploring a pretty large input space because it's all the feature combinations. We're also not correctly taking advantage or making use of the dependency kinds. So there's some more work that has to happen here. Arguably I should note that down in testprop test of to do split these by their dependency kind dep one.
02:36:25.030 - 02:37:02.620, Speaker A: But the prop test fast passed the, that's nice. And so now we'll go back here and see for test add prop test and I want to look at these jobs. And in particular I want to look at stable. So the restore here, there was no cache, so it didn't make use of anything. This is going to take about four minutes, right. That's how long the build takes and then it's going to save. And then hopefully in the prop test job we should see that speed up.
02:37:02.620 - 02:38:10.764, Speaker A: Fantastic. Okay, so we have. Oh, and this should also be improving our coverage. Oh, but our coverage doesn't run the prop test. So that's another good question, whether test coverage, should this also run prop tests. And I guess let's do market todo. The trade off there, of course, is that when you add this instrumentation for coverage, it also executes more slowly.
02:38:10.764 - 02:38:44.410, Speaker A: And so having prop tests run more slowly to get coverage is unclear whether it's worth the trade off. So let's see. So we have gone through all the bits of CI, we've done a bunch of talk about prop testing. We've implemented some prop tests. We've looked at configuring CI for prop tests. We looked at, we didn't really talk about documentation, although arguably that's maybe a topic for another day. Anyway, I think that's probably where I want to end this off.
02:38:44.410 - 02:39:56.460, Speaker A: I still want to see just that this does roughly what we want. But beyond that, I think I'm happy with today. While we wait for this to finish, are there any follow up questions from what we've done today? Check that you access the restore cache key correctly. Are you even allowed to reference that key from different steps? Yeah, I'm pretty sure you are, because one of the points for the cache is that it is cross job and I think even cross workflow. So if we go back to here, you see there are caches and the caches are, I think, entirely global or they're per branch maybe, but that's very much intentional because that's sort of how the cache has to be to be useful. But are there other questions about how we configured CI or about any of the stuff that we talked about today so far, just while we wait for the final job here? No, everything makes perfect sense. I'm glad to hear it.
02:39:56.460 - 02:40:42.980, Speaker A: Let's see. I initially claimed this would be 2 hours. It's almost 3 hours, but I said two to three. So I think I'm okay. Come on, come on. It's interesting too, because I think in theory, coverage here could reuse the same cache. Where it gets tricky is that for coverage you need to instrument your code with coverage.
02:40:42.980 - 02:41:29.302, Speaker A: And I forget whether this also means you instrument your dependencies. I think the challenge is you end up modifying rust flags to pass c instrument coverage. And even if that doesn't affect the build of your dependencies, cargo takes it into account for the fingerprint, for whether it's allowed to reuse the cache files. Can you cache, target, and rebuild? Only when cargo lock changes. That's effectively what we end up doing here. Okay, so the build finished in five minutes, and now we're going to see here what saving the target file looks like. So saving it took, what, 17 seconds? So that now finished.
02:41:29.302 - 02:42:04.240, Speaker A: So now let's go look at this prop test job, which hopefully now shouldn't need to do a five minute build. That's the main thing we're going for here. Install stable. Hopefully this should be able to restore the cache target directory from the step that just ran. Yep, it did that. Downloaded very fast. Cash restored.
02:42:04.240 - 02:42:50.410, Speaker A: Shouldn't need to update the git repository. Given that we pinned the revision in cargo toml, these warnings from the patch are a little unfortunate. I really hope this can now avoid doing the build. Oh, I can't wait for sparse registries to be stable. Arguably I should just inject that configuration right here to make the CI faster, but it's about to land anyway. I think it's going to be the default in 169. Come on, git clone.
02:42:50.410 - 02:43:43.680, Speaker A: So many rockets and hoorays. Oh, come on. The clone of the index is so slow. I'm so excited that we have sports registries downloading crates. That's fine. So it has to download all the crates because we didn't, we're not caching home cargo, right. And as a result, it has to redownload all the sources.
02:43:43.680 - 02:44:35.724, Speaker A: But it shouldn't recompile. Oh, but it recompiles some of them. That's almost certainly because of build scripts. So cargo has this property where build scripts will build scripts can say rerun me, and therefore recompile if any of these files have changed. But the way that you indicate that is is, or the way the cargo detects that is solely based on the timestamp of the file and not the contents. And so that's why when we redownload, the timestamp is newer, so it rebuilds, which is why it ends up rebuilding cargo, because one of cargo's dependencies has such a build script. Still, two minutes, though is way faster.
02:44:35.724 - 02:45:25.090, Speaker A: But that means we can tune this too, by having it cache the home cargo directory as well. So we can go here and in save, we can have it also save that and then restore. We can have it also restore that. Can't you pass a flag to save those sources to target and not use home cargo? No. So the stuff that's in home cargo are the source tarballs. It's not the target artifacts. The target directory doesn't store those things.
02:45:25.090 - 02:46:10.368, Speaker A: We could override cargo home to make that be a subdirectory of target, but it's a little dicey. So this is now also cash cargo home for build script timestamps. But this did now run a lot faster. And in fact, in theory, if we do get push of this should run even faster than that. I think this is a good place to stop, though. We're getting towards the end. I don't think there's too much more to cover here.
02:46:10.368 - 02:46:34.880, Speaker A: You can follow this pr if you want to see me tweak the final bits, but I think this is a good place to end. Thank you all for coming out. Hopefully this was useful. My guess is we'll probably do. I might do another stream on this, just where I write documentation, because I think there's enough to cover in that separately, but for now, I think I'm happy. All right, thanks, folks. See you later.
