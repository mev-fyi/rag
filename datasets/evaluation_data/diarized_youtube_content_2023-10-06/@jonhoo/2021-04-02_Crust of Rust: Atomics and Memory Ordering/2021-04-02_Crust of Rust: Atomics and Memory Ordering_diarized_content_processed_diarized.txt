00:00:06.520 - 00:00:44.581, Speaker A: Hello folks, welcome back to yet another Crust of Rust video. In this one I'm going to tackle a topic that keeps being suggested over and over, and that is atomics and memory ordering. If you don't know what those are, that's fine. I'm going to sort of go through them in a decent amount of detail over the course of this. But this is a topic that I've been hesitant to go into, partially because it's a topic where there aren't. There's not really that great documentation for how stuff works. So it's very easy for me to explain something and then get it wrong, which I don't like to do.
00:00:44.581 - 00:01:41.121, Speaker A: Not because I'm worried about being wrong, but more because I'm worried about putting content out there that people then rely on and that content is wrong. And the other reason I haven't tackled this because I felt like I was still sort of grappling with some of the concepts myself and so I was worried that that might translate into sort of a poor explanation of some kind. I feel like now I'm at a point where I've sort of read through enough of the documentation. I've worked with this material enough that I think now I can both give a correct and understandable explanation of what's going on. So that's what we're going to try to do today. The, the sort of core of the video today is going to be focusing on the Rust atomic types and the memory ordering that's observed in Rust. And so the stream will be Rust specific, but at the same time most of this translates to basically any other language that has Atomics in this kind of sense.
00:01:41.121 - 00:03:12.167, Speaker A: So the sort of C C to some extent Java too, although the memory model is a little different and same with Go, so hopefully you should be able to take some of the things that you learn here and apply to other languages as well. This, the underlying memory ordering stuff is useful to know regardless of what language you're working in. So most of what we're going to be talking about today is going to be the Atomic, the standard sync atomic model from the Rust standard library. And this module has pretty good docs on sort of the high level ideas of why we have these types, what they're used for, but there's just a lot of detail there that makes it hard to get things right and sort of understand all the subtleties of the interactions of the different types. What we're going to start out with is just like why do we need atomics in the first place? So you see here that the documentation lists a bunch of types like atomic, bool, atomic, eyesize, atomic, use size, atomic, I8, et cetera. And you might wonder, well, why not just use bool or isize or usize, why not just use the primitive types? And there are a couple of reasons for that. The primary one is that if you have a primitive type like a bool or usize or something that's shared across thread boundaries, there are only certain ways to interact with that value that are safe.
00:03:12.167 - 00:04:54.103, Speaker A: And when I say safe here, I mean it. Both in the sense of data races are undefined behavior. So if you don't control how different threads interoperate on the same memory location, you run into undefined behavior, which, as we talked about in some of the previous streams, is just bad. But the other reason is because it makes sense to have slightly different APIs for these types, because what you're doing when you're operating on the atomic versions of these types is really that you're issuing different instructions to the underlying CPU and placing different limitations on what code the compiler is allowed to generate. And we'll get into what those differences are and why they're important. But the core of it is that if you have shared access to some memory value, you need to have additional sort of information about that access to let the CPU know. When should different threads see the operations that other threads do? How do they have to synchronize? How do you know that? If one thread writes to a value and another one reads one, what are the guarantees about which values the reader will read? Will it always read exactly the latest one? What does the latest one even mean? But also for other reads and writes in the program, in both of these different threads, which of those are visible to this other thread? In general, if you had something like a usize, and let's say you found sort of a sound way to do data racing, so you had a thread that just like wrote to this value and a thread that just read from the shared value, and it was a standard usize.
00:04:54.103 - 00:06:04.855, Speaker A: If you did that, there is actually no real guarantee that the reading thread is going to see the value that was stored ever. Now, in practice, the CPU and the memory system will usually make that be the case, but you're not guaranteed for that to be the case. There's no, there's nothing in the specification that says that this should be the case. And this is where atomics comes into play. It sort of lets you place limitations and rules on the use of this type and the value and what values can be exposed when. Now when I say sort of the specification or the the sort of rules here for the compiler and such, what I'm really talking about is the languages memory model and the Rust reference, which is sort of the. So you have the Rust documentation, the standard library documentation and stuff, you have all the RFCs, and then you have the Rust reference and the Rust reference at least the idea is that it should fully specify the language such that if someone else came along and wanted to implement a Rust compiler, like a compiler for the Rust language, they would know exactly what to implement and the behavior of those things.
00:06:04.855 - 00:07:12.847, Speaker A: And it has a section on the memory model and it says Rust does not yet have a defined memory model. Various academics and industry professionals are working on various proposals, but for now this is an underdefined place in the language, which you know is kind of unhelpful if you're trying to write anything that uses atomics and concurrency. Now in practice it's not actually that bad because in general, partially because Rust relies relies on llvm, Rust generally follows the C memory model and in particular like the C11 memory model I think is the most sort of recent variant of that. And so what we're going to be following here is actually the memory ordering documentation from C. It's not necessarily because C is exactly what we match here. Sorry for the bright screen, this one doesn't have a dark mode. We're going to be using this because it has fairly good documentation for what the different memory orderings mean and good examples of what goes wrong.
00:07:12.847 - 00:07:58.063, Speaker A: In general, we're not going to be reading this page too much. It's more that this is where much of the explanations I'm going to be giving you, the examples I'm going to be using, and just much of the guarantees that I'll be talking about are coming from just so you're aware. So let's start out with a type like atomic Usize. This is the atomic equivalent of the Usize type. If you were to look at it in memory, like the in memory representation, it's exactly the same as a usize. The only real difference between an atomic Usize and a Usize is that it has different methods on it, which we'll see in a second. And you can only really access the value through those methods.
00:07:58.063 - 00:08:48.769, Speaker A: You can't get the usizer without calling one of the methods. It's not like it's castable into a usize trivially, for example, and it's the same size as a Usize 2, the only real difference is what instructions get generated when you access this value. And if we look at the methods on atomicusize, you see that there's a constructor that gives you a new one, the atomicusize. And this applies to all the atomic types are not inherently shared, so they are sort of values that are placed on the stack. So if you want to share them across thread boundaries, you can't just like create a single atomic U size and give out. I mean, you can, you can give out just shared references to those threads, but shared references to something on the stack won't be static. And so that usually gets you into a pain.
00:08:48.769 - 00:09:50.865, Speaker A: So generally what you do if you get one of these atomic types is that you stick it on the heap using something like box or more frequently, an arc, and that will allow you to share a pointer or reference, depending on how you want to use the language here, to that shared atomic value, which you can then update. The reason this works, and the main differentiator between the atomic usize operations and usize operations, is that you can operate on an atomic usize using shared references to self. So notice that for a normal usize, you would need an exclusive reference to the usize in order to modify it. That is not the case with atomic use size. Instead, a shared reference is sufficient. And the reason for that is because the compiler generates sort of special CPU instructions that make it safe for multiple threads to access this value at the same time. Okay, so let's just first go through sort of what are the methods.
00:09:50.865 - 00:10:57.345, Speaker A: We'll go into what each of them mean and how you might use them in a second, but just to sort of get a survey of what kind of stuff we have to deal with. So, first and foremost, there is load and store, and they do basically what you would expect. Load will load out the value that's stored in the atomic usize. So in this case it returns a usize and store, takes a usize and stores it into the atomic usize. Similarly, there's swap, which sort of does both, and you'll notice that these take an additional ordering, and we will talk plenty about ordering. If you look at ordering, ordering is just an enum that has these different variants, and what these means is like part of what this video will be about, what kind of semantics they establish. But fundamentally what ordering does is it tells the compiler which set of guarantees you expect for this particular memory access with respect to things that might be happening in other threads at the same time.
00:10:57.345 - 00:11:47.365, Speaker A: The other methods on this are compare and Swap, compare exchange and compare exchange week. We'll talk about the differences between those later on in the stream. These are basically ways of reading a value and also swapping out its value, but doing so conditionally and in one atomic step. And when we mean when I say one atomic step, I'm going to show you in a little bit of a second what that means. But basically if you do a load and then a store, there's a chance that some other thread comes along and does something in the meantime it gets to run between your load and your store. And a compare and swap is of a single operation where no thread can get in between. There are also a bunch of these fetch methods.
00:11:47.365 - 00:12:52.675, Speaker A: So fetch add, fetch, sub fetch and Fetch nand. These are basically operations that similarly try to avoid something happening between when you load the value and when you change it. So FetchAdd for example, will atomically so as a single step, load the current value and add a value to it without it being possible for any thread to get to execute in between or modify the value or read the value in between those operations. And we'll talk about why that's useful and why these are separate from the compare exchange methods in a second. Before we dive into actually using one of these, I'm going to take some questions on what I've talked about so far because I went through a bunch of things fairly rapidly here and I think it might be useful. Just sort of make sure we have a shared foundation that makes sense. Don't U64s on X86 have atomic access? Or I'm confusing it with something else? So on some platforms the non atomic types have additional guarantees.
00:12:52.675 - 00:13:41.733, Speaker A: So for example, I think on like Intel x86, if you have a 64 bit value, any access to it is sort of automatically atomic, assuming it doesn't cross a cache line boundary. I think like it needs to be an aligned access. And that means that in theory, like if you're down in the RAW assembly and you know that you're on intel x64, you can do this without atomic usize. But realistically, in the standard library, what we want to do is expose sort of a common interface that will always work. And so this is why we can't really expose like just for usize. On x64 you can use them through shared references. So instead everything is modeled through here.
00:13:41.733 - 00:14:27.485, Speaker A: It's just that on something like a target platform, an architecture like that, Atomic U64 for example, will generally be free. It's not quite true either, but close enough. Why Is it non exhaustive? Yeah. So ordering, if you look here, is non exhaustive. This is a great keyword if you haven't used it before, which basically says that no code is allowed to assume that these are all the variants that will ever be in ordering. So if you match on an ordering, for example, you always need to include like an underscore branch and otherwise or else branch, because the standard library wants to be able to add additional orderings later on if necessary. The biggest one, I think is consume ordering, which is something that C has but Rust doesn't currently have.
00:14:27.485 - 00:15:32.957, Speaker A: I don't know whether that will be added, but the idea is that if we want to have the ability to add things later, it needs to be marked as non exhaustive. Is the ordering enum related to the different memory models of the architectures, like x86 or ARM? Yes, the different orderings are. It's not that they're related to the memory architecture, they're related to what guarantees the operation gives you, how different architectures implement. Those guarantees will vary from architecture to architecture, as we'll see in a second too. What's the difference between atomic and mutex? So an atomic, there's no locking with an atomic, it's just multiple threads can operate on this value at the same time in some reasonably well defined way. But it's not like with a mutex, right? What happens is one thread gets to access the value at a time and all other threads need to wait. And the mutex usually guards a larger section of code, right? It says I grab the mutex, I run this code, and then I release the mutex.
00:15:32.957 - 00:16:16.265, Speaker A: And no other thread is allowed to execute anything under that mutex. While I'm in this critical section, as it's called with an atomic, there's nothing really like that. You can maybe think of it as a mutex that guards just a single memory access if you wanted to, but. But it's much more efficient than that. Did those atomic operations then not block other threads? Potentially no. All atomic operations are lock free. They're not necessarily weight free.
00:16:16.265 - 00:16:48.457, Speaker A: This is on certain architectures that don't have fetch add, for example. It's like implemented in terms of compare and swap. So you may have to wait for other threads, but they're not. They're considered lock free. Like there's no mutex in there. The atomic operations are not just per cpu, like per CPU architecture. It's not like they just modify the instructions, they also change the compiler semantics.
00:16:48.457 - 00:17:46.325, Speaker A: Again, as we'll look at in a second when we start looking at the different orders. So they limit both what the CPU can do and what the compiler can do about a given memory access. Yeah, so the point here is, even if you're on something like intel x86 64, you might still want to use the atomic types because you need guarantees from the compiler as well. Store, swap and friends all take a immutable reference to self, not mute self. I guess that means they rely on unsafe cell. You know, that's a good question. I think you're right.
00:17:46.325 - 00:18:19.043, Speaker A: I mean, we can check that pretty easily. Of course, it's a macro. Let's see what we can find here. Oh, there's a lot of macros in here. Let's see if we can find the struct definition for this. I see a self v dot get, which is usually an unsafe cell. So my guess is that all of these atomic instructions contain an unsafe cell that they then called get on to get the pointer to the value and then they issued the actual assembly instructions on that.
00:18:19.043 - 00:19:02.595, Speaker A: And in fact, if we go all the way to the top of this macro, we should see the definition of it. Yeah, so here, pub struct atomic type. So this is what the macro is going to expand to. It just has a V which shows an unsafe cell of the inner type. And you see that this is actually just the int type. So if you have an atomic usize, it's really just an unsafe cell usize, and then it uses the sort of special instructions to access the value behind that. And this ties back to the older unsafe cell or the unsafety stream where we talked about interior immutability, where unsafe cell is the only way at sort of the fundamental level to get mutable access through a shared reference.
00:19:02.595 - 00:19:43.161, Speaker A: You said atomics are generally shared via arc rather than box, but the only reason for that is that ARC is send sync while box is not. So it's simply more convenient. Right? That's not quite true. So if I box a value and place it on the heap, then now I have an owned value. But if I spawn two threads, those two threads both generally require that the closure you pass in is static, has a static lifetime. If you pass a reference to the box to both threads, then now the. Like that box reference does not have a static lifetime.
00:19:43.161 - 00:20:23.795, Speaker A: It's tied to the stack lifetime of the box. With an arc, you can clone the arc and give each thread its own individual, own, individually owned and therefore static ARC to the to the atomic U size. And so this is why ARC is usually used over box. You don't have to do it that way. So for example, there's box leak. Box leak will leak a value on the heap, so it will never call the destructor, which gives you back a static reference which you can then pass to both threads. Great.
00:20:23.795 - 00:21:04.821, Speaker A: Okay, so now that we have sort of a shared foundational understanding of why these types are, let's go into what you might actually use them for, and in particular what this memory ordering business is. So here we're going to do a new lib and we're going to make it bin and we're going to call it not hang, that's from a previous stream. We're going to call it Atomics because feeling boring today. All right, fine. Just to make it stop complaining. Watch this now. Like break breaks my install or something.
00:21:04.821 - 00:22:02.977, Speaker A: Great. Okay, so let's say that we tried. We want to try to implement our own mutex type. So I'm going to define a mutex t and a mutex t is going to hold a unsafe cell T, right? Which is the value we're going to be given out. Cell, unsafe cell and also what else are we going to need? Sync, atomic and we're going to need atomic bool and ordering. So a mutex is a combination of a boolean that marks whether or not someone currently holds the lock and then an unsafe cell to the inner type. So that if we hold the lock, we can give out immutable reference.
00:22:02.977 - 00:22:58.961, Speaker A: Right. And then this is going to have mutext and I'm not going to implement the sort of guard mechanism because that's not that interesting for this particular stream. Instead I'm going to do like a with lock which takes a FN1 that's given immutable value to T returns R. So withlock is going to take basically a closure, a function that it's going to call once it has the lock. And let's for now just make this be call F. This is not actually going to work. But for now, and I want to caveat this with what we're going to implement here is known as a spin lock.
00:22:58.961 - 00:23:27.767, Speaker A: And the reason for that is we're going to, if the lock is currently held, we're going to spin until it's no longer held. We might like yield or something, but fundamentally it's a spin. Don't use spin locks like you almost never want to spin lock. You almost never want to implement your own mutex. There's a great article by mathclad on spinlocks considered harmful. I'll post it in chat Here. It's a great article.
00:23:27.767 - 00:24:13.845, Speaker A: Read it. And don't implement your own spinlocks and probably don't use a spin lock in the first place, but we're going to do it because it's a good sort of exercise in understanding what this is for. So how's this gonna work? Well, I guess we need a new method here too. Let's make these Pub Wow, I can't spell today. Pub fn new which is gonna take the value to initially create the mutex width. It's gonna create a self worth locked which is gonna be atomic bool new. And I'm gonna have some constants here just to make the code a little nicer.
00:24:13.845 - 00:25:14.345, Speaker A: Locked bool is true and unlocked is false. So it's going to start out being unlocked and the value is going to be an unsafe cell. New T. So far, so good. So now we have a constructed mutex and the question now becomes how do we take the mutex? How do we lock it? Well, so let's start out with sort of the naive way to do this, right? Which is while self locked. I guess this needs to be self. While self locked, load not equal to unlocked spin self locked, store locked, call f, capture its return value, set it back to unlocked and return read.
00:25:14.345 - 00:26:02.815, Speaker A: So the boolean here is going to be if the thread is locked. So true means locked, false means unlocked. All right, so this is sort of the naive implementation, right? While the. While the lock is unlocked. Oh yeah, you're right. This should be here self v get and it's going to be unsafe because we need to. Basically what we're asserting here, right? Safety is we hold the lock, therefore we can create a mutable reference.
00:26:02.815 - 00:26:34.245, Speaker A: All right, so great. Now we have something that seems reasonable, right? Like we're waiting for the lock to become unlocked. Then we store the fact that it's locked so that no one else can get the lock. Then we call the function with a mutable reference and we create the mutable reference because no other thread can be in the critical section at the same time. Trust me, I will get to compare and swap. I know this is wrong, but just hang with me for a second. Just trust me on this, please.
00:26:34.245 - 00:27:06.957, Speaker A: And then we're going to store that it's now been unlocked so that other threads can now access the value. Great. So far so good. So this code has some problems. And the first is that it doesn't compile. And why doesn't it compile? Well, it doesn't compile because we need an argument here, right? This Method. As we looked at in the documentation, this requires an ordering and it's not immediately clear what this ordering is going to be because we don't know what orderings are yet.
00:27:06.957 - 00:27:25.795, Speaker A: I haven't told you. Maybe you already know, in which case that's great. But for now, who knows? We're just going to do ordering relaxed because we're relaxed people. And so it seems fine for this to just be ordering relaxed. We don't need to be super strict about things we like. Anarchy, apparently. Ordering.
00:27:25.795 - 00:28:03.365, Speaker A: All right, so we're just going to. We're just going to just relax, load, whatever that means and see if it's unlocked. Great. It now compiles and in fact, if we try to run this, it would probably work. So here's what I'm going to do. I'm going to create a lock mutex new and I'm going to do a little bit of ugliness because why not? We're going to call this. What type are we going to use here? We're just going to use one.
00:28:03.365 - 00:28:44.003, Speaker A: And this is going to be a tick static of whatever type we implemented. And then we're just going to spawn some threads. So we're going to do and respond five threads and what all the threads are going to do. This is going to be sort of a silly test. We're just going to have lots of threads. They're all going to try to modify this value at the same time through the lock. And we're just going to see whether it ends up doing the right thing.
00:28:44.003 - 00:29:13.969, Speaker A: Thing, because we totally got this right. Right. So each such thread is going to do l dot with lock and in. They're going to take the value and all they're really going to do is they are going to say V + equals 1. This is going to be for in. They're going to do this 100 times. Actually, let's do 100.
00:29:13.969 - 00:29:50.261, Speaker A: Let's do 10. Threads are going to do each of this 100 times. And I guess we're going to have to wait for these threads. So handles is a rec. In fact, let's do this the hardcore rust way. So we're going to collect all the thread handles just so that we can wait for them afterwards. So we're going to spin up 10 threads.
00:29:50.261 - 00:30:28.805, Speaker A: Each thread is going to increment the value 100 times. What am I doing here? Right. And I'll get to that in a second. That's worth explaining. And then at the end, we're just going to do for handle and handles. Handle Dot join. Because we're going to wait for the thread to exit and then once all of the threads have exited, surely now we can assert that l dot with lock V if we just read out the value from behind the lock, this should now be 10 times 100.
00:30:28.805 - 00:31:06.597, Speaker A: Right? Because we have a lock. So all of these increments should exactly happen. There should be no bad stuff going on here, right? And this complains that Unsafe Cell cannot be shared between threads safely, which is correct. Like Unsafe Cell in general does not implement send or sync because cannot be shared between threads. It just. Because, like Unsafe Cell is inherently just so unsafe. So we generally want people to opt into send and sync for that type.
00:31:06.597 - 00:31:54.655, Speaker A: In this case, we know that Mutex is in fact sync sync for Mutex T. And this is important one, where T is sending. So we're going to implement sync from UTEX so that you can concurrently access it from multiple threads as long as the value T is send. And the reason we need the T is send bound is because, like, the lock can be taken from multiple different threads and each of those threads might take the value. In fact, I think this needs to be send and sync because it might access the value as well. No, it does not. We never concurrently access the thread from the inner value from multiple threads at the same time.
00:31:54.655 - 00:32:19.027, Speaker A: All right, so now we have code that surely works, right? Someone pointed out that the collect isn't needed. The collect is needed. If you didn't have the collect here, it's true, this would still be an iterator, but you would join each thread the moment you spawn it. So you would only have one thread running at a time. Initial values one. Oh, you're right. This should be zero.
00:32:19.027 - 00:32:38.553, Speaker A: Good call. All right, great. So we have perfect code, right? If I now. What did I call this? Atomics. If I now run this, I'm going to run it with release to make it fast. Great. So this works, right? Whoo.
00:32:38.553 - 00:32:57.405, Speaker A: Our lock is perfect. All right, what if we increase the concurrency here? We're gonna do this a lot more times. So this is now gonna be 100 times a thousand. Nice. Works so well. Great. So clearly this code is entirely right.
00:32:57.405 - 00:33:30.425, Speaker A: Fantastic stuff, right? It works. Ship it. Nothing more to worry about. Except it turns out there are some problems with this. It's just that it's really hard to reproduce the problems. So, in particular, remember how I talked about one of the reasons we need atomics and why we need operations like compare and swap is because here we're doing a load and then we're Doing a store. But in between here maybe another thread runs.
00:33:30.425 - 00:34:14.259, Speaker A: So imagine the two threads call with lock concurrently. And let's imagine the mutex is currently unlocked right now both of these, both of those threads are going to see that the thread is unlocked. So both of them are going to leave the while loop. Both of them are going to get down here, they're both going to store locked to the value, but they don't see that the other one has stored locked because they've already left the loop. Then they both get immutable reference to the same value, which is undefined behavior. They both call F with it, and then they both unlock the mutex. So clearly, like, it's possible for that to happen.
00:34:14.259 - 00:34:56.725, Speaker A: It's just that generally the computer is so fast that this just doesn't happen. And like, we can, we can do things to try to encourage it to happen. So for example, here we could do like a yield now here, just to make it more likely that some other thread is going to come in in between. If we now run this, you see that it panics. We didn't get to the final value, right? We got to some other value that's slightly smaller. And the reason for this is because two threads ended up executing the closure we passed to with lock at the same time. So they both read the same old value, they both wrote down that old value plus one.
00:34:56.725 - 00:35:24.535, Speaker A: But because they both ran at the same time, that means that we're losing some of the increments because they get overwritten by another thread that runs concurrently. This is also just undefined behavior. Like the compilers are allowed to do garbage here. It does something that's somewhat reasonable, but you can't generally rely on that. And you might be like, okay, yield now. Clearly there's not a yield now here. And so why would some other thread get to run? And there are a couple of different reasons for that.
00:35:24.535 - 00:36:23.015, Speaker A: One is there might be multiple cores on your computer, right? Like this computer has like 16 cores or something, which means that if two threads are running on different cores, you can't control the relative operations of when different threads do different things. So here it could totally be the two threads are running on different cores, and they're both in the while loop at the moment. They both see it being unlocked and then they both proceed. The other reason, even if you have a single core, you can end up in a situation where the operating system will generally limit how long a given thread gets to run for, and then do something called preemption, which is basically stop the program forcibly in the middle because it's been running for too long just to ensure that all the threads on your system get to run, and when it does, that you have no control over. So you might be preempted at any point in time, including between this load and the store. Now, this is unlikely. It's unlikely the preemption happens exactly there, but it can.
00:36:23.015 - 00:37:18.973, Speaker A: And that's sort of what we're modeling here with the yield now, is we're pretending that the thread gets preempted here. Okay, so before I go into how we solve this problem, let's talk about what just happened here, why it's wrong, and just make sure everyone understands that. Would sleeping a bit while doing the plus one equals create concurrency issues? So if we slept in here, that would probably not increase the concurrency issues, because if you're in here, you've already taken the lock. It's taking the lock that's racy, not what you do under the lock. So sleep here would probably not have the same effect as the yield. Now I inserted. Isn't the compiler already predetermining the sum? No, the compiler is not predetermining the sum here because this is behind a different thread.
00:37:18.973 - 00:37:53.835, Speaker A: So even though technically this could be computed statically, the compiler doesn't. It doesn't do that across thread boundaries. Would thread sanitizer catch this? We'll talk about thread sanitizer later in the stream. But thread sanitizer would catch something like this because you have. Basically you have two threads concurrently writing to one memory location, which is the kind of stuff that it's a write write conflict, which thread sanitizer would generally catch. We will also talk about loom, I promise. Please trust me on this.
00:37:53.835 - 00:38:39.615, Speaker A: It seems like sleeping there would result in more lock contention and thus more races. No, actually you have more lock contention if the critical section is shorter. So sleep would make it less contentious. Well, it's not quite true either. I don't think the sleep is all that important and it's not necessary to demonstrate the problem. So are atomics related to multithreading or concurrency? I mean, atomics themselves are useful for concurrency, and multithreading is a form of concurrency. It doesn't requ.
00:38:39.615 - 00:39:30.105, Speaker A: If you don't have multiple threads, it's unlikely you will need atomics. If you have multiple threads, even if you only have one, core atomics are necessary. Isn't sequential consistency by default in rust? No, there's no default sequential consistency in rust. It is true that for code that you Write that doesn't use atomics. Like if you're operating on a single thread, for example, there you don't have to worry about these problems. And the reason for that is because if you look at the memory model, any sequence of operations within a given thread are guaranteed to be observed in order. So they might execute out of order, but the effects will appear as though the program ran top to bottom in sequenced order.
00:39:30.105 - 00:40:26.595, Speaker A: So that might be what you're referring to, but there's no default of sequential consistency. This is why all these methods take an ordering that you have to explicitly pass in. All right, so it's now clear that something here is broken, right? And one of the reasons it's broken is because this is race between this load and the store, where multiple threads might win that race at the same time. And so therefore we need to find some way to avoid this sort of this race condition of two threads observing the mutex being unlocked at the same time. And the way we do that is using the Compare Exchange operation. You'll notice that there's also Compare and Swap. Compare and Swap is often what the CPU instruction is referred to.
00:40:26.595 - 00:41:08.605, Speaker A: In general, you want to use Compare Exchange rather than Compare and Swap as like. Compare and Swap is deprecated too, for exactly this reason. The reason you want to use Compare Exchange over Compare and Swap is because Compare Exchange is strictly more powerful. In fact, I think the implementation of Compare and Swap used to just call CompareExchange. And the reason it's more powerful is because CompareExchange lets you specify the memory ordering differently for whether the operation succeeded or failed, whereas Compare and Swap does not. And we'll talk about what that means too. Compare Exchange week is interesting and we'll talk about it in a second.
00:41:08.605 - 00:41:45.995, Speaker A: Yep, great. Okay, so if we look at Compare Exchange, we see that it takes the current value, it takes the new value, and then it takes two orderings. And we'll talk about why there are two orderings and what they mean in a second. And the first line of the documentation is pretty helpful. Stores a value into the atomic integer, or in our case, Boolean, if the current value is the same as the current value. Right. So let's see what that would look like.
00:41:45.995 - 00:42:26.475, Speaker A: Compare Exchange, we're going to go from unlocked to locked. Still just ordering relaxed because we don't know what this means. And in fact, now the store goes away. I realize this is maybe hard to see. There we go. I wish it would format this differently. In fact, maybe I'll do that here just so that it's easier to follow what's going on.
00:42:26.475 - 00:43:45.783, Speaker A: Okay, so Compare Exchange takes what the current value should be for us to update it, and what it should be set to if the current value is what the first argument was. So what will happen here is the CPU is going to go look at the value, the atomic bool here, see if it is unlocked. So that is, if it is currently false, then and only then set it to true and do that in such a way that no other thread gets to modify the value in between when we look at it and when we change it. So compare and Compare Exchange is a single operation that does the read and the write. And notice we can do this in a loop, right? Because if the current value is locked, then the value will not be updated because the current value is not unlocked. And so Compare Exchange will return an error, and so we loop and try again and Compare Exchange will return an error if the, if the value was not updated, and it will return. Okay, if the value was updated.
00:43:45.783 - 00:44:16.527, Speaker A: And in either case it will. So the value contained in the ok, or the error is going to be the current value, whatever it was at the time when the CPU went to that memory location. So if you get an error, it's going to tell you, here's what the value was when I, when it wasn't equal to what you passed me. In our case, we don't need the actual value. All we care about is whether it was updated. So hence the call to iserror here. But if you have something like an atomic usize, for example, it might actually matter what the old value was.
00:44:16.527 - 00:45:06.915, Speaker A: Like, imagine you're updating a counter, right, and you're trying to increment it by one. Then the next time around the loop, you want to use the updated value to do the increment rather than the value you've read previously. All right, so does this make sense? Does it make sense why Compare Change solves this particular problem? Because here there's no space in between the load and the store. They're just one operation, one atomic operation that's performed on the memory location we're operating under. Now, in practice, you don't actually want to do it this way. And there are two reasons for that. First, this means that we are going to try to do.
00:45:06.915 - 00:45:59.751, Speaker A: Well, Compare Exchange is a fairly expensive operation because if you think about it, if every CPU is spinning during Compare Exchange, they're all going to try to get sort of exclusive access to the underlying memory location. And in practice, what that means is imagine you have like eight cores, right? And one of them is currently holding the lock. All the other threads are trying to get exclusive access to the value that's sort of, that holds the true or false. And so what they're going to do is each, each core is going to say, give me exclusive access of this value. Which is sort of a coordination effort, right? It needs to coordinate with all the other cores to say, I now own this memory location to make sure no one else is writing it at the same time. And then it's going to look at the value and it's going, oh, it's not the current value. And then some other core is going to say, now give it to me.
00:45:59.751 - 00:46:58.789, Speaker A: And then just the, that memory location is sort of going to bounce between the cores, rather the ownership of that location and memory is going to bounce between the cores. And this is very inefficient. CPUs are not, it's not that they're not great at it, it's just inherently an expensive proposition to coordinate exclusive access amongst all these cores. If you're curious about this, I highly recommend you look up the messy protocol, which explains a lot about like how this actually works on the hardware level. It's a super nice protocol to know about if you want to understand sort of performance implications at this kind of low level. So the messy protocol basically says that a given location in memory, it's actually talking about cache coherence and cache lines, but I'm going to refer to it as location in memory. A location in memory can either be shared or exclusive.
00:46:58.789 - 00:47:52.669, Speaker A: There are some other states too, but basically shared exclusive. So in compare exchange, the CPU requires exclusive access to that location in memory, which requires coordinating with everyone else. Alternatively, a given location in memory can be marked as shared and multiple cores are allowed to have a value in the shared state at the same time. So in general, if we can have a value stay in the shared state while the lock is still held, that's going to avoid all this sort of ownership bouncing. And so what you'll see in certain spin lock implementations is another inner loop which does this. So notice that this doesn't actually change anything, like the behavior is still the same. It's just that if we fail to take the lock, then we're just going to read the value.
00:47:52.669 - 00:48:46.313, Speaker A: So notice that this doesn't do a compare and swap, it doesn't require exclusive access, it's read only access. So if we fail to get the lock, then we're going to spin and just do reads, which allows that value to Stay in the shared state, which means that we don't have all this ownership bouncing. And then the moment it changes because some core takes exclusive access to it, probably because it's doing an unlocked store, only then do we go back to doing the compare, the expensive compare exchange where we try to get exclusive access. If we fail to get the lock, then we fall back to doing this read only loop. So this is actually a fair amount more efficient when you have high contention. But again, don't use a spin lock. Do you think the performance degradation would be visible if you just redid your test? Again, no.
00:48:46.313 - 00:49:34.231, Speaker A: These optimizations are. You're talking like nanoseconds. They only really matter if you have a lot of contention on a particular value. And when you have a lot of threads, a lot of cores are doing that contention. So you're usually going to see these kind of things show up in. If you plot the performance like throughput or good put, usually by the number of cores in a graph, what you'll see is if you have a lot of contention, like everyone is trying to get exclusive access, then as the number of cores goes up, the good put starts to sort of flatten out or even go down because the course are spending all of their time just trying to like fighting over who gets exclusive access to the value. Whereas if you do something like this where you.
00:49:34.231 - 00:50:24.089, Speaker A: You sort of avoid that collapse, so you might still not see linear growth, which is like if you double the number, of course you double the good put, but you will see at least usually see a better curve because you avoid some of this performance collapse is compare exchange then much faster than locking a mutex. It's hard to say a single compare exchange is not that expensive. The biggest difference between compare exchange and a mutex is that a mutex has to wait. A compare exchange will never wait. Right. What will? A single compare exchange call is going to go and try to do the operation you told it and then it's going to say either I succeeded or I failed. So it doesn't like if it failed.
00:50:24.089 - 00:50:42.155, Speaker A: It's not like it blocks the thread or anything. You can choose to do that. Like in this case we spin. So we are sort of holding up and waiting for that to succeed. A mutex, on the other hand, if you lock it, then your thread will be blocked until you have the lock. This is sort of the difference. Think of compare exchange as a much more primitive operation.
00:50:42.155 - 00:51:16.361, Speaker A: In general, what you'll see is that compare exchange is often but not always used in a loop. So like keep Retrying with some updated current value until you succeed. Very often, although not always, there are some algorithms where if you fail the compare Exchange, there's some other useful work you can do. So you go do that and then you try the Compare Exchange at some other later point in time. But. And this is where weak comes in. So remember we looked at the documentation.
00:51:16.361 - 00:52:00.015, Speaker A: There's Compare Exchange and then there's Compare Exchange Week. The difference between these is fairly subtle, but basically it comes down to Compare Exchange is only allowed to fail if the current value did not match the value you passed in. It is not allowed to fail under any other conditions. CompareExChangeWeek is allowed to fail spuriously. So that is, even if the current value is what you passed in, it's allowed to fail. It usually won't, but it's allowed to. The reason this matters is because there are some.
00:52:00.015 - 00:52:31.005, Speaker A: What's the best way to explain this? It comes down to what operations the CPU supports, so on. Like intel x86, there is like a compare and swap operation. It's not technically called that, but effectively there's an operation that implements compare exchange. It does exactly that. One operation. It's one atomic instruction. On ARM Intel, I guess slash AMD.
00:52:31.005 - 00:53:23.343, Speaker A: Really, I guess x86. On ARM though, you don't usually have a compare and swap operation. Instead what you have is a. Oh, what's it called? You have like a locked LDRAX and str. So this is load exclusive and store exclusive. And what? So you have two operations and load exclusive you can think of as takes exclusive ownership of the location and memory and loads the value to you. And store exclusive is only if I still have exclusive access to that location, that is no one else has taken it away from me.
00:53:23.343 - 00:54:34.723, Speaker A: Only then will I store and otherwise I'll fail. And you could imagine that some other thread took ownership of the value, for example, just to read it, or to overwrite it with the same value that's already there. In that case, the store exclusive operation on ARM will fail, even though the value might still be the same, but it will fail nonetheless. The upside of doing it with these two operations is that the StoreX is really cheap, right? Like you don't have to then go and grab exclusive access, but it might mean that you fail the operation without needing to. So on arm, Compare Exchange is actually implemented using a loop of LDREC and STRX because it needs to implement the semantics of compare exchange, which is only fail if the current value stayed the same. And this means that on ARM processors, this loop ends up being a nested loop. And nested loops are.
00:54:34.723 - 00:55:14.015, Speaker A: They're not inherently a problem, but they tend to generate less efficient code, they generate more Registry pressure and stuff. So this is why we have Compare Exchange Week, where Compare Exchange Week is allowed to fail spuriously. And so it can be implemented directly using ldrx and strx. And of course on x86 64, compareexchangeweek is just a compare and swap. So it doesn't have to like, it just always works. It doesn't generate spurious failures. And the idea is that if you already call it in a loop, you should use Compare Exchange Week.
00:55:14.015 - 00:55:51.367, Speaker A: If you are not calling it in a loop if you're expecting it to. If you're. If you wanted to only fail if the operation, if the current value had changed, then you use Compare Exchange. So in this case, because we're calling it a loop, this should be weak. All right, did that make sense before we continue? Oh, it's called Load Linked and Store Conditional. That's what these operations are. Good call.
00:55:51.367 - 00:56:40.377, Speaker A: ARM 64 does have compare Swap, but there are a bunch of other ARM variants that do not. Let's see. Okay, so that. That seems like it was fairly clear as to why, well, what the difference between Compare Exchange and Compare Exchange Week is. Okay, so I guess let's leave that comment up here. Stay in S when locked, just sort of to keep the code commented is that all the conditions when weak is allowed to fail, Weak is allowed to fail spuriously. So a weak can fail for all sorts of reasons.
00:56:40.377 - 00:57:34.419, Speaker A: It will only succeed under the one condition you expect it to, but it can fail for whatever reason it feels like, whereas Exchange can only fail if. Sorry, the non weak version can only fail if the current value changed. All right, so so far so good. We're now at the point where we have something that looks like it can't have this problem. And in order to test that, I guess we could put like a thread yield now in here and maybe one in here too, and then try to run it again. All right, so nothing fails. Okay, so now clearly our code is correct.
00:57:34.419 - 00:57:48.735, Speaker A: Right, Ship it. It's all done now. We fixed the race condition. There are no more problems, of course. No, that's not true. I wish it was, but it is not. And the reason comes down to this ordering business.
00:57:48.735 - 00:58:43.755, Speaker A: Let's see, do we want to do ordering first or fetch first? Let's do ordering first. Yeah, let's do ordering first. Okay, so as we discussed, there is this ordering Enum. And you see that there are a couple of different variants here. There's relaxed, release, acquire, acquire release, and sequentially consistent. And these, as I described before, basically dictate the allowed observable behavior. When you have multiple threads that interact at some memory location, they sort of dictate what's supposed to happen if or what is allowed to happen when you run this code.
00:58:43.755 - 00:59:24.535, Speaker A: Okay, so ordering relaxed means that there are no guarantees. And when I say no guarantees, that's not quite true. Like, you're still guaranteed that the operation will happen atomically. Like, if you do a load, you can't like see some bits that were stored by one store and some bits that were stored by another store. Like, the operation is still atomic. But that is the only thing that's guaranteed. And to demonstrate just how extreme that is, I'm going to give you a little fun test case.
00:59:24.535 - 01:00:06.805, Speaker A: Let's do relaxed. Too relaxed. So here's what's going to happen. I'm going to spawn two threads. I'm going to say X is box leak. Box new atomic, I guess Usize atomic view size new zero. The reason I specifically give the type here is because.
01:00:06.805 - 01:00:27.045, Speaker A: Oh, I can't. There we go. It's because box leak returns a static mutable reference which I couldn't move into two threads. So this is my way of casting it. Basically Sync, atomic, atomicize. I guess I can have it a test. It doesn't really matter.
01:00:27.045 - 01:01:31.985, Speaker A: I'm gonna have an X and Y that are both numbers and I'm gonna have two threads I can't spell and one thread is going to do. Let's call this thread 1 and this thread 2. Thread 1 is going to read Y with relaxed ordering. And then it's going to store that value into X. And I guess we can have it return R1. T2 is going to load X and then it's going to store 42 into Y. And then we're going to do R1 is T1 join unwrap.
01:01:31.985 - 01:02:21.066, Speaker A: So we're going to join both the threads so that we have the values T2. All right, so we have two threads and one thread is going to read Y and store X. The other one is going to read X and store to Y. And the surprising bit that I'm going to tell you here is that it is possible to have R1 be equal to R2 be equal to 42. This is a possible execution of this program. Let's see why that's weird. So here R2 loads x and then it stores 42 somewhere.
01:02:21.066 - 01:03:04.945, Speaker A: That's what thread 2 does. And I'm telling you it's possible for R2 to be 42, even though the store of 42 happens after R2 is read. This should be extremely surprising to you, but it is possible with ordering Relaxed. And the reason is because when you have multiple threads executing concurrently, by default, there are no guarantees as to. That's not quite true. Close enough. There are basically no guarantees about what values a thread can read from something another thread wrote under Ordering Relaxed.
01:03:04.945 - 01:04:13.595, Speaker A: So even though you might think, well, either this hap like this happens before this, so how is it possible to have them happen the other way around? The answer to that is in atomic operations generally what happens is that there is a modification order that's stored per value. So this thread might see how we can explain this. So for there's sort of the there's the modification order for Y and there is the modification order for X. Let's make this the other way. So for Y, the modification order is that it is 0 and then it's 42. For X, it's that it's 0 and then it's 42, right? When you load a value, you can see when you load a value with ordering relaxed, you can see any value written by any thread to that location. There's no restriction on when that has to happen relative to you.
01:04:13.595 - 01:04:58.619, Speaker A: The only restriction is if there's a synchronization point between the two of you. And the spec talks about this in terms of happens before relationships. We'll get back into those in a second. But for ordering Relaxed, there is no guarantee that just because you read or write the same value, there's like a. There's no guarantee that you happen after or before relative to some memory ordering. So in particular, this load of X is allowed to see any value ever stored to x that includes 42 because it's in the modification set of X and there are no constraints on which subset of that. Which.
01:04:58.619 - 01:05:35.229, Speaker A: Which range of that modification order is visible to this thread might seem like time travel. The other way to think about this is that the compiler is totally allowed to reorder these two operations. It's maybe an easier way to think about it. Similarly, the CPU is allowed to execute them out of order, right? Just for optimization purposes. The reason it's allowed to do that is because there's no. There's no sequencing operation between the two. Like, this operation does not depend on anything from this operation.
01:05:35.229 - 01:06:23.315, Speaker A: It doesn't use R2, it doesn't access X. This one doesn't write into Y. So there's nothing that strictly requires them to happen in this order. You might wonder, well, why doesn't the CPU just CPU and compiler just run them in order? And the reason is because very often there are significant performance gains from sort of rejiggering the execution of a program. If you run the operation slightly out of order, you might get much better performance, you might utilize memory better. And so under either of these conditions, the reverse execution might happen. And if the code looks like this, it's totally obvious why R2 might be 42, right? And with ordering relaxed, there's nothing that guarantees that this won't happen.
01:06:23.315 - 01:07:29.955, Speaker A: Think of it as relaxed does not establish an ordering between these two operations or between this thread and this thread, and therefore R2 is allowed to see 42. Does that make sense? It's weird. It doesn't make sense. I know, it's really weird, but if you think about it again, this reordering makes a lot of sense. Like if you were a programmer, like imagine you, you were working a huge code base and like you don't know about these other threads, you only. You're only looking at this in like a 10,000 line long file, right? Would you ever think twice about swapping these two lines? They don't depend on each other, so why not just swap them? Like, is there a downside to it? Probably not. But of course the observable execution outcomes are different, or in the case of relax, they're not, because these are interchangeable as far as the memory ordering specification is.
01:07:29.955 - 01:08:22.425, Speaker A: Yeah. The other way to think about this is there's a sort of speculative execution as well, of the CPU is allowed to run this operation before it runs this operation sort of speculatively, because it knows that it's about to be executed. And in this case there are no branches or anything. So speculative execution usually comes up in if there's a question whether it might be executed. So if there was like an if here on something that's not related to R2 or X, like Z is equal to 3, right? This is where you run into speculative execution, where the CPU doesn't know whether or not this is true yet, but it still executes this operation in a way that it can later undo, just so that if the condition is true, it's already started the operation. So that's where like Spectre and Meltdown come in. But in this case there's no.
01:08:22.425 - 01:09:10.903, Speaker A: It's not really speculative execution, it's just out of order execution, which today's CPUs are very good at because it's necessary. It makes programs much faster. Wait, so when I write code, the CPU can take any instruction that does not depend on others and start by that? Yes, sort of. So there are a bunch of constraints as to what reordering the CPU and compiler are allowed to do and what they're not allowed to do. But in general, the they're allowed to reorder anything that doesn't have a clear sort of happens before relationship, which is the formal way to specify this. Like, if there isn't a dependent. Think of it as like, if your program is like a graph of dependencies, you're not allowed to reorder.
01:09:10.903 - 01:09:54.054, Speaker A: Like if A depends on B, if A depends on B, you're not allowed to reorder them because A depended on B. But if there's no relationship between A and B, if there's no dependence relation, if there's nothing that says that B has to happen first, then they're interchangeable and that's what the specification gives it. And this is also where it becomes important that memory ordering and memory semantics are not just about the CPU or the architecture. They're also about what the compiler is allowed to do. Right. It could be that the compiler reorders these, or it could be that the CPU executes out of order and they're equivalent. Like, as far as we're concerned, all that matters is whether it's legal for them to be reordered.
01:09:54.054 - 01:10:58.385, Speaker A: Yeah, and so one of the reasons this might happen, right, is it might be that we already have Y in like we have exclusive access to Y already, but we don't even have shared access to X. So the load from X is going to have to wait for the memory system to do something. But we can do the store right now because we already have it exclusive. So we're going to do that first because we can do it efficiently. Okay, so this is an example of the kind of shenanigans that can happen when you're using relaxed, because relaxed just doesn't imply any ordering. It doesn't imply that anything happens before anything else. And why does this matter? Well, here, remember, we're taking the lock with relaxed.
01:10:58.385 - 01:11:36.039, Speaker A: And so here's an example of something that might happen. So let's imagine that instead of F, this just did this, Right? It can't do that because we would have to specialize to U size. But let's imagine that it just like directly did the plus equals one. Great. Okay, well, this is relaxed. So this can just be moved up Here, this operates on self locked. This operates on self.
01:11:36.039 - 01:11:55.921, Speaker A: V. There are different parts of memory, There are different locations. There's nothing that says that this reordering is not allowed. We know it's not around. Like, this is not okay. If these execute out of order, this is really bad. Now this operation is executing while some other thread might be holding the lock.
01:11:55.921 - 01:12:21.275, Speaker A: And we're violating the exclusivity of a mutable reference. Right? This is bad. Not okay, not okay. Compiler not okay. Cpu. But based on the ordering we gave, this is fine. Okay, so how do we fix this? Well, this is where we have to use a different memory ordering than relaxed.
01:12:21.275 - 01:13:06.157, Speaker A: Relaxed is too weak. So let's then look at what are the alternatives? Well, if we go back to ordering, we see that the next thing sort of up from relaxed is release and acquire. And these sound a lot like the terms we use for locks. You acquire a lock and you release a lock. And that's no accident. It's because these memory orderings are basically designed for being used in the context of acquiring or releasing a resource. Okay, so let's leave the compare exchange weak for a second and first talk about the store.
01:13:06.157 - 01:13:32.071, Speaker A: So here's another example of a reordering that's valid with relaxed. This can just move down there. Why not? Totally fine. All good. No fires here, right? But of course, no, this is not fine. We don't want to allow this. So it's not just the acquiring of the lock that we need to ensure that things don't move above, but it's also the release of the lock.
01:13:32.071 - 01:13:58.763, Speaker A: We want to make sure that nothing moves below. And this is where the release ordering comes in. So let's switch back to the spec. This is going to be a little bit bright, actually. Maybe it'll say here. Yeah, okay, let's use this instead, because it's dark mode. So for the release ordering, when coupled with a store, all previous operations become ordered before any load of this value.
01:13:58.763 - 01:14:59.229, Speaker A: With acquire or stronger ordering in particular, all previous writes become visible to all threads to perform acquire load of this value. And this ordering is only applicable for operations that can perform a store. There's a lot of language there, and all the language is important. But basically what this means is that if we do this store with release, then any load of the same value that uses the acquire ordering must see all operations that happened before the store as having happened before the store. There's an additional restriction. If we go back to the memory order, this will be bright. So if we look at memory order Release a store operation with this memory order performs the release operation.
01:14:59.229 - 01:16:03.355, Speaker A: No reads or writes in the current thread can be reordered after the store. So this is an additional sentence that actually not in the Rust version, which seems unfortunate that it should be in there. But basically the second part we saw is nothing can be reordered to after a release store. Great, so that's solved. The other thing is that as you noticed in the documentation, it said all previous operations become ordered before any load of this value with acquire. So that is the load that happens as part of this compare exchange if it uses acquire ordering must see any operations that we did in here. This might sound weird, like why does that part matter? Well, if this is relaxed, then the next person to take the lock is not guaranteed to see the modifications we made to memory here.
01:16:03.355 - 01:17:00.975, Speaker A: This comes back to the stuff we talked about down here with relaxed, right? Is that this thread, this operation might see values that like it. In fact, if we reorder them, this is allowed to see zero for obvious reasons, right? We do a store to Y, but we load from X. This thread might have not have run them in the meantime. But there are more extreme cases too where it just like it might be that this thread just will not see 42 in the time that it executes. Even if we like looked at the wall clock time and saw that yeah, T1 ran after T2, it might still see zero, because that's all ordering relaxed guarantees. And so in our case with the mutex, if we keep this as relaxed, then the modifications we made to memory here may not be visible to the next person who takes the lock, which is a huge problem. Right? This is not okay.
01:17:00.975 - 01:18:06.753, Speaker A: So this needs to be release this for now, let's say this has to be acquire. Now we have the two guarantees we have. This cannot be reordered after this, and anything we do in here must be visible after an acquire load of the same value. So whoever next takes the lock must see anything that happened before this store. The way to think about this is that the acquire release pair establishes a happens before relationship between the thread that previously released the lock and the next thread that takes the lock. And the happensbefore relationship establishes that anything that happened before the thing that did the store also happens before anything that happens after the load. And there's an additional part to this which is if we go back to the bright spec again, acquire says a load operation with this memory order performs the acquire operation on the effective memory location.
01:18:06.753 - 01:18:42.215, Speaker A: No reads or writes on the current thread can be reordered before this load. So this cannot be reordered before this load, which is the other property that we needed. Nice. So this has to perform release. This has to perform acquire. There is. So there's also acquire release, which is written as ack rel.
01:18:42.215 - 01:19:24.825, Speaker A: So acquire release, when you pass it into an operation that does a read and a write, like compare exchange, right? It reads the value, but it also writes the value. The acquire release says do the load with acquire semantics and the store with release semantics. In our case, the question becomes, do we care whether the store is release or not? So the store in this case is storing that. We now hold the lock. And here we don't actually need that to have release semantics. The release semantics are established down here. Acquire releases is more commonly used when you do a fetch ad, for example, or some kind of.
01:19:24.825 - 01:19:54.535, Speaker A: We'll get to what those mean in a second. But acquire release is usually used when you're doing a single modification operation where there's no critical section like you're not going to do a later store or anything. You're just doing one operation that you want to synchronize with other threads. So in our case, this can be acquire. All right. Does this stuff so far make sense? I'll get to this last argument in a second. Let's see.
01:19:54.535 - 01:20:43.731, Speaker A: Is it only acquire ordering or acquire and stronger? It's acquire and stronger. So sequential consistent. Sequentially consistent ordering is acquire and release and more. Great. So it seems like what we went through so far makes sense. So the question then becomes, what is this extra parameter for compare exchange and compare exchange week? The extra parameter is. Imagine that the compare exchange looks at the memory and goes, this value has changed.
01:20:43.731 - 01:21:30.861, Speaker A: So I didn't do the store. This ordering is what ordering should the load have if the load indicates that you shouldn't store? In our case, when taking the lock, you can think of this as what is the ordering of failing to take the lock? You could imagine cases where if you fail to take the lock, you still want to establish a happens before relationship. The cases where you need this are pretty rare, but they do exist. In our case, we don't need that. If you fail to take the lock, that doesn't mean that you have to now like synchronize with the thread that last released the lock or the thread. Yeah, the thread that last released the lock. That's not important.
01:21:30.861 - 01:22:22.683, Speaker A: All that matters is the moment you do get the lock, you have to establish sort of a sequential relationship, or rather a happens before relationship between the thread that last held the lock and yourself, because you're about to operate on the stuff in there. So here we can keep this relaxed. The reason this matters is because if you fail to take the lock, you don't want to then have to do sort of coordination with the other threads or the other cores rather to get like exclusive access is not important. Great. Okay, so hopefully this now makes sense. Like now we have a working example of a spin lock. Spin locks never work, but it is a working example of a spin lock.
01:22:22.683 - 01:23:26.897, Speaker A: This in fact, I will tell you, is I believe, sound and correct implementation. Not one you should use, but a correct and sound implementation. As you might wonder, well, why didn't these problems show up when we ran this with relaxed? Right, when this was relaxed and this was relaxed, like we ran the test, we ran the binary, we did lots of iterations with lots of threads and lots. Of course, why didn't anything fail? Why did we still observe the sort of correct output? And the reason for that is because my machine is an X8664 machine. And on X8664 the architecture basically guarantees that acquire release semantics for all operations. It's a fairly. It gives fairly strong consistency guarantees by default, that is you can't opt out of it.
01:23:26.897 - 01:24:01.845, Speaker A: It's just the memory architecture, the CPU architecture is such that all operations are guaranteed to be acquire release. That's not true for all platforms on arm, for example. That is not generally true. If you specified ordering relaxed, you will get relaxed memory semantics. And so this is one of the problems with trying to sort of test concurrent code by just running it lots of times. And that is you're still only testing it on your current hardware and your current compiler. Sort of like with undefined behavior like we've talked about in past streams.
01:24:01.845 - 01:24:46.659, Speaker A: It's just you can rely on the current state of affairs for you being representative of future executions. It gives you some idea like our previous implementation, which was like obviously wrong. The one with the load and store separate, that one did break. But just running it lots of times is just not sufficient to do testing of this kind of concurrent code. We'll talk about how you might do something much better a little bit later on in the stream. This load can stay relaxed. Yeah, because here too it doesn't.
01:24:46.659 - 01:25:39.635, Speaker A: We don't need to establish any happens before relationship here because we haven't taken the lock yet. So this can still be relaxed. In general, the cases where you want to use relaxed is when it doesn't matter what each thread sees. So for example, if you do something like maintain a counter, like for statistics or something, you can generally have it be relaxed because if one thread doesn't see the increment of a different thread, it doesn't really matter. If the counter gets updated slightly earlier, slightly later in relative to execution order, it doesn't really matter. And so there Relax is great because it imposes the least amount of restrictions on the CPU and compiler. And so they can execute, the compiler can generate more efficient code and the CPU can execute the instructions more efficiently.
01:25:39.635 - 01:26:37.475, Speaker A: But for anything where the ordering between threads and the relative ordering of instructions matter, you might have to look at stronger orderings. Okay, so now that we've talked about acquire release, let's look at in combination the fetch operations and sequentially consistent ordering. So, so let's first look at fetch. The fetch operations, the. Oops. Okay, so if you look down here, you'll see that there are a bunch of fetch operations in addition to load, store and compare, exchange. So it's fetch, add, fetch, sub Fetch and Fetch, NAND, Fetch or etc.
01:26:37.475 - 01:27:33.559, Speaker A: And if you think of like, you can think of load and store as sort of being the like bread and butter, the nuts and bolts, like the, the very low level primitive that you can do other things with. And then you can think of compare exchange as like the sledgehammer, like do this or don't do it, and do it atomically. And there's nothing like it's either set this value to this value or tell me if the value changed. The fetch operations are gentler versions of a sledgehammer, like a mallet or something, rubber mallet. More concretely and more helpfully, perhaps the fetch operations are. Instead of just saying if the current value is this, set it to this. So imagine something like you read the current.
01:27:33.559 - 01:28:27.083, Speaker A: Imagine you have a counter or something, read the current counter and if the. And then you do a compare exchange of the current counter value and the current counter value plus one, which will fail if the counter has been changed. In the meantime, you can do a fetch add, which is sort of like instead of saying what the new value should be, you tell the CPU how to compute the new value. So that way a fetch ad will never fail again. For the counterexample, the fetch add, you say do a fetch add of this value, add one, and then the CPU is going to go to, it's going to get exclusive access to the value. It's going to read the current value, and regardless of what the current value is, it just adds one to it and stores it Back. This means that fetch ad just doesn't fail.
01:28:27.083 - 01:29:26.747, Speaker A: It doesn't matter what the current value is. The new value will just be set to one plus whatever that was. And the reason it's called fetch add or Fetch sub and et cetera is because it also tells you what the value was when it incremented it. So this again is to get at the race condition between doing the load and the store, where fetch assumes that you care about what the current value was. You can throw that value away, that's not important. But if that, there's no other way that you could learn which value you just incremented, right? If you did, if you just had like an atomic add operation, you couldn't combine that with a load to figure out what value, what the value was that you incremented to or from. Because if you did a load and then an atomic increment, then there's a slight space in between there where some other thread could sneak in.
01:29:26.747 - 01:30:21.095, Speaker A: And similarly, if you did like the add and then a load, there's also a space in between. So fetch add is a single atomic instruction, but instead of just dictating what the updated value should be, you tell it what the operation should be. This is perhaps best illustrated by the fetchupdate method, which is a little bit of like an ugly duckling. So fetchupdate takes a closure that is given the current value and should return the new value. So think of this as like the sort of generic version of Fetch add and Fetch sub and stuff, where you could imagine fetch add being like Fetch update with a closure that adds one to its argument. The reason I say the fetch update is like a little bit ugly and weird is because the CPU has built in support for doing add atomically or doing subtraction atomically or. And atomic like bitwise and atomically.
01:30:21.095 - 01:31:05.245, Speaker A: It doesn't have this for an arbitrary closure. And so really what fetch update is, is a compare exchange loop that's implemented for you. So if we look at the source for fetch update. Oops, that's not at all what I wanted to do. I just made my browser full screen and that did not interact well with my streaming setup. Fetch update. Okay, so fetch update internally loads the current value and then does a while loop and does compare exchange in a loop.
01:31:05.245 - 01:32:02.945, Speaker A: So all it really is doing is doing the CompareExchange loop for you. And you'll notice that they correctly use CompareExchange weak because it's in a loop already. Right. So this ties back to what we talked about earlier, and the difference between non weak and weak CompareExchange. But this is why I say the FetchUpdate isn't really the same as the others, because in general a fetch add will just be a single atomic instruction, whereas fetchupdate is actually a compareexchange loop. The reason why fetchupdate still sort of fits here is if we go up and look at the documentation for atomic, you'll see here under portability, all atomic types in this module are guaranteed to be lock free if they're available. So for example, like if the platform supports atomic access to U64s, the atomic U64 type will be available.
01:32:02.945 - 01:32:53.665, Speaker A: If it doesn't, it won't be. This means they don't internally acquire a global mutex. Atomic types and operations are not guaranteed to be weight free. This means that operations like fetch or, or fetch add may be implemented with a compare and swap loop. So the idea is that if the architecture you're compiling for doesn't support an atomic increment, for example, then the standard library on that architecture implements fetch add with a compare and swap loop or compare exchange loop. So in other words, if you call fetch add because you want to avoid compare exchange, that is the right thing to do because it means that you get to take advantage of the atomic increment operation if it exists on your architecture. But there might be architecture where it ends up being a compare exchange anyway.
01:32:53.665 - 01:34:21.365, Speaker A: Okay, so now hopefully you understand what fetch the fetch operations are for. In general, these are used for things like if you want to give like unique sequence numbers to a bunch of operations that happen concurrently, then rather than like have a lock which is like next sequence number, you take the lock and then you read the sequence number, you increment it, and then you release the lock, you just do a fetch add on an atomic use size instead. And that guarantees that every call to get a sequence number will get a distinct one, because every increment will happen and the fetch will ensure that you read the value that was there when you updated it. So no thread will read the same value if every thread increments by one, for example. And it will generally be a fair amount more efficient than doing certainly a Mutex, but even a compare exchange loop on platforms to support it. Okay, that then brings us finally to sequentially consistent ordering. And this is going to be another instance of head explosions, maybe.
01:34:21.365 - 01:35:02.589, Speaker A: And here we're going to need a different example because our lock is now correct. The example I'm going to give you here is as follows. This is mutextest. Yeah, yeah, I'll do that test. So here's what I'm going to do here. I'm also here going to have a Sorry. While I move around sync.
01:35:02.589 - 01:35:43.665, Speaker A: Atomic Atomic. These are going to be atomic bools. They're going to start out false and then Z is going to be atomic use. And here, watch this. This is. This is fairly involved to demonstrate the difference between acquire release and sequentially consistent. This is also the same example as used in the C memory order page, except sort of translated into Rust.
01:35:43.665 - 01:36:32.575, Speaker A: So we're going to have one thread that stores true with ordering release. So remember, release is for stores and acquire is for loads. We got to have another thread that is a store to Y. We're going to have one thread that does while not X load ordering acquire in a loop. And then it checks if Y again will acquire. Then it's going to fetch, add 1, 2, Z. And this one is going to be relaxed.
01:36:32.575 - 01:37:19.495, Speaker A: We're going to call this T1. And then we're going to have a T2 that does the same thing, except in reverse order. So T2 is going to spin until Y becomes true. And then if X is true, then it's going to add one to Z. And then down here we're going to wait on T1 and we're going to wait on T2. And now the question becomes Z is going to be Z load ordering. And here I'm just going to use sequentially consistent for uninteresting reasons to think about this.
01:37:19.495 - 01:38:00.485, Speaker A: But I just want to read whatever the final value of Z is. So I'm going to do it this way. This ordering you can ignore. And the question is, what are the possible values for Z? Which essentially boils down to is zero possible? Oh, what did I do? Every now and again I find weird vim bindings. Is 0 possible? Is 1 possible? Is 2 possible? More than 2 should not be possible because there are only 2 increments. So that would be insane. So we're gonna assume that's not possible.
01:38:00.485 - 01:38:42.419, Speaker A: Let me zoom out a little bit so we can fit that whole thing on screen. Hopefully that's still legible. All right, so I'll give you a second to sort of consider this code and talk through it a little bit and then just check chat. In the meantime, if something. If some things are implemented under the hood with Compare Exchange, why don't they. Why doesn't everything return a result? Because Fetch add always succeeds. The fetch methods always succeed.
01:38:42.419 - 01:39:39.875, Speaker A: This is why, if it's implemented with Compare Exchange, it does Compare Exchange week in a loop until it succeeds. So fetch add will never fail, and therefore it doesn't need to return a result. Okay, so let's work through this here. And I'm going to do these from last to first. So I'm going to start with whether two is possible and then go back up. Is the Mutex panic safe? That Mutex is panics safe, but it won't propagate panics. All right, so two is clearly possible.
01:39:39.875 - 01:40:25.851, Speaker A: And just to make sure, I demonstrate why, let's call these, I guess, tx and ty because they respectively store true to x and y. So 2 is possible under the following execution tx, ty, t1, t2 if the threat. Imagine that we just had one core. The threads ran one at a time. This is possible, right? TX runs and sets x to true. Ty runs, sets y to true. T1 runs.
01:40:25.851 - 01:40:46.837, Speaker A: It immediately sees that X is true. After observing that X is true, it sees that Y is true, so it increments Z. So z is now 1. Then T2 runs. It immediately sees the Y is true. It then sees that X is true, both of which, because tx and ty have run, and so it increments z by one. So now we get z equals two.
01:40:46.837 - 01:41:38.223, Speaker A: Okay, so this is trivially possible, right? Okay, is one possible? One should also be possible, right? So we run tx, then t1, then t2, I guess. Then ty, then t2. So with this kind of execution, when t1 runs, x is true, but y is false. So t1 runs. It waits for X to become true, which it is immediately because TX already ran, it tries to load from Y. TY has not run yet, so Y is false, so it does not increment Z. Then Ty runs and Ty stores true to Y.
01:41:38.223 - 01:41:58.315, Speaker A: Then T2 runs. T2, observes the Y is true, so it immediately exits the while loop. Then it checks whether X is true and X is true, and so it increments Z by 1. So now Z is 1. Great. So 1 is a possible outcome for Z. Zero is more complicated.
01:41:58.315 - 01:42:47.789, Speaker A: So let's sort of try to work through this the same way we did one and two. Basically. Can we find some execution of threads where the outcome zero is possible? Well, we have a couple of restrictions, right? We know that T1 must run after t X. The reason we know that is if you run T1 first, T1 is going to spin until TX runs, right? It has the spin loop. So we know that T1. Like even if you run T1 first, it's just going to wait for TX anyway. So T1 is going to happen sort of after TX, it's going to be placed after.
01:42:47.789 - 01:43:20.675, Speaker A: It's going to have to execute after TX in order to complete. Similarly, we know that T2 must run after Ty, right? That must be the case. For the same reason, if Ty hasn't run and T2 tries to run, it's just going to sit in a loop. And so at some point it's going to be like preempted or something, or Ty gets to run another core. Then T2 is going to observe that. Now Y is true, so it exits the while loop and then it completes. So we have these two restrictions.
01:43:20.675 - 01:44:33.803, Speaker A: So given that what are what execution would allow z to be zero? Well, we know that tx, let's just arbitrarily pick tx goes first, right? Then at some point later t1 goes, great, so how we have this execution where we don't know where the other threads are going to run, but we know that this must be the case. Okay, now let's try to place t2 in different locations. If t2 goes first, then we know that t1. So. So this is the pattern, right? So if t2 goes here, then we know that Ty must go before that. And this execution t1 will increment C, right? Because t1 runs after X and Y have both been set to true. Okay, back to the drawing board.
01:44:33.803 - 01:45:21.605, Speaker A: Let's try another one. So let's say that T2 goes here, it goes after TX. Well, Ty still has to go before T2 is going to get to do anything useful. So if whether Ty goes here or whether Ty goes here, right? It might be either of those. But in either case T1 and T2 will increment Z, because they they're both going to go after the things that set X and Y. Okay, what about if we place T2 at the end? Well, if we place T2 at the end, I guess Ty could go here. But if Ty goes there, then TX has already run.
01:45:21.605 - 01:46:09.895, Speaker A: So T2 will T2 will increment Z. If Ty went earlier, T2 would still increment Z. So there's no place, given this restriction, there's no place we can place T2 in this kind of execution order such that one of them, one of T1 or T2 won't increment Z. So it seems impossible to have a thread schedule where Z equals zero. It seems impossible. Does this ring true so far? Does the walkthrough here makes sense? So far it seems impossible. Right.
01:46:09.895 - 01:46:53.979, Speaker A: Okay, and now this is where it gets super weird. The way we've currently Written this, zero is possible. And this may not come as a, as a huge shock, but it is possible, and here's why. So it is true that there's no thread schedule that would allow Z to be zero, but we're not bound by thinking about thread schedules. Thread schedules is just like the human desire to put things in order. But in reality, computers don't have to have a single order that things run in. You have multiple cores, and those cores can show old values, new values.
01:46:53.979 - 01:47:34.605, Speaker A: All we're subject to are the exact rules of acquire and release semantics, which is what we've given here. So if we go back to the modification order of X, right? So we talked about how there's like a modification order for each value. So modification order For x is 0 as false and then true. The modification order for Y is similarly false and then true. So let's look at what happens when T1 runs. Okay, so T1 runs and it observes that X is true. Great.
01:47:34.605 - 01:48:14.533, Speaker A: So we know that T. So here's a valid execution. T1 is going to observe this. It's going to observe true from the modification order of X. And so what we know a couple of things. That means we know that, remember from the documentation of acquire and release, it said that if you observe a value from acquire, you will see all operations that happen before the corresponding release store. Well, the corresponding release store is here in this thread.
01:48:14.533 - 01:49:18.055, Speaker A: There are no operations prior to the store, but if there were, we would be guaranteed to see them here because we're, we're synchronizing with tx, but there are none. When we here get to the load of Y, this acquire synchronizes with whichever store the value it gets stored. There's no requirement that that's any particular store of Y. If there had been a store of Y in this thread, if this did Y store, we must observe that Y store because it happened strictly before this store, which happened before this load because of acquire release. So if there was a store here, we must see it, but there isn't. So we're allowed to see any value for Y. That means we're allowed to see this Y or we're allowed to see the Y that was stored here, regardless of whether Ty has run or not.
01:49:18.055 - 01:50:08.125, Speaker A: Even if Ty has already run, sort of, strictly speaking, in wall clock time, it doesn't matter. The memory system is allowed to still show us false, that is T. Y is T1 is bound to get true from the modification over order of X. But it's allowed to see either of these regardless of whether T Y is run. And the same thing applies to T2, right? This load synchronizes with this store. So after this load we must see all memory operations that happened before this store to Y. There are none.
01:50:08.125 - 01:50:21.887, Speaker A: Great, we're done. No requirement that we see anything that happened in here. Sorry, in. In here. No relationship to that thread. There is technically. Actually, I'm lying a little bit here.
01:50:21.887 - 01:50:55.555, Speaker A: So when you spawn a thread, these threads all happen after the main thread that spawned them. So we must actually see this false. It's not like we could see a value written somewhere else independently. But we must see this false or anything that happens later. We can't see. Imagine that this thread did like X store true. Then the loads down here must see the store because it happened before them, because this thread spawned this.
01:50:55.555 - 01:52:03.325, Speaker A: So when T2 runs, even though it will synchronize with this thread and therefore must see must see any previous writes here, there's no requirement that it sees any particular operation that happened to X, because there's no happens before relationship between the store here in TX and the load down here. There just isn't any. This is not about reordering, right? So remember how the acquire loads says that you're not allowed to move any operation from below to above an acquire load. So the compiler is not in the compiler or CPU are not allowed to reorder these. Not allowed Right, by the acquire semantics. So that's not what's going on, it's just that this load is allowed to see any previous value for X subject to happens before, which does not include the operation of Ty. So therefore T2 must see this.
01:52:03.325 - 01:52:35.289, Speaker A: But it can see either of these. I guess if I get rid of this, it might be clearer. So T2 can see either of these and T2 can see either of these, and that's still valid. And if that happens, then you could imagine that both of these threads run. So Both TX&TY run. Then T1 runs. It observes X being true.
01:52:35.289 - 01:53:00.851, Speaker A: It does not observe Y being true, even though T is run, because there's no happens before relationship there. Imagine that it's already in cache in the CPU or something. It just uses that value. It doesn't bother to check again because it's allowed not to. Therefore this value is false even though Tyran. Therefore it does not increment one this one for some reason. Like it spins until Y is true.
01:53:00.851 - 01:53:49.635, Speaker A: So let's say that it observes that Y is true. Great. It's no requirement that it observes that X is true, even though Tx is run, so it does not increment Z, so Therefore Z is 0. So this is weird, right? This is a really weird way to think. But the way to try to get at this with your brain is that acquire, release and in general, all memory ordering is about which things, which concurrent things happen before which other concurrent things. And if there's not a happens before relationship between two operations, then it's sort of up in the air whether one sees the other. The seams was a giveaway.
01:53:49.635 - 01:55:18.615, Speaker A: Yeah, you're right. Okay, so you might wonder, well, why is this allowed? Like, why do the designers of languages and memory ordering and memory systems and CPUs and compilers have this be possible? And the answer is, because if you looked at something like a mutex, this is fine, this doesn't cause any problems, and it gives the CPU and the compiler more leeway to rearrange your code. Imagine, for example, like here, X and Y are just independent variables, so why should we establish an arbitrary sort of connection between them when there really isn't one? If there was, then suddenly we're enforcing that the CPU and the compiler must execute things in order. They must get exclusive access to certain operations, and it's just technically not necessary. So it would be imposing a cost that you can avoid. Think of this as like, if you want stronger guarantees, you have to opt into them, because by opting into them, you're also opting into the cost of them. If every operation enforced sequentially consistent operation, then you would have no way to opt out of it.
01:55:18.615 - 01:56:02.735, Speaker A: You can imagine that you have a default that you can override. And that's exactly what Rust gives you, right? Every operation takes an ordering. You must provide an ordering. And so if you don't want to think about these problems, you just always provide sequentially consistent, like ordering sec cst, you can always do that. But if you do, you're also then requiring the cost that comes with that. All right, so the question now becomes, how does sequentially consistent operation change this? Well, if we go back to memory ordering, let's see what the Rust thing says. Sexist.
01:56:02.735 - 01:56:51.805, Speaker A: I don't know how to pronounce the abbreviation. Like acquire and release and acquire release, with the additional guarantee that all threads see all sequentially consistent operations in the same order. And notice that this is all sequentially consistent operations. It's not all sequentially consistent operations. This memory location, it's all. So if we go back and change this program to be sequentially consistent for all of these, we can leave the counter as relaxed because it doesn't matter if we make these all be sequentially consistent. Now zero is no longer possible.
01:56:51.805 - 01:58:15.515, Speaker A: And the reason for that is because if this thread observes that X observes X is true, and then Y is true, it establishes a happens before relationship between this load and this load, right? Some thread observed that X was set to true and then Y was set to true. And that means no thread is allowed to see Y being false, is allowed to see X being false after Y being true because some thread saw the opposite ordering. So that's what the text is trying to get at, that every thread or there must exist some ordering that's consistent across all of the threads. If some thread sees that X happened, then Y happened, then no thread is allowed to see X not happen, even though Y has happened. And so in this case, if we here end up with X is true and then Y is true, then here Y is true. Therefore X must be true. It's not allowed to see X being false because that would be inconsistent with the memory ordering that this, that these sequentially consistent operations saw.
01:58:15.515 - 01:58:57.523, Speaker A: Notice though, that sequentially consistent ordering is only with relation to all other sequentially consistent operations. So if some of these, like if this was release and this was release, this would not give us the guarantee we needed. Actually, this might. Yeah, this might. But if this was a acquire, it would not. Because here we're allowed to see X being true, and then we're allowed to see X being true here. But there's no ordering relation between these.
01:58:57.523 - 02:00:07.565, Speaker A: No sequentially consistent ordering has been seen by a thread that has X true, then Y true. So this is subtle stuff. But in general, sequentially consistent only really interacts with other sequentially consistent acquire release does interact with sequentially consistent. So sequentially consistent is always stronger than acquire release. So if you have a operation that is acquire and you do a, if you have a store that's release and acquire that's sequentially consistent, then the load with sequentially consistent will indeed be sort of happen after the release store. All right, fairly involved, but basically sequentially consistent is acquire release, with the additional guarantee that all sequentially sequentially consistent operations must be seen as happening in the same order on all threads. And therefore, in this particular example, if all of these are sequentially consistent, then Z equals zero is not possible.
02:00:07.565 - 02:00:52.685, Speaker A: All right, nice. We did that on exactly the two hour mark. Very good. I'm going to talk a little bit about testing shortly. Like how do you not make these mistakes? But I'm very happy with the timing. Okay, so as we've Seen memory ordering is real subtle. It's just like, hard to make sure you got it right.
02:00:52.685 - 02:02:23.579, Speaker A: Or rather, I think it comes down to the human brain is just really bad at emulating all of the legal executions. And as we talked about, like, you can test this by just running your code lots of times in a loop, or like, making your computer be really busy with other tasks to cause more weird interleavings to happen. But it's not a very reliable way to do this kind of testing because it might depend on the architecture, might depend on the operating system scheduler, it might depend on the compiler, it might depend on which optimizations you have enabled for that compiler, it might just depend on, like, how likely is the thing to happen? Maybe you need to execute your code, like billions and billions of time in order to get that one execution where the code is wrong. And so surely you're like, there must be a better way. And the other way to think about this is like, it might even be that there's a problem in your code, but it doesn't cause a panic, right? Like, if you think about the counter we had early on with our Mutex, where the values didn't end up adding up to 100,000 or whatever the value was, right? It added up to something slightly less. The only reason why the program crashed, the only reason we noticed, because we had an assertion that checked that the value was right at the end. But if you have a program that just like, does a bunch of operations that takes locks, just assumes that everything is right, it might not be obvious if something went wrong.
02:02:23.579 - 02:03:21.475, Speaker A: If the lock had this kind of bug in it, Two things run the critical section at the same time. Maybe you write a log entry twice, maybe the log gets truncated, maybe your backup ends up empty, right? Like, who knows what happens when you just. A Mutex just isn't a Mutex, right? It's very hard to predict. And it might not crash your program, it might not be detectable sort of immediately. And that's scary, right? But it also means that even if you ran it for like 10 years at lots of cores on lots of different hardware, right? Like, your code might hit the bug, but there's nothing that notices that you hit a bug. And these two problems of how do you make sure that you test all the possible legal executions and how do you know if something bad happened are sort of separate problems. And they're often handled a little bit differently in general.
02:03:21.475 - 02:04:07.715, Speaker A: For the second one, like, how do you detect these bugs when writing concurrent code like this with atomics you want to stick in a lot of assertions just to make sure that you did the right thing. You can make them debug assertions if you really want to, and then run your test suite with like in release mode, but with debug assertions turned on or something like that so that it doesn't impact release builds too much. But you do need to have ways to detect these problems. There are also some automated systems. So for example there's. So Google has a bunch of different sanitizers that are now built into a lot of compilers, like I think both Clang and GCC and MSVC probably others have many of these sanitizer built in. There's a nightly flag to use it in Rust.
02:04:07.715 - 02:05:13.795, Speaker A: And one of the sanitizers is the thread sanitizer. And thread sanitizer runs your code in like an instrumented way where every load and store you do in your program, like every memory operation gets tracked. Like it gets special instructions added to it to get to track what it modifies and when. And then as your program runs, the thread sanitizer in the background detects if you ever do like if you ever have two threads that write to the same memory location or if you have a thread that writes to a memory location after another thread is read for it, but there's no happens before relationship between them. So it basically tries to detect when you have unsynchronized operations on a single memory location that can't detect all problems. Like there might be logic problems you don't detect this way. And if you look at the documentation for the thread sanitizer algorithm towards the bottom here, they give like the state machine and what assertions they check.
02:05:13.795 - 02:06:32.377, Speaker A: So here sort of the algorithm specified, but there are things that aren't detected yet. There are problems you can't detect this way. And also the thread sanitizer is like you run your code and it will detect if that execution of the code did anything bad. It might be that this particular execution was fine, but you still have a concurrency bug. And this comes back to the how do you make sure that you exercise every legal execution of your program? And so that's then much harder actually to solve because you need to sort of figure out how to explore all the possible behaviors on all possible compilers, CPUs architectures according to the spec. Now the best tool I know of for this is a tool called Loom and Loom. It's a Rust project that implements a paper which that's fine, this paper called the CDS checker which is a paper written for C and C atomics, but it translates to rust pretty well.
02:06:32.377 - 02:07:19.325, Speaker A: And basically what that paper outlines and what LOOM implements is a strategy for taking a concurrent program instrumenting it. And this is not automatic instrument instrumentation. It's like use looms, synchronization and atomic types instead of the ones from the standard library. So use like the loom atomic U size, the loom mutex, the loom channels, and whatnot, use those instead of the standard library ones. And then what LOOM will do is you pass it like a test closure. And it will run that closure multiple times. And every time when you do a load through one of the LOOM data structures, it will feed you back one of the possible legal values.
02:07:19.325 - 02:08:27.443, Speaker A: When you do a store, it will sort of keep track of all the values that have been stored, so that on other loads it will expose your execution to one of those possible values. And then every execution of the closure exposes you to a different possible execution. So as LOOM executes, it will run all possible thread interleavings, all possible, like memory orderings. So when we looked at the remember we talked about modification order for each variable where T1 was allowed to see either value for Y or for x, and t2 was allowed to see either value for x. Loom will make sure that there's one execution where it sees the false value for where t2 sees the false value for x, there's one execution where t2 sees the true value for X. And in general, we'll do this for all of the operations in your program. And if we look through like, the example here is going to be pretty illustrative of how this works.
02:08:27.443 - 02:09:41.165, Speaker A: So you see here we use a bunch of the atomic types, but we use them out of LOOM instead of using them out of the standard library. The idea here is that if you're running your loom tests, you use loom primitives, and if you are building your code for release or just a standard test suite, you use the standard library primitives. There's like a bunch in the documentation. There's a bunch of explanation of how you go about doing this. But the basic idea is that if you write a loom test, everything in your test, so that includes inside the library and inside any libraries that the library uses, sort of all the way down, you want to use the loom primitives instead. And then inside your test you call loom model, and loomodel is the function that will try all these different execution paths you pass in a closure and Then inside of that closure, you write the code that you want to test, and Loom will then call that closure over and over and over and over again. And for example, down here, right, like this thread does a load with acquire, every time the closure executes, this load might get a different value.
02:09:41.165 - 02:10:24.035, Speaker A: Or you see that there are lots of threads, each time through which threads execute in which order will differ. And Loom will make sure that it totally explores all the possible legal executions. Of course, the downside of this is there might be an insane number of such possible executions. Imagine you have 10 threads that each do like 10 stores and 10 loads. You end up with 10 to the power of 10 to the power of 10. Or that's not even right, that's even a larger number than that. But you end up with testing every possible interleaving that's legal according to the memory specification that you're using, which can be insane.
02:10:24.035 - 02:11:24.025, Speaker A: So Loom is somewhat limited in that. I mean, not because of the implementation, just because of the fundamentals of there are so many possible legal executions. Loom has a bunch of implementation things that are, from the paper that we just saw, that looks at reducing those. So imagine that a thread does two different loads, but they're like, of different values. Then it doesn't matter which one executes first. Like, even though the memory model allows them to be reordered, the correctness of your program is unlikely to depend on whether those loads happen in order. And so this is a bad example, because in the case we just looked at, the load ordering might have mattered, but the paper basically has a complete specification for what executions, what possible executions cannot differ from one another and therefore can be eliminated.
02:11:24.025 - 02:12:18.671, Speaker A: Like, you only need to run some of them. Loom implements a bunch of those to try to help. But ultimately there's like a limit to how complex your test cases can be when you run them under loom. Like, you can only have like so many modifications to a given atomic variable, only so many threads, so many instructions. But even so, LOOM is the best tool I know of to try to make sure that your concurrent code is actually correct under only the assumptions that the memory model gives you, rather than the assumptions that the current compiler or optimization or CPU might give you. Let's do some questions before I do more on Loom. Are there any sort of toy programs one can think of to try and drill this into my head? Like, the spin lock was a pretty good example, but this last example seems crazy sauce.
02:12:18.671 - 02:13:25.225, Speaker A: Yes. So the example between acquire release and sequentially consistent I don't have a less contrived example for you. I think my recommendation would be to look at some papers that implement concurrent data structures and notice where they use sequentially consistent ordering as opposed to other orderings. And then generally the paper will explain why I think it's hard to come up with like a simple motivating example for one like the one I just had is the simplest one I can come up with, but it's still complex. Like, even though there's little code, the thinking and reasoning is complex. I don't have such a good concrete example where sequentially consistent is necessary. Let's see.
02:13:25.225 - 02:14:14.049, Speaker A: Yeah, so someone pointed out that the name loom actually makes a lot of sense. It spins threads in many ways, right? So loom for the cases we've looked at so far. So loom has some limitations. Some of them are known problems. Some of them are more fundamental problems with the approach or are just impossible to model. So for example, relaxed ordering is so relaxed that you can't fully model all the possible executions even with something like loom. So one example of this is in the relaxed example we have.
02:14:14.049 - 02:14:52.843, Speaker A: Let me pull that up here. So remember here where this load might see 42, which is stored by this store. Imagine how you would try to emulate that, right? Imagine you pass this into loom. This method call occurs first. So loom has to like if this is. This is a load from like a loom atomic U size, right? That load like loom has to produce some value for that load. And it doesn't know about 42 yet because it hasn't.
02:14:52.843 - 02:15:34.135, Speaker A: This method call hasn't happened yet. But the ordering relaxed allows 42 to be returned from this load. But loom doesn't know that 42 is even a value yet. So how can it return it from the load? Now if you go read the paper and it's a fascinating paper, there are ways to do this where because you know that you're executing the closure many times over, you can like execute the closure. Remember all the relaxed stores you saw the next time through return from load a value that will be returned from a later store. Continue executing. See if that store still stores that value.
02:15:34.135 - 02:16:02.459, Speaker A: And if it does, you've successfully like done the sort of reverse dependency. If it doesn't, you discard that whole execution because that race no longer happened. It's real weird. Read the paper. It's fascinating. But fundamentally, without some really fancy tricks, modeling relaxed properly is just very difficult. I will like Loom is a really cool and helpful project.
02:16:02.459 - 02:16:38.375, Speaker A: That I know. Just like there's a lot more we could do with it, I know that the project is looking for contributors. So if you're interested in this concurrency stuff, please go contribute to loom. I would be so happy to see that project do even better than what it does now. There's another example of where, like, this kind of contribution could be helpful. So LOOM currently doesn't really model the sequentially consistent ordering in that it won't enforce. So let me restart that sentence.
02:16:38.375 - 02:17:27.789, Speaker A: As we saw, sequentially consistent ordering is Acquire Release plus some more guarantees. LOOM doesn't currently model those additional guarantees. It runs sequentially consistent as if it was Acquire Release. Now, the reason for this is partially because implementing the requirements or the additional guarantees of sequentially consistent ordering is actually fairly complicated. The paper talks a little bit about this too, but the translation and implementation is fairly complex. What LOOM does is it just downgrades every sequentially consistent to Acquire release. And that means that LOOM won't miss any bugs because Acquire release is weaker than sequentially consistent.
02:17:27.789 - 02:18:01.344, Speaker A: But it might tell you that your code has a bug when it doesn't. So it won't give you a false negative, but it might give you a false positive, which is probably what you want, but is also unfortunate. There's an open issue for it. There's also an open. Like there's a test case for it that's in the code but is currently ignored. This is something I know is like high on the priority list to fix, because sometimes if you have an algorithm that depends on sequentially consistent ordering to be correct, LOOM can't currently tell you that it's correct because it'll downgrade that ordering. And this is something very worth fixing.
02:18:01.344 - 02:18:40.684, Speaker A: But what's important is that it does model Acquire release correctly, which is generally the thing that you're not able to test well on. Something like an X86, right? So in the case where this was still Release, Release. Acquire. Acquire. Acquire, Acquire. So let's say we kept this code the way it was and we did like a assert any z 0. So remember, z equals 0 is a possible outcome with Acquire release, but it's not a possible outcome with sequentially consistent.
02:18:40.684 - 02:19:23.865, Speaker A: If you ran this through Loom, LOOM would correctly cause this assertion to panic. That is, it would actually explore this possible case with Acquire Release. So in other words, like LOOM will catch real errors and it will catch some of the ones that are the hardest to actually reproduce. But there are some cases where it will give you a False positive, but hopefully those will be fixed. And similarly for Relaxed ordering, there are some orderings it just cannot model. So like arguably the answer is you shouldn't be using Relaxed for anything critical in a data structure anyway. But if you do, LOOM doesn't quite capture it.
02:19:23.865 - 02:20:08.389, Speaker A: I actually recently pushed a large documentation like revamp to loom. So much of what I'm talking about now is actually written in the Loom documentation. I highly recommend you read it. It explains both a bunch of how Loom works, how you use it, what its limitations are, and how you can work around those limitations. I think that documentation should be fairly good at going through this and if it inspires you enough to go contribute to loom, like please do especially things like documentation. Now that you watch the stream, you should be in a really good position to try to document a lot of the things that Loom exposes. And yeah, so someone asked in stream.
02:20:08.389 - 02:20:47.807, Speaker A: I see the Tokyo uses Loom as it caught serious issues and it really has like. I know that Tokyo uses Loom especially for like the Scheduler which does a lot of lock free business and it's caused several like critical bugs there which like luckily never as far as I'm aware made it into production. It was like hard to say. But yeah, Loom has absolutely caught real issues. One thing you run into with Loom is like because the test cases have to be. They have to have like a state space that's reasonable to explore. Just because they try every possible execution, you sometimes have to run them on like limited use cases.
02:20:47.807 - 02:21:32.675, Speaker A: Sometimes there are more intricate use cases that can still trigger a bug. I found that generally it can explore a state space that's large enough to still catch interesting cases and the worst ones. LOOM also requires that execution is deterministic. Like if you did like a syscall or something in here, Loom would re execute the syscall each time. So you may have to do some mocking in or just like with normal tests really in order to be able to use Loom efficiently. But, but apart from that, I've been very happy with Loom for concurrency I think. I think that's all I wanted to go through.
02:21:32.675 - 02:22:12.921, Speaker A: Let me see if there's anything more in Loom that you need to know about. Yeah, so here, see there's even like the relaxed ordering example is like given as an example limitation of something we can't model. Sweet. I think that's all I wanted to cover. Now that we're sort of at the tail end of the stream. Are there Any questions at the end? Now that we've like been through all the weirdness, all the hopefully all the explanations, is there anything that's still unclear that I can try to help with more explanation on? Oh, memory barriers. That's a good one.
02:22:12.921 - 02:22:34.895, Speaker A: Good call. So let's go to atomic. So if we look at the atomic module in the standard library, you see that there are all these like atomic types. Does the ordering enumerate? There are constants. So these are for. These are from before. The new method on the atomic types was const.
02:22:34.895 - 02:23:13.945, Speaker A: This was the only way you could get a const atomic. This is the other way to share one between threads, right? Instead of sticking it in an arc or leaking a box, you can just make a constant. It also has three functions. Spinloop, hint, which should probably never have been in the atomics module at all, which is why it's now deprecated. It's now been moved to the hint module because it's not really an atomic instruction, it's just it was there because it's often used in atomic contexts. So in our spin lock, for example, in the while loop, you might want to call spin loop hint. I'm not going to go into what that means.
02:23:13.945 - 02:23:54.657, Speaker A: It's not terribly relevant. This compiler fence. Compiler fence is weird. The compiler fence is a way to ensure that the compiler won't move a given loader store or not a loader store like you place in a fence. And the compiler is not allowed to move operations under the fence to above the fence or above the fence to below the fence within that threads execution. This is only for the compiler. The CPU can still execute things out of order, so you very rarely need compiler fence.
02:23:54.657 - 02:24:45.085, Speaker A: Specifically it's mostly used for making for preventing a thread from racing with itself, which can only really happen when you're using signal handlers. In general, you won't need this. And then finally there's fence. So fence is important. Fence is basically a an atomic operation that establishes a happens before relationship between two threads, but without talking about a particular memory location. So remember that when we talked about load and store and acquire release, those synchronize like a load acquire synchronizes or happens after a store release of the same memory location. Right.
02:24:45.085 - 02:25:54.465, Speaker A: Offense is a way to say synchronize with all other threads that are doing an offense. So if we go back to the memory order. So the memory ordering document has like really detailed instructions on what exactly these happens before dependencies mean. So if we go down to happens before, where is the release Acquire. Oh yeah, here's also the note about on certain systems, release acquire ordering is automatic for the majority of operations. Only weekly order systems like ARM have special operations here. Let me see if I can find fences.
02:25:54.465 - 02:26:47.975, Speaker A: I feel like there used to be a section on fences here. I guess there isn't. That's too bad. But the documentation on fence is pretty good too. Basically what it does is that every fence synchronizes with another fence if there are memory operations before and after the fences that happen to synchronize. So a fence is basically a way to say synchronize a little bit later or a little bit earlier without doing the actual load and store. And again, I recommend that you actually read through the instructions, the documentation here to understand what it's for.
02:26:47.975 - 02:27:31.775, Speaker A: But essentially offense is a way to establish a happens before relationship between two threads in a way that. Where the happens before is sort of moved a little bit relative to the actual memory access. You're right. The atomic has to be a static, not a const. But you can only assign const values to statics is what I meant. Oh, volatile. Okay, so someone asked about volatile.
02:27:31.775 - 02:28:16.391, Speaker A: Okay, so notice that the atomic module has no mention of volatile. There's nothing volatile in here. And that's for a good reason. Volatile is a keyword that is just unrelated to atomics. But since why not? It's a common question. There are the operations read volatile and write volatile that exists in the pointer module in Rust. A volatile read sounds like it has to do with atomic because it's often explained as ensuring that you go to memory.
02:28:16.391 - 02:28:48.985, Speaker A: So people think like, oh, maybe the reason why concurrent operations might race is because one CPU does an operation in a register rather than in memory. But if it goes to memory, surely other threads must see it. And therefore I'm going to use volatile. And that's not really what volatile is for. Also, volatile doesn't establish happens before relationships across thread boundaries. What volatile is for is when you're interacting with memory mapped devices. So imagine that like your.
02:28:48.985 - 02:29:23.955, Speaker A: I don't know what's a good example of this. Your network card, like the. That has to like send and receive packets. It maps itself into a particular region of memory, like at a particular range of addresses. And let's say that the packet queue is mapped into that region of memory. It might be that reading from a part of that memory ends up modifying the memory. An example of this is many device.
02:29:23.955 - 02:30:07.245, Speaker A: Many memory map devices have regions of memory where there's like a ring buffer and there's a Pointer to the head and the tail of the ring buffer. If you don't know what ring buffers are, just bear with me for the next three or four minutes. So when you. The usually the head and the tail dictates where new writes are added and where the reader is reading from. And these might be different threads that operate. One operates only on the head, one operates only on the tail. And often these kind of device mapped memory regions have the side effect that if you read from the tail, you also reset the tail or there's some other side effect of reading.
02:30:07.245 - 02:30:46.565, Speaker A: And in those cases, imagine that you read twice from a given variable that is in memory map memory. If you didn't use read volatile, the compiler would probably do the first read. Realize that reads are read only, so it caches. It sort of caches the first read into register and the second read just reads from the register. But in reality you need both to go to memory to have a side effect on the device mapped memory. That is what read volatile is for. It's a way to tell the compiler that it must go to memory and the operation cannot be reordered relative to other operations.
02:30:46.565 - 02:31:30.295, Speaker A: So a volatile operation cannot be moved up or down by the compiler, for example, because it might actually have side effects. There's a section on this. Sorry, bright screen again, this is a section on this at the bottom of the memory order documentation, which talks about the relationship between atomics and volatile and memory ordering. And the answer is basically, they don't interact like they're not for the same purpose, even though sometimes it might seem that way. Whew. Okay, great. I think.
02:31:30.295 - 02:32:27.337, Speaker A: I think that's all I wanted to cover on atomics. Is there anything that still feels unclear? Anything you'd like me to go over one more time? Anything you feel like I haven't touched on? Now's your time. Can you tell I'm going a little hoarse? That's what happens when I talk for like two and a half hours. I have to. I have to wait for about 10 seconds for YouTube comments to come in in case anyone has questions from there. So for those of you who are watching this on video on demand, it might seem weird when I'm like, are there any questions? And then I just sort of sit there for 20 seconds. Is the delay to YouTube comments coming back to me? Anything particular about atomic pointer? Okay, so atomic pointer might seem like it's a little different from these other types.
02:32:27.337 - 02:33:04.285, Speaker A: Like these other types are sort of primitive types. They seem simple, like they're numbers and booleans Atomic pointer. It's not really special. Like atomic pointer is an atomic usize where the methods are specialized to the underlying type being a pointer. In fact, my guess is if we look at the source for this, oh, it does actually store a pointer. Now, nice. But atomic pointer, you see that it doesn't have like it doesn't have fetch add, for example, because that's not an operation.
02:33:04.285 - 02:33:45.615, Speaker A: Technically you can do a fetch add on a pointer, like using assembly, but it's like probably not going to do what you want. You run into all sorts of undefined behavior. You lose like pointer provenance if you start thinking about those things. It's complicated, but basically it's a specialized version of atomic usize that operates on pointers and keeps pointer provenance provenance. So you see, like it provides load, provides store and rather. And it then operates on pointers which are represented as us, sort of in memory, if you will. It also does compare exchange and it has fetch update, right? So remember, fetch update is really just a compare exchange loop.
02:33:45.615 - 02:34:39.115, Speaker A: So it's sort of like having a nicer interface to doing a compare exchange loop. So yeah, there's really nothing that special about atomic pointer and it still requires unsafe to turn that pointer because it's a raw pointer into a reference. There's one more thing actually that I can mention while I'm at this. So most of the atomic types you will see have a get mute, which takes a mutable reference to self and gives a mutable reference to the inner type. This is safe because if you have a an exclusive reference to the atomic itself, then you don't need to use any like special memory ordering or exclusive operations or anything. No one else has a reference to that atomic, so you don't need to use atomic operations on it. So that's what get mute does.
02:34:39.115 - 02:35:06.745, Speaker A: In fact, atomicusize could implement D mute, it just can't implement D ref, which is kind of interesting. I wonder if it implements as mute. No, because as mute. So D ref Mute extends D ref. So you can't implement draft mute without dref. And I think as mute is the same. It requires as ref, so it can't.
02:35:06.745 - 02:36:18.031, Speaker A: Even though technically this type can implement either, it doesn't. How do atomics interact with regular stores? So if I cast an atomic to a mutable pointer to U32 and pass it to C, I mean, in C you also have the option of doing atomic operations using like memory order, in which case it would be subject to the same things, although I don't actually know how the compiler operations happen across there. But if you did an atomic operation in C, like if you did a store with a memory order sequentially consistent, that would still be enforced on that operation. Think of it as. It's not that the value is special, it's that the compiler knows to generate special instructions for it. When you're using an atomic U Size or Atomic U32, if you passed that as a raw pointer to C and then did just like, just like dereference it with star, that would. That's even weaker than a relaxed load of that value.
02:36:18.031 - 02:36:55.015, Speaker A: So like it. It's the same as if you were to cast it to a. A raw. Well, you couldn't even do this in Rust, actually. Yeah, if you cast it to a raw pointer to a usize and then dereferenced it, which I don't think is even sound like, you might just run into undefined behavior. What is consume ordering? So Rust doesn't currently implement consume ordering. I don't know that it's used very often in CNC either.
02:36:55.015 - 02:37:28.905, Speaker A: I should have warned you. Sorry. So Release consume I haven't looked at too carefully, but it looks like the rules of what things are visible to the other thread changed a little. My guess is that this is like more specialized to cases where you don't need full release acquired acquire. Or is the other way around that Release acquire is weaker than release consume? I'm not. I'm not quite sure. It's not available in Rust currently anyway.
02:37:28.905 - 02:37:59.955, Speaker A: Probably for good reason. I don't know, it's really hard to define Consume ordering. Great. Yeah, I mean, looking at the. I forgot to warn you again, looking at the description of release consume, it looks like it's just annoying. And if you look here, since C17, the specification of Release consume ordering is being revised and the use of memoryorderconsume is temporarily discouraged. So I think it's just not worth trying to learn it at the moment.
02:37:59.955 - 02:38:18.993, Speaker A: And even if you learn it, chances are that information will be outdated. All right, with that, I think we're going to call it. Call it a stream. Thank you everyone for coming. I hope that was interesting. I hope you learned something. I hope this does not leave you more confused than you were.
02:38:18.993 - 02:39:15.365, Speaker A: I wish you luck in writing lock free atomic code. Just keep in mind that in general, don't write lock free code unless you have to. Right? Like I want to end on this note that if you can just use a mutex and most of the time you can do that, it will just cause you much less headache if you really need to get into atomics, like use loom, use thread sanitizer, run it through Miri, which I think now has some concurrency support, like get other people to vet the code, do everything, like find a paper that describes the algorithm you're trying to implement. Just do your best to make really sure you get it right, because there's a lot of subtlety, as we've seen, and the best way to avoid the complexity of atomics is to not use atomic X. So in particular, don't use them unless you need to. And with that, I will see you next time. So long, Farewell.
02:39:15.365 - 02:39:18.385, Speaker A: I hope it was an educational experience.
