00:00:05.920 - 00:00:51.462, Speaker A: Welcome back, everyone, to yet another crust of Rust episode. I'm trying to find a way to, like, fit in as many of these as I can before. Like, I move to LA and like, all these changes in my life happen and it'll be a little bit unpredictable when the next episode is going to be. So I'm trying to cram in as many, like, good episodes in the midst of my thesis writing as I can. Um, I. For those of you who aren't aware of what crust of Rust is, this is a variant of the live streams that I normally do where I try to tackle sort of beginner intermediate content is the best way to describe it. This is stuff where you've read the Rust book, you have some familiarity with the language, you've maybe built some things, but you're, you're looking to understand how some of the maybe slightly more advanced topics work.
00:00:51.462 - 00:01:48.170, Speaker A: So if you look at some of the past videos that I've done, I've done things on lifetime annotations, declarative macros, iterators, smart pointers, interior immutability, a lot of the topics that, like, once you start getting deeper into rust, you start seeing some of these things pop up and you might wonder how they work. And in order to do this next stream, or the one we're about to do, I tweeted out to ask people, what would you like to see next? And there was a pretty overwhelming plurality for looking at the standard sync NPSC module. And NPSC is basically, I mean, it's not basically, it is a channel implementation that comes in the standard library. A channel, if you're not familiar with it, is just a way to send data from one place and receive it somewhere else. The MPSC part of it is a multi producer single consumer. So this means you can have many senders, but you only have one receiver. So it's a many to one channel.
00:01:48.170 - 00:02:52.128, Speaker A: And in the stream, what we're going to do is basically implement our own channel and see the ways in which it compares to the both the standard library channel, but also some of the other channel implementations that are out there, some of the other ways to design these channels, and some of the considerations that come up when you do and when you decide how to use them. Before we dig into that, let me. Oh yeah, and you can like, if you're interested in these crust of rust streams, just follow me on Twitter, or subscribe on YouTube or Twitch and you'll be notified whenever I do any upcoming stream. So as I mentioned, the standard library has a built in mechanism for these NPC channels, and usually for any crate that provides something like this, you have a receiver type and you have a sender type. In the case of the standard library, there's also a sync sender type, and we'll talk a little bit about why that is and how it's different. And as you can see, the examples are fairly straightforward. When you create a channel, you create a sending handle and a receiving handle, and you can move these independently.
00:02:52.128 - 00:03:28.670, Speaker A: You can give the receiver or the sender to some different thread and give the opposite side of the channel to some other thread, and now they can communicate over that channel. The channel is unidirectional, though. Only the senders can send and only the receiver can receive. You can clone the sender, but you cannot clone the receiver. Hence the multi producer single consumer part. Before we dig into how to implement a channel, let's first just make sure that we're all on board about what a channel is and maybe why it might be useful. So if you have questions about that, let's get those out of the way first before we start to get into the weeds of the code.
00:03:28.670 - 00:04:19.930, Speaker A: Oh yeah, there's a discord server as well. All the chat will also be in discord. If you're interested in watching this, if you're watching the video on demand, what does crossbeam do differently to the standard lib? I forget, that's something we'll cover after we've done our implementation. What kind of data can I send through a channel? So the channels in rust are all, they take a generic parameter t. So if we look at the sender, for example here, the sender has a type parameter t, and you can send any teeth through that channel. And the, when you do, when you create the channel in the first place, like if you call the channel method, which is the thing that gives you a sender and receiver, it's parameterized by that type. So the sender and receiver are both parameterized by the type of the thing you're going to send and receive.
00:04:19.930 - 00:04:48.780, Speaker A: Yeah, they're very much like, oh, I can zoom in. Yeah, sure. So they are very much like go channels. Channels are something that exists in most languages, and very often they function in a similar way. You have senders and receivers. In rust parlance, it's specifically the NPSC channel, whereas in some other languages you have many to many channels. For example, I forget what ghost channels do there.
00:04:48.780 - 00:05:28.788, Speaker A: The data has to be send though, right? Not quite, actually. So imagine that you have a channel, but you never give away the sender or receiver to a different thread, then the t is being sent on the same thread. And so it doesn't actually need to be send. And I don't know what the standard library makes this distinction, but hopefully it does. Yeah, so you see that the sender is send. If the t is send, what this means is you can construct a channel and send stuff over it. That's not send as long as you don't move the sender or receiver across the thread boundary.
00:05:28.788 - 00:05:59.450, Speaker A: But if you do, then t must be sent. Are there any constraints of what kinds of types you can send through the channel, though? Does it have to be sent? It does not have to be send. And no, there are no other constraints. You can send anything, any type through a channel. Remember that it's not like serialization, it's not tcp, it's nothing like that. It is really just sending the data that's stored in it. If you send a vec, for example, it's going to send the length, capacity and the pointer to the data across the channel.
00:05:59.450 - 00:06:51.568, Speaker A: Does the data need to be sized or can it be din? I think it has to be sized. So remember from our, one of our previous streams, we talked about the fact that the size trait is an auto trait, but it is also an auto bound. So unless you say question mark size, like this thing doesn't need to be sized, every t that you write has to be sized. So here, because there's no question mark sized, t has to be sized. While you go, can you point out differences between other implementations of channels? We'll look at that later in the stream. We'll specifically look at what are other implementation strategies than the ones that we pursue. How is the sender thread distinguished from the receiver? They have different types.
00:06:51.568 - 00:07:51.794, Speaker A: So when you call channel, which constructs a new channel for you, you get back two halves, if you will, a sender half which has the sender type, and a receiver half which has the receiver type. Can the data be non static? Sure, but you need to own it. What's the point of a channel if you're not going between threads? There are some cases where you might not go between threads, but you might have multiple sort of, you might have things that you want to execute in parallel, but not concurrently or concurrently, but not in parallel. So you might have one thread that's like an event loop or something, right? And it might end up sending to itself, and you might still want that sender receiver abstraction. So that would be one example, tests are another. Who owns the data in the channel. The channel object.
00:07:51.794 - 00:08:44.020, Speaker A: Yeah, the channel type itself owns the t. So if you send something on the channel, you don't receive it and you drop the center and receiver, the channel will make sure that the data gets dropped. What's the performance impact of the channel? We'll look at that a little bit when we write the implementation. How does it do back pressure? We'll look at that as well once we start to get into implementation. All right, so it seems like the question now are more about implementation details. So let's start with that. As always, we will start with an empty project, new lib and we're going to call it Panama because Panama was a channel that enabled communication between.
00:08:44.020 - 00:09:25.154, Speaker A: There are also good reasons not to call it Panama, but it was the first thing that came into my head. Okay, so we're going to have a, we're gonna have a pub struct that's gonna be a sender and it's gonna hold the t. We don't know what's gonna go in there yet. And we're gonna have a receiver t. And then we're gonna have a function channel generic over t and it's gonna return you a sender t and a receiver t. And by convention the, the sender type comes first. So you return a tuple wherever where the first part of the tuple is the sender and the second part of the tuple is the receiver.
00:09:25.154 - 00:10:12.540, Speaker A: And who knows what this is going to do. Yeah, so this is the setup that we have. And the question becomes, well, what is the actual implementation we want? And here we have a lot of possible choices. I'm going to go with one that is just very straightforward and demonstrate some useful concurrency primitives in rust. But that is not necessarily the most performant, in part because implementing a very performant channel requires a lot more subtlety and trickiness that'd be hard to cover in the stream. So in particular what we're going to do is we're going to be using other parts of the sync module. In particular we're going to be using mutex and arc and convar.
00:10:12.540 - 00:10:50.334, Speaker A: So Mutex we talked a little bit about in the stream on smart pointers and interior mutability. Mutex is a lock. Mutex stands for mutual mutual exclusion. So the idea is that you have a really want the default to be to collapse. You have a lock method and the lock method returns a guard. And while you have that guard you are guaranteed to be the only thing that can access the t that is protected by the mutex. And the way this works in practice is if two threads both thread to lock the same mutex, one will get to go and the other one will block.
00:10:50.334 - 00:11:30.974, Speaker A: It will have to wait until the other one releases the guard and then it gets to go. So this ensures that there's only ever one thread modifying the t at any given time. Arc, as we talked about in the stream on interior immutability and smart pointers, is a reference counter type. It's arc because it's an atomically reference counted type, which means that we can use it across thread boundaries, which obviously we want for a channel. It's kind, it's not useless on a single thread, but we would certainly want it to work across thread boundaries. This is also the reason why we're using Mutex instead of something like Refcell and then convar. Convar is interesting.
00:11:30.974 - 00:12:20.170, Speaker A: If you haven't done a lot of concurrency work before, you might not know what a convar is. A conditional variable. A convar is a way to announce to a different thread that you've changed something it cares about. So think of this as like if there's a receiver who's waiting because there's no data yet, and you have a sender that sends something, it needs to wake up the thread that's sleeping, right. The thread that was waiting to receive something and go, there's now stuff you can read, and that's what a convar lets you do. And together these are very useful concurrency primitives that give you a very nice model for how to write concurrent code in a, in a safe, in a safe way. We might not even need any unsafe code in this implementation of channels.
00:12:20.170 - 00:13:03.050, Speaker A: Okay, so what we're going to do is we're going to define, and this is a pretty common pattern in rust when you have things that are, that are shared, like there are multiple halves or multiple handles, the point to the same thing. We're just, we're going to declare an inner type which holds the data that is shared. And for us that's going to be sort of the, the things in the channel. This is effectively a queue, right? Because if a sender sends something and then the receiver receives something, the receiver should be, should receive the thing that was sent the longest ago. Right. Now let's start with this being a vec. It's not actually going to end up being a vec, but for now that's a useful starting point.
00:13:03.050 - 00:14:04.270, Speaker A: And then we're going to say is that the receiver has an inner which is just an arc, mutext, and the sender has the same thing. So the sender and receiver actually contain the same stuff, at least at the moment. Standard sync. We want arc and mutex and we'll want convar. And now we even have our way to create this channel in the first place. Right? So what we do is we create the inner, which is going to be like an inner, and a queue which is going to just be an empty vec to begin with. And then we're going to have the shared inner be an arc of a mutex of an inner, right? And then we're going to return a sender of that inner and a receiver of that inner.
00:14:04.270 - 00:14:56.860, Speaker A: Alright, this is not a t, but an inner tache. Ah, the heat is making me type more poorly. Great. So that now compiles does the rough structure that we've laid out here makes sense? There are obviously plenty of things missing, and there are some things that aren't quite right, but it's a general idea of how we're planning to set up this shared state. Makes sense. Refcell does runtime borrow checking, right? Yes. Mutex in a sense is also a runtime borrow check, but it doesn't borrow checks so much as borrow and force.
00:14:56.860 - 00:15:52.626, Speaker A: If two threads try to access the same thing at the same time, it'll block one thread, whereas Refcell will tell you you can't get this mutably at the moment. Why does the convar always need a muketex guardhouse? We'll get back to that in a second. Once we start adding the convar, why not make it a linked list? We'll touch on that for alternative implementations. Can you zoom your Vim text a little? Absolutely. How's that? Is it possible to specialize the struct so that if t is not send, the arc mutex would just be an rc? No, not easily. You can't specialize the definition. Yeah, no, you would have to have a sort of unsync version.
00:15:52.626 - 00:16:31.626, Speaker A: So this is something I forget if the standard library has something like this. I don't think so. But you could imagine that you had like a standard Unsync or unsendental NPSC, although I think the actual use case for those is less clear than for a cross thread channel. In general, channels are used for things to ascend. I've seen a lot of people using mutexes from parking lot and channels from crossbeam. Does Convar have a similar better implementation you won't know of? Parking lot also provides parking and notification, which is what convars give you. So yeah, you could totally use the stuff from, from parking lot as well.
00:16:31.626 - 00:17:38.742, Speaker A: I know there's been some talk of trying to take the parking lot implementations of things like Mutex and Convar and make them the standard library ones, and that might happen someday. Why would you not put the mutex in inner? Yeah, I guess we could do that. That's certainly a, this is a change that I would have done in about five minutes. So make this be this and you'll see why that's actually necessary in a bit. And in fact here we can even go default. So we'll use the default implementation of Becky. Why does the receiver type need to have an arc protected by mutex if the channel may only have a single consumer thread? So, okay, so the question is, why does the receiver need to have a mutex? And the answer is because a receiver send and receive might happen at the same time, and they need to be mutual, mutually exclusive to each other as well.
00:17:38.742 - 00:18:55.150, Speaker A: Right? And so that's why they all need to be synchronized with the mutex. Is there a difference between an arc mutex and a Boolean semaphore? A mutex is a Boolean semaphore, effectively. So no. But I don't think there's a reason to use a Boolean semaphore over the implementation in mutex in particular, what mutex buys you is that it integrates with the parking mechanisms and user mode few texas that are implemented by the operating system. So with a Boolean semaphore, if someone else. A Boolean semaphore is basically a Boolean flag that you check and atomically update. The problem there is if the flag is currently set so someone else is in the critical section, someone else has the lock, has the mutex, then what do you do with a Boolean send for? You have to spin, you have to repeatedly check it, whereas with the mutex the operating system can put the thread to sleep and wake it back up when the mutex is available, which is generally more efficient, although adds a little bit of latency at that point you can just use a queue, right? I don't know what the question is.
00:18:55.150 - 00:20:02.260, Speaker A: Why is the arc needed? So the arc is needed because otherwise if there was no arc here, then the sender and the receiver would just have two different instances of inner. And if they did, then how would they communicate? They need to share an inner because that's where the sender is going to put data and where the receiver is going to take data out of. All right, so let's just start implementing and see what happens? So for Ascender, we want a obviously send function. It's going to take a mute self and it's going to take the t that we're going to send and it's going to return something. For now, let's just say that it returns nothing and we'll see why that's a problem later on. And similarly for receive, actually let me move this up here. And then for receiver we want a receive method which does not take a t but returns the tache.
00:20:02.260 - 00:21:12.770, Speaker A: Okay, so what would send do? Well, send is just gonna, first is gonna take the lock self dot, inner dot lock. And you'll notice if we go back to the documentation from Mutex, you'll see that lock returns a lock result. The answer for this is imagine that the last person who took the lock, the last thread that took the lock, panicked while holding the lock, right? So, so it might be in the process of updating something under the lock, but then the thread panics. So that might mean that the data under the lock is now in some like not quite consistent state. And the way that the lock communicates this is when the thread panics, it releases the lock, but it also sets a little flow flag in it to say the last thing that accessed this panicked. And so what lockresult does is you'll see that it's a, it's either a guard or a poison error, with a guard basically telling you if you get an error back, it's saying the other thread panicked. You should know about that.
00:21:12.770 - 00:22:16.508, Speaker A: And of course you could always choose to ignore the fact that it was poison, that you can ignore the fact that the other third panicked, but you, it could also be that you don't want to ignore that. In our case, we're going to unwrap this for now inner queue. So we're going to lock the queue and then we're going to do Q dot push t, right? And the receiver is going to do sort of the opposite, right. It's going to, it's going to lock the queue and then it's going to pop the queue. Now there should immediately be some, some obvious problems with this. The first is that this is not actually a queue, right? If the senders pushes and the receiver pops, then if you have two sends and then receive, the receiver would get the last thing sent rather than the first thing sentence. And the problem here, of course, that we're using VEc, we're using it like a stack.
00:22:16.508 - 00:23:14.298, Speaker A: Now in theory you could remove the first thing from a vec, but what you end up doing is you have to shift all the other elements down to fill the hole of the thing you removed. In practice, the way to do this is to use a ring buffer. We might cover those in a later stream, but for now know that there is, in collections there's a type called Vecdec or VeCDQ which implements, it's basically a fixed amount of memory, or it's sort of like a vector, but it keeps track of the start and end position separately. So if you put you, if you push to the end, then it pushes it to the end. If you pop from the front, it removes the element and then just moves like a pointer to where the data starts. And so this way the data might end up wrapping around the whole thing, but it can be used as a queue as opposed to a stack. And this allows us to have a, to have send to a push back and Q.
00:23:14.298 - 00:23:46.860, Speaker A: Do a pop and receive, do a pop front. You don't want to swap remove. Someone suggested that as an alternative. If you swap remove, what that will do is the last thing sent will become the next thing to be received. It changes the order of the elements in the vector. Apparently Vecdec is the correct pronunciation. Thanks, Atlas.
00:23:46.860 - 00:24:43.370, Speaker A: All right, so the other problem here, right, is, well, when we receive popfront, as the compiler tells us returns an option, it doesn't return you a t because it could be that there's nothing in there. And then what do we do? We can't just, I mean, one option here, right, is we can provide a try receive method that returns an option t, right? And so it will try to receive something, but if there's nothing to receive, it just returns none. That seems totally fine. I'm going to remove it now because we're going to change some things later. But really we want to provide what's known as a blocking version of receive. We want to provide a receive that if there isn't something yet, it waits for there to be something in the channel. And so we need to figure out what to do here.
00:24:43.370 - 00:25:39.320, Speaker A: And the answer here, this is where the convar comes into play. So here we're going to do something. We're specifically going to have a, we're gonna split it into a txokay or TX, available Convar and an r. Actually we're just gonna do avail. Is that what I want to do? Yeah, we're gonna go with available for now. It's not quite true, but it's close enough and the convar needs to be outside the mutex because the idea is that, imagine that you're currently holding the mutex and you realize you need to wake other people up. The person you wake up has to take the mutex, that's sort of the assumption.
00:25:39.320 - 00:26:07.554, Speaker A: But you're currently holding the mutex. So if you tell them to wake up while holding the mutex and then they wake up, they try to take the lock, they can't, they go to sleep. And then you continue running and then you release the mutex. Then now no thread is awake and you end up with what's known as a deadlock. No thread can make progress even though it is possible to make progress. So this is why the convar has to be outside the mutex. The idea is that you sort of let go of the mutex at the same time as you notify the other thread.
00:26:07.554 - 00:27:27.470, Speaker A: This is why, to get to the question that was raised earlier, why the convar requires you to give in a mutex guard, you have to prove that you currently hold the lock and then it will make sure that does this step as one step, as an atomic step. So here what we're going to do is we're going to match on Q pop front and if it's some t then we're just going to return t. But if it's none we're going to block. So we're going to do self inner available wait and we're going to wait on the queue. Now of course the problem, as the compiler also points out, is that okay, we wait, but then what? So this actually ends up needing to be a loop, right? So we're going to be doing this in a loop. And not only that, but if you look at the signature of wait, you'll see that the weight actually gives you a mutex guard back. And the idea is that if you get woken up you automatically have the mutex, someone else chose to wake you up and you now are sort of, it basically hands the mutex to you and then you do something appropriate with it.
00:27:27.470 - 00:28:29.620, Speaker A: And so instead of having to lock it each iteration to the loop, what we can do is this, right? And this too can be poisoned if the previous holder was poisoned. And so what we're going to do is we're just going to keep looping. But this isn't going to be a spin loop, right? So if we end up in the non clause, then what we're going to do is going to wait for a sort of signal on this available convar and the operating system will make sure that the thread, only the bed thread, goes to sleep. And then only wakes up if there's some reason for it to wake up. And this also means that now the sender needs to make sure that it notifies any waiting receivers once it sends, because otherwise imagine that some thread enters this loop and it's just like sleeping, and then ascend happens. We need to make sure that this is thread wakes up. If it doesn't, we have a problem.
00:28:29.620 - 00:29:17.500, Speaker A: Right, so we're going to use the convoy for this as well. So the oops inner so it has a notify one and a notify all call. And we're going to drop the queue here. So we need to drop the lock so that whoever we notify can, can wake up. And then we're going to notify one thread. And because we are the sender, we know that this will be a receiver that we wake up. Does that make sense at the moment? Vector double ended queue is a vector.
00:29:17.500 - 00:29:57.622, Speaker A: Yeah, basically. I mean a vector is just a, a vector with a head and tail index. Isn't that kind of loop the raison d'art? I don't know how to pronounce that in French. Should probably not try of async, not quite. So this is fine. This loop is not a spin loop where you need async await is more if you're is generally when you are I O bound, not CPU bound. It's for slightly different reasons.
00:29:57.622 - 00:30:36.022, Speaker A: It's basically so that you don't need to have a million threads running. Actually, I lied for the need for wait to take a guard, come to think of it. So you'll notice up here, notify one does not require me to drop the mutex. It doesn't require me to hand in the mutex, but you sort of need to do that regardless. But wait requires you to give up the guard. The idea is that you can't wait while still holding the mutex. You need to give up the mutex in order to wait because otherwise whoever would have woken you up can't get the mutex.
00:30:36.022 - 00:31:14.214, Speaker A: And so that's why it requires you to take the guard. How is it protected from convar spurious wake ups? Yeah, so one thing that can happen with convars is when you call wait here. The operating system doesn't guarantee that you aren't woken up without there being anything for you to do, and that's what the loop does here. Right. So imagine that you're woken up for some other reason, like not because a sender happened, but just because of a signal to the process or some other random reason. Basically the operating system doesn't guarantee that you wake up for a reason. Then what you'll do is you'll loop around, you'll check the queue, you'll realize it's still empty, and then you'll go to sleep again.
00:31:14.214 - 00:31:37.600, Speaker A: So that's fine. How can someone send. We have the mutex locked now when we're receiving. So it blocks insertions. Yeah, so that's. Wait, gives up the lock just before it goes to sleep, and so that allows the sender to proceed. I'm not using ale anymore, I'm using Coc Vim.
00:31:37.600 - 00:32:26.484, Speaker A: Coc neo vim, which gives me the like inline type annotations and errors. Wouldn't the lock be dropped after the notify? Yeah, but I specifically drop it before the notify, so that when the other thread wakes up, it can immediately take the lock. How does the operating system know which thread to wake up? It doesn't. So when we do notify one, what that means is notify one of the threads that's waiting on this convar. Specifically. And because we know there's only one sender and many receivers, we know that that must be a receiver because this is the sender. Wouldn't it be nicer to use brackets around let queue and send instead of drop? I mean, that's true.
00:32:26.484 - 00:33:12.940, Speaker A: We could do this instead. I don't know that that's any nicer. I prefer for it just to be an explicit, to be explicit about the release. Don't we need to take the lock in the loop? So the way the weight works on a convar is it consumes the mutex from you, because it needs to give it up right before it goes to sleep. But if you're woken up, it takes the mutex for you. So that's why we reassign to the guard, because we get it back when wait returns. There's a wait with timeout, right? Yeah, there is a wait with timeout as well, which does not necessarily give you the guard.
00:33:12.940 - 00:34:07.456, Speaker A: Actually, I think it does give you the guard as well. Wouldn't only one thread empty the entire queue and only allow other threads again once it's empty? No, because notice that we return when we manage to pop something from the front which releases the mutex. Is there a notification variant which takes the guard to drop it for you? Not as far as I'm aware, no. If n threads are waiting, one of them is randomly chosen to be woken up. Yeah, notify one does not guarantee which thread is woken up. This is a notify all, which notifies all the waiting threads. Is it possible for the queue to be locked between the drop and when the receiver locks from another sender, there's only, remember this is, yeah, so it can, it can be that there's another sender, it can be that there's another sender that also manages to push to the queue, but that isn't really a problem, right.
00:34:07.456 - 00:34:40.871, Speaker A: It just, the receiver will still eventually get to go. So in the current setup, right. Remember, senders can basically never block here, right. The senders, when a sender gets the lock, it always succeeds in sending. So there are never any waiting senders in the, in the current design. We'll talk more about that in a second. All right, so this setup will actually work pretty well.
00:34:40.871 - 00:35:29.772, Speaker A: In fact, we can try it out here. Available is going to be a cond var new. And then the other thing we want to do is we, right, the other thing we want to do here is we want to make sure that the sender is clonable, right. So your first instinct might be to derive clone here. Now you can clone the sender. Unfortunately derive clone, at least at the moment, actually desugars into impul t clone clone for sender t self to self with some auto generated stuff by the compiler here. So this is what the, if you put derive, this is what it turns into.
00:35:29.772 - 00:36:09.042, Speaker A: And one thing you'll notice about this is that it added the clone bound to t as well. Very often this is what you want, right? Because inner, if the structure deriving clone on contains a t, then t does need to be cloned in order for you to clone the whole type. In our case, though, arc implements clone regardless of whether the inner type is clone. Sort of what reference counting means. You can clone an arc and there's still only one of the thing inside. And so for our implementation of clone, we don't actually need t to be clone. We want this implementation, and that's the reason why we need to implement clone ourselves manually.
00:36:09.042 - 00:36:50.530, Speaker A: Luckily it's pretty, pretty simple, though. It's just inner is self, inner clone. And the actual way to write this is here. This is technically legal, but it's usually not what you want to write. The reason for this is imagine that inner also implemented clone rust won't know whether this call is supposed to clone the arc or the thing inside the arc, because arc dereferences to the inner type. And this, the dot operator sort of recurses in, in, into the inner drefs. And so usually what you want to do here is use our clone to say that I specifically want to clone the arc and not the thing inside the arc.
00:36:50.530 - 00:37:14.330, Speaker A: All right. Couldn't lock block? Yes, lock blocks, that's the whole point of lock and that's what we want. Right. Like send and receive should be blocking methods. If, if you run, well, I mean send. If you run send will only block for short amounts of time. But if you try to receive and there's nothing in the channel, we want the thread to block.
00:37:14.330 - 00:37:38.940, Speaker A: I don't understand why you're talking about waking up multiple receivers while implementing NPSC. Sorry, you're right. I misspoke. I meant there's only one receiver and therefore notify. One will notify the right thing. There are never sleeping senders in our current setup. Will there be a max size on the queue that will need senders to wait? Not currently, although I'm about to get to it.
00:37:38.940 - 00:38:12.310, Speaker A: It's also easier to read that it's a trivial clone that way. By removing the homebound. Yeah. Is there a way to disable auto draf? Yeah, you just don't use the data operator. You use the sort of unified method calling syntax. Why does rust bubble e inter, whatever OS equivalent up into the weight function? It often doesn't have a choice. The wait implementation basically just ends up being the OS implementation.
00:38:12.310 - 00:38:41.410, Speaker A: Often they want to add as little as possible in between. Accepted like poisoning. So yeah, I think wait specifically says it does not give this guarantee that you don't get what are known as spurious wake ups. Okay, so let's just like check that this works. So we're gonna do mod tests. A test. And we're gonna do like a ping pong test.
00:38:41.410 - 00:39:15.490, Speaker A: We're gonna create TX and RX, I guess here we're gonna do super. And that's gonna create a channel I can't spell. And then we're gonna do TX, Seth. And we're going to send 42. And then we're going to cert equals RX receive 42 and. What did I call this thing? Panama. That's right.
00:39:15.490 - 00:39:44.260, Speaker A: And TX and Rx both need to be mute. Actually RXD. RX needs to be mute. Actually RX doesn't technically need to be mute, but we're going to make it be mute. I'll show you that for a second. And send. Also technically it doesn't need to be mute, but we might as well make it be mute, which is why they need to be mute up here.
00:39:44.260 - 00:40:12.730, Speaker A: Now if we run it, great, the ping pong test succeeds. So we now have, we have an implementation that works. There are still some things wrong with it, but we have one that works. The only problem with Arc clone is that it cannot coerce to trait objects. You have to do it manually. For example, Arc clone as Arcdin trait. Yeah, that's true.
00:40:12.730 - 00:40:52.716, Speaker A: Do you think TX and RX are good names for channels? I know STD docs use that, but I've always hated it. I like TX and RX, but it's true. It sort of comes down to personal preference. All right, so the first and most obvious problem with this is that the receiver imagine that there are no senders left. So here we're going to do, we're going to make this be closed. So we're just gonna immediately drop the sender and now the receiver. It's not even clear what the receiver should do.
00:40:52.716 - 00:41:30.740, Speaker A: Right. Like, what happens when I now call receive? Is it gonna block forever? Even though there are no senders left? I guess they need to give this a type. There are no senders left and the receiver tries to receive. There can never be any future senders because in order to get a sender, you have to clone a sender, but all the senders are gone. So if you run this test, you'll see that it just hangs forever, which is obviously not great. So realistically, what we want is some way to indicate to the receiver that there are no more senders left, that the channel basically has been closed. And the easiest way to do this is to have a.
00:41:30.740 - 00:41:45.810, Speaker A: We're going to change the naming here a little and call this shared. Shared. Shared. Shared. Shared. Shared. Shared.
00:41:45.810 - 00:42:24.528, Speaker A: Shared. Shared. I'll show you why I changed it to shared is because really we want some additional data that's guarded by the mutex. So we're gonna have the mutex protect an inner t. And the inner t is gonna hold the queue. This is now gonna be inner and it's also gonna hold like ascenders, which is gonna be a use size. Now of course this is gonna be shared dot.
00:42:24.528 - 00:43:09.356, Speaker A: Inner, dot lock. Right. So the lock now guards both the, both the queue, but also this additional u size that we added that should say inner. That should say enter. Great. And now what we'll do is every time you clone a sender, we're going to increase the number of senders in that. In that value.
00:43:09.356 - 00:44:01.170, Speaker A: So when you clone, it's actually going to take the lock and then senders is going to be increment, incremented by one. We drop the inner and then we clone the shared. And then similarly, we need to now deal with the case where a sender goes away. So when a sender goes away we also need to grab the lock. And then one thing we want to keep track of here is like whether we were the last one. If the number of senders is now zero, what might happen is that like the receiver was blocking and then the last sender went away. We need to make sure to wake it up otherwise it might never wake up.
00:44:01.170 - 00:45:07.080, Speaker A: And so we also call self shared available notify one if we were the last. And then what the receiver has to do is it now basically needs to return an option t, right, rather than just a t because it could be that the channel truly is empty forever, in which case we want to return none. So now what we're going to do is in the none case, if the inner dot inner senders is zero, then we want to return none. And I guess we can do this a little bit nicer. And only if the the sender count is more than zero do we actually want to block and wait. So this is addition of keeping track of the number of senders. Makeshi sense.
00:45:07.080 - 00:45:42.780, Speaker A: I guess I'll show you that the test actually ooh, right. This test is now not quite right. The inner is going to be an inner default. This can derive default. Actually let's just not do that because that requires t to be default. So we're going to have an inner via queue which is going to be an empty vec deck. And the number of senders initially is one.
00:45:42.780 - 00:46:21.338, Speaker A: And this is going to be a mutex new over that inner state. I guess I need a semicolon. This now is inner. And now the test here needs to assume that here we got some right. This close test should in theory now now succeed. We can assert now that if we try to receive after the sender goes away, we get a none. Ooh, fun.
00:46:21.338 - 00:47:11.202, Speaker A: It does not work. Let's figure it. Figure out why closed hangs forever. Can't the receiver check if shared is unique? Potentially. Actually it could be that we can get away with. Yeah, actually you might be right that we could use the reference count in the arc instead. It gets a little complicated because of weak references, but because we're only using strong references here, it might be that we can get away with.
00:47:11.202 - 00:47:49.650, Speaker A: So with arc there's a strong count which you can give self, you can give an arc and it tells you how many references there are to that arc, how many instances of that arc there are. And if there's only one then that must be the one of the receiver. Therefore there are no senders. You're right. It's a good optimization. So then we can get rid of the senders field and we no longer need to deal with the case. Actually, this is the complicated case.
00:47:49.650 - 00:48:48.870, Speaker A: If you drop a sender, you don't know whether to notify, because if the, if the count is, if the count is two, you might be the last sender or you might be the second to last sender and the receiver has been dropped. So I think we're going to keep it the way it was. It's also easier to read. There are plenty of optimizations you can make over this implementation. I'm more trying to build us like a representative way in which it might work. Could use an atomic us and shared rather than creating inner you could, although the moment you take a mutex, there isn't really that much of a value to it. It would mean you don't have to take the lock in drop and clone, but those should be relatively rare, and the critical sections are short enough that the lock should be fast anyway.
00:48:48.870 - 00:49:30.116, Speaker A: Wouldn't you want to notify all for drop? No. So when the last sender goes away, that means that there's only the receiver left. So there will be at most one thread waiting, which will be the receiver, if any. So someone pointed out the receive should probably return result instead of option. I'll get to that. Can you overflow the sender count? In theory, probably not in practice. Is there any immediate benefit adding to the mutex rather than atomic use size? I mentioned that a little bit, yeah.
00:49:30.116 - 00:50:34.752, Speaker A: Patrick, I'll get to your question later. What's the difference between vect new and vect default? None. I think the error was initializing senders to one in the constructor and then calling clone on the sender we returned. So in channel, what we're cloning here is we're cloning the shared. We're not cloning the sender, so this won't increment the sender account. Can you get false sharing in between the vector and the sender count? You could, but they're under a mutex anyway, so that shouldn't matter. Can't you just notify every time a sender is dropped? You could, but that would cause a lot of extra wakeups.
00:50:34.752 - 00:51:12.430, Speaker A: Like you want to avoid spurious wake ups because they're costly. They're waking up a thread that didn't need to be woken up. There's no correctness issue to waking up more threads, but it is a performance issue. All right, great. So now we need to figure out why this didn't work. Why did the close test not work? So when we run it, it hangs forever and presumably it hangs down here. Yeah, so it hangs on the receive.
00:51:12.430 - 00:51:54.450, Speaker A: And so the question becomes, why does it hang on the receive? We take the lock, we try to pop from the front. If it's some, we return it. If it's none and there are zero senders and return zero. So here's what we're going to do. We're going to debug print this value, see what comes out. Okay, so the senders is one. So then the question becomes, why isn't the sender's count decremented here? Now? Probably won't let me do that.
00:51:54.450 - 00:52:28.224, Speaker A: Drop sender count was this. Oh, it's not dropped. Huh. Or maybe it never gets the lock. No, then it should hang sooner. Yeah. So for some reason the sender is not being dropped.
00:52:28.224 - 00:53:01.280, Speaker A: Why is that? I guess if I do this, does that make a difference? Interesting. Okay, so I was under the impression that assigning to underscore would drop immediately, but I guess that's not true. I think there was actually an open discussion about this for a while. Okay, so the it actually is correct. It's just that this does not drop tx apparently, which I found weird. I thought that was the case, but apparently not. Where's an explicit call to drop? We'll do what we want.
00:53:01.280 - 00:54:15.756, Speaker A: I was a little surprised that that was broken because this implementation is pretty straightforward. Okay, does that make sense so far? Why not use atomic use size instead of a mutex? Well, we need the mutex for the queue, and because we have the mutex anyway, there's no, the atomic u size doesn't save us anything because we have to take the mutex. We might as well just also update the count under the mutex anyway, since we have it anyway. Do you ever use GDB to debug rust programs? I do, but I find the print debugging is easier for the, especially for small examples like this. I guess we can get rid of this. Great. Okay, so it turns out there's still a problem with this implementation, and that is it can go the other way around.
00:54:15.756 - 00:55:02.972, Speaker A: So this is a closed TX. But what if there's a closed RX? Like what if we drop the Rx here and then we try to do a TX send of 42? Actually, this isn't really a problem. This let's remove the type annotation here. So if I do this, I guess mute and rx. If I do this, this test will run just fine. Fine. But the question is should it run fine? If I try to send something on a channel where the receiver has gone away, maybe the right thing to happen is that I should be told that the channel has been closed rather than the send just sort of blindly succeeding.
00:55:02.972 - 00:55:52.730, Speaker A: It's not entirely clear what the right answer is here. This is sort of a design decision of whether send should just always succeed, or whether it should fail in some way. I think in this case we're just going to keep it the way it is because it's okay, it's fine. But in a real implementation you might imagine that you actually want send to get back a signal, like send returns a result or something if the channel is closed. Some implementations do, some don't. Keep in mind though, that if, if you wanted this to be able to fail, then you have to make sure that you give back the value that the user tried to send. Like if the send fails, the user should be given back the value they tried to send so that they can try to send it somewhere else or log it or something like that.
00:55:52.730 - 00:56:45.460, Speaker A: And basically the way you implement this is you add a sort of closed flag to the inner, just a boolean that the sender sets. And if the sender drops, just like we have a drop for, sorry, if the receiver drops, the closed flag is set in its drop and it doesn't notify all. Although there aren't senders blocking this particular implementation, but it sets that flag. And when you send, if the flag is set, you return an error rather than pushing to the queue. Can we resurrect a drop channel? No, if the sender goes away, you have no way to send anymore. In our particular design, because the sender and the receiver have the same implementation. In theory we could add a method that lets you construct a sender from the receiver.
00:56:45.460 - 00:57:39.870, Speaker A: Most implementations are not quite as symmetric as this one, and you can't easily create a sender from a receiver. And in our implementation you could get a receiver from a sender, but we wouldn't want to provide that because then people could create multiple receivers which, which would not work right? It would be wrong with our notify one. Actually, this particular channel would kind of work with a multi producer multiple consumer. But we're, we make some assumptions that make that annoying in the future. There's an audio video sync problem with the stream. I tried twitch instead. Sometimes it's better.
00:57:39.870 - 00:58:36.640, Speaker A: YouTube gets confused and is slow. Alright, so now let's look at some design decisions here that, that might not always make sense. The first one here is that like every operation takes the lock, and that's fine. If you have a channel that is not very high performance, but if you wanted like super high performance like you had, you have a lot of sense that compete with each other, for example, then you might not want the sends to contend with one another, right? Imagine that you have ten threads are trying to send at the same time. Realistically, you could perhaps write an implementation that allows them to do that. The only thing that really needs to be synchronized is the sender with the receivers, the senders with the receiver as opposed to the senders with one another, whereas we're actually locking all of them. I'll talk a little bit about what that implementation might look like later.
00:58:36.640 - 00:59:08.956, Speaker A: The other thing that you might have noticed is when we looked at the standard library. The standard library channel has a receiver and then it has two different sender types. It has a sender and it has a sync sender. And both of these, if you construct one, you get a receiver. So the receiver type is the same, but the sender types are different. And these are, the difference between these is that one is synchronous and the other is asynchronous. Now this is not the same as async like you might be aware of.
00:59:08.956 - 00:59:46.146, Speaker A: It's not that kind of asynchronous. What they mean when they talk about a synchronous channel is whether it forces the senders and receivers to synchronize. That is a, imagine that you have a sender that's much faster than the receiver. In the current design that we have, the sender would just pursue, produce content much faster than the receiver could consume it. And the queue would just keep growing. If you have a synchronous channel, what that means is that the sender and receiver sort of go in lockstep. Basically the channel would have a capacity, it would have a limited capacity.
00:59:46.146 - 01:00:17.328, Speaker A: So at some point if the sender sends so much that the, and the receiver isn't consuming it as fast, the channel would fill up and the sender would now block. And so the primary difference between a synchronous and an asynchronous channel is whether sends can block. In our implementation, sends can't block. Right. The send here, all the send does. It takes the lock pushes to the vec and then, and then drops the lock and notifies the consumer. And that works fine.
01:00:17.328 - 01:00:49.882, Speaker A: But it does mean that there's no back pressure. Right. If the sender is too fast, nothing in the system is told that the receiver isn't keeping up. So the advantage of a synchronous channel is that there's back pressure. The sender will eventually start blocking as well. Obviously this creates some additional challenges, right? Like now you might have blocking senders, and the receiver might have to notify the sender and be like, hey, I know that you were blocking, but I just received something. You can now go ahead and send.
01:00:49.882 - 01:02:13.680, Speaker A: The way this works out in practice, in a design that is based on like mutexes and convars, is basically you need two convars. You need one for notifying the senders and one for notifying the receiver the way we're currently doing, but you can guard them by the same mutex I think is true now in practice, once you implement these designs, there are some other implementations you can go with that. They're a little bit better suited, and we'll talk about those in a second. In particular, our channel method, right? It does not take any kind of capacity, it's just an infinite queue, whereas if you look at the standard library, you'll see that there's a channel and there's a sync channel, and the sync channel function takes a bound, which is basically the channel capacity, and returns a sync sender and a receiver that functions very much the same way as our sender and receiver types, except that the sends here are synchronous. Questions about that before I'm going to move on to like alternative implementations a little bit down the line. Why not have senders use weak? If sender send tries to deref the week and returns, none then fail. And if the weak count is zero, then you can know that the receiver receive will fail fail.
01:02:13.680 - 01:03:21.252, Speaker A: So if the senders use weak, yeah, you could have the senders use weak. In general, I don't think I would optimize this particular implementation too much, because there are better implementations like we'll see later, but that are more complicated. If the senders use weakness, what you would do is when you send so weak is a version of arc that doesn't increment the reference count, but you have a way to try to increment the reference count if the reference count hasn't already gone to zero. And so the sender would try to upgrade their sender, and if they succeed then they know that the receiver is still there and they try to send. Now, one downside of weeks is that every time you try to send you have to atomically update the reference count and decrement it after. So it actually adds a decent amount of overhead. Is there a way to have a convar without a mutex? Not really, no.
01:03:21.252 - 01:04:15.280, Speaker A: As you see, the convar wait requires you to have a mutex card wouldn't send technically block if ascend caused a VeC resize. So this is an important point. I've spent a bunch of time on resizing in my research recently, and this, in OkU's call to pushback is not necessarily free, right? The pushback, it might be that the vector, the vector we're using has capacity 16 and you're pushing the 17th element to it, in which case it also like allocate a new vectech of capacity 32, copy over the 16 elements, deallocate the old one, and then push the element. And that takes some time. Now, this still isn't blocking. Like if you resize, it's not blocking, it's just the send just takes longer. But it is true that it does mean that the send takes longer and that the in the meantime, you can't do sends and you can't do receives.
01:04:15.280 - 01:05:26.230, Speaker A: In practice, for, for most implementations of these things, you don't use a vectec and you don't have this problem. Given the implementation you currently have. How hard would it be to write an iterator implementation which consumes values from the channel until all senders are gone and then ends with none? Really easy, actually. So if we do impl, we could even do iterator for receiver. And the type item is going to be t next is going to take a mute self, it's going to return an option self item, and it's just going to call self receive. And now receiver is an iterator. Are you planning to make a video about your resizing insights? Maybe it could be interesting.
01:05:26.230 - 01:06:18.156, Speaker A: Might be like a thesis talk video. Is there a good way to send multiple items at once? In theory that would be pretty easy to add. You could have like a send many that just appends so it shouldn't be too hard. And you would only need to take the mutex once. Okay, so there's actually, there's one more optimization that many implementations do that I think it's worthwhile mentioning here. And that is because we know that there's only one receiver and this is the first place we're going to try to encode that assumption into our code to make it more efficient. Because we know there's only one receiver, we don't really need to take the lock for every receive.
01:06:18.156 - 01:07:50.868, Speaker A: Instead, here's a trick we can do. Bear with me here, I'm just gonna write the code first and then talk about it in a second. Here's where we're gonna do queue. If not inner queue is empty, then what we're going to do is swap this. Oh, all right. This optimization is a little cool, and it's something you'll see in a lot of other implementations that don't necessarily use mutexes. The idea is that because there's only one receiver, any time we take the lock, we might as well steal all the items that have been queued up rather than just steal one, right? Because no one else is going to take them.
01:07:50.868 - 01:08:41.682, Speaker A: And if we call receive again, we might as well just like keep a local buffer of the things that we stole last time, right? So what we're going to do is when someone calls receive, we're going to just like first see if we last time we took the lock, whether we still have some leftover items that were there at the time, and if so, we can just return from there. We don't even have to take the lock. Only if the sort of buffer is empty do we need to take the actual lock. And when we do take that lock, then we try to take the front item. If the queue is empty, then we do the same thing as before, we have to wait. But if the queue is not empty and we get an item, then we check are there more items in the queue? And if there are more items, we just steal all of them. We swap that vec deck with the one that we have buffered inside of ourselves, and we leave the empty self buffer.
01:08:41.682 - 01:09:21.810, Speaker A: It must be empty because we pop front returned none, we leave that in its place. So we just swap the two. And now in subsequent calls to receive, we just end up popping from the buffer until that's empty again. And so this means that now, instead of taking the lock on every receive, if the, we only take the lock once every time, there were no send, no additional sends between every time we lock. If that makes sense. It's a neat little optimization. It's not really double buffering, but it has a little bit of that flavor.
01:09:21.810 - 01:10:12.794, Speaker A: It is true that this ends up keeping sort of twice the amount of memory, because you have two vectx that are both going to be growing as you add more items and you're going to be swapping between them. So you do end up keeping two vectors in terms of capacity. Won't the receiver buffer optimization trigger a lot of extra memory allocator activity? Only twice the amount which is also amortized. Right. The resizes happen. Every power of two pushes or power of two size and so in theory, this triggers twice as many resizes at predictable intervals. This is also a good way to have to reduce the amount of contention.
01:10:12.794 - 01:10:55.198, Speaker A: Right. This means that now the lock is taken fewer times, and that means the lock will be faster to acquire. All right, so now that we have that implementation, and we've talked a little bit through it, how did you come to that optimization? Looking at implementations, channels. Got it from somewhere else. Yeah, this is a pretty common implementation. If you look at some of the more optimized implementations, they generally pull this trick. It's important the swap is used rather than just discarding the local buffer each time.
01:10:55.198 - 01:11:33.070, Speaker A: Yeah, you could imagine that instead of doing this right, we did like self dot buffer equals like if we use standard mem, take for example, which just allocates a new vectec and leaves that in place of the one that's there. But that will be much more inefficient because the, the one you leave there is a new allocation. You would deallocate the old cell top buffer. This lets it reuse it. Do you think it might be faster without the branch for the swap? Could be. I mean, we could do this. There's nothing really stopping you from doing it.
01:11:33.070 - 01:12:24.204, Speaker A: It, this does, yeah. In terms of which one is faster without, the branch is probably faster, but not, probably not by a significant amount. The branch predictor should be pretty good at that, because generally either your channels are usually empty or usually not empty. If the channel is usually empty, then the branch will usually be false, and so the branch predictor is going to do well. If the channel is usually non empty, then the branch predictor will predict that it's not empty, and so it will do well. Oh, the branch predictor. The cpu has a built in component that observes all your ifs, all your conditional jumps, and it tries to remember whether it took the branch or not the branch last time.
01:12:24.204 - 01:12:57.674, Speaker A: And then this is where sort of speculative execution comes into, into play, where if it runs that code again, the branch predictor is going to say it's probably going to take the branch or it's probably not going to take the branch. So start running that code under the assumption that it will or won't, and then if it doesn't end up doing that, then like go back and unwind what you did and then do that stuff instead. Or maybe receive can just return a list of values. It could. It's usually nicer for receive just to return an option. So you can use it as an iterator. And this way it'll just be fast.
01:12:57.674 - 01:13:46.830, Speaker A: Regardless, if we returned a list, we would have to allocate the list every time. What about extending buffer with inner queue drain? This would save memory, will probably be a lot slower. It wouldn't actually save memory. You would still end up with both of them having to have capacity. All right, so now I think it's time that we, now that we have an implementation that's pretty reasonable, it's time that we try to talk about some alternative implementations. Usually what you'll see is that there are multiple, so there are two kinds of implementation differences you're going to see. These are either, these are usually referred to as flavors.
01:13:46.830 - 01:14:25.774, Speaker A: So. Well, actually there are more than two, let's say there are multiple different kind of flavors, flavors, you'll see. And usually they take one of two approaches. One is that there are different types for different implementations of channels. We saw an example of this in the standard library where there are two different sender types for, for the different flavors, right? One synchronous flavor and one asynchronous flavor, although not async, but asynchronous, as we talked about. For channels. The other approach is to instead just have a single center type, and then under the hood have basically think of it as an enum, although that's usually not how it's implemented.
01:14:25.774 - 01:15:21.006, Speaker A: Of under the hood, it like figures out what type of channel it is, and that way you can use the same sender type no matter where you are. In practice, the implementations tend to vary in what they do, but they all usually have this notion of flavors. And the idea behind flavors is that you, you have multiple implementations of your channel, multiple backing implementations, and you choose which one you use depending on how the channel is used. So flavors, I can't spell. So there are some common flavors that we've seen. One is synchronous, synchronous channels. One is asynchronous channels, another's rendezvous channels, which we haven't talked about yet.
01:15:21.006 - 01:15:48.510, Speaker A: And the last is one shot channels. These are usually the flavors you see. Sometimes they're represented as different, like explicitly different channel types, but very often they're sort of under the hood. You won't see whether or not they're there. It's something that is dynamically chosen. So a synchronous channel. This is a channel where send blocks were send can block.
01:15:48.510 - 01:16:24.548, Speaker A: Usually it has limited capacity. This is a channel where send cannot block. And this is usually unbounded. So any number of sends you can build up much as stuff possible in memory. A rendezvous channel is a synchronous channel with capacity equals zero. So the idea here is that a rendezvous channel is really, it doesn't let you send things. It's usually a channel that you use only to synchronize two sides.
01:16:24.548 - 01:17:26.358, Speaker A: Very often you see rendezvous channels just have the, have t bede like unit, like the, the empty tuple. And the idea here is that it's used for, I don't want to say time synchronization before like thread synchronization. The idea is that if you have, you have one thread that you want to kick another thread to make it do something, you don't actually want to send anything to it, you just want it to do stuff. That's where you get into a rendezvous channel, right? You create a channel that has capacity zero. So, and what capacity zero means is that you can only send if there's currently a blocking receiver, because you can't store anything in the channel itself. So the only way to actually send is to like hand it over to a thread that's currently waiting. And that basically means that both threads must be like one thread must be at the receive point of its execution and one thread gets to send point and now they've rendezvoused, they, they're both at a known location and then they can move forward from there.
01:17:26.358 - 01:18:21.252, Speaker A: This is often achieved with something called a barrier. You find one of those in standard sync as well, but you can do it with a channel. The channel version is, is still, it ends up being a two way synchronization because the receiver also can't proceed until the sender arrives. I'll get to questions. I've done the last flavor and one shot channels are channels that you only send on once. So usually these can be any capacity, although in practice only one call to send. So these are often things like imagine that you have an application where you have a channel that used to tell all the threads that they should exit early, right? Like the user pressed control c or pressed x or something, and you want them just all to shut down.
01:18:21.252 - 01:18:59.406, Speaker A: You might have a channel that you only send on once and you don't send anything useful, although you could, and then all the like that some thread is running somewhere and when you send the signal the thread is going to like drop what it's doing and shut down. And so that channel is only ever used for one send. So it's a one shot channel. And these flavors are different enough that you can have different implementations that take advantage of their patterns. And we'll look at some of those in a second. All right, let's do questions about flavors. I've heard the term bounded and unbounded for synchronous and asynchronous, in case others are wondering.
01:18:59.406 - 01:19:52.170, Speaker A: Yeah, synchronous channels are often called bounded channels because that's what they are. Asynchronous channels are often called unbounded channels. If you look at the ones in Tokyo sync, and I think also the ones in crossbeam, in fact, in many of the other implementations, these are referred to as unbounded and bounded, in particular because of the potential confusion with async. Ronde vu. Rendezvous. Right, sorry. So basically what you should use if you need a convar, but don't have a mutex with locked data.
01:19:52.170 - 01:20:34.922, Speaker A: So a rendezvous is not a mutex because it doesn't guarantee mutual exclusion. It is sort of like a convar in that you can wake up another thread. That's true, but it doesn't give, it doesn't give you any way to also guard the data. More like Unix pipes. I mean, all channels are sort of like Unix pipes. Can rendezvous channels actually send anything useful? Yeah. So the way this works, you can totally send data in a rendezvous channel, right.
01:20:34.922 - 01:21:11.312, Speaker A: It still has the t type, but the idea is that if the sender can only send if the receiver is present, and the receiver can only receive if the sender is currently present, if they both are present, then you can just like hand the data over. You can think of it that way. It's just that the sender can't like put data somewhere and keep going because the capacity is zero. But if there's a handover, it can hand data over. Yeah. Seems rust has a lot of specific impulse design of channels, while goal just has a simple channel implementation. While go only as a simple channel implementation.
01:21:11.312 - 01:21:45.794, Speaker A: As far as I'm aware, go has all of these implementations. So specifically, in go there's only one type. Just like in rust, there's only one type, at least for things like crossbeam. These flavors aren't different channels. Like, they're not different types in the type system, they're different implementations that are chosen between at runtime and from memory. Go does the same thing. The way it works is basically, initially you assume that the channel is a one shot channel, and the moment an additional send happens, you like, upgrade it to be a different type of channel.
01:21:45.794 - 01:22:30.044, Speaker A: And so this means that the first send will be more efficient in a sense, than the later ones. Similarly, a rendezvous channel, you know, it's a rendezvous channel because the capacity is set to zero. So you can just choose that flavor and synchronous and unsynchronous. You choose based on what the capacity is set to. Could a synchronous channel or sync channel where t equals unit be a rendezvous channel? Yes, that is, that is like a rendezvous channel is any channel whose capacity is zero. Specifically it is a synchronous channel for any t where the capacity is set to zero. Kind of like a baton pass.
01:22:30.044 - 01:22:59.390, Speaker A: Yep. Okay, so in the last few minutes, what I want to talk about is different implementations. I also want to try to touch on async as an actual async, as an async await. In futures we'll see whether we get to that. So for a synchronous channel, what we implemented was a mutex. We didn't actually implement a synchronous channel, we implemented an asynchronous channel. But you can do a synchronous channel with a mutex plus convar as well.
01:22:59.390 - 01:23:53.500, Speaker A: And usually what you do behind the scenes is very much the same thing. Use a vectek and you just like have the sender block if the vector happens to be full. So the implementation is fairly similar for an a. If you want to not use a mutex, what do you do? There are a couple of different approaches here. The simplest way is that you use basically an atomic vectec or an atomic queue. And the way this usually works is you have head and tail pointers, just like the way that a vectac is implemented, but you update them atomically. And this means that you can now you don't need to take a mutex in order to send, which happens to help a lot.
01:23:53.500 - 01:24:59.614, Speaker A: And as long as you update the sort of head and tail in the vector atomically, there are ways to ensure, like there's basically an algorithm for how to implement this data structure in such a way that no thread ever tries to touch data that another thread is currently touching. And then for wakeups you use sort of the thread parc and thread, thread thread notify primitives that are in the standard library, or you can use the ones from parking lot if you wish to ensure that things are woken up appropriately. Basically you need some signaling mechanism, right? Where if the sender is asleep because it's blocking, the receiver needs to wake it up if it receives something, because now there's capacity available. Or similarly, if the receiver is blocking because the channel is empty and a sender comes along and sends something, it needs to make sure to wake up the receiver. And so you need some kind of notification mechanism. Often it's park and notify, although it doesn't have to be. And this kind of like atomic vector is very often the implementation you see.
01:24:59.614 - 01:25:49.288, Speaker A: It's basically a fixed size array where you atomically keep track of the head and tails. That's also the only implementation I really know about there. I think Flume, which is a one of the implementations I'll point out later, I think it actually uses a mutex, but it does it in a slightly smarter way than we have. We have a sort of dumb implementation of mutexes, but there's some smarter tricks you can play, and some of them have been mentioned in chat already of like tricks you can play with the mutex implementation to make it slightly faster. Like take advantage of the fact that there's an arc there that I believe flume does. And I think crossbeam uses the atomic vectec approach. Or they're not actually using a vectec, but they're using a sort of head and tail pointer implementation, asynchronous channels.
01:25:49.288 - 01:26:45.776, Speaker A: Similarly, the thing we did was mutex, Convar and a vecdec. In practice, Vectx have some sad properties like resizing. So very often what you want to do, and this is one of the few places you actually want to do this, is you use a linked list. So what that means is you never resize, right? Because when a sender comes along, you just append to the linked list, or you don't even have to append, you can pop from the linked list, sorry, not pop. You can push to the front of the linked list and then what the receiver does is it just steals the whole linked list, it sets the head to null or to none, and it steals the whole linked list and then it walks it backwards. And that is an implementation that doesn't require resizing, it doesn't have the memory problems that the vector does. And it plays the same trick that we did below, where if you take the mutex as a receiver, you can steal all the items rather than just one.
01:26:45.776 - 01:27:40.080, Speaker A: Usually you want this to be a doubly linked list so you can efficiently get to the end, or you can just keep track of the tail. And for non mutex implementations, usually what you see here is an atomic linked list. This is often referred to as an atomic atomic linked list or just an atomic queue. So here it's basically the, it's not a ring buffer like a vecdec is, but it's an actual atomic queue. Usually that the implementation is a linked list, but it doesn't have to be in cross beam. What they do is actually kind of interesting. It is an atomic, I don't know what to call it, like block linked list.
01:27:40.080 - 01:28:53.506, Speaker A: And the idea here is that rather than, rather than have like every push be an atomic operation to append a new item to the list. What you do is you sort of mix this like atomic head and tail thing with an atomic linked list. So instead of having a so this is a linked list of t. This is a linked list of like a atomic vector dec t. So only occasionally do you need to do the sort of only occasionally do you need to actually append to the linked list, which is a problematic operation because if imagine you have two senders that both want to send at the same time with a linked list, one of them is going to succeed in updating the next pointer of the tail of the linked list, but the other will fail and we'll have to retry. If you have a list of these blocks of t's, then only occasionally does a thread actually need to update the next pointer. Usually they just need to increment the tail, which turns out you can do concurrently using fetch add.
01:28:53.506 - 01:29:34.692, Speaker A: And so this sort of block atomic linked list turns out to be a lot more efficient in practice. And here too you need some signaling mechanism for waking people up for rendezvous channels. It turns out you don't need the linked list at all. All you really need is this wake up primitive and then a single place in memory to sort of store the item for the handoff. I haven't looked too carefully at the flavor implementations of this. I know that crossbeam has one, the standard library has one. I don't think Flume has this optimization yet, but the trick to play here is basically you can get rid of the whole linked list part and you can have a much simpler implementation that just synchronizes the threads.
01:29:34.692 - 01:30:14.060, Speaker A: Almost all you need is a mutex and a convar lim practice. I think these things, they're a little bit smarter. And a one shot channel, if you know that you have a one shot channel, is the same thing. You don't need a linked list of any kind, you only actually need to store the one t. And because you know there's only one item, what you can do is basically just have an atomic place in memory that is either like none or some, and you can just atomically swap the element in there. Think of it like a single slot, and so the sender just fills the slot and the receiver consumes the slot and marks the thing as sort of empty at the same time. Or completed at the same time.
01:30:14.060 - 01:30:40.430, Speaker A: So these are basically, you can write more specialized implementations that are faster for those use cases. Okay, let's discuss that. Brieftain briefly. We're getting towards the end here. How do async await channels different implementation? We'll look at that in a second. YouTube stream is gone. Yeah, YouTube is not always great at streams.
01:30:40.430 - 01:31:30.016, Speaker A: Doesn't a linked list guarantee to always end up doing an allocation deallocation on each push and pop? Yes, with a linked list you will be, you will be allocating and deallocating on every push or pop. Right. The push is going to allocate a node to stick on the end of the linked list, and a pop will have to deallocate that node. This is another advantage of the sort of block linked list variant. Of course, often the allocation, the memory allocation system is not your bottleneck usually, especially if you're using something like jmalloc, you have basically thread local allocation. So this turns out to be fine. But it is true.
01:31:30.016 - 01:32:18.364, Speaker A: That is a downside. So you're really measuring memory overhead versus memory allocator performance. Wouldn't a smart list just keep spare nodes around? So one option is to basically keep a pool of these nodes and reuse them. The problem is now you need to atomically manage the pool, which also needs a bunch of synchronization primitives to do correctly. But in theory you can. It's not clear that you can write a better implementation of a reusing pool than the memory allocator can allocate and deallocate memory. Maybe, but it's unclear.
01:32:18.364 - 01:33:31.756, Speaker A: You might want to use an arena allocator, and that might work well. All right, so I think the last thing I then want to touch on, because we're sort of out of time, but I'll do it anyway, which is there were some questions about like Async await. What do you do with async await? It's pretty hard to write a channel implementation that works for both async await, like the futures world, and for the blocking thread world, because the primitives are a little bit different. Right. If you do ascend and the channel is full, then in the async await world what you want to do is you don't want to block, you want to yield to the parent future, yield to the executor, ultimately yield to the task, and then at some point in the future you'll be woken up to poll again. And that sounds a little bit like sort of waiting on a convar, but in practice it's not quite the same, because you actually need to return rather than sort of, you don't get to sit in the current function. And the same thing for receive, of course.
01:33:31.756 - 01:34:24.008, Speaker A: And the notification primitives are a little bit different, although they do have the same flavor of like you notify a waker that's going to cause that other thing to be pulled again. So they're similar, not quite the same. Where it gets hard is to write an implementation that internally knows whether it's being used in a context, like in future context, or in blocking context, without exposing that to the user. Very often what you might end up with is like some additional type parameter that is like waker or like signaling mechanism, which gets really ugly. Now there are ways to do this. If you look at both flume and cross beam, I believe have both, both blocking and asynchronous versions. If you look in the code, you might be able to see how they do that.
01:34:24.008 - 01:35:07.570, Speaker A: And basically it requires a bit more bookkeeping. You have to be a little bit more finicky with the types. And usually what you end up with is a channel that looks much the same but not quite the same. And at runtime it basically ends up diverging into different ways of managing the underlying datastore or the underlying data structure, depending on whether you're in the blocking world or not. For example, often if you're in the blocking world, you can do some additional optimizations you can't always do in the async world and vice versa. They're just a little different. But in practice, the data structure that's used, like whether you're using a vector or an atomic linked list or anything like that, that is fairly similar and the flavors that exist are fairly similar.
01:35:07.570 - 01:35:45.256, Speaker A: You could probably beat the allocator, because we always need allocations of the same size. Allocators are really good at taking advantage of repeated patterns. So it's not clear to me actually, because it's really hard to write good performant. Basically our bridge collection and the memory allocator has had a lot of practice with it. You could use a bump allocator or something like that. Do you use any channels in your thesis? Yes, many. I would never roll my own channel.
01:35:45.256 - 01:36:26.350, Speaker A: It's a bad idea unless you specifically want to work on concurrency stuff, which like is fun. But I use the Tokyo channels because I needed one though with async and the Tokyo one I know pretty well, I don't have a particularly strong feeling about it. Noria is not really channel bound, so the decision wasn't that important. I couldn't use the standard library one because it didn't support async await. And I think crossbeam, at the time when I started adding them didn't. But I might be wrong about that. Great.
01:36:26.350 - 01:37:19.130, Speaker A: Okay, so just to give you some pointers of where to look next, if you, if you're curious about this, if you want to see how a real implementation work, I recommend that you actually have a look at the standard library implementation. If you go look at the NPSC model module, you probably don't want to read it through like the Docs RS interface, but it has really good documentation, like what's going on under the hood, like what is the implementation. Some of the optimizations that they do, like internal atomic counters. Similarly, if you go to crossbeam, this will be bright for the people of you who are reading this at night. In crossbeam there's a crossbeam channel subdirectory that holds the crossbeam channel implementation. And if you look at it, you'll see that there's a flavors directory that holds all the different flavor implementations. You see, array is the one that's like for synchronous channels that does this head tail business, this list for the atomic block linked list.
01:37:19.130 - 01:38:01.820, Speaker A: And some of these are for things like rendezvous channels and one shot channels select we didn't really get into, but there's usually a bunch of additional stuff you need to do to support selection. Selection is the ability to, for example, receive from one channel or the other, whichever sends first, which requires some additional mechanisms in the implementation. There's also flume. So flume is a different implementation of channels that popped up fairly recently. It has a very different implementation to what crossbeam does. There's no unsafe code. And part of the idea here is that it uses mutexes under the hood, but in a slightly more clever way.
01:38:01.820 - 01:39:16.942, Speaker A: I think the experience I've had is that crossbeam is better for very high contention cases because it doesn't use mutexes, whereas flume is often faster for cases where contention is lower because then mutexes end up not adding quite as much overhead. All right, I think that's all I wanted to cover. Let's see. Are there questions about this before we end for today? Like sort of at the tail end of the channel here? Any thoughts on benchmarking channel implementations? Benchmarking channel implementations is hard. I know there's been some work on this, I forget where that was. I think burnt sushi did a bunch of benchmarking of channels looking at like go channels, the standard library channels flume and cross beam channel his chan crate, which I think is deprecated now, and that's worth looking into. In general, when you benchmark channels, you want to try to basically benchmark all of the different flavors because they do represent real use cases.
01:39:16.942 - 01:40:18.100, Speaker A: You want to benchmark things where you send, the things you send are large, the things you send are small. There are many senders, there are few senders. So for example, if you have a single producer, single consumer case, you might be able to optimize better for that, basically write a flavor for it, and that might be able to perform much better than your sort of general multi producer, single consumer version. You want your benchmark to test cases where the channel is usually full, for a bounded channel, cases where the channel is usually empty, which basically means you adjust the relative rates of production and consume calls. The number of senders is important, like how do you scale with the number of senders? So you basically want to do a grid of all the possible configurations and then try to benchmark each one separately. Rendezvous channels are like the default go channels with zero capacity. Yeah, that sounds about right.
01:40:18.100 - 01:41:12.662, Speaker A: A bump allocator would be really good, since you would likely allocate memory atomically. Quite possibly, and also because you don't need to drop anything in that case, because the memory has already been handed off. So the drop implementation is a no op. So you might be able to use something like bumpalo, which is pretty cool. How do they support async without tying it to a specific executor like Tokyo? So the primary reason for the current lack of harmony in the async await ecosystem is around the IO traits like Async read and async write. And also the spawn feature being able to run a future in the background for implementing a channel. You don't need either of those.
01:41:12.662 - 01:42:16.830, Speaker A: All you need is like the primitive that's provided by the standard library, which is the waker trait, and the ability to sort of yield or go to sleep, and the ability to wake something up or notify something. And those are the same. They're from the standard task module in the standard library. And so you can use those independently of what the executor is. And so that's why a channel, sort of trivially, is cross executor. If you have a sleeping sender thread and you'd like to wake it if the receiver is dropped so they can free up resources, is there a standard way to do that? Just have it wake up every few seconds to check? No, you do it the same way we did in our implementation here, right? You implement drop for you would implement drop for the receiver where it will do a notify all to wake up all sleeping senders which could then do whatever freeing up of resources it needs to do. All right, I think we got it.
01:42:16.830 - 01:42:43.850, Speaker A: I will as always, put the recording up on YouTube. It might even have the intro that I demoed early to the people who showed up to the stream early. It should be up hopefully in like a couple of hours. I'll tweet about it as I always do. And apart from that, thanks for watching. Hopefully you learned something. Hopefully there'll be some more of these and as always, just like follow me and you'll learn about upcoming ones.
01:42:43.850 - 01:43:08.530, Speaker A: I also announce them on Discord now. There's like a channel with automatic updates whenever I go live, whenever I upload a video, or when there are upcoming streams. And that also includes other rust streams, streamers. We have a bunch of them on there now, including Steve Klabnik, which is pretty. So like join the discord. I'll put the link somewhere, maybe someone put it in chat already and I'll see you there or on Twitter or in the next video. Thanks for joining.
