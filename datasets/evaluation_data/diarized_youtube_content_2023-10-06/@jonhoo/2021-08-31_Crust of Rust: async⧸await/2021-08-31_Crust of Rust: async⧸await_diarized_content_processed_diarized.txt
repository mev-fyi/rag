00:00:06.280 - 00:00:32.840, Speaker A: Hi, everyone. Welcome back. It's been quite some time until since the last stream, and it's because I've been on vacation, which has been really nice. It was an odd vacation, but that's a story for a different time, q and a or something. So this is going to be another crust of rust. So it's going to be one of the shorter streams of. And in particular, what I wanted to tackle this time around was async await.
00:00:32.840 - 00:01:31.010, Speaker A: Async await is something that, like, has. I've been asked about it a lot. I have a stream on async await. I'll link it up here in the final video. But that goes a lot more into the technical detail of how does the machinery work and how do the traits work and the bits underneath. But it doesn't really talk about just like how should you think about async await? How should you use async await? What does it look like for you as an end user or as a developer, rather? And so I wanted to take a crust of rust as sort of a step back in complexity and just look at the mechanism, but from like a usability standpoint and just talk about more sort of the intuition and the mental model you should have more so than the nitty gritty details of how it actually functions. For that, you can see the other video.
00:01:31.010 - 00:01:56.700, Speaker A: In case you were unaware. I'm John. I have a Twitter account. If you want to know about upcoming videos, you can go follow me there. I'm also writing a book, and async is actually one of the reasons why I'm writing this book, which is the original Rust book. The rust programming language, which is fantastic, doesn't cover async. Like, there is no chapter on Async, and I think they are going to add one.
00:01:56.700 - 00:02:37.780, Speaker A: But one of the challenges has been the async ecosystem has been so much in flux, has been hard to actually write one. I do cover async in my book there. I cover it more in terms of the lower level details because this book really is for those who want to take a step beyond the basics and understand how stuff actually works in rust at a deeper level. But go check that out if you're interested in rust, which you probably are, given that you're watching this, also read the Rust book. If you haven't read the rust book, it's fantastic. Now, there have been some efforts at sort of explaining async programming in rust. There is the Rust Async book, which still has a lot of to dos.
00:02:37.780 - 00:03:24.976, Speaker A: It was written a little while ago. It explains things in, at least to me, a little bit of an odd order. I think there are some efforts underway to sort of rejig it to make it easier to follow and sort of go through a more maybe pragmatic approach of how should you think about async await before diving into the details of how it works. It might be that by the time you watch this video, like much later, this book is the place to go. But I thought for now, a video might be a good way to sort of fill out some of the things that it doesn't really talk about. I also highly recommend you take a look at mini redis. So Mini Redis is a little project put out by the Tokyo project, which is a reimplementation of a redis client and server.
00:03:24.976 - 00:04:20.376, Speaker A: It's incomplete. It's sort of a toy example, but it tries really hard to show you how to structure an asynchronous application client side and server side. And it's really well documented, sort of written to be a piece of code that you can read to understand what's going on and to understand some of the design principles and thinking that goes into it. I highly recommend you give that a read if you get a chance. Now, I will also say that, here, let me pull this up for a sec. I will say also that while there are a lot of different executors out there for asynchronous code, I will generally be focusing on Tokyo just because it's the most widely used one. It's the most sort of most actively maintained one that now most of what I'll be talking about will be independent of the executor.
00:04:20.376 - 00:04:47.776, Speaker A: I'm not going to go into sort of Tokyo specifics here because it's not really relevant how you think about async code, but I just want to sort of lead with that as sort of a flag that I'll generally be having that in mind. Let's see. Oh, no, it's got the wrong video title. That's all right. We'll survive. Actually, I can fix that right now. Let me fix that right now.
00:04:47.776 - 00:05:28.866, Speaker A: Give me 1 second. That's annoying, but I can fix it. Update titles. Update. Now, this should not say that. It should say, let's see, in theory. Now, the title should be.
00:05:28.866 - 00:05:46.412, Speaker A: Should be something semi reasonable. Um, okay, so let. Let's start out with something that is code, right? Like, I like code. We like code. And we're going to call this patience. Patience. And we're making a.
00:05:46.412 - 00:06:05.838, Speaker A: Let's make it a bin patience. Right. Because you need to await it. Get it? It's pretty, pretty clever, pretty clever. So let's go in here and look at your main function. All right, so we'll leave the main function as it is. Let's say that I write async event foo.
00:06:05.838 - 00:06:50.404, Speaker A: All right, so the sort of simplest async function you can write. All right, let's allow dead code. That's fine, like so. So we have an async fn foo. What does that mean? Right. Well what, what is the, what is this async keyword? What does it do? Well, first and foremost, the async decorator on functions just really means this output equals async. This, these are the same now I guess use standard future.
00:06:50.404 - 00:07:17.470, Speaker A: Future. So if I have an async fn foo one, let's make this foo two. These two are equivalent. They do the same thing. In fact, this basically gets turned into this. So the async keyword on a function is not special, it's just a sort of transformation directive to the compiler. There are some differences, but we'll get to those in a second.
00:07:17.470 - 00:08:09.550, Speaker A: Now, notice that this, this output variable for the future is the same as the return type of the function. So if this was a user size and it returned, say zero, then this would be output usize and async zero. And what this syntax means. So we're returning some type, we're not going to name it, but some type that implements the future trait. And the future trait means it sort of signifies a value that's not ready yet, but it will eventually be a use size. So in this case for the async block, we just return zero immediately. But this thing that we give back is sort of from the JavaScript terminology, a promise that I will eventually give you a use size.
00:08:09.550 - 00:09:04.672, Speaker A: I'm not going to tell you when, it's just at some point in the future this will resolve into a use size. And in fact, if I here say let x is foo one, what happens now? I don't actually care about this. Let's go with unused variables too. So if I now create a variable x that is the result of calling foo one or foo two, their equivalent, right. Then x now is not a use size, right? This would not compile, and in fact the compiler is pretty helpful here, right? It tells me consider awaiting on the future. And what it's trying to tell you is that foo one is not a useize. What foo one returns is a thing that will turn into a use size eventually, and it's telling me I need to await.
00:09:04.672 - 00:09:46.976, Speaker A: And what await means is don't run the following lists of instructions until this actually resolved into its output type. That's all await really means. What might surprise you though is if I here, let's say add a print line foo, right? Which, which would be equivalent to doing this println foo. These are the same. Then this will never actually print foo. And I can show you that patience cargo run. So prints hello world.
00:09:46.976 - 00:10:13.070, Speaker A: But it does not print foo. And this might seem counterintuitive, right? Like we called foo one. The first thing in foo one is print foo. So why doesn't it print foo? Right, like this is a future. Sure, but this code is right here. There's no await in here. The reason for this is because a future does nothing until it is awaited.
00:10:13.070 - 00:11:01.540, Speaker A: And in particular here, the future just describes a series of steps that will be executed at some point in the future. And we'll get back to exactly how those get executed. So here the first time this future is awaited, then it will print line foo and then it will immediately return zero and resolve. Now there might be other cases here. So for example, imagine that in here we called foo one. Dot await, right? And then we print line c, one, foo two. Right? So here you can imagine that if we here do foo two, we await the future that's returned.
00:11:01.540 - 00:11:39.184, Speaker A: It's going to start running because we're awaiting it. Then it's going to print one. Then it's going to call foo one, which is going to produce some other future. And then it's going to wait foo one. And it could be, in this case, foo one will return immediately because it resolves to zero, but doesn't do any like network operations or anything that might have to wait. But imagine that it did imagine that this was like read to string, string of some file, right? Then that reading of that file might actually take a while. It might not be ready to yield the bytes or whatever that it read, or the string that it read immediately.
00:11:39.184 - 00:12:49.980, Speaker A: As what will happen is that the program will sort of wait here because we told it to, right? We told it to await until the result of this future is, is ready. And so it'll print foo one and then it won't print anything for a while because it won't print until this has resolved. If on the other hand, we did this right, then it would print foo two straight away because this would just return a future. But we're not telling it to await anything, so no actual work happens. And therefore, the way that you can think about async blocks is really that they execute in sort of chunks. So let's say, let's just sort of copy paste this a bunch and say let's do something like this. And this is going to be file one, file two, file three, file four, right? You can sort of think of this as there's sort of a first time it executes until here, wait heredeze, and then second time.
00:12:49.980 - 00:13:51.080, Speaker A: And when I say time here, what I mean is when this has completed, so it'll start executing here, it'll run all the way until here, and then it won't do anything. It'll sort of yield back. You can think of it as sort of doing a standard thread yield now, sort of in a loop. So it'll do something like, in fact, let's try to write out an example of what this does. So few is going to be this, while not few is ready, yield now and then fu try complete. You can sort of read it like this, that it's going to check whether that future is done. And if it's done, then I guess here, like result is take result or something, right? Like it.
00:13:51.080 - 00:15:01.092, Speaker A: So while it's not ready, it's just going to sort of spin and yield in practice is not what happens. But you can have that as sort of the mental model that when while it's not ready, it lets other things run. And then every now and again it's going to check whether the future has made progress towards completing and making progress for a future sort of means getting to the next await point. So when a future does get to run, it'll continue executing from the last point it yielded until the next point where it might fail to make progress. So, so let's say that, let me return this to what it was, which was this. So let's say that this future has sort of gotten to run a few times, and right now it's sort of waiting on this, right? That's how far it got last time. And now let's say that the read of file three finishes.
00:15:01.092 - 00:15:56.602, Speaker A: So now that the read of file three finishes, this await resolves, right? So you can think of like we exit the while loop that's yielding to other people and then we get to execute. And let's say here that we had like a expensive function, right? In fact, let x pass x to expensive function. So because there's no awaiting here, once this await resolves, we get to run. And we get to run, sort of imagining that there's no async, we just execute instructions which include potentially calling all these expensive functions. We might call, we might do all sorts of things and we're not going to get interrupted. Other things aren't going to run in our place, although at this point we're in the standard threading model, right, of our thread might get interrupted and some other thread might get to run. But apart from that, there's no, there's no magic related to Async.
00:15:56.602 - 00:17:20.644, Speaker A: At this point we're just going to keep executing as if we were a normal function all the way until we create another future and then await that future. At that point we're going to sort of yield again and wait on that to complete. So once this awaited we got to execute this chunk of code which contained no awaits, and we execute all the way until the next await point. All right, does this basic sort of async lets you chunk computation? Does the rough idea or this rough structure here make sense? Before we continue, there are definitely differences between JavaScript promises and this. I'm not saying they're the same, I'm more saying that in terms of the naming, it's a useful comparison that a future can also be thought of as a promise, not in terms of the actual JavaScript type promise, but in terms of the word promise. Alright, so you can think of an async function as, or any async block. In fact, anything that is a future as executing in chunks, it runs until it has to wait, basically until it cannot make progress anymore, and then it yields, which is effectively what this await turns into.
00:17:20.644 - 00:18:42.580, Speaker A: So, so I gave you one desugaring of a weight. Another desugaring of await is something like let few, let's say we write let x is read to string file dot await. You can sort of think of that as being rejiggered into, into creating the future. While it's not ready. We're going to do a, or maybe even while trying to find a nice rustic way to sort of say that this works. So we're going to loop, and if we're going to say if let sum result is fut dot try check completed, then break with result. Otherwise we're going to few dot try make progress and then yield.
00:18:42.580 - 00:19:41.370, Speaker A: Where yield you can sort of think of as a, as a thread yield now, but, but it has this additional property that you actually end up returning all the way up to where this future was first awaited. Like think of it as there's sort of a stack of things that are awaiting each other, right, you have, right. So here main waits for foo two foo two waits for read to string. Let's say read to string waits on foo one. So you can sort of think of this like a call chain of things that are awaiting each other. And whenever you yield, you actually return all the way to the top of that call stack and sort of return to there. But the next time something calls, like try check completed or try to make progress, it continues from the yield point of sort of the bottommost thing that previously called yield.
00:19:41.370 - 00:21:37.782, Speaker A: So this is sort of the way to think about a weight that it is a loop that's yielding whenever it can't make progress. Does that roughly make sense? So you might wonder, okay, why do we do this? Like why is this interesting? So the reason this is interesting is because imagine that you have many of these, right? You have a bunch of futures. So you might, for example, have one future for reading from the terminal, right? Like you're waiting on the user to write something into the terminal, and you're also waiting on, let's say, a new connection coming in over the network. You have two futures, and you don't really control when either of them will resolve, right? You don't control when the user types something into the terminal, and you don't control when some external program is going to connect. So you sort of want to wait on both of them, and you don't really care which one happens first. Well, if you were in a threading system, what you would have to do is you would sort of do, let's say, how would we do this? So we're going to have a read from terminal and we would like spawn a thread of, which would like lock IO standard in lock. And then we would like for line in X dot lines.
00:21:37.782 - 00:23:13.756, Speaker A: And then we would do something on user input, and then we would separately have a thread that's like read from network, which would do thread spawn. And it would say mutex is standard net TCP listener bind, and it would do, you know, bind on some port, and then it will, while letheme stream is x dot accept, this has to unwrap and then do something on stream. And now this isn't too bad. Why does this require an argument? I standard standard in? Yeah, yeah. I don't actually care about the compiler errors here, so this isn't too bad, right? We have one thread for handling terminal reads and one for accepting connections, but it gets worse if we have to have one thread for every operation that we have to do, if only not even thinking about performance. But it just gets annoying to wrap your head around that now. Okay, let's say that for each stream we need to call like handle connection, right? So we want to handle connection of the stream, but now we have a single thread that's managing all of our connections.
00:23:13.756 - 00:23:55.758, Speaker A: And imagine like a bunch of users are connecting to download some large files or something. You'd really like to be able to use many threads, or at least if like if one connection is full, you want to be able to write on the other. And that gets really weird if there's one thread, because at some point like handle connection has to call like stream dot write. And imagine one of the clients is really slow. So you're not going to be sort of stuck writing to the slowest client, even though all these fast clients you're able to write to. So we maybe then need to like spawn a thread for each stream, right? They're going to handle the connection for that stream. And now we get like a thread handle back.
00:23:55.758 - 00:25:33.434, Speaker A: We need to do something with that so that we remember to wait on the thread if we're trying to shut down the server. Now we have all of these threads running and like it's possible to write programs this way and many big programs are written this way, but, but it feels weird somehow, right? What, what I'm going to propose to you with the async model is you can actually make this simpler. The async model lets you model this in a way that matches your thinking better. So let's say that there's a read from network, not, not these, not these variables I declared up here, but some function, right? So I'm going to call that network and then there's going to be a read from terminal, right? And let's say these are both futures then now what I can do is I can say I can do select, and we'll talk about what select means. I, and then I'm going to say stream is, and there are lots of different syntaxes for here. I'm just going to sort of make one up network await and line is terminal. So this the reason why do something with line here, the select macro, which exists in a bunch of different libraries.
00:25:33.434 - 00:26:33.220, Speaker A: Like there's one in Tokyo, there's one in the futures library. What it does is it waits on multiple futures and it tells you whichever one finished first. So under the hood, what really happens here is that it, I'm not going to define what it is yet, but the code, the select macro, if you will, is going to try, sort of remember how we wrote the desugaring down here of try check completed and try to make progress, right? It's going to try to make progress on network. And if it does make progress on network, then it's going to give you a stream and call the code that's in here. If it doesn't make progress on network, then it's going to try to make progress on the terminal. And if it makes progress on the terminal, like if the terminal now has new lines, then it's going to run this code with the line that it got. If neither of them make progress, then it yields, and then at some point in the future it's going to retry, right, and try to check progress again.
00:26:33.220 - 00:27:41.950, Speaker A: And in practice, it's actually a lot smarter than this. What really happens under the hood here is that when you yield, you don't just say yield, you say yield until this thing happens. So for a network socket, for example, what you yield is yield until something happens on this network socket. On this file descriptor on the terminal, you say yield until something happens on this input channel or this, in this case, standard in, right. And behind the scenes, whatever is running the future and sort of has this loop of, of trycheck completed and try and make progress and such, what it's actually going to do is use some operating system primitive under the hood to be smarter about when it tries, when it retries. So it's not actually doing like a spinny loop or something, it just goes, I'm going to try this, I'm going to try this. If neither of them succeed, I'm going to sort of go to sleep and then retry once the operating system has notified me that it's worth retrying.
00:27:41.950 - 00:28:49.542, Speaker A: And it can be even smarter than this, right? It can even realize that, oh, nothing has changed about this network socket. So I don't even have to try to make progress on this, but the terminal state did change. So I'm going to retry this operation. Okay, so that's one of the reasons why futures is handy, is because you have this, you have this nice way of trying multiple things at once. Because a future is not code that executes all at once. It is this chunked operation that also sort of describes how to try again later, which makes it really nice for doing any kind of programming wherever you have primarily IO, whether that's networking or disk or even timers, like anywhere where it's not just like spinning on computation there. Async doesn't really add that much, but if you ever have reason for some part of your code to wait for something else to happen, then what Async allows you to do is sort of give up the threads time.
00:28:49.542 - 00:29:30.484, Speaker A: Like if you have a thing that needs to read from, read from disk while it's waiting on the disk, Async allows something else to run instead. Right. And you can see that here, while the network is not, does not have anything for us to do, we can read from the terminal instead. If neither of them have anything to do, then we can sort of go to sleep. We don't have to do anything. And that's what async sort of allows you to, to express very nicely. The other way to think about Async await is sort of as a big state machine, right, of you're in the current state of the future and there are multiple paths forward.
00:29:30.484 - 00:30:38.972, Speaker A: One path forward is that something becomes available on the network socket. Another path forward is something becomes available on the terminal socket. And then you sort of follow the appropriate edge in the state machine and then you run the code in the next one. Now, in the case here, right, where we don't really have a branching, so select, you can think of the select operation as allowing you to branch the execution here, there's no real branching right here, there's just, we're just saying if we can't do anything here, we just await, and if so we just yield, right? And then we're going to continue running from here whenever it's possible for us to make progress. So it's really the state machine in this case is we're either in the state of being here before. Let's make before reading file one, or we're in the state of, before reading file two, or when the state of reading before file three, and the edges between these are file one has been read, file two has been read, file three has been read. And these compose really nicely, right? So here, let me see if I can express.
00:30:38.972 - 00:31:35.480, Speaker A: Let's add another thing here. Let me tidy this up a little because there's a lot of annoying errors in here. And then I'm going to do async, then read to string like so, just to make it stop yelling at me. Expensive function takes nothing great. It's yelling at me less now. So let me get rid of these as well and say, these are fine, I can leave those in. So let's say that we also here said foo, which is foo two, dot await.
00:31:35.480 - 00:31:57.870, Speaker A: So now when food, when we call foo to, right, we get, we get something that doesn't execute straight away. It doesn't execute until we await it. And now imagine that the network has nothing to do. The terminal has nothing to do. So foo two gets awaited. Okay, so it gets awaited, which means that it's going to run until it can't run anymore. So it runs through one.
00:31:57.870 - 00:32:51.430, Speaker A: Then it runs read to string one of file one, which is a disk operation. So it has to wait so it yields. So at this point there's another opportunity for the select macro to run either of these other two branches. If there's still nothing on the network and still nothing on the terminal, then it might be that the read of file one is completed, in which case it's going to continue from the previous yield point of foo two, which is this await. So it runs this and then it reads file two and then it yields again. And when it yields, control flows back up the stack, in this case to the select, and the select gets to retry these again. Right? So this, this await sort of being a return is what enables this whole mechanism to work is that every await is a opportunity for whoever is above you to choose to do something else.
00:32:51.430 - 00:33:31.392, Speaker A: And it could be that they don't decide to do anything else and they let you run instead. It could be that they let some other things happen and then continue to let you run. Right, so here it could be that, well, this is maybe a bad example, but let's do foo is foo two. So we're going to store the variable out here. Foo dot await. Let me make this a loop, right? So imagine now that foo gets run for a bit. It reads file one, it reads file two, it's reading file three, but it yields.
00:33:31.392 - 00:34:00.268, Speaker A: And at that point the network has a new stream. So we execute this. So whatever happens in here happens, gets run and we don't even check on foo again. Foo doesn't get to do anything more because it's not being pulled. It doesn't get to run in the background, it's not its own thread, it's cooperatively scheduled. It relies on the sort of parent of it, which is the select, to actually continue awaiting it. Right? So foo doesn't get to run on its own.
00:34:00.268 - 00:34:35.834, Speaker A: It's whoever has the foo is responsible for actually awaiting it again. So in this case, if a new stream comes back and we eventually finish running this block of code, at that point we exit the select, we go around the loop again, and then the select happens again. Are there any new streams now? Let's say no, there are no lines from the terminal. So now we continue awaiting foo. So now maybe file three finished reading and foo awaits again. But this time I'm reading file for. So it gets to make progress, right? And then maybe there's a line from the terminal, we go around the loop again, and so on and so on.
00:34:35.834 - 00:35:31.160, Speaker A: So, so the way to think about this is really this, this idea of cooperative scheduling, where if I don't run, I'm going to let whoever is above me decide who runs next, and it might not be me, right? And so this brings us into the topic of cancellation. If you don't want the future to keep doing what it's doing, you can just drop it, right? The trick is, if you don't await it, it doesn't get to do anything. Awaiting it is what drives its progress. Now, you don't really have a way in sort of straight line code like this to cancel, right? Because once you call await, you don't have any control of execution anymore. What you've told rust is, I don't want to keep running until you have the promised value from this future. And when that happens, run the next line. So you don't have really have a way to cancel here.
00:35:31.160 - 00:36:38.466, Speaker A: But if you wanted cancellation of this operation, what you would do is something like select, let's say, done is this, or cancel is going to be, let's say, what's a good example of this? Let's say there's a cancellation here. There's a cancellation channel, which is like a Tokyo sync NPSC receiver. So cancel is going to be cancel await. So here we're just going to sort of continue or fall through to print line below if we get a if. So you can. Another name that was proposed for select by without boats, I think is race. And that is a good way to think about this.
00:36:38.466 - 00:37:54.190, Speaker A: We're sort of racing these two against each other, and whichever completes first, whichever await can produce its result, first gets to run and the other one does not. It just sort of gets cut short at this point, unless you sort of, unless you're looping and trying more. So in this case, if we get a wait, then we're gonna return or something. In fact, you could sort of imagine that this is like a result instead, right? But in this case, let's just say that in that case, we just return zero and we don't execute any of the remainder. So this would be the way that you do cancellation is you describe the circumstances under which you should cancel the operation. Does that make sense that you get to basically, async isn't magical. It just describes the mechanisms for changing or for cooperatively scheduling a bunch of computation by describing when under what circumstances code can make progress and under what circumstances code can yield.
00:37:54.190 - 00:38:46.552, Speaker A: So this might then make you wonder, well, is it just turtles all the way down? Like at some point you need to get to a point where like, like here, right? Where I say, I said read from network, and then I'm going to await that. Well, ultimately that does like a system call, it does a read system call from the operating system saying, give me these bytes. And the operating system goes, I don't have the bytes. What do you do? Well, this is where the notion of an executor comes in. And we're not going to talk in huge depth about executors because there's a lot of mechanisms going on there. The basic premise of an executor is that you're only allowed to await in async functions and async blocks. So this wouldn't actually compile.
00:38:46.552 - 00:39:52.736, Speaker A: It would only compile if this was an async FN. But then you see the compiler complaints main function is not allowed to be async because ultimately at some point in your program, you're going to have this giant future that holds the entire execution of your program, but it's a future, so nothing runs yet, right? So you have this sort of top level future that describes the entire control flow of your application. Something has to like run that in a loop, right? Something has to have the loop that we expanded earlier. That's like try to see if it completes, and if it doesn't complete, then what it can't yield, because there's nothing above maintained, right? Like something, if it can't make progress, what does it do? Because it can't yield. You can imagine it just spins in a loop, but in practice that's not really something we want to do. And so an executor, a sort of primitive executor, is one that just pulls futures in a loop and does nothing else. It just keeps retrying aggressively instead.
00:39:52.736 - 00:41:10.424, Speaker A: In practice, what happens is that the executor crate, so Tokyo would be an example of an executor crate provides both the lowest level resources like network sockets and timers, and also the executor loop at the top. And the two are sort of wired up together behind the scenes. So imagine that you're doing like a read from the network and you call await. What's going to actually happen is Tokyo's tcp stream is going to do a read from the network, realize that I can't read yet, and then it's going to sort of store its file descriptor, store the socket id if you will, into the executors sort of internal state and say the next time you go to sleep, watch for this file descriptor changing its state. Like tell the operating system that it should wake you up if anything changes about this file descriptor. And then at some point when, when we've yielded all the way back up to maintain backup to the main executor, the main executor, instead of just like spinning in the loop, what it's going to do is it's going to take all of those resources that, that it knows it needs to wait for and it's going to send them to the operating system and say wake me up. Like put me to sleep, but wake me up.
00:41:10.424 - 00:42:31.204, Speaker A: If any of these change, if the state of any of these change, wake me up because I need then I have more work to do. And this is sort of the e poll loop for those of you familiar with Linux system calls, but doesn't have to be epol, right? On Windows it's using, the name escapes me. I forget on macOS is using KQs, but basically there's sort of a different implementation of the outer executor loop on different operating systems to best make use of the underlying mechanisms. So in practice, what you do, so with Tokyo, for example, you, and you do something like this and that allows you to write async fn main. But what that really is, is just a procedural macro that rewires your code a little bit, turns it into fn main, and then does the following, it does like Tokyo let runtime is Tokyo runtime runtime new and then runtime dot block on and then async of and then the remainder of the function. That's the transformation that it makes. So your main is not async, right? Even though you wrote async fn main, the procedural macro turns it back into a non async fn main.
00:42:31.204 - 00:43:23.780, Speaker A: Because like when Linux executes your binary it needs to just have a function to call a regular function. It doesn't know anything about rust async. And then what the thing it does in main is create a runtime, which is this executor, this, this sort of big loop that tries to pull the future that's passed a block on. And then if it can't make progress on that, go to the operating system, say wake me up if any of these things change. And then loops that, until the future resolves, until the future has no more tasks to run so the example here would be, this is an infinite loop, right? Nothing this, this async will never finish. It never resolves into its final value. But you can imagine that the moment we get something from terminal, for example, we're going to break, which means that the moment someone writes something on the terminal, this loop exits.
00:43:23.780 - 00:43:52.670, Speaker A: We're going to drop the network future and do no more work on it. We're going to drop the foo future and do no more work on it. And at that point, we're at the end of the async block. So this async block resolves into just unit. The empty tuple that point block on finishes because it the future has resolved. The future was given, has resolved, and at that point, we're at this end of execution, and then main exits. So that's sort of the higher level flow of how these features get executed in the first place.
00:43:52.670 - 00:44:28.890, Speaker A: All right, again, let's pause for a second and see whether all of these little bits and pieces that I've explained roughly make sense together. There's a lot more to talk about here. And sort of, how do you use this in practice? What are some of the pitfalls? But let's see that everyone's following along. And please do ask questions. This is pretty hairy stuff. It's weird and convoluted, and there are lots of moving pieces. So if you have a question, chances are other people have questions, so please ask them.
00:44:28.890 - 00:45:21.570, Speaker A: I'm pretty new to this. I only know about how the JavaScript event loop works. Where would event Loop fit into this? Like a comparison? You can sort of think of the executor loop, the outer executor loop, as being the event loop, right. Its job is to keep running the sort of available futures until there are no more futures to run, and at that point it's done and the program is finished. Now, the difference between the event loop and JavaScript in here is that you can choose your own event loop. You don't have to use Tokyo. And in fact, Tokyo has multiple runtimes or multiple variations of runtimes that you can choose to use.
00:45:21.570 - 00:45:59.410, Speaker A: And arguably you can also write your own event loop. This, this loop select right here is an event loop. It's sort of your own event loop within the context of the larger event loop. You talked about futures yielding. How does this yielding look like in code? In general, you don't have to worry about the yielding yourself. Whenever you do await implicitly the so think of dot await as being sugar for doing this yielding. You can't actually, actually use the yielding machinery itself directly in your own code.
00:45:59.410 - 00:46:52.008, Speaker A: If you try to implement a future yourself, like manually by implementing the trait, you don't have access to the yield keyword. Instead, you have to basically manually implement the state machine, which is what the futures ecosystem and rust used to be before async await landed. And trust me, this is a lot nicer in general, though, these days, it's very rare that you need to write your own future implementation. Best way to pass data to Tokyo spawn. We're going to talk about Tokyo spawn. Um, why is Tokyo Main a macro and not a simple function? So I'm not sure I follow. So the idea here is that this is very easy to write.
00:46:52.008 - 00:47:23.536, Speaker A: It's like a very simple setup instruction. You could imagine Tokyo having like a Tokyo colon colon main that you passed in an async block to. But it's, it reads a little more weirdly. I think there might actually be a Tokyo main like this too. There's no reason it couldn't be one or the other. So does Tokyo re implement KQ or lib uv? It does not re implement KQ. Tokyo uses.
00:47:23.536 - 00:48:21.340, Speaker A: Well, Tokyo uses Mayo. And Mayo is a crate that abstracts, overdose, KQ and epoll and whatever the windows thing is, which still escapes me, but it basically gives you access to sort of something like Lib uv. Basically an operating system event loop, or not even event loop, but just event register. You can say, I want to go to sleep until any of these events occurred, and then you just go to sleep, and then the operating system, through whatever mechanism Mayo chose, will wake you up when one of those events happened. And you can make progress now you might wonder, well, what if? So down here, we waited on cancel, which is a channel receiver. So if you await on a channel receive, there's no file descriptor you can give to the operating system. The operating system doesn't know anything about something like a receiver.
00:48:21.340 - 00:49:09.360, Speaker A: In those cases, there's a little bit more going on under the hood here. In practice, you should just trust that the executor knows how to wake its own futures up. So in this case, if you're using the Tokyo runtime and you're using a Tokyo channel, it knows how to work that out. And in fact, the pieces that are being used here are fairly runtime independent. So for example, you can use. So there's a crate called futures, which has also a lot of just utility mechanisms for futures. And I think one of the things they have is a channel as well, and you can use the futures channel with Tokyo because they use the same underlying mechanisms that the rust language provides.
00:49:09.360 - 00:50:21.908, Speaker A: So the executor does have to include mechanisms for dealing with non operating system based events. What happens if network await runs, but terminal await and foo await don't? Then on the next loop of a select, does network await get run again? Yes. So the way a select works is it selects among all the given options. It doesn't remember anything about it having been run in the past. In practice, the way this actually works, if you want to sort of redo things, is you would do this, and the borrow checker will complain about this. Like, if I had actually imported the select macro, it would have told me that I have to do this because otherwise the first time through the loop, network ownership of network is sort of transferred to await. And so the next time around the loop, the bar check would be like, well, network has been moved and you can't use it again.
00:50:21.908 - 00:51:24.130, Speaker A: So in practice you would do something like this, which allows it to be reused across multiple iterations of the loop. But yes, it will consider all of the cases. It doesn't have memory about past attempts. Sort of a lot of what I'm saying has a little bit of hand waving. Right? Like, we're not going into the real mechanism details, and you should consult the documentation if you rely on any particular corner case behavior here. But the general idea is that select selects over all branches every time. Okay, so here's another good question, which I was going to get to, but a question came up, which is what happens if an abandoned select arm has side effects? So here, let's say, okay, let me see if I can come up with a good example here.
00:51:24.130 - 00:52:46.180, Speaker A: Let's say that one of the operations that we want to do is a file copy. So we're going to have f one, which is going to be like Tokyo fs file open foo and f two, which is going to be file create bar. And then we're going to do copy is Tokyo IO copy mute f one to mute f two. And then one of the streams here is going to be copy await. Let's just remove the loop for a second. So first of all, you'll notice that I'm using Tokyo fs file instead of stdfs file. That's because if you do operations on a standard fs file, there's no await, there's no async functions on it, because the standard library doesn't define async functions in general because they rely on this integration with the executor that you need in order to get the cooperative scheduling and the sort of smart wake up that we talked about.
00:52:46.180 - 00:53:19.640, Speaker A: So you do actually need to use the asynchronous version of IO resources. And then here what I'm going to do is create a future that's going to, it basically takes a thing that implements read and a thing that implements write, or more precisely, a thing that implements async read and a thing that implements async write, and it's going to read from one and write to the other. Great. So that's one of my select arms. Now, as I mentioned, the way that select works is it tries all the branches until one succeeds. So imagine that it tries this, nothing happened, addresses nothing happened. It tries this, nothing happened.
00:53:19.640 - 00:54:16.132, Speaker A: It tries this and it writes like a megabyte. And then it has to wait on the disk like the disk is saturated and so it can't make progress, but it hasn't finished copying the file yet. And then we go back to the stream and let's say that the stream completes. So now we exit the select and then down here, we're now in a state where some bytes have been copied from food to bar, but not all. You can imagine that we copy dot await here to sort of complete the copy, but it's very easy to forget to do that. And there isn't really a good solution to this problem. This is something that you need to identify has happened is that whenever you use something like select, you're now in a world where you might have partially completed a future, right? It's hit some yield point in the middle of its execution, but that means that the sort of trailing end of the future hasn't gotten to run yet.
00:54:16.132 - 00:54:54.148, Speaker A: And at that point, if you drop the future, it doesn't get to finish, it doesn't get to do the rest of its work. It just gets terminated. At that point, it gets canceled. And so now you need to reason about the fact that your, your program might be an intermediate state. This is a particular, I don't want to call it problem, but this is a, this only really affects selects. Because if you think about something like here, that can't really happen, right? This either gets completed, well then it sort of has to get completed. There's no way for you to cancel the operation midway through.
00:54:54.148 - 00:56:25.528, Speaker A: In fact, the only way that this operation, or indeed any operation that you call dot await on gets, does not get completed, gets interrupted, is if there's a select somewhere up the chain. So in general, when you write select, you have to be careful about what happens if one branch runs and then another branch completes? So the first branch didn't run to completion, but the second branch did. Where does that leave you? So this is a an error case that you need to be concerned about when you're using selectival. As you presented, the executor is making the assumption that the futures are not greedy. How do you avoid a future not taking all the compute time and grind the all async to a halt? This is also a great observation, and I'm very glad you made it, because it means that people are understanding sort of the mental model here, which is, this is cooperatively scheduled, which means other things get to run as long as the thing that's running occasionally yields. If you have a future that just is just a busy spinning loop, or uses say, stdio file, and then calls like read on a giant thing or on a network socket that just never gives any bytes, that thread is blocked. If it used the async TCP stream, then the read would yield if it couldn't complete.
00:56:25.528 - 00:57:26.362, Speaker A: But if you use a standard IO TCP stream which doesn't know anything about Async, and you do a read, its implementation is to block the thread, do nothing more. So certainly not yield, just block until the read completes, which might be never, there might never be a byte coming from that particular TCP channel ever again. And then you're sort of in the dog outs. Like, this is a really bad situation, because now none of your other futures get to make any progress. They're never pulled again, because the other one doesn't get to run, because nothing, the thing that is running, the thing that's holding up the executor, doesn't yield. And so it is very important that you lean into this cooperatively scheduled world. And this is also why you have to be very careful about using blocking operations, that is, operations that aren't aware of async yielding, or very compute intensive operations.
00:57:26.362 - 00:58:23.430, Speaker A: When you're an async context, there are some mechanisms that exist for, for improving the situation. So for example, in Tokyo, in Tokyo, this is function called spawn blocking. This is the one called block in place. And these are, if you're in an asynchronous context and you need to run something that, you know might block for a long time, whether that's because of computer, because it's doing a syscall or something, you can use these methods to sort of tell the executor, hey, I'm about to block. Make sure that other tasks get to run the way they do that we won't get entirely into today. It's a little bit too technical, but I would, I would recommend that you read the documentation for these methods to understand the differences between them and what the trade offs are. But there are ways to sort of signal that, that you're about to block, and that the executor needs to do sort of take the appropriate steps to make that be less of a problem.
00:58:23.430 - 00:59:14.926, Speaker A: I am intentionally not mentioning future poll. Does that mean that select with a huge number of cases will potentially be slowed down by trying all options out every time? You might think so. And the answer is, it depends on the implementation of select. In general, if you have a select with like a million cases, that seems like a problem. But given that select actually forces you to write them out, that seems unlikely in the first place. Now, it is possible for you to have such a large select. For example, you could imagine you have a code generation pipeline or something that generates it.
00:59:14.926 - 01:00:49.090, Speaker A: And in that case, I think most select implementations are optimized for few cases rather than many cases. But it is possible for select to be smart smart in the sense of only pull the ones that might be able to make progress. I'm not going to go into exactly how they do that, but basically there's a way for select to integrate with the rust machinery for dealing with futures and wake ups, to sort of, when a future becomes runnable through whatever mechanism, like a file descriptor was ready or ascend happened on an in memory channel, there's a way for the select macro to sort of inject itself into that notification that this future is ready, and sort of get a signal to update its own state when that happens. So you can imagine that the select keeps almost like a bitmask across all of its branches, and when that notification comes in, it flips the bit for the appropriate branch, and then the next time the select comes in, or the next time the select is awaited, it'll only check the ones where the bit is set. In practice, I think in general, selects don't do this because it's a bunch of machinery, and most selects have few branches. You could imagine having a writing a select macro that only did this trick if you had many branches. I don't think any of them do that now.
01:00:49.090 - 01:01:41.720, Speaker A: This is a good time, actually. I'm going to mention join in a second, but let me see if there are more questions about selects. Is a select macro fair? As in can it happen that only one branch will run forever? So it depends on the implementation for example, if you look at the futures crate, so the futures crate has a select and a select biased. And the select one is if multiple futures are ready, one will be pseudo randomly selected, a runtime, so that one sort of tries to be fair. In practice, it might not be entirely fair. Select biased is a variant of select that always runs if multiple already it runs the first one. So that would not be fair.
01:01:41.720 - 01:02:39.182, Speaker A: So it depends on which select you use. When you use fuse with select, I'm not really going to talk about fuse. So fuse, the point of fuse. Let me talk a little bit about fuse because you might actually run into something that requires it. Fuse is a way to say that it's safe to pull a future. It's safe to await a future even if that future has already completed in the past. Right? So let's take the case where I have a loop over this, right? So the way we had it, and we go through the select, and let's say both network and terminal are both ready.
01:02:39.182 - 01:03:03.374, Speaker A: So the select sort of checks on both of them, and both of them say, I'm ready, here's the value. But then, or, yeah, let's say they both say I'm ready, here's the value. What does this select do? Well, realistically, it's not going to ask both of them, it's just going to ask one and then the next one. Right. So it checks with network. Network says, I'm done, here's the value. And it goes, great, thank you.
01:03:03.374 - 01:03:52.626, Speaker A: I'm going to run this branch. We go through the loop again, and now the select still includes this branch, even though this future has completed. But the select doesn't remember that it's completed and that it doesn't need to check this branch again, because the select just knows about being called once the loop is not a part of the select. And so this future needs to be safe to pull again, even though it has already yielded its value. And that's what fuse describes. Let's see, how would you continue a half completed, then abandoned arm? You can await the same future after the select. It depends on how you select on it.
01:03:52.626 - 01:05:10.370, Speaker A: But if you select like this, then this is awaiting a mutable borrow of this future. So after the loop, you can still await that, that value. This is one of the reasons why in general, you will want to select on mutable references to futures rather than the future itself. Because if you did this, that would only work if this wasn't a loop, then the network would be moved into the select, into this await, and you wouldn't be able to await it later, it would just be dropped at the end of the select. So, but whereas if you just await a mutable reference to it, then that mutable reference sort of ends when the loop ends, and so you would get to await it again later. How bad of a performance hit is it to use async? When you don't call any async stuff down the line, there isn't really any overhead to like. Async doesn't add any overhead for executing code, right? Like if you just have a, let's say you do like a matrix multiplication in the middle, like you just have, you have an FN matrix multiply, which currently implements a matrix multiply, you just add the async keyword.
01:05:10.370 - 01:05:52.750, Speaker A: This doesn't make it any more expensive, right? It just, the same code ends up running. It's just that it gets wrapped in the future type and you have to await it to get the result. But the await doesn't do anything. It doesn't, it doesn't change the generated code in any meaningful way. So there's no overhead to marking something as async. I think maybe the, the analogous question you're after is what's the overhead of doing an asynchronous IO read versus a synchronous I o readdeh? And there is a little bit of overhead there, because now you need to tie into the executor machinery. There's a little bit more work there.
01:05:52.750 - 01:07:01.522, Speaker A: But in general, the additional system calls that happen get amortized across all your futures or across all of your resources, so it doesn't really add much. And usually the benefit you get from the benefit you get from not needing to spawn like thousands of threads or hundreds of thousands of threads. More realistically, it usually ends up actually being faster to just have the fewer threads that are run by the executor and then have them cooperatively scheduled. And there are a couple of reasons for this. One common one is that you don't need to cross the operating system boundary as often. If you have lots of threads, like actual real operating system threads, then if one has to block on a read, then the read like you do a system call, and the operating system has to context switch to a different thread, which is not free, and then runs that thread instead. With async, if a read fails, the operating system returns to the same thread and says I can't make any progress.
01:07:01.522 - 01:07:36.850, Speaker A: It yields the executor continues running on the same thread, so there's no context switch, and then just pulls a different future. And so in general that ends up being a little bit more efficient. In practice. It's hard to say which one is like objectively faster because it really depends on your use case. But I would say on balance, if you're doing things that are I o bound, like web servers, for example, Async is probably going to lead to more efficient use of your resources. And also, I think, easier to read code. Maybe not necessarily easier to reason about, but easier to read.
01:07:36.850 - 01:08:23.470, Speaker A: Are these proto threads? Yeah, they're user space threads is one way to think about it. Let's see, what would select return. So select is a future. This returns, well, sort of. So select generates a future for you and then calls a wait on that future. In general, you can construct a select manually too. It's usually a lot more hassle, but you can construct the select future for a block your yourself.
01:08:23.470 - 01:09:17.430, Speaker A: It's not quite true. Select expands into basically a loop with a, with a match or a bunch of ifs. It doesn't, it doesn't really generate a future, but you could make it a future by wrapping it in like an async block and then assigning that to a variable and then it awaits it. Like select doesn't actually expand to a type, it expands to like rust code that does the appropriate stuff to make this work. But you could make it a future if you wanted. It wouldn't really have a meaningful return value apart from a future though. Great.
01:09:17.430 - 01:10:15.380, Speaker A: Now there are a couple different paths we can go from here. The I want to start with talking about join. So we talked about how select is sort of branching the control flow of saying do this or do this, whichever happens first. The other operation you can do is a join, which is saying wait until all of these futures complete. An example of this is let's say that you wanted to read ten files, right? So you're going to do like files is actually let me go down here. Let files is zero to ten map I nope. Tokyo fs file open format.
01:10:15.380 - 01:11:17.470, Speaker A: Alright, so here I have an iterator of futures, right? And I want to wait for all of them to complete before I continue with my program. Imagine that I'm like concatenating them, or I'm computing the hash across all of the bytes or something. So I really need to wait for all of the bytes to complete. So in that case there is the, the join operation and there are a couple of different joins. So the, in the futures crate there is a join macro which looks a lot like the, the select macro, except that you don't really specify the branches, you just give, you just list the things you want to join on. So here, let's say I collected this into a vector so I could say join. And usually you assign this to like.
01:11:17.470 - 01:12:09.834, Speaker A: So this is going to be a little bit janky. Let me, let me make this three instead. File one, file two. File three is join of file zero, files one, files two. So this is saying, actually this would be something like read to string. So this is going to run all those three reads in parallel. And notice the reason I need to do this, right? Let's say that I wrote let file one is file zero await, file two is file one to wait, and file three is two await.
01:12:09.834 - 01:12:59.530, Speaker A: So compare this to this. So in this first instance, what's happening is I'm first reading file one from start to finish or file zero from start to finish. Then I'm reading file one from start to finish, then I'm reading file two from start to finish, and then I get to complete with my program that works. It'll give me the right result. But the downside of this approach is that it's sequential. This means that rather than give the operating system all the read operations and have it like read the disk in the most efficient way, or access the file system in the most efficient way, or let's say these were network sockets and I wanted to read all the bytes from multiple network streams. It might be that one stream has no more bytes yet, but another stream is ready.
01:12:59.530 - 01:13:54.478, Speaker A: I wanted to keep reading those, like use the cycles to read those bytes while it's waiting for the first one. In this first case, it won't get to do that because I've said don't execute the next line until this file has been read. So it doesn't even get to this line. It doesn't get to start the next operation until the previous one is completed. What join lets you do is say, run all of these operations concurrently, and then once they're all completed, give me all of the outputs from the three futures. So this one is more efficient, might not be the right word, but it allows the operations to continue concurrently, which is a big, big benefit generally, because it lets you overlap compute and I o. So that's, that's sort of the join macro.
01:13:54.478 - 01:14:32.990, Speaker A: And it can be a little bit annoying to use it this way because you need to explicitly enumerate all the things you're joining. Like imagine if this was not three but 100, and you clearly don't want to like list out 100 things here and then assign it to a tuple with 100 elements. But this join is really convenient if you just have a few things. Generally all of these provide you with, with multiple ways to do things. So you see the Tokyo one is similar, but generally there's also, I forget what it's called here. It's not. I'm surprised.
01:14:32.990 - 01:15:14.500, Speaker A: Well, I think there's one here. Yes, you see those like there are multiple functions like join, which joins two things, join three, join four, join five, etcetera. But then there's also try join all. So try join all takes an iterator over things that are futures. Ignore try future for a second. Or actually a try future is a future whose output is a result. So this is going to try to join all the things in the given iterator.
01:15:14.500 - 01:16:15.828, Speaker A: And the reason why the join space is a little weird is because in general you probably care about the, you care about the fact that the output result order, you can map back to the input order. So I want to know that these are the bytes for file zero. So if I do the, what was the try join all right, of files file bytes, I want to make sure that file bytes zero is equal to files zero, right? And try join all will do that. It will make sure that the result sort of output is in the same order as the input, even if they completed out of order. Right. It might be that the read here completed before the read here, but then trigonal was sort of reorder them at the end so that the output matches what you expect. That reordering is not free.
01:16:15.828 - 01:17:18.694, Speaker A: There are ways to opt out of it. So there's a type called futures unordered, for example, which, as its name implied implies, gives you the results out of order. If you don't care about the order, this is generally more efficient. So here you say you create a new one, you can push futures onto it, and then after you've pushed the future onto it, you can then, well, it implements stream, which we're not going to talk too much about, but it implements iterator, where the outputs of the iterator are the results of the futures completing. So the idea here is that you stuff all your futures in there and then you loop over. You basically await the futures in order multiple times, and each time you await it you get one more thing, one more output from one of the futures you stuck in, but you don't know which one. So this might be helpful if, for example, the result contains all the information you care about, and you don't necessarily care about the input, because the output describes which input it was from.
01:17:18.694 - 01:18:07.734, Speaker A: So that can be more efficient. So multiple, multiple weight is cascading. Yeah. So another way to write this would be file zero, dot, then files one, dot, then files to in sort of pseudosyntax, whereas this one is do all of them at once and join us. Like promise all you can think about it that way. Yeah, there's probably also join all which does the same thing. Or you can manually construct one of these.
01:18:07.734 - 01:18:44.480, Speaker A: There's futures unordered and there's futures ordered and these. So I mentioned how select isn't necessarily smart about making sure to only check the futures that might be ready. But join is because it knows that there might be, there might be a lot of futures you're joining over. Like imagine I, you're downloading like all the dependencies of a cargo project. Well, there might be thousands of them, and you want to download them all in parallel, or at least some subset of them in parallel. And then there are many branches and you want to make sure you don't like, have to check all of the futures every time. You only want to check the ones that have actually made progress.
01:18:44.480 - 01:19:34.998, Speaker A: And so in general, the join operations like futures unordered will implement this little like hook into the runtime system to make sure that it only checks the ones that have had an operating system event, sort of readiness event happen to them. Yeah, and join all and try join all. Use futures ordered under the hood, not futures unordered. Great. So that's join, and join is great, select is great. But all they do is allow things to run concurrently. That does not mean that they get to run in parallel.
01:19:34.998 - 01:20:20.990, Speaker A: And this is a big important distinction that is often missed when people deal with futures. Let's see where to start with this. Okay, the Tokyo runtime actually starts up multiple threads, and each thread is able to sort of await a future. But remember that awaiting a future, that's not really what I want to say. I'm going to say it differently. Let's imagine for a second that the runtime only had one thread. So when you call block on, you give it a future.
01:20:20.990 - 01:20:54.456, Speaker A: It doesn't know that that future contains a bunch of other futures inside the one, like this function. This is a method on a type in Tokyo, right? It just gets one future, and it can await that future. It doesn't know about this await. It doesn't get to look at the code inside. It doesn't know that there's a join or futures order or anything. It just gets one future. So the only thing it can do is run that future.
01:20:54.456 - 01:21:37.316, Speaker A: Like try to see whether that future can complete. And if it can't, then go to the operating system, go to sleep, and wait for an event to happen, and then try making progress on that future in return. When it does, that future internally checks its inner future and so on down the stack. Or if it's a select, it checks all of its contained futures. But ultimately, at sort of the top, there's only one entry point into execution. And that's kind of unfortunate, right? Because it means that because there's only one future, there's no advantage to having multiple threads, because there's nothing for those other threads to do. There are no other futures.
01:21:37.316 - 01:22:00.752, Speaker A: There is only one. And therefore, if you had ten threads, because say you had ten cpu cores, there's still only one future. So only one thread can await that one future at any one time. Because awaiting a future requires immutable reference to it, an exclusive reference. And so multiple threads just can't do it. And even if they could, it wouldn't make sense. Like what would they do? There's only one piece of code.
01:22:00.752 - 01:22:43.394, Speaker A: You don't execute the same code multiple times. And this means that even if you do something like a join, and so you're doing all of these operations concurrently, they're happening concurrently on one thread, which means that that's probably not actually what you wanted, right? Imagine that you're writing a web server and you have like a, a loop over accepting TCP connections. And for each TCP connection you get a future for handling its connection. You stick it into like a futures unordered and then used to wait on the futures unordered, right? To do like. In fact, let's, let's try to write this out. So here's what I'm going to do. Let's make up a TCP server.
01:22:43.394 - 01:23:06.720, Speaker A: So we're going to have accept is going to be a Tokyo. Net TCP listener. Bind to 0080. Notice that I haven't actually added Tokyo's dependency. I don't get completion or anything like it. That's fine, stop yelling at me. Great.
01:23:06.720 - 01:24:08.630, Speaker A: And then I'm going to do something like loop, I'm going to select on. I'm gonna have a let mute connections is gonna be a futures stream future, maybe futures unordered, new. And I'm gonna select over this and say either I get a new stream, in which case I'm gonna call, call. I'm going to have like an async fn handle connection which takes TCP stream and then does who knows what with it. It does things right. Like there's a, let's just say there's a to do in there or something, you know. So in this case I'm going to say connections dot push handle connection of stream.
01:24:08.630 - 01:24:52.040, Speaker A: And down here I'm going to say nothing is going to be connections. Eight, this won't compile for a number of reasons, and it's not really important. I just want to demonstrate the higher level problem. The reason we need to have this branch down here is because something needs to be awaiting all of the futures. Remember, a future doesn't run unless it's awaited. So if we just have this, there's nothing is awaiting the futures unordered, which means that nothing is awaiting. The futures are inside of there, which means nothing is awaiting any of the client connections, which means none of the client connections are being served.
01:24:52.040 - 01:25:38.258, Speaker A: So we do need to await on connections, but we also want to sort of see if there are new connections coming in. So we need to wait on this accepting as well, which is why we need select and you can think of this futures in order. This is basically a join, right? I want all these to execute concurrently. This of course won't compile because I have a mutable reference here and a mutable reference here, and they're being used concurrently. Not okay, but I'm going to ignore that for a second. There's a different, more fundamental problem I want to get at, which is this is still just one top level future, which means there's still, even if the runtime had as many threads as you, of course only one of them gets to run at a time. It will get to multiplex across all the different connections.
01:25:38.258 - 01:26:25.050, Speaker A: But imagine that there are 100,000 connections. That thread is going to be completely busy dealing with all those connections. It's not wasting any time, like it's nothing asleep or anything, it's doing work. It's just there's more work than it's able to handle on its own. But all the other threads can't help out because there is only one future. So the way that you can help this problem and introduce parallelism, not just concurrency, is instead of having this this connections, we're going to get rid of that. And in fact we can get rid of this too, which means that we can get rid of this too, which means this can become while that.
01:26:25.050 - 01:27:07.334, Speaker A: So let's say I have this. There is a function that's provided by basically every executor called spawn. In this case, let's say Tokyo spawn. And what spawn does is it's sort of a hook into the executor, whatever that executor might be, that you give it a future, and it gives that whole future. It moves it to the executor. So it sort of is as if you gave it to the runtime. So now the runtime has two futures.
01:27:07.334 - 01:27:57.550, Speaker A: It has this future, which you passed a block on, and it has this future, which means there are two separate futures, which means that two threads can run them at the same time. So with this spawn, if one thread is busy doing this work, so accepting connections, another thread can handle the future for a particular connection. Notice that this is not a thread spawn. Right. The threads respond by the runtime, and there's a fixed number of them fixed, but we're giving additional futures, sort of sticking them on the job queue for that pool of threads. This is why Spawn generally requires that the future you pass in ascend is because otherwise it couldn't be sent to another thread to work on. There's like spawn local, but we won't really talk about that right here.
01:27:57.550 - 01:28:56.402, Speaker A: In general, Spawn also requires that the future you pass in is static, because it could be that inside, let's say that inside of here, we also do a Tokyo Spawn right inside of this with some other thing that does, you know, whatever. It needs to be static because it doesn't know the lifetime of the runtime. And in fact, the handle connection async function might complete, right? This outer async thing might complete, but this spawned async future still needs to be running, and therefore it needs to be tick static. It can't be associated with the lifetime of handle connection. Imagine that this had like a, I don't know, X is a vector, and this tried to use at X. Then if handle connection returned, but this future still tries to run, X would be dropped. But this has a reference to x, so that's not legal.
01:28:56.402 - 01:29:46.430, Speaker A: So that's why spawn requires static. So this is the way that you introduce parallelism into asynchronous programs, is you need to communicate the futures that can run in parallel to the executor. Now, it could be that the executor doesn't have that many threads, right? Like with the Tokyo runtime, you can set like worker threads to like one, in which case there's only one thread. So it doesn't really matter whether these are can run in parallel, because there's only one thread. So there's no parallelism. But in general, you want to use this pattern so that these futures can run not just concurrently on one thread, but in parallel on multiple threads. All right? That the reason why we need spawning makes sense.
01:29:46.430 - 01:31:08.826, Speaker A: Okay. Yeah. So this is why it's important to remember to spawn, and often why when people who aren't very familiar with async await start writing async await, they find that their program performance drops a lot, and it's because they're not spawning anything. So their entire application is running on one thread. And when your entire application is running one thread, of course it's slower than if you had multiple threads because nothing gets to run in parallel. What's the best way to pass data to Tokyo spawn? What are the best practices to handle errors in async when we call spawn? So spawn is a little bit weird because just like thread spawn, you don't really get to communicate with the other thing sort of implicitly, right? It's, it's just running somewhere else and you have no control over it. So you need to apply the same kind of techniques that you would use in a multi threaded program, which is if you want to say, share data between this and that.
01:31:08.826 - 01:31:42.536, Speaker A: Or let's say that I have two things. I want to spawn and I want them to share access to some vector. I would have to do like Arc new, mutex new, and then this can lock and then do whatever. And this can lock and do whatever. In practice, I would have to do like X one is Arc clone x. X two is our clone x this, this gets x one. And it's an async move.
01:31:42.536 - 01:32:58.650, Speaker A: And it's an async move. So they both get their own arc, and I need semicolons. So they each get their own arc, and they both have a mutex that guards the underlying value and everything is happy. Or you can like have them communicate over a channel. You can have them communicate over, like, if they're, if it's read only over static memory, like you have all the same techniques available to you as you do in a multi threaded program, and you really should think about it in the same kind of way. Now, there is one exception to this, which is if you have a, if you spawn something and you want to communicate the result of that back to the, the thing that spawned it, at least in Tokyo spawn, what you get back is a sort of join handle, similar to if you do a thread spawn, where let's say that this ends with zero, right? Then if you do, you can sort of assert equal that join handle dot await is going to be zero. If you don't await the join handle and just drop it, it's the same as if you drop a thread handle.
01:32:58.650 - 01:33:54.528, Speaker A: It just does nothing. Like it doesn't terminate the thread or anything. It just, you don't get to learn its result value. So this is one way to communicate the outcome of a spawned operation back to the caller. Now if, one thing to keep in mind is that if you spawn, just like if you spawn a thread, if you spawn a future, and I, let's say that, like, I don't know, it calls definitely errors, right? And x, now let's say this like returned a result, right? And it's the error case of the result. What do you do with the error? You don't have anywhere to communicate it necessarily, because you don't have a way to communicate with your caller. I mean, you could return it, but there's no guarantee that they're awaiting a join handle, and you can't really like, you don't have anywhere to do it.
01:33:54.528 - 01:35:12.628, Speaker A: But the question is the same as if you spawn a thread, or if indeed, if an error happens in main, like what do you do with that error? You need to have either, you could just print it as standard out, you could log it to a file. You could use some kind of logging framework, like tracing, to sort of emit an error, an error event that gets handled somewhere else. In general, that's the kind of approach I favor, where if you have an error that you can't propagate any further, you use an event distribution tool like tracing to decouple the production of events and the subscription to events. So that'll be the way to go. Is there any benefit on calling Tokyo spawn and immediately awaiting on it? So there can be, it's a little, it's fairly uncommon to do. The advantage of doing that is that you get to let other things on the current thread keep running while something, while that operation is running elsewhere, if that makes sense. So imagine that you have an operation that has to do like deserialization.
01:35:12.628 - 01:36:08.580, Speaker A: So it's somewhat cpu heavy. It's still IO, so you probably want to do it in async context and not blocking context. But you could do like deserialize over here and then join handle auto await. And now that await is going to immediately yield because this spawn hasn't returned because it's spinning due to deserialization. So this future is going to yield. And imagine that it's in a select or a join or something, other futures on other tasks on the current thread, like on the thread that's running handle connection, they get to run, and then this deserialization operation gets to run on a different thread and gets to do the cpu intensive operation. So it gets to happen concurrently and in parallel with these other tasks, continuing to make progress.
01:36:08.580 - 01:37:08.980, Speaker A: So that might be one case where it could make sense to spawn and then immediately await. It's rare that you actually want to do this. How is the Tokyo spawn connected to the runtime instance created above? There are sort of magical thread locals that are used basically, so runtime new just creates a normal value. There's nothing special about it. Block on will set some special thread locals inside of the executor, so that when you call Tokyo spawn, it checks those thread locals to find the current runtime and then spawn on there. And then similarly, when the runtime eventually runs that pass in future, it sets the same thread local. So when that future calls Tokyo spawn, it can find the executor and so on.
01:37:08.980 - 01:38:04.050, Speaker A: It's not a singleton, right? So, so you could have multiple runtimes, and if you call Tokyo spawn in the context of one runtime, it will spawn on that runtime, not on the runtime, and this can be valuable. So, for example, there are some services where you might care about. You might have prioritized traffic. For example, like imagine you have control plane traffic and data plane traffic, and you want to make sure that control plane messages are always handled. So one thing you could do is, for example, have, but there's relatively less control plane traffic, but you do need it to be handled. You can imagine dedicating, say, two cores to control plane traffic and have everything else for data traffic. And what you do is you create two runtimes, one with a thread count of two, one with a thread count of however many course you have left.
01:38:04.050 - 01:39:03.610, Speaker A: You spawn all of your control plane operations on the runtime with the two threads. You spawn all the data plane operations on the other runtime, and then both runtimes get to continue running, right? Like they're both active at the same time, but you know that there are two cores are reserved for cold control plane traffic. If you had a singleton runtime, you wouldn't have this operation. You can imagine that the executor itself supported like priorities and stuff. That gets somewhat complicated because it also needs to integrate with the operating systems, runtime controls, and priority controls. It's nice to be able to do this explicitly, so there are advantages to that, although it's a little harder to discover it's true. What should I do if there's an expensive function, like hashing a password that I don't want to block async execution of a thread.
01:39:03.610 - 01:40:01.200, Speaker A: That's when you use something like spawn blocking or block in place. What happens if you Tokyo spawn before creating any runtime? It panics. It says there is no runtime. So rust rust futures do not depend on thread locals. This is an important distinction. There's nothing in the async support in the rust language or the standard library that requires thread locals. Tokyo uses thread locals in order to make the, and like some other executors do this as well, in order to make the interface slightly nicer, like otherwise, imagine that you didn't have thread locals.
01:40:01.200 - 01:40:40.070, Speaker A: Spawn couldn't be a free function, so you would have to do something like runtime spawn. But that means you now need to thread the handle to the runtime throughout your entire application in order for anywhere to be able to spawn. Deep down in the stack, you could make it explicit like this. And in fact, Tokyo lets you do this. There's a runtime handle, I think, and then you can pass the handle around. You can do handle spawn, so you can do it explicitly without the thread locals. But in general, the observation is that this is so common that it's done with thread locals, because that way the interface is a lot more ergonomic.
01:40:40.070 - 01:41:29.370, Speaker A: The downside, of course, is that this means that Tokyo doesn't really work well on like an embedded context where you might not have thread locals, but there's nothing in the rust async sort of primitives or language support that requires it. Let's see. When you create a runtime, it doesn't really allocate a lot of memory. There's like some control data structures, but they're not generally very large. There's not a lot of overhead to the runtime itself. It doesn't need to keep sort of. Actually, I'm about to talk about that, so I'll save that for a little bit of a second.
01:41:29.370 - 01:42:13.920, Speaker A: Okay, so one thing we haven't really talked about much is what a future actually is. We talked a little bit about it, right? Like before we added all the spawn stuff. In fact, let me go. No, let me write a new one. So down here. So I'm gonna have an async fn foo, because I like foositive. And here's what I'm going to do.
01:42:13.920 - 01:43:18.110, Speaker A: I'm going to have an x, which is going to be a byte array, and then I'm going to do Tokyo fs. Let's say that there was a read into and I want to read file dat into xdev. I'm going to just ignore error handling for now. So n is the number of bytes read and then I want to print line all of the red bytes. So this is a fine use size. I don't actually give the type signatures. So let's think about what actually happens here.
01:43:18.110 - 01:44:56.130, Speaker A: We talked about how an async function or any async block is really just a chunked computation, right? So there's one chunk that starts here and ends like this. Like this is one chunk, and then between every chunk is an await, right? So this is sort of the, the operation that happens in between the chunks. And then chunk two is going to be let n. Let's get rid of the annotation. N is equal to few dot, let's say output, right? Because at this point it should have resolved because the await sort of finished and it does this, you can sort of think of it as this is how it's, oops, this is how it gets divided up, right? This is, we talked about a state machine, right? This is state one, this is the, this is state two, and this is the edge between state one and state two is future completing or food completing here. And what's a little bit weird here is let's try to think about where x is. X is a 1024 byte long byte array.
01:44:56.130 - 01:46:32.024, Speaker A: Where is that stored? You might normally say like it's on the stack, right? It's a, it's a local variable in a function, so it's on the stack and that's why this works out. But if, when you look at it here, like this is really a yield which is really a return, right? So let's say yield which is really return, but when you return that X goes away, right? Like the stack frame goes away, but the future has a reference to x because it needs to, it needs to have a reference to X so that it can write into it. So it's not okay for x to go away. So where is x? It's not on the stack because the stack goes away when this yield happens. In practice, what actually happens is that when you write async event or when you write an async block, the compiler actually generates a sort of that, that state machine we've talked about, chunk one, chunk two, it really generates something sort of like an enum where each chunk contains the, the state for that part of the state machine. And the state here really means any local variable that needs to be kept across a wait points, right? So if I here said, like, let Z is vac, but I never actually gave out a reference to Vec. Like Z is never used in any later chunks.
01:46:32.024 - 01:46:59.358, Speaker A: It doesn't need to be saved anywhere. It can just be on the stack when Foo is invoked, or when Foo is continued. It can be put on the stack and then just be dropped at the end. It doesn't need to be saved anywhere, but x does. So really, x ends up being here as a U 8024. In practice. It's a little more convoluted to make sure the references stay the same and whatnot.
01:46:59.358 - 01:47:42.746, Speaker A: But you can sort of think of it this way, the same thing with the future itself. It needs to be stored somewhere. So this also stores a few, which is like a Tokyo fs read into future, which actually has a sort of lifetime of tick x, right, because it holds a reference into x. So this is sort of self referential, which is another reason why you can't actually write this yourself in any meaningful way. And then in Chunk two, Chunk two doesn't actually have any state, right, because it defines n. But n is a local variables not kept across the wait points. So the n can just be dropped.
01:47:42.746 - 01:48:26.420, Speaker A: So there's no state in chunk two. It does continue to use the state from Chunk one, though. So you could arguably say that, like, this sort of gets transitioned into here, but there's not actually two copies of it. This is, you can think of this more as sort of a union, maybe, where some of the state actually stays in the same place. So that references continue to be valid. It's all kind of convoluted. But realistically, what happens is that the, what this actually returns when we say it returns, an impul future, right? That impulse future really is a type impul trait, means a type that has a name.
01:48:26.420 - 01:49:24.100, Speaker A: But I'm not going to tell you what the name is. And in reality, what this type is, is this statement, this generated state machine type that, like every time we try to await this future, every time we continue it, what we're really doing is continuing within the state machine type. We're invoking a method on the state machine type of that gets an exclusive reference, immutable reference to that type, so that it can access the future, so that it can continue to wait, that access, and internal local variables and the like. And this, this conversion from ASIC event really ends up like rewriting a bunch of these. So that instead of being few, this is like self dot feud and self dot x. They get rewritten to be re referenced, be references into that state machine. That's a struct that we end up passing around.
01:49:24.100 - 01:50:41.816, Speaker A: So in main, or in this case, let me just pretend that it's an async main for a second. When I say let x is foo, the value of x is of type state machine, right? So when I do x await, what I'm really doing is sort of, I'm sort of calling the await method on this particular state machine. That's sort of what I end up doing. In practice, that's not really the desugaring, but you can think of it as I'm really continuously awaiting this state machine. The reason this, this distinction is important is because this state machine contains a decent amount of stuff, right? It contains all of the values that are kept across a weight points. And in this case, that's say 1024 bytes, which is decently large, right? So if I, for example, do bar and pass it x, I actually have to move a fair number of bytes. This is like a pretty decent mem copy.
01:50:41.816 - 01:51:27.750, Speaker A: Imagine that I was like reading a bunch more bytes, right? Then now suddenly calling bar where bar. Let's say that bar just takes an impulse future and I pass it x, which is a future. Then what I'm actually passing it is this entire state machine, which includes a lot of bytes that have to be mem copied. And so futures can actually end up getting really large. And the other reason for this is, let's imagine, remember here, it needs to store the future that it's currently waiting on. So let's say that down here, this called my other future, or some library. Execute dot await.
01:51:27.750 - 01:51:52.772, Speaker A: Whatever this future is, we don't know how large it is. It's controlled by some other library. That future needs to be stored inside of our state machine. If we have a select, we need to store all of the futures. We have a join, we need to store all of the futures. So futures end up containing all of the futures that they in turn await. Which means the futures can get really, really, really large.
01:51:52.772 - 01:52:36.242, Speaker A: One way you can see this is imagine that you're doing profiling on your application what you, what you'll see in some asynchronous code bases, and you'll see that mem copy shows up a lot. And it's usually because you end up with really large futures that you end up passing around your program. Even just like returning a future, right? Like this needs to return a future. So this is sort of a memcpy into this variable, and I can keep happening up the stack as you return futures or pass them around, stick them into vectors, whatever. And so this is something to watch out for. Of course, the way that you can solve this problem is by boxing your futures. So if you place them on the heap, this problem sort of goes away.
01:52:36.242 - 01:53:20.290, Speaker A: You could either have this allocation be on the heap, or here. We could say this is going to be box new of foo. In practice, we need to use box pin for reasons we're not going to get into. But at this point, x is still now a future. But when I pass x to bar, I'm just pointing, passing a pointer to a heap allocation that bar can then treat as a future rather than the entirety of the state machine that I constructed. This is another reason to use something like Tokyo Spawn is because Spawn is going to stick that future into the executor and then just keep up a pointer to that future. This is why it's actually useful to have the, the spawn handle is because you can store that.
01:53:20.290 - 01:53:55.460, Speaker A: And then, like here, if I, instead of doing this, did Tokyo spawn of this dot await? This now doesn't have to store this future, whatever size it might be. It just stores the pointer to it and then can await it. And so we end up doing fewer of these mem copies. We don't end up with a. The future is just growing by growing the onion. Will that make sense? Yeah, it ends up being a union with the size of the largest chunk. State is the right way to think about it.
01:53:55.460 - 01:55:02.860, Speaker A: Why can futures assigned on the stack be moved? Don't they need to be pinned? We're not going to talk too much about pin, but basically there's nothing that prevents you from moving a future. In fact, there's nothing in the rust language that prevents you from moving any value. But once you have started awaiting a future, you can't move it again unless it's on Pinnae. But there's nothing inherent about a future that means you can't move it. Can you have an async function that internally creates a vec of futures from itself recursively? I know recursion requires indirections. In other words, is putting in a VEC count as indirection. Putting it in a vec is not enough, but if you stick it in something like a join handle, I think you can.
01:55:02.860 - 01:55:44.550, Speaker A: Or, yeah, if you stick it in a join, it depends. I don't think join is enough, actually, because a heap allocated like a vector is really a heap allocated array, and an array needs to know the size of its elements and the size of its. And it compute. No, it and it can compute the size of its elements because the size of the pointer is known. So I think that's fine. Although you would have to use a join, not a veC, in order to actually select or await the set of futures. You should be able to do that.
01:55:44.550 - 01:56:46.032, Speaker A: Okay, now that we've talked about this, there is a little bit more I want to talk about sort of here at the, at the tail end, you may have heard about asynctrate. So one thing that the people are sort of missing in, one thing that is missing in the implementation of async await as it stands today, is the ability to write the following. I'm gonna go with service because it's a trait that I know decently well. I want to be able to write async if n call request. I'm gonna just, no, I'm gonna make it simple. I'm gonna have a struct request instruct response. I want to be able to write this right.
01:56:46.032 - 01:57:21.920, Speaker A: I want to be able to define a trait that has an asynchronous function in it. Currently this doesn't work. Functions and traits cannot be declared async async trait functions are currently not supported. Consider using the asynctrate crate. And the reason why this is not supported is because, remember, this is really sugar for implied future output equals this. And here we had a different error. Impultrate is not allowed outside of function and method return types.
01:57:21.920 - 01:58:12.434, Speaker A: The reason for this is, let's say that I write fn fn foo and I take an impulse service. And I say let x is x dot call request. Let's make that fut. How large is fute here? Or let's say I take a din service. Oh, of course, this needs to, let's say this is mute self. This is muted. How large is future? What depends.
01:58:12.434 - 01:59:04.830, Speaker A: Right. Let's say that I have struct x and I impose service for x and I implement async call async fn call mute self request, have to return response. And inside here I say here I just return a response immediately. So the size of this is very small. And for y, what I do here is I do let z is 00:10, 24 four. Then I do Tokyo time delay. Or it's called sleep.
01:59:04.830 - 01:59:54.966, Speaker A: Now I forget sleep for 100 dot await and then I drop z. So this is used across an await point and then I return response. The size of future depends on what stack variables are there in the async block that was used for the future. And so few doesn't have a size. It doesn't have a known size, and so the compiler doesn't know what code to generate here because it doesn't know the size of you. You can imagine that if this was like impulse service, then maybe it could figure it out. But this is now a type that you can't name.
01:59:54.966 - 02:01:02.250, Speaker A: So let's imagine that I wanted to return a foo call, and I want to write struct foo call, and I here I'm going to do foo call of. So it's going to hold the future, right? What's the name of this future? It's sort of like service Colon, like type of service colon, colon call. Maybe that's easier if we say like s service, and this is an s and this is s call. But this doesn't really work either. You can do this with like existential types, but it gets weird. So really there isn't, there isn't really a good way to deal with async fn call like this in what we've described so far, because the, the type of, the thing that it produces isn't known anywhere. It's not written anywhere.
02:01:02.250 - 02:01:51.330, Speaker A: So there are, there are two ways around this. One is the async trait crate, so it lets you annotate like this. And what that does is it basically rewrites all of your async events. You need to also place it on any implementers of the trait, and it rewrites all your async events into pin box. Din future output equals response, and it does the same down here, rewrites that into box. So it rewrites the signature, and they'll also rewrites your body into async move of this. And that works.
02:01:51.330 - 02:02:38.146, Speaker A: If I remove asynctrate now, well, I'm not importing pin, but now it won't complain, because this is a type with a well known size. It's a heap allocated, dynamically dispatched future, which rust already knows how to reason about. So the size in, I removed the block that actually called this. But if I, if I do, if I have a service and I call call, the thing I get back, I know the size of it, I know how to sort of await it. I have all of that information readily available to me because it's just a din future. The problem, of course, is that now you're heap allocating all of your futures, which means you get dynamic dispatch, you don't get sort of monomorphizations. And some of the nice rust optimizations also, you're doing a lot.
02:02:38.146 - 02:03:12.508, Speaker A: You're invoking a lot more memory allocator pressure. You also have indirection for all of your futures. So imagine that this was not service, but this was async read and this was a read. Now, every time you do a read, you do also do a heap allocation and an extra pointer direction. So that might actually get fairly expensive, as this is why async trait works really well for sort of higher level things, which traits often are. It doesn't work so well if you have to use it at the bottom of your stack. Or rather it might not work so well.
02:03:12.508 - 02:04:01.960, Speaker A: Always measure first and then see whether it's a problem. The other approach you can take here is to declare a read. If we go back to service here is declare a an associated type called future, which has to implement future output equals response. And then instead of having this be async fn call, you say that FN call has to return self call future. The reason this works that here I can nice say type call future is equal to. Well, this is also inconvenient to use. I'm going to get rid of this because it's annoying.
02:04:01.960 - 02:04:49.306, Speaker A: Type call future is equal to this. This is going to be self call future. So I can use, I can do the same thing, thing here, right? Box didn't future output equals response. So I can make this be the same thing as what it was for, for async trait. But I can also not do that if I want to choose to do so. Like this could just be response, it could just be an async response. The problem I now have is how do I name this type? So the reason this works is because now the associated type, like rust, knows how to communicate associate types to callers, so they can now know how large the, the actual type is.
02:04:49.306 - 02:06:19.460, Speaker A: You have a way to name the return type so that you can use it in other structures and stuff. The problem of course, is when you implement this trait, it's not entirely clear how you name any type that isn't pin box did. There is a feature called type alias infiltrate which would let you write this, which sort of feels like we're almost at async definite in traits. There are some other corner cases to fix out, but this is why the asynctrate crate is often needed, and why it's hard to get async traits without doing something like boxing. All right, does that make sense why we have async trait, what it's used for? Yeah, so one of the observations, right, is that the compiler could in theory generate all this, right? If you write this, certainly you could imagine that the rust compiler could, like, behind the scenes, rewrite it into this, right? And also automatically rewrite any call to it into the appropriate thing here. There's a little. There's a lot of magic going on there.
02:06:19.460 - 02:07:14.886, Speaker A: Like, suddenly what I write is this, but it gets turned into like, an async move and an associated type that gets auto named. For me, the name of that type is not always entirely obvious. It's a little bit maybe I now need to also say, like, where self is sized, because dynamic dispatch wouldn't work with associated types. It's not impossible. And this is why I think we will get async functions and traits eventually in rust. But there's a lot of design decisions to be made for how this should actually be exposed, how it should work behind the scenes, should you be able to customize what this additional function is called. There's just like a lot of design decisions that have to be made, and there's a fair amount of just complexity in the types of needs to be worked out too.
02:07:14.886 - 02:07:44.602, Speaker A: If you're interested in this, I highly recommend, like, go look at all the discussion that's happened about this. It is something that, like, is clearly wanted. It's just getting it right is hard. Great. So let's see. Are there more things I wanted to talk about? So we talked about join, we talked about futures in order to talk about Spawn. Talked about number of worker threads.
02:07:44.602 - 02:08:21.116, Speaker A: We talked about asynctrate. We talked about spawn blocking. We talked about send and static futures, pin and box futures. We talked about how blocking code is problematic because it doesn't let other tasks run. Yeah. There's one more thing I want to touch on, and this is something that is, I don't want to say controversial, because it isn't really, but let's say that I have an async. Well, let's do that.
02:08:21.116 - 02:09:15.220, Speaker A: I have an async fn foo. Actually, I don't even need these. I can write this here. So let's say that I want two futures to share state. So I'm going to say x is arc new, arc new, mutex new of zero. And then I'm going to do Tokyo spawn. Actually, I'm gonna do let x one is our clone x spawn async move.
02:09:15.220 - 02:10:05.558, Speaker A: Bear with me for a second while I tab out some code. This is going to be x two. All right, so I really just wanted to. Okay, we're going to do mod Tokyo and do fn spawn. And we're going to say it takes impul future and does nothing with it. You stop yelling at me now. Async pub fn yeah, pub async fn.
02:10:05.558 - 02:10:35.970, Speaker A: Great. Stop yelling at me. So here I have two asynchronous threads and both of them are accessing the same arc and they're using a mutex. And in this case they're using the mutex from the standard library. So there's also Tokyo sync mutex. Move that down here just so that I can make it not yell at me anymore. Oops.
02:10:35.970 - 02:11:53.002, Speaker A: Pub mod pod mod sync pub construct mutex. I'm going to t mutex. Okay, so there's an argument ongoing about should you use the standard library mutex or should you use the Tokyo Mutex? Or like an asynchronously enabled mutex? And the answer, and the reason why there's a discussion about it is let's imagine that I had this do let x is x one lock. And then I did Tokyo fs read to string file dot await, and then I did x plus equals one. This is clearly a very stupid loop, but let's say that's what I wrote. So now I'm in a position where I take this lock and then I go await. Now at this point, let's say, in fact, let me make this a select.
02:11:53.002 - 02:12:17.380, Speaker A: That's what I'm going to do. That will illustrate the problem better. So I'm going to move this up here, select this. Fine, I'll leave it as spawn. That's fine, it's fine, it's fine, it's fine. Okay, let's imagine that I have a runtime with only one thread. Now imagine that this gets to run first.
02:12:17.380 - 02:12:55.798, Speaker A: So it takes the lock, and then it goes to read a string, and then it calls await. And now it goes to run this like this yields because it can't read anymore from the file. So now it goes to run this other future instead. It's been spawned, right? So the thread sort of drops this one, which is yielded, or it doesn't drop it, but it just sticks it on. I'm going to run you later. And then it tries to run this future, and then it tries to grab the lock, but the lock is held by this future, and so therefore this blocks. And because it's a standard library mutex, it just blocks the thread.
02:12:55.798 - 02:13:54.550, Speaker A: It doesn't know anything about asynchrony, it just blocks the current thread. But that means that the executors one thread is blocked, which means that it doesn't get to continue reading from the string. Because that would require continuing executing that future, which means that this future never ends up dropping its lock guard, which means that the lock is never released, which means that this lock never completes. And so we have a deadlock. This is why it's problematic to have standard library mutexes, right? It's because you can end up in these deadlock situations. Now this would still be weird if you have an asynchronous mutex, right? If we use the, the, like a Tokyo mutex or some other asynchronously enabled mutex, what would happen is this would lock, this would try to reach a string, it would yield, this would try to lock. But because it's an sort of async aware lock, when it fails to take the lock, it would yield.
02:13:54.550 - 02:15:03.840, Speaker A: Rather than just block the thread, it would yield, which lets this future run again, which lets this complete eventually, which lets it drop the mutex card, which lets this continue and eventually succeed. So that's why you need async enabled mutexes. Now that said, one downside of asynchronous mutexes is that they are a decent amount slower, and that's because they need a lot more machinery in order to be able to do this sort of yielding on demand and know when to wake each thing up. It's fairly complicated what they have to do internally. And so this is advice that in general you actually want to use a standard library mutex as long as your critical section is short and does not contain any await of points, any yield points. So in this case, our critical section, that is, the section under which we hold the lock, contains an await which is just ripe for deadlock potential like we just saw. And so here it's not okay to use a standard library, a standard library mutex.
02:15:03.840 - 02:16:08.724, Speaker A: But in the case we had before of it just does this here, the critical section is very short and it doesn't have any wait points, so there isn't really a risk of deadlock here. The reason I say the critical section has to be short is if the critical section was like, do a giant matrix multiplication, then sure, there's no wait point, but you're still holding up that thread and not letting other futures work. So it's sort of similar to any other operation that is like a long term operation, but it's a little worse because you're also holding the mutex which might block other futures on other threads in the same way. Right? Like they may be unable to make progress. Imagine that, okay, you imagine you have a runtime and executor with two threads, and there's a future on like this future runs on one thread and this future runs on the other thread. And this is doing like a matrix. This is doing, let's say here matrix multiply, which is, let's say super slow.
02:16:08.724 - 02:17:40.340, Speaker A: So this thread is running the matrix multiply while holding the lock. This other executor thread is trying to run the second future tries to take the lock and blocks, right? Because this is a standard library mutex, so it blocks. And at this point, because it doesn't yield any other futures on that other executor thread also don't get to run as you're holding up a lot more work. So that would be another instance where you would want it to be an async aware mutex so that this other executor thread, its future would yield when it fails to take the lock and let other futures run in this thread at the same time while this matrix multiply is finishing. But in the case where the critical section is short and there are no await points, it's totally fine to use a standard library mutex, and often you might want to because they can be significantly faster to acquire and release, which could be really important if you have, if you have operations and you need to do a lot, or where there's potentially a lot of contention where a mutex might do a lot better than an async aware mutex. Okay, so at the tail end here, there's a question of like how do you detect these kind of problems? If there's like blocking or something is running for a long time, or you might have a cancellation you didn't expect. There aren't currently any good tools.
02:17:40.340 - 02:18:31.389, Speaker A: I know there's some work on like a Tokyo console, which is basically, it sort of hooks into the executor and figures out how long has it been since a given future yielded? How long has it been since the last time a future was retried? So there's like some of that kind of monitoring that could point out these problems. It's not complete yet. It's not something that's really tied to Tokyo either. I mean, this one is written for Tokyo, but you could imagine instrumenting any executor in the same way of sort of noticing when particular patterns show up, like a future hasn't yielded for say, more than a second. That seems like a problem. Then you could highlight that to the user. Oh yeah, and it looks like Clippy has some links for this too.
02:18:31.389 - 02:19:13.270, Speaker A: Can you elaborate on the conceptual difference between a thread spawn and a Tokyo spawn. A Tokyo spawn gives the future. It is passed to the executor for the executor to run whenever it wishes. Concurrently with other futures. A thread spawn spawns a new operating system thread that will run in parallel with everything else in your program and is outside of the executors control. A thread spawn also does not take a future, it takes a closure. So if you wanted to await a future inside of a spawned thread, you would need to create your own executor inside of there.
02:19:13.270 - 02:20:23.840, Speaker A: Conversely, if you Tokyo spawn something, you're not guaranteed to have your own thread, and so you do need to be co op. Like you have to cooperatively schedule. If you use Tokyo Spawn, you have to have yield points, because otherwise you might block the executor. If you do a thread spawn, that's not a problem because the operating system is able to preemptively disrupt you. Basically think of it as threads are not cooperatively scheduled, and so they can do blocking operations, whereas Tokyo spawn, or just like futures tasks that you spawn, are cooperatively scheduled and therefore need to yield in order to let all the futures in the system continue running. Okay, I think those are all the things I wanted to touch on for async and await. And you'll notice that we haven't talked much about the lower levels, right? Like, we haven't really looked at the future trait any of like pin or context or waker.
02:20:23.840 - 02:22:08.434, Speaker A: I all of the bits and pieces that executors go through, those are valuable things to know about, but they're not really that relevant to try to understand just how do I use async? So hopefully this gave you a good survey. If there are questions now at the tail end of like the again, these higher level bits of what should my mental model be for async? Await. What are the intuitions? What are the sort of high level techniques? Now's a good time to ask them, and I'll spend some time just answering some questions here at the at the end, when I await inside a future, will the current future get scheduled on the same thread when it resumes to start progress? Or can it get scheduled on another thread when a future yields it, it just yields to whatever awaited it, right? So imagine that you just wrote something that was like, it takes a impul future. So imagine that I did this. Uh, if inside of f, you await, and then you end up yielding, you just go back up to b and it's up to b. What happens at this point, right? B, in this case, has chosen to await. And so it's just gonna keep it's gonna because you can't make progress.
02:22:08.434 - 02:22:54.630, Speaker A: It can't make progress. So it's gonna yield to, and eventually it's going to yield all the way up to the executor. At that point, the executor just takes that future and sticks it back onto the sort of job queue of futures which is handled in general by all of the worker threads, at least in a multi threaded executor like Tokyo's default one. But this depends on your executor. There are executors that are single threaded, and if you have a single threaded executor, it would execute on that same thread. Because there are no other threads in Tokyo, you're not guaranteed that it's the same thread. You could imagine that there are, or there are ways to say only run me on one thread in different executors, like in Tokyo, there's like a spawn local which gives you some of these guarantees.
02:22:54.630 - 02:23:33.268, Speaker A: And you could also imagine that you had like a instead of b doing an f dot await, it could do like a loop f dot pol I said I wasn't going to mention poll, but I will. Poll is the way to sort of check progress on a future. So you can imagine that b doesn't actually use the await keyword at all, so it never yields. It just loops you in a busy loop. If it does this, then f will get to run again immediately and it will be on the same thread, because there's no yield here. So there's no opportunity for the executor to reschedule you on a different thread. But in general this won't be the case.
02:23:33.268 - 02:24:46.720, Speaker A: So in general, all the way up the stack, it's going to be awaits all the way up to the executor, which does have the option of rescheduling you on a different thread. In general, the at least I know Tokyo tries to keep you on keep a future on the same thread, because it generally helps with cache locality and such, but it doesn't guarantee it. If you really wanted a future to not be sent across threads, you would just not implement send for it. Of course that makes it a lot harder to work with that future, but that would be the way that you would enforce that statically. What would be your favorite method to get an async stack trace? That is, to show the async call graph up to a certain point. So I don't have a great way to do this. It is true that if you print just a regular stack trace inside of an async inside of a future, you will get the call trace going up to the executor.
02:24:46.720 - 02:25:20.824, Speaker A: And usually that can help. The problem really stems from spawning. Right. If I spawn this future, then, and then let's say I panicked right here, then this panic would not show main. This may be a bad example. Let me have a async fn Foo, and it does a Tokyo spawn of an async that panics. To illustrate the problem.
02:25:20.824 - 02:26:14.570, Speaker A: This panic will not include Foo because this is a future that gets put onto the executors job queue as sort of a top level future to await. And then the executor, one of the executor worker threads is gonna pick up the future and await it. And at that point, Foo is no longer involved. All Foo did was like put the future on the queue. But when that executor thread awaits this future and it panics, the backtrace for that panic will only say the executor pulled this future, so it'll point you at this future, but that trace won't include Foo. I don't have a great way to solve this problem. I know that with, with something like tracing.
02:26:14.570 - 02:27:14.950, Speaker A: So there's a, if you haven't looked at tracing, it's great. So the tracing library is a sort of logging system that lets you emit events and then have subscribers to pick up those events. And one really neat thing that exists is tracing futures. No, that's not what I meant to do, and also, sorry that it's bright. So tracing futures, let me see if there's a good example of this is you can take a future import this trait called dot instrument, and then give it a, a sort of signifier that indicates the surrounding scope. So in this case, what we would do is something like spawn async and then dot instrument, like infoo. It's not quite what you would write, but, but you get the idea.
02:27:14.950 - 02:28:16.556, Speaker A: So this produces a future instrument, takes a future and returns a future, so this is still a future, and then that whole future gets spawned. And now, even though the panic isn't what you would do here, if you emitted like a tracing, tracing error, for example, that said, oops, this tracing error would include this in its sort of event path. And so that gives you a way to trace it back to where that future was originally spawned. And so that's one way to get at this that I've had some luck with, but I don't have a good sort of general purpose non instrumented solution. Sadly, best practices to call async code from synchronous code. Try not to do it. It's really hard to do.
02:28:16.556 - 02:29:05.080, Speaker A: Right. You can use something like futures block on. The downside of block on is that you don't really get this cooperative scheduling. You can use like you might end up with your program. Well, you run into some of the problems, like a particular future might be using IO resources from Tokyo, which the futures executor doesn't know how to execute. And so you get a runtime panic. That's one case you can get into if you just like blindly try to block on futures, basically.
02:29:05.080 - 02:30:13.240, Speaker A: And the other problem is you don't give the caller control or the user of your library control over execution. Basically, you're enforcing an asynchronous runtime on them rather than letting them choose the runtime, which tends to make people sad because whatever, like imagine that. Maybe one good way to get at it is imagine that I'm writing an asynchronous application, and my asynchronous application calls your synchronous library, and I use like spawn blocking or something. And then your synchronous library creates an asynchronous runtime in order to block on a future internally. So now I have a nested asynchronous runtime that causes all sorts of problems. Sometimes it's runtime panics, sometimes the two are incompatible as you get up with like other runtime panics, sometimes just a performance problem where now you spawn twice the number of threads that don't really cooperatively schedule with one another. It's just, it's just a nightmare.
02:30:13.240 - 02:31:44.484, Speaker A: I would say that if you have asynchronous operations internally, expose them as asynchronous, and then leave it to the user to choose how to make them synchronous. So basically don't if you can avoid it. Are there use cases where you want to use threads instead of futures? There are so anything that's very compute heavy. Asynchrony doesn't really add very much and tends to sort of get in the way because all of your operations have to be marked as blocking, and then it just gets more annoying to write them because you need to pass them around as closures. So there, I would use something like rayon instead if it's very compute heavy, if you don't really have IO, if you have a program that, yeah, if you have programs that don't really do IO, there's no real reason to use Async. It is true that in general, writing non async code tends to make at least simple code or streamlined code or single execution code easier to read gives you better compiler errors. The borrow checker generally works better if you're not in async context, the backtracks work better.
02:31:44.484 - 02:33:01.130, Speaker A: So if you're writing just sort of straight line code like you're writing a simple command line tool or like a converter or something like I probably wouldn't bother with async because it does make your task a little harder. If you're writing something where you think that it might be IO heavy in the future, or you might want to do like handle multiple different types of events, like this is where select is really useful and emulating the same pattern and threaded code is a little bit of a pain, then I would lean away from threading. But if you really just don't need the mechanisms that async provides, then use normal threading. Tokyo by default doesn't bring in any runtime. And then there are feature flags for enabling IO frame enabling or like file IO enabling networking, enabling timers, enabling the single threaded runtime, enabling the multi threaded runtime. So in general you, you opt into the different features that you want. If you call Tokyo Spawn and you're not on the Tokyo runtime, you get a panic.
02:33:01.130 - 02:33:46.998, Speaker A: All right, I think that's where we're gonna end the stream. Two and a half hours, that seems about right phrase and Kuwait. I hope that was useful. I hope you feel like you have a better intuition for what's going on under the hood and some of the pitfalls. And if you're really curious about how this is actually accomplished, I recommend that you go, you can look at my older video about sort of how async really works and the video on how pin works. Some of that is going to be a little bit more technical, but it is maybe helpful knowledge if you really end up digging into the details here. But hopefully this has given you the high level intuition and knowledge you need in order to be productive with async.
02:33:46.998 - 02:33:57.830, Speaker A: Await. At least that's my hope. Great, thank you all and I will see you next time, which will hopefully be another hazard. Pointer stream. So long everyone.
