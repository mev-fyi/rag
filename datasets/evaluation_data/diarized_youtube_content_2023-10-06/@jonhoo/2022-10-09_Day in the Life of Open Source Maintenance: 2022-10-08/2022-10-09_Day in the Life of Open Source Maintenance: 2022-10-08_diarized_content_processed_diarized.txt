00:00:01.760 - 00:00:43.725, Speaker A: Hey folks, this was entirely impromptu. I had some GitHub notifications I wanted to get to over the weekend and I figured if I'm going to be doing open source programming anyway, why not stream it? And if anyone thinks it might be interesting, they can watch it. And for people who don't care, they can just not watch it and it's fine either way. It cost me very little in this case. I have like, I think I about two pages of GitHub notifications to get through. I have no idea how far we're gonna get. I'm not, you know, I'm not aiming to get through all of them today necessarily, but I figured, you know, there's a quite a variety of issues and PRs to look at here, so I'll just, you know, sort of start from the back and move forward.
00:00:43.725 - 00:01:50.145, Speaker A: If you've watched the previous one of these streams where I do open source development, you'll see that you'll recognize some of these notifications as things that I skipped last time and I am also going to skip them this time. I think this one is probably the first one I haven't really looked at in a while. So this is a PR I filed to Rust itself to make it so that the AR64 build of the Rust tool chain builds LLVM with support for compression. It's a good question, why did this get stuck? I got stuck because we need Libz for AR64 because I think it gets cross compiled. I don't think I want to pick up the context for that here. I think the fix here is actually pretty simple. There's a lot to catch up on, so I'm going to mark that as unread and come back to it later.
00:01:50.145 - 00:02:20.515, Speaker A: The current starting experience is not user friendly. They're totally right. So this is for a crate called Tracing Timing. Tracing Timing is a subscriber for the tracing crate. So tracing lets you emit events. Think of it as you can do logging, except it's more structured and it doesn't have to just be logs. It could be metrics, any kind of observability event that you want for the execution of your program.
00:02:20.515 - 00:03:04.195, Speaker A: And then tracing sort of separates the things that produce the events and the things that consume or subscribe to the events. And in this case Tracing Timing is a subscriber to events. It consumes events from tracing. And in particular what it does is it looks at the time between each event and then it keeps a histogram of where you end up spending most of your time. So this can be useful for tracking down, you know, let's say that you have a request handling function or something, and it has a bunch of steps. You can use this to figure out which steps are taking the longest across many requests. It's a little fiddly to set up.
00:03:04.195 - 00:04:02.765, Speaker A: And, you know, they quote from the docs here of the docs, basically saying you're sort of on your own here. This is mostly a documentation problem, although part of the complication here stems from the fact that it does concurrent recording and if you have multiple requests, it tries to not do any synchronization between the requests because that would make logging these events slow. And instead it puts it on the consumer of the library to say, now I want to gather the data from all the different threads that might have been gathering histogram data. That makes the API a little awkward, but it is also. It also lets the subscriber implementation be a lot more performant. Documenting this is fairly tricky. It's also not a crate that I actively use anymore, not because it's not useful, but because I don't really write software that needs this particular functionality these days.
00:04:02.765 - 00:05:09.745, Speaker A: I use this a lot for Noria, for trying to figure out when a particular read or write request to the database is being processed. Where does that time go? And subsequently you have, where can I shave it off? Where are there weird distributions where some part of the code is usually fast, but sometimes has, you know, large peaks you can look for, like bimodal distributions, for example, in your histograms. But these days, because I work more on build tools, that doesn't come up quite as much. So here I'm going to mark this as unread too. There's a lot of context to bring back in, and I don't think it would be very useful to see me remember how this works and write up a bunch of docs. So someone's asking in chat for, you know, if they were to contribute to tracing timing, where would you start? I think this is a great place to start. Like, I think the documentation for tracing timing isn't awful, as in, you know, it does document how to use it, what its output look like, how to extract the histograms, how it interacts with the subscribers.
00:05:09.745 - 00:05:38.789, Speaker A: So it's not as though this crate has no documentation. It's more that the. As. As the issue says, the. The starting experience here is, is kind of painful because there are so many knobs that you sort of need to get right. And part of this might be just putting in More examples to the crate for like this is how you set it up, this is how you run it and I would be totally happy for someone to just like contribute to those. In fact, this is a crate that I would be happy to basically give away.
00:05:38.789 - 00:06:40.715, Speaker A: I, I don't have a strong need to own it myself and general giving away open source projects is a little weird because I don't want to just give it away wholesale to the first person who comes along. I want to sort of mentor them into it a little bit, shepherd them into it and then when they've demonstrated that they have the know how to responsibly take care of the crate going forward, then you know, I would happily hand over the reins. And the same is true for timing. So I think if someone wanted to contribute to this, you start by tackling a problem that is not about fixing the low level bugs in the code or anything like that, but more things like the getting starting experience or the API or the documentation. Start there and then work your way into the technical details. And I'm happy to take PRs on any frontier really. You don't really need to, you know, focus on a particular thing to be able to get your PR in.
00:06:40.715 - 00:07:47.119, Speaker A: Whatever PR you think would be useful to contribute I would happy to take a look at and you know, the open issues are always a good place to start. All right, Mark is on red. That brings us back to here. So you'll notice some of my notifications here are they're red but they are not marked as done. Usually these are things where there's no more action necessarily from my side in terms of that issue or pr but there's some separate follow up work I need to do. So in the case of this crates index diff for example, this is a bug in crates index diff that or not a bug, let's call it a missing configuration flag that I would like to get added and it's work that I should do at some point and contribute a PR and you know, there's no rush to do so. It's just think of it as sort of a very low priority to do list.
00:07:47.119 - 00:09:09.295, Speaker A: Arguably I should just like bookmark it for later but I never look at my bookmarks or this one when multiline pasting and TMUX new lines are stripped. This is a bug that I occasionally run into on my work machine when ssh to other machines and running VIM in TMUX and it's this is more of a reminder or an easy way for me to find back to the issue if I find it again, I want to share some more information about the context in which it happens, but there's nothing for me to actively do about it. Sharded counter optimization let's go look at this. So this is in Flurry. So Flurry, if you remember, is our port of the Java concurrent hash map to rust. The work here is one of the to dos that we left when doing that porting was that the concurrent histogram has a counter, an atomic counter that's used to keep track of how many elements are in the map because you don't want to count them, just like by walking the whole map, because that's going to be really slow. But if you keep a single atomic integer, that's also not great because all that means all axises that update the map have to contend on this one integer value.
00:09:09.295 - 00:10:04.249, Speaker A: Some of them are going to increase it, some of it decrease it. And computers can be fairly efficient about doing those kind of counter updates. But even so, especially once you get to multi socket architectures like numa nodes and such, that doesn't really scale. And so one of the optimizations within the Java concurrent hashmap is they use effectively a sharded counter. So rather than having one counter that all the threads update, you keep some larger number of counters and how you choose and which one you choose to update can vary. And then you have readers read all of those counter and sum them. The idea here being that writes now only contend with other or updates only contend with other updates that access the same shard of the counter and readers just do reads so they parallelize really well.
00:10:04.249 - 00:10:56.545, Speaker A: They don't need to take an exclusive lock on the underlying memory location. And so this is someone who, well, so there used to be this PR which is looking to add shared counters and there was a lot of great work there. And they posted a bunch of benchmarks. Let me see if I can find these. Which actually it was a little unclear what the win was, but in some cases you can see here for example, so this is with 8 cores using the shared counter versus just a single atomic counter. You can see that it ends up running faster or more operations per second than the equivalent atomic counter. And if you have fewer threads it doesn't really give a speed up, which also also makes sense.
00:10:56.545 - 00:12:08.815, Speaker A: And then ultimately because they ended up moving away from the implementation that was in this pr, they opened a second PR and closed the first one that has these other operations instead. And part of the thing I asked for in this PR was for the Logic to keep this sharded counter to be extracted into a separate crate. And so they did that in the form of this fast counter crate, which is again using a basic sharded concurrent counter. Now the question is, where did I leave this off? Right? So this is one of the things that's complicated about being, you know, being a maintainer and asking for, basically asking for another dependency is you don't have to audit that dependency, right? In many cases it's not feasible to do a full review of all of your dependencies. In practice, like I just don't have the time. But in this case, this is pulling out a fairly performance critical part of Flurry, the counter that is. And I want to make sure that the underlying implementation we're swapping it out with is actually a sensible one.
00:12:08.815 - 00:13:02.585, Speaker A: So in this case, the PR here, you'll see if I look at the diff, is actually fairly straightforward. So it takes a dependency on fast counter with some features and then it replaces the counter that we have in Flurry with a concurrent counter. Instead of the atomic eyesights we previously used, it creates a new one where the number of shards is the number of CPUs. This is a pretty common way of sharding. It doesn't have to be this way. You can also do things like shard by CPU socket or by, you know, by numa node or something that makes the operation for choosing which counter to update a little more complicated than if it's just the number of CPUs and you can just use the current CPUs ID as the index into the list of shards, for example. But number of CPUs is fine here because this is one per map.
00:13:02.585 - 00:13:50.559, Speaker A: Generally we're not going to be very concerned about the memory use of, you know, let's say you have a, let's say you have 200 cores, right? Then this is 200 times the size of an integer. That's how many counters you're going to keep for one map. It's not great, right? But also, 200 cores is a lot of cores and the memory here is probably not really that bad. It makes a note about. This is probably for the, this is for the length operation on the map. It just adds, adds a note, which is something I requested that the read you do might be in some sense different from the truth. When you have a concurrent map, there are going to be updates continuously at the same time as you're calling length.
00:13:50.559 - 00:14:48.131, Speaker A: So what is the true length really? If you read in the middle of multiple updates and so really what this is saying is that in the absence of concurrent updates, you get inaccurate results. But concurrent updates that occur while the sum is being calculated might not be incorporated. So basically this is sort of an approximate read of the number of items in the map. And you see this just calls the sum method on the concurrent counter rather than loading the single counter that we had before. Yeah. So here I made the optimization that previously we had an optimization where so you can do multi updates in the map. You can do things like add multiple items, remove and add an item, or just replace an item in place.
00:14:48.131 - 00:15:48.951, Speaker A: And in the cases where the length of the map doesn't actually change, you don't need to do any atomic operation. And this is the example here, right? If we compare the delta of the count to the previous count, sorry to zero, and if it's equal to zero, then we just load the counter, we don't actually update it. And this is an optimization that I said I wanted us to keep. The other observation I made here is that the count we want to make sure it takes an eyesize rather than an usize or rather. Actually that's not quite accurate. I want the counter that internally in the library that we're using to hold you sizes rather than eye sizes because counts are always positive. But I wanted to be able to add I sizes, right? Because if you have a number that's a count, you want to be able to subtract from that number.
00:15:48.951 - 00:16:21.597, Speaker A: And in fact that's what we did here. Right? Count used to be an atomic you size. And so what that meant is if the eyesize was negative, we would have to call a different method with the positive value to subtract. But this has the benefit of you can technically store a larger count by a power of two because you don't need to store the sign. Practice, this doesn't really matter. You're not going to get to a count that's like 2 to the power of 64 anyway. And we'll see.
00:16:21.597 - 00:16:52.129, Speaker A: So it looks like they actually made a change here. I haven't looked at what that was removes on NSC unsafe. Okay, so you see, the change is actually super simple here. But that's because all of the complexity of that sharding is being moved into this fast counter crate. So let's see what the updates were since last time. You see, I posted this August 20th. They got back to me August 21st and it's now October 8th.
00:16:52.129 - 00:17:49.733, Speaker A: I apologize, Jack Thompson. Right. So I made some other observations. That one is to remove the need for an unsafe cell, they kept a thread local to keep track of the current thread id and they used an unsafe cell to be able to modify that thread local. And instead you could use a cell which allows you to mutate through a shared reference, as long as you're not sharing the cell amongst multiple threads, which you don't, because it's a thread local, so it's guaranteed to not be shared on Nightly. As I made the observation that on Nightly you don't actually need to use this thread local at all, because you can just use the thread id from the standard library which has a. As u64 just lets you get the an identifier for the current thread.
00:17:49.733 - 00:18:29.565, Speaker A: Anyway, sorry, by a factor of two, not a power of two. You're entirely right. And the other was why do you even need Nightly here? Can't you just do this on Stable? And I think they used Nightly because on Nightly there's a. You can do thread locals without pulling in an external dependency. Or rather it's not even that, it's. You can do it without a macro. There's a macro in the standard library called thread local, and on Nightly you can use an attribute thread local instead, just on a static variable.
00:18:29.565 - 00:18:57.195, Speaker A: That win didn't really seem worth it to me. If that's all we're doing in order to get like if. If it's really just a syntactic thing, it doesn't seem very worthwhile. But they're saying originally the performance difference was greater. I guess we'll see how that came back September 7th. Been looking into this and a few thoughts and findings. Thread id as u64 proved to be significantly slower.
00:18:57.195 - 00:19:34.685, Speaker A: Oh, interesting. I wonder why. What does thread ID as U64 do? It just reads self zero. That's interesting. Oh, I'm guessing they call thread ID new. Yeah, what they're probably doing is. So there's a.
00:19:34.685 - 00:20:03.571, Speaker A: There's a thread current that gives you one of these thread objects and you can call. You can call ID on it to get the thread ID, and then you call as U64 on it to get the U64. I'm guessing that they're actually not storing the thread. Or either that or.id is slow. Self inner. Oh yeah, inner is an arc.
00:20:03.571 - 00:20:28.065, Speaker A: So maybe they store the thread rather than the thread id. That's my guess, because then you do have to go through an arc, which is a pointer indirection. It shouldn't really matter that much. It's a pretty simple pointer chase. That's interesting. 2 to 3. X slower.
00:20:28.065 - 00:21:28.295, Speaker A: Eventually show the counter being slower than a regular atomic U size on every core count. So a cell for a thread local. That's real weird because a cell in R is just a repertransparent over an unsafe cell. And when you call set on a cell, it's the same as a self replace, and a self replace is just a memreplace of an unsafe get. So something's real weird, like a cell is just an unsafe cell. Interesting. It made it slower than a regular atomic U size.
00:21:28.295 - 00:22:31.225, Speaker A: That's wild. Okay, so this is a good point, this last one, which is. Let me make this a little bigger so it's easier to read. So this last one is interesting. The observation here is that I think the way this is set up is that every time you modify the counter, you actually, instead of trying to figure out which thread am I on or which CPU am I on, and then operating the appropriate counter, you pick randomly. Or actually maybe it uses a thread id, I'm guessing, given the code here. But imagine that, that when you, when you remove an item, you're not necessarily on the same thread that added the item, right? So imagine, let's say that, you know, all the threads happen to choose the same counter every time.
00:22:31.225 - 00:23:11.485, Speaker A: So that one keeps incrementing and is basically the value of that is the size of the map. And then some thread comes along and tries to do a delete. So it deletes a thing from the map and it starts to decrement a counter, but it decides to decrement a different counter than the one that everyone has happened to be using so far, which is fine because it's a sharded counter, but that other counter is zero, so it can't decrement that counter. And it doesn't know which counter has all the values. And so if you make all of them eye sizes, you don't have to deal with this problem because the sum across all of them is going to be fine anyway. So you can just decrement and make it minus one, and it's fine for one counter to be that. That's interesting.
00:23:11.485 - 00:23:54.203, Speaker A: Okay, sometimes the GitHub quote thing is real weird. Let's try that again. Quote reply. All right. I'm always torn on whether to apologize for giving slow replies, because on the one hand, I don't really have anything to apologize for. All of this is a volunteer work anyway. But on the other, like, they did the did a thing and wrote up about a thing, and I didn't get back to them for, you know, a month.
00:23:54.203 - 00:24:06.125, Speaker A: And that's not. I don't feel good about that. So I'm. I'm torn. But I don't know, maybe I'm too nice. But I like it. Let's see.
00:24:06.125 - 00:27:19.895, Speaker A: So here I think what I want to do is how about we say looking at the STD source, it seems like the arc is in the thread type, whereas the thread ID type is just a plain integer. I'm guessing this ends up being this ends up slow because you're either calling thread current on every access, which is slow, or keeping the thread around, which means going through the arc each time to get to the numeric id. I think the way to fix this is to store the thread ID in a thread local. That way you should have super cheap access to it. Thread is it thread id or no lowercase D. Okay, as U64 is just a field access without having to keep an ID counter yourself self let's see Producing cell over unsafe cell super weird. The code for cell is really just an unsafe cell with repper transparent and cell set just does mem replace unsafe 0 which should cost as much cost exactly as much as just setting the value and certainly not as much as running atomic operations.
00:27:19.895 - 00:29:53.199, Speaker A: Could you give the raw numbers you observed to see if we can figure out what's going on? Oh yeah. So this is about removing nightly. Nice. That's enough there and a totally valid reason for using an eye size for the charted counter. I think I'd still prefer for the API to return a use because the sum should never be negative though maybe we which I think we can simply assert with a debug assert so it doesn't affect release performance Release build performance. It's fine to make add take an eye size though I think since it's a little weird but not the end of the world Great. Oh, did I typo? I probably typoed.
00:29:53.199 - 00:31:26.565, Speaker A: Where did I typo sharded? Did I typo sharded somewhere sharded? I don't see my typo. Maybe I'm wrong. Let's see what are people saying in chat? Not sure I like it passing in CPU count when creating the counter versus later use of ad which doesn't take in a thread ID also so I would think of number of shards is so the argument to new is the number of shards you want and it doesn't have to be the number of CPUs just like if the I think if I remember correctly the internals of the library is it uses the thread ID modulo the number of shards to Figure out which shard to update. And so I don't think it makes sense for add to take an argument. Although there's a decent argument for shared counter, it doesn't say shared counter anywhere. There's no short counter. It might make sense to have a, you know, an additional mechanism for saying I want to do this add on this particular counter, but I think that's rarer to be used in practice.
00:31:26.565 - 00:32:39.167, Speaker A: And what kind of project do you need such a library? It's super interesting, but I work in web, so I can't imagine what to use it for. Actually in web development it comes up quite a lot, not necessarily needing a shared counter, but you do need a shared map quite often, right? So imagine things like imagine you want to keep a cache in your. So you have your little web server running and you know, there are all of your different request handlers get to run in parallel and let's say that they want to cache some stuff in memory to make it faster to access. Well, now you want to share state using a key in memory between threats, which is what a concurrent hash map is for. And if you are going to be doing that, then you want that shared hash map to be as fast as possible and work as well in parallel as possible. And if ultimately your shared map works really well in parallel, except for the counter which ends up being centralized and contended on, then ultimately it's going to end up being a sort of performance problem, especially as you start scaling to very large number of cores. And so that's why you care about this.
00:32:39.167 - 00:34:06.489, Speaker A: So you as an individual web developer might not use the sharded counter library directly, but you're decently likely to use something like, like a concurrent hash map or use some like let's say web framework that provides caching for you by using a concurrent hash map internally. Right? The alternative would be you keep like a mutex over a regular hash map. But that means that every request handler you have, they're all going to serialize all access to the cache, which you'd probably rather avoid. Is there actually a case where a sharded counter would be a negative number? It's a good question for a sharded counter. The question is more about the algorithm that runs the sharded counter. Could it ever, because of all the concurrent accesses, could it ever be that a decrement happens before the increment that it's supposed to have incremented? And in general that shouldn't be true, right? Because at least the. Well for counters actually it's weird because counters are usually updated lazily because they don't need to be perfect.
00:34:06.489 - 00:34:45.545, Speaker A: And so usually the concurrent algorithm you're using tends to have these mechanisms that ensure that you can never remove something until it's been added, right? That. That shouldn't be possible. There has to be some kind of. In a map, it's often by key, where access to a given key is going to be sequential, so you can't have it be removed before it's inserted. But for the counter operations, maybe. In which case, maybe there's an argument for it's possible for a brief period of time, a counter to actually be negative, right? So imagine that. Imagine you initially, you start out with an empty map.
00:34:45.545 - 00:35:34.665, Speaker A: One thread does an insert, another thread does a read and a remove, and the two race. So the insert code runs, and it runs all the way up to where it increments the counter. And if you assume that the counter increment happens at the very last, outside of any other synchronization, then the insert hasn't returned yet, but the value is still observable in the map. And then a read runs, it finds the thing that was inserted, and a remove runs to remove that value. And the remove runs to completion before this thread gets to continue. So that includes the decrement. And so there's a brief period of time after that remove finishes, but before the insert finishes, where the total set of the counter is actually negative.
00:35:34.665 - 00:36:58.469, Speaker A: That's a good point. I think you're entirely right. Second thought, in a concurrent setting, with lazily updated counters, it's entirely possible that a counter ends, that the sum of the counters ends up being negative for some shorter period of time. So I think we should actually leave the return value as an eye size. Someone's asking why thread ID modulo CPU count? It's not modular CPU count, it's modular the number of shards. The idea being that you want, in general, you want different threads to access different shards, but you might have many more threads than you have CPUs or certainly then you have shards. In fact, for something like this, it's not obvious that you should choose number of CPUs as the number of shards.
00:36:58.469 - 00:38:10.005, Speaker A: As I said, one example is you could choose the number of NUMA sockets, for example, or you could just choose like three, right? Just three is going to cut your contention by a third, assuming the distribution of the thread IDs is roughly uniform in terms of their access. And like, there's a big question here of what is the right number to choose? And it partially depends on your algorithm for choosing which shard to use on access. You don't have to use thread id. You can pick randomly, although generating a random number is also takes a bunch of cycles, because reads might have to access this. But even for writes, you want generating the index to be relatively cheap as well, and certainly not require any synchronization, which is why the thread id, if you have it in a thread local anyway, is a pretty cheap way to do it. Another is choose randomly, although then you have to generate a random number, but you might get more uniform access over time. Another is use the CPU ID modulo the number of shards you have, right? And if the number of shards is the number of CPUs, you're guaranteed to not contend with other CPUs.
00:38:10.005 - 00:38:46.495, Speaker A: But getting the CPU ID is actually fairly costly. Not to mention you might be rescheduled between when you check your CPU ID and when you access the shard. So it's not perfect that either, even just because of things like Interrupt. So it's actually a fairly tricky game that you're trying to play here, but you can choose anything. Like you can choose in an asynchronous concurrent execution context. You might choose the task ID or something that works fine too. It's just you want something that is fairly likely to give somewhat uniform of an axis pattern or distribution across shards, rather.
00:38:46.495 - 00:39:43.815, Speaker A: Okay, so I think we're now done with that one. And you know, with my luck, they're gonna respond before I even finish the stream. Okay, so this is a thing in Cargo. So Cargo recently landed the ability to pass additional configuration options using Cargo Dash dash config. And you can give here you don't have to give a file name. You can give either a file name in which car, in which case Cargo is going to read that file as a cargo configuration file and merge it with whatever configuration applies to the build. Or you can give, you know, dash dash config build target equals like a single TOML key value pair.
00:39:43.815 - 00:40:26.637, Speaker A: And this is convenient for just, you know, ad hoc configuration where you don't necessarily want to change your entire cargo config, you just want to change something for this run. If both an environment variable and that are specified at the same time, the environment variable overrides the file passed in over the cli. This is counterintuitive. And also the documentation is unclear about this. Right? So the documentation claims that configuration values specified this here, meaning with dash dash config take precedence over environment variables. And so this is just wrong. And I guess the steps are going to demonstrate this.
00:40:26.637 - 00:40:57.607, Speaker A: So in the cargo file we're going to do. Huh. So this is actually. The steps here are problematic for a different reason, but let me talk through it first. So the idea here is you set an environment variable that tries to specify this value. This is a neat trick to know about. With Cargo, you can set basically any configuration option using environment variable just by turning it in all uppercase, turn every dot into an underscore and prefix it with cargo underscore.
00:40:57.607 - 00:41:46.475, Speaker A: So cargo underscore, build underscore target underscore dir is going to set the build target dir configuration value in cargo and then cargo build config dot cargo food automo. Oh, actually, ignore me. I was going to say that cargo is going to automatically load this file as well, which gets weird, but it doesn't because this is called food toml. So the observation here is that one configuration file is passed in here or 1.1value set for this flag is passed in here, a different one is passed in here. And based on the documentation, it's supposed to prefer this value because it was passed with config. But instead the build happens in from env rather than from futaml, which suggests that this value is preferred.
00:41:46.475 - 00:42:30.205, Speaker A: And I agree this is straight up incorrect. I'm being cc'd because I landed. I stabilized config in the first place. I didn't implement all of dash config, but I helped stabilize it. And yeah, that's interesting. So actually the reason why this happens is somewhat interesting. Let me see if I can't explain what's going on here.
00:42:30.205 - 00:43:55.445, Speaker A: So in Cargo, there's a bunch of stuff that parses configuration files. What specifically was this called? This was called load file load values unmerged. Un unmerged Load values unmerged. Okay, what calls this? Yeah, so. So this is the thing that loads CC walk tree here. This is the part of Cargo that loads the cargo configuration files from cargo config toml all the way up the tree from above where you run that cargo command and you see it's unmerged because it just returns a vector of every value it finds. And in addition to that, there's.
00:43:55.445 - 00:44:45.895, Speaker A: So there's load values from. There is load file load includes. Where did I end up adding this code? This was in include CLI args as table. So this is the part that parses the configuration options out of. Out of the config args. And you see here, this is the part that if the argument is a file name that we load, that file. Otherwise we basically create a single element toml document and return that instead.
00:44:45.895 - 00:45:39.895, Speaker A: And I have a feeling we're going to have to return to this line. So this calls load file with this last argument being set to true, right? Because it's a config include rather than one that was just found in a normal walk through the stack. And so the question now is what calls the cliargs merge? Cliargs, that's fine, that just merges. If you have multiple dashes config, it merges them together. So that's entirely reasonable. What calls that reload rooted at. So that loads all the values from a given path.
00:45:39.895 - 00:47:06.825, Speaker A: So this is what cargo uses to when it has decided where it's going to run from. Load all the config values, config files from cargo config from above where we are, and in addition merge all the CLI args to cargo. And this is the thing that allows you to set unstable cargo flags in the cargo config. And so the question becomes why the config that gets set here? Why does that get overridden by environment variables? And I think I know the answer to that if I can find where that gets set from. Oh, so it's like envelope. Yeah, so see here, this is the macro that's used to extract values, get value typed and you see it gets. So get CV grabs it from all the configuration that we've built up by reading things from disk or from config values from the arguments and get envisioned just returns that value from the environment if an environment variable by the appropriate name has been set.
00:47:06.825 - 00:48:18.399, Speaker A: And so here we see if both are set, like if it's both set in the config and in the environment, then if the definition is higher priority than the environment, then it returns the value from the definition, otherwise it uses the one from the environment. So the question is, was this is higher definite is higher priority thing? Let's see if we can find that. Okay, so let's see if we can dig that up. Is higher priority. So CLI is higher priority than environment, CLI is higher priority than path, and environment is higher than path. Okay, so there's one bit missing here which is if it is a. If it's a.
00:48:18.399 - 00:49:05.035, Speaker A: Well, so what's tricky here is dash has config with a file name I believe gets recorded as a path. In fact, we can figure that out if we go back to this file and we go to the line that we were at previously. See, I knew we're going to get here. So fn load file does here right CV from toml definition path and you see it uses definition path even though it was actually loaded from a command line parameter. So really what we and part of the complication here is this includes argument. It's actually a little tricky. It's false if we're walking the tree the normal way.
00:49:05.035 - 00:50:08.927, Speaker A: It's true when you use dash config and then a file name. But it's also true for an experimental cargo feature that allows you to put include statements in cargo configuration files to basically embed another config in the cargo config. And for those we wanted to inherit the definition that was already in place. So it's not just as though this should be definition cli. If this was true, I think what we actually want here is that load file here should set the definition as follows. If this is not an include then use path, which is what it does today. If it is an include and it's from the CLI directly, then use cli.
00:50:08.927 - 00:51:12.815, Speaker A: If it's an include and it's not directly on the cli, then use the definition of the context we're being parsed in. So this is where it gets tricky. The way to think about this is if we're loading a path because of an include in another cargo configuration file, we should inherit its definition. Because if it was loaded from a dash dash config, then we should also be marked as being loaded from config. If it was from a path, then we should also be loaded from a path. Now, I'm not actually going to implement the fix for this right now, but what I'm going to do is instead comment on this and you'd see weiheng is already weiheng is one of the cargo maintainers has already commented will eventually convey to config file definition path is the lowest priority of the three in comparison with the nvar occurs here. Yep.
00:51:12.815 - 00:51:48.969, Speaker A: So the nvar always wins over the configuration. We may want to construct a config value with definition cli here when load file is triggering is triggered by config cli. The doing that loses the path information of definition path regressing the error message. Right. So this is another good observation that. So the definition parameter to a config is what cargo uses to. If there's something weird about the configuration to tell the user where that configuration came from so they can go fix the problem.
00:51:48.969 - 00:53:28.395, Speaker A: If we assign it as definition cli even though it was a file that was included, then the error message is going to say you pass this on config but that's not really what was happening. Pull request definite cli sum let's see. So this hasn't been reviewed yet. Let's go in ahead and see what change they made. Yeah, so it became an enum right, instead of a bool and so load values unmerged went there load unmerged include is file discovery. So I think this is actually wrong. And load values from uses and let's see, load file depending on why it was loaded, sets the appropriate definition and load includes.
00:53:28.395 - 00:55:16.355, Speaker A: Oh yeah, actually no, maybe this is doing the right thing. So load unmerged include calls. Feels weird that this uses file discovery because I feel like it might not be like if we go down to the thing that loads where is our. Where is our. This one cliargsys TABLE Load file Y load CLI oh yeah, load includes. So the way they dealt with this is the load includes takes a whileoad and so if we're in the path, if we're in the regular file discovery loading then we go through load includes and we pass in file discovery. If we're in the config setting we'll call load includes on any nested includes with cli and then it is going to load the file with that appropriate definition and this is going to preserve the path even if it was loaded through cli.
00:55:16.355 - 00:56:24.265, Speaker A: Nice. And now it's no longer be parsed as a path, it's going to be parsed as a cli which fixes the behavior but the path is still preserved for things like printing. Nice. And now you know, this is where the old definition path was being injected, just always in load file. And now depending on why it was loaded, it'll generate the right thing and then it's going to nestedly load includes using the same reason why it itself was loaded. So you get the recursive case. Yeah.
00:56:24.265 - 00:57:29.735, Speaker A: Right. So the tricky case here is when a config of a file ends up triggering an include of a file and it looks like that's being treated correctly here. You know, the one question here is should cargo warn you about this? Right. Like in the original issue here of you know, an environment variable but also an explicit config flag, which one should take precedence? Should it really be the config? Should that take precedence over environment variables? It's hard to say. I do think that in general explicit arguments are preferred over environment variables. Right? Like think of something like, you know, think of something like make. So if you run NCC equals you Know GCC make CC equals clang install.
00:57:29.735 - 00:58:12.321, Speaker A: What would you. Which CC would you expect it to use? I think you would expect it to use clang and I think it would use clang. So I think the precedence here is pretty sensible. It does mean. So someone pointed out in comments that if you run Cargo by pointing it at the exact same file as it would have used anyway. Right? So if you do, you know, by default Cargo will look at dots slash sorry dot slash dot cargo slash config dot toml. So if you did you know, cargo build dash dash config and passed in that path, then you're basically giving that file higher precedence in the hierarchy.
00:58:12.321 - 00:58:57.011, Speaker A: But I think that's intended, right? Like you explicitly said, use this file. So I think that's reasonable. I think the, the documented behavior here is right. And, you know, now that we've gone through all this work of figuring out that this actually did what we expected, let's go see whether we can just approve this. I don't have permission to merge CR merge PRS and Cargo, but we should take a look. So this is checking that it's a test that creates two config files and an environment variable. And it says if we set.
00:58:57.011 - 00:59:55.087, Speaker A: Yes, you see, this is the trick, right? Cargo underscore K is going to set the same as K in the cargo config file. If we run that, we would expect to get CLI1. So if you have multiple configs, the latter takes precedence and they both take precedence over the environment variable. What's the difference between the first test and the second test? Oh, this one is also including a path, and then that path takes precedence over all of the above because it comes last. And if you also give a key after the path, that takes precedence. Nice. So you'll notice this first one is a test just of the config builder type that Cargo uses internally.
00:59:55.087 - 01:00:51.485, Speaker A: And then this one is an actual integration test of Cargo proper. I believe she's using the config builder. This is also using the config builder. So what's the difference between these tests? Oh, this is a test for the include feature specifically. So this is if we have a dot config. If we have a config that sets an included, then we want to make sure that the included file gets loaded at the right priority as well. That makes sense.
01:00:51.485 - 01:02:02.295, Speaker A: Okay, so we only found one bug, I think, which is this. Say priority approve. Looks good to me. Do one typo in the test name. No prove. Nice. All right, so I think that means I can now mark this as done because I did did my work.
01:02:02.295 - 01:02:56.801, Speaker A: All right, I can close these again. What are we back to now? Add error status enum and remove webdriver error. Okay, so this is. We're jumping to a different crate again. This is why catching up on my GitHub notifications is often, I don't want to say painful, but often takes me a while is because at this point I have so many different repos that I have to bring in all the context for each one. Not just because it's stream, but even for my own sake, I have to bring in the context or, you know, repopulate my cache or something of why was this an issue? What were we talking about? What was my last round of review? So Fontaccini is a Rust library for the web driver protocol that lets you control browsers programmatically. And in particular, it aims to be a relatively low level library.
01:02:56.801 - 01:03:19.485, Speaker A: It doesn't try to, you know, be. It doesn't have. Try to have like ergonomic, fluent ways to do things. It tries to map pretty directly onto the WebDriver client API. And then the idea is you would use a crate like 34. Like, I think it's literally called 34. Yeah, I don't know why, I don't know what to call 34.
01:03:19.485 - 01:04:09.947, Speaker A: But it's a web driver library for Rust and it tries to be, you know, really nice. Like it tries to have an ergonomic interface and it's actually built on top of Fantaccini. It didn't used to not be, but now it is. The idea being the Fantaccini provides like the underlying mechanisms and then this just provides the ergonomic mappings on top. And this, this person, Steve Pride, is the person who owns or maintains 34. And so we went a lot of back and forth on whether 34 should build on top of Fontaccini, what changes should we make? And ultimately, you know, we managed to do some releases where it's now built on Fantaccini. One of the problems the Fontaccini has is that it uses the webdriver crate, which is published by Mozilla.
01:04:09.947 - 01:04:55.849, Speaker A: And, you know, that's not a problem in and of itself, except that the web driver crate tends to get updated fairly regularly. Like every. In fact, I think it's quite literally every three months they cut a new release and everyone is considered a breaking change. And that means that at least in theory, Fantoni would have to cut a breaking release every three months as well, in order to upgrade its version of WebDriver. And the reason for this is because we have some of the webdriver types and we can look at what those are. But basically the webdriver crate provides. That's a bad example command.
01:04:55.849 - 01:05:45.601, Speaker A: It provides basically the protocol type definitions for the WebDriver protocol. It doesn't provide any of the mechanisms, but things like these are the parameters to a get named cookie request to the browser. And it happens to be the same crate that, what's it called, marionette, the Firefox WebDriver implementation. So that is the thing that receives these commands and applies them to Firefox. It uses this crate to understand the things that we send. So it's really nice to be able to just use something that we sort of know is correct because it's being used by the other side, at least for some browser. But the problem is there are some parts of Fantaccini that actually exposes these types directly.
01:05:45.601 - 01:06:32.649, Speaker A: And one of the big ones is we expose webdriver error. And the reason we do that is because, you know, sometimes we get an error back from the web driver server that we're interacting with that we don't understand. Like it doesn't have a nice semantics, like, you know, no such element or something. It's really just like an error code and a string. And in those cases we, we try to just return what it gave us. In some cases, you know, there's an error status here that has a bunch of standard variants and we try to just propagate those onwards so that we don't have to replicate this entire enum in our crate as well. Problem is, again, that means that part of our public API is part of the webdriver crate.
01:06:32.649 - 01:07:26.541, Speaker A: And whenever that's the case, it means that a breaking change to that crate would mean a breaking change to our crate in order to upgrade. And that's becoming a problem. We don't really want to have to do that because as you see, web drivers are already on version 46 and it's just going to keep ticking up, it's not going to stabilize. And so what we've decided to do is, after much back and forth, is to inline all of the types that we use from webdriver in our public API into our crate so that we control them. And that way remove webdriver from our public API so that we're no longer tied to them for breaking changes. And so that means copying over this error status enum that we just looked at, remove the re export that we had in our public API of that type. And internally we can keep using the webdriver, but we need to map it to our internal variant.
01:07:26.541 - 01:08:16.581, Speaker A: And there were a couple of other smaller fixes we made at the same time. So you see, we essentially copied over error status. Most of the comments are public documentation from WebDriver Spec, which is the other reason why it would have been nice to not have a copy them over, because sometimes there are updates to the documentation. Would be nice if we just got that automatically, but unfortunately not do we need to remove the comments that are not in the spec? I'm not sure what the MPL license requirements are. Ooh, I think for MPL you're allowed to reuse, but you have to mention the original project and its license and there was a couple of other things. So for Fantaccini, we. We used to have our own error enum that, you know, lifted some of the webdriver errors into nicer to access errors.
01:08:16.581 - 01:08:52.617, Speaker A: Like no such element is so common you want to access it, but instead. So we have this command error type enum in Fantaccini and we used to have one variant that was just. This is a standard one that came from webdriver and, you know, no such element is a standard error type. But we happen to just lift it into the upper enum to make it easier to access. And with this change, we're actually going to stick it back. We're going to remove the convenient access one and just have them all be under standard and it's a breaking change, which is okay in the case of Fantoni. We're already on.
01:08:52.617 - 01:09:30.615, Speaker A: I think I thought we were on an alpha. Yeah, I mean, we're going to have to do a breaking change anyway. But I'm okay with that because the hope here is that by doing this breaking change, we can stop doing breaking changes whenever webdriver changes. Oh, it's 34. Because the 34th element is selenium, which is the name of one of the primary originators of WebDriver. Nice. Okay, so where did I get last? I made a bunch of comments 21 days ago.
01:09:30.615 - 01:10:24.755, Speaker A: That's not too bad. And right, so here the challenges. It used to be really nice if you tried to click an element, then the error type that we returned just directly had a variant called no such element. Now it has a standard which internally contains an error whose kind is no such element. Now it's really annoying to detect whether the reason it errored was because the element didn't exist, which is what this comment is about. You would have to do this, which is unfortunate. And so the proposal is what if we have helper methods that just quickly check this for you without you having to do all these matches.
01:10:24.755 - 01:11:07.275, Speaker A: And it looks like I asked for that and they've added that. I don't know what these are. Okay. And that's fine. All right, let's go look at these changes. Files changed. I think there's a way for me to say changes from changes since your last review.
01:11:07.275 - 01:11:52.235, Speaker A: So get those first and a to hide annotations. What's this? Right. Our webdriver error type is. Oh, this is just a simplification we made to the constructor. Let me see if I can find that down here somewhere new. Yeah. So the error type we have can either hold a static string reference or an actual string, depending on whether you have a relatively standard or a custom error message that you have to dynamically construct.
01:11:52.235 - 01:12:37.835, Speaker A: And the way you represent that is with the COW type, which can be either a reference or an owned value. And we just said that the lifetime of that reference has to be static if it is going to store a reference. And that makes the API a little awkward because now you have to either call into or specifically bring in the COW type and use the appropriate borrowed or owned variant. And so the proposal I had was let's use simple into because both string references and capital S strings implement into cal static str. And so this means that now we just stick the into new. And it means that any call to it like the one that we saw up here no longer has to use into. It'll just do the right thing.
01:12:37.835 - 01:13:25.135, Speaker A: No such element is now gone as a variant. It comes through standard. Was is Ms. What? Why did I name it? Is Ms. Weird is detached shadow root. I'm torn here. Like, you know, this seems maybe excessive, but I guess we might as well just have one for each.
01:13:25.135 - 01:14:22.235, Speaker A: That seems fine. It'd be kind of nice if we could generate this with a macro. Right? Like, this is a lot of repeated code that's almost the same. So what I'm going to do here is actually it would be nice if we could turn all of these nearly identical definitions into a macro. Something like, let's see if I can cook up one of these on the fly. Is helper. And it's gonna take.
01:14:22.235 - 01:15:33.255, Speaker A: Here's what I'm going to do. I'm going to write a little bit and then I'm going to explain the syntax. It's going to be name. Actually we're going to do something like this maybe, or maybe that can be. It can't be a type, it can't be a path, Might be an ident, actually. And then we're going to do. So this is what we want to generate.
01:15:33.255 - 01:15:53.179, Speaker A: So here's what this macro is going to generate. It's going to. For each element. So the syntax here for the argument list is the outermost bracket. Doesn't matter. It's just a. It's just required for the syntax.
01:15:53.179 - 01:16:37.897, Speaker A: The next one is when you call this macro, I expect there to be a literal curly bracket to start this next. Ooh, this next dollar parenthesis is saying this is a group. And you see it ends with comma star, which is saying this is a group of multiple elements, zero or more, that are separated by comma. That's what that syntax means. So star is zero or more, comma is separated by comma. And I don't think I actually want that in one. And the things that there can be zero more of are ident.
01:16:37.897 - 01:17:08.843, Speaker A: So ident is any identifier. This can be things like variable names, function names, type names. So it has to be some identifier followed by fat arrow followed by some other identifier. And the syntax here is $, then name of the meta variable. So the thing that you're going to use as the substitution in the. What's this called? The transcriber. So in this case, the first one I'm going to call variant is going to be mapped to variant.
01:17:08.843 - 01:18:26.785, Speaker A: The other one's going to be mapped to name. And then what I wanted the code I want to generate, why can't I? There we go. That's what I want. And then what I want here is actually this should be name, this should be variant. And so someone asked the very important question here about can macros generate doc comments? And the answer is, it's complicated, but yes. But they can only do it in a very particular way, which is, I wonder if this will work and you're gonna hate me for this. I think it has to be like this.
01:18:26.785 - 01:19:09.627, Speaker A: And then. So here's a secret you may not know about Rust, which is that triple comments are actually attributes and they're attributes that read doc equals right, Rust reference. Let me check that. I'm not lying to you. Where is my comments.comments line. Comments begin with exactly three slashes are interpreted as special syntax for DOC attributes.
01:19:09.627 - 01:19:51.147, Speaker A: That is, they are equivalent to writing doc equals and then a string. And so here's what you can do. You can say doc equals dollar variant. And I think it's like stringify. I think it's that. I mean, okay, maybe, maybe I should actually do this somewhere else to see that I'm not completely lying to you. Do I have a temp in here? Great.
01:19:51.147 - 01:20:31.735, Speaker A: Cargo new expand, expand, CD expand. And it's going to yell at me somewhere. Who knows why yet Expected one of. Right, so this is for the repetition. I need to say star again. And here it's complaining because that has to be a semicolon. That has to be a semicolon.
01:20:31.735 - 01:21:41.665, Speaker A: Where's the semicolon have to be? And then if I now say, you know, enum error status or I guess I'll have to do, you know, enum command error and it's just kind of a standard which is going to be an error status. An enum error status is going to be foo and bar. And then what I would like to be able to do, right, is I would like to be able to do em simple command error and I want to do like is helper. And in fact, in fact I'm going to remove this extra one because it's not technically needed because you can use any type of bracket for invoking a macro. And I'm going to say, you know, is foo or Sorry, foo is going to map to is foo. Bar is going to map to is bar. That's what I would like to happen.
01:21:41.665 - 01:22:17.799, Speaker A: And I think it doesn't allow trailing commas. That's fine. We can do that by just saying question mark is 0 or 1. So I'm saying 01 comma at the end. And so we're actually going to do this errors error status. And it's still mad at me. It's going to.
01:22:17.799 - 01:23:19.273, Speaker A: It's going to auto format once I fix whatever it's complaining about here. Is it a tuple or struct variant? Fine, Stan. This is like a web driver error and so there's a struct web driver. The only reason I'm typing these out is I want to make sure that I can just copy paste the macro at the end and it'll generate the appropriate setup equals cannot be path for error status. That's fine. This is going to derive partial e and e and remove the semicolon. Consider removing this semicolon and it's going to generate pub.
01:23:19.273 - 01:23:59.255, Speaker A: That's fine. We're going to just make all of these be pub so that it doesn't yell at me. Unmapped, unmatched. End of that. Where did I mess up somewhere. Not quite sure why it's complaining about this. So If I now run cargo expand, let's see what it generates.
01:23:59.255 - 01:24:38.605, Speaker A: This feels like it generated correctly because I don't. It's just the rust analyzer being confused. You see, this is the expansion of our macro. So that worked. Now one question is going to be, does this actually, you know, generate the right thing? Right. Because I'm not sure whether when we parse this markdown, it's going to end up adding like white space or something. So if I run cargo duck.
01:24:38.605 - 01:25:03.815, Speaker A: No depths open. Nice. So if I go to command error. Yes. You see, it adds a. It adds a white space here, which means that the link isn't going to be valid. It adds a white space before and after.
01:25:03.815 - 01:25:43.745, Speaker A: Yeah. So I think we can do concat. I think we can do this. And in fact, yeah, technically we could do this. It's nice to keep it relatively self contained. I have to put the dot in there because otherwise the next line is going to be. This is going to be a space before the period.
01:25:43.745 - 01:26:49.855, Speaker A: But now if I do this, what do we get? Commander? And that links appropriately to the right variant. Nice. So that means this macro is, to borrow a great term, the shit. So let's go ahead and do this and then do. You can then replace all of the methods with something like this. Preview. Nice starter review.
01:26:49.855 - 01:27:08.645, Speaker A: Yeah. So this is a good example of the kind of things that I really like macros for. It's just like it makes anything that looks really like a repeated pattern. It can make it so much more concise. Right. Like reading this. This is much nicer.
01:27:08.645 - 01:27:49.475, Speaker A: Okay. And then these we can all move past. Seems fine. Yeah. So this is a thing I asked for, which is for all of these error status variants, there's a. In the standard, there's like a string description for each one that you're supposed to include whenever you trigger it. And because it's statically known, it's just good practice to actually return it as a static string.
01:27:49.475 - 01:28:20.631, Speaker A: Because who knows, it might be useful to someone. When you implement error, you get to define a description, Right. And usually that's through the display trait. The problem with display is it doesn't give you a static string. Or rather it's not a problem. There's error description used to require this and it was really annoying. But so here what we're doing is we have a description that someone can use to get the static string if they really want to, and then display just writes out that description.
01:28:20.631 - 01:28:56.393, Speaker A: So we implement display. But if you really want the static string, you have a way to do it and that way when we serialize, we don't have to convert it into a string and then serialize a string, we can serialize the static string instead and save an allocation and this into cal. We already talked about it seems fine. This now becomes nicer because it can just use the helper. This becomes nicer because it can use the helper. Can use the helper. Can use the helper.
01:28:56.393 - 01:29:53.795, Speaker A: I'm guessing there are a bunch more of these and these have just check annotations. So the one thing I'm curious here then is was there anything else? One thing that annoys me a little bit about GitHub's UI is that like, I kind of want to see the diff like from including this one. And I also don't want these to be resolved because I want to check that I have resolved and not just they have resolved them. So, for example, I asked to derive hash and E. I'm guessing that's done in this commitment. Right. So this removes the sort of hoisted variants and stick just leaves them in standard.
01:29:53.795 - 01:30:33.449, Speaker A: That's fine. That goes away in the next commit. Like we saw, this is an internal converter that turns a. Actually, this shouldn't be necessary anymore now because I guess it just wraps in standard. That's fine. Those are no longer hoisted. This now derives eek and hash, which is fine because error status, you know, who knows where people might want to stick them? They can't be ordered in a meaningful sense.
01:30:33.449 - 01:30:58.785, Speaker A: But by having them implement eek and hash, you could, for example, store them in a hash set. Might be useful. So that one is indeed resolved. This is. It's weird that it's called an error code, but returns a string and it is indeed. That's now been turned into description like we saw, and an implementation of display. So that one is indeed solved.
01:30:58.785 - 01:31:51.187, Speaker A: Yeah, so this was another one. It used to be that there was an implementation of from the error status type in the webdriver crate for our implementation, our copied version of error status. And that's not okay because this impl is a part of the public API. So someone could in theory be taking a dependency of Fantaccini and a dependency on WebDriver and relying on this from implementation working for the particular major version of WebDriver that they are using, which would break if Fontaccini ever upgraded its version of webdriver because the from trait would be implemented from a different version of the error status type. So I think ultimately that was removed, I hope. Yep, that was taken out. That's Great.
01:31:51.187 - 01:32:17.077, Speaker A: What else do we got here? This is the ability to parse an error status from a string. So it's sort of the. It's the inverse of turning it to a string. And I just said that should be basically a parse. You know, there's an argument for. It's a little annoying that we have to define. We basically keep the need to keep these strings in two places.
01:32:17.077 - 01:32:45.473, Speaker A: It means they have to be kept in sync. It'd be nice if we didn't have to do that. There are macros that let you do this sort of two way conversion. We could write a trivial macro to do it ourself. Right. To generate both the description and the parse in one go. So actually let me.
01:32:45.473 - 01:33:07.545, Speaker A: I think that's what I want to do. I think I want to go here. Look at the files changed. Look at this is in source error. And over here, get rid of the annotations. Said get rid of the annotations. There we go.
01:33:07.545 - 01:34:29.825, Speaker A: But here too, it might be nice to not have these string literals written out two in two places in the code here and in fromster. Maybe worth turning them into. Maybe worth turning them into constants. Or writing a little helper macro to generate both this method and the from and the imp simple fromster at the same time. Great. What's this one? The message. Okay, so where does that come from? So the difference between try from and from is that try from can fail.
01:34:29.825 - 01:35:14.925, Speaker A: It can like return an error if the conversion didn't succeed. This gets to now store static. That's nice. If the thing is static, this was an unrelated thing that got resolved without a change. And there's a test somewhere further down here that now checks no such element. No such alert. Yeah, these we all looked at and this one is a stale element reference.
01:35:14.925 - 01:36:11.585, Speaker A: So I think this just used to be wrong. Interesting. Okay, so at that point I think what I want to do is actually. So these all look like they've resolved. So I'm torn here because, you know, neither of these are really blockers. I've left two comments that are more suggestions for improving the niceness of the code, but I wouldn't consider either a blocker. If you want me to merge this as is, I'm happy to do that as well.
01:36:11.585 - 01:36:38.405, Speaker A: Looks great. Now positivity always inject some of that. People are helping with your software. Great. Submit review done. Okay, well we've got. We've gotten through a whole three things.
01:36:38.405 - 01:37:39.265, Speaker A: Okay. Native sassle support. Okay, so this is for a different crate entirely. This is for the IMAP crate. This is why I need to give away ownership of some of these crates soon. Let's see. Yeah, so this is for the IMAP crate and it's specifically.
01:37:39.265 - 01:38:22.935, Speaker A: The IMAP protocol has multiple ways you can authenticate with a server, the most obvious one being, you know, you give a username and password. But there are other ways to do it and there's in fact a. A protocol authentication standard for how you can have different authentication schemes and that's known as sasl. And I forget what SASL stands for. It's like something SASL Simple Authentication and Security Layer. Sorry for the bright screen. And the way that the IMAP crate has traditionally done this is it has an authenticator trait.
01:38:22.935 - 01:38:38.665, Speaker A: So if we go look at. Yeah, Map crate, which has an alpha and has been. Has an Alpha for. For 3.0 for a while now, partially because of things like this. Like I want to land that because it's worth using. So the authentic.
01:38:38.665 - 01:39:14.055, Speaker A: The authenticator trait, all it really requires that you implement is a process method that takes a challenge from the server and it returns a response that can be written back to the server. So it's very straightforward. But authenticate wasn't really written to specifically work with, you know, sassle. Like it doesn't. It's a very manual thing to have to implement right bytes to bytes, the current authenticator system, most of SASL can be implemented. There are a few advantages to using an external crate. So the idea here, I assume, is that there's a.
01:39:14.055 - 01:40:10.685, Speaker A: There's a crate that specifically does sassel through, I guess a library that does it for you that supports, you know, whatever, multiple different authentication standards. Rather than requiring people to implement this authenticate traitor themselves, which, you know, can be really complicated for some of these authentic authentication standards, they're not always trivial to implement internally. Feature like sasuke Security layers and channel bindings aren't really doable with the current approach. I don't know what a channel binding is. Sorry again for bright screen. I don't think you can turn the RFCs dark. Interesting.
01:40:10.685 - 01:41:07.205, Speaker A: Oh, I see. So here the idea is that you can have the TLS session be the thing that authenticates you. And so that way you don't actually have to do any authenticate, but you do need to tell the server that you wanted to do that. Interesting. Let's see what the diff they propose. So basically what this PR is asking is like, hey, I want to make a change that's sort of like this. Would you be okay with this kind of A change basically tying IMAP to this particular Sass crate rather than the current authenticator API.
01:41:07.205 - 01:42:08.119, Speaker A: So the first problem, of course, is that this is a git dependency, which we can't have if we're going to publish to create IO, which is fine. I'm guessing that's just because they're doing development and so they wanted something easy to bind to. So this allows you to create a SASL config, right? So you create a client builder, you construct a sassel config with whatever stuff and you call client authenticate and you give in. A SAS will config in a mechanism. Yeah, so instead of calling dot login, so if you look back at the API, IMAP has a client and a session. So a client is just. You're connected to an IMAP server and in general you're going to call login in order to turn that just connection into an authenticated session that lets you access email.
01:42:08.119 - 01:42:55.205, Speaker A: So you see it returns a session if the login is successful. Or you can authenticate using an authenticator. And here the proposal is that authenticate, instead of taking this or, you know, our own type here, it takes a SASL config. My first thinking here is these don't have to be mutually exclusive, right? One option here is that we could have a sassel that takes the sasl config if you want to authenticate with sasol, but if you don't, if you just want a relatively simple challenge response scheme, for example, or for testing, actually this might come in super useful. You can use an authenticator instead. And once you authenticate, you have a session and then everything works the way it usually did. So let's see, it removes authenticator, which is really just the definition of the trait.
01:42:55.205 - 01:43:36.661, Speaker A: I think it now brings in our sassel and of course we would want to bring this in under a feature. So authenticate now takes a sassle config. Why arc sassle config and what's sassle client here? That seems weird. I don't know why this requires that. It takes an arc. Interesting. Yep.
01:43:36.661 - 01:44:30.909, Speaker A: So it passes the mechanism here, then it doesn't auth handshake using the sassel session, where previously we were like manually parsing the. The basically the binary data that comes from the server, you know, turning it into bytes by base64 decoding, sending it through the authenticate trait that we were provided, and then base64 encoding the response back to the server server. And this has to do the same thing, like it still has to do the Decode. But when it has done the decode, where does parse. Parse authenticate response? Where does that come from? Oh, I see that. That already exists. That's mostly the base 64 decode.
01:44:30.909 - 01:45:36.285, Speaker A: So what does it turn things into after it does the decode, when it has the data, It'll do authenticator step 64. Yeah, so I think this is, you know, you give it, you run the next step of the authentication process, then there might be multiple. And so here we're going to step 64 with the challenge that we just decoded. Allow it to write back to the output and then write the output back as U64. And once we're done, we tell Sasol that, okay, there's no more data coming from the server, just finish authentication. So overall this, this looks pretty reasonable to me. I, I think the.
01:45:36.285 - 01:47:50.055, Speaker A: Okay, let's, let's, let's see. Useful thing to add though. I'd want to add it behind a feature flag. I also think we could do it in an additive way that is keep the current authenticate based APE authenticate based API and then provide a sassel authenticate method called say whatever we call SASO login sassel Auth maybe the sassel feature is enabled. That does all through sassle. Authenticate may still be useful on its own for users who are working with relatively simple or experimental APIs. Mechanisms such as during testing.
01:47:50.055 - 01:49:39.747, Speaker A: One thing that surprised me when glancing over the changes was the need for arc wrapping the config. Can that be avoided? Why is that required by the SASO client API? Great comment, done. What we got next? Oh yeah, note about the git dependency. Good one. Oh, also, we'd want to make sure by the time this lands that we can take a regular craze IO dependency on our sazle rather than a git dependency, so that we can in turn probably continue to publish IMAP on creates that add interactive mode. Okay, so this is a PR to a different project, which is Roget, which we used for this is our wordle solver and someone submitted a PR that adds an interactive mode. And by interactive here I don't mean playing the game, I mean helping you play a game.
01:49:39.747 - 01:50:17.063, Speaker A: The idea here being you're on Wordle.com and it's asking you, what's your guess? You can run Rajit in interactive mode. It'll tell you what you should guess and you have to type back in what the real wordle told you. The correct and wrong answers were like basically, which ones are gray, which ones are green, which ones are yellow. Put that in and it'll tell you what thing to guess next. Basically, how can you use Rajit as a wordle cheater, if you will? And this is a surprising amount of, you know, back and forth. You're trying to figure out where's the right place to land this.
01:50:17.063 - 01:51:23.985, Speaker A: Like how should we change the API to enable this kind of interactive mode? Ultimately, I think we got pretty far. Let's see. So the ask I had was right. So one challenge here is, you know, the UI is pretty minimal here and it's complicated because, you know, it gives you a five letter word and ideally you want and this is what the person's saying in the response here. When you input the correctness of your guess that you get from wordle, you kind of want your response to align up with the output from the program that told you what letters to guess, just so it's easier to visually match the two. What I was saying in my comment was it's not obvious that you should use C for correct, M for misplaced, and W for wrong. That's not clear from if it just says colors, which is what the prompt is here.
01:51:23.985 - 01:53:07.615, Speaker A: It is true though that it's night if they line up, but they wouldn't if the prompt is as long. I think one option here is to have the this prompt appear further up. That's a good point. What if we print out these instructions further up in the output? Print out these instructions just once further up in the output like before we even print the first word to guess where we can have longer description of a longer description of the expected input without breaking the alignment. Also, colors is a little weird here given that we take CMW as input. Not G, Y, not green, yellow, gray. Oh, these were just some minor suggestions to the error output.
01:53:07.615 - 01:53:45.585, Speaker A: Oh yeah. So this is a fun little change. Let me see if I can see that in context here. Maybe line 155. So they had this. This is a really useful thing to know about. Oh, how can I see this before they force pushed here maybe.
01:53:45.585 - 01:54:56.779, Speaker A: So they had this loop up here where they're walking all the over all the characters of a string. They're trying to map each character to the appropriate like enum variant. So they're basically parsing the string one character at a time and if any of the characters is the wrong character or rather one that we don't have a mapping for, they produce an error. Then they collect all of those into a vector and then they look whether any of the things in the vector is an error. And if the thing, then they get the thing at that location in the vector and they question mark it to turn to propagate any errors. So this is basically find the first thing that is an error, get it and question mark early returns, which is why this is unreachable, to propagate that error. And then because they've done this, they now iterate through all of the things that are parsed, map them to unwrap because they know they're all okay because there's nothing here.
01:54:56.779 - 01:55:53.867, Speaker A: And then they collect it into a vector again and then they try to turn it into a five length array. So it's a really like convoluted way of saying I want to just error if there's something bad, if there's something that's not an appropriate character. And this is where it comes in really handy that you actually have the ability in Rust to collect into a result of a vector, which made that whole code makes it a lot simpler because now you can just do the same thing we did. Right? So match and produce the appropriate OK or error collect into a result vec, which has the semantics of keep collecting into a vec or keep collecting into an OK vec rather. But if you get an error from the Iterator, then get rid of the whole OKVEC and instead just return that error and then we can question mark that. And that gives us back just the vector and propagates an error and then we try into. To turn it into an array.
01:55:53.867 - 01:56:24.797, Speaker A: So it just makes it a lot more concise. So that was. This has been fixed. This has been fixed. That was my comment here, which they've done. And then what are these diffs? These are probably just rebases. I think this is all just force push.
01:56:24.797 - 01:56:45.845, Speaker A: Yeah, rebase on top of main. Let's go look at the changes. So I left the one comment here that was just, you know, about the prompt. These conflict with each other. That seems fine. If interactive. The Play Interactive.
01:56:45.845 - 01:57:29.975, Speaker A: Play interactive takes a guesser. Oh, they. They've added that. Nice. Ah, you already added instructions at the top. Nice. What is the actual.
01:57:29.975 - 01:58:16.715, Speaker A: So there's a coverage failure. I feel like I've probably broken coverage. So that seems like just a separate problem. All the others are fine. I still think colors is a little weird, but let's not block on that. Start a review, finish review, approve and merge. Thanks for sticking with this.
01:58:16.715 - 01:59:33.693, Speaker A: Excellent. Done. Update dependencies A hash and quick xml. So this is in a different crate Again, this is in Inferno, which is our port of Flame Graph to Rust that updates some dependencies, in particular a hash and quick xml. And I asked them to also update Criterion. Yeah, so for Inferno we bring in a bunch of test cases from the Flame Graph perl implementation, which means that we have a git submodule that holds all the data files for that. And so if you don't do git submodules update in it, then you don't git doesn't download the sub module for you.
01:59:33.693 - 01:59:50.893, Speaker A: And so the tests are going to fail. Should arguably add a build script or something to make that nicer. All right, so let's go look at the diff here. Cargo lock. That's fine. Cargo toml that seems fine. So that's been viewed.
01:59:50.893 - 02:00:21.705, Speaker A: Let's just mark that as viewed too. That looks like a clippy lint. That's fine. This is a quick XML change of now it just has a new instead of owned name and borrowed. It's interesting. So if we go to quick XML and we look for new. So what was this? Byte start.
02:00:21.705 - 02:00:42.875, Speaker A: Yeah, so it's now an into cow. So that would. That's the reason you don't need to separate borrowed and owned anymore. So that seems pretty reasonable. And that's probably going to change a bunch of things in the internals here. No longer lead from plain string. That seems fine.
02:00:42.875 - 02:01:17.135, Speaker A: It's all fine. Decl. Oh, nice. So we don't have to raw print these. This is now an XML declaration version one standalone. No, it's a doctype where we just keep the same doctype as before. That's fine, right? Svg this is more things with attributes.
02:01:17.135 - 02:01:37.883, Speaker A: That's fine. Change to new. Change to new. Wonder why. This is from content. Oh, this is because this has attributes as well. And so from content also parses the attributes.
02:01:37.883 - 02:01:56.085, Speaker A: So let's go back and check that I didn't mess up this anywhere else. These are all just attribute. These are all just tag names so they don't need from content. Same here. Same here. Even if they did, one of the tests would fail. So that seems reasonable.
02:01:56.085 - 02:02:26.655, Speaker A: Okay, this is byte start new stop with attributes. This iterator has just been rewrapped a little bit. Otherwise it looks fine. Same with stop. Same with this. This looks fine. Looks fine.
02:02:26.655 - 02:03:00.439, Speaker A: Oh hey, a bot that I can ban. Block user. Nice. Love spam from Escaped from. Escaped from escaped. So these all look like pretty straightforward API changes. New new.
02:03:00.439 - 02:03:28.899, Speaker A: That's all fine. Oh, this is also a clippy lint of if your pattern is a single character, then use a character Instead of a string. That's fine. And then this is all changes to the expected test output. I don't actually know what this is going to render really. It's not going to let me look at the rich diff. That's fine.
02:03:28.899 - 02:04:01.865, Speaker A: Yeah. These are all just SVG changes to the tests, which seems fine. And there are no more than that. Great. And so the next question is criterion a hash or quick XML in our public API? I'm pretty sure they're not. Also, it wouldn't really matter if we had to do a breaking change for Inferno, but it matters a little bit. Collapse has the collapse trait, which does not have any quick XML in it.
02:04:01.865 - 02:04:28.199, Speaker A: And these just implements collapse. That's all fine. All these are just options, so these aren't going to have anything in them. Differential from files just takes options from readers. Takes options. Nothing in here. Flame Graph has from files.
02:04:28.199 - 02:04:59.493, Speaker A: These are just going to be simple. This is an enum. This is an Enum. Options has lots of fields, but none of them are from a hash or quick xml. Same with this default. There are crates that help you do this. So there's the cargo public API command and the semver check API or command.
02:04:59.493 - 02:05:24.155, Speaker A: Both of these help you do this. In this case, I'm pretty sure there's nothing like. You know, it would be weird for us to explain Expose like a hash or quick XML in our public API. We do expose rgb, but that's fine. Color multiplet Basic palette. Yeah, all of this is very, very non public. Nice.
02:05:24.155 - 02:05:40.161, Speaker A: Excellent. Thank you. Approve. Well, here's a problem. I can't. I can't scroll down enough. There we go.
02:05:40.161 - 02:07:54.735, Speaker A: Approve. Great. Proven run. Hopefully that those tests are gonna pass. While we wait for that, I'm gonna take a bio break. All right, I'm back. Let's see.
02:07:54.735 - 02:08:27.705, Speaker A: Why are these still. Some of them are failing. Seems unfortunate. This is Cargo format complaints. This is feature may not be used on the stable release channel in the AITOA crate. Oh, interesting. This means.
02:08:27.705 - 02:09:37.335, Speaker A: Oh, this is from the minimal versions. Okay, so let's talk about minimal versions a little bit. One challenge here is I run one test step in most of my CI that puts all of my dependencies at the lowest version, the lowest version within the summer range that's allowed by my dependency closure. The idea here being that, you know, if I did require some newer feature, I want to make sure that that's declared in my cargo tunnel. And the reason I want to do that is the failure mode is kind of weird. If someone has Imagine Someone built my project ages ago and so they have a lock file from that build, then they do a git pull and then they try to build. If my minimum dependencies aren't declared correctly, then they're going to get a really bizarre build failure because they're using an old version of a dependency that my project doesn't work with anymore, but I haven't upped the dependency and so therefore they don't get to know.
02:09:37.335 - 02:10:38.689, Speaker A: That's why it's just nice to do that check. It's not really a requirement, but it's nice. Now what this is running into is I guess by up updating some of these dependencies, probably Quick xml, we also got a new version of ITOA and the update to minimal version step that I do is going to upgrade to the lowest version of ITOA within the sember range, which is going to be version 0.4.0. Now of course the build that we do runs with some particular version of Rust and it runs with a relatively recent version of rust. When ITOA04 came out or was building, I guess it was using a feature. It was using a nightly feature when a particular feature was enabled, which is a little weird already. But it has since been stabilized on newer versions of Rust.
02:10:38.689 - 02:11:29.315, Speaker A: So what this is complaining is first of all, feature may no longer be used on the Stable release channel, but also that feature has been stabilized so you can use it on Stable. Now there's not a great way to solve this particular problem, right? Because the ITOA dependency comes into us. Actually it might come in directly. Let's go see if that's true. Cargo Tomo we have a dependency on ITO1. Where is ITOA04 even coming from? That's also interesting. That's real curious.
02:11:29.315 - 02:12:33.065, Speaker A: I'm going to guess I have no idea why any of these would make it so that we get in such a low version of aitoa. Okay, here's what I'm gonna say for the There was something else. There was a failure down here. Trade bounds other than size on const fn parameters are unstable. This is with quick XML 00:25 that's interesting. So what is quick so for quick XML? What's its actual version? This is a thing that crates that I.O. started doing for me, which is very annoying, which is if I try to go directly to a crate it'll just load forever unless I delete my cookies for that page.
02:12:33.065 - 02:14:08.027, Speaker A: Huh. Quick XML 00:25 because this is just a build error in Quick XML on line 64 of Source Writer that's interesting Source Writer line 62 const yeah here this commitment drop const event because it basically requires to use a particularly new version of Rust. Yeah and you see it actually fails on Rust 159 and so this is another CI step that I run which is I try to make sure that the crate continues to build with a somewhat older version of Rust. There's a lot of debate in the Rust community about which versions should you support? Should you even bother trying to support old versions of Rust? I like to do it at least on a best effort basis. And so this is basically saying it fails to build on 159 and it fails to build because Quick XML basically in their 0.24 release added a const FN for a generic function which doesn't work on Rust 159. Apparently it only works on newer versions.
02:14:08.027 - 02:14:45.975, Speaker A: I'm guessing this PR will have more information. It does not, but it does add a CI test now to see that they don't regress on this. And I'm guessing there's been no release of this. Let's see. Right. As though there's no actual release for this yet. So let's go ahead and say so what we're going to do here is Looks like we have a couple of CI failures.
02:14:45.975 - 02:15:46.675, Speaker A: The minimal versions one seems to be because we somehow now pull in ITOA 0.40 which is super old. Not sure why. If you run Cargo update C minimal versions and then Cargo tree I itoa 0.4.0 you may get some pointers to what's happened. That said this I'm fine merging even if that one is red. It could be that rebasing that merging from main will fix the problem for you.
02:15:46.675 - 02:17:29.274, Speaker A: The MSRV CI step 1.15.9.0 is because of Let me link the actual issue. I guess maybe there isn't an issue is because of a MSVR regression in quick xml. It's been fixed upstream but there hasn't been a release yet. I'd actually like to hold off on merging this PR until that lands at which point we should bump the quick XML dependency to 0.25.1 and then the check stable format CI step seems to complain about a the lack of cargo format being applied to a line we changed should be an easy fix. Comment Great.
02:17:29.274 - 02:18:44.084, Speaker A: All right. I think with that I can now mark this as done and I think with that we're let's do one more do this as the last one because this was a longer discussion. Basically one of our dependencies for this is for a different crate. Every single one we've looked at has been a different crate. This is where the OpenSSH crate, which gives you a sort of programmatic API for doing SSH operations and SSH into machines and running commands there. And we have a dependency on a crate called OpenSSH Mux Client, which is, instead of interacting with SSH through the or OpenSSH through the SSH command, we actually directly implement the protocol that the SSH command uses when talking to an SSH command master control daemon that you can set up to reuse SSH connections. We do that through the OpenSSH MUX client crate.
02:18:44.084 - 02:19:49.195, Speaker A: The problem we have, and as I pointed out in here, we actually expose, and this is related to what we did with Fantaccini. We have one particular error variant in R crate that over here that you can see. So we have a, we have an error Type in the OpenSSH crate which is public, and one of its variants is an OpenSSH MUX client error that you can sort of propagate up. And this one, you know, means that the OpenSSH MUX client error type is in our public API, which means that if there's a breaking change to this crate, we have to do a breaking change to our crate. And in this case this is a breaking change to that crate. So if we did take this upgrade from Dependabot, we would have to do a new major version of OpenSSH as well. And I think the maintainer of OpenSH MuxClient, who is also a contributor to OpenSSH, didn't realize that this was the case and so did this breaking change without quite realizing the implications.
02:19:49.195 - 02:20:37.595, Speaker A: And so, yeah, so this is one of the tools, semver checks that tries to detect whether something is a breaking change. And in this case, I guess just it missed the fact that this wasn't a breaking change. And so things got sad. So ultimately they released. It turned out that this particular change they made was actually not a breaking change to OpenSSH MUX client. So they released a new minor version of that rather than the new major version which we then just bring in automatically due to the release of SSH format 0.13. The next release might have to be 01611, which would be a breaking change.
02:20:37.595 - 02:21:45.309, Speaker A: They're going to try to group together a bunch of different breaking changes. Once that's a good idea. Okay, I move all the AIR types into separate crates, then mark them as non exhaustive. Will this fix the problem? That's interesting. So the proposal here is if we Made the Error Types of OpenSSH MUX client be in a separate crate. So a crate called like openssh muxclient errors I guess then, and we make the error enum variant or the error enum in that crate be non exhaustive so that we can add new variants to it over time without it being them, without adding the variance being a breaking change. Then our crate could take a dependency on that crate and now the OpenSSH MUX client crate could keep doing breaking changes and it's no longer part of our public API and there wouldn't be breaking changes to the error crate and therefore we wouldn't need to do breaking change.
02:21:45.309 - 02:22:30.313, Speaker A: So that's an interesting proposal. It's a little weird to have a crate just for your errors, but I also sort of understand the attraction. We basically have. We have four options here. One is to do this, have a separate crate. The other is to try to remove the crate entirely from our public API, which would mean basically not exposing those errors. Like basically you could imagine, you know, that we could turn them into a, you know, a string or something.
02:22:30.313 - 02:23:53.045, Speaker A: There's a variant of that. So the third option we have is to expose that error as a like a box didn't error, which would mean that we're no longer tied to it and people would consumers of the open would have to like downcast them into the appropriate OpenSSH MUX client error type. And that would work like it would. It would mean that it's not technically a breaking change for us to bump OpenSSHmux client, but at the same time it would break our users nonetheless because the downcast would start failing for them if there was a breaking change, because the downcast would be to a different type, because it would be a different version of that type. And then the last potential solution here is we could inline the error type, basically keep a copy of that error type in our crate, and then basically do the same thing as we did for Fontacini with WebDriver. It's not very attractive here, especially because, you know, it means we have to keep up to date with the changes to that error type. And I don't love the idea of having to keep a separate copy.
02:23:53.045 - 02:26:26.415, Speaker A: I actually think I kind of like the, you know, keep errors in a separate crate proposal here. It's a little more cumbersome for the people who maintain that crate, but not really for us. I guess if they're in a workspace, it's not that bad. Yeah, it's a bit more inconvenient for you in maintaining OpenSSH MUX client, but it would solve the problem as long as it would solve the problem as long as you don't have to make breakfast making changes to that error crate. Keep in mind though, and this is an interesting point, keep in mind though that if the error if the errors in that crate internally contain errors from somewhere else like other crate such as SSH format and the error crate needs to update its dependency on SSH format to propagate new errors, then that would still be a breaking change unless the new version of SSH format's errors go into a new variant and both the old and new versions stay around as dependencies. I don't know what kind of dependencies you're taking in. Your errors though may not be a problem in practice.
02:26:26.415 - 02:28:29.465, Speaker A: It would certainly reduce the likelihood that breaking changes in OpenSSH MUX client would require a breaking change in OpenSSH. Another option here is to type erase the errors and store them as boxed in error plus send plus sync static. Those extra bounds is because we want them to be sentenced sync. In general you want your errors to be and you want them to be static so that they support downcasting type raise the errors and store them as that which would technically allow us to get away with not doing a major version bump for open OpenSSL when OpenSSL MUX client does one although in practice it could still cause problems for user for OpenSSH users who rely on downcasting. Although how many of those are there really cause hard to debug problems Comment oh yeah, someone made the point in chat and this isn't about this specifically but in general try to keep your PRs somewhat targeted because it makes it a lot easier to maintain. Otherwise you end up with like can you fix this thing and also this thing and also this thing that there's a lot more churn in the pr. I would rather have many small PR so that I can land independently.
02:28:29.465 - 02:29:49.073, Speaker A: Someone made the point in chat here that this is an they write this is an odd symptom of the weirdness of Rust's error handling story. All the options are kind of unsatisfactory and I I don't know whether it's about Rust right? Like I think this is more about the implications of semantic versioning right? Like if the if the only toggle you have is either the whole crate had a breaking change or the chase or the changes entire backwards compatible you can't do things like, you know, individually version your types. If you could then this problem would sort of go away but in reality what's happening is here. This is more about strict typing, where if you really looked at the types, they did change, this is a different type. And if you want the compiler to do type checking for you, you wanted to warn you about these kinds of things at compile time. Because if the type is different, the semantics may have differed or different fields might be available. So like, I don't buy that this is like an unnecessary problem.
02:29:49.073 - 02:30:38.205, Speaker A: I think it's just a hard problem. And one way you can get away with it is sort of you can, you can do duck typing here, right? Of saying, which is basically what the box didn't solution does, of saying, all we're going to store here is this is some kind of error and not the concrete type. And then you give access to things like, you know, it can. You can print it. The way you get at this with things like, you know, in Ruby, for example, you can throw an exception and you know, there aren't really different versions of a dependency. If something has roughly the same shape, users are going to keep being able to use it as long as they only access fields that exist in both the old and the new version of that shape. It doesn't matter that the type is actually the same.
02:30:38.205 - 02:31:50.755, Speaker A: But in Rust that's not true because we have strict typing. And the error context API might actually help a little bit here because it makes it more defensible to use something like a boxed in error. The context API is basically a way to say on this type, give me a field that has of the type U64, right? Or of the type I don't know, you know, of the type, some type, and you can't use strings with it. I don't think you can't use like keys, it's only by type. But that would mean that it might not matter whether you can downcast the error into a specific known concrete error type, but rather can I access this kind of context through that error? And so that means basically you're the users of your library are less likely to rely on. Can I downcast this into this version of this concrete error type? And more likely to just rely on. Does the opaque error that you gave me, does it have these properties that I care about? But it's still not perfect? It's definitely not.
02:31:50.755 - 02:32:35.095, Speaker A: Yeah. So you have the semver hack for this, where instead of having a separate crate for error, you have the old version of the crate, take a dependency on the new version of the crate and re export its error type. And that way the types are actually from the same version and things just continue working. It would have the same property here. I don't know if it's better, right, to keep it in a separate crate. I guess we could. We could December hack Semver trick is what it's called.
02:32:35.095 - 02:33:58.235, Speaker A: Oops, that's not what I meant. You could also use the SE trick specifically for your error type when you do a new major release and it have the same effect as as having a separate error crate, although it makes it harder for users to realize that they can rely on the backwards compatibility of the error type and only the error type across major versions. Versions. Great. Done. All right, I think we're going to stop there. We got through one page and there's another page but it's getting late and I need dinner.
02:33:58.235 - 02:34:20.491, Speaker A: But thanks for joining. I'll see when I do another one of these. You know my notifications keep piling up. I have two new notifications since we started the stream, so like there's always more work of this and who knows, the next one will probably be impromptu as well. Hopefully that was useful. We went through so many different repositories. But thank you all.
02:34:20.491 - 02:34:24.755, Speaker A: Have a great rest of your day or night depending on where you are and I'll see you some other time.
