00:00:01.320 - 00:01:08.295, Speaker A: Hello folks, welcome back to yet another stream. This time it's sort of another iteration of the open source maintenance streams. I've done a couple of these in the past and basically every time it is mostly out of necessity, right? So I maintain a bunch of open source crates, libraries, applications, all sorts of things. None of them are like, you know, the used by most of the ecosystem, nothing like that. It's just this is a bunch of them and everything like this ends up just requiring a bunch of ongoing maintenance, whether that's issues or PRs or you know, feature requests, whatever it might be, support. And in addition, you know, I make contributions to other open source projects that need to be followed up on. And you know, whenever I take a vacation, if I fall ill or I just have a bunch else to do for some period of time, I end up falling behind on my GitHub notifications of things that I kind of need to deal with because there's no one else that pick up that thing.
00:01:08.295 - 00:01:49.099, Speaker A: And when that happens, you know, I need to spend some time catching up on those things. And I figured, you know, why not turn on the camera for it and show people what that process is like. And so that's what these streams are. They're not intended to be a sort of learning resource in the same way that many of my other Rust streams are. It's not like I'm going to teach you a topic or teach you this particular software. Rather it's sort of a day in the life kind of thing, if you will, of let me just show you what the process is like going through, like working in open source and hopefully you learn a bunch of bits here and there. Like most of the projects are in Rust.
00:01:49.099 - 00:02:29.867, Speaker A: And so, you know, especially when reviewing PRs, I'll generally talk you through. I would do this differently or this doesn't quite work or this is a backwards compatibility hazard or whatever it might be. And similarly you might learn things about project structure or even just projects that exist or how to make contributions, whatever it might be, or just to get some insight into what this all looks like from the sort of other side of open source. So we're just going to dig into my GitHub notifications. We're going to start from the back and make our way forward and get however far we can. I mean, I've allocated about five hours for this. I don't think we're going to catch up in five hours.
00:02:29.867 - 00:02:54.095, Speaker A: I think in five hours we're going to get, you know, maybe a third of the way through and this is the bane of my existence. No, no, I. I enjoy it. I think it's fun. It's just it. It does end up taking a surprising amount of time, especially once you start falling behind. Just before we kick into it, I just wanted to highlight again that I do have a discord where I announce these streams.
00:02:54.095 - 00:03:27.183, Speaker A: You can, if you, if you want an invite to it, it is just John who? No, it is not. It is discord. John who Eu. If you go there, it will take you directly to the invite. I'll put that in chat as well so that you have it and you can just join there. The only channel you'll normally see is the announcements channel and the YouTube channel that automatically posts whenever there's a new YouTube video. This is also, you know, I announce if I'm giving talks, if I'm not doing a stream that I'd originally planned or if I'm doing sort of ad hoc streams like this one.
00:03:27.183 - 00:04:07.261, Speaker A: There are channels too, but those channels are specifically for people who Sponsor me on GitHub sponsors or Patreon or whatever. But I figured I'd get that out of the way just as an additional way for where you can figure out when I do a stream next. Okay, let's dig into it. So GitHub notifications, I have 158 of them. This is after I've already pruned a few that are just notifications of releases or like rust issues that I follow. I'm sure there are more that have been added to list since I started tagging some of them off. But like rust issues that I'm tracking but I'm not actively doing anything with.
00:04:07.261 - 00:04:51.827, Speaker A: There are a bunch of dependable things in there. So you know, realistically it's not actually 158 like action items. We're going to start from the back. We're not going to start all the way from the back just because some of the things on the last page here are things that are a little bit more like either long term or I have to think harder about them. And I've said this for a while, like if you look back to some of the older open source streams, you'll see that some of these are recurring from there and I just haven't spent the time. But like some of them are simply just non trivial like bump to hyper one for example. This is a breaking change I need to issue to Fantoni and I actually maybe we could do that one because it's mostly done.
00:04:51.827 - 00:05:27.175, Speaker A: It's just about doing a release. Sure. Why Not. Why not? Actually? Okay, so Fontaccini is a. It is a rust crate that allows you for interacting with web browsers through the Web Driver API. So the WebDriver API is a way to do sort of browser automation or browser orchestration. It's supported by a bunch of different browsers through these little proxy processes.
00:05:27.175 - 00:06:02.571, Speaker A: And so for Firefox there's something called Gecko driver, for Chrome there's something called Chrome Driver. I think there's something equivalent for Safari. Internet Explorer supports the standard as well. And Fontuccini is not intended to be a sort of, you know, end user API. Arguably the description here is wrong. It is actually a very, it's an API that very closely wraps webdriver. It's not a, it's not a super high level API in the sense of, you know, giving you the sort of abstract way of expressing chained operations.
00:06:02.571 - 00:06:58.805, Speaker A: It directly maps to the WebDriver API. And the idea was that either you can use this directly or someone else writes a crate around it that, you know, gave some more ergonomic ways to access it. I wrote Fantaccini many, many years ago, I guess seven years ago is when I started the project. And one of the changes that sort of happened since then, relatively recently, was this sort of move to hyper 1.0 in the ecosystem. And Fantoccini of course, interacts with the WebDriver proxy processes over HTTP, which means it uses hyper and requests and whatnot, and also relays some of that to the user of the library. So for example, it gives you a way to get a HTTP request that has all the cookies that the browser session has for a given domain so that you can fetch it directly without going through the browser.
00:06:58.805 - 00:07:39.855, Speaker A: There's a bunch of that sort of stuff. So it ends up integrating fairly closely with some of the Hyper stuff. So that means that the upgrade to Hyper 1 was not entirely trivial. If we look at the, the delta here, you know, obviously a bunch of it is just version bumps, but some of the things that are painful, which you'll know if you've gone through the Hyper 1.0 upgrade yourself, is the changes to the body type. So the body type used to be a struct and now it's a trait. And that means that anything that uses like hyperbody for example, now needs to change to use a concrete type that implements the trait instead.
00:07:39.855 - 00:08:45.753, Speaker A: And there are a bunch of helpers for this. I'm not going to go through the entire change here because it's one that I've already done, but it's more like this is definitely a breaking change. There's a bunch of public APIs that either name the hyper error types or the hyper like body types. So this type, this method here, for example, with raw client for this is the thing that gives you a hyper client that has the cookies for a given domain that, you know, gives you back a hyper response. So obviously if we bump the major version of Hyper, then we've changed which type is shown in this API and therefore it's a breaking change. And then there are various other things here as well, but most of them have to do with just there's a breaking change because of the major update in the dependency and then a couple of minor mostly changes to the API. You'll see here that there are some places where we name like legacy types out of some compatibility layers that Hypr has released to make it easy to easier to move from the previous version to 1.0.
00:08:45.753 - 00:09:34.315, Speaker A: Those are generally not in public typeso. Like here, for example, you see, we use the response future type from hyper util legacy, but that is in a private enum, and so therefore it's not a part of the breaking change. And the reason I point this out is because you have to be really careful about what types you put in public APIs. If a public API or public type, or public enum, whatever it might be, holds a type from a given crate, then a new major version of that crate, or even just changing what that type is to something that say, not in the legacy module, would itself be a breaking change. Which means that if we do a breaking Change now for Hyper 1, and then down the line we want to change this type to not be a legacy type. If this was a public type, that would itself be a breaking change to Fontacini. And so we wouldn't really want to set ourselves up for having to do another one shortly in the future.
00:09:34.315 - 00:10:02.355, Speaker A: I'm trying to see if there's anything else here that's really worth talking about. Most of this is just hyper 1.0 stuff. This was a change to. That's not particularly important. This is just a change to the testing for how you need to start a server. So that's not terribly interesting.
00:10:02.355 - 00:10:33.091, Speaker A: Okay, great. So this is not too bad. Now, one of the reasons why I didn't merge this after I finished it, well, there were a couple of things actually. So one of them is if we look here at this. So this is the error module. I'll make this a little larger, might be helpful. So the error module, for funtoccini it defines a command error, which is sort of the error we use more or less everywhere in Fontoccini.
00:10:33.091 - 00:11:24.795, Speaker A: It's an enum that has a bunch of sort of sub errors. Like this is an error that came from webdriver, this is an error that came from a parser, this is an error that came from Hyper, and unfortunately this required that I added a and a variant to that error enum that surfaces errors out of the hyperutil legacy crate. Now, I'm not too worried about adding this here because if in the future we're able to represent this with real hyper types, we do have the hyper error type in here. So we would just stop producing this particular variant. So it's not like it basically means we're not forced to make a breaking change. It does mean, however, that we can never remove the dependency on this particular version of hyperutil. And that is, I think, pretty unfortunate.
00:11:24.795 - 00:11:46.435, Speaker A: But I don't have a great way around it. Like there's basically a bunch of stuff that was removed from Hyperin 1.0, but that we need in order to have a sort of ergonomic client. I could maybe move us to request. I don't know that that's really better here. So I'm inclined to just keep this the way it is. So that's one of the reasons why I've sort of been hesitant to merge this.
00:11:46.435 - 00:12:55.311, Speaker A: The other is we have a bunch of pending changes in Fontochini. So if you look at Fontaccini, if you look at the version number here, you see this bumps this from version 0:20 rc7 to 0:20 rc8. And this is all legal, right? So you're allowed to do breaking changes between pre release versions. But the argument I was thinking of was, well, that's true, but there are a bunch of people using RC7 because it has a bunch of other, you know, useful breaking changes that people might want. And so if I Cut this as RC8, it just feels kind of weird to have this hyper one change, which is a very big change, right? If you're a user of Fontuccini, then you also have to upgrade to Hyper one. And that might be a massive undertaking for you, but pinning it, you having to pin to RC7 feels very arbitrary. And so what I proposed here was that we do a release, like not an RC release, but an actual release of version 0.20
00:12:55.311 - 00:13:29.675, Speaker A: of Fontaccini, just a full release, no RC. And then we start this hyper one move as 00:21:0. And I think we don't even really need the RC here. You know, this is still pre 1.0 for Fontaccini itself, even though it's seven years down the line. So I may be okay just cutting a 021 here, but it's more. I think we should maybe release the current version of Fontaccini as a non RC and then do a major version bump, right, as in 0:20 to 0:21 for the hyper one change.
00:13:29.675 - 00:14:20.255, Speaker A: And so I propose that to one of the authors of a crate that traditionally used Fontuccino under the hood. So this was a crate called 35, which is sort of a higher level API for browser orchestration. And they've made a bunch of contributions to Fontaccini in the past. And in this case I think they were on board with this probably being a good idea, but they've also started to no longer depend on Fontaccini and so therefore it's not really relevant to them anymore. And so that meant there wasn't really a push for me to make this release. But I figure now we might as well just do it, especially because it's pretty straightforward. So if I now go to Fontaccini, I do a git pull and I guess it's worth checking in here.
00:14:20.255 - 00:15:13.385, Speaker A: If we have other things that we want to land in 00:20 before we cut 00:21, let's go ahead and look at these other ones. Okay. Someone has introduced spell checking. I mean, that's nice and all. I don't really want to also run whatever this typos things is, but I'm happy to take it without these. So I'm going to do this. And the reason is just I don't love having lots of CI jobs that just run like random scripts.
00:15:13.385 - 00:16:02.305, Speaker A: So I'm going to go ahead and do that and I'm going to go ahead and do that. In fact, I can just commit directly to their branch. I'll do this too, not add another CI step though. So I'll approve that and then commit this as well. And so now I'll make this a little smaller. Now if we look at files change, it's just the typos. So now we can go ahead and merge this.
00:16:02.305 - 00:16:30.353, Speaker A: And because this is just typo checks, I'm just going to squash it. Fix a bunch of typos. Confirm squash. Great. So that's one. Oh, this is a URL fix. That seems fine to me.
00:16:30.353 - 00:17:11.925, Speaker A: Let me just double check that. That is actually a valid URL for what we're talking about. Perfect. Okay, great. And this one too. I don't really need Cion. It's just a URL fix, squash and merge, confirm and then this is raw request builder struct to handle raw HTTP requests with optional cookie handling.
00:17:11.925 - 00:18:27.113, Speaker A: Interesting. Okay, so this is just a way to construct a RAW request, but that doesn't really fix this issue. So the issue here is that webdriver has a long standing bug. Basically that's this one over here that you can tell webdriver to get all the cookies from the browser, but you can only do so for the current domain. So the browser needs to be on a website of a given domain and then get all cookies, will return all the cookies for that domain. You can't request cookies for a domain that the browser is not currently on. And that's really awkward if I want to start my own HTTP request that uses the same cookies as the browser.
00:18:27.113 - 00:20:09.261, Speaker A: And so currently the way Fontaine does this is it tells the browser to go to like a random non existent URL on the domain and then grab the cookies and then go back to where it was, which is obviously kind of stupid, but it's the only thing you can do that the spec supports this pr, adds a feature to implement it, adds a sort of request builder for building requests for a given URL. I don't know that this really helps. It just feels like it's reimplementing some of hyper because what I'll show you. So what Fontaccini currently has, if we go to RC7 is when you have a client you can call where is it Raw? Not raw, raw client 4 or with raw client for which you give a URL and you give a method and then you give a closure that is given a hyper or an HTTP request builder and that gives you back the response. And so this way you can issue arbitrary requests and we will sort of map in the required bits for getting the cookies and stuff for it. This I guess is sort of a generic request builder type, which I don't think really adds anything over that API because all it allows you to do is set the method, set the URL. I guess you can set the cookie URL.
00:20:09.261 - 00:20:56.739, Speaker A: So one of the complaints here is that we hit this sort of arbitrary URL that we construct and there's no way to configure what that URL should be. And so this would let you configure what the cookie getting URL should be, I guess, but I don't really love that. I guess that's this issue. Oh well, I'm the one who proposed this in the first place. Great. Okay, so this is what I was thinking, that what you really want is just a way to construct, to configure where Fontoccini goes in order to get that cookie stuff. And I guess someone did actually propose doing that with an additional method.
00:20:56.739 - 00:22:12.365, Speaker A: And then I said, well, what if we just do a builder instead? Interesting. Okay, let me look at this again. So RAW request lets you set the method, the URL map request. So map request is. Because our current like with RAW thing allows you to set a closure that gets called on the HTTP request before it's actually issued. And so this is to preserve that functionality, I suppose, and then send gives you back the hyper response. And so this is something we're going to have to port to Hyper 1 as well.
00:22:12.365 - 00:22:52.695, Speaker A: What's awkward to me here is I would like the current code that we have for RAW requests to reuse this, because otherwise it feels like we're just doing this twice, right? Where we're implementing all this logic that's in send here. We're also implementing that over in. Over here, in. With RAW Client four. Right. That also does this extra code. And so I would like to avoid duplicating that in here.
00:22:52.695 - 00:24:12.043, Speaker A: And in fact, if we go look at the current code for actually, where did that live? It just lives in Source client. Right. So the code that they've written over here basically replicates this code that we have over here, although it doesn't even fully replicate it, it writes it differently, which also makes me sad. So I would want us to deduplicate this. The question now, and this is sort of an eternal issue for things like this, is do you tell the author of the pr, hey, please change your thing, or do you just merge it and then change it yourself? Now, given that this was opened two weeks ago, I'm sort of inclined to ask the person to do this themselves. I think what I'll do here is I will. I'll merge the bump to hyper one.
00:24:12.043 - 00:24:29.361, Speaker A: I'll do a release of 00:20 without that change. Then I will do a. Then I will merge the bump to hyper one. Then I will leave a comment on this, Mr. PR. I mean, merge request, pull request, whatever. I'll leave two comments here.
00:24:29.361 - 00:24:55.725, Speaker A: One, please make this. Please make the with request stuff. Reuse this stuff rather than having the implementation twice. And two, now that we've moved to Hyper 1, could you please upgrade to Hyper 1? So I think I'll do the. Make those two notes for them to update. If this had been like two years ago, then there's no way that person would still be around and still be motivated to do that. But two weeks is not that long.
00:24:55.725 - 00:25:39.461, Speaker A: So I think that's what we're going to do here. So that means the next steps are we're going to pull here, we're going to do a. Here's what we're going to do. We're doing a cargo update. Actually, we're going to do a prep 020 cargo update. We're going to do a change to cargo toml actually, I want to do a cargo outdated slash R as well. I like to do this before I do a release to see whether there are any dependencies that I can upgrade that are not part of the public API.
00:25:39.461 - 00:27:01.347, Speaker A: Like hyper obviously is part of the public API, but base 64, for example, is not. And so for base 64 we could move this to 00:22 and by moving it to 00:22, people who are stuck on Fantoni 00:20 would still have, you know, dependencies that are as up to date as I can give them. And in fact it looks like there were no breaking changes in base 64 00:22, which is interesting. Let's see what actually changed here. Repository release notes 00:22 okay, this seems like nothing that breaks, that changes us bump base 64 major and then we're going to mark this as actually 020 minor fantasy. Oh my Lord. And then we do a dis so that we get the cargo lock to be happy and we push that out.
00:27:01.347 - 00:27:59.012, Speaker A: And so now if I go over here and make that a release bump and release RC7 as 0200 great pull request. And then I will also put in the description here, I suppose prior to Hyper 1 bump merging in which will be a bigger major change, bigger breaking change. And then we'll. We'll let CI run and then I'll merge this. I'll tag it as a release. Then we will merge the hyper one change, which should rebase just fine on top of this or merge fine on top of this. And then we tag that and then the question is should that be an RC or should that be sort of beta, or should that be an actual 0.21
00:27:59.012 - 00:29:32.685, Speaker A: release? I'm inclined to just make it a 021 release. If something is super broken, then, you know, it doesn't hurt too much if we do another making major release right after. So I'll let that CI run in the background and I guess I'll prep the review here, which is just here. Let's do. Any particular reason why the code here diverges so much from how with raw client 4 I think end result is the same. So that's just an observation I had separately and then I want to see I guess up here now that we have this, I'd like RAW client for end with raw client 4 to both use this so that we don't duplicate the logic. And then I'm also not super happy about the docs here.
00:29:32.685 - 00:31:26.945, Speaker A: If you look at the docs for the rest of Fontaine, they're actually quite extensive. Then in particular if we look at the something like with RAW client for I guess actually they're repeating what we already have to see a bit more detail here. Raw request actually means something like I like to give a proposal so they have something to work from. Sometimes just cop they end up just copy pasting my example. But even so I just want to give some like guidance of what I'm after. Otherwise it can be hard to take, you know, the idea I have in my head of what the doc should look like for them to write something that matches that without something to go on. Creates a Build a direct request to the to a remote site without going through the WebDriver inst WebDriver host while preserving cookies from the web driver.
00:31:26.945 - 00:33:04.015, Speaker A: This is connection Would be good to give some more details here about what this is and why it needs to be there. Links to this and that probably also be good for added context. Otherwise developers not already aware of this limitation, this web driver limitation, uh, won't know what this means. Um, someone in chat is asking isn't it faster to update that comment yourself in that case? Um, so sort of. Well, one of the reasons why I don't want to do that is because I'm asking for a bunch of different changes. And so me making one change to this is actually more annoying for them to deal with because now if they try to push this pr, they get like, oh, there's already been modifications on this branch. Then they need to like pull and merge or pull and rebase.
00:33:04.015 - 00:33:32.301, Speaker A: But also because I want them to feel like they own the thing. So me writing this does not mean use this string. It means I'm looking for something more like this. And then sometimes, you know, they come back with I don't actually think that's quite accurate. Or they end up writing a bunch more which is good. Like they give more detail. This is more for me to sort of prompt them as to, you know, I think there's more context needed here for the developer who will be reading these docs.
00:33:32.301 - 00:34:39.215, Speaker A: And I think this is A common failure mode for programming documentation is like they're written with they're not written with a user of the library in mind, they're written with yourself in mind as the reader. And when you read creates a new request builder you go yeah, that's what it does. Whereas to someone who hasn't used the library before or has just picked up the library, what is a raw request? Why do I need a builder for it? Right, there's sort of a there's an impotence mismatch here or an expectation mismatch that I want to highlight. And then I guess the last bit is down here. And remember, I haven't posted this yet, so I will write this in the past tense as though Hyper1 has already been released has landed. This will need to change slightly to match the hyper 1.0 cement types and such.
00:34:39.215 - 00:35:58.005, Speaker A: Shouldn't be too bad in this instance I think 2, 4, 5 may give some helpful pointers for how to adapt the code the changes to with raw client 4 add review okay, so now hopefully this should be all good. The Mac OS and Windows tests always take forever. Still running. Should probably be Unwrap or else. Oh yeah, you're right, good catch. This should probably be Unwrap or else so that we don't construct the box unnecessarily. I also don't think these should need to clone because they consume self.
00:35:58.005 - 00:37:09.285, Speaker A: Oh, did I? Oh yeah, you're right, in may not since we consume self here. One of the things that's interesting is when you when you know you're asking someone to make decently large changes to their pr. I often try to resist the urge to also do a review of the PR at the same time. Right. Things like, you know, avoiding clones or the unwrapper else because I know that I'll have to do another review when they make that change anyway and so I want to not overload them with too many things at once. Like for example, this might mean that they end up changing a lot of this code to basically copy paste the withraw client for code instead. Okay great.
00:37:09.285 - 00:38:02.653, Speaker A: Alternatively, keep it in an option and just if let sum around the call to this later on and then I'll wait to do that until this is merged. Minimal versions always complains in this case oh, this is just a spurious error. I see this annoyingly often. That's fine. I'm fine merging without that one. But the Windows ones take forever and so do the Mac OS ones. But we'll give it a little while.
00:38:02.653 - 00:39:03.015, Speaker A: Okay, so the order here is going to be merge this, tag the release, merge that, tag a release, submit this review. Great. And then we can go over to Notifications to see what the next one is. Not all of them are going to be this long, I hope, but they really should not be. Let's see, while we're waiting for this, any reason you don't have Clippy lints enabled to catch things like the unwrapper else? So I actually do have Clippy enabled, but There was the GitHub action that posts clippy notes has basically not been maintained for ages. And so it stopped working somewhat recently because no one was maintaining it. And I think I thought I had updated it with something else.
00:39:03.015 - 00:39:35.005, Speaker A: Let me see if I can dig that up. Workflows. I think it's under check. Yeah, so I've replaced it with this one which should be working. So I'm actually a little surprised it didn't show any annotations here. I don't know what that is. So there is a clippy job enabled.
00:39:35.005 - 00:40:05.503, Speaker A: Just a minor thing. Notice that you're using code cover the code coverage. It uses tarpaulin and some other outdated tools. Any thoughts on code coverage in Rust and Cargo LLVM Cov? I actually quite like tarpaulin and Tarpaulin now also has LVM based coverage support. I don't think I would call tarpaulin an outdated tool. Also, CodeCOV does not actually require you to use any particular tool. You can use Cargo LLVM CoV and upload to CodeCOV.
00:40:05.503 - 00:40:49.215, Speaker A: CodeCOV is just a place where you upload coverage files. So the two are sort of disconnected. All right, is this ready to merge yet? Well, we haven't really made any meaningful changes to this and we have. We have Firefox on Windows working and we have Chrome on macOS working. So I'm satisfied this is saying this is going to work. Merge commit because I want to keep these individual commits separate. Merge that if we go back to main, we do a pull and then we do my little script.
00:40:49.215 - 00:41:35.415, Speaker A: So now you see a tagged V020 and then we push tags and git push and cargo publish. So now boom, Tarpaulin is configurable. You can choose whether to use the sort of old style or the new style. Great. So change has now been cut. So that's comment. And then we're going to do hyper one and we're going to do.
00:41:35.415 - 00:42:50.455, Speaker A: What's my log here? I'll do git merge main. Yeah, Cargo Tunnel is unhappy and it is unhappy for this reason. This is now just going to be 21 and then I guess here the main thing we're going to pick up is the base 64 change and otherwise we're going to keep the things we had. And then for the cargo lock, we're going to remove the cargo lock and then we're going to check and then we're going to add cargo lock and then we're going to merge, continue, and then we're going to run cargo outdated R. Okay, so there's an update to hyper rust TLS to 27, which feels like a thing we should update as part of this major change. Anyway. Do we have anything else that's updated? Does this also still build? Is the other question.
00:42:50.455 - 00:43:22.945, Speaker A: Great, so it still builds. Let's see if it actually passes our things. Russell? 00:27 it was through 27, right? Yep. Okay, and then we push that and then we'll see if this now still passes CI before we merge it. So this guy's done. And then we gotta wait for yet another CI run here. These are always fun to wait for.
00:43:22.945 - 00:44:15.799, Speaker A: Okay, let's see if we can do another one sort of in parallel while that CI is running. You'll see here actually if you look through my notifications, some of them are like issues that have been open for a while and things like here like OctoCrab having been upgraded to hyper HTTP 1.0. This is an example of an issue where I think I filed this issue originally that I wanted them to upgrade. And it's because I have another project that depends on OctoCrab and that project can't bump to hyper 1.0 until OctoCrab bumps to hyper 1.0. And so this is there to remind me to update that other project that it can now move to hyper 1.0. So there's nothing for us to do here.
00:44:15.799 - 00:44:49.395, Speaker A: This is not a thing that I have to follow up on in this issue. It's just it means that I have follow up to do in other software and that particular one is, I don't think public yet. And so that one I'm just going to leave for now. And this one is actually an example of something that I want to do a stream on. This is a really subtle thing around how drop check works, what it interacts with phantom data. And this is actually just a change to the Rust documentation. I'll link it here in chat because it's a super interesting read.
00:44:49.395 - 00:45:39.215, Speaker A: But so this one is like the thing for me to do with this One is more of a hey, do a stream on this. It's not really a thing for me to do in that issue and arguably I should just like bookmark this somewhere else. In fact, maybe I should really just start one of those. Do that at some point. I'll do that after stream, I'll start a bookmark collection for these instead of just keeping them in my GitHub notifications, which I agree feels maybe, maybe not great. This one we already did. This is someone running the same tool and trying to get it into CI for a different crate that I own.
00:45:39.215 - 00:46:47.331, Speaker A: Yeah, I mean those are all great. I just don't want your CI job. So we'll do same thing we did for the other one. In fact, I don't even have to do that. I think I can just edit it directly here and then remove the spell check. Thank you. So this one we're now going to leave for CI to be happy with.
00:46:47.331 - 00:47:35.105, Speaker A: Although there's no meaningful change here, but we'll read it run anyway. Bump crates index from 2. 5 to 3. Okay, so cargo Index Transit is a crate that's I built on a stream a while ago and the idea was to basically build a compatibility layer between a bunch of different Crates IO types that represent things on Crates IO. Like there's a type in Cargo that represents a thing in an index. There's the crates I own next crate, and there's one other one as well, which is here, let's look at the Cargo toml. So there's the Cargo crate, there's the Crates IO crate, and there's a Crates index crate, and it basically implements a compatibility conversion between all those three.
00:47:35.105 - 00:48:14.335, Speaker A: And this is an example of there's been a major version release of one of them that we would need to cater to. What's interesting here is that this still runs fine. I guess the minimal versions has changed. But what is the actual diff here? Oh, it's a diff to http1, which I don't think matters for Cargo Index Transit because it doesn't actually use any of the HTTP stuff from the underlying crates. In fact, we don't access. We don't do HTTP access at all. So this one, I don't know if I ever actually published this to Crates IO.
00:48:14.335 - 00:49:01.105, Speaker A: I guess I did, but then in that case this is fine to just merge. Let's see why these failed. So coverage tends to fail kind of spuriously sometimes. Yeah, this just failed to upload to code cov because of a rate limit. And this is crossbeam utils. No Longer builds with some newer version of Rust, which implies that over here we have a dependency on crossbeam channel. So this is just a minimal versions, things that we're going to have to fix.
00:49:01.105 - 00:49:58.631, Speaker A: So both of these I'm comfortable with merging in spite of. So let's go ahead and merge this one and we're going to do cargo index transit and then we're going to do cargo nightly update Z minimal versions and then we're going to check if it still builds. It does not great. And so it's not building because of crossbeam utils. So here actually is a. This needs to change. So minimal versions is a way where you take the minimal minor and patch versions within the major version of every dependency in your tree and see that your thing still builds.
00:49:58.631 - 00:50:41.087, Speaker A: So the idea is basically to check that you've specified the correct lower bound for your dependencies. And currently if what you run into is if I take a dependency on Cargo and then one of Cargo's dependencies is incorrectly specified like they've specified they need 1.0.0, but they actually need 1.2.0, then I get a build error when I use minimal versions because my thing picks 1.0.0, because all my dependencies claim that they're compatible with that. And so I have this separate section in here that is specifically correcting the lower bound for transitive dependencies. And that's all well and good.
00:50:41.087 - 00:51:26.851, Speaker A: The problem with this one is currently they're listed as dev dependencies, which means that they actually get pulled in and built as dependencies of my project, even though they don't need to be. So this is trick you can pull which is to have this. So you specify a dependency block that only applies for the following targets and then it's a list of targets that's empty. And so therefore those dependencies are not pulled in for anything. And then you also mark them as optional to indicate that you don't like they don't actually ever need to be built. And so we can do this for all of these. And then we do optional equals true.
00:51:26.851 - 00:52:15.035, Speaker A: And the reason we do this is because they're not real dependencies, we don't actually take a dependency on them. And then we're going to add in here and I guess I'll do a check with this too, Cargo plus nightly. And in fact if I do get checkout cargo lock, you can see what effect what effect this would have if I do this and then do a get diff. Oh, why did not change my cargo lock at all. That seems Wrong. Interesting. Oh, I think it just found other things we need to fix.
00:52:15.035 - 00:52:50.149, Speaker A: Okay, that's fine. So if I do a cargo car up nightly update, minimal versions, then what does it fail to build with? Crossbeam Utils 065. Okay, so we'll do Cross Beam Utils 0.6 and then usually what's. What's funny here is also if you look at the utils and we look at the change log. Oh, that's moved far past us. Why do we.
00:52:50.149 - 00:53:34.305, Speaker A: Why is there even a dependency on crossbeam utils? Tree dash I cross beam utils. Where does that come from? It comes from cargo's dependency on. Ignore that one might actually have been bumped separately anyway. So how about we bump cargo itself and see if that fixes the problem? So let's do a cargo update and then an outdated R. And then now we can do. We can get rid of some of these. Maybe like this probably can go away.
00:53:34.305 - 00:54:05.529, Speaker A: And you see P384, whatever dependency we had that gave us that dependency has also moved on. So for that one we can also probably get rid of this. And then Toml Edit has had a bunch of changes. So that's now on 22.15. We can probably get 22.0 and crates IO has moved from 0.35to 0.40.2.
00:54:05.529 - 00:54:34.279, Speaker A: Actually, we'll do. We'll do 15 just because we're probably gonna have to make some code changes. But why is there no update for the cargo crate? Like there's no way the last version of the Cargo crate is 0 70. Yeah, 0 80. That's what I thought. Why did that not show up in cargo? Outdated. Let's see what we get now.
00:54:34.279 - 00:54:55.445, Speaker A: Now there's no way this builds straight away. That would be wild because we use a bunch of like the deep types from these dependencies. And so it's pretty likely that. Yeah, exactly. That some of the things have changed. Although maybe not. Too bad.
00:54:55.445 - 00:56:01.915, Speaker A: Could not find config in cargo. Oh, wasn't there a change to cargo recently where the cargo crate now has another. Is it cargo util that now holds config whereas config gone? No. Interesting. What crate did config move into? Maybe cargo platform, but I don't think it was cargo platform. Or maybe they made config be private. It's also a little disturbing.
00:56:01.915 - 00:56:58.925, Speaker A: One of the ways to look at this would be if I just look at something along the lines of oops. Oh yeah, a bunch of this has changed. Is it cargo ops? Why is that not linked in this like if I do compile ws, what arguments does it take? It takes a workspace. Okay. And a lot of these take a global context. I think they've just renamed config to global context is what they've done. So I'm torn here because this is a much bigger change on a crate that arguably doesn't matter.
00:56:58.925 - 00:57:36.635, Speaker A: So I'm tempted to just leave this as is and not fix it. I think that's what I'm going to do, just because it's not that important to bring this crate up to speed because as far as I know, no one's using it. Okay, then we will not do that. As interesting as it might have been, we did learn about that trick though, so that's nice. This thing I'm guessing is not going to be a problem, so we're going to go ahead and squash that. Fix a bunch of typos, confirm squash. Excellent.
00:57:36.635 - 00:58:09.811, Speaker A: So that guy we've merged next IMAP release will include those. This I can then just mark as done and move on. This guy. How's this doing? It failed on Windows on both Firefox and Chrome. Missing nasm. Oh boy. Failed to run custom build command for AWS LC sys.
00:58:09.811 - 00:59:29.195, Speaker A: Oh, this is the AWS lib crypto implementation that I guess they're using on Windows. Oh boy. Why? Okay, so this is going to require a change to this is going to require a change to test Windows and then the bits on Windows that are going to be annoying are. Where's the bit where we install? Oh, this hasn't merged for a while from my CI thing. So actually let's go back to main and do CI sync fetch CI. So I have a repository where I keep Most of my GitHub CI config so that I can easily reuse it across multiple projects. And so that is EAs for me to grab updates on the like and so I can just do get merge CI slash main.
00:59:29.195 - 01:00:20.275, Speaker A: Although that didn't bring so much either, which is interesting. That just brought semver checks, which is interesting, but not what I was after. No hyper one interesting because there should be. Ah, here it is. Okay, no, that's not it either. There is a rule in here for installing open SSL on Windows. Yeah, it's this bit that I guess I've removed because I probably haven't needed it.
01:00:20.275 - 01:01:28.483, Speaker A: However. Yeah, I think we're going to have to also install something here for NASM or for the AWS lcd. So here's what I'm going to do. I'm going to do this and see whether anyone else is talking about this particular setting. Okay. AWS LCRS build instructions guidance on installing build requirements windows requires NASM. Okay.
01:01:28.483 - 01:02:30.485, Speaker A: NASM. So GitHub CI windows NASM. Aha. See, there's always someone who's done it before. That's disturbing and sad, but okay. I guess what we'll do here is up here install NASM for AWS lcd. But we only want to do that on Windows though.
01:02:30.485 - 01:03:34.615, Speaker A: And the rule for that is over here, if Runner OS is that give Windows NASM for WSLC push. Let's see if that makes Windows happy, because all the other ones seem to be happy. It's just Windows. Okay. Told you this one was going to take a while. Let's see. Chat when developing an API for a library, how much of a leaky abstraction is mutability? Like if deciding if the user should be abstracted to it with interior mutability or let them have to define mute.
01:03:34.615 - 01:04:14.473, Speaker A: I don't think of mutability as a leaky abstraction. I think of types as leaky abstraction. So if you expose the concrete type of a field, then you're committed to that field staying that type mutability less so I mean, it's mostly a concern if you later on want to make the thing not mutable, but that feels like a thing you're more likely to know in advance, whereas the concrete type is more likely to change down the line, I think. So I would. I wouldn't think of mutability so much. It's an important part of your API, but I wouldn't think of it as a leaky abstraction. Is this a stream for asking random questions? Not so much.
01:04:14.473 - 01:04:36.083, Speaker A: You can ask some. Whether I get to them or not is unclear. It's primarily focused on me catching up on GitHub notifications, which is happening slowly but maybe, surely. How's the weather? It is raining outside and it's nice. I like rain. I don't know if you can hear it, but it's nice. Yeah.
01:04:36.083 - 01:04:57.219, Speaker A: I'll put my microphone closer to the window and see if you can hear the rain. Maybe not config any. Doesn't match anything. That's correct. Config any matches? Nothing. John's favorite topic. Windows.
01:04:57.219 - 01:05:26.349, Speaker A: Oh boy. Not even ZED is now available in Linux. Do you plan to try it? Maybe. I think ZED is kind of interesting, but as I've said before, without a compelling reason to try something else, I don't really want to try something else. And I don't see anything that's compelling there for me. I actually chatted to the Said folks back when I was leaving aws. I considered starting to work there and then I ended up being more interested in Helsing as an option.
01:05:26.349 - 01:06:31.395, Speaker A: But I do think what they're building is really cool. But the other thing for me is, and I don't know whether this is the case for Zed, I would have to double check is I want a terminal, I want an editor that works in my terminal and that works over ssh because I want the same editor everywhere and I don't know whether Zed does that. You can link to workflows in other repos. Not quite. So what I do with my CI thing is no is that it is just a git repo that holds a GitHub subdirectory with the various things that I want to be there in CI and I just make commits to this to change it and then I add this as a git remote to my main repo and then I just merge from it. So I don't commit this. Like I don't put the URL to this anywhere in my in my other repos I just add them as a remote and then I merge allowing unrelated histories and from that point forward I can just merge to get the updates and they just seamlessly merge into my CI.
01:06:31.395 - 01:07:06.627, Speaker A: Can I post links here? I don't know whether YouTube lets you. You can try, but probably not just links. I think only I can do that. Why do you run Windows CI? So for Ventoccini? I do that because I want to make sure that this works cross platform. Right. It's a browser orchestration library and it's kind of useless if a browser orchestration library only works on Linux, for example. So here it actually does matter that it's and like Windows and macOS are fairly different environments there and so you actually need to check that they work there.
01:07:06.627 - 01:08:05.661, Speaker A: It's not. It's not a given that if it works on one platform it'll work on the others. Is there any way to make CI context aware to avoid running jobs for parts that have not changed? So sort of you can configure GitHub actions to only run a job if certain paths have changed or paths under a given directory, for example. What's tricky with some of these is like if documentation in a file has changed GitHub Actions, there's no not really a trivial way to say you don't need to rerun CI because only comments changed. You could in theory do it, but even Then, you know, let's say that someone changed a doc comment and changed text in a doc test in a doc comment, then you really want to rerun that doc test. So it's not immediately obvious how you tease these apart and actually get a more granular understanding of whether something needs to be rerun. We can.
01:08:05.661 - 01:08:33.195, Speaker A: Yeah, it's faint. And also the. My microphone is set to be very low gain because it's so close to my mouth so that it doesn't pick up, you know, environmental noise. Which also means that it doesn't really pick up other things very easily, which is by design. Zed is editing via SSH in preview right now. I don't mean editing via ssh. So I'm not talking about running Z locally and then editing a file through ssh.
01:08:33.195 - 01:09:09.678, Speaker A: I know a lot of people do that, but that's not what I'm talking about. I'm talking about I want to SSH to a computer and then run my editor there to open a local file. And there are a bunch of reasons for this. Like I want the ability to run like sudo editor on the other side, for instance, I want the ability to, you know, if, if, if I'm running some install script or something there and that script wants to start an editor, I want it to start my editor. There's a bunch of that kinds of things. What's your opinion on LLMs in general in the context of software development? Do you use any during work? I don't. This is another one of those.
01:09:09.678 - 01:09:48.012, Speaker A: I haven't seen a compelling use for it. I don't think I spend most of my time writing boilerplate. I think it really depends on the kind of work that you do. In my case, for I would say a decent chunk of the work that I do, I write code that no one has written before. There aren't good examples of the code that I'm writing. For example, I'm doing a bunch of implementations of CRDT algorithms from papers, and LLMs are not going to generate the right code for that because the code does not exist anywhere. I think LLMs are really useful if you're doing things like using an API that lots of other people have used before that's well documented.
01:09:48.012 - 01:10:24.043, Speaker A: Like you're interfacing with S3 or something, or you're writing like a pretty standard web server that's doing some rest thing. Like for those kinds of things, it makes a lot of sense to have LLMs generate code for you. For the kind of things that I do, I don't think it really makes that much of a. Makes that much sense. I don't think it could generate a lot of the code that I end up writing. Another workaround for the Windows build issue with Russell's is to use the ring feature instead of aws, ls, lcrs. So I don't define any features for Hyper Russell's.
01:10:24.043 - 01:11:34.055, Speaker A: I think, yeah, it's an optional dependency and I don't declare any features of it. I'm just using the default ones. And so I don't. I kind of don't want to. I don't want to specifically dictate a feature here because I want the user of Fontaine to be able to choose the library they ultimately end up using. So arguably the way that should work is this should take default features equals false so that it's truly up to the caller, which actually, now that I think about it, let's see if that even builds. Yeah, you can't do with native roots because that requires usually having, well, Hyper Russell's.
01:11:34.055 - 01:12:18.485, Speaker A: With native roots, it requires Ring or AWS lc. Yeah, and I don't really want to specify either, but I would rather than just have it be whatever the default is. What do you mean neovim working through ssh? How would you do that in neovim? It's just installing neovim on the remote. Yeah, exactly. I just run it locally. Locally on the remote. Host path matching for GitHub CI is also very bad with mono repos.
01:12:18.485 - 01:13:22.017, Speaker A: I believe it. How do you feel about the collaborative editing that ZED comes with and maybe more in general about pair programming? I'm a huge fan of pair programming. I think often though, you don't really need there to be collaborative editing as much as one person needs to be able to see the other person's screen. And arguably it's a feature, not a bug, for the pair to not be able to directly edit the code because it encourages them to be sort of overly controlling or sort of take over. But at the same time, collaboratively editing on a document is super valuable. It's hard for me to say because I haven't used that collaborative editing feature. I do a lot of not quite code reviews, but code walkthroughs with teams internally at Helsing, where, you know, a team has built a thing and they want to know whether they've built it well.
01:13:22.017 - 01:13:56.065, Speaker A: And so I will sit down as a book an hour or two with them and just walk through their code and talk through, you know, here's the thing that maybe I would change. Here's the thing, this is a good design. This is. This has the following trade offs that you're making. You should decide whether those trade offs are worthwhile. And for that sort of stuff, you know, we, we walk through the code, but it's not really a writing code exercise, it's a looking at code and reading code exercise. It's relatively rare that I end up actively pair programming with someone, but for that I could see it maybe being useful.
01:13:56.065 - 01:14:46.041, Speaker A: I have not tried Helix Again, no compelling reason to use it. I was saying you seemingly can get the same effect as you're achieving by using workflow links. Don't know if that's any better than what you're doing. I don't think so. My memory at least is that reusing GitHub workflows across repos is super painful. I don't really want to build and actually there's another reason why, which is one of the things that my current setup of merging from a dedicated repo has is that I can make local changes so I can merge from my main CI thing and then I can edit the files. I can just directly edit the files and commit and then the next time I merge, those changes are preserved through git merges.
01:14:46.041 - 01:15:49.475, Speaker A: So it actually works out quite well for me. How much time per week do you dedicate to open source projects? Really depends on the week, really depends on the load. Sometimes it's as much as, you know, a few, like actually like a full weekend, but usually it's more like a couple of hours a week. It should be more like I. I'm realistically, I'm behind on a lot of my open source projects and continue to be behind, but it's just life gets in the way. Any particular reason not to have a fixed typos job in CI? The main reason for me to not include the typos job is because I don't really want to have things running in my CI because things that run in your CI are relatively privileged and so I don't really want to have things that are sort of third party and relatively low value run in CI. It's just not worth the trade off for me.
01:15:49.475 - 01:16:35.393, Speaker A: There's an argument that by setting permissions here it might be okay, but it also makes your CI more complicated and slower and I think for something like typos they don't really matter most of the time and you can always just run it ad hoc so it seems like the value is low and the sort of relatively constant cost of adding new things to your CI is relatively high. Is your new vim a custom config or a boilerplate config? It's a custom config. I've written it from scratch, basically. I've taken a lot of things from different configs, but it's written by me. The problem with screen shares, you just get a limited view of the page. That's the main benefit of collaborative editing. So yes, that's true.
01:16:35.393 - 01:17:34.841, Speaker A: Like if people can go to different parts of the dock at the same time, that's true. However, I also think there's value in forcing the same view of the doc so that you can talk about the same thing and also forcing you to be synchronized about what you're talking about, rather than some person being distracted somewhere else in the doc. So again, it depends on whether you're using this for like pair programming where it really is. We're working on this like one person is the driver and one person is sort of the observer or pair or whether you're doing this as we're collaboratively editing a document. So for the second case, I totally agree that you want fully collaborative editing across the whole document. If you're actually pair programming on a problem, I think it's less clear. Great.
01:17:34.841 - 01:18:07.019, Speaker A: Let's see if the CI has ended. Windows jobs are still running because of course they are. Why did nightly fail? Yeah, this. This one test is spurious and it's extremely annoying, but I will rerun that job should be fine. Oh, this Windows job at least built. So then I think it's. For crying out loud, it failed the test.
01:18:07.019 - 01:19:00.563, Speaker A: But it did build this time at least. But it looks like a lot of them fail, which is fascinating. All right, I think we're gonna move on to other PRs while that continues to run. Right? So this we dealt with. I'll just open a bunch of these because many of them I think we can just not do anything about. This is an experimental feature that someone proposed to cargo for basically something that's higher level than the extern C abi. So it gives like first class primitives for things like vectors and not just for pure arrays.
01:19:00.563 - 01:20:15.465, Speaker A: This one I'm just monitoring because when the RFC ends up landing, I want to read the rfc, but I don't actually follow all the discussion. So we can mark this as done. This is a PR to missing semester. So missing semester is this MIT class that I ran as part of the IAP student teaching month of January a few years ago where we talk about a bunch of things that are sort of missing things that are never really Taught in CS courses, but that we think are important things to know anyway. Stuff like how Git works and we get a bunch of misters to this sort of randomly on like we've published all the lecture notes and everything. And then there is always something small. The man exercise for Windows users in general, this course is focused on Unix systems or Linux, so I'm inclined to keep this as is.
01:20:15.465 - 01:21:10.625, Speaker A: Besides, nothing in this line actually says that you have to use man searching is also totally fine. So this one we're just going to not do close. The reason Windows CI is so much slower is because GitHub CI has way fewer Windows hosts, so they take longer to be scheduled. And also because, I don't know, the build tends to be a lot slower. Whoa. Okay, Windows failed. Why did it fail? Timed out receiving message from renderer.
01:21:10.625 - 01:21:41.037, Speaker A: That's disturbing. An existing connection was forcibly closed. Incomplete message. That's fascinating. So the Hyper 1 bump just breaks windows, I guess. Wait, what the Chrome job. Oh, this is 30 minutes ago.
01:21:41.037 - 01:22:12.155, Speaker A: Okay, good. It's like we're there for a sec. Let's see if Firefox also fails. Firefox job is also failing. So Hyper one on Windows for some reason causes a bunch of the Testify timeout. Timed out receiving message from renderer. Interesting.
01:22:12.155 - 01:23:28.145, Speaker A: I don't have a great answer for this, although this is an error from rendering. Oh, this isn't a hyper error. This is hyper. Hyper did get a response from the web driver driver, like the web driver proxy adapter, whatever, and it said that it got a timeout from the renderer, which is basically the browser process. Interesting. What about the Firefox ones? I want that to fail too, to see if we get a different error there. Chrome driver.
01:23:28.145 - 01:24:11.565, Speaker A: Well, that's not going to be very helpful. It might actually be that we need every version druid breaks unless you pass cryptic arguments. Oh good. Oh good. It could be that that's the change here. Although CI, Windows. Windows.
01:24:11.565 - 01:25:11.735, Speaker A: Oh, it could actually be that we just need to update the driver instances here. So, newest version of gecko driver 034. Let's try 0:34 for gecko driver and Chrome driver. We're just installing the latest one. So I'm guessing here that there's then some bug with Chrome driver that like it needs to be started with a particular set of processes. But which ones though? That's gonna be May 2023. They said to change to headless new.
01:25:11.735 - 01:25:55.465, Speaker A: Oh boy. Enable automation. Oh, they target the gpu. But is this for. Are these arguments for the Chrome driver? Or are these options for the. Oh boy. See here, Chrome driver has none of these options.
01:25:55.465 - 01:26:20.685, Speaker A: Okay, so that means we're going to have to change. But. But this shouldn't have changed because of the upgrade to Hyper. That's what's really weird here. Oh boy. The Firefox one is also still failing, which is interesting. I'm pretty sure that this is like related to the GPU stuff.
01:26:20.685 - 01:26:55.355, Speaker A: I'm. I'm just baffled by why. Oh, we already passed disabled GPU for Chrome. Let's see. Okay, maybe we also need this Enable Automation. But I don't understand why this would have changed with the Hyper 1 upgrade. Maybe we just haven't run.
01:26:55.355 - 01:27:16.769, Speaker A: Maybe we just haven't run CI for a while. No, we ran CI. No, that's not even true. Oh, okay, here's what we need to do. The PR that we just landed for this. How did CI fare here? Because we ended up merging without it. Yeah, here.
01:27:16.769 - 01:28:03.461, Speaker A: At least Windows on Firefox worked fine. How did Windows on Chrome fare? Windows on Chrome failed before the Hyper 1 bump. So this I think is just Chrome changed on Windows. Okay, great. So now we got to try to figure out how to fix Chrome Driver on Windows. No Sandbox, disable gpu, disable extensions and DNS prefetch disable. I don't think that should matter.
01:28:03.461 - 01:28:46.719, Speaker A: I think it's this Enable Automation thing. But I'm just curious about what other let's do here tools search in the past year. January 2024. I'm also facing the same problem. Okay, great. Oh, good. No one's.
01:28:46.719 - 01:29:42.593, Speaker A: No one has an answer to it. Sweet. March 2024, no response. What about this guy? It's not about increasing wait time. That's a L you missed on Enable. Oh, oh, on Enable. No, Enable Automation seems like it should not have Dash Dash, which is itself is pretty wild I guess.
01:29:42.593 - 01:30:26.411, Speaker A: Actually Chrome options. Remember someone this PR we already looked at, which is someone gave us the updated URL for this and so we can just go look what this is. Common use cases, Custom profile, Start maximize Non standard location, location, block dialogs. Chrome options. This is a part of args, which means we need this page. Why is this on a random blog somewhere? Oh boy. Oh boy.
01:30:26.411 - 01:31:13.165, Speaker A: Okay. Automation Enable indication. The browser is being controlled. Okay, so that probably doesn't do anything that we care about. Headless. And we have disabled GPU already. So this is all very unhelpful because this has to be something that's changed recently.
01:31:13.165 - 01:33:08.795, Speaker A: October 2023 Chrome Webdriver 100 okay, okay, but what did you change? What did you change? Tell me about what you changed. I don't know what Catalon Studio is, but what changed in 866 tags? Compare. Compare V8.65 to 866. All right, what changed? That's completely useless. That's completely useless. I'm guessing it's probably not WebDriver Chrome 115 Driver restructuring the new Chrome.
01:33:08.795 - 01:33:44.365, Speaker A: Okay, Chrome for testing. Oh, oh, okay, so maybe Choco install. See, this is what open source maintenance is often like when you maintain things that integrate with other systems is a seemingly simple thing. It takes you ages to figure out why something is no longer working anymore. More. Okay, now I want. Where is the.
01:33:44.365 - 01:35:14.555, Speaker A: How can I see the chocolate? Here we go. Okay. Chrome. Okay, Chrome driver. That's fine. But I also want Chrome testing Choco Chrome for testing. Interesting.
01:35:14.555 - 01:36:38.279, Speaker A: Let's do GitHub Actions Windows Chrome for testing. Wow. Okay, so I'm guessing this is now what we have to do here. Okay. And then we might actually not even need this change we made here. Firefox and Chrome to Chrome for testing. So the default Choco Chrome driver package seems to install Chrome for testing.
01:36:38.279 - 01:37:19.289, Speaker A: But does it install Chrome driver for testing or also Chrome for testing? Let's see. Files, dependencies, discussion. Discussion is completely unhelpful. Okay. Oh boy. Okay, so we know Firefox failed because navigation timed out. That's interesting.
01:37:19.289 - 01:37:51.145, Speaker A: I wonder. So. So this might also just be because the Firefox thing we were installing, the Gecko driver was also old. So I guess now we will see if CI is happy now and again only the Windows ones because all the other stuff still works when I go back and close all these tabs. Nopes, that was too many. Great. Back to notifications.
01:37:51.145 - 01:39:35.275, Speaker A: Okay, this is someone debating an issue that's not a fantastic issue. So that can just be closed. This is someone playing around with the TCP implementation made a PR in the wrong repository. Great, that can be closed Rust IMAP how to handle deflate compressed data streams Are there any plans for related support for the IMAP crate? So I don't think you can have deflate on the TCP connection to the server specifically for things like attachments. I don't think late can be used on the entire for attachments and the like Crepe does not create just gives you the raw multi parts. Decoding their contents is up to you. Comment and I guess I can do.
01:39:35.275 - 01:40:25.463, Speaker A: Oh, there's so many. There are so many pages. I'm hoping many of them Will be a lot faster than the one we just went through though. Tracking issue for public and private dependencies. I really want this. So this is the ability to say for a given dependency in your cargo time to be able to say this is a public dependency or this is a private dependency, with the idea that if you have a private dependency, it's always safe to bump its major version because none of its types appear in your public API. And this is just a major foot gun where people fail to take into account all the types that are reflected in their public APIs and therefore issue accidentally breaking changes.
01:40:25.463 - 01:40:49.083, Speaker A: So being able to have this delineation in your dependency closure dependency listing be super helpful. It looks like they're hoping it might make it into 2024, but it's help wanted for that to happen. So I would love to see that happen, but I'm not going to be the one driving it. Unlikely for this edition. Possibility of future edition. That seems fine. It's sad.
01:40:49.083 - 01:41:40.783, Speaker A: I really want it, but I will. What is this though? What is cargo? Public API creates. Oh no, that's different. All right, so that's not helpful. Stabilize the lint Unnameable types. Ah, so this is a lint that Rust had for ages. That was sort of the ability to tell you if you have a public type, like a type that's marked with pub, but that is not reachable from outside of the current crate.
01:41:40.783 - 01:42:17.699, Speaker A: So for example, you have a private module and inside of it you have a public type. Then even though the type is public, there's no way for a user to actually name that type because the module it was in was not made public. This is usually an error, right? Usually if you declare a type pub, then you intended it to be reachable. Sometimes it's not though. So sometimes the type should have been pubcrate instead of pub. Sometimes it's because you need a type to be pub so that it can be in a public in interface, but you don't want users to ever name that type. Those are exceedingly rare circumstances, but they do come up.
01:42:17.699 - 01:43:10.395, Speaker A: And so that the unreachable pub lint, I think, was basically split into multiple lints, one of them being this unnamable types and with unnameable types being the lint that will probably warn by default because you probably didn't intend for it not to be nameable. Yeah, that's fine. This was just a. I think I just had this subscribed to because I was interested in this landing in the first place, but there's nothing to really? Follow up on here signal handling. An RFC from 2015 on signal handling in Rust and a comment from June saying would love to see progress. Okay, great. This is an easy one to just mark as done.
01:43:10.395 - 01:45:13.685, Speaker A: When I try to connect to GeckoDrive from my Ubuntu server, this comes out the code for spawn is this with headless. Thank you. What do you say to something like this? This. These look like they're gonna fail. Not gonna lie. Okay, I guess what I have to tell this person is that for us, I mean, I can't debug this for them, but what I'll do is we use in CI, so if that doesn't work, I'm afraid you'll have to do some debugging on your own. Close your windows CI passed on Chrome 125.
01:45:13.685 - 01:46:07.965, Speaker A: Interesting. Oh, you dug back in the build logs. But then what's going on here? Okay, so then it's not the then it's not the Chrome for testing bit. Although I mean this did seem to at least still do. If we look at install Chrome here, it did at least install Chrome 126 in Chrome Chrome Driver 126. So I'm. I'm fine keeping that change because at least it makes sure that these are in sync.
01:46:07.965 - 01:47:30.005, Speaker A: So that seems fine. I wonder whether it just like didn't start because I run start process and maybe that fails. What's weird, right, is that Firefox is also failing and so I wonder whether what's happening here is that the browser isn't starting. Because if the error we're getting is that the proxy, the drive, the driver handler, so to speak, is unable to talk to the browser times out when talking to the browser, may the browser crashes or exits or doesn't start in the first place. Because these are. The fact that Firefox and Chrome are both failing is sort of a. A telling oddity here, right? It could be that we need to.
01:47:30.005 - 01:49:16.165, Speaker A: That we need to explicitly start Chrome and start Firefox. I want. I wonder whether GitHub Actions Windows Gecko driver and then I want to search for sometime in the past month maybe I wouldn't be surprised if the NPX command didn't update path. I mean that could be the case too. Although then we should see the start process fail and it doesn't, right? If we look at the Chrome job install Chrome Chrome driver and there's no error after that. There's also no message after that. So who knows, maybe PowerShell doesn't exit with an error if the if start process fails.
01:49:16.165 - 01:50:21.981, Speaker A: But no, it must be starting because otherwise, I mean we'll, we'll see at the error that that comes out of here. But. But if it truly is like the. The connection to the renderer, then that must mean that the Chrome driver does start, because otherwise we'd get a an error trying to talk to the Chrome driver in the first. It's not helpful. No, I mean the start process must be working because otherwise we'd time out talking to the driver in the first place place. And at least that.
01:50:21.981 - 01:50:44.469, Speaker A: That was not the error mode we saw earlier. All right, let's. I'll let those keep running in the background and we'll. We'll see. Okay, notifications, let's keep on going. The relent, relentless march of progress. Open open this has been resolved.
01:50:44.469 - 01:51:11.249, Speaker A: Open Open open Unable to generate a flame graph from the collapse stack data. So this is in flame graph and I've been tagged into it. I've replied and someone has replied with me too. Okay, great. Done. Unsafe field. So this is another one of those RFCs that I'm interested in.
01:51:11.249 - 01:51:57.999, Speaker A: This is basically being able to mark individual fields as unsafe so that accessing them requires unsafe to indicate that there's some additional invariant maintained for that field. Again, this is something for I want to read the RFC when it gets closer to landing so I can mark that as done for now this is another RFC for adding a diagnostic blocking thing. Oh, an annotation to be able to say that a function is blocking and therefore should not be called in async context. So this is neat. There's another one I'm just tracking rust up is not safe for concurrent use. This seems like a plus one. Me too.
01:51:57.999 - 01:52:46.645, Speaker A: So that can be done Adding sums to histogram. So this is an HDR histogram, which is a crate for keeping histograms over large batches of data efficiently. I have a use case where I'd like to get an approximate sum of all the values in a histogram. Specifically, I'm looking at tracking the latency of calls in each of your histogram and then doing something like histogram sum. It looks like this can be achieved through iterrecorded. I was wondering if there might be some interest around dedicated method for this. It's getting wet over there.
01:52:46.645 - 01:53:42.495, Speaker A: Hey there. These kind of utility functions are always kind of hard to say whether they are a good idea or not. So HDR histogram on the histogram type, all the values are stored in buckets and then each Bucket holds like what the sort of what the value of the bucket is histogram wise. And you have exponential buckets is one of the ways this can be efficient. So it stores like approximately buckets, basically. And then for each bucket you have a count of the number of things, number of samples that have been in that bucket. And we have, if I minimize this, we have a method that gives you the length, which is the total number of samples recorded.
01:53:42.495 - 01:54:49.307, Speaker A: So that is the some of the counts across the buckets. But I think what this person is after is a sum of the some of the values. So arguably I should draw this, but I think I can do it in an editor too. So you have something like that. What the histogram really looks like is 0 through to 100, you have, you know, 52, 50 samples, 100 to 1,000 or I guess 0 to 10, you have 10 samples, 10 to 100, you have 50, 100 to 1,000, you have 25, 1,000 to 10,000, you have 5. All right, so you could imagine that this is what you have. And then dot len would currently give you 10 plus 50 plus 25 plus 5.
01:54:49.307 - 01:56:08.645, Speaker A: I think for dot sum, what they're after is 10 times, you know, 0, 0, no, 10 times 5. So that the average of bucket 1 plus 50 times 50 plus 25 times 500 plus 5 times 5,000. Roughly that, that's what I, that's what some would mean to me. But this is a really weird value, right, because it's like you don't actually have the sum of the samples, you just have the sum, you have the product, the average sample, so to speak, for each bucket. So this actually is weird to me, trying the latency of calls and then doing something like histogram sum. It's an interesting idea, although it ends up being suspect. You'll end up with semantics, with having to decide on the semantics you want out of that call depending on the application nature of the values.
01:56:08.645 - 01:58:23.985, Speaker A: In particular, this is because the buckets record samples in a lossy way, so the original sample value is not actually preserved. So your sum would end up being more like the count of a bucket times the buckets mean times the mean of the buckets count of each bucket times the buckets ranges mean, which is odd. Alternatively, we could keep a running sum of the recorded values. However, that feels like a thing that could just as easily be tracked outside of the histogram and thus probably should be. I'm not going to close it because maybe they have some more thoughts, but that's my Initial thought unsafe pinned Aliasing of pinned mutable references this is actually an instance of an RFC that has landed and I want to incorporated it probably into a video into like an unsafe chronicles talking about pinned and unsafe. So this one I'm going to mark is unread and it's another one that I'm arguing should bookmark just like the previous one rather than leave it in my GitHub notifications. All right, how's this progressing? This is now failed.
01:58:23.985 - 02:00:13.825, Speaker A: It's still timed out receiving message from renderer so again this must mean that the the Chrome driver runs. We definitely run the Chrome driver so that means NPX must be adding to path. So if we then go here and I want to do this and then I want to search for Chrome126 and then maybe past month this is very old someone else with the same problem but not super helpful. I wonder whether just search selenium for this June 6th January 11th okay. Nothing in there. Automatically closed. Automatically closed.
02:00:13.825 - 02:01:30.975, Speaker A: Hmm. I actually think one thing that we should arguably add in here actually let's do this. Let's do Chrome driver 1 2, 6 change log Aha. Chrome 126 Media JavaScript Privacy Accessibility New Origin trials Further Reading that's not what I want. Also not what I want. It's also really strange to me the Firefox is having the same problem. That's the thing that really gets me here.
02:01:30.975 - 02:02:17.321, Speaker A: It makes me wonder whether this is not a Chrome changed kind of problem, but if it's a like networking on Windows changed or something I guess let's look at this guy. So here Chrome failed, but Windows worked. Sorry. Chrome failed, but Firefox worked. Okay, so the Firefox failure is due to Hyper 1 but the Chrome failure is not okay. Could try a job installing 125. That's true.
02:02:17.321 - 02:04:25.325, Speaker A: Let's try that. How do I I install? Can I just do at 125? Like can I can I just write at 125 or do I need to give a specific version? And then I guess the question is also what to do about Firefox. I guess I want to see what the error that comes out of this is known good versions. Oh boy, that is a big JSON file. 12125125 this guy. The other thing I was thinking is we could Try to run 126 and see if it was a bug that's since been fixed it. I guess we why do they give another two zeros? Sure I'll Give another two zeros.
02:04:25.325 - 02:05:40.655, Speaker A: No, with Chrome for testing they have the same versioning I think now. I think that was one of the things they really wanted to go for. That is very old, I guess for Firefox actually we didn't change anything compared to this job. So we should be able to hear look at the Firefox error one more time. Navigation timed out after a long time. I do want this timeout to not be 300 seconds because I want these to fail way faster and I want the same from Chrome. Actually, I guess let's look at Chrome options.
02:05:40.655 - 02:06:47.385, Speaker A: I want to look at this for a timeout and see if I can change that time out, but I guess I probably cannot. So we'll go to this timeout. Oh, maybe that's actually a property of Chrome driver. Nope. Okay. Just wanted to see if there was a way I could make it have a shorter timeout. But I guess probably not as a Firefox still fails in the same way.
02:06:47.385 - 02:08:04.825, Speaker A: Firefox Options, WebDriver, Android. I don't want Android things. I want profile, Android preferences, envision capabilities, vendor specific capabilities, Firefox capabilities. That's unhelpful too. Okay, then I guess it's going to be Gecko driver help. Do you have anything for the timeout? You do not. Okay, then I guess we're stuck with navigation timed out after the driver process could be crashing.
02:08:04.825 - 02:08:44.639, Speaker A: I mean it could be, but. But again, I'm not going to be able to reproduce it locally because it runs fine on Ubuntu and on macOS. Right? It's a. It's a Windows problem. I don't have a Windows box test on here. I am curious though, whether there's a way. Oh, good.
02:08:44.639 - 02:09:32.335, Speaker A: Closed is unconfirmed. What? It's just really fascinating to me that the. Oh, I have an idea. Okay, try Chrome 125. I have an idea. Which test is this? Failing Actions and actions uses sample page URL which uses local host. Okay, here's what I'm wondering.
02:09:32.335 - 02:11:15.055, Speaker A: For tests we spin up our own web server that we point that we point the browser at and it could be that in the hyper 1.0 upgrade this code is wrong somehow. If so, it's wrong in such a way that it hangs on Windows but doesn't hang on other. Doesn't hang on other platforms. But it could still be wrong because this code did change because we're using Hypr to sort of host the thing as well. So if I look here at files changed and you look at tests common where is tests common new multithread worker threads. 1 I don't immediately see a way in which it's wrong, but it is code that changed if I'd like.
02:11:15.055 - 02:13:02.565, Speaker A: This seems like a pretty straightforward transformation to me, but it is an interesting candidate, right, because this would also explain why Chrome would be hanging it would be an instance of it tries to browse to this URL and for whatever reason that request just hangs forever. So that would imply that they try to connect, they don't get a connection closed. Chrome was also failing before hyper 1.0. That's true. However, Chrome might have been failing for two reasons, right? There's a. There's a world in which Chrome was broken because did we ever look at what the error we used to get for Chrome was? Yeah, it was the same one, wasn't it? It could be that Chrome is broken for two reasons and Firefox is now broken for one. Right? But I'm wondering here whether the.
02:13:02.565 - 02:14:05.435, Speaker A: The URL that we connect to here for actions null. For example, we try to go to the sample page. The sample page is HTTP localhost colon and then a port, and the port is the one that we got in from. If you look here from Setup Server, right? So Setup server up here. No, that's Start Server. Setup server up here, starts the server, sends the port and then awaits on the server, and so it sends back the port that you're supposed to connect on. It could be that maybe the port is zero.
02:14:05.435 - 02:15:02.355, Speaker A: It shouldn't be after binding, the local outer port should have been filled in. But it could also be that we're binding to v.4 of localhost, and maybe this then needs to be 127011 rather than it can be local host, right? Maybe on Windows now it connects to. That wouldn't explain why Hyper1 changed anything, though, because Chrome hasn't changed, or rather Firefox hasn't changed to like resolve locals differently. This isn't a matter of like the Windows host got updated. The only thing we've changed is hyper1 and. And the URL does not get resolved by our hyper.
02:15:02.355 - 02:16:22.473, Speaker A: It get passed as is to the. To the browser. So it would have to be on the server side. So we should be getting a handle file request request to get the sample page. Could it be that it's like this? No, we do need to give body the thing that goes to a page takes a. No, the web driver command for go to does not take a timeout, not the underlying web driver command. I mean, we could Time out the issue call.
02:16:22.473 - 02:17:43.285, Speaker A: But that's not really what's causing us problems here. And if Windows ended up issuing like multiple, multiple connections, that shouldn't be a problem because we're calling Accept in a loop respawning serving that connection to handle the file requests. There's like nothing really that fancy in here. So my intuition was also oh, maybe it's like a Windows firewall issue or something. But again, the only change between the main branch and this this PR is that we bump to hyper 1.0. Right? It's the same Windows hosts with those have not changed and we're connecting over. We're connecting over HTTP so there's no like TLS weirdness going on here.
02:17:43.285 - 02:19:18.765, Speaker A: I don't think it's an issue in Hyper either. I think it's. And just to check, we have Tokyo with full features and we've. We have Cargo update, right? We're not running behind. I get connection refused locally. Well that's certainly interesting. But again, why just on Windows? Why just on Windows? So I'm curious Simon, could you try changing this to like 127001 in Tusk common and then see if the test passed for you then And I guess it might also be interesting to print out what the port is to make sure it's not zero.
02:19:18.765 - 02:20:11.755, Speaker A: But this is like such a simple web server. Like there's no. There's nothing that strikes me as weird here. Unless I suppose if the my 125 attempt failed because it says just use at 125. Why. But I tried. This is what I tried to do.
02:20:11.755 - 02:21:27.223, Speaker A: Oh, you don't get to use the last. Okay, fine, Chrome. So without this guy and I guess that it's fine, I'll just do at 125 try harder. One thing that maybe could be happening is that I forget whether HTTP 2 was the default. It could be that the browsers are now like requiring http2 but I was already using just http1 in the past, so I don't see why that would have changed any what now My. My Rust format changed. This job failed.
02:21:27.223 - 02:22:56.667, Speaker A: Okay great. Windows Chrome. Well, we could try right? Like if I do this and then just to see that it would one way to test if it's a WindowsU to write a PowerShell script that tries to curl the test server. The problem is the test server is spun up as part of the test suite not getting the port logged in no capture didn't work in test common if you change sample page URL and run the Action test because not all the tests use this. So you specifically need to look for the. You need to specifically run the actions null test. Yeah, like it's weird that these tests pass on Linux and macOS too.
02:22:56.667 - 02:24:28.531, Speaker A: If it was HTTP 2, why would Chrome ON or Firefox ON Windows require HTTP 2 but not require it anywhere else? This is for what it's worth. Why open Source maintenance of often consumes so much time is because you know, things like this, especially if you have a lot of libraries you maintain, you come back to them and then you don't have like the. You don't have an environment set up for one project in the same way. Like if I was professionally working on maintaining Fontoccini, I would have a Windows host, a Mac OS host, Linux host. I would have just like, I would just spin this up on Windows, do a bunch of local debugging there and fix the problem. But when you're not really doing that, when you're owning them in a sort of ad hoc capacity like this, these things end up just consuming so much time. What difference does Chrome and Firefox have between the oss? I mean, nothing really.
02:24:28.531 - 02:25:25.453, Speaker A: I would assume that Firefox and Windows Firefox or Chrome have like at least a similar like web driver and network stack. The main thing I can think of is like networking issues in Windows, but that doesn't explain why the. Why it works on main. So it has to be, it has to be something that changed with Hyper 1.0 as a server on Windows, right? It has to be. I, I think it has to be because that's the only common bit here. That's the only thing that explains all the symptoms.
02:25:25.453 - 02:26:20.409, Speaker A: It works on Main, but doesn't work on Hyper 1.0 and works with Hyper 1.0 on Linux and macOS but not on Windows and it does not work on Chrome or Firefox. Now Chrome might be a different issue, but even so, why the change to the multi threaded runtime? That was because I think new current runtime went away. Oh no, it's because the current threaded runtime does not support. Does not support something that was needed with hyper 1.0. I forget what it was.
02:26:20.409 - 02:26:56.035, Speaker A: Now there's something we started having to do with hyper 1.0 that meant that you can no longer use the current thread runtime because current thread runtime does not let you start blocking threads or something. There's something that there was a reason for it. And so you start a new multi threaded runtime with one worker thread instead. Connection refused. No Connection could be made because the target machine actively refused it. And this is with one two seven, with 001.
02:26:56.035 - 02:27:44.725, Speaker A: So for some reason the server is not starting up or is not accepting connection. It does let you spawn blocking throws. Then I don't. Then I'm not sure why I did the move. But I also don't think it's responsible here. Could the default port that HYPR is choosing be chosen by Windows? No. So the way the port zero works when you bind to a port is that the operating system chooses the port.
02:27:44.725 - 02:28:57.509, Speaker A: And so when you do this, what's supposed to happen at least is that the OS chooses a port that's not in use and gives that to you. And then when you bind, when you bind the port, then the operating system told you which port you chose and that's what you should get back from the local adder here. And then it's that address port port is what we connect to. It could be that from STD here is broken. Although I think we used to do from STD as well. And the old code we did server binding. I mean, there is a change here in that we now bind using a synchronous TCP listener.
02:28:57.509 - 02:30:20.935, Speaker A: Then we call local ladder and then we turn it into a Tokyo TCP listener. Previously we were using server here is from the old hyper. So I guess we're going down this path, right? So server server bind would create an adder incoming and an adder incoming here I believe is just. Yeah, standard library TCP listener bind, which is what we do. And then from STD set non blocking true. And then TC listener from STD and then from listeners. Interesting.
02:30:20.935 - 02:30:53.005, Speaker A: Here's something. TCP listener from std. The caller is responsible for ensuring that the listener is in non blocking mode. Oh good. Oh good. Mind set non blocking to true. Unwrap.
02:30:53.005 - 02:31:52.247, Speaker A: I swear, if this is, if this is it, I'm gonna quit software forever. No, I wouldn't. I wouldn't do that. Does that make sense if it's only failing Windows? I think maybe it does. Because the. If you don't set non blocking to true, what that means is whenever we try to do a read, we're blocking the current thread, the current executor thread. We only.
02:31:52.247 - 02:33:09.335, Speaker A: We have an executor with only one thread, which means blocking. It means no other futures run, including the ones that are supposed to serve files. So blocking the. Blocking the listener task means that the, the, the one that's supposed to handle file requests is just blocked forever. So it could be that what we're seeing is that on Linux, Why would it be different on Linux? Maybe it automatically gets put into non blocking mode when it gets added to an E poll or something. Yeah, I'm not sure why it differs, but the behavior for operating systems when it comes to sockets and file descriptors are wildly different. So it could be that the semantics of not of forgetting to set non blocking on Linux and on Windows, so they just work out in such a way that it ends up working on Unix based systems.
02:33:09.335 - 02:33:58.155, Speaker A: I'm going to guess that this fixes Firefox and then I'm going to guess it does not fix Chrome. Now, unfortunately, because I pushed, we didn't get to see whether this one would have succeeded on Chrome. Did it get far enough that we get to see something? Yeah, this didn't seem like it would have fixed Chrome on its own because these were running for so long. However, that could be the sort of there are two issues kind of ordeal. Right? So it could be that Chrome was not working both because of 126 and because of the this problem. And so by fixing this problem, we might now unblock Chrome to get a different kind of error. It'll be an interesting thing to see.
02:33:58.155 - 02:34:44.935, Speaker A: But now Ubuntu fails. Why does Ubuntu fail? No. Oh, did I do something stupid? Oh, this is HTTP 2 for tests. This needs to be HTTP 1. Back to HTTP. Don't never change multiple things at once. Okay, let's see how that goes.
02:34:44.935 - 02:35:24.461, Speaker A: Oh boy. Okay, where is my Windows job? Windows Firefox, Windows Chrome. Oh boy. All right, close all those things. Now let's back to continuing the notifications. Okay, let's open some more. And again, notice how quickly we've been going through the other ones.
02:35:24.461 - 02:35:51.765, Speaker A: So this, this really is a sort of. Is a discrepancy, but they do happen and they tie you up so much. Naming conventions for traits. This. Someone wrote something in this thread recently, but not that I see. Maybe it was removed. Okay, done.
02:35:51.765 - 02:36:27.015, Speaker A: Unboxed closures. This is going to be like a me too. Okay, great, Done. MSRV Aware resolver call for testing. Okay, this is not relevant. Trim paths. So this is someone who took an RFC I made to Cargo a while back and then it's basically rewriting the RFC because it needed a bunch of changes.
02:36:27.015 - 02:37:18.965, Speaker A: And this I think is not too interesting. So we can close that one. Oh, that's too bad. So there was an RFC to make Cargo write information about all your transitive dependency closure, like which version of each dependency you built into the binary for sort of tracking purposes later on that you could sort of track what a given binary was built with for sort of supply chain type things. And I think the conclusion here was to postpone the RFC because there's just not resources to build it and it's not entirely clear what the right way to build it is. I think I read the the summary for why here and that the RFC needs a bunch of changes. So this one we can mark is done.
02:37:18.965 - 02:38:05.455, Speaker A: Great tracking issue for hashmap occupied entry, replace key and replace entry. These are things that I've wanted for a project at work actually where I don't remember why I wanted these. I think I just wanted to replicate this API in an internal data structure. So if this gets closed that's fine, I will just close it myself too. It has not yet been closed. It's proposed to be closed. That's fine.
02:38:05.455 - 02:38:29.285, Speaker A: Bump hyper for Fontaccini. This we've already dealt with because we ran cargo update, handle sig term and signals from user space. I'll take that one in a second. I'll take this one in a second. Flaky test fix. This is in Factory. So this is.
02:38:29.285 - 02:39:34.015, Speaker A: This is client, a Rust client for interacting with Factory, which is a job server, sort of like Sidekick. And this person, Rustworthy has been doing a lot of really good work. So for example, they converted the entire library from sync to Async, which ends up being quite important for something like this because when dealing with jobs you really want to be async. You do a lot of IO rather than compute. And there's been a bunch of follow up work from that pr. If you really want to see one of the biggest PR's discussions you could take a look at where is it support async? No wait, is it support Async? Yeah, it is support Async. So this is the Mr.
02:39:34.015 - 02:40:33.301, Speaker A: That changes the whole library to be async. And it doesn't look very long until you realize that most of the reviews were conducted over here on reviewable and I think the. That's fine. Okay, when you see that this has like 41 comments, that's 41 iterations back and forth of batches of comments. I was. That was a whole work and I guess this is a flaky test. Simpson making insertion like okay, so this is a test that, okay, we're reading some stats from the server, then we enqueue a job and then we want to see that the server stats indicate that at least one job has been pushed and that apparently is A flaky test.
02:40:33.301 - 02:41:14.145, Speaker A: Why it's tempted to make assertion like total enqueued this time is greater than or equal to total enqueued last time plus one. But since we've got a server shared amongst numerous tests that are running parallel, this assertion will not always work. That will be true in the majority of test runs. Imagine a situation where between our last asking for the server date data state and now they've consumed all their pending jobs in other queues. This is highly unlikely, but is possible. So the only more or less guaranteed thing is that there's one pending job in our local queue. It is more or less because another test may still push onto and consume from this queue if we copast this test's contents and forget to update the local queue name.
02:41:14.145 - 02:42:54.159, Speaker A: Interesting. Although if we share cues between tests, that feels like it unlocks a whole different set of problems. So instead work pretty hard to ensure that tests indeed have different queue names, likely to run into much larger and hard to debug issues down the line. Happy to merge this as is to resolve the flakiness. If we can make the assertions stronger by upholding the invariant that every test has its own queue unique queue name, that's something I'd like us to do. Start a review. The reason I want to merge this even though it asserts something weaker is because the thing it asserts isn't that important and flaky tests are really annoying.
02:42:54.159 - 02:43:21.221, Speaker A: Really bad. So I'm going to go ahead and merge this. We can this one we can just rebase and merge. Great. Done. All right. Something failed and something succeed.
02:43:21.221 - 02:44:27.005, Speaker A: Oh, Windows Firefox passing. So that did fix Firefox on Windows. What about Chrome? Are we ready to look at Chrome? Chrome tests are also passing. Fuck me. Okay, so next question now is if we now move Chrome to be stable instead of 125, will Chrome still pass? Oh, but alert. Accept test has been running for over 60 seconds, so still something weird. Why is so sad to hear you quitting software? I know.
02:44:27.005 - 02:46:36.715, Speaker A: Alert. You know what's really disappointing here is why does the dismiss test pass but the accept test hangs? Oh boy. I'm just trying to see if there's actually any difference between them at all. So this is the dismiss test, right? And I want to see what is different about the acceptance test. And then I want to, I guess delete this guy from here. The only difference between those tests is that one calls dismissalert, the other calls Accept alert and the accept test looks for okay and find here does not is not Loop. It's just a single lookup.
02:46:36.715 - 02:47:20.197, Speaker A: So this feels like a race condition. So how does Accept Alert difference? Yep, Accept alert and Dismiss alert. Just issue web driver commands. Good, good, good, good, good, good. Well, Firefox passed all the tests on Windows. Yay for that. Chrome seems like it's stuck on this.
02:47:20.197 - 02:47:42.625, Speaker A: Alert, accept test. Okay. Minimal versions. That's a spurious test failure. So that one we're okay with. It's annoying, but we're okay with it. I'm going to guess that this is a race condition somehow on that test.
02:47:42.625 - 02:48:59.967, Speaker A: So the question is what to do about it. What we're going to do about this is absolutely nothing. And instead, and I know this will make people sad, we're going to do this instead, which, no, that's not what I want. I want Windows Chrome. I'm just going to turn this back into stable, therefore, also rerunning the tests and see if the tests just start passing on their own. All right, give me a Windows Chrome job I can look at. All right, well, we'll let that run then.
02:48:59.967 - 02:49:37.433, Speaker A: There are more factory PRs. So this is right. Where did this start? This started with issue four. So Factory lets you write your own way to connect to the factory job server. It lets you write both producers and consumers. So producers are things that make new jobs, consumers are things that subscribe to jobs, do them, and put the result back into the job queue. And one of the things that I wanted in there was the ability for someone to like Control C and stop the process.
02:49:37.433 - 02:50:33.055, Speaker A: There wasn't really a mechanism to do that currently, or at least not cleanly, to tell the workers that are currently running to sort of abandon their current jobs. It doesn't specifically have to be Control C. Even though this issue originally started there with signal handling, I actually think I want to relax that a little bit and instead say that there needs to be some way to interrupt the loop. Right. So there's sort of a main loop similar to what you would do for an HTTP server. You have a main loop that sort of runs forever, like it never exits, and you want some way to interrupt that loop. And I think where we ultimately landed here, we had a lot of sort of discussions back and forth and where did this land up? Okay, let's look at the diff here.
02:50:33.055 - 02:51:02.735, Speaker A: Change since last view. Okay, this doesn't really change anything. That's fine. Okay, so here too, we have a worker, Builder worker is sort of the connection to the job server. And now there is a. The ability to set a shutdown timeout And a shutdown signal. The signal here basically being a.
02:51:02.735 - 02:51:59.083, Speaker A: I know this because I've gone through this a couple iterations on this PR is a future that if it resolves then we stop polling the workers, we stop looking for more work, we stop iterating on the ones that we're currently working on and we just sort of tell the server all of these workers have departed now and then return. So it's a graceful shutdown signal similar to what? Like Axum has something similar and as the docs hopefully will say here set a graceful shutdown signal as soon as the prior to future resolves the graceful shutdown will step in making the long running operations return control to the calling code. The grateful shutdown itself is a race between the cleanup needed to be performed and a shutdown deadline. The latter can be customized via shutdown timeout. Right. So this is. You can also set a timeout for if you ask for graceful shutdown.
02:51:59.083 - 02:52:42.755, Speaker A: But for example one of your job processing things CPU bound so it's not yielding back to the scheduler, it's not giving us the opportunity to interrupt it. Then we also have a shutdown timeout which is when I send the Signal, give it 30 seconds to finish. If it still hasn't finished, then just exit anyway. So yeah, you do client connect. You queue a job, whatever create a signaling future that you use to indicate this is sort of the thing you passed a graceful shutdown. In this case we're using cancellation token from Tokyo Util. Great.
02:52:42.755 - 02:54:48.675, Speaker A: And then we create a worker which is the thing that consumes jobs pass it with graceful shutdown. That signal future register a handler for jobs that just returns immediately connect unwrap, run it on that queue and the run here will basically run forever because it's you know, even if there is a job in the queue whenever you process that job and then you know, you just wait for more jobs. So run this normally will never exit send a signal to eventually return control. But I think then what's missing here is that after this okay so here I think we need to is there a try join on join handle. I think it has a try join it as an is finished we should check that is finished is false prior to the call to token cancel. But maybe after waiting a few seconds and we should ensure that we can await after the after the token cancel to test that indeed was initiated. Otherwise I think the docs for this are pretty good shut out timeout.
02:54:48.675 - 02:55:21.355, Speaker A: This will be used once the worker sent termination signal defaults to none. So the naming Here is a little weird, right? Because this. This is not shut down within this amount of time. It is when you have to shut down. This is the amount of time you have. But I think that's probably okay. Okay, this seems fine.
02:55:21.355 - 02:55:40.415, Speaker A: A lot of arguments to new now, which is a little awkward. Listen for heartbeats. Note, this method is not cancellation save. We're using an interval timer internally. That would be reset. Okay, that's fine. Good to document, I suppose.
02:55:40.415 - 02:56:32.337, Speaker A: What's forever for run C's reason. I see. So this is now that run. So the run function used to be able to terminate only in one way, which is if the factory server was shutting down and then, you know, there are no more jobs coming, so you might as well exit now. There are multiple reasons why it might shut down. It might be a graceful shutdown, or it might be. Or it might be that the factory ran out.
02:56:32.337 - 02:57:45.865, Speaker A: So I don't love the name run seize reason. How about run sees Reason is a very weird phrasing. It could be C's cause, which is very funny, but I think maybe. Maybe I'm overthinking. Maybe running reason is fine, but it's not necessarily a shutdown. Right. It's like the reason why run returned.
02:57:45.865 - 02:58:23.471, Speaker A: Arguably. It could just be called done. Right? So. So run returns a. Done returns A. Like in some sense it's an end of file, right? But. But it's not really.
02:58:23.471 - 02:58:38.705, Speaker A: It's a. It is a term. Termination cause. But I think. I think run seize reason is fine. It's weird, like halt reason. Sure.
02:58:38.705 - 02:59:22.965, Speaker A: But. Oops, what did I do? Like halt reason is okay, but is it better than run seize reason? I'm not sure. Exit is fine. Honestly, the more I think about it, the more I think see's cause is not that bad. Actually. It could be stop reason, right? Run seize as in no longer running is to stop. Right.
02:59:22.965 - 03:00:18.673, Speaker A: Like when you cease to run, that means you're stopping. I guess it can mean you're walking, but that. That really means stop. Okay. This I don't think is right because we've landed on the terminology of graceful shutdown. So I think. Would non exhaustive be helpful here? That is a good point.
03:00:18.673 - 03:01:20.005, Speaker A: I think you're right that this should probably be non exhaustive as it feels likely there may be more stop reasons in the future. Also, I think it's unlikely that callers will benefit from exhaustive matching on the se. That. That is the thing. No. Oh, good. I think it's unlikely the callers will benefit from exhaustive matching on this Enum, that is the thing that you give up by putting non exhaustive or one of the things you give up, right? Is that people will not be able to do a match on this without a wild call card arm.
03:01:20.005 - 03:02:22.675, Speaker A: A signal from the factory server received. Oh my lord. I keep hitting control. The factory server asked us to shut down to indicate that the server is shutting down factory instructions. Feels weird. It's really more so that one is graceful shutdown. The other is is a.
03:02:22.675 - 03:03:22.979, Speaker A: We don't actually know whether it's the server shutting down or the server is telling us to quit. The two are I think indistinguishable because a user can do something like log into like the factory control panel, select the worker and say quit and then the server will send a quit to that particular instance. I don't think you can separate the two. So maybe it is server initiated, at which point maybe it is server instruction. Feels weird to have factory in the name there. Run this worker on the given queues. We'll run the worker until an IR occurs, errors returned or until the server tells the worker to disengage.
03:03:22.979 - 03:05:28.425, Speaker A: Okay, is returned or a signal from the. It's a little weird to list okay twice here, but I think it's okay. Especially given the following paragraph and the number of workers. Note the 0 can also indicate that the graceful shutdown period has been exceeded. Either we've already taken this feature and pulling it elsewhere or they never provided it. This feels weird to remove the shutdown signal when we call run because you can imagine that you you actually call run. The server tells you to exit and then you want to start up again, but you want to reuse the same signal handler and you think it's still there, but it's not because we take would be nice it nice if we could give an estimate of how many workers actually remain here.
03:05:28.425 - 03:06:35.117, Speaker A: This is fine for now. Okay, so this is the graceful shutdown case and this is the. We're told by the heartbeats to exit, right? So this is really subtle. So cancellation safety basically means that if you drop a future if you call a function the returns of future and that function is not cancellation safe. That means that if you drop the future, the function returns. You cannot call the function again or if you do, it will have like weird semantics like it might drop bytes or drop heartbeats or whatever it is. So the argument that's being made here is that okay, listen for heartbeats is not cancellation safe.
03:06:35.117 - 03:07:48.465, Speaker A: Therefore we're not okay to call it again on the same connection because you Know, if we look up at the documentation that was added here for this because it uses an interval timer internally, that would be reset. And also it's used on a connection where we do write all and write all is also not cancellation safe. So the argument that's being made here is we're either marking the worker as terminated, in which case we don't start back up, it can't be started back up because it's terminated, or we recreate the connection, in which case, you know, we start a new timer, we start a new connection, and so we don't reuse the broken connection. And this I think is just code that was already there and hasn't changed. It's just the indentation has changed, which we can confirm with this. Just to skip white space changes. Yep.
03:07:48.465 - 03:08:32.205, Speaker A: No further down here. Why did this get changed from have to has all workers that should still be have have exited. This is why the whitespace independent view is useful. Okay, that seems fine. That seems fine. That seems fine. Okay, but I am still worried about this.
03:08:32.205 - 03:10:35.737, Speaker A: This means that if the user decides to reconnect using the same worker, their graceful shutdown future is no longer there and active. I think we should only it if it resolves because then it's definitely not useful again in the future. That is a little bit weird though, because it means that. It means that if you try to reuse it after a graceful shutdown, it'll be gone, which will be weird. And then we should also document on with graceful shutdown that if the future resolution resolves, it is then no longer is then removed and no longer taken into account. If the connection is reused, the worker is reused. Okay, run this worker.
03:10:35.737 - 03:11:35.215, Speaker A: Until the server tells us exit, our connection cannot be reestablished. Right. Run to completion is sort of a convenience constructor here that says exit the process. And this is sort of a handy thing to call for main. For example, if you just want to write a main loop that runs sort of handling factory, then you want it to just shut down. Okay, I'm confused why this can't just call oh, because we try to reconnect. I see.
03:11:35.215 - 03:12:36.303, Speaker A: But this seems weird. Well, why can't this just call self run and allow run to deal with the graceful shutdown logic? Feels weird to have to redo it there. I don't think that's actually necessary. And I guess this is especially if we make the change around. Take above that I suggested that might be the reason they've done it. Okay, and then we have a bunch of test changes that just assert this New property that we got. Arguably.
03:12:36.303 - 03:13:17.675, Speaker A: This should be now a instead of this returning a tuple, I'm wondering if it should return an actual type. How do you feel about returning a struct here rather than a tuple here with pub fields? Okay, that's now viewed. No, I'm lying. That's not viewed. Oh, that is viewed. Okay, those are straightforward. And this is now.
03:13:17.675 - 03:15:12.151, Speaker A: Okay, so this is a test connects the worker that's going to run for a long time and queues the job. Make sure the job received it, signal to return control and then see that it's still exited even though it didn't need to. Although I think this test needs to run forever, I think this actually needs to be an infinite loop in each loop to avoid wasting cycles to truly test the cancellation logic. Otherwise the test will succeed even if the graceful shutdown machinery didn't work. Oh, I guess that's not true because run would still just keep running even if you eat the job. So I wonder if you even need the sleep here. All right, that's fine then.
03:15:12.151 - 03:16:02.525, Speaker A: Okay. And then I should also go look at C at the discussions we've had to see if any of them can be resolved. This has been made a duration. It should be an option that's been changed already enum to tell them which case that's been added. Yep, Control C has been removed because Control C is just a special case of what you use to define the graceful shutdown future. Yep, that's been out of docs for that. Makes me happy.
03:16:02.525 - 03:16:34.135, Speaker A: The re export has gone away. That's no longer needed. That's fine. That was changed and that's moved. Okay, Resolve. Nice. Okay, and then we can do review something like.
03:16:34.135 - 03:17:22.925, Speaker A: Nearly there now. Mostly smaller nits left. Submit. Okay, how's this looking? What? Why did that job stop? I'm confused. Okay, macOS on Firefox failed, but that's probably just the. Yeah, that's just the damn spurious test again. Windows Chrome failed again.
03:17:22.925 - 03:18:47.405, Speaker A: But why, Crucially, why can't I open this to see why it failed? I want. I want to open this post. Run has been running for 10 minutes. Yeah, I don't really understand what's going on here. Why is this job just like hanging Cancel? I guess I can retry it. What's interesting though is this means now that we're in the same state as Main is in and so that makes me inclined to merge this because this feels like Windows Chrome is broken on Main and also broken on Hyper 1 and it's unrelated to the Hyper 1 change. But now this job is just stuck.
03:18:47.405 - 03:19:48.345, Speaker A: I guess I can just push another commit to try to trigger it, but I would rather just retry these two jobs. This is weird, right? Like this suggests that some process is just stuck somewhere and is not being killed. Oh, this can be. This is done now. Well, all right. Empty bump CI. I don't like it, but it does the trick.
03:19:48.345 - 03:20:44.955, Speaker A: Okay, add tracing. This is also factory add tracing examples for run and run to complete completion. Okay, so this is basically adding logging instructions. This is fine. It's an example. So showing trace logs as well as seems totally fine. What else are the comments I left last time that's been resolved.
03:20:44.955 - 03:21:02.325, Speaker A: This is in examples run line 23. So that's up here. It used to do an iterator now it just access the orgs directly. Great. That looks fine. It's an example. So easier to read is better.
03:21:02.325 - 03:21:50.385, Speaker A: The tracing info can just be a print line at the end that's now just a print line. That seems fine. Source worker health what was the change there? This method is not cancellation Safe moves to 57. Okay, that moved to 57. Okay, so the change that was made there to that comment has already been moved into the PR that we just reviewed. The rest of this is just adding trace events, which seems totally reasonable. So this has been resolved.
03:21:50.385 - 03:22:52.305, Speaker A: That's the thing that's already been fixed elsewhere, which means now we are happy with this thing. So looks good now. Is there a plane landing? Pretty sure there's a. There is like a Unicode character for plane landing. Yeah, see, it's just GitHub doesn't want to give it to me. Excellent. How many commits? A bunch of commits.
03:22:52.305 - 03:23:28.785, Speaker A: So we're going to squash it. Confirm, squash and merge. Amazing. Done. Okay, back to this guy. How's this not looking? This is now running a bunch of things. Main one we care about is where's the Windows one? This is my Windows job.
03:23:28.785 - 03:24:14.305, Speaker A: Windows Chrome keep that one open over here add support for blocking handlers. Okay, so also in factory the argument here is now that the whole library has been async, we still want to make it possible for people to write blocking handlers. So handlers that are not async that just need to do a bunch of compute, for example. And in particular the way we want to do this is running them as blocking blocking tasks like Tokyo blocking tasks so that they don't end up holding up the executor, for example. So we basically want to make it easy to do the right thing. If you have a Blocking task. And I think we got pretty close on this.
03:24:14.305 - 03:25:13.775, Speaker A: I think it was mostly smaller things. Let's go ahead and open the files changed because I think the diff was pretty small. Right. So the debate here is Tokyo has this thing called spawn blocking where you can give it a closure. You can be an async context, you give it a closure and Tokio will run that task on a dedicated thread that is allowed to block and will not hold up the async executor when doing so. That feature comes with this warning in the Tokyo doc saying that if you have a lot of CPU bond code and you really want to limit the number of threads they use it, you should just not use the Tokyo stuff. The Tokyo stuff is for you're wrapping something needs to be blocking.
03:25:13.775 - 03:25:38.315, Speaker A: You're not using it for like a general purpose blocking thread pool. That's not really what it's for. Like use Rayon, for example. And the way that's manifested is in. That's in the Tokyo docs and then this. The contributor here has added some part to our docs. So if we go to look at the change, you'll see that there is a.
03:25:38.315 - 03:26:39.635, Speaker A: Where is it over here on Builder here there's a method on the builder that is register blocking fn, which is like if you get a job of the following with the following type, then use a blocking handler to respond to it. And they've added this paragraph to it that says note that it's not recommended to mix blocking and non blocking tasks. And so if many of your handlers are blocking, you want to launch a dedicated worker process where only blocking handlers will be used. And I was pointing out that I'm not sure I expect this to be necessary. It should be fine to mix and match them. And so I think maybe the distinction here is are most of your tasks blocking? If so, you probably want a separate thread pool. But if only some of them are, then I think mix and matching is fine.
03:26:39.635 - 03:27:55.715, Speaker A: Consider launching a dedicated runtime. Okay, so they want to give a nudge. I think this is too strong of a nudge. I think giving a nudge in this direction is fine, although I think the nudge is currently too strong. I wonder if we just want to say something like. I like the phrasing something like. Because it indicates it doesn't have to be this, it just has to be like this and say you can mix and match async and blocking handlers in a single worker.
03:27:55.715 - 03:30:18.581, Speaker A: However, keep in mind that however, blocking is not no active management of the blocking tasks in Tokyo. And so if you end up with more CPU intense, intensive blocking handlers executing at the same time, then you have cors the asynchronous handler tasks and indeed all tasks will suffer as a result. If you have a lot of blocking tasks, consider using the standard async job handler and implementing and manually and explicitly consider using the standard async job handler and add explicit code to manage the blocking tasks appropriately. Okay, so that's one thread. And the other discussion is on how these are stored. So because we now allow the handlers for jobs to either be async or synchronous, those are different types of closures, right? One returns a future, the other does not. The box job runner is a.
03:30:18.581 - 03:31:00.009, Speaker A: And I think it's defined in the same file here, a little bit higher up maybe. Oh, it's in runner. The box job runner is worker runner. So we have this trait called job runner that just lets you run a job. That's all it does. And then we have a box job runner type, which is the box din of that. So if you have a, if you have an asynchronous job handler, then you store one of those.
03:31:00.009 - 03:31:53.105, Speaker A: If you have a synchronous one, then you store. Well, currently the PR stores an arc din of that closure. And what I was pointing out was like, okay, why does this need to be an ARC when the box job runners are box, right? That feels weird, especially because these callbacks are already stored in an arc, right? Because you register all your job handlers once and then you spawn up a bunch of tasks that handle the incoming jobs and invoke those callbacks. But that means they all need to share those callbacks so they can all invoke them. So all the callbacks are stored in like an arc hash map, basically. And so there's already an outer arc, so why does there already also need to be an inner arc? And so this is the debate we get down into here. And this is the thing that actually runs a job.
03:31:53.105 - 03:32:50.259, Speaker A: It looks up in the callbacks hash map the handler for a given job kind here sort of job type, and then it matches on that handler. And if it is async, then the thing we used to do is just call, call the function.run of job, right? And that's what we still do. And we map that to an error in the comments end up getting in the way here. But in the synchronous case here, CB then we call spawn blocking, and then we move the call into the spawn blocking Closure and then we call call the function, which is synchronous function. I think the reason why they have an ARC here is because they, you know, spawn blocking requires the thing. You pass it as static.
03:32:50.259 - 03:33:50.345, Speaker A: That means it can't have a reference to something that lives outside. But here what happens is we are, you know, we have a reference into callbacks here and callbacks here is an R cache map. And so the CB that we get out of sync here is a borrow inside of that hash map, which means it's a borrow inside of the arc, the ARC hash map of callbacks, which means that it's not static. We can't move it into the closure that's required for spawn blocking in order to call the function. And so by making the thing an arc, they're able to do an ARC clone just of that one handler and then move that ARC into there and then execute the job now. And this is where I said here, this is why you ARC each individual sync function. You could instead just arc clone all of handler like the entire callback.
03:33:50.345 - 03:34:30.335, Speaker A: The entire callback hash map to avoid having an ARC inside of an arc. What is this comment? Yes. And getting self callbacks cloned instead because the individual handlers are not ARC in the registry, while the registry itself is arc. Yeah. This is basically what you end up with, right? Is that you. You do the lookup. You look at the handler type.
03:34:30.335 - 03:35:25.925, Speaker A: If the handler type is sync, then you need to arc clone the callbacks. And then you need to move the clone of the ARC hashmap into spawn blocking so that it owns the entire arc. And then you need to look up again and panic if it's not there because you know that it is of that type because you matched on it further out. Yeah, that's. That's exactly what I was thinking. I agree it looks pretty weird. It's an interesting question, right? Whether it's better.
03:35:25.925 - 03:36:15.945, Speaker A: I mean, I think it is. All right, let me see. Why is. I'm curious whether people can still see me because chat suddenly went very quiet. Someone in chat say something just to see. Okay, you see me. Great.
03:36:15.945 - 03:36:57.425, Speaker A: Okay. Okay, great. I agree it looks pretty weird. This code is pretty awkward and it's sad to. It's sad that we have to double match. I guess it's an interesting question of whether it matters. It's not quite box versus arc, right? It's whether a double ARC is worth avoiding the.
03:36:57.425 - 03:37:29.595, Speaker A: The annoying double lookup. Double arc versus double lookup. And honestly, I think the double ARC is fine here. Like, the more I think about it the more it's sort of the perf. There's not really a performance overhead. You end up with a less contended cache line. You end up with slightly higher memory use because you need to allocate space for the counter.
03:37:29.595 - 03:38:46.545, Speaker A: Should just stick with what we had with the double arc. Even though it makes me sad. Can it be an enum where each variant is arced? Not easily, because you need to share the whole hash map between all the threads or between all the work handlers and those clones I think happen more often. We could maybe move the arc down. I don't know that it helps here. Okay, I. I think then we'll.
03:38:46.545 - 03:39:28.331, Speaker A: We'll resolve this. We'll resolve this. This was resolved. There's not a discipline the way it's now. Oh, I see. So this is basically for spawn blocking. Now we spawn blocking in Tokyo will actually catch panics and return them as errors that you can manage.
03:39:28.331 - 03:40:31.699, Speaker A: It would be nice to catch panics here for the futures as well. Let's do that in a follow up. Maybe the tech debt and then the. This test I think I've looked at in the past and it looks fine. So I think then that's fine. Okay, so this is now. Basically this is okay except for the one comment I pointed out.
03:40:31.699 - 03:41:54.535, Speaker A: So let's then do viewed viewed looks good subject to the one documentation change. Great. Done. All right, how's this looking? Chrome succeeded on a bunch of them and then started failing. Dev tools. Active port file does not exist. What? Well, the fun never stops.
03:41:54.535 - 03:42:38.455, Speaker A: Oh, there's a recommended fix now. Ah, 125 working. 125. No pipes. Debug pipes. Dev tools. Okay, but in progress.
03:42:38.455 - 03:43:24.139, Speaker A: Accepted. Okay, so this is a bug in chrome. Comment number 36. There's a workaround. Is that true? Remote debugging pipe gives timeouts instead. Oh, good. Pin our web test to Chrome 125.
03:43:24.139 - 03:44:11.549, Speaker A: Okay, great. So we found what the problem is. So now Chrome failures on Windows are now due to this. So that means we'll now do125.125 pin to 125 due to this guy. Although I thought this guy still failed, didn't it? Oh, we never saw that one finish. We never got to see that one finish.
03:44:11.549 - 03:45:01.585, Speaker A: What did this person say here? 125 and no pipes. There's a timeout. 125 plus remote. Remote debug pipes. Okay, so that means we need this argument as well. So that's going to be in test Common for Chrome. Great, so it's not our fault.
03:45:01.585 - 03:45:37.081, Speaker A: Work around this guy. Let's see how that works. Maybe. Maybe it wasn't actually our fault. That would be nice. Slowly but surely. And this one has been stuck.
03:45:37.081 - 03:45:53.625, Speaker A: When was it reported? In March. Oh, good. Great. Oh, all right. I need some food and some more tea. So I'm gonna. We're gonna take a short break for eating.
03:45:53.625 - 03:54:01.197, Speaker A: Going to the bathroom, getting more tea. So let's say another five minutes or so and I'll see you back here. Well, we'll keep going. And let's see, I run in the background in the meantime, so. See you in a second. That can't possibly have been scary, right? That wasn't scary. All right, let's see how this is doing.
03:54:01.197 - 03:54:35.393, Speaker A: Still eating, but you know, let's see. Windows, still compiling. Oh, good. Okay, this is just release notifications. So we can ignore that. This has been already closed. So we can ignore that release notification.
03:54:35.393 - 03:55:08.895, Speaker A: Can ignore that. Bump engine X from 125 to 127 and it failed CI. Ah, this is the flaky test fix. Which means that this is now fine. Which means now we're okay bumping this in rci. So that's fine. That can be merged squash and merch.
03:55:08.895 - 03:55:35.925, Speaker A: Thanks Dependabot. Very kind of you. Done. Bump factory version in the Enterprise factory CI job. Okay, that seems fine. Proof, squash and merge. Done.
03:55:35.925 - 03:56:52.515, Speaker A: How's this job doing? Little book of Rust macros for the latest Rust episode. I don't understand. Understand. Oh, this is for rotation station. I guess they're meaning that it should be added to the show notes. Maybe I am filing a PR because I don't know where they mean. Tracking issue for cargo config.
03:56:52.515 - 03:57:28.095, Speaker A: Displaying config values. That's fine. That doesn't seem to be important. And foreign build scripts where the cargo is checking or building discussion from. Is CAR project looking for someone to implement this? No, it's still in discussion. Oh, this is whether build scripts should get to know in what context they are being run currently. Build scripts don't get to know, they just have to.
03:57:28.095 - 03:58:54.115, Speaker A: Like you can't optimize a build script for check specifically. This doesn't seem super. No, my windows open. Okay. So not too much new information there. Oh, interesting. This is so the cargo build cache currently, in order to see whether a file has changed, it just looks at the modification timestamp of the file, not whether the contents of the file has changed.
03:58:54.115 - 03:59:37.213, Speaker A: This can be really annoying if you're doing things like R syncing where like the M time might change even though the contents are the same or if you revert the change to a file, fixing it is surprisingly difficult. And this is essentially a tracking issue for fixing that. I'm curious about this. I'll continue to subscribe to it. This feels. I don't even why am I subscribed to this? I use this ages ago. I do not need to be subscribed to all of this.
03:59:37.213 - 04:00:48.255, Speaker A: Done Just skipping through some of the Inferno ones because I think there are multiple that have built up and so I want to sort of batch them together and see if there are any other funi or factory ones first sort of going through here. Seeing a lot of these you see are Rust ones so that we can close them out pretty quickly. They're often just has something changed in this issue. This I think is just I want to see when it's resolved. New release of Cargo Public API that's interesting, but nothing to really do about Derived default suppresses warn of dead code I don't know why I'm subscribed to this. Private Enum variants I do really want that, but this is just more. I want this as well.
04:00:48.255 - 04:01:34.015, Speaker A: Oh, this one is so annoying. Okay, so this is for Rust format. If you have really long strings, then Rust format just gives up on formatting the surrounding text. And it gets really bad if you have chains of function calls because then the remainder of the chain of function calls does not get formatted. And if the chain includes things that call say closures and those closures have bodies, the bodies of those closures also don't get formatted. And it's just very surprising. You end up with a bunch of code that's not for formatted.
04:01:34.015 - 04:02:08.745, Speaker A: Yeah, things like this. But this is just a. It is still a problem. Default to v0 symbol mangling. This one I'm just keeping an eye on. So this is when Rust initially came out, it used like a symbol mangling. Here is how you turn Rust type names and function names into debug symbols that are the things that get printed in backtraces and stuff.
04:02:08.745 - 04:02:58.599, Speaker A: Because you need an encoding because the debug symbol files only allow restricted character sets. Often it's just like it's only letters and numbers. And so the original Rust was sort of trying to emulate C or C really and using a lot of the same encoding standards just so that Rust things would show up roughly right in most tools that knew about C. And then a little bit later someone standardize again. This is what the Rust mangling will actually look like so that we can accurately capture the naming of Rust types. Because the approximation to C types or encodings was sort of lossy and they called it V0. And this is basically a proposal to make V0 name mangling the default because it is more expressive, it is not as lossy.
04:02:58.599 - 04:03:39.815, Speaker A: The reason not to do it is because there are a bunch of tools that would need to specifically support the Rust name mangling format. Not just they can't go by and get by anymore on just supporting the C1. And so I think they've sort of been holding back on a bunch of popular tools getting support specifically for the Rust mangling. And so this is just keeping track of, you know, what's left. Oh, nice. Looks like they will move for V0. Oh, okay.
04:03:39.815 - 04:04:12.381, Speaker A: Interesting. Okay, so maybe progress will be made. Okay, this is a. Me too. I can close that. This, I think, was the thing I cared about for work. I'll leave this one open because it's a.
04:04:12.381 - 04:05:03.055, Speaker A: I think I cared about that for work for some reason. Semantics of the expectation attribute. What is the expect attribute? Lintric. Oh right, I forgot about this rfc. So this RFC is specifically for this thing called the expect attribute. And the expect attribute is. Let's see if I can find the RFC link here.
04:05:03.055 - 04:05:49.675, Speaker A: Rendered the expect lint attribute. So the idea here is that sometimes when you write tests for code, you want to check that a lint gets triggered. And this expect attribute is I guess now been added or it being closed means the semantics have been finalized, I suppose. Oh yes. Stabilize lint reasons. Nice. That's cool.
04:05:49.675 - 04:06:43.025, Speaker A: So I'm guessing this will end up in release nodes at some point, but it is interesting that now you can say, you know, I expect this block to trigger the following lints. Cool. This is why I subscribe to a lot of tracking issues, is to get reminders of things like this that I cared about in the past. Great. And I guess this is just the same that this is now closed and landed. Great. What else do we have? Unblock cargo feature metadata.
04:06:43.025 - 04:07:15.435, Speaker A: Move to a tracking issue. Mmm. Oh right. This is so currently in your cargo tunnel, when you declare features, you just declare them like this. Right. It's just a name equals array feature data. The plan is to do this.
04:07:15.435 - 04:08:02.795, Speaker A: And I forget what syntax they used for the. Is this really not doc. Okay, that's unhelpful. The idea is to basically say something. I think it's extremely extends maybe. So the idea here is that they want to allow you to write a sort of more verbose Way to declare a feature similar to what you can do for regular dependencies. And the reason they want to do that is so that you can in theory add other fields such as description equals.
04:08:02.795 - 04:08:47.715, Speaker A: But this is sort of the start of moving to a table based instead. So that's cool to see that landing. I definitely want to subscribe to that because it is a handy thing that I want to see. Land fix and dress with WebKit driver running vanishing this WebKit driver. This is from 2018 from myself. Empty string for the browser name. Empty string for the browser name.
04:08:47.715 - 04:09:19.515, Speaker A: Well, until someone comes with an actual use case for this, I'm just going to leave this one as is. Migrate to alternative compression for Crates IO cr. Crates. Oh, this is. This is also a debate. So this was open in 2016. So Crate files, like the things that are hosted on Crates IO, the things that are published when you use Cargo Publish are really just compressed tar files, compressed tar balls of your source code.
04:09:19.515 - 04:10:37.305, Speaker A: And by default they are compressed using just flate2. And the argument is, well, there are better compression algorithms than that. How about we use something else to make them smaller so that they're faster to download, faster to upload? And so this is sort of an analysis of what are some alternative compression mechanisms, how much both, how much smaller are they and how much slower are they? And it looks like there's just a bunch of debate on like, exactly which algorithm should be used. And also how do you do backwards compatibility? Like, do you ever stop uploading the old ones? Because if so, you're sort of dropping support for older versions of cargo that don't know how to extract those old crate files. Okay, so this is just a bunch more debate about this. Okay, I don't. This is not a debate I actually need to be part of.
04:10:37.305 - 04:11:08.219, Speaker A: It's just interesting. What are we eating? This is homemade pizza. I made homemade pizza. And then I've put beetroot hummus on it. No specifying dependencies for individual artifacts has been closed. Okay, that's fine. Rustine crate ownership policy.
04:11:08.219 - 04:11:47.545, Speaker A: Oh yeah, this one was sort of contentious on. The idea here was to figure out, you know, if you have a crate that's abandoned and someone wants the name, what do you do? How do you handle that? And I think they settled on a policy that ultimately got sort of ratified. I'm interested. Oh, no. Hummus on pizza is real good fix. Cargo hack job for the Bin target. Okay, so this is something where we did something.
04:11:47.545 - 04:12:45.815, Speaker A: Oh, I see. This is. This is specifically when you run something like cargo Check in a repository. Your options are you can run cargo check, which just checks lib and bin. You can run cargo check dash all targets, which includes examples and tests and benchmarks. Or you can give, you know, dash lib dash tests or whatever. But.
04:12:45.815 - 04:13:21.221, Speaker A: But what's weird is sometimes, you know, you kind of want to just hit all the targets, but not for instance, examples. Examples tend to be an instance of something where you don't really want to cover that one. Benchmarks tends to be a similar one where they often require nightly. So you don't want that to be a thing that you run your exhaustive feature testing on. For example, which is what Cargo hack does. It compiles with every possible combination of feature flags. And so there you don't want to pass all targets, but you also can't pass no targets because the default set is just lib and bin.
04:13:21.221 - 04:14:13.745, Speaker A: You want to include tests. And so what I've ended up doing for my CI thing is to do dash lib dash tests, which covers lib and tests. Problem is if you have a crate that has no lib but only has a bin, then dash lib will actually error in cargo. Cargo will say, there's no library here. Similarly, if you pass bins and you don't have any binaries, cargo will complain. And so this makes it really awkward to figure out what the sort of the right thing to pass in here. And then I guess someone here is saying, okay, can we just change cargo to not error on this case? Yeah, because bins will warn, but dashes lib will error.
04:14:13.745 - 04:15:25.535, Speaker A: That's interesting. Yeah, I'm curious to follow what happens to this. And I guess this is the general question of like, is there a way to cargo check everything except benchmarks? Because it benchmarks are surprisingly annoying. Oh, there's been a lot of discussion here and this feels like a thing not worth trying to catch up on right now. Interesting. Yeah, I'll subscribe to this and see what. What comes out of it.
04:15:25.535 - 04:16:27.775, Speaker A: But that means there's nothing to really be done here for this. So that's good. How's this doing? Well, it's running a bunch of the tests, but then it. Then it seems to get stuck again. Feels like we need to do some kind of like rate limiting or something for Chrome, which, you know. Okay, here's something I'll show you. What is that thing called? It is called serial.
04:16:27.775 - 04:17:53.051, Speaker A: So all Firefox tests are run with serial, meaning we only run one at a time. The Chrome tests are not. They're run in parallel so what if we just make all the Chrome tests be. Also be serial? Why? Why are these sometimes in different order? Let's just make these all first be in a consistent order and then add the same annotation to all of the Chrome tests. And so that way rather than. I mean, it means the tests run slower, but if it also means that they actually run, then I'm happy. Because currently what actually happens is that Chrome for Chrome, we run all of our test suites at once, which means we only open like a window for every test.
04:17:53.051 - 04:19:04.675, Speaker A: And if you have like 30 tests, that means you're opening 30 windows at the same time. And I could totally see that end up being quite slow. So how about we just do this? All of the Chrome tests going to be serial as well. And even for the ones where we have relatively few tests, I'll do the same thing. Because otherwise, when we eventually get more tests, it might suddenly stop working and that seems pretty frustrating. So let's just make them all serious. So I think that covers all of them.
04:19:04.675 - 04:19:56.855, Speaker A: Great. Make all Chrome tests serial as well. Poor Windows. It can't handle so many things at once. The amount of CI hours we've spent on this now is pretty depressing. All right, what else do we have? This we've dealt with. Oh, this is like a debate about the semantics of pointer types.
04:19:56.855 - 04:20:32.485, Speaker A: It's not been resolved yet. That's why I'm watching it. This is a debate about a bunch of improvements to macros of adding count so you can say count and then like a dollar variable meta variable from a macro pattern and it'll return how many instances there are, how many repetitions there are of that token, which currently is. You have to do in super hacky ways. And this is just giving you a standard way to do it. But people have opinions, not bad opinions. People just have opinions at a general mechanism setting rust flags and cargo for the root crate only.
04:20:32.485 - 04:21:29.305, Speaker A: So this is. I want to build all of my dependencies as normal, but then I want to build my crate with a particular set of additional rust flags. This could be things like, you know, that's a good question, actually. What would it be? I wonder if there are good examples. Target CPU native, but I thought you would have to pass that down. The debate here is basically, should there be a way to specify rust flags just for the root crate and not for all the dependencies? And I guess this was proposed to be postponed, which seems reasonable. They didn't have a concrete use case for it.
04:21:29.305 - 04:22:02.721, Speaker A: I'd also need naming needs documentation okay, that's fine. Not too much to do about that one. All right, let's look at the Inferno ones maybe, because now we have a couple of those that have come up. Oh, there's a Fantoni one too. Okay. Inferno. So Inferno is a crate that draws flame graphs.
04:22:02.721 - 04:22:33.245, Speaker A: So if you run a benchmark or something, you run a profiler on it, you sample a bunch of information about where your program is spending its time. Inferno will draw flame graphs of that profiling information. So this is stuff like these. You may have seen them in places before. I don't know why that load slowly but things like this, so they show you where you're spending your time. And it was originally, it was implemented as a Perl script. And then this is a port of that Perl script which is still maintained to rust.
04:22:33.245 - 04:23:40.135, Speaker A: It uses dash map because it allows you to do to process the input file in parallel. And then you need to keep track of like how many times have you seen a given sample, which means it's concurrent access. Do need a concurrent hash map. I forget whether I think actually, sadly, Dash map is in the public API. Is that true? Yeah. So this is pub enum occurrences that holds a dash map, which means we can't actually do a major release of it without doing a major release of Inferno itself. Yeah, Inferno was built entirely on stream, so there's a.
04:23:40.135 - 04:25:18.615, Speaker A: There's a bunch of videos of that port. This is public because it is part of the sealed Collapse Private traits API, but is in a crate private module, so it's not nameable. Interesting. Does that mean we can bump it? Because that would be nice. Ah, okay. It only shows up in implementations of the collapse Private trait. I think let's pull up another one here.
04:25:18.615 - 04:26:36.965, Speaker A: Collapse that doesn't occurrence is new. So we have a pub trait that names this type in a bunch of places and then what the claim is that occurrences. Okay, common is a pub crate. That means it's not publicly visible. If we go to lib, there's no re export of common either. So inside of common, the only places where occurrences shows up is in the collapsed collapse private trait, which is public, but it is sealed because it is also in a private module. So that means no one outside of our crate can implement this trait.
04:26:36.965 - 04:27:24.365, Speaker A: That means no one outside of this trait is actually dependent on occurrences because they can't be implementing this trait. And also they can't name the occurrences type directly. So that means they're never given one and they can never construct one. And so I think, I think what that means is it's actually safe for us to change it. Just looking through to see if there's any sort of weird looking signatures here, but I don't see any. Okay. I think that means that it's actually safe for us to bump dash map here.
04:27:24.365 - 04:27:58.895, Speaker A: Interesting. And I think there's actually nothing. Let's. Let's do a. Here's what I want to do. I want to do a git pull. Then I want to do a bumps and in bumps I want to run.
04:27:58.895 - 04:29:35.655, Speaker A: I want to run git sub module update for each poll. I guess I can just CD into the one that we have. So we have a sub module that pulls in the pearl thing in order to run the Pearls things test suite as well. Why does that fail a bunch? That's somewhat disturbing. Oh, it's because the underlying files have changed. If you look at Tusk Common collapse some random one of these. Line 28, I guess.
04:29:35.655 - 04:30:47.155, Speaker A: Where's the one that actually runs the Flame Graph things Collapse perf. But which file am I in tests? I think it's Test Collapse perf that does all these. Yeah, here. Okay. Collapse perf test upstream. So what this does is it runs all of these tests based on the files that come from the upstream. The upstream Perl version.
04:30:47.155 - 04:31:09.201, Speaker A: So a bunch of these are these upstream ones. We're using inputs and outputs that come from upstream. So there we should actually match them directly. Right. Because the comparison files, the output files are also in that repository for the ones that are down here. Here we're comparing against data that we have generated. So this is.
04:31:09.201 - 04:32:14.725, Speaker A: We're using the inputs from the real Flame Graph project. We're using outputs that we have defined. And this is things like improperly demangled rust symbols. But what's interesting here is if you look at like this guy for example, this is in one of the upstream ones, which means it really should. It should be matching upstream. It's a little bit disturbing that it doesn't. So what's left versus right here if we go back to common wait.
04:32:14.725 - 04:33:19.355, Speaker A: Oh yeah. This is from pretty assertions. Okay, so line is what we got and buff is what they had. Interesting. So we do actually just end up with something completely different. Why that's fascinating. I want to see what actually changed in this file.
04:33:19.355 - 04:34:59.472, Speaker A: Add another test source. Are these all in vertex? No, some of them are. Okay, let's do Java stacks then update tests. Okay. I wonder whether they've actually changed how they run the tests. It does feel like something's actually changed here that causes us to now end up with very different results. So what are the things we.
04:34:59.472 - 04:36:12.535, Speaker A: We end up producing this, but not this. But there isn't even a big tcp. It looks like these lines are just different. I actually wonder whether if we go look at this file line 143 like it could be that it's actually just the line order or something that's different. So see, we produce one that includes K free. Is there one that includes K free here? There is. So specifically, what's the one we were looking for? K free SKB release data SKB release data 1.
04:36:12.535 - 04:37:01.154, Speaker A: But before that TCP clean RTXQ. So that's not the same line. Interesting. So we're just producing completely different stuff here. This feels very strange. This feels like. Like the test files are almost wrong.
04:37:01.154 - 04:38:31.734, Speaker A: Delta test. What happens? I forget what we end up running. We end up running kernel, for example. That is what we end up running. I wonder whether there's actually been a change here somewhere. This feels like something that is worth fixing, but not right now. So I'm actually going to do rename this to Flame Graph Update Bump Flame Graph sub module for newer tests and then I'm going to push that because that's actually separate from the thing that I wanted to fix.
04:38:31.734 - 04:39:36.395, Speaker A: So I'll go ahead and open that as an Mr. So that we have it open the upstream. The tests from upstream now. We now fail, which is disturbing. Okay, so this one is just something that's gonna be a headache for me at some other point in time. Interesting. This feels like it could be partially related to this question mark.
04:39:36.395 - 04:40:33.965, Speaker A: Okay, so this is. There's a bug somewhere where we mismatch what Perl does. So that feels like a separate thing that needs to be fixed. Ah, this is a fix for that which I'll review in a second. I wanted to do this one first. So this bump to Dash Map sub module, put that back to what it was, please. So for Dash Map, I'm actually tempted to just move to Papaya now that it's out.
04:40:33.965 - 04:41:10.855, Speaker A: But I guess we can do this bump separately first. Anyway, what's the. What's the actual release note for six? It doesn't actually say why it's breaking though. Like what's. What's the breaking change? Fascinating. But it seems to like it mostly built. The things that changed are it bumps, minimal versions.
04:41:10.855 - 04:42:01.995, Speaker A: So what I'm tempted to do here is cargo tunnel minimal. Let's look for dash map version 60001, I guess. And then I want to run cargo MSRV to find what the new minimal version is because it's no longer cargo164. The reason I want to check this is because I don't want to bump the minimum version requirement of Inferno too much. It's used by Cargo Flame Graph, for example. So if we bump it, that means Cargo Flame Graph also needs to bump the lowest version of Rust it works with. And that makes me sad.
04:42:01.995 - 04:43:06.995, Speaker A: So I just want to see how much it bumps it by. If it's just like a couple of versions, it's not the end of the world. I guess while that runs, we look at this again. Oh, nope. Even with er though. Did I update elements? Yeah, I did. I don't know, man, why Chrome on Windows is so unreliable here.
04:43:06.995 - 04:44:06.095, Speaker A: I'm tempted to just land it. Anyway, I guess we'll see whether the. We'll see whether the errors are the same as what we've been seeing, but this feels like a thing where it's not worth trying to push us any further and we should just land it and do the release. It's a little sad. I wish I could make the Windows Chrome CI green, but alas, I don't think it's meant to be. And where did this land? Msrv is now 165. Okay, that's fine.
04:44:06.095 - 04:45:03.215, Speaker A: So then we'll do bumps. So we'll do dash map, which changes GitHub workflow test. Nope, check MSRV to be 65, which is dash map. 6 bump dash map. And I guess that replaces. And I want to, I guess also run this to see if. See why minimal versions is broken.
04:45:03.215 - 04:46:08.195, Speaker A: Whoa. Get unchecked. Mute requires that the index is within the slice. That's wild. I don't even know how to figure out where the origin of this is. I guess I do Rle decode fast. This comes from libflate.
04:46:08.195 - 04:46:48.005, Speaker A: Okay, so we'll. We'll do the bump here. I want to keep these contained so that we can control which. So that we can actually control which things cause which test to fail or break. This one I think is going to be just fine. Then we'll go back to main. We'll do minimal versions.
04:46:48.005 - 04:47:26.785, Speaker A: We'll do Cargo toml look for Flight. And I think this is actually a lie. I think we need a much newer version of Flight. So let's actually look at our cargo lock. What do we have in our lock for libflate? What is the Latest one of Flight 2? Oh, it's Libflate 2.1. Update to minimal versions. Let's See if that'll run.
04:47:26.785 - 04:48:48.295, Speaker A: No. So it's probably in RLE Decode Fast that has 103. Oh, interesting. These two are get rid of this. Do we actually use regex? I don't think we do. Yeah, I don't think we do. I think regex here is actually one of those 4Z minimal versions target dot config any dependencies.
04:48:48.295 - 04:50:38.705, Speaker A: I don't think we need even that. Right. And we needed to do the same thing for RLE decode fast no longer builds with newer rust so early decode fast. Next we're going to bring in 03. Why is that being great? Checkout cargo dot lock and cargo T and get diff and we no longer I think need to bump this one. I think it was just the RLE thing that got that triggered a newer rustlint. So we'll do add P fix RLE decode broken version of that.
04:50:38.705 - 04:51:25.395, Speaker A: And this is a change that we can make independently of the dash map bump. Right. So this way I can open it as a separate PR over here. Let's see. So dash map now fails on ubuntu minimal versions which we're fixing nightly doc stable coverage and whatever this guy is. So nightly a hash fails, codecov fails because rate limiting. Okay.
04:51:25.395 - 04:52:15.375, Speaker A: So on nightly we have a problem because of the version of a hash we have. So that's also something that we can fix independently of this, which means this is good to land. So we'll do bump of. No, we'll do rebase and merge. This guy confirm not a breaking change because occurrences is only used in a sealed trait. Great. And then this should fix the minimal versions and otherwise should leave CI working the way it was.
04:52:15.375 - 04:53:22.585, Speaker A: And it indeed fixed minimal versions. So this one we can rebase and merge, confirm, rebase and merge, delete and then we can get pull on main. And now we got to fix this a hash thing which I think will do with a hash a hash fixed nightly. So this is bump a hash to fix newer nightly builds. And I wonder actually whether there's a here if we go to a hash and look for issue. Oh, in fact it was that one. So this was actually filled in.
04:53:22.585 - 04:54:29.355, Speaker A: Oh, it was fixed in 087. So we can do even better here. We can update cargo toml to say that cargo update pa to precise a hash at 0.8.7 a amend that closed the dependabot. So that's good. And now we should be able to do this guy create pull request. Hopefully that should fix our nightly builds.
04:54:29.355 - 04:55:58.829, Speaker A: I think the simple Weight test that would cause local to fail. But I don't think it's local that gets stuck its element. And that's the only sleep there. But it is certainly true that this should be this don't sync sleep, but I don't think that'll make an actual difference. Okay. Did that fix our tests? Yep. I don't see any failures on nightly.
04:55:58.829 - 04:57:01.423, Speaker A: Let's all also make sure we didn't accidentally bump the minimal rust version. I don't think we did. Great coverage. Still fails because of the rate limiting, but that's okay. Confirm. So now. So now I think we're getting close to being ready to do a release of Inferno with the changes we just made.
04:57:01.423 - 04:58:29.545, Speaker A: There are a couple of other things I want to do, though. First, we have this PR right here that seems to be fixing some inconsistency between Inferno and the original Flame graph. Interesting. So this seems to be. Seems to be some change where they take into account. Okay, so when you run perf record, I think it records. Or maybe for certain types of events, it records how long the sample was too.
04:58:29.545 - 04:59:12.445, Speaker A: So this would be. I guess we can just look at the PR that this person did. So they say. Okay, so they. This is the parsing of a perf line. So this is what the lines that come out of perf look like. It is the name of the process, the idea of the thread, then the.
04:59:12.445 - 04:59:44.815, Speaker A: No, the idea of the process, the idea of the pro. The. The name of the process, the ID of the process, the ID of the thread. And then in the sort of trail here, there. This is the. This right here is the event type that was gathered. So this is like, you know, it took this.
04:59:44.815 - 05:00:14.337, Speaker A: The. It took this many cycles or it took this many, in this case, CPU clock cycles. This, I think, is the times stamp of when that sample was taken. And I don't know what the square brackets is. And then this is the address where you were at in the program when you took that sample. And so previously, what we would do is we would split by colon. So we'd split.
05:00:14.337 - 05:00:50.261, Speaker A: You know, we parse out the stuff that's before here. We split this by colon and we look at. So we split it by three by colon. So that gives us the time, which is that thing that we skip. That's the thing that we skip here. And then it gives us the type of the event. And yes, that's what we get out here.
05:00:50.261 - 05:01:26.615, Speaker A: Event is Bicolon's next. So it'll be the next thing from the Iterator, which is the thing that precedes the next colon and the thing after that again is a description of the event, which is this, this bit here. And so that's what we parse down here. And I guess the observation is for some of these samples, like this one, you'll see this is the time of the sample. And then in here it's not just the name of the thing, it's actually for how many cycles were you observing what that process would doing, what that sample was doing. And so it is that this, this thing in between the columns is actually as. As this says, the period and the event.
05:01:26.615 - 05:02:05.995, Speaker A: The event name comes last. The period is what comes next. Interesting. Yeah, I could see that. That seems reasonable to me, however. Yep. And reset for the next event.
05:02:05.995 - 05:02:28.755, Speaker A: Yeah, I buy that. This. This looks about right. I'm not even sure whether you need. Oh, I see that after event. Yeah, that's fine. Okay, so the one thing that's weird here is this doesn't change any of the tests.
05:02:28.755 - 05:03:20.281, Speaker A: So does it pass the test suite? Seems dubious. Update with rebase because I have a suspicion that this will not pass the tests or we don't have any tests that have periods. And I guess this could be part of why we fail when we run with the upstream test suite is because if. If they've adapted to this change, that would potentially change results by a decent amount. I guess we'll see here. That'll be interesting. We also have no, that's for Fontaine.
05:03:20.281 - 05:04:13.785, Speaker A: Okay, how's this now doing this fails with session not created. Failed to construct timed out messaging timed out receiving message from render. Honestly, I think this is just another instance of that same problem. I'm going to just go ahead and merge this on Windows is still flaky hangy on Windows, but about the same as on Main. So going to land this now. Squash and merge. Confirm squash.
05:04:13.785 - 05:05:05.495, Speaker A: Okay, so now if we go back to Fantoni and we go to Main, we do a git pull and we go back to Cargo Toml. If you look at outdated, there's nothing outdated. And this is now 021. Are there any other changes to land on this now? I don't think so. So in that case we're going to do tag. Am I actually not going to use my script here instead? I'm going to do 02.1.0 and I doing that because I actually want to write the tag message myself because I want to give a little bit more details.
05:05:05.495 - 05:05:27.597, Speaker A: Release version 02.1.0 with hyper 1.0 and friends. Actually is there Even more context to give. I don't think so. I think that's the main breaking change here. Great.
05:05:27.597 - 05:06:17.599, Speaker A: Push tags git push and Cargo publish released as 0.21.0. Well, yeah, there we go. Comment. All right, done. And now this that we left here. Can now say request changes submit. And I guess I'll also hear say sorry it ended up landing right in the middle of the hyper 1.0
05:06:17.599 - 05:07:20.805, Speaker A: change. Excellent. Okay, back to this guy. Yeah, so this, this fails a bunch of tests. Why? Yeah, it's going to fail a bunch of tests because all the sample counts are different. So it actually needs to update the tests. So I think what I'll do here is I'll help this person a little bit to.
05:07:20.805 - 05:08:28.745, Speaker A: No, not here in Inferno. We'll do. We'll do git remote add orkabit slash git fetch orkabit and then we'll check out their branch which is fix period. If we now run cargo test in here, those are all going to be errors and I think there's a. I guess not. I thought there was a command line argument, like an environment variable that we had. Yeah, infernal bless tests.
05:08:28.745 - 05:09:40.495, Speaker A: I thought we had that. Which I think is supposed to overwrite them, but I guess not. Huh. All right, well, this is where arguably we should just be using Insta instead, actually, because if we were using Insta, this, it does support something like this rather than me having to do this manually, which is going to be a pain. Yeah, this is going to be a huge pain. So maybe I just look for that. So I think we have something very similar here.
05:09:40.495 - 05:10:54.873, Speaker A: Yeah, which is here now maybe it'll just overwrite them. So now let's look at what we have. What is the actual diffuser? Checkout flame graph. Wait, why did a bunch of these files change? Oh, they changed because of line endings. They change because of line endings. Well, I don't really want to override the line endings here. I only want to override them if the compare fails.
05:10:54.873 - 05:12:33.345, Speaker A: I think so catch a wind returns a result and then if we do if let error E is that clone. Sure. So and then STD panic resume unwind E checkout tests, data sub module reset and then let's do this now. Hopefully the diff should be more manageable. Gif dis dash W Interesting. Color words, please. Great.
05:12:33.345 - 05:13:46.985, Speaker A: So it's only really the counts that have changed. And having looked at the original change, I think that original change is probably fine. I don't know why there's a change to this SVG file That seems unexpected. Add test data collapse Perfect. What does the actual change here quote apostrophe. If I check out that, does it still. Oh, wow.
05:13:46.985 - 05:14:33.775, Speaker A: I don't understand why that has changed. Because this file didn't make any modifications to that. Am I. Am I going insane? Get submodule update init CD Flame Graph reset hard. Oh, this is. This is actually potentially going to be quite painful because what will happen is. Yeah, now we get a bunch of errors saying, oh, your counts aren't matching the things that come out of Flame Graph.
05:14:33.775 - 05:15:49.555, Speaker A: The challenge is if we the fix to this is to update Flame Graph to a newer version, but then we also hit other changes in Flame Graph. So the question now becomes which commit of Flame Graph do we need to move to? And that I think is going to be this presumably changed a bunch of the tests. Okay, so we need to move to whatever commit ended up landing as a result of this. So we want to move to this commit and then we just hope Check out this fetch. What remote V. Oh, this maybe. Oh, I guess I have to add the remote for this.
05:15:49.555 - 05:16:40.655, Speaker A: Add the remote for this I guess upstream. What Remote add upstream. Fetch upstream and then git checkout that. And now if I run cargo test now we get all these other errors because there was some. So that means there was some change to Flame Graph prior to this commit. But after the commit that we were previously on and so this is where I said it was going to be painful because that means we actually need to solve this guy a little bit at a time. We need to first solve until that commit and then we can solve after that commit.
05:16:40.655 - 05:17:31.593, Speaker A: So this means add add Flame Graph. So this is bring tests and Flame Graph along this I think we want to. This is a good change regardless. So this one submit will update in it I guess. Get stash pop. This is allow blessing test run for collapse 2. And this one I'm just going to push because it's fine.
05:17:31.593 - 05:18:23.239, Speaker A: It's in test. Oh boy. Pull, rebase push like this. So if we now go into the Flame Graph update, I think what we actually have to do is we have to CD into Flame Graph and then we have to find the commit before the one that changed all the accounting which is where is it? I had it open a second ago. Here we want to do the one before the merge commit. The merge commit was. Merge commit was a 14.
05:18:23.239 - 05:19:20.349, Speaker A: So this was the parent commit. So if we now do here get checkout that one. So this should now have not the. It should have the changes to Whatever the underlying bits are. But it should not have the changes to sample countering. So if we now look at this and then we go here and we say comp compare and we want to compare from the show P. We're going to compare from the commit we're on on main to the commit that is before the changing of the accounting.
05:19:20.349 - 05:20:28.325, Speaker A: Interesting. What has changed? A bunch of things have changed. Many of them just remove lines. Interesting. Interesting. Oh, why am I not being given a commit list? Am I being confused about the order of these? So if I look at this commit and I look at this commit. Oh, this commit does not belong to any branch.
05:20:28.325 - 05:21:31.417, Speaker A: Oh boy, we're on a fork of flame graph, aren't we? We're on a fork of flame graph because I have a fix pending with flame graph that was never merged and so flame graph doesn't take that one into account. Which means I need to update my flame graph. Oh boy. So that's the actual problem here. I need to update my fork of flame graph to be the latest flame graph. But with this change. Oh boy.
05:21:31.417 - 05:22:30.255, Speaker A: Minor others get clone. Where is my fork of flame graph? Yeah, copy clone this. And I want to get. I want to add upstream flame graph here. Okay, remote add upstream. Is this guy fetch upstream? No. So what is the branch that I wanted to merge this change from this change.
05:22:30.255 - 05:23:31.795, Speaker A: So now this is from perf last sample. Perf last sample that has one commit and then we want to rebase that onto this, which is before the sampling change. So I want to rebase onto this. Oh, that is the commit before. Okay, so then it may be maybe we got lucky. Maybe the only change that actually matters is the. Is the big change, the sampling change.
05:23:31.795 - 05:24:46.785, Speaker A: In which case we need to rebase onto this. The only thing I want to keep is include last stack in perfcollaps. Right? There's only one commit and it's that one. And if I now run test sh force with lease. So we'll push that one out and see if. Wait, why does this. Oh, because there have been changes since then too maybe.
05:24:46.785 - 05:25:40.365, Speaker A: So get fetch upstream log. Oh, this is just completely wrong. Mid fetch upstream reset hard upstream master. I need to rebase onto this. I was just completely wrong. Here we go. And then for this guy, what I actually want to do, I think is I want to rebase abort.
05:25:40.365 - 05:26:48.603, Speaker A: I want to get reset head. I want to keep only my change and not the. I want to keep only my change, not the changes to the test because the tests I can just regenerate Anyway, and then I'll do this and commit just that, not commit any of the test changes. Then I will do the rebase onto the latest flame graph, which should be easy. And now the question question is how do I regenerate all of the test files? Because that's really what I want to do. And I think. I think the way to do that is to change this to.
05:26:48.603 - 05:27:31.309, Speaker A: Instead of diffing, just writing it to outfile. And now we can go back to diffing. And so now that looks like it's the same change again has a bunch of. So. So this particular change is like it's a case of perf fails to take into account the last event that comes out of perf. And so everything is off by one. And so you end up with a bunch of these, like the count increased by one and then some of these.
05:27:31.309 - 05:27:41.455, Speaker A: I think it's just. There's an event that just gets missed completely. So it. It perf. The real flame graph counts it as zero events and therefore doesn't print it. And we count it now as one. So it appears.
05:27:41.455 - 05:29:11.665, Speaker A: And so now if I do this and the push force, then this should now be up to date with master. And this is now that commit which is what we need to move Inferno to fetch upstream hard this fetch origin. No get. Yeah, this though will not work because we also now have the sampling things in mind. So we need to check out orcabit fix period. Nope Branch fix period CD Flame Graph reset. This also bring Flame Graph diff our flame graph change ref.
05:29:11.665 - 05:29:52.615, Speaker A: And now does that pass the tests? It does not. So there's some change here. This is. This means there's some change that has happened upstream since that counting change happened. And specifically, at least now the diff is much smaller. It's some of these L things. So that means if we look at the delta from this commit compare this to head.
05:29:52.615 - 05:30:54.383, Speaker A: Then what has changed? What has changed? What has changed? Well, what I really want is the list of commits, if I can. Thank you. Specifically, I want to know what changed the test files. So in fact I can just go to something like Test results per vertex stack. Test results per vertex stacks collapsed PID Collapsed pid. Wait, but that file hasn't changed. Oh, that's the input file.
05:30:54.383 - 05:32:45.449, Speaker A: Right. So I actually need to compare that to not. Yeah, perf vertex stacks collapsed pid What? But that file has definitely changed, right? Otherwise we changed it. Okay. Used to have this dash L in a bunch of places and we also have the elsewhere. So their behavior did not Change because then the results file would have changed. So why then are we now seeing failures here? We produce something with lio they don't l.
05:32:45.449 - 05:34:37.069, Speaker A: Net lio they still they produce things with Lio2 something something's wonky here. Maybe it's this. No, that looks different spaces and file paths Fault handler until vtune license comment this is take period into account. If I'm comparing from that commitment then how does this even. How can there be older changes? Fix uninitialized value warrant empty input I'm so confused. 9:28 left is what we produced. So we produce stuff that includes these l Java in for example.
05:34:37.069 - 05:36:57.705, Speaker A: Okay, let's do Java faults01 collapsed PID in results right? Test results perf java faults collapsed PID okay, test results perf Java faults 01 collapsed PID so that file was regenerated. What's the history of this file? Updated us 7 years ago and then this thing and this thing seems to only as far as I can tell seems to only have. Let's do blame actually no, let's do why does their version of the file not contain, for example l. Org. But it does have I just like done something stupid here. CD9EE is the latest commit, right? CD9E is the latest commit and then plus our change. Well now I'm test results perf Java faults collapsed pid color words diff mass diff Upstream master to here.
05:36:57.705 - 05:37:48.671, Speaker A: Okay, that's not helpful. Okay, there's a bunch of l Orgs in the diff before. There's not a bunch of logs after. I think I know what's happening. No one has run the upstream flame graph test suite in a while. I just regenerated a bunch of the files and they changed. I tell you man.
05:37:48.671 - 05:38:30.455, Speaker A: Here, look. Dev others Flame graph master get pull log V let's just reset origin here. No master to be synced with upstream master. So now this is on CD90E. So this is upstream master. Now if I run testsh it fails. This is just upstream flame graph.
05:38:30.455 - 05:39:53.185, Speaker A: So upstream flame graphs tests it doesn't pass its own tests. Well then how are we supposed to pass their test suite if they don't pass their own test suite? And I'm telling you, if I now change test sh and I make this do this and I run test sh, turn it back to doing the diff and then I get diff test Results perf Java faults 01 PID Then here we have L. Org. L. Org L. Org right. We all see the lorgs and down here there's no lorgs.
05:39:53.185 - 05:40:23.795, Speaker A: There's no L Orgs in the diff. So. Okay, so now we're gonna have fun. Fun little journey with figuring out why the lorgs changed. Okay. Get. Get bisect script.
05:40:23.795 - 05:40:55.215, Speaker A: Get bisect start. Git bisect script dot slash test. Sh. Git bisect start. Start is actually. No, we know that. That commit was okay.
05:40:55.215 - 05:41:25.285, Speaker A: No, I did something stupid. Get bisect stop. Get what? Bisect stop. Abortion. Okay. Get bisect start. Get bisect script.
05:41:25.285 - 05:41:55.015, Speaker A: Get bisect bad no good. This. Get bisect bad head and then get bisect script. Test script. What? Isn't it. Isn't it script? Oh, get bisect run. I'm sorry.
05:41:55.015 - 05:42:18.855, Speaker A: Well, how about that? It is the one that I opened earlier. Remember we're looking through the history here. This one. I opened this one. I was like, I wonder if this does something weird. Swear, man. Oh, wait, it broke the tests.
05:42:18.855 - 05:42:48.965, Speaker A: We'll just fix the test afterwards. Thanks. I. I swear. Okay, so here's what we're going to do. We're going to. We're going to check out this.
05:42:48.965 - 05:43:47.205, Speaker A: Then we're going to regenerate the tests. This diff. Regenerate tests following 147. Then we're going to go here. We're going to reset hard to master, get, pull, check out master, get pull everything. Good. Then we're going to cherry pick this, which we can't do because it conflicts with.
05:43:47.205 - 05:44:45.183, Speaker A: I don't even know what it conflicts with. Why does it conflict? Per Java faults, one collapsed pit. You know what it conflicts with? It conflicts with the counting change. Oh boy. Does that mean. It does, doesn't it? That means that this change. It.
05:44:45.183 - 05:45:15.699, Speaker A: This change updated the tests, but it updated the test locally on the branch and therefore it did not take into account the changes that were on main which were happening concurrently. Which was this other change that. Okay, it's all fine. It's all fine. Just. It's all fine. I almost wrote a very bad branch name instead.
05:45:15.699 - 05:46:02.642, Speaker A: I'm going to do this. I'm cool and composed. This isn't a git problem really. This is a they're not running tests in CI problem. Okay. Okay. When did these different ones land? So this guy landed September 18, 2022.
05:46:02.642 - 05:46:47.045, Speaker A: When did this land? November 2023. So. Oh, it's because I didn't regenerate from the merge commitment. That's why the merge commit is this one. So if we do this is just to make doubly sure. If. Nope, that's not at all what I wanted.
05:46:47.045 - 05:48:45.011, Speaker A: If we do this and then this and then this and then it still doesn't really explain oh because it landed after. Okay, so now that's this and now if I do this and then I reset hard to head and then I cherry pick this then now that appliance cleanly and if I rerun the test now they pass. Okay we just. So this is was planned. This was planned but never happened. Maybe worthwhile to make CI run test sh I'll submit a separate PR that does so. Okay, all right, so that one and now how about we CI pliz and we're just gonna.
05:48:45.011 - 05:50:48.965, Speaker A: We're just gonna do it real nice. Nothing, nothing fancy because I don't think they want it. We're going to do Inferno GitHub workflows test into GitHub workflows test on YML. It's only going to have read on contents run on prs and on merge to master cancel run if PR is updated name is just test sh runs on Ubuntu. No strategy. The steps are just to check out there's no sub modules and then that's all. So now let's just open this requires to land first to pass.
05:50:48.965 - 05:51:59.145, Speaker A: And this is run test CI in CI create pull request. All right. Obviously it's not going to run CI, which is fine, but just to check that it actually works if, if I go to my CI please branch actions, general actions. What? Oh, it's because it's not. It is because it is not a. It is not master and it's not a pull request to my branch. So therefore it won't run it.
05:51:59.145 - 05:54:06.215, Speaker A: But in order to just sanity check that it runs and it won't run on my PR there because basically protections. So now just to see that it actually does a thing just to see. Excellent. Okay, great. So now after all of that, what this actually all means is that the change in this pray. Okay, so that's going to have to be rebased on top of this. But in order for all of this stuff to work, what we're going to have to do is take this change and also port this to Flame Graph.
05:54:06.215 - 05:55:43.175, Speaker A: And what that will mean is going to perf search for Java. And here instead of this saying equals Java, it should be starts with Java. And then somewhere further down we need whatever this is which is. Oh, this is Perl. So we have a nice little unless here. But this is going to be. So unless is the same as if not funk I think is what this is doing.
05:55:43.175 - 06:04:38.275, Speaker A: So it's just don't add this suffix if it's already been added and same thing here don't add the suffix if it's already been added But I think it's actually a little more sophisticated than that because this doesn't have a trailing dollar which means which means it doesn't have to be the end of the line. What's interesting is that means that dot star here is completely useless. It serves no purpose if now iron cargo t why collapse jit perf DD stacks All of this is in DD stacks so something changed in DD stacks as well I think what changed indeed DD stacks did something so in mine get log test results perf DD stacks any of them that's just to take period into account so wait but this did change all the counts to be that so and I generate that so oh that's interesting. So this actually means that my change here now is wrong somehow show p Ah yes this one is not supposed to be a 1 perf PL member stack should be this Yep okay so this this actually did find a bunch adjust for adjust for I've completely lost 250 push rev parse head so this needs to do git pull reset hard this CD cargo test finally passes the dam tests so now we can write that this also fixed also port this and then further bump flame graph further bump our flame graph port now if I do git push oh I guess I need to git remote set URL URL work a bit Am I allowed to do this? Because I guess that's what I have to do GitHub.com colon this great which I'm allowed to do because of the because they've enabled maintainer access to the print so now this is all in place that has been ported this is now fixed what do you mean? It's out of date with the base branch Ah right and then merge merge main push fix in took the liberty of updating this PR that required a bunch of semi related changes including bumping our patch on top of flame graph to account for not being merged wow. We've had some doozies of things I was hoping would get way further in my notifications but alas I have to that's I have to end pretty soon I'm hoping to land this first because you know this can now be closed is will be fixed by this close with comment this is a fantastic thing which yeah we'll look at that while the CI runs why not incomplete adder consider div class abc the result of adder on these should be the same What? Not the case. What output do you get? We just pass this directly on to WebDriver, so it's unlikely.
06:04:38.275 - 06:05:21.495, Speaker A: There's an issue in Funtochini here. If you could submit a PR with a new test fails on this, that'd be helpful. Okay, great. Done. I mean, we got through like what, 55 things and not all of them trivial either. I'm gonna go ahead and mark both of these as done because we have them open. I'm also going to mark that as done because that's just a release.
06:05:21.495 - 06:05:41.907, Speaker A: And then we can just run through the we were wondering ones because they're just. Yeah, yeah, yeah. This is just trivial. You can do all of these real fast while this is running. Oh, this is done. Fails with coverage. That's just because of rate limits.
06:05:41.907 - 06:06:12.165, Speaker A: So this one we can now do like this. And I'm going to do a merge commit here. Even though these commits aren't perfect individually, I actually think having some of the exploration here is useful. And then I forget whether Inferno has a changelog file. It does. Okay, so I will update that as well. So, unreleased.
06:06:12.165 - 06:07:41.735, Speaker A: What have we changed? I guess git log 0119. No, 19. Yeah, none of this is really. So the real changes are updated dash map to V6 and then also this PR so take period in account when collapsing stacks. So that's 3:12. Nope, that's 319. And also I guess pointing out that bumping flame graph here also required implementing, which was luckily pretty easy.
06:07:41.735 - 06:08:39.409, Speaker A: So the actual change for that is correctly detect or more reliably detect Java and avoid double annotating in line N. Yeah, and I guess that's also. We'll just mark it as 319 because that has the context for it. Great. That's an unreleased. So then we can merge this. Updating the change like doesn't matter.
06:08:39.409 - 06:08:54.535, Speaker A: So then I'm going to merge this anyway. Oh boy. Okay, merge this right here. Confirm merge. Then we'll go back to main. We'll pull. We will.
06:08:54.535 - 06:09:33.025, Speaker A: This can be. This can go away. This can go away. Now that we're back here, we'll do a release. We'll actually do a cargo update. GC B release 0, 1120 cargo update and also bump. I guess Cargo outdated R Criterion end vlogger and quick xml.
06:09:33.025 - 06:10:08.305, Speaker A: I wonder if this still works. I don't understand. Are there any benchmarks? Oh, it's. That's. Oh no, it is running them. Okay, great. So what's the.
06:10:08.305 - 06:11:57.045, Speaker A: What's the actual Change for criterion? Criterion05 change log MSRV bumped to 164. That's not a problem because we already have 160, so I think that's fine. What was the other one was quick XML to 0:35. That feels like something that's gonna hurt repository 36 even. Oh, I don't want to make that change right now, so I'm going to not do that. The criterion one we can change and end vlogger. We can also change I think update majors and then we'll go up here, make that 20.
06:11:57.045 - 06:13:00.925, Speaker A: Oh, and I guess this is 65 now, right? Because of dash map. Correct MSRV for dash map and bump version. So the quick XML bump seems like a thing we're going to have to do separately. So this is now not just fixed but fixed fixed in there. And then I'll open Mr. For this create pull request. Oh, and this also needs to change log.
06:13:00.925 - 06:13:59.382, Speaker A: This is then 0, 11, 20 released 20, 24, 07, 14. Whoa, I haven't updated the list down here in a while. 15, 16, 17, 18 should really automate this. Yes, I know. 20. Okay, so this is 12 to 13, 13, 14 to 15, 15 to 16, 16 to set. Oops.
06:13:59.382 - 06:15:03.915, Speaker A: 16 to 17, 17, 18, 18 to 19, 19 to 20. And then this is going to be 20 to head. I was using control A to bump and then I'm sure this is going to complain about something, but maybe it's just the coverage. The big question is whether we've accidentally bumped MSRV as well. But I don't think we have. Oh, maybe we have clap and it's MSRV policy. Makes me very sad.
06:15:03.915 - 06:17:37.465, Speaker A: But I think there's a way because it's just Criterion pulling it in V via its CLI stuff. Can I make it not do that? Maybe I cannot. Is there a way for me to avoid the clap? Yeah, can clap be made optional? So we're going to go ahead and get checkout main cargo dot lock. Then we're going to do this. Then we're going to add cargo toml add cargo lock diff cached and then we'll do here, don't bump criterion to 05 until. Until clap is optional to avoid Ms. Excessive MSRV bumps criterion to 04 again for MSRV.
06:17:37.465 - 06:18:47.647, Speaker A: So this is the reason I always run releases through CI is so that they catch things like this. So while that's running, we can do. We were wondering really quick because it's super fast. We just do here brand, branch, bump, push origin. Do I have a bump that I haven't deleted. I have not bump and then we do CD into server Cargo update, we CD into client NPM updates and that way pretty much all of these are going to just resolve themselves. Is it upgrade that also updates the what is it that.
06:18:47.647 - 06:19:30.685, Speaker A: What's the NPM command that also updates package JSON? I guess it doesn't really matter. I don't care that much about updating the the package JSON anyway. Bump all the dependencies at once. It's just because Dependabot has decided that it wants to update each dependency in a separate PR and that is annoying. So instead we're going to just create one and then close all the tickets. Okay. This now is hopefully just coverage.
06:19:30.685 - 06:20:34.713, Speaker A: Oh, to update package Jason, you need to do it manually. That's annoying. Clap45 what I thought that's what I I swear. Okay, so maybe we can actually do this. Maybe we just need to update-p clap precise 4.5.3 I don't actually remember which clap version is okay with which. 65.0
06:20:34.713 - 06:21:51.445, Speaker A: check five four zero envlogger requires rusty 171 people demand newer rusty. That's true. The problem is when I bump the rust, the minimum Rust version of Inferno, that means that the Cargo Flame Graph command will stop working for older versions of Rust if you want a newer version of Cargo Flame Graph. And so it just feels really unfortunate to be locking people who are using older Russie out of newer version of Cargo Flame Graph essentially completely unnecessarily. Right. Like both here for clap and for end vlogger, neither of which it feels like should really care about this stuff. It makes me sad.
06:21:51.445 - 06:24:03.375, Speaker A: Cargo update dash P just sort of binary searching here for what the last four I guess it was 18 upgrade while retaining and then our cargo toml has to change to 71 and that also means our GitHub workflows test. No check has to change here 71 for N vlogger and clap bumps for bumps to maintain MSRV of 171.0 and there's not really a great explanation of why exactly that. I guess. Now to 1.71.0 for nvlogger and dash map and clap and dash map mention MSRV bump. Okay, so that's going to run in the background and then our bump.
06:24:03.375 - 06:24:48.125, Speaker A: All dependencies, all checks have passed. Merge. Confirm merge. And now at least in theory, Dependabot is going to go and close all of these on our behalf and then I don't have to look at them. Dependabot is yeah first one closed. Boom. Second one closed.
06:24:48.125 - 06:25:13.995, Speaker A: Boom. Go away closed. Go away closed. Go away closed. Nice. All right. Come on.
06:25:13.995 - 06:25:48.565, Speaker A: Little CI released in 0. 11.20 is the message that is going to be posted here in a second. Same thing for I guess this guy coverage just rate limiting. Minimum version stays at 171 windows thing. I don't want to wait for this. I'm just going to squash because the details aren't important.
06:25:48.565 - 06:27:03.635, Speaker A: Confirm delete branch checkout main pull tag push dev miner inferno tag push tags and get push and Gargo publish and yeah, we are getting some warnings here. Ignoring test. Ignoring test collapse guess as tests collapse. Guess is not included in the published. Interesting. All right. But it does run those tests, right? If I run Cargo T.
06:27:03.635 - 06:27:28.273, Speaker A: Yeah, it does. Okay. There are some warnings here that I would love to fix, but I don't doesn't matter enough to deal with right now. I think if someone wants to do a PR that makes this. Makes Inferno not have warnings anymore, that would make me happy. It's a call for action. Great.
06:27:28.273 - 06:27:55.595, Speaker A: So this is now out. This is now out. Close with comment. This one's done. This one's done. This one's done. It's not quite happy enough here because.
06:27:55.595 - 06:28:19.215, Speaker A: Because in server we have some things. And also this. It's not tracking. It hasn't. Rebase this again, I guess. Rebase please. Same thing here.
06:28:19.215 - 06:28:59.275, Speaker A: Dependabot rebase please. And then these are major version bumps and I'm curious as to why. Lambda HTTP lambda HTTP. What is the actual bump from 0.11 to 012012. Okay, well that doesn't seem very important. Cargo Toml let's just.0.
06:28:59.275 - 06:30:13.015, Speaker A: What am I doing? There we go. Does that just kind of work be interesting to see Maybe. Great. Commit bump pull request. What the bump create pull request. Bump lambda create pull request. And then these have now been closed.
06:30:13.015 - 06:30:53.409, Speaker A: Great. This also needs to be rebased for dependent bot to realize it can be closed. These should be good. Once this lands, I'm guessing there's no further update here. It's interesting though. I'd be curious to see whether this actually. What changed here? Time serialization, the runtime.
06:30:53.409 - 06:31:19.815, Speaker A: Like none of this actually seems like it's a breaking change, so I'm inclined to just see that. That's fine. Prettier is now closed. Great. And then once this is done, we're done. And this I have to deal with separately. Okay.
06:31:19.815 - 06:31:41.955, Speaker A: I think. I think we're now way over time and it's. It's time to finish this up. I'll click the merge button for this and end the stream. I hope that was useful. There was. We spent a lot of time on basically two of the two of the PRs took like 80% of the time if not more, but we managed to catch up a decent amount.
06:31:41.955 - 06:32:13.175, Speaker A: And I hope that was useful to watch and that you got some insight into the work that goes on behind the scenes for this stuff. And maybe you learned some rustlings too. That would be nice. Thank you all for hanging out with me. It's more fun to do this when I'm and I'll see you all whenever I end up doing the next stream. I've had a lot of requests for decrusted streams, a lot for Crust of Rust. I have a couple of other stream ideas and mine included some impulse streams.
06:32:13.175 - 06:32:27.135, Speaker A: We'll see what ends up happening next. But at least now I'm slightly more caught up than I was. Oh, that's been merged. This is done. Close that. And this is for another day. Thank you all and I'll see you next video.
