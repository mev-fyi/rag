00:00:00.280 - 00:00:46.825, Speaker A: Hi folks, welcome to another stream. This is going to be an open source maintenance stream again. You know, as I mentioned in the previous one, given that I have been moving for the past few months and haven't really been keeping up with my GitHub notifications, there are now a lot of them that I sort of need to get to just because I maintain a bunch of open source things and I can't just leave them on the ground. If you remember, last time we started with like 120ish notifications and we got it down to I think 91. And now a week later we are back up to 126 unread. So we are back up to higher than we were when we started last time. Now, to be fair, the number is actually not that bad.
00:00:46.825 - 00:01:26.595, Speaker A: It's not as though things get worse every week. At least that's not usually the case. You'll see on the left here a lot of these notifications are from rustling rust, e.g. rFCs, unsafe code guidelines. So these are usually issues where I'm subscribed to an issue because I'm just sort of following it along and it's not really work that's required by me. Before the previous stream I had already sort of trimmed those out so that only the ones that had work remaining were in my notification list. So realistically, this is probably More something like 100 notifications maybe, which means that we're a little up from last week, where we're still down overall.
00:01:26.595 - 00:02:16.205, Speaker A: So we did actually make progress, I believe so. I think it's not as bad as it might seem. We're just going to dive into it for context here. If this is the first time you join one of these streams, or if the first time you're watching one of these recordings, this is me just doing regular open source maintenance work. We're going to be bouncing around between a lot of different projects. Basically, I'm going to be walking my GitHub notifications list from the oldest to the newest and we'll see how far we get. I'll try to give a little bit of an intro to what each project is as we get into the first issue, but this is not intended to be a sort of slow guide to each of them, so I'm going to be moving fairly quickly because otherwise we're not going to make enough progress on these.
00:02:16.205 - 00:03:05.101, Speaker A: If you're curious about any of these projects, then feel free to ask in chat. Some other people might have contacts there. Some of them already have streams elsewhere that you can look up and otherwise just browse the repo yourself and if you want to contribute, then feel free to contribute. It might add to my list of notifications initially, but one of the reasons why I'm doing these streams is my hope is that it might let more of you get some insight into the open source maintenance side of things and maybe be interested in that yourself. And so while initially sending contributions to this repo gives me more work over time, I'm hoping that it'll let you learn a project well enough that I can also make you a maintainer so that the workload on me goes down. So it's sort of a. I expect more work in the short term, but hopefully less work in the long term.
00:03:05.101 - 00:03:37.905, Speaker A: We'll see whether that pans out. Okay, let's just go through these one at a time and you'll see. I open these in batches. I don't really know why I do that. It's just something I've gotten used to. This one is actually not what I'm going to do because someone did this after last week's stream. So we found this one, last week's stream, and I mentioned that there are a couple of changes we might actually need to make to the code to make this land because you see a bunch of the CI tests tests fail.
00:03:37.905 - 00:05:06.625, Speaker A: So someone submitted a PR for this that technically is now at the front of my GitHub notifications. But I figured given that it's here unless it's eliminated signature for arbitrary change. No, let's see more ways for an arbitrary U size. Okay, so this sounds like it's actually kind of unclear what the right fix is. So there's a breaking change in Quick Check, which is this like property testing library that we're using for a tone. I think I'm going to leave this conversation to just sort of hang for now. So we're not going to deal with this one at the moment because it seems like there's some unresolved issues in Quick Check itself.
00:05:06.625 - 00:06:10.505, Speaker A: Static unique domains. Okay, this is in haphazard. This is our implementation of hazard pointers in Rust. So we ported the hazard pointer implementation from Facebook's C Folly library into Rust. Makes it impossible to sign such a domain to a static variable. Okay, so in hazard pointers, one of the ways in which hazard pointers work is that you, you have things that can be pointed to and then you have hazard pointers, which are pointers with sort of guards to those objects. And one of the things we want to guard against is someone using hazard pointer value that's allocated in this sort of group of hazard pointers and a protected pointer from this other instance of hazard pointers and that you start trying to guard this value with this pointer.
00:06:10.505 - 00:07:17.065, Speaker A: That won't work because all of the logic only considers the hazard pointers within the same instance. And so the way that we fix this in haphazard is we have this notion of domains which are basically there's a unique type for each instance of hazard pointer, so you can't accidentally mess them up with each other. And what this person is asking for is a way to declare a static domain so that they can have a domain of hazard pointers they can use throughout their application. But the way that we did unique domains, I believe was by using a lifetime. And lifetimes are always unique in Rust, but that means that it can't be a static. So what they're proposing is that instead of having this unique domain function which is going to return a value of a unique type, whether we can instead have a. A macro that you can pass in the name of a static and it will construct a unique type for you.
00:07:17.065 - 00:08:25.547, Speaker A: And this person is basically saying, would you accept a PR that implements something like this? Great idea. I think there are some details to be worked out, but probably easiest to do so via a PR so we can talk about the actual code. I do will probably want to have two different macros, one for producing a value and one for producing a type. But that we can also take in the pr. Do you think contributing to open source software is a good way to learn Rust? Yeah, I think so. I think the trick, as always, is to just start. Start writing Rust code.
00:08:25.547 - 00:08:43.445, Speaker A: It doesn't really matter what project you're working on. It doesn't really matter whether it's open source, but open source is most accessible to you. Just. Just work on something you think is interesting. Great. Boom. This is to my dot file like my config repo.
00:08:43.445 - 00:10:00.331, Speaker A: The behavior is likely different from Excel, so it copies the entire line to system Clipboard. If this is selection, it copies that. This is a part of my VIM config where I have a keyboard shortcut for pasting whatever is in the system clipboard into vim and a keyboard shortcut for copying what is copying what is in the VIM selection into the system clipboard. Because that integration isn't always great by default and someone's commenting that this is sort of wrong. Ah, okay. So the previous one that I had will copy the entire document. So if I press leader P.
00:10:00.331 - 00:11:01.911, Speaker A: So in my Case space P in vim, then it will take everything that's in the clipboard and sort of dump it to the current location in the file. And if I press leader C, so space C, it will copy the entire file into the system clipboard. And what this change does is it makes it so that the thing that gets copied is whatever has been selected visually. So in Vim, you can do like a visual selection using the V key and it will only copy the lines that have been selected, rather than necessarily the entire file. I'm not opposed to that. The only challenge here is that this relies on VIM actually working correctly with the system clipboard, which usually works like this syntax here of double quote plus. Double quote means choose which register to do the yank into.
00:11:01.911 - 00:11:26.633, Speaker A: And plus is the system clipboard register. So this will yank into the system clipboard register. I'm not opposed to this, but I have to. This one I have to feel out a little bit. So I'm not going to deal with that now because this affects my personal editor config and I don't want to test that right now. Formatting to format. Oh, all right.
00:11:26.633 - 00:11:55.983, Speaker A: That seems fine, if that's how it's supposed to work. This has been broken for a long time. I believe that. Yeah, a bunch of people make PRs or file issues for my file configs, which is very handy. Bump prettier. This is for. We were wondering.
00:11:55.983 - 00:12:19.395, Speaker A: So this is for the Q and A site that I built. So it lets people ask questions for streams both before and during. And this is just dependable. Wanting to update the dependency. This bothers me so much. This is not a dependable issue. But the fact that in JavaScript, like the default behavior is that you actually want to bump the lowest version here.
00:12:19.395 - 00:13:00.827, Speaker A: Like I wanted to always pick the maximal version, not the minimal version. Like this bump should only really need to bump the lock file. But fine, I'm okay with this. I think you need to explicitly set async equals false in the VIM LSP settings. Why? All right, next one. This is a change to Icebreaker. Oh, I brought this ages ago.
00:13:00.827 - 00:13:31.095, Speaker A: This is a way for. This is like a little, little web widget you can run that lets student ask questions. Students ask questions anonymously during lectures. So the idea is that like the TAs of the class have Icebreaker open. Have like the admin interface of Icebreaker open. The students can open the student URL and then submit questions and then the TA see them and they can ask the question on behalf of the student. Because A lot of students are hesitant to raise their hand in class.
00:13:31.095 - 00:13:47.095, Speaker A: They don't want to stick out. They don't want to seem stupid. Whatever it might be. It worked really well when we used it in a couple of classes at mit. And it's handy because you can just spin it up for a class and then spin it down again. There's no persistent state to it. It really is for just any single lecture.
00:13:47.095 - 00:14:09.665, Speaker A: This seems fine. I don't mind doing this update. I don't. This isn't something that I actively maintain, but it just kind of works. There's nothing to really maintain here. Else the formatter might run while you edit text, which can lead to problems. Well, notice that this one was specifically when you press a keyboard shortcut, it does formatting.
00:14:09.665 - 00:14:53.190, Speaker A: Okay, there's one more here. Currently, C Bind gen seems to include dev dependencies. This is just something that I've been watching in the background. I don't think that matters. Okay, so we got through one little page. Let's go one page back and open a bunch more. OpenSSH maybe important to have.
00:14:53.190 - 00:15:31.335, Speaker A: Okay, so OpenSSH is this. It's a binding to talk to an OpenSSH client in order to talk to a server. So it doesn't implement the SSH protocol. It lets OpenSSH handle that. It knows how to talk the protocol to a local instance of OpenSSH, which then connects to the server on your behalf through something known as the muxing client or the muxing protocol. This is one where nobody zoo has been basically the primary maintainer of this. Now, for a while, it may be important to have the SSH server perform DNS resolution, as the client often does not use the same DNS server as the server.
00:15:31.335 - 00:16:32.619, Speaker A: We use OpenSH to set up SSH tunnel via Bastion host. Our customers often want to establish a tunnel to connect to a host whose name is not resolvable on the client side of the tunnel. All right, so this has already been approved. Let me just look over it real quick. Okay, so this is basically making it so that when we set up forwarding, we no longer require that. What's given to us is a socket adder, which is a resolved host name. Like it's just an IP import and instead it can be a string.
00:16:32.619 - 00:17:48.045, Speaker A: Like it can actually be just a path, not path, a host name or a DNS name that then can be passed verbatim to the server and then the server does the resolution. But this change, though, isn't okay because this changes the public API of socket, like all of the internal parts here. Are fine, but this is a problem. This changes the public API and so would end up being a major. Requiring a major, major version bump. And I don't think we have one scheduled for OpenSSH. Yeah, major version bump.
00:17:48.045 - 00:18:46.425, Speaker A: Would you mind making this a separate constructor instead? We have to be a little careful about these kinds of changes. Maybe we should integrate. There's a crate called. Called Cargo Public API. Is that the name of it? So Cargo Public API. I thought there was another one too, because there's Cargo public API and then there's cargo semver checks. They both do sort of the same thing, which is you check whether the del.
00:18:46.425 - 00:19:31.019, Speaker A: Whether a PR introduces breaking changes, essentially like whether the public API of the crate changed. Yeah. So let's see how this compares with Poco Republic AI uses rustock focuses more on API diffing and not API linting. Yeah, I mean, I don't mind too much. Let's do. Maybe we should add a CI step that invokes or this one to catch these things automatically. At least in most cases.
00:19:31.019 - 00:20:07.621, Speaker A: These tools aren't perfect. They don't catch every single one. It is raining outside. Yeah. So this is something that we really need here. And this also means we can't do a release of the OpenSSH crate until this is fixed. So I'll do.
00:20:07.621 - 00:20:24.985, Speaker A: Mark is unread left. Right. Okay. So this is in a. Oh, wait. There was a question. As an author and a maintainer for some of these repositories, how do you go around remembering parts of the code in the prs? Because I struggle in two and three repositories I maintain on the company's git.
00:20:24.985 - 00:20:53.639, Speaker A: I don't know how. I think the way that I remember these is that I have worked on all of them so much. But in some cases also pattern matching. Like, it's not that I remember all the code of OpenSSH. It's more that in this case, this changed the type signature of a function that's marked pub on a type that's marked pub. And that just is a breaking change. So I don't need to know extensively how this API is structured.
00:20:53.639 - 00:21:51.341, Speaker A: I just. I can tell that that is a breaking change because it's almost the definition of a breaking change. The type signature of a public method on a public type changed. So it's not so much that I remember the entire code base. Has there ever been a GitHub review feature that you really missed? Yeah, GitHub is really bad at what? Once you get to reviews that take like multiple iterations where some comments from a previous iteration might not be resolved, but the person still pushed a change to that line. Or if if I I leave a comment, the contributor makes a change, thinks they have fixed the thing I asked for, so they mark my comment as resolved, but I disagree with their fix. I won't know because it won't be visible to me anymore, the comment will be collapsed.
00:21:51.341 - 00:23:14.557, Speaker A: So I have to remember to go through and check all the comments that have been closed whether I agree that they should be resolved and so that the sort of it's not that I don't trust the contributor, it's that I as the maintainer, want final say as to whether something has been resolved or not. Okay, so this is in left right. This is an atomic primitive for a sort of concurrency primitive that works a little bit like a reader writer lock, except it's heavily optimized for cases where you have a lot of readers and where you're willing to duplicate the thing that's behind the lock in order to improve performance Performance Recoverable errors from fallible Operation Currently, there's no channel through which errors can be recovered that doesn't require a significant workaround. Knowledge of the success or failure of an operation is key to certain systems. It'd be nice to have a method on the right handle that would return the result of the operation instead of an exclusive reference to self. Such a method could be called try appendix. Ideally, this return type would be any type that was a tritrate and that type would just be returned by the methods.
00:23:14.557 - 00:24:06.035, Speaker A: However, that trait is still nightly only. The try append method on the right handle would then only be available if the operation given to it implements the trait. Moreover, blanket implementation for absorb can be provided. Okay, so in this library, the way that you structure writes is you don't perform writes, you queue writes. You queue them as operations to the underlying data structure. And the reason you have to do that is imagine you have a a left right over hashmap. So you actually keep two copies of the hashmap, one that the writer writes to and one that the readers read from.
00:24:06.035 - 00:25:14.215, Speaker A: When I do a write, I apply the write to this map, but eventually when the two get swapped, then I still need to apply that write to this map as well. And the way that works in practice is that you enqueue an operation an operational. You enqueue an operation into a log and that operation is going to be applied to both maps. And so these methods that they refer to here of absorbed first and there's also a absorb Second those absorb methods are absorbed first is the first time when the operation is absorbed into the first map and absorb second is the operation is being absorbed into the second map. And what they're proposing here is that it would be nice if there was a way for operations to fail and for that failure to be communicated back to the writer. So currently the idea is that if you stick an operation in the log, it's just going to be applied to both things, the left and the right. And there's an assumption that that application cannot fail.
00:25:14.215 - 00:26:31.071, Speaker A: Let's see. And so they're proposing Try Absorb first and sync with. I'm not sure I follow though, because how would you actually return this to the right? So this API is the API for implementing the data structure, but it's not what the writer sees. So if we go to left right, then right handle. So you see, the operations that you have here are publish, which is basically swap the two maps so that readers get access to the more updated version, and Flush, which essentially does the same thing. And then there's append, which is append this operation to the log. And maybe I'm misremembering, but yeah, all append does is it just extends the operational log.
00:26:31.071 - 00:27:08.769, Speaker A: It does nothing else. It doesn't apply the operation at all. That applying the operation doesn't happen until you call publish, I believe. Yeah, so it's only down. It's only down here that we actually apply the operations, which is in publish and I believe flush just calls publish. Is that right? Yeah, flushes like only publish if there are any pending operations. So it would have to be published.
00:27:08.769 - 00:27:48.495, Speaker A: That's made fallible. So try append wouldn't really work. I also don't think we would need try drop. So if you look at the requirements here, absorb is like the data structure needs to be able to absorb the operations from the operation operational log. And so this is the trait that makes the data structure claim that it can do so. So it needs to be able to absorb the operation the first time. If it is the first time the operation is applied, it needs to support it and it needs to be able to absorb it if it's the second time the operation is applied.
00:27:48.495 - 00:29:36.165, Speaker A: The only difference between these is the first time you just given a mutable reference to the operation and the second time you're given the operation itself because you don't need to keep it in the log anymore. So this is the SP split of those is an optimization and then sync with is an optimization for the case where you do the first ever swap where one map for example, is empty and the other one has been filled with a bunch of things, then you don't actually need to apply the operational log, you can just clone the first one instead and save yourself potentially a bunch of expensive operations. So that's what sync with does and then drop first and drop second. These just drop the data structure and I don't think that needs to be try based. But that's why they're proposing this fallible absorb here. Can you give a motivating use case for when operations might fail? I'm not opposed to this, but I want to make sure we land this complexity only if we think the solution actually addresses a real use case. Also, I think the fallibility needs to be exposed to writers in publish I.
00:29:36.165 - 00:31:01.175, Speaker A: E. Try Publish since append only appends to the log and does not and cannot apply the operation at all. At that point I wonder whether the phallib the error is even still useful. Because what? Because it might happen to some operation that's in the middle of the operational log which then leaves us in a weird middle state more in the weeds. I don't think think we need try drop since drop shouldn't be fallible. Comment yeah, left right is kind of similar to rcu. It's a little bit different in that it only ever keeps two copies and it always keeps exactly two copies, but it is similar.
00:31:01.175 - 00:32:03.945, Speaker A: Okay, done testing of client and server side Rust interaction. This is in Fontaccini which is a browser orchestration library. So similar to Selenium it uses webdriver to do this automation. I have a Tauri app where the front end is written using Vas Bind Gen and buzzm pack. This means I have Rust both on the front end and on the back end for best performance of transmission of large arrays have used this front end. Yeah, I don't think this applies. So basically they're asking whether they can use Fontaccini for communication between a web server written in Rust and a web front end written in Rust that communicate over webassembly.
00:32:03.945 - 00:33:04.395, Speaker A: That's not really what this is for though. Fantoni is for making browsers do things I suppose if it's a Tauri app, so it's like a web based app, so the browser is also under your control and so you can use Fontacini to get that built in or embedded browser to do things. And I guess what they're looking for is whether they can make the encoding of things. If you call JavaScript via Fontaccini to be more efficient, but I don't really think so. Steve Pride is someone who maintains basically more user friendly bindings on top of Fontacini. So Fontacini is like bindings of the low level web driver protocol. And so he has a lot of experience with the sort of basically this whole space of solutions too.
00:33:04.395 - 00:33:49.619, Speaker A: For Tower, this has been connecting the Chrome driver to the web view via the Chrome debugging port and then use Fontaine to send commands to the Chrome driver. Yeah. So you can make the embedded browser do things the support doesn't implement, trying to interact with the Rust parts of Tauri or the front of code itself and doesn't really do that. Yeah. So the back end you could have your backend use fantastic. To tell the front end to do something, but it doesn't really enable you to do communication between the front end and the back end. At least that's not what it's intended for.
00:33:49.619 - 00:34:21.959, Speaker A: You could use it that way. Like you could, you could orchestrate the browser in the sense of sending it some JavaScript to run that the backend dictated. But that feels very roundabout. Like, yeah, I think this is the right question to be asking and then we'll see if we hear back from them. So I'm not going to add anything to this issue. This is where it's really nice to have other people also contributing to these. Okay, this is an HDR histogram.
00:34:21.959 - 00:35:05.927, Speaker A: So HDR histogram is an implementation of a pretty cool way to store histograms. So the idea being that if you have you're collecting lots and lots of data and you want to keep a history, let's say you're recording metrics like latency of requests and you actually want to store the full distribution of requests. So you don't want to store the average request time, like the average latency for processing a request. You actually want to store. Like if one page load took a second and one page load took a millisecond, you kind of want to record both. And so you want to keep a sort of histogram of how long did it take to process requests. Then the naive way is to store every value, but that ends up taking a lot of space.
00:35:05.927 - 00:35:53.807, Speaker A: HDR histogram is this really neat way to compress the representation of that histogram. So you store semi accurate data, but you preserve a lot of the shape of that histogram for querying later. So it's a really neat way to do that. And this is the implementation of that in Rust error when iterating over Histogram with only zeros recorded returns an empty iterator. Looks like the problem is with the iterator as count at returns the right number. Okay, so you create a histogram, you record a zero and then you want to iterate over the recorded values. Oh, that's interesting.
00:35:53.807 - 00:36:46.055, Speaker A: So this happens in core mod. Oops, Lib. I think it's just lib. The lib file is ginormous here. Actually no, it's going to be in the iterator implementation iter and I think we have a like a shared implementation of this. But let's look at recorded. Yeah.
00:36:46.055 - 00:37:40.625, Speaker A: Okay, so there are two parts to iterators here. All the iterators are implemented using this picky iterator trait. And the idea being that a picky iterator is one that might only walk a subset of the values of the histogram. As you have to implement this pick method to say do I want this bin to be counted or not? And here it looks like we select bins where the count is not equal to zero and we haven't visited that index yet. Visited start as none. Okay, so we should return some already. So this feels like it would yield bucket zero if the count is non zero for it.
00:37:40.625 - 00:38:19.151, Speaker A: So that means it's probably this picky iterator thing that's wrong. So it starts at the beginning of iteration. Total Count index is 0. Counted index is 0. Current index is 0. Okay, next ended starts as false self currentindex equals his distinct values. So I would assume that distinct values here should be one.
00:38:19.151 - 00:40:18.915, Speaker A: Although let's go back and look. So basically it tries to detect whether it's already walked all the values. Distinct values and distinct values is self counts.len and this is the thing that actually records a value mute at value mute at index for value, index for value. That just gets you the index at. So something's off here because it doesn't look like this Updates counts that updates total count, self counts set counted index resize and so what's the code they give? They create a new one. I think we do call restat.
00:40:18.915 - 00:40:59.625, Speaker A: I think it's self dot counts that's off somewhere. But let's keep reading and see if there's anything else. So so if distinct values is 0, if that ends up being 00 here, then current index is equal to 0 is going to be true when you first start out and therefore end it is going to be set to true. It's going to return none. So that might be true. Have we already picked the index with the last non0 count of the Instagram the Instagram the histogram. If the last picked index is greater than or equal to the max value index.
00:40:59.625 - 00:41:44.505, Speaker A: Oh, it might actually be this one. So the last picked index is greater than or equal to the max value index. The max value index is going to be zero because it's going to be the index of the bucket that has zero in it. Then it's going to call self picker more which in the case of recorded always yields false. So I think actually this implementation is wrong. I think this should be self.visited.is. i think this needs to compare whether self visited is equal to that index.
00:41:44.505 - 00:44:06.675, Speaker A: Yeah, I'm pretty sure that's what's going on. So it's the more implementation here that's wrong. All right, let's see here. Good catch. I think what's going on here is that 194 we hit down to line 28 and recorded iteration always returns false from fnmore. What it probably needs to do instead is return self visited is equal to sum index passed in two more so that it will iterate over that first bin at least once. Could you test out that change and replace port back? If that doesn't work? If that doesn't work, then could you instrument picky iterator next link to that one.
00:44:06.675 - 00:45:42.761, Speaker A: There's some debug to see if any to see where it exits with none and what values cause the conditions that lead to that outcome. Great. Let's see. There's a comment mentioning zeros and some original implementation on one of the methods. Most of those values, especially towards the end, will be zeros, which the original historic histogram doesn't yield. I don't think this is relevant. It is true that our implementation diverges, but I think our approach is still correct, which is iterate until we reach the total count and then iterate only until more returns false.
00:45:42.761 - 00:46:43.829, Speaker A: So I think it's just the implementation of more here that's wrong. Um, how come you don't just text test that fix yourself? So the reason I don't test this fix myself and I talked about this in the previous stream too, is because for two reasons. One of them, as you saw, we have a lot of notifications to get through and this is something that this person can relatively easily do themselves. And so I would rather them do that in parallel with me looking at the various other notifications I have that are blocking on me specifically like this issue, someone else can make progress on the other issues. Probably require my input for someone to make progress on them. So the idea here being I want to spend my time to unblock other people. The second reason is if I can get this people to experiment with a fix, potentially provide a PR that's a pathway to them becoming a contributor and maybe even a maintainer down the line, if I just push the fix for them, they're not going to learn anything thing.
00:46:43.829 - 00:48:11.891, Speaker A: And so this is a way to enable them to learn and potentially contribute and potentially aid in the maintenance of this library going forward. It's also true that this as someone mentioned in chat, this makes me think that they have use cases like they have something bigger than this test case they give me where this fails and so they can hopefully then test their fix on hopefully they can test their fix on the real data set they have as well, and not just in this limited setting. Great done. This is in the implementation of the IMAP protocol Refactor Tag assertions to Fatal error this is a long one. Is there anything I can do to help this TPR be merged? Let's see. So who is the original? I think my comments above still stand, so it's really just a. Just a.
00:48:11.891 - 00:49:29.615, Speaker A: I can't type. Just a matter of making those changes. If is still interested in picking that up and that's probably easiest. But if they're otherwise preoccupied or no longer need this, then feel free to open a new PR that builds on this one so we can land it. This is also fairly common actually in the open source world of someone makes a contribution, we iterate a couple of times where I leave some comments and they make some modifications and then eventually they switch jobs or they finish studying or they go on vacation or they start a new job or they just no longer use this crate, whatever it might be. And so the PR is left in this half state where it's like mostly done, but there are a couple of outstanding issues. And then this was August of last year and then a year later someone goes what happened to this? I need this feature or I need this bug fixed.
00:49:29.615 - 00:50:25.287, Speaker A: Where do we go from here? And the options are either the original author continues to make progress, they just forgot about this which happens, or this new person should just pick up the PR from where it was and just make progress from there. And so that's what I'm trying to spell out here that like I don't have a preference for who does this. If the original person is still interested, they're usually the best person to drive it to completion. Although a year later they might have Lost all that context. But I want to encourage anyone to make progress so that we don't end up stuck in this sort of back and forth dance. All right, done. But this is another example of something where it kind of requires me to come in as the maintainer and make a statement as opposed to spending time trying to fix that bug in HDR histogram, where I think that person who reported it can make progress, given the comment I left.
00:50:25.287 - 00:51:02.507, Speaker A: So it's more about unblocking other people. Another IMAP issue, idle concurrency. I'm in a situation where client is sent so the idle protocol in IMAP is you connect to the server and you send this command called idle. And what that will do is you're not able to send any more commands and instead you tell the server write to me. Like write a message in my direction when something changes in the mailbox. Like if a new email has been added or something's been marked as deleted or whatever. And it's basically a way for you to do push instead of pulling.
00:51:02.507 - 00:51:43.255, Speaker A: So the other way to do this is you like send a message to check the inbox every, you know, whatever, 30 seconds. If you send idle, the servers instead are going to actively push a message to you when something has changed. It's a better use of the medium in a sense. I'm in a situation where a client is sending emails continuously by smdp. My other IMAP client connects, fetches all messages from the inbox, and starts idling for more messages. Nevertheless, if just an email is received between my program does fetch and idle, the notification of more messages is lost forever. Yeah, this is a classic race condition, right? You do a fetch and then you do an idle, but someone sends something right in between.
00:51:43.255 - 00:52:27.055, Speaker A: I do not know if there's a workaround for this issue. The only idea that comes to mind is to open two IMAP connections, one for idling and other for fetching. But I understand the protocols intended to work in all scenarios using one connection. That's interesting. I wonder what mail clients normally do from this. This feels like something Stack overflow might have an answer to IMAP Idle race condition fetch. This is another thing you get really good at is the right way to Google for things.
00:52:27.055 - 00:53:21.035, Speaker A: Sorry for the bright screen. A bunch of code that does IMAP. Search idle done. Search idle done. Is it possible that some messages arrive between the search and the idle and will only be received? A properly implemented server will notify of the new messages as soon as you start idle if it hasn't already notified you about Them in response to some other command. Interesting. That doesn't seem relevant.
00:53:21.035 - 00:54:34.945, Speaker A: Doesn't seem relevant. That doesn't seem relevant because it really is this one. I'm surprised that there's no racing while starting up the idol. Yeah, this. This is interesting. This is in canine Male canine mail this revision. Let me see if we can dig up this.
00:54:34.945 - 00:55:33.945, Speaker A: I wonder how we would even find this. Like this feels like a revision number that's used in. That's used in like SVN. Like when is this from? 2009. Hmm. Finding this might actually be frustrating because I think canine mail, which is a. It's a mail client for Android that.
00:55:33.945 - 00:56:28.145, Speaker A: I don't know if their git history goes that far back. 0x right? Was this. Where was this email from? Did they say trunk 2.0? Okay, so let's find the tag for 2.0 and then look at the history of 2.0. How on earth are we even going to find revision 1012 a limited race condition? Aha. That was entirely by luck.
00:56:28.145 - 00:57:18.265, Speaker A: Cause multiple connections to Idle on the same folder simultaneously. No, that seems different. What if we just search for race condition idle for commits? Yeah, it's definitely that commit, but this feels probably unrelated. That's too bad. This might just be like a fundamental race condition with imap. So curious whether there are any. Even the spec doesn't really talk about the race conditions here.
00:57:18.265 - 00:58:46.525, Speaker A: Yeah, it's hmm, interesting. I think what I'll do here is. That's a great question and unfortunately not one I think I have a great answer to. Neither does the Internet it seems. I think this may just be a fundamental limitation with Idle. The stack overflow answer a linked suggests that servers will eliminate this race condition on your behalf, but I don't know how true that is. In practice at least I couldn't find any references to it.
00:58:46.525 - 00:59:46.865, Speaker A: The son question also suggested maybe using two connections, so you may have to do something like that. Unfortunately, if you do come up with a good solution, please do. Please post it back here. Especially if there's something we can do in the IMAP crate to make it easier to work with. Comment? I've read the IMAP spec a bunch of times and I'm pretty sure it doesn't talk about this. There is. I mean, we can look at the IMAP revision 2 spec, which might talk about this.
00:59:46.865 - 01:00:45.673, Speaker A: The EDL command untagged. Yeah, there's no. There's no discussion of this unfortunately in the spec for Idle. That's unfortunate. IMAP isn't exactly known for being well thought Out. Yeah, you're not entirely wrong. Tracing timing.
01:00:45.673 - 01:01:49.611, Speaker A: Okay, so this is a subscriber I wrote for the tracing ecosystem, where in tracing you can emit spans, which are basically a way to group events or to relate events. So span, you can start a span and then the events that you emit within that span know that they belong to the same span. And so in the subscriber, it's possible to sort of essentially query about information for the span from a given event within that span. So you can imagine a span being something like a request. And that way any event that you log within the span of that request is associated with that span, is associated with that request. And you can do things like look up fields of that span of that request when you're printing an event within that request. Tracing timing utilizes the same structure and computes a histogram of inter event timings.
01:01:49.611 - 01:03:14.243, Speaker A: So imagine that handling any given request does like, I don't know, receive, parse, generate, response, serialize, or send, right? Those are the four events that we log within spam. That's a request. What tracing timing will do, it will record the time between receive and parse, between parse and generate response, and between generate, response and send within each span, and then give you the histogram of those inter event timings across all requests, across all spans. And so it's a pretty handy way to like figure out which parts of your event processing may be slow. So here, for example, like if we have here a request span and inside of that span, we do fast and we do slow, right? And we do a lot of work in between fast and slow, and we do a little bit of work before fast. Then if you run this with tracing timing, it'll give you these histograms to show that the fast event usually has a latency of, let's say, about 175 microseconds with this kind of distribution. And the slow work has an average of or a median of like 600 microseconds with this kind of distribution.
01:03:14.243 - 01:04:02.915, Speaker A: So just it's a very handy way to get a quick overview of where your request processing or just general where your processing is spending its time without having to put in extra bookkeeping and stuff for that information. Like it can just reuse your existing tracing annotations. And this issue is basically someone observing that it's a little bit awkward to use, which is true. Like it relies on multithreaded recording to HDR histograms. And multithreading recording is just complicated. And the multithreaded recording API for HDI histogram is also a little bit painful, which is my fault, but it's because it makes it really performant. But it does make the API awkward.
01:04:02.915 - 01:05:44.155, Speaker A: And the argument here is that it's just like easy to get this wrong, which is totally true. Yeah, it's like easy to accidentally deadlock, for example. Yeah, this is just help for this person. Yeah, it is a very low level API. Like the things you have to do is you have to create this builder of histograms and then you have to create a dispatcher for it. And then after all the operations have happened, you need to call this like with histograms things, which extracts the histogram from the tracing state and then you need to actually print the histogram, which doesn't happen by default. Yeah, and what they're proposing here is, for example, it'd be nice if this type just implemented debug for you and would just do something reasonable.
01:05:44.155 - 01:07:21.531, Speaker A: Sorry for the silence here. Timing is a bit on the back burner at the moment. I would love to improve the starting experience, or rather I would love for the starting experience to be better though have limited time at the moment to make that happen myself, sadly. If any of you want to take a stab at improving the at adding a debug, an impulse debug, extending the examples and maybe even making the interface harder to get wrong, especially with regards to deadlocks, which are a pain, I would be happy to take a look at some PRs. So yeah, I wrote a comment. What I wrote here, and this is sadly very common, I maintain a lot of libraries at this point and not all of them I'm actively using myself. And I unfortunately don't have the time to actively be doing feature development for all of them.
01:07:21.531 - 01:08:21.653, Speaker A: I wish I did. And so what I wrote here is a fairly common sentiment is that sorry for the silence, it's on the back burner. I would love for the starting experience to be better, though I have limited time at the moment to make that happen myself. But if anyone wants to take a stab at adding impldebugs, standing examples, or maybe making the interface harder to get wrong, especially with regards to deadlock, then I would be happy to look at some PRs because in general I know that for these libraries it's probably untenable for someone to just take it over. I would want them to submit some PRs where I can guide them through the implementation, guide them into how to make it better before I sort of give them the keys to the kingdom. So I do want to spend time on reviewing PRs, but actually implementing new features here is not something I end up with enough time for maybe one day if I end up doing this full time. But even so, it's a lot better if people who are actively using this themselves are the maintainers.
01:08:21.653 - 01:08:56.419, Speaker A: In this case, I'm not really using tracing timing myself very much anymore. Not because I don't like it, but because I was primarily using it for Noria and I'm not really building a system like Noria at the moment. And so I just don't really need it. And so that's why I would love for people to contribute who are actively using the project. Tracing time is also really cool. Like it's both a cool. It's a cool tool, but the implementation is also interesting.
01:08:56.419 - 01:09:36.225, Speaker A: So if anyone is looking for something to like delve into that has some concurrency and stuff, then please have a look. Let's see. Oh, right. This is something that came up in the previous stream where there was someone filed an issue around differential flame graphs. So the ones that show how much slower or faster different stack frames got in a flame graph. And we commented on this in the previous stream. Oh man, they have Covid.
01:09:36.225 - 01:10:00.194, Speaker A: That sucks. So let's see. Okay, so this one is currently a draft and I wrote in there some in the previous issue. They basically outlined an explanation of how they think we can make these differential flame graphs better. And we read through it last time. We're basically like, this seems like a good idea. The current thing is confusing.
01:10:00.194 - 01:10:34.755, Speaker A: Absolutely fine with changing the way that this information is output. And so I guess they're gonna, oh, tracing timing is the one. And so I basically told them, go ahead, make this change. And I guess they already have a draft PR up so we can take a brief look at it. But it sounds like they're gonna sort of re replenish that PR, if you will, once they. Once they get better. Let's see.
01:10:34.755 - 01:11:47.335, Speaker A: Old and new. I wonder if these labels still make sense if Negate differentials is on. So Negate differentials is a way to treat old as new and new as old, kind of. And so they're implementing that functionality here. But I don't know if these labels are going to make sense if negate differentials is on. If X is equal to Y, then don't output anything because it didn't change. So if not negate differentials, then it's new divided by old.
01:11:47.335 - 01:12:43.759, Speaker A: So it's a multiplier. It is. Right. So if it now spends twice as many samples in a given stack frame than what it used to. It's going to say the ratio is going to be 2 so 2x compared to this is going to say 2x and then the old value. So it's going to say equals equals 2x and then the old value followed by the word old. I'm not sure I follow the phrasing here.
01:12:43.759 - 01:13:55.855, Speaker A: This will look like if for example we spend as many samples in some frame this will look like equals to x. I don't know, 1, 2, 3 or 2, 5, 6 old which I think is maybe overly concise. How about. So I think the idea here Unicode X oh this is the compose key on Linux which is fantastic. Lets you create all sorts of characters. So I can type like a Norwegian by writing O and then a slash and the unicode X is compose key xx. No, the old.
01:13:55.855 - 01:14:45.235, Speaker A: Oh you're right, it's going to be old 256 wait, actually no, it's not going to be that either. It's going to be the count name. So it's going to be this which I think it may be overly cons. Overly concise. This will look like. I think yeah, you're right compared to Isolde here, overly concise. Well so the question is also what it prints.
01:14:45.235 - 01:16:41.025, Speaker A: The other thing that bothers me, it doesn't include the function name. It doesn't include the function name. Is that printed elsewhere? And the old or new sample counts I think we'll want to include at least the function name and one of the two absolute numbers. I'm also not sure if old and new are understandable labels in this. In this instance especially for negate differentials. Maybe because like 2x old cycles is like very dense. It's like twice as many cycles as what were there previously.
01:16:41.025 - 01:18:12.557, Speaker A: I guess old cycles does kind of get at that but how about they may even be unnecessary. I think actually be helps here. I think if you just say it's twice the number of cycles like it's sort of implicit that is compared to the old. So I'll just leave that as comments. Great Hasmail. Oh this project I've basically abandoned because I rewrote it in Rust. Hasmail is a little tray icon.
01:18:12.557 - 01:19:11.875, Speaker A: It's just a tray icon application that checks whether you have new email and if it does it changes the tray icon and then triggers a notification. This is really handy to just like you know, if you don't want to run a full mail client, you're using Mutt or something but you do Want email notifications. That's what housemail gives you. But I've rewritten this in Rust and it's called Buzz and so I would just recommend people use Buzz instead. I basically don't maintain the GO implementation of this anymore. The new implementation in Rust lives in, which should hopefully be easier to customize and understand. Close.
01:19:11.875 - 01:20:12.295, Speaker A: I think I already have it in the readme actually that this is. This is now just Buzz. No, apparently I do not. Let's do that right now. The GO implement this project has been rewritten in Rust and is now maintained under Is now commit changes. Boom. I should arguably archive the repo too.
01:20:12.295 - 01:21:56.205, Speaker A: Okay, Fontaccini, can you automate the Gecko driver? Session initialization I was working on a personal project which requires web scraping, so I looked into this library problem came to mind is that you need to have something Gecko driver already running, which I don't really know how to start programmatically. Can someone explain it to me? Oh, this is like, how do you run? Like in order to orchestrate or manage a browser, you need to run this little daemon that implements the WebDriver protocol on one side and knows how to talk to the browser on the other. So in the Chrome world, this is a tool called Chrome Driver. For Firefox, it's called Gecko Driver. And this is basically someone asking like, how do I run that tool? Which is not really something that Fontaccini deals with, as it should just be a matter of running the geckodriver command. Beyond that, running the webdriver host is sort of outside the scope of this project. Close is not planned.
01:21:56.205 - 01:22:26.145, Speaker A: Done. Okay, let's see where we're at. More things. This one we can close. All right, let's keep going. Bump Ouroboros from 0:15.5 to 0:16.
01:22:26.145 - 01:22:55.295, Speaker A: Heroes is unsound. Ooh, interesting. Ouroboros is this. It's a. It's a library for implementing self referential data structures in Rust. So specifically, imagine that you have a buffer or like a vacuate that holds like data you read from the server. And then you want also in the same object that holds that vec, you want to hold like the parsed representation of that vecuate.
01:22:55.295 - 01:24:08.145, Speaker A: And so we do that a decent amount in the IMAP library because you can imagine that like if you get a. If you fetch an email, then that email is mostly going to be like the vecuit is mostly going to be the message of the email. And so in the parse representation we just want to keep a string reference that is really just pointing at a subset of the VECU 8 rather than having to replicate that entire string and allocate it twice as we use Ouroboros for that. But apparently it's unsound and so there's some fix in Ouroboros itself and it passes all the tests. It's just rolling. Ubuntu the next question now is does it doesn't matter whether this is a backwards compatible change because IMAP is about to get a breaking change, a new major version anyway. This rolling release update is something that's fine that's been fixed elsewhere.
01:24:08.145 - 01:24:50.715, Speaker A: Great, so this seems reasonable merge. It does really mean that I need to decide when we're going to cut the new breaking change of imap. Currently it's released as like alphas and the reason is because we still have a couple of outstanding changes that I know will be breaking and so I don't want to Release like IMAP 3.0 until we've done all of those because otherwise I'm going to have to Release an IMAP 4.0 not too long after. The problem is all of these are very volunteer based, so who knows when they'll land. It's a tricky balance.
01:24:50.715 - 01:25:30.121, Speaker A: In 0.15 the fix has been implemented. In 016, 265 is now merged and according to the advisory, 016 does not have this issue anymore. So I think we're good. We will only be good for I'm at 3.0 when that eventually lands though, which is awkward. All right, let me see if I can backport it.
01:25:30.121 - 01:26:13.105, Speaker A: I really. I don't even remember whether Ouroboros is used in 0.2 because that's fairly old by now. But if we look at something like Fetch. Yeah, I'm pretty sure we don't use Ouroboros in 02. So if I go to Cargo TOML and then dig up the version from not from main but from 241. Yeah, Ouroboros isn't in there.
01:26:13.105 - 01:26:57.845, Speaker A: Great. I'll cut a new alpha soonish. If I forget, please ping me. Great. Close with comment. Unsoundness here means that basically there's undefined behavior in the library and I think that changes in all versions of Ouroboros 015. I think it is mostly a.
01:26:57.845 - 01:27:23.467, Speaker A: I think the reason they didn't yank it is because it's not normally a security issue. Like you have to be pretty intentional about triggering it. Triggering it. It's not something that like anyone who uses Ouroboros is now subject to huge problems. It's more like potential unsoundness from my skim of that advisory. Great. So this I'll have to cut an IMAP alpha.
01:27:23.467 - 01:27:46.949, Speaker A: That's fine. This is now done. That's now done. Beautiful bump open us is all from 0 10:48 to 0 10:55. Oh, did not like that. This is in Fontaine. Oh, we got this in multiple things.
01:27:46.949 - 01:28:09.459, Speaker A: I guess this is because of a. Some kind of security advisory. Probably. I think the only reason it's there. Yeah, it's because of a security issue in OpenSSL. So that's why it's specifically cutting this even though it's a minor release. This seems fine.
01:28:09.459 - 01:28:21.971, Speaker A: I don't. It's. It's not really a problem because IMAP has a library. So this is setting the minimum bound. This lock file doesn't apply to our consumers. So this bump isn't actually something that we need to absorb. It's not important.
01:28:21.971 - 01:28:50.015, Speaker A: I don't need to do a new release because our consumers have their own lock files anyway. But I think the reason it triggers this for us is because I check in the lock file even though it's a library. So this is fine to merge, but it's not important. Arguably we should bump the lock file, but not to the cargo Toml in this case. But it's fine. It doesn't bump an MSRV or anything. So I'm okay with it here.
01:28:50.015 - 01:29:50.705, Speaker A: It breaks a bunch of things. Why? If we do add dependabot, rebase, see if it can come up with a better change plus one from dependabot. Great, let's go to the next one in the meantime, automatically cleaning up SSH connection on connection failure. Okay, so this is an OpenSSH. The way that OpenSSH works is it spins up this OpenSSH muxing client that does connection negotiation so that we don't have to implement the OpenSSH protocol. And the way that we talk to that sort of local OpenSSH client Muxing client is through a UNIX domain socket. That UNIX domain socket is a file that's created in a temporary directory that we construct called SSH connection with a bunch of random letters at the end to make it unique.
01:29:50.705 - 01:30:26.373, Speaker A: And currently, I guess the observation is that if connecting fails, then we don't delete this directory afterwards, so the user ends up with like a bunch of crufty directories if they fail. Oh, and they have panic equals abort set. So we don't get to run destructors. Yeah, exactly. Yep. Great. Thank you.
01:30:26.373 - 01:30:57.725, Speaker A: Someone else already responded and closed the ticket. This is why it's great to have other maintainers on projects. Is this now rebased? Yeah, now it's running all the tests. Why does checking on Nightly not work? Unknown feature Proc Macro SP Shrink. That's disturbing. That's very disturbing. I wonder why this is.
01:30:57.725 - 01:31:11.299, Speaker A: It's just an OpenSSL bump. Like why is this causing a strife? It's also only there for minimal. Minimal versions. Like it's not. This doesn't change anything. I guess we'll see. Okay.
01:31:11.299 - 01:31:38.491, Speaker A: Rust cicon so this is a library that I built. It's not even a library. It is a collection of GitHub Actions CI configuration files that I use to configure the CI on all the Rust projects that I own. So it has like if we go look at the repo. I did a stream on this one. Actually it's just a dot GitHub. It has some configuration for code cov and for dependabot.
01:31:38.491 - 01:32:07.903, Speaker A: And then it has these different GitHub Actions workflows that will run your test suite with all the different features enabled. Will run. Check that your minimal supported Rust version is correct. Run Miri, run loom, Whatever. It's just I use this as the default and there's a lot of technical detail that went into creating the CI configuration. And you noted that it wasn't documented particularly well in the YouTube video covering it. I've added my understanding what you wrote as well as some basic instructions how to incorporate this into an existing repo.
01:32:07.903 - 01:32:37.405, Speaker A: I took the approach. Oh, so this is basically adding documentation to this repo so that people understand what they're merging in. Yeah. That's nice. And then someone left a review. That's fine. Great.
01:32:37.405 - 01:33:19.395, Speaker A: Love other people cooperating. All right, let's look at this one. GitHub docs md in this folder there's a configuration for code coverage dependabot and CI workflows. Checks the library more deeply than the default configurations folder can be merged using a Allow unrelated histories merge strategy from this which provides a reasonably sensible case for writing your own CI on. By using the strategy history of the CI repos included in your repo and future updates to the CI can be merged later. To perform this merge, run that. Fetch CI merge Allow unrelated history CI merge.
01:33:19.395 - 01:33:50.255, Speaker A: Yep. That's fine. And this will also work even if you do another merge later then that flag is just unnecessary. That's fine. As consumers of this library would build with their own lock file rather than the version specifying this library's lock file. Yep. That's why we don't do patch updates.
01:33:50.255 - 01:34:16.377, Speaker A: Even though Dependabot still triggers patch updates for dependencies like we saw with OpenSSL. But I think it only does it in the case of security advisories. Okay, so we looked at this file. We looked at that file. This configuration allows maintainers of this repo to create a branch and pull requests based on the new branch, restricting the push trigger to the main branch and service to the PR only gets built once. Yep. Yeah.
01:34:16.377 - 01:35:19.909, Speaker A: So this restriction in the CI file, if I didn't have this, then if I create a branch of my own project and then push to that branch in order to open a pr, all of CI runs twice. One's for there's a branch of the repo and one's for there's a pr. And so this ensures that only branches only the main branch gets its pr, gets CI run, new code is pushed to PR branch then cancel in progress workflows for that pr ensures that we don't waste CI time and return results quicker. So this is if I post a PR and then while some CI jobs are still running I push a new commit to that pr, then the existing PRCI run will be canceled like any in progress task will be canceled, no new ones will be run and then it will only be run for the new commit. Instead get early warning of new lints which are regularly introduced in some beta channels. Yep. Doc generation on nightly to get doc config.
01:35:19.909 - 01:35:42.319, Speaker A: Yep. Combinations of feature flags Feature power set runs for every combination of features. That's right. Determine the minimal rust version that's supported by this crate. This is not quite right. This isn't quite right. This doesn't determine the msrv.
01:35:42.319 - 01:36:49.475, Speaker A: It checks whether a predefined whether the configured MSRV can indeed still build the current crate. So it's a war. It'll warn you if you accidentally bump the msrv but it will not find the MSRV in such a case. Great. This workflow, this is a no STD checks with the library is able to run without the STD library. This entire file should be removed if the crate does not suppose no std. Maybe also add a line here saying to saying that some of this repetition is unfortunate.
01:36:49.475 - 01:38:09.725, Speaker A: Maybe also add a line here and in the other non check files saying that all the shared configuration lines like on push are documented in check yml and then remove the repeated comments like if new code is pushed from all the other files unbalanced brackets in the review Did I Where I don't see IMBALANCE brackets. Great. This workflow checks for unsafe code in crates that don't have any unsafe code. This can be removed. Runs miri, address, sanitizer, leak sanitizer and loom. Yep. When scheduled rolling jobs on a nightly basis, your crate might break independently of any given PR updates to rust nightly and up to this create crates dependencies.
01:38:09.725 - 01:38:55.455, Speaker A: That's fine. Section checks Updating the dependency of this crate to the latest available to satisfy the version cargo tumble does not break this crate. This is important as consumers of this crate will generally use the latest available crates subject to the constraints in our cargo Toml I. They will not update to 2 if we specify 1.3 in the other non check files. Really? Did I? Oh yeah, you're right. I completely read past that.
01:38:55.455 - 01:39:53.649, Speaker A: Good catch. Okay, this file is missing top level documentation and so is check actually is missing top level documentation. Chat is helping. It's true. Enable the CI template to run regardless whether the lock file is checked in or not. Yep. This action chooses the oldest version of the dependencies to ensure that this is compatible with the minimal versions of this grade and its dependencies require.
01:39:53.649 - 01:41:38.217, Speaker A: This will pick up issues where this crate relies on functionality that was introduced later than the actual version specified. For example, when we choose just a major version but a method was added after this version for example and IE are should always be followed by a comma. Should well near should basically always be followed by a comma according to most style guides. I think here I want oldest version of oldest version of dependencies permitted by cargo toml because it's not the oldest version. Like that would be like 0.01. It's the oldest version that's permitted by the semantic versioning requirements we have in cargo toml. Yeah, so for EG and IE there are three ways there's eg not followed by a comma or optionally followed by a comma.
01:41:38.217 - 01:42:39.471, Speaker A: So there's this one BioRegx and this is only recommended by the Economist style guide and no others. Then there's eg with a comma which is most I think this is most British style guides and then this eg without a space which is most American style guides. I may have gotten common American and British mixed up here, but with comma is common in both and without is only common in one. So the general consensus is there should be a comma. There's a really interesting discussion on the stack overflow for English stack English comma after I. E. It's this one I think.
01:42:39.471 - 01:43:24.713, Speaker A: And there's a long discussion in no, it's not that one. It is from Sometimes DuckDuckGo is not very good at finding things, but maybe it is that one. It's from in American English, comma after I. E. Not in British. Ah, it's this one. That's the one I was after.
01:43:24.713 - 01:44:13.991, Speaker A: And then this article which talks at length about this and about whether there should be dots in between them, whether there should be commas after and the general recommendation is you should basically always use a comma after. Anyway, this is neither here nor there. This particular check can be difficult to get to succeed as often transitive dependencies may be incorrectly specified. For example, a dependency specifies 1.0 but really requires 115. There's an alternative flag available that uses the minimal version for dependencies create while selecting the maximal versions. Alternatively, you can add a line in your I forget where I already did this.
01:44:13.991 - 01:45:05.541, Speaker A: I did this in it might have been an imap we already did this. No. Oh, maybe it's actually in this one. Yeah. So this is the other way to sort of make minimal versions be happy is you specify an optional dependency where you give the minimal and then you place it behind target config any which is never true. So that way it affects your generation of your lock file but you never actually take the dependency. So we will add that in there in your cargo.toml
01:45:05.541 - 01:47:38.455, Speaker A: to artificially increase the the minimal dependency which you do with toml See also and I think I want to recommend that people simple do this needed to allow foo to build with minimal versions the optional equals true is necessary in case that dependency isn't otherwise required by your library that dependency isn't otherwise transitively required by a library. And the target bit. Let me see if I can actually dig up the history history for that line. Someone added that fairly recently I thought. Hmm, I don't remember why I have config any there and the target bit is so that this dependency edge is never actually never actually affects cargo build order I think is why so normally if there's a direct edge between two crates then cargo is always going to make sure it builds one before the other. But in this case we basically want to or it has to like it knows that this crate what's the Let me see if I can do this up right. It thinks there's a direct dependency between this crate and that crate when in reality that might not be true for minimal versions we might specify this just because some transitive dependency we needed to lift the floor of but we don't want cargo to think that there's a direct build edge because it might make it not do optimizations for build order.
01:47:38.455 - 01:47:51.649, Speaker A: It might otherwise do. We'll add that in there. Run Cargo test on macOS and Windows. Use LVM. Come to build and collect coverage. That's fine. Okay.
01:47:51.649 - 01:48:25.169, Speaker A: Viewed and viewed. This is great. Thanks for taking the time. Left a few notes in line quest changes. Submit review done. What's the Z prefix in a flag means? It's the way that Cargo expresses flags that are nightly only experimental unstable flags. All right, how did this open SSL PR go? Did it.
01:48:25.169 - 01:49:08.029, Speaker A: Does it still fail? It still fails. So many things I don't quite understand why. Oh, this is because Wikipedia has changed. Okay, very annoying. Actually, there's a. There's a bunch of tests in Fontoccini that basically queries Wikipedia to check that we can actually interface with a real website. But when Wikipedia changes, finds all inner, then the test starts failing, which is really stupid.
01:49:08.029 - 01:49:43.201, Speaker A: Like I should make this not be as dumb. So on line 170. So that's here, the thing on the left. That's fine. Minor git clone. Fontoccini. Yeah, I know, it's a tarot.
01:49:43.201 - 01:50:14.081, Speaker A: So we've started getting better at this. So if you see here, there's a local RS test suite for Fontaccini, which is all based on local files. And then there's remote, which is all based on Wikipedia. It used to be that all the tests were based on Wikipedia and it was a huge pain. Now most of them are local. If you want to do something that's fairly simple work, then please, please, please take the remote test from Fantaccini and port them to be local tests instead so that this problem goes away. It should not be very hard.
01:50:14.081 - 01:50:52.715, Speaker A: And it's a great way to just do a PR to Fantoni. That matters. But for now, I'm going to fix this by replacing these with that. And then I'm guessing this other one also fails sub element. And so that's going to be the same thing where. Whoa. Why is that there? That's a weird annotation as this too.
01:50:52.715 - 01:51:39.505, Speaker A: Oops, that's not what I wanted. That's the only one that. Those are the only two that failed, right? It finds sub. It finds all for Firefox and for Chrome. Great, I changed again. PDF changed again. Someone please turn this test local.
01:51:39.505 - 01:53:55.585, Speaker A: Okay, this is one of those where I might as well do the fix myself, because this particular fix, because I just. I know how to do it. And so I do this as a PR just to see that CI actually passes and then I can land this and then I can rerun CI for the I can rebase this dependabot and then everything should be good. Okay, this is for the errata for us for stations in chapter 8 listing 14 sum to okay return value is result I think we need to use okay because I use sum T in question mark Operating destructuring assignment All's changed. Let's see, listing 814 says this thing server this thing. So the changes from sum here to okay when you call socket accept it depends on what library you're using for async too, but TCP listener calls accept dot await. Yeah, I mean this one's tricky because the idea here was that you want to handle or the reason I wrote it this way because in some ideal world when you call accept you get back a result of option where the error is something went wrong and the none means there are no more strings like the socket is closed which isn't really an error in the same way.
01:53:55.585 - 01:56:57.405, Speaker A: So this should only be while let okay, if you want to ignore the errors and if you don't want to ignore the errors, then this doesn't need to be a this can just be a While let stream equals there's no need for the okay here because a question mark already unwraps where did I go here? So this kind of depends on which asynchrony library you're using for TCP listeners BS in general, accept will return just a result TCP stream not result option TCP stream even though I think the latter makes more sense. When that is the case, I would argue you should still use when that is the case, you would either need to write while let OK stream equals socket dot accept auto 8 or while let stream equal equals socket auto 8? I think the latter is actually better because it won't silently drop errors on the ground and when exiting. But it's also a little bit of a weird pattern. It's an infallible match. I don't really know what the book should recommend here because like it, it's not talking about any specific implementation of a library. So we it could easily just be talking about a sort of broadly and accept that returns the result option. I wonder what except in the standard library does TCP listener accept it also returns a result over TCP stream.
01:56:57.405 - 01:58:15.979, Speaker A: Yeah, which makes me a little sad. Like for the for many parts of the book one thing I worry about is giving code and then people getting tripped up by something that isn't relevant to the thing that I want to talk about. Yeah, you can also have a loop that just does this inside. So we could do. Or at that point it could even just be loop let stream equals socket accept await. Like one thing I worry about with the book and when writing in general is like people getting tripped up by some pattern that looks weird that's not really related to the thing that I'm trying to explain. Like in this case, I think what I'm trying to talk about is the sort of structure of how to handle multiple clients concurrently.
01:58:15.979 - 01:59:46.981, Speaker A: So I start with this as the example, then I expand it to be like, what if you now join on the clients, what if you spawn the clients to talk about things that happen concurrently or things that happen in parallel. And I worry that that gets lost a little bit once this becomes a strange looking pattern like this one or where the structure sort of changes like this one. And I like the advantage of the way that this was written is that people don't really question it. Like it's true that it doesn't work if you're using Tokyo or even indeed if you pretended that the standard library was async and you just use the standard library because they don't differentiate between getting an error from Accept and having Accept telling you there are no more connections coming, which to me are different conditions. So this is a sort of trade off between technical accuracy and what is more helpful. There are a bunch of things that are technically incorrect about this. Like for example, what you actually get back from an accept is you get a tuple of TCPSTREAM and socket address.
01:59:46.981 - 02:01:16.525, Speaker A: But that's not something that I handle here technically. Also, you would need to put an empty OK line here because otherwise this wouldn't type check. But those things aren't important, so they're not in here. However, I think the weirdness of that setup is of that code layout might trip people up even more. The code is already semi somewhat inaccurate compared to what you'd have to do in the real world because it or eg cept returns a tuple, not just a stream stream. And the function would need a trailing to type check. And I think the less distracting thing is actually to keep it the way it is where the reader can imagine a reasonable API that matches.
02:01:16.525 - 02:01:51.183, Speaker A: Yeah, so another option would be to give except the signature that I prefer. But the thing is, this entire chapter, there's no definition of accept. Like I don't in the book give a definition of accept because I don't really want to Talk about accept. Like accept isn't important to the thing that I'm describing is I just assume that people roughly know what accepting a connection means in tcp. This isn't about tcp. And so that signature is never given. It's always inferred by the reader.
02:01:51.183 - 02:02:17.575, Speaker A: And that's sort of what I'm getting at here. Right. Like to keep it the way it is where the reader can imagine infer a reasonable API that matches the code. So I'm going to close this. Okay. How did this go? Whoa. This did not go how it should.
02:02:17.575 - 02:02:40.585, Speaker A: That's interesting. Well. Whoa. Why did it not work? It clicks. Did not work because. Ooh, I got a 503 from Wikipedia. Service temporarily unavailable.
02:02:40.585 - 02:03:52.905, Speaker A: That's interesting and not what I would have hoped. I'm guessing this one might be the same. Now this is something else that's like the rolling test that's failing. Why is this failing? It clicks by locator also failed. Okay, so it suggests that it clicks by locator is maybe broken gecko driver Firefox. What's it called? It clicks by locator. This is because I don't have normal Firefox installed.
02:03:52.905 - 02:04:26.667, Speaker A: I only have the Firefox Developer edition. That's fine. This is all normal Firefox as well. How about now? Open me a browser please that passes. So this is a transient error then that's fine. Maybe I get rate limited by by Wikipedia wouldn't be entirely unreasonable. The errors are transient Wikipedia 503s.
02:04:26.667 - 02:05:02.667, Speaker A: So we're good. Comment merge with administrator privileges. Merge delete this branch and then Dependabot rebase again please. So dependabot is going to rebase and hopefully that's going to make things work. Okay, another PR to inferno from dependabot. Bumping index map. That's fine.
02:05:02.667 - 02:05:34.997, Speaker A: That's a private dependency. We don't have that in the public API as far as I remember. And yeah, that's fine. This is just used internally. I'm curious now though, what is the. What changed in index map 2? Do they have a change log in here? They do not. Do they have one on the repo? It's not a change log on the repo.
02:05:34.997 - 02:05:57.495, Speaker A: Ah, releases. MSRV STD feature is no longer auto detected. Serde 1 has been removed. Get mute now returns that that would be caught at compile time. Index map now have additional things reserve. Exact equivalent is re exported. Hashbrown was updated in certes seq.
02:05:57.495 - 02:06:14.515, Speaker A: Okay, so these are all non breaking for us. Oops, did I accidentally Close that. Yeah, I did. Okay. This seems fine. Thank you, Dependabot. I don't think I care.
02:06:14.515 - 02:06:47.617, Speaker A: Yeah, I don't. This is a fine change to just do. It's out of date with fine. All right, Dependabot, rebase. This one's going to keep running. Shell Escape is being overzealous bug in OpenSSH trying to do a curl and knows that the arg would fail. Turns out Shell Escape is adding a few more singled and double quotes.
02:06:47.617 - 02:07:09.605, Speaker A: Try something like this and review the data structure. I think you need to use a different crate as Shell Escape hasn't been updated in years. Why are they quoting themselves in here? That is the intended way. Every argument has to be passed separately. Is same as process in Tokyo. Process. Yeah, that's right.
02:07:09.605 - 02:07:43.015, Speaker A: Great. Nothing for me to do. Done. Very exciting. How about asking the guys at the Internet Archive you can use a corner of their page for tests. That's not a bad idea. Wikipedia is arguably a bad target because it keeps changing.
02:07:43.015 - 02:08:08.405, Speaker A: There was a period where Wikipedia changed their whole design and that was a nightmare. But I really want these tests to just use local files instead. I think if we're going to change them, we should change them to use local files because there's no reason for them to be used in external resources at all. It just makes them more annoying to run. It means you can't run them if you're on a plane or something, which there's nothing preventing you from doing. So. So I would rather just fix them properly.
02:08:08.405 - 02:08:44.435, Speaker A: Okay, what are we down to? 105. So we were at what, 1:35. So we're down 30. This is pretty good. Okay, bump Russell's connector in IMAP. That's fine because this is going to be a. I don't understand why CI hasn't run for this.
02:08:44.435 - 02:09:33.299, Speaker A: That's interesting. Network request interception unfortunately isn't part of the web driver spec and so at least not yet. So it's not something Fontochini will support. As things currently stand, there's like the ability to look at which requests the browser is making. Essentially like the Developer Tools networking tab. But that's not something Fantastini does. This has been closed because it's been superseded.
02:09:33.299 - 02:09:51.139, Speaker A: That's easy. This has been closed because it's been superseded. Great. Let's work for me. Rusty icon. The matrix for MSRV needs quoted SEM or it will truncate put 170 in there and it will actually test 1.7 as it appears.
02:09:51.139 - 02:10:25.958, Speaker A: GitHub auto converts this to a float that's wild. That's awful. That's absolutely terrifying and awful. Thanks for catching it. So if you put 1.70 here, it will treat it as a float and 1.70 is the same as 1.7
02:10:25.958 - 02:11:19.145, Speaker A: and so therefore it's using 1.7. That's terrible and gross. Squash and Merge Quote MSRV version to avoid float parsing Confirm squash and merge that's disgusting. GitHub But I understand this is not their fault. Feature request Rust support this is in some. I don't even know why I'm following this ticket the feature request for AWS code Build Docker images Unsubscribe and done bumps Velta this is where we were wondering that seems fine. I don't really care.
02:11:19.145 - 02:12:10.765, Speaker A: 358 to 359. What could possibly have changed Squash and merge here? I do need to manually do a release of this to the website. There's no automation that pushes it out to like the S3 bucket that hosts the interface. It would be nice to set that up. Maybe I'll do it one day. But for now this is going to be manual bump Russell's connector I don't really know what to do to trigger this one like this Seems fully reasonable. I just don't know how to Russell's connector Also this should probably 0.18.0
02:12:10.765 - 02:13:14.141, Speaker A: so here's what I'm going to do. This is going to re trigger CI so I guess that's nice. Add comment and then I'll do the same here to propose here that this should be zero because when you increase the effective major version then you assume that you can start a patch version 0 and then commit the suggestion. And now let's see if that kicks off CI. Yeah, something is queued. Great. This is an issue I filed against the T ages ago where if you parse toml using the TOML crate then the way the thing you get back lets you deal with things like an array and the TOML crate has its own definition for array so that it keeps track of like inline comments and stuff.
02:13:14.141 - 02:13:49.195, Speaker A: So if you print it back out, the toml you print back out includes any comments that were in the stuff that you brought in. So it's not just a vector, it's actually a separate type. But this means that because it's a separate type it's not like a vector. You don't have access to common collection methods like retain, which is really really handy to have. And so this is basically just asking can we add retain as a feature of Array and the table likes. That's great. Did it for the wrong crate.
02:13:49.195 - 02:14:24.165, Speaker A: Ooh, I love it when other people do things that I want to happen. Great. So this adds retain, which is calls retain on the underlying value, which is what I would expect the implementation to look like. Beautiful. That's fantastic. I don't remember what I wanted this. I think it was maybe some stuff I was doing at Amazon.
02:14:24.165 - 02:14:48.309, Speaker A: I don't think I'm using the tomokrate in any of my personal projects. Well, I'm glad this happened. Great. Done. All right. Bump OpenSSL Infanticini now looks like it's passing tests. The rolling Ubuntu One is failing because that's still.
02:14:48.309 - 02:15:09.425, Speaker A: I don't understand where this comes from. This is like it runs cargo update and then tries to build and apparently that doesn't work, which is a little disturbing. Bump index map with the rebase. That's still working. This is probably that same issue actually. Yeah. So I think there's something wrong in Proc macro.
02:15:09.425 - 02:15:29.665, Speaker A: It might be that I have to switched the dependency to be like 2.0 instead of 1.0. Given that syn and Quote both got bumped and that the 1.0s don't actually work anymore with newer versions of Rust. It's almost certainly what's going on there. Which means I probably have a dependabot PR further up in the queue. That's like bumping SYN and Quote.
02:15:29.665 - 02:15:58.995, Speaker A: But we can ignore those errors then in here. But let's have CI keep running there. This CI is running. So now we're just waiting on a bunch of CI. This is the downside of using the open source funded CI jobs is it limits how many jobs will run concurrently, which is very understandable. But when you're doing batch work like this CI like, you'll see a bunch of these jobs haven't started yet. Well, now they've all started, so my argument doesn't hold anymore.
02:15:58.995 - 02:16:21.325, Speaker A: But it only runs a few of them at a time across all your projects. And so you just need to wait and sit patiently. Libflate and Rust zip. Okay, there's another dependabot, libflate and Inferno. So this is also internal only. This just deals with how we. How we compress.
02:16:21.325 - 02:16:50.425, Speaker A: How we compress representations of SVGs or something. Thing. Let me see here. Oh, it's even. It's only in dev dependencies even. I think this is because we allow you to pass in a compressed flame graph or maybe we use compressed flame graphs for tests. So yeah, this doesn't matter for anything.
02:16:50.425 - 02:17:12.027, Speaker A: It's in dev dependency seems fine. What's this? Check nightly doc. That's that same thing. That's fine. And we'll do a dependabot rebase for this to make sure I didn't do anything wrong. And so now we're gonna have to wait for CI this one. I think I'm happy enough with that.
02:17:12.027 - 02:18:02.695, Speaker A: I'm just gonna merge this confirm which also means dependabot is gonna have to rebuild because this is to the same repos. So this one's done. This fails on macOS latest because yeah, this cookie test is broken for some reason. This is a spurious test failure that I haven't figured out yet. But all the reasonable ones are landing. So I'm just going to merge this so we don't have to wait for it. Those failures are not due to this bump is basically the observation Russell's connector.
02:18:02.695 - 02:18:34.065, Speaker A: All the CI here seems to be running fine except for beta and that's deprecated functions in chrono. That's fine. Converge this and libflate. That's now going to rerun. Fantastic. Okay, fix bias towards the first element due to floor. Okay, so this is in the zipf crate.
02:18:34.065 - 02:19:11.205, Speaker A: So zipf is a. When you generate random numbers, normally what you want with a random number generator is to generate uniformly random numbers. So the probability of sampling any given number in the range is equal across the range. So the probability if you generate numbers between 1 and 10, it is equally likely that you generate a 1, a 2, a 3, a 4, a 5, a 6, etc. Zipf is a different kind of distribution. So rather than being a uniform distribution, it is a skewed distribution. So the likelihood of getting a 1 is much higher than the likelihood of getting a 10.
02:19:11.205 - 02:19:46.405, Speaker A: And it's sort of an exponential curve kind of. I think technically it's called log normal, it's not exponential. So the elements further to the left in the sampling range are much more likely than the next element to the right of it, and so on. So you get this gradation. And one of the reasons you might want to use zip is if you're running benchmarks, for example, and you want to emulate that you have a skewed distribution. So for example, the thing I use this for was emulating the number of votes on an article. This is for like the Noria work.
02:19:46.405 - 02:20:16.875, Speaker A: So imagine that you're running a benchmark and you're generating a bunch of articles. Something like hacker news, like generating a bunch of articles and you're generating votes on those articles. Then it's not as though all articles get roughly the same number number of votes. In reality, some articles get way more votes than others. And so you can use something like zip to generate that distribution of votes. So when you. When you are about to generate a vote, the way that you choose which article is you generate the article ID to vote for using zip and the zip fly for you.
02:20:16.875 - 02:20:59.945, Speaker A: Here is a port of a very fast implementation of how to generate these skewed numbers. There are some very slow ways of doing it, but there's a Java library that does very fast implementation of this distribution. And so this is a port of that to Rust. This PR address is a correctness issue where the generator favors the first element more than it should and emits the last element less than it should for a given exponent. This is due to the implicit use of floor, which makes emitting the last element only possible by an exact match. We did thus shift the likelihoods from the first element, which was favored, to the last by adding 0.5 before doing the floor, as the original code also does.
02:20:59.945 - 02:21:31.911, Speaker A: Oh, nice. Someone's porting it to Julia. Nice. So the original implementation here and the port here. So this is the original Java code. It has a loop, it does the integral inverse. It takes the X, it gets back at 0.5
02:21:31.911 - 02:22:39.737, Speaker A: and takes the floor, and then it takes a. If K is less than 1, then k is 1, which is the same thing we get by doing a max here. But I think this can use round. I think there's a round operation for f64 round the nearest integer to self halfway between two integers, round away from 0.0. So I wonder whether instead of adding 0.5, we should round. Although I guess that's really.
02:22:39.737 - 02:23:58.311, Speaker A: I wish actually that round returned a U size like an integer type instead of an f64, because otherwise casting from the f64 to an integer is not, I don't think, guaranteed to give a give the right integer. But the reason I would like to use the round is it might be faster than doing this. Add 0.5 and then cast, but that seems fine. Whoa. Why did this change? Why did this change? Has this code, like, changed recently? No, it changed seven years ago. Oh, this is in a test? Yeah, this is in a test, Right.
02:23:58.311 - 02:25:09.545, Speaker A: So what this is doing is this test just generates a bunch of random numbers using the SIF distribution and then checks that the frequency matches what you would expect to see after generating a zip distribution. And it does that by seeing whether how much we're off by compared to what we would expect the frequency of each bucket in the distribution to be is less than some like, I don't know, like wiggle room or jitter that we're allowing. And this used to have a special case for handling the first and last because those were more wrong. But that was because of that bug which is now fixed. Beautiful. All right, this seems great. Although why doesn't this have CI Marshall? Because it should have CI.
02:25:09.545 - 02:26:53.801, Speaker A: That's interesting. CI just did not run for this workflow Run completed with jobs. What I don't understand why not? Let me see. Is there source revision somehow not contain the workflows? No, it has the workflows. So why didn't any of them run? All right, let me make some change here then I guess I'll just something arbitrary rounds towards zero. So we add to ensure. Great.
02:26:53.801 - 02:27:18.069, Speaker A: And then commit suggestion really just to poke CI. Great. Will that trigger CI that triggers CI? Great. No idea why CI didn't trigger otherwise. Let's see. That's going to run in the background. This is the bump of LIBF late and inferno and that looks fine.
02:27:18.069 - 02:27:39.705, Speaker A: Merge squash and merge commit. I finished close. I would be very surprised if this ended up if this fix was not correct. But it's just good to have CIB green anyway. All right. Oh, we're down to 94. Look at us go.
02:27:39.705 - 02:28:28.755, Speaker A: Let's see. Just waiting for this to run. Java can be fast. Yeah, of course you can. If you have a fast algorithm, then it'll be faster than the slow algorithm. Oh, round can't return an int because some floats are too big. That makes sense.
02:28:28.755 - 02:29:15.105, Speaker A: This is setting on GitHub to require approval before CI runs. Maybe that's on. No, because then it usually shows me a button to say like approve and run and it didn't do that. Here, let me go ahead then and do this is almost certainly going to be fine. And then I'll do. I'll do a release of zip straight away because that one's pretty easy to release. All right, another page.
02:29:15.105 - 02:29:36.999, Speaker A: This I'm pretty sure I've already dealt with. Yeah, this is like a. A regression in tracing. That was backwards incompatible. That's fine. I can get rid of that. This past merge.
02:29:36.999 - 02:30:30.573, Speaker A: No squash because I don't care about my additional little commit. And then we'll do git pull and then we'll do Cargo Toml Release 701. Why can't I run cargo Check. That's disturbing. Found Create config if compiled by an incompatible version of Rust C almost certainly because that folder is out of date. There we go. Great.
02:30:30.573 - 02:31:15.015, Speaker A: So release 701 with bias fix and tag. So this is a script I have that just walks the git history and finds which commitment was the one where I changed the version in cargo toml and then add a tag to it and git push and cargo publish. There we go. And zip701 is tagged and released at least in 7.0.1. Oh, I forgot to actually leave a review on this one. That's a great catch. Thank you.
02:31:15.015 - 02:31:55.459, Speaker A: Beautiful. Okay. More more. Making very good progress through this support. Hot and cold flame graphs. I've not actually used these before, but they are supposedly very useful. Oh yeah, this is for also plotting off CPU time.
02:31:55.459 - 02:32:54.649, Speaker A: So stuff time spent waiting. So normally flame graph do sampling so they only sample what the CPU is actually running. So if your process is like asleep, like if it's waiting and accept it's not going to be sampled, it's not going to show up as where you spent your time. And that can be a little misleading because it can make you think that your code is like spending all of its time in function A, when in reality it's actually spending most of its time asleep in function B. And hot cold graphs will show those waiting parts in blue. Great idea. I think this is mostly a matter of how the input is collected and then just figuring out which frames to code to use.
02:32:54.649 - 02:34:04.305, Speaker A: Red ish blue ish for depending on the on something in the output of perf script would be really neat indeed. Done. This is a feature to we were wondering. This is the Qi Q&A site this PR introduces step two over from here, right. So this is when I built we were wondering. I was definitely relatively new to front end development, which I still is, and new to the sort of framework I was using. And it turned out there were a couple of architectural decisions that should be changed somewhat for the system to just work better.
02:34:04.305 - 02:34:45.855, Speaker A: And this person came along and basically implemented a bunch of things to make it better like debounce and stuff so that animations would be nicer. But that particular PR was very large and so I asked them can you split this up into smaller PRs? And this is one of them. So this is moving. What's event here? Oh, event is information about the current Q and A session. So we were wondering supports like you can go create an event and we were wondering I can go create one. And they end up with separate like unique identifiers. And those are separate events.
02:34:45.855 - 02:35:44.995, Speaker A: And each event has associated questions. And I think a Title I forget exactly. But mostly like it has a separate like admin key and it has a separate list of questions. And previously which event was currently open, like which event the current page is on was stored in a sort of local variable in svelte here. And the observation is that it would actually be better to make event be in the store, which is a special type of object that svelte has that's like persistent across page reloads, I think. And it also lets you be reactive with regards to if that variable changes somewhere in the code, then some other code, any code that has read that variable gets automatically run. So that's the reactive part of it, which you don't get with normal variables in the same way, I think event Event.
02:35:44.995 - 02:36:46.881, Speaker A: Right. And we end up with in the previous style. Sometimes when you change this variable, it ends up loading things twice because it doesn't realize that it changed to be the same value. So here we're going to subscribe to the event load questions for that event. Okay, this all seems like fairly straightforward changes writable null. Okay, this all seems reasonable to me. This seems entirely reasonable to me.
02:36:46.881 - 02:38:02.409, Speaker A: Thanks for it. Back up. Also, didn't they say something about they were moving me and hope your move went well. Did you have a chance to also test this out yourself? I assume that means it worked without any meaningful changes to the behavior of the page. Just so I don't merge it if they didn't test it because then I need to test it myself. The reason I use then catch in this code is because svelte didn't basically required it. I think because you couldn't have a block in the outer scope of a JavaScript file that might have changed, but it was by necessity.
02:38:02.409 - 02:38:39.769, Speaker A: Like I have other code in there that's async awaits style. Okay, so that was this one immutable iterator plus iterate with tokens. What is this? This is in stream unordered. Okay, this is a different crate. Again, Stream unordered implements a stream that multiplexes multiple streams. So the idea is that you have similar to how you have a, you know, join for futures. Or this is really more like futures unordered.
02:38:39.769 - 02:39:33.285, Speaker A: So there's a Tokyo. No, not Tokyo Futures util. The futures util crate has this thing under stream called futures Futures unordered. So a futures unordered is a set of futures and the futures unordered itself is a stream. So you stick a bunch of futures in there and it gives you a stream of the values that those futures resolved into in arbitrary order. So if you push the futures in A, B and C, the things you get out of the futures unordered is going to be the future values the moment any of them respond in that order. So if B resolves first, the first thing the stream is going to yield is the value from B.
02:39:33.285 - 02:40:15.445, Speaker A: If C then resolves, then the next thing the stream will yield is the value from C's future and then A. When A resolves, there's also futures ordered, which preserves the order. But what that means is imagine that you stick in futures A, B, C and D and they resolve in the order D, C, B, A. Then futures ordered still has to wait. It has to buffer the responses from D and C and B. It has to keep them in like a vector basically until A resolves, and only when A resolves can it release them in the order abcd. So futures unordered, if you can tolerate the change of ordering, tends to be more efficient because it doesn't have to buffer anything.
02:40:15.445 - 02:41:22.535, Speaker A: Now, stream unordered is a similar kind of abstraction, except the thing that you stick in there are streams and not futures. So it basically multiplexes these streams together so that you get one stream that logically combines or joins or merges multiple asynchronous streams under it. I haven't looked at this crate in a long time, but that's mostly because the stream API has stayed fast, rarely static, so so there hasn't really been a need to change this crate. Immutable iterator plus iterator with token exposes the existing iter and iter pin ref via the methods iter and iter pin changes iter and iter pins item to return use size and that the use as being the token of the stream. Changing the item type of an existing type would be breaking change. As far as I can tell, these types are incapable of being constructed as the fields are private and no other methods I could see returned at constructed. Hence this change shouldn't be breaking.
02:41:22.535 - 02:42:37.085, Speaker A: I attempted an implementation for stream unordered iter pin, but it's way out of my depth. ZPR is missing its implementation. Okay, working on a project that queues many streams into stream inordered that are related to a single connection. When this connection is closed, I need to be able to clear all active streams to be useful if something like this following is possible. Stream inordered Push some streams Remove all streams Streams in iteration map token seems to remove id Supposed to talk in a similar fashion with immutable iterators. Let's Pull up the docs here. Stream and ordered.
02:42:37.085 - 02:43:16.425, Speaker A: So when you push a stream or when you insert a stream, you get back a usize, which is like the token for that stream, and sort of a unique identifier for that stream. So that in the future, when the stream yields an item. Let me see here. Stream. When it yields an item, then it also tells you which stream yielded that item. The idea being that even though all the streams have the same value type or item type, you might actually care which of the underlying streams it came from. And this usizes is that identifier.
02:43:16.425 - 02:44:05.583, Speaker A: And you can also use that to like remove a particular stream from the set, for example. So the question now becomes why do they want this? So there's an iter. So this is for. Right, so there's an itermute and intermute here allows you to iterate over all of the streams in the set, like all the currently known streams. So intermute here, you'll see has. Oh, interesting. Yeah, it's just a mutable iterator over all of the streams.
02:44:05.583 - 02:45:29.981, Speaker A: But this iterator doesn't include the token, and I think that's what they're after. They want the iterator to also yields the token of the stream, which is really a bummer that itermute doesn't do. So let's look at what they added here. Iterator for ITER pin Ref. Next all. Yeah, this is final, though. I don't know if it's okay to hold on to this reference after you do this operation, so we might have to read the ID up here, but that's fine.
02:45:29.981 - 02:46:03.555, Speaker A: This is. Iterpinref is sort of the underlying type, I believe. I don't think it's exposed in the. Yeah, it's not exposed in the public API. It's just like a helper for implementing iterators. This should really not add an unpin bound. Yeah, that's not going to fly.
02:46:03.555 - 02:46:52.765, Speaker A: ITER returns an iter. Oh, I see. So we had an ITER type. It was just not exposed. Oh, this seagull is going nuts outside my window. Yeah, I don't think this should require unpin. Why does this require unpin? Intermute requires unpin.
02:46:52.765 - 02:47:39.167, Speaker A: Oh, because you're taking a mutable reference to self. So I'm not going to go deeply into PIN here, but basically, the moment you stick something into a pin, or let me rephrase, you have to stick things in a pin before you're allowed to pull them as a future or as a stream. Once you put something into a pin. You're not allowed to expose it without the pin wrapper mutably. And so here, you know, we have. We've already pulled things as a stream because that's what this whole data structure does. But we're giving out mutable references to the streams, and we can only do that if the underlying type is unpinned, that is if it's allowed to be extracted from from within a PIN after we've put it in a pin the first time.
02:47:39.167 - 02:48:22.629, Speaker A: So you see that bound does not exist for iterpin mute, so that's fine. So iter. I don't think ITER actually technically needs this bound because it doesn't expose mutable references. But I think it's okay. We could always relax this later if we can find a way to do it. I don't think ITER PIN is important to expose, but I. I don't know why this should suddenly require unpin.
02:48:22.629 - 02:49:49.595, Speaker A: That doesn't seem right. The other thing that's awkward is that I wish it's going to be confusing that ITER mute here. Where is my here source iteration iterator for? Yeah, this also only returns a reference to S, but they change that here, but only for iter. I think it's going to be confusing that ITER mute does not include the token, but ITER does. I think we need to be consistent here, and I think the way we do this is I think we should just not expose this right now. Unless we have a concrete use case that benefits from the pinning in the at case. These seagulls are really just going for it out here.
02:49:49.595 - 02:51:34.665, Speaker A: I don't think this extra bound should be necessary. I'm not 100% sure that it's safe for us to still access task here. So let's Instead read task id. Read instead have let id is self task.id directly next to. Let's stream further up and then just use ideas directly here. I think it'll probably be pretty confusing for iter to include a token, but iter mute not to.
02:51:34.665 - 02:52:59.713, Speaker A: Let's instead add this as ITER with token and add a iter mute with token as the mute version. Then in the next major version we can get rid of the non the non token iterators and just have them all provide tokens instead. Yeah, great. Done. Add support for Fluid drawing. It's from 2019. Oh, someone left a comment.
02:52:59.713 - 02:53:46.065, Speaker A: This landed in 2019 and then someone left a comment last month. It's really expensive to render for complex graphs. Oh, that makes sense. So this change was basically moving from having pixel based widths for all the frames in flame graphs to using percentage widths instead. Now, percentage widths means we don't have to compute a bunch of things ourselves, but it means the browser has to compute those things instead. Which is what this observation is, that this is now actually a lot more costly to render. Like it went from what, 200 milliseconds to almost 3 seconds of layout calculation.
02:53:46.065 - 02:54:51.985, Speaker A: Zoom is also a bit slower with 40 to 60% of the time spent in update text. Yeah, so we switched to monospace fonts here, which made that a lot better. Let's check with the monospace font. Yeah, there's definitely a cost involved with having the browser do this work. I think there's a d decent argument for us just making monospace fonts the default. So with monospace fonts, this update text method, which has to walk all of the text elements to like truncate them so that the text fits and doesn't overflow. With monospace fonts, we can pre compute how many characters to show.
02:54:51.985 - 02:55:20.883, Speaker A: And we know that that's just how always how many we're going to show. But monospace fonts are not the default because they don't read as nice. What do you think? Done. This one's already released. It's going to be closed. Could be closed. Non numeric num locked numpad keys canopies.
02:55:20.883 - 02:56:10.829, Speaker A: This is an issue I filed with obs where you can't put like on numpads. You know how you can have num lock on and off. If you have num lock off, then like the 7 key on a numpad for example, is home or the 0 key is like insert. You can't bind specifically the numpad insert key or the numpad home key to things in obs. The home key on the numpad is treated the same as the home key on your normal keyboard. But I really want to be able to bind things to the specifically the numpad keys. And so this is basically someone pointed out how you can do this, like how you can implement this distinction in observation.
02:56:10.829 - 02:56:50.975, Speaker A: And this is just I need to write a pr or if someone else wants to write a pr, this is potentially a good way to land a change in obs. I'll send you the link in here. I'm not going to deal with this on stream though. It's going to be a bunch of implementation work. Okay. Fontaccini expose the returned remote capabilities on the client. So Tackle195 can find a Way to get the original new session command response which contains the WebSocket URL the web driver creates for communication.
02:56:50.975 - 02:57:41.325, Speaker A: Okay, so this is when you connect to. When Fontaccini connects to a web driver host, there's a response back from the. When you establish a session, that response isn't. Isn't currently exposed in the client anywhere. But it turns out there's some useful stuff in that response that users of the library might want. Supposed to be a public getter access the remote capabilities. All right, let's look.
02:57:41.325 - 02:59:34.675, Speaker A: Oh, what is capabilities here, though? Is that a type we own or is that a type WebDriver owns? It's crate we own. Pretty sure. I think we've already abstracted away all of that capabilities is a serde Jason. That's fine. Okay, so this is when we connect to webdriver we do like a handshake to establish a session which is essentially the same as a browser window. And will we get back from that new session thing here? They want to preserve the capabilities field of that capabilities is an object. Then we return the new session response here.
02:59:34.675 - 03:00:52.131, Speaker A: Okay, issue map, map handshake response capabilities. Then we store the remote capabilities. Interesting, I like that. The only question here is, so this extracts the capabilities from the response, but I'm wondering whether there might be other other things from that response the user wants as well. So rather than just extracting the capabilities, maybe we should store the entire response. I also forget whether on Fontaine. Yeah, remote, because capabilities is like an overload determined WebDriver because when you connect to WebDriver you send it, these are the capabilities I want.
03:00:52.131 - 03:01:39.937, Speaker A: And then it responds with these are the capabilities that I have. And so when currently Fontaccini will refer to these just as capabilities, which is, you know, the right term. But here we want to talk about like the. This getter to see what the web driver host sent us back when we connected. It's currently added as a function called remote. Where is it a function called remote capabilities? But I almost wonder whether it should just be called capability capabilities rather than. Rather than remote capabilities, because there are no other capabilities that are relevant here.
03:01:39.937 - 03:02:41.685, Speaker A: Like, no one's going to think that if you call dot capabilities on a client, it gives you back the capabilities that you sent it when you connected. So I think I'm okay with. Yeah, I think I'm okay with this just being a. Being called capabilities. This just being called abilities. Actually, I doubt anyone would expect that this is the capabilities we originally sent to the server. And we can also clarify this in the Doc Text.
03:02:41.685 - 03:04:22.175, Speaker A: What do you think? Seeing this made me wonder, are there other fields of the response outside. Outside of capabilities may be relevant? I guess. Let's see whether the response that we get back whether the response we get back only ever has those fields. Oh, it only ever has session ID and capabilities. Okay, great. That's fine then. So there's nothing else that we expect to ever be added to this, but maybe more like here or more like here? Just store and expose the entire new session response rather than only the capabilities.
03:04:22.175 - 03:05:14.711, Speaker A: There's not much else in there at the moment, but the session ID may be handy and who knows if more fields may be added in the future. This is a great addition. Thank you. Left a few questions, but none of them major proof. I like to. Even if I have like non blocking comments, I'll usually approve it and then leave the comments. And the reason for that is mostly for myself.
03:05:14.711 - 03:06:17.385, Speaker A: When I later come back and look at this pr, I will know that I have already looked at the PR and so I can sort of reread my own assessment of whether this was generally okay and we'll approve and run the CIA. How do you batch these? It would kind of make sense to me to batch this kind of work by repo. Yeah, so sometimes I'll like look at all of the things for Inferno for example, but when I'm doing a big catch up like this, I kind of like to do it by time because some of these people have been waiting for a long time and I'd rather get to them sooner rather than prioritizing sort of arbitrarily things for a particular repo. But that's a personal preference. Normally there aren't enough that I need to batch, and I think it would make sense to do them by repo too, just to keep the sort of cache state. Although for my sake it doesn't matter too much. I know these projects well enough that I can context switch between them fairly aggressively.
03:06:17.385 - 03:07:11.377, Speaker A: I think for the purposes of a stream it might make more sense because as you've probably observed by now, I jump around a lot between these different projects and that's harder for you to keep up with because you don't know these as you haven't been exposed to these as much as I have. And so the jumping is maybe jarring. So it's a good observation. Okay. Webber Unsoundness and Safe code for 2022 closed in 38. Oh, that's great. So this is something that we already fixed in 2022.
03:07:11.377 - 03:07:47.015, Speaker A: We just didn't Close the issue. Nice rocket ship because it was already fixed. Beautiful. Document Cargo features in ReadMe users who create who use this crate as a library will very likely want the CLI feature disabled. And it's also not clear to me what the name after feature is for even after looking at the code. So the section in the readme or maybe the root of the crate, docs the list, what every feature is and which are enabled by default would be really valuable. I completely agree.
03:07:47.015 - 03:09:11.305, Speaker A: I really wish there was a standard way to document features in Cargo Slash the REST ecosystem Feature document features in the Cargo Rust ecosystem. There's some discussion in but no hard guidance as of yet. Maybe you so this person has been a contributor for a while and knows the code base pretty well too. You could take a stab at this after you land. 298, 297, 295, 296. Where's the one where they fix it? They have a draft PR 294. Great.
03:09:11.305 - 03:10:18.005, Speaker A: Okay. Yeah, I wish this discussion went anywhere, but it did not really. Okay, release OpenSSH version 10. Yeah, that seems reasonable. I'll do that off stream though is basically at this point this person is like the main contributor to OpenSSH and the main driver for bringing it forward and it's unfortunate that they end up bottleneck for me on me for doing like new releases and stuff. So what I'll do is I will give them permissions to like run CI and stuff like basically admin privileges on the repo and then also give them publish privileges on crates IO. But I will do that off stream so I will save this and market doesn't run.
03:10:18.005 - 03:10:38.165, Speaker A: What's the time? It's basically lunchtime. I think this is a decent place to stop. We caught up two weeks ago. That's pretty good. We're at 86 and we started at what, 120 something. So we went through 40 issues. I'm happy.
03:10:38.165 - 03:11:12.705, Speaker A: I'm also gonna mark this one as done because I see that it has been superseded. Okay, I think that's where we're gonna stop for today. I'll pro. What I'll probably do is I'll do another one of these maybe next weekend if I have the time. Like I have to catch up with this backlog anyway, so I might as well do it on stream. Hopefully you found it useful and interesting and thanks for joining me and watch out. If you're watching this on recording, there might already be a video that continues from this exact point in time.
03:11:12.705 - 03:11:15.325, Speaker A: So thanks and I'll see you later.
