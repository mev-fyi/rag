00:00:02.720 - 00:00:03.256, Speaker A: Cool.
00:00:03.273 - 00:00:30.885, Speaker B: All right, guys, let's get started. So today we have a guest lecture by Zhang Jengset. He was a PhD student with us a number of years ago, and he ta'd this class many times. And he's built a bunch of things in this class, including the anonymous question system that you should feel free to use to ask him questions. But also he built a number of lab assignments and lectures and stations for us. So he knows a lot about this class. And when I asked him to give a guest lecture, he said one thing we don't actually teach well enough is actually reasoning about the security of lots of software.
00:00:30.885 - 00:00:39.157, Speaker B: And he thought, you guys should know about this from his experience working at Amazon. Now he's working. Well, he'll say what he does there, but welcome.
00:00:39.261 - 00:01:05.405, Speaker A: Thank you. Hi, folks, My name is John. So, as Nikolay said, I am previously from mit. I was in PDOS for many years and then now I work basically maintaining all the rust build tooling and infrastructure at Amazon Web Services. I'm John Hu on the Internet. If you ever want to find me anywhere else, feel free to send me questions after this lecture if you're interested. Like, I'm always happy to talk about anything, really.
00:01:05.405 - 00:01:46.459, Speaker A: And use the anonymous question system. Like, if you don't feel comfortable raising your hand, then just use it. I will happily take questions whenever you have any. That's the best way for you to learn and for me to figure out what I'm not explaining well. So when Nikolay reached out to me asking, you know, what should I cover? He basically gave me the open floor of, like, what do you want to talk about? And I looked through the semester notes and I was like, there's one thing that I spend a lot of time thinking about at Amazon that is not represented in the curriculum at all. And it is supply chain security. And over the course of the next hour, I'll hopefully be able to convince you that supply chain security is hard and important and we don't know how to do it well.
00:01:46.459 - 00:02:41.563, Speaker A: And hopefully you'll be scared by the end and then you'll go do your research and figure out how to do it well. So what does supply chain security really mean? Well, supply chain security is basically the observation that all of the software that you use matters, with an emphasis here on all. Not just the code of the current thing you're building, but all of the other software that's part of that pipeline, be it the dependencies you take, be it the other software that's present on the host, be it the stuff that you use in order to build your software, all of that. And it's not just whether that code is insecure, it's also whether it could be insecure, whether it could be manipulated. And we'll look at some of the ways in which that both can happen, but also has happened repeatedly in the past. Where I want to start here is there was a survey done of projects, both open source and private, over the past many years. There's a survey that runs basically every year from a company called Sonotype.
00:02:41.563 - 00:03:16.793, Speaker A: And one of the things they found was that there's been almost 800% increase year over year in the number of supply chain attacks over the past three years. So this is clearly a growing cause for concern. And not just that, but six out of every seven vulnerabilities and projects come from their dependencies, not from the project itself. It is not that you wrote a bug in your code, it's that other people wrote bugs in their code and you're using what they built. And what's interesting is it's not just this one study. This pops up everywhere. So there's this organization called anisa, which is the European Union Agency for Cybersecurity.
00:03:16.793 - 00:03:51.795, Speaker A: And how that ties into that acronym I'll leave you to find out on your own. But they ran a survey this year where they tried to figure out on behalf of the EU, what are the biggest cybersecurity threats for 2030. And the number one thing is supply chain compromise of software dependencies. And that's what we're going to talk about today. Now, interestingly, in the eu, we've started to see regulation for this. So there's regulation now in the EU that requires companies and parts of governments as well to document how their supply chain is secured, both hardware and software. We've seen this elsewhere too.
00:03:51.795 - 00:04:42.661, Speaker A: Like in Japan, there was recently a law passed that mandated that both certain government branches, but also key critical infrastructure like the power grid, was required to guard a lot of their supply chain. Because that's where they worry about like state nation actors coming in and attacking their infrastructure. Japan is not the only place. Even here in the US we got an executive order in 2021 that basically said, among other things, fix your damn supply chains. And then of course, there wasn't really anything that came out of that for a while, because figuring out how do you fix that is really, really hard. In 2022, NIST put out this 400 page document on how to like, even reason about whether your supply chain is secure and what's in it. And I don't recommend you go read this document.
00:04:42.661 - 00:05:47.861, Speaker A: It is very long and there are a lot of words when there don't necessarily need to be. But some of the things that we'll talk about in this talk are very brief summaries of what came out of this document and other related kind of service that have happened in the US There aren't any laws yet that require you to secure supply chain or even prove that you know what your supply chain is. But we expect that to actually come pretty soon, probably in the next year or so. And there are a lot of companies that are scrambling to try to get on top of this now, ahead of when there might come regulation. The UK is also giving out supply chain guidance mapping, trying to tell companies, how do you even figure out what all your dependencies are and how should you represent them? And so this is a worldwide phenomenon that we're seeing and something that the industry is only really starting to realize over the past three to five years. And when you have a moment, I recommend you go to this particular URL. This is from that Sonotype survey that I mentioned earlier, where they go through the past 25 years or so and look at all of the issues and attacks that they've seen that depend on supply chain vulnerabilities.
00:05:47.861 - 00:06:20.031, Speaker A: And it is filled with huge problems and it is a very scary thing to read. And if you look, especially over the last few years, you see a lot of them crop up and we'll talk about some of them a little bit later. What's particularly worrying about supply chain security is when you talk to engineers versus managers. Engineers all know that this is a problem. They all go, we have all this code here that we don't know how works. We haven't checked it, no one has looked at it, we don't know what it does. And managers all kavanaugh and claim we have a total overview of everything we use.
00:06:20.031 - 00:06:49.519, Speaker A: We know it's all fine. If there's ever a problem we can fix, fix it within a day, no problem whatsoever. So this is clearly a problem that engineers know is real. But further up in these companies, people are like, yeah, it's probably fine. It really is not. So what really is supply chain security? Well, supply chain security is basically these three questions. Do you know what you are deploying? Where do you know where it came from? And do you know what's in it? And the answer to all of these three better be yes.
00:06:49.519 - 00:07:54.475, Speaker A: But in practice it's almost certainly no. Or at best, eh, sort of. So let's go through these one at a time. Let's start with what are you deploying? Where? So this is all about really deployment logging. This is about being able to answer questions like this, like what software is currently running at a given host? What software was on this host at some given point in time earlier? Why did a deploy happen to a given host? Where are the artifacts of some version of some software deployed? At what time were we no longer using a given version anywhere in our deployments? And what configuration did that software have on that host at that time? Some of these are for knowing where we might be vulnerable, right? Like if you learn there's a vulnerability in some software that you're using, you want to be able to say, well that's in use on those 10 boxes over there, but not on this data center over here. So that's sort of very standard. But some of it is for where and when were we vulnerable? So you discover that there was a vulnerability that was first like started being seen in the wild in May of 2021.
00:07:54.475 - 00:08:48.381, Speaker A: Do you know exactly what window you were vulnerable during and which of your hosts may have been compromised? This is all in order to figure out after the fact where you might now have problems you need to go investigate. Some of them are for proactive analysis, like how many different versions are we using at once? Like imagine you pull in OpenSSL into your dependency tree for your software. There are a lot of versions of OpenSSL out there. There are a lot of variants of OpenSSL. If you discovered that you're using like 15 different versions across all of your fleets of hardware, that's probably a problem and you should go clean that up and this kind of analysis can tell you that. Note here that I'm using artifacts of software version and not software version. And this is because, as we'll talk about later, it can be really, you can get into really weird situations where it's a compromise of, let's say your C compiler.
00:08:48.381 - 00:09:37.655, Speaker A: If your C compiler is compromised, then your C compiler isn't deployed to any of your hosts. But things that you built using that C compiler may be deployed to lots of your host. So you want to look at that whole supply chain. Now the conclusion from this particular segment is really you should just log everything. And in fact, every time you deploy software anywhere, you should log exactly why and how that happened, how was it initiated, when did it happen, what went into it and what was deployed to. And by now, you know, as security conscious students, you're aware of the fact that this system that does the logging is itself something that needs to be audited and logged. Because if a hacker manages to subvert your logging, then they could just hide the fact that they were that they did anything bad in the first place by obscuring your logs.
00:09:37.655 - 00:10:37.545, Speaker A: So this system needs to be append only. It needs to be durable, it needs to be kept long term. Like, you might not know that there's a vulnerability in the wild until like seven years later, in which case you still need to be able to go back and figure out that there was a problem. This first one is a little weird. Like, why track how it was initiated? And what does that even mean? This comes from a class of attacks that we've seen a lot over the past, I want to say three years, which is around leaked credentials from continuous integration and continuous deployment systems like Travis, CI, Heroku, GitHub Actions, those kind of things, where those are pipelines that usually have access to your deployment keys and your deployment secrets. And so if one of those services get compromised, which happens all the time, then suddenly, now people have the keys to deploy software on your behalf. And so you really want to trigger what caused this, which credential was used in order to publish a new version of our software so that you can then go back in and validate that if necessary.
00:10:37.545 - 00:11:16.059, Speaker A: Um, and this append only bit is important too. Even if you roll back your software to an earlier version, like, you just, you deploy something, you discover there's a bug, and now you deploy the old version again because it's going to take you some time to fix it. You want to record that there was a period of time where you were using that other version of the software in case that was the window during which you were vulnerable. So everything has to be logged for a long period of time. And every host matters here. We're not just talking about, like physical servers that you put software on. If you put things on developer environments, you need to track that, because those might be compromised.
00:11:16.059 - 00:12:03.519, Speaker A: If you deploy to beta environments or testing environments or benchmarking environments, you need to track those. If you deploy to embedded devices like robots or drones, you certainly want to know what every single one of them is running at any given point in time. Customer devices are even worse. Like if someone, if you deploy software to like Android phones or to pacemakers or something like it, you really want to know exactly what version of every piece of software went into that. So if there is a problem, you can notify the people that it might affect and hopefully update them in time. There are other environments too, that aren't even really computers, that are just sort of abstract computing resources like Lambdas or Cloudflare workers or whatever it might be. That's also software that you run where a compromise there might lead then to a compromise of your database.
00:12:03.519 - 00:12:51.329, Speaker A: And you really want to track what goes in there too. Okay, so let's move to the second question here, where it came from. This one is surprisingly difficult, but let's start with a relatively simple question. Can you trace every artifact that you deploy back to sources that you trust? And this turns out to be not quite a turtles all the way down kind of problem, but it's pretty close, as we'll see. You want a verified path from only trust anchors to your deployment. And what do I mean by trust anchors? Trust anchors and security are sources that you assume are trustworthy rather than derive are trustworthy. So these are things like you might just say, I have a contract with Microsoft.
00:12:51.329 - 00:13:26.495, Speaker A: I'm going to assume that everything that's signed by their key is indeed from Microsoft and they have their shit together. You might assume that it might not be a sane assumption, but you might assume that. And you might assume it because it's OK if it turns out something is wrong there, because you can go sue Microsoft because you had a contract with them. Right? So these are often soft kinds of things that affect your security assumptions. Sometimes the assumption is, I trust the software that a niche writes. So if it came from a niche's private key, I'm fine deploying it, but anything that I have to derive from that, it has to derive back to Anish's key. Otherwise I don't trust it.
00:13:26.495 - 00:14:27.531, Speaker A: And there's sort of two paths here. One is, if you just downloaded an artifact from the Internet somewhere and then you deployed it, then you have a couple of questions to ask yourself. The other is, if you built it yourself, you have to ask a different set of questions. If you downloaded it from the Internet, then the question of course is do you trust the entity that built it and do you trust the entity that you downloaded it from? And this might be a simple yes or no question, depending on where you got it from. But it's also a question of how do you know that that entity actually built it? If you download something from GitHub releases, GitHub didn't build it. Some engineer built it and uploaded it to GitHub. Do you trust the engineer that built it, and do you trust that that's the same person who authored the software in the first place? And even if you think you Know what entity published the thing that you downloaded? Did that entity verify? The other questions we're about to get to that you should ask yourself when you build something, and crucially, how do you know? Even if someone puts on their website, we checked all of these things, trust us, they have a stamp on there and they'd sign their name.
00:14:27.531 - 00:16:29.775, Speaker A: It doesn't really matter if you don't have a way to verify that they actually went through the steps, because then that just means that you are trusting them by assumption. So what are these questions you might ask yourself if you are building from source, which we generally assume to be the safer thing to do? Right? You build it yourself. Well, how did you get the source? Where did you get it from? How do you know that this is actually the true source for the software that you're intending to build? Is that source what the author intended to publish? Like you got it from GitHub, but does that mean that the author intended to publish that code to GitHub and there's been no interference in between? Do you trust the tools that you downloaded the source with? Do you have a malicious git installed on your computer? Malicious curl, malicious kernel, malicious router in the middle, and you didn't verify your TLS certificates? Do you trust the tools that you verified the source with? Let's say you downloaded it and you ran like, you know, GPG verify signature, and you have the public key of a niche and it says, yes, the niche signed this. You trust that you don't have a malicious GPG on your box? Maybe. How do you know? Do you trust the tools you built the artifact with, your gcc, your clang, your, you know, Glasgow Haskell compiler, whatever it might be? How do you trust that? Do you trust the host that you're building the source code on? If you're building this on your laptop, what's the chance that your laptop has some other nefarious software on there that might actually be running in the background and modifying the source code just between when you check the signature and when you run your compiler? Now, all of these sound like I'm just being paranoid, but in reality, this is what we mean by supply chain security. You want to make sure that there's no point in the pipeline from, like, the author's brain to your deployed artifacts, where someone can sneak in and modify or infect in some way, that supply chain. And hopefully even just from these questions, you realize this is really hard because there are so many places this can go wrong, and some of these are just fundamentally difficult.
00:16:29.775 - 00:17:03.535, Speaker A: Problems as an example of some of this. So in Java there is this website called Maven Central that is a repository of third party software. So anyone can upload software to Maven Central. And if you're using Java, it's relatively easy to take a dependency on something that someone has published there. Now, in Java, generally what people publish are jars or Java archive files, which are essentially just binary blobs. They're a zip file of Java bytecode. So if you get one, you don't really know what the source that went into that is.
00:17:03.535 - 00:17:37.194, Speaker A: But Maven Central has thought of this and they allow you to publish the source code alongside the jar. So you as an author can upload a jar and then also upload the source code. The problem is there's nothing that guarantees that the two actually map in any meaningful sense. The two are separate artifacts in Maven Central and one is source and the other is the jar. But you don't have to give any guarantee that that jar came from that source. It's just whoever uploaded it gets to say that they were the same. But there's no way for you to verify that that is actually true.
00:17:37.194 - 00:18:05.115, Speaker A: Ultimately, you need to choose what you're going to trust here, and that might be the authors. Like you might want to go all the way back there. It might be that you mark particular instances of source code as trusted, like this hash of the OpenSSL source code I, John, have looked at and I know it's ok. And therefore you build it. And that way you don't need to trust the authors. Maybe you trust tools that you run over that source code to do vulnerability scanning or whatnot. You trust that those are ok.
00:18:05.115 - 00:18:44.827, Speaker A: But you have to trust something. And figuring out what those things are and making that explanation in your threat model is really, really important because it can come back to bite you otherwise. And tainted sources are real. And what I want to do now is give you some examples of all of the ways in which these parts of the supply chain have been exploited over just the past two years. So in 2021 there was a big hubbubaloo, especially in the sort of commercial world around dependency confusion. There was a researcher who observed that a lot of software these days takes third party dependencies. Great.
00:18:44.827 - 00:19:15.545, Speaker A: And they wanted to try to hack into PayPal for unrelated reasons. And as part of that they found some public source code that was part of PayPal. It was on a GitHub repository somewhere. There weren't any vulnerabilities in that code itself, but they found this little bit of code. And it might be hard to read the red text here, but these are just more package names that Happen to include PayPal in the name. And when they looked really carefully, they discovered that all the ones in blue here were on the public registry of Node JS packages. Npm.com
00:19:15.545 - 00:19:49.155, Speaker A: totally normal packages someone owned. The ones in red though, were not present there. So they were presumably some kind of internal private PayPal dependencies that weren't published anywhere. And the security researcher went, I wonder what happens if I publish one of these? Like I'm going to take pplogger and I'm just going to create it on npm.com and I'm going to stick my own code in there. They did so, and the code they stuck in there didn't do anything nefarious. It would just ping their server, like a server that they controlled to let them know if someone had downloaded and tried to build the package.
00:19:49.155 - 00:20:47.245, Speaker A: They did that for these packages and within about 15 minutes of publishing it, they started getting pings to their server from inside of PayPal. And then they started doing this for a couple of other package names that they thought might be in use of different companies. And over the next couple of days they managed to get direct contacts, like there were servers at these companies contacting their build server and could run arbitrary code as part of the build at Tesla, at Uber, at Yelp, at Netflix, at Apple, at Microsoft, at PayPal, and the list goes on. Because these companies didn't have their build infrastructure configured in such a way, they would prefer first party like private internal dependencies to third party ones. The way their builds would work is that when they look through this dependency file, they would first look on npm.com and if it was there, it would stop searching and just use whatever was there. And only if it wasn't found there would it go to the internal repository.
00:20:47.245 - 00:21:17.067, Speaker A: And so all of these companies found themselves scrambling for fixing their internal build infrastructure. Another example of this is the SolarWinds attack from 2020. This is a little over two years ago, so I apologize for lying to you. Now, SolarWinds you may have heard of, because this was a big news event. It was called the largest cyber attack ever. And the subtleties of the attack are a little convoluted. But very briefly, SolarWinds was a company that produced a piece of software called Orion.
00:21:17.067 - 00:22:11.487, Speaker A: And Orion was sort of an IT security monitoring software. And I know, oh yes, the irony, this was a piece of software that SolarWinds sold it to companies for companies to install on all of their hosts and especially any like laptops or desktops that they gave to their employees to monitor things like do they have all the security updates installed? Is there any nefarious software running on this host to give some kind of like remote management, admin capabilities, logging of like temperature, like just things that it wants to monitor about the devices that they own. And SolarWinds Orion had a built in auto update feature like a lot of this software does. Seems pretty reasonable. It would download updates from a server the SolarWinds controlled and those updates were signed by a private key at SolarWinds. Seems pretty reasonable. And then one day there was a new auto update for SolarWinds Orion.
00:22:11.487 - 00:22:51.975, Speaker A: And this auto update happened to include a DLL, like a Windows shared library file that included a backdoor that would dial out to a server, fetch a payload and then execute it locally. The update was signed, so it came from SolarWinds. No one really knows how it got signed. We do have an idea of how it managed to get picked up by the autoupdate system, which is SolarWinds was running an FTP server that this software would connect to and download the update. The password on the FTP server was SolarWinds123. And so of course people would upload files there. We still don't know how they managed to get a valid signature in there though.
00:22:51.975 - 00:23:59.381, Speaker A: But this just demonstrates that it's hard to get these things right. This company was signing their updates and yet it still hit them. How would you even detect if something like this was happening? If you were running a company and you installed this to monitor the security of the host that you manage, how would you detect that there was a backdoor in the thing that was supposed to detect if bad things were happening on the host? Another example came up in 2021 and this is a different kind of attack. Again, all of these are different attack vectors. So you need to think about Some researchers at the University of Minnesota ran a study on the feasibility of stealthily introducing vulnerabilities in open source software via what they called hypocrite commits. And hypocrite commits, by their definition was essentially a commit, where the author is introducing a legitimate bug fix or feature into an open source project, but as part of the code that they submit, that code has an intentional error in them that they will later exploit. And the way they chose to do this was they submitted changes to the Linux kernel and these were legitimate fixes to the Linux kernel.
00:23:59.381 - 00:24:39.343, Speaker A: Like they actually fixed problems or introduced features that people wanted. They went through reviews and they almost ended up landing on the main branch of the Linux kernel. And then it only got caught at the very last end where someone said this code doesn't look quite right. And it was like some off by one error, like it looks completely fine. But the authors knew it was there and were placing planning an exploit later on for when it actually made it into Linux. And this is really worrying because it means that if it had made it into the Linux kernel, this is the true source code, like endorsed by everyone who reviewed that code. If you downloaded and checked all the signatures, everything would check out.
00:24:39.343 - 00:25:09.721, Speaker A: But the moment you deploy that software, you're now vulnerable to the attacker who introduced those commits in the first place. In this particular case, the Linux kernel banned the entire University of Minnesota from making contributions to the Linux kernel, which we can argue about whether that was the right solution or not. But clearly this is a worrying problem. This attack vector is very real and scary and we don't have great defenses for it. Except make sure you review code well. Right. We also have.
00:25:09.721 - 00:26:03.149, Speaker A: I alluded to this earlier credential leaks and these happen constantly. There's a company called GitGuardian that runs a sort of survey every year of just public credential leaks. They scan things like GitHub repositories and they just look for secrets that people have committed. Like you get a secret token from AWS or you get a Secret token from GitHub Actions or Travis or Heroku and you put it in your source tree and you commit that file and you push to GitHub or you commit your like SSH private key on GitHub and then you happen to have a public repo. They found that in 2022 there were 10 million new sequences exposed on GitHub. In 2022, one GitHub code author out of 10 exposed the secret in one of their public GitHub repos. This happens all the time.
00:26:03.149 - 00:26:43.589, Speaker A: Travis CI had a break in in 2022 where attackers stole 100,000 npm.com logins. This means that the attackers were able to publish new versions as though they were the author of 100,000 npm packages, if not more. This is worrying because it means that if you take a dependency on anything from npm.com how do you know that the version that you downloaded was actually published by the author that's listed there? It looks like it was uploaded by the right person, but it could just be someone who has that credential. This makes it hard to trust that any third party artifact or code is actually from the author. And you might say, but John, just have them sign it.
00:26:43.589 - 00:27:21.445, Speaker A: And we've talked about how there are problems with that too. But even even if you could, many of these repositories like npm.com like rust crates, that IO like PI PI do not support signing. You just cannot sign sign these packages that you upload in a way that the standard tools will verify. So there isn't even a way to sign, except you have the token that lets you upload. And sometimes these registries like npm.com and I'm picking on NPM here just because there are a bunch of examples and they're pretty large, not necessarily because they're doing anything particularly bad, but those registries can be compromised themselves.
00:27:21.445 - 00:28:19.065, Speaker A: If someone hacked into npm.com they could publish new versions of any dependency that they they wanted or modify the code of existing ones if you want to go to something else. Like in. In 2021 the PHP projects Git repository was hacked. So PHP, for various reasons decided to run their own git server rather than use something like GitHub or GitLab. And running your own infrastructure is always hard, but in this case their git server got hacked into and then someone introduced a commit into the PHP git history authored by one of the lead developers with the subject line fixed typo and the contents of the diff of that commit was introducing a remotely executable backdoor into all PHP servers. So if a new PHP version had been cut from this commit, then anyone who ran PHP as of that version you would be able to get remote access to just by passing a particular HTTP header.
00:28:19.065 - 00:29:04.771, Speaker A: How do you detect that this happens? In this particular case they managed to find out about the break in a couple of hours later and managed to revert that commit. But again, this is modifying like the version control. The thing that you would assume is the ground truth for a project, and it looks like it's from a legitimate author too, because no signing was involved. We also have rogue maintainers. Like sometimes it's not even the process that's the problem, it's the actual author that's the problem. In 2022 we had a bunch of instances of this. So for example, there was an open source developer of the Faker JS & Colors JS JavaScript libraries who was just fed up with companies using their code without paying for it and decided to just completely corrupt those packages.
00:29:04.771 - 00:29:36.381, Speaker A: They published new versions of each of them that got automatically picked up as a dependency and those corrupted versions would Just break your software. They would print some string like pay money or something 10 times and then just exit your program. This is a legitimate version from the author, but it is malicious. This top right one is funny. This was a sort of civil war amongst developers who published packages that helped you write malware. So on npm.com there are a bunch of packages that are intent.
00:29:36.381 - 00:30:21.405, Speaker A: They're like libraries for malware authors. And there was like a feud between some of them where one of these libraries, if it detected that the other library was present on your machine, would go in and rewrite that other version of the software to do something else. Again, this is the author being malicious. This bottom right one too. This was a maintainer who was wanting to protest the invasion of Ukraine. And the way that they did this was they modified their JavaScript library to have a background thread that would constantly ping a server and look up your GUIP range. And if it was Russia or Belarus, it would corrupt all the files on your hard drive and make the contents of them be just the heart emoji.
00:30:21.405 - 00:31:04.541, Speaker A: Like, how do you defend against something like this if you have a dependency on this thing? That was totally fine in the past. This was the Node IPC package. This is difficult, right? Because ultimately we have to either choose to trust particular instances of the source code or the author. And if you choose instances of the source code, getting new updates is really hard. If you choose the author, they can do things like this. Back in 2016, just to demonstrate how far back this thing goes, the Linux Mint distro had someone hack into their download server and modify the disk image that people would download in order to install Linux Mint. And they replaced it.
00:31:04.541 - 00:31:44.663, Speaker A: And I think it was replaced for about a month or so. That version of Linux Mint, if you had downloaded it during that time period, you would install Linux, everything would seem to be fine, but your kernel would have a backdoor into it. They hacked into the website, so all the hashes checked out, because the hashes are on the website. How would you detect that this happened? And fighting these kinds of tainted sources is really difficult. There's no doubt about that. There are some things we can do to make things better. Things like SIG Store, which is a protocol for signing artifacts that go up into package repositories.
00:31:44.663 - 00:32:43.255, Speaker A: There's stuff like the update framework, which is a protocol that registries like NPM or PYPY or Crates IO can use to ensure that new versions cannot be masked from other users, that new versions to get released do have to be eventually displayed. That the registry can't start modifying versions of software that they've previously published or claim to release new versions without the knowledge of the author. So that helps a little bit. You can mandate two factor authentication for publishing to any of these registries. To mitigate things like leaked credentials, you can have automated monitoring of known risks, like if someone files a CDE for, let's say this colors are JS library, you would want to know immediately in an automated fashion that you're now at risk, right? And so some of that can help. But ultimately you're at the mercy of authors. And what this means is when you're taking dependencies, you want to think really critically about who you are willing to take dependencies from.
00:32:43.255 - 00:33:35.075, Speaker A: Is it a company or is it an individual? If it's a company, is it one that you can have a contractual connection with? If it's an individual, can you do the same? Can you pay them to make it less likely that they do something bad or otherwise? Just generally, how trustworthy are they as someone that you think you can rely on over time? It's difficult. There's a lot more to this list too. Think of things like, if I, as the publisher of some source code, if my box that I'm publishing from is compromised, then I can do all the checks that I want and I cannot have leaked my credentials anywhere. But something else might still tamper with the source code before it gets published. There are tools here like automated scanners that try to detect nefarious code patterns or whatnot. You can use those, but they're ultimately all heuristics. They help a little bit, but they're not going to solve this problem entirely for you.
00:33:35.075 - 00:34:15.981, Speaker A: This reminds me of this quote from Carl Sagan. If you wish to make an apple pie from scratch, you must first invent the universe. It feels a little bit like that, right? Where if you don't want to have any vulnerabilities in your dependency closure, you have to write everything yourself. That includes your compiler, includes your kernel, includes like everything. And so ultimately you have to choose somewhere where you say, stop, I'm going to trust this thing. And hopefully with some decent reasons for why you think that thing is trustworthy. Ok, third question, what's in it? And this might seem similar to the previous question, like, what's the difference between where it came from and what's in it? But what's in it is a little bit different.
00:34:15.981 - 00:35:00.895, Speaker A: This is a list of all the stuff that ended up getting pulled into the build or affecting the build, so that later on you have some insight into all of those like edges in the dependency graph that you might need to think about. Ultimately you might have one artifact, you might have a single binary that you deploy to your hosts, but you have many inputs to that. And we've talked about regular dependencies, right? That's one thing. But you can have dependencies from the build host. Like imagine that I have OpenSSL just installed on my computer. It's not getting pulled as part of like my NPM dependencies or my Rust dependencies, but it is linked into the final binary or it might even be a runtime dependency. So only at the time when I run it does it like Dynamically link with OpenSSL.
00:35:00.895 - 00:35:45.305, Speaker A: These kinds of dependencies from the build host need to be captured downloads during the build. There are a bunch of packages in the open source ecosystem that when you do the build they'll helpfully go like, oh, you don't have Lib SSH installed, so I'll download it for you, build the source code and then include it in the binary that I ship. Very helpful. But it does mean that now it downloaded Libss as part of its build and you need to track that you have that dependency in there. Because if there's a vulnerability in Libss, you need to rebuild your software and update it because it ultimately made its way into that binary. Same with vendored or inline sources. Sometimes projects will just copy paste source code from some other project or the entirety of that other project source code.
00:35:45.305 - 00:36:25.175, Speaker A: You really want to track that too, because now there's a dependency edge between you and that project that you might need to track. Sometimes they just bundle binary artifacts. And I'm particularly here looking at the Nvidia graphics drivers for Linux where there's just a binary blob in there that's just like this is the firmware that Nvidia gave us and we have to use this blob and no one really knows what's in there. You probably want to track that. You pull that into whatever you deploy to, because there might be issues with that as well. And of course any of these above transitively. So even if none of your direct dependencies have these problems, your transitive dependencies, your indirect dependencies might have these problems.
00:36:25.175 - 00:37:22.903, Speaker A: Finding all of these is really tricky, even if you have the source code, and if you don't, it's doubly so. There, there exists some software that tries to just like automatically find all of these kinds of semi hidden dependency edges for you, but they're all heuristics based, they're all best effort. And ultimately here heuristics will only get you so far. We need something that can do better, that can truthfully represent every piece of software or code that is in your actual software that you deploy. And so that's where we get to the sort of, let's call it the second main topic of the day, which is the software bill of materials. The software bill of materials, or bill of materials in general, is essentially an inventory, sort of like a recipe, but really more of a list of ingredients. It is a list of all of the things that made their way into or affected a given software artifact.
00:37:22.903 - 00:38:09.913, Speaker A: And a lot of the things that I talked about earlier, like the EU directives, the US directives talk about companies having to supply these for every piece of software that they have deployed anywhere. And so this is something that we're seeing increasingly being adopted by build systems by companies and being mandated by law. Now, software billing materials, as we'll get to in a second, are also sort of a trust exercise because they're not verified. They are an assertion by the author saying, I pulled these things into my software. They are a contribution from the author that you may or may not trust. The observation here though is that it's better to have this than to only rely on heuristics. Ideally you do both.
00:38:09.913 - 00:39:19.031, Speaker A: So you have a declaration of what went in, and then you also have detection of other things that might have made it in that aren't in the bill of materials. And ultimately that should give you a more complete overview of all of the places where you might have vulnerabilities. And these bill of materials have existed elsewhere ages. They started in car manufacturing and since they've just been deployed everywhere from like the aviation industry to bike manufacturing to book manufacturing, everywhere. And the basic premise is that you list, like in car manufacturing, for example, if you have an engine in a car, you list all of the parts and you list when they were made, what factory they were made in, how did they, what was like the transport route that they took to where they are, when were they produced, who put the engine together, what location was it put together at, so that you have a full inventory of anything where, if there's a problem with any of these things that we know that this engine was affected. And it turns out that having this inventory helps for a lot of things, it helps for designers. So if you are designing, whether it be software or a car engine, you can say, this part goes here.
00:39:19.031 - 00:40:17.035, Speaker A: And when you say this part, there's some actual Meaningful way to identify that part. And as for sales, like, which parts do I order in order to build one of these engines in software, this might be something like which licenses do I need to pay for in order to be able to deploy the software? Or which software in my deployment stack includes gpl? And so maybe I need to think twice about whether I'm using it in this way. It helps for manufacturing. Like when you're putting a thing together, which part goes where? Think of it like a Lego instruction manual or Ikea or something which is like piece like 3B goes here in repair, which part broke in software, this would be something like debugging, where you might want to go back and say, well, our software broke when we bumped the version of lib SSH from 1.2.3 to 1.2.4. That's when our application stopped working. And so you know that by virtue of the fact that your bill of materials says that was the change between these two deployments.
00:40:17.035 - 00:41:21.841, Speaker A: And recall, is the affected part present? If someone discovers that a particular engine part had a manufacturing error in it, they can go back and look at what are all of the places where like this batch of parts that we know is faulty made it into. And you can recall from those people saying these things are now problematic. And in software, you know, it's a similar kind of thing where you publish something like a CVE saying this version of this software is known to be bad, don't use it, and hopefully the people on the consuming side know to look for those CVEs. Now, this kind of provenance or origin information is useful for a number of different things. It's useful for security, right? It tells you if something is at risk, something like a cve, but it can also tell you how it's at risk. So for example, if, let's say there is a vulnerability in Bash, actually, BASH is a terrible example because it affects everything. Let's do libss or libz, the compression library.
00:41:21.841 - 00:42:09.955, Speaker A: So in libz, let's say there's a problem in libz and LIBC is used to build the Haskell compiler, which is used to build the documentation generation system for a test package of a dependency that you use. It's probably fine if there's a CVE for libz, it probably does not affect your stuff. It might, but it's a very extreme path. So by virtue of knowing that that's the way that you got libc, you actually know that you don't need to care about this problem. Or similarly, if you detect that I'm not using LIBC anywhere, then you know you're not at risk. There are a lot of CVEs published every day and not having to deal with all of them is valuable. And of course, obviously it can tell you if you are at risk and how, and you might need to deal with it in that case.
00:42:09.955 - 00:42:57.335, Speaker A: As I mentioned, it can help with license and compliance information, but it can also help with supply chain funding, at least in theory. It will tell you all of the open source projects that you depend on, and maybe you should go pay some money to them so that the authors don't turn malicious or just, you know, out of the goodness of your corporate heart. But there are all sorts of ways in which you can use this. Use having this inventory to improve your processes, like waste identification. Like I mentioned before, if you have 15 versions of OpenSSL, maybe that's a problem. But even imagine that you discover that you have like five different implementations of parsing X509 crypto keys. Well, maybe you should just have one of those parsers, because one is bad enough, right? Just to limit the amount of exposure that you might have both to different kinds of authors, but also two sources of bugs.
00:42:57.335 - 00:43:57.701, Speaker A: Quality assessment is one you can do too. So you can go through your dependency graph and look at which of these things are no longer maintained, which of them are end of life from the manufacturer, which of them have now been taken over by a different corporation, and I no longer trust it. Which of these haven't had updates in the past six years, and so we assume it's been abandoned. Having this inventory lets you answer those questions, or at least know where to start. There's also an argument that often comes up with bill of materials that this is really a roadmap for the attacker, right? Like you're telling the attacker everything that goes into your software and maybe that's bad. And it is true that it's a list of potential weak points, but at the same time, attackers already have a sort of leg up when it comes to exploiting your software, because usually they have a list of things that they know how to explain. And so they really just want to see are you vulnerable to any of the things that they know how to exploit? And they can do that by just probing for the weaknesses directly.
00:43:57.701 - 00:44:51.525, Speaker A: They don't need this list. And in some sense they don't care about this list because if this list is missing something that they might be able to exploit, they would still want to exploit it, right? They already have decent heuristics and other kind of channels for figuring out is this a kind of target that I would want to attack. So this kind of software bill of materials is more incrementally useful to us as defenders than it is to the attackers. Now, what goes into nasbom? I've talked a lot about the sort of abstract notions of why they're useful, but what actually is it? Well, it's a hierarchical list, and we'll get back to what a hierarchical list even is. But it's a hierarchical list that includes a list of records, each one holding the following eight fields. The component name. The component name is just like the name of the library or the name of the software.
00:44:51.525 - 00:45:28.807, Speaker A: The version string, which is the version. The hash, which is a cryptographic hash over the artifact that it's talking about. So if you brought in, you know, OpenSSL into your build, you might say this is OpenSSL version 1.01 F or something, and it has this hash. The reason you want the hash there is so that you can detect that you're actually using the same 101F as was published by the author and not someone that's been manipulated in the meantime. The uid, which is a sort of unique identifier used to distinguish if there are multiple variants of a given version. Like, it might be that I build, you know, 1.2.3
00:45:28.807 - 00:46:36.525, Speaker A: of my software, but there's a developer edition and there's a nightly version, and there's a, I don't know, beta version, and there's a production version and there's a pro version, there's a home version, enterprise version, and all of them are the same version, but there are different variants. And so you could track that in that field. The supplier name and author might be a weird set of fields, but they're there to allow people to incrementally adopt these bill of materials, which is, if I build my software and I take dependency on something Anish has written, I might have a bill of materials for my software, but Anish might not have published any for his. And so as a result, I might want to populate some rows on Anish's behalf in my bill of materials, just so I track those dependencies as well. So the supplier name here is who wrote the software, who is the supplier of this software? The author is who wrote this row of records. So if the supplier name is equal to the author name, that means that it's the author making an assertion that this is what my software includes. If it's different, it Means that one of the consumers is saying, I have looked at this dependency and I think it includes these things.
00:46:36.525 - 00:47:31.449, Speaker A: So one is more authoritative than the other. The relationship is how is this row related to the parent row in the table? So at the root of the sort of hierarchical tree, as we'll see in a second, you might have your application, the thing that you're actually building and producing a bill of materials for. And there the relationship will be self saying this record is for the application itself. And for things that are under that self row, you might have things like is included in or was built by. So this is how is that component related to the rest of the tree of software in this list? And the relationship assertion is a little bit weird, but it's basically a claim about the list of dependencies and how much you know about it. So it is, for example, I know that there are no dependencies, which is different from I haven't listed any dependencies. So you need that way to differentiate.
00:47:31.449 - 00:48:39.479, Speaker A: Or I might list three dependencies and then I can say in the relationship assertion, do I know that there are only these three? Or is this a partial list? So it's a way to communicate how much do you know about the graph? Yeah. Why do you need both a hash and a uid? Is it that the UID is human readable and the hash is not? So the idea is that for any version UID pair there should be one correct hash. And so the UID is generally going to be human readable mostly, but it doesn't have to be necessarily. It's just it is a supplier provided value. The hash is. This is what I observed and so I'll give you an example a little bit later of what might be an issue if you discover that the hash doesn't match what the author gave you, but the hash is something you fill in as the consumer of. What did I actually end up with? There are multiple data formats for representing bill of materials.
00:48:39.479 - 00:49:11.393, Speaker A: Bill of materials is sort of a. A data model more so than a data format. Two of the common ones are the software package Data Exchange, SPDX and the Software Identification Tagging Standard, which is swid. There are a bunch of different ones, but these are the sort of two major ones. I'm not going to go into either of the data formats because they're not that interesting really. There's a bunch of XML and XML Schemas are not that interesting to talk about and they're usually tools that exist to convert between them. Now the sort of magical properties of these bill of materials is you can combine them.
00:49:11.393 - 00:50:22.275, Speaker A: So I can write something for my software and one of my suppliers can write one from their software and I can take their sbom and concatenate it to my own to get a better view of the totality of my dependencies. Right? That gives me a deeper view into the dependency graph. Because if I just say I depend on Anisha's library, but Anisha's Library has like 100 dependencies and I don't declare any of them, I'm missing a view into that part of my application. But if I take Anisha's SBOM and concatenate to my own, then now I end up with a much deeper view of the dependency graph. And it's okay for SBOMs to be incomplete, but what we want here, and in fact one of the design goals for the construction of S bombs in the first place, was that it has to be incrementally useful. If we require everyone to adopt sboms all at once before they are useful, then no one will do it. And so instead we start with saying you can get some incremental benefit by just keeping one of your own dependencies or the subset that you know, and then you can grow that set over time as more and more of your suppliers and other sort of partners produce their SBOMs as well, and then they combine in this nice way.
00:50:22.275 - 00:50:44.405, Speaker A: What's interesting though, is that there's no requirement for SBoMS to be signed. Anyone can write one. It's just like for these existing formats, just an XML file and the supplier name and the author is a free form string. It doesn't have to be cryptographically secure in any meaningful way. It can just be a name. And we all know the problems with using names for things like this. They're not unique.
00:50:44.405 - 00:51:31.467, Speaker A: But in general, if you want the kind of guarantees that we've talked about for bill of Materials, you really want these S bombs to be signed. So that if I get a S bomb from Nikolai, I know that it's not just someone who wrote Nikolai in an XML field, It is actually Nikolai publishing the true S bomb for his software. So what does an S bomb look like? The bottom here is essentially the data model representation, and the top is just a visualization of what that looks like. So in this case we have an application that sort of the top level thing that's being built, you see, its relationship is called self because that is the thing we're building. And in this case, the application is built by ACMA Corporation and it's supplied by ACMA Corporation. Right. So that record is fine.
00:51:31.467 - 00:51:51.193, Speaker A: It's authoritative. We know that it's true. And when we built the application at version 1.1, we got a hash of 1, 2, 3. Great. We also declare that the relationship assertion here is known, meaning that we know that this application only has two direct dependencies, the browser and the buffer. Those are the only two dependencies.
00:51:51.193 - 00:52:10.437, Speaker A: There are no others. That's the claim we're making by setting known in the last column here. So let's look at Buffer first. So Buffer is supplied by Bingo, whoever or whatever Bingo is. Bingo published version 2.2. Bingo did not publish an S bomb. So ACME has written their own record for Buffer here.
00:52:10.437 - 00:52:37.144, Speaker A: And what they're saying is that when we built Bingo, we ended up with a/423. We have no idea what's inside of Bingo. So the relationship assertion here is unknown. It could be that Bingo has no dependencies, it could be that it has lots of dependencies. We just genuinely don't know any of them. For the browser, you see here that the supplier is Bob and the author of this record is also Bob. What this generally implies is that Bob wrote somewhere that the hash for 2.1
00:52:37.144 - 00:52:59.533, Speaker A: is 223. So Bob has guaranteed that that record should have this format. But Bob didn't declare anything else about their software. They haven't told us what the dependencies are. They didn't publish a full SBoM. They probably just gave that hash. And so we happen to know that this compression engine needs to be there for Bob's browser to work, for us to build it.
00:52:59.533 - 00:53:23.443, Speaker A: This could be something like if we try to build it without the compression engine, we get a linking failure. So we know that has to be there. We don't know if there are other things that get pulled into that source. Maybe we got browsers just like a binary artifact from Bob, but all we recorded is this hash that we got and the compression engine. You see, the supplier is Carol. But here ACME wrote that record as well. So we didn't get anything from Carol, we didn't get anything from Bob.
00:53:23.443 - 00:53:50.087, Speaker A: This is just the version of the compression engine that we built and brought into this. But we do know that the compression engine has no dependency. The root tier means the list of dependencies is empty, and we know that it should be empty. How they know that, I have no idea. This is an example from the spec. It seems weird for ACME to know this, but they happen to know this. It could be, for example here that they Got the source code for the compression engine, they built it from source.
00:53:50.087 - 00:54:15.813, Speaker A: They ended up with an artifact with a hash of 323. And they know that in order to build it they didn't have to link with anything. And there was no other source code being brought in. Maybe the source code for this library is very straightforward. Now let's imagine what happens here if this was concatenated with another SBoM. Let's say that Karel comes along and Karel now publishes an SBOM for the compression engine. And Carol's SBOM is indeed just one record.
00:54:15.813 - 00:54:55.937, Speaker A: There are no dependencies, and that record has supplier Carol version 3.1. Author Carol UID434 relationship is self for Carol's S bomb. And then we concatenate into our own relationship assertion root. But the hash in Carol's s bomb is 2, 3, 4. What does that mean? Well, what that means is Carol is saying my compression engine, as of this version, when you build it, should have the hash 234. And if it's doesn't hash to 2, 3, 4, that means you're using a different version than what I intended to publish. And there are a bunch of reasons why this could be.
00:54:55.937 - 00:55:58.029, Speaker A: It could be we use different compiler flags, it could be that we happen to patch the source code in some way, or it could be the sign of something malicious happening, but at least as an indication here that something is off and then we can use this to augment our S bombs, right? So here all of the rows are either self or included in, but you can imagine there being other kind of relationship assertions. Like was patched with. Like if we had a row here that says this compressor engine is Carol's original source and the hash of that source and this patch and the hash of that patch file, then now we might actually be able to validate that we are using Carol source. But because we're using a patch on top of it, we wouldn't end up with the same binary artifact. And so the hope is that by growing your S bombs in this way, you can do more and more sophisticated analysis over your dependency chains. And so the kind of the dependency edges you can declare here, the kind of relationships you can build, there are all sorts of things. Like this is basically a.
00:55:58.029 - 00:56:46.175, Speaker A: It's not quite a free form text field, but it's pretty close. So you can have things like was built by, for which version of Clang or GCC or GHC or whatever it might be you were built by. Whether something was present when built. Like you can imagine here, for example, SolarWinds Orion might be one of the things where if it was present when you built it at this version, then maybe you don't no longer trust these software artifacts generated by if you do source code generation from other things patched with red data from. So this last one you can use for things like configuration files. So you might now have a way to capture what configuration a piece of software was running with directly into your SBoM. So you can use this later on to figure out what are all the places where this configuration file was used as of this version.
00:56:46.175 - 00:57:38.045, Speaker A: You keep adding info over time and that improves the insight you have into your dependencies. And then you can also improve the analysis separately from what's in the SBOMs. Okay, so where we ended up was these are the three questions, what you are deploying, where, where it came from and what's in it. And I hope that I've convinced you that the answer to all of these should be yes. I hope I've also convinced you that it's really hard to answer yes to any of these questions. And there are a lot of open research problems in here too. If that happens to be the kind of thing that you're looking for of figuring out how do we verify all of these things that I've talked about? How do you prevent these kinds of attacks? It is very, very tricky, but hopefully it's something that we can solve over the coming years because we're seeing more and more attacks take the form of exploiting any of these three things.
00:57:38.045 - 00:57:55.725, Speaker A: That's all I had to say. Thank you. And if you have any questions, please ask. You can also ask anonymously and then I'll just take questions from down here. That's fine too. And I promise I'm not scary. Mostly.
00:57:58.875 - 00:58:00.366, Speaker B: Does Amazon publish SBoMS?
00:58:00.476 - 00:58:59.085, Speaker A: So one of the things that's interesting with SBoMS is that there's not a requirement that you publish them. Usually the way that the laws around this work and the way that the guidance works is if you are vending libraries, then you should publish sboms with those libraries. Whereas if you're vending services, where the abstraction boundary is more of a, like you have an API, but that's the extent of it, you're not actually linking with the software then as the requirement is that the service provider has an SBoM so that if there's a problem, they have a way to navigate backwards through their dependency graph. But there's not a requirement that they publish it to customers because it shouldn't matter to our customers. Which version of OpenSSL we are using on the server side. Now, you could argue that it's better the more we share of this, and that is true. But no, most companies are not going to be publishing the response bombs is my general expectation, and Amazon currently does not.
00:58:59.085 - 00:59:57.785, Speaker A: Someone asks, what's your favorite type of attack? My favorite kind of attack. My favorite kind of attack. Ooh, I really like hash collisions. Because there's sort of an assumption in a lot of the software that we build that hash collisions can't happen. We tend to write software in a way where if the hashes are the same, then we assume that the artifacts are the same, not just for supply chain security, but like everywhere. Like, if you look at your build tools, your package managers, if you look at like cryptographic checks, like, everything assumes that if the hash is the same, the message is the same, and if you can manage to break that, you break a lot of things by assumption. We saw this happen with MD5, we've seen it happen now with SHA1, although only for subsets of its security guarantees.
00:59:57.785 - 01:01:14.989, Speaker A: I don't think we're going to see it for, you know, SHA256 or any of the more advanced schemes that we have now for a little while, but there have been some interesting variants on this attack. So one of the attacks is currently going around is it's really hard to generate a collision where the hashes are the same. But it turns out to be quite easy to generate a hash where the first and last N digits are the same. And it turns out when humans compare hashes, which we do a decent amount of the time, we tend to look at the beginning and see that it's the same. And we tend to look at the end and see if it's the same, because in the middle it's really hard to know where they line up. And so these attacks rely on cases where users are comparing hashes like, is this the fingerprint that you expected this file to have, or is this the public key that you intended to communicate with? Or is this like the certificate that you expect the server to have? And if you just compare the start and the end, it turns out that's not enough anymore because of these attacks where you just generate things until you find a collision and you have enough random bits in the middle to play with, that you can actually generate collisions pretty effectively. It's a really interesting kind of attack where the target isn't computers, the target is humans, because oftentimes we end up trusting humans in this way.
01:01:14.989 - 01:02:05.785, Speaker A: We shift the Burden on checking certain security things onto users, onto people. And people are bad at cryptographic security just because we don't work that way. Yeah. Someone asks, why is it hard to see where things come from? Can we not just look at the library imports? Why is it hard to know where things come from? Can't we just look at the library imports? So there are a couple of answers to that. The first is that assumes that you have the source code. If you don't have the source code, there are no import lines to look at. The second one is you can look at the import lines, but how do you know where they're importing from? When you write import foo, what does that mean? You know, ultimately that's going to bring in a source file from somewhere that gets brought into that module.
01:02:05.785 - 01:03:28.633, Speaker A: In C, it's like a literal hash include. It might be a path, but it might also be from any of the system include paths, which gets set by the build environment and so who knows what's there? When you say, you know, hash include uint or standard lib or standard IO, what really happens is the compiler looks through the system include search path for the first file that's called like standard lib.h and then it brings in that file. But how do you know how that file got there? Or what if someone modifies your system include search path to include some other things at the beginning where they control the contents. If you're taking dependencies through like JavaScript modules or something, then you run into kind of like the PayPal attack, for example, where does the thing that brings in the dependencies, does it prefer internal or external sources? Because if it prefers external sources, you might say import PayPal logger, but that might be a different PayPal logger than the one you intended. Someone asks, do you have any advice for someone who wants to make a career in security? If you want to make a secure career in security. So security is this interesting field where there are so many different kinds of security.
01:03:28.633 - 01:04:47.717, Speaker A: And I think one of the things that's useful is to figure out which kind of security is interesting to you. Is it to like hack on individual bits of code and sort of figure out whether they're breakable, whether there's an error there, whether it's exploitable, and like figure out how to chain rops in order to manage like a jail escape. That's very low level, sort of attacker oriented kind of security work. Is it to work very far on the other side? Unlike security policy, it's a very different kind of security work, but also often more impactful if you can manage to do it well. And then there's all these sorts of positions in between that are more about how do you do defensive programming. So if you work at Amazon in build, for example, you might think about things like, well, how do we sandbox builds? If we want people to be able to use the public build tools for these languages, like we want people to use NPM or Yarn, we want them to use pypy, we want them to use Cargo and Crates IO, then we want to enable them to use those tools, but we want to do so in such a way where they don't accidentally shoot themselves and the company and foot and figuring out how to build technological solutions that mean the developers don't have to worry about this so much is also really interesting. So that's more like infrastructure security.
01:04:47.717 - 01:06:01.695, Speaker A: And then you have operational security, which is security on the sense of like, how do we know that these server racks are secure? How do we know that our network firewall is set up correctly? And knowing roughly which of these directions you want to go into makes a lot of difference in terms of the kind of things that you end up learning and deciding to focus on. The other thing I would say is that it turns out there are a lot of really cool problems in security and many of them you don't know about until you start following the people who do them. So I generally recommend that you go follow security oriented people on whatever social platform is cup of milk or whatever we want to use as the expression today. So, so the, the idea here is that there are security professionals in all sorts of spans here that you can follow. And it's not necessarily that the work that they do is interesting to you, but they might then loop in, loop you into other parts of the security sphere there where people are tackling problems that you might not know about. So examples of people you could follow, like Swift on security is fantastic. If, if you don't already follow them, they're on, I think, all of the different social networks now.
01:06:01.695 - 01:07:04.727, Speaker A: Tomas Pitacek, who is, I think TBQF or TQBF on Twitter and a bunch of other places, Matthew Green, who's a security researcher, off the top of my head is hard, but you can message me after and I'll send you a whole list. But there are a bunch of people that it's worthwhile to follow them just to see what kinds of security problems are they thinking about, what kind of security issues are they seeing on the horizon. And then start digging around there, start Asking questions, start reading the things that they link you to in these slides, which I'm sure we'll find a way to upload in the speaker notes. I've put links to all of the news stories for many of these backing stories and regulations and whatnot, and some of them have really good other resources too. So, for example, the dependency confusion attack was surfaced on a website called Bleeping Computer. Fantastic name for a website. And they publish a lot of interesting new developments in the world of security breaches.
01:07:04.727 - 01:08:08.105, Speaker A: So that would be another place to monitor. So just keeping on top of what exists out there is a really good way to figure out what draws your attention, what are you drawn to, and then you can start learning more and focusing more on that. Yeah, you said that Amazon does not publish S bombs, but actually use them internally, and how big do they get? Ok, so the question is that I said that Amazon does not publish SBoMS, but do we use S BoMS internally and how big do they get? So Amazon records a lot of this information internally. I don't know whether we specifically record them in the SBOM format, but this kind of provenance data does get very large. I can't tell you how large, but very large, to the point where we need specialized systems to query them. Because it's so large. Because as I mentioned, you want to know what you're deploying where, which means that every single deployment to every single host gets logged somewhere.
01:08:08.105 - 01:08:39.583, Speaker A: Somewhere. And for every deployment, we want to track where it came from and what's in it. So we essentially have a one SBoM for every deployment to every host. And so this accumulates a lot of data constantly. And so it's hard to run a system like this at scale. Are supply chain attacks like detected a lot faster? It's probably harder to do. Are supply chain attacks detected a lot faster? Not necessarily.
01:08:39.583 - 01:09:23.291, Speaker A: The SolarWinds attack took a while before it was detected. I think it really depends on how good your infrastructure is. For many of the examples I gave, they were detected quickly because they were often destructive. If you look at the sort of rogue maintainers or rogue authors examples, all of them broke their packages. And so that's a very visible kind of supply chain security attack. Solar winds, much less so, but there you might be able to detect that there's like anomalous Internet traffic, for example, where suddenly a bunch of your hosts are contacting this random IP address. That's probably a sign something is wrong, but it really depends on how the attacker tries to get in.
01:09:23.291 - 01:10:04.605, Speaker A: Right. If you have A targeted attack against the business, you might not even need it to reach outside at all. Right? You might have it sort of sleep in the network or modify source code over time so that when the deployments eventually happen, then you get a remote code executable kind of thing. And so they're not. I don't think supply chain attacks are inherently more detectable, but it is true that often they are. They have a big sort of fan out in terms of impact, which is sort of the reason they're attractive, right? Is that if you compromise one npm package, you might compromise 10 companies or more. But it all depends on how identifiable the attack itself is.
01:10:04.605 - 01:10:52.905, Speaker A: Does that affect like 10, is it easy to have it only like affect one company and not trying to do like a very targeted. Is it easy to have it affect only one company? I mean, it's all code, right? So if you can run arbitrary code, you can do whatever checking you want. It's, it's, it depends on how much insider knowledge you have about the company too, right? If you know that the host names for developer machines at this company always start with the following like six letters, then you could use that as a proxy for figuring out should I execute here or not. You could use things like IP ranges. You can query an external server that then tries to identify based on where the request came from. So there are some means you can use here, but it's often hard to target specifically unless you have insider information to go on.
01:10:54.485 - 01:10:55.837, Speaker B: Another audience question.
01:10:55.941 - 01:12:04.247, Speaker A: How Valuable is a PhD for a career in security? Oh, it's funny. Okay, so the question is, how Valuable is a PhD for a career in security? We were discussing this during lunch earlier today, actually, which is at least for me. The PhD itself was not useful. And what I mean by that is like the diploma didn't matter, but the six years that I spent doing all this learning and getting to explore a lot of these spaces and spending a lot of time in just educating myself, learning how to dig deeply into problems and really focus my attention on them. Getting to sort of not put on blinders exactly, but really just immerse myself in some of these fields and just learn whatever was interesting I think was extremely valuable for my career later on. Like now my current job is not in security in the sense that I own the build infrastructure for a programming language. But it turns out that there are a lot of security implications of owning the build infrastructure because that's where all the vulnerabilities come in.
01:12:04.247 - 01:13:21.587, Speaker A: And so having that security exposure and security understanding has then been extremely helpful in making me not just do my job well, but also in educating the company about the kind of security risks that I see flowing through my role. And so that's all to say I think the degree is not the thing that's valuable, but the ability to spend that amount of time with that amount of focus on the things that you care about matters enormously. And you get that freedom more so in academia than you do if you go into sort of a junior position in at a larger commercial company, unless you land like a junior security engineer position where maybe you get to do a little bit more pen testing and the like. But being able to explore this through academia, I do think is helpful. Yep. What types of companies are looking for security engineers? What types of companies are looking for security engineers? So what's interesting is the companies that aren't looking for security engineers are the ones that most need security engineers, at least. My experience is that companies are constantly understaffed when it comes to security engineers.
01:13:21.587 - 01:14:49.175, Speaker A: Like, there are so many security problems everywhere, and there are so many things that again, as I mentioned at the sort of head of this, the engineers know about many of these security problems, but leadership does not necessarily know that they're there, how critical they are, how hard, how severe they are, and how at risk they put the business. Like, how critical would it be if something went wrong here? And that leads to this interesting split where everyone who's an engineer at these companies knows that you need more security engineers. But the company doesn't necessarily prioritize hiring security engineers, even though they should, which doesn't really help in terms of the question. But usually a lot of companies need them and the question is just whether they are seeing, seeking them out. Sometimes you'll find that if you start to talk to people at any given company and you ask them about their security posture, whether you can talk to some of the security people, they might go, yes, we are definitely hiring, but it's not necessarily clear that there'll be like an open job position that's easy to find, that's about security. And again, like, if you look at something like my role, which is not a security role, if we published a job posting for my job today, there would be nothing about security in there. Even though so much of what I spend my time on is security, it's the integrity and security of the build process.
01:14:49.175 - 01:16:09.163, Speaker A: So oftentimes you can find other positions where you know that security is important and it's not necessarily a security job. But you go into it with the security mindset and you make the job a lot about influencing the security aspects of the work. You have a lot of questions today. Someone asks what are some useful skills to learn or classes to take for a job in security? Useful classes to take or skills to learn for a job in computer security? Well, missing semester of course is the class everyone should take. I think this class is really useful and I mean I'm biased here of course, but like I think one of the things that this class does pretty well is expose you to a broad range of secure like classes of security issues and ways to think about security and hands on experience with them. And that is really, really valuable in terms of. I don't know that there are other classes where I would say you should definitely learn this thing because it's not so much about learning the things that a class teaches you as much as it's getting hands on experience with some of the problems and problem domains.
01:16:09.163 - 01:17:15.559, Speaker A: Like for example, if you take the distributed systems class, you're going to find a bunch of race conditions in your code and race conditions are a great place for security bugs to hide. And so debugging your distributive systems lab is going to teach you things that are useful for doing security. Would I say that the distributed systems class is one you should take in order to get better at security? Probably not. It's not a security class, but it will help you learn the relevant or some relevant skills. I think in terms of getting things to learn on your own to become a more attractive job candidate in the field of security, you're going to want to look at things like debugging skills is going to be really useful. I think following a lot of the security discourse that's happening, like following these people on social media is one example. But just in general, keeping up with security news is actually a really useful way to keep on top of like what is the security industry currently thinking about? What's the current space of problems? And when you find that security news, don't stop at the headline.
01:17:15.559 - 01:18:59.473, Speaker A: Don't stop at like the third reprint of someone published a blog post about a security researcher who like actually go back to the origin material find like for the PayPal attack, for example, the dependency confusion attack, there's a really long blog post by the person who the security researcher who originally found the problem giving the full details of how did he discover this? How did he find the GitHub repository initially? How did he go about trying to exploit it, how did he roll it out, how did he expand it to Multiple companies and going all the way back to those origin stories and then reading about the techniques that these people are using, I think is one of the best way to accumulate skills. And I don't know that there are specific named skills that I would say, as much as it's more sort of by osmosis, learning a collection of techniques, because I think insecurity, one of the, one of the things that differentiate the sort of, I don't want to say best hackers because it's sort of the wrong term here, but the best security minded people are the people who are relatively well rounded in terms of being able to consider multiple different kinds of attacks and how they might build on on top of each other. If you get really, really good at buffer overflows, you can do some really impressive attacks, but at the same time you're limiting yourself to a very small subset of how you might be able to achieve a given goal. Remember that attackers, and therefore also defenders are relatively goal oriented. They're not mechanism oriented. The question isn't how can I use a buffer overflow to take down XYZ or to steal money from Y, it is I want to steal money from Y. How do I achieve that in the best possible way.
01:18:59.473 - 01:19:51.945, Speaker A: And the more strategies you have available to you, the more skills you can lean on, the better you're going to be able to do that. And as a defender, you need to think of all the things that the attacker might do so that you can defend against them. Dependency confusion, for example, has basically nothing to do with securing code. It has to do with what does a name mean, what is the policy for how we bring in third party dependency? And if you only focused on buffer overflows, you would miss that entire attack vector. And so rounding out your skills there is really valuable. Rather than focusing on honing specific skills to the extreme, I'm just looking at you now in case you have more. I'll also take them directly from you all if you want to do that instead of via a person here.
01:19:51.945 - 01:20:22.045, Speaker A: But someone asks, what's your preferred text editor? Prefers text editor? Neovim. That's the right one. It's not preferred, it's just the best one. I'm glad everyone is in agreement. I know we have some Emacs people here, but. All right, well, we're almost at time. Thank you everyone.
