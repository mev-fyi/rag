00:00:00.360 - 00:00:32.325, Speaker A: Hi. Welcome everybody. I'm Larry Rudolph from the LABS team. Part of the goal of the LABS team is to reach out to academia. And this is a special talk. I met John at CSAIL when Trema and I were there talking about security of firmware. And we had this idea that we should use Rust and so we met so that the MIT REST expert or the Northeast REST expert or the Worldwide REST expert, I don't know which one to call yourself with.
00:00:32.325 - 00:00:57.711, Speaker A: But his research is also on databases, novel ways of databases and coordination and conflicts and stuff that I really like. So I invited him to give a talk. I said, you have to talk about both things. And John said, no problem. I guess he comes here from. I guess you got your undergrad in Australia and then you went to London.
00:00:57.863 - 00:01:00.359, Speaker B: And I was from Norway and you're from Norway.
00:01:00.527 - 00:01:16.635, Speaker A: You know, you went the wrong way and then finally sort of came back up and ended up at mit. And he's a grad student at mit, csail. But let's. So he's going to give a talk and then there's be a deep dive or extended Q and A if you want to stay and ask. But let's make him welcome.
00:01:21.305 - 00:01:51.429, Speaker B: Thank you. So I'm John. Thanks for the introduction. I'm here to talk to you about Rust in general and in particular about a research system that I've been building at MIT for the past three years. It is essentially a fast concurrent database that tries to get a database slightly differently from how we've been designing them in the past. And part of what I want to impart is what it's like to use Rust to build something that's sort of large and real in some sense, because I think that's a lot of the reservations that people have about Rust. So I'm John.
00:01:51.429 - 00:02:20.295, Speaker B: I'm a PhD student at the MIT Parallel and Distributed Operating Systems Group. I work on a database called Noria. There's now about 60,000 lines of Rust code that we've been developing over three years. I also maintain several open source Rust libraries. I work a lot on the asynchronous IO part of Rust. I've done about 30 hours or so of live Rust coding streams you can find online if you're interested. And so if you want to look up any of that, then you can find me online at any of those URLs or just shoot Larry an email, he will forward you to me.
00:02:20.295 - 00:02:46.561, Speaker B: I'm sure he will do that correctly. As for this talk, there are some things we're going to Cover and some things we're not going to cover. And I think it's useful to just set that up in advance. In particular, I'm going to look at Rust from a relatively high level. I'm not going to dive into like the syntax and code. I am going to talk a little bit about Rust ownership because it's one of the more novel parts of Rust, one of the things that a lot of newcomers to Rust struggle with. But I will not sort of go deep into any of that.
00:02:46.561 - 00:03:28.211, Speaker B: We can do that in the deep dive afterwards. I'll also give the overall architecture of Noria this new database and in particular dive into one of the interesting data structures we had to design for Noria that hopefully will appeal to the engineers in the room, which is, I assume, basically all of you. I will not talk a lot about sort of query execution, query planning, deep database stuff. If you want to read any more about that, you should look at the paper or come talk to me after. In general, the idea of this talk is to look at higher level aspects of using Rust for real. And so therefore the first thing we're going to look at is what is Rust like? Why am I talking to you about Rust at all? Well, Rust is a programming language by Mozilla. It's relatively new.
00:03:28.211 - 00:04:00.363, Speaker B: It reached 1.0 in about four years ago. And it's for systems programming, which is a very vague term. But the basic idea behind systems programming is you want to have a language that lets you build systems that do stuff. So they're not necessarily just sort of tools or command line utilities, but they're for writing like servers or clients or basically anything as a system. So it's relatively poorly defined. But the Rust team likes to use a bunch of these sort of quotes that define what the language does.
00:04:00.363 - 00:04:25.361, Speaker B: One of the ones they're relatively well known for is this fast, reliable, productive pic 3. And the idea is that in a lot of existing production languages, you're sort of forced into this. I have to choose away something. I have to choose to not get a particular feature. Like in C, you don't get safe code, but you get really high performance. And Rust wants to do away with those trade offs and instead say, let's try to do all of them. In particular, Rust takes this approach with concurrency.
00:04:25.361 - 00:04:54.415, Speaker B: So in Rust they have this slogan of fearless concurrency. You should be able to write code that is concurrent without worrying that you have data races and your code is going to crash all the time. The Rust language is also very community driven, so it is entirely Open source, everything is run on GitHub. There's an RFC process for extending the language, for extending the compiler. And I have several things that are now in the compiler available in the language. And you can too, and you can get involved in that process. And that is one thing that is really neat about this language as opposed to many other languages.
00:04:54.415 - 00:05:37.725, Speaker B: Rust is on a six week release cycle, so new things also get released fairly quickly. Now you might wonder, well, how is Rust different? We've heard a lot of there are other programming languages out there, like why are we not all using Haskell or ocaml or Go for that matter, what is different about Rust? Well, the first thing is Rust is a really powerful type system. You can think of this in terms of sort of more like Haskell than it is like C. This turns out to let you express constraints in your code a lot better than you can do. If you don't have good types, you get zero cost abstractions. So the idea is that you can have abstraction layers, you things like interfaces, but the code monomorphizes so that the code is still fast. You get the same kind of speed as C.
00:05:37.725 - 00:06:28.523, Speaker B: You have the borrow checker, which tries to guarantee that you have no data races at runtime, so the compiler will check that you have no data races in your code. And the tooling of Rust is just really, really excellent. It has actually taken the time to learn from the past sort of n decades of programming language development and infrastructure development and incorporates that into the language. So what would you use Rust for? Well, Rust is really good for writing low level systems code. So the sort of space that's traditionally occupied by C or C. And part of the reason for this is because as opposed to many other languages, it explicitly tries to be interoperable with C and C. That means that if you have existing libraries, you have existing systems that are built using those languages, you can take Rust and you can swap out just small parts of it and still have it work well with the existing software that you have.
00:06:28.523 - 00:07:08.885, Speaker B: And in fact, this is why Mozilla developed Rust in the first place. It was sort of the Firefox browser to be able to swap out components of it with something that was highly concurrent and written in Rust, so they knew that it wouldn't crash. But you can also write high level software in Rust. So Rust, because of its type system is quite pleasant to work with. And you will find that you can actually write code that looks almost like Python code, except that it is also fast and also like the compiler will Check if you have an incorrect type as opposed to, you know, crash at runtime. In fact, I've found that I write both high level systems code, sort of network servers like I would in Go. And I can write tools and infrastructure like command line utilities in Rust and feel perfectly happy doing so.
00:07:08.885 - 00:07:36.819, Speaker B: But let's dive into that. Like why specifically? I've talked in very vague terms. Let's look at some specifics. So, first of all, and this is an important point in Rust, lots of concurrency bugs are impossible. The compiler will not let you compile a program that has a database in it. And if you come from basically any other language, this is very surprising, right? In C you will write a program and you expect it to crash at runtime the first few times that you run it. This is not the case with Rust code.
00:07:36.819 - 00:08:00.643, Speaker B: It can crash at runtime for sure, but there are lots of these problems that just will not be there. You also no longer have things like use after free or double free. The entire ownership system guarantees this. Also at compile time, the type system provides things like enums. And you might think, oh, I know about enums. Like we have enums in other languages. In Rust, enums are what known as abstract algebraic data types.
00:08:00.643 - 00:08:30.511, Speaker B: And algebraic data types are basically enums that can contain other stuff. In particular, for error handling, Rust uses the result enum. The result enum is generic over two parameters. The value that is contained if you get an ok, and the value that is contained if you get an error. So if you think of writing go code, for example, you often return this sort of tuple of the value if everything went correctly and an error. And you need to know, like if error is nil, then you do something, otherwise do something else. Or you might like return a null pointer or something like that.
00:08:30.511 - 00:09:12.323, Speaker B: If something went wrong in Rust, you just return a result that the compiler forces you to check is either ok, and then it will give the value inside that ok, or it's an error and it will give you the value of that error. So this means that it forces you to explicitly deal with things like errors. Null pointers as well are encapsulated in enums. There's an option type that is either none or it's sum that contains the value that it's actually pointing to. All of these also come at zero cost at runtime. And finally, Rust has this thing called pattern matching, which is sort of. You can think of it as the switch statement, but on steroids it lets you do things like if this is, let's say this computation succeeded, and also the value it gave was between 4 and 17.
00:09:12.323 - 00:09:52.625, Speaker B: And also this other value is the string foo. You can do really advanced pattern matching, which helps write code that feels more Python esque or even javascripty than it feels like low level C code. I talked about the tooling a little bit. So Rust has dependency management built in. So you can think of things like NPM for JavaScript or PYPY for Python or even for Go, things like Go get, except it's much more advanced. You can do dependencies that are path dependencies that are git dependencies that are dependencies on the central library infrastructure that Rust has. And all of it is just built into the default tooling that you get and works really well.
00:09:52.625 - 00:10:28.303, Speaker B: Documentation is built into the language for every library, every file you write. If you take a function, a module, even a constant, you can prefix it with a comment with three slashes instead of two. Now that is documentation. If you write code in that string, it will be automatically turned into a test so that your documentation cannot be out of date in any code samples that it has. All of this is integrated into the language. Any library that you upload to the repository of libraries that Rust has, all of that code is automatically documented and interlinked so that any documentation you look up will just always look the same. It will always be interlinked with other crates.
00:10:28.303 - 00:11:26.539, Speaker B: It's just actually fabulous to use, and I do not use that term lightly. And finally, and I mentioned this a little bit, when it comes to C and C, it is really easy to do ffi, so foreign function calls, whether that is calling into a C library, a C library. There is interoperability with Python, with Ruby, with Go, that you can make calls into those languages or have those languages call into Rust very, very easily. And that means that it's easy to integrate into existing code bases. Now, all of that said, there are some things that Rust is not good for, and I think it's important to highlight those, in part because the language is improving in these regards, but also because it's worthwhile to know. The first of these is that Rust is not yet great for highly specialized systems. And by that I mean if you have something like intel, dpdk or very highly vendored libraries that are large and expensive and written in some other language, it is not trivial to just sort of start using it.
00:11:26.539 - 00:12:08.473, Speaker B: From Rustland, you could take DPDK and generate sort of bindings and Rust for all of the C libraries they give you, but it would be a lot of work and if no one else has done that work yet, that's a lot of overhead you have to do and that you might not want to maintain. That is getting there slowly but surely. People are developing really good infrastructure for DPDK or for Amazon EC2 or for whatever other sort of large project that exists out there today. But it is slowly getting there. The other thing is Rust is not great for rapid prototyping. If you're just trying to like do a quick and dirty write something up, just you're just going to run it once, you don't really care if it crashes. If you just want to write something and not worry about the details.
00:12:08.473 - 00:12:29.485, Speaker B: Rust is not the language for that job. Because Rust will force you to write correct code. Not always. It will not always be correct, but it is pretty unrelenting about if you're. If you're going to write code and it's going to compile, it shall not have data races. Like you are not allowed and the type system is fairly involved. Like it forces you to deal with errors, it forces you to deal with the case when things are null.
00:12:29.485 - 00:13:20.163, Speaker B: And that's sort of contrary to the idea of rapid prototyping. Linux kernel programming also a little bit finicky with Rust currently, because you don't have things like libc, the standard library. It is possible some people have done it, but writing kernel modules in Rust still requires a bit more work. GUI programming, because GUI programming is hard and there are bindings to GTK and Qt and Win32, even for macOS. There are a bunch of bindings for the UI libraries there, but the bindings are still pretty young in part because the language is young too. And asynchronous networking is one thing that Russ is doing a lot of work currently on building good primitives for dealing with asynchronous computation in general and IO in particular. The language is taking a pretty interesting approach that looks a little bit like promises in JavaScript and it works pretty well.
00:13:20.163 - 00:13:53.403, Speaker B: I've been using that in Noria in the database I've been building and it's highly performant, but the code can get a little bit nasty to write because not all the high level abstractions are in place yet. And so that is also somewhere the language is still developing. Okay, so let's get to the database part of this talk. We've talked a little bit about what Rust is and now I'm going to switch sort of sites entirely and talk about databases for a second. But hopefully something that you find interesting. So Noria is this project I've been working on for a while. And the basic observation is that you have a bunch of applications that issue queries like this.
00:13:53.403 - 00:14:27.095, Speaker B: This is extremely common that you just have your application issue lots of SQL queries. In this case it has a stories table and a votes table. You count up the votes for every story and you want to extract the story title and the vote count for some specific story. All well and good. What this ends up being in a traditional relational database is it turns into something like this. You have a front end that issues that query to the back end database. The database does some query planning and decides that, oh, in order to execute this I have to count the things in votes and then I have to join it with stories and I'm going to do a filter and then I'm going to give the results back.
00:14:27.095 - 00:14:57.025, Speaker B: This is all very well and good. This is what basically every database out there today does. The problem is for most applications, almost all of the stuff that you do is read. Almost all your operations are selects. And the selects in a database are pretty expensive because every time you do a read you have to redo this computation. So even if the data that's underlying you hasn't changed, you have to do all this stuff on every read. And if 90% of your requests are reads, that is a lot of wasted cycles.
00:14:57.025 - 00:15:26.929, Speaker B: And we don't like to waste cycles. So some of you might think, oh, we solve this, we're just going to put a cache there, right? And caching solves everything. Like we're just going to stick memcached or Redis or something in front of our database and woohoo, everything is fast. But unfortunately it's not quite that simple. In fact, your application is more likely to look like this, where you have to make sure that like every time you write you invalidate the appropriate cache. But you don't invalidate too much because then you throw your entire cache away. And now when you do a read you might miss, you have to go to the database.
00:15:26.929 - 00:15:53.471, Speaker B: You can run into really weird situations that many people don't even think about. Like if you have a skewed distribution of popularity in your database, then it might see a lot of writes and a lot of reads. You get a write for a popular key you invalidated. All of the reads now start missing because they were all for that popular key. All of them go to your database at the same time and your database falls over. This is known as Thundering Herd. And it requires a lot of really sophisticated MITIGATION to get around.
00:15:53.471 - 00:16:50.725, Speaker B: There's a Facebook paper on how Facebook tries to manage their MySQL and memcached caches. It runs into this problem and the solutions are highly non trivial. All of this seems like so much complexity in order to make your application fast. So what we did in Noria was essentially observe that all of this is stupid. The database has your queries, so why can't the database just maintain the cache? Wouldn't that be a lot easier? So Noria, instead of doing all of that business, does a regular sort of query planning like you do in traditional databases. But instead of executing the query on read, it executes the query on write and it does so incrementally. So every time a new, every time a new write comes in, it's going to look at all the queries the application has given it in the past and it's going to basically take the existing query results for that query, take the new write, process it through the query plan and update the cache results in place.
00:16:50.725 - 00:17:25.783, Speaker B: This may seem a little strange, so let's do a quick walkthrough of that. So here's the same query that we looked at earlier, but notice that there's this results thing at the bottom, which is what's known as a materialized view. You can think of it as your cache. It stores the current results for that query and you see that the Currently for some key A, the story title is foo and the vote count is 41. Now a new vote comes in for story A. It enters the this is a data flow graph that basically represents the query that we're executing. The vote for A comes into the count operator.
00:17:25.783 - 00:17:49.229, Speaker B: The count operator goes, oh, the previous count was 41, the new count is 42. So it emits a record that says a is now 42 instead of where it was previously 41. That record then hits the join operator. The join operator goes, ooh, I got a thing for A. I'm going to look it up in stories and find the title. So it Then forwards A is 42 and the title is foo. That then enters the cache and the cache goes, ooh, I have a key for A already.
00:17:49.229 - 00:18:14.143, Speaker B: I'm going to replace it with the new value for a. So Now A is 42. So now any at this point, any read that comes in is going to read the old vote count. And at this point, any query for that view or for that query that comes in is going to see 42 as the vote count. And so the application, all it has to do is Do a single lookup into the cache. It never has to do any invalidations. The write just go to the database and the database does the right thing.
00:18:14.143 - 00:18:39.855, Speaker B: Your cache is always up to date. So that's the basic idea behind Noria, and it turns out this works really, really well. I'll show you some benchmarks later. This is a great idea, but it does have some challenges. Like this is research after all. The first of these challenges is you can't just store all the results for all the queries you've ever executed. That is not okay, right? You don't have enough memory for that, at least not yet.
00:18:39.855 - 00:19:26.029, Speaker B: And so you need some mechanism to ensure that you can evict things from your cache. But that is not straightforward into this design. Imagine that we threw away the all of the results for the story A, and then a new vote comes in for the story A. The count operator no longer knows what the previous count was, so it can't omit what the new vote count is. And so you need a mechanism in your system to ensure that even if you throw something away or something gets asked for that you've never been asked for before, that you have some way of computing it or recomputing it. This is the problem of partial materialization that we will not get into in this talk, but it is a really interesting problem. The second one is that I've just shown you a single query, and I've shown you a query that does not change over time.
00:19:26.029 - 00:20:02.331, Speaker B: But in reality, applications don't just issue one query, they issue many queries. And beyond that, over time, the application develops, the application grows and evolves, and it might change existing queries or add new ones, and the database has to react to that. The database has to know what to do. And it is not trivial because existing data flow systems, so you might have heard of things like NIAD, or for that matter, even Spark, does not really deal with things changing while they're running. You sort of have to throw the thing away and start up a new instance that has a new set of queries. We don't want to do that because that's a lot of extra work. We want the system to just dynamically adapt.
00:20:02.331 - 00:20:32.999, Speaker B: And that is something that Noria is able to do. And you will have to read the paper to figure out why. The third challenge I'm going to tell you about, and this is the one we're going to dive into, is you'll notice that in Noria we have this results table. That is where all of your writes go. This is basically your cache, but it's also where all the reads go. Now you have lots of readers and lots of writers all trying to hit the same thing, right? That is problematic. You can't just do that unless you have some kind of synchronization, because a writer might update a record while it's being read.
00:20:32.999 - 00:20:49.327, Speaker B: So we need some kind of synchronization here, and we want that to be really fast because there are going to be lots of reads and lots of writes to this at the same time. So we're going to dive into challenge three. Um, for one and two, you should read the paper. I can send out the. The link for the paper afterwards. The. I promise you.
00:20:49.327 - 00:21:27.435, Speaker B: The solutions to the other problems are also very interesting, but I do not have the time to cover them today. But we will talk about that last challenge because it happens to introduce and require a pretty interesting data structure. Uh, before we do that, though, we're gonna have to talk a little bit about some basic computer science. Uh, in particular, we're gonna talk about this notion of ownership. So, ownership is a key tenant of Rust. And the basic idea is, similar to what RAII is in C or in many other languages, that if you own something, then you are responsible for that thing. So if you own some piece of memory, you are the one who's responsible for freeing that memory whenever it's appropriate to do so.
00:21:27.435 - 00:21:55.025, Speaker B: In particular, Rust extends this with if you own something, you also get to choose who has access to that resource and how. In Rust, there are basically three types you could have for some type T. So think of T here as any type in your system. It does not really matter. It could be a string, it could be a number, it could be an array, whatever. For any T, you can either have just a T, which means that you own that thing. Owning it means that you are responsible for freeing it.
00:21:55.025 - 00:22:33.553, Speaker B: You can have a mutable reference to T. That's how to read that second part. A mutable reference to T is just like a pointer to T in any other language, but with the additional contract or the additional guarantee that no one else is able to read or write that T. As long as you have that mutable reference, you have exclusive access to that T, but you do not have any responsibility to free it after you're just borrowing it. The last type is an immutable reference to T. It is also just a pointer to T, but with the additional contract that you are not allowed to modify T, because there are going to be other people who are also reading T. These are the basic three types in T.
00:22:33.553 - 00:23:39.885, Speaker B: And this is basically the entire ownership system in Rust, that these are the three types. And Rust will, at compile time, check that you have not violated these contracts. It's going to check at compile time that for every variable, for every piece of memory, for every resource in your program, you never have both a mutable reference and an immutable reference at the same time, that you never have multiple mutable references, either of those not okay. Your program will not compile. It is fine, though, to have multiple immutable references, right? Because you can have lots of things, read things at the same time, and you can think of this always almost as the compiler requires you to prove that your program does not do this. And it turns out that's all you need to guarantee that your program has no data races, right? If you never have two things, if you never have something modify a thing while it's being read or modified, you cannot have data races, because you will either just have multiple readers or you'll have a single writer, which is fine. You also guarantee that you only ever free things once, because the owner is responsible for freeing it, and you can only have one owner.
00:23:39.885 - 00:24:17.595, Speaker B: Furthermore, the borrow checker will also check that you don't use anything after you have gotten rid of it. In particular, this is how the borrowing system works. It adds this notion of a lifetime. So if you borrow any T, that borrow of T is assigned a lifetime. And you can think of the lifetime as how long you're allowed to access T for. So, for example, if T lives on the stack, the lifetime that's given out for any borrow of T is going to be tied to the stack frame of the thing that has T. And the compiler will check that when T goes away, when that stack frame is popped and T is freed, there are no outstanding references to T.
00:24:17.595 - 00:25:09.283, Speaker B: So if you tried to take, say, a reference to something stored in the stack, like a buffer or something, and give it to another thread, the compiler would go, no, that thread can live longer than this stack frame. Therefore, your program is not safe, so it will not compile. So why am I telling you all this? This seems like a lot of sort of CS nonsense. Well, it turns out you can frame the problem that we looked at in the beginning of this concurrent access to the results table as ownership, right? Because what we're trying to do is allow concurrent reads and writes to a shared structure, which Rust has just told us is not okay. You are not allowed to have something mutate and something read at the same Time. The way Rust mitigates this is by forcing us to use synchronization. So there are certain primitives in Rust that allow you to take an immutable handle and get a mutable handle to a table.
00:25:09.283 - 00:25:55.531, Speaker B: One of these you have probably heard of, it's called a mutex, right? In Rust, you will not be allowed to access a shared value without using something like a mutex doesn't specifically have to be a mutex, but it has to be something that safely provides mutable access to something that is shared. So let's look at what a mutex looks like in Rust. In Rust you have a mutex in this case that contains a T. Again, T can be any type. And notice that in Rust the mutex wraps the T, so you have no way of getting at the T without also taking the lock. When you lock a mutex, it takes a immutable reference to self. Here I've annotated, annotated it with a lifetime of MTX for mutex, right? So the thing you get back from mutex can't outlive the mutex because the mutex would have gone away.
00:25:55.531 - 00:27:03.587, Speaker B: And what you get back is this thing called the mutex guard, which you can think of as a sort of credential that you are currently holding the lock. So when you take the lock, you take an immutable reference to the mutex and you get back a handle to the inside of the lock. That inside of the lock has a get method that as long as you have exclusive access to the credential, you get exclusive access to the T, right? So what this entire chain is saying is that as long as the mutex is still alive and you have the lock, you can get a mutable reference to whatever is inside the lock. And what this is giving us is mutable access to a T through a shared reference to the mutex. And it does this by mutual exclusion, right? The internally, the mutex knows that as long as it does its bookkeeping, right? This is safe to do because no one else is going to have a mutable reference to that T because they wouldn't have the lock, they wouldn't have the credential, therefore they can't access the T. And you might go, well, isn't this problem just solved? Like you just wrap our results table in a mutex and everything's good. Well, not quite so.
00:27:03.587 - 00:27:26.769, Speaker B: Mutexes are terrible because, well, they can be okay, but in this particular instance they're really bad because they force you to have sequential access. If you think of Our results table, you either have to do a read or a write for any given unit time, which is terrible because we want to do things in parallel. We want this to be concurrent and fast. And Mutexes are neither concurrent nor fast. So you might go, aha. But I know. I know what you're going to say.
00:27:26.769 - 00:27:51.839, Speaker B: Use an RW Mutex. Yeah. So a reader writer lock is something that you may or may not be familiar with. The basic idea behind a reader writer lock is that you can have multiple readers take a lock at the same time, or you can have a single writer take the mutex, but you cannot have both. So this has a similar flavor to ownership. Right. You can either have a single mutable pointer or exclusive pointer, or you can have many immutable ones.
00:27:51.839 - 00:28:15.791, Speaker B: And that's great, right? It lets us do many reads in parallel. Awesome. It's all we wanted. And then every now and again, you sort of have to take a break and do some writes. Is that good enough? Do we think that this solves our problem? So in order to answer that question, we need to look a little bit at how reader writer locks work. So this might be a little small to read, but the exact details are not important. Just trust me that this is actually how reader writer locks work in P threads.
00:28:15.791 - 00:28:41.395, Speaker B: Um, the basic thing to observe is that there's a lock in here. So whenever you take a reader, the reader part of a lock. So the thing that's supposed to let many threads take the reader lock at the same time. Whenever you do that, you have to take this exclusive lock first. You don't take it for very long. Right. Notice that you release it before you return, but you have to take it whenever you take the read handle and whenever you give it back.
00:28:41.395 - 00:29:16.783, Speaker B: So what this means is that it is okay if you have a long critical section, because taking that lock is very little of your workload. Right? If you're taking the read lock and then doing lots of work and then releasing the read lock, taking the read lock is probably not where you're spending your time. And so the overlap is going to be almost perfect. You get lots of concurrency. However, if you have a short critical section, then you're going to take the reader lock. You're going to immediately release it because you only did a few instructions in between. If you're doing that, taking the reader lock is going to be your bottleneck.
00:29:16.783 - 00:29:43.847, Speaker B: But taking the reader lock is a sequential operation. And so now you're basically ending up with your program being sequential. Again, all we're doing while holding the reader lock is doing a hashmap lookup. Hashmap lookups are really fast. Therefore we're basically benchmarking the performance of taking a lock. And I'm going to prove that to you by running a benchmark. So this is a reader writer lock on top of a hash map.
00:29:43.847 - 00:30:14.367, Speaker B: And what we're doing along the X axis is that we are increasing the number of threads that are doing reads of the map. And I'm showing on the Y axis the number of reads per second and the number of writes per second. And you can see that initially it's fine, right? Like we get more threads and we get more read throughput. That's great. But over time we're getting dominated by taking this lock in the read. So you see, with few threads we do get the speed up because there's some parts where the hashmap gets get to happen in parallel. But when you get to more threads, you're spending all your time contending on the lock.
00:30:14.367 - 00:30:34.921, Speaker B: That's not good. The question is, can we do better? We like this to be a straight line. That would be the best thing we could possibly hope for. You can't really have it be super linear. That would be weird. So now we need to do without locks. So remember how locks provide a safe wrapper around aliased or shared pointers, right? This is what we talked about earlier.
00:30:34.921 - 00:30:54.485, Speaker B: It gives you mutable access through a shared pointer. But aliasing brings data races, right? This is the whole problem that we haven't seen, that you might just have two things that point to the same thing and try to mutate it. So we're going to have to need unsafe now. In many of your heads are going to be like alarm bells going off now. Unsafe. I've heard about this in Rust. Like, this is terrible.
00:30:54.485 - 00:31:08.145, Speaker B: This basically gives up all guarantees and Rust is now basically C. Please breathe. It will be ok. So it turns out. Ooh, what did I do? I reset it. Oh, no, try again. There we go.
00:31:08.145 - 00:31:34.653, Speaker B: It turns out that writing unsafe code is really just writing C code. Whenever you type the unsafe keyword, Rust is saying you're now allowed to do the things you're allowed to do in C. There's nothing more or less magical to it. In fact, all of the basically everything that unsafe lets you do is it gives you the ability to dereference a raw pointer, nothing more, nothing less. Which from clan seems obvious. Like, of course I can dereference raw pointers in Rust. You cannot.
00:31:34.653 - 00:32:17.745, Speaker B: It is not OK to dereference raw pointer you can dereference references because they have a certain contract associated with them. But a raw pointer dereferencing it, who knows how many other people have that same raw pointer? It is not ok, but unsafe lets you do it. In particular, we can introduce this concept of a raw pointer. So a star mut of T is just a pointer to T with no additional contract. It says nothing about who else might have that pointer. All it's saying is that I know the memory location of a T, it has no lifetime associated with it, which inherently is unsafe. Right? Who knows? This could point into the middle of the stack somewhere and that function might have returned, who knows? And then unsafe lets you take one of these and turn it into a mutable reference to T.
00:32:17.745 - 00:32:57.407, Speaker B: So basically you can give it out along with the contract saying, I promise that this pointer is now exclusive to you. No one else is using it. And doing that is unsafe because you are internally maintaining some invariant that makes that true. You can also do things like changing types using this. This is basically pointer typecasting in C. The idea of unsafe is that you are now responsible for guaranteeing that there are no data races because there's no associated contact with the pointer anymore. You are claiming by writing unsafe that you know that this is ok, which is what you're doing all the time in C.
00:32:57.407 - 00:33:23.255, Speaker B: It's just in rust. You have to be explicit about when you do that. And what's nice about this is that now you know exactly which parts of your code to audit if you crash with a data race, because it's all the places that have unsafe. A place that does not have unsafe is forced to follow the contract. So we're going to use unsafe and we're going to come up with the following plan. So this is the data structure that we're going to end up implementing. We're going to keep two maps.
00:33:23.255 - 00:33:58.255, Speaker B: We're going to keep two pointers. One pointer is going to point to one map, the other pointer is going to point to the other map. All of the readers are going to go through the top pointer, and all of the writers, in this case, let's just imagine there's just one is going to go through the bottom pointer, and we're going to make sure they always point to different maps. So all the readers initially go to the left map. All the writes initially go to the right map. Whenever the writer decides that they want to reveal the writes that they've done to the readers, this could be every write, or they could decide to do it less often. Sort of give eventual consistency.
00:33:58.255 - 00:34:43.127, Speaker B: But whenever they decide that the rights should be revealed, they're going to swap both pointers. So the writes are then going to go to the left and the reads are now going to go to the right. And notice that the reads now get to observe the new writes, and then the writer is going to reapply the writes that it has done to the right map to the left map, so that now the two maps are again up to date. But there's a problem with this. If you just did this naively, there could still be readers left in the left map. Imagine that there's some reader who read the pointer and then it's going to start to look through the buckets of the hash map, but then the writer swapped the pointer. So now the writer thinks that they're allowed to modify the map, but the reader is still looking through the buckets of that map.
00:34:43.127 - 00:35:11.273, Speaker B: That is not okay. It's a data race and your program is going to be wrong. So we need some mechanism to deal with this. Right? Now the way to deal with this is not to add locks, because we're not allowed to do that. Right? Locks would trivially solve this problem. But locks are the problem in the first place, so we're not going to add any locks. We need to know when the writer has exclusive access to the left map, when are there no more readers left? The way we're going to do this is by introducing epic counters.
00:35:11.273 - 00:35:44.803, Speaker B: So every reader is going to keep just a local counter. It's not going to be synchronized in any way. They're just going to keep a local counter to that reader. And every time they do an operation on the map, they're going to increment that counter. So they're basically like a counter of how many reads I've done. And then what we're going to do is we're going to have the writer look at all the epoch counters, is going to do the pointer swap, look at all the epic counters until it has seen all of them tick up by one. I want you to think about that for a second.
00:35:44.803 - 00:36:30.535, Speaker B: Why is that ok? Why is that sufficient? Why is that invariant enough to guarantee that there are no readers? Well, think of it this way. Whenever a reader ticks up its count, you know that for it to do another operation, it must read the atomic pointer. So if it's in the middle of some read, then who knows whether it's read the pointer or not? But if you see it increase, you know that it has finished whatever operation it did in the Past and it is now about to read the atomic pointer again. So if you observe a tick in every reader after you swap the pointer, you know that all of the readers must now have seen the new value. Therefore, they must all now be in the right map. And therefore the writer has exclusive access to the left side. Now, this is not sufficient either.
00:36:30.535 - 00:37:23.505, Speaker B: Now, some of you may be able to realize why. In particular, what if a reader is inactive? What if you have a reader that just never does any reads? They're just going to sit there like they created a handle to the map, and maybe they did some reads in the past, but now they've stopped doing reads. The writer is going to just keep waiting, keep waiting, keep waiting and see the counters increase for all the readers, but whichever ones are inactive, this seems problematic. What do we do about this? So we're going to pull another trick. We're going to say that you're going to increment your epic counter twice. You're going to increment it once before you start reading, and again after you finished. So what this means is that if you ever observe a counter to be even, then you know that that reader is not reading the pointer.
00:37:23.505 - 00:37:59.183, Speaker B: You know that that reader is inactive. So again, we still have the guarantee that if you see it increase by one, you're all good. You can ignore that reader. We have the additional constraint that if you ever see it being even, you can also ignore that reader. And the way to think about that is if that value is even, it means that the reader is not currently looking at the map, which means that if it were to look at the map, it will see the new pointer. Therefore, seeing it even is all we need. These two invariants combined gives us the guarantees that we wanted, namely that the readers are taking no locks, the writers are taking no locks.
00:37:59.183 - 00:38:32.169, Speaker B: All the read operations are entirely local to that thread, and we still have safe access to both maps. So writes can proceed at full steam. Reads can proceed at full steam. So this is pretty good, right? Remember that this was the graph that we had for the reader writer lock. Now, what we're going to now do is look at the exact same experiment, but using this other map. And we're going to have the write swap the maps on every write operation. So there's no amortizing or anything, it's just going to swap on every single operation.
00:38:32.169 - 00:38:57.225, Speaker B: What we get is this on the same axis. And you see that the writes are like a lot faster than the ones for reader writer lock. They are still going down, but that's sort of what we expect because there are more rights happening. Sorry. There are more reads happening and so the writer gets to take their turn less often. So that's as expected, but it is doing a lot better. But where are the reads? Well, we have to zoom out a little bit.
00:38:57.225 - 00:39:20.247, Speaker B: So this is the reader writer lock on a slightly different scale. And you see that it's really not that good. Like reader writer locks are terrible. Here's how we do. Can you see how much it matters to not use locks? Right. In fact, this is with 16 Threads, we get to do 25 million reads per second. That's pretty good.
00:39:20.247 - 00:39:36.325, Speaker B: That's pretty good for a little data structure. So this is actually a crate that you can use in your own projects. This is a library that is put out there. It's fully documented. All of the internals are documented as well. The benchmarks are open sourced. There are seven lines of unsafe code in this entire library.
00:39:36.325 - 00:40:16.145, Speaker B: And the places where it's unsafe is basically to ensure that whenever we know that an invariant is true, like for example, we know that we have exclusive access to the left map as a writer, then we then were willing to turn that raw pointer that we keep into a rust mutable reference because we know by virtue of our invariance that this is now safe. So that's pretty neat. Seven lines of unsafe. There are seven more lines of unsafe that avoids keeping both maps. You still keep two maps, but you deduplicate the data between the two maps. Then you need another seven lines of unsafe. But I feel like that might be okay.
00:40:16.145 - 00:40:53.065, Speaker B: So let's go back to the SQL. I've given you sort of a micro benchmark here of how this performed with just that basic operation and compared to reader writer locks. But what happens at bigger scales. So let's just look at this query for a second. This is a relatively common query that you see a lot in web applications, right? You're just like counting the number of votes for a thing. It happens in Reddit, it happens in hacker news, it happens in lobsters, it happens in all sorts of websites that do this. So what we did was we took this exact query and we ran it on MySQL we ran it on system Z, which is a database system I can't name because they have a benchmarking clause.
00:40:53.065 - 00:41:17.165, Speaker B: We ran it against pure memcached. So just memcached obviously doesn't execute SQL. But in memcache we just do multi increments. So we just use the incor operator in memcache and multigets. So it's single round trip for all the operations. And of course that's sort of not even a realistic benchmark because it's not persisting anything. But let's just go with that.
00:41:17.165 - 00:41:45.885, Speaker B: And then we also ran it on Noria, and Noria uses this evmap, this map that we just talked about. The results are like this. So the X axis here is the number of requests per second that we give the system. And the Y axis is the 95th percentile latency that we observe for requests. Notice that MySQL and System C fall over almost immediately. I think the number there is somewhere around 200,000 operations per second. Memcached gets to about 7 million operations per second and we get to 14.
00:41:45.885 - 00:42:14.879, Speaker B: The difference between us and Memcache D is surprising here, right, because we're actually doing full SQL execution. But it's because our reads are just reading from cache and our cache just uses this data structure, so there's no locking. Memcache, on the other hand, does do locking on reads. It locks the tables that it uses. And that's why we're about twice as fast as memcached. So it turns out that this kind of data structure optimization actually really helps. So let's look at something that's not as micro as this.
00:42:14.879 - 00:42:44.775, Speaker B: This is still pretty micro. It's single query. So what we did was we took the Lobsters website, which is sort of like hacker news. The difference is that it is open source and we were able to get production traffic traces from it. We took those and we built a workload generator that basically emulates user requests. So it just loads different pages with the same kind of distribution as you see in the real workload. And then we issue the same queries as loading that page would have issued.
00:42:44.775 - 00:43:16.235, Speaker B: What we see is this. With MySQL you get around like 1,100 page requests per second. You can serve with Nori, you get about 5,000 in the latest version of the code. MySQL actually gets worse because it turned out that we were giving it an easier query than we thought. So MySQL is really around like 800 or 900. This is not thousands 800, 900 page requests per second. It was on a 16 core server, whereas Noria gets to about 6,000.
00:43:16.235 - 00:43:42.885, Speaker B: So this is a pretty significant speedup that we see both from caching and from this additional data structure. So I've talked to you a lot about sort of data structures and performance now. So let's go A little bit back to what this experience has been like. This has all been to show you that this is a real system. Like, this is not a joke. It is a real system that is very fast and unreal workloads. So now let me try to tell you a little bit about what it's been like both to learn Rust and use Rust with this system.
00:43:42.885 - 00:44:11.375, Speaker B: So the first thing I want to tell you is Rust is different. You cannot go about learning Rust the same way you learn all these other languages you've learned in the past. It is true that its syntax is sort of relatively C or ML, like. Like it's not that bad just to look at it, but that's a little bit deceptive. It is not just another C or C and go. If you just try to get the gist of it and just like, hack your own code, you will hit a wall. And that wall is the Borrow checker.
00:44:11.375 - 00:44:39.015, Speaker B: And you might have heard this phrase of fighting the borrow checker. That is what you're going to end up doing. You're going to be spending a lot of time sitting there. The compiler yells at you, going, you're not allowed to do that. And you go, why? Why? And then you're just going to be screaming into the void because you don't understand the model that's underlying it. I've tried to give you some inkling of what it's like and it's not actually all that complicated, but you do need to learn the model. So the learning curve is basically learning borrow checking.
00:44:39.015 - 00:45:07.639, Speaker B: Now, there are a lot of depths to that, but even just a cursory understanding is going to get you pretty far. I really recommend that you read the Rust book. So the Rust Programming Language book is an online book written by the Rust team. It is very good and you should read it start to finish. This is not something we normally do when learning a language, but with Rust, that is actually worthwhile. Doing it will save you so much headache and so much time. There's also something called Rust by Example for those of you who sort of prefer learning by reading code.
00:45:07.639 - 00:45:38.421, Speaker B: Rust by Example goes through many of the same concepts as the Rust book does, but it does so by giving you code examples every step of the way. And you can compile and run the code samples too, to try to figure out and sort of understand how tweaking the code helps. These two tiny URLs at the bottom are links to resources for, basically outlines for how you should go about learning Rust. They may or may not be useful to you. But I recommend you check them out if you're interested. Um, the other thing is you really want to work with the language. And this does not just apply to Rust.
00:45:38.421 - 00:46:12.747, Speaker B: Um, but I think it applies especially to Rust if something feels hard. If you get stuck on something, it could just be because your program is wrong. That is true, but in many cases it's because the language is sort of trying to tell you something. Like if you get stuck on borrow checking, it's probably because the approach you're taking might not be the right one. So one example of this is if you end up with cyclic data structures, like you want to represent, say a graph, for example, and you try to represent it just by having pointers everywhere like you wouldn't see. You could totally do that. You might have to either do unsafe or reference counting or something.
00:46:12.747 - 00:46:48.165, Speaker B: The alternative is store all the nodes in a list and then store the indices in the graph. And now you have no cycles, right? So you're sort of fighting the borrow checker because you're trying to represent something in a weird way. Like you could use reference counting, but you should really sort of second guess your own design if the compiler is really fighting you on something. In some cases the compiler is just being stupid. So there's an IRC channel. There are also discord channels and gitter channels and all the other things, but in particular the IRC channel, this is called Rust on Moznet, is really good. There's also a Rust beginners channel there.
00:46:48.165 - 00:47:31.995, Speaker B: If you get stuck, ask a question and someone probably from the Rust core team will come back to you in like five minutes. It's fantastic. There's also a secondary argument to this, which is if the compiler, if borrow checker understands your code, it's a higher chance that someone else would understand it too. If you start adding lots of unsafe annotations, just like do lots of pointer type aliasing, you can do that. But chances are like six months later or three months later when you come back and look at this code, you will not understand why you did what you did, or what invariants are being enforced by the unsafe and no one else is going to understand it either. So if you can avoid unsafe and not have to fight the borrower checker, that's probably better. I also recommend that you learn the isms of the language.
00:47:31.995 - 00:48:12.855, Speaker B: Rust has some things that other languages, current languages don't really have. So Iterators is a good example of this ends up being a slightly functional pattern. So those of you who experience with Haskell might be more familiar with them. Iterators are basically, you can walk any collection, you can think of it as a generator in Python, and you can do really neat things just based on iterators that make your code a lot easier to read and write. Pattern matching is fantastic. Error propagation in Rust and null pointer propagation in Rust is really easy using the question mark operator, which you'll become intimately familiar with if you start writing Rust code. And they're just going to save you a lot of time to learn them.
00:48:12.855 - 00:48:45.275, Speaker B: I wrote a blog post a while ago when I discovered many of these myself that you may find interesting if you start diving into Rust. There's also a really good tool built by the community called Clippy. So Clippy is a tool that will take your Rust code and tell you all the things that you probably shouldn't be doing. It's like, basically someone wrote a thing that's going to do peer review for you and it's going to yell at you for like, don't do that, don't do that, don't do that. It's great. It's like go Vet. For those who used Go, I also recommend that you build crates.
00:48:45.275 - 00:49:09.267, Speaker B: So Crate and Rust is basically a library, but they're really, really easy to make new ones. The overhead to making a new one is really low. The overhead for adding a dependency of one is really low. And the reason is because it basically forces you to do separation of concern. It forces you to write modular code. It also gives you faster builds because Rust compiles multiple crates in parallel. And so it speeds up your build process.
00:49:09.267 - 00:49:49.387, Speaker B: And finally, it gives you much better documentation. It incentivizes you to write documentation for each module separately. And Rust has, amongst other things, a lint that you can turn on in the compiler itself. In the language that says deny missing docs, you set that at the top of your library, and now your program will not compile unless everything is documented. It's fantastic. The other thing that's really neat and that I've observed over time in Rust is that refactoring is really straightforward. And this is in part because of the type system, right? You can decide to move a bunch of stuff around and then you let the compiler guide you to all the changes you need to make.
00:49:49.387 - 00:50:22.765, Speaker B: And when your program finally compiles, chances are it's basically right, because your types work out, and that means that the code is all now in the right place. You can do compiler guided refactoring, and you can basically trust the compiler. This is for those of you who programmed in Haskell in the past is a similar kind of feeling of when it compiles, it's probably pretty close to correct. There's still going to be bugs. Trust me, there will still be bugs. I also recommend you take advantage of the ecosystem because the dependency management is so good. It is really easy to add dependencies on projects that other people have written.
00:50:22.765 - 00:51:05.355, Speaker B: There are a lot of really good libraries out there that you can just add a dependency on and it works. The code formatting tools are good enough in Rust that you can look at other people's crates and basically understand what they're like. The formatting is consistent, the documentation is consistent, and because it's auto generated for all published libraries and interlinked automatically, it is really easy to just look up a crate. You're going to see its documentation, you see how it fits together with yours, and if you upload your library, it will be automatically interlinked with that crate that you depend on as well. The ecosystem is still a little bit young, so there might be some libraries you don't find, but then maybe you can write that library. I'm going to leave you with this quote that I think is really good. It's a little bit long.
00:51:05.355 - 00:51:44.793, Speaker B: I'm going to read it out loud, but you can also read it internally. One of the value propositions most frequently lauded by Rust developers is its freedom from data races. The compiler will literally not allow you to build code that could ever produce a situation where two threads can mutate the same data. This teaches us two things. Loathing of the Rust compiler and its borrow checker until we make things work and two, the extent to which we written unsafe race prone code in other languages. I've learned a great deal from that second bullet point. I look back at some of the code I've written in the past and I realized the only reason it worked is because some other library had guardrails in place or the lack of data races was an accident.
00:51:44.793 - 00:52:32.773, Speaker B: In other words, my lack of failure is more attributable to random chance than skill. I really feel like this when writing Rust code now that it has made me a better programmer. I feel like I now see data races in my code and I realize that the compiler is about to yell at me. If I go back now and write Python code or C code, I notice that there are data races where because I think all the Rust compiler would never accept this program, so therefore it must be wrong. If I look at old code that I have written in other languages, I realize that that code is probably broken. I wish I had the Rust compiler to tell me what was wrong with it. I've tried to cover both low level details about Noria and high level details about Rust in general.
00:52:32.773 - 00:52:48.995, Speaker B: If you have any questions at all about any of the stuff I talked about today, then feel free to follow up me either by email, that's my email, or I'm on GitHub and Twitter and all the other things under the username on the right. Feel free to reach out to me or through Larry if you want, and I'm happy to take any questions you might have. Thank you.
