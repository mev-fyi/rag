00:00:01.120 - 00:01:09.846, Speaker A: Hello, folks. Welcome back to another stream. This time is going to be another, I guess we're calling them impel rust streams now, but this is going to be fairly different from the other impl rust streams in that we're not tackling like, you know, an algorithm from a paper or some deeply complex technical convoluted data structure and algorithm concurrency primitive thing Majig. Instead, we're actually going to tackle something that is ostensibly a fairly straightforward problem, which is we're going to play the wordle game, and specifically we're going to write a program that's going to play wordle for us, and we're going to try to make it as fast as possible. So my hope here is that this is actually more approachable than many of the other impel rust videos. We won't probably get into too many weird unsafe pointer manipulation type things, and instead we're just going to write rust code, and then we're going to be performance profiling and then optimizing that code over time. Now I'm going to do a quick wordle intro for those of you who either have heard the name and have no idea what it is or haven't even heard the name wordle.
00:01:09.846 - 00:02:00.010, Speaker A: Is this actually, let me give you a dark mode here, because that makes people happy. The wordle is a fairly simple word game where you are trying to guess a five letter word from the english language. There is a sort of dictionary list of which words you can guess, and you get six guesses to try. And when you make a guess, you are told which letters you have guessed correctly, which letters you have guessed that aren't in the word, and any letters that you have guessed that are in the word but aren't where you put them. So, for example, let's say that I open here with bears, right? So here I learned that there is a b, and the b is indeed in this first position. In the final word, there is an e, but it's not here. There is not an a.
00:02:00.010 - 00:02:36.462, Speaker A: There is an r, but it's not here, and there is not an s. There could be more e's, there could be more r's. We don't know that from the guests that we did right now. So, for example, let's say that we think that it's going to be br with like Br seems like a thing, the word start with, and then there's going to be an e somewhere. So we could say broil. Ooh, broil. Okay, so you're not restricted to using letters that you have guessed in the past.
00:02:36.462 - 00:03:00.322, Speaker A: Like, I know that there is supposed to be an e in here, but I can still guess something that doesn't have the e in it. Part of the reason why I'm going through this and choosing particular words is also because I have done today's wordle. So I know what the answer is, but let's do broil. Okay, so we now learned that Br is, in fact, correct. There is an I somewhere, but it's not in that position. There's no o and there's no l. So at this point, we're in a pretty good position.
00:03:00.322 - 00:03:13.500, Speaker A: Right. So we know that there's an I and an e in the word. It could end in an I, but that seems weird. So it's probably an I there. And so it's something that starts with Bry and has an e in the last two. Right. So let's try bridegest.
00:03:13.500 - 00:03:33.340, Speaker A: Okay, bride. All right, that's pretty good. So it's a word that starts with Brian, ends with e. And what other words are there? We're Brian, for example. And I apologize for the spoilers after the fact, but. So in this case, we managed to guess the correct word in four guesses. Four guesses is.
00:03:33.340 - 00:03:56.560, Speaker A: You could describe it as par, right. Like, it's pretty good. In general, you're going to get it in for sometimes it'll take you more, taking less. Generally, either you have to get lucky or. Or you're just really, really good at this. But in general, the expectation is that most guesses are going to end up around four. And I think if you look at the wordle stats, that's also the sort of bell curve you end up seeing.
00:03:56.560 - 00:04:36.712, Speaker A: So that's the very basics of wordle. There's one word every day and everyone is guessing the same word each day. Now, wordle has a couple of, a couple of data sets that are of relevance to us. It has a dictionary of words that you are allowed to guess. So, for example, if you try to guess a word that's just all a's, it's not going to accept that guess and you get to try again. So there's sort of a dictionary of allowed words, and then there is a dictionary of answer words. And what we're going to do is we're going to try to solve the game using only the same knowledge as a player would be.
00:04:36.712 - 00:05:27.550, Speaker A: So we're not going to use the list of, you know, known answers. Instead, we're just going to use the list of allowed words, which includes all of the answers. But that's the only list that we're going to be using here. So that brings us to today's topic, which is three blue, one brown. If you haven't heard of this YouTube channel, I suggest you check it out. It's basically a YouTube channel where Grant Sanderson just goes over a lot of interesting topics from science is the best way I can describe it. I really like Grant's explanation style, but in particular, this one video posted back on February 6 of a video to solve wordle using information theory.
00:05:27.550 - 00:06:06.960, Speaker A: And if you haven't watched this video, I highly recommend you go watch it. I'm going to put the link in chat because we're going to be following the algorithm from this video. So like, I'm not going to explain the algorithm in detail as we go through because there's sort of an expectation that you've watched this video. If you haven't and you're watching this like play, press pause, not play pause, press pause, go watch that video and then come back and resume. There is a follow up video as well where it turns out that there was a bug in grants implementation. This one doesn't affect us that much. It doesn't really change the algorithm.
00:06:06.960 - 00:07:11.294, Speaker A: It has a couple of other interesting bits about how to do better if you incorporate the list of known answers and a little bit of discussion of whether it ruins the game to have this kind of a solver. But it's not required watching, whereas the first one I think really is because this is the algorithm we're going to be following. And when I saw this video, I got really excited because I love doing stuff like this where it's just, you're just trying to build an algorithm to solve a problem. But it got me particularly excited because I've been looking for something that I can do, like a performance optimizing video on in rust, where we write a program and then we optimize it and make it faster and faster. And I haven't really come up with a good like sample project for profiling. You really need a real code base to benchmark. It's kind of hard to benchmark and profile something that's just like an arbitrary little code snippet.
00:07:11.294 - 00:07:46.000, Speaker A: You really need a full program, and this is a great example because there's a pretty large search space. So the program is going to run for a while. You also have the ability to run it in multiple passes. We can have it try to play the game over the course of multiple days and then profile across all of the runs and see where our bottlenecks are, and then we can optimize, and we can pretty easily figure out a where, where we can optimize, and we could try to figure that out. So, so this is a great candidate for trying to do that kind of performance profiling and optimization. And so that's what we're going to do today. We're going to implement the algorithm from this video.
00:07:46.000 - 00:08:46.370, Speaker A: We're also going to obviously have to sort of implement wordle, and we are going to run the program, see how long it takes to run, and then we're going to iteratively optimize its performance to try to make it as fast as we can. In particular, we're going to be using a tool called hyperfine. Hyperfine is really great. It's a tool that basically lets you run multiple commands that ostensibly do the same thing, and that it measures their performance and tells you how much faster or slower one is compared to the other. And crucially, it does this by running each command multiple times. So it tries to gather things like the variance and execution time, and actually give you some measurement of the statistical significance of the performance difference, if any. So the hope here is that we don't just do premature optimization, we actually measure and see which optimizations make a difference, and then hopefully iteratively make the program faster and faster that way.
00:08:46.370 - 00:09:27.592, Speaker A: And we're probably not going to get into the depths of inline assembly and that kind of stuff. We might look at the assembly, although I think it's unlikely. I think they're going to be much lower hanging fruit than that. To easily fill out this video, we might also get into multithreading. I haven't quite decided yet whether I think that's important. One of the reasons for this is because multithreading tends to hide the true performance of your program. So once you multithread, it's a little harder to benchmark.
00:09:27.592 - 00:10:34.836, Speaker A: But also, things tend to be so fast when you use many cores that it's hard to actually figure out what's slow. Whereas if you run it on one core, it's very obvious where the time is being spent, and all optimizations are very visible very quickly. And so it's almost like you kind of want to optimize it in single threaded and then make it multithreaded once it's as performant as you can make the single threaded part, and then you were to compare them. So you want to see what was the cost, cost of doing this multi threading in the first place. All right, so I think that's where we're going to start. As I mentioned, I'm not going to go too much into the details of the video, but I'll mention that at the core of it lies the idea of information theory. This is a really neat notion that, again, the video explains really well, but the basics of it is that this formula allows you to sort of compute how much information you can expect to get from a given guess.
00:10:34.836 - 00:11:19.300, Speaker A: And the idea is that we're going to play wordle, and at each step of the game we're going to pick the word that is likely, that gives us the highest expected amount of information for that guess. And that way, in theory at least, you should be whittling down the list of words as quickly as possible. There are obviously ways to improve on this algorithm. We're not going to focus too much on the algorithm itself here, though. We're going to implement the algorithm in a fairly straightforward way, and then we're going to focus on the optimization part. But this is obviously something where you could spend hours and hours and hours and days and days and days on just twiddling with all the things and getting the score slightly better. That's not what I want to focus on here, because I want to focus on the rust part of things rather than the algorithmic side of things.
00:11:19.300 - 00:11:52.840, Speaker A: Okay, great. In order to do this, to implement this thing, there are a couple of things we're going to need. The first thing we're going to need is the wordle dictionary, because without the wordle dictionary, we wouldn't even be emulating the right game. And it turns out that it's actually really easy to get the wordle dictionary because it's all in the source code. So specifically, here's what we're going to do. We go to application, and then we go to. No, not application sources.
00:11:52.840 - 00:12:19.224, Speaker A: We open this JavaScript file, we format it, scroll down a little, and down. Where is it? It's trying to be helpful. Here we go. We have two arrays, ma and oaehethere. And if you scroll right. Oh, it's cutting it off. It's fine, I've already copied it elsewhere.
00:12:19.224 - 00:12:39.128, Speaker A: But ma is the list of all of the, all of the target words. There's one for each day. And what wordle really does, it just walks along this list. So if you ever want to look up the answer for, you know, today or tomorrow or yesterday, you can look it up using this list. It is in order. And then there's oaehethere. Oa.
00:12:39.128 - 00:13:11.860, Speaker A: Here is the dictionary of all allowed words, but it does not contain ma. So you sort of want the concatenation of these two. Once you have that, then now you just have the dictionary words. And so that is easy enough to get started. I've already fetched these locally. I'll show you in a second. The other thing that we're going to need is in order to compute which words are more likely to be the answer or not without relying on the true list of actual answers.
00:13:11.860 - 00:14:08.456, Speaker A: What we're going to use is the Google books n gram dataset. This one is openly available and it's a very large data set that basically has, well, it has multiple different n grams. An n gram here is word n grams. So 2 grams, for example, are all of the sets of two consecutive words across all the books that Google books has scanned across all the years over which they have scanned books, and the counts of each one. So in the 2 grams data file, for example, you might find an entry for the world, and the world is going to have a count for each year that Google books has records for. And similarly for hello world, there would be a count for every occurrence of hello world for each year going on. And it'll be a sort of curve.
00:14:08.456 - 00:14:29.934, Speaker A: And there are viewers for this and you can scan it. It's a really neat, um, thing to be able to scan. It has its problems. We're going to start there, and specifically we're going to use the 1 gram data set, which has 1 gram. So single words. So it basically has the count of single words that appear in all of the texts the Google book scans. So if we.
00:14:29.934 - 00:14:52.398, Speaker A: I actually have. I've cheated a little here. I did a little bit of this in advance. So this is the entirety of the 1 gram dataset from Google books. You see, it comes in like 24 Gzip files and it's pretty large. That's why I did it beforehand. And what we're going to do here is use rg.
00:14:52.398 - 00:15:31.100, Speaker A: So rg is Ripgrep, the command line tool that's like grep but much faster and written in rust. And it's great. And what I'm doing here is searching, actually, let me maybe give you a, a sample of one of these files. So this is, I'm just picking out some random lines from one. So you see that the construction here is that every, that's a terrible example, but I guess we'll do it. I should not have used tail. That was still.
00:15:31.100 - 00:16:21.930, Speaker A: Do I have a better one? Sure. So here, you see there is a, at the start of every line there is a word, so this is the 1 gram, and then this is the year, the number of occurrences of that word in that year, and the number of distinct books that that word appears in. Right? So you'll see that the left number is always higher than the right number. And you see that there's a bunch of tabs separated entries for this. So you see that, you know, this word appeared once in one book in 1816, and if you go back to 2012, still one in one. And it's interesting that in the middle there are a couple of higher ones that those are probably miss scans, because in general this data set is cumulative. So this might be that they've fixed up the way that they scan.
00:16:21.930 - 00:17:19.650, Speaker A: In any case, the exact format here isn't that interesting. Instead, what we're going to do is we're going to use Ripgrep to scan for all of the entries here that start with five letters. This underscore bit here is because the Google books datasets, they annotate all the words with, with a. Actually, maybe they, yeah, so with like noun, verb, I guess some of them, they don't even annotate, which is a little interesting. But like this is an adverb, this is a verb, etcetera. The annotations aren't always accurate, but, you know, it is what it is. So what we're going to do here is search for all of the five letter words across all of the files, and with this sort of suffix.
00:17:19.650 - 00:18:11.718, Speaker A: And the dash z flag to ripgrep makes it scan compressed files. The dash capital I flag, you know, that is a good question, right? Because we're printing multiple files, we wanted to not print out the file name that it found the match in. We just want all the matches and then we're going to stick that into five letters text. And so I've already run this command because that takes a while on this large data set, and that has about 2 million lines. So there are about 2,000,005 letter 1 gram in this data set. And if we look through it a little bit, you see that it has the sort of expected, the expected structure, and then we can do a bunch of massaging here. In particular, we want to sort of trim off this bit over here and we want to extract only the count as the end.
00:18:11.718 - 00:18:55.144, Speaker A: There are many, many ways to do this. The way I ended up doing this was using awk, because I like awk. Let me see if I can dig up the command I used. Actually, I can just do five letters txt and find the command that I used there. So Awk is a great little tool for managing files that have separated records that you want to do something with. In particular here, I'm telling it that the field separator is tab, and I'm telling it to print the first thing, the first field. So that would be the word followed by a tab, followed by the last field, which is nf here as number of fields of this file.
00:18:55.144 - 00:19:56.604, Speaker A: I'm going to substitute every underscore here with a comma, and then I'm going to run awkward with comma as the separator and then print out the first, which is then just going to be the stem of the word I, and the second to last field, which is going to be the occurrence count for the last year in the data set. And then I'm going to stick that in the five letters occur file. So at this point, we have this file, it has the same number of entries, right? Because this is not filtering anything out, it's just trimming out the fields. And in fact, if we look at it, it just has, you know, the counts for each of these words. And then we need to do a little bit of normalization, which we can do again in a number of different ways. What I ended up doing was replacing all capital letters with lowercase letters using TR and then sorting that. And that gives us the list in sorted order.
00:19:56.604 - 00:20:38.976, Speaker A: One thing you'll notice with five letters in lowercase sorted is that there are multiple entries for the same word, and this is because they had varying capitalizations. So we really want to combine all of those. And there are ways to do that too. I know this is not directly rust related, but it's just to show you how I got the data set. So five letters, LC sorted text. What I did to this, and this is a little bit nasty, but this is using awk again, is I'm using AwK to just combine consecutive records with the same word and combine their counts. This is a very simple Awk program.
00:20:38.976 - 00:21:13.870, Speaker A: You could do this in whatever you wanted. This was just an easy way to do it on the command line. And I wrote that into this combined file. And if we look at five letters, LC sorted, combined, you'll see that there are about 900,000 distinct words here. If we look at the head of it, it has just one entry per each. And then if I do lc sorted combined text, there are all sorts of interesting things we can do here. We can look at the.
00:21:13.870 - 00:22:02.520, Speaker A: Where do we have the, where's my search for this? So we can do things like sort by the count and show the top ten. So these are the top ten most frequently occurring words in this data set. We can also do things like compute the total count here. Oh, that's not what I meant to do. Let me dig up where that is. So what we're going to do here is extract the second field, combine them all with plus characters, and then pass them to a calculator. And that shows us that the total number of occurrences here is about 1 billion.
00:22:02.520 - 00:22:56.456, Speaker A: So there are about 1 billion occurrences here. And then we can divide, in order to get the probability of any given word, we can divide the number of occurrences of that word by the total number of words that occur anywhere, which we can do like this. I don't want to sum them, I want to. I also only want the, no, I only want the sort of this. I should have pruned my history a little bit before I did. This would have made it nicer. So we're going to pick the like top ten words again by, by rank, and we're going to print out the current count divided by the total count that we just computed and multiply it by 100 to get a percentage.
00:22:56.456 - 00:23:35.150, Speaker A: And so the probability of which occurring if you pick a random word out of an english book is about 2.5%. Great, so that's a good place to start. Of course, then we need the wordle wordlist. I told you I had already imported that as a JSON file. And so here, this is the JSON of all of the two lists concatenated. You see, here is where there used to be a split, because it goes from things that are not in order to things that are in order. And then I just use JQ to pull out every entry and sort it out.
00:23:35.150 - 00:24:17.960, Speaker A: And now we have a dictionary file that has all of the possible or legal guesses in wordle, including the answers. And then we can use the join command to join. That's not what I want. I want this command. So we're going to join the dictionary txt file, which has all of the valid wordle words with the frequency count file. And what join does is it looks for entries that are common to the two files. So this is basically going to only, it's going to output the counts for all the words in the wordle list, but no other words.
00:24:17.960 - 00:25:01.136, Speaker A: And the dash a one here is to include words from the dictionary that we don't have account for. And we can of course do sort nk 22 here as well. We can reverse sort and head. And so now this list should be the same as the list that we had before, except all of the words here are legal wordle guesses. So if we look at this combined list has about 900,000 words, whereas the original wordle dictionary only has about 1300 words. And the, the join that we did should have the same number of words. Right.
00:25:01.136 - 00:26:04.050, Speaker A: But the difference is that it also has the occurrence count for each of those words. And so that's where we're going to start. And for words that we don't have counts, that is something we're going to need. We can just look for anything that ends with a letter, right? So that would be anything where we don't have a count. These words are allowed by wordle, but don't have an entry in the Google Ngrams dataset. It's unclear whether these are really words, but, you know, they're, they're in there. So what we're going to do here is stick this into dictionary with counts dot text, and then we're going to edit that file and we're going to look for a to e at the end of the file and we're going to just make all of these be, have a count of one because you know that they didn't appear in the n grams data set, so it seems like they're pretty unlikely to occur, but we need to give them some value and don't want to, don't want it to be zero.
00:26:04.050 - 00:27:02.108, Speaker A: So now we have a list of, you know, space separated words and occurrence count for all of the words in the wordle dictionary. And that is a good place to start. I think that is sort of the data set that we're going to need. There is technically one more data set that we're going to want, which is this one answers dot Jason. And the reason we want answers dot JSON or really the corresponding text file in order, is because we have to have something to run our bot against. We could run it against every possible word and see how well it does it guessing. But realistically, we want to run it against the real wordle game, right, which means we want to play every possible wordle game.
00:27:02.108 - 00:27:40.880, Speaker A: And every possible wordle game is to run each of the answer words. So we're not going to allow our program to use knowledge of this list in its algorithm, but we are going to use that as basically the test set. So that's where we're going to, that's where we're going to use that, that file. But that's the only place we're going to use that file. So at this point, it's time to start writing rust. And for that, we are, of course, going to need to know what to name our dev streams. And I have already come up with a name for one, and that's going to be Roger.
00:27:40.880 - 00:28:36.950, Speaker A: And there's a little bit of backstory here. Roger's thesaurus is sort of the original thesaurus. If you don't know what a thesaurus is, it is basically a way to look up words that are related to other words. It's not just synonyms, but it's things with related meanings that lets you explore the knowledge space or the related word space in a very abstract way and find better words to accurately represent what you're trying to say. And there are all sorts of cool tidbits about Rogit and Roger's thesaurus, but this seemed like a good place to start for a wordle solver, to call it, you know, name it after a thing that lets you explore words. So we're going to start out here with Rojit, and we're going to copy in wordle answers. Text is going to go in here.
00:28:36.950 - 00:28:57.780, Speaker A: And wordle dictionary with counts. It's going to become dictionary text. Yeah. Roger does also. Five letters, which is very, very nice. All right, so now we need to start somewhere. Fn main.
00:28:57.780 - 00:29:30.334, Speaker A: That seems great. So we're going to need two things. We're going to need a function that lets you play wordle, and we're going to need a function that tries to do our guesses. And so the way this is probably going to look is something like play. Play is going to take. Hmm, that is a good question. What exactly is it going to take? I think we're going to go with.
00:29:30.334 - 00:30:12.230, Speaker A: It takes an answer, which is a static string, and it takes a guesser, which is going to be some g. That's going to be generic, that we haven't determined yet. And it's gonna play six rounds where it invokes the guesser each round. And guesser probably doesn't need to be a trait. Right. Maybe it's just really a function. Let's see how that ends up going.
00:30:12.230 - 00:32:20.202, Speaker A: So what is the guesser going to do? Well, when it needs to make a guess, what information is going to be given? Well, it's going to have to return a word. It's going to be allowed to take mute self. And in order to guess, what is it going to be told? It's going to be told all of its past guesses, I think, is the way to go about this guess. So what is a guess? A guess is the combination of the word that you guessed and, and the sort of mask, I guess, which is going to be a say correctness of five. And what is correctness? Correctness is either green and let's give them better names than green, but we can document that this is green, which is correct, this is yellow, which is that it is what's a good word for this present. And gray, which is missing. Misplaced is maybe misplaced.
00:32:20.202 - 00:34:03.370, Speaker A: I kind of like misplaced. And I guess we could just call this wrong, like a gray one just isn't there. So it is going to be given a list of guesses which are, I guess, history maybe. And string is going to be the next guess that it wants to make given this history. So this is the basic structure here, right. And then what main is going to do? We're going to have something like answers or I guess games is maybe the way to describe it and that's going to be include structural and we're going to do for game or for answer in games, split whitespace, we're going to play against that answer. And we don't really know what the guesser is going to be here yet, but we're going to be playing against the guesser and we can probably have a, just for convenience, we're going to implement this for atmute where t implements guesser.
00:34:03.370 - 00:35:01.860, Speaker A: This is just so that it's not really necessary. Let's not do that for now. It's fine. So this is going to pass in the guesser, which doesn't have to be mute anymore. And that's all that's really going to do. And I guess this is going to be something like, you know, instantiate the algorithm and let's say that we call the algorithm, you know, naive new and maybe we make, actually let's go ahead and create a lib rs where we stick most of this. So pub, event, play and pub.
00:35:01.860 - 00:35:44.630, Speaker A: Pub. And these can be pub too. That's fine. And so this is going to do, I guess what we'll do actually here is have a mod algorithms. My thinking here is that if we end up implementing multiple different implementations, we could just have them all be in the same binary and then switch based on a flag or something. So it's easy for us to benchmark multiple ones at once. So in order to do that, let's CD into rogit and make their sir slib algorithms.
00:35:44.630 - 00:36:32.280, Speaker A: I mean that algorithms. And we're going to call it, let's do naive first and it's going to use super and it's going to get, I guess it needs guesser. Guessing correctness. Correctness is a little long. And this is also going to be pub. And if we go to main, this is going to be rogit algorithms naive. And this is going to be rojit play.
00:36:32.280 - 00:37:08.740, Speaker A: Right. And I need algorithms rs, which is going to have pub mod actually just going to have mod naive. Pub use naive naive. And this is going to be, I guess, suppose crate. And then we're going to implement guesser for naive. Why not? The code actions not work for me. That makes me sad.
00:37:08.740 - 00:38:24.718, Speaker A: So we're going to take guess and this for now is going to be to do great. So now we have a sort of basic structure for our thing and I think we're going to implement play first because that seems straightforward enough. All right, so how does playing wordle work? Well, we have a target answer. So I guess what we're going to do is initially, I guess, for let mute history is vecnute for I in. So here we could actually do, I think normally you would limit wordle to, you know, you can only guess six times. What we're actually going to do here is not limit the number of times you can guess because we want to know whether the bot succeeded or failed. And it's nice to actually get the distribution in the tail rather than just like it got it in six or less or it didn't get it, in which case we don't know how bad it is.
00:38:24.718 - 00:39:16.130, Speaker A: So we're actually going to let it guess forever. But this is one place where you could, in theory, limit the number of rounds. So the guesser needs to guess I. So for each sort of iteration of the loop, we're going to have guess is going to be guesser guess and we need to give it the history. Currently, initially the history is empty and it's going to grow over time. And then we need to do something in order to compute the correctness based on that guess. And then we're going to do history dot push guess, which is going to be the guess and the correctness.
00:39:16.130 - 00:40:16.400, Speaker A: And then we just keep going. And we are going to say, though, that if the guess is equal to the answer, then we return. And here we actually have it return the like which iteration it succeeded at. So here we're going to return I and maybe this is weird, right? We want to say starting with one because we want to say that it, if you guessed correctly on your first guess, then we should return one and not zero because you did one guess. So if the guess is equal to answer, then we return I. Otherwise we compute the correctness and keep going. And I guess we called this mask, didn't we? We might not need this asterof might take care of that for us.
00:40:16.400 - 00:41:05.306, Speaker A: And. Oh, interesting. It doesn't realize that this is an infinite loop. It should not be possible to reach over here. Did we not call it word? Okay. And we need to have something that computes the correctness. So we're going to need a check which takes the answer and a guess and gives you back a correctness, which arguably could be impl.
00:41:05.306 - 00:41:53.318, Speaker A: It could just be a method on correctness is actually maybe nicer. So we're going to do impl correctness and we're going to have it be called compute and we're going to have to figure out what this does at some point. But that means that this is going to be correctness. Compute, answer, guess. Oh, right. It doesn't actually return that because the correctness is an array of length five. Correctness of length five.
00:41:53.318 - 00:42:37.224, Speaker A: And so this is going to be just check, I suppose. You know, I still want it to be a method on here and I'll just have it be this. It looks nicer. All right. Oh, it'll exit at usize max. I suppose that's true. Did not finish within usize max guesses.
00:42:37.224 - 00:43:12.468, Speaker A: That seems like its own kind of problem. Like if we're going to run out of memory before that happens because we're accumulating history is going to be a vector of the length of the number of guesses, right. We could bound this loop if we really wanted to. But like if we could guard against something like a, you could imagine that you implement a guesser that just, you know, never succeeds. Like you could have a guesser that always guesses the same word over and over and it's not the answer. Right. So it actually just runs forever.
00:43:12.468 - 00:43:56.456, Speaker A: So we could bound this if we wanted to. We could say that like realistically, wordle. Let's see, wordle only allows six guesses. We allow more to avoid chopping off the score distribution for stats purposes. So let's say we allow, you know, 32 guesses. Why not? And then we're going to make this be an option. Use size and this will be a sum and this will be a none.
00:43:56.456 - 00:45:28.050, Speaker A: That seems fine. Okay, so now we need to compute the, the correctness giving an answer and I guess, and here we're going to assert that answer dot lent, assert eek, answer lend is five and assert eek, guess lend as five. Oh, you're right, that is fine. Equals. So here comes the question we're going to do, I suppose, how do we want to do this? Correctness initially is going to be actually, let's make correctness a derive debug clone, copy partial equals, and eek correctness wrong of five. We're going to start out saying all of them are wrong. Then we're going to go, maybe the guess mechanism should return an option.
00:45:28.050 - 00:46:29.780, Speaker A: No, the guesser should always be able to give a guess right. Like the assumption is that the guesser is able to guess the correct answer. You could implement one that can't, but then that's a bug in your guesser, right? If you, if you know statically that it might not be able to find the answer, that seems like its own kind of problem. Like that sounds like a case in which the guesser should panic. We could have it return a result, I suppose, but let's skip that. All right, so there are a couple of ways we could implement this. So we could do something like for I in zero to five, if answer I is equal to guess I, then CI is correctness correct.
00:46:29.780 - 00:47:03.940, Speaker A: I suppose we can do this. That's fine. All right, this is the annoying thing about using UTF eight is you can't easily index into this. We happen to know that this is valid. ASCII. I have an optimization plan for later to make this be a five length byte array, but I kind of don't want to do it yet. I want the code to be as naive as possible in the sort of initial iteration.
00:47:03.940 - 00:48:01.060, Speaker A: So I suppose what we will do instead is for a. All right, you're going to hate this, but for IaG in answers, cars dot zip, guest dot. Cars dot enumerate. If a is equal to g, let's see. So if they're equal, then that is correct. I. Else if actually, here's what we're going to do.
00:48:01.060 - 00:49:36.530, Speaker A: This is where the bug, by the way, in Grant's implementation was, is in the logic we're about to write a. What we want to do is see if there are any other occurrences of this letter in the guess. So if g if answer cars enumerate, dot let sum. It's going to be a little bit annoying because we want to check whether this character occurs anywhere else and we haven't already marked it as misplaced. That's where the tricky part here comes in. Oh yeah, you might be right. We might actually need to do the iteration twice.
00:49:36.530 - 00:50:47.070, Speaker A: Mark things correct or mark things green. I suppose mark things yellow. I think we're actually going to have to do an inner iteration here, which is so obviously we can continue if that is the case, because we have already marked as green, but otherwise we're going to have to look for if. Let sum j is answer. Dot cars, dot enumerate, dot actually dot position. No, I think we need enumerate, find map because we're going to have to map out the eye. What we want to find here is.
00:50:47.070 - 00:51:24.510, Speaker A: Yeah, there are, there are a lot of ways to do this. Like we could also mark. Even marking everything gray is non trivial. It's the same problem. What we want to do is we want to see whether. So here is for a given character in the answer if. Ooh, that's almost the wrong c.
00:51:24.510 - 00:52:18.440, Speaker A: I guess a is fine. No, it can't be a. Actually, this second loop should just loop over the guess. It doesn't need to loop over the answer. And then if C of I is equal to correctness correct, then it's already marked in green. So this is from the answer if C of I. Actually, we can just match on that.
00:52:18.440 - 00:53:48.530, Speaker A: Yeah, I mean, we could also do, we could track the count of how many times we've given out each letter. I'm trying to find like an, a nice way to represent this algorithm, and I don't want to use something like a hashmap because this is going to be run for every guess. So it actually has to be fairly fast. We might not be able to do this with a simple iteration. We might actually need to keep track of the counts for each letter. I think it's actually just like marked is bool something like false of five. And then for Inc, dot iter, dot enumerate.
00:53:48.530 - 00:54:29.650, Speaker A: Actually we can just do four. I see. And see if C is correctness correct instead of marked, we can say used. If it is correct, then use of I is true. And then we can walk through here. And if C of I. And so now this is going to be look through all the characters of the answer.
00:54:29.650 - 00:55:53.140, Speaker A: And if you find a, if a is equal to ghdev and not used, I then return some j, then used. Used is true. Otherwise none. I'm going to explain this logic in a second because I. This is something we need to write a test for to see that it does the right thing. Then C of I is equal to correctness misplaced, and c dot iter, dot enumerate c of. And in fact, we don't even need to, we don't even need the value.
00:55:53.140 - 00:56:47.922, Speaker A: It's just if any then return true else or just false, I suppose. All right, so the intention here is we, for each letter in the, in the answer, we need to mark whether that character has been used to mark something green or yellow. All of the green things have obviously been used to mark something as green. And then we walk through all of the, all of the characters of the guests. If they're already correct, we have nothing to do. They're definitely not yellow. Otherwise, if there are any characters in the answer that are equal to the guest character and haven't already been used to mark something as yellow, then we mark it as used and then we say, yep, this is a yellow character, otherwise it is not.
00:56:47.922 - 00:57:34.230, Speaker A: And you need to keep, keep looking. So I think that would do it. All right. So that should then be all we need to return c. And let's immediately write some tests for this because I have very little confidence that this is actually correct. And so we're gonna have a sub module here for compute and tests. This is going to be, I suppose, basic.
00:57:34.230 - 00:58:44.326, Speaker A: So we're going to have, we're going to assert equal, I guess here we're going to use created correctness. We're going to cert equal correctness compute of, let's say if the answer is ABCDE and we are guessing ABCDE, then we should get back correctness. Correct five. Right. This is certainly the simplest test. I have done something in naive. It needs to implement naive pub fn new return self should arguably just be, just implement default.
00:58:44.326 - 00:59:25.080, Speaker A: Really use of moved value. Oh no. Where in main like so. All right, so our basic test works. Let's now see if something, I guess all green, all gray. If I guess f, g, h, I, j. Alphabets are hard.
00:59:25.080 - 01:00:09.080, Speaker A: All of them should be wrong. And if I do all yellow, ABCDe and I guess rotate it by one, so EABCD, then they should all be misplaced. See if that's the case. That's fine. And let's do something where there is repeat green. So we're going to do AABB and we're gonna guess AA CCC. What we should get back is.
01:00:09.080 - 01:01:26.790, Speaker A: And here we could actually, we can make a little macro to make writing these correctness things a little nicer. We're gonna have the syntax here be something like we're going to say that M is going to be turned into correctness misplaced. This is going to be, this is going to look real funky. That's going to be wrong. It's going to be correct. And we're going to do some repetition of tokens is going to turn into, and let's say we do this and we're going to say that it's going to turn into for each of these we're going to call this mask. It's going to turn into mask of the TT.
01:01:26.790 - 01:02:22.030, Speaker A: So each character here is going to be repeated in here by calling the macro and then separating by a comma. I'm lying. This should just be that. And now we should be able to have this be just mask of Ccccc. If I wrote my macro correctly, which perhaps I did not, oh, it ends up being one token. Fine, we're gonna have to do this. No rules expected this.
01:02:22.030 - 01:03:04.650, Speaker A: Fine, let's make it an ident then. Why? Oh, you're right. I don't actually need the square brackets. There we go. And I think this can now be tt and then we can make these be square brackets and then things look nice. This is going to be w w w. This is going to be, mmm.
01:03:04.650 - 01:03:45.084, Speaker A: And this should be c c w w. Let's see if that is indeed true. Great. So repeat, green works fine. Repeat and yellow. So if I do here ccaa c, then I should get wrong. Wrong.
01:03:45.084 - 01:04:23.138, Speaker A: Misplaced. Misplaced wrong. Repeat some green. So this is going to be, if I have one correct but one wrong, then this should be marked correct. This should be marked misplaced. Great. So this seems pretty good, right? So here we have something that, it seems to actually be computing the correctness correctly even when they're like weird repeats or some of them are yellow.
01:04:23.138 - 01:04:52.680, Speaker A: If you can think of other use cases here. I guess let's, I got one random one from chat. From chat. One which was if the answer is a zzac and I guess aaabb should only mark one yellow. Is that true? That one should be correct. That one should be misplaced. That one should be wrong.
01:04:52.680 - 01:05:33.690, Speaker A: That one should be wrong and that one should be wrong. We passed that. Fantastic. I'll call this doctor eamon from chat. You have a test k named after you. Now here's another one. It's a poc baccc aadd.
01:05:33.690 - 01:06:09.270, Speaker A: So heredehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehehe the first a should be wrong, the second a is correct. The d's are all wrong. We passed that. All right, last one from ricuelo, abcde and aacde should give wrong for the second a. All right, so the first one should be correct. The second one should be wrong. This should be correct.
01:06:09.270 - 01:06:25.690, Speaker A: This should be correct. This should be correct. Amazing. All right, we pass all the, all the from chat tests. That's good. I now have some more confidence that these are correct. We should arguably document it, but I'm not going to do that now.
01:06:25.690 - 01:06:57.932, Speaker A: All right, so we actually now have the, the play algorithm done right. Like this is all there really is to wordle. It's marking the correctness appropriately and just giving the guesser the ability to guess. Most of the complication is in. What do you do for the, for the guesser? Oh, we do need to check whether the guess is valid. That is true. Oh, that's going to be a little annoying.
01:06:57.932 - 01:08:16.890, Speaker A: So we're going to have impul, we're going to actually have then a pub struct wordle, which going to have a dictionary. It's going to be a hash set of str. We could actually make this a perfect hash phf. I wonder if I can, I wonder if there's a ha ph sethen, could I make this happen? No, it has to be a macro. I can, I might be able to do this with a const function if there was a const constructor for this set. Set. Is there a constructor for this? No, it has to be a macro.
01:08:16.890 - 01:09:11.770, Speaker A: The reason that would be nice is because the dictionary is the same. So really we could, and it's known at compile time, so we could compute the set at compile time. That way there's no need to construct this word on parse athenae runtime. And one of the reasons why I don't want to do it at runtime is because that's going to be measured by the benchmark, which makes me a little sad. But I guess it's fine for now. It just makes me sad. So here we're going to say const, dictionary is a stir is going to be include stir of dot, dot, slash dictionary.
01:09:11.770 - 01:10:00.420, Speaker A: And dictionary is going to be hash set from. And I guess we're going to use collections hash set. Actually, you know, we could just have this be a sorted vector, although it would give us log two lookups, which makes me a little sad. Oh, we could use a build script. Would that help? Yeah, I could make the build script emit what Phf requires. That's interesting. I'm going to leave that as an optimization we might do later.
01:10:00.420 - 01:11:05.430, Speaker A: It's a good idea. So dictionary, split, whitespace, every other element. Is there a method on iterator? Advance by is nightly only by. Yeah, so we want stepbytwo. The reason we want stepbytwo is because, oops. Is because every other value is the frequency count from iter. And then we're actually going to have play be a function on wordle that takes self.
01:11:05.430 - 01:11:56.190, Speaker A: You can use lines. Oh, you might be right. Well, I could use lines, but if I use lines, then I also need to map line. Fine, I'll use lines. Line dot split once by space. Expect every line is word plus space plus occurrence. I guess it's not really the frequency, but frequency and dot one.
01:11:56.190 - 01:13:17.770, Speaker A: And then we don't need the stepby. And so here, if not, or then I guess we could just do assertion that self dot dictionary, dot contains the guess. This is also something where we could make this be weird. We could make this be in debug assert. Right? Like the guesser shouldn't be guessing things that aren't in the dictionary, but like, because we're not, we don't have a human player who doesn't know what the dictionary is, right? So I suppose we could give them the dictionary, but I kind of don't want to do that. So let's leave this as an iterator right now. Or does it allocate, does lines actually allocate internally? I think that's when you read a, that's only when you read a file.
01:13:17.770 - 01:13:50.768, Speaker A: I mean, this is easy enough to find out. Stir lines source. Yeah, it's just a split terminator. Yeah, it just returns string slices. So that's fine. So that's fine. Great.
01:13:50.768 - 01:14:05.202, Speaker A: And I'm going to make this. So this dictionary is also going to be used by naive, for example. Right. Because it needs the frequency counts. It's not actually going to be that naive. It's arguably the real implementation. It means naive in terms of performance, not in terms of algorithm.
01:14:05.202 - 01:15:04.190, Speaker A: This is arguably poorly named. Maybe unoptimized would have been better, but we can't change that now. Yeah, so this dictionary is going to be used by other files as well. But here all we need is the set of files. Great. So now we have this and what we now want to do is, I guess we could, we could even have a test here for game, you know, which is test fn play. So here we're going to do wordle, new wordle play.
01:15:04.190 - 01:16:28.816, Speaker A: And I kind of want to implement impel, imple guesser for FN of history guess to string, so that it's easy to write, you know, test implementations, basically anything that doesn't need state. And that's just going to do self history. And now I should be able to say I want to play with this closure. I'm going to play against moved and it's just going to return moved to string and I want to assert equal. I should get a score of one. Use crate wordle and it should be sum of one. And it's complaining because this closure doesn't implement fn.
01:16:28.816 - 01:18:14.670, Speaker A: I thought that it closures automatically were fn if they didn't capture anything, but I guess maybe not. Fine, I'll name it. So we're just going to call this guess and it's not going to use history, it is going to use guess. And now what? What? Why? And I can't easily make this generic over f because then I can't have implementation later, which makes me sad. This is fine, I'm fine. Requiring it to be an actual fn. What? Why is it sad about this? The trade guesser is not implemented for that.
01:18:14.670 - 01:19:40.830, Speaker A: The following implementation was found, but aren't these two the same? Yeah, but I don't want the trait. I specifically want the function pointer because that is what I pass it here. That shouldn't matter. Yeah, I want a raw pointer. It's intentional. But I think like doesn't this match that? Am I blind? Oh, does the trait need to be in scope? No. Well, that's real unfortunate.
01:19:40.830 - 01:21:25.714, Speaker A: Yeah, there's like some lifetime here not matching up, but I'm not quite sure why. I mean, I could have this take, you know, mute g instead, and then I could have this be mute of this. It still won't play ball, huh? Alright, that's fine. We'll go back to what we had and then I'll just have a helper here that's called, let's say guesser, which takes a history as an ident and then it takes a block, I guess impl, which is a block and it returns struct g. It implements g. It implements guesser for GhDev. It can't be the FN trait because then I would need a blanket implementation here for all like generic over t.
01:21:25.714 - 01:22:08.814, Speaker A: And then you can't have another implementation of guesser. It would be considered potentially overlapping. The other alternative here is to make play take a closure, always like take an f and mute instead of taking a trait. But I don't really want to do that. I suppose like the guesser is always going to be kind of like this, but I like the ability for the guesser to contain state that it might mutate over time. I think this is going to be basically essential for things like, you know, caching things we've computed in the past. Implement guesser for G.
01:22:08.814 - 01:23:51.388, Speaker A: Guess is going to be this, this is going to be history and this is going to be info. And now I should be able to do, I guess let's do something like ident is equal to that. And then what I'll do here is return an instance of g here and now I should be able to do let Guesser is guesser of history. And then I think now this should be fine. This needs guesser. And I guess I can make this config test to get rid of the warning. And main is complaining about this because it now needs to do rojit wordle new and then this is going to be w play crashed.
01:23:51.388 - 01:24:26.160, Speaker A: Huh? Every line is word plus space plus frequency. Is a lie. It claims. Really? Oh, did we miss one? No. Dictionary. I don't understand. Which word am I missing? Add view.
01:24:26.160 - 01:25:04.086, Speaker A: Wow. That search was just entirely wrong. Why did I do a to e, a to z? I have no idea why. That's how I wrote that search. That is just wrong. Great. Fantastic.
01:25:04.086 - 01:25:56.844, Speaker A: Okay, so we can play a game now and we can see that if we immediately guess the right word, then we're good. And then we could do something here like, you know, we can make this fancier, say if history dot, what's a better word than moved? Right? Because we're right. And if history Dot len is equal to one. Is equal to one, then we return right. Otherwise we return wrong. And this should now take two guesses. I forget what the it's like.
01:25:56.844 - 01:26:17.650, Speaker A: They're called like impressive. And that's a good question. Let's. Let's figure out what the. If I go here. Okay, so it's genius. So this is, this is genius.
01:26:17.650 - 01:26:54.056, Speaker A: This is impressive. No, this is magnificent. This is impressive. This is splendid. This is great. And this is few. I mean, these are all the same, right? This is just going to be genius.
01:26:54.056 - 01:27:05.590, Speaker A: Is guess it immediately. Magnificent. Is guess it after one. This is guess it after two. This is guess it after three. Guess it after four. Guess it after five.
01:27:05.590 - 01:28:09.260, Speaker A: Like after five. Previous guesses. Ooh. Really? Is right or wrong not in the dictionary. No, they are so. Alright, so something's wrong. Guess wrong is not in the dictionary.
01:28:09.260 - 01:29:00.810, Speaker A: Why? Oh, yeah, you're right. I need to update the some two, some three, some four, some five, some six. How is wrong? All right, give me this hash set because something ain't right. It has all the numbers. That's because this needs to be dot zero and I'm an idiot. Great. There we go.
01:29:00.810 - 01:29:37.640, Speaker A: All right. Fantastic. Okay, so we now have some confidence that our guesser does the right thing. Oops. We want to check that if we have something that only ever guesses wrong, then it eventually terminates. Great. So now we have that checked in as well.
01:29:37.640 - 01:30:34.568, Speaker A: Fantastic. So I think here what we're going to do is go back to naive and go ahead and why does it claim that naive isn't there? Does it still claim that naive is not there? So I'm going to run this with the release and it's going to panic and not yet implemented. Okay, great. So we're going to get add dot and we're going to commit this as basic structure in place. All right. So now we need to go through this, this naive implementation. The intuition here is that we're going to keep a list of possible, a list of words that match all the information we have so far remaining.
01:30:34.568 - 01:31:58.942, Speaker A: We're going to call it and it is going to be holding basically the remaining word list. And how we're going to hold that, I guess we can, we'll not think too carefully about it at the moment. So we're just going to say it's going to be a hash map from static strings to their frequency counts. And this is also going to bring in the dictionary and back in Lib. We're going to have remaining here be this word count is going to be that, count is going to be count parse. And we're going to expect that every count is a number and then we're going to return word and count. This is a parsing that really could happen just once.
01:31:58.942 - 01:32:50.870, Speaker A: So we might want to bring in, you know, like one cell or something to cache this. But again, this is named naive and it's named naive for a reason. So let's keep it like this for now. And when we're given our history, what we're going to do is we're going to, for each of the remaining words, each of the candidate words, if you will, we're going to compute how good of a guess this is. And this is where we get back to the three blue one brown video which talks about exactly how we do that. But basically we're going to loop over every word and count in self dot remaining. And we can do this because we know that both of these are copy.
01:32:50.870 - 01:33:46.070, Speaker A: And for each word here we could also track the how good every word is. But I. We only really care about the best one. It might be nice to keep a list of the top ones, like of the top ten or something might help with debugging. But for now we're just going to keep the best one. And if, let's sum. What are we gonna.
01:33:46.070 - 01:35:25.660, Speaker A: We could have this be a tuple, but I like having name fields. So we're gonna call this, we're going to derive, debug, copy and clone. We're going to call this candidate. It's going to be a word and it's going to be account if let sum c is best, else if there is no current best, then the best one is obviously the current candidate. But in here is this one better is going to be the big question. And in order to have, you know, a notion of which is better, we need like a score or a goodness, right? And we're going to say that that is going to be a floating point number for now. So here we're going to say if c goodness is less than, or rather if our goodness is greater than c goodness, then best is going to become a candidate word count goodness.
01:35:25.660 - 01:36:14.244, Speaker A: But we need goodness. We need to compute the goodness in here somehow and we don't really know how yet. And then we're going to here assume that we found some best word. So we're going to do best unwrap word tostring. This is going to be an option candidate. Great. Obviously we want this to consider the history.
01:36:14.244 - 01:37:05.600, Speaker A: Like there's all sorts of things that we haven't done yet. Specifically, if the history dot length, if let sum last is history dot last, there's a to do here which is update self dot remaining based on history. Right. So the idea here is that if we previously made a guess, then we just now learned which ones we got correct. Basically the correctness mask for a previous guess as we need to go through an update remaining based on that new information which we don't like, that might whittle down which words are actually possible guesses. We also need to figure out how we compute the goodness here. Right.
01:37:05.600 - 01:38:16.890, Speaker A: This is clearly also a to do. How do we compute this? Which one do we want to do first? Do we want to compute the goodness first or do we want to update remaining first? Let's do updating remaining first. So the idea here is that we have some mask in last and we want to only retain things, sorry, in in remaining that match the updated information we got. So we're going to do self dot remaining dot retain. And each of these, right, is a word and a count. And I don't think we need the count here. The count is irrelevant for whether we retain something and we only want to retain a word if last dot mask dot or if last dot matches word.
01:38:16.890 - 01:39:02.262, Speaker A: Alright, so this last odd matches, of course we don't have yet. The idea here is to figure out whether a given, a given piece of information matches a word. So if we're told that, you know, the first letter is a green a, then it wouldn't match. Such a match function would not match a word that starts with b. We don't actually need to scan all of the past guesses because we've already pruned ourselves down to things that match those past guesses. So that's an easy optimization. I want to generate a matches method that looks awful.
01:39:02.262 - 01:40:05.760, Speaker A: I guess that's where it decided to place it. This one is going to be common to all the guessers, so we might as well just make it general, which is a given. This is just a boolean thing where either a given guess, this is not really guess, right? This is outcome of a guess. So maybe this should be something like result response. I don't know what a good name for this is. I kind of like guess, right? Like this is a guess and the information associated with the result of that guess. Attempt, maybe.
01:40:05.760 - 01:40:38.834, Speaker A: But is attempt better than guess? I'm just going to keep it guess for now. That's fine. All right, so whether a word matches a guess, let's see. So first check greens. So for. You're gonna hate this too. For I'm not gonna write this yet.
01:40:38.834 - 01:41:30.850, Speaker A: In self, dot, word cars, dot, zip. And here too we should assert eek, self, dot, word Dot len is five and word len is five. Dot zip, self, dot, mask, dot. It, er probably don't need that. Dot zip, word dot, cars. And so this is going to be the outer zip is going to give you the thing from word. The inner thing is going to give us the previous guess and the mask.
01:41:30.850 - 01:42:35.740, Speaker A: And then we're going to match on the mask. And if this is correctness, and the one is fine to do this with, if correctness is correct, then if g is equal to. If g is not equal to w, then we can return false. Like here we know that, basically we know that a given letter in the previous guess was green, and that letter doesn't match the letter from the current word. Therefore, this word that we were given cannot possibly be the right thing. Can we share this logic with the checker? We probably could, but I don't know that there's a benefit to doing so because it's a different algorithm. It's doing something different.
01:42:35.740 - 01:43:13.800, Speaker A: We could make this play in hard mode if we wanted to, but I think that's a modification we can do later. If they're the same, then we don't know yet. We need to consider the other possibilities. If it's wrong and g is equal, then we also know that it can't match. Right? You guess. You guess the letter that the previous guest told you was wrong. So this is an unlikely case, but it can happen.
01:43:13.800 - 01:44:19.470, Speaker A: And if it's misplaced, then we don't know what to do yet. So first we're going to check the greens and then down here comes the next question. Yeah, I think we're going to have. The logic is like similar to the computation for correctness, but I don't immediately see how we reuse it. Right. I think we probably are going to need a sort of used thing here too, and I guess enumerate just to really hammer it home. So then used I is true.
01:44:19.470 - 01:45:17.980, Speaker A: So we've, we've used that green and then we're gonna walk. Then we're gonna go walk. All of the might be. We could do this in one loop. We're going to walk all of the characters of the. Just the word. It might be that we can just use this, actually.
01:45:17.980 - 01:46:23.554, Speaker A: Yeah, I think the second loop might be all we need if. I guess so. Maybe we have all the information here. Maybe what we want here is actually, if. If g is equal to. If the previous guess is equal to the word, then if the match is correctness, correct, then return no, then return false. So if we're matching the previous letter and the.
01:46:23.554 - 01:47:19.690, Speaker A: And that was not correct. No, this is wrong. I am trying to wrapping my head around this. So I think we can do this in one pass because I think all you have to do is search for an unused letter that is yellow. Right. So what we do is we're going to search for the. I'm so sure that there's a simple loop for this and I'm just trying to arrive at it.
01:47:19.690 - 01:48:52.430, Speaker A: I think if m is equal to correctness, correct, then we have a fairly simple branch. If that index is correct and if g is not equal to wooden, then we know that it's wrong. Otherwise we need to see if there are any unused yellows for this character. So if self, dot, word, dot, chars, map, self dot, mask. If there are any unused indices, find map. Actually, position is probably fine, so we don't need the enumerate. Bear with me here.
01:48:52.430 - 01:49:57.926, Speaker A: If we find any other characters, I guess we can just look for the first unused one. In fact, if they're equal, then we do nothing. No, that. No, we still need to. If it used to be green and now we guessed a different letter. Yeah, I think this is, this case is just done, right? The previous guess was correct in that location, then if the characters are different, then we know that there's nothing to do otherwise. Use of I is true.
01:49:57.926 - 01:51:11.758, Speaker A: True. And we continue otherwise. I guess if m is correctness misplaced and the character, I don't think we need that as a special case. What I want here is to find the first occurrence of that character, if any, in the previous guess that is unused. So if unused of. So we do need this map if not unused, which is terrible. I'm going to explain what's going on here once I wrap my head around it myself.
01:51:11.758 - 01:51:55.990, Speaker A: So if now it's called used. That's why I'm confused. If it's used, can't use this to support this character. If G. I need to give these better names than G and W. Okay, so we're looking for the first unused occurrence of the current character in the previous guess. That's what this is searching for.
01:51:55.990 - 01:52:56.090, Speaker A: So this checks that it's the same character if it's used. This checks if it's used. So otherwise, this is. To be concrete about it, we're looking at an x in word and have found an x in the previous guess. The color of that previous x will tell us whether this x might be. Okay, so here we're going to match on what that previous guess was. No, that's not what I want.
01:52:56.090 - 01:53:57.380, Speaker A: That should say zip. If that previous guess was correct, then it should already be marked as used, or we should have returned. So this should be unreachable. Although this implies that maybe we do need to do the loop twice. Once to mark all the used. Oops. And then here.
01:53:57.380 - 01:54:43.536, Speaker A: Yeah, so this should be unreachable. All correct guesses should have resulted in return or be used. Right? Because if a correct. If a correct guess, you guessed the same, then it's used. If a correct guess, you guessed something different, then it's wrong and we've returned. If it is misplaced, then use of j is true and we can return sum of JD. We might not even care about the index here.
01:54:43.536 - 01:55:09.690, Speaker A: Actually, this might be the same case as we did earlier. So maybe this is just if any. So this is going to be false. False. It's going to be true. And if the. If the other occurrence of this is wrong, then we know that.
01:55:09.690 - 01:56:23.820, Speaker A: So this is like you guessed an x in some position, and in the previous guess you were told that there is no x. Well, then your whole guess must be wrong. So this is sort of a, this is a return false kind of situation, which really is a return false from the whole thing, which we can't do because we're in a closure. We can make this just be a for loop, I suppose, like a nested for loop. Right? Because this is really an early return. There shouldn't be any downside to early returning here, though, to not early returning like this being return false should be fine, because even if you keep going, you're not then going to find a better candidate. Right? Okay, so if we do find a letter we can use, then word might still match.
01:56:23.820 - 01:57:33.740, Speaker A: Otherwise, I guess this can be plausible is plausible is initially true. If we get here, then plausible is false. So if we found one of these, then the word might still match, else if not plausible. So in this case, we should assert plausible. It shouldn't be possible to get here and also have taken this path because the return true here should have caused this to return early. If it's not plausible, then we know that it can't match. This is we hit this case somewhere previously.
01:57:33.740 - 01:58:35.454, Speaker A: Otherwise we have no information about character w. So word might still match. So this, I suppose is the character w was yellow in a previous. In the previous guess. And there's an additional part here, which is we shouldn't allow misplaced. If j is equal to I right, then you know that it must be wrong, which we can just deal with up here, but we might as well just do it down here too. W.
01:58:35.454 - 01:59:53.030, Speaker A: I guess this should just be w because we have a concrete one for this. W was yellow in the same position last time around, so it must still be. I guess that does that make the word plausible? If in the previous guess we got a yellow here, that means that this cannot be the answer. So w was yellow in the same position was yellow in the same position last time around, which means that word cannot be the answer. That match cannot be the answer. So plausible is false, and we return false. And I think this means that we might hit the assertion failure because it might then look for a misplaced later position.
01:59:53.030 - 02:01:12.370, Speaker A: So I think we actually want if this and plausible plausible. I guess we could call it still plausible if we wanted to, but I think that should do it. And now I don't think we need whatever this last loop was, and we don't need whatever this bit down here was. So if we walk through all of these and it's still plausible, then true. All right, this also clearly is something that we need to test because I have no confidence that this is actually true. So this is going to be guess matcher. Use crate guess fn matches.
02:01:12.370 - 02:02:08.128, Speaker A: All right, let's see. So how do we make one of these? I guess we construct a guess which is going to be, you know, word ABCDe and mask, which is going to be here. We can use the, the masker we made. So let's move this to up here and I'll make that config test two. And the mask here is going to be mask of. Let's say that it was. Let's say all of them were correct.
02:02:08.128 - 02:03:00.510, Speaker A: This is the, certainly should be a trivial check, right? So ABCDe, that should certainly be the case. Is the yellow in the same place assumption always true? What about guessing against Abab? A first with yes. Remember, this is only one iteration. This is telling you based just on your previous guess. Is this new word feasible? Because in general, that's all we need. Right. Because we have already pruned out candidates that didn't match the earlier guesses.
02:03:00.510 - 02:03:42.872, Speaker A: We've already pruned those out in a previous pass of the solver. So we only need to solve this incrementally for one step at a time. So let's see that this, in fact 32. That's fine. All right, so at least our, our very simple check here worked. I kind of want a nicer way to write these. So let's write another macro.
02:03:42.872 - 02:05:10.990, Speaker A: I love macros for, for testing. So guess is going to take a word which is going to be, I guess, a literal. And it's going to take a mask which is going to be a token string. Plus this is the mask that we used from before is equal to matches prev. And I guess next it's going to turn into this mask of mask plus matches. Next. Is it not lit? Does it actually become literal? Nice.
02:05:10.990 - 02:05:56.586, Speaker A: Okay, so I should be able to now write check. And I'm going to actually include the square bracket here. Check that ABCDe plus cCCCC matches ABCd. And maybe matches is wrong. Maybe it should be allows. So see that, that still works. Yeah.
02:05:56.586 - 02:06:28.220, Speaker A: So see how much nicer that check is. So it should also be the. And then we can, haha. Then we can do even better. We could do disallows and say that that should be assert not this. We could even share them and have it be. It's gonna be annoying.
02:06:28.220 - 02:07:31.654, Speaker A: Bang is gonna be a TT question mark. You're gonna hate me for this. Bang question mark. And now this is going to turn into check one. One prev mask allows next. And yeah, that's what I was worried about. I don't really have a way to make that nice.
02:07:31.654 - 02:08:51.788, Speaker A: I think we'll just leave it the way it was. Little bit less macro magic is good sometimes, because I can say now that ABCDF and ABCDE, that pattern should preclude this guess from being considered correct. And it does. For example, if I get yellow for the same character four times, we then know where it's located. That's true. Do we need to prune given all of the logic? Yeah, you might be right here. You might be right that doing it incrementally is insufficient.
02:08:51.788 - 02:09:47.630, Speaker A: We actually need to check whether it matches the whole history, which is going to be a little bit sad. Yeah, that's a good point. The point from chat here is if your history is like, you know, a in the first position is the only things you know, is that a in the first position is yellow. A in the second position is yellow from second guess. A in the third position is yellow from the third guess. A is in the fourth position is yellow from the fourth guess. Then you know that a is in the fifth position.
02:09:47.630 - 02:10:26.100, Speaker A: But you can't determine that by scanning incrementally. Is that true? No, I don't actually, I don't think you're right. Because any word that has a in the first position will be eliminated by the first scan of guess. Right? Because if a word has a in the first position, then it cannot match a pattern that has a is yellow in the first position. So it gets eliminated in the first round. And therefore, by the time you get to the fifth round, you've eliminated all the words that have a in the first position. All the ones that have a in the second, the third and the fourth.
02:10:26.100 - 02:11:38.500, Speaker A: And therefore the only ones that are left are the ones that have a in the fifth position. So yes, I think, I think doing it step by step is actually sufficient. All right, give me some more tests here. It should be the case that if I guess completely different letters ABCDe and I get all wrong, then it should allow a were a guess that has only different letters. And then we can shift this by. We can do the good old shift all by one. Should allow e abcdeze, right? Oh, e ABcd.
02:11:38.500 - 02:12:24.018, Speaker A: All right, we got another one. Oh, I need to, I guess I should name these, huh? From chat. I'm gonna keep them all as one here and then I'm just gonna do them as comments. So aaabb plus c mw w w should disallow a CCAA. Okay, so is that true if a is correct in the first position? Yeah. A is incorrect in the second position then. And a is wrong in the third position.
02:12:24.018 - 02:12:59.880, Speaker A: That means there can't be three a's and therefore this should be disallowed. And there are no b's. But there are no b's there either. Great, we got that one. We are going to do the same. Information theory algorithm implementation is three blue, one brand, and in fact, we're getting there, so that's certainly a thing to check. I'm gonna go with John from John.
02:12:59.880 - 02:14:28.764, Speaker A: Currently I'm beating chat baa, where the b is wrong, the a is correct. Then this should allow AACC, but it should disallow this. Right? So it should disallow this, because this pattern told us that a is yellow, but we guessed it again, so it can't be right. Ooh, on 240, is that. Oh, the first one I wrote. Am I wrong or is the algorithm wrong? If there is no b, there's a c here, so that one's correct and that one is yellow, then I think it should allow this. Right? Right.
02:14:28.764 - 02:15:32.420, Speaker A: If I have baaaa and I'm guessing AA CCC, and I know this one is correct and that one matches, I know this one is yellow, and so I moved it there and I know that there are no more a's and that's fine. And I know that there's no b here, and that's fine. So here, there's a bug in the algorithm somewhere, because that should be allowed. So let's figure out why this one breaks a. Let's stick it in its own debug test over here. Oh, I got another for chat retoban. A, b, c, D, e, plus wooden ww disallows BcDea.
02:15:32.420 - 02:16:15.888, Speaker A: Why should that be disallowed? It's saying all of them are yellow and I have all the same letters, but in different arrangement. So why should that be disallowed? I think that should be allowed. I'm gonna comment that out for the time being. All right, let's see if we can figure out this, this debug thing and why it's confused. Oh, they're all gr. Oh, you're right. They're all gray, right? Yes, they're all wrong.
02:16:15.888 - 02:16:55.480, Speaker A: I agree. Yeah, that passes. So why is this one broken? Let's see if we can figure that out. So I want to check all of the early returns and check out which one we end up taking. We end up taking the one at 168. Okay, so we take this one down here. That's good, because this up here seems pretty straightforward.
02:16:55.480 - 02:17:58.416, Speaker A: So that means we end up with not plausible. So do we take this not plausible, or do we take this. Not plausible. We take this one, and in fact, we take that one twice wrong. So let's get some more information here about this one. I want, I want debug of IWJG and M. So it's looking at I one.
02:17:58.416 - 02:18:40.679, Speaker A: So it's looking at the second a and. Yeah, the second a. And it's looking up the third character of the previous guess, which is also an a. And it's looking for a matching a. Oh, I see what this is. This is because we are re evaluating things that have already been determined to be correct here. So basically, these ones we should be skipping over when we walk here.
02:18:40.679 - 02:19:23.580, Speaker A: So if, which we can do just by doing this, because we know that must be correct. Or we'd have returned above in the previous loop, the earlier loop. Right? We would. If, if we're in the correct case, then we should have returned false here. And the reason why it fails, if we don't have this condition is because we set those to used so we don't get to use them again down here. Here. And so this loop fails to find a matching letter because it doesn't consider any that are correct.
02:19:23.580 - 02:20:02.609, Speaker A: All right. And if I'm not using g here, then why am I even including it? Maybe I should just do word cars. Zip mask. Nice. Let's see how that goes. That does it. And if I do all of them.
02:20:02.609 - 02:20:43.942, Speaker A: Okay, back in business. And I can get rid of this and this, and we can go back to the guesses. Coyote, Candida's derived impulse for clone and debug. But these are intentionally ignored. Doing dead code analysis. Oh, that's fine. All right, give me some more optimization.
02:20:43.942 - 02:22:25.830, Speaker A: ABCDE, where the first three are correct and the fourth one is misplaced, should disallow ABC FG because the last character must be a d. That's true. There are some optimizations here we could do, but that seems, that seems successful. Still wondering why you can't just copy the game logic, taking the candidate word to be the answer and comparing the two correctnesses element by element. So the proposal is that we could do this by saying, see, if I'm, if I'm getting you right here, that should mean that you want us to do correctness compute of the candidate word to be the answer. And I forget the argument order here and the previous one to be the guess. And that we should assert equal that with mask of this.
02:22:25.830 - 02:23:48.766, Speaker A: And I assume the opposite here. Yeah, I mean, that seems like it does the same thing. Why does this work? It's a cool insight. Let's see, if we take the new guess as the, or the new word as the answer, and we are computing what correctness you would get for the previous one. Oh, why are we guaranteed they're the same? I'm trying to figure out why this works. Right. So, so for example, this would be, you're trying to find the answer.
02:23:48.766 - 02:24:36.940, Speaker A: So if your guess was the answer, it would have given the same correctness answer. Oh, I see. So the assumption. I see. So what we're exploiting here is that if the thing we're checking were in fact the answer, then the previous guest should have given that same pattern of correctnesses. I still can't wrap my head around why this will always produce the same correctness. I guess the algorithm is deterministic.
02:24:36.940 - 02:25:51.270, Speaker A: Hmm. I also don't know. I think the compute is actually more accessible. Expensive, maybe? Not actually. All right, I'm going to keep this the way it is and processes in the background rather than bottlenecking us on this particular thing, because it seems like the two are equal here. I mean, we could even do property testing, right, to check that they actually produce the same answer. For now, I'm going to stick with the one we have, and then we can always remove that later on.
02:25:51.270 - 02:27:29.962, Speaker A: It's not that I don't believe you that they're equivalent. It's that I can't on the fly realize why they are, which means I can't explain it. All right, so back to our naive algorithm. So we now have a way to prune down these, and now we get to how do we compute the goodness? And this is where the three blue one brown video talks a lot about information theory. And basically the way that you compute the goodness here is you compute the expected amount of information that the expected amount of information from this guess, which means like the combination of how likely is this to be the answer given the information we know, how far and the word distributions that we have, and how much information would we gain if it isn't the answer? Right. So the idea here being that we want to balance these two, where we both want to try to guess the thing that's most likely to be the answer, but we're okay picking something that is slightly less likely to be the answer if it gives us a lot more information if we get it wrong. And the way that this is computed is using this formula, that is the sum for each I of the probability of I multiplied by the log base two of PI.
02:27:29.962 - 02:28:33.120, Speaker A: In the video, it's explained as one divided by we're going to do this instead. They're equivalent as far as logarithms go. And the reason why this is minus is because p of I is already a probability, so it's already less than one. So the logarithm you get back is going to be negative, and so we negate it to get a positive one, which is the right way to compute this value. So the question then becomes, what is PI? Right? So what is the probability of this word being, you know, the, the correct answer. And the probability of this word being the right answer is really just how the, the relative likelihood of this word combined with the other possible words. There's a little bit more where we have to consider the possible patterns, but let's just figure out what the probability of the word is first and then we'll go from there.
02:28:33.120 - 02:29:25.260, Speaker A: So we're going to need something to compute the probability of a word, which means that we're going to need the total count of the remaining possibilities. In fact, I think, I think I want to do this a little. No, let's do it silly first. So we want total count of the remaining words. Iter dot map, because we want to grab out the counts. I guess this can be c dot sum. And I'll let this be a usize.
02:29:25.260 - 02:30:59.932, Speaker A: And here we want, you know, the p of word is the count of this word divided by the total count. But we're going to want to do this computation as floating point because otherwise we're going to lose all the precision. And in theory, the value here is the p of word multiplied by the p of word log, two minus and then this whole thing dot nic of course, you might wonder, where did the sum go, right? Oh, is it not neck? Really? Guess we could do abs, but I don't really want to do abs because we do that. Of course, we're missing the sum here. The sum here is over all possible patterns. So this won't actually do quite the right thing. Thing.
02:30:59.932 - 02:31:44.150, Speaker A: Because if you think about it, imagine we have a current set of candidates. The expected amount of information for any given word that we might guess next is the probability of that word being. The answer is like it differs depending on the. Let me try to find. This is where I want to avoid repeating and botching the stuff from the three blue, one brown video. But it is important enough to repeat that I have to find the right way to articulate it. Alright, here's the basic intuition.
02:31:44.150 - 02:32:52.660, Speaker A: Arguably I should draw, but I'm gonna not do that. Okay, we have a bunch of words. Let's call them word one, word two word n. Right. And we want to figure out the goodness of word one. The goodness of word one is the sum of the goodness for word I. The goodness is the sum of the goodness of each possible pattern we might see in the, as a result of guessing it molten multiplied by the likelihood of that pattern occurring.
02:32:52.660 - 02:33:44.440, Speaker A: You're going to see how this ends up tying to this formula in a second. And the intuition here is that the probability of a word is the sum of the probabilities for all of the patterns that we might see. I don't know if that's actually true. We'll get to that in a second. I think we need to start coding this, and then I think it's going to become clear. So how many possible patterns are there? Well, there are three to the fifth possible patterns, right? There's correct, wrong and misplaced, I guess we called it, for each position. So here's what we want to do for each possible pattern.
02:33:44.440 - 02:35:09.348, Speaker A: How do I want to express each possible pattern? It's really like the, it's not the power set, but close enough to it. And I mean, we could statically compute this list, right? It's just because it's, the set of candidate patterns is always the same. So in fact, maybe let's do this. Let's have this be a helper on correctness, which is going to be something like pubfn all or permutations maybe. And it's gonna return something that implements an iterator where the items are gonna be self five. Yeah, it's not quite three to the five. You're right.
02:35:09.348 - 02:35:47.530, Speaker A: There are some patterns that are just impossible. But we might as, so it doesn't really matter whether we can, it might be an optimization, but we would just find no candidates that match that. Right. So it's not really a problem to include them in the list of permutations that we check. It's an optimization, it's not a correctness problem because you would find no candidate words to place in that bucket. So the probability of that bucket would be zero. I wonder actually if there is a helper here we could use from iter tools.
02:35:47.530 - 02:36:50.990, Speaker A: I feel like there almost certainly is, just because I don't want to reinvent the wheel here. Header tools, permutations. I mean, really what I want is Powerset, but I don't quite want the power set because I want the power set, but only of a certain length, which is a little nicer. Ah, there's a macro for it, the iproduct macro. Iproduct oh, perfect. Cartesian product. Yes, that is indeed what I want.
02:36:50.990 - 02:38:01.620, Speaker A: Excellent. So let's do cargo toml iter tools is 00:10 and what I want here is iter tools iproduct. And I guess this is going to be, it's a little awkward, but it's fine. Self correct, self misplaced, self wrong. 12345. Unresolved macro. Really no telling.
02:38:01.620 - 02:39:04.486, Speaker A: And currently that gives a tuple, which I think I can map using from. Okay, this seems like an implementation that should exist. Come on. Standard library, right? Like a tuple where all the elements are of the same type of a given length should implement into an array of the same length of that type. That makes me sad. All right, fine. IBCD two abcde.
02:39:04.486 - 02:40:54.830, Speaker A: That's fine. All right, so what we're going to do here is loop over all possible patterns for pattern in correct, I guess let's call this patterns not correct correctness pattern. Yeah, this is where we're going to sort of be able to very quickly see how naive the naive one is because here what we're going to have to do is compute a version of remaining that only has the words that match this pattern on this word. Right. So to show this in code, it's going to be self dot remaining dot iter dot filter candidate. And we're going to filter on pattern. No, we're going to filter on guess of words.
02:40:54.830 - 02:41:35.740, Speaker A: It's candidate tostring. We could optimize that a little bit. Mask is pattern matches. No word matches. Candidate cloned in pattern. Total is zero. I'll explain this in a second.
02:41:35.740 - 02:43:23.700, Speaker A: Remaining is in pattern left in pattern cloned collect map. I guess we could, we could clone first. We do let g is guess if those matches in g word. In fact, we could do filter map, then g word word tostring. If gmatches, we can optimize this a decent amount for things like allocations, but I'm going to not do that right here. And I also want the in pattern total to be plus equals the count. Let's bring that in.
02:43:23.700 - 02:45:25.366, Speaker A: GmaT is candidate left in pattern. So this is computing for every pattern, find all of the, find the subset of the remaining words that also would exist if, if we guess this word and that pattern matched. Right? So in this world, like in this, inside of each item of this loop, we're considering a world where we did guess word, right? So word is the outer loop and got pattern as the match, as the, I guess as the correctness. I'll put that up top here. Now compute what then is left? Alright, so that's what's left here. And then what we want to compute is the, let's see here. So the probability of us actually getting this pattern is the total count of the words that are left if we get this pattern divided on the total divided by the total of words outside of that pattern.
02:45:25.366 - 02:46:44.956, Speaker A: Right. So the, the pro, the p of this of us getting this pattern is the in pattern total. We might not actually need the left in pattern here, but we might be able to use it divided by the. I guess this should be like remaining count divided by the remaining count. Yeah, I think this can just be for r in remaining for candidate and count in remaining. I don't think we actually need the, I don't think we actually need the words we would pull out. So the probability of this pattern is that, and that's the thing that we want to sum across.
02:46:44.956 - 02:47:31.724, Speaker A: Right. So we could turn this whole thing into like a, just a functional iterator in practice. What we're going to do is sum is zero and we're going to do sum plus equals and then p of this pattern is the actual. Yeah. Is each PI. So we're going to do sum of p of this pattern times p of this pattern, dot log two. And then at the end we're going to say sum is sum or 0.0
02:47:31.724 - 02:48:45.950, Speaker A: minus the sum and that's going to be the goodness. Right? So what we're computing here is, given a particular candidate word, if we guess this word, what are the probabilities of getting each pattern? And we sum together all those probabilities and then we use that to compute the entropy, the information, the information amount in guessing this word. So at least in theory that should do the right thing. And I guess we don't actually use the count of the candidates so we can get rid of that. And we don't use this either, which is interesting. I think that's right. And now the question becomes if we go back to main, let's just say this is correct for now.
02:48:45.950 - 02:49:40.000, Speaker A: Match. I guess if let score, I guess some score is play, then we're going to print line eprintline failed to guess. And let's just see what happens if we run this. Well, it takes a long time to run. My guess is that this is because this computation is very slow. It's also single threaded. Right.
02:49:40.000 - 02:50:17.570, Speaker A: If we go over here and look at htop like it's, it's spinning over here, it's doing its thing, it's using my core. Where is the actual. Why is this sorted the wrong way? That's very unfortunate. Six, back up to the top. Come on. There we go. It's just spin my cpu.
02:50:17.570 - 02:51:00.722, Speaker A: Of course, it's because it has to consider so many possibilities here. I think we might be able to do just minus some. Yeah, great. So the question now becomes, okay, this is clearly too slow, right? In fact, let's try to do like, let's just to monitor that it's making progress. Do this. Okay, so it is making progress, right? It is. It is running that loop, but it just is really, really slow to the point where naive can't even solve one game.
02:51:00.722 - 02:52:08.990, Speaker A: Like, it can't even finish one game because there's so many possibilities to consider here. In fact, let's try to print out here is better than just to see that it's like roughly doing the right thing. So we're claiming that word is better than C word, because goodness is better than C. Dot goodness. And I guess we could say here, starting with this, goodness is this nan. Okay, so clearly something is real bad here. It's not going to make any progress if all of the values here are nan.
02:52:08.990 - 02:53:18.206, Speaker A: Let's do debug of this and debug of this. All right, those seems like, those seem like plausible numbers. Debug of this and debug of this. Those are still also plausible numbers and debug of the whole thing. Those are still plausible numbers and debug of some is Nandez. That seems real unfortunate. Oh.
02:53:18.206 - 02:54:05.620, Speaker A: Because it sometimes gets back nan from this computation. See if we can spot ah, log two is infinity. Sometimes the probability is zero. That makes sense. Yeah. So we're gonna say if in pattern total is equal to zero, then we can continue, right? Because then if there are no. If there are none in here, the probability of this is zero.
02:54:05.620 - 02:54:31.450, Speaker A: So the information is zero. It's not even worth considering. And we don't compute the log two of zero. There we go. Okay, so now it's doing something reasonable. So sum plus equals this. See what it says.
02:54:31.450 - 02:55:17.836, Speaker A: Alright, so it seems to be doing like something interesting here, right? It's concluding that some words are better than other. We don't really like. We could look into the data set for why. But like, it's interesting at least that it is computing good initial candidates. Cui is better than boons, apparently. And these all seem like, you know, plausible looking words. One thing that I do did notice in the, in the three blue one brown video is that grant hard codes the first guess, because it so significantly reduces the, the search space.
02:55:17.836 - 02:56:04.150, Speaker A: So how about we do the same thing and we say if history dot is empty, then we're going to return. And I think hard codes it deterstained. Coach is better than couch, cooch is better than coach. But there are still a lot of possibilities here. Huh. Okay, so it finished its first game and got five. I guess let's have it actually print score.
02:56:04.150 - 02:57:36.630, Speaker A: And let's not have it do this and not have it do this. There is one other optimization that I think was implemented here, which is I believe that grant also implemented a sigmoid over this probability. So rather than saying which is infinitely more probable, several orders of magnitude more probably than couch, something that has as much smaller count than which, we flatten the curve using something like a sigmoid, where it becomes more of a cutoff than it becomes like an actual exponential difference. So the sigmoid, I think you want to apply. That's a good question. We could apply the sigmoid actually to the, to the counts themselves rather than redoing it on every probability. But even so, let's just see what happens if I run this for a while.
02:57:36.630 - 02:58:30.250, Speaker A: I guess let's also print out what the answer is for each one. So we'll do guest this in starting with cigar goodness minus zero. It's a pretty small goodness guess. Cigar in five. Well, five is less than six. Like it, it did successfully guess a word. As far as wordless concerned.
02:58:30.250 - 03:00:00.190, Speaker A: This is where it's nice to, like have visualization tools and not just have, you know, this debug output. So we actually could actually see, you know, how it makes its decisions for a given game and where things like top ks would be good. Right? Like, instead of just keeping track of the best one, keep track of the top ten and then display them before you guess so that we can see that it's actually making somewhat reasonable choices. Guessed rebutt in six, sissy in five, humph in four. Okay, so it is like playing the game and doing something reasonable, right? It's really slow, but that's sort of the point. All right, let's do go ahead and git commitment, first iteration of naive. And again, like I guess we should put in here, you know, to do apply sigmoid.
03:00:00.190 - 03:00:48.490, Speaker A: All right, but let's now try to figure out where it's spending its time. Now, there are a lot of ways to do this. I kind of like Roger. Why did I close one of these? I really like using the like, fundamental tools. It makes me happy. So what I'm going to do is I'm going to run PGRP. No, I'm going to rerun perfect record call graph p 64542.
03:00:48.490 - 03:01:24.490, Speaker A: So I'm just recording some stats about this. And now I have a perf dot data file, and it's 200 megs large. Great. And then I want to infernal collapse. These are the tools that cargo flame graph uses under the hood perf stacks. I guess I can just do inferno flame graph. And I want this to be into perf dot svG.
03:01:24.490 - 03:01:49.150, Speaker A: Whoa, weird event line. Oh, right, I lied. Perf script. There we go. It's perf script. I don't need I. And then I pipe that into inferno collapse perf.
03:01:49.150 - 03:02:24.850, Speaker A: And I pipe that into inferno flame graph. And that gives me perf svG. All right, let's see how that looks. So we're gonna go to home. John, Dev streams Rojit and I want perf svg. All right, so flame graph is. It displays.
03:02:24.850 - 03:03:00.258, Speaker A: It's going to be annoying that my face is in the way. I apologize for that. But flamegraph is a tool that basically displays where your program is spending its time. And the visualization here is that the width is the total number of samples. And so if something is half the width of the screen, that means half of the times your program was sampled, it was spending its time in this location. So we can say all of our time is spent in, you know, main, which is unsurprising. But you can then zoom in to see what does Main then call and where's that time spent.
03:03:00.258 - 03:03:36.266, Speaker A: So if we go all the way up to Roger Main, we can go back up to main guess. And we see that in guess, a lot of the time is spent in Malloc and free, and then some time is spent in guests matches. So this makes a lot of sense, right? Our time is spent allocating and deallocating and then computing whether a given guest matches. Not surprising that that's where a lot of our time goes. But we can do this better, right? We can. We can optimize this a little bit. One thing you might have noticed is that these stack traces aren't ideal.
03:03:36.266 - 03:04:14.790, Speaker A: There are a couple of reasons for that, and one of them is that we're running in release mode. And so we don't actually have as much debug symbols as we would like. So we can do profile dot release debug equals true. And let's try to run that. And I'm gonna perf record PGRP. Rogit, once this is running for a bit, let's see if that gives us any yeah. So here, now you see we have much better information in our, in our flame graph.
03:04:14.790 - 03:04:46.980, Speaker A: See, if we zoom into play. What do we got here? All right, so all of it, most, basically all the time is spent in guessing, right. In calling the guess method of the guesser. And so now where does that time goes? A lot of the time is spent in tostring inside of guests. A lot of it is spent in dropping strings in guests. And then a fair amount of spent is spent in matches. Let's see if we can't do anything about these allocations.
03:04:46.980 - 03:05:18.610, Speaker A: So we call tostring here in the type loop where we compute over patterns. And that seems really unfortunate. Let's see if we can do better that. But before we do, let's go ahead and actually this is so slow that I can't even really run hyper fine on it because it wouldn't complete. We could limit this to a smaller number of guesses. That is one way to go about it. But do I want to do that? Actually, here's what we're going to do.
03:05:18.610 - 03:06:13.050, Speaker A: Let's go back here and give us a way to run different algorithms using the same binary. So let's say that. Let's say that we do. It makes me sad that naive has to each game, it has to recompute the dictionary, but it's fine. Let's go ahead and bring in clap here because I like clap. So here I want derive. Clap, use clap, clap, clap, clap, clap, clap, clap.
03:06:13.050 - 03:07:03.070, Speaker A: Let opts is opt should arguably look at the manual and copy paste the example. No, clap, clap and root. All right, fine. I'll open the documentation. It's clap parser. Oh, you know, close enough. I still want games to appear up here because it's a constant and I guess it is this.
03:07:03.070 - 03:08:40.250, Speaker A: And what I want is implementation. And that's going to be a, I want that to be an enum. Enum implementation and I want it to be either currently just naive and we're going to say that implementation is going to be implementation. And then I do actually want one more thing, which is short, long a max, which is a maximum number of games to play from STR for implementation, type error is equal to string. There are ways to just derive these for enums so you don't have to write the implementation yourself. But instead what I'm going to do is just do it myself because I don't want to look up where to do it. Format unknown implementation.
03:08:40.250 - 03:09:24.110, Speaker A: This one s. So that's going to be error of this. It's going to be okay of this. And this has to be qualified error because why name it fully. All right. And so guesser, I'm going to have a little helper here, which is going to be play. And you'll see why in a second.
03:09:24.110 - 03:10:05.830, Speaker A: It's going to take all of these bits. It's going to take a maker, it's going to implement FN mute is fine. And it's going to return a g. It'll be generic over G, where G is a guesser. So the idea here is you're going to pass it a constructor, it's going to run the constructor and then play using that. And we're going to hear match on ARG's implementation. Implementation.
03:10:05.830 - 03:10:58.468, Speaker A: And if it is implementation naive, then we're going to call play with a closure that runs Rojit algorithms naive new, which we can just replace with this because it's the same thing. And now this can go away and play can stay there. This has to be mute. That's fine. The reason I do it this way is because I want this to be monomorphized to the particular guesser. I don't want like dynamic dispatch for every play, for example, because then you would end up with like dynamic dispatch over the calls to guess, which seems excessive. Clap is a macro for each of enums.
03:10:58.468 - 03:12:02.200, Speaker A: Ooh, does it clap argenum instead of parser? Arg enum, debug clone copy. And I'm guessing I need to remove my from string implementation. That doesn't seem right. No, it still requires from string, although maybe I'm just arg enum. Oh, I guess I can say possible. Oh, no, it should do that for me, right? Oh, I need to set this here. So this is going to be arg enum as well.
03:12:02.200 - 03:12:33.840, Speaker A: Nice. That makes me happy. And I'm going to open this again because otherwise rust analyzer doesn't realize that I have new traits. And Max is an option use size. It doesn't require that you pass one. Arg's Max. Max is an option use size.
03:12:33.840 - 03:14:11.844, Speaker A: And here we're going to do this. Dot take Max unwrap or else use Max value or use size. Max, I suppose unwrap or use as max implementation naive, right. Fantastic. And if I pass Max, say three, it should now run the naive implementation, but only for three games. So if I say, say one, let's do two, then that will continue to run in the background. The reason I want to do this is because I can now go to naive and I can copy paste this whole file and call it this alox, and is going to be the same as naive, except we're going to optimize the allocations a little bit.
03:14:11.844 - 03:14:58.860, Speaker A: The only reason I do this, normally you wouldn't keep all of these parallel ones around, right. But it makes it much easier for us to compare how the different ones are doing. So I can do the same thing here. Great. So now we have the naive implementation, and I can go to allocs and see if I can't make this a little bit better. So I'm going to make guessde. Oh, this is gonna, it's actually gonna be really awkward, because I'm gonna want to change some of the types which are gonna make the other implementations not work anymore.
03:14:58.860 - 03:16:20.440, Speaker A: I'm gonna have guests have a lifetime and allow word here to be a to be borrowed, because there's nothing that really requires that this be owned. Right. All of our implementations on it just care about you having a string reference. And the reason why this matters is because over in naive this will remain cow owned, but over here it can be cow borrowed now, which means we don't have to allocate a new copy of the string for every guess. That's actually the only difference. I think we're going to apply here, and just to see that this actually still runs. So let's run this without a max, and then try to benchmark this one.
03:16:20.440 - 03:17:31.386, Speaker A: You see it's still kind of slow, like it's still clearly not spinning around, but we're gonna stop it and then we're gonna do allocs. And if I now open Alox SVG, you see that now guess is not spending any of its time in this allocation stuff at all. It's in fact spending almost all of its time in the iterator next and in matches. And we should be able to see this right in the performance. So if I do so we have this as the compiler artifact. So hyper fine is basically you give it a bunch of commands and all run each of them. So I'm going to say please run Rajit implementation naive Max one and run this implementation allocs Max one.
03:17:31.386 - 03:18:23.172, Speaker A: And I think you can name them. I think it's name naive and name Alex. I guess it's just n. And now you'll see it's going to run this command multiple times to measure the variance and its execution time, which like here we don't really need the variance, because we happen to know that it should basically not vary at all. And because the runtime is so long, you see the ETA is about three minutes for it to finish. Benchmarking naive. And that's because it's going to run it multiple times.
03:18:23.172 - 03:19:01.910, Speaker A: We can tell it to not need to run things as many times, but we'll see here what it comes out to. And then we should be able to tell that. We should be able to tell that the one where we remove the allocations is faster, even though they both looked like they were slow. That was just like we eyeballed and didn't return immediately. Whereas this should actually tell us, is there a statistical significance to the diff that we made while that's running in the background? Let's go look at these. So now let's zoom in on matches here. My face is in the way.
03:19:01.910 - 03:19:21.780, Speaker A: Let's see if I can get my face to disappear. Actually, can I just. Sure. That works close enough. I think now it's not cut off. Right. It's not ideal, I know, but otherwise I would have to switch to obs and stuff.
03:19:21.780 - 03:19:42.640, Speaker A: So matches. I know it might be small now, but we have a bunch of time spent in cow draf. So that certainly suggests that we're like. That suggests the problem here is the number of iterations, not what you're doing inside each iteration, because D ref does very little. Right. It's a. It's following a pointer.
03:19:42.640 - 03:20:17.530, Speaker A: So once that starts to show up, it suggests that you're doing it a ridiculous number of times. So, like, at this point, it's. It's more helpful to reduce the number of iterations than it is to reduce the amount of work you do within each iteration. But even so, it's worth doing. All right, so we're also. This is iterator next, which ends up doing next code point. And this is also next code point.
03:20:17.530 - 03:20:40.220, Speaker A: Okay. So we're spending a lot of time just iterating over code points in UTF eight strings. Okay, that's not ideal. Let's see if we can do better for that. And that's all inside of guess matches. Let's see if we can improve the code of that a little bit. Let's go back here.
03:20:40.220 - 03:21:26.740, Speaker A: I'm gonna. Me doing things in the editor is gonna affect the benchmarks that are running here by a little bit. The hope is that I have enough cores that it shouldn't really matter, but let's go look. All right, so next code point is a problem. Now, of course, we have a second problem here, which is that once we start changing guess naive is no longer going to work. Well, actually, no, maybe we can. Or rather naive is also going to benefit from the same performance improvement that we apply because we're fixing, you know, guests matches as opposed to fixing something just for alloc or just for a new implementation.
03:21:26.740 - 03:21:53.236, Speaker A: So we're going to have to be a little bit more sophisticated about the way that we do benchmarking. Or we're just going to say, okay, naive, we don't, we don't care about naive anymore. We're just going to benchmark different binaries instead. Okay, so this is saying finish naive. Now the mean time to solve again, just one game is 19.5 seconds plus -0.3 seconds.
03:21:53.236 - 03:23:16.430, Speaker A: And allocs currently seems to be about 9 seconds mean, okay, so we cut the runtime in half by removing these allocations. That's pretty good, right? That's pretty good. Let's see how much better we can do if we fix up this. So let's say that Gchars is going to be self word hasbytes and let wooden cars is going to be self is going to be word asbytes. And then here we're going to loop over g chars and actually we can just do bytes, bytes. I'm just going to make all these iterations be bytes instead of cars. And it's going to yell at us here, right? Like that's, we don't really care about the, oh great, I don't even need the iter.
03:23:16.430 - 03:23:47.640, Speaker A: It's going to yell at us here. Because now we're trying. Actually maybe it won't, maybe this is just going to be fine because we're now, so we're effectively now just comparing the bytes of the string rather than the characters. All right? Yeah. So Alex ran about two times faster than naive. Great. So we now know that Alux is better than naive.
03:23:47.640 - 03:24:35.920, Speaker A: Now we're going to have to do a little bit of a trick. We're gonna have to copy this file to Roger UTF eight, because once we compile it with this fix. Now if I go cargo run, in fact, let me do, let me run that one. And then I try to gather some more data here. And now this is going to be allocs. No UTF eight. No UTF eight SVG.
03:24:35.920 - 03:25:25.170, Speaker A: All right, so now guess matches. You see, now the time is spent in draft and then nothing really. And an eek and an iterator. Any check. Okay, so this is now, ooh, any check. Oh, right. So this is the inside of the closure that we end up applying here, right? So that is the closure over in this closure, this is the one place where we could try to just like replace this with.
03:25:25.170 - 03:26:04.100, Speaker A: Let's do that in a second. So let's try hyper fine here now. And now we're going to go the one that we copied over here with implementation allocs. So that's going to be previous, let's call them old and new. And new is going to be that one with Max one. Let's see what we get. Oh, we called it Roger Utf eight.
03:26:04.100 - 03:27:14.600, Speaker A: So old should be the same as the new, like Alex in the previous run, right? Should be about 9 seconds. Yeah. So because that one hasn't changed, it's the same binary we used previously. And because we know that there's not too much variance here, what we can do is do Max. Where's the max min runs? We want to change min runs to be like two because we're not expecting a lot of variance and running it ten times seems excessive here. That matters a lot more if you have programs that are much faster, right. If you have a really fast program, you need to run it many times to filter out a lot of the noise around things like process execution and spin up time.
03:27:14.600 - 03:27:59.220, Speaker A: How do you benchmark the parts of measure? Can you add trace points or something once you want to do that? There are tools you could use for this. Like you could also use criterion, right? Like you can write this as a benchmark inside of rust, rather than benchmarking the whole binary. I kind of like benchmarking the whole binary, like for things like this. But we might add criteria and benchmarks for this too. Okay. Yeah, so you see that, that also significantly speeded things up. But, but nothing as much as maybe we would have hoped, right? It would be nice if iterating over the bytes reduced all this cost for us, but it doesn't really, it's only a little, it's only 50% faster.
03:27:59.220 - 03:30:14.730, Speaker A: So in fact, let me, let me go ahead and just commit some of this. I'm gonna first commit the change here to support choosing an algorithm, and then I'm going to add allocs and not the bytes change yet. Add implementation without tostring. I also want to Gitignore star SVG and star data. Actually, let's do perf data and perf data old and also rojit star because we're going to end up with more binaries here, ignore war files, and then we're going to have commit iterate over bytes, not cars for ASCII. And then one thing I actually want to do here is, let's say head four. I want to reword that one and this one speed up.
03:30:14.730 - 03:31:04.180, Speaker A: So this was a gets a two x speedup. This is gets a 1.44 x speed up. All right. And then I wanted to try one more thing here, which is let's copy this one to rogit bytes and let's try, remember how someone pointed out that we could just use the other implementation directly? Oh, these are all going to fail now. That's fine. Borrowed.
03:31:04.180 - 03:31:48.960, Speaker A: Ooh, no, I do not. What did I do? Cow. Borrowed of this. Use stator. Borrow cow. Is that gonna give me my test back? Great. So this is a fixup of this one.
03:31:48.960 - 03:33:01.510, Speaker A: That fixup just merges together the commits and the git history. Right? So someone pointed out that we can just use the correctness compute function here. So how about we just do that? Just return this of self word and self mass. So if this is equal to that, we're just going to reuse that implementation. What? What did I do? Oh, let's see how that does. Whoa, I guessed a lot faster. Let's see if that's a meaningful difference.
03:33:01.510 - 03:34:15.660, Speaker A: So we're going to do old is going to be, what did we call it? We called it bytes against the current one. Let's see what we get here. Whoa, that's interesting. So that is a speedup of 20 x. And what this suggests, it's that this method is way faster than the scan that we implemented here. This makes, this sort of means that I need to understand why this is correct, because this is definitely a worthwhile optimization. So a word will match a pattern or.
03:34:15.660 - 03:35:39.280, Speaker A: Oh, I see. Okay. The insight here is that if we say that the, the target word here is the correct answer, then guessing the previous word should give you this mass, this set of correct bits relative to that answer, and that set of correct things should be the same as, like, I'm almost there, I believe you. I mean, okay, so we could obviously do this. A is self dot matches word, let b is equal to that cert eek, a and b and then return a. Yeah, so they don't always return the same thing. That could be a bug.
03:35:39.280 - 03:37:04.010, Speaker A: Right? Like let's do self word word mask, self dot mask tears. If the previous guess was tears and the new guesses, brink and this pattern, then the two disagree. All right, so let's make this into a test case of I guess matches debug from crash. So this is saying that if we check tears with wrong, misplaced, misplaced, wrong, wrong. So the question is, what should that do against Brink? Let's just work out whether this should be allowed. So there's no t. That's fine.
03:37:04.010 - 03:37:20.672, Speaker A: There is an a. This does not have an a. So this should be disallowed. Right? Because there's supposed to be an a somewhere and this one doesn't have an a. There's supposed to be an r. There is an r. It's not supposed to be an e.
03:37:20.672 - 03:37:46.950, Speaker A: There's no e, should not be an s. There's no s. So the a is, it doesn't check. Okay, so the, the problem is that it doesn't check whether you've used all your yellows. So I'm guessing this is actually bug in our matches. That's what I think. Cargo tea from crash.
03:37:46.950 - 03:38:22.840, Speaker A: Right. Of course. That just hits the same assertion we just inserted. Let's get rid of that for a second. Yeah, so when we delegate to our own matches that we wrote, that test case fails, which suggests that our implementation of matches is incorrect. But the correctness compute seems to be right. So I guess we will leave this test and.
03:38:22.840 - 03:40:12.976, Speaker A: All right, fine, fine, fine, fine, fine. You will get rid of this and we will say that this is just this to do explain why this is equivalent. The pattern shouldn't be able to distinguish between word and answer. That's what getting a specific pattern means. I see. The pattern is in some sense like an XOR where whether you give it the, whether you use the guess, guess with pattern gives answer or answer with pattern gives guess are interchangeable or indistinguishable. So the idea is that if guess, if guess g gives mask c against answer a, then guess a, then guess a should give mask c.
03:40:12.976 - 03:40:58.170, Speaker A: Should also give mask c against answer g. Yeah, so it was slow and wrong. Nice. Now if I run this, it passes. That's great. So we're gonna, what was the speed up 18 times 18.36 faster.
03:40:58.170 - 03:41:33.798, Speaker A: Okay. Use correctness computation. Use correctness computation for matches. This gives a 18.4 x speed up or about 18 x speedup. If something is correct, it can't move. Yeah, so that's commutative.
03:41:33.798 - 03:41:49.590, Speaker A: If yellow, it needs to move, which is commutative. And if Gray, another letter is needed, which is commutative. Okay. Yeah, I believe you. I believe you. All right, so let's figure out where time is being sent, spent. Now let's do the same thing.
03:41:49.590 - 03:42:25.342, Speaker A: So we're going to r this and now it's going to be fast. So now we're just going to. Yeah, look at that go. And also, because we fixed matches, it actually does better now than it used to. Which is its own kind of fun. Alright, let's grab some data here and print that to Newmatch, Newmatch SVG, new. Oh no, new match.
03:42:25.342 - 03:43:30.640, Speaker A: All right, where's our time being spent? Guess so we got iterator next, which does iteration over a hash map. Oh, this is the iteration overdose remaining. And I think remaining actually doesn't need to be. Let's copy that to. What are we calling this new match? Actually, no, this can be a different, this can be a different implementation. Alox with vec, remain. Vecrem, because we'd like to be very brief here.
03:43:30.640 - 03:44:10.320, Speaker A: So back to main here we're gonna have vecrem, vecrem, vecrem. All right, give me vec. Remain here. Oh, I called it the wrong thing, didn't I? Maintain. Fine, fine, fine, fine, fine, fine, fine. That's great. All right, rec, remain.
03:44:10.320 - 03:45:03.240, Speaker A: So here, the observation is that remaining doesn't actually need to be a map, because we never look up into it, we only ever iterate over it. So why not just have it be a vec of a tuple of these things? Now we can do Vecna from it, or of that, we do use it for retain, but that's fine too. And we use it like this for iteration, we iterate over it here. So alox. Oh, right. So vecra main doesn't even need hashmap in here. That seems pretty promising.
03:45:03.240 - 03:45:50.898, Speaker A: So Alex does some things, and vekrem. Vekrem goes faster. It gets cissy in five instead of four. That's probably because the order is different. So ties get broken differently, which is interesting that it matters. Well, it's very interesting that it matters, but. Okay, all right, so hyper fine, please give us this one.
03:45:50.898 - 03:46:32.218, Speaker A: And I want Vekrem versus Alex. Show me what you got. I can run much faster now. Ok, so. Oh, it shouldn't say Alex and naive. This is old and this is new, but even so, the names don't matter. Okay, so it's slightly faster, but if you look at it like, the variance here is pretty high too, so like it's a little better, but not clear it makes that much of a difference.
03:46:32.218 - 03:47:40.050, Speaker A: But even so, like a speed up of 10%. I'll take it. Algorithms vac remain. Switch to Vec for remainders. This brings a 110 percent speed up. All right, what else we got? What else we got here? Let's look at the flame graph again and then bring that into to this is now vecrem. I'm gonna close these older ones because they don't really do much SVG.
03:47:40.050 - 03:48:06.330, Speaker A: All right, we see some other things start to creep in over here now we've got something over here. That's fine. What do we got here now? Compute. There's next code point now. Oh, we got next code point back. We know what to do about that. So that's going to be in lib chars.
03:48:06.330 - 03:48:50.664, Speaker A: So let's do, let's copy that over to vec vecrem. And now we're going to make this be bytes again. Bytes, bytes, bytes, bytes. Ah, it seems, seems faster still. Let's see if hyperfine agrees. This is gonna be dot slash rojitvecrem against this one. Oh, for the max.
03:48:50.664 - 03:49:52.480, Speaker A: Yeah, we could totally increase the max. Now the biggest thing that we'll do is it means that we get the benchmark across more answers, which means we get to benchmark the earlier part of the process better too, which seems fine. That's a good question actually, is whether we always. Okay, so here's a problem that was, oh, maybe it's not that bad. Okay, no, nevermind, I thought I spotted a bad thing. Okay, so clearly this is faster, right? This is git commit. Use bytes to compute correctness.
03:49:52.480 - 03:50:49.180, Speaker A: This brings a speedup. Let's have that run in the background, do some more benchmarking. And now this is going to be bytes. Let's see, we get bytes SVG. All right, where does the time go now? You keep getting more things to creep in on the side, and that's because we're getting better at optimizing the remaining bits. So what do we got here? Now we have iterator, next zip, get unchecked from iter bytes and any, so this is the closure that we have in correctness. Compute.
03:50:49.180 - 03:52:05.450, Speaker A: So which closure is that? That's this closure. So this closure, we're now spending almost a third of execution time is spent in this closure. So let's see if we can do better here. That seems pretty straightforward, although certainly one thing we can do here is this does an index lookup and this looks an index lookup, which means that they're going to be bounce checked. So how about we do ziptainous used dot it er mute. Then we don't need the enumerate and we get a and used. And if it's not used, we set used equals true.
03:52:05.450 - 03:52:41.782, Speaker A: So this should get rid of the bounds check. Let's see if that made a difference. Oh, I forgot to copy the binary first. So this then is bytes. I guess I overrode an earlier bytes, but that's fine. And run this. And now I want hyperfine of bytes against this one.
03:52:41.782 - 03:53:25.796, Speaker A: Let's see how that does. This one's only barely faster. Let's make sure that it actually is like, let's say we're going to run it against a couple of more solutions. Solutions. And we're going to run it a couple of more times because I want to make sure that we're not just measuring noise here. And it does actually make a difference. It's interesting actually, because this suggests that the bounce check was probably removed already.
03:53:25.796 - 03:54:23.100, Speaker A: Like the, it is, it is faster, actually. Statistical outliers. All right, all right, let's, let's just settle down here. There's a lot of noise here, but yeah, see, old is faster than new here, which is interesting. It suggests that this actually slows things down, which also suggests that the bounce check was probably already eliminated previously because this should definitely not have the bounce check because it's doing a zip on the iterator. So it knows that every yielded item is in bounce. You see, old here is way faster than new.
03:54:23.100 - 03:55:09.490, Speaker A: So that suggests we should keep this the way it was, which is interesting. Alright, so that's, this is an example of an optimization that turned out not to be worth doing. So we're not going to do it. What else do we get here? Okay, here's an obvious one that's been brought up in chat. Two, which is why are we carrying strings around here when we know that everything here is five bytes? Five bytes. That seems silly. Why are we doing that? So what we're going to do is pub type word is going to be a u eight of length five.
03:55:09.490 - 03:56:06.668, Speaker A: And we're going to say that this is going to be a word. Try into dot bytes. Try into, expect every word should be five characters. Collect, try collect word. Fine. Oh, as bytes. Trying to bytes gives you an iterator over the bytes.
03:56:06.668 - 03:57:02.170, Speaker A: As bytes gives you a slice. All right, so this is going to take a word and we're going to say that this is going to return a word, word, word. They're all going to be words, word, word. Where we got these strings, word. I'm getting, you know, that sense when it feels like it's not a real word anymore. Word, word, word, word, word, word, word, iter. Iter.
03:57:02.170 - 03:58:05.380, Speaker A: There, these are already bytes. There's no reason not to just iter. Like there's no need for a dot bytes anymore. Crate word. And now this is going to be b. So the b prefix here just means make this a byte string instead of a utf eight string. Same thing here, same thing here, same thing here.
03:58:05.380 - 03:59:37.540, Speaker A: All of these are byte string. You get a byte string and you get a byte string. And you get, get a byte string. And this could arguably do a vim macro here, but I'm almost done. It's always one more sorry, not sorry. Let's just see that this actually compiles, which it, of course, does not, because consider dereferencing the borrow. Oh, that's fine.
03:59:37.540 - 04:00:37.220, Speaker A: This is because that produces a static reference to an array of that length, whereas what we're telling it, it needs to return is the actual, the actual five bytes, which is fine, I think. What else we got? 39 string from UTF, eight unwrapd. That's fine. Can't compare these two. That's fine, too. What else we got, right. Naive.
04:00:37.220 - 04:01:19.020, Speaker A: So we're going to bring in word. This is going to be word dot as, by actually, let's do word is word as bytes. Dot try into dot. Expect every word is five characters. Every dictionary word is five characters. And we can do this for all the algorithms. So for allocs, we do the same.
04:01:19.020 - 04:02:42.014, Speaker A: And for vec rem, we do the same. We'll stick in word here. This is going to be word, word, word, word, word, word, word, word, word, word, word, word, bring in word. What else we got? Expected string invane, write this, returns a word. We might even be able to remove this top thing now. Ooh, candidate holds a word. Yeah, I think we might be fast enough that we can even compute the first word this way.
04:02:42.014 - 04:03:13.812, Speaker A: That's not to say that we necessarily want to. Oh, I think at this point, naive doesn't really make sense anymore, because there's no, there isn't even a notion of a cow here, or there isn't gonna be. So we're gonna get RMD algorithms naive. Bird is the word. Word is the word. Word, word, word, word, word, word, word. You're not wrong, main.
04:03:13.812 - 04:04:00.380, Speaker A: Let's get rid of naive because it doesn't exist anymore. And over in algorithms, allocs. I don't want to substitute all stirs with word, because it does structs and stuff too. But I can do this. I also do. It does make me wonder whether we actually would compute tears if we tried. I'm guessing probably not, because we don't do the smoothing.
04:04:00.380 - 04:05:31.720, Speaker A: Main 43, right answer is going to be a rogit word, and it's going to be answer as bytes. Try into unwrap vimreplace doesn't like backslash b because it doesn't have good enough regular expressions, which makes me sad. But all answers are five letters. And I guess we'll do this answer b. So that answer can stay a string. I think answer here can just be word. Why does it need to be a reference anymore? 278.
04:05:31.720 - 04:06:28.310, Speaker A: I can't implement display because word is a type alias. Oh, this is why. Get off my lawn. All right, how about that? Great. Whoa. Those ran real fast. All right, so in theory, now let's see, what's the last one we made? We made bites, right? So let's do a hyperfine of rojit bytes with this one and see how we do now.
04:06:28.310 - 04:07:31.090, Speaker A: So now there are no more strings in our new implementation. That looks like it's only barely faster. It does look like it's faster, but not by much. Wow. 1.02. You know, I think this is because, I think this is because we're not implementing a slightly different optimization, which is, I think most of our time now is actually spent in setting up a guesser. I should have implemented this first.
04:07:31.090 - 04:09:15.988, Speaker A: I'm going to implement that first. I'm going to stash these changes, and then I'm going to go to, and then I'm going to copy algorithms, vec remain to algorithms once in it, and we're going to add one here. And you may pretty quickly realize where this is going, but this is going to be. This initial load is unfortunate because it doesn't really need to do what it's currently doing. And in fact, it's reading the entire dictionary at least once and allocating a vector for it every single time we, every time we load it, which seems unfortunate. So I want to see whether that makes a difference first. So I undid the string change just to do this first.
04:09:15.988 - 04:09:54.120, Speaker A: So here we're going to do is bring in one cell. Nice. And there are a couple of places, actually where we could use this. We're going to go ahead and do. I'm going to have to stash these changes, too. And then I'm going to have to do this, and then I'm going to copy this to rogit. I guess this is just vecrem, which we already have.
04:09:54.120 - 04:10:50.850, Speaker A: What I want here is I want all the places where we load the dictionary into memory to only happen once. It already only happens once for wordle. So this loading is going to be every time we start a new question, if you will like a new. There's a new answer. We construct a new one of these, and each one has to construct this vector, which seems necessary. So what we're going to do is, where's my. This thing you're going to say static initial.
04:10:50.850 - 04:11:45.420, Speaker A: It's going to be one of these. And I guess I'll bring in the synthescope too. One cell is like lacy cell, but better. So what we're going to do here is this is going to be cal static of this because it's either going to be a borrow of initial or it's going to be owned once we start pruning it down again. This is. I've undone the string modification just to test this out first. So here I forget what the.
04:11:45.420 - 04:13:56.010, Speaker A: It's the get or initial get or in it. Let's see where it's gonna be complaining here. It's probably gonna be complaining down here where we make remaining, it's my guess the magic of cow dot. What is that method call called cow to mute. And there's an argument here, actually, that instead of two mute, which is going to immediately clone the thing, I think what I want to do here is if. So, this is going to clone the vector that has the entire dictionary in it and then call retain, which seems successive. That's not really what we want here.
04:13:56.010 - 04:15:12.050, Speaker A: Instead, I think what I want to do is match on this. Match on self dot remaining. Actually, just do. If matches self remaining cow owned, then self dot remaining dot to mute, which will do nothing because it's already owned else. Self dot remaining equals cow owned of self dot remaining iter map or filter dot collect type mismatch. It's complaining that, oh, cow owned of anything. It doesn't really matter.
04:15:12.050 - 04:15:49.180, Speaker A: And then it's complaining over here because collect exists for bool. Aha. Try that. A value of vic that cannot be built over elements of type that that's correct needs to be copied. All right, so let's go ahead and try to compare these ones. Hyper fine. Give me the previous place where I did this one.
04:15:49.180 - 04:17:04.722, Speaker A: And I want m five and I want to compare vecrim with, let's say seven to once init. It's just called once. Let's see how that goes. Yeah, so that's, you see, it's faster. It's not faster by very much. This makes me think that our, our overhead is starting to come from something else. I mean, it is faster, but like, by how much? You know, I just want to gather a little bit more, even though the variance here is pretty small.
04:17:04.722 - 04:18:30.620, Speaker A: But like, it's not even clear that it's actually faster. That doesn't seem very much faster. I wonder why it should be using less memory that's true. Mmm. It's just that it's within the noise. Yeah, like, it's all within the noise. It is, like marginally faster here, but that's interesting.
04:18:30.620 - 04:19:32.060, Speaker A: I wonder. This suggests that there's, like, optimization going on. In fact, I want to see where that time is going once and guess. That's interesting. All of the. Okay, so all of the time is still going into compute, which suggests that, like, it just doesn't matter that much that the initialization is faster because compute takes so much of the time. You are also right that it is actually allocating.
04:19:32.060 - 04:20:28.380, Speaker A: It's allocating more memory, technically, because we're only running one game at a time. If we were running multiple games concurrently, this would make a difference, but one game at a time. We're now always keeping the vec of the entire dictionary around, and then we're also keeping around the shorter vector of remaining guesses. Hmm. So it's like worthwhile for later. It's just not worthwhile right now. Hmm, that's interesting.
04:20:28.380 - 04:21:26.310, Speaker A: All right, let's. Let's keep this one. Algorithms once init, add onceinit implementation. And here we can note, not actually faster since bottleneck is computed. All right, and now we can go back to our thing where we have word. Now, this can be word, word. Where's our try into? It's over here.
04:21:26.310 - 04:22:33.530, Speaker A: And I guess we can copy that into once. Right, and this is going to be word as well. All right, so let's see whether that made a difference. So here we're going to runce with implementation once. Let's. Do you know, Max of. Max of five is probably fine against this one.
04:22:33.530 - 04:23:32.080, Speaker A: The sigmoid shouldn't necessarily reduce the number of iterations. It should just mean that we don't prefer more likely words quite as strongly as we do now. All right, so this is the benefit of moving to moving to bytes. So it's not that great, actually. I mean, we could try making these just be without the indirection here as well. This is now going to be not a static, but just word. And then it doesn't need to do this.
04:23:32.080 - 04:25:07.180, Speaker A: In fact, why is this a cow and not just a word? See if we can't. Yeah, these should just be words. What are we even doing here? Why are we passing around? Arguably the same with guess, but, you know, word, word, word, word, word, word, word, word, word, word, word. No rules expected. Token. Can I make this be that? Is it going to yell at me if I do this? Because I guess it's technically a separate token. I want to see if it even runs? Because I don't think it does yet.
04:25:07.180 - 04:26:09.180, Speaker A: Cow shouldn't be needed. This might make a difference once initial. This takes a word. It's a little unclear why it doesn't allow me to. Oh, it's because I have to do this. There we go. That's fine.
04:26:09.180 - 04:26:40.210, Speaker A: It's not actually clear. This is better. Oops. This has to be this to pass around the actual bytes everywhere. Arguably what I should have done is benchmark this before and after this change. But, you know, you can't always get what you want as the song goes. Yeah.
04:26:40.210 - 04:27:09.800, Speaker A: So same in Vec remain. It's gonna be this twice. No, just once. This is just going to be word. This is going to be this and this. Oh, word. This.
04:27:09.800 - 04:27:41.490, Speaker A: This shouldn't need to do that. This can just be word. Remove all the pointers and no more need for cow and alox. Oh, did I not remove this one? That's why I needed this. And that's why I needed that. And that's why I needed this. And over here, same thing.
04:27:41.490 - 04:28:23.520, Speaker A: No, this one too. Which makes this only take one, which means this can go away. This can go away. Candidate is here. Word is here. Parentheses are required to understand this expression. All right, and I don't need cow anymore.
04:28:23.520 - 04:28:51.970, Speaker A: Oops. And same invect remain. I don't need cow anymore. And same in lib. I don't need cow anymore. So now what do we have? What was the previous one called? It was called once, so hyperfine. Give me once versus this one.
04:28:51.970 - 04:30:12.200, Speaker A: It's true the word is only five bytes, but U 64s can be passed around really efficiently. And now we suddenly have like a packed representation. Let's do ten. Yeah. In fact, look, now that we're not using references anywhere, we're always passing around the, like, five byte array. It actually looks like it used to be faster. The new one is lower.
04:30:12.200 - 04:31:23.150, Speaker A: It's actually, it seems like it's faster to actually pass around string references everywhere than it is to pass around the words, which is pretty interesting. We have one more place where there's a reference to a word, and that is the wordle dictionary. Actually, I want to try to benchmark this and see what we get here. Bytes. Bytes. Svg. Yeah, like, all right, so we've got some time over here and guess.
04:31:23.150 - 04:32:26.638, Speaker A: Equality for slices. Where is the, where is there an equality for slices? Oh, equality for arrays. Yes. There's a lot of equality of bytes here, which is not terribly surprising, right? Like that. We're just spending a lot of time doing that. Oh, why does compute have such a large gap between the two columns? The time in between here is spent in the bytes of compute. So there's no function to be called.
04:32:26.638 - 04:33:09.126, Speaker A: It's just, that's just where the time is spent. There is a tool for visualizing these, but I do not remember. It's called cache grind. Actually, we can look at this with. Why am I opening that with perf report call graph dwarf. Perf report g. Once in it, guess.
04:33:09.126 - 04:33:59.830, Speaker A: Guess matches. So that's compute. This is the command line interface of perf itself. So let's annotate compute. Well, that's not very helpful. What is the thing for showing? Oh, that's entirely unhelpful. Ah, s really? Why does it not, mmm.
04:33:59.830 - 04:34:57.970, Speaker A: Only available for source code lines. Why is it being unhelpful about this? I thought annotate would do that. Like normally. That should also show the. I definitely had perf to show. Like, annotate should give me the real source code here. I'm a little surprised that it doesn't, like, if I go here, well, if I do, if I do this.
04:34:57.970 - 04:35:33.090, Speaker A: Well, that's not at all what I wanted. Um, yes, you see, there are two columns here. There's children and self. Children is how much, how many cycles were spent on this function, and any function that this function calls, like with this function in the call stack. And self is how many cycles were spent on the assembly in this function, not counting its children. And that's where you see compute is really big. Like how many, how large of a fraction of the samples of this function were spent in the function zone assembly.
04:35:33.090 - 04:36:34.630, Speaker A: But why am I not getting the debug symbols here? That's definitely a little disturbing. No, they shouldn't be optimized out because I have debug equals true here. But what might matter is build rust flags equal dashy force frame pointers. Yes. Let's see if that makes a difference. Build right. Cargo.
04:36:34.630 - 04:38:33.080, Speaker A: Cargo config tomlike. Is that not the name of it? Force frame pointers. Why does it think that this is clean? That's what I want to know. It's gonna force it to recompile here. Is it even picking up my force frame pointers? It is not so, see, you're not supposed to need frame pointers when you're using dwarf, but I found that every now and again the dwarf info is just not enough. It is really weird that there's exactly one pop in there, which is why I'm trying to get the actual info from this, but I want to see why is build rust flags not taking effect? I guess I can just go back here and then do. No, rust flags equals c force frame pointers equals y.
04:38:33.080 - 04:39:28.080, Speaker A: Oh, is there, can you hear like a water fountain in the background? Hmm, weird. All right, let's see here what we. Yes, some people can. Okay, let me plug it out. Interesting. Yeah, it's for the cats. All right, let's see.
04:39:28.080 - 04:40:29.174, Speaker A: Well, that still didn't give me anything. Why, why, why rust perf. Annotate, annotation source code. It's a very good question. Oh, if you hear it the whole time, you should have, should have let me know. I mean, we could run it through cash grind instead, I suppose. All right, fine.
04:40:29.174 - 04:40:49.680, Speaker A: We can use call grind instead. Given that it doesn't seem like it wants to give this to me. That's fine. I. No, that's not what I want. And do I have Valgrind installed? Do I have QT? Q. I do not.
04:40:49.680 - 04:41:32.290, Speaker A: Qt cache grind. Q cache grindenne. Yeah, you might be able to do this just by using, you might be able to do this using like dev mode and then setting the op level, but I kind of like to not have to do that using call graph LBR. I don't even know what that is. Let's try it, see what happens. No, sorry. That's fine.
04:41:32.290 - 04:42:39.474, Speaker A: Okay, so what we're gonna do is use Valgrind. So Val Grind is a great tool for things like this. We're gonna do call grind on this, is it not call grind? Grind, call grind. That's why. So if you're not familiar with Valgrind, Valgrind is a tool that does on the fly rewriting of your binaries to have them include a lot more debug information. It gives you really, really good detailed information about where you're spending your time. The big downside with Valgrind is that it slows down the execution of your program a lot, as you can see here.
04:42:39.474 - 04:43:10.144, Speaker A: But it does give you really good information. So here, I'm going to terminate it and let's see what we now get. If I go over here, I'll close this and then I'll open this and I'll try to open dev streams. Where is Rojit? Call grind. Let's see what we get here. All right, maintained. Call emap.
04:43:10.144 - 04:43:55.190, Speaker A: Alright, so this gives a more like visual representation of what's going on. Let's go to correctness. Compute. So this is showing actually the time spent on initialization. This is the whole thing that we optimized with once in it. As you see, we are spending a fair amount of our time in there, right? This is not zero amount of time. Where do I go into? Why can't I see it here? Oh, I guess it's just, this is just the, if I go to source code here, it should show me where that time is going.
04:43:55.190 - 04:44:37.472, Speaker A: Once initial oh, I'm looking at guess. That's not what I want. I want to look at one cell initializer. Oh, interesting. So this is in guessdez for once in it. Where is that time going? So you see, it says 22% of the cycles are spent in, in the guest function. And let's see where, you'll see.
04:44:37.472 - 04:46:22.390, Speaker A: It's a little hard to tell because you see a bunch of time here is like just read zero. And it's because optimization means that where you think the code is running is not necessarily where it's running, but here it should tell us. You see like, oh, it's not giving me inline information because it doesn't know what the source library is, but I can tell it that I'm pretty sure. View configure source annotations add rustup no. Um, uh. Choose no where is dot rust up tool chains stable am I running stable? Running beta? Glad I checked. Run beta people not share but lib rustlib source rust and that should be for that target.
04:46:22.390 - 04:46:57.960, Speaker A: Let's see if it now can find it. There we go. So now you see, it shows the origin source in the standard library. So it can tell me where that, where we're actually which, which source code instructions in the standard library is where we're spending our time. All right, so let's go back to the compute, and let's look at the source code of compute and see where compute is spending its time. 9% of the time is spent in calling the function. That's good.
04:46:57.960 - 04:48:00.452, Speaker A: You know, arguably we could inline compute. In fact, maybe that is what we should do. Initializing this array takes 1% of our execution time, if equal to correct. That's interesting. Yeah, so this, we're doing this comparison twice, and we might be able to skip that. This is where all of our time is going. There's a little bit of time here, which is going to be drop, but I don't think we allocate anything interesting in compute.
04:48:00.452 - 04:49:02.880, Speaker A: I wish there was a way to see the actual code that gets executed here, but this comparison is where all the action is happening. Interesting. Yeah, I mean, it's not that much more helpful. I'm guessing this is the pop that we saw in perf like over. Where's our report? Let's run this again and then grab some data and then show me the report. And then when I go down here and annotate compute, like I'm guessing this pop is a part of the return. I wonder why that pop is so expensive.
04:49:02.880 - 04:50:04.530, Speaker A: Feels wrong. But yeah, I feel like there's not a lot more to gain out of here. It's interesting, these assertions. Oh, we don't even need these assertions anymore because word is now fixed length v equals g and not used. Yeah, we might be able to make this not branch, which would be nice. So pop doesn't drop anything, right? Like pop is just an assembly instruction that just, I mean, it doesn't do anything, it just abstracts from the stack pointer, although it pops into a register, but not anything that should meaningfully affect our runtime. Here.
04:50:04.530 - 04:51:01.056, Speaker A: 3% in there. Interesting. Yeah, I think it's probably a lost cause to try to optimize compute further. Like it might be possible, but I'm inclined to leave it the way it is for now. Oh, you, you're right, it could be triggering page faults, but that's also going to be tricky for us to optimize here. So this suggests to me that we need to, instead of trying to do micro optimizations here, we need to try to do macro optimizations on the which patterns we consider, for example. Or we could try to make this concurrent.
04:51:01.056 - 04:55:08.350, Speaker A: Both of these would obviously speed things up. I'm inclined to try to do algorithmic optimization first and then try to do it multicore. But first I have to go take a bio break. So let's take five and then meet up again and I'll do, alright, let's see, maybe it's mis predicting, it could be there's no prediction here. In fact, if you look at the code like there isn't even an early return, this compute is extremely optimized. You know, like in fact there are no branching instructions here. Interesting.
04:55:08.350 - 04:57:13.600, Speaker A: All right, so let's go look here at something else. One thing I want to look at is I want to see the sequence of guesses, actually. Let's, so one question here is, do we want to keep this move to words over strings? It's not clear to me, actually. Like it's not, it's not faster and the strings generalize much better. Um, yeah, I think what I'll do is do a bytes, words are bytes, words use u 85 for words, not faster. And go back to master. And then one thing I want to look at here is which word does it choose? And I think actually what I want to do is go to main over here.
04:57:13.600 - 04:58:48.800, Speaker A: I want it to print tares, grand augur and cigar. I want to see what happens if I change it to not have this optimization. It's certainly a lot slower. The observation here being, of course, that the first word you guess is always the same, right? Because you start out with no information. So one game or another game makes no difference to you. In fact, it looks like it can't even generate the I. More I want to see what the first guess is if we don't hard code it, but that seems like it's hard pressed to get that to happen.
04:58:48.800 - 05:00:00.900, Speaker A: I guess I can try this that someone else suggested. This is perfect event open returned invalid. So I can't use cycles pp as someone suggested in chat. It's, it's running. It's doing something over there first. Let's go back here and look at first, see what we get out of there. Yeah, I mean, it's still spending all this time in that closure a little bit and get unchecked of an iterator of the zip.
05:00:00.900 - 05:00:49.994, Speaker A: Is it still running, really? Um, I guess we could say is better than this word. Is better than c word. Goodness. C dot. Goodness. Just because I want to see something here. Arons is better than Eonstein alert.
05:00:49.994 - 05:02:30.608, Speaker A: Is better than arrows. This is probably one place where the lack of the sigmoid is gonna come back to bite us, because it's gonna heavily favor more common words, more so than it probably should. It's interesting, though, remember how the word that was proposed in the video was tares and Aretz is the same letters, just in a different arrangement. So I wonder whether it might actually arrive at the same. There's also in Grant's videos, he uses like, a special algorithm for computing the first word, which takes into the account the expected information two steps down the line rather than one. And that's, I think he arrives at Crane, and then in the follow up video, he's like, actually it's crate, not Crane, which is appropriate because, you know, rust. Yeah, this is nothing.
05:02:30.608 - 05:03:49.400, Speaker A: This is going to take forever to finish, but, like, it's basically arriving at tears. Except arts, which is a different rearrangement of it, which is, I think, interesting. I'm going to cancel that because I don't think this is where we're going to get our performance boost from right now. But what I will do is I want to see if I run this and don't print out these and then I want to tee that into file called, let's say from tears text. I want to see what happens if we start with a different word and see what the, the scores are meaningfully better or worse. Like really, we should just be compu keeping track of the average score, right? Okay, so I ran some there and now I'm going to go back here and I'm going to change this to be crate. And this is from crate.
05:03:49.400 - 05:05:18.476, Speaker A: Oh, why is it so slow for some of those? Why is it so fast for the first ones? Definitely a lot of variability in the performance here. Alright, and now what I want to do is I want to do this and look at the difference between from tears and from crate. Yes, you'll see. Crate seems to do, it's sometimes slower, but it definitely seems to be doing better than tears does. Alright, so how about we just switch it to crate? Because, you know, I like crates tears. Great. Naive tears.
05:05:18.476 - 05:06:17.368, Speaker A: Great. And I guess we could wait, do I not have a vec? Aha, great. I want to cargo r this and then I want to hyperfine. I want to see whether that makes a difference. Just which word we start with in terms of how fast it is shouldn't really matter that much. But there are two sort of macro optimizations I want to implement. The first one is that is the observation that whether two words, whether word plus pattern or whether basically the output of the compute is deterministic based on its arguments.
05:06:17.368 - 05:07:11.490, Speaker A: So we should monomorphize as in remember the outcome for any given three tuple and just store that. In fact, we might even pre compute it because we're going to be reusing it over and over and over again for every single iteration. Whoa, it's so much slower with crate. Yeah, so I guess this is because there's like some pathological case in with crait, wherever it doesn't eliminate as many words in the first step. So it needs to do the like expensive search a second time more and there, hence the speed difference, even though it performs better. That's fascinating. All right, in that case, I'm not gonna change it to crates yet.
05:07:11.490 - 05:08:28.480, Speaker A: I'm gonna keep it as tears because I wanted to be fast. But what I am going to do is I want to copy source algorithms once init to be pre calc. Alright, so pre calc, pre calc is going to pre calculate all of the, basically all of the combinations of word, word pattern. Whether that's feasible is going to be interesting to see. It might not be pre calc, precalc pre calc. Alright, so here is the way this is going to work. Sorry, I meant memoise.
05:08:28.480 - 05:09:15.188, Speaker A: Yeah, you're right. Not monomorphize. You're also not doing the waiting to try to get the actual guess yet. You're just trying to get down to one candidate. I mean, it is still weighted by the probability of the word. So it should still, the probability of the word should still matter. Yeah, because if you have two equal options, if you have two options left where both of those two options are actually.
05:09:15.188 - 05:10:42.590, Speaker A: Ooh, that reminds me of another optimization we can do if you have, I have too many thoughts in my head at once. If you have two possible candidate words, then both of them are going to have, are going to be computed as the sum of the probability of getting a pattern by the inverse of that probability. Oh, I see. Which doesn't take into account the probability of the current word, it only takes into account the probability of the pattern. Yeah, you're right. Do I want to do that one before pre calc? Alright, let me, let me leave a to do here so that we remember to do it, which is goodness. So that's going to be the goodness here, weigh this by p word.
05:10:42.590 - 05:11:19.528, Speaker A: So that's a to do. The other thing I thought of was over here, which is to do, don't consider correctness patterns that had no candidates. That's in the previous iteration. Right. Because why would you. There's no reason to. But let's try pre calculating here.
05:11:19.528 - 05:12:52.700, Speaker A: So this is going to be match or something. And it's going to be, how are we going to represent this? This is, and in fact we don't actually need the word list, right. We just need the number. So this is really going to be a sort of, maybe it's a hash map and it's a map from a static STR and another static STR and a correctness five to use size to f 64 m to use size. The sum of their weights, I think is the value we care about here. And how big is that? That's going to be humongous. So we're going to go down here and instead of here calling matches, we're going to do matches is match get or in it.
05:12:52.700 - 05:15:12.358, Speaker A: And then we're just going to say matches, dot in pattern, total plus equals matches, get, word, candidate, pattern, unwrap or zero. And then this is going to move up to where we compute it. Now we have to be careful here. There's an additional optimization we can do here, right, which is, this is a matrix that is symmetrical where if we compute, you know, the, this is the same observation as why we can use correctness computation for matching, which is if, you know, the value of abyss versus words is going to be the same correctness as words versus abyss. So we only need to compute the sort of upper left triangle of this, as long as we always look up the words in the same order. So if we're going to say if word is less than candidate, so the key is going to be if word is less than candidate, then word candidate, otherwise candidate word pattern, pattern. And this is going to be key, why? But its trade bounds were not.
05:15:12.358 - 05:16:38.940, Speaker A: Oh, they need to implement hash, which correctness does not currently do. There's a chance that this is slower because of the hashing. Would it make sense to only make the memoization for common words? Maybe a word is cal copied. So here we're going to do for every word. It's true. I mean, we don't need to have all of the entries in here, but like why not for word one in initial get forward to in initial doc get. And then we could do something like here, if word two is less than word one, then break.
05:16:38.940 - 05:18:35.060, Speaker A: And then we're going to have this compute, let mute out is hash map new borrowed word one mask is going to be pattern. So this also then has to be for pattern in correctness patterns. And then we're going to do, oh, I guess actually this is just a, it's a bool, it's not a u size. What am I talking about? And then we're going to do out insert word one, word 2g, matches. And then this is going to be, if matches unwrap in pattern, total plus equals count. And then this returns out. We know that initial has been initialized and we're going to insert here word one, word two pattern.
05:18:35.060 - 05:19:45.810, Speaker A: Of course we can even tell it how large this is going to be. This is going to be let words is initial get unwrap. Let patterns is equal to correctness patterns. And this is going to be words len times words len times patterns, count divided by two, just to avoid the. Yeah, it's true that there, there are a lot of uncommon words that we end up populating here, and there might be an optimization where we don't have to pre compute them. But I just want to see what the, what the outcome of this is like, whether this is even feasible. I'm not even sure.
05:19:45.810 - 05:20:29.822, Speaker A: Pre calc. Oh, did I do something silly? I did. This has to be pre calc. Precalc and pre calc. Memory allocation of that many bytes failed. Okay, so in other words, we can't have a thing of all of the words, but we can probably have one be. We can probably do this if we only do the common ones.
05:20:29.822 - 05:20:51.300, Speaker A: So let's try that. Words is going to be this. And then I want to. Words sort. Bye. Key where each. I want to sort by the count.
05:20:51.300 - 05:21:37.698, Speaker A: Sort by sorts. In what order? Unstable sort by sort. Unstable by key is fine. I forget whether it sorts. I think it sorts an ascending order. So we want it to be minus. Count cannot apply unary operator minus to use size.
05:21:37.698 - 05:22:13.000, Speaker A: Unsized values cannot be negated. That's true. I guess there's also in, there's a negative, not neg. Rev. Rev camp reverse. That's the one I want. Instead of doing this, I'm going to do standard comp reverse, which is a type that reverses the ordering of whatever type you're given, like so.
05:22:13.000 - 05:23:14.270, Speaker A: And then I should be able to just say we're going to take words and we're only going to grab the first 1024. And now over here, this is not going to be unwrap. It's going to be unwrap or else. And the. Or else is going to have to compute the pattern. Just going to be here. G matches his candidate.
05:23:14.270 - 05:24:30.770, Speaker A: What does that do? So that's a smaller number, but it's still pretty large. Doesn't seem that much faster, you know, and I'm guessing it's cause you end up, you end up pruning a lot of the. Let's just check that this is, you know, somewhat reasonable that if I, if I debug print, you know, words, the first ten words, just to see that they are, in fact the most common ones. They are. I'm guessing that most of them get pruned out early on anyway. So let's do 2048 just to, you know, try something that's too much memory. All right? I mean, even 512, it might make a difference.
05:24:30.770 - 05:26:07.780, Speaker A: And then we try hyperfine for this. And we do once over ten and pre calc over ten, see if that makes a difference. Mmm. Doesn't bode well. Does not bode well at all. That is very slow. That is, in fact, much slower than I would expect it to be because I guess we are now doing this lookup, but something here seems wrong.
05:26:07.780 - 05:26:41.990, Speaker A: All right, let's grab some stats here. I still have tears here, right? Okay. Yeah. Good. Pre calc, precalc. SVG, hashmap get. Yep.
05:26:41.990 - 05:27:38.914, Speaker A: Hashing. Hashing and table lookups are real slow. I mean, this might be better with the b tree map to save us from the hashing. So now this needs to be partial ord and ward. That's fine too. 73. I don't need that anymore.
05:27:38.914 - 05:28:08.240, Speaker A: That's fine. Oh, you're right. This is the standard library hashing. But it looks like it's not even like the hashing is part of it, the. The sip hash. But even just the lookups are where a lot of the time goes. I wonder whether I actually want to see whether we end up in this case.
05:28:08.240 - 05:28:51.288, Speaker A: Echo. I just want to see if we even. Yeah, it looks like. It kind of looks like we never go in this. Yeah, we. We never succeed at. Oh, all right.
05:28:51.288 - 05:29:41.160, Speaker A: So we do use it sometimes. Interesting. But pretty rarely word candidate. Like, it's very often just not used at all, which seems really surprising. Like it didn't for cigar and rebut. Didn't use it at all here. It used it a decent amount and then not for a while.
05:29:41.160 - 05:30:49.124, Speaker A: Which makes me think that many of the popular words basically. Oh, you know why this is probably because tears already eliminates many of the popular words. That's my guess here. So I don't think this gets us very far. I don't think it's feasible to have, like, it's such a large search space and it's not clear the picking the popular ones actually matters. That's interesting. All right.
05:30:49.124 - 05:32:59.590, Speaker A: I mean, we can add this, but it doesn't seem like add slower with pre calc. The problem is it's too large. The space is too large, you know, and I don't want to start with a less good word just to satisfy this bit. Alright, so let's then copy this and use weight. So what I want to do with weight is apply a weighting of the nope. Algorithms needs to be added here too. What I want to do here is make sure that in the goodness we actually compute in the probability of this word among its, among all of the remaining candidates, which should just be a matter of saying here, p word is going to be count as f 64 divided by remaining count as f 64 and p word times that.
05:32:59.590 - 05:33:11.050, Speaker A: This is where you. We should probably have the. The sigmoid again. I just want to see what happens if I run with this. So this is. Wait. Oops.
05:33:11.050 - 05:33:35.320, Speaker A: It's not what I meant to do. Oh, that does. I see some threes. What was the previous one we had was once and that's already in from tears. And then this is going to be into. So use. Wait.
05:33:35.320 - 05:34:17.560, Speaker A: Text. I don't expect this to generally be faster, but I do expect it to do a little bit better in that it's going to say it gets some in two now, in that it should be more often trying a word that is more likely to be the answer. Like it's going to give that a little bit of an additional boost in addition to just it giving you more expected information. So basically, ghost, it tries to go for the win more often. You see this, like sometimes it goes over. In fact, we should see this for here use. Wait.
05:34:17.560 - 05:34:52.434, Speaker A: So for PI to four, four to five. Four to five. Five to four, five to three, four to three, five to four, four to three, three to two. But I wonder if we look on the left here, do we get any sevens? We don't. On the right we do get a seven. That's the word that this one hasn't tried yet. But I think generally what this is going to do is it's going to more often try a word just because it seems more likely rather than go for the one that gives it the most information, which is likely to narrow down the list of candidates further.
05:34:52.434 - 05:36:06.328, Speaker A: So it's going to be able to push to lower values, but it's also going to end up going over sometimes because it doesn't get as much information as the other one does. And we can try to be a little bit more robust about this, which is something like scores is zero, games is zero. And then here we're going to do something like games plus equals one, score plus equals s. In fact, we're always going to do games plus equals one. And then this is going to be counted as a score of plus equals. I don't know, eleven doesn't really make, I guess, fine, we won't count the games where it doesn't complete because we should be printing those out separately. And then at the end, what we'll do is print out average score as 0.2.
05:36:06.328 - 05:36:45.160, Speaker A: FDA of scores. F 64 divided by games is f 64. And now I can do Max, you know, ten, right? F is for other languages than rust. So average scores 4.3 across the first ten. And for once the average is also 4.3. All right, let's, let's run with a few more then.
05:36:45.160 - 05:37:42.510, Speaker A: In fact, because we don't care about the implementation weight, Max 30, we don't actually care about the concurrent performance here, so we might as well just run them next to each other. Maybe 30 was low, maybe I should have done more, but. All right, so that did four. Let's do maybe 64 of these and that did 3.67. So it seems to be better for, you know, some quantity of better. Why so many repetitions of word I? The output? I don't think there are repetitions of the words, are there? I don't see repetitions. So without this weighting, the average score is 4.05.
05:37:42.510 - 05:38:31.230, Speaker A: And with this is 3.75. So this is going to be that way. Go for the win more often. Alright. And then let's do one last one, maybe, which is going to be from wait to prune this one. I actually expect to help a lot. What I want to do here is not reconsider patterns that we've already eliminated.
05:38:31.230 - 05:40:16.708, Speaker A: Prune, prune, prune, prune, prune, prune, prune, prune, prune, prune and prune. So what we're gonna do now is we're gonna say the prune is also gonna have patterns, which is gonna be a vec of. Do I want it to be a vec of patterns? Yeah, let's have it be a vec of correctness five. And patterns initially is going to be a vecnue. And down here, if patterns is empty, actually, if the history is empty, then self dot patterns equals vec new. Otherwise we're going to assert that self dot patterns. No, it's not going to be vec new, it's going to be self dot patterns is correctness patterns dot collect, which arguably, arguably could be a once, because that's always going to be the same set of patterns.
05:40:16.708 - 05:41:44.260, Speaker A: So let's do with cow static of that, because why should we recompute those over and over and over again? Correct five. So this is going to be cow borrowed of patterns get or init. And that's really just correctness patterns collect. Great. So in this case, it's going to start out as cow borrowed of patterns get dot unwrap. We can unwrap because we know that it was constructed up here in new. Otherwise, just as a sanity check, there should obviously be patterns left if we are still guessing.
05:41:44.260 - 05:42:57.000, Speaker A: But what we can do is for pattern in. In fact, we can do even better here. We can say self dot patterns, dot retain pattern. This should be to mute. There's another one where I don't want to clone the whole patterns thing every time. So let's do matches. Is, does pattern match? It's going to be a closure that takes a pattern and does this stuff otherwise return true? I guess check pattern is fine.
05:42:57.000 - 05:44:09.340, Speaker A: Yeah. And then what we'll do is, I guess we'll match. We'll say if matches self dot patterns cow owned. So this is the same trick, right, where we don't want to ever copy the full vector and then trim it down. We want to start it out with the initial, sort of seeding it with the initial set. So if it's already owned, then we're going to do self dot patterns, dot retain, check pattern. Otherwise we're going to self dot patterns.
05:44:09.340 - 05:45:38.040, Speaker A: Is cow owned of self dot patterns, dot it, er, filter check pattern collected pattern here should be a correctness five, which it won't be because, because it requires, it expects a reference. That's fine. We can take a reference here. We can take a reference here. This iter dot copied dot filter cannot oh patterns to mute which we know it won't do anything because it's already owned, right? So now we're using weight as the comparison point. And I want to try that against prune. Whoa.
05:45:38.040 - 05:47:12.452, Speaker A: I think that made a difference, wouldn't you say? All right, so let's tee this into, wait, txt and then we're over here, we're going to do prune tproon txt just to see that they actually get the same set of results. Now we're talking. And then I want to diff, wait, and what did I just make? Prune. It's disturbing that they're not the same. That's a little disturbing to me. Like why, why are they different? Like it does slightly worse, but it unstable sort. But do I even do a sort here? Like I don't, I'm not sorting like there's nothing non deterministic here.
05:47:12.452 - 05:48:21.140, Speaker A: There's no hash. Like there's no, there's no hash here. There, this just walks the games in order. I mean, the dictionary is a hash set, but that's just used to check whether an entry shows up. So why is this not deterministic? That's extremely disturbing. Right? Sol's algorithms, weights, I don't understand. There's no difference.
05:48:21.140 - 05:49:29.860, Speaker A: I don't understand. Why is this execution not totally deterministic? Retains should be deterministic. Hopefully floating point operations are deterministic. I have no idea. That's so weird. But I, okay, well maybe patterns isn't. Why wouldn't it be though? That just iterates over these in order that allows guesses.
05:49:29.860 - 05:50:29.962, Speaker A: I'm gonna just ignore this problem. Ha ha, he says, and I'm just gonna compare these. I have no warmup because I don't believe in warmups. Wait, 32 and prune, 32, but there's no hashing of any note here. The only hashing is in the dictionary, and the dictionary is just used to check whether a word is legal or not. It returns a boolean. It should be deterministic, different number of remaining.
05:50:29.962 - 05:51:07.050, Speaker A: They all start with the same word and they prune using the same function. Wow. Well, I think that pruning helped. It's almost 60 times faster. Yeah, right. Source algorithms, prune, I like that one. Add a mechanism for pruning patterns.
05:51:07.050 - 05:52:24.300, Speaker A: This brings a, that's a very, very large speed up. How do you handle candidates with the same score? If they have the same score, then the first candidate should be chosen and the candidates we walk remaining in order. Remaining is a vector read from the dictionary. So like there's no unstable sorting except in pre calc, which we're not using. I mean, I am discarding patterns as I loop through, but I'm only discarding patterns that are empty. You know, there's no iteration of hash sets. The hash that is only used to look up whether a word is legal.
05:52:24.300 - 05:53:35.816, Speaker A: Well, I mean, it also doesn't fail on any games. No prune. So like, also, to be clear, this is why I think it makes sense to optimize the single threaded one first, because we might not have realized that we could make this modification until if we're in the concurrent setting. Okay, all right, I wanted, I want to do one more. All right, so we're going to go from prune to, there's like the whole sigmoid here. Oh, actually I want to see what happens if I switch from tears to crate. Now, can it actually run with crate? So previous ones, like 3.9
05:53:35.816 - 05:55:10.972, Speaker A: something, it's slower when it starts with crate. Aren't you calculating probability based on remaining? Yeah, but I'm only pruning patterns that have nothing remaining in them, so they shouldn't affect the overall account. Anyway, starting with Craig did not change the score. Is interesting, but it is slower. Why, I wonder? Well, I mean, there are all sorts of reasons why that might be. Also we can't be croupped pruning any patterns that has nonzero matches or it seems unlikely that we are because otherwise we would be eliminating categories that still had words in them, which should mean that we just fail to resolve at times. It might be possible for us to check this.
05:55:10.972 - 05:56:16.820, Speaker A: Bye. Doing main. If we just have, we could have this just iterate over the entire dictionary rather than just the answers and we should make sure that we solve all of them. Probability is floating point based on remaining count, which is now different. The remaining count shouldn't be different. We're not pruning self remaining, we're pruning the patterns that we consider. And we only prune a pattern once it has no words in it, at which point the count there is zero, so it doesn't affect the count at all, so it shouldn't make a difference.
05:56:16.820 - 05:57:38.480, Speaker A: Please. I don't see how it does. So what I wanted to do was copy prune to cut off. And this one I'm not entirely sure of, but I have this intuition that, like, in terms of choosing what to guess, I wonder if there's a way for us to say we're only going to compute the probabilities for the most probable, like, you know, n over two most probable words we have left, rather than computing the probability for all of them and then picking. And the idea being that a word that's super unlikely, we don't even want to check what the expected information is for it, because it's unlikely that it's going to be high enough to make up for how much more probable a given word is. So here I am going to introduce on the stability, let's see, which is going to be cut off, and this is going to be the last one. And then I have to go eat.
05:57:38.480 - 05:58:12.450, Speaker A: Oh, yeah. There are also lots of tricks we could play with correctness, for example, by using bit fields and stuff. But that seemed. Seemed excessive. All right, so here's the idea. The basic idea is going to be let mute words. It's going to be that.
05:58:12.450 - 05:58:47.310, Speaker A: And we're going to do words sort unstable by key wor count, reverse of count. So we're going to sort it so that the most frequently. The most frequently used words are first and retain retains that order. Right. Because it just. It just removes things in place. It doesn't shift anything around.
05:58:47.310 - 06:00:30.830, Speaker A: And then my thinking here is that I want, instead of this being that, I want it to be iter take, honestly, maybe just the first 50. You know, like, it seems unlikely that a word that's, well, maybe 50 is bad, but let's do n self dot remaining len, and let's do n over two. And I want to do hyperfine of prune versus cutoff. Ooh, that failed on line 113. All right, fine. So we're going to do stop. And instead of doing this, we have to stop after we've gotten to at least that many candidates.
06:00:30.830 - 06:01:24.726, Speaker A: That's me being silly, which is here I plus equals one. If I is greater than stop, then break. All right, so that's half. And what I want to do now is, did I already compute for prune. Maybe I didn't. 128. Prune.
06:01:24.726 - 06:01:57.560, Speaker A: Txt. I guess the average score is all I really need. Sure, let's just run that, I guess. And then I want to run implementation cutoff. Just gonna let them both run. And I want to see if they get the same score. 395, 394.
06:01:57.560 - 06:02:25.936, Speaker A: Okay, so they're basically the same. And now I want to see if I make stop be. Oh, why did I set that to the same length? Divide by two. That looks faster, but I'm not sure I believe it. Yes. So see how much that saves. And now the question is.
06:02:25.936 - 06:02:54.470, Speaker A: And the score is the same. In fact, I bet you we can do, like, way better than this. I bet we could. I bet we could hard code this to, like, ten and have it not even make a difference. Oh, interesting. So reducing to ten didn't really make much of a difference compared to just having, you know, like, 149, 148. They're basically about the same.
06:02:54.470 - 06:03:42.980, Speaker A: What if I do like 65 four or something? They're fast enough anyway, so that is 154. And if I instead say, cut it off, a ten. Okay, so half gets us, like, most of the way there. Yeah. So one example this came up in chat. It's possible that the length of remaining is generally less than whatever number I said, like, less than ten. So it doesn't really make a difference.
06:03:42.980 - 06:04:31.108, Speaker A: You see, the score it achieved is the same. 394, which I think half then is a good estimate to start with. But I actually want this to be. This should probably be something like Max ten or something, where if we have very few candidates left, we probably want to consider all of them. Cargo r don't want it to include that. Yeah. So it's still faster.
06:04:31.108 - 06:05:02.310, Speaker A: It's not quite as much faster, but I worry about it when it gets down to, like, there are only ten more options. I wanted to carefully consider all ten options rather than just, you know, going with the five most likely. I don't want. I don't want it to do this. Halving all the way down and maybe 100 is too much, you know? But I don't really know where to start here. Like, you know, always can when you. Whenever you get below 20, consider all of them or something.
06:05:02.310 - 06:06:33.474, Speaker A: Nice. Don't search unlikely candidates. This brings a speed up a nice. So I think we're now at, like, several, like, probably two or maybe even three orders of magnitude faster than the solution we started with. We still haven't done the sigmoid, but it's interesting because the sigmoid should only improve the quality of our guesses, not the speed. I wonder, is there, like, a cheap way to approximate a sigmoid? Someone mentioned ten h, which, like, maybe is like, if I do, if I just do, you know, ten h stat. 394.
06:06:33.474 - 06:07:06.660, Speaker A: Okay, so the. The tan age did nothing. That's fine. Try remaining length divided by three. Well, in that case, we can just check. Yeah, I guess we can check. The same thing divided by three is faster.
06:07:06.660 - 06:07:43.870, Speaker A: That's fine. I'll do that. There's a trade off here, right? The fewer candidates you consider, the more likely you are to discard a candidate that would actually have been better than the ones you chose. 32 3200 x speed up. Sounds about right. Okay, I think that's where we're going to stop today. We're at a bit over 6 hours, which seems like enough.
06:07:43.870 - 06:08:11.586, Speaker A: Enough coding time. I'm going to push this somewhere. I'll push it to GitHub, and then I'll tweet out the link, I suppose. And, like, by all means, try to make this faster. I don't think this is something I'm going to maintain over the long period of time. This is more. I thought it was an interesting exercise in implementing an algorithm like writing performant rust.
06:08:11.586 - 06:08:49.840, Speaker A: That looks pretty fun. All right, with that, I think we're done. Are there any sort of last minute questions before we end? Anything you're curious about or would like to see? All right, in that case, thank you all for watching. I hope you learned something. I hope you found it interesting. And otherwise, I will see you in another stream. Bye, everyone.
06:08:49.840 - 06:08:51.100, Speaker A: Bye.
