00:00:01.920 - 00:00:45.835, Speaker A: Hi everyone. Welcome back to yet another I guess, I guess we might as well just start calling them Impel Rust streams because that's what everyone is calling them now anyway. Which I guess means I need an intro for them. I need like an impul Rust intro where I don't know what the crust of Rust was easy to make an intro for because Jay just eats the eats a pie. I don't know what Impel Rust is going to be, but regardless, this is the fourth installment and I'm going to claim that this is the last installment of the stream where we port the implementation of hazard pointers from C, specifically from the Folly library to Rust. I'm not going to go over what hazard pointers are and everything in this stream. We're pretty far along.
00:00:45.835 - 00:02:04.655, Speaker A: But the reason I say that this is going to be the final part, and this is related to the tweet I made about this in the first place, is that I think we've gotten so deep now that for anyone who's not interested in hazard pointers, they're like, okay, we need to do another stream that I'm actually interested in following. And also there's like diminishing returns from this particular stream. Now because this is a topic that we've sort of covered so much ground on, I wanted to do one more because there's some stuff around testing and benchmarking that I think it's going to be worthwhile to cover. I'm hoping it won't be that long. Like I'm guessing this will be about a three hour stream to cover those bits because mostly it's going to be porting the tests and benchmarks from Folly and then probably try to adapt them to be a little more Rust. Like so in particular, we're probably going to make some of the tests be represented using Loom so that we get the sort of more exhaustive checking rather than the probably more spin testing that's in the C implementation. And probably find, if we find benchmarks, which I'm pretty sure in Folly, we're going to probably port those to Criterion, which is this really nice benchmarking library in Rust that gives us statistical significance for results and that kind of stuff.
00:02:04.655 - 00:03:08.351, Speaker A: So I think that's going to be interesting. And then even if at the end of today's stream we're not quite done with those things, or even if we are, whatever is left. I'm probably just going to do sort of on my own time asynchronously to make this be a library that people can actually pick up and use. I want to cover just before we dive in. There have been a couple of changes since the last stream that people have submitted by prs that I think are worth talking about, so I'm going to bring those up real quick. So the first one, this hasn't landed yet. This is a if you remember from the previous stream we have this sharding operation that we use in order to One of the things we ported in part three was this split of having the to avoid contention as much on the list of free hazard pointers, we sharded that list and then you look in like the appropriate sharded list.
00:03:08.351 - 00:03:35.845, Speaker A: The sharding function we used there was a little naive and someone's implemented a much better one. It's unclear whether it's worth the trade off. There's another PR over here, so this is PR number 12. It's really exciting. There's some interesting discussion here if you're interested in this topic. I don't think it's going to land quite yet because we need proper benchmarks to see whether it's worth it, and that's where today's stream comes in. But it is a neat change that's worth looking at.
00:03:35.845 - 00:04:37.205, Speaker A: The other is a PR of three different fixes and a feature that all landed sort of at the same time. One was it turned out there was an error in how we updated the Next and previous free pointers when we introduce this linked list of hazard pointers that we could walk for the free list. The second is that we wouldn't correctly link the list that we returned from allocate. When you batch allocate a bunch of hazard pointers, those weren't properly relinked before they were returned. Finally there was a head assignment that was wrong in a loop and all of those have been fixed now and have also gotten tests added. So that's really nice. And as part of that PR we also got a sort of this hazard pointer array implementation or feature that Folly has, which is this I want to allocate four hazard pointers at once.
00:04:37.205 - 00:05:21.375, Speaker A: We now have an API for that in haphazard. It's pretty straightforward really. It just means that you steal sort of a chunk of things from the free list at once rather than steal them one and then one and then one and then one which lowers the override amortizes the cost of getting these hazard pointers in the first place. It's a fairly simple API. It uses CONST generics in order to give you back an array of the size that you specified, which is really neat. We also landed no std support. This is a really cool PR2 of just figuring out what this should look like if you only have alloc.
00:05:21.375 - 00:06:08.187, Speaker A: Mostly this was straightforward. There were only really, I think two big caveats to no STD support. The first one is that we can't use hashset. Remember how we walk all of the addresses that are currently guarded by a hazard pointer and we stick them into a hash set and then we walk all of the sort of things that have been freed and check whether any of them are guarded. So we use a set for that and we were using a hash set. We've now switched to a B tree set because hash set depends on hashmap and hashmap depends on the standard library because it relies on the hashing primitive, which. No, it relies on.
00:06:08.187 - 00:07:08.953, Speaker A: Yeah, it relies on the denial of service resistant hashing that is in the standard library, which depends on randomness, which depends on the operating system. There are ways to work around this. Like we could, if we wanted to work sort of harder to use hashset here through something like hashbrown and then use maybe a hash for the hashing and then just not have collision resistant hashing. It seemed fine to just switch to B tree set here anyway. Doesn't cost us that much. The other thing that is a little tricky is if you remember from last time, we have this sort of batch deallocation mechanism in haphazard and we inherit this from folly where when you freeze something, it doesn't necessarily get reclaimed right away. Instead what we do is we stick them on this free list and every time something is freed, we go, or sometimes when things are freed, we go and check the free list and see if we can reclaim a bunch of items.
00:07:08.953 - 00:08:02.995, Speaker A: And one of the ways in which we determine that it's time to go do that is if a certain amount of time has passed since the last time we did it. The idea being that we want to give guarantees both in space and in time for how long we can hold on to garbage. Like if there's a certain amount of garbage, we want to reclaim that garbage. Or if a certain amount of time has passed, we want to reclaim that garbage. Otherwise that memory would just sort of potentially sit there forever and never get deallocated if more garbage wasn't allocated. But in a no STD environment, of course, you don't have time, you don't have a notion of time. So the question becomes, well, what do you do about that? And the answer is we basically just disable that feature and say that if you're in the no standard mode, then we only do collection or reclamation based on the number of items that are in the trash and not how long it's been.
00:08:02.995 - 00:08:32.815, Speaker A: We might be able to improve this. So you could imagine using things like the RT DSC real time synchronized clock. It's. I think it's rtdsc, which is like the CPU instruction for getting the current time. I think there's a crate called quanta that uses that directly. So we might be able to find some way to reintroduce the notion of time here. But it seemed not worth blocking this PR on.
00:08:32.815 - 00:08:59.643, Speaker A: There's a. There's a third, smaller piece here too, which is. There's at least one point in haphazard. I think there are two where we have a sort of spin loop where if we detect that someone else is also collecting garbage at the same time, we will yield and then check again to see if they completed. Of course, yielding is an operating system primitive. It's not something that the RAW hardware supports. So we had to figure out what to do with that.
00:08:59.643 - 00:09:58.725, Speaker A: We ended up using the standard hint spin loop method, which is a way to instruct the cpu. This is a spin loop, so you might be able to run another hyper thread or something. It's not perfect, but it's the closest approximation that we have. That was neat. That's landed. This was a neat one that I actually completely missed when we were reading the folly code, which is that the CompareExchange function from the C standard library, which is what Foley uses, has this mechanism where if a CompareExchange fails, remember how you pass in what you think the current value is what you want to replace the value with. If the current value is the same as the previously observed current value, then CompareExchange will either tell you, OK, I made the swap, or no, I didn't make the swap.
00:09:58.725 - 00:10:32.845, Speaker A: Here's the value that was there instead of the one you. You expect it to be there in C, it will actually update the value that you passed in. As this is what I expect the current value to be, it will update its first argument. Whereas in Rust it just returns that in the sort of error case. And that means that you can see this in the diff, actually that we miss this case of we need to update our current value before we loop again in C. This happens for you. That's why there was no code to do this in the C code, because it wasn't necessary.
00:10:32.845 - 00:11:09.487, Speaker A: But then that means we missed it in the port, so that's now been fixed. And we also looked through all the other compare exchange examples and made sure that they actually match the expected semantics. So that's where we're at. That's sort of what's happened since last time. I did also go ahead and look at the history of the sort of commit history of folly in the synchronization folder and the last. The commit from the last stream is this one, and after that there aren't. I looked through sort of briefly.
00:11:09.487 - 00:11:52.575, Speaker A: There are no commits around hazard pointers except this one, which I don't think is relevant to us. This is for the thread local implementation which we have not ported, so that commit is irrelevant and all the later commits are to other bits of synchronization, so they don't actually matter for our implementation. So luckily there's nothing to sort of forward port. There are no known bug fixes or anything as far as I can tell. All right, so one more thing before we dive into the test, which I have open here is I want to talk briefly about the survey because it was really fun. I don't know if you can see the survey results. I guess not.
00:11:52.575 - 00:12:29.731, Speaker A: So this is the survey results for me, asking whether people wanted a Part four. And the results were fairly unhelpful, but made me do this one last stream, which is like 40% of you wanted another one. Only 3% of you actually care about the library, which is fine. I mean, that's still a decent number of people given the number of votes. So that incentivizes me to actually get the library out there. Like a third of you want to just do something else instead, which is why this will be the last part. And then somehow a quarter of you just like answering polls and don't actually care.
00:12:29.731 - 00:13:06.567, Speaker A: So I thought that was a fun result to share. All right, so the I'll send the haphazard repo link in chat just so that you all have it. I'll also send the. Sorry about that. The link to the hazard pointer C standardization proposal so that you all have that for reference to. Oh, I see what's happening. So some people in chat are observing that I'm on the work side of my office.
00:13:06.567 - 00:13:39.643, Speaker A: I'm not actually. I realized last time that my video was flipped, so I realized it's because I held up a copy of my book and everyone's was like, it's backwards. And so I have flipped the video, but I am sitting in the same spot. So we're all good. Let's see. Great. Okay, so now that that's out of the way, let's go ahead and pull up the code here.
00:13:39.643 - 00:13:58.945, Speaker A: I have a diff. Why do I have a diff? Oh, yeah. This is an observation I had separately. I don't think we're going to go into that right now. So in our Tests directory, we have the loom tests that we've written. We have a couple of those. We might expand some of those.
00:13:58.945 - 00:14:27.215, Speaker A: Today we have the Lib test, which arguably should get a better name. What I want to do is copy Test Lib to Test Folly. And the reason I want to do this is because. Great. Is because I want to. This is the same argument for why we've been. When we've been porting, we've been using the same names for things and try to keep the code organized.
00:14:27.215 - 00:15:11.339, Speaker A: Roughly the same way, have the sort of same functions as the C code does is because it's much easier to then sort of forward port bug fixes and features that land in Folly into this and in theory, the other way around. Right. If we discover that there are problems or performance enhancements that we can implement, it should be easier to figure out where in Folly those could be contributed back to. And I want to do the same for the test so that it's very clear how our test suite maps onto theirs. Now, the hope, of course, is that our test suite becomes more expansive through things like Loom. But at the very least, the tests that they have, I want us to have represented in one place where we can easily look at the comparison point between the two. So let me just see that.
00:15:11.339 - 00:15:22.439, Speaker A: Cargo test works. Cargo test works. Great. So we have that. And if I do test Folly. Great. Zero tests.
00:15:22.439 - 00:15:43.585, Speaker A: Okay. That's where we want to start. Are there any questions before we begin? Let me make this larger, actually, because how wide are their lines? We can zoom more. I think that's the limit. Great. That's why line wrapping still matters, folks. Let's use zoom.
00:15:43.585 - 00:16:09.001, Speaker A: I'm doing. I think that was covered extensively in the Q and A video from. Is it last weekend? Two weeks ago. All right, let's see here. Define. Oh, I see. So.
00:16:09.001 - 00:16:33.283, Speaker A: So they have. It looks like they have benchmarks in here, too. The benchmarks. Let's do the. I guess we'll look through and see what they have the most of. But my intuition here is that we should do the tests first because the benchmarks were more likely to want to iterate on. Oh, yeah, this is a VIM thing.
00:16:33.283 - 00:17:01.745, Speaker A: I do a lot is instead of doing like D G for Delete to end a file. I often just do like 1000 dd or 10,000 dd. I don't know why. I think it's because I can Never remember whether shift G or G.G. are bottom of file. And so it's faster to just use the thing that I know will work. But a Vim Pro would just use D capital G.
00:17:01.745 - 00:17:46.671, Speaker A: It's funny actually. So looking just at the top of the test here, remember how when we wrote our test we made this type called count drops and lo and behold they have a count class that counts the number of destructor calls. So I'm guessing we might find this helpful. They also have ink constructors, which is a little interesting. I guess that's because they have a move constructor which we don't. Let's take a look here. Node.
00:17:46.671 - 00:18:29.835, Speaker A: I wonder why they have. I see they're probably going to implement some of these on top of a linked list. So if you remember, the Folly library has support for working with linked data types where a given object might have children and if you deallocate the object you also retire all of its children. And they probably have tests that sort of test that child recursion, which we don't have an implementation for. So I think that will be probably less relevant for us. Although we might want a data type like this in order to sort of. This is basically a test of the basic.
00:18:29.835 - 00:18:54.365, Speaker A: They need a data structure to test with. Right. Where they can do retires and stuff. So we may want to do something similar. Construct a list of a given size. This just creates a giant linked list. The allocation of a linked list.
00:18:54.365 - 00:19:34.495, Speaker A: Yeah, helpers for walking the linked list. So a lot of this is just data structures that they're going to test over. Basic object test. Oh, I see what's going on here. Okay, so they have a count type, but the count type is really just a. It's just a static global that they use to keep track of things. We probably don't want to do this because it means you can't run the tests in parallel.
00:19:34.495 - 00:20:08.407, Speaker A: Right. If you have a static global like this, then running tests in parallel won't work because they will all increment and decrement separately. So you can't reliably check the results. But if you look at it, Node, for example, calls inc ctors whenever a node is created on this static sort of stats keeper, basically. So I'm going to guess that if we look at the test down here. Let me highlight this line. If you look at the test on here.
00:20:08.407 - 00:20:50.245, Speaker A: Yeah. It clears the counters, it allocates and immediately retires the object. It does the same for this node rc. It acquire, link safe and unlink. I see. And I'm going to guess here that unlink on node RC is like a. Where's node rc? It's up here somewhere.
00:20:50.245 - 00:21:20.735, Speaker A: Node rc. I don't see an unlink. It might be that they inherit. Oh, they probably inherit this from here. This is going to be a little bit annoying because they rely a lot on the sort of base class they have that provides these additional functionalities. It's not going to be too painful, though. I'm just wondering if they have an unlink method directly on the list or where they get that from.
00:21:20.735 - 00:22:20.177, Speaker A: Acquirelink, save. Okay, so these are really just a bunch of create and then maybe acquire, maybe not, and then immediately reclaim, and then at the end, check that the number of constructors match what you expect and the number of destructors is the same and what you expect. But notice that because this is using the static, you couldn't run two of this test at the same time. And you also couldn't run any of these other tests that also rely on C underscore at the same time, because if you did, let's see if there's anything else here that checks C underscore. Yeah, this test also calls like clear, for example, on the stats, so that wouldn't work. All right, I'm a little tempted to skip this first one. I guess we can do it.
00:22:20.177 - 00:22:44.431, Speaker A: I guess we might as well just add this. We maybe we should just port this whole linked list so that we have it, because it sounds like it's something they're going to use in a bunch of their tests. The counters are atomic. It's not the atomics that are the problem. The problem is that imagine if you run two tests in parallel. One calls like increment ctor's. That's atomic.
00:22:44.431 - 00:23:23.591, Speaker A: One calls clear, which is going to undo that increment. But then this test progresses and expects that number to be one, but it's been cleared, so it's zero, so you can't run them in parallel. Yeah, this. This business over here smells an awful lot like they are. Oh, no. Why does the search bar go away when you zoom in? Let's see where that comes from. Yeah, so that comes from.
00:23:23.591 - 00:24:14.235, Speaker A: This has pointer, object, linked type, which is something we haven't implemented. Yeah, which is like link counted objects to automatic retirement. This is stuff that we haven't implemented in ours, which is what makes me hesitant to implement that ourselves, because I would rather port those tests when we actually have the necessary features, which is what makes me want to skip that first test because it seems like it specifically relies on this sort of linked property. But we could maybe do like the first two, I guess, just so we have the basis for the test. So let's go over here and do. Just to sort of get into the rhythm of it. It's a basic objects test and we don't need the test suffix because we know it's a test.
00:24:14.235 - 00:24:59.195, Speaker A: And do they actually use the generics here? They might. It's a little un. Clear, but they might be. So the question is, how are we going to do the equivalent of what they're doing for constructors and destructors? I guess we can do this. We can have. All right, let's go ahead and do the same basic structure that they have, which is. I really don't want to do it that way.
00:24:59.195 - 00:25:56.211, Speaker A: Let's do struct stats and it's going to have an atomic use. And I guess we can call it count because that's what they call it just for and retires. And then I think what we'll actually do here is say that node is going to have a static reference to account. And the way we're going to do this is probably with box leak. We're just going to leak a counter per test. And that way each test has its own, but we still produce static values and they're so tiny that it doesn't really matter to us. Leaking these like it's not.
00:25:56.211 - 00:26:25.023, Speaker A: It doesn't actually constitute a problem. It would be a problem in something like a benchmark where you might run it like millions of times. But realistically, for. For these tests, I think. I think realistically these aren't gonna. These allocations aren't going to matter. All right, So I guess this means.
00:26:25.023 - 00:27:38.215, Speaker A: Well, what else do they have in node? Is it just a singly linked or a doubly linked list node up here is value and next and value is a usize and next is a. A tomic pointer to a node third to a node. And what else do we got? So there's imple node fnu, which takes a value and returns itself. That's easy enough. And it also takes, I guess, a next, which is a starmute node. And it takes a bool which is equal to false. Why does it take a boolean? Takes a boolean that's equal to false with no name odd.
00:27:38.215 - 00:28:45.935, Speaker A: That's fine. We don't really care about that. We can leave that off. And then this is going to do, I guess it also has to take the count static count and it's going to create a self where the count is there, the VAL is there, and next is in a atomic pointer new of next. And we're also going to do count ctors fetch add one and we're going to do why on earth does my tab to complete not work anymore? That makes me sad. And I guess we'll implement a drop for node 2 just because we want self count detours. Fetch, fetch, add.
00:28:45.935 - 00:29:29.457, Speaker A: What else do they have on node? I guess they have a value. They have a next and so I guess we can provide next which takes itself and returns. You know, a. I'm hesitant. I feel like maybe this should be const. This is. This is going to be self.next.
00:29:29.457 - 00:30:17.155, Speaker A: load ordering acquire atomic pointer. New requires a mute, so I guess it needs to be muted. That's fine. All right, so we got new and next and I guess we might as well have VAL on here too, right? So value of. Oh, this is the problem of having a different keyboard for work and for home is that I keep putting my finger in the wrong place. Great. So now we have this basic objects.
00:30:17.155 - 00:31:09.585, Speaker A: So we're going to allocate account and I guess here we're actually going to do a derived default and debug. So count is going to be count default. And actually I think what I want here is impl count. And we're going to have something like a static which returns a tick static count. And it's going to be a box leak of self default. So box leak will write box new box leak box new. And it can't be static, so it should be global.
00:31:09.585 - 00:31:36.279, Speaker A: It's not global. It is. I mean, it's static. It's test or test local. Maybe we could use a thread local here actually. But given that, I expect these tests will end up being multithreaded, we're going to stick with this instead. So now count.
00:31:36.279 - 00:32:09.615, Speaker A: This is going to be count test local and then let's see what they actually have in this test. Down here they have a clear which we don't need. We just create a new one for each test. They have a mute num is zero Then they do in a scope they do num plus equals one. That seems fine. They do let object is node new and they do retire. Right.
00:32:09.615 - 00:32:45.097, Speaker A: So here we're good. We got to think back to what our API is, which is. I mean, we can probably use the global domain here, but let's have a domain per test too. That seems pretty reasonable. And. Right, so remember, our API is still a little bit ugly here in that you need to sort of. The API is using an atomic pointer and you need to use this hazard pointer object wrapper.
00:32:45.097 - 00:33:39.535, Speaker A: Actually, I think you might. We might not even need this, um, given that we're not inserting it into a data structure. So I'm just going to do this. Right. So the first element is the domain you want to make an object within sort of guarded by, and the other one is the object you actually want to make. And node new here is going to be count and they're probably using zero and I guess the value doesn't really matter here and there's no next. And then they immediately call X retire.
00:33:39.535 - 00:34:27.575, Speaker A: And I think. Right, and we need to use haphazard. This complains because it's unsafe. Right. So this is the reason why we need the whole atomic pointer bit. So if I do this, that should do it. Really.
00:34:27.575 - 00:35:05.975, Speaker A: A deleter. Right. And the deleter here is going to be where's our retire is this business? Yeah. The API is not very ergonomic here, which does make me sad, but it sort of has to be. So X retire. I don't think we need this to be an atomic pointer though. I think it just needs to be a raw pointer.
00:35:05.975 - 00:35:32.563, Speaker A: Right. So the interface here is we allocate a new node. That's fine. We have to wrap it with this hazard pointer object wrapper. I would like to get rid of this wrapper here. But what it does is just associate the object with a pointer back to the domain so that when we later retire the object, it knows what domain it should use to retire itself and what domain is going to reclaim it. Rather than that having to be passed in every time you retire.
00:35:32.563 - 00:35:54.747, Speaker A: Which is sort of more error prone. Right. You'd rather just have the object. No. And then we have to turn it into a raw pointer because otherwise if we just kept this, then when we drop it, it's going to be just sort of dropped normally. It was never shared in the first place. So we create a raw pointer and that's what the API operates on too.
00:35:54.747 - 00:36:42.341, Speaker A: Right. It only operates on raw pointers because it doesn't know whether it was allocated through a box or an arc or what. Maybe we should make this API be sort of require box. Or maybe we should make an API where there's like a simpler version of the API where it assumes that everything is boxed. That way you wouldn't need to pass which deleter to use because we would always assume that it's Dropbox and you could probably give the box directly. But the reason we use raw pointers here first and foremost is because we're assuming this is going to be used in a library that uses atomic pointers in various places because it needs to swap pointers for the data structure concurrent operations. And that means that all you will ever have are raw pointers.
00:36:42.341 - 00:37:17.245, Speaker A: And therefore that's what the API has to operate on too. And it also couldn't be a box here because then when the box goes out of scope, it's deallocated, but it might be shared. That's sort of the whole purpose of having hacker pointers in the first place. So it needs to be a raw pointer. All right, so that is. Let's just see that this first bit works. So now we should be able to do assert eek.
00:37:17.245 - 00:38:17.105, Speaker A: Actually, I guess we can make a sort of some convenience methods on here. Like ctors is going to be a usize and that's going to be self ctor's. Actually, what did they use for the load? I'm guessing it's an acquire load, but I want to make sure it's just a dot load. What is the C atomic default load ordering? There should never be a default load ordering. That's a bad idea load. It defaults to sequentially consistent. Okay, so that means this is an ordering const and same with detours.
00:38:17.105 - 00:39:15.095, Speaker A: And then here we should do asserteek num with count ctor's. Right? So we have registered as many objects allocated as we expect and then they call where is it has pointers cleanup. That's interesting. I forget whether we actually expose something like that. Do we have a cleanup on domain? Because if not, we probably should. We don't have a has pointers cleanup. So let's go and figure out how they have implemented theirs has pointer cleanup.
00:39:15.095 - 00:40:10.407, Speaker A: That's fine. That's just what it forwards. What does it forward to? Haspointer domain domain.cleanup. do we really not have a cleanup method in domain, for example? That's interesting. I wonder why we didn't implement that. Because that seems extremely useful. You know, I mean, let's.
00:40:10.407 - 00:41:20.545, Speaker A: Let's see what happens. Like if I do PUBFN cleanup, what do we do in the other ones? Like in the. In the other tests we have, we called it Eager Reclaimed. I feel like Eager Reclaim is not a real method. I think that we probably shouldn't have made Eager Reclaim, should have called it cleanup and should have implemented it the same way as here, which is bulk and reclaims fetch add one, do reclamation zero. Ah, we don't have this wait for zero bulk reclaims. So that sounds like something we need.
00:41:20.545 - 00:41:58.755, Speaker A: So let's stick that in here too. I mean, this is good though. This means that we're going to end up with a more standard cleanup than this thing that we just sort of hacked together, I think. So let's go ahead and find where this is. So that is declared free has pointer Rex. So here we're going to have fn wait for zero bulk reclaims. And who knows what that's going to do.
00:41:58.755 - 00:42:42.949, Speaker A: It's going to be a while loop for oh, that's wow. Let's do while true for now, it's just yield now. Which means that here we need a. We need this little bit because if you're using the. If you don't have the standard library enabled, then you can't yield. So this is just a spin loop. And what about load num.
00:42:42.949 - 00:43:50.735, Speaker A: Bulk reclaims? That's probably not a method that we have, but it is a method they have which is an acquire load of that and bulk Bulk reclaims load ordering acquire. And it is greater than zero. Why not not equal to zero? All right, fine. Great. Let's see that the tests still pass if we do that. 309. Oh, the old method we have used to so the eager reclaim we had used to return the number of things reclaimed.
00:43:50.735 - 00:44:44.559, Speaker A: So I guess what we can do here is we can keep having eager reclaim because eager reclaim is just going to do is going to return the number of things reclaimed. Or remember, do reclamations return the number of things that end up reclaiming. An eager reclaim is sort of a try. Actually, maybe this should be try cleanup. Try sort of implies an error result is possible. But in some sense this is like a maybe cleanup or do the best that you can. But cleanup is like block while waiting.
00:44:44.559 - 00:45:16.945, Speaker A: Right? This is a block. And then this is going to be that. And arguably the other tests should be using cleanup. Maybe. But the advantage of eager reclaim is that you can assert that the number of things you expected to be reclaimed are reclaimed. But here let's stick with what the C1 does and if we run the folly test. Okay, basic object passes.
00:45:16.945 - 00:46:22.215, Speaker A: Great. So if we now go back to this one, to the test they have node rc. What's the difference between node RC sounds like reference counting, but node RC set deleter. Not entirely clear. It doesn't sound like it's reference counting. It looks like it is. It Sort of takes out.
00:46:22.215 - 00:46:55.665, Speaker A: Oh yeah, this is the bit that's related to the linked object stuff. So I think we're going to just ignore that part of the test because we don't have the functionality implemented, which then means all these other ones are also sort of out of scope. That's fine, but at least we have the basic structure here. Copy and move test. Let's try that. All right. Copy and move.
00:46:55.665 - 00:47:45.655, Speaker A: Now, this is going to be a little different in Rust than it is in C, because in C you have move constructors, whereas in Rust you don't. I mean, you don't have move in. Well, only types that implement copy are move movable or are. I should rephrase slightly. Everything is a move in Rust, but you are not told when things are moved. Instead, when you move, the thing you move out of is no longer accessible and therefore you don't need a move constructor because like passing something to a function is a move. Whereas if, if you want the behavior from C of like moving is a copy sort of, then that's what the copy trait in Rust gives you.
00:47:45.655 - 00:48:34.255, Speaker A: Which is why the semantics of this is going to be a little weird in Rust, maybe. So here this is going to use the count stuff at all. So here there's just a structure obj which has a. A which is a U size object bas. Yeah. So this is the move constructor, or this is the copy constructor. This is the move constructor for their.
00:48:34.255 - 00:49:20.465, Speaker A: The object base here is the same as our object pointer wrapper. So we don't really have the same thing here because I guess we. I guess we can look at what their move constructor does, but I'm going to guess that this doesn't really apply for us at all actually. So has pointer object object base. Right? So this is the same as our. Oops, no, has pointer object wrapper. Right, where we provide.
00:49:20.465 - 00:50:18.085, Speaker A: This is the thing that we provide, the retire function on the. Basically the thing that implements this trait for you. So what else do they provide? They provide a set reclaim a pre retire. But what are the constructors for this constructor for? This is the same as the constructors for hazard pointer object. And what are the constructors for hazard pointer object up here probably. No, that's the cohort stuff. Object list object.
00:50:18.085 - 00:51:24.425, Speaker A: Interesting. This seems to just in all cases make a really. Just duplicate the pointer. That's all it really does. As far as I can tell, it just creates another pointer to the same thing, which kind of makes this test nonsensical. Like I Think what you end up with is, I think the equivalent rust code rather is here is a zero sort of. I guess they don't even give a value, which means it defaults to zero, I think in C.
00:51:24.425 - 00:52:15.695, Speaker A: So this is really just create another raw. Another raw pointer with the wrapper. And for us the wrapper is in the pointed to memory. So really if we're going to go by the same thing, P1 is this and P2 is just P1, like that's all it really does. Which means the P2 is now a pointer to the same bit of memory, which is fine. And then they call P1 dot retire and P2 dot retire. I see what this is testing, actually.
00:52:15.695 - 00:53:26.315, Speaker A: That's interesting. So I think what this is testing is if you retire the same pointer multiple times, does it cause a problem? And it's interesting to me that it does that because I wouldn't expect this to be okay. Right? If you retire the same pointer multiple times, should that even be okay in the first place? That sort of implies that we're going to do reference counting, right? Imagine there's a race here where this retires and then reclamation runs in some other threads and reclaims the memory and then we call retire again. That suggests that it should reclaim that memory again because it might have been reused. So that's interesting to me. Like I'm. I'm surprised that they expect this to be okay.
00:53:26.315 - 00:55:12.205, Speaker A: It's also interesting that for them the. Let me look more carefully at this thing. So. So an object is a reclaim function, pointer, a domain. This is sort of the children stuff and the actual object pointer to the object. I see. So I think for them, if we were to think about it in sort of rust terms, I think for them the construction is really this, right? Where the wrapper holds the raw pointer rather than the raw pointer being to the wrapper with the object, which is different because in that case this test tests something different, which is if you have multiple wrappers that point at the same object, does cloning them do the right thing or does moving them do the right thing? Even then, I wouldn't expect retire on the same underlying thing to be okay.
00:55:12.205 - 00:56:56.905, Speaker A: Though I do wonder whether wrapping it this way around makes more sense. I guess it's a question of is the domain used to retire a property of the allocated memory or a property of the pointer? I think there's an argument that it's a property of the pointer because the. Well, this is one place where inheritance make things nicer. Because if you look at test lib I guess if we look at the type of X here, X is an atomic pointer to a wrapper of the type. And if haspointerobject wrapped the pointer, then you would end up with a double indirection here, I guess has pointer object. The wrapper could be generic over the pointer type as well. Or just require that you use atomic pointer.
00:56:56.905 - 00:57:56.365, Speaker A: I mean, I think we could tidy up the API here if we wanted to. I think the way we would tidy this up is to say we're going to require that you use box and we're going to require that you use atomic pointer. If we did both of those things, we could encapsulate those decisions in the hazard pointer object wrapper. And then I think that could just be the top level type. So rather than sort of all of this, you would just write this and it would do the allocation for you. Or maybe it takes an argument that's already a box and it returns you this type which internally contains an atomic pointer to a box from raw. Question is, should we do that change now or should we leave this alone? This is where I'm like, this could easily be many more streams.
00:57:56.365 - 00:59:36.629, Speaker A: All right, so I'm going to leave a sort of XXX here, which is idea. What if this contains atomic pointer mut where star mute is box from raw and argument to constructors is boxt will make for nicer API. It will certainly make for a nicer API because we could hide and maybe we could rename this to atomic pointer. Right? Or maybe just atomic. I think I've seen this in. I forget what library is. This is in crossbeam where you have this like atomic type that is sort of garbage collected behind the scenes.
00:59:36.629 - 01:00:27.749, Speaker A: And I think the idea here would be the same. The domain reference value here is maybe a little sad, but I think that's a pretty interesting idea. I think that would make for a nicer API. And then the question becomes is, should it be clone, should it be copy? And then we have the copy and move test. I think without making that change, this particular test doesn't really make sense. I do think that actually. Let's look at retire.
01:00:27.749 - 01:01:25.765, Speaker A: Do they have a sort of protection in retire for if you double retire? The same thing retire is down here. Pre retire set reclaim, push object. Pre retire check set reclaim. There's nothing here that sort of prevents a double deallocation, which is a little worrying. Aha. Pre retire check only for catching misused bugs like double retirement. So if next not equal this interesting.
01:01:25.765 - 01:01:58.675, Speaker A: So then shouldn't that test always fail because this is a double retire, I guess. Okay. The fact that this test doesn't fail means that what they expect this to do is to clone the underlying object. Which I guess is what this dereference ends up doing. Right, that you are creating a new object that's a clone of the previous object in Rust. Of course, you can't do this until. Unless the underlying type is copy.
01:01:58.675 - 01:03:01.235, Speaker A: Like this wouldn't be sufficient for you to say to clone it. So I think this test is just weird to port. So I think we should skip this one for now. That's fine. I think it was still a useful discussion. Basic holders test. So this creates a holder, which is a look at the make global.
01:03:01.235 - 01:03:26.699, Speaker A: And I guess they just. Oh, each one is made in a scope. Okay, that's fine. So this is really. Just. See that it doesn't immediately fail when you like drop the holder or something. Oh yeah.
01:03:26.699 - 01:03:38.765, Speaker A: Here we need to say how many. Which is going to be two. This is the const. Generic parts that landed in a pr and we don't have local. So this is just. See that they can be created. Great.
01:03:38.765 - 01:04:20.037, Speaker A: Would it be safe to. I'm guessing you mean safe to implement copy on it for us. I'm guessing you mean implement copy on the wrapper. If we made this change. If we implement clone and. Or copy, there's not really an or. If you implement.
01:04:20.037 - 01:04:57.403, Speaker A: In order to implement copy, you must implement clone. So really just if we implement clone, we must have double retire protection. So it's okay. It's just we need to watch out for that. There's nothing inherently problematic about allowing it to be copyrighted. It's just it's up to the caller to ensure that they don't retire the same thing more than once. Or if they do, we need to make sure that we don't do the wrong thing.
01:04:57.403 - 01:05:28.861, Speaker A: As a result, we could define it undefined behavior, but we could also just have a check for it, you know. All right, so the basic holder test was very simple. Basic protection test. Basic protection we got here. All right, this should be easy. So this is just. This is very similar to the basic objects, except we're also going to actually sort of protect the value here.
01:05:28.861 - 01:06:04.375, Speaker A: So we create a domain, we create a node, we create a holder. And I guess we don't really need num here. That seems excessive. So we do this and we're not going to retire it yet. We're going to do that down here. We make an H. We do H dot reset protection.
01:06:04.375 - 01:06:55.025, Speaker A: Not entirely clear why we want Reset protection here because it should start reset anyway. Oh, reset protection. Why does their reset protection take an argument that's interesting. They probably have an override for this. This is another thing that we didn't do, which is I think they have sort of overloading for reset protection where. Yeah, where reset protection with an argument is sort of a shorthand for reset and then protect this thing. So if you look at protect, it resets protection.
01:06:55.025 - 01:07:51.247, Speaker A: F of P. That's interesting. So they're. Their protect is to do try protect, which is really just a call to reset protection. I don't know why they have this. So this is. Oh, right.
01:07:51.247 - 01:08:11.691, Speaker A: Okay. So this try protect. Right, right, right. Okay, I remember. So try protect is you don't give it a pointer to guard, you give it a pointer to a pointer to guard. So the idea is load this pointer, protect it, and then check that it's still the value that was in there. Because otherwise it's useless to protect it.
01:08:11.691 - 01:08:38.307, Speaker A: Because imagine that this is the. You tell it to protect the next pointer of some object you're holding. And you want to know what the new next is. Like what the actual value of the next is. All you have is an atomic pointer. And so you want this thing to protect whatever the next value is. But in order to know what value to protect, it needs to load the atomic pointer.
01:08:38.307 - 01:09:11.265, Speaker A: So it loads the atomic pointer and then it protects that value. But it could be that in between those two, another thread went and like deallocated that next. So it needs to reload the atomic pointer again to see that it protected the right value. If the two differ, then it needs to sort of retry. Right. It needs to reset the protection and do the whole dance again until it succeeds. Which is this business here, right? It's like while we fail to protect, so while the value changed in between the two, just try again.
01:09:11.265 - 01:09:52.319, Speaker A: And so in this case, reset protection is a little bit different. It is protect this value that I've already loaded. So notice that that's different. It's not saying load this for me and then protect whatever you loaded. It is just protect this value. Just blanket protect this value. It's a little annoying maybe, that it's a very unsafe API really, to sort of say just blanket protect this.
01:09:52.319 - 01:10:34.755, Speaker A: It's sort of ripe for abuse or for misuse, I should say, but we can totally do that. So what do we end up doing here? Not this one, but down here. So R. Try protect. Right. So this is one place where our implementation just looks a little different. But it sort of is the same thing, which is that reset protection is really just a call to reset has pointer which is really just this call right here.
01:10:34.755 - 01:12:07.645, Speaker A: It's just we called it protect rather than reset to this value which may or may not be null. I see. And so what they're saying here is I think instead of us saying that reset protection is just going to take a pointer, we should say let's say protect raw and I guess over somewhere over here. So this is going to be protect raw and this is going to take a mute to T. It's not going to return anything, which means that we don't need the lifetimes for any of these and we don't need an F. So what does our reset do? Right, so you notice here reset and protect are really the same thing, right? They both just do a store. And in in the folly implementation, this is just one function called resetprotection that takes a pointer argument and we sort of separated them.
01:12:07.645 - 01:13:20.825, Speaker A: And I guess that means we can just call this of source. I guess it does have to be static. That's fine. And this has to be. We have to do the same thing as up here and I'm going to erase that comment for now because it's not appropriate here. So over here then instead of reset protection, this is going to be protect raw of object. So the idea here being trade pound implement hazard pointer global.
01:13:20.825 - 01:13:57.445, Speaker A: That's because it's not global. It's because this is not make global. This is make in. This needs to be make in domain. So I'm actually very happy that that compiled there was a terrible error message. But what we were trying to do here was and rightly complained about this is we were trying to protect. So we allocated an object in one domain in this domain that we made just for the test and then we made a hazard pointer in the global domain and we tried to protect one using the other.
01:13:57.445 - 01:14:41.743, Speaker A: And the compiler was like, nope, that's not okay. Let's look more carefully at that error message that we got. So you see it says the traitbound hazard pointer object wrapper node of unit implements hazard pointer object global is not satisfied. The following implementations were found. So it's really hard to understand this error that that is what it means. But the F here stands for family, right? This is the notion of like domain families that we set up earlier to get type level safety for ensuring that you protect using the right domain. So maybe this is a good indication that instead of this being F, this should be like domain family or something.
01:14:41.743 - 01:15:21.595, Speaker A: Something more verbose and here the fact that this is just like unit is also confusing. I wonder where this comes from. Like, can I make, while we're here, can I make this test this error a little nicer? So that returns a global. Oh, the unit comes from here. This should arguably be unique domain. That's what that should be. Same with this should be unique.
01:15:21.595 - 01:16:21.595, Speaker A: Unique domain. And now what do I get? Yeah, so now you see that the family here is this like random closure that ends up always being unique. But that closure is not the same as global. Of course, that's not what the error tells us. Yeah, I don't know that there's a good way to make this error nicer, unfortunately. Like even if F had a different name here, I don't know that it helps you that much. Ah, so someone pointed out in the chat, doesn't this only protect you against local domain versus global? And it depends, right? So if you create your domains this way, like if you always use unit, then that's true, because unit is equal to unit.
01:16:21.595 - 01:16:40.415, Speaker A: But if you use this macro we provided, then they will actually always be distinct. So If I create domain one and domain two, they are. They have different Fs. So I can demonstrate this. Actually, if I do this is in domain one. Oops, let me do that. Not here, let me do that down here.
01:16:40.415 - 01:17:26.785, Speaker A: So domain 1, domain 2, domain 1 make in domain domain 2, then this will still fail. And the reason for that is this unique domain macro we have internally ends up using an anonymous closure type. And the closure type is tied to the current line or like the current location in the source code. And therefore any two closures are always distinct types. And that's why we ended up doing it this way, so that this ends up not working, which is nice. It doesn't protect from everything, but it gets you pretty close. So now if I do this, this will compile.
01:17:26.785 - 01:17:58.767, Speaker A: All right, so back to the test. It resets protection. Then it calls object retire. Then it asserts that the number of constructors. Then it calls domain cleanup. Then it checks that the number of destructors is zero. Right, because it's.
01:17:58.767 - 01:18:53.555, Speaker A: We have retired it, but it's still protected. Then we do H reset protection. Then we do domain cleanup, and then we check that the number of destructor runs is one, just to make this. Stop yelling at me. Great. Is there a way to restrict creation of the domain to just that macro? So not really, because the problem is the macro has to be able to call new. The closest we can do is where is this new? Right.
01:18:53.555 - 01:19:41.815, Speaker A: So this is our new method and obviously what we could do here is we could do doc hidden and we could say prefer unique domain Unique domain. The reason not to do this is because sometimes you have to name your domain type. Like you might have to. Like you might want to stick your domain in a struct somewhere and then you need to be able to name it. And you can't name it if it has a closure type in it. So you may actually want to be able to choose your own. I think what we should do here is construct a new domain with the given family type.
01:19:41.815 - 01:21:26.865, Speaker A: The type checker protects you from accidentally using hazard pointer Using a hazard pointer from one domain family the type F with a an object protected by a domain in a different family. However, it does not protect you from accident. You from mixing up domains with the same family type. Therefore, prefer creating domains with where possible since it guarantees a unique F for every domain. The color of your screen changed periodically. No, I think that's a stream encoding issue and it's really annoying. It's something that I've tried to fix in the past and then sometimes just randomly comes back.
01:21:26.865 - 01:22:12.195, Speaker A: All right, virtual test. So virtual is probably not going to matter for us because we don't really have that M. I don't even know why this has to be a virtual. It just looks like it allocates a bunch of things and then resets and retires them. Like, I don't. I don't know why it has to do with virtual. Like it is true that it.
01:22:12.195 - 01:22:37.941, Speaker A: It checks that it can still access the value after retiring while it's still protected, but doesn't seem very relevant. I don't like. I don't know. I don't understand the. I guess it's because the destructor is virtual. I don't think this matters to destruction test. So this is interesting.
01:22:37.941 - 01:23:20.355, Speaker A: This seems like it would be better suited for a loom test, but it is. There aren't multiple threads, so it doesn't really matter. But it makes me wonder like why 2000? Why does this need to be a large number? I mean, we can write the test. I'm just like destruction. It sounds even better in Rust because it's not destruction test, it's just destruction. This is the destruction function. So we have a struct thing very well named and it has if next.
01:23:20.355 - 01:23:59.255, Speaker A: Oh, that's interesting. That's really interesting. Okay, so this is thing is really a linked list we're dropping. It drops the next thing in the. So it's creating a Thing is a terrible name for this. It should be named something else, but it's really a head drop. Next is like one name for it.
01:23:59.255 - 01:24:31.275, Speaker A: It's a linked list. So it has a pointer to the next thing. And when you drop it, it's going to retire the next element in the list. It doesn't necessarily get reclaimed straight away, but it gets retired. And then we're going to allocate a long list of these and then we're going to retire the head of the list and then we're just going to wait for cleanup to finish because eventually it's going to reclaim all the items. So that's kind of interesting. So this, let's do head drop Next.
01:24:31.275 - 01:25:06.911, Speaker A: So this is going to have a next which is going to be a starmute self. It's going to have a domain. I'm not sure why it needs domain here. I mean, it's easy enough, I suppose. Let's do domain. This is also the reason to not use a global domain here because our tests are gonna run in parallel using the global domain. Let me just check that.
01:25:06.911 - 01:25:51.685, Speaker A: I did it for that. That's fine. Using a global domain here would make tests like this really weird. Where imagine one test demonstrates some broken behavior, so it ends up never deallocating an object or something. Then this test would block forever because it's waiting for every item to be deallocated, but it can't deallocate every item, so you end up with this cross contamination between tests. So let's go ahead then here and say unique domain. And this is one place where we might actually not be able to use unique domain because domain here needs to be able to name this.
01:25:51.685 - 01:26:31.085, Speaker A: And this is going to be a little annoying, but let's say static domain for now. Remember how every item we drop has to be static, so if it holds a reference to the domain, you know, it needs to be a static reference. I don't think we're going to be allowed to do that, for example, which is also unfortunate in terms of the API. So the problem is of course here what F do we use? Maybe we can get away with this. That might be okay. And it holds a value, say use size. That's fine.
01:26:31.085 - 01:28:07.629, Speaker A: And we're going to have an impldrop for head drop next of F. And it's going to be if next dot is null then if not then it's going to retire. And I guess actually this is not going to be quite self just because we don't have inheritance here. This is going to be a Hassle pointer object wrapper to self. And I guess it's probably going to have like a domain, which means this is going to have a domain which is going to be a little awkward, which means this is need to store a domain. And F is going to be the family here. So this is going to be then next dot retire deleters box F is going to be static.
01:28:07.629 - 01:28:38.205, Speaker A: That's fine. And this is going to require F static. That's fine. Right. So this is the whole like when you drop retire next. So I guess head retire next. I guess this may be a better name here.
01:28:38.205 - 01:29:24.853, Speaker A: And then we're going to do down here. So the odd the head they call it last. I guess we can call it last too. That's fine. Is going to be a head retire next of next is a null pointer and val I guess is zero. And this is going to be box from raw box new. No, it's not.
01:29:24.853 - 01:30:19.829, Speaker A: I take that back. I think it might be able to figure this out, actually. So we're going to here say mute and then we're going to do for let's do I and 0 to 2000 because that's what they do. We're going to do last is equal to. And then we're going to do another one of these. And next here is going to be last. And let's say VAL is I and let's say from one because you know, why not? Even though that's not what they do.
01:30:19.829 - 01:30:29.325, Speaker A: Oh, actually no. That is what they end up doing. They. I see. Okay. So this can really be. All right, fine.
01:30:29.325 - 01:31:05.745, Speaker A: We can do it this way instead. Which matches what they do as well. And then they do last dot retire and then they do cleanup. And it's true, the value field is never read here. It's like arguably we can just remove it. We don't need this value. The value doesn't matter.
01:31:05.745 - 01:31:23.135, Speaker A: So let's just get rid of it. That's fine. And we don't need the value. Oh yes. Okay. So we did get away with not having to name the domain because we just say we're generic over F. Whatever the domain family is, this type works for that.
01:31:23.135 - 01:32:07.055, Speaker A: Now let's see if that works. Nice. So this is neat right here. What we're doing is we're sort of seeing this chain destruction where all of these should be deallocated. Of course we could here stick in like dtors is atomic usize new and then say that here. In fact, why not box leak. So this is going to be detours.
01:32:07.055 - 01:33:09.571, Speaker A: I want. I Don't think I can do this because it's not allowed to close over the environment. But it can be here. Then self.detays.fetch add just to this is a thing that they're not doing. But I want to do it and I want to do it so that we can check down here that detours load is 2000. Nice just to.
01:33:09.571 - 01:33:47.893, Speaker A: Because otherwise like there's nothing really that says that when cleanup returns there are no objects left. Like maybe there's like a retire call missing or something. But this way we check that all of the things are dropped even though they're retired one at a time. Maybe instead of values the value that implements drop. I don't think we need to. We can just reuse the drop for head retire next. All right, sweet.
01:33:47.893 - 01:34:24.995, Speaker A: What's the next test? I'm guessing that we're going to get to some benchmarks further down here pretty soon. So here. Okay. Move tests. So these may not be relevant again. This might be, yeah. I mean, because we don't have a move constructor, we know that moving has no semantic meaning anyway.
01:34:24.995 - 01:34:56.205, Speaker A: Like moving can't have. There's no implementation for move so it can't mess anything up. So we could have this test. It just doesn't. Wouldn't do anything. And like even moving into an existing variable, like they. It just makes no.
01:34:56.205 - 01:35:53.837, Speaker A: Makes no difference. But I suppose we can write it anyway. I don't know if I think move is a keyword, so it might not let me do this. So I guess let's call it move test. This test is mostly irrelevant in Rust since there is no move constructor to check the correctness of. So it does a 4i in 0 to 100 it says let's X is all right. So this is using node again, where this is going to be I.
01:35:53.837 - 01:36:37.545, Speaker A: And we need our count thing again here too. So this test is also using the global domain, which I don't think is necessary for us. I think we can easily do this. And then inside of it it creates a protector. Let me dig up where the. Let's bring all of these down here. So they have HPTR0 is that thing and they call protect raw on X.
01:36:37.545 - 01:37:26.901, Speaker A: This is where they call reset protection with a. With an argument. Right? Then they immediately retire X. Then they do hptr1 is equal to h ptr0. So this is the move that is irrelevant to us really. I don't know what this assert is supposed to check. It's a self move I.
01:37:26.901 - 01:38:04.435, Speaker A: So it's a shadow. I guess is like the closest equivalent here. It's not really a self move, right? So it's. It's that they're taking the address of the thing and then doing a move constructor of the dereference of that pointer they took. So that's not that relevant here. And then they have a let HPTR2, which they don't assign anything to. And then they say HPTR 2 is equal to HPTR 1.
01:38:04.435 - 01:39:04.837, Speaker A: So this doesn't need to be mute, but this does. And then they assert E x value, which we'll get to in a second, is equal to I. And then HPTR2 reset protection. And then down here they do domain cleanup. It's interesting that they even use node here because they don't actually care about the counts, I suppose here. So one thing that's interesting is for us, protection returns a reference to the value. So really in normal rust here, right, like this bit would be unsafe and what you get back is an actual reference to the value that you can keep using.
01:39:04.837 - 01:40:05.513, Speaker A: But because we're going to move the holder, that reference is going to be invalidated, right? So if we actually use protect here instead, we would get back a reference instead here we're going to do a sort of unsafe X. I guess star X. How do we get inside of node.val? right? So the idea here is that X is still protected from the call to protect RAW earlier. So this is definitely sort of unsafe, but this is what you end up with in the sort of C code. Great. I wouldn't expect this to not work because here the moves.
01:40:05.513 - 01:40:34.475, Speaker A: There are no semantics to move in rust anyway. So like it can't really mess up the safety state array test. Great, we can actually write this now because we is array a keyword. Array is not a keyword. Great. So this is probably going to be very similar to the previous test. So let's start with that here just to have some code to go from.
01:40:34.475 - 01:41:20.753, Speaker A: So here we still create the object, but we do make many in domain of domain and in particular I think. I think the value is three. They call that hp. And then they do. They do this move assignment again, which is real annoying. I kind of don't want to use the same API they do. Like we have a safer API in Rust here, right? Which is instead of this sort of reset raw, we have one that can actually give you the value back, assuming it live.
01:41:20.753 - 01:42:15.405, Speaker A: Okay, fine, fine, fine, fine, fine. I guess we can have protect RAW give back. That's interesting. I definitely want that to be the case. That you can index into it. But I suppose. Right.
01:42:15.405 - 01:44:01.605, Speaker A: So if we do let H pointers, H pointer dot hazard pointers, then I can do this and I guess protect RAW could return. No, I think the right thing for is I was thinking like it could return like a reference to T the same way protect does, but I don't know if that's meaningful because the reason protect can do this is because it can load the value, protect it and then see that the same value is there with protect draw we have no guarantees. Like it's just the caller promised that this, that there is no race basically. Which means I think it's on them to do the dereference as well. It's not okay for us to transform it into reference for them. So we go back here, they take the move assignment which we're just going to skip, but then they retire the object and then X is still protected so they can dereference X and then we can do H pointer two reset protection. Great.
01:44:01.605 - 01:44:33.575, Speaker A: What else we got? Array detour. Full TC test. So this is testing with thread locals. Oh, I see. It's because we have this like we have a thread cache of pointers. I don't think that matters to us. Local test doesn't matter because we haven't implemented pointer.
01:44:33.575 - 01:45:38.315, Speaker A: Local link test doesn't matter because we don't have support for links. Same thing here. Auto retire test also doesn't work for us because we don't have link support. Free function retire test. So this is if you have a custom deleter, right? So remember how the deleter currently that we're using is this like drop box and this is if you have a custom deleter then sort of checking that actually gets invoked, which I suppose we can do. That's easy enough. Test Free function retire.
01:45:38.315 - 01:46:36.935, Speaker A: That's interesting. So I guess here what we're really doing is something like foo is new, you know, Int of 0 and we are all going to do this in a domain as well because we want test to run in parallel. And I wonder why they call. They. They end up calling retire in like a bunch of different ways. So here we want not Dropbox this time or I guess it can be Dropbox. That's fine.
01:46:36.935 - 01:47:48.437, Speaker A: Food two is going to have a custom retire here. So remember how we have this drop box which is a. An unsafe DIN reclaim and what we want here is actually this. So I think the, the awkward part about making this unsafe is that it can't be a closure because I don't think you can inline declare. I don't think you can inline declare unsafe closures. I don't even know if you can have an unsafe closure. But let's say custom drop here and the custom drop is really just going to be here they use delete in our case.
01:47:48.437 - 01:48:31.195, Speaker A: We're going to do the same thing as what Dropbox does, which is this. And this is now going to be custom drop. It's going to be pointer and Right. And this trait is only. Right. This is the whole. We had to re.
01:48:31.195 - 01:49:07.565, Speaker A: Assign it to a const to get the compiler to realize that these are okay, which was annoying. But I think maybe we can do this. I expected a reference and I think we need to do like. Was it this? Oh man, I forget what we had to do to get the. The signature here to work out because we need to. Oh, actually maybe it was just din. Maybe it was just.
01:49:07.565 - 01:50:08.745, Speaker A: I. I guess it. I think it's like Custom drop is custom drop. Oh, actually maybe it was just this as din deleter Trade bound implements. Deleter is not satisfied. Was it just this? Oh man. Because I remember we had to play around with this so much to get it to accept that this function signature can be used as a.
01:50:08.745 - 01:50:49.137, Speaker A: Can be used as a Dropbox. And this is because function definition types are different from function pointer types in Rust. Oh, and this is because we're relying on this implementation. So deleter should be implemented for the function type, which I think we can get by saying that. Right. We need to specifically declare the type. I remember this being really annoying.
01:50:49.137 - 01:51:39.905, Speaker A: I think it's. This is custom drop. So this course is it into a function pointer type. And then we can do that and then we can do this and then we can do that and this can be a const. And this is a. It's real stupid there. I don't understand what is it complaining about? There's some very long.
01:51:39.905 - 01:52:05.763, Speaker A: Oh, it's just a warning in here. Unnecessary unsafe block. That's fine. And this is going to be futu dot retire. So this is the way that you end up. Yeah, we could make a macro to make that cast. That's not a bad idea actually because it is a really annoying.
01:52:05.763 - 01:52:27.621, Speaker A: It's really annoying to have to do this cast and have to do. You have to do it through a con so that it's static. Yeah. See, it has to be an unsafe but it's. Yeah, actually maybe if I do this, maybe that's the way to go about it. There we go. You are correct.
01:52:27.621 - 01:53:15.605, Speaker A: That is a better way to do it. Okay, right. So they do that. They do this and then they have this retired is. And I'm gonna box leak and atomic bool here. And then in here they create a custom domain just for this test, which is odd. We're not going to do that because that's so.
01:53:15.605 - 01:54:22.945, Speaker A: I see. So this is going to be a sort of retire. Retire retire struct. And we're going to impulse retirer. And it's still just going to be that like. But it is also going to have a text static atomic bool and it's going to do self0 dot store true release. And I guess we're going to do the same thing up here except we can do it a little simpler, I think, which is we do this.
01:54:22.945 - 01:55:15.965, Speaker A: What do they call it in here? They call it foo3 very, very good names really, if you think about it. Roar of retired. And this needs box new. It does not need to be mute Cast requires that this lives for static. Yeah, the deleter bit is really annoying. Like, I understand why it needs to be static, right? Because it. It can't guarantee when it's going to reclaim this thing and it's only at reclamation time that it's going to call the retire.
01:55:15.965 - 01:56:22.151, Speaker A: But it means that it's weird for it to be an object. Like, I wonder what they end up doing for this, right? Like they. They like. I'm wondering if this even. Is this even really a customer tire or is this just an object? I don't think this is a deleter, even though the name sort of implies it. I think this is just an object. Right? Maybe I'm lying.
01:56:22.151 - 01:57:01.491, Speaker A: I could be lying. Other return tires do they have. There's only this one, and it takes a pointer to the object and a reclaimer, but it doesn't. This doesn't give a reclaimer. I think this just gives the object. So I don't think this test tests what they think it tests. Right? And you actually.
01:57:01.491 - 01:57:58.879, Speaker A: You can see this because it doesn't call delete either. So I think this is just like a drop test. I don't think it's even a custom retire like this one is. It's still kind of tempting to have a test for this, but I don't think that's actually what it's testing. I think this is just testing that drop gets called with the default deleter or default, which in this case is sort of the test we already had. Right. It's sort of the same thing as we're testing in this destruction test or in basic protection, which tests node.
01:57:58.879 - 01:58:38.445, Speaker A: I don't want this. This last bit. I'm going to leave a comment here saying third test just checks that detour is called, which is already covered by other tests. It is not using a custom deleter. Deleter retire. So I'm just gonna not have that. And I don't even know why this.
01:58:38.445 - 01:59:14.655, Speaker A: Their version of this test doesn't even call cleanup. So, like, what is it for? Cleanup test. This smells like a loom test. This smells a lot like a loom test. Yeah. Because here they have like a bunch of different threads and the threads are spinning and whatnot. So let's make this into a loom test.
01:59:14.655 - 02:00:08.505, Speaker A: And we're going to do this with. I'm kind of tempted to put it in here. This is the cleanup test. This is cleanup test from folly. And then we're going to go up here and we're going to say this. All right. I'm also wondering if it's a mistake for all the loom tests to be using a global domain, but you know that it is what it is.
02:00:08.505 - 02:01:12.925, Speaker A: So each run is gonna have a domain and one of these counters, which sort of means that this count type needs to be brought over here as well. That's fine. So let count is count for test. This is a place where our leak is actually going to become a problem. But I think what we can do here is just share it and then do count dot clear. This is where we're going to need a clear method on count. We're going to self ctors store zero and detours and retires.
02:01:12.925 - 02:01:57.775, Speaker A: So here they make a lot of threads. I think we can probably get away with two, but sure, for TID in zero to two, remember that loom will already take care of interleaving. So we don't actually need a lot of threads here. Even two might be excessive. And then they do thread spawn. And I forget whether. Yeah, we have already imported that.
02:01:57.775 - 02:02:23.647, Speaker A: That's great. So we're going to do thread spawn. So we could have a normal test that does all this threading too. So I'm not opposed to that. But it shouldn't be necessary. Right. Like loom should be covering the interleavings that are relevant here.
02:02:23.647 - 02:03:15.905, Speaker A: The things that loom don't cover are more things like the very strong consistency of sequential consistency and the very weak and relaxed the stuff in between. Loom tends to cover pretty exhaustively. All right, so in here for J. In for J. So this is like striping. So all right, let's do. So we have these.
02:03:15.905 - 02:03:57.481, Speaker A: What I don't understand why is the indentation there? Weird. Great. Right? Because it's. It skips by num threads. So the other way to express this is for J in zero to. Well, we could even either perforate the loop or we could just loop by. It's annoying that there isn't really.
02:03:57.481 - 02:04:59.607, Speaker A: Actually there is a range for this, right? There's like a range is 0 to thread ops.just so that this won't yell at me. I thought it was like skip by range and I'm pretty sure there is a iterator. I think it overrides the step. Step by. Yeah, So I think we want this dot and this consume self. Right.
02:04:59.607 - 02:05:56.415, Speaker A: So step by fine const num threads which is I think what they're using over here, right? They called it. Yeah, num threads is two num threads. So we're going to step by num threads and then we could do. And then it's from tid. So for J in this, which is what this translates into. Right? It's from this to this, stepping by that. And each one creates a new node, which means we need to bring the node type over to L Doom 2, which is a little annoying, but not the end of the world.
02:05:56.415 - 02:07:03.665, Speaker A: I think we specifically want these to use standard sync atomic because we don't need it to be considered in the ordering of the loom test. The atomic pointer here though probably does need to be a lumatomic pointer. That's fine. And then if we go back to our test here just to sort of grab the node creation bit so they create a new one and then immediately retire it. And this too seems like more operations that are necessary. It's just going to cause more interleavings for loom to explore. So I'm guessing that this number can be reduced to.
02:07:03.665 - 02:08:05.815, Speaker A: And then afterwards it's going to store threads. Done. So we're going to need a sort of. It's going to be I guess a atomic U size but it's going to have to be static because I guess we can just have a arc new and standard sync arc. Great. This is going to be I guess an arc new of atomic bool new of false num threads. And we're going to have one of these for each one.
02:08:05.815 - 02:08:56.125, Speaker A: And we're going to say done TID store true ordering release. And this is going to be a standard sync atomic. And I guess I need to kick back up this thing. And we specifically want volley cleanup. I think once we finish this test, I want to see if I can find A benchmark so we get a chance to cover that too. This can be j. That's fine.
02:08:56.125 - 02:09:59.735, Speaker A: Associated for test. All right, it's not for test, it's test. Local standards sync atomic. Actually, this is also always tricky when you're using loom is like figuring out whether you should use the loom version of the atomic so that it gets sort of a part of the loom scheduler, or whether you can get away with using the standard atomic and sort of hide it from the loom scheduler. In this case, I think we do want it to be a part of the loom scheduler. So this is actually going to be the. The real atom, the atomic bool from loom.
02:09:59.735 - 02:10:51.159, Speaker A: And then this is going to be while not main done. So this is threads done and then there's main done. So this is threads done and we're going to do then so there's main done here and threads done here. I don't know why they used threads done and not like a barrier. Because I think realistically that's what this is. Right? It's trying to synchronize on all of them moving forward. Right.
02:10:51.159 - 02:11:19.029, Speaker A: Every thread here is waiting for. Oh, they're not waiting for each other. They're only. Well, they're waiting for main to be done. And main isn't going to be done until all the threads are done. Where does main done get set to true later? Okay, so there. It's not quite a barrier.
02:11:19.029 - 02:11:58.775, Speaker A: There are things that happen in between. All right, fine. So while not main done load, this one's tricky because here we kind of want to tell loom that this doesn't matter or rather that this thread can't make progress until. Until this value changes. And I think there is a way to do that. Yielding. Yeah.
02:11:58.775 - 02:12:28.065, Speaker A: So as long as we use yield now, even yield now is too weak. Right. Like yield now is telling loom some other thread needs to make progress. But in this case, we actually know which thread has to make progress and we don't really have a way to communicate that here. One way we could do this is to have like a channel that we block on or something in instead. And that way loom will actually know what unblocks this one, which may be necessary. Otherwise loom is going to end up with too many permutations here.
02:12:28.065 - 02:13:29.655, Speaker A: But that's fine. Ordering Acquire thread yield now. Great. So it does that and then include the main thread in the test. So it does the same thing here for the main thread, but this one is. Has no step by and starts from 0 and uses I because you know, of course can't have them do different things. And then here this has a.
02:13:29.655 - 02:14:21.725, Speaker A: Oh, threads done actually doesn't need to be a bool. It's just a. It's just an atomic usize for how many threads are done. So this is a fetch add of one. And then I guess here we can do a while threads done. I think really this, there should be a barrier here on the threads. But while threads done is less than num threads then yield now.
02:14:21.725 - 02:15:19.795, Speaker A: Right. The problem here is loom is going to do something like imagine the main thread and one thread have entered this loop. Loom thinks it will make progress if it just bounces between them, when in reality that doesn't cause any progress. So those are wasted executions, they're wasted to explore. And the only way to tell loom about this is to let it know that it's specific threads that need to make progress, not just any thread. And this is going to blow up the execution tree size, which is going to make the test run long. All right, so down here we want to assert =ops +main ops, the number of constructors and then domain cleanup.
02:15:19.795 - 02:16:07.269, Speaker A: And then we want to do the same for destructors because all of them have been retired as part of the test. And then main done store true ordering, release and then join all the threads. Yeah. So the main done is a barrier, right? Because it stores this and then it then here it join all the threads. Right. So. And all the threads are blocking on main done.
02:16:07.269 - 02:16:54.709, Speaker A: So that is just definitely a barrier. Does loom have a barrier? That's the next question. Sync not yet supported in Loom. It could cause an infinite loop. Yeah. Can we do better? Here is the next question. Oh, okay.
02:16:54.709 - 02:17:13.074, Speaker A: You're gonna hate me for this, but I think it's gonna work. It's awful. It's. You're. It's. It's awful. It's gonna be awful.
02:17:13.074 - 02:18:44.838, Speaker A: So we're gonna take the mutex in main. All of these threads are going to try to take the mutex and this is going to only going to drop the mutex when it's done. And when main drops its mutex, then all of these threads can resume and all they'll do is finally get the lock and then drop it and terminate. It's great, right? Dot map. I'm just rewriting this so that we get the handles. Yeah, it's awful, right? But it works. I think in fact we could do the same thing for the threads done.
02:18:44.838 - 02:19:25.494, Speaker A: But I'm not going to do it. I mean a barrier is a semaphore, but barrier is not supported. So we use Mutex. I was thinking if we could do something with RWlock to get both of them at once, but I don't think we can. Threads T join unwrap. All right, so we join the threads. Cleanup after using array.
02:19:25.494 - 02:20:29.799, Speaker A: I don't understand. This test down here has nothing to do with any of the stuff that happened before. Everything has terminated. There's nothing left. So why is this random array test here? Why did they stick this here? I mean, I'm. Okay, I'm just going to put that in here because. Where's our array? Our array test? Didn't we have an array test? Move test.
02:20:29.799 - 02:20:51.525, Speaker A: Right array. So they just randomly as part of cleanup. Array part or array part of cleanup. That just. Is a separate test they made. And this first part of it makes no sense. This one just.
02:20:51.525 - 02:21:53.935, Speaker A: Just sees that it's possible. This is the same as this one. I don't understand why. I mean, I'll have it there, that's fine. But why make many in domain. Oh wait, is there a Loom notify? Ooh, that's tempting. Yeah, we could use notify the other way around.
02:21:53.935 - 02:22:32.525, Speaker A: Yeah, we could totally do. We could totally use notify to. To fix the other way. It's a little awkward though, because we'd still need the counter. So it would really just be a way to sort of let Loom know what is waiting. But I don't think it matters because the current scheme, I think is not going to confuse Loom because it. It goes through this, then it yields.
02:22:32.525 - 02:23:00.115, Speaker A: So some other thread has to run is what Loom is aware of. Right. And the only other threads there are are these threads. And if any of them have done their fetch ad, then they're blocked on lock, so they're not runnable. So Loom knows that the only other thread it can run is the one that hasn't done the fetch AD yet. So I don't think we actually need the notify here, but it's a good, good call out. All right, so this is just a random.
02:23:00.115 - 02:23:34.675, Speaker A: We're just going to stick an array test in here. That's fine. I don't. I don't mind. So we have a h, which makes two of these. We allocate two nodes, P0, P0 and P1. We protect both of them.
02:23:34.675 - 02:24:30.579, Speaker A: It's a little sad that it's not directly indexable. Arguably it should be. I think the reason why you can't just Do H and then directly in index is because. Is because it would be a little useless because the borrow checker wouldn't be able to realize that basically the borrow checker has special knowledge of arrays. It knows that if you index into. If you use one index and use a different index, they're independent. Actually, maybe it doesn't even know that I was going to say that with this you could actually use two hazard pointers at the same time.
02:24:30.579 - 02:25:35.062, Speaker A: But I don't know if you even can. So maybe, maybe this really could just be that we implement like the index trait straight on array index mute. More importantly, the concern, right, is that if you try to use index zero to protect one thing and then use index one to protect a different thing, then the borrow checker is going to be like, no, both of those require immutable references to the array and therefore everything is sad. Ah, so this is why it works. So this returns you an array of mutable reference to each element so you can use them independently. If you index, you're going to borrow the entire array for the one index and therefore you couldn't use multiple hazard pointers that are in that array at the same time. So we're going to do h0 protect raw p0 h1 protect raw p1 p0 retire.
02:25:35.062 - 02:26:18.857, Speaker A: Oh, what did I do? P1 retire. End of scope. Actually, that's a sort of interesting test, right? So this is checking that. I mean, it doesn't actually matter that I made this up here, but I'll do what they do. This is effectively checking that dropping this. So in turn dropping this is going to remove the protection we set here. Therefore the retires are going to take effect there.
02:26:18.857 - 02:27:09.129, Speaker A: So we're going to get the cleanup. So now we can go back here to the where's my ctors? So now I want to assert equal ctors 2 and assert equal detours here. Use named lifetimes but not named generics. Is there a reasoning for that? I find that people are sufficiently confused by lifetimes that things like tick A are not necessarily obvious, especially once you have more than one lifetime. Keeping track of which is which. It needs a name for generic parameters. That does happen too, but often it's slightly more obvious what's going on.
02:27:09.129 - 02:27:41.669, Speaker A: I think domain family is maybe one exception, but I think we have better conventions there. Like for lifetimes the conventions are just like tick A, tick B, tick C, tick D. Whereas for generics we have like T for a contained type. I think we're using like D for a deleter. So we could Expand them. I'm not opposed to expanding them, but I think it's all right. So let me first check that I didn't break that test.
02:27:41.669 - 02:28:41.675, Speaker A: Okay, great. And let's go back to the loom test. No method fetch add. What do you mean there's no fetch add Threads Done is a function that's not. Oh, really? So the standard library recommends that when you clone an arc, you use arc colon colon clone rather than dot clone because it makes it clear that you're cloning the ARC and not cloning the inner element. But it sounds like loom doesn't have that. Great.
02:28:41.675 - 02:29:41.645, Speaker A: But it does have this. So, like, I feel like it should still work. So why does it not Main done. Threads done. Am I missing something stupid I've done here? I mean, does that make it better? No. Well then. Oh, thread.
02:29:41.645 - 02:30:28.545, Speaker A: It's not giving me type annotations because it's behind a config, so it's not actually compiled. Ah, an unknown type atomic you size. That's what it's complaining about. That's where the other thing stems from. There we go. That should do it. Count is borrowed here, but count is static.
02:30:28.545 - 02:31:52.395, Speaker A: May outlive borrowed value count move borrow value domain. Yeah, this one's also a little awkward, but maybe we can do this. We can't do that either, but we can maybe do box leak box new here. The problem, right, is that we're spawning a thread, which means they need to be static, so it can't live inside of loom model because that's not static. And then this now becomes not referenced domain which is not implemented the copy trait. Right? That's because box leak actually returns a tick static mute. So if you look at the signature of box leak, which it returns a.
02:31:52.395 - 02:32:46.515, Speaker A: Well, any lifetime. But in this case we're using static, but it's immutable reference. Immutable references are not copied because they can't be aliased. So we need to cast it to a shared reference so that it works out. Cannot access loom execution state from outside of a loom model. Are you accessing a loom synchronization primitive from outside a loom test? Am I? I didn't think I was. I probably am because.
02:32:46.515 - 02:33:48.255, Speaker A: Yeah, I see what's going going on here. So create the domain that we create internally contains like atomic pointers and stuff, which we want to use loom, but that means they have to be made inside of loom, which means that we're gonna have to do this a different way. What does loom model Take specifically, can I give loom model. It's an fn. The problem, right, is that it needs to be static for it to be passable to spawn. Because I. I assume that the spawn requires static, which sort of implies that it needs to use the globals.
02:33:48.255 - 02:34:55.515, Speaker A: Actually, maybe I can do this with a lazy static. Maybe that's the way to go about this. So LOOM exposes a lazy static. So if we do loom lacy static and we're gonna have a static domain, and I think here we're gonna have to end up with like a sort of weird family. Like we're gonna have to use a named family because we can have a generic. Here there's another example of somewhere where you, you need a named family. And here we can then do domain new of this.
02:34:55.515 - 02:35:52.525, Speaker A: And now this can do this and same thing here. This needs to use D and this needs to use D. No rules. Expected that this token. Is it like static ref. Is that what they need me to write? Expected reference found struct D. Yeah, this needs to be D and D.
02:35:52.525 - 02:36:36.635, Speaker A: Aha. Well, I panicked with something else. Model exceeded maximum number of branches. Yeah, of course it does. So this is probably because our numbers are too high here. So we could set this, for example, to like If I do Seb 17 and 9, is it going to complain? Well, it doesn't look like it. I can hear my computer fan spinning up.
02:36:36.635 - 02:37:13.329, Speaker A: Unfortunately, Loom is single, threaded, which is. You know, one of the great ways to speed up LOOM would be to let it run multiple model simulations in parallel. It doesn't currently do that. Can you put the domain in an arc? Will that mess things up? We could put the domain in an arc. I don't think it needs to be static. We could have an ARC per thread instead. Well, the CPU usage is not that interesting because it's single threaded.
02:37:13.329 - 02:38:03.675, Speaker A: So you see, one course it's real busy. What I do want though, is I want Loom to be more helpful to me and tell me what how many iterations it's on, which I think is like loom. I don't want to turn on loom log, but I do want it. Oh, did I not run it with no capture? No, I did. Oh, wait, why am I running it with loom log? I don't want it to run with loom log because that slows it down. I do want to just see something though. Multi reader protection.
02:38:03.675 - 02:38:45.171, Speaker A: Multi reader protection. Actually, the. What I really want to see is I want to make sure that it actually is running multiple iteration okay, it is. It's running lots and lots of iterations. So this one, at least in theory, is running through and all the asserts are working fine. And there are probably lots of iterations here. Let's try something smaller.
02:38:45.171 - 02:39:33.045, Speaker A: Right, so let's say there's going to be two threads, the main thread is only going to do three operations, and the big threads are only going to do five. I just want to see if we can make it terminate. Yes, this is the problem with atomics, right. Is there so many possible interleavings to consider that even with a small number of operations? Like if you just think about it, right? Every. Every retire here, for every thread, for every operation, including both the main ops and the. The other two threads, all of those combinations need to be checked. And like in some cases one thread is gonna.
02:39:33.045 - 02:40:01.225, Speaker A: It's gonna end up reclaiming. In other case, another thread is not going to end up reclaiming. And like all of these cases need to be considered by Loom. So there are just lots and lots and lots of iterations. I'm surprised it's not printing the number of iterations it's done. I thought that was a thing. It did by default, but apparently not.
02:40:01.225 - 02:40:56.791, Speaker A: The other thing we could try is Loomax preemptions. Aha. So preemptions here is how many times a thread should be interrupted while executing. So generally threads get to run until they yield, like until they do like a blocking system call or something. But the operating system is allowed to preempt them to stop them early and loom. The setting I just set is telling Loom that you shouldn't interrupt a thread very often or as often as you normally would consider. And this reduces the number of possible executions because the number of possible interleavings are fewer.
02:40:56.791 - 02:41:25.547, Speaker A: Because threads yield less often. It can probably not make this very large even still. Yeah. So it still takes a while to run, but it lets us explore more of these. These more of these interleavings. Is it better unclear in this case? I think it's probably better to have more operations just because we know that there are limits. Actually the.
02:41:25.547 - 02:42:45.387, Speaker A: Remember how there are limits on reclamation that are these like the number of items we use before we start doing collection. We might want to set a lower limit for LOOM here to force it to do collections with lower numbers. But this is probably the reason why they set it to 1007 in that test, right? Because they wanted to get over the count where you're forced to do a reclamation which is set to 1000 which makes it tempting to say config loom and config not loom. So with Loom, if we set this to like five, right. And same thing probably for the number of shards do two is probably enough. So now we. As long as this number is.
02:42:45.387 - 02:43:30.205, Speaker A: So they used 1007. I don't know why 7, but let's do I guess 10 or maybe it should be prime. And I don't know why they chose. I think they had 17 for main ops. I don't know why they chose 17, but let's do seven. So that panic, that's actually kind of good for us. Ah, model exceeded maximum number of branches, so that's still too many.
02:43:30.205 - 02:43:59.125, Speaker A: Because what we're also doing by reducing this threshold. Right. Is that Loom is going to be forced to deal with some thread decides to do reclamation, which again, sort of increases the size of the expiration space. If it didn't have to think about reclamation at all, there's a bunch of code paths that aren't explored. And so Loom doesn't have to think about their interleavings, but now we're forcing it to think about more of them. Okay, so that ran. That's promising.
02:43:59.125 - 02:44:29.165, Speaker A: And main ops here, I think main ops is unlikely to make that much of a difference. Like the main. Main thread probably doesn't really need to participate in this. It's probably going to be the same. Exceeded maximum number of branches. That's fine. I'm trying to see if I can do like oops, yeah, let's do three and seven and see if I can do that with here.
02:44:29.165 - 02:45:17.099, Speaker A: What about yielding stuff? Isn't that the way to reduce the number of branches? So yielding doesn't reduce the number of branches, it just says some other thread has to run, which it does indicate to loom that don't continue running me, run someone else. But it doesn't necessarily guarantee forward progress. All right, so I'll leave this running in the background to try to find a bug. The fact that it hasn't crashed yet is a good sign, but. But it's still exploring the state space. Right. All right, so let's see then if we can find a core test.
02:45:17.099 - 02:46:05.675, Speaker A: Destruction test, cohort, safe children test, fork test, LIFO test, swimmer test, wide cast. Why is this just labeled tests? Oh, I see. This is the thing that actually initializes the test and potentially runs multiple copies with different underlying types or underlying schedulers. Like here. Yeah, so see, it runs the destruction test using different. Different underlying types and different domains. Now show me a Benchmark, please.
02:46:05.675 - 02:46:39.645, Speaker A: Reclamation without calling. This is an interesting test. Reclamation without calling Cleanup. So this is just seeing that if you do nothing eventually your stuff will be reclaimed, which we don't support because we don't have asynchronous reclamation at the moment. We have time based reclamation but not asynchronous reclamation. There's no collecting background thread benchmark drivers. Aha.
02:46:39.645 - 02:47:17.621, Speaker A: Barrier. Wait, that's funny. I wonder why they do. I guess that makes sense. They want all the threads to be ready to run. Then they allow the main thread to start the timer and then they release all the threads and do the work. Bench.
02:47:17.621 - 02:48:02.227, Speaker A: All right, let's see what this. Yeah, so we're not going to really deal with the. This is. It's interesting that they need to do this. So I wanted to use criterion for this because criterion is great. I want to see whether there's a way in criterion for me to say timing loops batched it or custom. Custom might let me do it just specifically because.
02:48:02.227 - 02:48:43.175, Speaker A: Well, it's still for looping and we don't. This isn't necessarily a loop, but this will let me do the. Yeah. Iter custom would let me do the sort of correct setup here. Which is specifically if you're doing a multi threaded benchmark right, you need to make sure that you don't bias the sampling based on some thread took a while to start and therefore the others were sort of slow. And that's why they have this barrier for synchronized start. But I want to see what the actual test does.
02:48:43.175 - 02:49:26.965, Speaker A: Rep fn so repfn here is just the function that is being benchmark. So this is just benchmarking harness both this thing up here and this thing here. Our harnesses holder bench. So this is a benchmark of. Wow. This is the benchmark of making 10 million hazard pointers across some number of threads. All right.
02:49:26.965 - 02:49:57.915, Speaker A: I mean that's an easy enough one to make. This is still running. That's. That's good. All right, so criterion has the. Getting started is real nice. So we're just going to do this and here what I want is what are we going to call this one? I guess we can call it folly.
02:49:57.915 - 02:51:02.795, Speaker A: And then we're going to do make their benches. And then we're going to have benches folly.rs bring in all of this test Folly. I'm going to want all of these. There's no Fibonacci in here. Instead I guess we're going to benchmark make holders. And the benchmark here is really just actually I'm Going to leave this up for a second to sort of explain Blackbox, because Blackbox is its own little annoyance.
02:51:02.795 - 02:51:42.415, Speaker A: So here we're going to do. Let H is. Let me just grab a example of this, like so, and let me remove these. This is just a very straightforward one to start. And this is just going to keep running. That's fine. And I want to run Cargo Bench.
02:51:42.415 - 02:52:23.247, Speaker A: So the criterion set up here is that you have a. You have a bunch of groups of benchmarks. Criterion main here is just these are my groups. In this case we only have one group, it's called benches, and each group has some number of benchmarks. In this case we only have one benchmark called Criterion Benchmark, and each benchmark has a bunch or can have any number of benchmarking functions. In this case we only have one called Make Holders, and that benchmark runs a number of iterations. So we're going to do iterations of doing this operation.
02:52:23.247 - 02:53:31.447, Speaker A: In this case, that operation is just making a global hazard pointer. Black Box here is an interesting function where it's unstable in the standard library and Criterion provides its own version of it that works on stable, but it's not quite as powerful. And the idea here is sort of straightforward at sort of a high level, which is you might imagine that we wrote this like this, right, which is just call the make global function. The problem here is that the compiler might decide that this is dead code and eliminate it. So your benchmark takes no time and therefore it looks like everything is super fast, but it's just because it did nothing. Blackbox is a function where they're still trying to figure out what exactly kind of promises it's going to make. But one way to think about Black box is that it is a function where the mapping from its one input argument to its return type is unknown to the compiler, hence its name.
02:53:31.447 - 02:54:42.907, Speaker A: It's a black box to the compiler. So the compiler is not allowed to assume anything about what the function does with its input or how that is transformed into its output. In practice, Black box is the identity function. It just returns its input, but the compiler isn't allowed to know that, and therefore blackbox could, for example, have side effects. Therefore the compiler can't eliminate the production of the input. Now, this doesn't necessarily save you, right? So, for example, like in this case, I'm using Blackbox on the make global, which forces rust or forces the compiler to actually call this function to produce the argument, because it needs to give that argument to Black Box, but it doesn't prevent Things like here, let's imagine that the compiler realizes that this is loop invariant. There's nothing that stops the compiler from hoisting this call and then doing this, right? If it realizes that that is okay, it's still providing an argument to blackbox.
02:54:42.907 - 02:55:17.155, Speaker A: Black Box is still getting a valid argument that was produced. But there's nothing here that tells the compilers not allowed to do this optimization. In this case it shouldn't matter. Like the iteration is taking care of us by criterion, but it's somewhat tricky actually to make compilers not optimize away benchmarks. All right, so here we're running the benchmark. I mean, I'm running the benchmark and I'm also running loom. But because they're both single core at the moment, notice that we haven't.
02:55:17.155 - 02:55:55.129, Speaker A: This is not running multiple threads, even though that is definitely a thing that we want to do. So you'll notice I ran Cargo Bench up here and then I ran it again and it tells me how long each call took. So this is not the total run of the benchmark. But one call of the closure inside the iteration function took this long. And you'll notice here it told me the change too, and that's because I had already run, I'd already run Cargo Bench and then it took 5.392 and now it took 5.639. So it's.
02:55:55.129 - 02:56:17.455, Speaker A: It's faster on average. But looking at the ranges, sorry, it took 5.39, it now takes 5.63, so it's slower. And it's also telling me that the range is slower. So it's measuring like the. I think this is probably the P95 and the P5.
02:56:17.455 - 02:56:55.615, Speaker A: It's also doing like a statistical T test. Criterion is trying really hard to give you statistically meaningful information. So when it tells you that it's regressed, it's not just the number I saw was higher than the previous number is given the variance that I see in the results. Is this a statistically significant regression in performance? The fact that I regressed here is probably because this time is so short. Like 5Ns is like too little. And you can see it told us they found some outliers and stuff about the samples. Criterion is using black box for other things.
02:56:55.615 - 02:57:54.895, Speaker A: It can't inject black box into your code in the closure, so you still need to use it in the closure. Ooh, there's an HTML report. Where does it place the HTML report, I wonder? There's a place where it's in like criteria. Normally this would be in the target subdirectory, but I'm. I'm an annoying person and so it's not there for me. Right, so we're gonna have to do index. See what this looks like.
02:57:54.895 - 02:58:28.155, Speaker A: Right. So here it shows me the distribution of timings for calling this particular function. It's showing me across iterations all of the samples that it got and the linear regression across there. There seems to be a sort of of dual mode here where sometimes it takes this long and sometimes it takes that long, which is interesting. Like there sort of seems to be a step function like a bimodal of sorts. And here you see, because I ran it twice, it shows me the change since the previous benchmark. So there's a lot of really good statistics in these reports.
02:58:28.155 - 02:59:07.339, Speaker A: Is there a way to tell criterion to run until you have a statistically significant information? That's always what it does. So if you look back at the result here, you see that it says. Ooh, it didn't. You only see it when you run it. I think you see warming up for three seconds. And then it says collecting 100 samples in that long. And the number of samples it collects depends on the variance and saw in the warm up period.
02:59:07.339 - 02:59:43.213, Speaker A: So there's like, there's definitely like a lot of sleek stuff here. And you see here I ran it again and now it said changes within the noise threshold. Loom is still running. That's great. So this is like an obvious first sort of step here. The question now is what do we actually want to benchmark? Like this is how long it takes to make a hazard pointer. But really what we want to do is how long does it take to make hazard pointers while they're also being made in other threads.
02:59:43.213 - 03:00:21.475, Speaker A: Right. That's really the measurement we want to make. There are a couple of ways we could go about doing this. I don't know that criterion has a great way for dealing with multiple threads sort of thread contention. Let me see. I thought there was a. I can't find it now here.
03:00:21.475 - 03:01:34.483, Speaker A: You know, I wonder if maybe if I go to. Yeah, I didn't think so. Async benchmarker is not what I want. I want. Pretty sure I've seen a way to do this, but I might just be completely misremembering. So I mean there's Iter custom which is basically like it gives you a function to call when you know how long something has taken. Venture also takes time to drop.
03:01:34.483 - 03:02:12.535, Speaker A: That's fine. We want to. So. So one of the things they point out here is that the dropping of this value, you want to think about whether is whether should be measured or not as part of the test. Right. So iter custom takes a closure that is told how many iterations to run and has to return the amount of time spent. See if I can.
03:02:12.535 - 03:03:03.165, Speaker A: Yeah. So like instant now and then start elapsed. So this is an example where we could do iter custom. And then in here let start is instant now and then start elapsed. So here's what we're going to do. This is sort of the closest I think we can get to what they're doing while being within the bounds of how criterion expects us to do this, which is we do barrier is going to be an arc new of. Barrier new of.
03:03:03.165 - 03:04:10.637, Speaker A: Actually, I think there's a way to parameterize this test too. So if you look at criterion benchmarking with inputs. So over here there's a bench with input. Bench with input which we can do concurrent holders and we want a range of values. I sort of want to not give the range, but that's okay. So here what we're going to do is let group. So I'm just following the sort of setup here.
03:04:10.637 - 03:05:29.335, Speaker A: Let mute group is C bench group concurrent new holder. So the idea here is we create a group and each benchmark within the group is going to be for a particular thread count. And then I want to do four threads in and let's just do sort of 1, 2, 4, 8. It's a good place to start. Group dot. And I guess here I need to figure out what I actually want here. So then this is going to be group dot bench with input and it's going to be N threads and then it's going to be B iter custom and just to sort of bring in the types so that this is a little less painful.
03:05:29.335 - 03:06:50.491, Speaker A: And I actually can get some formatting and stuff again. So this is going to be n threads plus one because it's going to be the current thread that's doing the timing. This is sort of mirroring the. I don't know why it's saying file not included in module tree. That's weird. Great. So in here now we can do the sort of bit that, that we wanted to get set up here, which is the original benchmark, which is going to be where's the setup they had here for in zero to n threads thread spawn move this and it's going to bear.
03:06:50.491 - 03:07:58.835, Speaker A: So we're gonna have to have a clone of the barrier for Each thread and it's gonna wait on the barrier twice in a row and then it's gonna do the operation. And I guess we're gonna do this so that we can wait on them at the end. And we're probably going to want to pull this out because all of our tests are going to have the same setup for the benchmarks. But I'm just writing it in line now so that we can test it for this one benchmark and then we can reuse figure out a way to pull out this particular benchmarking harness so that it's easy for us to write other tests that have a different body for basically this line. Right. So that's going to be this part. So that's the operation.
03:07:58.835 - 03:09:02.115, Speaker A: And then here is where it gets. This is where we do barrier dot weight. Then we get the start time, then we do another barrier await to sort of release all the threads to do their actual work. And then I guess we do for thread and threads thread join unwrap and at that point it's elapsed. Oh, and we want to do domain cleanup. How are we going to do that? I guess that's going to be global. How do we do global cleanup? I forget where are.
03:09:02.115 - 03:10:11.775, Speaker A: Ah, it's domain. Colon, colon. Global is where we put it Cleanup and the time measurement. And that's going to be the total closure is expected to take one argument. So this is going to be the number of iterations that we want to do so for in iters. So Right. So the basic setup here is this benchmark is going to be a group of where the group is called concurrent new holder.
03:10:11.775 - 03:10:50.621, Speaker A: Each benchmark in the group is going to be for a particular thread count. And for each thread count we're going to run, we're going to be given the number of iterations to run. And so this might be called multiple times. Right? Because Criterion might run it once with like five iterations and be like, there's too much noise here. Run it again with 100 iterations. When we're told the number of iterations to run, we spin up that number of threads and we make sure that the threads have all started. And then we take the current time and then we let all the threads go.
03:10:50.621 - 03:11:14.835, Speaker A: And then each thread is going to run that many iterations of creating a new hazard pointer in the global domain. Then we're. So we're going to join all the threads, which is when they're all done. Then we're going to call cleanup and then we're going to measure how Much time elapsed. So that's our setup here. Let's see what this does. Right.
03:11:14.835 - 03:11:41.235, Speaker A: So this is with one thread, it's collecting 100 samples. All right, so here it says creating a new holder when there's one thread takes about 5.8 nanoseconds. So remember, it was 5.5 or something, right? And it's saying it's taking a little bit longer because now we also have to like spin up the thread and stuff. Really. This is measuring the overhead of doing the barrier weights and the thread joints.
03:11:41.235 - 03:12:18.487, Speaker A: So here we're seeing this, right? So as the number of threads go up, the time it takes to allocate a new holder is also going up. And the question now, for example, is 701 divided by 8. So you see it's not linear. Right? Having with one thread it took 5.8. With eight threads it took 700, which means 87 per thread. So clearly there's like contention here somewhere. So if we go back, I wonder if I can now load this.
03:12:18.487 - 03:12:52.245, Speaker A: And it's going to be concurrent new holder report index. Yeah. So here you see average time given the input. And you see this looks very much like a sort of exponential curve here where we're clearly not scaling well with the number of cores in terms of making new holders. So that seems like potentially a problem. Interestingly, it looks like it's sort of bimodal for eight threads. Loom is still running in the background.
03:12:52.245 - 03:13:29.437, Speaker A: Let's see if that makes a difference. Loom is still just chugging along. And in fact, here what I can do is just rerun it and see whether it claims that there were any performance improvements. The bimodal could be because of that. For example, I think I only have 12 cores, so eight, there's a decent chance that something got scheduled at the same time. So for one thread there was no measured difference for two, no measure difference for four. So you see the how long it takes to estimate also changes.
03:13:29.437 - 03:13:49.245, Speaker A: No change for eight. It runs fewer iterations. Right, because it realizes that each one is longer has regressed. Okay, so it got worse when I shut off loom. That's interesting. I wonder why. Interesting.
03:13:49.245 - 03:14:24.475, Speaker A: So here it suggests that there is, there's a decent amount of contention in allocating new holders. This is something we, we definitely should be looking into and figuring out where that contention comes from. I suspect that the, the folly implementation has the same problem. And this is probably contention on that allocated holders linked list because everyone is going to be contending on the head. So as you have More threads allocating holders you can tend. You start failing on the compare and swap. And as you fail on the compare and swap, you're wasting work.
03:14:24.475 - 03:14:48.813, Speaker A: And so more threads means more wasted work. So it. So it slows down over time. That's what I'm guessing is happening here. I don't think this is in and of itself a problem because allocating new holders should be relatively rare. Right. In general, you would expect that people keep holders around and what they actually are worried about is the performance of protecting pointers.
03:14:48.813 - 03:15:33.135, Speaker A: Right. The idea is that any given thread can just like really have one holder and then reuse it for all of its different accesses. And so that's sort of the next thing that we might want to benchmark, which is going to be something like, you know, that the next thread they. The next test they have here is testing arrays. So we could test that object bench, you know, object bench might be interesting. So this is just allocating and retiring, but nothing is protected. So I mean we could do that just for concurrent new holder.
03:15:33.135 - 03:16:13.575, Speaker A: So this is an example of where we probably want to start generalizing out this infrastructure because now we end up. Or even just writing a macro for it honestly would get you really far. Right? So we could do a. Maybe. Maybe we should do it. So macro rules, a good example of a macro being helpful. Folly bench.
03:16:13.575 - 03:17:25.895, Speaker A: And it takes a name, which is an identity and it takes a iter, which is a block and it takes an n iters which is an ident and it produces name. This is stringify name. All of this stays the same. This has to be a variable so that it can be accessed inside the block. Actually it doesn't need this. This is just going to be here iteration because the actual value of the iteration shouldn't matter. The thread ID arguably should be exposed, but it doesn't seem like it's super important for right now.
03:17:25.895 - 03:18:29.283, Speaker A: And I think that's all so. So this should mean now that we can do here I should be able to do folly bench, concurrent new holder and a block that is just this. And then we should be able to do concurrent retire and have that be. So what does concurrent retire have to be? Concurrent retire has to allocate an object. So I guess here we can do just this is back to our tests, our folly test the node that we had. And in fact it doesn't even need to be node. This can just be an int.
03:18:29.283 - 03:19:54.463, Speaker A: So if we go back to the thing that makes foos, it can just do this. And this should probably be with Global or object. I forget what it's called with global domain, like so and I mean, let's see what happens. And to be clear, these are definitely micro benchmarks. These don't necessarily tell you, oh look, our thing scales well or our thing works well for all data structures. This is a micro benchmark for how long does it take to make a holder? How long does it take to allocate and retire an object, including the cleanup phase, crucially. So like it's, it's not as though these are the only benchmarks you should have.
03:19:54.463 - 03:20:38.605, Speaker A: You should have end to end benchmarks where you have a data structure that uses this and the benchmarks from there are going to matter a lot more. But it is useful to have these to spot the low level regressions as well. Well, our macros seem to work. So here retiring seems to take 76Ns. So when there's one thread during retires it takes 76Ns, two threads 185, four threads 272. So this seems much more linear. It's not quite linear, but it's not awful.
03:20:38.605 - 03:21:20.895, Speaker A: So if I go here and go to concurrent retire report index, this is a much better line. It's not perfect, but it's much more linear than the other one was. So this suggests that our retires are actually scalable in some sense. And of course we haven't looked at things like what if they're protected and checking for protection and that kind of stuff. But retire and reclamation seems to be all right. I'm running over time and I need to eat. So I think we're going to end it there because now we have some benchmarks and we have a sort of benchmarking setup that we can reuse later as well.
03:21:20.895 - 03:22:07.875, Speaker A: And we have a bunch more tests. So I'm going to commit these, send them in and because this is going to be the last stream, please, like, if you feel like you want to port more tests, if you feel like you want to port more benchmarks, do so submit PRs. That's the way we make this library better. I think the big thing I'm going to do in my sort of, when I get some asynchronous time is going to be also along these lines of sort of adding more of this, but also trying to document this whole thing and maybe make the API a little more ergonomic like the ways we've discussed on stream. If you want to do that, do it, send a pr. I'll be very happy. But those are sort of high on my list because if someone were to actually use this, I would want an interface that's nicer than what we have right now and certainly better documentation.
03:22:07.875 - 03:22:53.355, Speaker A: All right, I think that's where we're going to end it. Any last minute questions before we end? What is the biggest user of hazard pointers? Is it useful to port that to rust? So hazard printers are used for basically any time you want to implement a concurrent data structure. When you don't have garbage collection, you need to figure out basically when it's safe to free those objects. And hazard pointers is one very well known way to do that. Another one is EPIC based garbage collection, which is also really nice but has slightly different trade offs. That's what you get with something like Crossbeam epic. Crossbeam EPIC has some other problems too where it's not.
03:22:53.355 - 03:23:37.181, Speaker A: Its performance is not great in some kind of concurrent situations. That's not necessarily a property of EPIC based reclamation as much as it's that particular implementation that can be improved. But this is another way to do this, Another common way to do this. A final quick overview for why this is better than spamming arcs. So there are a couple of reasons why you don't want to use arcs. The biggest one is that if you're using arcs. Well, there are two reasons actually why you can't use arcs here.
03:23:37.181 - 03:24:18.189, Speaker A: There are two big reasons. The first one is as a race condition. Imagine that you're doing an atomic pointer load and it gives you back a pointer to an arc, right? An atomic load is just that. In this atomic load, all it gives you back is a pointer. And let's say that that's a pointer to an arc. Then now there's a race condition between you incrementing the reference count and the arc getting dropped by whoever is operating the data structure and that memory being reused for something else. And you now modifying the memory location of where the reference count used to be.
03:24:18.189 - 03:25:13.365, Speaker A: So there's a reference count. There's a race condition on the reference count value field. It's kind of maybe possible to work around that with weak references, but it's pretty painful. The other problem with ARCS is let's say you solve that problem. You're now making it so that every read has the right to memory and it has to write to shared memory. Imagine that you have lots of threads all reading the same key from a hashmap, a concurrent hash map. Every read has to clone the arc, which means that every read has to increment the reference count and all the reads are accessing the same reference count field so they're all contending on one value and that's how you end up with performance collapse right? Because you might have 100 threads but all 100 threads are blocked on updating one value and they have to do that serially so they all slow down.
03:25:13.365 - 03:25:32.753, Speaker A: So those are the two main reasons you don't want to use arc. Sweet. I'm going to end it there. Glad it was helpful. It was fun to go through this long both in time and space series on hazard pointers. I think it was fun. I don't know what we'll be doing next.
03:25:32.753 - 03:25:49.615, Speaker A: I think we might return to the wait free lock free sort of adapter that'd be really fun and try to make use of this library there. That's sort of why we stopped working on it. Or we'll find something completely different I guess. Tune in next time and find out. See you all Everyone.
