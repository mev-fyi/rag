00:00:00.160 - 00:01:09.148, Speaker A: Hi folks, welcome back to another rust stream. This one is a little different from others I've done, and I've been doing some streams that are different lately, and it's partially because I'm trying to find new ground to cover that's important, and partially because not everything fits into the sort of crust of rust structure where I sort of take one topic and then explain how it works. In particular, what I wanted to go over today is I have a crate that I take dependencies on and it annoys me every time because it builds slowly and I want to make it build faster. And this, I think is a problem that a lot of people end up having with rust code where it doesn't build as fast as maybe they expect from some other languages or they're not even used to having compiled languages and they're like oh man, this compilation thing is really slow. Um, and they want to figure out what they can do about it. Um, now, uh, faster than lime has a great article on this that goes through a lot of the different steps that you can take and which ones work and which ones do not. I highly recommend reading it if you haven't already.
00:01:09.148 - 00:02:34.620, Speaker A: Um, my take here is a little bit different than what this article goes after. So my goal isn't so much to make it faster for me to build, but more what can I do about the source of the crate that will make it faster to build for everybody? There's not always something you can do. It might be that the outcome of today's video is we can't fix this like the compiler needs to get better, or the underlying frameworks we use need to get better, or support for procedural macros need to get better, or something along those lines. There is the potential that the outcome of this video has no change, but even so, hopefully it'll be useful to see the process that I'm going through to try to figure out what is the underlying sort of, where is the underlying slowness coming from. What we will probably do is do some of the sort of standard tricks, just because, you know, if a lot of your build time is taken up by, let's say, linking, it becomes a little harder to profile the time that comes not from the linker. So we might do some of the like tricks that make it faster just for me, just in order to make it easier for us to figure out where the remainder of the time goes. So the crate in particular that I'm thinking about and that I want to optimize is cargo itself.
00:02:34.620 - 00:03:10.748, Speaker A: And I apologize for this being bright. Let me go ahead and fix that for you all before you start screaming at me. Dark default. There we go. These are also, let me make all of these dark too, before we even get there. So cargo itself, most people are used to using cargo as a binary, right? So you use it to build your projects, to build your binaries, to publish your code, whatever. But cargo also is released as a library, and the library API is basically permanently unstable.
00:03:10.748 - 00:04:10.344, Speaker A: Like every release of Rust is a new zero point x release for cargo, which means it's considered backwards incompatible, so it's not super easy to work against. Goal in general is most things you should be able to do through the stable binary interface as opposed to the library interface. But every now and again you really do need to drop to the library interface of cargo, especially if you're trying to do things like implement a cargo sub command, which I think I'm going to do an impul rust stream on at some point. But for right now, it doesn't really matter what we're going to use it for. In fact, what I'm going to set up is basically a crate that uses cargo, but uses cargo to do basically nothing. All I want is for that crate to build faster because as part of building that, it has to build cargo, and cargo has a bunch of dependencies and also cargo itself takes a while to compile, as we'll see. And so I want to see whether there's anything at all we can do here.
00:04:10.344 - 00:05:10.930, Speaker A: And that might mean improving some of the underlying, some of the underlying dependencies of cargo so that those work better or so that they compile faster. Now, there are a bunch of things that we're not necessarily going to be getting deep into as part of this, but I'll talk about those as we get to. So let's just get. Oh man, my stream title is wrong. All right, let me fix that real quick, just because it's gonna be annoying. Update titles, amazing. Making rust crates compile faster.
00:05:10.930 - 00:05:28.650, Speaker A: No description, or I guess we can put the main description. Where's my tweet? And for those looking at the recording. I know, I know. It's okay, it's okay. Sh. It's okay. Great.
00:05:28.650 - 00:05:56.442, Speaker A: Channel titles updated. I don't know if this will actually update the stream itself, but hey, at least we tried. Okay, so let's just start out by doing a cargo new, let's make it a binary. We're going to call it cargo nothing. Nothing. Because that's funny. All cargo sub commands are named cargo something.
00:05:56.442 - 00:06:29.232, Speaker A: So when you run like cargo space nuffin. What it does is it looks in your path for a binary called cargo Dash nuffin and runs that instead. So there's very little actual magic to it. And what we're going to do is we're going to have cargo nuffin take dependency on cargo. I forget what the latest cargo is. Yeah. Zero six three.
00:06:29.232 - 00:07:05.334, Speaker A: That makes sense. Zero six three. And then I'm actually gonna do a little thing here and make a cargo config toggle. So when you run cargo on your project it also loads this configuration file from your current directory, every parent directory of that directory and your home directory. And this is a place where you can, you can put configuration for cargo itself. So if you put it in your home directory it's going to apply to any project you build with cargo. If you put it in a particular project directory, it'll only apply to that project.
00:07:05.334 - 00:07:40.468, Speaker A: And if you put it at the appropriate place in the file system hierarchy then it will affect anything under but not anything above. And the reason I'm going to do this is mainly because I want build jobs equals htop because my computer is also encoding the stream. So I wanted to not take all of my cores. Let's go with something like twelve. So this is how many cores of cargo is allowed to use for builds. So I'm gonna set jobs twelve. We're gonna put some other things in here eventually.
00:07:40.468 - 00:08:24.610, Speaker A: The other thing I'm gonna do is I'm gonna stick in the sparse registries. Sparse registry equals true, the unstable feature. The reason for this is because it's just faster and I want to help test it. There was a call for testing for this, in case you haven't seen it. This is basically a feature that lets, that changes the way the cargo interacts with the crates IO registry so that instead of like git cloning, the whole thing or git pull, it just fetches is directly over HTTP, like individual HTTP requests and it's way way way faster. And of course because that is an unstable feature, we also have to do rust up override set nightly. So now we have a nightly toolchain.
00:08:24.610 - 00:08:54.976, Speaker A: And let's start out by doing a cargo build. We're going to use the new timings flag that stabilized in rust 161. I want to say and just do a build, see what happens. And let's see. Oh, my computer is very busy. Very very busy computer. And hopefully this doesn't mess up the stream too much.
00:08:54.976 - 00:09:25.640, Speaker A: It shouldn't given that I said build jobs. But you know, who knows. Yes, you see this seems to be one of the crates. That's certainly slowing things down. You see, it spends a lot of time compiling Toml edit. It's spending a bunch of time building cargo itself. And this is why I really want to improve the build time of this, because again, notice we just added the dependency.
00:09:25.640 - 00:09:52.410, Speaker A: We didn't add any code that actually invoked cargo. All we did was add these sets, dependencies, and it adds so much to the build time. And that makes me sad. Come on, cargo, you can do it. Nice. Okay, so you see it saved a timing report to here. So let's see what that looks like.
00:09:52.410 - 00:10:41.990, Speaker A: So this file, if you haven't used this before, it's a very handy tool for figuring out where your build time goes. So when you run cargo timings, it produces this HTML file and it gives you this waterfall diagram of what has to be built. Because remember, in order to build the sort of nested dependency tree, cargo needs to walk the tree from the roots, from the things that don't have any dependencies, and then build sort of going down so that all of the dependencies of a thing are built before the thing itself. This is why, you know, at the root tier, you see things like Libc, which don't have any dependencies of their own. And then further down you can see other things that can't be built until those things have been built. And you see most things compile very quickly, like I two, a 0.2 seconds.
00:10:41.990 - 00:11:12.990, Speaker A: Great. Notice that this is not building in release mode. Even regex syntax is pretty early in the tree and does take a while to build. But it looks like there aren't too many things that actually depend on it. I also suspect it's seen a lot of optimization in this regard. You'll notice also that these have, you see there's sort of a left and a right half of each bar. So the blue part, the left part is the, the time it takes to build the metadata about the crate.
00:11:12.990 - 00:11:49.980, Speaker A: So anything that's needed by a dependency, but not necessarily all of the like, optimized code and stuff. So the, once the blue part of the bar has been done, then dependence keys can start getting built. But you can't actually link the like, final binary artifacts until the purple bar has ended as well. But you can at least start compiling the next crate because any, all the like type information and stuff is ready. It's cargo timing. It's a flag for cargo build. So it's stabilized in cargo itself, I think in rust 161.
00:11:49.980 - 00:12:29.454, Speaker A: Right. So we see that as we go down here we're pushed further and further, right, because these things have to wait for all the earlier things to build. And this is one of the reasons why trying to keep a small dependency tree can help. But especially one that's not very deep is because you don't have to wait quite as long until you can even start your build. You see, like lib ssh two, libgit two certainly end up pushing things further right here. It would be nice if we could try to optimize some of these or reduce the depth here. I think that's probably too large of an effort for us to take on right now.
00:12:29.454 - 00:13:08.010, Speaker A: Um, let's see here. So cert derive takes a while to build, and that, that doesn't surprise me a great amount. So cert derive is basically a giant proc macro that needs to know like how to parse the rust code that it needs to generate, derives from, and then has to have all of the code for doing those derivations. OpenSSl took longer than I would have thought. Maybe because it runs bindgen. Little unclear. Certainly that's a pretty big push to the left.
00:13:08.010 - 00:13:45.758, Speaker A: And you see nothing else builds at the same time. In fact, I'm going to scroll down a little. So you see, you get this plot at the bottom that shows you where your cpu time was spent. And initially we're doing like 100% of the work that we can. We're just fully saturating the cores and that's fantastic. But then we get to a point where there's only like a very small number of dependencies that we actually need in order to build the things that come after it. But they end up being a sort of choke point, like a bottleneck for the build where we can't do any more work because we need to wait for that thing to finish.
00:13:45.758 - 00:14:27.510, Speaker A: And you see a couple of those, right? So the first one over here is, it might be clap or it might be serive. It's a little unclear. But even if you look at the sort of derive and the openssl here, they dwarf in comparison to toml edit and to cargo itself. Right? And that's unfortunate. Notice also that cargo, generally the metadata for cargo takes a long time, and building the sort of final artifacts at the end takes a long time. And that's why this builds end up taking over a minute from scratch. Build, even though it had plenty, of course, is because for a bunch of the time it can't do anything useful.
00:14:27.510 - 00:15:16.816, Speaker A: And so I think what we're going to focus on this time is to figure out what can we do about Toml edit and what can we do about cargo? Is there a way that we can reduce the amount of work that's required to build these? Because that would be fantastic. And you see, it tells us a little bit here at the bottom in sort of table form. What are the worst offenders here? So regex syntax, tomledit cargo itself, clap and regex. So there's nothing terribly surprising here. This build script taking 8 seconds seems unfortunate. This seems like it's building lib two from scratch. So this suggests that if we actually had lib, so maybe it's actually building it from source, like a vendored copy, which suggests that I don't have libssys h two installed locally.
00:15:16.816 - 00:15:38.130, Speaker A: So if I did, that might go away. And in fact let's go and see whether that's the case already. Libsh two. I do have libysis h two. Interesting. Unclear then why this runs for so long. It sounds like it doesn't pick up my.
00:15:38.130 - 00:16:41.990, Speaker A: Oh I, you know, I think I know why this is. If we go over here, we look at this h two, we go to the repo, it's in the documentation. I think this I only know because I've run into this in the past, which is in order to get Libus's h two to pick up the system installed version of libsh use. I'm looking at the build rs. Aha. We need to set libssage to sys use package config to one. So that's one of the things we're going to try is we're going to the next time we build it to sort of diff the output of these is we're going to run with that equals one and see if that gets rid of libssage from there.
00:16:41.990 - 00:17:16.702, Speaker A: The other thing that might hit us here is the amount of time we spend doing linking. So linking is sort of separate from rusty. Like rusty is not a linker and by default it'll just use your system linker. But there are linkers that are often a fair amount faster than that. So for example, there's the mold linker, which is a fairly new development. And mold is significantly faster, especially for larger link tasks. And so we can try to use mold as well and just see whether that shaves a little bit of time off.
00:17:16.702 - 00:17:55.120, Speaker A: And again, this doesn't help everyone. Right. This only helps for my build, but just to sort of see the kind of tricks we can play with over here. So I think there's an instruction over here. Yep. And so let's go back here to cargo nothing and edit our cargo config file and set linker equals clang. And I think these days it's actually ldpathem.
00:17:55.120 - 00:18:57.860, Speaker A: And let's see what happens if we build a new timings. Well, this is clearly cheating because I'm not doing a from scratch build, which suggests maybe it didn't pick up my rust flags. Am I misremembering? I think you can set rust flags and build. Yeah, that's interesting. Oh, I think I also have, let me see if I still have that file here. Hmm. Oh, I have you, cargo targeter.
00:18:57.860 - 00:19:58.206, Speaker A: This is working around a particular setting I have locally. So I have the environment variable cargo targeter set to a different directory that happens to be on a different drive. And that means that I don't get a target directory in the current directory that holds, you know, the output artifacts of the build, which is very handy because it means that the target directory is shared across anything I build with cargo. But it is a little annoying in cases like this where sometimes I just want to blow away the target directory to do it from scratch build. Like in this case, I want to see what the entire build stage looks like now that I made these changes. And yeah, there are other ways you can run things with mold, and that's fine. And I believe there's a way for you to check whether mold was actually used, just to double check that we didn't do anything silly.
00:19:58.206 - 00:20:41.590, Speaker A: So I'm going to go ahead and use this command once this build finishes. So let's do this. Target debug cargo. Nothing. Nope. It build with new, which means it didn't pick up my rust flags, which is a little interesting. Why, why, oh why, oh why, oh why? I mean, I guess I can try to do it the way that they want me to.
00:20:41.590 - 00:21:11.982, Speaker A: Oh, did it actually give me a warning? Unused config key build linker. Oh. All right. Well, let's see if I do it again. It looks like it picked up the rust flags now because see, it decided to rebuild all of the dependencies as well, which it only does that if it detects that. Like something changed, that affects everything that I build, such as rust flags changed. Right.
00:21:11.982 - 00:22:30.550, Speaker A: Because if rust flags change, that means you can't reuse any of the past artifacts you did because there might be an option that changes the code generation behavior of rusty itself. Let's see, we still, still waiting on cargo and tomoledit and notice that changing the linker is, first of all, only going to improve things for you. But also it's not going to affect most of this build time. The linking only really happens in a meaningful way at the end when we build the binary artifacts that then get linked together, all of the intermediate steps of doing things like type check, checking or monomorphization, like all the stuff that Rust C does, those won't be affected by this and they'll still be as slow as they were. All right, let's do our little quick check. Why did it yell at me again? I don't think it yelled at me again. I have mold installed.
00:22:30.550 - 00:22:52.820, Speaker A: Right? I don't have mold installed. Okay, great. Good job, John. No wonder it could. All right, let's, let's try it again. Build. Oh, I didn't.
00:22:52.820 - 00:23:58.200, Speaker A: It's claiming I didn't change anything, which I suppose is true. How about now? Build, build, build. So maybe now you, you get some sense of why I really want cargo to build faster is because when you're working on a sub command, like very often, you don't have to go through this, right? You're not going to do a from scratch build. But anytime, like, some dependency gets updated, it has to do this whole waterfall because the problem is cargo is the only, it's the only dependency we took here. So that means that if anything changes in my list of dependencies, cargo has to be rebuilt. So even with incremental builds and then, I mean both, like rust C, incremental, but also I have built it before and I'm now building it a second time for both of them. It has to rebuild cargo and cargo is slow to rebuild and so I have to go through this process and that is unfortunate.
00:23:58.200 - 00:24:52.520, Speaker A: Yeah, same with CI jobs is the same, right? Where it has to build everything from scratch. I worry less about CI builds because it's time that I'm not waiting. The problem is when it's time where I'm, I don't understand why. All right, fine. I'm gonna, instead of mucking around with this, I'm gonna just do mole run. Build, build. It shouldn't need the full path to mold because at least ldpath should accept a relative path and it wasn't warning me about it.
00:24:52.520 - 00:25:32.140, Speaker A: At least now you get to experience the pain of why I want to do this. Come now, come now, cargo. Come now, cargo. Come on, come on, you can do it. You can do it. You can do it. Come on, come on.
00:25:32.140 - 00:25:52.070, Speaker A: Yeah. All right. Hey. All right. It built with mold. Fantastic. All right, let's see whether that actually made a difference.
00:25:52.070 - 00:26:32.070, Speaker A: So if we go back here and let's go to the top, which has a sort of summary. Total build time 69.9 total build time with mold and with libss fixed 68.8, right, which is sort of as is expected. Like if we go down here and look now, you'll see first, Libsh has disappeared from this list, and that's because of that environment variable we set. So by setting this we're telling Libss H two the crate to try to use the system libss two instead of building from source. This can make a huge difference.
00:26:32.070 - 00:27:17.292, Speaker A: If you have any dependency that's like a native dependency for Libsh two, you have to set this environment variable. For most other projects like libgit two or OpenSSL, it will always try to use your system library first. So you don't generally need to do this. But if something in your dependency graph has this kind of like opt in to use the system one instead of the vendor version, then that can save a decent amount of build time. In this case it didn't. And the reason for that is I have enough cores and the libssage to build can happen in parallel with other things. So reducing its time doesn't actually reduce the longest time, right? So imagine that it had to build like libss two at the same time as 30.
00:27:17.292 - 00:28:04.418, Speaker A: Like it just so happens the dependency graph worked out this way. If Serdi takes longer to build than libss two, even if I make libsssh two finish building faster, the things after it still need to wait for certi. So I haven't actually saved any overall build time, I've saved build cycles, which is helpful if you're more core constrained than I am. But that's sort of all there is. And when it comes to linking, the linking should show up in the mainly at the end, which zoom. What's funny is you can see our binary here at the very end. See that right over here is our binary, and it looked like it shaved 0.4
00:28:04.418 - 00:28:54.624, Speaker A: seconds. And again, it's because this is tiny, like we're linking a tiny binary. So linking isn't really the problem, we end up saving slightly more than that, right? Any build script, for example, also has to be linked along the way, so it's not. And same thing with anything that uses native libraries along the way and actually builds them from source that also has to be linked. So it's not quite that small. But in practice the link step is probably not your biggest concern, unless you have a very hefty binary. Right that makes much less of a difference, unless you have either a very slow linker, like an old GCC linker, or if you have a very large binary, or a binary that transitively brings in a lot of code, like something that links with a lot of things.
00:28:54.624 - 00:29:24.492, Speaker A: And you see, if we compare the build graphs, they're like basically the same. You know, there's not a meaningful difference between these. The only way in which you can look at the difference is you can actually see the difference for lib s h two, which is right here around the, it needs to zero adjust a little better. But see this peak over here and compare it to the peak over here. See how those, it's like missing a little bit. That's lib ssh two that we cut out. It doesn't have to be compiled anymore.
00:29:24.492 - 00:30:14.960, Speaker A: So that's nice, but it doesn't really help us here. Right. Okay, so now that we have discovered some of the things that we can do and why they don't really help, let's try to figure out what actually goes wrong. Let me close these, and we'll close that one and close cargo, and we'll go back to where do we want to start here? Okay, let's start out with cargo bloat. So cargo bloat is a tool that is pretty handy. You'll see what happens when I run it. So what it produces is in your output binary, which symbols take up the most amount of instructions.
00:30:14.960 - 00:30:57.790, Speaker A: So this isn't, you know, how much code went into it. This is in your final binary. Where do the bytes come from? So this helps tell you, you know, if your binary is much larger than you would have expected, what is the cause of that? In our case, though, if we look at target debug, cargo nuffin, you see it's actually a very small binary. And you might, if you're coming from Sealand, you might be like, it's not small enough, but that's not really what I'm after here. Right. Like this binary does nothing. It's this large mostly because of like debug symbols and also probably partially because we didn't optimize.
00:30:57.790 - 00:31:12.526, Speaker A: Right. So we didn't build with dash hash release. So we can do that. We can do a dash hash release build. It's just going to be slower. We could do it with timings. I don't want to do that right now because I care a little bit less about binary size.
00:31:12.526 - 00:32:16.090, Speaker A: In this particular example, what I more wanted to show you was that there aren't that many symbols in here. You'll see most of the symbols in here are fairly tiny, like 19k for parsing out debug info. And this is for stuff like printing, backtracing. And again, it's because our binary doesn't do anything. It doesn't link with any symbols from cargo, which means dead code elimination takes care of, just like not including those in the final binary. If I change source main here and did something like config, is cargo configured new. And I think there's also a config dot configure default, and there's a configure which takes, you know, let's do zero, false, none, false, false, false, none.
00:32:16.090 - 00:32:59.300, Speaker A: No flags. This is what I mean by the cargo library interface is not friendly to newcomers. There's a lot of stuff there. But if I now do a build with cargo bloat, you see, now it starts to tell me about, you know, there's some cargo bits in here, there's some Tom'll edit bits in here. But I. That mostly tells me what code did that I did. I actually end up generating that has to run, that didn't get removed by dead code elimination, which doesn't tell me why the toml edit crate itself or cargo itself took a long time to build.
00:32:59.300 - 00:33:42.376, Speaker A: It just tells me how much stuff from those dependencies ended up in my binary. And, you know, that said, there is some interesting stuff in here, right? It seems like the formatting for inline tables from Tom'll edit is actually kind of large, same with deserialization from cargo configs. And this makes sense, right? Like, what we said was construct a cargo config, which means reading the config files from disk and then deserializing them into an in memory format. So that code obviously needs to make it into the binary finalized table. So this is also tamil parsing hash brown calls. That's fine. So nothing.
00:33:42.376 - 00:34:23.960, Speaker A: That's super interesting. But if I look at this binary now, it's 265, 64 megabytes. Remember, it was 3.9 megabytes. And this is just because previously rust could get rid of everything from cargo and all of its dependencies, because we didn't actually link against anything. Whereas now that we do have some code from cargo, we need to bring in, suddenly all of these little tendrils of dependencies mean that we pull in a lot of code into our binary that wasn't needed before. You'll see there's like here decompression, but none of these still are very large.
00:34:23.960 - 00:35:06.120, Speaker A: The big thing here is probably going to be debug symbols. So if I strip this binary, it's now 1.6 megabytes. So strip here just removes debug symbols. So again, this is why, you know, when people complain about rust binaries being large, usually it's debug symbols. It's not always, but again, here, debug symbols made a huge difference, because, again, all those dependencies that go, all those call chains that come from our little config default ends up touching a lot of code. And for every piece of code that could possibly be executed, for every instruction that ends up getting generated, we need the appropriate debug symbols, which means that so many debug symbols end up getting pulled in.
00:35:06.120 - 00:35:55.418, Speaker A: Of course, the downside of stripping out the debug symbols is if I now run cargo bloat, it can't tell me anything because it doesn't know any symbols, which is unfortunate. So let me go ahead and remove that again and then rerun the build. Rerun the build, does it not? Oh, it's because cargo. This is maybe interesting for some of you. Cargo actually hard links its binaries. So if you look at the hard link count for this file, it's two. So removing this file and then rerunning cargo just causes cargo to recreate a hard link.
00:35:55.418 - 00:36:31.924, Speaker A: It doesn't actually rebuild the file. I forget where the original is. It's not that one. It's in, I want to say, depths cargo, nothing. Yeah, it's that file. So if I remove that file and I remove this, then I think cargo will rebuild. Or I may just have broken cargo's expectations.
00:36:31.924 - 00:36:50.062, Speaker A: No, great. So now if I look at it, it's big again. So people strip binaries, because sometimes you're constrained for how large the file is. Again, this doesn't really help us, though, so let's try turning to a different tool. So there's another tool that. So cargo bloat looks at what's taking up space. In your.
00:36:50.062 - 00:37:38.382, Speaker A: In your executable, there's one called cargo LVM lines, which looks at the number, the amount of LVM IR. So this is the intermediate representation that rust C passes to LVM for building to figure out what is generating the most amount of work for LVM to then do code generation for. And if we go back to our build times here, this is sort of the boundary. It's not quite, but sort of the boundary here between the blue and the purple, where the purple is. Here's all the stuff that rust knows about this program. Give that sort of. It's sort of a miniature version of rust or a simplified version of rust, or I a slightly fancy version of assembly, depending on how you want to look at it.
00:37:38.382 - 00:38:42.430, Speaker A: The cargo then gives to LVM and says, now give me machine code for this. And that time is going to be LVM doing things like optimizations. And a lot of time can go in there, especially if cargo ends up passing a lot of this intermediate representation code to LVM, then it's giving LVM more work to do. And so what cargo LVM lines does is it tries to figure out what is the origin, the source for a lot of that iR. So if I run LvM lines, it's going to tell me, let's do something like head and 20. Well, that's interesting. So a lot of LVM from freeing from dropping configs, which is interesting.
00:38:42.430 - 00:39:28.480, Speaker A: The copies here, by the way, is. So imagine you have a generic data structure, like think a vector. Remember, vector is generic over the t that's contained. And what ends up happening is if you have, say, a vector of u, you have a vector of booleans, then cargo will generate, and you're using both cargo, or rust rather, will generate code for each one individually so they can be optimized, you know, according to what the inner type is. But that means that, you know, let's take vec push. You're actually going to have two copies of VEC push in your final binary. One for when you're pushing a U 32 into a Vec U 32, and one for when you're pushing a bool into a Vec bool.
00:39:28.480 - 00:40:31.002, Speaker A: And copies here is telling us how many copies are there of this particular, you know, generic invocation. It's interesting. The drop shows up a lot here, although one thing to keep in mind here with LVM lines is this is in fact, it's not that surprising if you think about it. This is saying how many lines of LVM is rust C giving to LVM when building this crate? And that's very little, right. If you look at main, like it's just the IR we generate for this code, which is very, very little. So this one is going to be more useful to run on the dependency that we think is slow than it is to run on this, this crate. Because again, if we go back to our build information stuff, the actual time it took to build our binary is very little, including all the code gen and linking and stuff.
00:40:31.002 - 00:41:35.060, Speaker A: So what we really want to do is run cargo LVM lines on cargo or on Tom ledit to see why they are generating so much work for LLVM. So we'll do that in a second. I want to see if there's anything else I want to talk about before get into that. I'm going to talk about one other trick first that I recently learned about. So there's this unstable feature in Rust C called share generics, and I think eventually the hope is to stabilize this. But what this feature or this flag to rusty does is imagine that you have two crates in your dependency graph and they both depend on hash brown. So the implementation of the hash table, both of them are constructing, let's say a hash table from string to u 32.
00:41:35.060 - 00:42:56.668, Speaker A: What's going to happen with monomorphization is that rust will monomorphize separately in each crate. So it's a monomorphization happens in the consumer, not in the vendor. Right? So it's not the hash brown crate that has to be built for every possible combination of type parameters for its types, because it doesn't possibly know all of the ways it might be instantiated. So instead it's when foo builds or first creates a hash map with string and bool, or when bar first creates that type, then it will be built. The more modernization, the copy paste and then build of that type will happen in the context of that crate. But this is kind of wasteful, right? If you have, if you have foo and bar and they both instantiate a hash map string bool, then why are we doing that monomization twice? Why are we doing code gen for it twice, once in each crate when we could do it once and then share it between them. So that's the idea for shared generics, is to try to identify opportunities for us to share that monomorphization so we don't end up with multiple copies of the instantiation of generic types and methods and functions when they're actually multiple copies of that same set of type parameters? I just want to see whether it matters here.
00:42:56.668 - 00:43:54.340, Speaker A: And the way we're going to see whether that matters is we're going to go into our, no, we're not even going to do that. We're going to go here and we're going to set rust flags equals C share generics, and we're going to do cargo build timings. So that's going to build everything. Again, it might make no difference, right? So this is going to make a difference for any, only for the types that happen to share monomorphizations. Which might be very few. It might be that the way the cargo dependency graph works out, there's not actually that much sharing that's possible. It's also a nightly feature, so who knows how well it'll actually work in the end.
00:43:54.340 - 00:44:35.028, Speaker A: But that's why we want to try it out. And the hope of course being that if this speeds things up, that is eventually something that's going to land in rusty itself. You don't need the nightly flag for it, because it should be a transparent optimization to us as the consumers of the rust compiler. We just expect that it's going to avoid doing unnecessary work multiple times. Certainly still takes a while. I think it saved about 2 seconds. Okay, so build time went from 68.8
00:44:35.028 - 00:45:06.502, Speaker A: to 66.9. Hard to say how much of that is just variants like just noise, but if we scroll down, let's see if there's anything useful in here to go down to. Remember. Also, this gets distorted a little bit by the fact that builds get to happen in parallel. So even if it could be that it made many things faster, but it didn't make them, it made the wrong things faster. Like things that already were being overshadowed by something else that took longer. So let's look specifically at Toml edit, for example.
00:45:06.502 - 00:45:38.520, Speaker A: So it used to take 23.26 and now it took 22 57 cargo from 40.3 to 39.23. Okay, so very small improvements, but improvements nonetheless. And so, you know, this seems like it's a worthwhile optimization. It's not like it does reduce compile times, and especially if it does this across the board, you know, it adds up to seconds, but it's not a magic bullet. It doesn't just fix this problem for any everything.
00:45:38.520 - 00:46:04.656, Speaker A: And the reasoning for that is, it's true, when something shares monomorphizations, this saves work, but that's not always the case. In fact, quite often they won't be the same. Right. If I make a hashmap string bool, you make a hashmap string mu 32. There is no sharing that can happen. And so this feature effectively does nothing. And there's a cost to it too.
00:46:04.656 - 00:46:58.260, Speaker A: This means that more sophisticated analysis has to happen in the compiler to figure out where the sharing is possible. So it might be that it even slows down some builds. Right? There's no free lunch here. Did it change the binary size? I'm guessing probably not, yeah, because again, remember there's very little code we actually end up pulling into our binary dead code elimination is going to get rid of most of it. So this would only, this is not just, is there any sharing in the entire dependency graph, this is, is there any sharing in the code that gets generated for creating a cargo config, configuring it and dropping it. And in practice there's probably going to be very little sharing there. And now I need to remove this and deps cargo nuffin this one.
00:46:58.260 - 00:48:00.800, Speaker A: Okay, so not, not very many helpful answers here, really. So far. Most of it seems to be telling us that we need to be looking at cargo and looking at Toml edit because that's where the source of the problems is. And looking at the current crate isn't going to tell us that much because the current crate builds pretty quickly. In fact, you see, with shared generics, building our binary took longer, probably because it has to do this extra searching and the base build is so fast. Okay, so let's then switch over to cargo and see what we can do over there. And I'm going to go ahead and copy r my sorority cargo here, no, cargo nuffin, cargo over here, just so that I get these same things.
00:48:00.800 - 00:48:29.150, Speaker A: And then I'm gonna, I'm actually just gonna ignore all of these. I don't really care about using mold. Right. It didn't really make much of a difference in practice. Shared generics didn't really make much of a difference in practice here. Lib ssh only fixes, you know, the bug arguably in the build process, and it didn't even speed up the build process by that much. So I don't care about it too much.
00:48:29.150 - 00:49:31.730, Speaker A: I will keep the resetting the cargo targeter just so I have the target directory in the current directory so I can blow it away more easily. So let's then look at, I want a cargo bill liberal. I don't actually care about building the cargo binary because that doesn't get built when I use cargo as a dependency and it's going to save a bunch of build time and linker time, not to mention because the cargo binaries are actually decently large. What else do I want to. Let's just kick off that build first and foremost. And the reason I kick off this build is because I don't actually care about the dependencies here anymore. Notice that all of the settings that we changed when we were dealing with cargo nuffin were like configuration settings for how cargo builds things, how rusty build things, and therefore they affect the entire dependency tree.
00:49:31.730 - 00:50:54.370, Speaker A: But looking at this right, realistically, all of the time that we want to get rid of is spent in cargo itself and building this crate so building all the dependencies is something that we can just build them once and then just tweak with cargo itself. So I think that's what we want to do here. Tomoledit has the same property, right, of we should be able to improve tomoledit as well. I do have a sneaky suspicion that what actually happens here is that cargo ends up recompiling or the toml edit ends up generating lots of IR. That cargo then ends up also emitting, probably because of things like deriving serialization and running, like sorted, deserialized and serialized on generic types that are defined in cargo itself. And so really what we're seeing here is the IR generated by toml edit is very large and therefore building both tomledit and cargo takes a long time, but we'll see that in a second. All right, so let's now go ahead and do lvm lines and see what lvmline says.
00:50:54.370 - 00:51:56.116, Speaker A: Unclear why this decided it wanted to build crates IO instead. Yeah. See how long building just cargo takes. So the hope here, right, is that this command is going to output what is the IR or what is the source function or type that ends up generating the most IR for LVM to then have to work on. And the hope is that that's going to point us to something where maybe we can just make it not be generic. Right. So imagine you have something that's fairly long, like it generates a lot of IR and it's also generic.
00:51:56.116 - 00:52:57.870, Speaker A: It might be that a bunch of the code is actually shareable between different types and doesn't need to be generic. Let's see here. There are a lot of copies of iterator trifold and option map and vec extend. That's interesting. Iterator trifold and option map make a lot of sense because they're, they take closures and so basically every time they're invoked, they're going to be a different type. So you see there are a lot of copies of them and they end up accounting for a lot of the lines in the upper binary. What I should have done is actually do this so that I can walk through the file instead Certi, Jason, deserialize, there are a lot of copies for.
00:52:57.870 - 00:53:51.520, Speaker A: And deserialize is generic over the reader. Is it also generic over the t? I think so. 31 copies of that. So this one's interesting. So notice here cargo's tommy. So deserializing a cargo toml project, which I think is just the toml. In cargo Tomml itself, there are only three copies of it, but it accounts for like half a percent of all of the IR that's given to LVM in the first place.
00:53:51.520 - 00:54:39.670, Speaker A: So this one is not even many copies. It's that the single implementation is very large. What else do we have down here? Surdy ignored map access. Yes, a lot of this like vex stuff is hard to get much out of B tree. This is inserts into b trees of various different types. So one way that you can speed this up, right, is to have more, more maps be the same type, but you might not even have the option to do anything about that. Yeah, so you see there are a lot of copies of these b tree types.
00:54:39.670 - 00:56:00.090, Speaker A: Same with hash maps, multiple copies of hash map being brought in. But we see even more here of the deserialize implementation for the various cargo Toml types. And if we look at cargo util, toml, toml manifest, for example, util, tomlmod, you see that it actually just derives deserialize. And it derives deserialize, which means that what? Oh, and you see Toml project here being the large type, it also derives deserialize. And so what this suggests is that the derivation of deserialized for Toml project generates a lot of code. And if you look at it, that's not surprising, right? Look at how many fields there are, and each of these fields are themselves pretty large. They all contain this maybe workspace which has tamil workspace field which has defined their generics over t string or vec.
00:56:00.090 - 00:56:54.276, Speaker A: And it in turn contains a toml value. I wonder whether, if we got rid of this, whether it would build a lot faster, because tomlvalue is a fairly complex type. I think because a toml value is, you know, it can be one of the base types, but it can also be a table. And a table is a map from string to value. So it ends up having to generate all this like nested deserialized code. But certainly I can see how this ends up generating a lot of irrede. It might be that we could write a more sort of manual implementation of deserialize here that ended up being more efficient, but that might be premature.
00:56:54.276 - 00:57:42.490, Speaker A: Commenting this out might be really interesting. I wonder if I go here and I get rid of this, I get rid of this and Toml dependency detailed. Toml dependency. Okay, so just to see if I now run no meta, I don't even know if it'll build like this. Of course it didn't. 1560. This is a very hacky thing to do, right? I'm just trying to see whether that generates a workspace root config which is not deserialized.
00:57:42.490 - 00:58:15.740, Speaker A: So what if we just here pass in like, none? Well, let us do that. Great. What else do we get? 2057. So custom metadata went down here. Can that be none too? Maybe. And 20. 217.
00:58:15.740 - 00:59:16.080, Speaker A: None. I just want to see whether the, the suspicion that it's actually the deserialized for toml value, that ends up being very large, whether that's the case. Because if so, it might be that the thing we actually need to do is optimize how much IR is generated for the deserialized implementation of tamil value, which would be in the toml edit crate. You can run LvM lines with rust flags. Hey, we have David in chat who made LvM lines. Let's do that. So env this.
00:59:16.080 - 01:00:12.152, Speaker A: So the symbol mangling version is rust has a. An old symbol mangling scheme. Symbol mangling being how do you take the actual name of a function so. Or a type or whatever that's generic and has instantiations of different types or closures and whatnot, and turn it into like an ascii string that you can embed in, embed in debug symbols on the like. And rust used to have one that was like, kind of like c, and then just had some kind of random ways in which it tried to shove all the rust looking types into that format. But that means that it's a little bit lossy in types, in cases where you just don't get as much information, you don't get as rich information out. So there's now a standardization of the rust symbol mangling scheme called v zero, because the previous one didn't even have a version of.
01:00:12.152 - 01:01:06.400, Speaker A: And it is not lossy in the same way. It includes more of that information in a way that you can reliably get it back out. And so I think what David is saying is that by doing this, you actually get every unique instantiation of the generic functions and you actually get to see where they're coming from or which types they're being instantiated with. Now, of course, I made the mistake of not timing the build without this step, so it could very well be that this was slower or faster and we won't actually know. What we will know, though, is whether we still see the same amount of ir show up for the DC realization stuff now than we did in the past. If it's less ir in general, that's going to mean shorter build times. And, I mean, we can easily stash, build, retry so that's not a great concern.
01:01:06.400 - 01:02:07.240, Speaker A: Let's see what we get out the other side here. Certainly still slow to build, but again, cargo is large, it's a large project, and it's not as though we expect it to compile in a second. Right? Like there is a lot of code and so it should take a while to build. It's more that we're trying to figure out should it take 40 seconds to build in debug mode? To which I feel like the answer should be no. But if the answer is no, where is that time spent and how can we remove it? It certainly seems like, you know, the deserialize of toml value is not the. If it makes a difference, it doesn't make a huge difference, right? Because otherwise this build would have finished already. So it's still taking a long time to build.
01:02:07.240 - 01:03:09.584, Speaker A: The question is, you know, can we reduce it by a few seconds? Because if so, that's still a big win. All right, let's see what we get out the other side here. Oh yeah, we get way better info now. That's nice. All right, so Tom will manifest, two real manifest. So this is actually the serialization, let's see, deserialization of inline tables of maps. Yeah, I think this is all of the, this is the Toml deserialization protocol.
01:03:09.584 - 01:03:51.432, Speaker A: So it's not, at this point, it's not the code that gets generated by serialize and deserialize. It's actually the. So the way Certi works sort of is that there are two parts to serialization and deserialization. There's the type part and there's the format part. So the type part is a type implements serialize or deserialize. And what that means is it presents a standard interface for walking that type to discover what is inside of it. So when you derive, serialize and deserialize, you don't really see this, but in general what this means is you create this visitor thing.
01:03:51.432 - 01:04:43.956, Speaker A: And what a visitor is, is a structure that implements a particular set of certi APIs that most people don't really think about. That allows someone to say, can this type be turned into a string? Can this type be created from a string? Same for, you know, integers, bools, the other primitive types. But also can it be turned into a map? Can it be read from a map? Can we turn them into array? Can be made from an array, but all of that is independent of the format. So that part is just about the type. And figuring out what can you construct this type from and what can you turn this type into in terms of other well known types? And then there's a format side of this. So this is the serializer and deserializer. Notice that the r at the end there and those implement formats.
01:04:43.956 - 01:06:13.544, Speaker A: So an example of this would be, you know, Sortie Jason or Toml. And what the format side of things does is it says give me a string or give me bytes and I will use my knowledge of the, of that format Spec to turn it into calls to the type based traits. So for example, if it finds a string, it's going to call serialize string on the underlying type that you said you wanted to deserialize into. When it finds a number, it's going to call serializenumber. And the idea here being that if you, if you have the implementation of all the visitor, the types or all of the things that a type can be turned into from, and you have a thing that gives all of those input types, then you can map them together to get deserialization and serialization for any type, for any format as long as they are compatible in terms of the base types that they use. So maps being a good example where if you have a struct in rust, it can often be represented as a map, and that's how it will be represented in something like JSON as what we're seeing here is, you know, the serialization ends up being sort of generic over two things. It ends up being generic over the format and the type, right.
01:06:13.544 - 01:07:11.300, Speaker A: So the type here comes from the derived deserialized, derived serialize, and that's generated by the sorted derived crate. And that just generates that mechanism for walking something. So again, if you do something like derived deserialize for a struct, what that's going to do in the serted derive crate is it's going to walk the struct and construct it by reading keys from a map where the names of the fields are used as the keys in the map. That's the sort of the expectation, the contract that it implements. So it's going to use that, and then it's going to use the TOmL format that comes from the Toml edit crate. And so that describes how do you walk a toml file and produce things like maps and strings and numbers. And similarly, given the type that's walkable, how do you turn that into TOMl constructs like ASCII constructs in toml such as strings and numbers and boolean maps.
01:07:11.300 - 01:08:32.479, Speaker A: And so what we'll see here is TOMl project which is a type and you see as deserialize, right? So this is the TOMl project type being used as or it's deserialized implementation being given to a, where we have it given to a TOml edit inline map access. So that's the format side of things. So this is basically creating a TOMl project from a toml table is the way to read this very complex type. That's what that turns into. And it makes a lot of sense, right? Like this is saying, basically this function that we see right here is all of the code that's necessary to turn a Toml table into cargo's internal struct representation of that table. Specifically for the Toml project type. And TOMl project is this thing which has all of these, all the fields that you can possibly set for the project section of a cargo TOMl file.
01:08:32.479 - 01:09:44.840, Speaker A: And it doesn't look like removing the toml value did a huge amount of stuff. But let's see if we can't. If I stash that change and then I run this again and I call that with meta or just remove the no meta because I want to compare, I want to see them sort of side by side and see did we meaningfully reduce the amount of code that gets generated? And it is true we will have reduced it somewhat because there's one fewer fields. So by necessity there's less code to do in that parsing. The question is, did we make it a meaningful dent in the build time? And notice that even if we did, it's not clear this is a sustainable change, right? Because the metadata is there for a reason. This was more me on the suspicion that maybe the toml value type has a particularly large amount of ir that gets generated for it. And so by not including it in this type, we're saving a bunch of IR and therefore a bunch of work for cargo LVM.
01:09:44.840 - 01:10:36.000, Speaker A: And what this is going to show us is basically how much did we save? I suspect the hashes are actually going to be different. But if I look at this file, actually maybe they're not. Let's see what happens if I do lvmlines text and no meta. And I'm going to zoom out a little here. I know you can't actually read the text. I just want to be able to sort of skim the difference. You see, the IR for a bunch of things didn't change, which is sort of what we expected, right? Because even though we removed these fields from some of the structs, there are a bunch of structs that didn't change.
01:10:36.000 - 01:11:17.060, Speaker A: And so we would sort of expect that they ended up generating the same amount of IR under the assumption that compilations generally deterministic. You see, some things moved places because they ended up using less time. And if you. I guess I can zoom in a little bit here somewhere like here maybe. So you'll see this first one went from 7531 lines of IR to 7499. So, you know, we cut it by a small percentage. This is specifically the turning a tommel manifest into a real manifest, because that needs to walk one fewer fields.
01:11:17.060 - 01:11:44.140, Speaker A: Same with the code for parsing out a toml project. Almost certainly. Yeah. So tamil project went from 6005 to 6003. So again, like 200 lines fewer of irnae. Not super impressive, but it doesn't make a small difference. Let's try to see if we actually did the cargo build here.
01:11:44.140 - 01:12:17.844, Speaker A: So again, this is with the changes stashed. So I'm just going to open them side by side and see in terms of the actual build time for cargo overall, did this make a difference? I suspect it's going to be tiny. Tiny. You know, point, percentage. The meta value did not seem all that big. So the meta value isn't large, as in like, the type is not large, but that's not really what we're after. What we're after here is actually more of a.
01:12:17.844 - 01:13:10.660, Speaker A: It's almost a little hidden from you, which is when that type says derive deserialize, that generates a huge function for you with lots of code that Rusty has to build. And so what we're doing by removing that field from r type, that means rdserialize doesn't have to call deserialize on that type, which means its ir doesn't need to be taken into account. It might not even need to be built at all. So let me do this, let me open that report and then do another build with timings. So this is the time it takes to build cargo itself without our meta change. And it took, you know, what, 39.4 seconds to just build cargo.
01:13:10.660 - 01:13:57.770, Speaker A: What's also interesting here is, you'll see is that, you know, for the LVM code generation, it actually ends up being able to use, you know, four cores, which is nice. Whereas this metadata generation step that we do before we hand stuff to LVM, it can't even use one cord fully or one code generation unit fully, which is a little sad. Although that might just be because it's one crate. That might just be because it's one crate. All right, so this ran our new build, and it went from 39.3 seconds to 32.45. Again, could be noise.
01:13:57.770 - 01:14:27.340, Speaker A: That's not at all what I wanted to copy. And in fact, you know, we could run timing on this, but. Oh, that's because it just ran cargo. So if I run this, I'm gonna have to do it again. Now the other one compiled the dependencies as well, so it's an unfair comparison. So I stashed again and I'm going to run the build again. So this, it's not actually a seven second difference.
01:14:27.340 - 01:15:56.820, Speaker A: Would merging surd into STD help? Merging serdi into StD would not make a difference here, because at least some of where this time is going is the serializa, the walking code for the TOML project type that gets generated, and the formatting, the code to handle the TOML spec, the tOML format, those both have a bunch of ir, and that's specific to the TOML project type, which is in cargo, and the tOML code which is in TOMl edit. And neither of those are about 30 itself more as the interface between the two. Okay, so removing that field in fact made the build slower, which basically means this is just noise. Like in practice it had no actual difference. That's interesting. So that suggests indeed that the, there's not actually that much IR in, in meta. Or it might mean that there are still other places where tomo value ends up getting deserialized and so we still have to compile the code for it.
01:15:56.820 - 01:16:38.642, Speaker A: In fact that code, I'll talk about that in a second. Yeah, this makeshi, the more I think about it, the more this makes sense, because the deserialized code for Tom will project. All it means to have this field is that we're going to call deserialize on this value. And this is the call to then deserialize. A Tommel value is just a function call within that code, so it doesn't generate that much IR. The IR comes from the fact that Toml values implementation of deserialized still has to be compiled because still probably being used somewhere. And so that IR still gets generated.
01:16:38.642 - 01:17:44.710, Speaker A: It's just separate from the implication of this. So I think in practice removing this is unlikely to make a difference. Can you replace the serialize and deserialize implementations with a dummy? Oh, certainly. Let's do, I don't think we even need serialize. So for ToMl project, let's do impulse 30 serialize for ToMl project. See what happens if we do that. That doesn't work because cargo really doesn't like if we have warnings.
01:17:44.710 - 01:18:44.460, Speaker A: IR is the intermediate representation, which is basically the language that rust uses to talk to LVM. So it's not rust. In fact, it's unrelated to rust. It's a language that the, that LVM accepts as input into its code generation pipeline. It's sort of kind of like assembly, but it's a little higher level. It allows more annotations that LVM can use for things like optimization later. Okay, so that did cut the build time not by much, but by a second.
01:18:44.460 - 01:20:10.790, Speaker A: And in fact, if we run LVM lines again, in fact, let's just look at the LVM lines we had last time. So Tomo project was one, Toml manifest is another. So let's do the same thing for Toml manifest. And the reason we're doing this is mostly just to convince ourselves that this is the reason why, or rather to sort of see how much could we possibly improve on this. And I think there was one more in there that sort of showed up, which was inheritable fields and intermediate dependency. So let's go ahead and find intermediate dependency and inheritable fields. See what that does.
01:20:10.790 - 01:20:51.800, Speaker A: Right. And that means these fields have to go away, because the, these certi instructions are basically a part of the derive macro. So because we remove derive derived deserialize, these won't be parsed anymore, although they will be picked up by serialize. So it's only for this one, which only derived deserialize. It did not derive serialize then there's now nothing to pick up this, this attribute. And same with this one. And I probably missed one more.
01:20:51.800 - 01:22:02.330, Speaker A: 296 no. 269 right. That's intermediate dependency, which is actually generic. So we need to say it is generic over any pknorthenne where p implements. In fact, for any p. For now, that's fine. See, what that gives us and what this will do is basically give us a sort of upper bound on how much time we could save in the build if we spent lots of times, like, hyper optimizing this.
01:22:02.330 - 01:22:39.356, Speaker A: And, you know. Okay, so this was getting rid of Tomo manifest, Toml project, and inheritable fields and intermediate dependency. And you see a cut build time from 32 seconds to 29.8 seconds. That's a decent cut, but there's still a bunch of time being spent somewhere, so let's go ahead and see what remains. No op. And it might be at this point that it's no longer deserialization code.
01:22:39.356 - 01:23:10.470, Speaker A: Right. Certainly it seems like deserialization was a decent part of the IR, we cut 3 seconds, we cut almost 10% off of build time, right. From 30 and cut 3 seconds, 10%. So that's pretty good. But it's not, you know, it doesn't mean the cargo is fast to build. So there's still going to be something left that we wanted to explore. And we could probably have noticed this if we looked at the old lines output.
01:23:10.470 - 01:23:58.340, Speaker A: Right. Because if you add up all of these together, so 0.20.40, we don't quite get there. So it has sort of, it seems like it has an outsized impact because it means there's less code for rusty to do and this less code for LVM to do. So it's not just a direct translation to the percentage of irrede. All right, so let's look at this no op and see what's left now. So there's still deserialization for.
01:23:58.340 - 01:24:34.430, Speaker A: So this is sort of jason for diagnostic server. That's interesting. This deserializing CLI unstable. And the reason for that is because if we look at, uh, that's under feature, I want to say source cargo core features CLI unstable. See if I'm telling the truth. Yeah. So this ClI unstable struct is constructed by this macro.
01:24:34.430 - 01:25:31.770, Speaker A: And this macro basically creates a field for each one of these. So again, the IR is, is quite simply a function of how many fields are there. Because the larger the struct is, the more ir that certain needs to generate for this type in order for it to derive serialize, because the derived serialize basically needs to walk every field. So it's a pretty direct translation. But then, you see now we start getting into, you know, things like cargo clean, generate some IR, but none of these are a lot of IR, which sort of suggests that, okay, some of this is, we're giving LLVm a lot of work to do. Right. If we look back at this one thing you'll see, I think if I scroll down enough here, is that when we cut down this time, a decent amount of the time we cut is actually from LVM co gen.
01:25:31.770 - 01:26:21.592, Speaker A: It's not all from LVM Cogen, but we're giving LVM less work to do. And therefore the LVM part takes less time. But the cargo stuff actually takes more time now, which is a little interesting. It goes from what, four maybe, let's say 5 seconds to closer to seven. But LVM takes a lot less. So reducing the IR here has helped LvM a lot, but it hasn't made the Rust C part any faster, which sort of makes sense, right? It suggests that the time that Rust C is spending is on things like type checking, which LVM doesn't care about. And by removing these deserialized implementations, we've removed a lot of sort of trivial rust code that the Rust C probably went through pretty quickly anyway.
01:26:21.592 - 01:27:37.480, Speaker A: You could imagine something, you can imagine a, you know, a trait bound or something that is really complicated for rusty to compute, but once it's computed and type checked, it, it doesn't generate basically any IR for LVM. And so the purple will be very small, but the blue will be very large. And what we did here, by removing the, the LVM IR stuff that the, the very large chunks of LVM IR is, we gave LVM less work to do, but we don't actually know where Rust C is spending his time, which is what we need in order to cut that blue bar. And again, there's still a lot of work here that LVM is clearly doing that we could probably optimize too. But let's take, let's switch gears a little bit and look at what can we do about that blue part, about the rust part of compilation and not the co gen part of compilation. One of the reason why it's important to focus on that blue part, even though the purple part is large, is, remember in that large build tree that we have in that, in that water flow diagram in the very beginning, remember I said that you can't start building dependencies until the blue part is done, right? So for example, cargo depends on Toml edit. Cargo couldn't start building until the blue part of Toml edit finished.
01:27:37.480 - 01:28:25.760, Speaker A: And the reason for this, right, is because to build cargo, you don't need the code gen output from LlvM because you're not linking yet. But you do need tommel edit to have gone through things like type checking, because you need to know what types are available so that you can do type checking of cargo. So that's the distinction here to think about. And so I think that's the reason why we really want to shrink that blue part, because it allows more things to start sooner. If imagine that the blue part of Tomlin was like zero, then cargo would get to start. You know, I don't know exactly how long this is, but that's about what, 8 seconds. Cargo would get to build 8 seconds, start building 8 seconds sooner, which is going to cut the whole build time by 8 seconds, because it gets to do more things in parallel back here.
01:28:25.760 - 01:29:32.320, Speaker A: So then the question becomes, okay, we've looked at what we can do about the purple part, which is generate less ir, that's the main part of it. And cargo lvm lines is a great tool for doing that. What do we do about the blue part? So for the blue part, let's see, I talked about this, I've talked about this, talked about this. We'll close that. So for the blue part, there's a different mechanism you can use, and that is rust C itself has a nightly flag called Dash C self profile. And what that does is it runs the compiler in a mode where it records where it's spending its time, and it records it in such a way that you can then open it up and like visualizers of various kinds to look at that in a graphical way. So let's go look at that.
01:29:32.320 - 01:30:32.674, Speaker A: And I'm going to skip ahead a little bit and go over to Amos's great article and steal this. So dash C self profile is useful in and of itself, but we want some additional information. And this additional information is basically, don't just record which function you're running like inside of the compiler. It has like a function that does monomorphization. And if it told us I spend all my time in monomorphization, wouldn't really tell us very much. We want to know what it is monomorphizing. And so what this additional flag does, it says when you're self profiling, then also include, in addition to the, the default events also include the arguments for every function.
01:30:32.674 - 01:31:34.774, Speaker A: And so what this is going to enable us to do is figure out which call to monomorphize did you spend your time in? And you'll notice I run cargo rust C here, and that's because I want to build the current crate with these additional z flags. But I don't need to add it to the rust flags environment variable because that would build my entire dependency graph with self profile, which I don't care about, I just want it for this invocation. So let's on the nightly channel of cargo. But this is the beta channel. Oh no, I didn't, did I? So it's gonna have to compile all the dependencies again. What? Oh right, the dash dash is needed here because these are arguments to Rust C, they're not arguments to cargo rust C. And the distinction is kind of annoying.
01:31:34.774 - 01:32:11.786, Speaker A: But you could imagine that the cargo rust see sub command took arguments, and those arguments are not necessarily the same as the ones that rust see itself as the command line tool rust circumental accepts. And this dash dash lets us separate the things that cargo rust see should interpret it as its arguments, as opposed to the one it should pass in. And this is going to have to build the dependency graph again. And that's fine. I just want to build lib. So that's an example. Dash dash lib.
01:32:11.786 - 01:33:12.880, Speaker A: Here is an argument to cargo rusts C. It's not an argument to rust C. And so that's why this dash dash is needed to separate the two. Look how many times we've compiled the dependency graph of cargo. So someone said in chat, is the build time a function of the number of IR lines generated? So sort of. Right. The more ir you give Lvm to do co gen on, the longer it's going to take Lvm to do that cogen, because every IR line is going to turn into somewhere between zero and n lines of actual machine code.
01:33:12.880 - 01:33:43.724, Speaker A: Some might be optimized, but that does mean that every line has to be evaluated by LLVM, and that means work for LVM itself. But that's not where all of the time goes. So again, for example, Rust C has to do a bunch of work that's independent of codegen, and that's what we're going to look at now. So, okay, it finished. And if I now look in the current directory, you'll see I have this file called cargo dash. Bunch of numbers, dot mm Prof. Data.
01:33:43.724 - 01:34:37.576, Speaker A: That's the file that includes all the profiling information that Rust C just generated as part of building cargo. You'll see it's pretty large because it holds all of this information about Rusty, what Rusty was doing at every step. This file is in a format that can be a little annoying to parse out. And the Mm here stands for measure me, which is a tool that was built specifically for Rusty's self profiling feature. You can't see this because I think it's behind my face, but it says under the about support crate for Rusty's self profiling feature, measure me comes with a bunch of tools. You just do like cargo. It's not quite cargo install these, but it's like cargo install, git, path to it, branch, and then the name of the tools.
01:34:37.576 - 01:35:22.650, Speaker A: There are a couple of different tools that come here. There's summarize, which gives you a table of where Rusty spent its time. There's stack collapse, and flame graph would let you produce flame graphs for where most of the time was going. And then there's crocs, and crocs generates a file that you can then open in a profiling tool like the chrome profiler to see like a timeline of where all the time went. And that's the one we want to use in this case because it gives us the nicest visualization. I happen to have prepared some of this earlier, so I have crocs installed and I'm just going to give it its file. I hope it doesn't spit this to standard out.
01:35:22.650 - 01:35:57.700, Speaker A: It did not. So it gave me a chrome profiler JSON file, which you see is 1.9gb large. I might regret this. Let's see whether I will regret this. Whether, for example, the stream ends up slow because I'm uploading too much at once. Open.
01:35:57.700 - 01:36:41.120, Speaker A: I heard a few years ago well that's good. I heard a few years ago the rust C gives very bad ir to LVM, which is why it's so slow. Do you know if that still is or ever was true? I haven't heard that. It might be true, but I don't know. Are these tools based off Inferno? Yeah, the flame graph tooling there is based off Inferno. The trace processor. Native binary is an accelerator.
01:36:41.120 - 01:37:39.210, Speaker A: Alright, sure. What's the worst that could happen? I'm just gonna run some random scripts off of the Internet. No one's ever been bitten by that before. Ah, see, it says the browser memory limit is 2gb per tab, but the file was 1.9gb. It totally is larger in memory. Does that mean it finished loading the trace? Do I have enough memory to do this? Is everything gonna fall apart? I think I'm okay. Well it's, it's loading.
01:37:39.210 - 01:38:09.220, Speaker A: I wonder how long this will take to load. There is nice. Crocs does have a mechanism for reducing events that are shorter than a certain amount of time, which reduces a lot of. Removes a lot of noise from this and would make the file significantly smaller. But we don't want to remove things. Trace loaded in 47 seconds. Nice.
01:38:09.220 - 01:38:32.630, Speaker A: Reload the UI and click. Yes, yes. All right, let's see what we get. Computing Android startup metric. That doesn't sound like what I want. I think I saw Jank in there. Loading overview.
01:38:32.630 - 01:39:28.078, Speaker A: This is going to crash my computer. Okay, so here's what we get is a visualization. This is a timeline visualization saying, you know, the left hand side is when did Rust C get invoked? And then there's a. This is a time diagram showing the entire time until rusty returned and the entire sort of. It's not quite a stack trace, right? Because these are events or emitted by, by Rust C itself. So this is when Rusty decided to emit an event as opposed to like every, every possible stack frame, which would be a lot more noise. And you'll see at the top here we have fairly nicely distinguished faces of generation, right? So first we have whatever's over here, configure and expand.
01:39:28.078 - 01:40:07.546, Speaker A: Then we have analysis, which is type checking and borrow checking. Then we have generate, create metadata, which is, it looks like exporting symbols and things like monomorphization. Then there's code generation, then it's this. Oh, then there's the self profiling itself. So self profile enables a mode where Rusty has to do a lot more work in order to do the profiling. So this is showing us that I spent a bunch of time doing self profiling. And this is linking.
01:40:07.546 - 01:40:50.702, Speaker A: And you see, linking is a fairly short amount of time here. Can you open this in chromium devtools proper? You can, but if you open the chrome devtools, the performance page, it now tells you to use perfetto instead. Okay, so we're going to ignore linking. We can ignore the allocating query strings. If you see further down here, you see it also shows where these, like if we have, it's using multiple threads to do code generation and stuff, and it's just showing you what all those other threads are doing. But that's all as part of code generation. And I suppose we could zoom in here a little bit on the code generation, maybe.
01:40:50.702 - 01:41:16.162, Speaker A: Well, let me do that. Yeah. And here, this is all code generation for different crates. Like which crate is this, I wonder? Arguments. Well, that's unhelpful arguments. So this is all the code gen stuff, but that's the purple part. Right.
01:41:16.162 - 01:41:47.410, Speaker A: So code generation is not actually what we care about here. Instead, I think the bits that we actually want are, why will it not let me scroll our analysis and generating the metadata? Like, that's where things get interesting. Why can I not scroll left and right? Which is what I really want to do. If I zoom in and out, it zooms in and out of the center. Aha. There we go. I cheated.
01:41:47.410 - 01:42:37.776, Speaker A: Amazing. Oh yeah. So this actually ties back to another point that I should have raised earlier, which is one of the ways to improve your build time is to make sure there are fewer things that build before you. And one of the great ways to do that is to opt out of default feature flags and only enable the ones you need. Because in general, feature flags tend to enable more dependencies in like transitive dependencies. And so if you remove them, they don't have to be built before you, and therefore you might be built in parallel with them instead, or they might not need to be built at all. So definitely reducing your use of feature flags, or rather reducing which feature flags and thus which optional dependencies you actually end up taking can improve your build time a lot.
01:42:37.776 - 01:43:19.066, Speaker A: I'm sure that applies to cargo too. There are almost certainly some of the dependencies here that we could eliminate, and that might be worth digging into inlining the Toml edit dependency. That might help. I think it would be really interesting to do the same analysis to the Toml edit crate to figure out where its time is going. My guess is I still think that they're the same. I still think that the what ends up making the blue part of cargo long is in big part, the thing that makes the toml edit thing long. And I'm guessing it has to do monomorphization.
01:43:19.066 - 01:44:20.598, Speaker A: I'm guessing that they both end up doing monomorphization over some type from Tamil edit. Because remember, monomorphizations happens to in the crate that you that names the type basically and shared generics wouldn't help here if they're naming two different types. But imagine that there's some function in Tomlin, and let's say the most obvious example being tostring, right? So imagine that TOml edit has a tostring function, and it's generic over which type it should turn into a TOML string. And the toml edit crate calls that function with one of its types. Let's say tomlvalue. That means it has to type check all of the like serialization code for tostring for tomlvalue. And that's probably a bunch of code, which means a bunch of work in the cargo crate it calls tostring with some cargo type, maybe Toml project that means it's going to generate a monomorphization of tostring, and that monomorphization is going to be a bunch of code that has to be type checked.
01:44:20.598 - 01:45:33.050, Speaker A: And their two copies, they're not shareable because they're not using the same type, but they both end up generating lots of code that needs to be parsed out. And this actually gets back to one really neat trick for trying to reduce the cost of monomortization in this way, which is non generic inner functions. Let me see if I can find that inner functions. So I'll put this in chat. This came up back in 2020, and it's a great article on if you have a function where the, the function has to be generic, but a bunch of the code in the function isn't, does not actually care about those generics. One example being, you know, if you want to have a nice outer interface, so you want to be generic over anything that can be turned into a path, but the body of your function only really operates on a path reference anyway, then what you can do is you can have this inner function that is not generic, that holds most of the body of your function. And then you have the generic part called the non generic inner function.
01:45:33.050 - 01:46:23.634, Speaker A: And that way there's only going to be one copy of this function everywhere, not just in your crate, but in any dependent crate too, because this function is not generic. So only a single copy is generated and that one is reused everywhere. And there's going to be lots of copies of this, potentially both in your crate and in consuming crates. But they will all be really short because realistically they're just going to do the sort of generic part and then they're going to call the non generic function and all of them share the same non generic function. And therefore you only end up doing the sort of expensive work once. And the generic, the non expensive generic parts you do many times, but they're all different. And this is something that I suspect Toml edit could benefit a lot from.
01:46:23.634 - 01:47:22.080, Speaker A: Right. So I'm guessing that they have a bunch of functions that are generic over either the type, right, that implements serialize or deserialize, or the, when you implement the serializer and deserializer traits, you're often generic over the read type or the write type, right. So the things you're, the underlying thing you're going to read from or write to, such as a file or a byte string or whatnot. And even though you're generic over both of them, chances are most of the actual code for producing and parsing toml doesn't need to be generic overdose those things. But if it isn't put in a non generic inner function or some other non generic helper function, then all of that code is going to be done for the, for each monomorphization, which means a lot of potentially wasted effort. And so we might be able to do some of that optimization. We might not actually do it today.
01:47:22.080 - 01:48:25.030, Speaker A: I'm just pointing it out as one of the ways you can help reduce the compile time from this kind of generic stuff. Okay, so let's go back to this trace. So what do we have here for analysis? We have type checking and type checking has collecting types, coherence checking, WF checking WF whole file check, mod type, who knows? Item types checking. It's interesting that a lot of this time is spent in Ops. Common for install and uninstall. Right. You see here for checkmod type and also for this one, that's the primary argument.
01:48:25.030 - 01:48:59.888, Speaker A: Cargo ops, cargo ad manifest, maybe cargo ad made. I'm just like looking for the big blocks. Okay, item bodies checking. So this is, you know, these things are looking at the sort of outer types. So this is, you know, function signatures and type signatures and stuff. And this is checking the bodies of methods to see that they type check internally, but also that they match the signature of the function. Oh, well, formedness.
01:48:59.888 - 01:49:32.642, Speaker A: That makes sense. That is a better interpretation of WF. So type check, item bodies. Let's see, let's zoom into this a little bit, see what we can find. So this seems like it has lots and lots of small things in it. There's nothing here that strikes me as like a particularly wide frame, although, you know, if you squint like this one is kind of wide parse links, overrides. This one is wide.
01:49:32.642 - 01:50:02.200, Speaker A: So that's cargo util tommyl because of course it is. Two real manifest. Let's look at this. Two real util toml two real manifest. Ah, so two real manifest is just a very long function. That's why it shows up so much. It has a very, very, very long function body, and therefore, you know, it's a bunch of work to deal with.
01:50:02.200 - 01:50:37.730, Speaker A: Okay. But it doesn't look like type checking is the big offender here. There are lots of functions in cargo that need to be type checked, but there's none that stand out as being particularly bad. So let's switch over to borrow checking. And again, this sort of ties back to how we talked about cargo is a big project, so we expect there to be a bunch of work. What we're looking for is particular offenders that we think we could improve on. So what do we have in here? Still nothing that looks very large.
01:50:37.730 - 01:51:05.352, Speaker A: This one maybe. Hey look, it's cargo util toml to real manifest again. So borrow checking. That function took 38 milliseconds. Right? So again, the time here, very tiny. But of course, because there are lots of things that are tiny, they add up. What's also interesting is this appears to all be single threaded.
01:51:05.352 - 01:51:45.980, Speaker A: You know, the code generation is multi threaded, but the analysis and generating the metadata all appears to be happening on one thread. You know, on this thread two, the other threads look to just do code gen and Lvm. So certainly one way to optimize this would be trying to make that happen in parallel, although I'm sure that's easier said than done. No, that's not what I wanted to do. All right, so generating crate metadata. What do we have over here? Come back. Optimized mirror.
01:51:45.980 - 01:52:27.570, Speaker A: Optimized mirror. These all, again, look like, you know, a small amount of work for every item. Nothing that pops out very much. Exported symbols collect and partition monomorphized items. Monomorphization collector graph walk. What do we have? Do we have anything here that looks very large? No, nothing that looks very large here either. Although what I wish I could do here is look for things that are kind of similar.
01:52:27.570 - 01:53:50.724, Speaker A: Right? Like. Well, even that wouldn't really. That one looks large. What do we got over here? What is this big one? I want to zoom in more over here and see what we can do. Yeah, it doesn't seem like there are any that really strike out. But one of the things that's challenging here, right, is that the arguments here are all generic, and so there's no distinction between types that are the same except for an inner type. An example here would be deserialize, right? So every implementation of deserialize might be small, but it might be that the deserialize for any particular thing, or rather that collectively deserializes the biggest offender.
01:53:50.724 - 01:54:59.320, Speaker A: But this wouldn't tell us that because we can't at a glance tell, you know, this range of tiny dots are all related to deserialization. I do wonder what the color coding here is, though. It doesn't look like it's related to the arguments. What would be nice, actually, is if the colors were like a hash of each prefix of the type or something. What's that thing over there? Hash map new okay, that one's pretty large, but nothing that really stands out. What about over here in partitioning? CGP partitioning assert symbols are distinct. I wonder why that takes so long.
01:54:59.320 - 01:55:42.502, Speaker A: Interesting. So here's also where I mentioned earlier that the tools here aren't great. As you see, some of the information is here. In fact, potentially most of the information is here. The problem is it's hard to generalize from this. And you'll notice that the reason why we get all these tiny slivers here is because these are many calls to the same thing, but with different arguments. And if we hadn't included the self profile flag that says include arguments, what we would get is bigger boxes, but we wouldn't really know what the box were.
01:55:42.502 - 01:56:14.140, Speaker A: We can try it. In fact, I think that's what the colors here are, is if we go back to the collector, for example. No, don't. That's not what I want. I think the colors here is the function. So this is optimized mirror and this is the same color. So it's the same function, whereas green is resolve instance.
01:56:14.140 - 01:57:18.360, Speaker A: So squinting at this, you know, it seems like there's about the same of orange and green, but they're like interleaved a lot. And this is where a flame graph might actually be helpful. The challenge is it's only, it's really just going to tell us the time was spent in, you know, optimized mirror, like a bunch of time was spent at optimized mirror and, oh, maybe it will tell us the biggest offender with each one. Let's try it. I mean, doesn't hurt to try. So let's go back to this. And I, oh, this is one thing I really, that really makes me sad about fish is that I don't immediately get my completions.
01:57:18.360 - 01:57:57.340, Speaker A: So flame graph of cargo mmdh. This might generate a very large file graph. Help. So now we should have a, is there, yeah. Rustc dot SVG. Let's see how big that file is. Well, it's not very helpful.
01:57:57.340 - 01:58:17.504, Speaker A: Thanks. That, that's great. Thank you. Oh yeah, you're probably right. Maybe I can, oh, you're right. I can just draw here. What am I doing? Good call.
01:58:17.504 - 01:59:02.728, Speaker A: This file clearly removes all the information that we care about. So let's try to bring some of that back if we can. We might not be able to, which is sad. This is because the default by default flame graph will remove things that are particularly small. So a lot of the smaller frames here end up going away. Although I guess what I really want to do is go over here to generate cargo metadata. But you see some of these end up being so small that we don't really get much to go on.
01:59:02.728 - 01:59:49.198, Speaker A: Like for optimized mirror, it just tells us that's where time was spent. Same with here. This ends up being a non time view of this one here where let's take monomorphization collector graph walk. If we go over here, it will tell us. So what was hard to tell here was like how much is green versus how much is orange. And here it'll tell us, you know, you spent this much time in optimize mirror, you spent this much time in resolve instance. But that doesn't really help us, right? Mir drops elaborated it does tell us which subcalls, but it doesn't tell us which items.
01:59:49.198 - 02:01:05.768, Speaker A: And part of the reason for that is each item is so small that you wouldn't easily figure out what the top. There's no obvious contender here because every instantiation of every type is considered different. So I think realistically what this is telling us is we would want some kind of like pass that we could do on the file that sort of turns types back into their generic form, like we might have, you know, deserialize of stdio file and deserialize of vecu eight. And I want to turn them into DC or elizive r so that they're combined bind and the output. And that way I should be more easily able to look at. Okay, what are the general problem areas? Because this is relatively unhelpful. And unfortunately, there's also no easy way to do the sort of reverse graph, which is almost what we want here, which is show me what is the slowest type, and I want the slowest type across all of the passes that are run on it.
02:01:05.768 - 02:01:45.990, Speaker A: That would be a really useful metric, but there's no real way to extract that information here. You know, if I go to the bottom or if I go to here, you know, this is talking about the type. Bad example, maybe. Let's take this one, right. So this is vector from iteration over package ids with a filter. I want that type. Tell me about the time spent across all passes on that type, because that should show which type is taking up collectively the most amount of time across all passes.
02:01:45.990 - 02:02:48.104, Speaker A: Yeah. So I think what you'd want here is like you want to produce all the same data with all the arguments, and then have a post processing step that sort of reduces or combines types that are sort of similar or very similar, or things that, you know, you don't care about, like closures maybe, and just removes those types so that those strings compare the same, then collapse them and then emit the flame graph or this thing. And what you should get is larger chunks where each chunk sort of, you know, is lossy. It combines information from multiple actual concrete types, but it might be able to direct you more in terms of, you know, which type. Which monomorphization should I be looking at? Yeah. So arguably, one thing you could look at here, right, is go back to the old symbol mangling scheme, because that one was lossy. It wouldn't include all of the information necessary to reconsider construct a type.
02:02:48.104 - 02:03:28.020, Speaker A: And so that actually means it does some of that work for you. But in a relatively unpredictable way. And so it's a little unfortunate because I don't know that we're going to get a lot further looking at just this data. And it's sad because that is sort of the extent of the tooling that we have today. There's not a lot more we can really do here to try to grab more data out of this. I wonder if there's a metrics. Well, this is unhelpful.
02:03:28.020 - 02:05:07.590, Speaker A: Oh, I guess this is very like Android focused query with SQL. That's weird. Yeah, because it's hard to tell from this. You know, is it just cargo is large or is it, there's some particular offender here that we just can't easily spot from this one option, right, is do random sampling, which is basically what I'm doing now, which is take the whole thing, just click random small things and see if you see patterns like things that are often there, but they all seem fairly distinct, which is interesting. Create dependency. Yeah, I don't see any like big obvious offenders here, which is kind of fascinating. The other way we could go about this is we could stop this one and we could do cargo rusty library, and then we could take this command and we could do perf record.
02:05:07.590 - 02:06:43.684, Speaker A: Oh, why did it fail? What is it trying to tell me? Did I break the code somewhere? Oh, it's expecting to get something from the build script. This is very hard to parse through, but see how there's a cargo package version patch in here? So if I do end cargo package version patch equals zero cardio. Cargo package version patch. Oh, and it wants cargo package version major equals zero. Cargo package version minor equals 663. Nice. See what we get here.
02:06:43.684 - 02:07:39.660, Speaker A: So what we get out of that is a perf data file. And then what I can do is perf script and see what happens. So what I'm doing here is rather than using the Rust C profiling, which the developers working on Rusty have done careful work to make sure it emits the right metrics and whatnot. And just saying, I just want to see where rust seed, the process is spending its time. I'm just sample it at random times, which is what perforcore does, and then I want a flame graph of where it spent its time. And I should have, should have stored the perf script into a file, but I did not because I'm a fool. Perf svg.
02:07:39.660 - 02:08:32.360, Speaker A: See, we got here, right? So this is much noisier, right? Because it's not this neat, you know, these are the metrics that we collected. But over here we have collector collectitems. So what's over here? Rust C interface passes analysis. And you know, this is much noisier data. But let's see. Rusty privacy analysis. Okay, so privacy analysis, where is it going? It's visiting expression.
02:08:32.360 - 02:09:39.054, Speaker A: So this is where it'd be nice to have the, the arguments to the function, but it looks like a decent amount of time is spent in just analyzing the visibility of different types. What else we got? If we go back up here? Here we have encode crateroot encode metadata monomorphization. So this is more useful if we wanted to optimize rusty itself. If we wanted to say, okay, I want the encode metadata implementation function to be faster. Where is it spending its time across all possible types. It's like spending a bunch of its time in whatever this closure is. So maybe we could tweak that ever so slightly and that would have a huge impact.
02:09:39.054 - 02:10:46.728, Speaker A: Maybe. Interesting, but I don't think this is going to help us much because it doesn't record the argument. So all we know is a bunch of time is being spent in this part of rust C without knowing why it's spent there in rust C. But so you sort of see my original point here, which is all of these metrics are kind of noisy, kind of hard to know what to act on. And you end up basically having to sort of read the code and look at, okay, what are patterns that this code is doing that we may be able to do better for? You know, same thing for, if we wanted to do the same kind of analysis for Toml edit, we would end up in a similar position where we would have to figure out where is it spending its time. Now that said, you know, toml edit is generating a lot of IR, which is weird because Toml edit, you know, isn't, it's not instantiated for any particular type. Maybe it's the IR for all of the standard library types that ends up being large.
02:10:46.728 - 02:11:45.750, Speaker A: That could very well be, which is not very satisfying. I was hoping we'd be able to do more than this. I quit the process, didn't I? That's too bad. I was hoping we'd been able to actually cut, I mean, we did cut it a little bit, right? So we did discover that the deserialized implementation for these types does generate a lot of ir, and it might be that rather than deriving deserialized for these very large types, maybe we can write more efficient manual deserialization for them. Or maybe this is an indication that in SeRd, which is the thing that generates that code, maybe sort of derived, could be made to generate code that is basically generates less ir in and of itself. And that would be a benefit for anything that derives, serialize and deserialize. I also think it would probably be worthwhile to do the same kind of analysis for Toml edit.
02:11:45.750 - 02:12:47.518, Speaker A: It could very well be that it too uses serted derive. In fact, we should be able to see that here. In fact it uses, yeah, it needs serdi, needs serdi derive. I'm guessing the Toml edit is the place to focus this attention. And in particular, I'm guessing the Toml edit could be made to have non generic inner functions that eliminated some of this duplication across the different monomorphizations, which would then lead to improvements to cargo, it would lead to improvements to tomlin itself. And maybe we could even insert a derive, have it generate non generic inner functions for anything that derives, and thereby get this, this compile time benefit for anything that uses it. Can you profile the code by running the tests? So profiling the code wouldn't help us at all here, because we don't actually care the runtime of the code.
02:12:47.518 - 02:13:20.726, Speaker A: What we care about is the time the compiler spends building the code, which is just like unrelated to how long the code takes to run. You can write very short code that generates basically no IR, but takes a very long time to run. Right. The trivial example being an infinite for loop. It takes forever to run, but it compiles real fast. Mini serta might be another way to go about this. So mini 30, if you haven't seen, is, let me dig it up here, mini 30.
02:13:20.726 - 02:14:43.890, Speaker A: Yeah, so mini 30 is sort of a, an attempt at doing, solving a very particular subset of use cases for surety, in a way that's better for that subset of use cases. It's not, you know, a general purpose substitute for Sordi. It's, as the readme says, is more of a proof of concept, and it specifically works well for. Well, first of all, it's designed to be really fast, but also for data where you don't want monomorphization or you don't care about it. And serialization is weird here, right? So I mentioned how you might want non generic inner functions, and that is true for the purposes of you want to avoid monomorphization, and so you want that shared code so that it compiles faster. But the flip side of that coin is if you don't generalize, if you don't generate specialized implementations for each type, that means you're giving the compiler less of an ability to specialize the optimizations to the particular interaction. So, for example, let's say you have, you know you're going to serialize into, and you're generic over.
02:14:43.890 - 02:15:39.810, Speaker A: Your serialization code is generic over anything that implements write. Well, writing to a file and writing to a vector of u eight s are very different. It might totally be that you can generate much more efficient code for when you know you're serializing into a vecu eight. But if you have a non generic inner function that just like always serializes in the same way to like an intermediate buffer or something, you would lose out on some of those optimizations. Or conversely, you would end up always having to serialize into evacuate, even if the goal is to serialize into a file, because you're trying to keep it non generic where you can. And so as you see, there are a bunch of caveats, basically for cases where you would want to use Miniserdi, or rather where you would not want to use it. So for example, it's only really built for one format, which is JSON.
02:15:39.810 - 02:17:01.880, Speaker A: There's an interesting observation here of maybe tomoledit relying on Certi drive and the general certi framework here is actually coming at a pretty high cost, especially for something like cargo. That isn't to say nanoserty is the solution here necessarily, but more that looking at a graph like this and looking at where it seems to be spending a bunch of its time, that this might be a pretty ripe area for optimization. And crucially, it might have a serious impact on the build time of cargo itself. I want to talk about one more thing at the end here. I know we didn't get to actually make cargo that much faster, but the goal here was more so than the journey, than the destination, to give you some idea of what tools you have available and what tools you don't have available and the kind of techniques that you can use. So one more thing I wanted to talk about is I did not open a tab for this apparently is what and want is not necessarily something you want to use, but it's an interesting idea to be aware of. So the observation with Watt is that procedural macros are really powerful.
02:17:01.880 - 02:18:29.239, Speaker A: So procedural macros to do a very, very quick recap is rust programs that take rust source code as input and, and produce rust source codes as output. They're basically fancy versions of macros. They get to get to be much more powerful in what they get to do. The challenge with procedural macros is in part that anyone who wants to use a procedural macro, you have to compile the procedural macro and then run it, and then compile the output, which means that if we look at this, this sort of step diagram of build certi drive basically produces a program that parses rust programs, which means that it has to be built to completion. The final binary has to be linked, and it has to be done before you can even start looking at the source code of these downstream things. You can almost think of it as it sort of ends up being a, almost like a build script that has to run. And what want is trying to do is to rather than compile the procedural macro in any consumer of that crate, you compile it when the procedural macro crate is built, and then you stick basically the compiled output, the compiled program into the crate.
02:18:29.239 - 02:20:23.246, Speaker A: So the idea being that here, basically when we build certa drive, we build the procedural macro as a binary, and then any consumer of it doesn't need to. I'm trying to find the right way to frame this explanation, actually. Let's see if the what Readme has a better way of phrasing this than I do. Yeah, so the idea here being that when I in fact, does this happen at publish time or does this happen at build time, we save all downstream users of the macro from having to compile the macro logic or its dependencies themselves, right? So I think the idea of what is that you actually do this compilation, when you publish your crate, you compile all of the dependencies. You compile the program, the actual macro, you compile it to a webassembly binary. And the reason you compile it to webassembly is you can run web assembly on lots of different platforms without having to recompile it for each platform. You compile it into WASM, and then you what you actually publish is that compiled WASM code and a very tiny procedural macro that has basically no dependencies.
02:20:23.246 - 02:21:37.770, Speaker A: That just takes as input the source code, runs it through WASM, and then outputs the modified source code. And what that means is if you look at something like the build graph for cargo, you wouldn't need to build certa derive, or rather the build of certa derive is just a copy of the webassembly, and then running certa derive would just run that webassembly. But you wouldn't need to compile any of certa derives dependencies because it wouldn't have any. It wouldn't have a dependency on something like a rust source code parser like syn and quote. Instead it would be a dependency free crate whose only job is to run a web assembly binary. And that way you're going to save a bunch of that compile time because you don't need to care about the bits that happened, that there were dependencies of producing the webassembly because that happened on the developer's laptop when they published the crate instead. Does that distinction make sense? Now what's nice about Watt is that you can opt into it for your procedural macro on behalf of all of your downstream users.
02:21:37.770 - 02:22:25.824, Speaker A: The downside is that it's not a super well supported use case. Like the tooling isn't really there for having a published time dependency. This is not something that cargo knows a lot about, and so it's a little clunky to get it up and running. It's a little clunky to be the publisher, but as a consumer you don't really notice, because all you're going to notice is that your procedural macros compile instantly and don't take dependencies. They might run a little slower, but they're going to compile much faster. And there's some questions like okay, why is this safe? Why is this okay? The answer to that is partially that webassembly makes it safe. So when you run webassembly you can sandbox it pretty easily.
02:22:25.824 - 02:23:23.436, Speaker A: And so you just run the webassembly because all it gets is like a string as input, and it produces a string as output. It doesn't need to have access to IO or anything like it, so you can completely isolate that procedural macro when it executes. And if you look at the readme here, you can see the sort of instructions for how you go about it. And as you see here, the procedural macro that you write is really just going to include the bytes of the WasM file directly into itself, and the execution is run that webassembly provided this string. It's a really cool way to try to reduce the procedural macro side of things. And in terms of bringing this back to compile time for something like cargo, the reason this would matter is you see here that serdi can't be compiled, or rather toml edit can't be compiled until Serdi is compiled. Serdi can't be compiled until serdi derive is compiled.
02:23:23.436 - 02:24:23.290, Speaker A: Compiling Serdi derive takes 5 seconds. So if you cut that to nothing, then you've already shifted all of the serdi stuff left by 5 seconds, or you know, to the next max of some parallel dependency. But Serd itself couldn't start until syn was compiled. And sin can't be compiled into quote is compiled, and quote couldn't be compiled until proc macro two was compiled, and proc macro two couldn't be compiled until proc macro to build script was compiled. Which means that there's this whole like dependency chain that we need to wait for before we can even build Certi and before we can build Tomledit. And if all those dependencies were at publish time, and instead those will all go away, those lines will move all the way to the left. And so we save a bunch of this basically wait time that is enforced by having to build the procedural macros in the consumer.
02:24:23.290 - 02:25:36.370, Speaker A: Of course, that doesn't mean that you end up saving 5 seconds plus 7 seconds plus a second or so. You don't end up saving actually all that time, you save some subset of it, because as we talked about, you still need to wait for all your other dependencies to build. So Serdi has dependencies on other things that surdy derive. So you'd still be subject to their sort of waterfall build time, but you should be able to shift things left in general. Okay, I think that's all I wanted to touch on today, which I know it's a slightly unsatisfactory ending that we didn't actually improve the compiled time of cargo, but hopefully this has given you an insight into how you would go about improving the compile time of cargo. And I think realistically what's going to happen is I'm going to go do this investigation for Toml edit, see whether I can do some improvements there to help myself for this pain that I've been feeling. I don't know that I'll do that on stream, but at least now you know all the same things I do about how you would go about doing that work.
02:25:36.370 - 02:26:27.390, Speaker A: Okay, are there any questions before we end like this was, I've been talking for a long time about all this stuff, and I'm sure people have questions about how do I, how do I do this for myself. And while I'm waiting for questions, I highly recommend reading this article, like start to finish. It goes through a lot of the tricks that we talked about today, and some of the other ones that even though they're not going to apply to other people using your crate, they are going to apply to you for your builds, and that might be all that matters to you. I'll also link in the description of the video. I'll also link a couple of other articles that are useful that we've talked about today, some of the tools that we've talked about. There's also this article that I thought was really good about where Rusty spends its time. I'll stick it in chat, too.
02:26:27.390 - 02:27:09.960, Speaker A: This is a much longer article, and it might not be immediately reusable. It's also a little older, but it's a great read on basically how to figure out where all this time is going in a very low level way. You should comment a pr if you end up doing something. Yeah, if I end up filing a pr that helps with this, I'll make sure to tweet it out. I think many people are going to be interested in it. Do you lose any compile time to monomorphization if you only implement a single concrete type? No. I mean, I wouldn't think of monomorphization as losing time.
02:27:09.960 - 02:28:08.410, Speaker A: Monomorphization is just making sure that you have compiled every instantiation of a generic type that ends up getting used. So again, vec U 32 and vec bool. Monomorphization just means compile Vec 32 and Vec bool if they're both used. But if all you're using is VEc U 32, it will only compile VEC U 32. There's, there isn't really an overhead to the the act of monomorphization, whether you have more than one or just one, it's just a matter of how many times over is that chunk of code going to be compiled. What do you think the story for build times and rust will be like in two or three years? It's hard to say. I think, as you know, given the graphs that we looked at today, too, a big win is probably going to come from paralyzing some of these stages.
02:28:08.410 - 02:29:03.048, Speaker A: But paralyzing them is really hard because there's a lot of information exchange that has to happen, and so it kind of has to be fairly tightly coupled. But hopefully some of that is possible to strip apart and I think procedural macros with an endeavor something like Watt. It doesn't specifically have to be watt, but doing more work, publish time might be really nice and be a decent build time win. Incremental compilation is probably just going to keep getting better. Lvm cogen gets better. There's also work on stuff like crane lift, which is basically a rewrite of a cogeneration engine that is tied much more closely to rust and is written in rust. So hopefully it might be able to be more safely made concurrent, for example.
02:29:03.048 - 02:30:03.960, Speaker A: And there's some hope that that's going to reduce code generation time. Things like moving to whether chalk Polonius being able to externalize some of these things might allow them to be optimized separately, which tends to make optimization a little easier. I don't know that I see amazing speed improvements coming out of nowhere. I think this is more of a gradual process of these things getting better over time. And there are some larger steps we could take, but they're also fairly complex. So I think it's going to look a little bit like a sort of more like a kind of log graph, then it's going to be a step function, but there are going to be some steps in there. Now, I do think that we might see some decent wins from some sub steps, but there are not a lot of, you know, magic beans here.
02:30:03.960 - 02:31:21.060, Speaker A: Do you think this is a stream you would revisit if the kinds of tools you're describing were built, such as aggregating calls by their arguments? Quite possibly. I do think that stuff like this tends to take a fair amount of time, like building analytics. Tooling for development is just, it's a bunch of work, and it's a bunch of work that is sort of ungratifying to do until you're done, like sitting down and being like, I'm going to write a new analysis tool for visualizing. You know, rust sea build times is probably going to start out as a hobby project, and that means it's going to take a long time to get to something that's, like, useful for people to use. But that said, once those tools do come out, I would totally start using them, and that seems like a good opportunity to teach people about them. One thing I forgot to mention, actually, is one of the reasons cargo is a particular offender for build times is because cargo can't use cargo workspaces. So you can't currently split cargo into subcraits, which is one of the best techniques for improving build times.
02:31:21.060 - 02:32:16.314, Speaker A: You can't do that with cargo because cargo is a sub module of rust, so it's already a member of the rust workspace. And so you. Because you can't have nested workspaces in rust, cargo is required to not be a workspace in and of itself, which is a real shame, because it means, you know, that cargo ends up being built as this giant blob and can't be broken up in a very meaningful sense. I do think that this is something. This is one of the reasons why the cargo maintainers really want nested workspaces to be a feature, because they want to use it themselves. It just turns out it's fairly complicated to get all the like nooks and crannies and corner cases figured out. But if anyone's looking for like a cargo project to pick up and are not afraid to get into a pretty deep process like nested workspaces, would be a huge win.
02:32:16.314 - 02:33:05.250, Speaker A: Here. Is the release build time critical? If the debug build time is fast, I think they sort of go hand in hand. Like the biggest difference for release builds is more in the code generation step. There are a couple more optimization phases I think in Mir as well, like in the middle representation, but a lot of it comes down to LVM doing a lot more work. So any improvements to rust C are probably going to be relatively independent. I do think it's important for release builds to be faster too, but I don't think it's as important as getting debug builds down. Do you think the compiler could be smarter and automatically implement inner functions? Could be one of the.
02:33:05.250 - 02:34:08.760, Speaker A: It could be that it already is kind of smart, but that it's sometimes complicated because, you know, let's take the one that we looked at earlier with reach a string that takes a path. Imagine you didn't have this inner function, and so you just wrote the function normally the way you would have, and you just stick path sref like in here. Then the compiler would have to, or this is maybe a bad example, but one thing that will often happen is you're actually using the generic bit somewhere in the middle or maybe multiple times. And really what you could do if you like as a human is you could take the parts that are non generic and the sections that are non generic and turn those into multiple intergeneric functions and then interlock, leave them with generic calls. I think it's going to take a while for the compiler to get smart enough to do that. It would be really cool if it could. I don't know of a way for it to basically make functions for you.
02:34:08.760 - 02:35:18.320, Speaker A: I put target Dir on a ram drive to avoid wear and tear, but the stream made me realize I should benchmark it to see if there are performing reasons to do it or if caching cancels it out. I've found that sharing a target directory across all my builds is the best way to speed up my builds. Putting them on RAM drive is nice, but like so I have an NVMe drive, I'm not concerned about the speed of the drive, the drive is very fast, but I am concerned about whether it has to recompile or not. And sharing a target dir helps with that. Do you think rust compile times are an issue with regards to rust adoption? I've heard this complaint several times from people who I'm pretty sure never used rust. Maybe. I think that you have to be kind of short sighted to decide that the reason we're not going to use rust is because it's slower to build than language of choice.
02:35:18.320 - 02:36:18.280, Speaker A: It could be among your reasons, and it could even be, you know, the reason why you start looking at alternatives. But I don't think it's a good reason in and of itself. Partially because it's a problem that's going to get better over time, and partially because if you say that's the only reason, you're discounting the fact that there are other parts of rust or other features of rust that save developer time. And so you kind of need to take that into account. You need to offset that in your estimation of how much time is actually being spent. It might be more annoying for developers to see slow builds, even though they overall save time because they do less debugging or something like that. There's a hard to measure developer frustration aspect to this, but I don't think slow build times in and of themselves are a worthwhile sole reason to not choose rust.
02:36:18.280 - 02:37:25.404, Speaker A: I don't know when crane lift is going to be officially used for debug builds. Yeah, going through your favorite crates or your most used crates, or even just the crates in your dependency graph that you take dependencies on and looking for dependencies that can be made optional, and filing prs with those projects saying, hey, I noticed you don't actually use this feature of this dependency. I got rid of it for you. Here's a pr can be really valuable because that's one of the ways where we collectively as a community end up shortening that waterfall. What about build time for proc macros? I remember those being potentially pretty slow. So I think the reason why proc macros are often slow is more because they end up taking a dependency on sin, and Sin itself is fairly involved, which I don't know that we're going to get rid of sin anytime soon. Sin is great for all sorts of reasons.
02:37:25.404 - 02:38:53.992, Speaker A: I think the solution is more going to come on something like published side compilation of Proc macros, because that has some serious other benefits too, like being able to isolate proc macros, which is something that currently is a bit of an annoyance in Rust, or a problem in Rust is that proc macros sort of get to do whatever they want. There's source based code coverage. So it's really helpful to understand which code runs multiple times. I don't know that source based code coverage is going to help on Rust C, or rather, I don't know that it would tell me a lot more than what the perf output tells me, which is, you know, which functions end up being called a lot. That would help me maybe improve if I wanted to make a contribution to rust C to improve the efficiency of a given function. But it wouldn't tell me which parts of my code base like the thing I'm pointing rust C at. How can I improve that so that rusty has to do less work? Compile times are usually long when you have a lot of proc macros.
02:38:53.992 - 02:39:48.460, Speaker A: Yeah, I mean, this is again, one of the motivations for Watt is that you won't, you're as a user of proc macros, you won't have to compile the proc macros, which is going to save you a bunch of that time. That said, you're still going to have to execute the proc macros, and that can take a bunch of time too. So when we look at the build of cargo here, it could totally be that a bunch of this time is actually running the proc macros for, say, derived, deserialize and derive serialize. That could be the game for tomoledit two, quite possibly. And it's a little sad that this doesn't highlight time spent running proc macros, because that could be a significant contributor here. And what certainly doesn't help with that. Right? If, if I build my proc macros on publish time, you're still going to have to run them, because something has to expand that derived deserialize into the appropriate impl.
02:39:48.460 - 02:40:34.460, Speaker A: In fact, if anything, it makes it slower, because I'm not going to get a native binary, I'm going to get a WASM code gen blob that's going to run. And wasm generally runs slower than a native binary. Are there benchmarks to compare build times between different programming languages? I haven't seen any. One of the reasons why it's difficult is because it's hard to get the same program if it's not trivial, and if it's trivial, it's not interesting. So it's tough to get a sort of apples to apples comparison here. Compiling rust binaries in a raspberry PI makes you feel it. Yeah, I believe that.
02:40:34.460 - 02:41:32.914, Speaker A: Do proc micros still build themselves in release mode if the overall build is release, even though they aren't part of the final code. I believe Proc macros will build themselves in release and release mode. I think there was a proposal to have proc macros always build and release, even in debug mode, because you only need to compile the proc macro once. So for incremental compilation, having the Proc macro be a release build means that your incremental build is going to be faster, because the proc macro will run faster. But I don't know where that actually landed. Does partial specialize specialize specialization improve overall times? I don't think it does. Are you worried about feature creep or the speed of adding features to Rust? I've heard this from some people.
02:41:32.914 - 02:42:06.780, Speaker A: No, I I don't think I've really been. I think Rust has been pretty good, actually, about preventing feature creep. I don't even see us adding that many features. If anything, Rust has been adding features fairly conservatively. Now, that said, I do think sometimes there's a stabilization rush, which is a little different from scope creep. Right. So if you look at something like generic associated types or async await when you feel like you're getting really close, it's tempting to say it's good enough.
02:42:06.780 - 02:43:10.094, Speaker A: Let's just get it out just in the final stretch. And I can see sometimes be a mistake, but I don't think it's like a general problem. In fact, I think the Libs team, the compiler team, the API team, have been really good about making sure that they really think things through and figure out that this is the right decision overall and the right balance, because sometimes you won't know about the problems until something gets widely used, and it won't get widely used until you stabilize. So it's very easy with hindsight to say this was the bad decision or this was a good decision. But having the foresight to know that this is going to turn out to be good or bad is often really difficult. Let's see. Okay, I think that's the end of the questions, too.
02:43:10.094 - 02:44:06.424, Speaker A: We're getting up on the three hour mark, so I think that's a good time to stop. Hopefully that was useful. Hopefully. I see a bunch of prs to all these projects now, fixing up features for dependencies, making dependencies optional, removing dependencies, and maybe someone will even take on the work of trying to get, um, Tom will edit, for example, to either have fewer things that need proc macros to execute, or have more efficient implementations of serialize and deserialize, or something else that I didn't even think of that would be really cool to see. Thanks for watching and I will make sure we take another video of this if there's more stuff to follow up on. Next stream is probably going to be on either build scripts and FFI or implementing a cargo subcommand because I think it's super interesting and it's slightly different. It's more of an impulse than a crust of rust or something completely different.
02:44:06.424 - 02:44:09.400, Speaker A: If I change my mind, I'll see you next time.
