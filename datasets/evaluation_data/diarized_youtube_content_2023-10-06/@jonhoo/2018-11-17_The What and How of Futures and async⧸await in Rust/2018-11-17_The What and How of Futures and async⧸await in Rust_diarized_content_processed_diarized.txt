00:00:01.000 - 00:00:49.645, Speaker A: All right, everyone, welcome back. This is a stream that people have been asking for for a while. We're going to tackle the whole idea of futures in asynchrony, in Rust, and we're not so much going to talk about how you use it, but rather how does it work internally in the language, what are the libraries like, how do they function, how do all the pieces fit together? And so the goal isn't so much necessarily that you're going to, like, write lots of futures yourself, although you might end up doing that. And it's not really about writing, like writing async await code. It's just like, how are these components written in the first place? And so what we're going to do is we're going to look at futures first. We're going to start looking at future 0.1, so the version that sort of released and the ecosystem at large is using.
00:00:49.645 - 00:01:43.865, Speaker A: Then we'll go into Tokyo, which is sort of one of the main executors for futures in Rustland today, and which deals with things like iOS or input output, like network connections, file systems, those kind of things. Then we'll look at the move to take Futures and move it into the standard library and what that looks like, how it differs from the existing future stuff and why it's different. Then we'll look at. We'll see a little bit how time works out, but then we'll look at pinning. So then you'll see that the new future stuff has the notion of pinning, and we'll talk about what that is and why that is. And then that ties in very strongly to the idea of adding async and await the async and await keywords to Rust. So we'll talk a little bit about what those mean, how that works under the hood, how that ties back to PIN and the future stuff in Standard library.
00:01:43.865 - 00:02:21.959, Speaker A: This stream. I've mentioned this a little bit before, but this stream is going to be more sort of me talking about how stuff works. We'll probably look at a little bit of code, we'll do some reading of rfcs. It will also be some drawing and diagrams and that kind of stuff. There'll be relatively little coding on my part, although if it's necessary, we'll do it. But this is really more of a goal. The goal here is more to go through all the stuff, how it works and explain it, because I haven't really found any good sort of cohesive explanations that cover the entire ecosystem and where it's going.
00:02:21.959 - 00:02:53.729, Speaker A: And so the hope is that this might be that. If you have any questions as we go, feel free to fire them off in chat. I'm monitoring that on the side because if you have a question, chances are someone else who's watching either now or later will also have the same question. So questions are awesome. Just before we start, I'm John. I do a bunch of these live streams and they're usually about things like we're building stuff in the Rust ecosystem. I've done a stream on open source contributions in Rust.
00:02:53.729 - 00:03:22.905, Speaker A: We've done several where we've implemented various asynchronous primitives. We did one on asynchronous ssh. We've done a couple on implementing the Zookeeper protocol in Tokyo and a bunch of other stuff that we've written. You can go back and look at my YouTube channel for recordings of all the streams. I also built a tool the other day. There's the video for building that tool is also online. That is basically a website where people can vote for what ideas they want to see next.
00:03:22.905 - 00:03:34.191, Speaker A: So usually what I've been doing is on my. I guess I should. This is my Twitter account if. If you want to follow me. That's unhelpful. This one. This is my Twitter.
00:03:34.191 - 00:05:07.993, Speaker A: Feel free to follow that if you want to hear updates both about the stream, about Rust in general, and I tweet about other stuff as well, which may or may not annoy you, who knows? But I try to keep it relatively high signal. So usually what I've done in the past when trying to decide what streams to do and what I did for this stream as well, and how this came to be is basically to sort of tweet out polls or just asking people, what would you like to see? And this particular stream came about because over basically the past year I've had a lot of people say, can't you just like cover how Async works? And in particular, I sat down with Brian Myers, who I met through Twitter actually, and through the stream in New York the other day, and he said, how about you just do one on Futures in Tokyo, Just explain how it all works. And I was like, that seems like a great idea, but I wanted a better mechanism for learning what things all of you want to watch as what we built the other day was this tool where I put in ideas for upcoming streams and you can vote on them. So in this case, there are four ideas here currently and it uses ranked choice voting, which is cool. You should look it up if you don't know how it works. The basic idea is that you can sort of rank your candidates and over time it runs, it runs elections to try to figure out what is the most preferred candidate stream. And then what I'll do is over time I will just take whatever is currently winning and then I will do that stream and then remove that and run another election.
00:05:07.993 - 00:05:27.235, Speaker A: It's pretty neat. You can catch the stream that I did, if you care. It's not written to be any kind of secure. It's broken in all sorts of security ways. So like don't abuse it. It's just there for you to be able to express what you would like to see. All right, all of that said.
00:05:27.235 - 00:06:11.847, Speaker A: Sorry, this just. Sorry, give me one second. That's not what I wanted to happen. I just lost the chat, which is annoying, like, so there we go. Chat back. All right, so let's dig into Futures. For those of you who sort of don't know what futures are at all, I won't go through sort of the entire story, but I will give you some basic background on futures and asynchrony invest or in general, for that matter.
00:06:11.847 - 00:07:00.969, Speaker A: The idea behind Futures is that we want some way to express a value that is not yet ready. So if you come from other languages, this is similar to JavaScript, promises, and many other languages have a similar kind of concept of a value that's not quite ready. But you can do, if you just wait a second or wait a little while, then the computation's value will be ready. You could think of this either as something that's compute heavy, like I need to compute a bunch of hashes and then eventually the hash is ready for you. Or you can think of this in terms of asynchronous IO. Like you have a network channel and you want to try to read from it, but there's nothing currently on the channel. And so what you get back is a future that says eventually there's going to be some stuff for you at a very high level.
00:07:00.969 - 00:07:32.017, Speaker A: You can think of this as a building block for doing lots of concurrent operations. So the idea is you have an asynchronous computation. So you have something like, I'm, I'm sort of reading from a network connection. I'm just not going to block the thread right now. The way this often comes up is imagine that you have a web server of some kind where you have, say, thousands of connections. You could spin up a thousand threads and have each one sort of re block and read on its own connection. But that seems sort of unnecessary.
00:07:32.017 - 00:08:24.335, Speaker A: Like, why are we doing that? Why do we need need all these threads instead? What you can do with futures is you can have one future for every connection and then you have a single thread or some set of threads that are sort of just looking at all of them and just handling whichever one is ready at the time. If you come from the JavaScript world, or even for that matter to some extent from Go, but Certainly in the JavaScript world you have this notion of an event loop. So you can think of your program as basically being single threaded. And there's just one thread that whenever you tell it to like read from a network socket or something, it's going to just remember that you asked it to do that. And then when the network socket is ready, it's going to tell the thread or it's going to sort of go, oh, that's ready now. And then run the computation. The way this often looks is file rest.
00:08:24.335 - 00:09:37.089, Speaker A: So the way this often works is you have something like, so let's see, I'm connecting to some server. This is not going to be real code, but just take for what it is. So imagine that I'm connecting to server one and server two, right? So and then I'm going to do like X dot write foobar and I'm going to do Y, write foobar and then I'm going to do X, read and bar bar foo. I don't know what these services do, it's not important. So think of a program like this where I have a single thread, I want to connect to both, write to both and read from both. Right Now I could have this code just work this way and sort of you're going to connect and then you're going to wait until the connection has finished succeeding. Then I'm going to connect again, wait until that connection is finished, Then I'm going to write to X and wait until all of those writes are finished.
00:09:37.089 - 00:10:23.595, Speaker A: Then I'm going to write to Y, wait until those are finished, then I'm going to read. Then I'm going to wait until X responds to me and then I'm going to wait until Y responds to me. This works, right? This is totally fine, but it's a little bit finicky because we're doing a lot of this waiting. So for example, why are we waiting for the stuff in foobar to be written text? Like why are we waiting for all that to go out to the network before we send anything to Y? Why not sort of send to X and to Y and then wait for X and Y sort of in parallel. And whichever comes back first, we're going to deal with first. And so in a, in a futures based world, what you would generally write is something along these lines. And then so this gives you back a connection.
00:10:23.595 - 00:11:06.141, Speaker A: And what we're going to do is we're going to write foobar on that connection. And then again, all of this is. The types here are all made up like think of this more as trying to explain the idea behind asynchronous computation. So here we get back the connection we do C read. And here whenever we get the bytes back from that, so let's say this returns the connection and the bytes that we read. Then we check that the bytes are barfoo, right? And here we don't even need X anymore, right? And then we did the same for Y. They basically did the same thing.
00:11:06.141 - 00:12:02.775, Speaker A: So the two futures are going to look the same, right? This is going to give you back a future. And this is also going to give you back a future. If you tried to like print this future or something, if you try to figure out what it was, it would just tell you, I don't know yet. Right? This isn't, it's not like I guess here, let's make this just be a right. So this entire thing evaluates to a boolean and, and this evaluates to a boolean, right? So if you were to look at this future immediately served on this line, we would do something like print line like the future. If we were to do this, this would not print true or false, right? What this would print is this is a thing that will eventually become a boolean. And so that, that's what we mean by asynchronous computation or futures.
00:12:02.775 - 00:12:35.205, Speaker A: Similarly here the type of this is also, this is something that will eventually become a boolean. And then where this gets interesting is now you have these two things that are basically just descriptions of what to do. You can think of this code as this code hasn't done anything yet. It's sort of lazy, it's not quite true. But you can think of it as it hasn't done all of the computation yet. It's just describing the steps that it's going to go through. Right? This is really just saying that whenever this finishes, do this, and whenever that finishes, do this, and whenever that finishes, do this.
00:12:35.205 - 00:13:00.455, Speaker A: But none of the things have finished yet. They might not even have started yet. And so this is where the notion of an executor comes in. So an executor is basically something that you can give futures to and it will just make sure that they get done. So if you have an executor, let's say X is an executor. I guess I should say A is an executor, right? Of some kind. We don't really know what executor is yet.
00:13:00.455 - 00:13:39.199, Speaker A: Then you can think of something like you're going to do a dot run. I guess we should call these FutX and Y. So we can sort of run Fut X and we can run. And only when we run them are we going to get back the X. That's this, right? But this still is sort of weird. This is saying run the entire X future and then run the entire Y future. That doesn't seem very much better than what we had initially.
00:13:39.199 - 00:14:09.725, Speaker A: And so what we might want to do is something like in this case, because we. We might not even care about the return values. In fact, maybe we want the assert back here. Maybe what we really want is to say just run those and have them happen. I don't care about the return value. That's when you can use the idea of a spawn. So here we would do so few decks and then, sure, we'll inspect this better, but let's use and then for simplicity.
00:14:09.725 - 00:14:52.899, Speaker A: So this maps to a boolean of whether it was equal. And we assert that it was equal. And then we do the same for Y, right? And so what spawn does is sort of tell this. Tell the executor. Just run this thing and make sure it gets run at some point and do the same for Y. And then we're at the end just sort of gonna like block on all. Let's say that that's what it's called, right? So what this is saying to the executor is just have these things run in the background and I don't care what order they finish in.
00:14:52.899 - 00:15:31.455, Speaker A: I just want them to be run at some point and just tell me when they're both done. So you might wonder, well, how does any of this help? Right? We've spawned these, but without knowing what that executor does, who knows? Like, how do we know they're running in parallel? And so this is where the whole idea of futures come from. And we're going to start looking now at the futures trait because that is where all the sort of magic comes from. So there are a lot of associated methods. Let's just focus on these top three lines. So a future. And keep in mind, both of these are futures, right? This is a future and this is a future.
00:15:31.455 - 00:16:02.215, Speaker A: A Future is a thing that has an item. So item is the value that it resolves to when it completes. In our case that would be boolean because this is a boolean and boolean because this is a boolean error which sort of goes away later. But let's talk about error for now. So error in our case is the error value that each of these futures, each of these intermediate steps would give you. Dpms. Stop blanking my screen.
00:16:02.215 - 00:16:38.565, Speaker A: So you can think of this as like tcp, TCP Stream Connect is basically going to give you back an IO error, right, if it fails. But in the futures world, what that really means is this future is either going to resolve into a boolean or it's going to resolve into an IO error. So think of this as like. Think of this as this could be a sequence of results where you use a question mark operator. So the error type just keeps getting propagated. So if you error, you don't execute any of the other and thens below you, you just execute your current one, it fails, and then the future just resolves into whatever that error type is. So that's what type error here is.
00:16:38.565 - 00:17:13.235, Speaker A: And then all of the magic for futures happens in the method poll. So let's zoom into that a little bit. So notice it has a pretty long docs in this because poll is very important in futures land. And let's look at what it does. So a poll, if you pull a future, it takes a mutable reference to self, so to the future itself and then it gives you back a poll of the item and the error. Okay, so what is a poll? A poll is really just a result async of the type and the error. So what is an async? An async is either ready or not ready.
00:17:13.235 - 00:17:39.745, Speaker A: And this is where we get to the heart of all of this, what the executor does. So what an executor does when it has a bunch of futures. So imagine that we have something like a struct. Executor. Executor. Think of it as sort of having a vector futures, right? It's not actually what it stores, but let's just imagine that's what it is. And when you do block on all.
00:17:39.745 - 00:18:34.985, Speaker A: Again, none of this is how the actual code is written, but I think it helps for exposition. What does what the executor is going to do is it's going to pull on all of the futures sort of, right? So what we're going to do is for like F in self zero, this we're going to call F and recall that F Pol. What pol really is saying here is there are three possible return values. It's error or it is async not ready or it's async ready. So if there's an error, then the future is going to resolve with that error. Right? So if poll returns an error, then that future error, then we might want some way to take some of the browser configure. Sure.
00:18:34.985 - 00:19:18.335, Speaker A: Is that better? So poll is going to. I guess we can match on it Might as well. So it can either return async ready and that includes a T, Right, which is the item values of the thing the future resolves to. It can return ok, async not ready, in which case we have to do something and it can return an error of some type E. Now imagine that we had some way of communicating back to whoever. So in this case we have the future. Right? So we don't really have anyone to tell about the result.
00:19:18.335 - 00:19:47.175, Speaker A: But let's think about run. Let's try to write this in a way where the return value is useful. Let's imagine that you had run all. We're going to change this to be instead of this, it's going to be run all fut x and fut y, I guess. Sure, sure. Let's do that. Right.
00:19:47.175 - 00:20:38.125, Speaker A: Does that make sort of sense? So we're going to have a runall method and instead of storing this inside the executor, we're just going to have an executor and it's going to be given the futures to execute vec future and it's going to return a vec of sort of the futures item. I know this isn't real types, but you can think of this as like I guess, results, future item, future error. Now this assumes all the futures have the same items and the same error. But let's just imagine this is fine for now. So what this function is really doing is it's going to keep like a results vector for F. Okay. So now our goal is to return the results of every future that we're pulling.
00:20:38.125 - 00:21:21.235, Speaker A: Right? We're going to later this future resolved, it returned ready and so now we have a result. I guess we're going to need something like, okay, so in this case the future resolved. It's basically telling us it's finished. So async ready means that the future said I have done all the work like I've done all the and thens and I have the result at the end. So the T here is the future item. So you can think of this as sort of like we're going to store in results I. Okay, Right.
00:21:21.235 - 00:21:55.445, Speaker A: And then in not ready, what this means is the future is telling us I have to do more work, right? There's a network packets I have to do or I have to do some more computation, come ask me later. And so in this case we don't really have anything to do with this future. This still has to do more work. So we just sort of continue. If we get an error, then we stall that error in results. Right? So given this loop, we're going to be given a bunch of futures, we're going to call poll on all of them. And poll you can think of as sort of do more work is really what poll means.
00:21:55.445 - 00:22:38.775, Speaker A: And then if it says I'm done, then we store the fact that it's done, whether that's an error or an okay. And if it turns not ready, then we have to basically call poll again is what not ready means. And so in our case what this means is we sort of need to do a loop here, right? Because otherwise we would never call poll again. Right. Now this is where you can see this start to sort of break down like results isn't long enough to hold all these results. We would need to know when we finish. But the basic idea I hope makes a little bit of sense, that not ready just means we have to call poll again to make more progress towards yielding the item in the end.
00:22:38.775 - 00:23:21.035, Speaker A: Right. And so I guess we can actually write this in a way where done is zero. Something like with capacity futures. Len? Sure, let's do something even stupider. We're going to not store these in order again. I realize these are kind of silly, but you'll see why in a second. Just to make this not be entirely stupid.
00:23:21.035 - 00:24:09.637, Speaker A: Ok, so now what we're doing is if we get ready, then we push that result along with the future that was resolved this way. So the index and I guess we do done plus equals one do the same here. And then this loop is now while done not equal to length. And then at the end we're just going to return results, right? So this means we're going to keep calling poll on every future until every future has returned async ready. Maybe cycle the iterator instead of the loop. So we need itermute because poll takes a mute self, otherwise we wouldn't have the mutable reference. Maybe cycle the iterator.
00:24:09.637 - 00:24:41.475, Speaker A: Oh, so this is broken in all sorts of ways, but I don't. It's the wrapping around it is not super important. It's more that you understand the idea that we're going to. If we're given a bunch of futures, we have to call poll on all of them. And then if any of them return not ready, we have to make sure to call poll on those futures again. Right now you might start to wonder the. There are many things you should start to wonder when you see this code, like what happened? So here, this means that we're going to pull a future after it has finished.
00:24:41.475 - 00:25:07.641, Speaker A: Why here? This seems like it's just going to be a busy loop, right? We're going to call Poll, it's going to tell us it's not ready, and then we're going to immediately call Poll again. That seems relatively inefficient. And you're totally right on both counts. In fact, you're not allowed to write this code. If you wrote this, your implementation of an executor would be broken. In particular, we need to dig into what Poll actually tells us. And so the poll documentation is actually pretty decent.
00:25:07.641 - 00:25:34.841, Speaker A: Notice that it says you want to query whether its values become available. We'll get back to this. Registering interest. It checks the internal state of the future and assesses whether the value is ready to be produced. Which means basically has your computation finished or has it not? And notice that implementers of the function should ensure that never blocks. Right. And so you can sort of see this from this code, if pole blocked, that we're not going to pull any of the other futures for a long time.
00:25:34.841 - 00:26:27.229, Speaker A: So they're not going to make progress either. Right? So you can think of poll here is just making progress on whatever you're doing, whether that's sending network packets, receiving network packets, doing some more iterations of computing, like waiting for a timer to pass. So poll is just going to do a little bit of work and then continue. And this should lead you to realize why calling run all on both future X and future Y means that we're going to make a progress on both at the same time. So remember, this is all single threaded, but we now have a single thread that is going to sort of poke at the X future and say, are you ready yet? And the say, I did some work, but I'm not ready yet. And we're going to poke at the Y future and say, are you ready yet? And it's going to say, not quite yet, but I did some work. And then we're going to keep alternating between them until one of them finishes and we're Just going to poke one until that finishes and eventually they're both going to be done.
00:26:27.229 - 00:27:19.455, Speaker A: And now a single thread has been doing both of the pieces of work in parallel just by switching between them and this extensive course to doing many, many computation. So you can have a single thread that manages say a thousand connections. And all it's really doing is checking all thousands of them, like, are any of you ready? And then one of them goes, yeah, I'm ready. And then it polls on that to make some progress and then it goes back. Is it an echo? Ooh, the drugs just kicked in. Let's see what I can do with that. Is there still an echo now? Or is it better like, is the echo now gone? This delay from speed to chat is pretty annoying.
00:27:19.455 - 00:28:11.995, Speaker A: Still the same, huh? And how about now? It randomly started in the middle of my sentence. That's so weird. So I just turned the microphone back off and on still. Huh. Well, that's weird. That shouldn't matter. And now I assume you can like basically not hear me and it's still there.
00:28:11.995 - 00:28:38.565, Speaker A: Huh. How unhelpful. Now it's good. What? That is the weirdest thing. So now it's fine. It sounds weird now. Little better.
00:28:38.565 - 00:29:27.655, Speaker A: Oh, I wonder, does that help? Like I. There's a. An internal echo that like there was another microphone turned on. Is it now better? Fantastic. That's so weird. Why did that suddenly kick in now? Huh? Oh, well, okay. So you can see how this setup allows us to handle many computations that are sort of like if you imagine that you have a bunch of computations and they're not really compute heavy, so they're not like taking up CPU cycles.
00:29:27.655 - 00:30:11.533, Speaker A: But instead all of the futures that you have are mostly waiting, like waiting for a network packet to either be able to be sent or to be received, or they're waiting for a timer, then they're all just sort of sitting there. And so this single thread has a lot of spare time. So if you had a thousand thread, one free connection, most of your threads would just be sitting there. Here we have a single thread that what it's going to do is just make sure to work on whatever is ready and just keep all the other things in the background. And so the where this is. So this is. This ties back to what we talked about with not ready, that when a future is not ready yet, so its final value is not there, then it returns not ready.
00:30:11.533 - 00:30:48.835, Speaker A: Right. But notice that here's where it gets interesting. In this situation, the future will also register interest of the current task in the value being produced. When the future is ready to make progress, it should unpark. Okay, so this park and unpark business seems important. You can think of this as when I call Poll, what you should do as the future is you should mark yourself as not ready by returning not ready. And then at some point in the future, you're going to make someone mark you as ready again.
00:30:48.835 - 00:31:42.749, Speaker A: Right. So here there's basically a contract where if you return not ready, you've arranged there's now some noise in the background. Sound is just the weirdest. I wonder if I can just do this instead. That might be better. How about this? Is that any better or is that worse and brings back the echo? I think that should be more what I want, but it's pretty unhelpful. Well, I guess the question is which? This is that one and this is this is that one.
00:31:42.749 - 00:32:14.407, Speaker A: Okay, great. Well, I guess, well, we're back to this. I think this is the right setup. I don't know what the noise is worse. I just wish audio would work. So you're saying now it's terrible or is it now fine again after I disabled this business, like now you can hear me and it's fine. Or now it's okay.
00:32:14.407 - 00:32:39.065, Speaker A: Now it's good. Fine. Okay, so there's this contract and not ready that says if you return not ready, then you must have arranged for yourself to be woken up again when you can make progress. Right? So Poll says I can't make progress. That's what. If not ready is returned, then that's what Poll means, I can't make progress. But I've like told my friend over there to wake me up when I can make progress.
00:32:39.065 - 00:33:52.399, Speaker A: The basic idea is like think of a future that is just like waiting on a timer, right? So you ask it, hey, are you ready? And it goes, no, I'm not ready. My timer has expired yet. What it's going with that future is going to do is let's imagine there's some other thread out there that's just running. This is checking time. It's going to tell that other thread, hey you other thread over there, can you wake me up when this time is reached? And so the way that works is there's this task notion. So where is the documentation for task, task, task, tasks, task current, Right? So you can sort of get a handle to yourself in futures world. So when I, when I pull this future, there's going to be some magic that makes sure that the this, this value sort of points to myself.
00:33:52.399 - 00:34:36.895, Speaker A: Points to sort of this executor, if you will. It's not quite this executor, it's. But you can think of it as this executor for now, when I call Poll Pol can call this method to get a handle to basically the current executor. So that's what this task is. Again, it's not really what a task is, but it's fine for now. That task item it gets, it can give away to whoever it wants, right? It can send it to a different thread, it can do whatever has to be. Same process, of course, but generally it's going to give it to some other thread and then that other thread is responsible for at some point calling notify on that task.
00:34:36.895 - 00:35:23.407, Speaker A: And what notify means is you previously couldn't make progress, but now you might be able to make progress, right? That roughly make sense. So here we're not actually going to get into that because it's annoying. And so you can sort of think of this as we're going to pull all the futures and then we're sort of going to go to sleep. So at the end here, sort of going to be a sub. Let's call it asleep. It's not actually asleep, right? Let's call it. So here the thread is going to go.
00:35:23.407 - 00:36:37.231, Speaker A: I pulled all the futures. All of them said they were not ready. So I'm just going to wait until one of them is able to make progress. And then on the side there's going to be sort of meanwhile here, meanwhile there's some other thread somewhere, like some other thread T notices that say a network packet arrived. And in particular it noticed that a network packet arrived, that this future that we pulled the return not ready was interested in, right? So again, think of a future of something like a TCP connection, right? If this is a TCP stream, then if a packet receives for that TCP stream, then the thread somewhere that it's like monitoring the TCP streams is going to notice, oh, a thing came in and this thread, when it returned not ready, it took its task handle and gave it to that thread. So that thread knows that when something comes in on this, this TCP connection, I have to wake up that task. So it calls Task Notify and that Task notify is going to unblock this thread.
00:36:37.231 - 00:37:36.615, Speaker A: Again, this is not actually thread park. There's some other mechanism that we have sort of wait for Task Notify to be called, right? And now you can imagine this being even more sophisticated. So for example, instead of just having one task that we give to all the polls we actually have, we sort of keep one task, one task per per future. So let's write some sort of pseudocode here. So here as the executor I'm gonna len task push sort of task new, right? I'm not gonna say what's in a task. There's some stuff in a task that lets you notify, right? But it's. Imagine that I could sort of create new tasks on my own.
00:37:36.615 - 00:40:00.237, Speaker A: Like you could easily imagine. It has to be something like mutex notify the executor when the result is ready isn't something similar to a callback strategy. Ah, so. So the notify is not necessarily the future is or when the result is ready. Notify just means I think you can make progress now, right? So imagine you have a TCP stream, right? So a TCP stream is a future that doesn't return until the entire stream goes away, right? So in that case if the TCP stream future gets notified, what that really means is just there's maybe something for you to do now because I received something or I sent something or something happened, right? And so you might want to check in on your stream again. So imagine that for every sort of top level future that we have, we create a task and then here what we're going to do is sort of tasks set current like tasks I and then down here we're going to do something like we don't really have a good way to do that here if as continue don't pull futures that can't make progress, right? So the idea here is that here f must have arranged for tasks I its task to be notified later, right? So this, this notion of a task both means that we know that there's no reason to pull a future that can't make progress, right? If we haven't heard a notification for it, then there's no reason for us to pull it. And then just before we pull it we sort of set this magical thing that set sort of sets what the current task is and then we pull.
00:40:00.237 - 00:40:41.237, Speaker A: So now if the poll tries to get a handle to itself to its own task, it will get tasks I. And so if it gives away that to some other threads, this might be a clone or something. Imagine that's a clone. Then if it gives away a handle to that task to some other thread, then that's really a handle to tasks I. And so if that other thread detects that there's something relevant for this future, it's going to notify. It's going to notify Tasks I which means that the next iteration we go through this loop, tasks I notified is going to be true. So we are going to poll again, right? But notice what this means is that if a poll fails to get someone to notify it, it will never be called again.
00:40:41.237 - 00:41:46.927, Speaker A: And so this is why part of the contract for poll if you implement a future, is that you must have arranged for yourself to wake up again. If you don't, poll won't be called again, you won't get to make any more progress. And so down here you can now imagine that we actually let this be a busy loop, right? The thread is just going to spin and it's only going to pull on things that are, that are ready to make progress and then eventually at some point all of the futures are going to complete and it can sort of go on its merry way. Of course, you could also imagine that this could be more efficient, right? Like down here it does something useful, but we don't really know what that useful is yet like currently. Let's just have it busy loop. It seems like the tricky part in all of this setup is to find some way to have these tasks be notified, right? Like how do I get something to be notified now? Futures doesn't actually. The futures crate doesn't really talk about this, right? The futures crate is only about expressing things that are asynchronous and the mechanisms that you use for waking things up.
00:41:46.927 - 00:42:58.975, Speaker A: It even has down here executor. So an executor is basically the thing that we wrote here. But executor here is really just a the definition or the, the types that you need in order to write an executor. So notify handle is sort of, you can think of this sort of akin to task like notify handle is the thing that we're going to make that we're going to set as the task that someone else is going to be able to call notify on. And internally it's like, I mean, who knows what it is internally actually unsafe notify. Gee, that's helpful. It's not clear, right? So the idea here is that if you are an executor, this is the with notify, right? So with notify this is really going to be something like executor with notify, right? And this appears not going to be task new, but it's going to be notify handle new, but all of this.
00:42:58.975 - 00:43:27.085, Speaker A: And you notice that notify handle also just has a notify. The ID here is so that you can have here. So actually what this does is instead of having a vector of these, you have a. You create your own notify handle. Let's ignore what this unsafe inside of it is. Yeah. And then down here, executor with notify.
00:43:27.085 - 00:44:13.493, Speaker A: So with notify is what should you do? So notice it takes a notify, which is going to be, all right, I guess this should be a notifier. So notice this is all still really just the task stuff, but now we're actually fitting into what Futures does. So it's a notifier, an id, which is going to be an identifier for the current future, which in our case is just the index into the vector. And then an F or F is why is T. Oh, that's fine. The closure F will be executed. Right.
00:44:13.493 - 00:45:07.695, Speaker A: So really what we want here is with notify says, just give me a notifier, I will set all the task stuff up for you and then call whatever you give me in this closure. In this case, what the closure is going to be is fold. And so we're going to match on this whole thing. And so notice how this is basically the same as what we wrote just written without sort of the additional vector and all this stuff. So the futures internally, the future stuff is written to have very low overhead, so you don't want it to do lots of vector allocations and whatnot. In this API now we just have this single notify handle and then we have these unique IDs that we use to figure out what's going on. And then notify handle, save notify.
00:45:07.695 - 00:45:54.453, Speaker A: I think that's what I want. Specifically, where's the notify trait? Where's the notify trait There. Right. So the real way to do this is just. So we actually, now that we. Now that we're decently close, we might as well do this anyway. Notifier is going to be arc new of something.
00:45:54.453 - 00:46:28.181, Speaker A: We're going to give that to this. So you could create a notify handle by giving it basically a thing that implements notify. It just happens to require that there's an ARC here so that it can easily make more of them. Right. So imagine that you're spawning, you're creating lots of futures, then all of them need a handle to the same notifier for your executor. And so an ARC seems like a reasonable thing for that to be. And then we need to implement the notify trait for what we give it.
00:46:28.181 - 00:47:15.645, Speaker A: So in this case, this is going to be like an enotifier, and we're going to do this in the stupidest way. So mynotifier. So mynotifier is really just going to be. What is it even going to be? A mutex of a vec of a bool. This is going to have an arc of a mutex of a vector. Again, this is a really stupid implementation that has all sorts of overheads. But my hope is to give you roughly an idea of what's going on.
00:47:15.645 - 00:47:55.815, Speaker A: So here, what we're going to do is we're gonna. Nope, we're gonna just use a mutex over a vec. And that vec is just gonna have sort of store this value that we use this notified, right? Whether it's notified or not. And we're gonna say that initially we should think of all the futures as being notified. We want to pull all of them sort of the first time. Otherwise they never get a chance to sort of set their notifiers right. So this is just a mynotifier and we're going to give that in and then we're going to implement notify for mynotifier.
00:47:55.815 - 00:48:40.473, Speaker A: And the notify trait does these things, provides implementations for those. And the main thing it does is have this notify method. So this is think of this as if something that's been given a task. So remember how in poll you can do task current, which gives you a task, right? And then on that task you can call task notify. So T is a task. Then you can call T dot notify. If you call T dot notify within this poll, what that's going to end up doing is calling notify on this notifier with this identifier.
00:48:40.473 - 00:49:37.105, Speaker A: And so that's what's happening here. And then let's just imagine that the way this actually works is this is going to lock the mutex, then it's going to set that index to be true. And down here what we're going to do here is self zero lock. So that is the the same my notif or the same the same mutex. We're going to lock it and look at I and if the ith element is true, sorry, if it's not true, then it hasn't been notified. I guess we could do this. And then of course, the moment we choose to pull it, we now want to say that it is not ready anymore, otherwise we're just going to keep pulling it.
00:49:37.105 - 00:50:36.329, Speaker A: Does that roughly make sense? Right, so the idea here is I guess this should be was or, yeah, was notified. Let's make it explicit. If the ifuture was notified in the past, sorry, if it was not notified, then we don't need to pull it. And now we said was notified to false because we're about to pull it, right? Okay. So now we have all the infrastructure that's necessary for notification, which is sort of causing a thing to wake up later. And that's all that's really defined in Executor. So the way we got into this was the question is what do we, how do we get notified to be called? Right? All of this infrastructure we've set up, all that really does is it's a bunch of infrastructure so that the when you call poll, you have a thing that you, you get a handle and you can call notify on that handle.
00:50:36.329 - 00:51:47.991, Speaker A: The question is, who calls notify on the handle? How do you know when you can make progress? If you return not ready, who's going to wake you up? Who are you going to give this task to? And that is what brings us from future land to Tokyo. So futures defines as we, as we talked about, so the general principles for, or the general interface for how to deal with asynchronous computation, the interface for things like tasks and notifiers and wakeups and executors, but it doesn't really define any implementation details, right? It doesn't give you an executor, it doesn't give you some other thread that's going to wake you up. It only defines the interfaces. Tokyo, on the other hand, is an implementation of both an executor and sort of the other infrastructure that you need in order to make things wake up. That is basically what Tokio provides. And in fact, as you can see from the documentation, Tokyo is an event driven, non blocking IO platform to write asynchronous applications with Rust programming language. And so in some sense what Tokyo is, is this business.
00:51:47.991 - 00:52:16.225, Speaker A: But this business was not too hard to write. Of course, it's a lot harder to write an efficient implementation. But it's basically this where Tokyo shines is it also gives you this thing that's going to wake you up. And in Tokyo that thing is called a reactor. So let's go to reactor or there are many things that can wake you up. The most common thing that can wake you up is that there's some kind of I.O. that you were waiting for.
00:52:16.225 - 00:52:40.983, Speaker A: Whether it's a disk reader, write that finished, a network reader, write that finished. Like some socket is ready. So like that kind of I.O. is the most frequent thing to wake you up, but there can be others and we'll get into those later, like timers for example. So actually maybe we should do timers first? No, let's do I O first. Right. So again, think of the case where F is a TCP stream.
00:52:40.983 - 00:53:03.395, Speaker A: I guess while we're here, we might as well make this proper. So this is going to be F. It's going to be F. Item is going to be F error where F implements future. Right. Remember, this requires all of your futures to be the same type. This is not true in sort of the real implementations, but you sort of get the idea.
00:53:03.395 - 00:53:52.423, Speaker A: Of course, normally you also wouldn't return a vector with the index is one of the values. That's also really weird, but such is life. Okay, so the question is, how can we do like what is a reactor? Well, a reactor is basically a thing that you can give a tuple of a task. So a thing to wake you up and sort of a handle to something the operating system is going to tell you is ready. So in order to understand this, we first need to talk a little bit about how Asynchronous AIO works in the first place. How much of a change is future 0.3? So we'll get into that a little bit later, in part because future 0.3
00:53:52.423 - 00:54:22.655, Speaker A: isn't fully stabilized yet. That's why this RFC is open down here. It is basically the same in terms of execution model, but there are some changes to the interface that we'll get into later. The reason I want to do it later is because Tokyo uses future 0.1. And so I want to talk about it in the context of Tokyo first and then we can talk about how this is changing going forward. Okay, so. So to talk a little bit about how asynchronous.
00:54:22.655 - 00:54:45.901, Speaker A: Asynchronous IO works in sort of. It works differently in different operating systems, but the basic idea is the same. So if you have some kind of IO handle, I can draw. Yeah, let's draw. It's time for some drawing. Who needs code? So I'm a terrible drawer. You're going to regret this.
00:54:45.901 - 00:55:05.021, Speaker A: I'm going to regret this. I guess. So in most applications you sort of have. Oh, I guess I need color white. Down here we have the kernel. Wow. Writing on this is hard.
00:55:05.021 - 00:55:21.395, Speaker A: So down here we have the kernel, and then up here somewhere is your application. Woo hoo. This is your application. It's a great application. You're super happy about it now let's erase that now. That's fine. What did I do? That's not what I meant.
00:55:21.395 - 00:56:04.453, Speaker A: So you have your application, you have your kernel, and your application has a bunch of outside connections, right? And the kernel, really those connections aren't to your application. Right? This is not really true. In reality, there are a bunch of connections that your kernel knows about, and it sort of has a, has a connection or sort of ties each of those connections back to your application. And the way it does that is that it internally has some identifier for each of these. And in Linux and Unix and macOS, not in Windows, quite sort of. So the idea is that each of these have sort of an identifier. Let's just imagine that they're numbers.
00:56:04.453 - 00:56:38.391, Speaker A: On Linux, they are in fact numbers. And your application, when you call, for example, tcp Stream, Connect, what you really get back is the number the kernel tells you, I connected and I gave it ID1. And so when you do connect here, in this case, imagine that you have four things already. You connect to some server, the kernel is eventually going to tell you, okay, that's now five. And it ties that also to your application. Identify. Yeah, so this identifier is the file descriptor in Linux.
00:56:38.391 - 00:57:24.389, Speaker A: In other operating systems they call different things, but this is basically a number that identifies this particular IO resource in the kernel or this resource in the kernel. So Connect is going to give you five. And then when you later, so let's say in your application you said something like X is connect, then what this, what X really is storing is just the number five, nothing else. When you call, say X write, and you give it, say foo, like you give it some string foo. What this turns into is really just you do a write system call. So a system call is you tell the kernel to do something. So that's the primary interface between these two is called syscalls, really what you're doing.
00:57:24.389 - 00:58:11.885, Speaker A: And this is usually a C API of some kind. So that's really just calling the syscall, right? With the number 5 and foo, right? So anytime you do an operation on X, you're really just doing an operation on a number. But the kernel knows about the mapping between these numbers and the actual underlying resources. Do all of that roughly make sense? So where this now gets tricky is imagine that I want to do, let's do clear this and then redraw just the basic things that we need. So we still have the kernel down here. It has a bunch of resources, it has numbered them. And your application is up here.
00:58:11.885 - 00:58:59.629, Speaker A: Draw it a little bit bigger this time. And it knows. You think of it as like it knows all of these numbers, but that's all it really knows. Can ask the kernel more things like if you Wonder what IP address did I connect to? There's like a system call you can do that says, hey, tell me more about number three. So in our future space world, right, we have these numbers, 1, 2, 3, 4, which are. Which we've mapped to say X, Y, Z and a Norwegian letter, because why not? I also chose the middle of the three Norwegian letters because it seems like more fun. All right, so we have these four variables that are all TCP streams.
00:58:59.629 - 00:59:37.325, Speaker A: And internally they're really just storing like 1, 2, 3, 4. But our application doesn't really know that, right? It just knows the names. And now we have some future that's computed over all of them. So imagine that we're going to do like X read, we're going to do the same for Y, we're going to do the same for Z and we're going to do the same for. Right, so all of these four we're going to do a read on. But of course, in our asynchronous model, we don't really want to first do Xs read, then do Y's read, then Z, then us. Instead what we want to do is sort of do all of the reads at once and just wait for whichever comes back.
00:59:37.325 - 01:00:15.497, Speaker A: This turns out to be a little bit tricky to do, right, because the system calls we have are read, right? It's a read of some number here, right? That's going to give you back some data. This is actually what the system call interface looks like. There has to be a buffer and whatnot. But generally like you can think of it as you read. You give it the number, the resource you're operating on, and it gives you back the data. So given that interface, how do we map that to doing these in parallel? Imagine we only have one thread. We want to execute these four in parallel on one thread.
01:00:15.497 - 01:00:39.649, Speaker A: When you do a read like that's going to block you. And so this is where the idea of non blocking IO comes in. The kernel has this like this additional parameter in here that's called usually flags. It's different on different operating systems. But let's just talk about Linux in general. It generalizes to Windows somewhat. The interface is a little bit different, but the ideas map pretty well.
01:00:39.649 - 01:01:30.337, Speaker A: So you can pass in flags and one of the flags that you have is one that's called, I think it O non block. Now this, this flag is not actually set on read, but rather it's something that you set on the socket. So you say that from this point forward, whenever I do an operation on say socket 4 I want that operation if it were to block. So if there's no data to read, I want you to return saying there was no data to read instead of just like blocking. So normally ignore this then. Normally when you do a read, if there's no data available, the kernel is just going to stop that current thread and does not return until there's data. If the socket that we're reading, like here, let's imagine we set this non block flag on four.
01:01:30.337 - 01:02:19.955, Speaker A: Then if this is four and there is no data, like there's no data at all, what the kernel is going to do. So imagine here I do a read of 4 and 4 has no data. What I'm going to get back is basically an error from the kernel and that error is going to be of the type would block. This is again different on different operating systems, but in general there's some error messages said you told me to read, there was nothing to read and you have told me not to block on four, therefore I'm going to just going to tell you this is an error I would have blocked. And similarly, of course we could set this non block flag on all of these resources. So we try to read it from any of them and it doesn't succeed. We're just going to get would block.
01:02:19.955 - 01:02:51.645, Speaker A: Okay, that's all well and good. So what this interface would give us is we could just sort of try to read X and then we could. And then if it returns data, that's great, then we can continue. If it doesn't return data, we then immediately go to Y and we try to read from Y. And so now we're just sort of polling them. Right? So this is very similar to the API we had in futures. We're just going to sort of try each one and then if none of them had anything, we go back to the beginning, we try each one, we keep doing this, but that is really inefficient.
01:02:51.645 - 01:03:20.935, Speaker A: Right, so imagine that we. This is sufficient though. Imagine that we have our executor down here, right? And we have these futures and each future is trying to block on a different stream. And so what the Remember how the core issue is is who's going to wake you up. So imagine that this future spins up another, or doesn't even spin up another thread. No, sure. This is my agenda spins up another.
01:03:20.935 - 01:04:51.889, Speaker A: So the second thread in our program and what that thread is going to do is it has sort of a channel into it from here that tells it a task and a one of these identifiers. So an ident. This would be a file descriptor, right? So every, every future, if you have a future and you call poll and it decides that it needs to do some reads from the network, and the network isn't ready yet, it sort of sends its own task and the file descriptor wants to wait on to this thread. And this thread is really just looping. And in every iteration of the loop it's doing like for task, an FD for each one. Then like do a read on fd and if this returns anything that is like not would block, then notify, I guess notify that task, right? And of course you would have to adapt this to also do things like D with other things than reads. You would also have to make sure that this read doesn't actually read anything, right? Because if this read actually reads, say 100 bytes from this file descriptor, then when we then go back to pull the future and it tries to read, those hundred bytes would just be gone.
01:04:51.889 - 01:05:21.651, Speaker A: And that's not okay. So writing with this interface is actually not straightforward, but you could imagine you could implement this way. But this is of course really inefficient because it means that first of all, sorry, you also need this other thread. And this other thread is going to spin, doing system calls. And imagine you're waiting on say a thousand sockets. Then this loop is just going to be trying to read from every single socket over and over and over again. So it's the time it takes.
01:05:21.651 - 01:06:06.871, Speaker A: So imagine that you're doing the spinning through, right? You're just reading from all of them until one of them returns not anything but wood block. And then say the like 500th socket is now ready. But you just checked the 500 socket right before it came ready, and you're on like socket 501. Then this future wouldn't be woken up until you've walked through all the other, all the other file descriptors and then gotten back to it. That seems really inefficient. And so this is not really the API we want. This is the API we'll want to use inside the future, right? If I am a future and I'm a TCP stream and I'm trying to read some data, I would do read, right? Because I need to read.
01:06:06.871 - 01:06:46.545, Speaker A: And if I get wood block, what that translates to is really I want to sort of register my handle with this thread and then I return not ready, right? So that, that part is all fine for using read inside of that, but for implementing this thing that wakes people up we don't really want to use this sort of non blocking interface because it's really inefficient and also not entirely clear how to even do correctly. Right. For example, imagine that you had to do this for. You do have. You would have to do this for rights as well. What would you write here to test whether it was ready to write? Write like no bytes. Is that even legal? I don't know.
01:06:46.545 - 01:07:03.669, Speaker A: I think then it would never block because it says, you asked me to write zero bytes. I did it. So you wouldn't even know whether it would block. Okay, so this is all clearly kind of stupid. So this interface is not really what we want. But there is a better interface. Ooh, that's a good question.
01:07:03.669 - 01:07:20.035, Speaker A: Can I erase just some of this? This is a new drawing program. Oh, that's awful. Can I somehow change. There we go. Brush size. Make it a big one. Yeah, great.
01:07:20.035 - 01:07:48.625, Speaker A: That's maybe larger than I wanted. All this goes away. All right, make it small again. Want it? Nope. D. Great. So this comes back.
01:07:48.625 - 01:08:05.965, Speaker A: This comes back. All right, so we want a better. Basically we want a better interface that lets us do this wake up business. And it turns out there is. There are different interfaces to use for this for different operating system. But we're going to talk about one called epoll. Epoll is what you end up using on Linux.
01:08:05.965 - 01:09:11.044, Speaker A: And all the other operating systems have mechanisms that are not quite the same, but have a similar end result. So epoll is this interesting system call that you can sort of think of as you give it a bunch of file descriptors and then you give it a bunch of sort of operations, but not the actual read or write operations. Just sort of says, can I read or can I write or imagine just those two. Right. In fact, we, because we're allowed to cheat, we could make this a lot nicer. We could just do. So we're going to have something like it's going to be given a tuple of FD and op where OP is either is sort of can read or write.
01:09:11.044 - 01:09:51.685, Speaker A: And then it's given multiple of these and then it returns a list of FDs. So let's go through this in a little bit more detail. So the idea here is that if I'm a thread, I can call E poll and then say, hey Colonel, I want you to block me. And then I want you to tell me if any of. For each of these pairs, if any of them, that operation is. This operation is ready for this fd. So if you do something.
01:09:51.685 - 01:10:47.299, Speaker A: So this operation is Ready for this file descriptor. Right. And what kernel is going to do is it's sort of going to internally add markers to each of these to sort of say, hey, so imagine that we call this with like four and read and one and write. It's going to add a marker to one saying, hey, if you detect that, I can now do a write. That a write would no longer block if I tried to do it. So basically the space in the outgoing connection or into four, hey, if you notice that there's data available to read, then the kernel is going to keep sort of a little thing over here, sort of like a notifier. This is very similar to what we did for future.
01:10:47.299 - 01:11:12.901, Speaker A: It's just internally in the kernel. So it's going to. I guess it's not going to mark on these, it's going to mark on 1 and 4 and then say, if you see a write, then come poke me. If you see a read, come poke me. And then this E poll is going to block until this thing gets poked. When it gets poked, it will return the E poll. And it's a little bit smarter than this.
01:11:12.901 - 01:11:29.935, Speaker A: In fact, ePoll is a system call. Yes. So similar to read and write, except it's. This is the interface instead. And if it's a little bit smarter, it initially, when you first call it, it's first going to check all of them. And if any of them are ready, it's going to return straight away. Right.
01:11:29.935 - 01:11:53.865, Speaker A: So it might tell you immediately that, yeah, four is ready for reading and one is ready for writing. Go ahead. That might just be what it returns. But if it detects none of them already yet, then it adds these watchers. And what it returns is these are the file descriptors that are ready. I think it even maybe gives you the ops as well. So what this returns is like four is now ready for reading.
01:11:53.865 - 01:12:35.515, Speaker A: Maybe one wasn't ready for writing yet before is now ready for reading. And now you might see how this fits into our whole executor ID here. So because all we really have to do is have this other thread, it's still been going to be given task and identity, and maybe now it's also going to be given an op. Right. And then inside of. Inside of this, inside of this other thread, that's going to wake things up. We're going to do something like this actually here we might switch to code again.
01:12:35.515 - 01:13:37.461, Speaker A: All right, so we're now looking at what's inside of that other thread. I guess let's do it at the bottom so this other thread like this is going to be the reactor thread. And the reactor thread, when you start it, you're going to give it. What are you going to give it? You're going to give it like a channel, really going to be a channel. Let's imagine that it's a channel of task and a file descriptor. FD for file descriptor and an operation where enum operation is read or write or I guess read or write. You can think of this as can read and can write.
01:13:37.461 - 01:14:35.195, Speaker A: But read or write and internally what the reactor thread is going to do, no loop, it's going to operate forever. It's going to keep sort of a waiting for which is going to be. Let's imagine that it's a hash. Mmm, no, sure. Let's imagine a taskmap from FD and operation to task on every iteration of the loop. It's going to make like a select set. So this is select here is the sort of set that we're going to give to epl and that's going to be waiting for keys collect and then it's just going to call epoll.
01:14:35.195 - 01:15:44.495, Speaker A: Remember, EPOLL will. EPOLL returns this out of file descriptor and operators. Right. So this is going to return something like FD and OP in E poll select. So we're going to do an E poll over all the things we're currently waiting for. And then for everything we get back, we're going to do task is waitingfor remove ftop and I guess this can even go away notify and we might even do something while let some TFDO is equal to notify me. So we're also going to sort of accept more of these, if there are any.
01:15:44.495 - 01:16:17.329, Speaker A: Then we're going to do this insert fdop. So this is our reactor thread. It doesn't really do all that much. It just accepts new things to watch for and then it just watches for all of them. And anything that's ready, it will notify. In fact, when you look at this, why does it even need to be a thread? Why does this have to be a separate thread? It doesn't. Right here.
01:16:17.329 - 01:17:04.703, Speaker A: Remember how when we were writing our executor. Oops, oops. Here. Remember when we're writing our executor, there's like some piece of time during our executor where we might not have anything to do. We've pulled all the futures, all of them have said that they're not ready. So how about we just do some useful work like run the reactor. So instead of having the separate reactor thread and have this channel, how about we just sort of do this, keep waiting for up here and then down here.
01:17:04.703 - 01:17:43.885, Speaker A: We're just going to just sort of gonna whenever, whenever we know there's nothing more to do. I guess technically we have to be a little bit careful here. Sorry. It's the deadlock that we need to fix. You could also imagine. Well, okay, I'll ignore that for now. So what we're going to do is whenever we know all the futures can't make any progress, then clearly we need to wait for something, right? We need to wait for something to happen so that they'll be ready again.
01:17:43.885 - 01:18:21.915, Speaker A: And why don't we do the stuff that's necessary to make them ready again. In fact, it's totally fine for us to block until they're ready again, which is what we're doing here, right? EPO is going to block on all of these select things on everything that everyone is waiting for. And then whenever you pull returns, that means something is now ready. And so we're just going to notify those and then we're going to go back to the top and then we're going to go through all the futures again. So now this is all still single threaded. There's none of this extra sort of indirection through a channel. All that is unnecessary, in fact, because all of this is now single threaded.
01:18:21.915 - 01:18:58.795, Speaker A: We don't really need this ARC and mutex, but we're going to leave it in because getting around it is a little bit annoying. But just believe me that it's possible to get rid of that extra. These extra indirections. We do have one issue though, which is now because we got rid of that channel and even if we did have the channel, how is poll. How does the futures poll send some? Like if we still haven't said how it adds something to waiting for. Right. How do I as a future I'm given no arguments to poll.
01:18:58.795 - 01:19:39.141, Speaker A: How do I sort of say to the wider world that hey, I want to wait on this file descriptor. I have no way of communicating that. And so in Tokyo with reactors reactor. Oh this I might need Docs RS Tokyo Reactor So Tokyo is split up into many, many sub crates that deal with specialized things and then Tokyo sort of putting all of it together. Why are we dropping was notified. Oh, sorry. The reason we drop was notified is.
01:19:39.141 - 01:20:18.169, Speaker A: Was notified is holding the lock and pull might try to notify itself immediately. And then if we're still holding the lock that notify would block and would never be released because we're still holding the lock. So it's just releasing the lock. We could easily do this as well with just this, just to prevent deadlock. Hello. Right, so let me save this just for posterity. Yeah, sorry.
01:20:18.169 - 01:20:59.695, Speaker A: So Tokyo has broken it into lots of small sub crates. Basically, there's one for the Tokyo executor, there's one for the Tokyo reactor, there's one for Tokyo timers, and the Tokyo crate itself just sort of brings in all of these things and is intended for anyone who uses futures. If you are implementing futures yourself, you should depend on the individual crates like Tokyo Reactor. And that's because that has all the stuff that you need to be able to notify. And so here we'll see that in Reactor. Reactor. Reactor.
01:20:59.695 - 01:21:26.889, Speaker A: Sorry. Yeah. So remember how we had this executor with notify, right? Which sets the notif the sets basically the task that's going to be returned by task current. Well, Tokyo defines this reactor with default. So it has handle, which is a handle to a reactor. And. And handle has a handle current.
01:21:26.889 - 01:22:17.535, Speaker A: So this is very similar to task current. So inside of a future, there's also going to be the ability to do handle or I guess Tokyo reactor handle current. And that's going to give you back a handle. And a handle is basically. So if we look at handle, if we look at reactor. I mean, no, sorry, where is the register? Yeah, so you can create a registration. And registration is basically like this tuple that we have of task and task and file descriptor and operation.
01:22:17.535 - 01:22:43.207, Speaker A: You say register with and you give it a handle. And the handle is going to be the reactor. And so this is saying sort of giving to the handles, telling the handle, hey, this is one of the things I now want you to notify on. And so given that we're implementing our own handle, Tokyo isn't. Because Tokyo just provides us with a reactor. This isn't an interface that we can write ourselves. Because Tokyo just provides you an implementation.
01:22:43.207 - 01:23:08.911, Speaker A: Right. But if we were to, the idea would be that we create a handle and handle has a method. If we could look at it internally, I guess that might not be helpful. Actually, it's a lot of very optimized implementation. But the idea is that inside of the future you can call handle current to get a handle to the reactor. In our case, the reactor would really just be waiting for. Right.
01:23:08.911 - 01:23:49.505, Speaker A: So this I guess would also be an arc mutex, probably. And then given that handle, of course, now you can add something to waiting for and if we go back to here, you notice that this with default. So in addition to doing this we would also do Tokyo reactor or if this was sort of generic it would just be reactor. Right. And we would say this with default we would sort of do like my handle. Like we'd make our own handle ignore enter for now and the closure that's going to be called is this. Right.
01:23:49.505 - 01:24:28.665, Speaker A: And so this sets the what handle? I guess this is going to be. Yeah, this sets what handle current is going to return and then my handle here would really sort of be waiting for. Right, this sort of makes sense. So inside of poll we now have access to the current task for Wake Ups 3 task current. We also have access to the current reactor through reactor handle current and so this, this we can use to notify this we can use to put something in waiting for which will also be the causes to be woken up later. Yeah, we'll we'll get to future 0.3 later.
01:24:28.665 - 01:25:17.335, Speaker A: Okay. So that is basically all the infrastructure that we need in order for. For poll for futures to be able to have themselves be woken up later when IO happens. Right. So inside of a future where can I down here? I guess so let's say we have some struct which internally contains like an FD a fault descriptor of some kind and then we're going to implement future for fruit. Its item is going to be nothing, its error is going to be nothing. Because whatever it's not important for this particular discussion this returns we're going to make it explicit just so we don't miss anything.
01:25:17.335 - 01:26:05.475, Speaker A: Right. So we're doing this actually let's imagine this is standard TCP stream with the exception that but with non blocking set. Right. So this means that if you tried to read from it and it failed it would give you. It would just say would block. So what this poll is going to do. Let's imagine that what we really want to do with this is just print every time there's stuff coming back from it.
01:26:05.475 - 01:27:19.991, Speaker A: So poll is going to do like self.fd. read right. Now let's match on that I could be which is it read some number of bytes, right? And if that's the case it's just going to print line that like got this many bytes rw Again this is with imaginary types. If it got error and the error was like an IO error would block. So if it got a wood block then it's going to return async not ready like this and if it got any other error then it's going to return that error, right? If any, I guess what we could do is like IO. Error closed again. This is not actually how streams work but it's not important if we got closed then now we know that we're not going to get anything more.
01:27:19.991 - 01:28:16.149, Speaker A: So now we're going to say that this future is ready, it's finished, nothing more to do for it, it's not going to do anything more that's useful work. So this might be the naive way to implement poll but of course now we have no way of waking ourselves up. So here we've returned not ready but we haven't arranged for ourselves to be woken up. So this implementation is clearly broken and so here we're gonna have to do something to make sure we are woken up. And in fact it turns out that making ourselves be woken up is not that hard anymore now right? We're gonna basically do handle current. So this gives us back reactor, right? And then the. Let's just ignore Tokyo for a little bit right now.
01:28:16.149 - 01:29:42.901, Speaker A: So reactor dot sort of register self FD Wow. Ah FD Imagine that we could do this, right? Sorry, that's still not sufficient and we're going to have to give it task current because it needs to know who to wake up. And now if you imagine if you were to sort of squint at it, reactor could totally be waiting for from up here. So that when we call reactor register it really just sticks the file description operation into there and the task current as the value of that map. And now we know that if it's ever possible for us to read again we'll be woken up appropriately. Even this code is a little bit broken though because imagine that we do a read and then we get some data. Then of course, well this wouldn't type check in the first place but I guess here what do we return in the OK case like the read succeeded and so we got some data, now what do we do? Well, we could return okay, not ready to indicate that, to indicate that there, there's going to be more data, right? It hasn't been closed so we might get more data.
01:29:42.901 - 01:30:12.755, Speaker A: But then of course then we have to do this in order to maintain the contract and that might be fine. Like this would still work just fine but. But imagine that there's a bunch of data available, we read a little bit of it. We don't really want to have to go through the entire song and dance sort of go to sleep, put us on the reactor. The reactor does epoll we get woken up and then we do a read again. If we could just immediately read again. So actually what we're going to do, and this is usually what you end up with in poll implementations, is we're going to loop.
01:30:12.755 - 01:30:44.295, Speaker A: So as long as it's possible for us to read more data, we're going to do it right. So remember that read will never block. Now read will always return immediately. If it would have blocked, it returns would block. So the read is going to come back, we're going to read some bytes and then we're going to immediately try again. And it's only when the read tells us there is no more data now that we register on the reactor and then return not ready. Right.
01:30:44.295 - 01:31:22.003, Speaker A: So I guess this is going to be something like printbytes read because that's what it does. And this is actually the implementation of a future that you would have. Now that's only somewhat true because the interface for reactor is not actually this. If we look at reactors, when you get a handle, the only method on handle is current and implements some other things. But that's basically unhelpful. Instead what you do is you create something called registration. So registration is an IR resource.
01:31:22.003 - 01:32:44.355, Speaker A: Right. So this would be a file descriptor and an operation, for example, I guess actually there might be a higher level background maybe. No polamenting. Yeah, this one. Yeah. So you would instead do this. So you would do something like pull evented new with handle and then you would give it the FD and then you would give it the reactor and then what does that give you? Dot polread like so so that's how you would actually write that code.
01:32:44.355 - 01:33:16.991, Speaker A: And if. Right. So if you, if you look at what this does, it creates. So poll evented is basically a str struct that contains things like the file descriptor. So it's basically the same. It's just wrapped in types and new with handle is saying I want to listen for this file descriptor on this reactor and then I want to check, I want to wait for ready. Now this actually returns if we look at it poll read ready returns a poll of.
01:33:16.991 - 01:34:08.645, Speaker A: I don't know what ready is here. Oh yeah. So this returns some stuff that returns either async ready with a thing that we're going to ignore for now or it returns okay, async not ready or it returns error. And if we look at what, where is it here? If the resource is not ready for a read, then not ready is returned and the current task is notified once a new event is received and this last part is the crucial bit that we needed. Right? Remember the contract for not ready as yet. You have made sure that you will be woken up again. If we call poll read ready and it returns not ready, then we know we're fine.
01:34:08.645 - 01:34:55.775, Speaker A: So here, if we get not ready, then we know that we can just return, okay, Async not ready and we know that the contract is maintained. You get error, then we just return that error onwards if we get ready here. So first of all, keep in mind, you will never have to write this code yourself because there is Tokyo Net. TCP stream does all of this for you. You will generally never have to write at as low as of a level of interacting with reactors. I don't think I've written any code where I had to interact with a reactor directly because you just go through things like TCP streams. But this is more the goal here is that you understand what's going on behind the scenes.
01:34:55.775 - 01:35:33.575, Speaker A: Okay, so Async ready basically tells you that if you were to try to read now, you would actually get something. So this is really just saying try again. So this is like the file. So this is really itself referred to as a socket. The file descriptor for a network thing is called a socket. So in this case, what the saying is that the FD socket became ready between when we read and called poll read ready. Now, of course the actual implementation is smarter than this.
01:35:33.575 - 01:36:01.011, Speaker A: It doesn't. It doesn't call read and do this read and sort of do an extra system call. It doesn't have to do that. It's smarter than that. But in our implementation this is fine. And so notice that this now this poll now follows our contract and it does all this registration with the reactor and all of it just does the right thing. And then down up here we're just going to.
01:36:01.011 - 01:36:43.351, Speaker A: If you imagine that this handle is really a handle to our waiting for, then this all does the right thing. And in fact now we're getting pretty close to what Tokyo does. So Tokyo has this idea of a runtime. So a Tokyo runtime is basically a thing that provides you with a reactor, an executor and then also a timer. So all of the things we've talked about for reactors so far also applies to timers. Right? I imagine that I have a future and I want it to resolve 10 minutes from now. Then who's going to wake me up in 10 minutes? You could spin up another thread and have it just check the time, all the time.
01:36:43.351 - 01:37:26.355, Speaker A: But in reality that's not really what you want to do. And so instead what you do is at the bottom of each loop, like this, you sort of check whether any of your timers have expired. And if they have, then you notify the appropriate tasks. If they have not, then you go down to ePoll and then instead of just using ePoll, use timeout and then you give like the min remaining timeout here. So what epoll timeout does is, is it will. It will block in the kernel, but it will block for at most this long. And so the reason you want to do this is to ensure that if there are timers you're waiting for, you'll sort of get back to them in a timely manner.
01:37:26.355 - 01:38:24.891, Speaker A: Imagine that the things you're e polling over, all of them just block for hours and hours. If you just blocked in here, you wouldn't check the timers again for hours and hours, which means that the timer that's supposed to go off in 10 minutes wouldn't go off in 10. And so instead you will do EPOLL timeout. And here the EPOLL would then return in 10 minutes and then you would go through the loop, see that some of the timers have expired, notify them, etc. And timer is very similar to reactor in basically all the ways. Tokyo timer, if you look at it, there's a with default and there's a timer handle current, right? So the setup is exactly the same as for IO, like the interface for interacting with it. And then of course you end up with here, Tokyo timer, with default, whatever.
01:38:24.891 - 01:39:00.125, Speaker A: And then this closure, right? So you can see that this starts adding up a lot because you have to give it handles to all the different ways in which it can wake up. And so this is why we have this notion of a runtime now. And a runtime is really just. It gives you all these three things. Is it possible you rarely reach that bottom part because there are many futures in the queue? Yeah. So imagine you have tons of futures and they're always already. You basically need to find the right way to switch between dealing with the futures and driving the reactor or driving the timer.
01:39:00.125 - 01:39:37.683, Speaker A: We'll get back a little bit to that question then, about what do you do if there are lots of futures? But the basic idea or the thing to keep in mind is the executor does have to deal with this. It needs to know that sometimes it needs to not execute futures because it has been a while since it's checked its other stuff. This is basically the idea of fairness. You want your executor to always make progress. And if it has lots of futures that it's calling pole on, then it is making progress. So in some sense it's doing the right thing. But what you're asking about is.
01:39:37.683 - 01:40:17.293, Speaker A: But that means that there are some things that are not going to be notified, right? So you're not providing fairness, you're not allowing all the futures to make progress. Instead you're taking just a small set of the futures, the ones that are currently ready, and just executing those. And so again, you have to think of this as if you only have one thread. Then if it's always if it's doing work, then it's being productive. So is it then better to check the timers and work on some other futures? It's unclear, right? You're already using all your cycles and so why is it better to check that other future? That depends on timer. Maybe it's not. But it is true that that means the timers might take a while to fire.
01:40:17.293 - 01:40:55.775, Speaker A: They might not fire straight away. And so the. Usually the executor has some kind of almost like time granularity of. In fact, if we look at tokyotimer, I think it says where is it Delay, maybe Delay has a resolution of 1 millisecond. So if you want a timer that's finer grained than that, it's not going to give it to you. I don't remember exactly the details around this. I think it's also related to the fidelity of the granularity of the timestamps you can give to EPOLL timeout.
01:40:55.775 - 01:41:38.447, Speaker A: But in reality, the executor also has to make sure that it checks the timer that often. So the notion of a runtime is really just all the stuff that we've written so far. All this with default, with Notify business and also all of the. So here, just like we had. Just like we had timers and reactors, we also have Tokyo Executor. And Tokyo Executor is basically the thing that we wrote and no, that's not what I want. Right.
01:41:38.447 - 01:42:09.165, Speaker A: So Executor with default basically just calls into the future Executor with notify and there's a. Where is the right. So this is just all the traits that you need around an Executor. And then the runtime provides an implementation of reactor and implementation of an executor and implementation of a timer there are. Right. So if you see what a runtime does, it spins up a. That's not true anymore.
01:42:09.165 - 01:42:45.405, Speaker A: This documentation should change. Documentation is wrong. If we look towards the bottom of runtime, you see that there's a runtime here, and then there's a module called current thread. We're going to look at current thread first, because that's more similar to what we've built. And then we're going to look at the default Tokyo runtime afterwards. So the current thread runtime is a runtime implementation that runs and everything on the current thread, which is the thing that we've been writing so far, and at the heart of it is a runtime. And all of.
01:42:45.405 - 01:43:21.835, Speaker A: All that runtime does is basically the stuff that we wrote here, but in a much more efficient way. And so when you create a runtime, you just create it. You get back a runtime, and on that runtime, you can get a handle to it if you want. So the. This is a runtime handle, and a runtime handle lets you spawn futures. So remember how really early on we talked about how you can. You can sort of do something like a dot spawn future X spawn does not let you learn the result of a computation.
01:43:21.835 - 01:43:46.115, Speaker A: Yeah, exactly. So if you spawn something, it basically means. I don't care about the result of this computation, just make sure it gets run on the background. So this is usually what you would do with managing connections, for example. So imagine that you have a. Like a server, which is going to be a TCP listener. New dot incoming.
01:43:46.115 - 01:44:13.195, Speaker A: Yeah, I'm going to list it on 4. Incoming gives you a stream, which we may or may not talk about streams. It's relatively unimportant. But think of this as like, this gives you a foreach thing. So this gives you a. Think of it. Basically, it gives you an iterator, but it's sort of an asynchronous iterator.
01:44:13.195 - 01:44:52.395, Speaker A: So this returns a future, not an iterator. And then foreach is going to be called for each new stream that we get. And basically what we're going to do is going to do Tokyo Spawn. So Tokyo Spawn is really just syntax for Tokyo. How do I want to run this? Sort of for Tokyo Executor handle current Spawn future. So if I write this, it is really just that. Yeah, keyboards are hard.
01:44:52.395 - 01:45:32.155, Speaker A: These are basically the same, except actually it's not executor. It's runtime. But you get the idea. And so in here, usually what I'll do is something like I'll spawn S. So S here is a TCP stream, right? And so usually I'll do something like my application handler over S and I'll spawn that. So this is instead of spawning a new thread, I create a new thing that has all the state for a connection. So this might be like client connection, whatever you want to call it.
01:45:32.155 - 01:45:58.267, Speaker A: And so I created, I guess, new. I create a new client connection over that stream, and then I spawn that client connection. So client connection itself is a future, right? That will resolve whenever the client disconnects. And that client connection we give to Tokyo Spawn. And now Tokyo Spawn is going to. Or the executor that Tokyo has is basically going to add it to the set of futures that we're operating on. So keep in mind here we have run.
01:45:58.267 - 01:46:42.085, Speaker A: All right? So what you could do is here instead of having run all, you have a vector of F future. And this instead of being run all is just going to be a run. This is going to make us sad. So let's not do it. Sort of. Sure, why not? So Spawn takes an F, does not return anything, and just a self.1.product. That's all spawn really does.
01:46:42.085 - 01:48:00.845, Speaker A: Now, of course this is not how it actually works, but you can sort of think of it this way, right? So if you spawn an executor, you're really just saying, hey, by the way, also execute this thing and I don't care about its result, right? So here we're spawning that client connection and having it be run in the background, and Tokyo will just automatically deal with that. Where were we? I don't need these anymore, so let's make those go away. So if you have a current thread runtime, so that includes a timer or reactor and an executor all running on a single thread, then you now have a spawn method which lets you spawn any future. And notice that it spawns only futures that do not have an error or item type, because you have no way of getting at its return type, right? If you spawn something, you sort of lose the handle to the future, so you don't know anything about it. It also has block on, which is you give it a future and it will give you back the result of that future. So think of this more akin to running that future directly and then still running the other things. So the executor is still running, but you, whenever this particular future finishes, you're going to return it.
01:48:00.845 - 01:48:43.431, Speaker A: Now, of course, because this is a current thread runtime, the thread can't do anything else while this is running. So if I call block on, it really is blocking the current thread until that future finishes, and then it gives me the result. And of course, because I once this returns, this thread is no longer running that runtime, as all of the futures that are spawned on that runtime will also not make progress. The reactor is not going to make progress, the timer is not going to make progress because we have nothing to run it. The current thread has returned and is doing something else. And that's also why this is run method. So run is sort of similar to our run all down here, right? Except that it doesn't return anything.
01:48:43.431 - 01:49:38.603, Speaker A: Run is just going to operate over all the all the futures that we have stored and it just assumes that they're spawned, so it's not going to return their value at all. In fact, we could now write this sort of like I'm not even going to write it, it's not important. So run is really our run all, but it just drops all results. And block on is sort of like run all except that it returns the result for a particular future, namely the one you gave it. Okay, so that's all current thread runtime and it's basically what we've written so far. But then of course, the observation that came from the chat here as well was what if I have many futures? Like I have lots of futures, all of them are doing stuff. Then it sounds like I need more than one thread.
01:49:38.603 - 01:50:10.965, Speaker A: How do I do that? Right, and that's when we get back to the Tokyo runtime itself. So remember how we looked at current thread runtime? There's also the general runtime. This is also what if you call Tokyo run and give it a future Tokyo run is going to use the default runtime. The way in which the default runtime is special is that. Again, this is not true. It uses a thread pool. Whoa, okay.
01:50:10.965 - 01:50:45.465, Speaker A: Executor. No, I think this is Tokyo pool. No, executor. What's it called? Thread. That's the one. So instead of using the current thread executor that we talked about this far, it uses the Tokyo thread pool executor. And the documentation here is pretty helpful.
01:50:45.465 - 01:51:16.355, Speaker A: It says that this is a work stealing based thread pool for executing futures. It supports scheduling futures and processing them on multiple CPU cores. Users will not normally create one themselves, but instead will use it via runtime, which we've talked about so far. We're not going to talk about blocking threads for now. And so here is sort of the basic description. Each worker has two queues, a DQ and MPSC channel. The dequeue is a primary queue for tasks are scheduled to run.
01:51:16.355 - 01:51:51.771, Speaker A: Tasks can be pushed onto the dequeue by the workers. But workers may steal from the dequeue the channel is used to submit futures while external to the pool. Okay, so here we're going to need to do some drawing. Again, switch back to white. So in this setting you're going to have your. All of this is inside your application and we're going to have your code here. As you can tell, this is clearly code.
01:51:51.771 - 01:52:36.905, Speaker A: You can tell because there's a curly bracket. That's how you know it's code. And then what Tokyo thread pool does is it's going to make down here pool. It's going to start up multiple threads. Say that you have four cores in your machine. So this is a pool of four threads. Each of these things is a thread, as you can tell because there's a little thread on them.
01:52:36.905 - 01:53:39.695, Speaker A: And each thread is going to have Q. So and your code, which has a. Somewhere in here there's like a let RT of type runtime, right? And with this runtime you have the ability to call dot spawn and give it a future. And what this runtime really is, at least kind of sort of is a sender. So this is an MPSC sender. And so if you spawn the future, that future goes through the sender and the sender just sort of points to the pool again, kind of sort of. And in fact, I don't think this is.
01:53:39.695 - 01:54:33.741, Speaker A: Let me double check this, right? And what's going to happen is every worker thread, this queue that they have is a queue of futures. So all of these are futures. So each thread has a bunch of futures that it's executing. So this is similar to the sort of vector of futures that we were storing in our own sort of single threaded implementation. So each of these futures, so each of these threads has a reactor, has a timer. Unclear, may or may not have a timer. It might be that it spins up a separate thread for timers.
01:54:33.741 - 01:54:58.719, Speaker A: I don't remember. But let's imagine that it has a timer too. So each thread has a reactor and a timer down here, which is, remember, just a part of the thread. This is not a separate thread. Just like in our executor, the reactor and the timer are driven by the thread itself. Just so with these worker pools. And so each of these workers is going to have the same kind of loop as we wrote in.
01:54:58.719 - 01:55:35.649, Speaker A: Ooh, I can drag and drop with a button press. So here, each of these is really. You can almost think of each of these as just a current thread executor, right? So each of these is basically one of the things that we wrote ourselves. They're basically indistinguishable. Their main loop is just take the next future from my queue, pull it, and then try to make some progress of these if necessary. Or I think it pulls everything that's in the queue. Like it walks as queue, pulls all of them and then puts them back on its own queue.
01:55:35.649 - 01:56:22.145, Speaker A: And then after it's pulled all of them, then it goes. So color go back to this one. So the steps for one of these is let's go in here. One poll every future in my queue, right? So that's the first thing it's going to do. And then there's like 1.1. If ready, then remove, right? So if poll returns, ready, there's nothing more to do for that future. So we just get rid of it.
01:56:22.145 - 01:56:47.825, Speaker A: If not ready, then put back on my queue. I don't know why I'm writing these. I don't know that the writing helps, but it's sort of. That's the protocol. Like for every future in my queue, I'm going to pull it once and then if it's ready, I get rid of it. If it errored, I get rid of it. Otherwise I put it back on my own queue.
01:56:47.825 - 01:57:29.355, Speaker A: After I've walked my own queue, then I'm going to check the reactor, right? So this is just like trying to make progress on the reactor. And then three is basically go to one. Now where this gets tricky is imagine that. Sorry. And the connection to spawn is that whenever I spawn a new future here, it just gets placed on a random worker. Random. This is a relatively recent change.
01:57:29.355 - 01:57:45.651, Speaker A: It used to. Used to go to the same thread as you're spawning from, but it basically goes to a random worker every time you spawn. So I spawn this future, it gets placed, say on worker two. I guess we can number these. This is one. Two. That's not.
01:57:45.651 - 01:58:03.515, Speaker A: That's a three. Three, four. So every time we spawn a future, it gets placed on a random worker. And then when what this might lead to is imagine that we get really unlucky. Imagine that. Ooh, eraser. Great.
01:58:03.515 - 01:58:39.405, Speaker A: Time to erase some things. This guy and this guy, just like they finished all their futures, all of them finished relatively quickly. And so they have nothing to do. And so these have nothing to do. Meanwhile, this guy has lots of futures and is super busy. And this has like some. Not terribly many, but a few.
01:58:39.405 - 01:59:22.421, Speaker A: This seems problematic, right? Because it means that this thread is going to be really busy because they have all of this work to do, right? So they're really quite sad. So this cpu, whatever CPU is running, this worker is just like spinning at 100%. Meanwhile we have these two CPUs over here that are idle. That seems stupid. And so this is why, if you recall back from here, it talks about how was it? This is a work stealing based thread pool. So what work stealing is, is this thread, when it goes in, pick another color. Let's pick, I guess that color.
01:59:22.421 - 02:00:01.219, Speaker A: No, this color. This color. So when this thread is on step one, oh, it sees queue is empty. Well, that seems silly. So instead of just sort of looping on its own queue and waiting until it gets something random, right? So it could get one of these, but that might take a really long time. There might not be anything more coming. And so instead what it's going to do is when it detects this queue is empty, then it's going to do like a special 1.3,
02:00:01.219 - 02:00:43.385, Speaker A: which is try to steal. The way that works is it's going to look at the other. So in this step it's going to look over here at this worker and go, ha. You have a ton of things in your queue that you're not doing anything with. How about I help you out? And it's going to steal some of the futures and pull them back to itself and put it basically, sorry, I guess this should go here. And it's going to stick those into its own queue and then it's going to start working on them. And so now this work stealing means that we are.
02:00:43.385 - 02:01:43.397, Speaker A: This work stealing means that now we're spreading the load. So if one thing is more busy than the others, then that load is now sort of moved. Of course now we're in a slightly weird position where some of the futures that we stole might depend on wake ups from here, right? So some of the futures in here waiting for notify to be issued from this thread. So there's an additional step here. What if you steal something and then it has to wait for I.O. or for a timer, then it gets put back sort of onto this queue. But it does mean that this thread, thread three gets to help thread out one out, right? So the futures that it steals, it does a bunch of compute on it pulls on all of them and then once they become not ready, it sort of returns them, but it returns them as not ready.
02:01:43.397 - 02:02:12.003, Speaker A: So it has still done some work for one. And similarly four is going to do a similar kind of work. Stealing will probably also steal from one. And so now we're spreading the load out and then we're still making sure that things end up sort of back where they were when things are settling down. And this is important because you could imagine that if thread one had to keep waking up thread three all the time and. Or sort of notified thread three all the time, you get a lot of communication, which is sad. So we.
02:02:12.003 - 02:03:31.185, Speaker A: This is why we return the future to one. But this is the basic idea of a work stealing thread pool, right? So in this setting you basically have a bunch of threads that are all independently an executor and has all the stuff that they need, but they sort of cooperate to make sure that all of the workload gets spread out nicely. Does that set up make sense or do you want me to talk more about these pools? I know this is very random, but I started learning Python today. I will not spend time on the stream talking about Python bugs because it's already pretty packed with things, but you should ask on like a Python IRC channel or something. So please don't take that here. But does this setup of futures and thread pools roughly make sense or do you want me to talk more about it before we move on? Makes sense to me. Great.
02:03:31.185 - 02:03:46.935, Speaker A: I have one data point. I know that this is. There's a lot of complication to this. Like there's a lot of. I like pools. Nice. I know there are a lot of weird interacting components here and it can be hard to follow the flow.
02:03:46.935 - 02:04:20.895, Speaker A: I think going back to look at this, look at the recording after might help and sort of go through it at your own pace and try to work it through in your head. Also the. If you read, I assume it picks the queue with the most extra work, sort of. So you don't actually. So in this work stealing setup, if whenever a thread tries to steal, if it has to check with every other thread, that's also pretty slow. Imagine you have like a machine with 40 cores. You don't really want to have to talk to all the 40 cores to figure out which one to steal work from.
02:04:20.895 - 02:05:04.595, Speaker A: So usually there's sort of an algorithm of you walk until you find some that seem to have a bunch of work and then you take it. So you do it somewhat eagerly so you don't end up interrupting everyone else. The only thing that might be worth mentioning is how blocking plays into this. I'll maybe get into blocking later. I will point out the issue though, which is imagine, and this is not just specific to pools, but, but even for executors in general, imagine that you have some future that is doing a lot of work. Like you call poll and then it like computes all the digits of PI. So it just like takes forever to return.
02:05:04.595 - 02:05:31.371, Speaker A: In fact, actually literally forever to return. Then now you're not going to make progress on any other futures. You're not going to check your timers, you're not going to check your reactor. And so it's just going to get stuck. That is problematic even in the, even in the stealing world. Imagine that here this thread starts computing PI. Then now it's not even going to wake up the stolen futures.
02:05:31.371 - 02:05:56.975, Speaker A: So it might be, it might have given some stuff away that. That's not going to make progress either. And you've basically now sort of lost the core and anything that's stuck. So let's do like yellow. Imagine that there's some futures down here that no one ends up stealing because no one is. Everyone else has their own work to do. These futures down here, no one is going to steal.
02:05:56.975 - 02:07:07.473, Speaker A: But this thread is stuck just computing PI. Now these futures we're just never going to do anything with, even if all the other threads are sitting idle, right? They're not going to steal it because they see this Q is pretty short. Now there might be ways around this, but you could see how if a future takes a really long time, then this is problematic. And what Brian is mentioning here is there is this. There's a mechanism in Tokyo that's relatively new where you can call Tokyo blocking, or technically it's in Tokyo executor, but blocking, which is you sort of mark the current pool thread as hey, I'm doing a bunch of work so steal everything I have. It basically sort of lets go of its own queue and says everyone else deal with it. So that I wanted to mention, there's another thing to mention, which is I mentioned how futures get returned to their original queue because otherwise they would have to get wake ups.
02:07:07.473 - 02:07:49.755, Speaker A: From this other thread there's a change that we want to make in Tokyo but haven't yet, which is how about instead of doing that wrong color, I'm running out of colors. Green. Green is not ideal for this. But let's use green fine here. No, not green, pink. What if instead of doing that we just sort of move the part of the reactor that's related to the futures we stole over here? That way we don't have to return the futures, we can just keep them here. It turns out that this, this pink stuff we don't really know how to do yet because it means that you have to move stuff from.
02:07:49.755 - 02:08:25.633, Speaker A: We have sort of E poll running in thread one and we have E Poll in thread three. And it means that in this set to E poll and this set to E Poll, we have to take like the part of this E poll that is related to the thing we stole and we have to move it to this one. And Tokyo doesn't have a way to do this yet, but something we're looking into how to add. Can you repeat why it's returning the future of it stole the future. Right. So that's a little bit what I got into here. But the reason is because otherwise we would have to do this.
02:08:25.633 - 02:09:07.271, Speaker A: So thread 3 steals a future from thread 1 then now thread 1 because it still has the resource, the thing that notifies that future. Then now three can't make progress without one telling it what to do. And then you have to sort of bounce back and forth between the two with these notifies. Whereas instead what we're going to do is three is going to do the compute part, so the poll and then it's going to return it to one so that one can just do the wakeups locally instead. I'd also like to learn more about Tokyo blocking. Yes, I think I got roughly into the idea of blocking. I don't want to go too much into detail in part because the design there is still evolving.
02:09:07.271 - 02:09:32.001, Speaker A: Like blocking doesn't work for current thread runtimes at the moment. Does reactor run in the same thread? Yes. Yes. So the reactor here is a reactor. So every thread in the pool also has a reactor. Remember, the reactor doesn't do reads or writes. Right.
02:09:32.001 - 02:10:23.805, Speaker A: Those are done in poll. All they do is E poll and notify up until. I mean, I can find out. Chocolate. Up until this landed in October, it used to be. So before this awesome PR landed, it used to be that Tokyo actually started. Sure, we'll keep the same color started like an extra thread that just had a reactor.
02:10:23.805 - 02:11:08.191, Speaker A: And then there was in that version of the world, lime green. This is again, this is not what Tokyo does anymore after that pr. But it used to like not have these, not have reactors in every thread. Instead it had a single thread that was the reactor for all the futures that would then sort of do all these wake ups. And this works relatively well if you don't have a lot of things to do. But what you run into is if you have lots and lots of connections, then now it might be your bottleneck by just waking them up. Like there's so much work to do that this thread is just really busy waking everyone up.
02:11:08.191 - 02:11:29.647, Speaker A: But everyone can't make progress until they're woken up. And so this becomes a major bottleneck. And so that's also why a while ago I wrote, and some of you may have seen it, this thing called Tokyo I.O. pool. And Tokyo I.O. pool was essentially this design of having a reactor per executor. But I didn't have any work ceiling.
02:11:29.647 - 02:12:11.439, Speaker A: I. I just spun up a bunch of current thread runtimes and they all just ran individually and the spawning was random. And then later Stephan came in and implemented this PR which changes the design of Tokyo to the one we talked about where there's work stealing and there's a reactor per pool. And that brings Tokyo pretty close to Tokyo IO pool because now you have many reactors. And then they also made this change which is being able to spawn tax onto random workers as opposed to a single one. And now you see that thread pool. So Tokyo IO pool takes about 34.
02:12:11.439 - 02:12:44.145, Speaker A: What is this? Milliseconds micros millis. 34 millis to wake up. And with this PR, Tokyo proper takes 46. So there's a lot of work in Tokyo to try to make this fast. And the design they've basically the only thing that's really missing now is the ability to do this move right? Of moving the. When you steal a future, also steal the part of its reactor that's relevant. Great.
02:12:44.145 - 02:13:32.045, Speaker A: Okay, so I think that's most of what I wanted to say about Tokyo. There's maybe one thing missing, which is a timer per. Yeah, so there's a timer per pool workers. So they have both a reactor and a timer. There's one more thing I wanted to say about Tokyo, which is I've sort of lied to you in that I've said that everything is E Poll. Whereas in reality E Poll is just a thing you use on Linux. So E Poll is Linux specific.
02:13:32.045 - 02:14:19.335, Speaker A: In reality, what Tokyo does is it uses a crate called myo. So MYO is a crate that's written by the same guy who maintains Tokyo. And it's basically you can think of it as an abstraction for doing the kind of things that E Poll does. Right? So it's a low level library for non blocking APIs and event notifications just on any operating system. It gives an interface that basically gives you a way to say I want to wait for all of these resources, these IO resources. Okay, so let's now close the book on Tokyo. I think that gives you an idea of how Tokyo works.
02:14:19.335 - 02:14:58.615, Speaker A: And there's really no more magic to it than that. There are a lot of optimizations internally, but now you understand everything that Tokyo does There's nothing more to it. In fact, if you look at the things that Tokyo gives out, clock is a way to deal with time that's not related to systems time. So it makes testing easier. Codec is probably going to be removed, but it's basically a way to. If you have something that is read and write or async. So we can talk about async read and write actually because they're a little bit interesting maybe.
02:14:58.615 - 02:15:29.443, Speaker A: So there's these traits called async read and async write. Really what async read and async write are is saying that something is read or write. So a TCP stream or a file or whatever is read or write. But if it's async read. So if you look at async read, it really just is just a trait over read. And all the methods are already implemented for you, so you don't have to add any methods. All it is is read with the additional contract that it's non blocking.
02:15:29.443 - 02:15:51.669, Speaker A: That if you try to do a read it will never block, it will just return wood block. And async writes is the same. So async readiness write are very straightforward. And then codec is just a way to map async read and async write into sync and stream. I guess we can mention sync and stream now. So these are not Tokyo specific concepts. They're just related to futures in general.
02:15:51.669 - 02:16:08.605, Speaker A: So a stream is. There's a lot of stuff here. Let's get rid of all of it. Great. A stream. Actually we don't even need to draw, just open it. I don't want that, I want stream.
02:16:08.605 - 02:16:29.307, Speaker A: A stream is very similar to a future. It has an item and an error. And it also has a poll method. It also uses the pull type. It also has item and error. But notice that as opposed to the future, as opposed to futures. So future, if you recall has just returns an item.
02:16:29.307 - 02:16:57.783, Speaker A: This returns an option item. And also the contract of poll is you're allowed to call poll after it returns ready poll on future. If it returns ready, you shouldn't call poll again because the thing is already finished on stream. If you call poll, it's going to give you either some or none. If it returns sum and some item, then think of this as an iterator. Then that means the stream gave you another thing. If it returns not ready, it means I'm not ready to give you another thing yet.
02:16:57.783 - 02:17:16.961, Speaker A: And if it returns none, it means I'm finished. Just like an iterator. This is like the next Method of an iterator. And the contract here is that if Pol returns ready none, then you shouldn't poll anymore because there's nothing more. It's sort of finished. Tangential question. How does someone profile Rust code to figure out what this.
02:17:16.961 - 02:17:55.327, Speaker A: That the single executor was the bottleneck with many connections. The single reactor, you mean? So this was actually a part of my research where I'm writing really high performance database. And so we have lots of connections and we notice that as you increase the number of clients, the throughput was not increasing with the number of clients, even though we know that the underlying database does scale. And so clearly there was some bottleneck. And then what you do is you just run top or H top or something. And what you see is there's a single thread that's at 100% CPU and the other threads are not busy. And then you try to figure out what is that thread.
02:17:55.327 - 02:18:30.535, Speaker A: And that thread was the reactor thread. So it's called like I.O. tokyo I.O. or something. And so that led me down, basically led me down to start talking to the Tokyo team and be like, hey, what's going on here? Why is there one thread and the other threads are not doing work? And in fact, you can look at the implementation of Tokyo IO Pool because it is really straightforward. Tokyo IO. So the implementation of Tokyo IO Pool is I think, a single file, because I'm a terrible programmer.
02:18:30.535 - 02:19:12.065, Speaker A: Single file, great. It's mostly documentation and like builders and whatnot. And so the build method that gives you a runtime is really, really stupid. It creates a channel, it spawns for each, for as many workers as you want, it spawns a thread. That thread is a current thread runtime. And it all it really does is it receives on a channel. And everything it receives, it spawns.
02:19:12.065 - 02:19:46.726, Speaker A: Sorry, let me rephrase. It spins up a current thread runtime, it sends a handle to that runtime back to the thing that's starting all the threads. So it's going to end up with one handle to each current thread runtime. So all of those are returned, those threads are just going to block on a signal to exit. And then a handle is really just a set of handles. So one handle for every current runtime, and it's going to spawn on one of them. That is basically the entire implementation.
02:19:46.726 - 02:20:33.703, Speaker A: Very little magic, right? So stream is basically just a future that you can pull more than once after, when it's ready. And it basically just gives you an iterator and then sync is sort of like the inverse. So a Sink is something that you can stuff stuff into. It's like a channel sender, but it's asynchronous. So remember how if you have a. If you have a channel from a sender to a receiver, right? And there's usually if you make an unbounded channel, this will never happen. So let's talk about a bounded one.
02:20:33.703 - 02:21:13.307, Speaker A: So you have a bounded channel and imagine that all of these slots are full, right? So now the sender tries to send another X, but this is full. What will happen in the normal sort of non futures world is that this sender will now block until the receiver takes something off. And now this slot is free again and now this X can go into there and therefore it's no longer blocking. And so the blocking world, this is all fine. But of course in futures we're not allowed to block. Like if you, if in poll you block, it's basically the same as if you ran forever. You really don't want to block.
02:21:13.307 - 02:21:50.675, Speaker A: And so the sender and the receiver, if you have this setting, then a sync. Anything that implements sync has a start send. I'm going to not talk about start send because it's annoying. Instead I'm just going to say this instead. There's a send that takes a T, or in this case I guess an X and it returns like polready. That's. This is an angle bracket.
02:21:50.675 - 02:22:30.391, Speaker A: What am I doing? This type is not helpful. Sorry. There's a poll async. Sure, async of nothing. And the error I think is nothing as well. And so the idea is that you try to send this X and it will tell you whether it succeeded or not, but it will never block. So if this is full, it's just going to say not ready and then notify the task whenever the receiver has taken something off.
02:22:30.391 - 02:23:31.443, Speaker A: So your send will later succeed. Remember, we always have to satisfy the contract of not ready means you've arranged for yourself to be woken up. So if I try to send and it's currently full, I put sort of a marker here so that the receiver, when they remove this thing they notice the marker and then they notify my task so that the next time I basically I'm woken up again, asked to send again. And then the definition here is really async sync of T which is either ready contains nothing or not ready which returns the T. So the idea here is that if you try to send and it fails, it gives you back the thing you tried to send so you can try again later. So here if we look at Sync, you'll notice that Sync has also an item and error. They're prefixed for uninteresting reasons.
02:23:31.443 - 02:23:55.445, Speaker A: They've split send into Start, Send and pull complete so that you can send things in batches. Like I can start send three X's and then I can wait for all three to complete. But the basic interface is you pull. This is where you get back the thing and you pull and it tells you when the. When the send is completed. So that's Stream and Sync. Sync is a little weird because we don't have.
02:23:55.445 - 02:24:39.405, Speaker A: Like Stream is basically the same as Iterator, but Sync doesn't have a good parallel. It's basically just a channel sender that is asynchronous. Okay, so that's Futures, Tokyo Sync and Stream. So now we go into rust. Going forward with this in particular, there is this RFC that generated a lot of discussion. I'm not going to go too much into the discussion because it's not relevant to what we're going to talk about. But this proposes moving everything related to futures from the futures crate into the standard library.
02:24:39.405 - 02:25:15.265, Speaker A: So that is task and future. And if we look at the Render rfc, it basically proposes this is a worthwhile RFC to read, but it proposes adding poll, which we've looked at. It's just ready or not ready Wake, which is like notify. It's basically just a thing that allows you to know. I don't know why they called it wake instead of notify. It's not important, but it's. If you have a wake, then you can wake something up and say, hey.
02:25:15.265 - 02:25:38.341, Speaker A: It's basically the same as notify. Wake means you might be able to make progress now. So the contract now is if you return pending, the function must also ensure that the current task is scheduled to be awoken when progress can be made. So it's the same contract. Just substitute notify for wake. That's fine. That's an example.
02:25:38.341 - 02:26:10.819, Speaker A: That's an example. And the future trait. So let's I guess contrast this with the one in futures. Future. Okay, so this is the trait that we have in futures, right? It has item error and this method for poll. The proposed thing to standardize is there's an output type, but notice there's not a distinction between item and error. There's just.
02:26:10.819 - 02:26:38.671, Speaker A: I guess I can zoom in here a little. Sorry about that. There's a future type. It has an output associated type and output does not necessarily need to have an error. It's basically the observation here that and the reason for this is there are some futures that just cannot error. Like if you wait for a timeout, then there's no meaningful error. Or if you compute a hash, then like eventually the hash is going to be ready.
02:26:38.671 - 02:27:20.373, Speaker A: Not a matter of erroring. And so the argument here is you're going to have some value that you get eventually. And of course there's a trivial mapping between future, between the futures future and the standard library future, which is just to say output is result of your okay and your error type. Whereas this forces you into in many cases to declare sort of a dummy error type. This does not do that. Instead it just says if your thing can fail, then your output is going to be a result the poll method. So that's why there's only one associated trait.
02:27:20.373 - 02:27:42.665, Speaker A: And then of course the only other thing on future is poll. Poll in the standard library version of future looks fairly different. But we're going to talk to what that means. So let's look at the return type first. The return type is poll of the output. So here it was a poll of item and error, but because we got rid of error, it is now just poll output. So that seems pretty nice.
02:27:42.665 - 02:28:50.481, Speaker A: You're given a local waker. Local waker is basically the same as what you would get from task current, right? So in futures, remember how if you ever want to be woken up again, you have to call task current to get a handle to yourself to like a thing that lets you wake up yourself that you can then give away like to the reactor, for example. In the standard library version of futures, that thing for wake up is. Is passed in explicitly. It is not something that you that's like passed magically somehow. Because think about this, it's a little bit magical, right? What we're saying is that poll, if you're in poll, then you can call this magical function and get a task, right? So if we go back to the executor, remember how we had to pull a little bit of a trick to call this with notify function and with notify is really just going to like hide away this. This notifier that we give it somewhere that pull that task current can get at it.
02:28:50.481 - 02:29:39.425, Speaker A: In fact, the way this actually works is it uses thread locals. So Russell says notion and the general programming concept that you can have thread local storage. So some stuff that just the current thread has stashed away somewhere. And with notify just sets a thread local variable that's sort of global to the pro or global to the thread that contains notify. And task current reads that Whereas in the standard library implementation, you just instead say that whenever you call poll, you have to give it a handle to itself or a handle to its waker. And so there's no longer any need for this task current. The thread local stuff and that magic, it does mean that pull is a little bit more verbose, but it also means there's less magic.
02:29:39.425 - 02:30:17.855, Speaker A: It also means you don't need thread locals, which can be a little bit of a pain to implement in certain execution environments. But this is also generally a good change that. It maps very directly to what we had in the old futures, just with slightly different mechanics, but the underlying principles are the same. And then we get to self. So in the old futures, bring us back to that poll, just take mute self. All well and good. In the standard library futures it takes pin mutes.
02:30:17.855 - 02:31:00.653, Speaker A: I'm trying to see what the right way to introduce this is. Sorry, before we get to that, are there any questions of everything leading up to this? Because this is about to be another sort of break to something that is relatively different. So if you have any questions about all the stuff we've talked about, sort of from the middle of Tokyo to here, now's the time. You can ask them later too. But now's a natural sort of break point. Or about local waker or. I don't remember.
02:31:00.653 - 02:31:40.699, Speaker A: Self can be specified with a type. Oh yeah. So that's actually not a feature that's specific to this, I.e. rust arbitrary self types. This is an RFC that landed a while ago, but it hasn't been stabilized at all yet. Doesn't have an rfc. Yeah, so the basic idea is that you can have self be not just directly a mute self or ref self or self.
02:31:40.699 - 02:32:15.217, Speaker A: You can also declare self to be something that depends on the self type. And this would be. You can only call this method if you have an RC of self. So if you just have a ref self, you could not call this method. This basically declares that I am callable on an an item of this type, but only if it is in the following form form. And so that's what this is using. It's saying that you can only call poll on self or on an item of this type if the type you have is a pin mute self.
02:32:15.217 - 02:32:42.675, Speaker A: Otherwise you cannot call poll. Yeah, it's really neat. I think it's currently behind feature, it's implemented and everything. There's some question about trait objects that are still being resolved, but it's basically you add this feature and then you can do it. I don't think you should generally need it. But does it work with anything generic over Self? Yeah, I think the proposal is. I'm not entirely sure.
02:32:42.675 - 02:33:11.221, Speaker A: So this is one of the things are still working out and why it hasn't been stabilized. But I think it's. The requirement is anything that de refs into Self. It might even say here somewhere. Right, so that's the original thing. Yeah. Basically.
02:33:11.221 - 02:33:59.585, Speaker A: There's a lot of questions around object safety and trade objects that we're not going to get into here, but I wanted to see if I could find. No, but yeah, the basic idea is that you want something that derefs the self, whether that's D or D mute or sort of D ref owned, but it has to be able to be turned into self. Okay. So it doesn't look like there are any particular questions about Tokyo or Futures stuff leading up to this. So let's look at pin. Pin is there for Async Await. I'm trying to think whether we should deal with Async Await first or talk about Pin first.
02:33:59.585 - 02:34:39.035, Speaker A: I think we need to talk about Async Await first. So let's do that. Async Await is a feature that a lot of people have been anticipating for a long time for rust. And it gives us two things that are kind of neat. So here's what it gives us. Imagine that you're actually. We can do it with our example from up here.
02:34:39.035 - 02:35:13.583, Speaker A: So writing this for Futex is fine, right? Like it's. But there is a bunch of noise here. We have to write all this and then stuff. And furthermore, it's a little awkward because Imagine that we have something like a buffer that contains foobar and. Or in fact, we like read it from the. From the user's input or something. I don't have.
02:35:13.583 - 02:35:38.435, Speaker A: This is currently a static string, which is annoying, but imagine that this is like. This is really a string. Sure. That's not what I meant. And then what we want to write is really what's in buff. You can't really do this because the future you get back is going to be tied to this lifetime. But that future we're going to like give to Tokyo Run.
02:35:38.435 - 02:36:10.225, Speaker A: No, I haven't really introduced it yet. If we're going to take this future now and sort of give it away to a thread pool to operate on. But this, I guess. Let's imagine this also uses Buff. Or like we want to keep using Buff down here, right? Like I'm gonna do something like Tokyo Spawn, Few decks or. Sure, Spawn. It's not important.
02:36:10.225 - 02:36:50.213, Speaker A: I Like send off few decks to be executed somewhere. But then I also, after that actually, sure, run. I'm gonna do this, right? So I'm gonna. I'm really just giving a reference to buff to this future. And then when it finishes I want to do something with buff or you can imagine that I have buff and I want to use it in multiple parts of this future execution. It gets pretty annoying. In fact, you can't really do it very nicely.
02:36:50.213 - 02:37:24.755, Speaker A: And even if you ignore those kind of things, like this is a weird way to write this code. Like I have to do all this like chaining of and thens. So async await tries to make all of that better by introducing two new keywords to the language. Well, it's called the keywords. So it introduces one new keyword which is async and a macro called await. And what that lets you do is it allows you to write code the following. The stuff above, so few decks, we can write it.
02:37:24.755 - 02:38:27.725, Speaker A: In this case, this is an async block. You can also mark functions that async and closures as async. So we declare, declare this as an asynchronous block. And inside of this we can say await await TCP stream connect and then we want to await C write. Actually, let's just not use this for now. It's not terribly important. Just to demonstrate that this is annoying, we're going to write foobar and then we're going to await C read I guess B and then we're going to check if B is barfu.
02:38:27.725 - 02:39:06.215, Speaker A: So async await lets us write the code that's above this way. So notice that now we've taken this sort of chaining and callback based thing and written as linear code. Now there's some discussion about a weight being becoming like a keyword. So you can do something like this makes it a little bit easier to read maybe. But for now it's just a macro so that we can iterate on the design. And this is pretty cool in particular because now this code is just a lot easier to read. Like it reads like the corresponding synchronous code.
02:39:06.215 - 02:40:15.089, Speaker A: It's also really handy for writing functions. So whereas before I would have to sort of write an FN like check fubar, right? That returns either. I could say that it returns like an impulse future and then I could do all of this like and then business that we did above. Or I could write a check foobar that is like a foobar check future and then here and then I declare some like Enum foobar check future and it can be either like connecting, in which case it has a TCP stream connect future. Or it can be like writing which has a write future, or it can be reading which has a read future. And then. Or it can be say that and then I have to like impulse future for this foobar, check future.
02:40:15.089 - 02:40:54.311, Speaker A: And all of that just gets such a pain, right? Impulfuture helped a lot as we can see, but it still requires you to write all this. And then business and it's still really annoying. In this new world, I can just say fn async check fubar ask an impulse future. And then I just write this code with a wait and it will just work. And notice how similar this is to the synchronous code. And that is basically the goal. But you might wonder, well, how is this actually going to work? Like, what does this turn into? Ultimately, this code sort of has to turn into a future somehow.
02:40:54.311 - 02:41:36.255, Speaker A: So what does it actually do? Well, async. What async does is basically construct a type for you that is an enum. Sorry, let me rephrase that a little bit. Async constructs a type for you that is a future and it will run from the previous await point to the next one every time it can make progress. So in this case, when you initially create this block, no codes gets executed. It just. Just a future.
02:41:36.255 - 02:42:32.835, Speaker A: Then the fr first time this async block, the return future, gets polled, it's going to start running from the start of the block until it hits the first await. That first await, of course contains a future or is given a future as an argument and it's going to pull that future. If that future is ready, then it keeps executing until the next await. If that future is not ready, then it's going to keep track of where it got to. So the first time this gets executed, like first time it's going to be there. The second time it's going to call TCPSTREAM connect and then it's going to notice that when we pull them on the connect, it's not ready yet because it just initially started connecting. So the second time we're sort of stuck here.
02:42:32.835 - 02:43:16.085, Speaker A: Then we're going to poll again. Poll on async future resumes here and it sort of turns this into a loop. So await. If we look at it, await basically sort of kind of desugars into this ignore the pin of future stuff for now. It basically desugars into a loop that pulls on the asynchronous future. So down here, this is sort of, kind of going to destructure into this. If it's ready, then we sort of.
02:43:16.085 - 02:43:46.723, Speaker A: Then we sort of yield X. So we return that out of the loop and if it's not ready, all right, fine, let's do it the other way around. Then we break with X out of the loop and if it's not ready, we yield. So think of this as we sort of store how far we got and then we. And then we return from the future saying not ready the next time the async block. So imagine there's a. Imagine that.
02:43:46.723 - 02:44:23.165, Speaker A: Oh, this is going to be easier. Sorry, just for exposition, imagine we finished the connect and then this. We're like now stuck on the. Stuck on the read. Sure, let's say we're stuck on the read. So we finished the two things above and we're now on this part. So this is going to desugar into something that's kind of a little bit sort of like this match read.
02:44:23.165 - 02:45:02.425, Speaker A: If it is okay Async ready X, then we're going to break with X. We're going to break with okay X. I guess now there's no Rakex. If it's Async not ready, then we're going to yield and that's what we're going to do. So that's basically what the await kind of sort of sugars into. Now there are a couple of things that are interesting about this. Notice that there's a bunch of code above this loop.
02:45:02.425 - 02:45:43.157, Speaker A: So when we hear if we sort of think that a return is not ready, think of it sort of kind of like that. It's like the future that we get back from async. If we get to this point, then we're going to return not ready. But it can't really be not ready, right? Because if we return not ready. Not really. If we return not ready, then the next time this async block would be pulled, it would execute from the top of the scope, which means it would re execute these pieces of code, which is clearly not okay. Right? We have already executed that.
02:45:43.157 - 02:46:21.835, Speaker A: So this is why it sort of needs to be the special kind of yield operation. Right? It can't really be a return because that would imply we're issuing all these instead. We're going to yield and yield. We're going to declare to be sort of a special operation. That means remember where I returned from and when I get called again, then continue from here. Continue from here on poll or on reentry. Right? So when I reenter this Block when poll is called again, then continue from here as opposed to from the top.
02:46:21.835 - 02:46:58.175, Speaker A: And what this means is if we continue from here, then of course, so we get notified of some kind, a poll gets called again, we continue from here. This is what's known as continuations, basically. Then we're going to sort of follow the bottom, follow the control loop. So we come to the loop, we do the loop again and now we do see read. And now it may or may not be ready, but at least it means we didn't re execute the stuff that's up here, right, which is what we wanted. And then of course eventually it's going to return ready. Then we give back the X and now B is that value that was ready and we can continue down here.
02:46:58.175 - 02:47:55.595, Speaker A: So that's the basic idea of Async Await, that we need this ability to continue from where we, where we stopped. But if you look at it, that is really quite complicated because first of all we need to note where we returned from. But more importantly, when we continue with where does C come from? We've returned, right? So has C just gone away? C we got from here, but we somehow need C in order to continue from here. We don't get to re execute this code. And this is where Async await gets tricky because C here is sort of like where is C even stored, right? It's sort of stored on the stack, but. But the stack is going to go away when we return. So where is C stored? And so what Async really does is Async basically turns into this pattern down here.
02:47:55.595 - 02:49:00.809, Speaker A: Probably going to do this, but mention the word generator. So I don't really want to mention the word generator because it isn't really a generator, it sort of is a generator. So for those of you coming from JavaScript and Python, it sort of is like a generator in that a generator also has to continue, but it's also not quite a generator because it's asynchronous and so it needs to deal with things like wake ups. But it is continuation passing in the same style as generators. There is in fact in the document or in the RFC for Async Await, there is in fact down here somewhere Async based generators like here, that async function should be the syntax for creating generators. That's not what they've chosen to do, but you're right, the mechanisms are pretty much the same. So sorry.
02:49:00.809 - 02:50:17.515, Speaker A: So what Async really does is it uses a pattern similar to what we sort of started writing out here for this very verbose check foo bar. Except the compiler is going to generate it for you. So when it looks at this code, it's going to generate an enum like compiler made async block with like some unique identifier that is unknown only to the compiler, right? And so this is not a type that you will ever see, right? The async block is really just going to return an impulse future, right? But in reality this is really compiler made async block, whatever. And the compiler is going to generate an impulse Future for if future 4 this compiler made async block, I guess I don't actually need the unique identifier. It's going to be confusing, but the compiler is going to generate all this. And so you will never see this type. It'll be an entirely private type to the compiler compiler, but it's going to generate this enum.
02:50:17.515 - 02:50:49.585, Speaker A: And what this enum is going to be is sort of like, think of it as like a step zero. So step zero is before the first time you pull. So there's no state. Sort of. There's no state except whatever the async block may have captured from its environment, right? So if there's a bar here and here we like do something so that we pull bar in. Then step zero would also contain a vec. See, this is A.
02:50:49.585 - 02:52:09.333, Speaker A: Then step 0 also contains a vec bool. So it's like everything that's needed for the before you even start that you capture from your environment. And then for every await it's going to generate a new step. So there's going to be a step one. And step one in our case is the await on TCPSTREAM connect, right? And inside of each variant it's going to store all the stuff that it has made so far. In our case, that would I guess be Z at this point and it would also be the connect future, right? It's going to have some type because the first time we call connect we're going to get back a future that we're going to have to keep polling, right? Connect gives you back a future and here we're saying await that future. So we're going to have to store that future somewhere and that's going to be here and that's going to be, if we're stuck on step one, we're going to keep pulling the sort of, I guess this is really going to be something like waiting on.
02:52:09.333 - 02:53:01.427, Speaker A: And anytime you pull on a step you're really pulling on whatever that waiting on is. So this is going to be Some info future and the output is going to be something that step two needs, right? It's not really going to be a step two, but it's like needs this, right? You can think of it as a step two here. Step two is going to be. Well it's still going to have the Z. So let's just like say that all these have Z ignored from now because it's not used. It is however going to keep the C which is going to be the TCP stream because at this point we do have the C, right? Because step one was a future that we've finished polling on. Well that eventually gave us the TCP stream.
02:53:01.427 - 02:54:05.995, Speaker A: And then in step two when we get to this await point now we already have the TCP stream because this poll eventually returned ready. So this TCP stream we now have and then what this is going to store is another sort of waiting on and that's going to be an impulse future whose output is going to be like you size. So this, this is future returned. The future that's returned from C dot, right? But we have to be a little bit careful here because we might start to wonder what's going on here because C dot write takes a mutable reference to C, right? This implefuture is going to sort of not quite consume C, but it needs C. And so it's sort of tied to the lifetime of C. But C is stored here. So this internally contains a reference to C, but C is stored in ourselves.
02:54:05.995 - 02:55:10.545, Speaker A: And for those of you who sort of have dealt a little bit with this, this is known as a self referential data type. So imagine I have some type foo and inside I have some data and data is say a vacuate. Then I also want to do store like a self ref which is going to be a slice, it's going to be last two or something. And half is going to be a slice into data, right? So half is a pointer into ourselves. I guess a better example of this actually for reasons they'll become clear later. So I have a data that's like a buffer that's stored inside of foo and half is going to point to half of that data. But this is really weird because imagine that we have some code that looks like F is foo.
02:55:10.545 - 02:56:01.607, Speaker A: How do I even make this right? So this is going to be something like 00:24. How do I set this? How do I set that type? I don't know how to set it right, I don't have a reference to foo yet. And let's say that imagine that Somehow I managed to create it in the first place. Now I do say Z, it's going to be box new F, so I'm moving F. So now instead of F being on the stack, F is now on the heap. But now F half, or rather Z half is still pointing to the stack. It's still pointing to where F was.
02:56:01.607 - 02:56:39.525, Speaker A: Because I haven't in this move, I haven't updated half. So half is now a pointer into somewhere that I don't control anymore, somewhere where the data really isn't. Imagine sort of the example, the trivial example of this. Here I change Z data 0 to be like false or I guess 1, the Z half still points to F which has been dropped. So that's not okay. But even it was, even if somehow that reference hadn't gone away, this value is now like wouldn't see one for sure. In fact it would probably just see garbage or something that's been reused.
02:56:39.525 - 02:57:30.035, Speaker A: But we need to be able to express this kind of type in order to have async await, because this future we're waiting on is using the stream. Okay, so clearly there's just something very weird going on here. We need to have a way to express this. Now there have been lots and lots of proposals for adding self referential data types to rust, and it turns out it's really quite hard. Of course one way you could do this is you could say that half is going to be a, a pointer, a raw pointer that we're going to manage ourselves. But we can't do that if the user is allowed to just like randomly move foo around. We have no way of guaranteeing that that pointer is always correct.
02:57:30.035 - 02:58:48.375, Speaker A: And so really this problem turns into one of can we guarantee somehow that foo does not move once we set half? Right? So at some point we're going to set half, we're going to set the value of half to point into foo, right? Imagine there's some like initialization stuff, like there's a foo init function or something, and that's going to like create data and it's going to create the pointer of half into data. And of course there's no way for user code to deal with the state in between. So the user code will only ever see sort of the half pointer being set correctly. But if the user ever moves foo, or specifically if data ever changes memory location, then now half would be no longer be correct. And so in addition to there having been proposals that have been sort of how do we create a self referential data type? It is sort of Reduced to How can we express that something isn't allowed to move? So there have been things like impul not move for foo is something we've seen. There have been a lot of proposals for this, but none that have really worked all that well. And this has been a major problem for how do you express Async Await? Because we need it for Async Await.
02:58:48.375 - 02:59:32.225, Speaker A: And finally a while ago Without Boats and Eddie B and some other people. I think Without Boats was sort of the first, first one to realize this. I think he links to the. I think they link to this somewhere here to the blog post they wrote Endeared without. Without Boars. That's new. Without Boats.
02:59:32.225 - 02:59:54.915, Speaker A: I guess it's not in my history. That's weird. Yeah, it's on a different URL so it doesn't really help me. Sorry. It's on a different computer so I can't even get to that here. Blog. Great.
02:59:54.915 - 03:00:41.945, Speaker A: I'm confused. Ah, there we go. So down here somewhere what they basically did was spent a bunch of time trying to think how can we do. How can we do Async Await? Right. So this starts out by talking about separate self referential structs then sort of talks through well what do we actually need? How might we get it? And then here they basically realize that we have all the parts we need in the type system. We need to just need some very small proposals. And this was back in January and this was basically the breakthrough that we needed for how do we express this in Rust.
03:00:41.945 - 03:01:04.113, Speaker A: And then there are a bunch of blog posts since on how to polish that API. In fact there was one very recently on. This one. No, this. This one? No, that's on. Well it was one of these. All of.
03:01:04.113 - 03:01:43.065, Speaker A: Anything here that mentions pinning is probably worth reading if you want to understand it. The observation was that we can get sort of support for this. Basically the thing we want to express with Async Await that something can't move once we've started, once we've. Once we have it be self referential using the notion of pinning and so pinning. Can you link to the voting site you made last time? Ah, sorry. Yes, this do I have. I probably do there.
03:01:43.065 - 03:02:31.187, Speaker A: Sorry. So the pinning API is what they came up with in the end and pinning has a lot of very sneaky details. There's a lot of intricate contracts that are being maintained. But I'll do my best to try to explain what's going on. So pinning introduces two new types. There is a pin. PIN type P where P is sort of a Pointer type.
03:02:31.187 - 03:02:53.125, Speaker A: So where P implements D ref. So there's a. I don't want to really express it that way. Struct pin. Great. And then there's a trait called unpin. This is a marker trait, so it's automatically defined for all types by the compiler.
03:02:53.125 - 03:03:26.387, Speaker A: Just like clone. Sorry, just like send and sync. And the basic idea is that this is the first. I really understand the need for pin API and its relation to 8. Oh, I'm glad. Glad to hear it. So where this gets tricky is remember how up here we need to be able to express the fact that once we've set this pointer, then foo will not move, and that is what PIN expresses.
03:03:26.387 - 03:04:08.717, Speaker A: So a pin, if you have a function foo and you get some argument x and that argument is a pin. So P here is any kind of pointer type. So a mutable reference or reference arc rc. Let's say you get a mute T. So take some T what this tells you. So if you get an argument of type pin, there's a contract. So basically, what if you get a pin? Whoever is calling you is promising the following.
03:04:08.717 - 03:04:46.191, Speaker A: They are promising that either T will never move again or T implements something, okay? That is the contract. Now, this does not say anything about T. If it's not. If you're not. If you're just given a T, right? So T here is any type. And so until you have it in a pin, there's nothing special. All this is saying that either T will never move again the moment you give up, the moment you get a pin, right? Of any type.
03:04:46.191 - 03:05:11.785, Speaker A: This is similar to if I gave you, say, a box T, right? Then I'm saying that once you get this, T will never move again or T is unpin. And remember I mentioned that unpin is a. An auto trait. So it's something that every type, unless explicitly. Unless it explicitly opts out of it. So impl. Not unpin.
03:05:11.785 - 03:05:54.435, Speaker A: No, no, no, no. What do I do? Not unpin for my type? Unless you have an impulse like this, then all of your types are going to be unpinned. Now, unpin is a little bit of a weird term that there's still some discussion about what these things should be called. The argument for calling them what they are is that unpinned should be read as a verb. So you should read this as if something is unpinned. It means that it you can. If you're given a pin of it, you can unpin it, and that's okay.
03:05:54.435 - 03:06:34.275, Speaker A: So unpin as in a verb. It's not. Yeah, this is Saying if you have a mytype, you can, and if you have a pin of a thing to a mytype, you cannot unpin it. We'll see how that explains no. So it's the question is, is it auto implemented because it's an empty trait? No. So you can have traits that have no members that are just a contract. Auto traits are special in that the compiler will automatically give every type that trait if all its members are that trait.
03:06:34.275 - 03:07:19.515, Speaker A: This is not true for every marker trait, although that. So that said, I don't know of any empty traits that are not auto traits in the standard library. In other crates you could usually easily use them for markers of varied kinds, but they're not necessarily the same. Yes. So unpin is special, unpin and pin are known, but. Well, sorry, unpin is known to the compiler and it has been marked as so in the standard library you're allowed to write autotrait, which basically tells the compiler this trait should be applied everywhere. You cannot define your own auto traits, at least not currently.
03:07:19.515 - 03:08:44.249, Speaker A: So it's a reserved special trait, it's not a reserved special name, but it's in the standard library and the compiler knows that it's an auto trait, and that's why it has this propagation, this auto propagation. And so the way to think about unpin is in terms of this contract, right? Either T will never move again or T is un pin. And if something is unpin, what that means is that it is if T is unpinned, it is not sensitive to being moved. Sort of, right? So off topic question about generics. Can you specify generic must not implement a trait? Not currently you can only declare that something does not implement a trait. You cannot depend on something not implementing a trait yet because it turns out to be fairly hard for the compiler to do negative reasoning, it would be nice for specialization, for example, but I don't think you can currently do it. So think of our referential data type foo up here.
03:08:44.249 - 03:10:02.019, Speaker A: So foo would not be unpin. Or the other way to think about it is if something does not have any self references, then it doesn't matter if you move it, right? It's not sensitive to being moved. So even if someone pin it and like if you moved it, it's not like your data structure would break, nothing would be wrong if it got moved. And so that's why if I just declare like a bar and that has like a V which is a vector bool and maybe some like buff and maybe it has like a Z that's an arc mutex atomic use that's a terrible type. Arc mutex hashmap topple of usizable string Akuno so complicated type. This type is unpin because it's not sensitive to moving. If you move it, the data structure is just as valid as it was, right? Foo, on the other hand, is sensitive to moving, right? But that doesn't mean that you can't move foo.
03:10:02.019 - 03:10:33.253, Speaker A: It's just that once you rely on this half value, once that's important to you, then you can't move foo, right? If you ever. If you. Once you know that you're going to start using half. So once half is something that you're going to access, then it matters. The foo is not moved, so therefore foo is not going to be unpinned. So here we're going to have in our code unsafe impul. Sorry, it's not unsafe.
03:10:33.253 - 03:10:56.013, Speaker A: Impl. Not unpin for foo. This is saying that foo is sensitive to being moved. And now what that means is. I guess we can bring this a little bit closer. Unpin is not unsafe. It is always okay to declare a type to be not unpin.
03:10:56.013 - 03:11:44.335, Speaker A: You would just. If you do it, you're doing yourself a disservice if it's not important, right? Because nothing. If you say that something can't or is sensitive to being moved, then that just means that people won't be able to move it if it's in a pin. The reverse, on the other hand, is problematic, right? If you have something that is sensitive to being moved, but you don't mark it as such, then that will be problematic. Now the name unpin is a little weird because you end up with this double negation, and that's part of what's going on in the discussion in the stabilization effort right now. We'll see whether or not it changes, but there's some argument for unpin being the right word, but it reads somewhat weirdly, like when you talk about it weird. If you create a custom self referential struct, it should impel, not unpin.
03:11:44.335 - 03:12:15.325, Speaker A: Yes, exactly. I don't think the compiler is going to be smart enough to do this automatically. So it. Because it doesn't know that half our half here points into self, right? Half could be a pointer to somewhere completely different, in which case foo is unpin. So it's only because the semantics of half is that it points into self that it is self referential. And furthermore, unpin only matters in the context of pin. So unpin on its own is not important.
03:12:15.325 - 03:13:02.127, Speaker A: It is only important in the context of pin and we'll see exactly how that all ties together. But imagine that I set half and then I just like never touch half again. Right? So half is pointing into self, but it's like never used. Then of course, obviously foo is still unpin, because even if foo is moved, nothing's going to break on foo. Or for example, imagine that I do some. What's a better example of this? We'll see how this ties into futures later, but I think that's the basic idea. Okay, so why does this all matter? Well, if you're given a T that is unpinned or not unpin, you can move it all.
03:13:02.127 - 03:13:35.645, Speaker A: Fine. Unpin does not restrict moving in any way. However, if you are given a pin of a type, then the contract is that either T. So the target. Let's actually write this out. There's an impldref impl. It's going to be a PTD for pin P where P de T.
03:13:35.645 - 03:14:26.531, Speaker A: I need to make sure I get this implementation right. I think this is right. Yes. Yeah, I guess. Type target is PD ref. Okay, so this draft exists for pin no matter what the type is. Right? Because if I give you a read only reference to the target of P's pointer, then it's still not going to move, right? You're just going to read it.
03:14:26.531 - 03:15:29.445, Speaker A: That's fine. Draft mute is where it gets interesting. So draf mute for pin where P implements drop mute. Sorry, the sorting here is a little bit important for clarity, so I'm going to take a little bit of time and T implements unpick. Yes, like so. Yeah. So notice that pin always implements dref into the target of the pointer.
03:15:29.445 - 03:16:00.153, Speaker A: Right? So that's this thing up here and it does so unconditionally. Right? As long as P implements dref, which is like box implements d reference ARC implements d ref RC implements d ref. Most things implement dref pointers, references. Of course, implement draft DREF mute is where it gets interesting. So imagine that we just implemented DREF and we did not have this restriction. We just said that it also implements D. This is problematic because this gives me a mutable pointer to T.
03:16:00.153 - 03:17:04.979, Speaker A: So what I could write now is the code that we had above up here, right? So I have a. I have a foo. Well, actually I have a. I have a pin box foo. Right? So remember, foo is is not unpin, so it's sensitive to being moved. And I have one of these now because it implements drough mute, what I can do is I can use mem swap or mem replace on the deref of F, right? So derefing F is going to give me the mutable deref of the box, which is foo. So this gives me a mutable pointer to foo, and I can just replace that with some other foo, right? There's nothing stopping me from doing this.
03:17:04.979 - 03:18:07.671, Speaker A: It's totally safe. The problem, of course, is now I just moved foo. So if I previously gave someone this pin, then the guarantee that I gave them, namely I told them the T will never move again if T is not unpin, right? So this sort of implies, the statement down here implies that if T is not unpin, then it will never move again. Foo here is not unpin, so therefore it should never move again once I have it in a pin. But here I'm causing foo to move, right? The old Z here is now going to be the old foo. So foo, the, the old thing that was inside F has now been moved. And so this is clearly not okay, right? And so the sort of insight here is we're going to require T to be unpin for draft mute to work for PIN for this to be safe.
03:18:07.671 - 03:19:57.645, Speaker A: Now, this will not compile because foo does not implement unpin, so this code will not compile. In fact, now we guarantee that just foo will never move once you put it in a pin, there's no way to get it out of the pin. Like PIN doesn't have a way to destruct it, right? And you cannot deref mute it, so you can't get a mutable reference to T. And so therefore you're not going to be moving T, you don't have a way to move it, right? So this is a core insight of PIN that you can only get immutable reference to the thing inside of it if it is unpinned, if it's safe, if it's not sensitive to being moved, right? Now, of course, there's one thing you can observe here, which is if I have some code where I know that I'm not doing member place, right? I have some code where I just want immutable reference into foo, because I'm going to change a string in there or something, that's all fine, I'm not moving foo, right? I'm not doing member place, I'm not moving foo in any way, then I know because of the code that I've written that it is safe for me to look beyond the P in a mutable way. And so that's why PIN P, there's an implementation on PIN which is unsafe as mute. I don't Remember exactly what it's called, but it's not important. It gives a mute P Wait, this should be target.
03:19:57.645 - 03:21:08.585, Speaker A: This should be target P target. So. So I can have this unsafe. This unsafe as mute, which given a yes, this is going to have. This is going to have where P implements draft mute, right? So this is saying this function is going to basically just do a de ref mute as well, but it's unsafe because it will only give you that mutable reference if you promise that you're not going to move the target. Right? Does the compiler consider draf mute similar to draft because of the names? No, no, the compiler doesn't do that. Although.
03:21:08.585 - 03:21:57.125, Speaker A: So the trait D ref mu extends d ref so in order to implement DREF mute, you also have to implement draft, but it has nothing to do with the names. Is it kind of sad that you can't get a mutable reference to mutated but not move it? I think your answer yeah, so the answer it would be sad if you had no way of getting at the thing the p points to if you sort of know that you're not mutating it. And that's exactly what we're getting at here. We do have an unsafe way to get to the target. But the unsafety here is you need to promise not to move T. Basically you need to manually uphold this contract. The T will never move again, right? If you're unpin.
03:21:57.125 - 03:22:42.155, Speaker A: If your target is unpin, you know that it doesn't matter to the type whether it's moved. And so therefore it's fine, we can give out the mutable reference because if someone chooses to replace it, that's fine. Foo doesn't care. And the fact that we. The thing that we promised in the past was just that if we move it, it's not going to matter. So this unsafe implementation comes along with this contract that you don't replace it. Notice however, though, that pin is only one level deep, right? So if I if inside foo there was something like a box string box use size no has to be string.
03:22:42.155 - 03:23:19.765, Speaker A: So foo now has. This has also itself has a pointer to a string. Notice that it using only safe code. It is totally fine for us to say it is totally. Oh, I don't know how to phrase this. If we have. Ah, a better example of this is actually the other way around.
03:23:19.765 - 03:24:07.049, Speaker A: Bar has a foo box. If we have a pin bar, right? Then it is totally fine for us to deref this because bar is. Bar is unpin, right? So it is totally fine for us to now move the Foo inside of bar. There's no problem with that. All we promised here was that bar is not going to move. That's all this pin promises does not say anything about foo inside of bar. And this sort of brings us full circle to futures.
03:24:07.049 - 03:25:06.043, Speaker A: So poll takes a pin mute self. Now think about what that means. When poll is called, we promise that self will not move or that self is unpin. And keep in mind here that if we have something. So pin also has this as mute self, which returns a pin, a pin of mute P target. And notice that this is also safe, right? So this is turning a. This turns for example, a pin box foo into a pin mute foo, right? That's necessarily safe.
03:25:06.043 - 03:25:54.577, Speaker A: A pin box foo promises the foo will not move, and a pin mutable reference to foo also promises the same. So we've done nothing weird by doing this, right? And so this is an entirely safe method because you still can't move foo. If I give you a pin move foo, you would still then end up having to either use this unsafe or the target type would have to be unpin. As this means that if we have a pin box foo, then it's trivial for us. If imagine the foo was a future, then of course now we can easily call F poll. That's fine, because this pin can be trivially turned into. Well, I guess it would be like this pin can now be turned into a pin mute foo.
03:25:54.577 - 03:27:01.941, Speaker A: And so if foo implements future, that is the same as a pin mute self. So now we're given this and to bring this then full circle to async await. Now that we have this pin, we have all the guarantees that we need for async, right? This, the compiler made async block people, not unpin for compiler made async block. We know it's not unpin because there's sort of self referential stuff going on here, right? But once we start polling, we know that it's not going to move. And before we start polling, it's fine for it to be moved, right? So if you just have one of these, you can move it as much as you want. It is only the first time you call poll that it's going to add the pin to it. And so this is why in the async await, if we look at what it actually does here, you'll see it pins the future that you give it.
03:27:01.941 - 03:28:07.235, Speaker A: It creates the future. So this would be like the foo, right? So it gets the future part of the foo, then it pins the mutable reference to the foo. So basically establishes the contract that from this point forward I will not move foo. And then it starts pulling, the future starts pulling the mutable reference, the pin of the mutable reference to foo. And so now inside of here, once we start going into step zero, step one, step two, all of those are calls to poll, which means that we're getting a pin of mute self, which means we know that foo will not move and therefore this self referential stuff is fine, because we know that this will not move anymore. But until you call await, until you start pulling it, you're free to move it, because we know that it's like in this state where moving it is fine. Today we're going through all of futures and Tokyo and async, await and pin.
03:28:07.235 - 03:28:49.605, Speaker A: I think it would be hard to follow from this point if you don't have an experience with it, but I recommend you go back and watch from the beginning, I think we've done a pretty thorough job of going through all the things. So whether something is pinnable or not is governed via unpin and not unpin, sort of. So every type is pinnable. You can always make a pin of some type key. The question is just what is the contact you're establishing? And the contract for all pins, no matter what the type inside of it, is that if you have a pin of something, then either. So remember, the type of PIN is always a pointer. It's either that the target of the pointer will never move again, or that the target of the pointer does not care whether it's moved.
03:28:49.605 - 03:29:36.595, Speaker A: So, for example, it's not self referential. Yeah, so in some sense, unpin or not unpin dictates which contract you are tied to when you make a pin. And so that is then what enables us to do this. Because now the moment you start polling this, you know that it will not move anymore. Or you. Or in the case where like you implemented your own future, if your own future doesn't have any self references in it, then it's trivially on pin, right? Self is going to be on pin. And so therefore writing your future is fine, because writing your future, you're given one of these.
03:29:36.595 - 03:30:06.045, Speaker A: Self is unpin. Therefore the D ref mute of the PIN is safe and you can use it. And so it's just going to be. You can basically just write self, dot whatever and you get a mutable reference to self because it's unpin. It doesn't matter. Okay? That is pin and that is what enables us to have async await. And that is currently what's about to be stabilized now some of the names might change.
03:30:06.045 - 03:30:49.225, Speaker A: There's a lot of discussion in this thread and about these double negatives. There's one other thing that is the proposal for stabilization adds and that is impl not is currently a nightly only feature. So this is similar to if you want to declare that something is not send like. Sorry, if you want to do something like imple not send for my type. This pattern of implement implementing a negative you can only do for auto traits because they're the only things that are not opt in. This is basically a way to opt out. This is a nightly feature, but they want to be able to ship pin without shipping that feature.
03:30:49.225 - 03:31:34.099, Speaker A: And so what they're adding is a a type called pinned, which is sort of stupid, but in the standard library. Sorry, the idea is not stupid, the name is annoying. So in stdpin they're adding a struct pinned which has no contents. So it's like similar to phantom data, it contains nothing, it's a zero size type. And inside of the compiler of course we're allowed to use negative implementations. And so there's an impl not unpin for pinned. The reason they do this is now, if you want to, if you.
03:31:34.099 - 03:32:46.925, Speaker A: So for foo, what we would do is instead of having this requires nightly, right? So instead of doing that which requires nightly, we just add like a not unpin this. And now because pinned is not unpin, foo will be not unpin because the compiler only sets auto traits if all members have that trait, right? So this adding this to the standard library means that now even though they don't stabilize this, you can now have things be not unpin unstable. I've been looking for more approachable resource to understand all this information related to futures in Async Await and I must say this has been the best for that. I'm glad to hear it. I do think this is part of the reason I did this stream in the first place was because there's a lot of interconnected components here and there's a lot of stuff you need to understand. And I think it's actually valuable to go through all of it, like sort of the entire like ball of stuff, because they all interconnect and understanding all of it is important. So I'm glad you find it useful.
03:32:46.925 - 03:33:19.427, Speaker A: Did it require many more compiler features or is it more on a library level? So that is one of the things that is really cool. Pop up. It's annoying. That is one of the things that is really cool about PIN and why I Recommend you go back and read those blog posts because there's sort of this gradual realization that we can do this basically without adding anything. So PIN is not special. It does require that you add an auto trait to the compiler. But adding an auto trade.
03:33:19.427 - 03:34:22.645, Speaker A: We don't want to add lots of them, but it's like pretty trivial. That's the only thing you added and didn't require a feature, right? It just required adding a type and an auto trait for it, which is already something the. Sorry, adding an auto trait to the standard library, which we already have all the other auto traits, so there's no feature needed in the compiler at all to add pin. And then the realization the PIN is all you need for Async Await. So Async Await is not. Async Await does require compiler features, basically, because the compiler has to generate this business, right? It has to implement it has to take code that looks like it has to take code that looks like this and turn it into this enum and the corresponding impulse. And so that is something the compiler now has to learn to do, basically sort of know what variables to capture and produce each step of this enum, but that's about it.
03:34:22.645 - 03:35:05.287, Speaker A: But. So, I mean, that was somewhat complicated, but it was more the realization that PIN is all that's needed to make that work. Does that answer your question? I think that was what you were asking. Okay, right. So pinned is the last thing that is proposed to be stabilized in stabilizing pin, right? So the stabilization efforts that we're going through are stabilizing task and future. So that needs to go into the standard library to get futures. Future requires pin.
03:35:05.287 - 03:35:35.325, Speaker A: So PIN is being stabilized and then Async Await. The RFC is landed. I don't know if the. I don't think there's a stabilization issue for Async Await yet. Yeah, probably not. So this is going to be the sequence of things that we're going to stabilize, I guess PIN first, because it's needed in futures, and then stabilize task in future. So that.
03:35:35.325 - 03:36:43.563, Speaker A: Because that's going to be needed in the standard library in order to have Async await. And then hopefully we'll see the ecosystem sort of move towards using the standard library future and task stuff. So that would include things like Tokyo has to move to it, which is a non trivial effort, but the work is sort of underway. One of the problems we're having is that the sort of pinning stuff, while it is really cool and probably basically what we need, it is a pretty big change to how you write Futures, because you now need to think about pinning and changing all of the ecosystem around futures and especially everything that's built on top of Tokyo and Tokyo itself. To use this just requires a bunch of work, but it is undergoing. And then once this is stabilized, then at least hopefully we could finish the implementation of and stabilize Async Await and then hopefully we would just have all of it. But that is still a little bit of a ways off.
03:36:43.563 - 03:37:41.305, Speaker A: I think the hope is to have Async Await in like two release cycles or something. I think there was some discussion, but I don't remember what the conclusion was in this, in this thread. But I think the hope is to get it out pretty soon. Although it's a trade off, right? Because these are really tricky things and you need really good documentation, which is one of the things that's currently kind of lacking. If you look at the both the RFCs and the implementations in the standard library that they're proposing to stabilize, the documentation is okay, but this is why I made the stream as well, right? To try to give people more wholesome introduction to all of the concepts that are involved. And the doctor Documentation of the standard library isn't quite there yet, where it gives you that kind of full interconnected picture of what's going on. My hope is that that's something we get into or manage to make a part of the stabilization effort.
03:37:41.305 - 03:38:20.515, Speaker A: We don't want to stabilize in a rushed way, but I do think we're getting to a design where now we're understanding why all the pieces are there and hopefully now you understand it too, in such a way that we can get to Async Await, which we all really want, in like a reasonable fashion. I wrote a little crate with one trait that wraps many pointers, many pointer types into a PIN type. It's called pinpoint. That's a great name. You can call pointer as pin. Yeah, so there's. There's a bunch of work on building.
03:38:20.515 - 03:39:18.089, Speaker A: So one of the things that's complicated about PIN is you want to be able to say if I have a pin, if I have a pin to bar. Sorry, a pin to box to bar. Or I guess that view bar. Is there a way for me to go to a pin? Mute foo for example, specifically like sort of project into this field is. Or it might not be the best example. Let's go with that one. Anyway, is there a way for me to do this safely through Bardock crew? It turns out it's actually fairly difficult to express what the requirements are here.
03:39:18.089 - 03:39:39.425, Speaker A: The requirements Basically, if I remember correctly, is. Oh, actually that's separate. Yeah. So exactly what this. What you need for this is a little unclear. There's a crate though, that's intended to do this for you, which is this pin utils crate. I think.
03:39:39.425 - 03:40:13.085, Speaker A: I think it was pinute. Could be wrong. It might be mentioned here somewhere. Yeah, I don't remember where it is. Yeah, pin utils. Okay, so pinutils has basically convenience methods for interacting with pinning. Pinpoint is probably a similar kind of thing.
03:40:13.085 - 03:40:54.775, Speaker A: And the hope is to provide some kind of mechanism for giving out this kind of sort of transitive pinning where you can say that if bar is pinned, I can give you a pin of foo. But we don't have exactly the mechanism for that yet. The only other things I wanted. There's one more thing I wanted to mention which is. And you may have seen this already for this fn new. In fact, we can look at this here. Creating a new pin based only on a reference.
03:40:54.775 - 03:41:17.453, Speaker A: No, that's not the one I want. Right. So the RFC goes through basically everything we've gone through here. But for new. I think this is missing something from a while ago. Yeah, this. This RFC is a little bit older.
03:41:17.453 - 03:41:48.225, Speaker A: This doesn't include. There was an older version of pin that had like pin and pin box, some pin arc and none of that is no longer needed. What I give is still the. Is sort of the new instantiation, but I guess this isn't the right one. If we go to the top here, I specifically wanted to show you the constructor. So there's a safe constructor when the target is unpin, there's an unsafe constructor if the target is not unpin. And I want to explain why that is.
03:41:48.225 - 03:42:37.925, Speaker A: So we're going to require that P implements d ref. Oh, I guess that's actually something I've missed in all of this. Like all of these need to have the bound P is draft if you want to create a new this is actually unsafe pointer P gives you a self unsafe. So the question is why is it unsafe? Right. So there's a safe implementation which is if P target implements unpin. So this is safe. But why is this not safe? This has this basically just as this, so that's safe.
03:42:37.925 - 03:43:52.945, Speaker A: Why is this unsafe? The reason is we basically want to avoid. In the case where you don't have. Where you have a type that is not unpin, we want to avoid ever giving out immutable reference to T. Right. Because if we do, you can mem replace and now the value moves Imagine that I write sort of a a malicious, if you will. Like I write like bad box and what bad box does is just contains a boxt and I implement DREF for bad box, right? Just ignore what the implementation is and then I implement DREF mute for bad box and it gives out a mutable reference to the thing underneath, right? Self0 so this all seems fine. This is all fine.
03:43:52.945 - 03:45:15.795, Speaker A: But here for a brief sec. Yes, for a brief second here I have access to immutable reference to T in DREF mute, right? Bad box Whenever the PIN chooses to mute to go through D refmute I as bad box implementation of DREF mute I can mutate self. So imagine the bad box in DREF mute Does a memory place with like T default or whatever or who knows mem uninitialized. I just moved the T, right? But the target is not on pin but I moved T after it's been pinned. And so clearly that's not okay. And so the reason that we need new to be unsafe, even that we need new to unsafe when T is not unpin, is because we need to guard against this. We need to make sure that at no point does anyone have a mutable reference to T and do something like this.
03:45:15.795 - 03:46:00.813, Speaker A: So the unsafe here is also like I promise that my D ref mute does not move T. It's basically the promise you're making in new unchecked. So foo contains a unpinned. There is no unpinned. Are you talking about pinned or are you saying that foo contains a type that is unpinned? So pinned a lot of code on the screen. Now this pinned is really just. It is really just this.
03:46:00.813 - 03:46:42.091, Speaker A: It's a way to opt to say that your type is not unpinned without having to write this line which you can't write on stable. That is the only thing that pinned does pinned, that is P I n D like this thing. That is the only thing it does. Now if foo contains. If foo contains something that is unpinned, that does not matter. Pinning is just a guarantee about the immediate dereference and nothing else. I hope the documentation renew unchecked will explain the required invariance and reason for them.
03:46:42.091 - 03:47:26.505, Speaker A: Yeah, so this is one of the things that's going on in this discussion is there's been a lot of discussion about like exactly what should the doc say? And I learned a lot of this from reading the stabilization thread. I don't know what it says now. It has changed a little bit, but the plan is certainly that it should that all the documentation will explain exactly what all the invariants are because they are really subtle. For pin. It's really subtle why all these things work and why it ends up being safe. And I am positive that throughout the things I've told you there are probably things that are false. But you got to start somewhere.
03:47:26.505 - 03:47:52.585, Speaker A: Let's see. Oh, you saw the my paper in the morning paper. Yeah. That was pretty fun. It's interesting because I have. I have had a bunch of academics that have said it the other way around that it's interesting to see someone who's an academic be in Rust circles. Okay.
03:47:52.585 - 03:50:48.839, Speaker A: I don't think there's anything else I wanted to talk about sort of futures or Tokyo or Async Await. Is there anything that you feel like you would like to hear more about in any of the stuff we talked about so far or other things you'd like me to cover about this stuff? Anything that's still unclear you want me to go into or you want me to talk more about as sort of directions we're going forward. I guess now is sort of the now. I think I've given you the entire package and if there are still things that aren't clear, I think now would be a good time to cover it. While you're thinking about that, I will go P SA SA let's see what comes for the next stream and when do you plan it. So if you missed it in the beginning. I wrote this website.
03:50:48.839 - 03:51:13.897, Speaker A: This is a previous stream that's been filed where you can vote on what upcoming stream ideas you'd like to see. It uses ranked choice voting. It's pretty cool. You just pick a unique username and then you drag and drop the ideas into which ones you would rather see me cover. The next stream will probably be in three weeks probably. I have a bunch of plans coming up. We'll have to see.
03:51:13.897 - 03:51:58.837, Speaker A: I'm not entirely sure. It looks like the next thing is going to be porting Flame Graph, which is a CPU profiling tool or visualization tool that's re really cool to Rust. We probably won't port the entire thing, but it's. It would be a slightly different stream so we wouldn't talk so much about Async and stuff, but rather do a more performance oriented thing which I think could be really fun. So it looks like that would probably be the next thing, but feel free to vote there. That'd be a good idea. What is this channel about? So this channel is about the Rust programming language and in particular introducing more advanced concepts and talking about and Writing more intermediate or advanced level Rust codes.
03:51:58.837 - 03:52:26.595, Speaker A: You can see like real code being written. I think. Yeah, I think watching the full recording later is probably a good idea. There's a lot of details here and it might be hard to follow in real time. I recommend you also go back and sort of draw some diagrams for yourself and see if you can explain it to yourself why this is. This is correct. There are lots of details that I've sort of skimmed over or skipped and so feel free to try to dig into some of this documentation yourself.
03:52:26.595 - 03:53:00.755, Speaker A: That's a good point. Tokyo has recently done sort of a big documentation effort. It's still underway, but if you go to Tokyo RS documentation, there's now here. So no going deeper. This section in the Tokyo documentation has recently been written and is basically going into things like the runtime model. Like why do you need to. Probably great.
03:53:00.755 - 03:53:45.639, Speaker A: It goes through things like what is exactly how do the runtimes work, how do you. How do the executors work, how does the work stealing work? How do you interact with IO and reactors, timers, how would you build a runtime yourself and what you need. There's also Tokyo Internals which is still being written that goes into even more detail. Even more stuff about reactors and non blocking IO. I really recommend you go read this. There's also the doc Push Repo which is all about improving the documentation for Tokyo. And so especially now that you watch the stream, I highly recommend you go like look at this and see.
03:53:45.639 - 03:54:58.685, Speaker A: So there are a bunch of issues about where we would like more documentation to be written. If you feel like there are things that you now understand or even if there's things you're still learning, like try to contribute to this because you could really help people understand how futures work, how Tokyo works, how the ecosystem works, how Async Await and Pinning works for that matter, like contribute to this Small and large would be really helpful in order to improve the documentation of the ecosystem. And that includes the standard library too. Like if you can help improving the documentation for stuff in the standard library related to this, especially sort of Futures and pinning without lands, that's great. Is there anything in Rust for CSP or Actor based concurrency? Yes, in particular the crate called Actix which is all about implementing the Actor model in Rust and I haven't used it too much myself. It builds on top of Tokyo. I don't know exactly how the integration works, but it's basically you have a bunch of things that are actors that operate on their own state and they only communicate by sending messages to one another.
03:54:58.685 - 03:55:40.945, Speaker A: This is very similar to like the Erlang model and it supposedly very good. There's a. It's primarily built for something called Actix Web, which is a web framework that uses actors to build sort of a microservice oriented architecture for websites. And I've heard good things. And now you may be better better equipped to understand how Actix works under the scene scenes too. I don't know how well this is documented for how the asynchronous part of it works, but at least now maybe you have a better understanding of how the internals might work for csp. Not really.
03:55:40.945 - 03:56:39.975, Speaker A: There's been some discussion about adding support for a yield keyword in Tokyo, but you sort of need it to be at a lower level than that. You almost need it to be a compiler feature basically to give you continuations. But now with Async Await, it might be that you can express CSP and yield in particular using Async Await, but I don't know that there are any immediate plans to do that. But I think that's something you would need to get closer to true csp. When implementing my own future, I'm fairly sure I understand what to do if I need to return not ready, but maybe do a two minute example of a simple custom future. A simple custom future. So there's one more thing I want to highlight before I do that.
03:56:39.975 - 03:59:26.905, Speaker A: One of the goals of Async Await is that you shouldn't need to implement future yourself as much because you should be able to just use async functions and async blocks and async closures and await to just write out what you would normally have done in the implementation of the future. Right? So this basically means that you, like many cases where you would previously implement your own future and keep an enum where you walk between different states. Like are you connecting to google.com, are you writing stuff out, are you reading stuff back and sort of manually handling the state machine of that future? You could just do as an async function instead using a wait and it will be a lot easier to write. As for the contract with not ready, it is really just a matter of any time you return not ready, make sure that something is going to wake you up and remember that you can always assume that if something else returned not ready to you, then they have arranged for them to be woken up so you don't have to do it. So this contract really applies transitively. Usually that is actually worth pointing out in the futures Crate, there's a macro called Try ready and what try ready does is pretty much, pretty much looks exactly like this, right? So the idea is that if you call, if you have some struct like my future and you have a bunch of fields and one of them is like your future, you have some inner thing that implements future of some kind kind, then when you're implementing future for your own future, right then you're going to have a poll.
03:59:26.905 - 04:00:56.731, Speaker A: It's going to return poll. So remember that poll is just an alias for this or I guess if we're going to do sort of new fancy futures, then really this is going to be this now, right? Remember that how future 0.3 just got rid of the error so we can just get rid of all of that error and result related stuff. So if you implement future for your own future, then like you're going to do a bunch of things in poll and at some point you are going to pull the inner future, right? Then you have to be prepared for the fact that the inner future might return not ready, in which case maybe you can make no progress, right? But. But it might also return something in which case you can make progress. And the try ready macro lets you do this. And so that's going to.
04:00:56.731 - 04:01:39.485, Speaker A: It's basically like the question mark operator, except for futures. This is going to be if poll returns not ready, then just return not ready from me as well. Otherwise give me the thing that was inside ready. The reason this works is remember this means that you're returning not ready. And the question is, have you arranged for yourself to be woken up? And the answer must be yes, because this inner future also must maintain that contract that if it returns not ready, it has arranged for the task be woken up. So it returned not ready and will be woken up. So when you return not ready, you will be woken up because it will be woken up and you're on the same task, right? Any, any chain futures are on the same task.
04:01:39.485 - 04:02:47.395, Speaker A: And so that is usually when you're writing your own futures, that is usually the way in which you uphold that contract is just that you're using other futures that are themselves upholding the contract doesn't await use the yield keyword internally. I sort of lied. Sorry, where's my. Maybe I've erased it. Yeah, probably. Okay, so if we go back to async await. So down here, where is it? There.
04:02:47.395 - 04:03:33.825, Speaker A: This. Right. So remember how we talked about what await sort of expands to see how the RFC says roughly expands to this. So as it says the yield Concept cannot be expressed with existing Rust code. And it's not really yield, right? It's really store all the stuff, all the stuff on the stack into the struct that I generated right into this special future that's like the async future and then return not ready. But it's also not just that because it also needs to say the next time you. Basically, the way to think about this might be when it.
04:03:33.825 - 04:04:33.415, Speaker A: When the compiler constructs this enumerated for something like this code is too far away. Let's move this code. So we sort of claimed that this async await stuff turns into an enum like this. What it really does is it does like a match on self. And if it's step zero, then it does this. If it's sort of this, if it's step one then it does like self.waiting on.poll
04:04:33.415 - 04:06:09.869, Speaker A: and it's sort of just like try ready of that, right? So this is like the next value or I guess C, right? And then if it does get to that, then it sort of kind of awaits for that and that it moves like self is equal to step two, right? And here it's sort of like self is step one. You see, sort of what I'm getting at, like this is really what async turns into, right? Like self dot whatever, I guess waiting on. So this is self step three. So this is how it gets around that problem of not re executing the previous stuff is because each of them is in a separate step. And so it just uses the self because self is an enum, it uses it as like as a state machine so that it can move between the different parts parts without executing the previous parts again. And so this is why the yield inside of a weight is not really a yield, it is like a set self kind of thing. Yeah, so.
04:06:09.869 - 04:06:36.665, Speaker A: So. So this is why Brian mentioned a while ago that like that you can sort of talk about async as generators. And that is true, you can sort of think of them that way. But they're not really generators. They're just magically or not magically created, but just very carefully constructed enums with matching. And they really are special. Like they require the compiler to generate very particular code.
04:06:36.665 - 04:07:02.521, Speaker A: The patreonly. Oh, I should remove that from YouTube. Yeah, because I'm an international student in the U.S. it turns out that I'm not allowed to have other sources of income and so I had to shut down my patreon, which is kind of sad, but such is life, I guess. I in some sense. I didn't start to do this to like make money either. I think it's.
04:07:02.521 - 04:07:37.165, Speaker A: I think it's fun to educate people and I think it's fun to write code and I think it's fun to have people to interact with while building things and sort of getting your feedback as we go. So that is sort of my reward. But it was a bummer, but there's not nothing I can do about it, sadly. But thanks, I appreciate the sentiment. I think. I think that's all then. Unless there are any more outstanding questions, I don't think there's anything more here.
04:07:37.165 - 04:08:16.331, Speaker A: So a lot of content we've covered, so going back and looking over it one more time is probably not a bad idea. Instead of drawing things, this is pretty drawing too. Get rid of that. Well, in that case, if there are no more questions, I think we're just gonna end it there. Four hours. It's pretty good. Pretty good.
04:08:16.331 - 04:08:54.991, Speaker A: I hope you found this useful, helpful, educational. I hope the ecosystem around futures and async computation makes a little bit more sense now. If you have other questions then feel free to sort of reach out. I'm on Twitter. I showed this earlier, but I am this person so feel free to reach out there or send me an email and then I will be happy. I'm also on Mastodon now for those of you who are a little bit more paranoid about like privacy and not wanting to give Twitter control over your social spheres. So I'm here as well if you want to follow or message me there.
04:08:54.991 - 04:09:20.125, Speaker A: I pay attention to that too. Reach out if you have questions. If you have ideas for upcoming streams too, I will add those happily to the voting site that we now have. It will remember your votes. So if I do one stream and then I do a second stream, it will still tally up sort of the remainder of your votes. But you do want to check back every now and again to see if there are new things to vote for. If those may be of interest.
04:09:20.125 - 04:10:02.855, Speaker A: I also really highly urge you to contribute to the Tokyo documentation push. Part of it is also not about Tokyo. Like the doc push is all about documenting basically all the stuff we've talked about today and the futures and Async ecosystem. Of course it's a focus on Tokyo, but it also discovered discuss what are futures like? What's the execution environment like? And contribute to that because it's going to help other people find better documentation to learn from in the future. And yeah, I'm glad you all came to join. Have a great rest of your Saturday and I'll see you in three weeks time, I guess. Bye.
