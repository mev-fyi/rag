00:00:06.480 - 00:00:31.419, Speaker A: Hello folks, welcome back to another crust of Rust. It's been a little while since we did one of these. I've had this one sort of in the back of my head for a while of. It would be good to talk about the different collection types in Rust. And you know, the docs for STD collections is pretty good. If you read through it. It talks about when should you use which collection and gives you essentially like a bulleted list of what each one is for.
00:00:31.419 - 00:01:17.015, Speaker A: It talks about the performance characteristics of these different implementations. It talks about the sort of complexity of the different sequence types and map types. So like, this is a very good page and if you want to take the time to read through it, I highly recommend that you do. What I'm going to do today is in some sense summarize the information here, but we're also going to dive a little bit deeper into what are the actual representations that are being used here and talk about sort of how you think about which of these you should be choosing in a given point in time. I'm not necessarily expecting this to be a super long one. I'm guessing maybe an hour and a half to two hours and we'll see. And hopefully, you know, there are a bunch of questions that we can address as we go through as well.
00:01:17.015 - 00:02:27.435, Speaker A: Okay, so in STD collections, which is the sort of place where the various implementations of data structures in the standard library lives, we have implementations for sequences, maps, sets and other things which currently only contains a single collection. One of the things that's a little interesting here is this set of types has not really grown since Rust 1.0. And there are a couple of reasons for this. It could be that there are other data structures that should be in here, but one of the reasons is this set is usually good enough for most use cases. And the other one is I think that there's been a sort of observation that there's a lot of value in having the implementations of these live outside of the standard library, so that they can be iterated on more rapidly, so that we can have multiple different implementations that make different trade offs. Once you put something in the standard library, you're not quite locking yourself into one data structure, as we'll see in a second, for what happened to hashmap. But you are locking yourself into an API, you can't really do breaking changes of the standard library.
00:02:27.435 - 00:03:26.113, Speaker A: And so as a result, once you stick something into the standard library, you sort of lock it in place and you might be able to change the implementation behind the scenes. But people are going to rely on basically every aspect of the thing you put in there. So even if you change it in such a way that it has slight runtime characteristic performance differences, then that too might be something that ends up breaking a bunch of users or at least making life hard for them. So what we've seen increasingly is that new data structures, whatever form they might hold, do not end up in the standard library and instead develop as just completely different crates. And so that's also why this MISC type is sort of. It's a one item collection, if you will. One item category is because it's not clear that we want lots more here because we might as well just have them be separate crates.
00:03:26.113 - 00:04:09.729, Speaker A: And this set of things that are in there at the moment are sort of very fundamental types where maybe it makes sense to have them in the standard library because people are going to reach for them all of the time. And it's a little unfortunate if they have to look for an external crate every time they wanted to use, say a vector. Right. So the way we're going to do this is we're going to first we're going to look at these in the order of the categories here. So we'll look at sequence types first. So first of all we have vectors and vectors I'm guessing you're all pretty familiar with. I'm not going to necessarily go into how vectors work because there's not a lot to it, but I will give you a very high level overview.
00:04:09.729 - 00:04:48.015, Speaker A: So a vector in Rust is a collection of a raw vector, which we'll get to in a second, and a usize. So I just click the source link top right here, which you can't see behind my face. There's one top right of every page and next to methods and stuff. You see the definition is a raw vector and a length. And the raw vector here is sort of an internal type to the standard library that represents a vector that is of a given size. And when I say size here, I mean capacity. Like it is just a chunk of contiguous memory of a certain size.
00:04:48.015 - 00:05:29.531, Speaker A: And it, the, the raw vector does not know how many items are in it. It just treats it all as potentially uninitialized memory. And the VEC wrapper around it keeps track of how long is this vectors, how many items in the vector are actually allocated. And so vectors have this distinction between capacity and length. The capacity is how many things can I store in this vector without having to allocate more memory. And the length is how many items are currently in here. So the moment the length hits the capacity, then trying to push another element onto the vector would require the vectors underlying allocation to grow.
00:05:29.531 - 00:06:49.001, Speaker A: And usually the way that ends up happening is you allocate a new chunk of memory, often that is twice as long, and then you copy over all the elements that are in the vector and then you push the new item that was added. And that's sort of hidden by the vector type in that all of its operations will just transparently reallocate this raw vector whenever it's necessary, whenever you reach the capacity. But this is one of the reasons that for the vector type, and you will see this for the other collections too, for the vector type you'll often want to use when you're able to this with capacity field. So what with capacity will do is allocate a vector who's backing memories enough to hold this many items so that you can push that many items without having to do reallocations. If you don't do this, if you do new, it'll allocate a fairly small vector and then every time you fill it up, it's going to reallocate and copy. And so as a result, if you're going to push, let's say a million elements onto this vector, if you use new and then you push a million times versus you use with capacity a million and then you push a million times, the new and push is going to be way slower than what the with capacity is going to be because it's going to have to do all this reallocation and copy work as well. And you'll see this in some of the other methods on here.
00:06:49.001 - 00:07:35.585, Speaker A: So you have things like try reserve, which is make sure that this vector has capacity enough to hold this many additional items. And you can query the capacity as you need. And it's useful to remember the vectors work in this way because often this can be a sort of performance implication too, where you find that your application suddenly has these like latency spikes. And usually those latency spikes are when a collection had to resize. And we'll see this for things like hash maps too, where hash maps have the same property when they fill up. You have to allocate a new sort of bucket allocation for the backing of the hash map, copy all the existing things over and then do the insertions to that new item. And you might wonder, well, what is the default capacity? And that is a somewhat sophisticated question.
00:07:35.585 - 00:08:06.965, Speaker A: If you look at new here, you see it uses raw vector new and raw vector new. Let's see If I can find that here. No rustling rust. So if we dig into library, it's an alloc, you'll find that a lot of the standard library collections are in the alloc crate and not in std. So there's Core, alloc and std. STD relies on operating system primitives. Alloc just requires a memory allocator and core requires nothing.
00:08:06.965 - 00:08:40.785, Speaker A: And a lot of the collections are in alloc, except for hashmap, which requires randomness, which comes from the operating system. So alloc source raw vec. I'm guessing that's going to be a raw vec dot rs. Yep, right here. Let's see if we can dig up new self. New in here? Yes. As you see here for the raw vector implementation, I don't want symbols.
00:08:40.785 - 00:09:25.195, Speaker A: You'll see that it tries to figure out what is the smallest vector that they're going to be willing to allocate. And that's going to depend on the size of T. So if you have a vector of very large objects, you might be willing to have a shorter vector capacity. If you have a lot a smaller t like a U8 inside of the vector, you might want to make sure that it's larger to begin with, because you're not you. You expect that you're going to be pushing a lot more of them. As you see here, the default size for a vector is 8 for anything that's a single byte, it's 4 for anything that is less than 1k and then it's 1 for anything that's larger. And then the growth.
00:09:25.195 - 00:09:56.327, Speaker A: The growth operation. Let's see if I can find that. I didn't have it linked, which I arguably should have grow. Yeah. So the new capacity is going to be twice. It's going to be whichever is more of how much is required. So if we know that we're going to push 1,000 elements, we're going to grow it by at least 1,002 times the current capacity.
00:09:56.327 - 00:11:11.399, Speaker A: And we make sure that it's always greater than the minimum non zero capacity. And so basically the vector ends up doubling in size each time. And someone asked in chat here, what does the A Here is the A equals global is which memory allocator. And this is a nightly only feature that lets you switch out which memory allocator is used to allocate the backing buffer. And the other thing that I wanted to talk about for vectors in particular is that vectors are, as I'm assuming many of you already know, vectors are a sequence where all the elements follow one by one and you're not allowed to have gaps in a vector. And so if you have the items from 1 through 10, if you remove item 5, what actually happens is item 5 is erased from the vector, and then all the later elements in the sequence are shifted over by one. And this might mean that let's say you have a vector of a million elements and you remove the first one, you're going to do a MEM copy of 999,999 items.
00:11:11.399 - 00:11:36.691, Speaker A: And so as a result, removals from vectors can be pretty expensive. There is an operation called Swap remove right here. And what Swap remove will do is when you remove an item, it'll take the last element of the list and then put it in place of the one that you just erased. And so that way nothing has to shift because there are no gaps. The downside of Swap remove is it doesn't preserve the order of the vector. So the order of the vector matters. Then you don't want to use Swap Remove.
00:11:36.691 - 00:12:35.543, Speaker A: But if it doesn't, Swap Remove is a lot more efficient than remove is. There are a couple of unsafe operations on Vector 2 that are useful to know about, like set length. So setlength is a way to both truncate a vector, although for truncation you'll usually want to use the actual truncate method, which is not unsafe. But the more important way you can use setlen is to extend the length of a vector into uninitialized memory or into memory that you have initialized yourself. So the trick here with setlen is imagine that you allocate a thousand element vector, but initially the length is zero, and you have some. Let's say you do FFI to C or something, and you pass C a pointer to the vector to the backing memory of the vector, which you can do using the asmut pointer here, which gives you a raw pointer to the stuff. And then the C API is going to write into that memory, and it tells you back, I wrote 500 elements then.
00:12:35.543 - 00:13:11.985, Speaker A: Now, on the rust side of things, you want to say this vector now has a length of 500. I know the first 500 items have been initialized. They're no longer sort of erased or uninitialized memory. And so that's when you would use something like setlen to tell the vector implementation. Just trust me. The first 500 slots of this backing memory are in fact valid entities now. And so you can unsafely set the length of the vector, and from that point forward you can index into those items the Things after the length are still going to be considered uninitialized memory and are not going to be accessible to you without unsafe.
00:13:11.985 - 00:13:44.079, Speaker A: And I think setlength will panic if the length is greater than capacity. At the very least, it's undefined behavior because the length cannot be more than the capacity. That would be a buffer overflow read. And so that's right, yeah. So capacity is the max amount. It's not the max amount you can allocate, it's the maximum number of items you can have without reallocating. And length is the actual amount of items that are in the vector.
00:13:44.079 - 00:14:23.199, Speaker A: So the capacity is the amount of memory that's being used and the length is the amount of memory that is initialized and accessible. There are a couple of other neat things here like retain, which is a useful method to know about. Let me see if I can. Where is retain? Retain Right here. So retain is a way where you can take a vector and you can say, remove all the items that match the given predicate. So where the following closure returns false. And the reason why retain is valuable compared to looping yourself is first of all, looping yourself gets annoying with the borrow checker because you're looping over a collection while trying to mutate the collection.
00:14:23.199 - 00:15:27.657, Speaker A: But the other reason why retain is nice is because if you remove a bunch of elements, it's going to be smarter. About that shifting that I talked about, if you remove multiple items, it doesn't have to remove one and then shift everything. Remove one, shift everything, remove one, shift everything. It's going to remove all the ones you tell it to remove and then sort of optimize the way that it does shifting so that it's not quite as expensive. Anything else special I want to talk about with vector? So vector dereferences to a slice, which is pretty, you know, should be unsurprising, right? There's a contiguous piece of memory, just like a slice is. And so when you have a vector, you can use it as though it were a slice and access all the methods that are on slices. The other thing that's useful to know about vectors is like boxes, they have a leak method where you can take a vector, you can call vec leak and pass in the vector, and that will give you back a mutable reference to a slice of the underlying memory that will live forever.
00:15:27.657 - 00:16:19.515, Speaker A: So this is doing the same thing as box leak does, which is just leaves it on the heap, never deallocates it, and gives you a essentially static reference to that memory. And this can be useful if you have some vector of, let's say, configuration data that you want to share across your entire application. Rather than have to pass this vector around, especially if you end up wanting to read from it from multiple cases, you can just leak it and then pass a reference that ends up being static everywhere in your program. You could also do this with reference counting and the like. But if you don't actually care about this being deallocated ever, because it truly lives for the lifetime of the program, something like vectly can be really useful. Why doesn't capacity fill with default values? So the reason why capacity doesn't fill with default values is because vector, and I think there's a note about this in the documentation. Let's see if I can find it.
00:16:19.515 - 00:17:02.861, Speaker A: Guarantees, right? So due to its incredibly fundamental nature, VEC makes a lot of guarantees about its design. And I recommend you read this section. It's pretty interesting, but it's essentially observing the vectors are a very low level primitive. It's used everywhere. Like a hashmap is implemented in part in terms of a vector, right? Um, and so you really want vector to not do anything or at least be able to opt out of any behavior that you might not want. And one such example of such a behavior would be if creating a new vector of a given capacity. If that ended up doing that many memory writes in order to fill those values, then vector would be a lot slower.
00:17:02.861 - 00:17:55.623, Speaker A: For use cases where you are okay with that memory being uninitialized, like imagine you're about to do a read system call, read from a TCP socket or something into a buffer. Well, it doesn't matter what memory is in that buffer before you read into it, because you're going to overwrite it anyway. So if you were forced to write all zeros into it before you did the syscall, then now you're imposing additional overhead on every single call for something that's just going to be overwritten anyway. Okay, I think that's all I wanted to. No, there's one more thing I want to talk about for vectors, which is the trait implementations. Where are the trait implementations? Down here. So it implements a bunch of things as you would expect, like sref and asmute and debug and whatnot.
00:17:55.623 - 00:18:55.755, Speaker A: But the one I want to talk about here is extend. So vectors and most collections implement the extend trait, which is saying if you have an iterator that produces items and you have a vector, you can tell the vector to extend itself with the items that are yielded by the Iterator. And one of the things that's neat about this extend implementation is it actually tries to be kind of smart. So if you go look at the source here, here. Well, there's a lot of magic here to deal with specialization and stuff, but the basic idea is I'm not going to find the code here right now. The basic idea here is that in extend, you could imagine the implementation of extend was just for each item yielded by the iterator, push it into the vector. But extend can be a lot smarter.
00:18:55.755 - 00:19:52.965, Speaker A: For example, if you pass a vector to another vector, when you extend, then you know the size of that other vector. So you can resize the capacity of the vector to fit all the items from the other vector and then just mem copy all of them at once, rather than all this overhead of looping and yielding from an iterator and pushing. So you can just chuck them all in there all at once with a memcpy and then set length, and then you're done. Similarly, if you have some other iterator where you happen to know the number of items in the iterator, so you know that this is an iterator over like 50 elements, you can do the same thing. You can reserve that much memory so that you know, at least you won't have to reallocate while pushing. This matters, especially if you might have to reallocate multiple times during that push. You can also have cases with iterators where you know that they can yield up to 100 elements, right? So this might happen if you have, let's say, a vector that you iterate over and then you call a filter.
00:19:52.965 - 00:20:34.365, Speaker A: Then if you know that the length of the original vector was 100, and you know there's a filter there, you know that it can produce at most 100 elements. And so the implementation of Extend can make use of this, where it can sort of query the upper bound on how many items might be yielded and then use that to inform reservations of capacity before doing all these pushes. So that's pretty nice. There's a bunch of other convenience methods here. They're really more about slices than anything else, like binary searches and sort and those kind of things. We're not going to go into those in too much detail. Okay, so that's vectors.
00:20:34.365 - 00:21:12.153, Speaker A: Any questions about vectors before we move on? How do you drop a leaked box or vector? You can't. So when you leak them, you do leak them, they will be leaked for the remainder of the program. They get deallocated when your program exits. Okay, so let's move on in the list. So the next thing in sequences move on list. Get it because it's a sequence. Oh, how does the VEC macro work? The VEC macro is fairly uninteresting, actually.
00:21:12.153 - 00:21:40.847, Speaker A: It just allocates a. It's a macro where if you get no elements, then it turns into vec new. If it gets any number of elements, it will do vec with capacity, that number, and then push all those items and then that's. And then return the resulting vector. So it's a fairly uninteresting vector. Actually, the small VEC crate differs from VEC in that it has a. So the vectors are all heap allocated.
00:21:40.847 - 00:22:43.047, Speaker A: So the raw VEC type we looked at is an allocation on the heap that you get through something like box new. And that means that every time you have a vector, you're going to do a memory allocation and you're going to have to do pointer chasing, where you're going to have to follow the pointer of the allocation into the heap and then read the items from there. Other crates like SmallVec, and there are a lot of implementations like this. SmallVec is not the only one, but we can look at it. So what smallvec does is it has this const N as well, this constant generic, that tells it how many items should you keep on the stack. So you'll see that the way this is actually implemented as a smallvec here is an enumerated of either a local buff, meaning a vector whose capacity is allocated on the stack, not the heap, or it's a remote buff, which is it's a normal vector that's on the heap. And when it's local like this, you don't have any memory allocation, you just keep.
00:22:43.047 - 00:23:30.415, Speaker A: It's as if you declared an array on the stack of that many elements. And so it'll keep track of like what is the length and what is the array, and it treats the array as the capacity. So if we look at local vec here, I'm guessing if we do the source local vec here, you see, is just an array of maybe uninitialized of that length, and then it keeps track of the actual length. And so the idea being that you don't have to do allocation as long as you have fewer than or equal to as long as you have up to and including n items. If you have more than n items, you do a heap allocation and you copy them all over. And this can generally mean that you have fewer heap allocations, slightly faster performance, because you don't need to do indirection via the heap. And so that's the general benefit of a type like this.
00:23:30.415 - 00:24:30.805, Speaker A: Do any of the data structures have a stable abi, say for C, F I? They don't have a. There's no ABI in Rust, there's no stable ABI in rust. But vec, for example, guarantees the way that it lays out memory, guarantees it's contiguous, for example. And so you can use the backing memory from a vector to construct a array pointer that you then pass to C, for example. So it depends on what you mean by the data structure, right? Like you won't be able to call, let's say the vector push method from C over F of I, but you can allocate a vector in rust and pass a pointer to it, and the length to C and C will be able to operate it with it. That's not the case for the other collection types quite as much. That is mostly a property of vec.
00:24:30.805 - 00:25:34.099, Speaker A: Why does VEC implement intuiter but not. Why does it implement intuitor but not Iterator? Okay, so this is also worth talking about. So if we look down at vector and we'll look for iterator, you'll see that vector implements the into iterator trait, which is the thing that you can call on a thing to turn it into an iterator, but it does not implement the iterator trait. You see, there's only into iterator implementations. The reason for this is because a vector is not an iterator, right? Because an iterator needs to keep track of which item am I currently or am I going to yield next? Or alternatively, which one did I just yield? And the vector doesn't have that. The vector is three fields, a pointer to the backing memory, the capacity and the length. And for an iterator you would need an additional item, which is the item that I'm about to yield.
00:25:34.099 - 00:26:49.545, Speaker A: And so that's what happens when you call into iterator on a vector, is it produces a new type that has a reference to the vector, and that reference is either read only if you got the iterator from a shared reference, or it is mutable, if you got it from a mutable reference to the vec or it's owned, if you have an owned vec that you turn into an iterator, so it has a reference to the underlying vector and then it has an item or an index thing. So if you look here at layer into iterator, see if I can find it's going to be annoying to find, but if you looked at the actual type that's used here, then you'll see that. Oh, maybe I can get to it here. Yes, great. There's a pointer and there's a pointer to the end, and it uses this instead of an index because it happens to be better for certain things, for zero size types and whatnot. But it basically keeps track of where in the vector am I going to yield next. So that's the reason why those are different.
00:26:49.545 - 00:27:42.071, Speaker A: Isn't there a size limit on the stack? There is. So usually you don't want to use a small vec and have like 1000 elements. It also means that if you pass it across function call boundaries, for example, it's going to have to be copied over between the stack frames, whereas with the heap allocation, you just pass the heap pointer instead. Why is the lifetime of vec leak not static? The lifetime of leak is any lifetime you want. And the reason why it's this way rather than returning a static, is it makes it a little bit easier for the bar checker to handle this call. If you return static, then it would need to shorten the lifetime into whatever lifetime you need, right? If you gave. If it always returned a static mutable reference, then now wherever you put it, it's going to put it in as a static.
00:27:42.071 - 00:28:25.355, Speaker A: And this ties into how variance works, where that means that if it's inside of something that's invariant, then the invariance is now going to be on static, and you might want it to not be on static. So having it be generic ends up with a. It ends up being a slightly. A thing that runs you into trouble less often than static. But they're sort of equivalent. Is there any reason not to use small vec over vec? Yes. So the reason not to use small vec is there's a little bit of an additional cost to keeping track of whether the thing is on the stack or the heap, right? It means you have to branch on, you know, an additional field telling you whether it's local or not.
00:28:25.355 - 00:29:20.151, Speaker A: And the other reason is because with anything that has allocation on the stack, anytime you pass it in a function call, anytime you pass it by ownership rather than the borrowing it out as a reference. The whole thing that's on the stack needs to be copied over. And so you might start to see MEM copies show up just because your stack frames are getting pretty large, whereas with the vector it's always just three fields. That also means that if you put a small vec into a Struct, the size of that struct is going to grow by the stack size of that small vec. It makes things larger. You could implement iterator for vec by defining next as remove 0, but it would be bad. Yeah, you don't want to do that.
00:29:20.151 - 00:30:19.195, Speaker A: The equivalent you have is dot drain. So there's a drain method on vectors and what drain does is you pass it a range and the range can be open ended, meaning remove every or drain everything in the vector and that gives you back a type that is an iterator that will remove all the items from the vector that it walks. But there's no sensible way in which you could implement iterator for vector for vector itself. Okay, let's move on to the next collection, which is vecdqueue or vecdec. I don't actually know whether it's pronounced vec, DEC or vecdq, but it doesn't really matter. So the description for this type is a double ended queue implemented with a growable ring buffer. And if you don't know what all these words means, I wouldn't blame you.
00:30:19.195 - 00:30:59.045, Speaker A: It's actually a very, very simple data structure. It is a ring buffer and a ring buffer is a vector. And let me maybe draw this, that might be useful. Okay, so for a vector, what we have is a memory allocation zero, this is the capacity. And then we have at some point in the middle we have the length. Right? So everything that's in here is. Oops.
00:30:59.045 - 00:31:47.875, Speaker A: Everything that's in here is filled and everything that's beyond there is uninitialized memory. Great, okay, so what then is a vec dequeue? Well, a vec dequeue or a ring buffer is similar. It's a contiguous memory allocation, but it has two pointers. It has a start and it has an end. And anything that's between start and end is considered initialized and anything that's outside start and end is not. And the reason it's called a ring buffer is because let's imagine that you kept pushing onto end here. So when you push onto end, what happens is that the start stays in the same place.
00:31:47.875 - 00:32:40.735, Speaker A: And then as you push onto end, it might go to the end of the capacity and then it will start from the beginning again and then end might end up being over here. And so it's still now the case that the start, oops, the start is over here, but the end now wraps around so it forms a ring. And the way that you know that the ring buffer is full is when end is Equal to start. There's an off by one here. But when the end reaches start, that means that your length has reached your capacity. And so if you try to push when end equals start, then you do the double the capacity, take all the items and push them onto the new, or copy them over to the new allocation. And then now you have twice the capacity, you can push more things.
00:32:40.735 - 00:33:32.281, Speaker A: So that's a ring buffer. That's the basic premise of how it works. Now, the reason why ring buffers are nice is because they can act both as a stack and as a queue. So if you remember from the computer science, or just in general in other parts of life, a stack is the thing that we usually draw this way, where a new thing that you add that you push onto the stack goes to the top. So when you push, it goes onto the top of the stack. And then the other operation you have on a stack is popular, which removes the top of the stack. So stack is often known as a lifo.
00:33:32.281 - 00:34:18.934, Speaker A: So it's a last in, first out. So the last thing that was pushed is the first thing to be popped. So that's a stack. And a queue is one where, oops, we often draw it this way and then when we push, it goes on to this end and people draw this in both directions, it doesn't really matter. So we push things onto one end, but we pop things off the other end. And so this then is usually called a. So a first in, first out.
00:34:18.934 - 00:34:55.965, Speaker A: The first thing that was pushed is going to be the first thing that is popped. So stack is last in, first out. A queue is first in, first out. So the thing that's been in the queue the longest gets to be the next thing to be pushed off. Yes. Hi, cats, do you want to come in here? Hello. So these two data structures are both useful for a lot of different things, for algorithms or just in general for keeping track of things like sending messages over channel.
00:34:55.965 - 00:35:25.515, Speaker A: That's a queue. The stack is used for things like function calls, where when you call a function, you push onto the stack the frame for that new function call, and when that function returns, you pop from the stack. So these are very versatile data structures and they can both be implemented using a VEC DQ or a VEC deck. Right. So with the VEC deck, if you wanted to operate with a queue and. Well, actually, let's go back and look at the data types involved. So a VEC deck has the following methods.
00:35:25.515 - 00:36:17.045, Speaker A: It has pop front, pop back, push front, push back. Now, you could do this on a vector Too, you can insert something at the front. The problem with a vector that starts at zero is that if you stick a thing at the beginning of a vector, the way you have to do that is take all the elements of the vector, shift them over by one to make room, and then stick the item in, right, which means a giant MEM copy to move all the things. Whereas if you have a. If you have a vector, then a push to the front just means decrementing start and then adding the element there. If you want to push something to the end, it means increment end and then push the thing there. And so pushing onto either end is a cheap operation.
00:36:17.045 - 00:36:47.135, Speaker A: You never need to shift anything around. And similarly, you can remove from either end on a vector. It's very efficient to remove from the end because you just remove that item and decrease the length by one. But remember, from the beginning is really costly because you have to shift all the elements over. Whereas with a vectec, if you want to remove something from the end, then it's easy. You remove the item and you decrement end by one. And if you were to remove it from the start, you remove the item and then you increment start by one, and it all just kind of works.
00:36:47.135 - 00:37:36.035, Speaker A: So vectech is a really versatile data structure, and I like that it's in the standard library. I use it quite often, as that might make you Wonder, well, if VECs are so magical, then why don't we use them everywhere? And there are a couple of reasons for that. One of them is that they are a little bit more. A little bit less efficient. And the biggest reason why they're less efficient is if you look at this bottom case here. So this case where the ring buffer has grown around, then imagine that you are, for example, iterating over these elements, right? So you have a. You're just like doing a loop where you're starting to walk here and you call like iterator or something, and you're walking.
00:37:36.035 - 00:38:09.235, Speaker A: So the way that the hardware will work is in general, as you're walking, any kind of memory, if it detects that you're walking something like one index at a time, what your CPU will do is like read ahead. So when you're here, the CPU might load in the memory that's over here. And so when you get here, it's going to load in the memory that's over here. And it's never going to use this part of the memory because it's outside the bounds. It doesn't know that you're going to read this memory next. So it's sort of defragmented in a sense. And so you lose a little bit of efficiency this way, which can matter a lot for especially typed loops.
00:38:09.235 - 00:38:57.999, Speaker A: The other reason why it can be less efficient is because computing the index for any given access, like if I say I want element number four, then element number four is going to be start plus four. But it's not just start plus four, because imagine that here. So imagine I have a vector over here, and imagine that for whatever reason start is over here, which, which can easily happen, right? I, I push a bunch of things and then I pop from the front. So start sort of slides towards the end. So if start is at the end, start +4 would be over here somewhere. And that obviously won't work. And so you have to have this decently complicated logic for determining.
00:38:57.999 - 00:39:44.793, Speaker A: If I give you I want index four to compute, well, where in the allocation does indent item number four live? And so this, there's this overhead to every access to a VEC deck that you don't have with a vector because of its more convoluted semantics. The other downside, like there are a couple of downsides. The other downside to a VECTEC is you can't turn it into a slice because the memory of a VEC deck is not necessarily contiguous. So if you have a slice, the assumption is that it's a contiguous piece of memory. That's what a slice is. But for a VEC deck, that's not necessarily true. If I want to take a slice of, let's say, this one right here, then the slice would start here and end here.
00:39:44.793 - 00:40:33.645, Speaker A: And that is not a contiguous piece of memory. And so as a result, you cannot take a VEC deck and turn it into a slice, so it does not implement deref to slice, which means that there's a bunch of other methods that you don't get access to, like binary search, for example, which is implemented not on vector but on slice. But a VEC does not deref to slice, so you don't get access to that. Now, there are ways to take a VEC deck and try to turn it into something that's easier to work with. So, for example, you'll see that there is make contiguous. So what make contiguous does is, as its name implies, it takes the vector and it tries to, or it doesn't try to, it does. It shifts the vector such that start is zero.
00:40:33.645 - 00:41:32.187, Speaker A: So what VECTEC will do, if you had a situation like what we had Here where the end is wrapped around is it would take start and it would do this in a different color maybe it would take start and it would move start over here and it would take the end and it would move the end over to the end of wherever this ends up being right. So this is all going to shift left and then this end is going to be copied onto the end. This is a fairly convoluted operation because if you thought about it just naively, if I took and MEM copied all of the things following start to the start of the vector, I'd be overriding the stuff that is now in the end of the vector or in the VEC deck, right? So I can't do that. So you have to do a bunch of sneaky stuff to make this copying safe. But the standard library does that for you in this make contiguous operation. But it does a lot of MEM copying. It's a fairly slow operation.
00:41:32.187 - 00:42:20.975, Speaker A: But the advantage is that after you do a make contiguous, you know that start is zero and you know that the end doesn't wrap around. And as a result, make contiguous can return you a mutable reference to a slice, because at that point in time it is contiguous. The moment you have, you know, after you've called make contiguous, if you drop this reference, then now that mutable borrow of self is no longer in effect. And so at this point, anything can happen. Someone might say, pop from the front or push so that it wraps around. And so you might have to call make and take again if you wanted to be able to do anything else. The other way that you can get out slices is where is it is using the ask slices and ask mute slices methods.
00:42:20.975 - 00:43:10.595, Speaker A: So what these will do is give you two slices, one slice of from the start to the end of the allocation, and one slice that is the from the start of the slice. No, from the. Actually, I think the way this is implemented is, yeah, it's from the start until either end or the end of the allocation, whichever comes first. And then the second slice is if it wrapped around the slice from the start of the allocation until the end pointer. And so this will tell you then like whether you're currently wrapping around. And it will give you a slice to each one. And you could do things like, well, if one of them is empty, then I know all the elements are in the other, and now I have a contiguous slice.
00:43:10.595 - 00:43:52.903, Speaker A: Let's see. Oh, VEC deck uses head plus length, not start plus end. That's fine. I mean, they're essentially equivalent. Can you Chain the two iterators from as slices. You can, but you don't need to. So even though a VEC tag is not contiguous, it does implement into iterator because it can easily do iteration for you.
00:43:52.903 - 00:44:30.969, Speaker A: It's just that it cannot be turned into a slice specifically. And it has the other things we talked about, like drain, it has retain as well, I think it has truncate, it has reserves. So it generally looks and feels a lot like a vector, just not in the ways in which it. It can't. One of the downsides of it not dereferencing to slices is that there are a decent amount of methods that are provided on slices that needed to be manually reimplemented for VEC deck because it just doesn't work the same when you don't have contiguous memory. So binary search, for example. I forget whether they've implemented it here.
00:44:30.969 - 00:44:58.515, Speaker A: Yeah. So this implementation of binary search on VEC deck and it doesn't get to use the same one that's implemented on slices. That's why it was added in a much later version. And it just is a separate binary search by. It's just a separate implementation from the slices. So it basically constructs the two slices, does binary search on both of them and then. Or does binary search on the one where the item has to be.
00:44:58.515 - 00:45:50.717, Speaker A: Because it has to deal with the fact that there are two slices here. The other thing that's kind of interesting that you have and you have this on vector 2 is rotate left and rotate right. So what rotate left and rotate right does is it will take rotateleft, will shift all items left wrapping around the element that's on the leftmost side and rotate right will do the opposite. And I think that's all I really wanted to talk about for VEC deck. It's a very, very handy data structure when you need to be able to manipulate either and of something wise to know is contiguous method or try as contiguous slice. It's a good question. I don't think there's a great answer for that.
00:45:50.717 - 00:46:38.125, Speaker A: You could add one in practice though. Like you could just call as slices and see if the second one is empty. That's going to tell you and that's also try as contiguous slice right, which is if the second one is non empty, that means that you can't get a slice. And if it is empty, the first slice is the contiguous slice without needing any unsafe code. What complexity is rotate? Rotate is o of N. It has to shift every element, but it only has to do it once. Why do Rotate, left and right have differently named arguments? That's a very good question.
00:46:38.125 - 00:47:25.805, Speaker A: I don't know. That sounds like something you could file a PR to fix. How does Vectec implement Extend? Well, I mean, it can easily implement Extend, right? If you think about the naive implementation, it's just while there is an iter, while there is an item yielded by the Iterator pass to Extend, then just push back the items that you get. So that's fairly straightforward. And it too does the same kind of optimizations like VEC does, where if the Iterator. If we know that the Iterator is going to yield a certain number of items, that we can reserve that additional capacity to make sure we have the space for it and don't need to add. Seems like Rotate should be a lot more effective compared to vec.
00:47:25.805 - 00:47:59.945, Speaker A: No, it's not more effective compared to vec. And the reason why it's not is because the problem with ROTATE is that you want this item to now be over here, and you can't do that just by moving start and end. So you could move start to. Oh, actually, no, you're right. You can do that because you make the start be here, you push it to the end, and then you make the end to be here. So you're right, it is a lot more efficient than Fervik. You're totally right.
00:47:59.945 - 00:48:27.843, Speaker A: Why does make contiguous ensure start is a storage index of zero. Couldn't it just make sure end is greater than start? Ooh, it's a good question whether it does that optimization. It might. Let's see. Make contiguous. I think I wrote part of this so I should remember. Yeah, so if it is already contiguous, then it just returns the mutable slice.
00:48:27.843 - 00:49:09.005, Speaker A: So it does do that. It doesn't guarantee that it starts as zero. What do you do if you have to prepare a buff array which has a specific alignment, such as Systems programming with 16 bytes of alignment? So for alignment you would have to do that on the data structure that you're storing. The vector itself is not going to enforce that alignment for you. I did the original implementation of the optimized VEC deck EXTEND implementation. It's really nice to read. Great, let's take a look.
00:49:09.005 - 00:49:36.215, Speaker A: Extend for extend. Let's go look. Extend implementation. Yeah, but it's all turned into SPEC EXTEND now, and SPEC Extend is more annoying to find. Spec Extend is like a specialized extend is what this stands for. So the idea is that you want to specialize the implementation of Extend to exactly which type the Iterator is so that you can be smarter about. If the iterator came from a vector, for example, then you know its size.
00:49:36.215 - 00:50:26.857, Speaker A: And yeah, iscontiguous does exist privately from what I understand, the standalib doesn't have additions. Does that mean that we have to stay backwards compatible forever? Yeah, the API has to stay backwards compatible. Okay, so that's VEC and vectec. Linked list is an interesting one because. So there's been some debate about whether linked list even should be in the standard library. The reason why, and part of the reason is because usually when you want to use a linked list it's because of some very specific properties about how you can work on linked list in efficient manner so you can embed the next and previous pointers into the underlying data structure that you have. For example, you can do things like intrusive linked lists.
00:50:26.857 - 00:51:26.265, Speaker A: And it's not entirely clear that a linked list from the standard library adds a huge amount of value. That said, a linked list is especially a doubly linked list can be really annoying to implement correctly. In Rust, there's a great article called Learning Rust with entirely too many linked lists that goes into some of the details of just how hard it is to get the borrow checker to work out right and not make weird mistakes about things like mutable iterators over linked lists. And you'll see too in the documentation for linked lists it says it's almost always better to use VEC or VEC DEC because array based containers are generally faster, more memory efficient to make better use of the CPU cache. So that's all entirely true. The main benefit that you get from linked lists is that it is fairly easy to unlink them in the middle and to link them back together later on. That is the main benefit you get from a linked list.
00:51:26.265 - 00:52:37.895, Speaker A: There are a couple of others about like the amount of memory it uses doesn't grow as capacity grows, but grows by element. So there are some other things too, but in general the really nice thing about linked lists is because you just have I'll draw it because it's easier to do that way. So when you have a linked list, the way a linked list works is you have a bunch of nodes that represent the elements that you're storing and they are linked together like this. And in a doubly linked list the links goes both ways so that you can easily iterate both backwards and forwards. And the nice things that you can do with a linked list is let's say I wanted to insert an element right here, then What I can do is I can just sever these two, I can add my new node and then I can update the next and previous pointers here and the next and previous pointers here. And notice that I didn't have to touch this part of the graph and I didn't have to touch the list and I didn't have to touch this. So here there could be like a million other nodes, and over here there could be a million other nodes.
00:52:37.895 - 00:53:25.003, Speaker A: But the operation of doing an insert at an arbitrary point or need a removal at an arbitrary point is order of one. So it's very cheap to do. And so we can see this if we go back to the documentation for collections, you know, linked lists. Here append is O of 1 because there's never a reallocation. Insert and remove is min of I and n minus I. Right. So, okay, so the reason for this complexity is because this gets back to one of the downsides.
00:53:25.003 - 00:54:46.375, Speaker A: In order for us to figure out, in order for us to do this removal, we need to find this node and this node. And the only way to find that node in this node, if this is like index I, is to walk all the links all the way up to I. So if we want to remove I, we have to walk all the way to I first to find that node so that we can update the next and previous pointers. And so it's not as though this is like just inserts and removals that arbitrary points is, are free. It is once you know where you are in the list, then a removal or addition or insertion is very cheap. And so that's often why the way the linked lists end up being used is if you are in a space where you have already walked the list, or you're already in the process of walking the list and then, or you are past a pointer to an item that you want to do something with or that you might want to remove, then you might want a linked list where you, you're able to do these like little tweaks in the middle of the data structure or in the middle of the sequence without having to do these expensive shift operations you have to do with vectors. Does it make sense to make a Linked list of T250 to avoid reallocation of the vector? No, you very rarely want a linked list.
00:54:46.375 - 00:55:52.495, Speaker A: It is only if you have this very specific property where you already know where in the linked list you want to operate and the operation you want to do is an insertion or a removal. The other thing that's nice here is like For a linked list, it doesn't actually matter whether you're inserting one or many. So you can take an existing linked list, so this is already doubly linked, and you could take this whole list and splice it into an existing linked list and you still only have to update this pointer to point here, this pointer to point here, this pointer to point here, and this pointer to point here. So splicing in a full linked list is still only order one. So it's still a constant time operation. But again, assuming that you already know where you want to splice in. And so if you look at the implementation of linked list, it tends to implement like the reason why it's unclear why it's in the standard library is because it has these operations like push front, push back.
00:55:52.495 - 00:56:36.961, Speaker A: And the problem is that that's not usually why you would want to use linked list. The reason you want to use one is so that you have access to these very low level details like manipulating the next previous pointer. But those pointers are crucial to maintaining the integrity of the linked list. And so in the standard library, you don't get access to the fields like the next and previous pointer. Those are private fields, but they are the way in which you can be efficient about your use of a linked list. And so this makes it unclear when you would ever use one of these in your standard library from the standard library instead of implementing it yourself, because the only reason to use it is to make use of the next and previous pointer manipulation directly yourself. And you can't do that for the standard library.
00:56:36.961 - 00:57:29.565, Speaker A: Now, there is some work on this in nightly using these cursor types. So the idea is that you start by getting a cursor to either the front or the back of the linked list. And when you have one of these cursors, the cursor allows you to move to the next node, move to the previous node, peak at the next and peak of the previous. If we look at cursor mute, it allows you to do insert after, insert before, splice after, splice before, split before, split after. So the idea is that instead of walking by index, you walk using this cursor type and then you use the cursor to do this kind of splicing or insertion and removal, but it is nightly only, and I think it's been nightly only for quite some time. Yeah, 20, 19. And so there's just not that much call for this.
00:57:29.565 - 00:58:08.235, Speaker A: You just do it yourself instead very often. And so as a result, this type is relatively underused in the Standard library. Okay, great. Yeah, So a cursor here is like a node handle. That's a good way to think about it. It has a handle to a given node in the linked list, which you can then do the sort of standard linked list operations on. And yeah, it's a doubly linked list, so you can move in either direction given a cursor.
00:58:08.235 - 00:59:00.875, Speaker A: That's also why the complexity here is the minimum of I and n -1 of I and n minus I mean is because if you say I want item number 40, then the implementation will look at, well, how many items are there in the linked list? And if there are, you know, 50 elements in the list, it'll walk from the back instead of from the front. So it'll be n minus I operations to get to the ith element. If there are 40, if you're asking for index 40 and there are 100 elements, it'll walk from the front instead. And that way it's order I instead of n minus I. How do you turn a linked list into a slice? You can't. So a linked list is just not a contiguous representation of memory at all. Every node is its own allocation, which is one of the reasons why they say it's less memory efficient than these other implementations.
00:59:00.875 - 00:59:52.853, Speaker A: So the way you would have to do this if you wanted to turn it into a slice is you would have to iterate over the linked list and collect into something like a vector, and then from the vector you can get a slice. All right, so those are the sequence types vec, DEC and linked list. Let's now talk about maps and sets. And the reason I want to talk about them together is because I really want to talk about these by column, not by row. So I want to talk about hash map and hash set and B treemap and B tree set as two separate things. And the reason why this is useful is in part, if we go look at hashset and B tree set, we look at the source. Really so many indirections.
00:59:52.853 - 01:00:27.433, Speaker A: Okay, the B tree one will show it. So for B tree set, the definition of a B tree set is a B tree map where the key is the set type and the value is just the empty value. Like think of it as a unit struct. It is just the value doesn't matter. And if you think about this, makes sense. So if you have a map, then the keys in the map are unique, which means that the keys in the map are a set. And so the way that you implement a set on top of a map type is you just have the keys be the values of the set and then the values be empty.
01:00:27.433 - 01:01:09.791, Speaker A: And so that way to figure out whether something is in the set or not. Right? Let's say you want a set of U8s. So what we're going to say is that is a map from U8 to nothing. The way that you check If a given U8 is in the set by using the map is, you know, the U84 is in the set if it is a key in the map, and if it's not a key in the map, then it's not in the set. To insert for into the set, you insert it as a key into the map. So that contains key check checks out later on. So that's the implementation that happens for B treemap to B tree set or B tree set to B tree map.
01:01:09.791 - 01:01:38.839, Speaker A: And same for hash set to hashmap. And you can do a couple of optimizations here, but in general, it's just all of the methods that you see on something like a B tree set is really just forwarding into the appropriate calls on the underlying map. So if you look at the implementation of difference, for example, difference is awful. Let's do intersection. That's even worse. Let's do union. Maybe that's unhelpful.
01:01:38.839 - 01:02:21.241, Speaker A: All right, fine. Let's do contains. So you see whether a set contains a given value is just whether the map contains the key of that value. So it's a very straightforward mapping. And then for some of them, like, you know, if you want to implement let's say intersection, what you have to do right, is walk all the items of self, all the keys of the map in self, and then for each key in self, see whether that key in self exists in the key set of the map inside of the other set. So you loop all your own keys and you check whether those keys are contained in the map of the other set. Sorry for intersection and then for difference, you know, the algorithm changes.
01:02:21.241 - 01:02:59.803, Speaker A: But the idea is the same, that you do this on the key sets of the different maps. Does that difference make sense? So it doesn't really cost more space. The values are empty, it just means that the values are not really used, but the values are going to be zero size types. So it shouldn't really make a difference. It depends a little bit on the underlying data structure. It's not always free. But for these, the mapping is mostly irrelevant.
01:02:59.803 - 01:04:11.001, Speaker A: It ends up being almost as efficient as a sort of native implementation of a set, because the zero size types sort of get eliminated the other thing is you get the same properties as you get for the key material of the underlying map type. So for hash set, it requires that the type T implements hash because hashmap's key type relies on that type. Implementing hash for B tree set, it's the same thing. It requires that the type T implements ordering ord, because the B treemap key type requires ord and as a result you also get the resulting properties. So a B tree set is indeed ordered because the keys of a B tree map are ordered, and that's what the set maps to. Are B tree sets ordered? Yes, B tree sets are ordered. Why use a set over a vector? The reason to use a set over a vector is because you want to eliminate duplicates.
01:04:11.001 - 01:04:51.855, Speaker A: Usually that's the main property of a set is that it is a collection where you know that all of the items are unique. And it's very efficient to check whether an item is already in the set. For a vector, that's not as efficient. Right. If you used a vector as a set, the way that you would have to look for whether a given item is present in the set is you would have to walk the vector and see if any of the items are equal. Or you could keep, you know, a sorted vector and then do a binary search over the vector, but then it's still log of n compared to a hashmap, which is lookups are order one ish. It's not actually order one, but it's kind of close enough.
01:04:51.855 - 01:05:19.335, Speaker A: I'm going to talk about hashmaps versus B tree maps or hash versus B tree in general. Okay, so now we can sort of ignore the set types. Although. Let's see. What do I want to talk about for sets? Is there anything specifically about. Or anything about sets in general? I want to talk about. It implements the standard set operations.
01:05:19.335 - 01:06:03.015, Speaker A: You know, intersection, difference, symmetric difference, union disjoint is superset, is subset. Anything else. Sets are pretty straightforward. The main things to know about is extend for a set will obviously ignore duplicates because that's the property of sets. Sets. Also implement bit and bit or bit xor, which allow you to do essentially use the sort of bitwise operators like, you know, a pipe for a bitwise or. Or an amp, a single ampersand for bitwise and in order to take unions of sets or intersection of sets.
01:06:03.015 - 01:07:12.175, Speaker A: So if we look at bit and for example, so if you have two B tree sets and you do a bitwise and between the two sets, then what you get is the intersection of the sets this is equivalent to calling intersection, you see right here. And similarly for bit or it's the same as calling the union and xor is the symmetric difference. I think that's all sets are pretty straightforward really. Is this set as fast as a vector? It's different. So a set is faster than a vector if what you want from the vector is a set. And then too it depends on the kind of operations and workload you're going to do to that set. But in general, the set types are going to be better for sets than a general vector is going to be great.
01:07:12.175 - 01:07:56.855, Speaker A: Any chance of a native try data structure? I don't think we're going to get tries in the standard library. I think the standard library is actually not going to grow its set of collections probably ever again because it's just so much better for it to be in a separate crate. There would have to be a pretty compelling reason for a data structure to be brought in here. It would have to be used just like everywhere. Again, linked list is unclear whether it should be here, which is considered a fairly fundamental data structure because the sort of limitations of the standard library is such that you often want to do it. Linked list in particular, you kind of want to do yourself. Okay, so now let's talk about the difference between hashmap and B treemap.
01:07:56.855 - 01:08:37.234, Speaker A: So hashmaps and B treemaps both implement the map abstraction. So the map abstraction is you have keys and every key maps to a value and you don't have duplicate keys, hence why they work for sets. And so if we scroll down here and look at the complexities, you see that hash maps look pretty attractive. Gets are constant time, inserts are constant time, removals are constant time. You don't get range and you don't get append. B tree maps, all of these are log N except for append, which is n plus m. And this might be.
01:08:37.234 - 01:09:19.976, Speaker A: If you're not used to complexities, then these are weird. But what you might see is that for hash maps, these have caveats to them, right? So there's this little tilde and there's also this little star. And it's because hash maps are a little weird. This tilde means amortized. So what that means is if you did lots of these, then in expectation or in average, they're all going to be constant time. The reason why it's not actually constant time is primarily because of resizes. So if you, well, forget it's a little bit different, forget it's because of linear probing, but let's do inserts.
01:09:19.976 - 01:09:58.423, Speaker A: So inserts, when you insert an item into a map, a hash map, then the hash map has a limited capacity because it's backed by buckets. I'll talk a little bit about the internal data structure of a hashmap in a second. There's also a separate video on that that I'll link up here sometime. But it's basically backed by a vector. And so the vector has a limited capacity. When the map exceeds the capacity of the vector, it needs to double the size of the vector, copy all the elements over, and then do the insert into there. And so it is amortized order 1, because the resizes happen every, you know, every 2x.
01:09:58.423 - 01:10:37.601, Speaker A: And so as a result, they happen less and less frequently. And so if you do lots of inserts, you'll generally experience the insertion being order one. And because of the way that the resizes work, you can think of when you do a resize that cost, think of it as being spread over all the insertions that didn't have to do a resize, and therefore they're kind of all order one. That's what amortized means in this context. And the til. So that's what the star is, that's the resizing amortization. The tilde amortization here is because of something called linear probing.
01:10:37.601 - 01:10:54.777, Speaker A: In order to understand linear probing, we need to talk about how hash maps work. So let's do that real quick. This will be a data structure refresher. Okay, so what is a hash map? A hash map. Oh, that's an awful color. Let's do this. A hash map.
01:10:54.777 - 01:11:23.985, Speaker A: Wow. That was supposed to be a straight line. That was also supposed to be a straight line. Okay, so a hash map is really just a vector. And usually draw the vector down, you don't have to. And so this is zero, and this is n or the capacity. And you know, when you want to insert, let's say you do an insert and let's use numerical values for now, just because they're easier.
01:11:23.985 - 01:12:06.549, Speaker A: Insert seven with some value foo. What happens? Well, you take seven and you run it through a hash. And what a hash does is it'll take the key and it will hash it, which turns it into an integer and turning an arbitrary thing into an integer, that depends on the size of the integer. But it necessarily means that you're reducing the space of possible values. So multiple values might map, or multiple keys might map to the same hash. We'll get back to that in a second. But for now, let's just assume that a hash is a way to take an arbitrary data type value and turn it into a number.
01:12:06.549 - 01:12:43.525, Speaker A: And so what comes out of the hash is some number. And in the case where the input type is numbers is not guaranteed to be the same number. But let's just assume for now that it is. So seven gets hashed, turns into seven. And then what a hash map will do is it will take the hash and use that as an index into the vectors 0, 1, 2, 3, 3, 4, 5, 6, 7. So this key is going to go into this slot and a slot usually has sort of storage space for the key and storage space for the value. And so the key slot is going to get seven in it and the value is going to get this foo in there.
01:12:43.525 - 01:13:16.249, Speaker A: And then when someone does a lookup, right? So if someone does a get of seven, then the same thing is going to happen. The seven is going to go through hash out of that comes in this case seven again. And that's going to tell it, oh, go look at this bucket. And then it's going to take the value out of there and return that back to the get. That's a very, very basics of a, of a hash map. Okay, so very roughly, this is what happening now. If you get a.
01:13:16.249 - 01:14:00.133, Speaker A: And then over time, you know, if you insert lots of values, you're going to start to fill up the vector and at some point it's going to be full. And when it's full, then you resize the vector. So you now have twice as many buckets and then you can add more elements in. So the next question then becomes, well, what if someone now does insert 8 and say bar? So the key here is different, but let's say that eight, when you put it through this hash comes out. Also a seven can happen, right? You're reducing the space here. Imagine the keys were strings instead of numbers. There are an infinite number of strings, but there are only so many U32s or U64s.
01:14:00.133 - 01:14:35.271, Speaker A: So you are going to have collisions. Well, now that is going to hit the same bucket as what the key seven hit. It's going to hit bucket number seven. And so as a result, when it looks into this bucket, it sees, oh, this is already occupied, it already has a key and that key is not equal. The existing key is not equal to 8. And so I'm not allowed to use this bucket. So when you run into these kinds of hash collisions, then the hash map implementation has a choice to make.
01:14:35.271 - 01:15:07.461, Speaker A: There are some implementations that will actually keep a. Every bucket is like a linked list so you actually can store multiple items inside of each bucket. This has some upsides, which is you don't. You always know the bucket and then you just walk the linked list. The downside is now you need a linked list, which means memory allocation, because you need every item in the linked list is a separate heap allocation. The other thing it means is that now you end up with this, this linear walk. So let's say that I was looking for key eight.
01:15:07.461 - 01:15:32.371, Speaker A: Well, now I need to walk this linked list all the way out to the end potentially to find key eight. And if I have to do an insert, then I have to walk all of them if the key is already not there. And so the complexity ends up worse. Like in general it tends to behave less well. And so that is one option that's not what the Rust standard library does. It does not have a linked collision handling. Instead it uses something called linear probing.
01:15:32.371 - 01:16:14.625, Speaker A: And there are multiple different kinds of implementations of linear probing as well. There's also quadratic probing and simd. And I'm going to sort of ignore some of the details here because they're not really relevant. The basic idea for this probing approach is that you actually do multiple steps of hashing. So if you find that the first bucket that you were after is already taken, then what you will do is you hash and the exact mechanisms are going to vary. But you can do things like, well, if my first bucket was taken, then what I'll do is I'll do a hash of this and this. And so now that's going to give me some other bucket number like say six.
01:16:14.625 - 01:16:51.751, Speaker A: And so then I'm going to go look at number six instead. And here, this is empty. And so I can take this slot and this still works for gets, right? So if you do, if someone now does a get of 8, right, then they're going to hash 8 through hash. That was supposed to say hash, go to seven, it's going to look in seven and see, oh, but the key that's stored there is the key 7, not the key 8. And so then it's going to do this hash of these two together, you know, through hash. And that's going to go to. Now bucket number six, it goes in bucket number six, finds a key eight is in there.
01:16:51.751 - 01:17:33.573, Speaker A: And so as a result returns that value. And so what you're going to end up with when you have this kind of probing is that over time your keys are going to be a little bit further away. So they're not going to be O of one, right? Because they might need. You might need to do this extra hash multiple times. It doesn't have to be a hash or other ways to do it too, but you might have to look in multiple buckets before you find the key you're actually after. Right? And so it's not quite order one, because it's not going to be in the first bucket you look at, but you're going to find it pretty quickly. It also depends on the nature of your hash function, the nature of your keys, how many of them hash to the same value.
01:17:33.573 - 01:18:13.775, Speaker A: So this is definitely tricky. It's actually pretty hard to analyze the cost of a hash map because it depends on so many things like the hash function. As you'll see, the hash also gets invoked a lot of times. So the hashing function needs to be pretty fast. And the standard library ships with a hashing implementation that is decently fast, but definitely not among the fastest you can find out there. So you can look up a bunch of crates that provide faster hash functions, which then significantly improve the performance of your hash maps. So, let's see, there were some questions.
01:18:13.775 - 01:19:22.365, Speaker A: When you do get seven, when you do a get seven, how does it know which bucket to look in without comparing to the keys in all the buckets? So the way it does so for the Get 7, right, it has the key, it runs the key through the hash, and that tells it bucket seven. So it looks in bucket seven, and in bucket seven it finds that the key slot indeed contains the key value 7. I should have used strings here. And therefore it knows it's on the right bucket. Why insert, though? Isn't changing a value the same operation as inserting? Yes, if you want to update a value in a map, it's the same thing as an insert. You can, you can do slightly better than that, but you can think of as updating a value is sort of equivalent to an insert. But you can also do, you know, in Rust, you can do a get mute of a key and what that will give you back is a mutable reference to the value at that key.
01:19:22.365 - 01:20:15.429, Speaker A: How many buckets does a hashmap have and how many values can each bucket store? So each. In this kind of implementation, each bucket can only store one value, like one key value pair. And the number of buckets follows a similar kind of rules to vectors, but the minimum capacity is usually larger. So I forget what the default size is. I think it's something like 64 buckets or 128 buckets is like the thing you get when you do hashmap new. If you do hashmap with capacity, it has that kind of constructor too, then it'll allocate way more buckets for you to begin with, or however many buckets you say, and then it'll grow the number of buckets by two times every time it determines that it needs to resize. You'll also find with hash maps that the fewer buckets you have, the more collisions you're going to have.
01:20:15.429 - 01:21:18.779, Speaker A: So if you try to fill a hash map, so you try to insert like 90 items in a hundred bucket map, your performance is going to be worse than if you have 50 items in 100 bucket map just because of the collisions which lead to this kind of probing. Why doesn't the standard library use a faster hash implementation in the first place? There are a couple of reasons for this. The primary one is that the implementation in the standard library is a cryptographic hash. So they try to make it really hard to predict what the hash output of a given key is going to be. And the reason they do this is because imagine that you are running a web service of some kind, and you're using something that comes in from the user's request as your key. Well, that when you hash the value, it turns into the bucket key. And if the user controls the key, and they can easily determine the hash function or determine what the output of the hash is going to be, then what.
01:21:18.779 - 01:22:19.289, Speaker A: What a malicious user can do is send you lots of requests with different keys that all hash to the same value. And as a result, they can basically do a sort of denial of service attack to you, where they're forcing your hash map into the worst possible performance by filling all the buckets because of all the collisions that they, that they generate. So what the standard library does in its default implementation is that when you create a new map, it'll generate essentially a random number that it's going to use to seed all the hashes for access to that map. So an attacker, even if they know the hashing algorithm, they don't know that seed that's per map. And so as a result, they can't figure out how to cause keys to generate the same hashes and cause these collisions and cause a performance collapse. And the hashing function is cryptographic hash, so it's very hard to reverse. So even if a user learns what hash they had, they can't then use that to determine what the seed was to generate additional collisions down the line.
01:22:19.289 - 01:23:16.695, Speaker A: So the standard library basically tries to be secure by default, but as a result it's also less performant by default, which is why you're able to slot in your own hashing algorithm. So if we go look at hashmap here, you'll see that there's a key, a value, and S here and S is the randomized state state that's being used. And the default one is from the standard library random state, which has this seed that it uses for hashing. And if we look down a little bit further, you'll see that the requirement for S is that it implements build hasher. And build hasher needs to be able to build a hasher. And a hasher is a thing that can hash a value. And when you hash a value it takes something that implements where do we have it? That implements hash, which is the thing that you derive for a lot of types.
01:23:16.695 - 01:24:09.615, Speaker A: So it's all a little convoluted. I'm not going to go through all the call chains here, but the basic idea is that the hash map is generic over what kind of hasher you want to use. So you can swap in your own faster version if you really want to. Yeah, and so the reason for the complexity of the reason why remove is amortized is because once you have this probing, if you remove a key, then you need to go to check anything that would be probed after you. So anything that might have collided with you and sort of shift those up. Because otherwise, let's say here that the key 7 was removed. Then a lookup of 8 would now go into the bucket that's supposed to hold eight, right? Bucket seven, find that it's empty and go and conclude well, eight isn't in the map.
01:24:09.615 - 01:25:14.485, Speaker A: And so that's why when you remove the key 7, you now need to move the key 8 into that bucket so that the probing still succeeds. So it's that convoluted too and needs to be amortized. What can downsides of these faster hashing algorithms be? Usually the downside is partially complexity and partially this kind of collision resistance. Wouldn't you need constant time per request for cryptographic security? Anyway, this isn't about cryptographic guarantees about the map itself. So it's not constant time access to the map, it's a cryptographic hash in the sense that it has pre image resistance. Like there's a. There's a bunch of properties of hashes that make them cryptographically secure, but that doesn't make the whole data structure suitable for use in cryptography necessarily.
01:25:14.485 - 01:25:49.453, Speaker A: The lookup is not non deterministic. To be clear, the lookups are deterministic. It's just that they are. If you have two different hashmap instances, then they're not going to use quite the same hashing. They're going to use the same the hashing algorithm, but not the same hashing result. They're not going to get the same hashing results, which is fine because they're in different maps. How would you know that something has collided? You would not generally know the collisions are hidden from you.
01:25:49.453 - 01:26:27.075, Speaker A: This probing happens behind the scenes. The only way you would know is that the performance is going to. If you have a collision, then lookups are going to be slightly slower for the key that has the collision. Okay, so let's then look at hashmaps now that we have a little bit of an understanding of what they do. So you see, you can construct them with a, with a capacity that tells you how basically how many buckets should there be. You can construct it with a given hasher. If you want to use a different hasher than the standard one, use the capacity and hasher.
01:26:27.075 - 01:27:07.889, Speaker A: You can iterate over all the keys. And the way this works is you just iterate over all the buckets or all the buckets that have the key slot filled with something. You can iterate over the values which is the same. You iterate over all of the buckets, and for each bucket you yield the value from the bucket. And if you just call iterate like dot ITER then what you get back is something that implements Iterator where it yields the key and the value. And it can do that because for every bucket it stores the key and the value in the bucket drain retain all the same things we've talked about. Oh, shrink to fit I haven't talked about for the other data structures either.
01:27:07.889 - 01:28:03.131, Speaker A: So this is if you imagine you do a bunch of inserts into a data structure and it grows a lot, and then you do a bunch of removals, then the capacity of the vector or vec deck or hash map is not going to shrink because it grows when you need space. But it doesn't know that you want to reduce the size of the allocation. So you can use operations like shrink to fit to, say, measure how many items are currently in the data structure and then reduce the capacity to fit only those items. And so that's also a resize. It's just a resize down instead of a resize up. So it still requires you to copy over items and that kind of stuff. What's interesting about hashmaps in particular is that when you do a Resize it might change the hash value that you have, because usually, you know, you know, imagine that the hash was like 23, then you have to do it modulo the number of buckets so that you get one of the actually valid buckets.
01:28:03.131 - 01:28:49.005, Speaker A: But that means that if you double the number of buckets, the hash function kind of changes. Because now A hash of 23 might actually be valid, might be a valid bucket. So usually when you grow a hash map, it's not just a mem copy, it also requires redistributing the keys amongst the buckets. Get key value, get many, which lets you look up many keys at once, which is likely only for now. Insert. Yeah, so the thing to remember with insert is that it is also an update. So an insert, if there's already a value for the given key, is going to replace the value for that key and it returns you back the value that was there, if there was one.
01:28:49.005 - 01:29:40.305, Speaker A: And removed, gives you the value that was removed. Remove entry is you give it a key and it gives you back both the key and the value. And the reason this might be useful is because you'll see this as indirection through borrow. We're not going to talk too much about borrow, but essentially what it means is imagine that you have a map where the key type is string capital S. You can do lookups into it using STRs like lowercase STR, even though those are not the same type. But the borrow trait here, the generic over borrow, means that you can use a borrowed version of the key that's stored in the map to do lookups. And so in this case, if you have a capital S string stored as the key, you might do a lookup with just a string literal and like an str.
01:29:40.305 - 01:30:18.365, Speaker A: And for RemoveEntry, what you get back is going to be the actual string capital S and the value. And this might be valuable because it might mean that you can reuse that allocation for something else down the line. So the other thing that I want to talk about for hash maps, and the same is the case for B tree maps, is the Entry API. This is something that I haven't seen a lot of other languages. It's not. I don't know that it's unique to Rust, but it is a very common way to work with maps in Rust, the Entry API. And I think this may be a better explanation of it up here somewhere.
01:30:18.365 - 01:31:03.779, Speaker A: The Entry API takes a key, so you call dot entry on a map, you give it a key, and what it gives you back is a reference to the bucket or to the key value pair that would go there. And if it's an. In fact, if we go look at it, it's going to be a little bit easier to explain. So what entry gives you back is one of these entry things. An entry is an enum that's either occupied or vacant. So an occupied entry, if you have one of those, then that represents an item that is in the map. And you can do things like get the key, get the value, you can replace the value, you can remove this key value pair from the map.
01:31:03.779 - 01:31:47.053, Speaker A: So it gives you a way to manipulate the item that's already in there. But if you did an entry and there was nothing there for that key, you get back the vacant variant instead. And for a vacant entry, the thing that you can do is look at the key or you can insert something into that empty slot. So the advantage of doing this is normally you'd have to write code like, you know, if let sum equals map dot, get mute and then potentially change that value. Else map.insert key and then value. And what that would do is if you, if you have a get mute and an insert, you're doing two rounds of hashing, two rounds of bucket lookups.
01:31:47.053 - 01:32:37.837, Speaker A: If you use entry, you only do one because you do the whole lookup. And then what you get back this, this vacant entry has a pointer directly to the bucket. And so now if you choose to insert something in there, you don't then need to redo all the hashing because you have a pointer directly to the bucket. And entry provides a bunch of other nice APIs like or insert, which lets you do like dot entry, some key dot or insert, which means give me a mutable reference to the value, or if there is no value, insert this one and then give me a mutable reference to that value. And so this is a really handy way to work with these collections. I wanted to see if. I think there's an example of the use of or insert and, or insert with and.
01:32:37.837 - 01:32:55.960, Speaker A: Or default as well. If you go up to the top here. Yeah. So playerstats, entryhealth or insert 100. So this is going to. If the health is already set in the map, it's going to keep whatever the current health is. But if it's not, then it's going to set it to 100.
01:32:55.960 - 01:34:02.227, Speaker A: And it can also. Here you say, you say dot entry attack or insert 100 and that gives you back a reference that you can then modify. And so what this means is if the attack modifier is already set in this map, then you're going to be modifying whatever it's currently set to. If it's not already set, you're going to insert 100 and then modify it. So this is a very nice interface for working with a value in a map that may or may not be set. Wouldn't it be easier to set it to empty but have the key of 7 still there so the 8 doesn't need to be moved? It might be easier, but then you're going to end up with a. So the proposal here is in the case where you remove the key 7 from the map, then instead of trying to figure out where the key 8 is and move it into that bucket that's now vacant, just leave that bucket that had seven in it and just keep storing the key seven in there.
01:34:02.227 - 01:34:30.457, Speaker A: So that gets still work. The reason you want to do the work to move 8 around is both to recover performance so that 8 no longer needs to do this unnecessary probing. It's partially because when you remove, you give back ownership of the key. And so the caller is going to expect that that key be dropped. And so if you keep it around, you're not going to drop it. And so, for example, memory allocation is not going to go down. And so those two reasons mean that you generally want to do this emptying.
01:34:30.457 - 01:35:26.019, Speaker A: You don't want to leave around a value or a key that's no longer actually there. Why? When iterating a hash map, it iterates also through the empty buckets with iter, it doesn't. Well, so what happens when you call it on a map is it walks the buckets, but it only yields key value pairs from the buckets that are non empty. And the reason why it has to do this is because what else would it do? How would it iterate over only the buckets that have stuff in them? There isn't really something that, you know, says in the hash map, all it really stores is a pointer to the buckets and sometimes maybe the length. But in order to also in order to be able to sort of. If someone told me to iterate over this right then and all I have is this pointer, how would I know how to sort of get to skip all these empty ones and start there, I would need to store some additional state. And this gets weird.
01:35:26.019 - 01:36:08.309, Speaker A: If like imagine this has some stuff in it and this has some stuff in it, I would need to know to go here and then I would need to know to skip this one, and then I would need to know to skip these ones and so suddenly that turns into a lot of extra bookkeeping that increases the size and complexity of the data structure and thus often reduces its performance as well. So this sort of brings us to the implementation of the standard library hashmap. So it used to be the case that the standard library hashmap was. Its implementation lived entirely in the standard library. And I forget exactly what the implementation was. I think it was linear probing. It worked okay.
01:36:08.309 - 01:36:36.007, Speaker A: It was just like a little bit of an unoptimized implementation. And then this crate called hash brown came around. And hashbrown is an implementation of a. A high performance hash map implementation called Swiss Table that came out of Google and someone basically ported that to Rust. And one of the really cool things they did was they did lots of benchmarks of this implementation. It's been very optimized. It doesn't use linear probing.
01:36:36.007 - 01:37:13.631, Speaker A: It uses like quadratic probing where it looks at multiple buckets and it can use SIMD to look at them in parallel. It's all sorts of fancy. It has lots of optimizations, Swiss Table does, and Hash Brown on top of it. And it turns out that it's just faster than the standard library hashmap in every way. And the interface is compatible with the standard library interface. So it took a while, but they actually now have replaced the entire standard library implementation of hashmap with the hash brown crate. So if you look at the source over here, you see that the implementation of hashmap in the standard library is just base, colon, colon, hashmap.
01:37:13.631 - 01:37:57.749, Speaker A: If we go up to the top here, you see that it uses hashbrown hashmap as base. So the implementation of hashmap no longer lives in the standard library. It lives in the separate crate called hashbrown. So if you go over here is this crate over here, and Amano is the person who implemented that and maintains the hash brown crate. Done a lot of other really cool work in Rust too, and it's really neat. If you go look at the implementation of hash brown, it has a lot of cool optimizations that make it just really, really, really fast. It also means that you can choose to use hash brown yourself directly rather than using what comes in the standard library.
01:37:57.749 - 01:38:39.285, Speaker A: There's not a huge amount of reason to do this. The main reason is because it comes with a different hasher. By default, it uses a hasher called a hash, which is faster than what comes out of the standard library. In general though, it's like, apart from the different in hashing, it's fairly similar to what the standard Library ends up using. And the way that this is usable in the standard library and it has supports for things like Rayon, which is nice, but one of the reasons why. Let me see if I can dig this up. It might just be over here.
01:38:39.285 - 01:39:22.385, Speaker A: Yeah, so you may still want to use this crate instead of the standard library. Since it works at environments without std, it only requires allocs. Remember how I talked about earlier that VEC and VEC deck and B treemap all live in alloc, not in std. And that's because the hashmap implementation in the standard library needs randomness, which comes from the operating system, which requires STD for the sort of collision resistance that they build in the hashbrown implementation does not require that you use the default hasher. And so it can be used in environments where you only have alloc. So it can be used in embedded systems and kernels and that kind of stuff. But otherwise the implementation is completely the same as what is in the standard library.
01:39:22.385 - 01:39:55.827, Speaker A: They're sort of drop in replaceable. The hasher is, as they say here, it's much faster, but it does not provide the same kind of hash denial service resistance. I don't think this is still true. I think the standard library now has this too by virtue of using hashbrown. But I forget whether that's the case. And you see there's a comparison here of. It was in Rust 136 is when the implementation of hashmap got replaced with hash brown.
01:39:55.827 - 01:40:36.745, Speaker A: And this is a comparison of the performance of different workloads of the old hashmap implementation and the one in hashbrown. And you see, it's just like faster across the board, often by significant amounts because it uses a smarter implementation, simd, all of that good stuff. It even supports like arena allocation and stuff, which is nice. Okay, so that's the standard library. I don't. That's a hash map implementation standard library. I don't know that there's anything else I really want to talk about there.
01:40:36.745 - 01:40:59.133, Speaker A: Right. So one of the downsides of HashMap is that it requires that the key is hashable. Not every type is hashable. Many are, but not all of them are. And it also doesn't impose any kind of ordering, as we'll see with B treemap, which means that if you iterate over a hash map, you get the keys back in random order. You don't get them back in the order that you inserted them in. You don't get them back in sorted order.
01:40:59.133 - 01:41:43.785, Speaker A: This is random Order and the order can vary every time you call iter. It's not even a stable iteration order, which is something that sometimes comes back to bite people, often a source of non determinism in their programs. Okay, do we have questions about hashmap before I move on to B tree map, how do you put the toolbar at the bottom? Oh, that's. You can look at my Firefox configuration. It's like a custom user Chrome CSS. It's in my GitHub configurations repository. Maybe I missed it.
01:41:43.785 - 01:42:26.745, Speaker A: Using hash brown is the same as the standard library hashmap, performance wise. Yes, the standard library hashmap is hash brown, but it uses the standard library hasher in the hash collision resistant one instead of the one that hash brown uses. How do you add different hash implementations if you're deriving hash, you don't and you don't have to. So if we look at the where's the hash trait there? So you'll see that there are two traits. There's hash and there's hasher. So hash is what data types implement. And what all hash does is it has this method called hash that takes a hasher.
01:42:26.745 - 01:43:10.015, Speaker A: And if we look at how to implement hasher or implementing hash, you'll see what it does is it just calls hash on the hasher that's passed in. So the hash trait is really just a way to sort of walk a data structure. And there's nothing here about like, you know, SHA one or turning things into an integer. It is just calling the hash or providing the data to the hasher. Hasher is the trait that actually defines the hashing algorithm. And so if you look at the bottom here, hasher is implemented for SIP hasher, which is the one that's the default in the standard library. And then if you look at a hash, for example, it also implements hasher.
01:43:10.015 - 01:43:58.211, Speaker A: So the hasher trait is the thing that dictates the implementation or the hashing mechanism. And the hash trait implements how to walk a data structure. Think of this as the difference in between serialize and serializer in serde, where serialize implements the way to walk the data structure and serializer implements the data format. Okay, so I think that leaves us then with the B tree map. So let's go look at B treemap. The collection source code has a lot of unsafe. Why is that? The implementation of data structures generally requires unsafe.
01:43:58.211 - 01:44:49.323, Speaker A: Because you're often doing a lot of raw pointer manipulation, for example. So it's usually tricky to write any kind of efficient collection, especially Once you want to do things like SIMD or this kind of quadratic probing, there's a bunch of complexity that you just can't express purely using safe code. And so you end up using unsafe code a lot in the standard library and in implementing collections, that's just a necessity of their implementation. Basically, the safety properties you have to guarantee in these collections is often. They're often too sophisticated for the borrow checker to check on your behalf. All right, so B tree map. So btreemap is a map implementation, just like hashmap is, right? So they both implement the map abstraction.
01:44:49.323 - 01:45:29.533, Speaker A: And the main difference between B TreeMap and HashMap in terms of their interface is that B TreeMap does not require that the key is hash. That doesn't require that it's hashable. Instead, it requires that the key is orderable. So it requires that you can put all the keys in some order. And the reason why it does this is because a B tree map fundamentally works very differently than a hash map does. So if we go down here and look at how would you do a B tree map? Well, a B tree map is. So let's start with the binary tree that many of you are probably familiar with already.
01:45:29.533 - 01:46:09.577, Speaker A: So a binary tree is one way to implement a map. The idea is that let's fill this maybe with values. So we do something like 7, 3, 1, 4, 9, 8, 12. Let's say that I here want to do a lookup for four. Let me do that in a different color. So I want to do a lookup for four. So the properties of a binary tree is that you.
01:46:09.577 - 01:46:48.701, Speaker A: And there are all sorts of guides on how binary tree works. I'm not going to go into too much detail, but the basic idea is that every node in the tree has two children. The children on the left have a lower value than the root node, and the children on the right have a higher value than the root node. So you see here for seven, all of the things that are to the left of 7 are lower than 7, all the things to the right of 7 are higher than 7. So if I want to do a lookup for 4, I go and compare it to the root of the tree, and 4 is less than 7. So therefore I go left, 4 is greater than 3, so I go right, and then I end up at a node where the value is same as what I was looking up. And so I've found my node.
01:46:48.701 - 01:47:18.465, Speaker A: And then you can imagine that instead of this just holding four, it's actually a bigger node that holds four and some value, and so I can return the value, right? So this is a. This is the way that you would do lookups in a binary tree. And so you can implement maps this way. It works totally fine. It's pretty common. Now, the downside of using a binary tree is that, well, there are a couple of things. The first one is that every circle here is an allocation.
01:47:18.465 - 01:48:01.495, Speaker A: Wow, I started well and then it went poorly. So every node here is an allocation, and so you end up with a lot of overhead because you have to allocate every single node and you have to keep for every value, right? For every value in this tree, you have to store the key, you have to store the value, you have to store the left pointer, and you have to store the right pointer. So there's a lot of overhead here. Imagine that the keys and values are like U8s or something. Then the left and right are both pointers. So you're storing, you know, no longer storing K +V, you're storing K +V + 2 times pointer size, 2 times U size times the number of elements. And so that overhead grows really, really large.
01:48:01.495 - 01:48:32.619, Speaker A: And then the. Even just following these means that if you want to look up four, you have to follow a lot of pointers. You have to follow log n pointers to. To go all the way to the bottom of the tree, which is where you might need to search in order to find a given key. So it's a really long path to go down here. And that means that a binary tree is often considered a. It has some nice performance properties, but in terms of runtime characteristics, there are a lot of downsides to them.
01:48:32.619 - 01:49:15.389, Speaker A: And so that's when we get to the abstraction of a B tree. And so a B tree is a little bit different. A B tree is one where instead of having the nodes hold one value, the nodes hold B values, hence the name of the data structure. So let's say that the root node here, so this is a B equals four tree. So here what we're going to do is keep pointers to the left or the right of each sort of collection of four things, and then another one over here. So we still construct a tree, but you see, the tree is a lot shallower. So now up here we might have, you know, seven.
01:49:15.389 - 01:49:53.695, Speaker A: Wow. Okay, fine. Let's do an actual 7, 7, 9, 10, 14. And then all of the values over here are going to be greater than 14, and all the values here are going to be less than seven. So, you know, two, three, four, and this is going to have 16 and 20 or something. And so now if I wanted to say look up the value four, what I would do is I would go in here, I would see that it is less than the lowest value in this node. So I need to go left and then I look through all of the values here and here I can use a binary search within this node and find that it's here.
01:49:53.695 - 01:50:36.707, Speaker A: So I only needed to traverse one pointer in order to find four. And so this is more efficient in terms of following, doing sort of pointer chasing. It's also more efficient in terms of allocations because now I only have to allocate this one, this one and this one. So it's fewer allocations, the overhead is less because I, you know, here I kept what, 4, 5, 6, 7, 8, 9. I had nine items, but I only have two pointers. As I'm reducing the amount of overhead that goes to just storing pointers, I still have to store, you know, the length and the capacity for each of these. And so, or actually the capacity I know is equal to 4, so I don't need to store it because it's equal to B.
01:50:36.707 - 01:51:06.029, Speaker A: So I only need to store the length. And so this ends up being a much more compact and cache friendly data structure too. It's cache friendly because the root node, for example, all these items are probably going to be kept in the data cache for this CPU because you always have to go through the route to do any lookups. And so the binary search in here is going to be a lot more efficient because it's generally in cache. Same with the nodes that are higher up in the tree. You're usually going to hit faster. The complexity of the data structure is roughly the same as a binary tree.
01:51:06.029 - 01:52:00.665, Speaker A: It's a little bit harder to insertions because you need to like imagine that I insert, imagine I want to insert a value that's going to go in here. Well, now 14 is going to be pushed out in order to make room for that item that goes in here. So they're a little bit more complicated to maintain, but tends to have a somewhat better performance profile. Log N is only if it's balanced. There are a bunch of caveats here to the data structure that I'm not going through, but basically this is the idea. And you can see here that this requires that the types are ordered because you need to be able to say that things that are greater than 14 or things that are less than 7 and all the ones in between here we know are greater than or equal to 7 and less than or equal to 14. So this all relies on items being orderable, the keys being orderable relative to each other.
01:52:00.665 - 01:52:44.793, Speaker A: There's a very, very different data structure to a hash map, right? And it is no longer roughly constant time, like amortized constant time to do lookups or insertions. Instead it's log N, where log n is the how deep the tree is, essentially. And yes, B is the same for every node. So I forget what the B is in the standard library implementation. Oh, does linear search within nodes, not binary search, which is interesting. The B where is the B. Is it defined at the top of this file? Maybe, yeah.
01:52:44.793 - 01:53:20.351, Speaker A: And there's like rules for how you split and stuff. There's all sorts of details to this that I don't want to go into too deeply. I'm just trying to see if I can find. Yes, it's a linear search, not a binary search, but it can be a binary search. They use a linear search because if B is small enough, the overhead of a linear search is actually pretty significant. I wanted to see if. Why is B not just a constant that is easy to find? All right, fine.
01:53:20.351 - 01:53:48.705, Speaker A: So this is in Alloc collections B tree map. Alloc collection B tree node. Nope. B tree maps, I think, are always maintained as balanced. So there, there's. You never have one long chain that's. That's much longer than the others.
01:53:48.705 - 01:54:10.937, Speaker A: For example, just trying to see if I can find where the damn B is. Aha. B is 6. So the rust under library B tree map is a B tree where B is 6. And that's just. They just found that 6 was the right value here. There's no way for you to customize it.
01:54:10.937 - 01:54:42.661, Speaker A: That's just what it is. And so you might wonder, well, why would I ever choose a B tree map over a hash map, given that the hash map has generally constant time lookups and insertions and such. And there are a couple of reasons. One of them is the B tree map is sorted by definition. So if you iterate over a B tree map, you always get the keys in order. This alone is nice. It means that you don't need to sort things because they're kept in a sorted manner always.
01:54:42.661 - 01:55:26.497, Speaker A: The other is if you have key types that aren't hashable, then B treemap is going to let you use those as the keys, regardless. Usually though, anything that's orderable is hashable, but the reverse is not necessarily true. Like, for example, floating point numbers are hashable, but they're not orderable, so they can be used as keys and hash maps, but not in B tree maps. So it's usually more this property of you want things to be sorted. The other thing that's nice about B tree maps is that their allocation behavior is a little nicer. So usually the overhead of a B tree map is relatively smaller than the overhead of a hash map because remember, hash map needs to keep number of buckets to keep collisions low. So usually you end up.
01:55:26.497 - 01:56:02.195, Speaker A: I forget what the saturation of hash brown is, but I think it's somewhere around like 60 to 70% to tries to keep the buckets about, let's say 70% full. In a B tree map, the nodes are generally fairly full and the tree is fairly full. So you're not wasting quite as much memory. So you know, in a hash map, if you have a thousand items, you probably have, you know, let's say 1500 buckets. So that means you have 1.5 times the memory usage of your actual data. Whereas with a B tree map that number is going to be smaller.
01:56:02.195 - 01:56:48.777, Speaker A: It's still going to be more than one. Like the factor of the overhead is going to be more than one or the required memory use is going to be a factor of more than 1. It's going to be something like 1.1 or 1.2, but it's going to be smaller. So if memory use matters a lot to you, the B tree map might actually give you slightly better allocation cost. And then the last thing that's really valuable with a B tree map, which the standard library is still lacking a little bit in this regard, is that it has a property that's somewhat similar to linked lists in that you have this cursor interface where at least in theory I can do something like look up key four and then say give me an iterator that starts from key four and then I'm going to walk right or left from there.
01:56:48.777 - 01:57:34.979, Speaker A: So like, what is the next key after 4? What is the previous key before 3? And so you can have this sort of lookup interface that's really nice. You can do things like remove the first item because they're orderable, there's a total order to them. So you know what's first. You can remove the last one and I want to see if they have added any of the. Oh, you can also append a B tree map to another because at least in theory, if I gave you a new B tree map that, you know, started from 21, so I have this whole structure over here, I can reuse a lot of the allocated nodes that are internal to the tree, I can reuse those. When I try to append this, I can't just stick it onto the right. Usually you want to rebalance it.
01:57:34.979 - 01:58:21.163, Speaker A: But I can at least reuse the allocations In a hash map, you can't really do that because it's just one giant allocation. And so if I gave you another hash map, you just take the values out and then drop my allocation. But in a B tree map, all of those six item allocations that are over in the new tree I can reuse when I grow my existing ones. Yeah. So range is the way that you can get an iterator that starts at a given value and then iterates from there or given key rather, and iterates from there or to there. You can get mutable iterators too. So you can say, give me a mutable iterator that starts at 4 and then walk all the items from 4 and upwards and that walk will actually be decently efficient.
01:58:21.163 - 01:59:10.771, Speaker A: With a hashmap, there's no equivalent to that. And similarly to how you can sort of merge B trees, you can also split a B tree. And because it gets to reuse the allocations for the split. So when you split a B tree, imagine we split it, right? Imagine we decided to split it somewhere over here, right? Then this allocation is going to go to the right map and this allocation is going to go to the left map. And then, you know, this allocation can go either way and then we're going to have to do one allocation. But that still means that if this was a giant tree, we're only doing one allocation, even though we split this giant tree and the things under there, we might not even need to rearrange at all. We might have to rotate a little bit to get things into the new slightly empty root node.
01:59:10.771 - 01:59:32.245, Speaker A: But it's a very efficient operation to split a B tree. Not true for a hash map. Lower bound, upper bound. It's just min and max indexing, ordering. Yeah. So it looks like they don't really have a. Unless this is on range mute.
01:59:32.245 - 02:00:05.259, Speaker A: Yes, range mute. You can walk in either direction. I was trying to find. I know there was a proposal for cursors for B tree maps, but I guess it's not in here yet. That's a shame. Let me see if I can dig up that tracking issue for B tree map cursors. Oh, I see.
02:00:05.259 - 02:00:44.221, Speaker A: So that's the prop proposal is the lower bound B use of this. I see. So you say give me a cursor to the B tree map starting at this value. And then let's see what you can do with a cursor. Right, so this is similar to the cursor that we saw in linked lists where you can move left, you can move right. But also you can do things like modify the values and the keys that you're walking through or actually just the values. The key, you can't change the key because if you change the key so that it should be somewhere else in the tree, your data structure is now broken.
02:00:44.221 - 02:01:43.607, Speaker A: So that's why it's unsafe. But you can modify the values as you walk through them. Okay, yeah. So you see, when we looked at the sort of complexity operations for maps, you see the range, there's no equivalent for in hashmap and similarly append, there's no equivalent for in hashmap and same thing for split. There's no meaningful way to split a hashmap because there's no ordering. Okay, so that's B treemap. Any questions about B trees before we move on, can you please comment on the unchecked versions that I see here? There's no unchecked in B tree.
02:01:43.607 - 02:02:19.945, Speaker A: There's an unchecked in as an unchecked in lower bound cursors. I think. Think. Yeah. So the unchecked here for key mute. This is again because if you're walking a B tree and if I give you a mutable reference to the key of the item you're currently at, then if you change that key, you might change like imagine you're on 4 here and I give you a mutable reference to 4 and you change it to 19. Then now the data structure is invalid because 19 is supposed to be over on the right here.
02:02:19.945 - 02:02:46.671, Speaker A: But you, but it's still over here and you just like rewrote it to 19. That's not okay. And so that's why that operation is unsafe. And the unchecked in the name is sort of implying this, that we're not going to check that you're not messing up the data structure. Hence this operation is unsafe. Curious about the number B. Why is it 6? I suppose it's a compromise.
02:02:46.671 - 02:03:09.075, Speaker A: Yeah. So the larger the number is, the more efficient the data structure is, but the more space it wastes. Right. So imagine that you, you have this be set this. Sorry. If you have this set to like 20, then you're going to have way fewer pointer chases. But your linear, linearly searching the buckets is going to be slower, which again is what Rust currently does.
02:03:09.075 - 02:03:48.805, Speaker A: And 2 you're going to end up with more space that's unused in each of these nodes. So you notice here, you know, because B is 4, you know, we're wasting this one, we're wasting this one, and we're wasting this one. So we're wasting sort of 3 capacity, if you will, by nodes that. Or slots for values that aren't there. But If B was 20, then in this map we have 1, 2, 3, we have 9 items, right? And so in this case, we would have an allocation of B. Wow. That was supposed to be a B.
02:03:48.805 - 02:04:18.175, Speaker A: B equals 20. If we had B equals 20, we would have, you know, nine items that were actually occupied, but then we would have. Over here, we would have 11 unused slots. So we're wasting a bunch more memory. And so the larger the value is, the more you waste memory, but the more efficient the data structure is. And so I think six was just like six seems like a good decision. And here I used four, so it's wasting even less, but it means that it's chasing more pointers.
02:04:18.175 - 02:04:59.327, Speaker A: B tree maps don't necessarily require more memory than hashmaps. Often the overhead is less. Why can't the list of buckets in the hashmap be appended with another list of buckets? Well, so, first of all, they're different allocations, so you can't append allocations to one another. That doesn't work. The reason it works here is because there are pointers, two separate allocations already, so you can reuse them across the data structure. But the other reason is because even if you could append the vectors, the hashes would no longer work out. Because, remember, when you take a hash of a key, you take it modulo the number of buckets.
02:04:59.327 - 02:05:57.591, Speaker A: And so if the number of buckets changes because you appended, then now the modulo changes, which means the hash changes, which means you might need to re. You might need to relocate a lot of the entries here and a lot of the entries here for the data structure to remain valid. If the complexity of merging two B tree maps is N N, is it not similar to removing elements from one hash map and merge it into the second hashmap? It's similar, but the difference is that the N plus M is not amortized for a hashmap, it would be amortized N&M and also it would require a reallocation. Whereas for the B tree maps you can reuse all the allocations, which also helps a lot for the actual performance. How do you guarantee that the key changing is correct? You don't. That's why it's unsafe. You, as the caller, have to ensure that you did not change the ordering of the key.
02:05:57.591 - 02:06:41.481, Speaker A: And the way that might happen, right, is imagine you have a key that has a bunch of. You have a key type that has a bunch of fields in it. But the implementation of ord, the OR trait for that structure only uses one of the fields. Well then if you change one of the other fields, you know you haven't changed the ordering, so therefore the operation is safe. You mentioned the B tree map is cache friendly, but it didn't see to be the case for me. Like empirically you tested it out because it's supposed to be more cache efficient because each of the B nodes, certainly towards the root of the tree, are likely going to be in the data cache of your your cpu and so that's going to be faster. I'm surprised B isn't 2 to the power N.
02:06:41.481 - 02:07:15.433, Speaker A: Wouldn't that help for cache lookups? It doesn't matter whether the B is a power of two. What matters is whether the B times the size of the data type is a power of two. But that's more likely to be the case if B is a power of two is true. I genuinely don't know why they chose six. Why can't we modify B? I imagine that you might be able to do benchmark driven optimization to find the right B value. It's interesting actually. I wonder whether we could make B tree map have a const generic B.
02:07:15.433 - 02:07:39.517, Speaker A: Now that would be six by default, but you could change. I think there are some parts of the algorithm where you code specifically to the B. But even then having a const generic should be doable. But at the time there weren't const generic. So B tree map is not generic over it these days. That might be an interesting thing that we could do. Yeah.
02:07:39.517 - 02:08:03.541, Speaker A: So. So why didn't they make it generic over const V? It's because const generic didn't exist in Rust 1.0. Are you covering probabilistic collections? No. I mean, hashmap is arguably a probabilistic collection, but. No, you're thinking of something like a Bloom filter. No, I'm only covering the ones that are in the standard library. Lookup is expected constant time for B tree map.
02:08:03.541 - 02:08:28.447, Speaker A: No, it's log n order of login or upper bound login. Because you need to. You might need to traverse all the way down to the leaves and the leaves are log n down. Can't you remove and then read with the new key and then it's safe. Oh, I mean, yeah, you can always. If you wanted to change the key, you can remove the old key and then insert it with the new key. That's fine.
02:08:28.447 - 02:09:03.673, Speaker A: But that specific API was to get a mutable reference to a key that's still in the map, and that has to be unsafe. Would this have a negative impact on compiler performance? If you made the B tree map be a const generic? It probably increases the compile time, but it shouldn't affect the runtime. Might B tree lookup be faster than hashmap lookup for smallish maps? If the hash algorithm takes a long time, yes. Small B trees can be faster than small hash maps. Sometimes you don't even want to use a B tree, you just want to use a vector. It's faster. Just walk a single vector.
02:09:03.673 - 02:09:45.645, Speaker A: But this is where like you benchmark to your actual workloads. If you use many different B's, wouldn't the compiler have to generate instructions for all the different versions? That's also true. So if we had B tree maps be const generic over B, then people would end up using multiple different Bs in different parts of their application. And as a result they would have to compile all of B tree multiple times, once for each B hashmap is more secure. That's not true. They're the same amount of secure. The hash map has hash denial of service resistance, but the B tree map doesn't need that because it doesn't have to hash.
02:09:45.645 - 02:10:11.385, Speaker A: If B was large enough, it would probably be worth it to switch to a binary search. Unclear. So that. Well, that's true for the implementation of searching within a node. Yes, if that's what you mean. When is a B tree faster than a hashmap or vec? When you measure that it is, there's not a hard rule there. It's not like when N is this much, because it depends on N depends on the size of your CPU caches.
02:10:11.385 - 02:10:50.385, Speaker A: It depends on the size of the data type that's in there. It depends on the architecture. So it's really not a straightforward answer. Okay, let's go to the very last data structure, which is the only one under miscellaneous, which is binary heap. Okay, so a binary heap is a binary heap is a heap, which hopefully you know what a heap is. But otherwise let's do a very quick overview. So, heap is a data type.
02:10:50.385 - 02:11:06.021, Speaker A: I don't know why I need to draw this. Actually, I don't. I might draw later. But a heap is A data type where you have two operations. You have insert, I think they call it insert. You have push. I don't like that they call it push.
02:11:06.021 - 02:11:46.695, Speaker A: But fine, it's a. You have push and you have popular. And unlike a stack, where the stack you push and it lands on the top and you pop and you pop off the top with a heap instead, what you do is think of it as an actual heap. And when you push something onto the heap, what actually happens is you should think of the heap as actually. I know how to describe this. Instead of using heap, I'm going to describe this as a glass of water. I guess it's going to be a square glass of water or a glass of liquids.
02:11:46.695 - 02:12:47.705, Speaker A: And you'll see why I draw all these lines. But let's imagine that these liquids. And what we're going to do is when you push something in here, what actually happens is this thing is going to sink down until it hits whatever its weight is. And then when you do a pop, what's going to happen is a pop is going to pop whatever is at the top of the glass. So the thing that is the lightest or the heaviest. Now, whether light things go to the bottom or heavy things go to the bottom, or whether high value things go to the bottom or high value things go to the top, is the difference between a max heap and a min heap. So in a max heap, the highest value things go to the top.
02:12:47.705 - 02:13:21.895, Speaker A: So a. In a max heap, then greater than goes up. In a min heap, then lower values go up. So it's kind of a priority queue. It's close enough, but essentially it is a way to chuck a bunch of values into a group. And when you pull things out of the group, you're going to get them in order. Order.
02:13:21.895 - 02:13:59.801, Speaker A: But crucially, a heap does not guarantee anything about ordering. It doesn't guarantee that you can walk them. It's not a map, it is purely a collection where the one operation you can do to get elements out is get me the max or get me the min. So that's the way to think about it, is you can push however many things you want in there. And when you do the pop operation, what that really means is get me the max or get me the min, depending on whether it's a max heap or a min heap. In the case of the rust under library, the binary heap is a max heap. And what that means.
02:13:59.801 - 02:14:41.743, Speaker A: So it is a priority queue. What it means is that when you do the pop operations, you see, you can push a bunch of values and when you do pop, what you get back is the value that is at the top of heap, and then you could do pop again, and then you get the value that is now at the top of the heap, and so on, and eventually you get none. If you iterate over a heap, it gives you the elements in random order. So there's no ordering within the heap. It is just a way for you to always be able to get whatever is the max. So there's peak, which peaks at the max, there's pop that removes the max, and you can ask for the length. And you might go, well, why wouldn't I just use a B tree set for this? Like, a B tree set gives you the same kind of properties.
02:14:41.743 - 02:15:33.361, Speaker A: You can add stuff to it, but you can also pop the highest value. And the main reason is because a binary heap is more efficiently implemented than a B tree map is, or a B tree set, because it doesn't need to retain all this internal ordering. In fact, the way a binary heap often works, I forget whether the standard library does, but I think it probably does, is it turns out you can implement a binary heap. And this is a really fun data structure to implement. I recommend you try it, if you haven't already. You can implement a binary heap using just a single vector. And it turns out that there's a really cool algorithm that lets you, like, subdivide the vector in such a way that the element that's at the front is always the max, and that when you remove the max, there's a sequence of operations that you do.
02:15:33.361 - 02:16:19.785, Speaker A: So you do a bunch of like, let's say we remove the max, there's like a bunch of swaps that you do that are dictated by some particular part of the algorithm. And after you've done those swaps, then the thing that is now the max is at the front. But if you walked the the the vector, start to finish, you get the max, and then you get the elements in seemingly random order. Essentially what it does is it flattens a binary tree inside a vector. It's very cool, but ultimately the only property it gives you is it gives you the max at the beginning of the vector, which is what the heap gives you. Now, one thing to be aware of for the binary heap is that. See if I can find it.
02:16:19.785 - 02:17:12.165, Speaker A: Where is my. I'll talk about the min heap in a second. Yeah, so the operations are push, which is order one. Roughly. The reason it's amortized is because it might need to resize the vector, grow the vector to fit the elements since amortized peaking is order one because it just looks at the front of the vector and pop is log N because when you remove an item, it needs to do all these swaps so that the new max propagates to the front of the vector. And that's log of n. And I wanted to see if they give you pop, no push.
02:17:12.165 - 02:17:43.095, Speaker A: Yeah. And so there's a cost to pushing one at a time. It turns out that if you append, it's a lot more efficient. There's all sorts of rules around this where you can be efficient about adding multiple items at once and the order in which you add them. This is all like data structures and algorithm stuff that we don't need to get into a lot. But the specific thing I wanted to talk about was duplicates. But interesting that it's not documented here.
02:17:43.095 - 02:19:08.385, Speaker A: Let's see if we go to play rustling and we use standard collections binary heap new and then do X push 2 X push 2 while at some X pop print line Z run. So this is the other thing that's a difference between a binary B tree set or B tree map and using a binary heap is that the binary heap allows duplicates. It just makes sure that whichever value has the max value gets popped first. But if multiple things evaluate to have the same ordering, like if they return ordering equal to each other, then it's just going to yield them in some arbitrary order. So it's like an unstable data structure. So here we get two twice, because there are two twos that have been pushed with a set that would not be true. So if you push two to a set twice, it would only yield the value 2 once because it eliminates duplicates.
02:19:08.385 - 02:19:57.313, Speaker A: And yeah, the binary heap implementation to turn into a vector is not super fancy like someone pointed in the chat. It's a binary tree that's written into the array line by line, depth by depth, if you will. But it's a very cool infrastructure. It's a very cool data structure to try to implement yourself. The reason why it's a binary heap, why it's called a binary heap, is because it's a binary tree represented in a vector format that happens to implement a heap. A heap is a data structure, but you can represent it using a vector, which is why it's cool. Creating a heap from a vector or iterator is asymptom, not asymptomatically.
02:19:57.313 - 02:20:32.931, Speaker A: That's not right. It's asymptotically more efficient than a loop with push operation. So you can, if you add A bunch of elements to a heap all at once. That's way more efficient than pushing them one at a time, it turns out. Why would I ever want to use a binary heap instead of a vector? Well, for a vector, you would need to sort the vector to make sure that you have the max at the front. And the downside of a vector is if you ever wanted to insert something, the thing that you insert might turn out to now be the max. And so then you have to shift all the elements and put them left.
02:20:32.931 - 02:21:37.215, Speaker A: That's not true with the binary heap representation. You only need to do log n swaps in order to get it to the front. When would I need the heap algorithm? A VEC can do all that the binary heap can do. It's true you could do this yourself with a vector, but it's actually pretty hard to implement a heap efficiently on a vector unless you just implement this data structure, at which point you might use the binary heap anyway, Especially around things like if you insert a new value into the vector. If you just have a vector and you want to iterate the values in order, then yes, you could take the vector, you sort it and you just walk it in order. Totally fine. The reason why a binary heap is useful is if you are adding things to this data structure over time, then if you had a vector, maintaining the vector so that it is sorted is actually fairly expensive.
02:21:37.215 - 02:22:38.007, Speaker A: Whereas with the binary heap you can do that because the whole vector doesn't need to be in order, only the max needs to be available. And the binary heap provides an efficient way to do that. And then we could also say, you know, if you. If what you want is to walk all the items in order and you don't care about duplicates, then you might use a binary map, because that way you can actually walk all, all of the elements without having to do this log n reordering every time you pop the max. Is the heap in binary heap in any way related to the heap where allocated data lives or just two unrelated uses of heap of heaps of things? They're unrelated. So a binary heap happens to be allocated on the heap because it's backed by a vector, but there's no, there's no other relation. It's entirely just the abstract notion of the English word heap.
02:22:38.007 - 02:23:17.845, Speaker A: Like a heap of things, a pile of things. A binary heap is not associated with the heap. Do, do, do, do, do. What would be an example use case for the binary heap algorithm. So using a heap like this often comes down to priority queues, which is why this is, you know, it says a priority queue implemented with a binary heap. And this comes into things like imagine that you're writing something that process. Like you're writing some of the processes jobs you have.
02:23:17.845 - 02:23:57.253, Speaker A: I don't know, you have a. I was going to say CPU scheduler, but I'm trying to think of something less weird. Let's say you're writing a task manager where you have like a web service somewhere and it wants to send a bunch of emails. And so you have a thread and what that thread is doing is just looking for what is the next email I should send. It pulls it off some kind of queue and then it sends the email, and then it takes the next thing from the queue, sends the email, and so on. Now imagine that some emails are more important than others. So some emails need to go out with like high priority.
02:23:57.253 - 02:24:39.763, Speaker A: They need to go out before your whole backlog of like newsletters or something, things like password resets. So you might want to make sure that those get processed, if any are there. Those get processed before all the newsletter emails. For that you can use a heap because it implements a priority queue. So you would have the ordering function for the jobs that you put in this queue. Order things that are password resets to be higher ordering, closer to the max value than your newsletter ones, like a higher value for some integer, for example. And so as a result you still take all the things that come out of your job producers and stick them in in a heap.
02:24:39.763 - 02:25:25.349, Speaker A: But when the this thread pulls stuff out of the heap, it's going to end up pulling the emails with higher priority first. Hence it's a priority queue. I'm scrolling up through the questions here. Does non nightly drain return in sorted order? No. So there's been some debate about this actually. So if you look at the iteration for binary heap, it returns them in arbitrary order. It does not return them in sorted order.
02:25:25.349 - 02:26:09.421, Speaker A: And the reason for this is because you can't. Or rather it would be really expensive to do so because the in memory representation of the binary heap isn't sorted. It only guarantees that the max element is at the front. And so when you call iter, what it will actually do is walk the vector, which is effectively an arbitrary order. As far as observation goes, there's intuiter, sorter and intuiter in theory could do this. Intuiter could keep giving you the max by calling pop in a loop, but then intuitor would be really slow to iterate because remember, pop is a log n operation. So now you're doing N log N to iterate over all the things in the heap.
02:26:09.421 - 02:26:41.353, Speaker A: Whereas intuitor sorted as a nightly only API that's going to yield the items from the Iterator in max order. The question was about drain, and I think drain is also arbitrary order. Yeah, so there's no. I don't think it on nightly either. It returns them in. In max order. Yeah.
02:26:41.353 - 02:27:21.185, Speaker A: So when I say priority queue, there's also. It's also sometimes called a partial sort or a partial ordering where, you know, the newsletter emails are maybe not sorted in relation to each other. Like it doesn't matter which one you send first because they all. It's one instance of the newsletter. So maybe it's sorted by like when the date of the newsletter, but within them. Like whether you send to Alice or Bob first doesn't really matter, but it matters that you send the password resets to that category of emails before you send the newsletter emails. So it's a partial ordering in the sense that there's no ordering within the newsletters, but there's an ordering across newsletters and other types of emails.
02:27:21.185 - 02:28:17.327, Speaker A: Heap sort, of course, is the other case in which you could use a binary heap, because that's what a heap sort is. Is it quicker than a Radix sort of the list? So a radix sort is a very fast sorting algorithm that you can use when all of your values are integers. And so if you have a vector, you could do a radix sort on the vector really efficiently. And that way you have them sorted and now you can do the. You can iterate over them in order. The question still becomes maintaining that over time it could be that you can just like insert the new. If you wanted to do a push into the vector, you push at the end so you don't shift all the elements and then you run a new Radix sort that would probably be more efficient.
02:28:17.327 - 02:28:56.835, Speaker A: But it only works if your values are only integers. And it only works because radix sort is so damn effective. Do you think there's any way we can make a min heap more ergonomic? It's very annoying to have to reimplement or. Okay, so this is the last thing I was going to talk about with binary heaps, which is this is a max heap. But what if what you want is a min heap? You want the things with the lowest value to come out first, like lower prior, like lower numbered priority. Like priority zero is most important. Well, the way that you do this is either you change the ord implementation for your data type so that it returns things in the opposite order.
02:28:56.835 - 02:29:34.915, Speaker A: Or there's an example for this here minheap. There's a type in the standard library called standard compare reverse and it is a. It's a single value tuple type that just holds a T. And the only thing this does is it reverses the implementation of ord of the inner type. So the implementation of ord for this type is. So normally compare is self compared with other. And what it will do is do other compared with self.
02:29:34.915 - 02:30:08.735, Speaker A: That is the only thing this type does. And so what you can do if you want a min heap is just before you stick your value into the heap, you wrap it in the standard compare reverse, which is like repro transparent and everything. And then when you pull it out, you take out the inner value from the reverse to get back the original value. As you see here, they import reverse. Here they create a binary heap. They push reverse of 1, reverse of 5, reverse of 2. And now when you pop them, you get the smallest values out first and you can do like zero to extract the original value you put in there.
02:30:08.735 - 02:30:51.703, Speaker A: So that's the way that you would get this max heap implementation, just be a min heap is you just reverse the meaning of max and min. Okay, is there anything else I want to talk about for binary heap? I don't think so. All right, so we're at the two and a half hour mark, so I'm gonna stop there. Here's what I'm gonna say though. So I teased this a little bit in the. In the tweet. There are two types that some may call collections, and it's option and result.
02:30:51.703 - 02:31:24.055, Speaker A: So option is really a vector of capacity. 1. There's one way you can think about option and result is a map from boolean to a by genius type. It's really. It's a single element map from a boolean to a by genius type, which is really weird. I don't recommend thinking about them in collections in this way, but they kind of are collections. Like if you look at them, they have operations like where is my.
02:31:24.055 - 02:32:07.389, Speaker A: They implement things like into Iterator, they implement from Iterator. So they kind of are collections. They're just a really weird kind of collection. Some might call them monads, but I'm not going to get into that debate now. The other way to think about option and result is that they are ways to. They are collections that are specialized. And the reason why people often call them collections without thinking about the M word is because they Think of the iterator trait, and in particular the iterator trait has a method called collection.
02:32:07.389 - 02:33:18.391, Speaker A: And when you collect, what you do is you take an iterative items and you collect them into a collection, right? That is the result of that iteration and collection. And you can collect into vectors, you can collect into binary heaps, you can collect into hash maps, whatever you might want, but you can also collect into options and you can collect into results. And so as a result, people might think, well, that means result is a collection, option is a collection. In practice though, that's not quite true. Like if you really dig into it and you look at from the from iterator implementation for option, you'll see that it's not from iterator of here. It is not an implementation of fromiterator where the iterator yields type t and it collects into an option t. It is an iterator where the items are options and what it yields is the first thing that is it yields none, if anything is none and otherwise it collects into an inner type that it's itself from iterator.
02:33:18.391 - 02:33:50.805, Speaker A: That's confusing. Let me see if I can do that better. So if you do, you know zero to ten dot, do I still need to do this? I forget doc collect this would not work. This is not a thing you can do. This doesn't work. Value of type option use size cannot be built from iterator over elements of type integer. If I do vac, then it works just fine.
02:33:50.805 - 02:34:34.815, Speaker A: And so this is an argument that option is not a collection. What does work though is this actually, let me. Yeah, so I can do this. And this is really saying that option is a functor. It's really saying that option can sort of map through a collection for. For into iterator result is the same way where if you have an iterator over things that are option, you can produce an option over some other collection from the iterators. And what this wrapping does is it means that if any of these things are none, then this is going to produce none.
02:34:34.815 - 02:35:40.005, Speaker A: It's not going to produce a vector at all. But if all of them are sum, then it's going to produce a vector of the inner values. It's sort of like an unwrap or none kind of ordeal, where if any of the iterators items are none, or if you think of a result, which is the more obvious one, if any of the iterated values are error, then the thing we collect into is going to be an error rather than an okay, so if I do Result here. And I do this, what this is saying is if all of the items yielded are okay, which they are here, then get an okay vector of those values. But if any of them are error. So if I do here chain standard error ones. So here this is going to yield 10 okay values and then an error value, then this collect will not produce an OK vector.
02:35:40.005 - 02:36:16.423, Speaker A: It will return with just error of this. And so the result here is not a collection, but it can wrap a collection and an iterator. So it's not really a collection itself. Oh yeah. It's more of an applicative than a functor. I guess I'm going to ignore trying to dive too deep into it because I think it's just going to be confusing. So one of the ways that you can see this is like result and option.
02:36:16.423 - 02:36:48.965, Speaker A: Do not implement a lot of the other things that we expect from collections like extend. Right. Because they're not really collections in that way, but they feel a lot like collections. And that's because they're like they share some properties with collections and I will not name the kinds of properties they share. My face is blocking the code. Oh, I'm sorry for the chain. There you go.
02:36:48.965 - 02:37:37.625, Speaker A: Okay, I think that's all I wanted to talk about today. Yeah, applicative is an applicative functor. That's what I thought. So functor is basically right, but it happens to also be applicative. Okay, I think that's all I wanted to talk about today. Is there anything else that you all are curious about for the standard library collections before we end for today? Yo, is that a mono from the category of endifunctors? It might just be. Oh yeah, sure.
02:37:37.625 - 02:38:21.275, Speaker A: This is the worst kind of collection. So the unit type is a collection. Unlike option and result, the unit type implements extend and it implements from iterator. And you might think this is stupid. Like why is it that you can collect into unit to discard all of the values? And the reason why you can do this is because it turns out to be useful if what you're looking for is errors. So actually we can use the example that's right here. So you have a vector.
02:38:21.275 - 02:38:48.909, Speaker A: You want to write all of those values out into standard out and the write ln macro returns a result that is either the number of bytes that were written or error. And let's say that I want to do all these writes and I want to know whether it failed or not. So what I get. But I don't actually care about the okay values. I don't Care about the number of bytes written. That's not relevant to me. All I care about these are the errors.
02:38:48.909 - 02:39:30.815, Speaker A: So I don't actually want to collect like a vector of the bytes that were written or anything like that. And so I can collect into a result where the okay type is unit. And what that will do is it'll discard all the okay values but still produce an okay value. If all the iterated items were okay, and if any of the iterated items were error, then this will produce an error which I can then inspect. And so that's why it's actually useful for the unit type to be a collection that just discards all those inputs. So it's an always empty collection. I didn't talk about plain arrays because they are collections, but they're not.
02:39:30.815 - 02:40:01.655, Speaker A: They're not the kind of collection I want to talk about today. Let's see. I'm curious how efficient very small hash maps are. Is it very much frowned upon if you create and drop a bunch of tiny hash maps? Quite often tiny hash maps are. It really depends. For tiny hash maps, it really depends on how many lookups are you going to do into them. Because hash maps are a little bit.
02:40:01.655 - 02:40:55.095, Speaker A: They're a little wasteful if you have a very small number of items because very often it's just as fast like search a small vector than it is to do the hashing and lookup in a hashmap that's small. They're not that expensive to construct. Allocating one is really just allocating a vector. So that's not going to cost you too much. But it's unclear that the runtime performance is going to be worthwhile compared to just having a vector new search. What if there's more than one error collect will take or collecting into result will take the first error and then stop taking from the iterator. And the other reason why the.
02:40:55.095 - 02:41:32.353, Speaker A: Actually, I'm not going to talk about that. It's not worthwhile. Can you point to some other cool collection crates that are useful to know about? The main ones are things around concurrency. If you want something like a concurrent hash map, then you can use something like Dash Map or Flurry, which we did a previous stream on. Or you could do things like look at the crossbeam crate which has a bunch of these things. So that's like the main other set of collections. And then you often find data structures that are optimized for very particular use cases.
02:41:32.353 - 02:42:03.539, Speaker A: I mean, evmap is an example of this where you have a hash map that's Optimized for concurrency. Where you have read heavy operations. I'm trying to think what else is like common data. So you have hash brown, you have index map. So index map is a pretty common one. So a hash map, when you do, inserts the yield items in arbitrary order. If you iterate, index map is a little different.
02:42:03.539 - 02:42:47.363, Speaker A: When you insert something, it's still a hash map behind the scenes, but it also remembers the order in which the keys appeared. So when you iterate over an index map, which you can use interchangeably with a hash map, when you iterate over an index map, you get the keys back in insertion order, which is sometimes really useful. Oh yeah, there's also just the category over on crates IO there's a category called data structures. Data structures 2800 crates. If you look at recent downloads, there's a hash brown. We talked about SmallVec, we talked about IndexMap, we talked about Semver. Unclear if it's a data structure either.
02:42:47.363 - 02:43:23.435, Speaker A: We can mostly ignore bytes, which is more about strings, crossbeam, as I mentioned, generic arrays, which is really the same as small vec kind of slab, which is an allocators, which we can ignore a hash, which we talked about, but isn't really a data structure. TinyVec, which is like SmallVec. So those are really. The PET graph is like a very different kind of data structure for storing graphs, which we're not going to talk about here. There are many ways to do it and PET graph has multiple ones implemented. Bit sets are kind of interesting, perfect hash maps. So this is for.
02:43:23.435 - 02:44:10.993, Speaker A: If you know all of your keys at compile time, you can generate an optimized hashing function for them so that you get a perfect hash map, one where every key maps to exactly one bucket. You don't need to do any probing and anything. So that can be really good if you have a data structure or a data set that happens to work for this. And you could argue that the MPSC channels in the standard library are also collections. Right? They're a queue. That's exactly what they are. Although there now I think the implementation has been swapped with the NPSE queue from Crossbeam and there are a bunch of different other implementations of concurrent queues.
02:44:10.993 - 02:44:44.371, Speaker A: We're not going to go into those too much. Ok, I think that's the end of what I wanted to talk about next stream. I don't know when it's gonna be. Probably not for a little while. I'm hoping that I'll get to keep to the schedule, but I'm moving to Norway in the start of June. And so I'll be in the middle of moving. So we'll see whether I can do another stream by then or whether the next stream will be in Norway, which will be exciting.
02:44:44.371 - 02:45:00.825, Speaker A: And I may have to change the times, time of the streams a little bit. But thank you all for coming out. I hope you found that this was useful and this will be on YouTube, of course. Or you might already be watching it on YouTube by the time you see me seeing this. All right, thank you, folks.
