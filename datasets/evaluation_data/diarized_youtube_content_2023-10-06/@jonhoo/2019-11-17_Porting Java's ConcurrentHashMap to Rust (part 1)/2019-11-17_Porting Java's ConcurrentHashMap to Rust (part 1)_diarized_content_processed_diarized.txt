00:00:00.200 - 00:00:20.705, Speaker A: Hello, everyone. Welcome back. It's time for yet another Rust stream. This time we're going to tackle something kind of interesting. We're going to try to port Java's concurrent hash map to Rust. This is going to be a bit of a journey. I have no idea how much we'll succeed at this task, but let's get into that in a little bit.
00:00:20.705 - 00:00:49.955, Speaker A: First, I'm John, just like I was last stream and just like I've been every stream, I've had a number of requests to say my surname. So my surname is Jenkset or in Norwegian. My full name is Jun Fadenrong Jens, which is just unpronounceable in English. So if you want to pronounce my name, just ignore all the GS and then it's fine. I will know that you mean me. So I do a bunch of these Rust streams. If you head over to the YouTube channel, actually I should pull that up.
00:00:49.955 - 00:01:23.577, Speaker A: So this is me, and there are a bunch of past videos here. I've tried to roughly organize them. There are all sorts of different topics we've covered, from network programming to sort of Async Rust stuff, to porting to other languages, open source contributions, and some just more educational material. So feel free to go back and look at some of those if you wish. I also have an Amazon wish list if you want to support my work in some way. Because. Because I'm a foreign student in the us, I'm not actually allowed to have a Patreon, otherwise I would.
00:01:23.577 - 00:01:55.937, Speaker A: But this is life for now. So I did a poll like start of this week where I sort of tried to get people to vote more. So there's this voting site up here where you can vote for what you want the next stream to be. And the winner for a while has been porting Java's concurrent hashmap to Rust. And so I tried to get people to vote more and this just stayed the top. So we're going to do it. This is this file from JDK13.
00:01:55.937 - 00:02:48.985, Speaker A: There are older versions of it well, but we're just going to use the latest one. And as you can see, it's a fairly substantial piece of library code. And the basic functioning of concurrent hashmap is that it is a hashmap just like Rust is, but you can access it from multiple threads concurrently without having to stick it in a lock. And the idea is that it should perform well even when there are many threads reading and many threads writing at the same time. This is a notoriously useful structure, like very many programs fit this. But it's also very hard to get right, especially because when you. You can have so many concurrent operations that all modify the map, including things like having to resize the hash map, like grow it or shrink it, which of course also needs to work well with the surrounding threads that might be reading or writing.
00:02:48.985 - 00:03:22.271, Speaker A: As we go through this, a lot of what we're going to be doing is sort of studying the Java code. I think it's going to be relatively straightforward even if you don't know Java. It'll be fine. We'll only be writing Rust code. We won't be writing any Java here, but. But we will be reading a bunch of Java and a lot of the documentation for this file as well. I suspect from the get go that this will be fairly tricky in the sense that the way Java works is a little bit different than Rust does.
00:03:22.271 - 00:04:24.711, Speaker A: It has a runtime and crucially it has a garbage collector. And while that changes many things in how you write code, it specifically affects concurrent code a lot. Because one of the very hard parts of writing good concurrent or writing concurrent code in general is that you need to deal with when to free things, when do you drop things In Rust terms, the reason this gets complicated is that if you can have threads just randomly accessing an object without, and you want it to be scalable, so you don't want synchronization between them or as little as you can, then even just knowing when no one is touching a thing anymore and no one will in the future can be really hard. The garbage collector just solves this problem for you. So in Java all you have to do is just let go of the pointer and then eventually you'll be freed up. Whereas in Rust we're going to have to figure out a way to explicitly clear up those objects and any additional synchronization that might be needed for that. Before we start, I asked right before the stream, I do a little bit of like video off, but just talking.
00:04:24.711 - 00:05:11.145, Speaker A: And so I'm just going to check with the, the people chatting to see whether there's anything in the setup of this that doesn't make sense or if you want more explanation and then we'll get started. So if the people in chat, if you have, if this setup didn't make sense, or if you feel like you don't really understand what we're tackling or what might be hard about it, now's a good time to ask because I want to make sure that we are all in the same headspace of what are we trying to do and why might it be difficult? And so I want to really spend some time on that before we dive into the implementation details. Yeah, we'll need reentrant locks. That's fine. We can use parking lot for that. I had a haircut lately. That is also true.
00:05:11.145 - 00:05:59.929, Speaker A: Yeah. So a common approach that people use in Rust today is that they just take a hash map and they stick a Mutex around it. And that works fairly well in many use cases. Like if all you care about is that some threads are going to be accessing things and it's not very performance sensitive, this isn't a problem. Where this becomes a problem is in very high performance applications where you want to use many of the cores on your machine and they are all going to be operating on some shared set of some shared map. Because here if there's a mutex, only one thread can access the map at a time, which might slow down your application a lot because you end up sequentializing the work across all your cores. So in general, I would say benchmark your application first to see whether you need a concurrent map.
00:05:59.929 - 00:07:01.825, Speaker A: But if you need one, then you probably don't like a mutex hash map is not sufficient for you. There are some other implementations of concurrent hash maps in Rust, but I figured implementing the one from Java seems like a reasonable thing that we would want in the Rust ecosystem. We will be using unsafe, hopefully as little as possible, but we will have to use unsafe in this code. And that's part of the reason I wanted to do this stream linked to the Java source someone already linked. Great, great. How will you test the program? That's also a great question. So the Java or JDK 13 and all the other Java stuff actually has implementations of a bunch of different tests that do things like concurrent operations.
00:07:01.825 - 00:07:41.725, Speaker A: And what we'll be doing is essentially implementing those same tests in Rust. I haven't looked at these yet, so I don't know to what extent they're like good tests, but there are tests. We might as well port them as well. In addition, we'll probably do some sort of straightforward testing of just like spin up some threads, have them do a bunch of concurrent accesses and check that things work out correctly. If you wanted to do something more rigorous, we could use. It's now under Tokyo RS there's a tool called LOOM that's really neat. That's basically a concurrent testing tool.
00:07:41.725 - 00:08:28.755, Speaker A: So it's based on a paper called CDS Checker, which is basically it tries every or not quite, but almost every possible permutation of thread operations and atomic operations, basically every relevant thread interleaving and checks that all of them produce or the test passes for all possible interleavings. And so this might be something we could use if we wanted to try something more rigorous. But just to start off, I think we'll start with the Java tests. Great. All right, let's get started. So the first thing we're going to do, cargo New Lib and pick a name. Now, this is always the hardest part.
00:08:28.755 - 00:09:16.075, Speaker A: We could call it like Concurrent Hash map, but it's kind of boring and I think there's already one that's called that. There's one on Crates IO called C Hashmap. But do we want to come up with a cleverer name? If so, now's the time. Otherwise I will pick a boring name. Always a clever name. Eric. Yes, this is going to have to be GPL licensed because the Java source code is also GPL Lesson License Hydra Map Flurry Hamap Atlas Global Scale Josh Map.
00:09:16.075 - 00:09:48.153, Speaker A: Josh Map is terrible, but I love it. Flurry is kind of cool. Chomp cache map con RSS Java concurrent hash Map in Rust. That's a very Java ish name. Certainly true. Okay, I think. I think we're going to go with Flurry.
00:09:48.153 - 00:10:27.635, Speaker A: I like Flurry. Great. Our project is now called Flurry Synchronic Concurrent Abstract Abstractive Manager Entity Storage with hashing. Also good. I also cheated a little and I downloaded the Java file so that we don't have to keep it open the browser all the time because that's annoying. Also that way I can look at it in dark mode. Actually gonna.
00:10:27.635 - 00:10:45.435, Speaker A: Yeah, that's great. So I'm gonna close these. This one I'm gonna leave open for now. Let me check that I have this in dark mode before we start. Mutex. Great. All right.
00:10:45.435 - 00:11:29.151, Speaker A: Factory Bean. Great. All right, let's stop being silly and start. I'm gonna open the Java here and we can get started. So I don't like this highlighting, but fine. Okay, so what we're going to start out doing is actually just reading the documentation here because I think it's useful to. Or in this case, at least when the documentation is well written, it gives a good indication of both the considerations that went into designing the original thing that we're porting and also gives us a bunch of details about the implementation that are useful to understand before we start.
00:11:29.151 - 00:12:10.127, Speaker A: Now, I will reveal that I actually skimmed these comments before we started because otherwise if I actually had to read it and understand it on the stream, it would probably be relatively boring to watch, but let's still walk through it. So it starts out saying a hash table supporting full concurrency of retrievals and high expected concurrency for updates. Okay, so this is already an important piece of information. Full concurrency of retrievals means that gets should never block. You should always be able to reads and they should finish basically immediately. High expected concurrency for updates. The intention here is you should be able to do many updates concurrently, but they may sometimes block.
00:12:10.127 - 00:13:02.657, Speaker A: So that's the way to understand this first sentence class based the same functional specification as hash table and includes versions of method corresponding to each method of hash table. Great, so this is the same thing that we want. We want an API that's basically the same as standard collection hashmap, but where the methods are concurrent. Now, one way in which this will manifest in rust is that most of the methods on standard collections hashmap take a mutable reference to self, whereas with concurrent hashmap it will actually take a immutable reference to self. Because otherwise you wouldn't be able to call methods from concurrently. Basically you wouldn't be able to stick it in an arc. Even though all operations are thread safe, retrieval operations do not entail locking and there's not any support for locking the entire table in a way that prevents all access.
00:13:02.657 - 00:13:43.735, Speaker A: Right, so this is basically what we talked about before, that reads do not lock and the table is not a giant mutex and so you don't have a way to block all operations. Retrieval operations do not generally do not block. I think the exception to that is if there's a resize in progress, they might take a little bit longer, but I'm not sure. I guess we'll get to that. So may overlap with update operations. So there's a question in chat like what does it mean if something is updated while you read it? And here's the explanation. Retrievals reflect the result of the most recently completed update operations holding upon their onset.
00:13:43.735 - 00:14:50.525, Speaker A: More formally, an update operation for a given key bears a happen before relationship with any non null retrieval for that key reporting the updated value. So to put this in slightly more layman's terms, if you do a read, you will see every completed update and you will not see any not completed updates. So for example, if you do a set, then get will see the value that you set for that key the moment that set returns. How is this different from C hashmap? So C hashmap does from memory per bin locking and this does not, at least not in quite the same way C hashmap is a relatively straightforward implementation if I remember correctly, but we can double check docashmap based on bucket level multi reader locks. It'll be interesting actually. We should run a benchmark of C hashmap versus what we come up with. It could be that there's some workloads with C hashmap makes more sense.
00:14:50.525 - 00:15:44.559, Speaker A: What's the expectation of concurrent iterators? My guess is that we'll get there for aggregate operations such as put all and clear. Concurrent retrievals may reflect insertion removal of only some entries. So if you clear the map, then a concurrent read might still see some of the values as present. The clear is not atomic with respect to the whole map. The clear is as if you did a removal of every value, even though performance wide it might be faster. Similarly, iterators return elements reflecting the state of the hash table at some point at or since the creation of the iterator. Right? So this is what the question was in chat, that if you have an iterator that's walking the map while it's being mutated, then the iterator is going to see some state of the map as as of the time the iterator was created or later.
00:15:44.559 - 00:16:53.145, Speaker A: So as you walk through the iterator you may or may not see updates reflected in that it is a design to be used only by one thread at a time. So this is interesting. This is a restriction. This highlight color is terrible. This restriction is interesting because in rust we can actually guarantee this, right? So this means that we just make our iterators not be sync and that way a given iterator cannot be used by more than one thread at a time. Bear in mind the results of aggregate status methods like size is empty and contains value are typically only useful when a map is not undergoing concurrent updates in other threads, right? So the idea here is that if you call size, the value you get back is the size at some point, but by the time you use that size for something, the map might have changed under you, right? Because any number of threads can be updated concurrently. Otherwise the results of these methods reflect transient states that may be adequate for monitoring estimation purposes, but not for program control.
00:16:53.145 - 00:17:56.945, Speaker A: So for example, if you do contains value or do is empty or something, you're going to get back like a boolean and it might say false, but by the time you act on the fact that the map was empty, it might no longer be empty because some other thread inserted a value. The table is dynamically expanded where there are too many collisions, that is Keys that have distinct hash codes but fall into the same slot modulo the table size with the expected average effect of maintaining roughly two bins per mapping, corresponding to a 0.75 load factor threshold for resizing. So the idea here is that in a hash table you basically take you have some number of bins and every key maps to one bin. And the more keys map to a given bin of the hash table, the longer it's going to take to read any key in that bin. Because when you do a lookup for a key, you find the appropriate bin and then you look at all the items in that bin. So you want every bin to be relatively short.
00:17:56.945 - 00:18:58.665, Speaker A: And so this is what they're getting at with this load factoring and roughly two bins per mapping that if you end up having many keys for a given, sorry, many keys for a given bin, then we're going to resize the table, we're going to make there be more bins, and once we've made there be more bins, then we redistribute all the keys amongst the bins so that there are fewer keys for every bin. You can disable Java Doc highlighting by sticking let Java ignore JavaDoc in your vimrc. Ooh, great. That sounds like something I want to do. Fantastic. Much better. Thank you.
00:18:58.665 - 00:19:48.085, Speaker A: I read the comment at some point in time to me that an iterator will read a snapshot. No. So an iterator does not read a snapshot. If we look again, iterators reflect the state of the hash table at some point at or since the creation of the iterator. So it the every time you get an element from the iterator, that element is gathered at or since the creation of the iterator, it is not as though the iterator takes a snapshot. Is hash code a Java concept that exists for every object? So Java has an interface which is roughly like a trait that called hashable. But I think there's like a default implementation of hash for every Java object.
00:19:48.085 - 00:20:41.621, Speaker A: But yeah, so we will bound our key by being hash just like in hashmap. Where were we? Yeah, there may be much variance around this load average of two bins per mapping as mappings are added or removed. But overall this maintains a commonly accepted time space trade off for hash tables. However, resizing this or any other kind of hash table may be a relatively slow operation. Right. So the idea here is that if you have a hash table with like a million bins and then you need to resize it to have 2 million bins, then you need to move, I think it's like in expectation, half of the elements. You're going to have to move 500,000 elements in order to do that resize, which is pretty slow.
00:20:41.621 - 00:21:09.645, Speaker A: So in general you want to avoid resizes and as they say, resizing this or any other kind of hash table. This is true for Rust's hashmap as well. Maybe a relatively slow operation. So when possible, it's a good idea to provide size submit as optional. So this is where the standard library hashmap has with capacity. We're also going to have something similar. Can you implement par iter from this map? Absolutely.
00:21:09.645 - 00:21:52.545, Speaker A: Par iter from this map makes a lot of sense and will actually be fairly fast Load factor we can basically ignore. They have this additional restriction which is for compatibility with previous versions of this class. Constructors may optionally specify additional hints and stuff that are not useful for us. We can ignore this. Note that using many keys with exactly the same hash code is a sure way to slow down performance of any hash table. So this applies here as well. To ameliorate impact when keys are comparable, this class may use a comparison order among keys to help break ties.
00:21:52.545 - 00:22:36.931, Speaker A: So this is an interesting point. So what they're saying here is that if you have many keys that hash to the same hash value, you either because of something weird about your data or because of an adversarial input, then they're actually going to fall back to using ord. So they're probably going to use a tree or something here for those keys. This smells an awful lot like an optimization, and so we might end up not implementing that for the stream just because we want to get to something working and then we can optimize later. Every object has a get hashcode method. There's a default implementation. It's a bad idea to use an object without overriding equals and get hashcode as a key.
00:22:36.931 - 00:23:09.515, Speaker A: Right. So Java basically has a default implementation of equals and hash for everything. Whereas in Rust we're just going to take keys that implement hash and eq. How are we going to deal with live iterators when we resize the backing array? It's a great question we're about to find out when we read about the implementation. That's my guess. A set projection of concurrent hashmap may be created. Yeah, so this is similar to how hash set in the Rust standard library is really just a hash map where the values are unit.
00:23:09.515 - 00:23:59.755, Speaker A: So you just use the keys. Concrete hashmap can be used as a scalable frequency map. From a histogram or multiset by using long adder. So I think long adder is roughly equivalent to atomic U size in rust. So the idea here is that if your values are atomic U size, you can use this to count the number of occurrences of things. So you do something like you do like an insert if not exist for the metric you're trying to measure and then you get the value and you do an atomic increment on it and now you end up with a histogram that you can concurrently update from any threads like hash table. But unlike hashmap nulls, we don't care about nulls.
00:23:59.755 - 00:24:41.005, Speaker A: Sequential and parallel bulk operations. Yeah, so this is going to be like par iter stuff, which we will probably deal with separately. Like this seems like a good thing to do as an extension. Yeah, so these are those bulk operations, so we're going to ignore those for now. Entry API seems nice, although also probably, probably something that is secondary. Speedups for parallel compared to sequential forms are common but not guaranteed. Okay, that's not important.
00:24:41.005 - 00:25:16.507, Speaker A: All right, so this is the. What we read now is sort of the high level description of what this type is. Think of this as like the doc comments on the hash table type basically. So this is something you'll see a lot in like industry code bases is that there's a. There's a document, there's a public documentation for the type and then there's a private documentation for the implementation. So that's what this business is getting at. Right.
00:25:16.507 - 00:25:49.671, Speaker A: So you see that as like a public doc comment. And this here is the internals comment. The primary design goal of this hash table is to maintain concurrent readability typically get but also iterators and related methods while minimizing update contentions. Secondary goals is to keep space consumption about the same or better than Java util hashmap and support high initial insertion rates on an empty empty table by many threads. Like populating the table should be fast. This map usually acts as a binned hash table. Okay, so that's as we expected.
00:25:49.671 - 00:26:12.367, Speaker A: Each key value map is held in a node. Okay. So keep in mind that in Java most things are heap allocated. Like basically most things are boxed. And my guess is the implementation is going to rely on that like it's going to rely on having pointers to things and being able to do like atomic pointer swaps. So we'll probably end up doing something similar. And so each key value mapping is held in a node.
00:26:12.367 - 00:26:43.793, Speaker A: Most nodes are instances of the basic node class with hash key value and Next fields. However, various subclasses exist. Okay. So we won't have subclasses in rust and we probably don't want dynamic dispatch in here. So my guess is that node will end up being an enum for us and then we'll store box node in various places. Tree nodes are arranged in balance trees, not list. Yeah, so this, this is probably what they mean when they earlier when they talked about the optimization around if many keys hash to the same value, they use a.
00:26:43.793 - 00:27:16.725, Speaker A: They use comparable. So this is probably what those three nodes are. So my guess is that we will not need tree nodes and tree bins. Forwarding nodes are placed at the heads of bins during resizing. Yeah, so this is probably related to. There was a question of what happens if you have a live iterator while you resize. My guess is these forwarding nodes are the way you realize that like the old map doesn't go away until everyone releases their handle to it.
00:27:16.725 - 00:27:54.185, Speaker A: And. But we're gonna, what we're gonna do is anytime we move again, I'm speculating, anytime we move a bin to the new resized map, we're going to leave in place like a marker, like a tombstone for any. For any iterator that ends up hitting it or a get that ends up hitting it. They will then follow that to the new resize table. So they will realize that they've hit something that's been moved. Reservation nodes are used as placeholders while establishing values in compute if absent and related methods. That's interesting.
00:27:54.185 - 00:28:40.519, Speaker A: So this is like if I do an operation like this is basically the entry API, right? If I do entry with a key and then I want to do something, if that key does not exist, I basically want to insert it. Then I need to leave something in place so that when I actually do that insertion, someone else hasn't stuck something in there in between. That's presumably what these are used for. The types tree bin, forwarding node and reservation node do not hold normal user keys, values or hashes and are readily distinguishable during search because they have a negative hash field and null key and value fields. Right. So for us this is just going to be an enum instead. Right.
00:28:40.519 - 00:29:18.357, Speaker A: So we're going to distinguish, instead of using this like negative hash business, we're just going to use an enum straight up for node. These special nodes are either uncommon or transient. So the impact of carrying around some unused field is insignificant. Yeah, so that is the one reason not to make node and enum is because we're making nodes slightly Larger and node is used everywhere. So we'll see. Maybe we just need node and then we just make the. I'll have to think about it.
00:29:18.357 - 00:29:43.021, Speaker A: Not sure yet. We'll see. The table is lazily initiated to a power of two size upon first insertion. Each bin in the table normally contains a list of nodes. Okay, so every bin is really going to be basically a linked list. So this means that there's no real limit on the number of items you can have in any given bin. Most often the list is only 0 or 1 node.
00:29:43.021 - 00:30:12.395, Speaker A: Yes, we want the bin lengths to be relatively short. Table axes require volatile atomic reads, writes and casses. So CAS here refers to compare and swap. It's an atomic operation you can do on any type that is basically a single word in the machine. So like 64 bit machine, you can do it on 64 bit. Sometimes you can do it on other types as well. And it's basically you tell the cpu, replace, replace.
00:30:12.395 - 00:30:56.823, Speaker A: If this value has not changed since I last read it, or rather if its value is still this, which I read previously, then replace it with this value. I wonder why an enum wasn't using the Java version. It's probably because Java doesn't have arithmetic types, so you can't have an enum that contains stuff. As far as I remember, the Java enums are just direct enumerations. You can't have stuff inside the variance, which means that the enum doesn't really make a. It doesn't help you. In this case, you would still have node and then it would have to have a field that is an enumerated.
00:30:56.823 - 00:31:25.895, Speaker A: It could be that they also didn't want node to be larger. It's quite possible because there's no other way to arrange this without adding further indirections. We use intrinsic unsafe operations. Yeah, so this is the same thing we're going to do because we need to. We use the top signed bit of node hash fields for control purposes. It is available anyway because of addressing constraints. Nodes with negative hash fields are specially handled or ignored.
00:31:25.895 - 00:32:17.301, Speaker A: Oh, this is just what we read earlier. I think that readily distinguishable doing search because of a negative negative hash field insertion via put or its variance of the first node in an empty bin is performed by just casting it to the bin. Yeah, so if you have a bin that's empty, you just do a compare and swap to place the node into that bin where the previous value is null. It's by far the most common case for put operations under most key hash distributions. Other update operations, insert, delete, and replace require locks. We do not want to waste the space required to associate a distinct lock object with each bin, so instead use the first node of a bin list itself as a lock. Lock support for these locks relies on built in synchronized monitors.
00:32:17.301 - 00:33:01.123, Speaker A: Okay, this paragraph is pretty important. So what it's saying is we're actually going to do per bin locking, but only if you have more than one item. So in the common case you'll have zero or one items in any given bin and then there's no locking. If you have zero and you want to insert something, you do a compare and swap. But then if you want to add more items after that first one, then you take a lock, which means that you only take a lock if there's a node there, which means that the lock can live in the first node rather than live in the subsequent nodes. Sorry, rather than live in the bin itself, which means that in the common case where you're just inserting into an empty bin, you don't take a lock. So nodes actually contain locks in this.
00:33:01.123 - 00:34:15.431, Speaker A: They also say locking support for these locks relies on built in synchronized monitors. So Java has this notion of every object is lockable and it without you storing an explicit lock in the object itself, you can just sort of say lock this object and Java will magically do that for you and guarantee that no one else operates on that object. In the meantime, in Rust we're probably going to have to make that lock explicit, which probably means we're going to end up storing it in node itself, which is going to add a little bit of additional overhead to storing every node, which is a little unfortunate. Java enums can contain stuff, but they contain constants based on the enum value. Right? But it doesn't have union types, which is what Rust. Well, no. So Java enums, they can have values, but you can't like destructure them.
00:34:15.431 - 00:35:06.319, Speaker A: They're not like you can't say that a type that you can't have an enum that's like either type A or type B, if that makes sense. You'll see this if you're not used to enums from Rust. This might be a little weird, but you'll see this once we start writing the code. Using the first node of a list as a lock does not by itself suffice though. When a node is locked, any update must first validate that it is still the first node after locking it and retry if not, because new nodes are always appended to the list. Once a node is in a first bin, it remains first until deleted or the bin becomes invalidated upon resizing. Yeah, so the idea here is that you could have a race where you someone has created the first node in a bin, you want to insert something after that node.
00:35:06.319 - 00:35:40.097, Speaker A: So you, you find the node, you take the lock, but then someone is now resizing. And so you taking that lock does not mean you own the bin because that node is moved. So after taking the lock, you then need to check that that object is still the prime one amongst in that bin so that your lock is still valid. Otherwise you're going to have to basically ret. Each variant of a Java enum has the exact same types for its fields. Yeah, so that's not true in Rust. It's not really more overhead.
00:35:40.097 - 00:36:26.365, Speaker A: Java has that overhead for all objects. So I believe that the way Java does synchronized objects is not that it stores a lock in every object, is that it has a global running monitor that knows if someone has a lock to something and so it's not actually stored in each individual object because that would make every object larger. Instead the monitor keeps track of which things have been locked. So we don't want to put the locks on the bins, because that would mean that if you want to insert the first thing in a bin, you still have to take the lock. And we need to store the lock for every bin, even the bins that are empty. So there are two aspects to this. The first is you don't want to take a lock if you can avoid it.
00:36:26.365 - 00:37:08.719, Speaker A: Like a compare and swap will be a little bit faster. Like an uncontended compare and swap be a little bit faster than an uncontended mutex take. So that's one part of it. But the second one is if we store the mutex in the bin, then any bin that has no node now needs to store a pointer and a mutex as opposed to storing just the pointer. So your hashmap is going to be almost twice as large. Great. The main disadvantage of per bin locks is that other update operations on other nodes in the bin list protected by the same lock can stall.
00:37:08.719 - 00:37:41.173, Speaker A: For example when user equals or mapping functions take a long time. However, statistically under random hash codes is not a common problem. I like this. So they're basically saying this pattern is in theory bad because you might end up holding the lock for a long time. However, in practice is not really a problem because of the way that the data works out statistically. Ideally the frequency of nodes and bins follows a Poisson distribution with a parameter of about 0.5 on average, giving the resizing threshold of 0.75,
00:37:41.173 - 00:38:15.647, Speaker A: although with a large variance because of resizing granularity. Ignoring variance, the expected occurrences of list size k. So this is a bin of that length. The first values are this. So about 60% of bins will be empty. That doesn't sound right. List size K unclear.
00:38:15.647 - 00:38:51.555, Speaker A: I think that's what they mean. So 60% of bins are empty, 30% of bins have one element, 7.5% of bins have two elements. And then as it goes, right, so you can see that most bins are going to be very small, which means that the cost like you will generally not contend on the lock that's per bin. Each object as an intrinsic monitor make. Each object has an intrinsic monitor, making them slightly larger. Nothing makes sure that intrinsic lock is held.
00:38:51.555 - 00:39:15.315, Speaker A: No one can't mute it. That object is that any object can be locked on. Is that actually part of the object header? I'm surprised. Seems like weird overhead. All right. Yeah. So the argument here is basically you, you're not generally going to have long bins and so generally you won't contend on these locks.
00:39:15.315 - 00:39:36.205, Speaker A: Log conditions, probably for probability. For two threads accessing a distinct element is roughly one by eight times the number of elements under random hashes. In practice it deviates significantly, especially if you have more than like the. Basically the length of your hash in rust. I think this is a U64. So like it's just. They're just not going to collide.
00:39:36.205 - 00:40:44.225, Speaker A: But it's more that think of the number of bins you have like you're going to collide more than that because it's not a uniform space. Similarly for dumb or hostile usages in which multiple keys are designed to have identical hash codes or ones that are for only in masked out high bits. So we use a secondary strategy that applies when the number of nodes in a bin exceeds a threshold. These tree bins use a balance tree. Okay, so this here is an optimization for if you have bins that are very long because of particular oddities in your input, then we're going to use a tree rather than a list for those because that means that even if you had like a million elements, you're just going to do a log N search rather than an order N search. This is an optimization that I think we're probably not going to implement this time around, but maybe in a follow up stream or as a pr. Okay, so it sounds like Java actually stores the implicit locks in the object headers.
00:40:44.225 - 00:41:07.045, Speaker A: So every Java Object has a built in header built in lock. At least that's what the Java people are telling me. So in that case, us storing the lock in node just seems fine. It's going to be the same over at the Java incurs anyway. The table is resized. The table is resized when occupancy exceeds a Percentage threshold. Nominally 0.75,
00:41:07.045 - 00:41:38.825, Speaker A: but see below. Any thread noticing an overfull bin may assist in resizing after initiating. After the initiating thread allocates and sets up the replacement array. Oh, that's interesting. So if a resize is happening, any thread that tries to access the map will actually help with the resizing to make it go faster. That's neat. However, rather than stalling, these other threads may proceed with insertions, etc.
00:41:38.825 - 00:42:11.625, Speaker A: The use of tree bins shields us from the worst case effects of overfilling while resizes are in progress. Yeah, again, so that's a part of the optimization that we're going to ignore. Resizing proceeds by transferring bins one by one from the table to the next table. However, threads claim blocks of indices to transfer via the field transfer index before doing so, reducing contention. Yeah, so if I'm transferring a bin, someone else shouldn't also be trying to transfer that bin. A generation stamp in the field sizectl ensures that resizings do not overlap. So only one recessing at a time.
00:42:11.625 - 00:43:14.485, Speaker A: Because we're using power of two expansion, the elements from each bin must either stay at the same index or move within a power of two offset. Yeah, so the idea here is that if you have, say, 64 bins, then what you're going to do is take every hash and do it modulo 64. So if you now double the number of bins to 128, then half of the hashes are going to be the Same value mod once mod 64, and half of them are going to be in the 64, more so in the in the lower half of the new 128 bins. You can work this out on paper if you want, but you'll basically see that about half, about half the hashes end up moving. Because of this, we eliminate unnecessary node creation by catching cases where old nodes can be reused because their next field won't change. On average, only about 1/6 of them will need cloning when a table doubles. Okay, so that's nice.
00:43:14.485 - 00:44:12.385, Speaker A: They sort of think about this fact where the things that don't move, and if the thing around them don't move, then we can just keep them the way they are and not do any copying of them. The nodes they replace will be garbage collectible as soon as they are no longer referenced by any reader that may be in the midst of concurrently traversing the table. Okay, so this is like the crux of all this, right, where we need to figure out how to deal with this, because for us, we don't have a garbage collector. So we can't just give out pointers to things because we sort of need to keep track of when no one has a pointer to something anymore so that we can free it. So this is going to be like a tough spot for us. One way to deal with this is like to we could stick every value in an arc. It's not particularly attractive, but it is one possibility.
00:44:12.385 - 00:45:18.615, Speaker A: Isn't Rust philosophy not to store locks inside the object or struct? Yes, but we will probably have to violate that here. Part of the reason is because reads are not going to take the lock. And so we actually we're going to have to work with that a little differently. We're probably going to use locks from the parking lot trait, which provides smaller, more granular locks. How can Rust support the optional tree node optimizations without requiring ORD all the time, even when it's not necessary? Or don't you want the optimization? Yeah, so this is one of the reasons I don't want to do the optimization now is because in Rust it's a little bit hard to say, have this tree map optimization if the type is also ored. For that I think you're going to need specialization. One option is to just always require hash and or.
00:45:18.615 - 00:46:05.249, Speaker A: But I don't know yet, I don't know yet exactly how we're going to do that. All right, we'll get back to how that works. Upon transfer of a bin, the old table bin contains only a special forwarding node with the hash field moved. Right. So for us it's not going to be a hash field move, it's just going to be a different enum variant that contains the next table as its key. And for us that's just going to be a pointer on encountering a forwarding node. Access and update operations restart using the new table.
00:46:05.249 - 00:46:52.481, Speaker A: Yeah, so it's basically saying you got here, you expected to find some stuff, go look here instead. Each bin transfer requires its bin lock, which can stall waiting for locks while resizing. However, because other threads can join in and help resize rather than contend for locks, average aggregate weights become shorter as resizing progresses. Okay, that's fine. The transfer Operation must also ensure that all accessible bins in both the old and new table are usable by any traversal. Right, so this gets back to the earlier question of what do we do with iterators? This is arranged in part by proceeding from the last bin table length minus one up to the first. Oh, okay, so when you.
00:46:52.481 - 00:47:47.559, Speaker A: When we have a set of bins and we want to move to a bigger table, then we're going to move the bins from the bottom to the new table rather than from the top. This means that if an iterator is walking at some point it's going to see the new table and then it no longer needs to go back to the old table because it knows that everything below there has already moved. Upon seeing a forwarding node traversals arranged to move to the new table without revisiting nodes to ensure that no intervening nodes are skipped even when moved out of order, a stack is created on first encounter of a forwarding node during traversal to maintain its place if later processing. Okay, I think this is going to be easier to understand when we look at the implementation. The need for these save restore mechanisms is relatively rare, but when one forwarding node is encountered, typically many more will be. So traversal uses simple caching scheme to avoid creating a. Right.
00:47:47.559 - 00:48:36.825, Speaker A: So the rough idea here is that you can take advantage of the fact that you know that once you hit one forwarding, you'll probably most of the subsequent things will probably also be forwarding things. The traversal scheme also applies to partial traversals of ranges of bins to support partitioned aggregate operations. Okay, so you can have iterators that don't necessarily start from the beginning. Also read only operations give up if ever forwarded to a null table, which provides support for shutdown style clearing, which is also not currently implemented. Okay, so the. The thinking here I think is imagine your program is shutting down. You can signal to the readers that the table has been cleared, basically that it's been freed.
00:48:36.825 - 00:49:28.915, Speaker A: So you can sort of empty the table entirely so that all reads will now fail without dropping the entire table. Oh, Norwegian. Nice. Lazy table initialization minimizes footprint until first use. This is the same thing the standard library hashmap does, where when you call new it doesn't allocate anything and also avoids resizing when the first operation is from a put all constructed with map argument or deserialization. These cases tend to override the initial capacity settings but harmlessly fail to take effect in case of races. Yeah, so this is the same as for hashmap if you do like if you use from Iterator for example, like if you collect into a hashmap, it will look at the length of the iterator if there is one, and then it will allocate something that can hold that many elements so that you avoid a bunch of resizes.
00:49:28.915 - 00:50:03.655, Speaker A: The element count is maintained using a specialization of long adder. So atomic use size. We need to incorporate a specialization rather than just use long adder in order to access implicit contention sensing. Ooh, that leads to creation of multiple counter cells. Interesting. Okay, so it sounds like longadr is actually a little bit more complicated than atomic U size. I think longadder actually does some optimizations to try to speed up if you have increments from multiple threads at the same time.
00:50:03.655 - 00:50:59.559, Speaker A: Like the creation of multiple counter cells makes me think that a long adder contains many of these. The counter mechanics avoid contention on updates, but can encounter cache thrashings if read too frequently. To avoid reading so often, resizing under contention is attempted only upon adding to a bin already holding two or more nodes. Okay, this bit I think is going to be much more obvious once we start looking at the implementation tree bends we're going to ignore for now. Yep, ignore tree bins. Maintaining API and serialization compatibility with previous versions of this class introduces several oddities we leave untouched but unused constructor arguments except weird arguments. Yeah, we can just ignore those.
00:50:59.559 - 00:51:32.001, Speaker A: We probably won't do serialization in this particular stream either. Great. All right. So in theory we now know sort of the internal and external design of this, which means that we can start the Rust one. So source Lib don't need this test. What I'm going to start out doing is actually, I think technically we need this. Unclear.
00:51:32.001 - 00:51:54.945, Speaker A: I'll deal with the license later. So we probably want all these doc comments to actually be retained across the Rust version because they're. They're very helpful to someone who wants to explore the code. Right. So let's leave those for now. These constants we are going to want though. Interesting.
00:51:54.945 - 00:52:37.867, Speaker A: Yeah. So we're going to do, I think all of these. Basically we're going to need. Probably not going to need the tree ify things. I'm going to tidy this up. I'm just going to copy all of them over transfer stride. So these we're going to need encodings for node hash fields.
00:52:37.867 - 00:53:01.805, Speaker A: So this we're actually not going to need. Also I think our hashes are going to be 64 bits rather than 32 bits, so we're going to have to make sure. That we actually deal with that. We're not going to need to use the sign bit for anything. We're going to use the enum Instead. Number of CPUs. That's going to be an interesting bit.
00:53:01.805 - 00:53:29.665, Speaker A: All right, so these I think are basically the constants. Maybe readers could be forced to hold some guard object. The drop of the guard would tell you that the reader is departed, allowing you to know when to drop all data. Maybe words, arcs everywhere. Yeah, that's what I'm thinking too. That we'll end up with some kind of guard. I think what we'll end up doing is something a little bit like cross beams.
00:53:29.665 - 00:54:09.995, Speaker A: They have this notion of a. I think they call it a pin, which is maybe a little unfortunate. So if you look at epic. Yeah, so you have this notion of if you look at an atomic. In order to load something you have to provide a guard and it gives you back a shared object that holds that guard. And so I think we're going to end up doing something like this. We might even be able to use these types directly.
00:54:09.995 - 00:54:54.725, Speaker A: The real question is going to be whether. Yeah, we're going to have to figure out whether we can use this to get the raw pointers. Maybe unsafe. Unsafe. Yeah. So we might be able to just use this for. Use this for node actually.
00:54:54.725 - 00:55:44.507, Speaker A: Like what does this do? Yeah, exactly. So instead of using box node, use atomic new and then just use this API for all our accesses. Might be one way to do it. It would mean that like our gets basically all our operations are going to take ref guard just like crossbeam does. And then we're just going to propagate that down into crossbeam somehow. Interesting. Yeah, we'll have to figure out how we do that along.
00:55:44.507 - 00:56:14.005, Speaker A: Adder is not an atomic U size. It is an array of counters that are given to the threads to reduce contention which is summed when you need to read it. Yes, that's what I said that it was the long adder seemed more like it was something where there are multiple counters to reduce the contention as that seems to be the case. Okay, so this seems like a useful type we might be able to use directly. He said you won't need concurrency level constant either. That's from old implementation. Yeah, I think you're right about that.
00:56:14.005 - 00:56:56.697, Speaker A: Unused but defined for compatibility. Great. So that can go away. Also we're going to make these rust style things. This is going to be a const. That's probably fine. Is further required because the top two bits of 32 bit hash fields used for control purposes.
00:56:56.697 - 00:57:18.565, Speaker A: So that won't actually be the case for us. So I wonder whether we won't. For us, the maximum capacity I think is just going to be use size, max value. Or rather. That's fine. Let's leave that for now and we can come back to it later. This is fine.
00:57:18.565 - 00:57:51.035, Speaker A: This will be a const. Usize as well. Needed by 2 array and related methods. Max array size. What a weird number. I don't think we're going to need this. The load factor.
00:57:51.035 - 00:58:30.515, Speaker A: Interesting. I wonder why that's final. All right, so great. So this is going to be a const. Why can't I type const anymore? It's going to be, I guess, F64. The max size would be very different for us since we're not using native values. Yeah, exactly.
00:58:30.515 - 00:59:12.935, Speaker A: It should be at least default capacity. Why do I keep typing con c min transfer stride and this. That's probably fine. Resize stamp bits. Interesting. This can probably be a U8 or something. But let's leave it to U size.
00:59:12.935 - 00:59:58.519, Speaker A: It's a constant anyway. It's fine. Oh, interesting. Okay, now I'm just sort of mechanically turning these into. Into Rust things. This I want to wait a bit with because ideally we won't even need it when is use. So use size is sort of used for anything that references.
00:59:58.519 - 01:00:29.225, Speaker A: Is going to refer to memory either like in terms of pointers or in terms of memory lengths. Y64 for float. Why not? We can make it F32 if we think that's sufficient. Probably don't want to use load factor explicitly in any of your code. It mentions itself that uses bit shifting instead. Yeah, the fact that it says isn't normally used makes me think that there might be code that actually needs it. But we'll see.
01:00:29.225 - 01:00:44.965, Speaker A: We might as well leave the constant in there. Right. I think it was effectively optimized out. I think it's cruft. This is not being honored anyway. Yeah, so we're probably. This is where Rust is going to be pretty helpful and tell us stuff that isn't used.
01:00:44.965 - 01:01:16.725, Speaker A: And also because we're doing a smaller implementation, if we don't use a constant, the compiler is going to tell us that that's the case. All right, so that's all the constants. This seems related to streaming nodes. Right. So node is going to be like a key type for us. In fact, I'm just going to get rid of that. So these constant we're going to keep and then we're going to say mod node because I think that's going to be large enough that we're going to want it in its own file.
01:01:16.725 - 01:02:06.131, Speaker A: So this is going to be pubcrate. A node is going to hold a key and a value and I guess we might as well keep the docs from here as well. Right. So keep in mind, a node is each of the things that appears in a bin. Right, interesting. So it says here this class, this type is never exported out as a user mutable map entry. So one that supports set value can be used for read only traversals and in bulk tasks.
01:02:06.131 - 01:02:18.865, Speaker A: Yes, I think the. I think this is something. Exactly. It's just going to be pub. Crate. It's never actually going to be pub. In fact, I think this whole comma just doesn't apply to the Rust code.
01:02:18.865 - 01:03:21.855, Speaker A: And we are going to have this, we know, is going to have multiple types and I think we're just going to have sort of a regular which is going to hold just a. Maybe this is really bin entry and then this is node. I think that's really what we want. That's going to be a struct of kv. Entry in a bin will generally be node. Will generally be node. Any entry that is not first in the bin will be a node.
01:03:21.855 - 01:03:43.365, Speaker A: I might clean that up later. Oops. Right, so this is gonna have. Oh, I see. It might be that some of these are not actually. No, they're all static. Weird.
01:03:43.365 - 01:04:00.817, Speaker A: Yeah, they're all static. Weird. Okay. Okay. Hash. They actually store the hash, which is interesting. So they're gonna memoize the hash, which we probably want to do as well.
01:04:00.817 - 01:04:21.713, Speaker A: We don't want to rehash things. We don't need to. I forget exactly what the. Give me hash. I think it's U64, but let me double check. So hash gives me a hasher gives me a U64. Yeah.
01:04:21.713 - 01:05:07.789, Speaker A: So we're going to have hash, which is going to be U64. We're gonna have key which is gonna be K. We're gonna have value, which is gonna be. This probably has to be an unsafe cell of V actually, because we are going to want to be able to mutate. Yeah. We're going to have to have a read only reference to the node and still be able to mutate the value. If someone does an update and we're going to have to.
01:05:07.789 - 01:05:35.965, Speaker A: That's going to be unsafe, but the V can change. That's important. Do we care about the memory footprint of U64 hashes versus U32? Unclear. However, I don't think we have a Choice given that U64 is what Hasher returns. Unless we wanted to add our own hasher trait, which I don't think we want to do. And it has a pointer to next. So next is going to be.
01:05:35.965 - 01:06:23.553, Speaker A: If we're going to use I guess cargo Tom will here cross beam equals what is crossbeam at 0.7. So this is going to use crossbeam. Epic Atomic. So this is going to be an atomic node. Kv what is a cell? So cell is. Cells are types that generally are types that give you interior mutability. So the ability to have an immutable pointer to something or an immutable reference and they get a mutable pointer to the underlying data or to modify it some other way.
01:06:23.553 - 01:07:17.665, Speaker A: Unsafe Cell in particular is one where you can get immutable pointer to the thing inside even if you just have an immutable pointer for like any type but the operation is unsafe, we could take the low bits of the hash. It's true. But I think we're just going to stick with U64 and see, I don't want to prematurely optimize this constructing this. I guess we can provide a constructor that's fine. Arguably these fields probably should just be pub crate. That's fine. Don't care too much about these.
01:07:17.665 - 01:08:02.455, Speaker A: Don't care about implementing EEK for it. Find. Ah, so find is what you use to. If you have a pointer to something to some node in a bin entry and you want to sort of chain the calls until you find an element that matches the key type, then you call find on it and find. We actually want to be implemented on bin entry, not a node because if you hit something that is not a node, you want the find to chain to somewhere else. And this is what it says here too overridden in subclasses. So my guess is like what was it called? Forwarding node.
01:08:02.455 - 01:08:28.315, Speaker A: Yeah, its implementation of find does something completely different. Right. It if it finds. Yeah. So if you end up calling find on a forwarding node, then it is going to move you to whatever the next table value is. Right. So wherever the forwarding node forwards you, so you're going to start looking there instead.
01:08:28.315 - 01:09:11.441, Speaker A: And so this is why we actually want this implementation on bin entry. So each actually so the next field here, I wonder whether has to be bin entry. Oh, maybe not. Maybe bin entry is actually going to be a bin. We're going to have to be a little bit careful here. But basically I'm wondering whether next here should be a bin entry or a node. Basically.
01:09:11.441 - 01:10:13.535, Speaker A: Can you Have a node that's followed by bin entry that is not a node and if not it should unclear. It might be that. That actually what we have is a struct bin and a bin has a sort of first and a lock. Hmm, we're gonna have to think about this one. No, I think you need to be able to atomically swap this. Let's stick with bin entry for now and then just see which I think means for now. This is going to be an atomic to a bin entry.
01:10:13.535 - 01:10:55.129, Speaker A: It might be that we can turn this into a node. Not sure yet. When was pubcrete added? Oh, that was added a while ago, I think. Doesn't next need to be an option? No, atomic can be null. So that's the way we're going to express that. I don't think we want binary to be a trait because then we would end up with dynamic dispatch everywhere, which although arguably that's what binantry provides anyway. But we probably want the ability to do things like match on this, right? We want to see.
01:10:55.129 - 01:11:38.965, Speaker A: We want to look at a given entry and look at whether it is a forwarding or not. So we actually care which of the variance it is. If we didn't, the dynamic dispatch would probably be just as fine. All right, so this is going to provide also pub. Create a find method and find is going to. This is going to be a pain. It takes a reference to self and it takes hash and it takes a key of type, I guess K.
01:11:38.965 - 01:12:46.981, Speaker A: So here we're probably going to end up pulling the same trick as the standard library does of taking a queue that can borrow K. But for now let's just take K and it is going to return a reference to a node to an option because this can return null an option node. Now this should just be known. This is already going to maybe cost us some headache because here we're taking a reference to a node, but we're actually going to need to guard this access. So I think we're going to need guards for like all of this because here there's going to return a reference to a node somewhere in the. In the. In the bins, right? Now imagine that node gets deleted.
01:12:46.981 - 01:13:40.305, Speaker A: How will we know when to free that node? So I think this is going to need a guard as well. But for now let's just write it out this way and then we can. We can deal with that later. So going back to our node type, I should have just remembered I should have searched where's find? Find. Okay, so find really just looks well we don't need K equals null. Well, first of all, this is going to be bound by where k is. Yep.
01:13:40.305 - 01:14:13.915, Speaker A: And what find is going to do. It's going to match on self and if it hits a bin entry node, then what's it going to do? It is going to. They've actually structured it as a loop, which is kind of interesting. I don't know if we can easily do that. Actually. We can. We can make it a loop here.
01:14:13.915 - 01:15:09.271, Speaker A: Mute N start and a start and we want to loop while. Well, this is going to be sum while. Let some. I'll get back to why we're doing this later. Does E comply that the value can be hash? No. So notice that the hash is passing explicitly here. So we don't actually require the key type to be hashable because the hash has already been computed.
01:15:09.271 - 01:15:37.441, Speaker A: There's going to be some. The caller of find is probably going to require that the key is hash. Just need the ref and the match branch. No, but I still like to write them. I don't think it needs to be a loop as long as it's properly tail recursive. So Russ doesn't guarantee tail recursion. Let's just write it like a loop to try to match the Java code.
01:15:37.441 - 01:16:04.675, Speaker A: Anyway, I agree that really this should be solved with tail recursion, but I don't know that we want to. Although we do know the bins should not be long. But fine. All right, let's not make this more complicated. It needs to be here. We're going to do if the. So this is just going to be N.
01:16:04.675 - 01:16:29.073, Speaker A: If the hash is equal to hash and the key is equal to key. Right. Then we are done and we can return N. So this is where you should see immediately the problem. Right. This is a reference to. Actually in this case it's fine.
01:16:29.073 - 01:17:01.781, Speaker A: Maybe in all cases it's fine in the other cases won't be fine here if N next. And here we're going to do a load. So actually, let's see what the Rust code. No, the Java code does here while e.next not equals null. So this is just a relaxed load, which is kind of interesting. How is next declared here? Yeah, it's the rule.
01:17:01.781 - 01:17:14.925, Speaker A: Oh, it's volatile. Okay. So this is actually a. We're going to reload here. So this is where we're immediately going to need the guard because load requires that we call a guard. Right. So we're going to call this load method.
01:17:14.925 - 01:17:59.161, Speaker A: So we're going to have to give an ordering to say what is the memory ordering of this operation? Basically, how does it relate to what other threads are doing? And a guard because we're going to get back a shared and so this is where this is going to come in. Which makes me think so much that this is going to be a. A node. Java has to. Because Java never has tele recursion. That's true. The node contains unsafe cells so that we can update the value later, although we don't currently need that.
01:17:59.161 - 01:18:54.035, Speaker A: But we will. Okay, so this is also going to take a guard. So this is going to be from epic guard and shared. It's going to take a guard and it's going to return rather than this, it's going to return a shared of g and node. In fact, this whole signature ends up getting. Becoming pretty weird. I think this has to also take a G self because we might.
01:18:54.035 - 01:19:27.007, Speaker A: It might be stored in the current node. Yeah, it's gonna be. It's gonna be a little bit weird because how do I create. Can I create a shared out of nothing shared? Create a null. But that's not what I want. Basically the issue we run into here is I have to produce a shared. But what if find is called on bin entry and the first node matches? Then we need to produce a shared.
01:19:27.007 - 01:20:19.035, Speaker A: But it doesn't really need the guard from shared for atomic. Oh, it implements from any raw pointer. Interesting. But is that unsafe? It's apparently not unsafe, which seems odd. Okay, that's fine. Then we just do shared from N as const. Right.
01:20:19.035 - 01:21:06.965, Speaker A: So if self. If we have a reference to self that lives for glong, then if that entry has the node directly, then creating a shared with that guard is going to be fine. The real question is whether there's any. Whether shared holds some like, weird stuff that does things on drop, which I don't think it does owned. No, that only matters when someone takes ownership of it, which we're not going to do. Okay, so there's no drop. So I think that should be fine.
01:21:06.965 - 01:21:30.555, Speaker A: I have to figure out how this factors into. How this factor factors into memory reclamation. All right, but now we can write load. Right, so load is going to take. It takes self, it takes an ordering. So we're going to want sync atomic ordering. And this is where it was odd to me that they.
01:21:30.555 - 01:22:21.923, Speaker A: They just do a straight variable read, which I think is just relaxed. But I think in Java volatile reads of volatile reads of volatile values are always producer happens before. So I think that makes this a release acquire. But I'm surprised that doesn't appear in the code get reference acquirer well, and we give him the guard. And what does that give us? Then we get back. If we have an atomic, we do a load, we get back a shared and so that's going to be next. It's going to be this.
01:22:21.923 - 01:22:54.163, Speaker A: If next is null, then we reach the end of the bin and we return none. Otherwise we return some next which is a little weird, right? Like shared already encapsulate this notion of null. So the option isn't really necessary because when you get a shared it might always be null anyway. Okay, so maybe we just want this. I just. I don't like that I want it to be option. I'm surprised the chair doesn't like.
01:22:54.163 - 01:23:53.495, Speaker A: I don't like the fact that there's an is null on shared load is acquire in Java, I think by default. Yeah, that sounds about right. Great. And that's just what that code does, right? It doesn't do anything else fancy find just does a straight search of a linked list, which is easy for us to do. Spread high spread xors higher bits of hash to lower and also forces top bits to zero. Table uses power to masking sets of hashes to vary only and bits above the current mask will always collide. Because many common sets of hashes are already reasonably distributed and because we use trees, I don't think we need spread.
01:23:53.495 - 01:24:20.365, Speaker A: We can just ignore this. We're not going to do this. This is up to the hasher. It is not something that we should decide to do. When I wrap into a shared option with act like an option secretly is shared. I mean, that's sort of what we do by doing an option shared. But then shared still has an is null method, so callers wouldn't necessarily know that they don't need to check it anymore.
01:24:20.365 - 01:25:04.555, Speaker A: So I think we're just going to ignore it. Table size 4 Interesting. What is this even trying to do? I don't want to do these. These seem like they might be important later, but not right now. Compare comparables Great. Table element access Atomic access methods are used for table elements as well as elements of in progress. Next table While resizing all users of the tab arguments must be null checked by callers.
01:25:04.555 - 01:25:51.085, Speaker A: All callers also paranoia pre check the tab's length is not zero or an equivalent check, thus ensuring that any index argument taking the form of hash value and ended with length minus one is a valid index. Note that to be correct with regards to arbitrary concurrency errors by users, these checks must operate on local variables, which accounts for some Odd looking inline assignments below. Note the calls. Interesting. Now the calls to set tab at always occur within locked regions as they require only release ordering. Hmm. Tab at tab.
01:25:51.085 - 01:27:02.359, Speaker A: Interesting. So this takes. Wow, this is a weird ass function signature. I think this is really just taking a pointer to a node and then doing an atomic load with acquire ordering. Why this business though? Oh, this is a list of buckets and it's saying get me this bucket and that gives you the first thing in that bucket and it loads that with acquire ordering. Great. Table size, four rounds of the next power of two from the use displayed capacity for the map.
01:27:02.359 - 01:27:48.931, Speaker A: Great. There's a built in standard library method for that so we can just use that. The reason I didn't add it was more because we don't need it yet so we might as well add it later. Can you talk a little bit about these atomic orderings? Okay, so atomic orderings are a bit of a minefield and I am sure I'll get some of this wrong. But the basic idea is that unless you do something special, reading a variable from one thread and reading from a memory location from one thread and writing to that memory location in a different thread. Variable might be a better word. Here you are not guaranteed that the reader sees what the writer writes.
01:27:48.931 - 01:28:46.033, Speaker A: In general, it's not a guarantee that the memory model of C in particular LLVM and Rust as well, you are not given that guarantee. One thread could write one to it and you will keep reading zero. You are only guaranteed to see a value if there's a happens before relationship between the two threads. Basically, if there's a synchronization point or a synchronization something, if you tell the CPU that there's a relationship between these two values or between these two threads such that one thread must see the results of what the other one did. And so this is called establishing it happens before relationship. There are a couple of ways to establish these relationships. In general, the way you do it is by doing either a store with a release ordering and a load with an acquire ordering.
01:28:46.033 - 01:29:58.185, Speaker A: The acquire load is considered to. Sorry, the release store is considered to have happened before the acquire load and therefore the load happened after the store and therefore the load will see that store. The other is you can use the ordering in Rust this is called secust, but it's sequential consistency which synchronizes which is considered. All other operations on that variable is considered to have happened before this happenedbefore relationship also guarantees that all operations before a release have completed or all of their effects have completed and are visible before all the loads following an acquire. So following a load. So think of this really as if there's a happen before relationship between this instruction and this thread and this instruction and this thread, then the CPU will guarantee that all the stuff before on that thread happened before the things that come after on this thread. That is the relationship that it establishes.
01:29:58.185 - 01:30:53.235, Speaker A: This also happens with sequential consistency. But there I think it's all loads and stores that match up. In general, you can always sort of go the safe route of marking everything as ordering sequentially consistent and then you won't have issues with this. For high performance concurrent code, you sometimes want to basically give the CPU more freedom to move instructions around to more efficiently use memory. You are right, I didn't misspell acquire. This topic is notoriously weird so that there's a lot of writing online and like C memory orderings, LVM memory orderings. If you want to read more about this, I recommend like actually reading into the docs.
01:30:53.235 - 01:31:28.185, Speaker A: The jocks for Java's volatile keyword are also pretty good about this and talking especially about happens before relationships. So that's a good resource to read if you want more like deeper stuff. I believe volatile in Java implies sequential consistency. I don't think that's true. I think volatile in Java is acquire release. But I'm not sure it's a good question. I mean, okay, let's just make it sequential consistency and then it's like a to do to look back at that later.
01:31:28.185 - 01:32:37.605, Speaker A: Great. So now these are no longer going to be these. These are going to be. There's going to be shared null and this is going to be this Great. So if we go back to source lib, here's going to be our main like type which is going to be flurry table flurry map flurry hash map Take a knv and it is going to have bins which is going to be a vec probably back might actually be a box this which you can construct from a vec of node because we don't. We don't want this to grow ever. Right.
01:32:37.605 - 01:33:35.345, Speaker A: A vec implies that it can be grown and there's no reason for it to. This is going to be a bin entry KV right. So we're gonna have a bunch of bins probably. I sort of don't want this to be boxed. I might not be able to get away from it though. I actually want the bins to be in line in the map but I might not be able to because people are going to stick the map behind an arc. Anyway, there is, I think technically a way to have zero sized fields in Rust, basically to have a field of arbitrary length inside of a struct, I think if you put it at the end.
01:33:35.345 - 01:34:01.629, Speaker A: But we're just going to keep it this way for now. I think Java has its own formally specified memory model, which is stricter than x86. I believe that sounds right. I believe you. Which is why we have to be a little bit careful about translating the Java code as well. It's worth noting that ordering semantics are as much a directive to the Compiler as the CPU. On some CPUs, there's no actual override.
01:34:01.629 - 01:34:24.357, Speaker A: You're right. You're right. This is why explaining all the memory orderings is a complicated topic. It's a hairy topic and it's one that requires a lot of subtlety. So I think explaining it in the middle of writing code is probably not the best way to explain all the subtleties of it. So I recommend that you actually read some of these resources. Last element in the struct may indeed be not sized.
01:34:24.357 - 01:35:07.685, Speaker A: Yeah, so one way we could do this is like bins is going to be this, right? So this is an unsized type, which is basically gonna force people to put it behind an arc. But I kind of don't want to do that right now. So I. I'm gonna box this for now and then say inline this instead. It might actually need to be boxed for. It might even need to be an arc for resizes. Gonna have to think about that.
01:35:07.685 - 01:35:44.465, Speaker A: Access to volatile variables in Java imply a memory barrier. Okay, so sequential consistency should be. Should be correct. Then inline this instead maybe might not be possible. We're gonna have to figure that one out. You're gonna have to think about that. All right, so on flurry hash map we might need some intermediate struct here actually to make dealing with resizes a little bit easier, but I think we can do that later.
01:35:44.465 - 01:36:20.331, Speaker A: So we're gonna have a FN which is going to be called tab at just to match with the Java version. And it's going to take a bin. Why does it say tab at? This is the table. I see. It's table at. So really this is just at I usize and that's supposed to give me a shared. Great.
01:36:20.331 - 01:37:35.705, Speaker A: So this is also going to take a guard. Actually, no, this is not going to take a guard. This is going to straight up give me a reference and it's going to give me a node bin entry kv and this is get reference Acquire. This is interesting though because what does this actually do? I think this computes a pointer which is the table, plus it basically gets a. A pointer to the element at this pointer plus this value. What's a shift? Look for a shift. Unsafe mechanics.
01:37:35.705 - 01:38:30.205, Speaker A: Scale. Interesting. So this business down here is related to resizing. So these are the fields where when you resize the table, these parameters change. So this is why the shift here is 31 minus scale. Interesting. So scale array index scale of the node class.
01:38:30.205 - 01:39:09.015, Speaker A: Huh. What does array index scale do? They never change during runtime. Yeah, that's what's weird to me. I think this is. Yeah, I think this is just computing the pointer to the node. I think that's really what this is doing because this takes the node class. Oh man.
01:39:09.015 - 01:39:59.925, Speaker A: Why is this at the bottom of the file? It's annoying. It shift the eye by that much and then adds a. So this is array base and array shift. Yeah, so this really just is a pointer to the ith element. We want to do reference, acquire, load of which is very straightforward for us actually. So these are going to have to be atomics, right? So this is going to be a cross beam epoch atomic and we're going to have. Each of these is going to be an atomic to one of these.
01:39:59.925 - 01:41:07.845, Speaker A: And then so that's why I think this is going to be a guard. So at is going to take a G. It's going to take. It's going to be a tick g self and a guard which is going to be a guard. So this is going to also include is atomic shared and guard and it's going to return a shared one of these because this is going to take self bins. So the reason it has to do this is because this is also going to be an atomic for handling resizes. We might have to atomically swap this value.
01:41:07.845 - 01:42:17.085, Speaker A: This is going to be self.bins.load and that load is going to take acquiring order acquire and the guard that gives us a shared to the slice and then we're going to have to map. So that gives us a point. This ends up giving us a pointer to the slice and then we need to get a pointer to the appropriate bin which is going to be the bin I which we're then going to have to load. Might need a map in here actually. Oh, maybe not. And then we're going to load here.
01:42:17.085 - 01:43:15.845, Speaker A: We're going to load here with ordering acquire. The real question is whether the. When the Java code here does. There are two pointer reads here, right? One is the Pointer read of this value and one is a pointer read of the pointer to the node entry and whether they both both need to be acquire Unclear. Let's look at a call to tab at. So tab at gives tab, which is table, which is a read of the field self table, which is a volatile. So I think that means this is going to be a sequential read of bins.
01:43:15.845 - 01:44:21.185, Speaker A: Yeah, which makes me think that actually what we're going to want here is a this and then we're going to have a struct table kv, which is going to have these operations and this is going to be an operation on table and this is going to be an operation on. Yeah, great. That's going to be better. So table is going to hold a bins and this is going to hold a table. Great. Okay, so sorry. The reason I made this change was when you resize, you're going to allocate a new table, right? So we're going to have an atomic pointer to a table and that's sort of the base of the map and if it changes you're going to have to switch to the other table.
01:44:21.185 - 01:45:01.565, Speaker A: Right. And so we really want some notion of a table that's separate from the notion of the hash map. And so that's basically the distinction I made here. And now table just becomes self dot bins I and the reference to the bins, how that is determined, it doesn't really care about. Right, it doesn't. The implementation of AT just says if you have a reference to a table already, this is how you get to a particular bin. And then it's going to be up to the caller of this method how they get to the table in the first place.
01:45:01.565 - 01:46:19.275, Speaker A: So that's going to be this. I really want like something like get something to basically. Great, let's just implement get because I think that's going to help us a bunch just to get the overall flow of the system. So we're going to pull KV flurry hashmap kv, we're going to implement get, take self, it's going to take key, it's gonna have to take a guard and it's gonna return an option. See, I really don't like the shared can be nice, but it's going to have to be option shared V. Yep, missing the struct keyword in pubflurry hashmap. Yes, you are right.
01:46:19.275 - 01:46:57.875, Speaker A: Does atomic bin entry mean that there's an extra pointer between the array element and the bin entry discriminator for the enumerated? Yeah. So atomic does A heap allocation. So here, this is where I mentioned in the very beginning of the stream, Java heap allocates all objects. Basically. Not quite true, but it's basically true. And so we're doing the same thing here where each node is going to be a separate heap allocation and each table is going to be a separate heap allocation. And part of the reason we need that is because we need to be able to do atomic pointer swaps.
01:46:57.875 - 01:47:44.545, Speaker A: I hate the fact that you can access members of a class without this in Java. Yeah, me too. It's having to figure out that it really means like self dot is annoying. Looks like the intent is to store the offset in array element aligned increments rather than raw pointers. I see. So the intent you say is for these. It's going to be the same thing.
01:47:44.545 - 01:48:06.107, Speaker A: It's going to be the same thing. Oh, the KK isn't doing anything. It's just I was control K is escape for me. So when you see case it's just I forgot to hold control. All right, so let's look at get and what get actually does. It declares a bunch of variables. It gets the hash.
01:48:06.107 - 01:48:39.505, Speaker A: Okay. So the first thing we're going to have to do is get the hash. I can never forget the exact details of hasher. So let's just do this here. We're probably going to want the hasher to be something that we're generic over. So if you look at hashmap hashmap, you'll see that hashmap and you may not have seen this in the past is generic over the key value. And this S and S has to implement.
01:48:39.505 - 01:49:32.385, Speaker A: Where are you Build hasher and build hasher has a thing that lets you build a hasher. It's like a whole chain of things. And I think we actually want basically the same thing here. We're going to say that our map is going to be this. The S equals here means that if you don't specify any we'll use random state. Random state is from collections hash map random state. And so this is going to take any S where S implements build hasher where build hasher also comes from here.
01:49:32.385 - 01:50:45.825, Speaker A: And build hasher is the thing that lets us create a hasher which is the thing that we use to construct a hash. So we can do this is going to be. We're going to have to actually star the hasher to S build hasher and then we're going to do like the actual hashing which I forget what the that gives you a hasher and with the Hash we're going to do. No, I don't want hasher, I want the hash trait. Right. So here we're going to require that K implements hash. And as long as K implements hash, then we can call key hash and give it a mutable reference to a hasher.
01:50:45.825 - 01:51:06.835, Speaker A: Yep. And then we. The actual hash is going to be H. Finish. Yep, finish gives us the U64. Great. So H now is the final hash for that key.
01:51:06.835 - 01:51:34.925, Speaker A: So the guards. I should. I should have mentioned this earlier. So crossbeam guards are basically ways to track one sequence of operations. So you. In fact, I think the docs for it might be useful. Just look at the docs for this.
01:51:34.925 - 01:52:27.265, Speaker A: The docs for guard is basically you. You create the A thing on your stack and that when that thing goes away, you are guaranteed that any pointers you loaded you've also discarded because all the pointers are tied to the lifetime of this pin. Right? And so the guard is really just there so that at some point you know that you are no longer holding any references into the target data structure. You've let go of all of them. And then you can combine this with a memory reclamation scheme. Because anytime the. Anytime the guard is dropped, you know that any of the references you took while you held the guard have also been dropped.
01:52:27.265 - 01:53:17.027, Speaker A: And therefore you might be able to do reclamation. So if you look at epic, you'll see here that basically what you end up doing is that if you want to remove an object, then you stash it away until all the guards have gone away. And then what we're going to do is anytime we accumulate garbage, we're basically going to note down sort of the current time anytime a guard is dropped or when. When guards are dropped, all threads increment their current time. And then when all threads have moved on from when the garbage was produced, we know that none of them have access to the garbage anymore. So we can clear that garbage. You'll see this when we get to the actual garbage collection.
01:53:17.027 - 01:53:49.875, Speaker A: But suffice it to say the guards are there to note down when you have finished using all the pointers that I gave you. I think you missed a self when calling build hasher. Yes, you are right. This should say self hasher if we need mute hasher. No, so the self. Hasher, this is. I should have called this build hasher.
01:53:49.875 - 01:54:04.975, Speaker A: It's not actually hasher. So this is a thing that builds a hasher. So H here is a hasher. Whereas S is a built a thing that can build hashers. So we actually Own this hasher. And that's why this is. This is fine.
01:54:04.975 - 01:54:48.273, Speaker A: Okay, so now we have the hash. What does it want us to do next? It does this spreading, which we're going to just ignore spreading. And it has this code, it reads the table, right? So table is going to be self table load ordering. And we said that reading any volatile in Java is equivalent to secular const. And notice here that we're using the guard because the pointer we get back here, we want to keep track of how long we hold on to that pointer to. Because if we create garbage, we need to know when to free it. This will.
01:54:48.273 - 01:56:03.355, Speaker A: The guards will become a lot clearer once we actually start dealing with freeing. All right, so if, if table is null, then we're just going to return none. Tab dot length. All right, so we need to find length. How is length declared? Keys elements. Where is length? No, man, where is length? Have I gone through the file? Like, am I starting? No, still going down. Where does length? Oh, it's just the length of the.
01:56:03.355 - 01:56:27.535, Speaker A: Oh, right. This is length. Here is the number of bins. And so the number of bins doesn't need to be atomically loaded. That's what confused me. So if table dot. So here's where this is kind of awkward.
01:56:27.535 - 01:57:20.709, Speaker A: If table dot bins dot len. Actually, yeah, length is zero, then we return none. Because obviously if there are no bins, then no key is present, clearly. Great. Okay, so that's that check and tab at right, so this is. We want to n minus 1, right? So n here is the number of bins, which we know is a power of two. N minus one is give you.
01:57:20.709 - 01:58:27.475, Speaker A: Is going to give you ones for each one. So let me write this down, it'll be easier. So imagine that your hash is something like this, right? It's not actually what your hash is, but let's say that your hash is something like this. If we have, say, take a trivial example, and let's say that we have four bins, right? So four bins means that we have two bits to store how many. Or four bins means that this is 0B1 00, right? So that's a binary representation of four. And then when we do the binary representation of four minus one, that gives us 0B011, right? And then we're going to end, we're going to take the hash. So this is the hash, this is the mask.
01:58:27.475 - 01:59:08.525, Speaker A: And then we're going to do hash and mask, which is going to be just the last two bits. Let me do this. Change that to a zero, that's going to be 0B 011 ended with 0B. Lots of things 010 which is going to be 0B 10. All right. So you might ask, why does it matter? Well, crucially, this has only leading zeros. Right? So we've basically cleared all of these bits and we now know that the value we get back fits inside the number of bits we have.
01:59:08.525 - 02:00:16.925, Speaker A: Think of this as like it basically takes the low bits that can index into the number of bins that we have. Which means that here we're going to do let mask is going to be table bins len -1 and then we're going to do the H with the mask. This is going to be as U64. So this is going to be the I or let's call it bin and then we are going to use tab at right, so we're going to do table dot bin given the guard. So this is going to give us the actual bin. Oops. So this is the bin I at that.
02:00:16.925 - 02:01:13.361, Speaker A: Do we need the guards in the public API of the map? We prov. Yes, it depends. So if we didn't, we couldn't provide a get method. The problem is that get, I guess you could wrap the. That's a good question. Yeah, you actually, you couldn't provide get if otherwise, because get returns a reference to a thing that's inside the map. And so the question becomes how do you know when that is no longer needed? You could imagine sticking the guard inside of the return type.
02:01:13.361 - 02:02:22.025, Speaker A: But all of our methods take the guard by reference and so if you move the guard into the return type, then the lifetime of those references would no longer be valid. Right. And therefore the borrow checker would say no. And so the alternative is to provide an API that's something like pubfn get end, right? Takes an F which is like an FN ones and it's given a value and returns an R. I mean this is an API we could totally provide, but so this is going to take ourself K and then which is an F and it's going to return an option R. Right? And then this is going to do guard is crossbeam epic pin. Then it's going to do let V is guard.
02:02:22.025 - 02:03:09.795, Speaker A: Actually it's just going to do guard dot sorry self get key guard map then or then of dereference the V. So that's one way to do that. But then of course accessing is a little bit annoying because you can't just hold on to the. Hold on to the reference and so we can provide this interface where the user doesn't have to give a guard. But if we do, we can't give them a reference that can just last for however long they wish. So I mean, keeping this is fine. We can do that.
02:03:09.795 - 02:03:44.845, Speaker A: Okay, so we've looked up the bin and if the bin is null, then we return. So this would be the case for like if the bin is. If the map is being destroyed, for example, then that. That is the only case where I think that second one can be none. Because we want bin I to be usize anyway. Isn't it better to do an H as usize instead of Len as U64? Yes, you're right. This H.
02:03:44.845 - 02:04:42.085, Speaker A: Well, the trouble becomes what if. What if you size is like U32 and the hash is U64? I think we want to do the masking operation in the hashes native value space. And then, and then we want the. Then we want this as us after the mask because that way we guarantee that the. The mask has the same number of bits as the as the hash does. Otherwise if we did H as us, we might truncate the hash value before we do the masking, which we don't want to do. In practice, this will probably never really make a difference.
02:04:42.085 - 02:05:08.555, Speaker A: But I think this is a little bit more correct. Right, so this bin is not null. And now if. Right. Obviously. Interesting. I wonder why this doesn't just call find.
02:05:08.555 - 02:06:21.379, Speaker A: Because these operations are what find does anyway. So I think this should just say node equals bin.find given the h and the key, which makes me want to. This is no longer option if node is null. See, this is why I really don't like this shared can be null business because it means we have to keep adding these checks and we can't do something like use the question mark operator. We're truncating with the mask, but we don't want there to be an additional bit of truncation. So specifically I don't.
02:06:21.379 - 02:07:13.225, Speaker A: It could be the case, but I don't know that casting a U64 to a U32 is guaranteed to just keep the low bits. I assume so. I assume that's how the cast is defined, but this way we're explicit about it. Couldn't you return something like a mutex guard? You can't. You can't do that with a crossbeam API. It might be that there's a way for us to do that unsafely, like find a way for us to move the guard without invalidating the lifetime that the things are associated with. But I think it's going to be hard, right? Otherwise.
02:07:13.225 - 02:08:15.599, Speaker A: So if the find succeeds. If the find succeeds, then we're done, right? Then we just do a. Then we just do a sum shared from node, dot, value, dot, get. So one question is, why is this safe? It's a very good question. So here. So remember how value was a. Did I call it value? I think so, yeah.
02:08:15.599 - 02:09:01.155, Speaker A: Value is an unsafe cell, and so value is an unsafe cell. And so therefore. And the reason we made it that was so that someone else can modify the value in the node. Like if someone does a replace basically for a given value, then we need to be able to change the value. But that's not okay if someone still has a reference to the old one. So I think this might actually not work. I think that we're going to have to also make this a.
02:09:01.155 - 02:09:25.845, Speaker A: An atomic. And we can probably check this by looking at. For. For our node type. What is the type of value? Also, if at any point you feel like I say something and you can't quite follow it, let me know and I'll stop, slow down and explain it. Like, this is. This is subtle enough that that's worth doing.
02:09:25.845 - 02:10:08.735, Speaker A: The value is a volatile. So my guess here is that when they access the value, they do so by pointers. Yeah. I think the only reason why this all works out is because at no point you give out immutable reference to the value. You can only read it or replace it. And the only way we can guarantee that is by using this. The atomic business.
02:10:08.735 - 02:10:48.989, Speaker A: Yep. Which means that here, this is actually going to be an atomic V instead. And then we're going to use that atomic to give us the ability to replace a value or to load a value. Those are the two options you're going to have. And if you replace a value, you're not allowed to drop the old value yet. You need to like stick it in the garbage collector because there might still be readers that have access to that value. Name of the font I'm using.
02:10:48.989 - 02:11:39.935, Speaker A: I'm using the font called Noto and I like it a lot. It's a nice font. So this is going to be a load and so we don't need shared from and we determine that load. This is a sec const and it takes a guard. And here I want to do let V is equal to this and I think I want to assert that V is not V is not null and then return some V. All right, so now we have get. Let's do insert as well.
02:11:39.935 - 02:12:05.339, Speaker A: And then Just see whether we have something that sort of works. Oh, we're gonna have to do resizing for that too. Okay, so we have get. So I guess now we do put. Right, so we're gonna do pubfn put self. It's gonna take a key and a value and it's gonna return an option V. Right.
02:12:05.339 - 02:12:41.545, Speaker A: So put, I guess insert is what we usually call it in Rustland. Insert returns the old value. If there was an old value there, it is not clear that we can do that. I think we're going to have to return here what they call an owned. Although I think into owned is unsafe. Yeah, so this is tricky because. Okay, so imagine that I replace a value.
02:12:41.545 - 02:13:38.157, Speaker A: There might still be readers that have pointers to the value, the old value. And so I can't actually just give back the V because it might not be ready yet. As in the caller doesn't actually get to own V. I think all I can give them, all I can give them here is a shared V. If anything. This is a little awkward. The basic idea here is that if you call insert the old value, there is some point in time where you are allowed to own that value again.
02:13:38.157 - 02:14:15.625, Speaker A: But that is when the. When the epoch is rolled over. So when all current consumers of the map have sort of moved on. Only at that point are you allowed to claim V as your own, given that it was removed. So this is almost like the V is like a reclaimable type of thing. Like at some point in the future you can say that I would like to own V now, and it's going to wait for the next global epic. So I think for now this is actually just going to return nothing, and then we'll have to deal with that in a bit.
02:14:15.625 - 02:14:42.227, Speaker A: Okay, so what does put do? Well, it calls put val. Okay, so that sounds an awful lot like we're going to need put VAL as well. So that's going to be put. Notice also that these methods all take immutable references itself. And that is intentional. Right. As we mentioned in the beginning, this is supposed to be a concurrent map.
02:14:42.227 - 02:15:19.165, Speaker A: And so the methods better not take mutable references. All right? Takes a key, takes a value, and it takes if absent, and it returns. They claim that it returns a V, so it's going to not return that for us because we don't know how to do that yet. We don't need to do null checks because this is not Java, which is nice. We have to hash. And so this is the same thing we had to do for get. Right.
02:15:19.165 - 02:16:10.505, Speaker A: We might Actually want a FN hash takes itself. Takes a key and gives you a. Gives you a U64 just so we don't have to like repeat the same code all over again. And so now we can just say let H equals self hash key. It's a little bit nicer. It also means that if we do things like spreading, we can implement it on this function rather than having to implement it everywhere. Can return a future to a V.
02:16:10.505 - 02:16:48.255, Speaker A: Yeah, so in some sense it is a future to a V. I think for that to work, we would sort of need crossbeam to support that idea. But it is a neat way to express that. Like if crossbeam had a way of saying basically of giving you a future, you could wait on until the next epic, that might work. I don't think it currently does, but that would be a nice addition. It would make for a nice API too. Bin count.
02:16:48.255 - 02:17:42.092, Speaker A: So what does this do? Well, it reads table. So this for loop is interesting. This for loop reads table and then does nothing with it. So this isn't really. This is an infinite loop where at the start of the loop you read table each time. So this is equivalent to this loop guard is crossbeam, epic, pin and guard. So the reason we need this and then here do all the operations on the table is because imagine that you do some operations on the table and then you realize that a resize has happened.
02:17:42.092 - 02:18:17.715, Speaker A: Well, then you need to sort of start over, right? You need to go back to the table where you were. You need to go back to the. You need to read table again to get to the new table. And that is what this will let us do. If table is null or table.bins.len is zero, which I guess is really just is empty, then init table. Ooh.
02:18:17.715 - 02:18:40.507, Speaker A: So I guess this means we're going to have to implement init table. Oh, I see. It's also for this reason. Now you can just continue here. This is just one. I wonder why they did this as ifs rather than with continues. Because this is really just if that then continue.
02:18:40.507 - 02:18:56.145, Speaker A: So that way you don't need to stack these. In fact, this is arguably just. You don't even need to continue. But sure. Oh, it's. I guess it's. Imagine that two threads both hit this condition at the same time.
02:18:56.145 - 02:19:23.294, Speaker A: They both call initable. One of them succeeds, the other fails. If we didn't do this, then table would. I mean, even if we did like table equals this table can still be null after this for the thread that failed. So it needs to Read table again. So that's why this loop is here. If ah, this is interesting.
02:19:23.294 - 02:20:37.035, Speaker A: So that what it's now going to do is last n minus 1 minus hash I equals n minus 1 and hash. What a weird inline assignment. So this n minus 1 and hash is really just bin I. Right. So we're going to probably want a. We definitely want this to be inlined fn bin which takes a reference to self and a hash and returns you a usize. And that's just the code that we wrote up here.
02:20:37.035 - 02:21:27.931, Speaker A: Self binslin that this and now this can be table.bin h I guess this should be bin probably. And now we can do the same thing down here in put where bin I can be this. And then what else does it do? And then it looks for the tab there. So we're going to do the same operation that the get did. If bin is null. Right.
02:21:27.931 - 02:21:52.145, Speaker A: So the case we're looking at here is we're going to look up the bin. Right? That's what this does. And if there's nothing in that bin, then we can do the easy case, right, of just taking. Creating a new node and just putting it there. That's all this is saying that this is sort of the fast path. Right. And the slow path is bin is non empty.
02:21:52.145 - 02:22:23.079, Speaker A: Need to link into it. Sort of. Can we pin a thread inside the loop? Yes, we can take this guard here. I don't think it makes much of a difference. I think it's going to be a slightly higher cost. Think of it as you want to hold guards. Not for very long because as long as you're holding the guard, you're holding up the epic.
02:22:23.079 - 02:23:08.735, Speaker A: So you're holding up any memory reclamation that might happen. But here, like whether you include the initial checks here or not probably doesn't make much of a difference. I guess actually one way in which this might make a difference is that here we might want to drop the guard. Don't hold up memory reclamation. You're right. While resizing, while constructing. Rarely relevant.
02:23:08.735 - 02:23:33.395, Speaker A: But there is a case where we do a resize where we might want to drop the guard. Unclear actually. But maybe there is some cost to creating a new pin too. So I'm not sure what we want to do there. All callers also paranoiacally pre check. Oh right. This is the doc string read earlier.
02:23:33.395 - 02:24:14.627, Speaker A: This is. You're referring to this? No, paranoia. This is this comment Callers check that the tablet length is not zero. That's fine. And it's a valid index. Why is that relevant here? This doesn't actually check that it's a valid index. This does that check.
02:24:14.627 - 02:25:02.065, Speaker A: This is the check they're talking about. I don't think the assignment to the local is explained by that comment though. Note that to be correct with regards to arbitrary concurrency errors by users, these checks must operate on local variables which accounts from odd looking inline assignments. That doesn't actually explain why they do this here. Interesting. Would have been interesting to see what that is. Actually what we should do is find the commit that adds that comment because it probably has like either it adds a test case or it like explains what the actual issue that went wrong was.
02:25:02.065 - 02:26:23.835, Speaker A: In any case in this fast case is cast tab at Cast tab at used in more than one place. It is okay, so this does not need to do that. It is just going to do. It takes an I and a nodes. Oh right. Current it's going to be a shared node bin entry KV and it's going to be a set new which is going to be a node bin entry kv it's not going to return anything actually it might have to return something. It returns a bool apparently.
02:26:23.835 - 02:26:56.009, Speaker A: Yeah, that makes sense. And this is going to compare and swap, right? So the idea here is that current is probably going to be null. It's not clear that it will always be null. But the idea here is that something is currently the first bin and we're and we know what the pointer to that is. So that's current and as long as that hasn't changed. Right. If some other thread had changed it, that's a problem.
02:26:56.009 - 02:27:48.365, Speaker A: But as long as that hasn't changed, we want to replace it with this new bin entry that we made. So the example for insert is the start of this bin is currently a null pointer and if it is still a null pointer we want to set it to point to this instead. Right. And so that's what we're going to do here. That's the compare and swap. And we're going to have to look at what the Compare and swap. Compare and set I guess is what we want actually which takes a current it takes a new what is P here? P is a pointer.
02:27:48.365 - 02:28:25.315, Speaker A: What is pointer? Oh, I see. We just need to allocate it. That's fine. So we're going to. This is going to do what does compare and set Return probably returns the pointer if it. Great. I think what we want is a result nothing or it gives you.
02:28:25.315 - 02:29:43.075, Speaker A: So this is going to take an owned this and it's going to if it failed it's going to give you that bin entry back so that you don't have to allocate multiple times just because you failed. That's what we want. And owned here is also from Crossbeam. So an owned is a value that you know that no one else has access to. So now for cast bin, it's going to take the current, is going to take the thing that you want to place there, is going to compare and set the current to the new and gives you this. Okay, so the question becomes what does it return? It returns a result which is a shared tag is trunky. I don't know what to use the tag for, but that seems nice.
02:29:43.075 - 02:31:14.345, Speaker A: The only thing I'm surprised by is when we do this, what happens to the previously owned value, right? Actually, so if this succeeds, then what we get back is a shared to the node that was removed. And if the node that was removed is not empty, then we're going to have to free it at some point. And so this result actually needs to give you back the shared node bin entry kb, which means that it's also going to have to take a guard. So this is going to take a shared list. So the idea is that if it succeeds, we're going to have to do something with this business like basically stick it in the set in put it in the queue for garbage collection later, right? If it failed, then we didn't change anything. But this owned we need to deal with now. And so what is this compare and set error there? We don't actually care about current probably.
02:31:14.345 - 02:32:13.395, Speaker A: So when a compare and set fails, it tells you what the only way it can fail is if the current pointer changed under you. Like it used to be null and now it's not null. And so rather than having to do another load, you can rely on the fact you now know what the value is. And so that is actually a value we probably want to keep. So we might as well just return this directly, at which point this starts to become pretty ugly because it has to include the T and the P, which is a little awkward. It's going to be node bin entry KV and that value and tied to G and a result. Great.
02:32:13.395 - 02:33:13.485, Speaker A: At which point it's arguably not worth having this be a separate method. Probably should be. These should probably both be inlined, except insofar as this is going to be what does this compare and set reference? So this doesn't say what the mode is, which makes me think that it's going to be an acquire release. So that is the one advantage is that this is Basically the same method as compare and set, but it takes a bin by index and it doesn't require you to give the ordering because we dictate what the ordering. So that seems fine. All right, so back to where we were here. We're going to allocate a new node.
02:33:13.485 - 02:34:26.301, Speaker A: Node is going to be owned new of. Well, this is where it's a little awkward because it's going to be a bin entry. Yeah, I'm almost sure that this is going to end up being bin entry. It's a little sad. Also, this can't return next. What am I doing here? This is find so bin entry and we're going to create a bin entry and bin entry requires us. It's going to be a bin entry node because it's going to have key, it's going to have value, etc.
02:34:26.301 - 02:35:36.485, Speaker A: Key is going to be key, the value is going to be value, the hash is going to be the hash. And next, we don't know what's going to be yet, but we believe that it's going to be atomic null in the fast path. Right. So in the fast path, this is this case we want table CAS bin. See, let me. Instead of having this call be called at, let's call it be called bin, because that's really what it is. So it's going to cast bin and it's going to cast bin by doing guard bin node, guard bin is empty.
02:35:36.485 - 02:36:19.865, Speaker A: So stick us at the front. It's a little sad. Yeah, I do use that a lot. It's true. So we're going to match on this. So if this was an okay then now garbage, then we're going to assert that now garbage is null. Right.
02:36:19.865 - 02:37:25.479, Speaker A: And then we're going to return none. If it's an error, then this is like it, like changed under us. Error changed. Then we're going to set node is changed dot or say what's the value here? Current and new. So we're going to restore node because that way we can keep using it so we don't have to allocate again and stuff. And bin is now going to be change current. Right, because our compare and swap realizes someone changed the thing under us and so it told us what the new thing under us is, as we might as well take advantage of that rather than.
02:37:25.479 - 02:38:08.865, Speaker A: Rather than having to do another load. And so in this case we don't actually need to read the table again because we still now know what the head of the bin is and so we can. We don't have to continue here. We can just go down here. So this is slow path. Bin is non empty. Although this has to be a while, because imagine that the way in which things changed was that the.
02:38:08.865 - 02:38:34.805, Speaker A: Oh, I see. Imagine that the bin won't make sense. Imagine that the bin used to be non empty and now it turned empty. But I don't think that can happen. So here we can assert that changed dot current. We can just do it here. That's fine.
02:38:34.805 - 02:39:33.865, Speaker A: We know that this can't be null, so we won't lose out on calling this again. Because if it were, then that means that current is null, which we already checked is not the case or was the case. It can't. If it changed from null to null, then our cast should have succeeded. So this is just a property of how CAS works, so asserting arguably isn't necessary. Yeah, old null pointer. And this is changed.
02:39:33.865 - 02:39:51.863, Speaker A: And here we know that. Yeah, we might as well leave the assertion in. So in the slow path we know that the bin is non empty. So that is this case. So now we need to see whether it is a. Whether it's one of these forwarding businesses. Right.
02:39:51.863 - 02:40:21.365, Speaker A: So this is. If there's a resize going on, then the bin header might actually point us at a different table entirely. And if so, we're going to say why does it even assign to tab here? Oh, it doesn't reassign table. I was wrong earlier for a for loop. This is just executed on the first iteration. The one between the semicolons is the one that's executed every iteration. So it doesn't actually reread table each time.
02:40:21.365 - 02:41:42.935, Speaker A: So that actually means that we want to read table only once at the beginning, which means that the guard has to be outside, although that's fine. I see. So then this is going to be this. Which probably means this is going to have to take the guard in which the compiler might yell as a. Yell at us for. It's unclear. Yeah.
02:41:42.935 - 02:42:28.615, Speaker A: And then where else do they assign to tab? They assign to tab here. So in the slow path. So here we're going to match on. We're going to match on the bin. And so this is going to be a bin entry. And if it's a bin entry, I guess we don't have this notion of moved. Right.
02:42:28.615 - 02:43:00.945, Speaker A: So moved is one option. We don't actually know what's going to be. There's going to be something in there, but that's unimplemented for now. But we do know that this is going to be something like. That's the. This like help transfer business. So this is going to be table is equal to table.help
02:43:00.945 - 02:43:36.513, Speaker A: transfer with bin or something actually moved doesn't actually contain anything. So that's fine. What does help transfer do? Let's find help transfer. Help transfer. No, it does. It contains a next table. So that's what moved will also have to contain.
02:43:36.513 - 02:44:35.115, Speaker A: It's going to contain a. What is it going to contain? I think this is just going to contain a const to a table. I want to not have that contain anything yet. We're gonna have to figure out what this next table field is and how to store it Transient. I think it's just a reference to the next table which I think is just gonna be a const to a table. Kv I think that's what it is. I worry about making it a const because then we might lose the.
02:44:35.115 - 02:45:05.865, Speaker A: The guard tracking. This is going to be next table and there's going to be. This is going to be next table. Somehow not a fan of test driven development. You're like two hours in and no tests. I will happily write tests. Once we have get an insert, there's no reason to have tests until we have any way to do anything.
02:45:05.865 - 02:45:39.085, Speaker A: But then I'm happy to port some tests. All right. And then the more common cases that we have a node. So these are. Obviously this is going to be a ref. And if we have a node then this is interesting. Interesting.
02:45:39.085 - 02:46:10.335, Speaker A: Oh, I see there's like a fast path here where in this case the bin. The bin is not empty. So we're going to have to take the lock. But if we've been asked to only there's like a special case of we've been asked to only do an insert. If the key doesn't exist and we already have access to the first elements, we might as well check whether it's the right element. And if so just return. Return early.
02:46:10.335 - 02:47:39.755, Speaker A: And so this is like if. If absent end no dot hash equals H and no dot key equals what was it here? No dot I think just no key is going to be fine because we haven't owned. Then fast path don't need to take the law fast path. If we happen to fast path. If replacement is disallowed and first bin matches is really what this is saying. Yeah. Then we can just return immediately and say that we did not get rid of any value.
02:47:39.755 - 02:48:40.299, Speaker A: Although this option business is a little weird, it's unclear what that value should be. Because if it should be like sum, if it was present, then I guess this should be sum. I couldn't write this much rust without hitting a million impossible compile errors. Well, we haven't tried compiling it yet, so there are probably a lot of compile errors, but it's because I know the code wouldn't compile because I haven't finished writing it yet. All right, so then we get to the interesting case which is here. I guess this should really be like head node is awkward, isn't it? All right, so this is the interesting path, which is bin is not empty. Need to link into it.
02:48:40.299 - 02:49:13.691, Speaker A: So we must take the lock. And so here we're going to get to the point where we're going to need a lock in here, which is going to be probably a parking lot lock. RAW mutex. Interesting. Implement RAW mutex. What does this trait do? Perfect. That's exactly what I want.
02:49:13.691 - 02:50:25.999, Speaker A: Okay, so we're going to DO parking lot equals 0.9. And then we're going to up here use parking lot raw mutex. And this is going to contain a raw mutex. So there's just going to be a raw mutex in there. And now down here we're going to have to do head dot lock, dot lock, which means that we're going to have to do like. Let's move these up, shall we? We're going to use parking lot lock API RAW mutex. All right, so what does raw mutex actually do? It just gives us lock and unlock methods, which is I think all we want.
02:50:25.999 - 02:50:59.171, Speaker A: Although we might want a guard, it's unclear. No, I think this is going to be low level enough that we're going to have to deal with the lock manually. So notice that this is not. Oh, did I misspell it? Parking log. Nice. Good catch. Yeah, that was a typo.
02:50:59.171 - 02:51:28.071, Speaker A: This was should be head. All right, so remember that normally in Rust Mutexes wrap data. In this case we can't really do that because we have a. Well, we could, but the value would be unit. We need to be able to get to the node without taking lock for reads. And so the lock just has to be like a raw separate lock. Yeah.
02:51:28.071 - 02:52:25.855, Speaker A: So this is the else case here where you see that in Java they do synchronized F and the equivalent of that for us is to take the lock for that object, which in this case is the bin. And now we get to the check that they mentioned of. After you've taken the lock, need to check that this is still the head. Right. And so the way we're going to do that is if we're going to do, I guess, current head is table.bin bin I guard if current head and here I think we're going to have to use. We really want to compare the pointers.
02:52:25.855 - 02:52:52.335, Speaker A: So we're going to do as raw. So if it's. If it's not equal to bin as raw. Nope. Then we can unlock. And if you look at the code, really what it does is this whole thing is based on that. If.
02:52:52.335 - 02:53:15.425, Speaker A: And so in that case we can just continue. We just need to try again. Nope. Try. Try again from the start. Right. So in this case we're going to go back up here and we're going to have to.
02:53:15.425 - 02:53:42.135, Speaker A: Why is this. Yeah, that's right. We're going to have to try this whole business again. Now this actually this allocation doesn't need to happen in a loop. That seems silly. Try again from the start. So that's going to continue up here and then we're going to go through the whole business again of trying to do these insert in this say so.
02:53:42.135 - 02:54:11.375, Speaker A: Yes, it is still the head. So we can now mutate the bin. So we now own the bin. Sort of the reason I put own in double quotes there is because we don't really own it in the rust sense of owning it. Right. Because it can still be readers that are looking at this bin. Note that there can still be readers in the bin.
02:54:11.375 - 02:54:47.973, Speaker A: And so we don't really own it in the sense that we could just like drop it if we wanted, but we own it in the sense that there will be no other writers in this bin. Now the synchronize uses the intrinsic lock two bits in the object header plus runtime plummery. Yeah, exactly. So we're not going to do that. Instead we use a raw mutex from parking lot. Is there a way to do scope based locking instead of RAW unlock? There is. So this is why I said, when I said maybe we want to guard.
02:54:47.973 - 02:55:21.103, Speaker A: That's what I meant. Maybe we do want to do that. That's fine. I mean we can do that. It's easy enough. So the trick there is going to be instead of using RAW mutex, we're going to use just mutex. We're just going to use a parking lot mutex instead and then have the type B unit at which point we could sort of use the standard library mutex.
02:55:21.103 - 02:55:56.755, Speaker A: This mutex is a lot smaller, which is kind of nice. So we're going to use mutex instead of raw mutex and it's going to be a mutex that has nothing. Right. So this is the part where it's a little bit awkward and so guard we're going to lock it and now that can go away. So I agree that that is nicer. It just. It just means that we have a guard that we need to carry around.
02:55:56.755 - 02:56:20.195, Speaker A: I mean this is how all like all Rust Mutexes are generally guards. That's the reason this one was called Raw Mutex. But I agree with. This is nicer. Okay, so what do we do now? Now we have the. Now we have the lock. Why does it check it this.
02:56:20.195 - 02:57:14.045, Speaker A: Ah, so here there's like the tree bin business. So notice that up here. Oh, interesting. So here we need to handle the case where this could be a number of different types. The head here could be sort of in their parlance, could be a tree bin, could be a normal node or it could be a reservation node. We haven't really talked about reservation nodes yet. So I think currently we're just going to.
02:57:14.045 - 02:58:10.785, Speaker A: We've already checked that it's not a moved and so therefore I don't think we can be in any of these other cases. So here it's going to be a to do tree bin and reservation node. But that can't happen in our implementation in part because we haven't added those types yet. Okay, so what do we do here? We set bin count. They have this bin count business which I think is actually. What are they using bin count for? Bin count is for deciding whether or not to tree if I something. So that's interesting.
02:58:10.785 - 02:59:01.211, Speaker A: That's fine. We can maintain a bin count even though I don't think we'll actually end up needing it. This is equals one technically. And then it is. Okay, so this looks like we are going to walk. We're going to walk the bin. Yeah, we're just going to walk the bins, the nodes in the bin.
02:59:01.211 - 02:59:59.515, Speaker A: And if we find the one that matches, then we update the value and if we ever get to the end, then we just stick ours at the end. So this is the thing that tries to compare the key and this is the business where if we're at the end then we stick it in. Otherwise we just keep going. So this is going to be. While this is going to be a loop, I think just going to make this simple and make it be a loop. No, I really think this should be a node. I realize I'm going back and forth on that, but it's kind of awkward.
02:59:59.515 - 03:01:18.465, Speaker A: I think we want. We want to find on node as well which, which does this business and then this is going to do. Is going to return End find hash key guard because that way this can be an atomic node, which means this N is going to be head. And now we can have this loop just be sort of the same walking loop as in the Java code. Notice that we don't use find here because we actually want to modify the thing as we go. And so we, we want to observe every next value. If find returned none it would be basically useless to us.
03:01:18.465 - 03:01:59.795, Speaker A: So we're going to say if n. Hash is equal to node. Hash and n key is no key. They do another comparison here, but that's just. They're checking whether the pointers are equal as well, which we don't actually need to do. The quality is going to handle that for us. So it's if that's the case, update this.
03:01:59.795 - 03:03:07.615, Speaker A: The key already exists in the map. So in this case. Right, right, right, right, right. So if we are told if absent, if we're hmm, this double negation is kind of weird, but the key is not absent, so don't update. Right. It's like a weird double negation kind of. So in this case we want to update the value and the way we're going to do that is n.value.
03:03:07.615 - 03:04:06.229, Speaker A: swap because we're going to remember value here is an atomic. So we're going to swap in the real value, which means I think that when we create this value is going to be owned new value. We're going to swap in atomic here, new ord and guard. Okay, so this is going to take no value ordering. What ordering does this use? This is a volatile store. It's just going to be sequential, consistent and guard. This is now garbage and we need to figure out what to do with that garbage.
03:04:06.229 - 03:04:37.795, Speaker A: Right? We can't, we don't necessarily own it because there might be readers that have access to it, but it's going to need to be freed at some point. So we need to figure out what to do with this. It's not clear yet why not call it is absent if it looks odd. This is not isabsent. This is an argument to the function saying like only put if the key is absent. So another way to phrase this is no replace. Actually no replacement.
03:04:37.795 - 03:05:23.885, Speaker A: And so if no replacement. Yeah, that reads much better. Right. So the question here becomes what do we do with the garbage? And we don't know yet need to dispose of garbage. But we do want to return and in this case we're going to return some because the key was indeed already there. And if we do no replacement, the key was also Already there. We just didn't replace it because we were told no replacement.
03:05:23.885 - 03:06:43.609, Speaker A: Right. And then we're going to do if n next load. It's interesting because given that we took the lock, I don't think this needs to be sequentially consistent read, but we might as well do it. Next is going to be n.next. load of ordering to do this ordering can probably be relaxed due to the mutex and then we want to say if next is null, then we've reached the end of the. We've reached the end of the bin and so now we can just put in our value JavaScript specialized put if absent method for this case. Yes.
03:06:43.609 - 03:07:30.805, Speaker A: So put if mapsent is going to call this put. If you look at what they've done here, they have a put val and that is used by put and put of absent. That's why this argument is here. It's just the implementation is shared because they are so similar. So that's why that argument is there. I just wanted it to have a more descriptive name for the purpose of implementation because the variable name was just weird. All right, so if nexus null, we're at the end of the bin stick the node here and so here what do they do? They just update? They say N next.
03:07:30.805 - 03:08:25.215, Speaker A: I think it's just store probably Store. Yeah. N next store. We have the lock so we know that no one else is going to be modifying under us ordering sec const and store also takes the new which is going to be node and then we return none and we didn't produce any garbage. Why does that break? They do a break which is interesting just presumably because of this. Oh, it's because of the AD count. Hmm.
03:08:25.215 - 03:09:10.435, Speaker A: That's fine. We can. We can break instead. Ooh, that breaks the inner loop. What breaks the outer loop. Oh, I see. The if bin count is only in this else which is kind of awkward, which I think means that bin count doesn't even need to be here.
03:09:10.435 - 03:10:07.095, Speaker A: Bin count can just be here. This is going to be old VAL is going to be equal to that. And then down here is where that. Whatever that bin count thing was like this thing because that is wrapping that whole synchronized. That's so weird. Why does the this if then bin count is going to be zero, so it's not going to do anything. So it might as well just be in here.
03:10:07.095 - 03:10:48.765, Speaker A: I guess if it recurs, maybe. But these don't change bin count. I think this bin count businesses does not need to be here. Something cool to try would be to benchmark the Rust version as the Java version just to see how close you can get. Yeah, I agree, that would be interesting. I don't think this bin count business matters. You can't.
03:10:48.765 - 03:12:19.065, Speaker A: Once you enter here, which is the only place bin count can change, you will never continue the outer loop because this is going to break. And so therefore bin count is only ever set here. Yes. And so this is like if bin count not equal to zero, which it can't be at this point, then to do tree ify threshold. If old val not equals null, which it can't be. Yeah, we're going to keep looping and either we find the key, in which case we break with some, or we get to the end and we break with none. This is missing a N equals next.
03:12:19.065 - 03:13:29.225, Speaker A: So regardless, we're going to break with what the old value was. And so here we're just going to return with the old value, but instead we're going to break with old value. The reason for that is. Oh, I see what they're doing. No, no, I see what they're doing here. If the old value is sum, then we might as well just return with old value because we're not going to. We haven't added a new element and so we don't need to increment the count, which I think is equivalent to say if old value is none, then increment count, return all value.
03:13:29.225 - 03:14:55.993, Speaker A: Yeah. So here we need to do whatever ad count does and ad count does. Let's see, adds to count and if table is too small and not already resizing initiates transfer. If already resizing helps perform transfer for work is available, Rechecks occupancy after a transfer to see if another resize is already needed. Because resizings are lagging additions. Okay, so this is a whole business. What is check here? I wonder? Bin count, what is check even used for? Huh? Alright, so this is the thing that basically if we ended up adding anything to any bin, if we replace something, we don't need to call this, but if we ended up adding anything to a bin, then we call this method.
03:14:55.993 - 03:16:02.065, Speaker A: To deal with any housekeeping we need to do as a result of there being at least one more bin, which might include doing a resize. Let's see, compare and set long this base count. What does this even do? Check if less than 0. If less than 0, don't check. Resize if less than equal to 1, only check if uncontended. Interesting. So what does this do? CS is equal to counter cells.
03:16:02.065 - 03:16:59.687, Speaker A: Okay, so we need to find what counter Cells is, which is presumably a field table of counter cells. All right, so this does some stuff. Full ad count. What does full ad count mean? Ooh, is gonna. This looks like it should be its own thing. I'm a little bit tempted to sort of skip this method in the sense that, like, don't do the counter cell business and instead just do a straight up atomic U size. It will be slower and it means that we can't detect whether or not there's contention, and so we don't.
03:16:59.687 - 03:17:41.845, Speaker A: So everyone will check for resize all the time, but it will still be functionally correct, which is why I'm tempted to do that. So this is going to be. Yeah, let's do that. This is going to be self.add count one and I guess this can also include bin count. That seems fine. We do want this to do bin count plus equals one.
03:17:41.845 - 03:18:37.925, Speaker A: Bin count is a weird name for this variable. It's really like how many bins did you look at? And it uses that to determine whether you should be the one who tries to resize. Like basically how much, how much have you invested in this insert? So think of it this way. Let me see if I can sum up what bin count is used for and why. What ad count does every time you add at least one element to the map. Like if you're not replacing a value, but you're adding a value every time, you need to check whether or not a resize has to happen. However, if checking whether or not you need to do resize is a little bit expensive because you need to synchronize.
03:18:37.925 - 03:19:28.963, Speaker A: Basically if you decide that a resize has to happen, you have to check that no one else at the same time checked that a resize had to happen. And so doing that is a little bit expensive. So we want to sort of reduce the cost of this resize checking all the time. And the way we're going to do that is if you have already spent a bunch of time doing your insert, you're going to be more likely to be willing to spend the additional time on doing the resize. If you, if your insert was really fast, then we don't want you to have to do the resize do the resize check. And so therefore, if the bin count, if the bin count is one, then you only looked at the very first bin and so you didn't pay too much to do your insert. And therefore we're not going to make you do all the resize checks.
03:19:28.963 - 03:20:07.401, Speaker A: But if you looked at many bins, then you spend a bunch of time. And it's more likely that the resize is needed because only at that point I see that's what they're doing. Only if you looked at two bins does it mean that there was a bin that has two entries. It's not really a. You don't need to resize if there's just one key per bin because then the resizing wouldn't help you. It's only once you start noticing that there are multiple keys per bin that you really need to do anything. And so if the bin count is one, this bin wasn't a problem.
03:20:07.401 - 03:21:19.765, Speaker A: But the moment you notice a bin count greater than one, you really need to to consider resizing. I also realized there's a case up here where here we need to do a self addcount 1, 0. All right, so great, great, great, great, great, great. Okay, so now I have a sense for what that does. So what we need is an implementation of add count which is going to take. There's probably an ice size now that I think about it, because it might be negative and a saw bins probably takes a ref. Self probably takes something else as well.
03:21:19.765 - 03:21:55.921, Speaker A: I'd propose a variable renamed to peak count. Well, it's not really peak either because peak sort of implies things that you. That if you were iterating through the bins, if it's called peak, that would make me think that you looked ahead. But there's no look ahead here. This whole tree bin thing seems like a half assed countermeasure to a hash being predictable. Wouldn't it be easier to throw that part of the design away? Just force the hash to be unpredictable? Quite possibly. I think they also use it for.
03:21:55.921 - 03:22:39.305, Speaker A: For if bins generally get long, like you don't have to resize as often. If so, if a resize takes a long time, then while you are doing the resize you might continue doing inserts. And if those inserts just keep accumulating a list while the resize is happening, that list might get long and a tree would mean that you get to amortize that cost into log n. I agree with you, it seems not that useful. But I mean it's there. Which makes me think that maybe they've benchmarked it and found it to be useful. Do you normally use this big font sizers for the stream? It's for the stream.
03:22:39.305 - 03:23:33.245, Speaker A: All right, so ad count. Right. Where were we here? Yes. I'm not going to do this this uncontended business because it'll be good to do it But I'm just not gonna do it. I'm surprised. Um, why would check be less than zero? Just don't believe that it'll ever be less. Oh, I see, I see.
03:23:33.245 - 03:23:57.819, Speaker A: They're really abusing the second argument here. The second argument is just like not an int, it's like an option int. It's that you can say don't check, just add to the count. That's what they're saying. So. So this argument is really like. It's a.
03:23:57.819 - 03:25:16.103, Speaker A: It's basically a hint. It's a hint to ad count about whether it will want to check for resize. Resize hint. Let's call it that for now. And we're going to document a little bit saying if resize hint is none it means the caller does not want us to consider a resize which I think is also stupid. I think really it is only negative if add is negative. Quite possibly delta here is going to be a negative value positive positive long delta compute if present delta equals minus 1 so the delta can go down and you still want to check.
03:25:16.103 - 03:26:03.225, Speaker A: Okay, fine, fine, fine, fine. All right. Does not want us to consider a resize. If it is some N the caller looked at N bins during the insert. The caller saw N entries in a bin during yeah, so we definitely want to keep a count. So that's going to be up here somewhere. So we're going to have a.
03:26:03.225 - 03:29:36.255, Speaker A: What did they call it? They called it counter cells but I think we're just going to do count and that's going to be a sync atomic atomic eyesize because it can be a negative. I don't think it can be negative. This is going to be an atomic use so we're definitely going to do self.count.fetch add if n is greater than equal to zero if n is greater than zero then fetch add N as usize else See that's a good question. Is there does atomic usize have a modify like probably not fetch update? No as we really need to do this because it only takes you sizes so else if n is less than zero I'm pretty sure like eyesize has an abs to give you the absolute value of something Bullets double check ABS ABS ABS Yep account is going to be because so fetch add returns the where is fetch ad it returns the previous value. Same with fetch sub we're going to do n ABS as us it's going to be minus N like so and here it's just going to be self count load and all of these of course take an ordering and this ordering is probably going to be Acquire Release because there's no reason for it to be anything else. I don't think it needs to synchronize with anything else.
03:29:36.255 - 03:30:27.517, Speaker A: Although let's not take the risk. I can't remember and I don't want to check. Great, so this means that now we know what the current count is. Right? And if I guess we can keep the resize hint down here. If resize hint is none, then at this point we can just return. Let saw bin len equals resigned synth Unwrap. You're doing ABS twice by mistake.
03:30:27.517 - 03:30:51.991, Speaker A: You are entirely correct, sir. Or ma'am. Or it. I guess given that it's. It's a pock bin length. Yes, that is indeed what we want. And so now the question becomes how do you decide? And there's this.
03:30:51.991 - 03:31:48.095, Speaker A: I think we want like. I think we want like a to do these. I'm leaving all these to DOS for you, the viewers, maybe me in the future. But switch implement the Java counter cell business here. And so here they say that's interesting. Even if it's zero, they do this. That's kind of interesting, in fact.
03:31:48.095 - 03:32:34.165, Speaker A: I see. Even if X is 0, they still do check. It's a little surprising to me, but okay, that's fine. I prefer neither sir nor ma'am. Then how about Overlord? How about Destroyer of Worlds? Does that fit you to do all of this? Yeah. Yep, that sounds about right. Maybe using if let here would be better.
03:32:34.165 - 03:33:06.187, Speaker A: Kind of. I like early returns because it avoids having to indent a whole bunch of stuff as an exercise to the reader. Yeah, it's basically exercise to the reader. Although in this case the reader might be me later. But we'll see. All right, so what now? We need to figure out whatever this code is doing. What is SC here? Just an int.
03:33:06.187 - 03:33:44.261, Speaker A: Great. Size control. What is size ctl. This is like the control field. And this seems like something we'll want. It's going to be a field up here somewhere. I'm probably going to want like comments on basically all of these that that are similar to what the Java comments are.
03:33:44.261 - 03:34:16.685, Speaker A: Because the Java comments as we've seen are very good for this. What do they use? They use an when negative. The table is being initialized or resized minus 1 for initialization. Else might. Okay. Otherwise when table is null. Okay, so this is actually an atomic eye size because it can be negative as well.
03:34:16.685 - 03:35:55.565, Speaker A: We probably want some consts here, but is not terribly important. Okay, so down here for ad count. What's this saying? So While. That's quite the while S. What is S? S is just the count. Also, this is a return that depends on check. I worry that by CS equals counters not equal to null.
03:35:55.565 - 03:37:44.491, Speaker A: So if counter cells I want to see whether counter cells will ever be null. It doesn't seem like it because I want to see whether it's safe to skip this code. It's not clear to me because it has to do this like full ad count business. Full ad count. This seems to just be a normal count. It doesn't do anything special. And I think what this is saying is that this is a case where we don't have the initial counter cell or counter cell for this thread.
03:37:44.491 - 03:38:48.635, Speaker A: And so therefore I'm surprised that this doesn't require a resize check. This is interesting. Compare and set long this base count. Compare and set long base count. Show me base count. Base count is just a an additional field. That's interesting.
03:38:48.635 - 03:39:42.295, Speaker A: I think it's always safe to do this check. I think these are early returns and so we can basically just ignore them, which means that we won't get the optimization around. If you try to. If you only saw one bin, then you're not going to try to resize because that relies on this uncontended check, which is what this comment says as well. So I think ignoring this and just using Atomic Eyesight is fine. It does mean that we'll check for resizes more often than we need. Having to multiplex all these magic numbers into an int seems ugly.
03:39:42.295 - 03:40:44.155, Speaker A: Well, so I think this business is actually they've taken an implementation of a Java primitive which is this long adder business, and they've sort of destructured it into the program because they needed access to the contended value. And so that's why it's a little bit ugly. But ultimately it's just an efficient way to do a multithreaded counter I think is really what it's doing here. And so ultimately it's really just maintaining a counter. What's the U that shows up all over the place in the Java code base? So I think that the U is a reference to the unsafe business here at the end. Right. You see all of this? I think it's just a reference to the unsafe unsafe bits of the code base.
03:40:44.155 - 03:41:16.895, Speaker A: Oh, I'm using Arch Linux Turbo. I'm not going to answer that on stream, but if you just like tweet at me or something and I'll get back to you later. Okay, so time to implement this check. So it reads in every iteration of the loop, it reads size ctl. So s here is actually the count. They use sum, but it's effectively the count. So let's write it as a loop and then see if we can simplify.
03:41:16.895 - 03:43:04.965, Speaker A: So the loop is let sizectl is going to be self sizectl load. This is a volatile so it's set const and if count is less than psyctl and we break and this is we're not at the next resize point yet, then we're going to read table which is going to be this business and table not equals to null. If table is null, then table has not been initial. Table will be initialized by another thread anyway. So we can break, right? So that's this. That's this. And if the if table.bins.len
03:43:04.965 - 03:44:39.645, Speaker A: is greater than or equal to the maximum capacity, then we break can't resize anymore anyway. Otherwise RS is going to be. Damn it, that's not what I meant to do. Resize stamp. What is resize stamp? All right, I believe you. So this is really just a. This just take takes a u size and returns a use size and it doesn't even take a self and it's n dot leading zeros or that.
03:44:39.645 - 03:45:51.855, Speaker A: So this is going to be self resize stamp. What do they use here? N which is. Might as well just start accumulating variables of the same name as the Java version and by the resource stamp shift. And now if. What do they call that? They call this sc. So we might as well if SC is less than zero. So remember how when we read the docs for size control, if it's less than zero, it means that there's already a resize happening and the number of threads that are helping with the resize is the negative value minus one plus one.
03:45:51.855 - 03:47:02.075, Speaker A: So I assume that's what it's going to look for here. Who. Why does it need a compare and set in? Oh, of course. So if SC is equal to RS plus max resizers or SC is RS plus one or next table. I wonder why there's a why is next table a field on self here? Oh, that's interesting. So moved isn't actually going to have this pointer. This pointer is going to be here.
03:47:02.075 - 03:48:10.885, Speaker A: But then why isn't the table? And we're gonna have to figure out what that even means. Like why is next table and table separate fields? Is so they have table and next table. Oh, it's until the resize is finished. I see. So we're actually going to keep a so Next table. We might as well just keep these fields given that the Java version uses them anyway. And now next table.
03:48:10.885 - 03:48:41.555, Speaker A: So moved is just going to be table health Help transfer self. Next table Help transfer. Let's make sure we did that correctly. The F. Interesting. I wonder why the F is even necessary. I feel like it reads something out of the F.
03:48:41.555 - 03:49:23.315, Speaker A: It definitely does. This is just bizarre to me. I don't understand why this is a thing. All right, we're gonna have to. We're gonna have to figure out what this does to do. Or I guess let's make it a fix me move dot next table versus self dot next table Many question marks. Clearly here's something we have to figure out.
03:49:23.315 - 03:50:22.715, Speaker A: All right, so now back to where we were up here. So if let Next table is going to be. Actually why don't we just do if this or this or then we're going to break. Then we're going to say NT is self Next table load and that's going to be. That's a volatile load as before. And if NT is null, then we're gonna break. There's also this transfer index field which I think is another volatile field.
03:50:22.715 - 03:51:14.055, Speaker A: Next table index plus one to split while resizing. Okay, that's also gonna be an int somewhere transfer index. Also probably an atomic usize. If self.transfer index. Load it's a volatile, it's definitely not a usize because it can be negative. So it's an atomic eye size.
03:51:14.055 - 03:51:58.621, Speaker A: So that if that is less than equal to zero, then we're also going to break. Otherwise we're going to do. I see. Otherwise these checks are basically. Are we allowed to join the transfer? Remember how if there's a resize then any thread that discovers that there's a resize might go help with the resize. This is basically. Are we allowed to help resize and if any of these match, then no, we are not allowed to resize, for example, because we don't know what the next table is.
03:51:58.621 - 03:52:24.183, Speaker A: How this can be the case, I'm not entirely sure yet. Oh, this is. The resize is finished, I think otherwise we're going to try to join. So this is a compare and swap again, right? Where we're going to check that no one else has joined in the meantime. Otherwise we would have to make these checks again and we're going to join the transfer. So that. That is what we're checking now.
03:52:24.183 - 03:53:17.057, Speaker A: So can we join the transfer? Try to join. So this is going to be self.syl compare and is there a Compare and set on atomic use ice. That was only compare and swap. Yeah, I don't think there's a. Nope. All right, so this is going to be a compare and swap.
03:53:17.057 - 03:54:17.139, Speaker A: And we're gonna swap SC with SC1 with ordering Const. And if this. I forget what that returns. Compare and swap, it returns the old value. So if that is indeed sc, that means that we get to join and so we're going to do self transfer table and nt. How can multiple threads help with the resize? How is that even possible? Well, so think about it this way. When you're doing a resize, you're moving a bunch of.
03:54:17.139 - 03:55:05.565, Speaker A: Like a hash table is really just a bunch of bins. And if you do a resize, you have a new bunch of bins that are all empty, and you need to move all the stuff from the old bins to the new bins. And the way that multiple threads can do that is each thread moves a bin. And so if you have many threads moving bins, then in theory it'll go faster than if just one thread moves the bins. Does that make sense? Else, if. Interesting. So that's interesting.
03:55:05.565 - 03:55:41.505, Speaker A: Okay, so this conditional. We double check that. I got that right. Yeah. So this conditional says that we're only going to. What's that? This long cast matters. I bet you.
03:55:41.505 - 03:57:07.477, Speaker A: Yeah, something's not right here. This conditional is that if there are more elements in the map than there should be, then we're going to do a resize. And then it does a bunch of things. But. But why would more than one thread even enter this? Like, how does size control become negative in the first place? Because, like, how can this be the case? Basically, if this is the case, how did we enter this loop? Oh, if SC is negative, then S will. This will always be true. I see.
03:57:07.477 - 03:57:57.395, Speaker A: So maybe a way to rephrase this is if. Oh, I see. Okay, so this is actually pretty tricky. What? This conditional is a little weird. SC is a field that if it is positive, it is saying where you will next resize. If it is negative, then it is how many people are helping with the resize. This conditional is such that we're going to not worry about resizing if the count is less than the next resize point.
03:57:57.395 - 03:58:53.675, Speaker A: But keep in mind that if a resize is currently happening, then SC is negative, so count is not going to be less than sc. Therefore we will not go into this branch. So we won't break, so we will help with the resize. So down here, if the size control is negative, then we're going to try to join if it is zero, which I think is what this last case is going to. If it's non negative then like we noticed that our resize is needed but not ongoing. So this is ongoing resize. Can we join the resize transfer? Resize is needed but has not yet started.
03:58:53.675 - 03:59:32.895, Speaker A: And so in this case we're going to try. It's an rs, not an sc. That's what threw me off. Then we're going to do self psyctl Compare and swap sc with rs +2. And what is rs? Rs is the resize stamp. And so this is where this gets tricky. This is going to be an eye size.
03:59:32.895 - 04:00:10.493, Speaker A: Aha. The resize stamp is going to set the. And no leading zeros. Oh, this is sneaky. This is going to make the number negative because it's going to set the high bit right. The resize stamp bits is the number of bits in the stamp. So it's basically going to set the negative bit.
04:00:10.493 - 04:00:41.591, Speaker A: I think that's what that's doing. Right. There's ors with one shifted left by resize stamp bits minus one and leading zeros is a trick for. I don't know what leading zeros is used for yet. I guess we'll find out. But so the timestamp plus two I don't know. There's a plus two there.
04:00:41.591 - 04:01:49.115, Speaker A: That's not clear. So if we succeeded at that, then we're going to transfer table and this is going to be. And there is no new table yet. And then you're telling me that after all that I guess this is going to be this. Otherwise Clippy is going to end up yelling at me. And then after all that we're going to do a count is equal to this because another resize may be needed. I don't know why this +2 is here.
04:01:49.115 - 04:02:39.425, Speaker A: That's not clear to me. Like why doesn't it just set this to a constant value to do figure out why this is RS +2, not just RS, which I think really just means we need to understand better what the. What the research stamp does. All right, great. So that. So that was AD count. Oops.
04:02:39.425 - 04:04:02.835, Speaker A: So now what are we missing? Now we're just missing we need to dispose of garbage. So that is one issue we have and I think the other issue we have is this help transfer business. Because this calls help transfer these call transfer. So we need to implement transfer which takes gives a ref self and a table and a new table which is a also a shared table. This is going to be a shared null. All right, so what does transfer do? Well, okay, that's help transfer. But what does the actual transfer do? Oh, man.
04:04:02.835 - 04:04:35.247, Speaker A: But a lot of this seems to be about trees, which we can ignore. But even so. Great. So that's the thing we're gonna have to do. All right. It takes a table and a next table. And now we're gonna have to figure out what this does.
04:04:35.247 - 04:05:17.589, Speaker A: So N is gonna be table.bins.len. ah, so this is where the NCPU business comes in. If you know how many CPUs you have, you can choose a better stride. We're going to pick us. Okay, so how does this compute? NCPU is greater than 1, then n. Right. Shifted by 3 divided by the number of CPUs.
04:05:17.589 - 04:05:48.995, Speaker A: Okay. So they're basically trying to. They're basically trying to ensure that you each CPU that helps with the transfer transfers enough bins that it's worth them helping. So you don't want the number to be too small. You want enough CPUs to be able to help that it goes faster. So I think what we're just going to do here for now is say that stride to do. Use num.
04:05:48.995 - 04:06:41.005, Speaker A: CPUs for stride to help determine stride. But for now we're just going to do min transfer stride. And then if next table is null, then we are initiating a. We are initiating a resize. Try to cope with out of memory. Yeah, we're gonna deftly ignore that. This allocates.
04:06:41.005 - 04:07:26.715, Speaker A: This is the allocates a new table. Yep, that's what that does. So this is just. I think this is gonna end up being a. Then table is going to be owned. New table. New or I guess table.
04:07:26.715 - 04:08:15.484, Speaker A: And then this is going to have to do. It's going to be interesting. This is going to be. I'm going to do it in sort of the stupid way first if I can actually. Maybe, maybe this will work. So this is going to be atomic null. And I want how many of them N left shifted by 1.
04:08:15.484 - 04:08:44.615, Speaker A: N left shifted by 1. Yep. The real question becomes whether we can find a constant size for this, But I don't think so. Great. This is gonna be bins. Let's see how well that works. This may or may not work.
04:08:44.615 - 04:09:46.805, Speaker A: It's unclear. And then now that we've made this table, now we want to set self.next table store. And we're just going to store. Or is it just called set maybe on atomic. No, that's going to be a swap because this is now garbage, which we're going to have to figure out what to do with. We're going to swap that with table with ordering.
04:09:46.805 - 04:10:56.219, Speaker A: Also sequential consistency and guard, which we're gonna have to. Probably gonna have to take in a guard here swap. Right. So there's an unimplemented here for what do we do with the garbage. And then we also want transfer index equals 1 transfer index store n. Yeah. Okay, so that's sort of the initialization we need to do.
04:10:56.219 - 04:12:00.585, Speaker A: And then there's a little awkwardness here, which is like next we need to set next table, which we can only do by reading next table again here though luckily we can do relax because we already established. Because we just stored it so there's no additional ordering requirements. Vanguard next n is next table.bins.len. yeah. So now this is like forwarding node business. Well, what's curious to me is why forwarding nodes need to store which table is next because there shouldn't ever be concurrent resizes. Hmm.
04:12:00.585 - 04:12:25.005, Speaker A: Oh, I think I know. It's because the. Okay, if you do a resize any. Any bin that you move, you're going to leave behind one of these forwarding nodes. Right? But the resize might finish. In which case we're going to set. We're going to set table equals new table and new table equals null.
04:12:25.005 - 04:13:16.465, Speaker A: But there might still be threads that are operating on the old map. Right? So they will need to know to move to here even though next table has now been changed. Which is going to be a bit of a pain to deal with, but that's probably fine. All right, so the reason to do a swap for next table is that there's no need to do a compare and swap. This method is only called if we know that. Sorry, this is only called with. With next table being null if we know that we are the ones doing initializing the resize.
04:13:16.465 - 04:15:05.481, Speaker A: The compare and swap happens here without this. Sorry, without this compare and swap, this call wouldn't be made with null. So I guess technically here we could say. Note, we know that we are the only that we own this resize due to the CAS of SC in add count. All right, so down here let moot advance is true apparently and finishing is false for what? Why do they have these weird loops that have like only initializers? I guess it's because they don't have a loop construct. But then why wouldn't you just declare the variable here and new bound zero while advance? Also why is there. Why is that loop even? Oh, there's stuff after the while.
04:15:05.481 - 04:16:40.735, Speaker A: Okay, Let how is this even a thing? I minus equal one. Okay, so this is definitely an ice ice. It's at least definitely signed. So if I is greater than or equal to bound or finishing advance equals. I'm just going to write this code and then we're going to try to figure out if we. Then we can discuss and see if we can figure out what it actually does after else. Wait, but this isn't well defined.
04:16:40.735 - 04:17:12.465, Speaker A: You can get to this without NextIndex ever being assigned to. Unless. Unless Java actually always evaluates this. Like imagine that this if evaluates to true. Will Java execute this assignment? I sort of. I guess so. Otherwise this code makes no sense.
04:17:12.465 - 04:18:42.265, Speaker A: Yeah, I think Java has to execute this because otherwise next index has no defined value. So I guess let next index self transfer index dot load ordering this. So this is else if NextIndex less than or equal to 0, then I equals minus 1. Advance equals false I. Really, these conditionals are weird. I guess they're probably going to make sense once we look at the whole code, but else if transfer index. Okay, transfer index in Java we don't need to pull that trick.
04:18:42.265 - 04:20:05.335, Speaker A: This is a compare and swap from next index to. Oh man, these inline assignments can go somewhere else. Next bound is if nextindex is greater than stride, then nextindex minus stride else zero. Next indexed Next bound. I think I see what this code is doing, but it's still taking form in my head. Next index then bound is nextbound I equals next index. Advance equals false.
04:20:05.335 - 04:21:07.733, Speaker A: Okay, I think this is basically this code is trying to. It's trying to decide which range of bins this thread is going to be responsible for transferring. So the transfer index is like the next the next bin that hasn't been transferred yet. And the next bound is the end of the next range we're going to transfer. And remember how we're going to transfer bins from the end and to the front rather than front to end. So the next bound is going to be nextindex minus stride. So the stride is how many bins do we transfer at once? So if the index is currently here, so bins go down, the bound is currently the next index to transfer is currently here.
04:21:07.733 - 04:21:34.575, Speaker A: That means that all the ones below have been transferred or there are threads currently transferring them. Then we need to choose a next bound which is the other side of the range, and that's going to be the next index minus the stride. So this is the stride. That's how many bins we're going to try to transfer in one Go. And then this is like a. I guess this is really a saturating sub. So let's make that be nextindex.
04:21:34.575 - 04:22:20.265, Speaker A: Saturating sub. That won't work. So let's leave it that way. Nextindex. Stride nextbound. And if this is doing yet, if the next index is less than zero, then the resizing must have finished, right? Because if the next thing we're going to pick up is beyond the end, there's no reason to continue because all the bins below the transfer index have already been transferred. So that's this business.
04:22:20.265 - 04:23:01.085, Speaker A: Otherwise we're going to try to increment the transfer index. And if we succeed at doing that, then we own the stuff in between, right? Then we're going to transfer everything from I to bound. That becomes ours. So really, this I here as I here is super weird. It's used as sort of like a Boolean flag. So we're going to have to look at what that's doing. I don't know why this code was written the way it was, but I'm sure it's for good reason.
04:23:01.085 - 04:23:36.003, Speaker A: It always executes that assignment and returns the value of the assignment. Okay, but it will execute it even if it's not going to evaluate that condition. No, I understand. So the discussion is around this statement in particular, I understand what this does in Java. That's not the question. For those who don't know, it will assign this value to this variable and then return the value of that variable. That's not the question.
04:23:36.003 - 04:24:20.235, Speaker A: The question is, oh, I understand why they do it now. Never mind. The question is, if this condition is true, will Java evaluate this condition at all? Because normally that's not the case. Like, if this was a function call, it wouldn't call this function if this evaluated to true. But what seems to be the case is that this will be evaluated even if this condition will not be evaluated. Which seems weird. The reason I believe it must be the case is because if this is.
04:24:20.235 - 04:25:02.071, Speaker A: Oh, maybe not. Why don't these use break? Why is this code written this way? It's bizarre, man. But all right. Okay. Gonna take the liberty of rewriting this a little. This is gonna break. This is gonna break because that way we don't need.
04:25:02.071 - 04:25:41.559, Speaker A: We don't read nextindex until we know that we need to read it. We don't read nextbound till we know we're going to need it. The assignment should be evaluated if the first condition isn't true. Yeah, I think you're right. I think you're right. Does porting it to Rust one to one in this style still guarantee thread safety? So it's not actually one to one. You can't just translate the code.
04:25:41.559 - 04:26:31.485, Speaker A: And there are in general, if you write totally safe Rust code, then it's not going to go wrong. The question is going to be there are some points where we're going to need unsafe, and in those places is where we're going to have to think really carefully about whether the code as we've translated it, follows the requirements that Rust gives us. But currently we haven't written any unsafe code in any of this. So furlough is. I think you're right. I think I misread the Java code in that I thought that NextIndex would be unassigned and then was used later here, but it's not actually used later. If this evaluated to true and so it wouldn't be set, which is why I'm going to rewrite it to read this way and instead it's a little bit nicer.
04:26:31.485 - 04:27:06.925, Speaker A: I still don't know why I can be negative, but that's what they say. So it's going to break. Great. Okay, so that's the while loop. So basically this is trying to claim a region of bin bins as belonging to this thread. So let's document that. Try to claim a range of bins for us to transfer.
04:27:06.925 - 04:28:52.003, Speaker A: I don't get how this can be standard Java, to be honest. Seeing the code, it's standard Java, it's like from the jdk, like standard library. Never meet your heroes, you know? All right, if I is less than 0 or I is greater than or equal to N or I plus n is greater than or equal to next N. Do something. All right, so either. So either transfer index has gone over like beyond the first bin, or I is below or I is after the last bin or I is beyond the size of the table we're moving into. As this just seems like this conditional is if the.
04:28:52.003 - 04:29:50.685, Speaker A: If the move is invalid. Which my guess is when we look at the Java code, my guess is this is going to be like we detected that the resize is finished or that someone else has finished it for you. A lot of JDK code is very odd patterns. My guess is it's utilizing optimizations are written for the JIT optimization. Quite possibly, Quite possibly. If finishing then next table is not oh, then self next table. That's awkward here.
04:29:50.685 - 04:30:25.195, Speaker A: We're going to have to. Okay, what we're really writing here is like table equals next table like self.table self.table equals self.next table self.next table equals null none or whatever. That's basically the code we're executing.
04:30:25.195 - 04:31:13.215, Speaker A: But we can't really do that. Right. Because these are both atomics. And so what this code does is actually in Java you can do this sort of straightforward forwardly. We need to be a little bit more careful. In this case, we're going to do next table. Unclear.
04:31:13.215 - 04:31:58.135, Speaker A: It's going to be self next table dot swap. This is one of the places where Java, because of the garbage collection, can sort of get away with not dealing with certain cases. Because here we need to deal with the case where next table needs to get dropped. That won't ever happen. Sorry. Where table needs to get dropped. Interesting.
04:31:58.135 - 04:33:06.785, Speaker A: So obviously like storing null here is not hard, but what we need to make sure of is that like this, but at the same time, not really. Let me just organize my thoughts here. So setting next table to null is fine because the Java code does it. So that's fine. Now we're going to set self table in here. We're going to store next table. The one that we've been populating.
04:33:06.785 - 04:34:38.824, Speaker A: My console return here is this is going to be a swap. That's not true. This should assert that that is null. When we set next table, then there's no nor garbage is getting generated yet. But down here, this is going to be now garbage. This is the old table, which we've now finished moving all the stuff out of. Yeah, so this do something with the garbage because at this point.
04:34:38.824 - 04:35:24.806, Speaker A: And also I don't know that I'm allowed to do this. Like I think swap pointer. It is implemented for shared. Okay, great. Load consume do. I'm not familiar with crossbeam API, but it looks like you're storing null into next table then swapping table for its value. Ah, this is self.next
04:35:24.806 - 04:36:22.745, Speaker A: table. This is next table, the local variable. So this is a difference to Java where you cannot talk about fields without explicitly saying self. So this next table is the next table that we've been operating within here. And this next table is the next table of self the field. And then we need self.psictl, which we're going to just straight up store in n left shift by 1 minus n triple shift left by 1 apparently and return.
04:36:22.745 - 04:36:42.431, Speaker A: Oh, that seems promising. So I assume that only one thread will be finishing. I don't know how they're gonna. Yeah, so this compare and swap is gonna be. Is. Is the. Basically.
04:36:42.431 - 04:37:56.154, Speaker A: So only one thread is going to succeed at that Compare and swap. Yeah, and that is. That is the thread that's going to end up doing the actual table swap. This bool is only set to true for one thread partaking in the resize. Therefore this branch is only taken for one thread, particularly in the resize. All right, so if psyctl well, so this one's interesting. So this does a read interesting.
04:37:56.154 - 04:40:12.925, Speaker A: This is a load and then a compare and swap, which I feel like there's a more efficient pattern for, but maybe not. Compare and swap SC with SC1 equals SC then if, say SC2 not equal to resize, self resize, stamp of n, stamp shift and return, we are the chosen thread to finish the resize. We'll also check if we can assist with any subsequent resize. Don't know why that advance equals true is necessary, but. All right, I equals N. Why does it say recheck before commit? Note Java code says recheck before commit. Does that mean that the developer was supposed to recheck it before they committed, or is that an instruction that it going to be rechecked before committing? Isn't load and compare swap just a swap? No, not quite.
04:40:12.925 - 04:41:12.815, Speaker A: If this was a load and then a swap, two threads could both succeed. The fact that this is a load and then a compare and swap means that only one thread will succeed. With this, I. I feel an awful lot like this could be a fetch sub, but I'm not. I'm not confident enough that I understand all the invariants to guarantee that that is true. All right, so this is sort of an indicator that the resize has finished. I don't understand why advance is set to true.
04:41:12.815 - 04:42:14.295, Speaker A: Because if advance is set to true, then it's just going to break here immediately anyway and set advance to false. So I don't know that. All right, so for this else if. Oh man. Okay, then continue. Great. I wonder why they don't use continues and they use these weird like assigning else ifs instead.
04:42:14.295 - 04:43:15.105, Speaker A: F is tab at I which is just table.bin I. Although I here is a. Oh, I see here we know that I is within range because of that bounce Check. If F is null, then something then advance is table.cas bin of I with with owned null. Huh.
04:43:15.105 - 04:44:23.145, Speaker A: With shared null replace with unimplemented. This is going to be a for move. Actually this is just going to be straight up an owned new bin entry moved. We just don't quite know how moved is going to work yet with ordering cast tab was sequential and I don't think we need the guard for caspin we don't need the ordering, we need the guard. That's what it is. If this is okay. And then continue.
04:44:23.145 - 04:45:00.055, Speaker A: Now here there's a. There's an opportunity for us to reuse this BIN entry. Moved. That might be nice to do. Reuse bin entry moved where possible. I think recheck before commits means setting I equals n is done for the next loop iteration before leaving the method. That doesn't really match what the comment says, though.
04:45:00.055 - 04:45:20.105, Speaker A: It's weird. All right, we're now going to check if it's a moved. Okay, let's. First of all, let's not call this F. This is a bin. If let's. Bin entry.
04:45:20.105 - 04:46:22.635, Speaker A: Yep. Moved is equal to bin, Then what? Then advance equals true. Already processed. Not sure how that can happen, but apparently it can. Okay, so here we get to. I see. So this is going to be a match on bin, and in this case it's going to be already processed, or it's going to be a BIN entry node, which we're going to have to move.
04:46:22.635 - 04:47:06.875, Speaker A: Where's my syntax error? I have a syntax error or something. Where's my syntax error? Syntax error. Syntax error. Syntax error. 3, 4, 6. Really? 3, 4, 6. Oh.
04:47:06.875 - 04:49:17.571, Speaker A: Ah, that's awkward. What is triple shift and rust? It's a very, very good question. Shift right. But how do I do a triple shift right, which is an operation on isize, and it is specifically the one that's called shift right, but it's like shift right but keep the sign. Java triple bitshift. I'm pretty sure it's shift right, but keep the sign. The sign shift right operator shifts a bit pattern to the right Bit pattern is given by a left hand operator.
04:49:17.571 - 04:50:22.645, Speaker A: Unsigned right shift operator shifts a zero into the leftmost position. So it's an unsigned right shift. So it's specifically. That's a good question. What does a right shift on. What does a right shift on an eye size do? Give me the implementation of Shift right. That's what I want to see.
04:50:22.645 - 04:51:25.315, Speaker A: Well, that's unhelpful. I want shift right for I size. That's also pretty unhelpful. Hmm. I need to know what Rust bitwise sign shift is. Great. Let's see.
04:51:25.315 - 04:51:59.975, Speaker A: Arithmetic right shift on sign integer types. Logical right shift on unsigned right. So on an I size type, which. It's a good question, what is in here? N is N comes from here. So N is a U size. So that means that this is correct. Great.
04:51:59.975 - 04:52:43.005, Speaker A: Yep. So this is Right. Great. So now my formatting should work again. All right. So if we get a node, then it's basically the same story as what we had for insert way back up here, which is we need to take the lock and then we need to check. There we go.
04:52:43.005 - 04:55:29.095, Speaker A: Then we need to check that we still own it. So that's the same as this. So now the question goes becomes what goes here and what goes here is this code. Which does what exactly? Actually, I'm gonna run to the bathroom. Sa your apply for people who kind of judged you. It's funny, I. I also realized that I promised on Twitter that I would start taking breaks during the stream, and I, I failed to do so.
04:55:29.095 - 04:56:19.825, Speaker A: I also realized that what we should have done was probably hold off on resizing and deal with the garbage collection instead. Instead. But we sort of need to do both anyway, and the resizing logic might affect how we end up doing garbage collection. And so I think this is probably still the right strategy. All right, so what does this code do? So, first of all, we're not going to deal with the tree bin and reservation just like last time. So in fact, we're going to steal the same to do that we left up there. But now we need to do what this business is.
04:56:19.825 - 04:57:48.875, Speaker A: So this run bit. Okay, I'm going to make a guess at what this code does. My guess for what this code does is that it looks for runs of nodes whose next pointers don't have to change because any such run, it can move all of them at once. So remember how we talked about if you, when you do a resize, you need to move roughly half of the nodes in every bin, right? Because they're hash, they're like placed in a bin based on their hash modulo n, where n is the number of bins. And if n doubles, they're now placed based on hash modulo2n, which means roughly half of the hashes are going to move and roughly half of them are going to stay. If you have a node whose, whose next neighbor is not moving, then the next pointer stays the same. As you don't actually need to move, you don't need to move, you don't need to change the next pointer, so you don't need to allocate a new node.
04:57:48.875 - 04:58:57.475, Speaker A: And if you have many of these in a row, you only need to change the first one or the pointer to the first one, because all the subsequent ones are going to be right. So you only need to change the first and last in any given run. And so this is my guess here is ln and hn is Going to be last and head maybe of N. All right, so let's see. Let run bit is F H and N. Why fh? Yeah Head dot hash and N. Okay.
04:58:57.475 - 04:59:58.655, Speaker A: Oh I see all the ones that have the bit set for the current N have their nth bit set will not move in mod 2n whereas all the ones that don't will move. I think it's the logic let last run is going to be head. Uh huh. Oh, this is gonna. This is not gonna be great. I'll tell you in a second. Okay, so we're gonna look through.
04:59:58.655 - 05:01:37.437, Speaker A: Okay. We're gonna keep iterating through through them while P next not equals to null which is equivalent to is null while and next here is an atomic so is null is going to be fine. Next load set, const and guard we're going to set B Very descriptive name is P hash and n as u 64 if b not equal to run bit. Oh I see. It's actually if you find a run of things that are going to stay or or run of things that are going to move, you can move them as a chunk. So that's. So run bit is what, what what is the current run of things that we're in as in things that are moving or things that are not And B is.
05:01:37.437 - 05:02:31.485, Speaker A: Is this thing moving or not? And if those are the same then we just added to the run. If those are not the same then I guess run bit is B Then we change what run we're going to do. Last run is P. That's interesting. I'm surprised that they only ever move it looks like they only really look for one run specifically they only looked for the last run it looks like. So rather than trying to move every run, they're just trying to move the last run. At least that's the way I read this code.
05:02:31.485 - 05:02:53.675, Speaker A: Right. Otherwise it would need to be. Otherwise they would need to do work in here to move stuff. Maybe it's a good idea to open the Java and Rust code side by side. I mean I guess I could do that. Yeah. Let me split.
05:02:53.675 - 05:03:40.745, Speaker A: Split actually then I want to do that. It's a good question. How? What kind of split do I want? I definitely don't want that kind of split. I think I want. Fine, let's do this kind of split and I want concurrent hash map. What? How do I get. I thought it was VSplit.
05:03:40.745 - 05:04:14.645, Speaker A: It was V split. Weird. Yeah. Does that make you happier? All right, so you're right. That is arguably better. Although the scrolling sideways a little annoying. Okay, so I think it's only looking for the last run.
05:04:14.645 - 05:04:57.625, Speaker A: It's the only way I can make this make sense. And then it says if run bit is 0, LN the LN and HN are low and high. So the things that are going to be in the. So think of this as hash modulo the new N. You're either going to be in the low, which is your bit Modulo n is 0. Where you're going to be in the high. Your bit Modulo n is 1.
05:04:57.625 - 05:05:43.473, Speaker A: So this is going to be. If run bit is zero, that means that the run is in is going to stay in the low, in the low half. And if the bit is one, then the things in the run are going to all move. So I think we're really going to do here is. Why does it even do this? It's a very weird piece of code. Also, I wanted to. Don't know why it even does this comparison.
05:05:43.473 - 05:06:20.939, Speaker A: That seems silly. Then it does let P is F, which is head. I can just do P equals head. I can reuse that. While P not equal to last run. Also last run is this should be run bit and this should be last run. Ah, yeah, you're right.
05:06:20.939 - 05:07:00.105, Speaker A: They need to be underscored. Last run. And there's gonna be a. It's gonna be a this and I think here, this probably doesn't need to be sequential anymore, but that's fine. Just going to leave it for that like that for now. And it says ph is P dot hash let P. Interesting.
05:07:00.105 - 05:08:03.575, Speaker A: I see what it's doing. So the trick is we're gonna, as I've mentioned now several times, we're gonna split the bin into two, right? Because the bin is going to become. There's gonna be a low bin and a high. There's gonna be a low. The old bin we have is gonna turn into one low bin and one high bin. The low bin is gonna basically stay where it is and the high bin is gonna be in the new part of the range. But both of them have to move from the OMAP to the new map.
05:08:03.575 - 05:08:42.795, Speaker A: And when we. And so after we've walked through this, we're going to set two bins in the new map, right in the new table. And we need to know what to set the low bin to. And we need to know what to set the high bin to. And so that's why it does this where if the runs are of zeros, then the low bits is just going to be the last run. It's just going to be the run that we found because the run was already going to stay in the low. If the run bit is high, then the high bin is just going to be the run that already existed and the low we have to construct.
05:08:42.795 - 05:09:33.325, Speaker A: All right, so then now that makes more sense. Which is going to be mute. Lobin is going to be. Is going to be atomic null and hibin is going to be atomic null. And if the run bit is zero, tail run is going. I don't see once with one these. Once these variables have reasonable names then now if the run bit is 0, last run is all staying in the.
05:09:33.325 - 05:10:53.045, Speaker A: Is all in the low bin. And so then low bin is going to be. See, I guess this is going to be shared. It's a little weird, but sure. Then the low bin is going to be last run, else last run is all in the high bin, in which case high bin is last run. So that seems more readable to me. And now this is just going to walk all the way up to the last run and if the to the low bin else to the high bin and if it's going in the low bin, then what that means is that we're going to have to allocate a new node.
05:10:53.045 - 05:12:01.605, Speaker A: I see, yeah. So one thing that's a little bit awkward here is I think we're going to require that keys and values are clone because think of it this way, while you're moving, while you're resizing the map and moving bins, there can still be readers who are accessing the old value, but you have to create the value in the old node. But you have to create a new node to place in the new bin. And they can't be the same because they're going to have different next pointers as you need to create a new one. Which means that you have to clone the key. The value we might be able to get away with not being cloned. It just means that the two, the two instances need to share a pointer, which I think is probably going to be fine.
05:12:01.605 - 05:13:14.405, Speaker A: But the key I think is going to have to be clone. Then low bin is going to be. So here we're going to have to do owned new of node I guess I probably want for our mod mod node, I think we want to use node star just so we can use things like bin entry directly. It's just annoying to have to prefix them. So this is going to be just a straight up node and the key is going to be P key clone. So this is where the clone comes in. The hash is going To P P hash the lock is going to be just a new mutex.
05:13:14.405 - 05:14:09.975, Speaker A: So that's just going to be. I mean it can also just be mute lock clone, which is fine because the lock is over unit anyway, which is trivially cloned. The question becomes what to do with value. Um, so here's the challenge. If we don't clone the value, then we now have two nodes, two owned nodes that point to the same value. Imagine that you drop one of the nodes. Yeah, imagine you drop one of the nodes.
05:14:09.975 - 05:15:28.445, Speaker A: Then if you drop one of the nodes, you don't know whether or not to drop the value because you don't know whether the node has been duplicated due to a resize or not. So I think here we're actually going to clone the value and then we're going to. I think it's going to require some additional like smartness to get rid of that requirement. I guess next here is going to be a little bit weird because it's going to be atomic. Oops, sorry, Atomic new. Huh. Unclear actually how we do that because I, I worry that.
05:15:28.445 - 05:16:32.455, Speaker A: And we need to figure out how we're going to do this. Maybe this does have to be atomic null. But then how can it be last run? Because last run does not give you an atomic. When we walk the. When we walk the bin, this should be when we walk the bin to find a run and we find the head of a run that just gives us a shared. But we need an atomic to set the next pointer, which is sort of weird. I think it's really atomic expects to own.
05:16:32.455 - 05:17:05.157, Speaker A: Oh, unless atomic does implements from. From shared. Okay, great, so I lied. This can be shared. I think this still means that we're in A for the value. We can also just use the same garbage collection scheme that we're going to use for other nodes. So maybe it doesn't matter.
05:17:05.157 - 05:18:03.335, Speaker A: Maybe it's fine for them to share values. Does atomic implement clone? Yes. What does cloning an atomic do? If I clone an atomic a copy of the atomic value. Okay, so great, so this is actually correct then that. Yeah, that this just duplicates the sort of atomic pointer. This does mean that we probably need to take care when deleting stuff. No, I think it should be fine.
05:18:03.335 - 05:18:54.675, Speaker A: Interesting. The key type is still going to be required to be cloned here, but the value will not. Sorry, this was me walking very circularly through this. If you want me to explain it again, just let me know and I will try to do so. Now that I feel like I understand it better Myself I realized that was a little bit probably hard to follow, but it was. It was because I had to convince myself of what was right as well. So this is going to be from low bin and this is going to be from high bin.
05:18:54.675 - 05:21:46.345, Speaker A: In fact I want to simplify this code a little bit and say that this is going to be mute lobin and then this is going to be this, this is going to be this, this is going to be let node is equal to that and then not clear exactly how we. So for owned if I haven't owned can I go to shared easily into shared? It's not really what I wanted to do. Hmm, gonna have to figure that out. Yep, still going. It's true. Arguably I should stop and have dinner soon as one question is where does this. So we allocate this node but where are we going to store it? I think this does have to be an atomic but I think that just means that this is going to be an atomic from that and this is going to be an atomic from that and then this is going to be an atomic from that and then this is now going to be link is equal to and then this there actually get into place.
05:21:46.345 - 05:22:52.255, Speaker A: It's half an hour before midnight for me. What's your current time? Currently it is 5:23. I basically just want to finish this function and then I think we'll probably end the stream there because I also need to eat which means that we're going to do essentially the garbage collection aspect of this for the next stream, which I think is going to be a good fit. But let's finish the linking though because I think that's really most of what's left. So we're going to walk these and that's going to end up with the high bin and low bin being correct. And now we need to actually set the bins which is going to be. So this is going to be next table set tab at so set tab at is one where you know you own that part of the range and so therefore you can just do a store.
05:22:52.255 - 05:23:56.237, Speaker A: You don't have to do a compare and swap. Oops. Yeah, so that's going to be down here we're going to have. We have CAS bin and then we're going to do I guess store bin which is going to take an I, a new, not a guard, not a lifetime, not a result and it's just going to do a store of new with release. So we're going to do a store of I with low bin and we're going to do a store of I +N. So this is the hibin. So notice that anything.
05:23:56.237 - 05:24:53.485, Speaker A: That anything that's in the hibin is going to be sort of N offset from the previous one because some things were 0mod2n. Some things were 1 2n 1mod2n. And so they are a distance of n apart. And then table and then we need to store a. Where is it an owned new bin entry moved to there so that anyone who now does a read in the old table knows that they need to go look at the new table. True. Now there's definitely something that's off here.
05:24:53.485 - 05:25:32.667, Speaker A: I think the type here is wrong. This is an atomic. The store actually required that we give it an owned. I don't think so. Atomic store pointer implemented for shared and owned but not atomic. Hmm. Yeah.
05:25:32.667 - 05:26:38.825, Speaker A: Where this gets a little awkward is that we don't really own this atomic and so something weird is gonna happen here where. How do we turn. This is gonna be a fix me because I have to fix it. How do we turn the atomic into a owned or into an argument to store? Not actually sure how we're gonna do that. When will the next stream be? If you have any idea, it's a good question. My guess is it will be either at the end of November or it'll be like mid December, but I don't know yet. This is just transferring a single run.
05:26:38.825 - 05:27:07.569, Speaker A: No. So this. It. It only finds one run, it transfers the one run in one go and then it transfers all the other things. So this is just identifying the run and then it. Think of it as it makes the run be the start of the new bin and then it just adds all the others to the appropriate admins. I don't see any unsafe.
05:27:07.569 - 05:27:30.829, Speaker A: I guess that's all crossbeam business. There's no unsafe yet. There will be unsafe when it comes to the actual garbage collection because their crossbeam can't really help you. Or it can to a small extent, but we'll definitely have to. We have to guarantee some invariants there. But there. There's nothing unsafe in what we've done so far.
05:27:30.829 - 05:28:04.935, Speaker A: Part of the reason for that is because a crossbeam, if you have an Atomic or an Owned, it does not. It does not get freed when you drop it. Or for owned, I think it does. Like if you drop an owned, it's going to drop the inner thing. But if you drop an atomic, it will not. Just like if you drop a shared, it will not drop the inner because it doesn't know whether or not you own it will we return to the same bin on the next iteration to transfer the next run. There's.
05:28:04.935 - 05:28:50.489, Speaker A: So we don't iterate. We never touch the same bin more than once when doing these transfers. It's just when a. When a thread is doing this transfer, it will reserve a chunk of bins, it will transfer all those bins and then it will look for another chunk of bins for each bin that it transfers. It's going to look for a run at the end and then it's going to. Then it's going to create two new empty bins. It's going to take the run that it found and make that the start of one of the new bins rather than have it start from null.
05:28:50.489 - 05:29:19.085, Speaker A: And then it's going to walk everything but the run in that bin and put them. Push them to the appropriate. The appropriate output bin and then it's going to link in the two output bins. If that made sense. Do you plan on staying in the US long term? Are you mainly here for studies? I am mainly here for studies. I might be here for longer after that, but I don't know. It depends on how life works out.
05:29:19.085 - 05:30:53.925, Speaker A: So the real question here is why can't I store an atomic? That's why it's awkward. It's this reuse that is a little weird because basically this is going to be an. It's going to be a shared sort of, but it's also kind of an owned. Right? Because down here we construct something that's owned. So the real question I guess is can I turn an owned into a shared? Because that's really what I want to do here, right? I basically want to leak owned because I want to create an owned which is going to allocate and then I want to leak that owned so that it doesn't get freed because I'm going to have to free it some later time. Shared implement like from owned. No.
05:30:53.925 - 05:31:28.915, Speaker A: So given that I haven't owned, how do I go about. Is there just an into shared? Well, okay. Into shared and give it a guard. Oops. I guess so. All right. So in theory that should do the right thing.
05:31:28.915 - 05:32:22.285, Speaker A: I don't think this to do is important. Okay. I think the thing that's now missing in terms of this transfer is there was a. There was a help transfer, right? Yeah. We still need to figure out how this moved business is going to work. Yeah, it's not clear how that works. Works.
05:32:22.285 - 05:33:46.085, Speaker A: Part of the challenge with moved is that if we stick a shared in there, that's not Quite right. Because a shared a share takes a lifetime and there isn't really a lifetime associated with this. I think a moved is really just a pointer to a table I guess. Okay, I think the what's going to happen here is moved is going to hold a raw pointer to a table and that's going to be the table that the thing is moved to when the resize finishes. We know. No, that's not true. Ah, the new table the as in what that pointer points to will not be dropped until any reader that can see a moved has gone away.
05:33:46.085 - 05:34:37.761, Speaker A: I think that's true. So that's why dereferencing that is always going to be fine. I think that's going to be. I think that's going to be the way that's going to work. So I think a move that might have been hard to follow but I think think this is going to be this. I'm going to put that type in there and then once we start working with moved actually I guess moved it's going to be right now moved is going to take here next table dot as raw. What did this? This doesn't look at the key at all.
05:34:37.761 - 05:35:31.437, Speaker A: So that is equivalent. This is going to be next table as raw. Yeah, I think that's how it's going to work out. And then we're going to have to figure out forget when it when it encounters a move we're gonna have to figure out what the right thing is to do there. Basically it needs to know that it's safe to follow that moved which I think it knows because in fact where is our get? So that's find here. It's going to be in this implementation. In fact let's write that right now.
05:35:31.437 - 05:36:51.735, Speaker A: So this is going to take the pointer and here's going to be my argument. So this is going to be forward forwarding node. This find is what we're going to have to port and I'm going to argue in fact this is going to be a fix me I guess implement forwarding node colon, colon find. I'm going to argue that here it's going to be to do shared from pointer. Is that a thing that I can do impul from? Why isn't that unsafe? Oh, derefing shared is unsafe. That's going to change a lot of things. Yeah, so there's definitely some unsafe missing in the code then I assume the chaired was just always deref but that is apparently not true.
05:36:51.735 - 05:37:28.145, Speaker A: Okay, so I'm still going to write up the argument here we have a reference to the old table. We got that in the. In the current. We got that under the. Under the given guard. That means. Let's see if I can articulate this.
05:37:28.145 - 05:39:04.115, Speaker A: We have a reference to the old table, right? Because otherwise we wouldn't have a reference to self. We got that under the given guard. Since we have not yet dropped that guard, I think the argument goes something like since we haven't dropped that guard, the this table hasn't been collected. And if this table hasn't been collected, the next table definitely hasn't been collected and therefore its pointer must be valid. Since we have a guard. Since we have not yet dropped that guard, no collection has happened since then. Since this table has not been garbage collected, the later table in next table definitely hasn't.
05:39:04.115 - 05:39:44.595, Speaker A: That is my safety argument, but I think I'm too hungry to figure out whether it's true. But I think it's true. Are you porting this for Nori or just for fun? No, this is just for fun. I mean, I might end up using it in Noria, but Noria already has a fast hash map. Yes, the beginning is why we watched the stream. I'll deal with that in a second because I'm going to round off. Okay, so that is my safety argument for why constructing that shared is fine.
05:39:44.595 - 05:40:31.635, Speaker A: That is really my safety argument for why it's okay to dereference that shared, given that it turns out that dereferencing shared is what is actually unsafe. So I'm gonna. We're gonna have to spend some time thinking about that next stream of why all of those are safe. I'm surprised that that's the way the API works, but I guess it makes sense. Okay, I think we're gonna end it there. We definitely have a lot more to do, but I think now I think the resizing is like probably the hardest part of this or the hardest part to port. And then it's gonna be the actual garbage collection part of it, which is where a bunch of the unsafe is gonna come in.
05:40:31.635 - 05:41:09.845, Speaker A: So I think this is good because it's given us a good just like deep dive into the internal structure of the map. Even just how the transferring works deals a lot with that. And so I think. I think for the next stream it's going to be primarily garbage collection and we're going to have to get this forwarding to work and then it's going to be given the shared. Apparently all of their drafts are unsafe. We're going to have to walk through and figure out why. Although those are safe, but I believe they all should be.
05:41:09.845 - 05:41:43.051, Speaker A: Okay, great. So I think we're going to end it there. They're probably not that bad. 87 compile errors, not too bad. Although I think one of them just like unimportant use statements. So all the errors are really didn't come later. All right, so I think as I mentioned, next stream is probably going to be either end of November or mid December.
05:41:43.051 - 05:42:03.929, Speaker A: My guess is mid December. Then I'll be back in Norway for like a month or something. So it'll be a bit between the next streams. And yeah, next stream is going to be part two of this Is the Plan. Thanks everyone for watching. I hope it was interesting. I hope it was possible to follow towards the end.
05:42:03.929 - 05:42:35.555, Speaker A: It's always rougher towards the end because you get so deep into the details. So I think taking a break is actually a great idea because it lets both you stop and digest all the stuff we've gone through and me just distance myself from the code a little so that the next time we come back, hopefully I'll be in a better position to actually tell you about things or to actually explain things in a slightly more articulate manner. All right, thanks everyone. It's great. See you next time.
